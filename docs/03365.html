<html>
<head>
<title>Feature selection methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/january-2020-week-3-36ee066423a9?source=collection_archive---------24-----------------------#2020-01-27">https://medium.com/analytics-vidhya/january-2020-week-3-36ee066423a9?source=collection_archive---------24-----------------------#2020-01-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/885a61c95ec79592f205a987a950d61d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tKdF1ykOK6o_Tz9D"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">维多利亚诺·伊斯基耶多在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="1d71" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">特征选择是最重要的步骤之一，有助于减少特征空间。减少的特征空间导致更快的模型拟合和改进的可解释性。没有人喜欢拥有数百个特征的庞大数据集，而且并非所有的特征都是平等的。</p><ol class=""><li id="7666" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated"><strong class="ix hj">过滤方法</strong> —这些方法有助于初始数据清理和具有一些可变数据的特征选择。这意味着识别和删除常量、几乎常量和重复的变量。数字变量和分类变量有不同的方法。数值常数可以通过查看方差来识别，如果方差为0或非常低，则变量可以被认为是常数或几乎是常数。通过比较相似值的两列来识别重复特征。我相信在删除任何重复的特性之前，最好先了解一下为什么值是相同的。属于这一类别的另一种方法是相关性，它给出了两个变量之间线性关系的统计度量。这适用于数值变量的情况。该值介于(-1，1)之间，负值表示负关系，正值表示正关系。我们不想要两端都有极值的变量。对于分类变量，可以使用卡方检验，该检验基于列联表给出了预期频率和观察频率之间的观察频率和预期频率之间显著差异的信息。我们使用返回的P值，并将其与阈值进行比较，以接受或不接受假设。其中一种方法是单变量特征选择方法，该方法使用每个自变量对因变量运行预测算法，然后计算评估度量、roc-auc(分类)和mse(回归)或任何其他您想要使用的度量。然后选择提供相对于阈值的度量的变量。</li><li id="2cce" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="ix hj">包装方法— </strong>包装方法包括向前和向后步进特征选择。在向前选择步骤中，流程从单个变量开始，拟合预测模型，计算指标，并评估模型。然后添加另一个变量并计算度量。提供显著改进的变量被保留，其余的被忽略。后退一步与此方法相反。它从所有变量开始，并删除那些在拟合模型中不提供改进的变量。这是一种迭代方法。这些方法是贪婪的，并且计算量很大。但是，它们为特定的分类器或回归模型提供了最佳的功能组合。</li><li id="5549" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="ix hj">嵌入式方法— </strong>这些方法在拟合模型时被合并。它使用正则化方法来惩罚可变系数。没有提供良好信息的特征被惩罚，并且系数被减小。有两种方法可以利用，套索和山脊。套索使用L1正则化，并将可变系数减少到零。另一方面，岭回归使用L2正则化，不会将系数降低到零，但非常接近零。因此，岭不能被认为是一个可变的选择方法。这两种方法都依赖于惩罚，并且惩罚值必须由用户决定。该惩罚降低了模型拟合噪声的可能性，并提高了泛化能力。惩罚太低，特征不会减少到较低的集合，太高，模型失去预测能力。</li><li id="ab53" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="ix hj">基于树——</strong>基于树的方法以随机森林为模型，利用特征重要性的性质。随机森林返回每个特征的重要性。该特征重要性基本上是该特征在树上的平均杂质减少。对于回归来说是方差，对于分类来说是基尼系数。然而，有一个缺点。它倾向于支持高基数特征，并给予相关特征同样的重要性。因此，尽早进行检查非常重要。一旦相关特征被移除，具有较高重要性的特征将很有可能改变该值。</li></ol><p id="b0c6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所有这些方法在python库中都有实现，可以用来执行操作。</p></div></div>    
</body>
</html>