<html>
<head>
<title>FCOS — Fully Convolutional One-Stage Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">FCOS——完全卷积一级目标检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fcos-fully-convolutional-one-stage-object-detection-ede02244b5ce?source=collection_archive---------12-----------------------#2020-04-01">https://medium.com/analytics-vidhya/fcos-fully-convolutional-one-stage-object-detection-ede02244b5ce?source=collection_archive---------12-----------------------#2020-04-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="deb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有没有想过用一种相对较新的物体探测技术来启发自己？你来对地方了！阿德莱德大学的学生做了一项关于全卷积一级物体检测的研究，从此被FCOS引用。(来源在下面)</p><blockquote class="jd je jf"><p id="f8a7" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated"><strong class="ih hj">about<br/></strong>FCOS是一种以每像素方式预测图像中物体的方法。与使用边界框坐标来计算IOU的其他对象检测算法不同，FCOS高度依赖于中心度阈值和要检测的对象的数量。</p></blockquote><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es jk"><img src="../Images/6c4132a8c2bc75b394eb438e8bc3217d.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*4DAXUR9qbq6Jxv0vo1_R5w.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图1，左图:FCOS正在使用(l，t，r，b)注释。右图:不明确的示例注释</figcaption></figure><blockquote class="jd je jf"><p id="12cc" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">如左图(图1)所示，在地面真相框的监督下，FCOS通过预测4D向量(l，t，r，b)来编码每个前景像素的边界框的位置。右图显示了两个边界框内的一个位置，在这种情况下，必须选择回归哪个边界框。</p></blockquote><h2 id="6ceb" class="jw jx hi bd jy jz ka kb kc kd ke kf kg iq kh ki kj iu kk kl km iy kn ko kp kq bi translated"><strong class="ak">特色金字塔网络(FPN) </strong></h2><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kr"><img src="../Images/15b087197ff166d531228a07f9019c96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xhFGd_5EshK9y4pANxOPGQ.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图2，特征金字塔网络</figcaption></figure><p id="c9d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">FCOS利用了一个特征金字塔网络(图2)，该网络具有特征集{P3、P4、P5、P6、P7}和类c，其中c*是边界框中的对象所属的类。对于特征图上的每个点，如果该点位于地面真值包围盒内，该点将被视为<strong class="ih hj">正样本</strong>。如果一个位置落入多个边界框中，它被认为是一个<em class="jg">模糊样本</em>。当出现不明确的样本时，它将选择具有最小面积的边界框作为其回归目标。模糊样本看起来是个问题，但实际上，在应用FPN后，模糊样本可以减少到4%。</p><p id="9f79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与其他对象检测模型不同，边界框的回归目标已更改为边界框边缘和点(l，t，r，b)之间的范围，而不是轴(x，y，x，y)。通过这样做，FCOS能够挑选更多积极的样本来减少失衡。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es kw"><img src="../Images/81ac14bc67092a1924e4e5979afbe7c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*mr622CE0N4Ku2V0MumFUrw.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图3，公式回归目标</figcaption></figure><h2 id="8cc3" class="jw jx hi bd jy jz ka kb kc kd ke kf kg iq kh ki kj iu kk kl km iy kn ko kp kq bi translated"><strong class="ak">网络架构</strong></h2><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kx"><img src="../Images/798072b37c1e0e0ad3ac28e9682ca46d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0PHYn45h68G5xeYeTHFlaw.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图4，网络架构</figcaption></figure><p id="eac6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">FCOS的网络架构，其中C3、C4和C5表示主干网络的特征图，P3至P7是用于最终预测的特征层。H × W是特征图的高度和宽度。/s' (s = 8，16，…，128)。</p><h2 id="79e7" class="jw jx hi bd jy jz ka kb kc kd ke kf kg iq kh ki kj iu kk kl km iy kn ko kp kq bi translated"><strong class="ak">网络输出</strong></h2><p id="d282" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">我们网络的最后一层预测分类标签的80D向量<strong class="ih hj"> p </strong>和具有边界框坐标的4D向量<strong class="ih hj"> t </strong> = ( <em class="jg"> l，t，r，b </em>)。我们不是训练一个多类分类器，而是训练c个二元分类器，其中c是类的数量。我们在主干网络的特征映射之后增加了四个卷积层，分别用于分类和回归分支。由于回归目标总是正的，我们使用exp(x)将任何实数映射到回归分支顶部的(0，∞)。</p><h2 id="4fc6" class="jw jx hi bd jy jz ka kb kc kd ke kf kg iq kh ki kj iu kk kl km iy kn ko kp kq bi translated"><strong class="ak">居中度</strong></h2><p id="7160" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">FCOS把地面真理箱中的每一点都当作正样本。这导致远离对象中心的位置产生大量低质量的预测边界框。为了防止这种情况，他们添加了有效的索引来抑制这种被称为<strong class="ih hj">中心度</strong>的预测边界框。中心度是一个描述点到地面真值盒中心距离的指标，作为一个分支添加在特征映射之后。定义如下所示。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es ld"><img src="../Images/74d3d0cca8c7df86557476dbbc86e5dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*STZgjL_2HOHPi8F6ZrO7ow.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图5，中心性公式</figcaption></figure><p id="eb9d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">中心度的范围在0~1之间。训练时，center-ness将使用<strong class="ih hj"> BCELoss </strong>(二元交叉熵)计算Loss_center。并且当模型用于预测时，中心度可以乘以分类分数以抑制低质量的包围盒。结果，这些低质量的包围盒很有可能被最终的非最大值抑制过滤掉，从而显著提高检测性能。</p><h2 id="6fe1" class="jw jx hi bd jy jz ka kb kc kd ke kf kg iq kh ki kj iu kk kl km iy kn ko kp kq bi translated">培训结果</h2><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es le"><img src="../Images/8ecef89f79363a33b2647987821fa9d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4LipANtlnvai7LZ1niCplw.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">图6，结果</figcaption></figure><p id="8f5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图6，FCOS与其他最先进的两级或一级检测器的对比(单模型和单标度结果)。在亚太地区，FCOS在相同的基础上领先基于主播的同行retina net 2.4%。FCOS也优于最近的无锚单级检测器CornerNet，但设计复杂度低得多。</p><p id="cd51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">来源</strong></p><p id="283b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lf" href="http://openaccess.thecvf.com/content_ICCV_2019/papers/Tian_FCOS_Fully_Convolutional_One-Stage_Object_Detection_ICCV_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank">志田、沈春华、陈皓、佟大为。FCOS:完全卷积一级物体检测，2019年8月。</a></p></div></div>    
</body>
</html>