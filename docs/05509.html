<html>
<head>
<title>Fast RCNN[1504.08083]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">快速RCNN[1504.08083]</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fast-rcnn-1504-08083-d9a968a82a70?source=collection_archive---------17-----------------------#2020-04-23">https://medium.com/analytics-vidhya/fast-rcnn-1504-08083-d9a968a82a70?source=collection_archive---------17-----------------------#2020-04-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="b265" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">物体探测器的单阶段训练</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/1a8a50218b147df307197ec95322f4c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NSCa3b4UyXOerTd3.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">演职员表:<a class="ae jn" href="https://en.wikipedia.org/wiki/Object_detection" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Object_detection</a></figcaption></figure><p id="a10e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我已经计划阅读主要的物体探测论文(虽然我已经粗略地阅读了它们中的大部分，但我会详细地阅读它们，好到足以写一篇关于它们的博客)。这些论文与基于深度学习的对象检测相关。随时给建议或询问疑惑会尽我所能帮助大家。我将在下面写下每篇论文的arxiv代码，并在下面给出博客(我写的时候会不断更新)和他们论文的链接。任何从这个领域开始的人都可以跳过许多这样的论文。当我读完所有的论文后，我还会写下它们的优先级/重要性(根据理解主题的必要性)。<br/>我写这篇博客是考虑到和我相似并且仍在学习的读者。万一我犯了任何错误(我将通过从各种来源(包括博客、代码和视频)深入理解论文来尽量减少错误)，任何人都可以随意地在博客上强调它或添加评论。我已经提到了我将在博客结尾涉及的论文列表。</p><p id="5f04" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们开始吧:)</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><p id="5aff" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">快速RCNN是对RCNN和SPPNet的改进。快速RCNN引入了一种训练策略来在单个阶段中训练模型。相比之下，RCNN和SPPNet分3个阶段训练(CNN、SVM和回归器，所有这些阶段都单独训练)。</p><p id="026c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">快速RCNN使用相似的区域提议网络来提议k个区域。类似于SPPNet，CNN特征对于每个图像只计算一次。他们介绍了ROI pooling层，这将在下一节中定义。SVM分类器由softmax分类器(完全连接的层)代替，并给出num_classes+1的概率分数(1个额外的类用于背景)。输出尺寸为4*(K+1)的类似全连接层用于预测每个类的边界框偏移。VGG在这里被用作深孔。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kr"><img src="../Images/05547331b05e6d73a4a8165dfe5414b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*PcQ4FQJ6OKVD96azkyvZ9A.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">演职员表:<a class="ae jn" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1504.08083.pdf</a></figcaption></figure><h2 id="4f14" class="ks kt hi bd ku kv kw kx ky kz la lb lc jx ld le lf kb lg lh li kf lj lk ll lm bi translated">投资回报池</h2><p id="1db6" class="pw-post-body-paragraph jo jp hi jq b jr ln ij jt ju lo im jw jx lp jz ka kb lq kd ke kf lr kh ki kj hb bi translated">为了用固定维度表示每个区域提案，使用了投资回报池。这个固定维度的形状是一个超参数，我们在这里用它作为H*W。CNN架构产生的特征地图之后是ROI pooling层。假设当前ROI提议候选的边界框尺寸是h*w。该h*w特征图被划分成大约尺寸为h/H*w/W的网格</p><p id="87a2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们用一个例子来理解一下:在这个例子中，我们的H和W超参数选择2*2。假设当前建议的尺寸为5*7(外部黑色矩形)。现在每个网格的大小将是5/2*7/2。因为除法不会是一个整数，所以我们的网格会有不同的大小，如图所示。一旦我们得到了2*2的网格，每个网格元素的最大元素就是2*2的输出特征。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ls"><img src="../Images/6f8168edf9fb0dde503be026ce187b7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qa-7lY5Ka2XW3E3o.jpg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">演职员表:<a class="ae jn" href="https://deepsense.ai/region-of-interest-pooling-explained/" rel="noopener ugc nofollow" target="_blank">https://deepsense.ai/region-of-interest-pooling-explained/</a></figcaption></figure><p id="e857" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这种情况下，我们将得到输出特征映射[[0.85，0.84]，[0.97，0.96]]。该操作独立应用于特征图中的每个通道(记住，输出特征图是具有C个通道的三维，C是我们从上一个Conv层获得的通道数，在上述示例中，输出的大小为(C*2*2))。</p><p id="2165" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，我们为每个尺寸为C*H*W的区域建议提供了一个固定大小的输出。该输出被展平，之后是完全连接的层。获得的输出ROI特征图(见图1)由分类器和回归器共享。</p><h2 id="4dde" class="ks kt hi bd ku kv kw kx ky kz la lb lc jx ld le lf kb lg lh li kf lj lk ll lm bi translated">多任务丢失:</h2><p id="252b" class="pw-post-body-paragraph jo jp hi jq b jr ln ij jt ju lo im jw jx lp jz ka kb lq kd ke kf lr kh ki kj hb bi translated">快速RCNN由两个完全连接的兄弟层组成，其中一层输出类别概率，另一层输出每个类别的回归偏移量。提出了多任务损失L:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lt"><img src="../Images/977eec8eb1671466bef5ec3d4a3cd9a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*IT25kHXDpr0oMtr3NlYKZQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">多任务损失</figcaption></figure><p id="2c6a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这里u和v是地面真实类和包围盒目标。L_cls是分类损失(负对数似然)，L_loc是平滑L1损失(如下图所示)。Lambda是一个超参数，控制两个任务之间的平衡。这里，u ≥ 1表示仅当u≥1时，lambda将为1，而当u = 0时，lambda将为0，这将在背景类的情况下发生，并且没有背景类的基础事实边界框。</p><p id="ac84" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">相对于只接受分类损失训练的人，这种多任务损失有助于提高分类准确度。</p><p id="6941" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">快速RCNN的主要贡献是一阶段训练。该模型在一个阶段中被端到端地训练。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="aac5" class="lu kt hi bd ku lv lw lx ky ly lz ma lc io mb ip lf ir mc is li iu md iv ll me bi translated">论文列表:</h1><ol class=""><li id="b08c" class="mf mg hi jq b jr ln ju lo jx mh kb mi kf mj kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1312.6229.pdf" rel="noopener ugc nofollow" target="_blank"> OverFeat:使用卷积网络的综合识别、定位和检测</a>。[ <a class="ae jn" href="https://towardsdatascience.com/overfeat-review-1312-6229-4fd925f3739f" rel="noopener" target="_blank">链接到博客</a></li><li id="95fe" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">丰富的特征层次，用于精确的对象检测和语义分割(RCNN)。</a> [ <a class="ae jn" rel="noopener" href="/@sanchittanwar75/rcnn-review-1311-2524-898c3148789a">链接到博客</a> ]</li><li id="30f4" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">用于视觉识别的深度卷积网络中的空间金字塔池。</a> [ <a class="ae jn" rel="noopener" href="/@sanchittanwar75/review-spatial-pyramid-pooling-1406-4729-bfc142988dd2">链接到博客</a></li><li id="5d84" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank">快速R-CNN </a> ←你完成了这篇博客。</li><li id="6a81" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated">更快的R-CNN:用区域提议网络实现实时目标检测。【博客链接】</li><li id="e0a7" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的，实时的物体检测。</a>【博客链接】</li><li id="fdf3" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank"> SSD:单次多盒探测器</a>。[博客链接]</li><li id="36b8" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated">R-FCN:通过基于区域的完全卷积网络的目标检测。【博客链接】</li><li id="3845" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1612.03144.pdf" rel="noopener ugc nofollow" target="_blank">用于目标检测的特征金字塔网络。</a>【博客链接】</li><li id="dbcf" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1701.06659.pdf" rel="noopener ugc nofollow" target="_blank"> DSSD:解卷积单粒子探测器</a>。[博客链接]</li><li id="6866" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank">密集物体检测的焦点丢失(视网膜网)。</a>【博客链接】</li><li id="0afd" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated">YOLOv3:一种渐进的改进。[博客链接]</li><li id="00ca" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1805.09300v3.pdf" rel="noopener ugc nofollow" target="_blank">狙击手:高效多尺度训练</a>。[博客链接]</li><li id="3a5d" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1904.04514.pdf" rel="noopener ugc nofollow" target="_blank">标注像素和区域的高分辨率表示。</a>【博客链接】</li><li id="2d1a" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1904.01355v5.pdf" rel="noopener ugc nofollow" target="_blank"> FCOS:全卷积一级目标检测</a>。[博客链接]</li><li id="8074" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1904.07850.pdf" rel="noopener ugc nofollow" target="_blank">物为点</a>。[博客链接]</li><li id="d39a" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated">CornerNet-Lite:高效的基于关键点的对象检测。【博客链接】</li><li id="b226" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1904.08189v3.pdf" rel="noopener ugc nofollow" target="_blank"> CenterNet:用于对象检测的关键点三元组</a>。[博客链接]</li><li id="d84e" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated">用于实时目标检测的训练时间友好网络。【博客链接】</li><li id="e790" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1909.03625v1.pdf" rel="noopener ugc nofollow" target="_blank"> CBNet:一种用于目标检测的新型复合主干网络体系结构。</a>【博客链接】</li><li id="4263" class="mf mg hi jq b jr mo ju mp jx mq kb mr kf ms kj mk ml mm mn bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1911.09070v2.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientDet:可扩展且高效的对象检测</a>。[博客链接]</li></ol><p id="deb0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">和平…</p></div></div>    
</body>
</html>