# NLP 表示技术第一部分

> 原文：<https://medium.com/analytics-vidhya/nlp-representations-techniques-e6d69096d4ad?source=collection_archive---------30----------------------->

嗨，这是我的第一个博客，所以如果有一些错误或想法不清楚。请联系我。我一定会改变它或者做些什么。

[Linkedin](https://www.linkedin.com/in/shivam-batra-34b63a17a/):https://www . Linkedin . com/in/shivam-batra-34 b 63 a 17a/

让我们开始:

当我们想对文本应用某种算法时，经常会遇到这个问题。它可以是分类、聚类，甚至是神经网络来产生下一个单词。您已经完成了预处理，如删除多余的字符、词条化、删除停用词。现在是将这些信息编码成数字或向量的时候了。我将一个一个地解释方法，这样你可以选择最适合你的用例的方法。

# 1)包话:

这是一个非常基本的技术，即使你想一想也能想出来。基本上你有一个句子列表。

例如:

> 我喜欢这部电影
> 
> 这部电影很无聊
> 
> c)我讨厌这部电影

然后单词包首先在训练数据或语料库中创建所有单个单词的词汇表。这是在预处理之后完成的，这样我们就不用考虑语料库中常见的单词和噪音，如 hey、am、in 或一些字符、数字。

预处理之后，你的句子看起来会像这样:

> a)喜爱的电影。
> 
> b)电影无聊。
> 
> c)讨厌的电影。

词汇表应该是这样的:这通常是一个字典或集合(没有重复的值)。在这种技术中，向量或嵌入的大小等于词汇的大小。

> {喜欢:1，电影:2，无聊:3，讨厌:4}

现在我们把句子拿出来，看看这个词汇，如果有一个单词出现，我们就把它标为这个单词在句子中出现的次数。

每个句子的编码表示:

> a)我喜欢这部电影→[1，1，0，0]
> 
> b)电影很无聊→[0，1，1，0]
> 
> c)我讨厌这部电影→[0，1，01]

词汇袋的问题:

它平等地对待句子中的每个单词，有时你不想这样。让我们假设你有一个电影评论的语料库，你想在这个数据上训练一个情感分析模型。像电影、演员、导演这样的词在语料库中是如此常见，以至于你不希望你的模型将它们作为像爱、恨或退款这样的词的同等参数来学习。所以我们需要更好地表达句子，并生成一个包含更多信息的向量。

# **2) TF- IDF**

单词袋的问题可以通过使用 TFIDF 来解决。让我们看看 TFIDF，它到底是什么。这分为两个部分。

TF-术语频率

IDF-反向文档频率

词频= **句子中出现的次数/句子中的字数**

基本上这会给我们句子中每个单词的权重。在我们的例子中，术语频率看起来像这样。

> a)喜爱的电影→ [1/2 1/2 0 0]
> 
> b)电影无聊→[0 1/2 1/2 0 ]
> 
> c)讨厌的电影→ [0 1/2 0 1/2]

IDF-反向文档频率

逆文档频率= **log(句子数/包含该单词的句子数)**

这给了你每个单词相对于整个语料库的重要性

这将应用于词汇表中的每个单词，值将如下所示

> 被爱:log (3/1) →某个值
> 
> 电影:日志(3/3) →日志(1) →0
> 
> 无聊:log(3/1)→某个值
> 
> 讨厌:log(3/1)→某个值

这个想法是，非常常见的单词将获得较低的值，而不太常见的单词将获得较高的值。当我们将术语频率和逆文档频率结合起来时。我们的句子得到了更好的表达。

我们例子中的 TF-IDF 向量:

> a)喜爱的电影→[(1/2 *某个值)(1/2* 0) 0 0 ]
> 
> b)电影无聊→[0 1/2 * 0(1/2)*某值 0 ]
> 
> c)讨厌的电影→[0 1/2* 0 0 (1/2)* ]

TFIDF 方法优于单词包，因为它不仅为句子嵌入生成 1 和 0。我们得到包含更多数据信息的实数。在第 2 部分中，我解释了 TFIDF 的问题以及其他选项，比如来自 Word2Vec 的单词嵌入，以及它们是如何生成的。如果想了解更多这里是 [**Part2**](/analytics-vidhya/nlp-representations-techniques-part2-86b2fd4e04b9) **。**