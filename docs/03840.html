<html>
<head>
<title>Deep Dream: Visualizing the features learnt by Convolutional Networks in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">æ·±åº¦æ¢¦:å¯è§†åŒ–PyTorchä¸­å·ç§¯ç½‘ç»œå­¦ä¹ çš„ç‰¹å¾</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/analytics-vidhya/deep-dream-visualizing-the-features-learnt-by-convolutional-networks-in-pytorch-b7296ae3b7f?source=collection_archive---------2-----------------------#2020-02-22">https://medium.com/analytics-vidhya/deep-dream-visualizing-the-features-learnt-by-convolutional-networks-in-pytorch-b7296ae3b7f?source=collection_archive---------2-----------------------#2020-02-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="69d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">å½“æ¶‰åŠåˆ°è®¡ç®—æœºè§†è§‰ç›¸å…³ä»»åŠ¡æ—¶ï¼Œå·ç§¯ç¥ç»ç½‘ç»œ(CNN)æ˜¯æœ€æœ‰æ•ˆçš„æœºå™¨å­¦ä¹ å·¥å…·ä¹‹ä¸€ã€‚ä»–ä»¬çš„æœ‰æ•ˆæ€§å¯ä»¥ä»ä»¥ä¸‹äº‹å®æ¥è¡¡é‡:å¤§å¤šæ•°è®¡ç®—æœºè§†è§‰ç«èµ›ï¼Œå¦‚ILSVRCã€PASCAL VOCå’ŒCOCOï¼Œå·²ç»è¢«ä½¿ç”¨åŸºäºCNNçš„åˆ›æ–°æ¶æ„æ¥å®ç°å…¶ç›®æ ‡çš„å‚èµ›ä½œå“æ‰€ä¸»å¯¼ã€‚</p><p id="09b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">å› æ­¤ï¼Œè¯¢é—®â€œåœ¨ç»™å®šçš„CNNä¸­ï¼Œå„ç§è¿‡æ»¤å™¨å­¦ä¹ äº†ä»€ä¹ˆç‰¹å¾â€æ˜¯æœ‰è¶£çš„ã€‚è¿™ä¸ªé—®é¢˜ä¸ä»…ä»æ™®é€šçš„â€œå¥½å¥‡å¿ƒâ€çš„è§’åº¦æ¥çœ‹æ˜¯æœ‰è¶£çš„ï¼Œæ›´é‡è¦çš„æ˜¯çŸ¥é“è¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆå¯ä»¥ç»™æˆ‘ä»¬éå¸¸æœ‰ç”¨çš„æ´å¯ŸåŠ›æ¥æ”¹å–„æˆ‘ä»¬CNNçš„æ€§èƒ½ã€‚ä¾‹å¦‚ï¼ŒILSVRC-2013æŒ‘æˆ˜èµ›çš„è·å¥–ä½œå“(Clarifai)æ˜¯é€šè¿‡æ”¹è¿›ä¸Šä¸€å¹´çš„è·å¥–ä½œå“(AlexNet)è€Œè®¾è®¡çš„ã€‚è¿™äº›æ”¹è¿›æ˜¯é€šè¿‡åœ¨AlexNetä¸Šåº”ç”¨ç‰¹å¾å¯è§†åŒ–æŠ€æœ¯(Deconvnets)é€‰æ‹©çš„ã€‚å‚è§<a class="ae jd" href="https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf" rel="noopener ugc nofollow" target="_blank">è¿™ç¯‡</a>è®ºæ–‡(ç”±ILSVRC-2013è·å¥–è€…æ’°å†™)äº†è§£æ›´å¤šç»†èŠ‚ï¼Œæˆ–è€…<a class="ae jd" rel="noopener" href="/coinmonks/paper-review-of-zfnet-the-winner-of-ilsvlc-2013-image-classification-d1a5a0c45103">è¿™ç¯‡</a>åšå®¢è¿›è¡Œç²¾å½©è¯„è®ºã€‚</p><p id="0b9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ä¸€ç§å«åšâ€œæ¿€æ´»æœ€å¤§åŒ–â€çš„æŠ€æœ¯æ¥å¯è§†åŒ–CNNå­¦ä¹ çš„ç‰¹å¾ï¼Œè¿™ç§æŠ€æœ¯ä»ä¸€ä¸ªç”±éšæœºåˆå§‹åŒ–çš„åƒç´ ç»„æˆçš„å›¾åƒå¼€å§‹ï¼Œè¿™äº›åƒç´ çš„å€¼è¢«æ…¢æ…¢è°ƒæ•´ï¼Œä»¥æœ€å¤§åŒ–æˆ‘ä»¬å¸Œæœ›å¯è§†åŒ–çš„å±‚çš„è¾“å‡ºã€‚è¿™æ˜¯åœ¨<a class="ae jd" href="https://www.researchgate.net/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network" rel="noopener ugc nofollow" target="_blank">è¿™ç¯‡</a>è®ºæ–‡ä¸­é¦–æ¬¡ä»‹ç»çš„ï¼Œå¹¶åœ¨<a class="ae jd" href="https://arxiv.org/abs/1312.6034" rel="noopener ugc nofollow" target="_blank">è¿™ç¯‡</a>è®ºæ–‡ä¸­é¦–æ¬¡åº”ç”¨äºCNNã€‚ç„¶è€Œï¼Œå¯¹CNNçš„æ¿€æ´»æœ€å¤§åŒ–çš„å¤©çœŸåº”ç”¨å€¾å‘äºäº§ç”Ÿéå¸¸é«˜é¢‘ç‡çš„å›¾åƒï¼Œè¿™äº›å›¾åƒçœ‹èµ·æ¥ä¸€ç‚¹ä¹Ÿä¸åƒäººä»¬æ¯å¤©é‡åˆ°çš„çœŸå®ä¸–ç•Œçš„è‡ªç„¶å›¾åƒã€‚ä¾‹å¦‚ï¼Œå‚è§<a class="ae jd" href="https://distill.pub/2017/feature-visualization/#enemy-of-feature-vis" rel="noopener ugc nofollow" target="_blank">æ­¤å¤„</a>å¯¹è¯¥é—®é¢˜çš„è¯¦ç»†æè¿°ä»¥åŠè§£å†³è¿™äº›é—®é¢˜çš„å¸¸ç”¨æ–¹æ³•çš„è®¨è®ºã€‚åœ¨æœ¬å¸–ä¸­ï¼Œæˆ‘ä»¬å°†é™åˆ¶è‡ªå·±ä½¿ç”¨ä¸‰ç§ç®€å•çš„æ­£åˆ™åŒ–æŠ€æœ¯æ¥ä½¿å›¾åƒæ›´æœ‰æ„ä¹‰:</p><ol class=""><li id="d54c" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">ä»ä¸€å¼ 28 x 28çš„å°å›¾ç‰‡å¼€å§‹ï¼Œæ…¢æ…¢æ”¾å¤§åˆ°æƒ³è¦çš„å°ºå¯¸ï¼Œæ¯”å¦‚è¿™é‡Œçš„<a class="ae jd" href="https://towardsdatascience.com/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030" rel="noopener" target="_blank"/>ã€‚</li><li id="5ad5" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">æƒ©ç½šå¤§åƒç´ å€¼</li><li id="0ed0" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">ä¸åˆ©äºå›¾åƒä¸­å¤§çš„åƒç´ æ¢¯åº¦ï¼Œå³ä¸åˆ©äºç›¸é‚»åƒç´ å€¼çš„ä»»ä½•æ€¥å‰§å˜åŒ–ã€‚</li></ol><p id="409c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹å§ã€‚åŒ…å«æˆ‘å°è¯•è¿‡çš„å„ç§ä¸œè¥¿çš„å®Œæ•´ä»£ç å¯ä»¥åœ¨æˆ‘çš„<a class="ae jd" href="https://github.com/praritagarwal/Visualizing-CNN-Layers/blob/master/Activation%20Maximization.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>ä¸Šæ‰¾åˆ°ã€‚è¿™ç¯‡æ–‡ç« æ˜¯åŸºäºç¬”è®°æœ¬ä¸­çš„è¯•éªŒ#6ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘å°†è¯¦ç»†ä»‹ç»ä¸€ä¸‹ä»£ç ã€‚é¡ºä¾¿æä¸€ä¸‹ï¼Œæˆ‘æœ€è¿‘è¿˜çœ‹åˆ°äº†ä¸€ä¸ªå¾ˆæ£’çš„Kerasâ€”â€”Kerasçš„åˆ›é€ è€…Francois Cholletå®ç°äº†åŒæ ·çš„æŠ€æœ¯ã€‚æˆ‘å¼ºçƒˆæ¨èå¤§å®¶çœ‹çœ‹ä»–çš„<a class="ae jd" href="https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html" rel="noopener ugc nofollow" target="_blank">å¸–å­</a>ã€‚</p><p id="28ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">è®©æˆ‘ä»¬ä»åŠ è½½ä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹å¼€å§‹:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="d4b7" class="kb kc hi jx b fi kd ke l kf kg">import torch<br/>from torchvision import models<br/>model = models.googlenet(pretrained = True)</span></pre><p id="31a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">è™½ç„¶æˆ‘çœ‹åˆ°çš„å¤§å¤šæ•°å…³äºæ¿€æ´»æœ€å¤§åŒ–çš„åšå®¢éƒ½å€¾å‘äºä½¿ç”¨VGG16ä½œä¸ºä»–ä»¬çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«çš„åŸå› ï¼Œé™¤äº†å°è¯•ä¸€äº›ä¸åŒçš„ä¸œè¥¿ï¼Œæˆ‘å°†ä½¿ç”¨GoogLeNetã€‚è¿™ä¸ªåšå®¢ä¸­çš„å‡ ä¹æ‰€æœ‰ä»£ç éƒ½å¯ä»¥ç›´æ¥åº”ç”¨äºä»»ä½•å…¶ä»–ç»è¿‡è®­ç»ƒçš„CNNã€‚</p><p id="bb6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ç”±äºæˆ‘ä»¬æ„Ÿå…´è¶£çš„æ˜¯å¯è§†åŒ–æ¨¡å‹å·²ç»å­¦ä¹ çš„å†…å®¹ï¼Œè€Œä¸æ˜¯é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬åº”è¯¥å†»ç»“æ¨¡å‹å‚æ•°ï¼Œä»¥ä¾¿å®ƒä»¬åœ¨åå‘ä¼ æ’­æœŸé—´ä¸ä¼šæ”¹å˜ã€‚</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="809e" class="kb kc hi jx b fi kd ke l kf kg">for param in model.parameters():<br/>    param.requires_grad_(False)</span></pre><p id="5209" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">è¯·æ³¨æ„ï¼Œæ¨¡å‹ä¸­çš„å„ä¸ªå±‚å¯ä»¥é€šè¿‡èµ‹äºˆå®ƒä»¬çš„å”¯ä¸€åç§°è½»æ¾è®¿é—®ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬åˆ—å‡ºæ¨¡å‹ä¸­ä¸åŒå±‚çš„åç§°:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="95e0" class="kb kc hi jx b fi kd ke l kf kg">list(map(lambda x: x[0], model.named_children()))</span></pre><p id="82d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">åœ¨GoogLeNetä¸Šï¼Œè¿™ä¼šäº§ç”Ÿä»¥ä¸‹è¾“å‡º</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="4ccc" class="kb kc hi jx b fi kd ke l kf kg">['conv1',<br/> 'maxpool1',<br/> 'conv2',<br/> 'conv3',<br/> 'maxpool2',<br/> 'inception3a',<br/> 'inception3b',<br/> 'maxpool3',<br/> 'inception4a',<br/> 'inception4b',<br/> 'inception4c',<br/> 'inception4d',<br/> 'inception4e',<br/> 'maxpool4',<br/> 'inception5a',<br/> 'inception5b',<br/> 'avgpool',<br/> 'dropout',<br/> 'fc']</span></pre><p id="8afa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">å‡ºäºæ¼”ç¤ºç›®çš„ï¼Œæˆ‘å°†(éšæœº)é€‰æ‹©åä¸ºâ€œinception4aâ€çš„å›¾å±‚ã€‚æˆ‘ä»¬ç°åœ¨å¿…é¡»ä¸ºè¿™ä¸€å±‚æ³¨å†Œä¸€ä¸ªå‘å‰çš„é’©å­ã€‚<a class="ae jd" href="https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks" rel="noopener ugc nofollow" target="_blank">æŒ‚é’©</a>æä¾›å¯¹æ‰€éœ€å›¾å±‚çš„è¾“å‡ºå’Œgrad _ ouputçš„ç®€å•è®¿é—®ã€‚é¡¾åæ€ä¹‰ï¼Œå‘å‰é’©å­åœ¨å‘å‰ä¼ é€’æœŸé—´æ‰§è¡Œï¼Œå¹¶å…è®¸æˆ‘ä»¬æŸ¥çœ‹/ä¿®æ”¹å±‚çš„è¾“å‡ºã€‚ç±»ä¼¼åœ°ï¼Œåœ¨å‘åä¼ é€’æœŸé—´æ‰§è¡Œå‘åæŒ‚é’©ï¼Œå¹¶å…è®¸æˆ‘ä»¬æŸ¥çœ‹/ä¿®æ”¹å±‚çš„grad _ ouputã€‚æŸ¥çœ‹è¿™ä¸ª<a class="ae jd" href="https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/" rel="noopener ugc nofollow" target="_blank">åšå®¢</a>å’Œè¿™ä¸ª<a class="ae jd" href="https://www.kaggle.com/sironghuang/understanding-pytorch-hooks/notebook" rel="noopener ugc nofollow" target="_blank"> kaggleå†…æ ¸</a>ä»¥è·å¾—æ›´å¤šå…³äºé’©å­çš„ä¿¡æ¯ã€‚è¿™é‡Œçš„å®ç°åŸºäºpytorchè®¨è®ºæ¿ä¸Šçš„è¿™ä¸ª<a class="ae jd" href="https://discuss.pytorch.org/t/visualize-feature-map/29597/2" rel="noopener ugc nofollow" target="_blank">è®¨è®º</a>ã€‚ä¸ºäº†æ³¨å†Œä¸€ä¸ªå‰å‘é’©å­ï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸‹é¢çš„å·¥å‚å‡½æ•°ï¼Œå®ƒè¿”å›ä¸€ä¸ªå‡½æ•°å¯¹è±¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å®ƒä½œä¸ºé’©å­:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="1a25" class="kb kc hi jx b fi kd ke l kf kg">activation = {} # dictionary to store the activation of a layer</span><span id="4215" class="kb kc hi jx b fi kh ke l kf kg">def create_hook(name):<br/> def hook(m, i, o):<br/>   # copy the output of the given layer<br/>   activation[name] = o<br/> <br/> return hook</span></pre><p id="6fbb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æˆ‘ä»¬ç°åœ¨æ³¨å†ŒæŒ‚é’©:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="5116" class="kb kc hi jx b fi kd ke l kf kg"># register a forward hook for layer inception4a<br/>model.inception4a.register_forward_hook(create_hook(â€˜4aâ€™))</span></pre><p id="2b1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">è¯·æ³¨æ„ï¼ŒPyTorchä¸Šçš„é¢„è®­ç»ƒæ¨¡å‹è¦æ±‚è¾“å…¥å›¾åƒâ€œå¿…é¡»åŠ è½½åˆ°[0ï¼Œ1]çš„èŒƒå›´å†…ï¼Œç„¶åä½¿ç”¨<code class="du ki kj kk jx b">mean = [0.485, 0.456, 0.406]</code>å’Œ<code class="du ki kj kk jx b">std = [0.229, 0.224, 0.225]</code>è¿›è¡Œå½’ä¸€åŒ–â€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†åœ¨å›¾åƒä¸Šå®šä¹‰ä»¥ä¸‹è½¬æ¢:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="b116" class="kb kc hi jx b fi kd ke l kf kg"># normalize the input image to have appropriate mean and standard deviation as specified by pytorch</span><span id="25db" class="kb kc hi jx b fi kh ke l kf kg">from torchvision import transforms</span><span id="7471" class="kb kc hi jx b fi kh ke l kf kg">normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],<br/>                                 std=[0.229, 0.224, 0.225])</span><span id="2dd0" class="kb kc hi jx b fi kh ke l kf kg"># undo the above normalization if and when the need arises </span><span id="e823" class="kb kc hi jx b fi kh ke l kf kg">denormalize = transforms.Normalize(mean = [-0.485/0.229, -0.456/0.224, -0.406/0.225], std = [1/0.229, 1/0.224, 1/0.225] )</span></pre><p id="d92f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ç°åœ¨è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥ç”Ÿæˆç”±éšæœºåˆå§‹åŒ–çš„åƒç´ ç»„æˆçš„å›¾åƒã€‚ä¸ºäº†å…è®¸åœ¨åå‘ä¼ æ’­æœŸé—´è°ƒæ•´å›¾åƒï¼Œæˆ‘ä»¬å¿…é¡»å°†å›¾åƒçš„â€œrequires_grad_â€æ ‡å¿—è®¾ç½®ä¸ºçœŸã€‚</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="bc33" class="kb kc hi jx b fi kd ke l kf kg">import numpy as np</span><span id="258c" class="kb kc hi jx b fi kh ke l kf kg">Height = 28<br/>Width = 28</span><span id="07ec" class="kb kc hi jx b fi kh ke l kf kg"># generate a numpy array with random values<br/>img = np.single(np.random.uniform(0,1, (3, Height, Width)))</span><span id="9d3f" class="kb kc hi jx b fi kh ke l kf kg"># convert to a torch tensor, normalize, set the requires_grad_ flag<br/>im_tensor = normalize(torch.from_numpy(img)).requires_grad_(True)</span></pre><p id="6d56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">è®©æˆ‘ä»¬è¿˜å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥åè§„æ ¼åŒ–å›¾åƒï¼Œå¹¶å°†é¢œè‰²é€šé“ç§»åŠ¨åˆ°æœ€åçš„ç»´åº¦ï¼Œä»¥ä¾¿ä½¿ç”¨matplotlibçš„imshowæ˜¾ç¤ºå®ƒã€‚è¿™åœ¨ä½¿ç”¨open-cvçš„resizeæ–¹æ³•è°ƒæ•´å›¾åƒå¤§å°æ—¶ä¹Ÿå¾ˆæ–¹ä¾¿ã€‚</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="a439" class="kb kc hi jx b fi kd ke l kf kg"># function to massage img_tensor for using as input to plt.imshow()<br/>def image_converter(im):<br/>    <br/>    # move the image to cpu<br/>    im_copy = im.cpu()<br/>    <br/>    # for plt.imshow() the channel-dimension is the last<br/>    # therefore use transpose to permute axes<br/>    im_copy = denormalize(im_copy.clone().detach()).numpy()<br/>    im_copy = im_copy.transpose(1,2,0)<br/>    <br/>    # clip negative values as plt.imshow() only accepts <br/>    # floating values in range [0,1] and integers in range [0,255]<br/>    im_copy = im_copy.clip(0, 1) <br/>    <br/>    return im_copy</span></pre><p id="53bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œæˆ‘ä»¬å¸Œæœ›æƒ©ç½šå›¾åƒä¸­åƒç´ å€¼çš„ä»»ä½•æ€¥å‰§å˜åŒ–ï¼Œå³æˆ‘ä»¬å°†æƒ©ç½šå›¾åƒä¸­åƒç´ å€¼çš„xå’Œyå¯¼æ•°ã€‚è¿™å¯ä»¥é€šè¿‡ç”¨<a class="ae jd" href="https://docs.opencv.org/2.4/doc/tutorials/imgproc/imgtrans/sobel_derivatives/sobel_derivatives.html" rel="noopener ugc nofollow" target="_blank">ç´¢è´å°”æ»¤é•œ</a>æˆ–<a class="ae jd" href="https://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html?highlight=scharr#scharr" rel="noopener ugc nofollow" target="_blank">æ²™å°”æ»¤é•œ</a>åˆ›å»ºå·ç§¯å±‚æ¥å®Œæˆã€‚æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªå·ç§¯å±‚ï¼Œå®ƒå¯ä»¥æ¥å—ä»¥ä¸‹ä»»ä½•ä¸€ç§æ»¤æ³¢å™¨:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="cc16" class="kb kc hi jx b fi kd ke l kf kg">import torch.nn as nn</span><span id="8237" class="kb kc hi jx b fi kh ke l kf kg"># class to compute image gradients in pytorch<br/>class RGBgradients(nn.Module):<br/>    def __init__(self, weight): # weight is a numpy array<br/>        super().__init__()<br/>        k_height, k_width = weight.shape[1:]<br/>        # assuming that the height and width of the kernel are always odd numbers<br/>        padding_x = int((k_height-1)/2)<br/>        padding_y = int((k_width-1)/2)<br/>        <br/>        # convolutional layer with 3 in_channels and 6 out_channels <br/>        # the 3 in_channels are the color channels of the image<br/>        # for each in_channel we have 2 out_channels corresponding to the x and the y gradients<br/>        self.conv = nn.Conv2d(3, 6, (k_height, k_width), bias = False, <br/>                              padding = (padding_x, padding_y) )<br/>        # initialize the weights of the convolutional layer to be the one provided<br/>        # the weights correspond to the x/y filter for the channel in question and zeros for other channels<br/>        weight1x = np.array([weight[0], <br/>                             np.zeros((k_height, k_width)), <br/>                             np.zeros((k_height, k_width))]) # x-derivative for 1st in_channel<br/>        <br/>        weight1y = np.array([weight[1], <br/>                             np.zeros((k_height, k_width)), <br/>                             np.zeros((k_height, k_width))]) # y-derivative for 1st in_channel<br/>        <br/>        weight2x = np.array([np.zeros((k_height, k_width)),<br/>                             weight[0],<br/>                             np.zeros((k_height, k_width))]) # x-derivative for 2nd in_channel<br/>        <br/>        weight2y = np.array([np.zeros((k_height, k_width)), <br/>                             weight[1],<br/>                             np.zeros((k_height, k_width))]) # y-derivative for 2nd in_channel<br/>        <br/>        <br/>        weight3x = np.array([np.zeros((k_height, k_width)),<br/>                             np.zeros((k_height, k_width)),<br/>                             weight[0]]) # x-derivative for 3rd in_channel<br/>        <br/>        weight3y = np.array([np.zeros((k_height, k_width)),<br/>                             np.zeros((k_height, k_width)), <br/>                             weight[1]]) # y-derivative for 3rd in_channel<br/>        <br/>        weight_final = torch.from_numpy(np.array([          weight1x, weight1y, <br/>weight2x, weight2y,<br/>weight3x, weight3y])).type(torch.FloatTensor)<br/>        <br/>        if self.conv.weight.shape == weight_final.shape:<br/>            self.conv.weight = nn.Parameter(weight_final)<br/>            self.conv.weight.requires_grad_(False)<br/>        else:<br/>            print('Error: The shape of the given weights is not correct')<br/>    <br/>    # Note that a second way to define the conv. layer here would be to pass group = 3 when calling torch.nn.Conv2d<br/>    <br/>    def forward(self, x):<br/>        return self.conv(x)</span></pre><p id="f720" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">äº‹å®è¯æ˜ï¼Œå¯¹äº3 x 3å†…æ ¸ï¼ŒScharræ»¤æ³¢å™¨ä¼˜äºSobelæ»¤æ³¢å™¨ï¼Œå› æ­¤æˆ‘ä»¬å°†ä½¿ç”¨Scharræ»¤æ³¢å™¨:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="e440" class="kb kc hi jx b fi kd ke l kf kg"># Scharr Filters</span><span id="eed5" class="kb kc hi jx b fi kh ke l kf kg">filter_x = np.array([[-3, 0, 3], <br/>                     [-10, 0, 10],<br/>                     [-3, 0, 3]])</span><span id="68fa" class="kb kc hi jx b fi kh ke l kf kg">filter_y = filter_x.T<br/>grad_filters = np.array([filter_x, filter_y])</span></pre><p id="5ab5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ç°åœ¨è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªä¸Šé¢å®šä¹‰çš„å·ç§¯å±‚çš„å®ä¾‹ï¼ŒæŠŠå®ƒä¼ é€’ç»™Scharrè¿‡æ»¤å™¨ã€‚</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="2f68" class="kb kc hi jx b fi kd ke l kf kg">gradLayer = RGBgradients(grad_filters)</span></pre><p id="bfa4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">è®©æˆ‘ä»¬ä¹Ÿå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒä½¿ç”¨ä¸Šé¢å®šä¹‰çš„gradLayeræ¥è®¡ç®—è¾“å…¥å›¾åƒçš„xå’Œyå¯¼æ•°ï¼Œå¹¶è¿”å›å®ƒä»¬çš„å‡æ–¹æ ¹å€¼ã€‚</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="b511" class="kb kc hi jx b fi kd ke l kf kg"># function to compute gradient loss of an image </span><span id="e93e" class="kb kc hi jx b fi kh ke l kf kg">def grad_loss(img, beta = 1, device = 'cpu'):<br/>    <br/>    # move the gradLayer to cuda<br/>    gradLayer.to(device)</span><span id="6167" class="kb kc hi jx b fi kh ke l kf kg">    gradSq = gradLayer(img.unsqueeze(0))**2<br/>    <br/>    grad_loss = torch.pow(gradSq.mean(), beta/2)<br/>    <br/>    return grad_loss</span></pre><p id="6a8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æœ€åï¼Œè®©æˆ‘ä»¬æŠŠæ‰€æœ‰ä¸œè¥¿éƒ½æ¬åˆ°GPUä¸Šã€‚å¦‚æœä½ æ²¡æœ‰GPUæˆ–è€…ä½ æƒ³åœ¨ä½ çš„cpuä¸Šè¿›è¡Œè®¡ç®—ï¼Œä½ å¯ä»¥è·³è¿‡ä¸‹é¢çš„æ­¥éª¤ã€‚</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="cb31" class="kb kc hi jx b fi kd ke l kf kg">device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')<br/>print('Calculations being executed on {}'.format(device))</span><span id="b2ac" class="kb kc hi jx b fi kh ke l kf kg">model.to(device)<br/>img_tensor = im_tensor.to(device)</span></pre><p id="ff6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æ­£å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œæˆ‘ä»¬ä¹Ÿä¼šæ…¢æ…¢æå‡å½¢è±¡ã€‚æˆ‘ä»¬å°†ä½¿ç”¨opencvçš„resize()æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹(å¦‚æœæ‚¨æ„¿æ„ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨<a class="ae jd" href="https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Resize" rel="noopener ugc nofollow" target="_blank">torch vision . transforms . resize()</a>)ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å¯¼å…¥cv2ã€‚æˆ‘ä»¬è¿˜éœ€è¦matplotlib.pyplotå’Œtorch.optimã€‚</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="7b09" class="kb kc hi jx b fi kd ke l kf kg">import cv2<br/>from torch import optim<br/>import sys<br/>import matplotlib.pyplot as plt</span></pre><p id="7651" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æˆ‘ä»¬ç°åœ¨å‡†å¤‡è°ƒæ•´æˆ‘ä»¬çš„éšæœºå›¾åƒï¼Œä½¿å…¶æœ€å¤§åŒ–æˆ‘ä»¬å·ç§¯å±‚æ‰€é€‰èŠ‚ç‚¹çš„è¾“å‡ºã€‚å‡ºäºæœ¬æ–‡çš„ç›®çš„ï¼Œè®©æˆ‘é€‰æ‹©ç´¢å¼•å€¼ä¸º225çš„èŠ‚ç‚¹ã€‚</p><p id="11bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æˆ‘å°†ä¼˜åŒ–å›¾åƒ20æ¬¡è¿­ä»£ï¼Œç„¶åä»¥1.05çš„å› å­é‡æ–°ç¼©æ”¾ã€‚æˆ‘å°†é‡å¤è¿™ä¸ªå¾ªç¯45æ¬¡ï¼Œä»¥å¾—åˆ°æœ€ç»ˆå°ºå¯¸ä¸º249 x 249çš„å›¾åƒã€‚</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="08bb" class="kb kc hi jx b fi kd ke l kf kg">unit_idx = 225 # the neuron to visualize<br/>act_wt = 0.5 # factor by which to weigh the activation relative to the regulizer terms</span><span id="c11b" class="kb kc hi jx b fi kh ke l kf kg">upscaling_steps = 45 # no. of times to upscale<br/>upscaling_factor = 1.05<br/>optim_steps = 20# no. of times to optimize an input image before upscaling</span></pre><p id="fb15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æˆ‘ä»¬ç°åœ¨å°†è¿è¡Œä¸¤ä¸ªåµŒå¥—å¾ªç¯æ¥ä¼˜åŒ–æˆ‘ä»¬çš„å›¾åƒï¼Œç„¶åæŒ‰å¦‚ä¸‹æ–¹å¼æ”¾å¤§å®ƒ:</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="e431" class="kb kc hi jx b fi kd ke l kf kg">model.eval()<br/>for mag_epoch in range(upscaling_steps+1):<br/>    optimizer = optim.Adam([img_tensor], lr = 0.4)<br/>    <br/>    for opt_epoch in range(optim_steps):<br/>        optimizer.zero_grad()<br/>        model(img_tensor.unsqueeze(0))<br/>        layer_out = activation['4a']<br/>        rms = torch.pow((layer_out[0, unit_idx]**2).mean(), 0.5)<br/>        # terminate if rms is nan<br/>        if torch.isnan(rms):<br/>            print('Error: rms was Nan; Terminating ...')<br/>            sys.exit()<br/>        <br/>        # pixel intensity<br/>        pxl_inty = torch.pow((img_tensor**2).mean(), 0.5)<br/>        # terminate if pxl_inty is nan<br/>        if torch.isnan(pxl_inty):<br/>            print('Error: Pixel Intensity was Nan; Terminating ...')<br/>            sys.exit()<br/>            <br/>        # image gradients<br/>        im_grd = grad_loss(img_tensor, beta = 1, device = device)<br/>        # terminate is im_grd is nan<br/>        if torch.isnan(im_grd):<br/>            print('Error: image gradients were Nan; Terminating ...')<br/>            sys.exit()<br/>        <br/>        loss = -act_wt*rms + pxl_inty + im_grd        <br/>        # print activation at the beginning of each mag_epoch<br/>        if opt_epoch == 0:<br/>            print('begin mag_epoch {}, activation: {}'.format(mag_epoch, rms))<br/>        loss.backward()<br/>        optimizer.step()<br/>        <br/>    # view the result of optimising the image<br/>    print('end mag_epoch: {}, activation: {}'.format(mag_epoch, rms))<br/>    img = image_converter(img_tensor)    <br/>    plt.imshow(img)<br/>    plt.title('image at the end of mag_epoch: {}'.format(mag_epoch))<br/>    plt.show()<br/>    <br/>    img = cv2.resize(img, dsize = (0,0), <br/>                     fx = upscaling_factor, fy = upscaling_factor).transpose(2,0,1) # scale up and move the batch axis to be the first<br/>    img_tensor = normalize(torch.from_numpy(img)).to(device).requires_grad_(True)</span></pre><p id="cc55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">åœ¨ä¸Šé¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬ä¸ºæŸå¤±å‡½æ•°å®šä¹‰äº†ä¸‰ç§è´¡çŒ®:</p><ol class=""><li id="28c7" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">rms:è¿™æ˜¯æˆ‘ä»¬é€‰æ‹©çš„å·ç§¯å•å…ƒäº§ç”Ÿçš„è¾“å‡ºå¼ é‡ä¸­å…ƒç´ çš„å‡æ–¹æ ¹å€¼ã€‚æˆ‘ä»¬å¸Œæœ›æœ€å¤§åŒ–è¿™ä¸€ç‚¹ã€‚</li><li id="5b82" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">pxl_inty:è¿™æ˜¯æˆ‘ä»¬å›¾åƒä¸­åƒç´ å€¼çš„å‡æ–¹æ ¹å€¼ã€‚å‡ºäºæ­£åˆ™åŒ–çš„ç›®çš„ï¼Œæˆ‘ä»¬å¸Œæœ›æƒ©ç½šå¤§åƒç´ å€¼ï¼Œä»è€Œä¿æŒpxl_intyè¾ƒä½ã€‚</li><li id="4f00" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">im_grd:è¿™æ˜¯åƒç´ å€¼çš„xå’Œyå¯¼æ•°çš„å‡æ–¹æ ¹å€¼ã€‚é€šè¿‡ä¿æŒä½ç”µå¹³ï¼Œæˆ‘ä»¬å¯ä»¥ç¡®ä¿åƒç´ å€¼ä¸ä¼šå‘ç”Ÿæ€¥å‰§å˜åŒ–ã€‚</li></ol><p id="88db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">å› æ­¤ï¼ŒæŸå¤±å‡½æ•°ç”±ä¸‹å¼ç»™å‡º</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="806c" class="kb kc hi jx b fi kd ke l kf kg">loss = -act_wt*rms + pxl_inty + im_grd</span></pre><p id="ea3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">å…¶ä¸­â€˜act _ wtâ€™æ˜¯æˆ‘ä»¬åˆ†é…ç»™â€˜rmsâ€™ç›¸å¯¹äºpxl_intyå’Œim_grdçš„æƒé‡ã€‚å› æ­¤ï¼Œä¸å›¾åƒä¸­çš„åƒç´ å¼ºåº¦å’Œæ¢¯åº¦ç›¸æ¯”ï¼Œæ”¹å˜act_wtæ”¹å˜äº†å•å…ƒæ¿€æ´»çš„é‡è¦æ€§ã€‚æˆ‘ä»¬è¿˜æ£€æŸ¥åœ¨è¿­ä»£çš„ä»»ä½•ä¸€ç‚¹ï¼Œè¿™äº›æ˜¯å¦æˆä¸ºnanï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ç»ˆæ­¢ä»£ç ã€‚</p><p id="08e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">æ³¨æ„ï¼Œåœ¨å¤–éƒ¨å¾ªç¯ä¸­ï¼Œå³æ”¾å¤§å¾ªç¯ä¸­ï¼Œæ¯æ¬¡æˆ‘ä»¬è°ƒæ•´å›¾åƒå°ºå¯¸æ—¶ï¼Œæˆ‘ä»¬éƒ½ç”Ÿæˆæ–°çš„img_tensorï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨æ¯ä¸ªæ”¾å¤§æ—¶æœŸçš„å¼€å§‹é‡å»ºæˆ‘ä»¬çš„ä¼˜åŒ–å™¨ã€‚</p><p id="946b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ç§å•Šã€‚æˆ‘ä»¬å®Œäº†ã€‚æ‰§è¡Œä¸Šé¢çš„å¾ªç¯ä¼šç”Ÿæˆä»¥ä¸‹å›¾åƒ:</p><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kl"><img src="../Images/edca18b03cf610fd5a6cbfb8b56bdecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oF5cLsqxUWBGhn7fo8F0UA.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">å·¦ä¸Šè‡³å³ä¸‹:ç¬¬0ã€ç¬¬9ã€ç¬¬18ã€ç¬¬27ã€ç¬¬36å’Œç¬¬45å€æ”¾å¤§ç»“æŸæ—¶çš„å›¾åƒ</figcaption></figure><p id="315e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">å¦‚æœæˆ‘æ²¡æœ‰åè§çš„è¯ï¼Œé‚£ä¹ˆè¿™æ ·äº§ç”Ÿçš„æœ€ç»ˆå›¾åƒä¼¼ä¹åŒ…å«äº†å¾ˆå¤šçœ¼ç›ä¸€æ ·çš„ç‰¹å¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æ¨æ–­ï¼Œè¿™é‡Œè®¨è®ºçš„å·ç§¯å•å…ƒå¿…é¡»åœ¨è¾“å…¥å›¾åƒä¸­å¯»æ‰¾â€œçœ¼ç›â€ã€‚çœ‹çœ‹å…¶ä»–å·ç§¯èŠ‚ç‚¹æœ€ç»ˆå­¦åˆ°äº†ä»€ä¹ˆå°†ä¼šå¾ˆæœ‰è¶£ã€‚ä»¥ä¸‹æ˜¯æ¯å±‚ä¸­å‰10ä¸ªæœ€æ´»è·ƒå•å…ƒçš„å¯è§†åŒ–æ•ˆæœ:</p><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/3da79c52ff8da65aef2eaf5a62f7e79a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M1HiWcAu5q7C62UrBl0FTg.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/cfa679328916796b3cd5991b1401acd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*94Yw7quhXdD8Mxm6quE-XQ.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/033e9476f2e30be5826b0f471378e772.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*smH55tJ8hHskjzktr88zJQ.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/9f58eb18c522fce8ef17bf2bd0a8ba75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gu-8cx4gjtd_kWpjtiHSRg.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/ee2ffa7ad53642f04edc4655eac171b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kobsTCX7EvEw1FfwiSxhFg.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/d29d34b700ccf3fa9eb39aa5203ffaaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DxyncEpHmKfCZPqnvuss3w.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/9e2b7d98262bcbc6d423c60be8aeefb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y6GPhiXOjZZnHsL84ODWuA.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/0d1914207e662d50326dcf1143b6e2ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZX653-cNFXXH7WLx5SJhYg.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/5912e2cac87bd0c26eb4af3cb7796153.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XHoOlUePRONlS-FXuxY-8w.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/7fd3d25003b1cdeb03b69053b2079549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EFiSse8e_HDQAHhvcGh9gw.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es ky"><img src="../Images/3876ebbc9c50c51010502f227fd88c54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uGzwGx49GDT03ydxf9Q80w.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/9e35269969fe4da27371dff7d511d47a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l6ANyUXT9NaIdajGi6USzg.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/479659902efa186d84a9812a8a01ddc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fh3NB1YQCtbNrcnSKsu_WQ.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/c5922c96e3ff9581c987f2e90f883610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*THCmRe9rC02YAOg19HLMlw.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/9ea7c9a05965bc54cbac72af745a56ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hGAvFIFfxg5wGkJifoZJig.png"/></div></div></figure><figure class="js jt ju jv fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kz"><img src="../Images/2994d678c86eb0cbdf95d742c17aded0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VEo71-qOPiLTDJrVgQlt8Q.png"/></div></div></figure><p id="1bf6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">çœ‹èµ·æ¥CNNä¸­çš„å¤§å¤šæ•°å•ä½æœ€ç»ˆéƒ½å­¦ä¹ äº†ä¸åŒç§ç±»çš„çº¹ç†ã€‚å¶å°”ï¼Œæœ‰ä¸€äº›å•ä½ä¼šå­¦ä¹ é¢éƒ¨ç‰¹å¾ï¼Œæ¯”å¦‚çœ¼ç›ç­‰ç­‰ã€‚æˆ‘ä¸çŸ¥é“ä¸ºä»€ä¹ˆï¼Œä½†å¯¹æˆ‘æ¥è¯´ï¼Œå®ƒçœ‹èµ·æ¥åƒåœ¨ç¬¬4aå±‚-ç¬¬4eå±‚å’Œç¬¬5aå±‚çš„å•ä½æœ‰æœ€å¯è¾¨åˆ«çš„ç‰¹å¾ã€‚åœ¨è®¸å¤šåœ°æ–¹éƒ½æœ‰äººè®¤ä¸ºï¼ŒCNNçš„é«˜å±‚æœ€ç»ˆä¼šå­¦ä¹ ç”¨äºè®­ç»ƒçš„å›¾åƒçš„å†…å®¹ï¼Œè€Œä½å±‚åˆ™ç›¸åï¼Œå®ƒä»¬æœ€ç»ˆä¼šå­¦ä¹ å›¾åƒçš„çº¹ç†ã€‚ä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œæˆ‘å¸Œæœ›æœ€åä¸€å±‚ï¼Œå³inception5bèƒ½å¤Ÿäº§ç”ŸåŒ…å«é«˜åº¦æ˜æ˜¾çš„äººç±»å¯è§£é‡Šç»„ä»¶çš„å›¾åƒã€‚ç„¶è€Œï¼Œå¯¹äºä¸»è¦åŒ…å«éå¸¸é«˜é¢‘ç‡æ¨¡å¼çš„ç›¸åº”å›¾åƒæ¥è¯´ï¼Œæƒ…å†µä¼¼ä¹å¹¶éå¦‚æ­¤ã€‚ä¹Ÿè®¸ï¼Œæˆ‘åº”è¯¥å°è¯•ä¸€ä¸ªå¤§äº3 x 3æ»¤é•œçš„æ¸å˜å›¾å±‚ã€‚Mahendranå’ŒVedaldiä¹Ÿä¸»å¼ ä½¿ç”¨æŠ–åŠ¨æ¥è§„èŒƒè¿™äº›é«˜é¢‘æ¨¡å¼çš„å‡ºç°ã€‚è¿™æ˜¯æˆ‘æ²¡æœ‰åŒ…æ‹¬åœ¨å†…çš„ä¸œè¥¿ï¼Œä½†å°è¯•ä¸€ä¸‹ä¼šå¾ˆæœ‰è¶£ã€‚</p><p id="0c15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">å¸Œæœ›ä½ ä¼šå’Œæˆ‘ä¸€æ ·ç©å¾—å¼€å¿ƒã€‚ğŸ˜ƒ</p></div></div>    
</body>
</html>