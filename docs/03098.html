<html>
<head>
<title>Target Encoding Vs. One-hot Encoding with Simple Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">目标编码与一键编码的简单示例</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/target-encoding-vs-one-hot-encoding-with-simple-examples-276a7e7b3e64?source=collection_archive---------0-----------------------#2020-01-16">https://medium.com/analytics-vidhya/target-encoding-vs-one-hot-encoding-with-simple-examples-276a7e7b3e64?source=collection_archive---------0-----------------------#2020-01-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="dfc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于机器学习算法来说，分类数据可能非常有用。然而，在它的原始形式下，大多数模型都无法识别它。为了解决这个问题，我们可以使用不同的“编码”技术来使我们的分类数据清晰易读。</p><p id="03b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本帖中，我们将介绍数据科学模型中使用的两种常见编码技术— <a class="ae jd" href="https://contrib.scikit-learn.org/categorical-encoding/targetencoder.html" rel="noopener ugc nofollow" target="_blank">目标编码</a>和<a class="ae jd" href="https://contrib.scikit-learn.org/categorical-encoding/onehot.html" rel="noopener ugc nofollow" target="_blank">一键编码</a>。</p><h1 id="427d" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">目标编码</h1><p id="f65b" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">根据上面链接的文档，目标编码被定义为</p><blockquote class="kh ki kj"><p id="d1d8" class="if ig kk ih b ii ij ik il im in io ip kl ir is it km iv iw ix kn iz ja jb jc hb bi translated">“用给定特定分类值的目标的后验概率和目标在所有训练数据上的先验概率的混合来替换特征。”</p></blockquote><h2 id="1d3f" class="ko jf hi bd jg kp kq kr jk ks kt ku jo iq kv kw js iu kx ky jw iy kz la ka lb bi translated">示例—目标编码</h2><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es lc"><img src="../Images/2265149981edcae9a419cd1e8ec7fb87.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*W77md1OC9HSuAFy9b0LEIw.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">表1:具有目标编码动物值的数据帧</figcaption></figure><p id="0ea0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好地理解这意味着什么，让我们看一个例子。在表1中，我们在“动物”列中有分类数据，在“目标”列中有二元目标。在最后一列，我们有编码的动物值。那么我们是如何到达那里的呢？</p><ol class=""><li id="8fed" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated">按每个类别对数据进行分组，并统计每个目标的出现次数(表2)。</li><li id="331b" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">接下来，计算给定每个特定“动物群体”的目标1出现的概率当我们这样做时，我们在表2中得到以下值:</li></ol><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mc"><img src="../Images/2e641be28dd573295048b8492734357f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*ldULQ2FIPhkxIo56YObrfg.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">表2:显示目标编码如何计算概率的简化表格</figcaption></figure><p id="b125" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.最后，在新列中加回，它给出了每个动物组的概率值。这显示在第一个数据帧中(表1)。现在你有了一个代表‘动物’特征的数值，可以被机器学习算法识别。</p><p id="c9a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，当您在sklearn中进行目标编码时，您的值可能会与使用上述方法获得的值略有不同。这是因为我们到目前为止只考虑了后验概率。Sklearn还查看先验概率，在这种情况下是目标为1的概率。在这种情况下，这个概率是0.5，因为我们有一半的时间目标等于1。然后，Sklearn使用该度量来帮助平滑编码值，以便不会给依赖于目标的目标编码特征太多权重。</p><h2 id="65aa" class="ko jf hi bd jg kp kq kr jk ks kt ku jo iq kv kw js iu kx ky jw iy kz la ka lb bi translated">这在代码中是什么样子的</h2><p id="5ff2" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在下面的代码中,‘category _ encoders’库用于快速完成目标编码(如上所述，不是手动完成的)。</p><ol class=""><li id="9b78" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated">导入库</li></ol><pre class="ld le lf lg fd md me mf mg aw mh bi"><span id="d5c5" class="ko jf hi me b fi mi mj l mk ml">import pandas as pd<br/>from category_encoders import TargetEncoder</span></pre><p id="bb67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.目标编码和清理数据帧</p><pre class="ld le lf lg fd md me mf mg aw mh bi"><span id="9f35" class="ko jf hi me b fi mi mj l mk ml">encoder = TargetEncoder()<br/>df['Animal Encoded'] = encoder.fit_transform(df['Animal'], df['Target'])</span></pre><p id="8720" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将输出表1中的数据帧。这在多个分类特征中也是可重复的。</p><h2 id="597f" class="ko jf hi bd jg kp kq kr jk ks kt ku jo iq kv kw js iu kx ky jw iy kz la ka lb bi translated">目标编码的好处</h2><p id="1128" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">目标编码是一种简单快速的编码方法，不会增加数据集的维数。因此，它可以作为一个良好的首次尝试编码方法。</p><h2 id="e0d1" class="ko jf hi bd jg kp kq kr jk ks kt ku jo iq kv kw js iu kx ky jw iy kz la ka lb bi translated">目标编码的局限性</h2><p id="fdd9" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">目标编码依赖于目标的分布，这意味着目标编码需要仔细验证，因为它可能容易过度拟合。这种方法也是特定于数据集的，只会在某些时候显示出显著的改进。</p><h1 id="974e" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">一键编码</h1><p id="6118" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">一键编码在概念上更容易理解。这种类型的编码只是“每个类别产生一个特征，每个二进制。”或者对于上面的例子，为猫、狗和仓鼠创建一个新特征。例如，在cat一栏中，我们表明一只猫以1存在，而它不以0存在。让我们看同一个例子来更好地理解这一点:</p><h2 id="3f60" class="ko jf hi bd jg kp kq kr jk ks kt ku jo iq kv kw js iu kx ky jw iy kz la ka lb bi translated">示例—一键编码</h2><p id="8c13" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">使用与上面相同的数据，当我们一次性编码时，我们的数据看起来像:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mm"><img src="../Images/3b7d5ee521b4df545b71e62e9e7c32d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*kz_RX5EZGXDT_gmgETtdyA.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">表3:独热编码数据帧</figcaption></figure><p id="6011" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，现在我们有三个新列:“isCat”、“isDog”和“isHamster”每个“1”表示该特征包含特征标题中的动物。如果有一个0，那么我们没有那个动物。再一次，我们现在有了机器学习算法可以解释的新特征。</p><p id="fc4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看到达这里的步骤。</p><ol class=""><li id="1a77" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated">导入包</li></ol><pre class="ld le lf lg fd md me mf mg aw mh bi"><span id="109f" class="ko jf hi me b fi mi mj l mk ml">import pandas as pd<br/>from sklearn.preprocessing import OneHotEncoder <br/>from sklearn.preprocessing import LabelEncoder</span></pre><ol class=""><li id="4b5c" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated">标签编码(给每个类别一个数值，即cat = 0) —显示在表3的“动物编码”栏中。</li></ol><pre class="ld le lf lg fd md me mf mg aw mh bi"><span id="aac9" class="ko jf hi me b fi mi mj l mk ml">le = LabelEncoder()<br/>df['Animal Encoded'] = le.fit_transform(df.Animal)</span></pre><ol class=""><li id="36c3" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated">一键编码和清理数据帧</li></ol><pre class="ld le lf lg fd md me mf mg aw mh bi"><span id="53ea" class="ko jf hi me b fi mi mj l mk ml">encoder = OneHotEncoder(categories = 'auto')</span><span id="7975" class="ko jf hi me b fi mn mj l mk ml">X = encoder.fit_transform(<br/>    df['Animal Encoded'].values.reshape(-1,1)).toarray()</span><span id="2f8e" class="ko jf hi me b fi mn mj l mk ml">dfonehot = pd.DataFrame(X)<br/>df = pd.concat([df, dfonehot], axis =1)<br/>df.columns = ['Animal','Target','Animal Encoded',<br/>                     'isCat','isDog','isHamster']</span></pre><p id="9724" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将输出表3所示的完整数据帧。现在你有了3个机器学习算法可以理解的新特征。</p><h2 id="9c88" class="ko jf hi bd jg kp kq kr jk ks kt ku jo iq kv kw js iu kx ky jw iy kz la ka lb bi translated">一键编码的优势</h2><p id="1b68" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">One-hot编码适用于名义数据，并消除了任何影响数据的较高分类值的问题，因为我们是以二进制1或0创建每一列的。</p><h2 id="bbd4" class="ko jf hi bd jg kp kq kr jk ks kt ku jo iq kv kw js iu kx ky jw iy kz la ka lb bi translated">一键编码的局限性</h2><p id="89a5" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">根据您拥有的分类特征数量和每个特征的类别数量，一键编码可以创建非常高的维数。这不仅在较小的数据集中会成为问题，在较大的数据集中也可能成为问题。在运行模型时，将PCA与一键编码相结合有助于降低维数。在基于树的模型中，一键编码也是有问题的。进一步讨论见<a class="ae jd" href="https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/" rel="noopener ugc nofollow" target="_blank">此处</a>。</p><h1 id="10a2" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">结论</h1><p id="c1a3" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">有许多不同类型的编码。我建议在深入研究特定数据集时探索其他选项。也就是说，这两种方法可以帮助您利用数据集中存在的分类特征。</p><p id="0563" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在我的<a class="ae jd" href="https://github.com/svideloc/EncodingBlogPost" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上访问我在这篇博客中使用的代码。</p></div></div>    
</body>
</html>