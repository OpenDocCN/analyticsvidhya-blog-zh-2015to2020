<html>
<head>
<title>Topic Modeling with Latent Dirichlet Allocation (LDA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于潜在狄利克雷分配的主题建模</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/topic-modeling-with-latent-dirichlet-allocation-lda-196c287e221?source=collection_archive---------5-----------------------#2020-06-14">https://medium.com/analytics-vidhya/topic-modeling-with-latent-dirichlet-allocation-lda-196c287e221?source=collection_archive---------5-----------------------#2020-06-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/6c9d10ab58950e2c6fe34f856a36849d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SdfvI7BQ53k4Fl1OwrKUEQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Ftheteachingtexan.com%2F2018%2F07%2F06%2Fa-fresh-and-bright-teacher-toolbox-diy%2F&amp;psig=AOvVaw18cTY-qcuMfzL8-dcNpqpl&amp;ust=1592241054285000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCJCvrqbmgeoCFQAAAAAdAAAAABAS" rel="noopener ugc nofollow" target="_blank">https://www.google.com/url?sa=i&amp;URL = https % 3A % 2F % 2ftheteachingtexan . com % 2f 2018% 2f 07% 2f 06% 2Fa-fresh-and-bright-teacher-toolbox-DIY % 2F&amp;psig = aovvaw 18 cty-qcumfzl 8-dcNpqpl&amp;ust = 1592241054285000&amp;source = images&amp;CD = vfe&amp;ved = 00</a></figcaption></figure><p id="3bcd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">我热烈欢迎所有的读者！</em> </strong></p><h1 id="0296" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">内容:</strong></h1><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/b390037c03907abd0dd472a97ef7e5a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*KVWUd7Upq5htsL_t9RVU8A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">内容</figcaption></figure><h1 id="ca74" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">1.潜在狄利克雷分配(LDA)简介:</h1><p id="ef1a" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">LDA代表潜在的狄利克雷分配。随着时间的推移，数据呈指数级增长。大部分数据是非结构化的，其中有一些是未标记的。手动标记每个数据是一项繁琐的任务。如果不是手动，我们如何标记如此大量的数据？LDA来救我们了。LDA是一种主题建模技术，用于分析大量数据，将它们聚类到相似的组中，并标记每个组。应该注意的是，LDA技术是用于无监督学习的，其用于通过将数据分组到相似的主题中来标记数据。与K-Means聚类和其他使用聚类中心之间的距离概念的聚类技术不同，LDA处理属于文档的主题的概率分布。</p><h1 id="52ce" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">2.LDA的假设:</h1><h2 id="9d18" class="lc jv hi bd jw ld le lf ka lg lh li ke jg lj lk ki jk ll lm km jo ln lo kq lp bi translated">2.1主题是单词的概率分布:</h2><p id="f1c7" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">它表示属于主题的单词的概率分布。假设，我们有两个话题— <strong class="ix hj">医疗</strong>和<strong class="ix hj">政治</strong>。像药物、注射、氧气等这样的词将具有属于<strong class="ix hj">保健</strong>主题的较高概率分布，而像选举、投票等这样的词将具有较低概率分布。另一方面，像选举、投票、政党这样的词具有属于话题政治的较高概率分布，而像药物、注射等这样的词将具有较低概率分布。在这种情况下，每个主题以更高的概率共享相似的一组单词。请参考下图以获得清晰的理解。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lq"><img src="../Images/fe58d5193d6f50b8080ca7bbab48f689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zyt5VkWhA8Sgonbpm0RxKA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">资料来源:https://www.udemy.com/share/101YmOBEYTcVxUQX4=/</figcaption></figure><h2 id="48dd" class="lc jv hi bd jw ld le lf ka lg lh li ke jg lj lk ki jk ll lm km jo ln lo kq lp bi translated">2.2文档是主题的概率分布:</h2><p id="2f81" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">它表示属于每个文档的主题的概率分布。如上例所示，我们有两个主题——<strong class="ix hj">医疗保健</strong>和<strong class="ix hj">政治</strong>。由于文档是各种单词的混合，每个单词将具有属于主题的概率分布。这导致了属于每个文档的主题的概率分布。迷惑，对！开始的时候我也很困惑，但是经过一段时间的思考，我终于明白了。让我试着用其他术语来解释。请看一份文件，上面写着— <strong class="ix hj">“我们观察到许多病人上个月康复了。政府的基金增加了药品的供应。”</strong>如果我们阅读这份声明，它听起来更像是关于<strong class="ix hj">医疗保健</strong>的话题，而不是<strong class="ix hj">政府</strong>。虽然我们在涉及政治的文件中有类似<strong class="ix hj">【政府基金】</strong>的词语，但由于其出现的概率较低，当与患者、康复、药物等词语相比时，该文件可被标记为<strong class="ix hj">【医疗】</strong>。请参考下图以获得清晰的理解。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/9e3a33b6323fde6eaac7bb795781170a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j8Na8mC955e56mzZSmdMdw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:<a class="ae iu" href="https://www.udemy.com/course/nlp-natural-language-processing-with-python/" rel="noopener ugc nofollow" target="_blank">https://www.udemy.com/share/101YmOBEYTcVxUQX4=/</a></figcaption></figure><h1 id="b04f" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">3.NPR数据集上的LDA:</h1><p id="b23a" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">我用NPR的数据集来理解LDA，它是如何被分组到相似的主题中，并被相应地标记的。参考下图，查看数据集的大小和头部。</p><h2 id="3449" class="lc jv hi bd jw ld le lf ka lg lh li ke jg lj lk ki jk ll lm km jo ln lo kq lp bi translated">3.1导入数据集:</h2><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ls"><img src="../Images/382f39374a152921af2f0053e4e9fb56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aZ8La510DxpLPQjFw1ji4g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">数据集图像</figcaption></figure><h2 id="bf25" class="lc jv hi bd jw ld le lf ka lg lh li ke jg lj lk ki jk ll lm km jo ln lo kq lp bi translated">3.2文本数据的矢量化:</h2><p id="85fd" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">我正在使用<strong class="ix hj">单词包(计数矢量化工具)</strong>对文本数据进行矢量化。我们甚至可以使用其他矢量技术，如TFIDF、Word2Vec等。我已经将参数“max_df”设置为0.90，以便丢弃出现频率超过90%的单词，并将“mid_df”设置为2，以便只包括至少在2个文档中出现的那些单词。此外，我将通过将stop_words设置为“english”来删除非索引词。</p><blockquote class="lt lu lv"><p id="f6a9" class="iv iw jt ix b iy iz ja jb jc jd je jf lw jh ji jj lx jl jm jn ly jp jq jr js hb bi translated">仅供参考:我们的目标只是理解LDA在主题建模中的基本应用。我不做任何探索性的数据分析部分。另外，我不是在做词干化或词干化、去除标点符号等数据预处理。可以进行探索性的数据分析、数据预处理等，以便更好地理解数据，获得更好的结果。</p></blockquote><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/aa72e7d886bc70e47f96da88803768fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*nbevuprdvzLmlq1qKVDTVA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">计数矢量器</figcaption></figure><p id="f7cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上图中，我们可以看到稀疏矩阵有54777个词的语料库。</p><h2 id="3dae" class="lc jv hi bd jw ld le lf ka lg lh li ke jg lj lk ki jk ll lm km jo ln lo kq lp bi translated">3.3文本数据的LDA:</h2><p id="5a54" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">是时候开始应用LDA将文档分配到相似的主题中了。这里，应该注意的是，主题数量(n_components)的选择仅仅取决于个人的领域知识和对数据集的理解。在这里，我选择5个主题进行分配。代码请参考下图。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/72ee7c3560e6010240a12620f35c04b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*GICc6Y3PTrW9zFtPdOcr7A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">LDA代码</figcaption></figure><p id="c23e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请参考下图，了解它生成的主题数量和专栏(单词语料库)。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/4debd3dd9e953d7032d1e28b937b1a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*CtnHbYHjSmQotFqSdEhzmA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">主题和功能名称</figcaption></figure><p id="f9ea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上面的图片中，我们可以看到LDA已经创建了5个主题和54777个特性名，它们是列名。</p><h2 id="cb6d" class="lc jv hi bd jw ld le lf ka lg lh li ke jg lj lk ki jk ll lm km jo ln lo kq lp bi translated">3.4带单词分布的主题分析:</h2><p id="5654" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">参考下图分析每个主题中的单词。我会在下面的图片中解释我所做的事情。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/a277a5b38755bb1d3f11272976aa5ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qS3781XKXdjdgw6pitx0Ng.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">主题分析</figcaption></figure><p id="b083" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我把属于题目概率最高的前50个词打印出来了。每个主题有54777个单词，有一个概率分布。我首先对它们进行了排序，并用' df.argsort()'得到了索引。使用获得的索引，我已经获得了特征名称，但是从最后一个开始，它代表了该主题的最高概率分布。</p><p id="be63" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果少读几个字，我们可以看到，<strong class="ix hj">话题- 0 </strong>谈<strong class="ix hj">医疗</strong>，<strong class="ix hj">话题-1 </strong>谈<strong class="ix hj">娱乐</strong>，<strong class="ix hj">话题- 2 </strong>谈<strong class="ix hj">教育</strong>，<strong class="ix hj">话题- 3 </strong>谈<strong class="ix hj">政治，</strong>和<strong class="ix hj">话题- 4 </strong>谈<strong class="ix hj">军事</strong>。</p><p id="c22b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们打印第一行，检查它是如何分配给概率分布的主题的。参考下图。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/b15feb0a62469571d51487159a8c50e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8LI5dQcNeGae-LTGF_A1LQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">测试一行</figcaption></figure><p id="1b22" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们阅读2016年华盛顿的文本<strong class="ix hj">‘即使政策可以是两党的，政治也不能……’</strong>这显然代表了政治声明。在上图的第二个单元格中，我们可以看到各个主题的概率分布，文字属于<strong class="ix hj"> topic- 3 </strong>的概率为<strong class="ix hj"> 92% </strong>。根据我们对图片<strong class="ix hj">【话题分析】</strong>的分析，<strong class="ix hj">话题- 3 </strong>其实属于<strong class="ix hj">政治</strong>。完美！</p><h2 id="c03a" class="lc jv hi bd jw ld le lf ka lg lh li ke jg lj lk ki jk ll lm km jo ln lo kq lp bi translated">3.5分配主题:</h2><p id="01f4" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">参考下面的代码为每一行分配主题。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es me"><img src="../Images/565535efada2cdc07b27d02fb38a7519.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*HAKiAJlHkt5vNmZKhDKnkg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">分配主题</figcaption></figure><h2 id="5903" class="lc jv hi bd jw ld le lf ka lg lh li ke jg lj lk ki jk ll lm km jo ln lo kq lp bi translated">3.6映射主题名称:</h2><p id="1e20" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">请参考下面的主题名称映射。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/58a19665522d5858cc9a863a86b5a6bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rpPVCNRkRbUiQNFyRyqfbA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">映射主题名称</figcaption></figure><h1 id="75c2" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">4.主题的可视化:</h1><h2 id="8743" class="lc jv hi bd jw ld le lf ka lg lh li ke jg lj lk ki jk ll lm km jo ln lo kq lp bi translated">4.1文字云可视化:</h2><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/08ac14a1fbd0233d8bd21528ae82a813.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*3ewLjWADXklq-Gyl-eZLlg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">主题- 0</figcaption></figure><p id="f30b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">字云为<strong class="ix hj">题- 0 </strong>。我们可以看到脑科学、研究人员、患者、疾病、保险等词汇。这涉及到<strong class="ix hj">医疗保健</strong>话题。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/19061d6e4d65dae4fad757982ce706db.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*psI1SfrF8uh6x-xWxfWGEQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">主题- 1</figcaption></figure><p id="c837" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">题词云- 1。我们可以看到宋立科，故事，家庭，电影，音乐，爱情等词。这就涉及到<strong class="ix hj">娱乐</strong>话题了。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/799896274e5890df50859eb5688fcf42.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*_8iFCfeQ-ToFh-uZ_ugaXw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">话题2</figcaption></figure><p id="f0b7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">字云为题<strong class="ix hj">-2</strong>。我们可以看到学生、大学、学校、学院、工作等词语。这涉及到<strong class="ix hj">教育</strong>话题。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/7470d034ec085c3c8bceb09c356e8ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*Z9eZ8mOIFA-IJNK82dXt_A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">主题- 3</figcaption></figure><p id="904b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">字云为<strong class="ix hj">题- 3 </strong>。我们可以看到像国会、共和党、新州、民主党、政府、委员会等词。这涉及到<strong class="ix hj">政治</strong>话题。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/98411d906879fa02344b58d1feb2b618.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*jFzSbTbIYJwYVfWdc7zPLA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">主题- 4</figcaption></figure><p id="4911" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">字云为<strong class="ix hj">题- 4 </strong>。我们可以看到像部队，ISIS，警察，安全，朝鲜，难民等词。这涉及到<strong class="ix hj">军事</strong>话题。</p><h2 id="f28c" class="lc jv hi bd jw ld le lf ka lg lh li ke jg lj lk ki jk ll lm km jo ln lo kq lp bi translated">4.2 pyl Davis可视化:</h2><p id="57c9" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">要安装pyLDAvis，请在笔记本单元格或CMD提示符或anaconda提示符下输入以下代码。</p><blockquote class="lt lu lv"><p id="4919" class="iv iw jt ix b iy iz ja jb jc jd je jf lw jh ji jj lx jl jm jn ly jp jq jr js hb bi translated">！pip安装pyLDAvis</p></blockquote><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/efac40d4ac71fbbb93fa9733984923b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*esuI-d-1sfdhU5-61BhpTQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">安装pyLDAvis</figcaption></figure><p id="94d9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">代码请参考下图。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/e62135fc9cee95b91c343302ce4426b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*oVA7eI_RMtkpqPgh6ggcQw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">pyLDAvis代码</figcaption></figure><p id="61d1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">输出视频:</strong> <a class="ae iu" href="https://www.linkedin.com/feed/update/urn:li:activity:6678155419195719680/" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">用pyLDAvis </strong> </a>可视化的视频</p><blockquote class="lt lu lv"><p id="a884" class="iv iw jt ix b iy iz ja jb jc jd je jf lw jh ji jj lx jl jm jn ly jp jq jr js hb bi translated">完整代码请参考GitHub资源库-<a class="ae iu" href="https://github.com/Sandeep-Panchal/Topic-Modeling-with-LDA" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj">Topic-Modeling-with-LDA</strong></a>。</p></blockquote><h1 id="634e" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">参考:</h1><ol class=""><li id="21eb" class="mn mo hi ix b iy kx jc ky jg mp jk mq jo mr js ms mt mu mv bi translated"><a class="ae iu" href="https://www.udemy.com/course/nlp-natural-language-processing-with-python/" rel="noopener ugc nofollow" target="_blank">https://www.udemy.com/share/101YmOBEYTcVxUQX4=/</a></li></ol><h1 id="9ee4" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">帮我连接:</h1><ol class=""><li id="f8ae" class="mn mo hi ix b iy kx jc ky jg mp jk mq jo mr js ms mt mu mv bi translated"><strong class="ix hj">领英:</strong><a class="ae iu" href="https://www.linkedin.com/in/sandeep-panchal-682734111/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/sandeep-panchal-682734111/</a></li><li id="8dd0" class="mn mo hi ix b iy mw jc mx jg my jk mz jo na js ms mt mu mv bi translated"><strong class="ix hj">GitHub:</strong><a class="ae iu" href="https://github.com/Sandeep-Panchal" rel="noopener ugc nofollow" target="_blank">https://github.com/Sandeep-Panchal</a></li></ol><p id="85c0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">谢谢大家阅读这篇博客。非常感谢您的建议！</p></div></div>    
</body>
</html>