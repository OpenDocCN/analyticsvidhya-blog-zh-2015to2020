<html>
<head>
<title>VAE Mini-Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">VAE迷你教程</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/vae-mini-tutorial-f6c3043c85c3?source=collection_archive---------19-----------------------#2020-09-01">https://medium.com/analytics-vidhya/vae-mini-tutorial-f6c3043c85c3?source=collection_archive---------19-----------------------#2020-09-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="cb4f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="17c3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">变分自动编码器是一个深度生成模型，明确地与密度函数一起工作。假设我们在<em class="kb"> X </em>上有一个名为<em class="kb"> p_D(x) </em>的分布。我们希望用神经网络(<em class="kb"> θ </em>)来模拟这种分布，以便能够从中获取新的样本。训练网络和学习<em class="kb"> θ </em>的普通方法是什么？<br/>学习<em class="kb"> θ </em>的一个自然训练目标是最大可能性:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kc"><img src="../Images/a0fc9023fe69ac2fffab3e72daec56bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*BkdTARhOYnPPH7Nu1yFYMA.png"/></div></figure><p id="8ddc" class="pw-post-body-paragraph jd je hi jf b jg kk ji jj jk kl jm jn jo km jq jr js kn ju jv jw ko jy jz ka hb bi translated">假设每个数据点都是从一个潜在的表示<em class="kb"> z </em>中生成的，我们可以将<em class="kb"> p_θ(x) </em>写成<em class="kb"> p_θ(x，z) </em>在<em class="kb"> z </em>上被边缘化。因此，我们可以将前一个等式改写如下:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kp"><img src="../Images/b9e8e35f3cfd2478d7e75cad5452785b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*JGQFHq0Q2_dFD5KkrwgTjg.png"/></div></figure><p id="6934" class="pw-post-body-paragraph jd je hi jf b jg kk ji jj jk kl jm jn jo km jq jr js kn ju jv jw ko jy jz ka hb bi translated">这个方程是棘手的，不能有效地优化。</p><h1 id="8dd7" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">如何克服顽固性？</h1><p id="7299" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">到目前为止，我们有一个网络(<em class="kb"> θ </em>)从<em class="kb"> z </em>映射到<em class="kb"> x </em>。这个网络为我们提供了在<em class="kb"> X </em>和<em class="kb"> Z </em>上的联合分布(称为生成分布):</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kq"><img src="../Images/262e99bbf862dd29ce7178b4d5cf13da.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*SnlJdCUDtd2AnAXRMQPVIQ.png"/></div></figure><p id="d452" class="pw-post-body-paragraph jd je hi jf b jg kk ji jj jk kl jm jn jo km jq jr js kn ju jv jw ko jy jz ka hb bi translated">我们创建另一个网络(<em class="kb"> φ </em>)来映射从<em class="kb"> x </em>到<em class="kb"> z </em>。这个网络也模拟了在<em class="kb"> X </em>和<em class="kb"> Z </em>上的联合分布(称为推理分布):</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kr"><img src="../Images/f935f9b247f03ec07afd282311123123.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*FjLLg5UJixLW3WDr5TEfAA.png"/></div></figure><p id="12ba" class="pw-post-body-paragraph jd je hi jf b jg kk ji jj jk kl jm jn jo km jq jr js kn ju jv jw ko jy jz ka hb bi translated">辅助的<em class="kb"> q_φ(z|x) </em>分布可以帮助我们克服困难。<br/>我们现在可以将我们目标的棘手整合改写为:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ks"><img src="../Images/19cf2090eed5e9e80f7ed73fa47ca08d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lFX1fbcnANjwVOorKBSiHQ.png"/></div></div></figure><p id="e9ae" class="pw-post-body-paragraph jd je hi jf b jg kk ji jj jk kl jm jn jo km jq jr js kn ju jv jw ko jy jz ka hb bi translated">(在第4行，应用了<a class="ae kx" href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" rel="noopener ugc nofollow" target="_blank">詹森不等式</a>。)</p><h1 id="4054" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">VAE目标函数</h1><p id="de56" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们将特定<em class="kb"> x </em>的目标函数定义为</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ky"><img src="../Images/44f0b8d745c343783530f92fd05ae8d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*RUqP_K81p0EUr5BR0y6tgg.png"/></div></figure><p id="2f65" class="pw-post-body-paragraph jd je hi jf b jg kk ji jj jk kl jm jn jo km jq jr js kn ju jv jw ko jy jz ka hb bi translated">如等式所示，这是可能性对数的下限。我们还将VAE的最终目标函数定义为:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kz"><img src="../Images/b3b77ded642451d4d3b5b8972979a631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*AXthFvewF1fM4K72bDT_Rg.png"/></div></figure><h1 id="3697" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">我们如何优化目标？</h1><p id="43f3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在前一个等式中，我们有3种不同的元素，我们依次重写。</p><h2 id="3a60" class="la ig hi bd ih lb lc ld il le lf lg ip jo lh li it js lj lk ix jw ll lm jb ln bi translated">预期</h2><p id="164c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们知道，如果我们有来自<em class="kb"> f(u) </em>分布的<em class="kb"> N </em>个样本，我们可以如下估计期望值:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lo"><img src="../Images/568badb11c2a69823c508982f7a68d2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*j9mCa4e832aJkYFl7FPCpQ.png"/></div></figure><p id="4cc2" class="pw-post-body-paragraph jd je hi jf b jg kk ji jj jk kl jm jn jo km jq jr js kn ju jv jw ko jy jz ka hb bi translated">由于我们可以从<em class="kb"> p_D(x) </em>和<em class="kb"> q_φ(z|x) </em>中提取样本，因此我们可以计算等式中的两个期望值。</p><h2 id="55db" class="la ig hi bd ih lb lc ld il le lf lg ip jo lh li it js lj lk ix jw ll lm jb ln bi translated">解码器分发的日志</h2><p id="4e6c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们可以解析地计算<em class="kb"> log(p_θ(x|z)) </em>，因为分布被设置为多元伯努利或多元高斯。</p><ul class=""><li id="3864" class="lp lq hi jf b jg kk jk kl jo lr js ls jw lt ka lu lv lw lx bi translated"><strong class="jf hj">多元伯努利情形</strong>:解码器的输出(在一个sigmoid层之后)表示为<em class="kb"> a </em>。我们有:</li></ul><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ly"><img src="../Images/12f2fa89b79a0146abe798d8db81f2a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*wvNPf7EbMkhcBbFCfjBLXw.png"/></div></figure><ul class=""><li id="8a9a" class="lp lq hi jf b jg kk jk kl jo lr js ls jw lt ka lu lv lw lx bi translated"><strong class="jf hj">多元高斯情况</strong>:解码器的输出表示为<em class="kb"> μ </em>。<em class="kb"> σ，</em>太，在方程中使用，但在实际中，通常设置为1。我们有:</li></ul><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es lz"><img src="../Images/fdc4914f4753a1d22a95db7bdee7e84d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*ZABvO8ZTxB18O1gVPnQc8w.png"/></div></figure><h2 id="8870" class="la ig hi bd ih lb lc ld il le lf lg ip jo lh li it js lj lk ix jw ll lm jb ln bi translated">KL术语</h2><p id="ddfc" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><em class="kb"> KL(q_φ (z|x) || p(z)) </em>也可以解析地计算，因为<br/>两种分布都被设置为高斯分布。<br/>我们知道对于两个多元高斯分布为</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ma"><img src="../Images/33d581ba0c9642bb9d27668f79060366.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*lnKKZ7Z-Toiaqf8MRE8_sw.png"/></div></figure><p id="4db7" class="pw-post-body-paragraph jd je hi jf b jg kk ji jj jk kl jm jn jo km jq jr js kn ju jv jw ko jy jz ka hb bi translated">它们的KL散度可以计算为:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mb"><img src="../Images/0643208b840344605094b9651ffe0d71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*wgTr-PjypDq8cbJoCG2o9A.png"/></div></figure><p id="7f15" class="pw-post-body-paragraph jd je hi jf b jg kk ji jj jk kl jm jn jo km jq jr js kn ju jv jw ko jy jz ka hb bi translated">因此，我们的目标中的KL散度可以计算如下:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mc"><img src="../Images/59e0f9fbfe425c513c3c74bbdd132036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*waKXiI1-d5mo7gSM5gd3OA.png"/></div></figure><p id="ac2e" class="pw-post-body-paragraph jd je hi jf b jg kk ji jj jk kl jm jn jo km jq jr js kn ju jv jw ko jy jz ka hb bi translated">因此，我们的目标函数中的所有项都可以有效地计算，我们可以优化<em class="kb"> φ </em>和<em class="kb"> θ </em>以最大化该方程。</p><h1 id="c213" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">ELBO的等效形式</h1><p id="e47f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">VAE目标函数有不同的形式，可能不能直接优化，但在理论分析中是有用的。你可以在扩展VAE框架的论文中看到这些替代形式。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es md"><img src="../Images/ff15776bc86517aa5818a30b1a709587.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*wF_oY0nsCGWQLezh-TUNzg.png"/></div></figure><p id="028c" class="pw-post-body-paragraph jd je hi jf b jg kk ji jj jk kl jm jn jo km jq jr js kn ju jv jw ko jy jz ka hb bi translated">这里提供了第一个的证明，其他的更容易导出:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es et"><img src="../Images/1136919f32d7b033da26281c2c7a9b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OJ0bY190eqKmsbzE4mYJuA.png"/></div></div></figure><p id="247b" class="pw-post-body-paragraph jd je hi jf b jg kk ji jj jk kl jm jn jo km jq jr js kn ju jv jw ko jy jz ka hb bi translated">前一个方程的最后一项是一个常数，因为它等于固定分布的负熵；所以在优化过程中可以忽略。</p><h1 id="ed50" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">参考</h1><ol class=""><li id="445d" class="lp lq hi jf b jg jh jk jl jo me js mf jw mg ka mh lv lw lx bi translated">D.P. Kingma和M. Welling，“自动编码变分贝叶斯”，CoRR，第abs/1312.6114卷，2014年。</li><li id="2829" class="lp lq hi jf b jg mi jk mj jo mk js ml jw mm ka mh lv lw lx bi translated">南赵，J. Song和S. Ermon，“Infovae:在变分自动编码器中平衡学习和推理”，《人工智能会议论文集》，第33卷，第5885-5892页，2019年。</li><li id="6d2a" class="lp lq hi jf b jg mi jk mj jo mk js ml jw mm ka mh lv lw lx bi translated">米（meter的缩写））Soleymani，“深度学习课程幻灯片。”大学讲座，2020年5月。谢里夫理工大学计算机工程系。</li></ol></div></div>    
</body>
</html>