<html>
<head>
<title>Pre-processing of Topically Coherent Text Segments in Python ğŸ’¬</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pythonä¸­ä¸»é¢˜è¿è´¯æ–‡æœ¬æ®µçš„é¢„å¤„ç†ğŸ’¬</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/analytics-vidhya/pre-processing-of-topically-coherent-text-segments-in-python-58f9b258596c?source=collection_archive---------4-----------------------#2020-01-15">https://medium.com/analytics-vidhya/pre-processing-of-topically-coherent-text-segments-in-python-58f9b258596c?source=collection_archive---------4-----------------------#2020-01-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="f1c2" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">å¦‚ä½•ä½¿ç”¨è‡ªç„¶è¯­è¨€å·¥å…·åŒ…é¢„å¤„ç†ä¸€ç»„æŠ„æœ¬å¹¶å°†å…¶è½¬æ¢æˆæ•°å­—è¡¨ç¤º</h2></div><p id="a2a1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">å®Œæ•´çš„<strong class="iz hj"> Jupyterç¬”è®°æœ¬</strong>å’Œæ–‡ä»¶å¯åœ¨æˆ‘çš„<a class="ae jt" href="https://github.com/maziarizadi/TextPreProcessingPy" rel="noopener ugc nofollow" target="_blank"> <strong class="iz hj"> GitHubé¡µé¢</strong> </a>è·å¾—ã€‚</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h2 id="2f19" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">ä»‹ç»</h2><p id="96cd" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">æ–‡æœ¬æ–‡æ¡£ï¼Œå¦‚é•¿å½•éŸ³å’Œä¼šè®®è®°å½•ï¼Œé€šå¸¸ç”±ä¸»é¢˜è¿è´¯çš„æ–‡æœ¬ç‰‡æ®µç»„æˆï¼Œæ¯ä¸ªç‰‡æ®µåŒ…å«ä¸€å®šæ•°é‡çš„æ–‡æœ¬æ®µè½ã€‚åœ¨æ¯ä¸€ä¸ªä¸»é¢˜è¿è´¯çš„ç‰‡æ®µä¸­ï¼Œäººä»¬ä¼šæœŸæœ›å•è¯çš„ä½¿ç”¨æ¯”è·¨ç‰‡æ®µçš„ä½¿ç”¨è¡¨ç°å‡ºæ›´ä¸€è‡´çš„è¯æ±‡åˆ†å¸ƒã€‚<strong class="iz hj">è‡ªç„¶è¯­è¨€å¤„ç†(NLP) </strong>ï¼Œæ›´å…·ä½“åœ°è¯´ï¼Œå°†æ–‡æœ¬çº¿æ€§åˆ’åˆ†æˆä¸»é¢˜ç‰‡æ®µå¯ç”¨äºæ–‡æœ¬åˆ†æä»»åŠ¡ï¼Œä¾‹å¦‚ä¿¡æ¯æ£€ç´¢ä¸­çš„æ®µè½æ£€ç´¢ã€æ–‡æ¡£æ‘˜è¦å’Œè¯è¯­åˆ†æã€‚åœ¨å½“å‰çš„ç»ƒä¹ ä¸­ï¼Œæˆ‘ä»¬å°†å›é¡¾å¦‚ä½•ç¼–å†™Pythonä»£ç æ¥<strong class="iz hj">é¢„å¤„ç†</strong>ä¸€ç»„æŠ„æœ¬å¹¶ä¸”<strong class="iz hj">å°†å®ƒä»¬è½¬æ¢æˆé€‚åˆè¾“å…¥åˆ°<strong class="iz hj">ä¸»é¢˜åˆ†å‰²ç®—æ³•</strong>ä¸­çš„æ•°å€¼è¡¨ç¤º</strong>ã€‚</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lb"><img src="../Images/ebded4b8abcabc0f3892c045e78bf18f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*PGB0w1JZslqA-hM0xGrmJw.gif"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated"><a class="ae jt" href="https://s3.amazonaws.com/codecademy-content/courses/NLP/Natural_Language_Processing_Overview.gif" rel="noopener ugc nofollow" target="_blank">å›¾åƒçš„æ¥æº</a></figcaption></figure><p id="c57a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">è¿™ç¯‡æ–‡ç« çš„æ¥æºæ¥è‡ªæˆ‘åœ¨è«çº³ä»€å¤§å­¦å®Œæˆçš„<strong class="iz hj">æ•°æ®ç§‘å­¦ç ”ç©¶ç”Ÿæ–‡å‡­çš„ä¸€éƒ¨åˆ†ä½œä¸šã€‚æˆ‘ä¹Ÿåšäº†ä¸€äº›æ”¹åŠ¨ï¼Œè®©åŸæ¥çš„ä»»åŠ¡æ›´æœ‰è¶£ã€‚</strong></p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h2 id="c6e0" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">ä»€ä¹ˆæ˜¯ä½¿ç”¨æ¡ˆä¾‹ï¼ŒNLPå¦‚ä½•æä¾›å¸®åŠ©ï¼Ÿ</h2><p id="1073" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">ç°åœ¨æœ‰å¾ˆå¤šæ±‚èŒç½‘ç«™ï¼ŒåŒ…æ‹¬seek.com.auå’Œau.indeed.comã€‚è¿™äº›æ±‚èŒç½‘ç«™éƒ½ç®¡ç†ç€ä¸€ä¸ªæ±‚èŒç³»ç»Ÿï¼Œæ±‚èŒè€…å¯ä»¥æ ¹æ®å…³é”®è¯ã€è–ªæ°´å’Œç±»åˆ«æ¥æœç´¢ç›¸å…³çš„å·¥ä½œã€‚é€šå¸¸ï¼Œå¹¿å‘Šå·¥ä½œçš„ç±»åˆ«ç”±å¹¿å‘Šå•†(ä¾‹å¦‚ï¼Œé›‡ä¸»)æ‰‹åŠ¨è¾“å…¥ã€‚ç±»åˆ«åˆ†é…å¯èƒ½ä¼šå‡ºé”™ã€‚<strong class="iz hj">å› æ­¤ï¼Œé”™è¯¯ç±»åˆ«çš„å·¥ä½œå°†æ— æ³•è·å¾—ç›¸å…³å€™é€‰ç¾¤ä½“çš„è¶³å¤Ÿæ›å…‰åº¦</strong>ã€‚</p><p id="4504" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">éšç€æ–‡æœ¬åˆ†æçš„è¿›æ­¥ï¼Œè‡ªåŠ¨å·¥ä½œåˆ†ç±»å°†å˜å¾—å¯è¡Œï¼Œå¹¶ä¸”å¯ä»¥å‘æ½œåœ¨çš„å¹¿å‘Šå®¢æˆ·æä¾›åˆç†çš„å·¥ä½œç±»åˆ«å»ºè®®ã€‚è¿™æœ‰åŠ©äºå‡å°‘äººå·¥æ•°æ®è¾“å…¥é”™è¯¯ï¼Œå¢åŠ ç›¸å…³å€™é€‰äººçš„èŒä½æ›å…‰ç‡ï¼Œè¿˜å¯ä»¥æ”¹å–„æ±‚èŒç½‘ç«™çš„ç”¨æˆ·ä½“éªŒã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª<strong class="iz hj">è‡ªåŠ¨æ‹›è˜å¹¿å‘Šåˆ†ç±»</strong>ç³»ç»Ÿï¼Œå®ƒåœ¨ç°æœ‰çš„æ‹›è˜å¹¿å‘Šæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…·æœ‰æ ‡å‡†åŒ–çš„å·¥ä½œç±»åˆ«ï¼Œ<strong class="iz hj">é¢„æµ‹æ–°è¾“å…¥çš„æ‹›è˜å¹¿å‘Šçš„ç±»åˆ«æ ‡ç­¾</strong>ã€‚</p><blockquote class="lr"><p id="a813" class="ls lt hi bd lu lv lw lx ly lz ma js dx translated">å½“å‰ç¤ºä¾‹æ¶‰åŠå¤„ç†å·¥ä½œå¹¿å‘Šæ–‡æœ¬æ•°æ®çš„ç¬¬ä¸€æ­¥ï¼Œå³ï¼Œå°†å·¥ä½œå¹¿å‘Šæ–‡æœ¬è§£ææˆæ›´åˆé€‚çš„æ ¼å¼ã€‚</p></blockquote><p id="3a35" class="pw-post-body-paragraph ix iy hi iz b ja mb ij jc jd mc im jf jg md ji jj jk me jm jn jo mf jq jr js hb bi translated">æˆ‘ä»¬æä¾›çš„æ‹›è˜å¹¿å‘Šæ•°æ®åŒ…å«å¤§é‡ä»¥ç®€å•txtæ ¼å¼è¡¨ç¤ºçš„å†—ä½™ä¿¡æ¯ã€‚æˆ‘ä»¬åº”è¯¥å¯¹æ‹›è˜å¹¿å‘Šæ–‡æœ¬æ•°æ®è¿›è¡Œé€‚å½“çš„é¢„å¤„ç†ï¼Œä»¥æé«˜åˆ†ç±»ç®—æ³•çš„æ€§èƒ½ã€‚</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="1070" class="mg kc hi bd kd mh mi mj kh mk ml mm kl io mn ip ko ir mo is kr iu mp iv ku mq bi translated">é—®é¢˜é™ˆè¿°ğŸ’¡</h1><p id="6ba1" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">æˆ‘ä»¬éœ€è¦ç¼–å†™Pythonä»£ç æ¥æå–ä¸€ç»„è¡¨ç¤ºæ¯ä¸ªæ‹›è˜å¹¿å‘Šå†…å®¹çš„å•è¯(ä¾‹å¦‚ï¼Œunigrams ),ç„¶å<strong class="iz hj">å°†æ¯ä¸ªå¹¿å‘Šæè¿°è½¬æ¢ä¸ºæ•°å­—è¡¨ç¤º</strong> : count vectorï¼Œå®ƒå¯ä»¥ç›´æ¥ç”¨ä½œè®¸å¤šåˆ†ç±»ç®—æ³•çš„è¾“å…¥ã€‚</p><h2 id="7921" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">æˆ‘ä»¬å°†é‡‡å–ä»€ä¹ˆæ­¥éª¤ï¼Ÿ</h2><ul class=""><li id="400e" class="mr ms hi iz b ja kw jd kx jg mt jk mu jo mv js mw mx my mz bi translated">æå–<br/>æ•°æ®æ–‡ä»¶<code class="du na nb nc nd b">data.txt</code>ä¸­æ‰€æœ‰æ‹›è˜å¹¿å‘Šçš„idå’Œæè¿°(çº¦500æ¡æ‹›è˜å¹¿å‘Š)ã€‚</li><li id="da4d" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated">å°†è¿™äº›æ‹›è˜å¹¿å‘Šæ–‡æœ¬ä½œä¸ºç¨€ç–è®¡æ•°å‘é‡è¿›è¡Œå¤„ç†å’Œå­˜å‚¨ã€‚</li></ul><p id="bee2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ä¸ºäº†å®ç°ä¸Šè¿°ç›®æ ‡ï¼Œæˆ‘ä»¬å°†:</p><ul class=""><li id="1b12" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated">æ’é™¤é•¿åº¦å°äº4çš„å•è¯</li><li id="3519" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated">ä½¿ç”¨æä¾›çš„åœç”¨è¯åˆ—è¡¨(å³åœç”¨è¯_en.txt)åˆ é™¤åœç”¨è¯</li><li id="ce73" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated">åˆ é™¤åœ¨ä¸€ä¸ªæ‹›è˜å¹¿å‘Šæè¿°ä¸­åªå‡ºç°ä¸€æ¬¡çš„å•è¯ï¼Œå°†å…¶ä¿å­˜(æ— é‡å¤)ä¸ºä¸€ä¸ª<code class="du na nb nc nd b">txt</code>æ–‡ä»¶(å‚è€ƒæ‰€éœ€çš„è¾“å‡º)</li><li id="da51" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated">ä»ç”Ÿæˆçš„è¯æ±‡è¡¨ä¸­æ’é™¤è¿™äº›å•è¯</li><li id="cd2e" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated">æ‰¾åˆ°100å¤šä¸ªå¹¿å‘Š<br/>æè¿°ä¸­å‡ºç°çš„å¸¸ç”¨è¯ï¼Œä¿å­˜ä¸º<code class="du na nb nc nd b">txt</code>æ–‡ä»¶(å‚è€ƒæ‰€éœ€è¾“å‡º)</li><li id="3671" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated">åœ¨ç”Ÿæˆçš„è¯æ±‡è¡¨ä¸­æ’é™¤å®ƒä»¬</li></ul><p id="37b4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">æˆ‘ä»¬ä¸ä¼š:</p><ul class=""><li id="4028" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated">ç”Ÿæˆå¤šè¯çŸ­è¯­(å³æ­é…ï¼Œåè¯çŸ­è¯­)</li></ul><p id="27a1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">åœ¨æœ¬ç»ƒä¹ ç»“æŸæ—¶ï¼Œæˆ‘ä»¬å°†è·å¾—ä»¥ä¸‹å‡ é¡¹è¾“å‡ºï¼ŒåŒ…æ‹¬å®ƒä»¬çš„è¦æ±‚:</p><p id="ca28" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi nm translated"><span class="l nn no np bm nq nr ns nt nu di"> 1ã€‚</span> <code class="du na nb nc nd b">vocab.txt</code>:åŒ…å«ä»¥ä¸‹æ ¼å¼çš„å•å­—è¯æ±‡:<code class="du na nb nc nd b">word_string:integer_index</code></p><ul class=""><li id="ba0a" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated">è¯æ±‡è¡¨ä¸­çš„å•è¯å¿…é¡»æŒ‰å­—æ¯é¡ºåºæ’åˆ—ã€‚è¿™ä¸ªæ–‡ä»¶æ˜¯è§£é‡Šç¨€ç–ç¼–ç çš„å…³é”®ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œå•è¯abbieæ˜¯è¯æ±‡è¡¨ä¸­çš„ç¬¬12ä¸ªå•è¯(å¯¹åº”çš„integer_index = 11)(æ³¨æ„ï¼Œä¸‹é¢çš„æ•°å­—å’Œå•è¯ä¸æ˜¯æŒ‡ç¤ºæ€§çš„)ã€‚</li></ul><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es nv"><img src="../Images/f2987425612b8e4ada501d5e72b8394b.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*IamDO4qHbniGJWVAXSRuyQ.jpeg"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">vocab.txtæ–‡ä»¶è¾“å‡ºæ ¼å¼</figcaption></figure><p id="6c3e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi nm translated"><span class="l nn no np bm nq nr ns nt nu di"> 2ã€‚</span> <code class="du na nb nc nd b">highFreq.txt</code>è¯¥æ–‡ä»¶åŒ…å«åœ¨100å¤šä¸ªå¹¿å‘Šæè¿°ä¸­å‡ºç°çš„å¸¸ç”¨è¯ã€‚åœ¨è¾“å‡º<code class="du na nb nc nd b">txt</code>æ–‡ä»¶ä¸­ï¼Œæ¯è¡Œåº”è¯¥åªåŒ…å«ä¸€ä¸ªå•è¯ã€‚å•å­—çš„é¡ºåºåŸºäºå®ƒä»¬çš„é¢‘ç‡ï¼Œå³åŒ…å«è¯¥è¯çš„å¹¿å‘Šçš„æ•°é‡ï¼Œä»é«˜åˆ°ä½ã€‚</p><p id="f9fc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi nm translated"><span class="l nn no np bm nq nr ns nt nu di"> 3ã€‚</span> <code class="du na nb nc nd b">lowFreq.txt</code>è¯¥æ–‡ä»¶åŒ…å«æŒ‰å­—æ¯é¡ºåºåœ¨ä¸€ä¸ªæ‹›è˜å¹¿å‘Šæè¿°ä¸­åªå‡ºç°ä¸€æ¬¡çš„å•è¯ã€‚åœ¨è¾“å‡ºçš„<code class="du na nb nc nd b">txt</code>æ–‡ä»¶ä¸­ï¼Œæ¯è¡Œåº”è¯¥åŒ…å«ä¸€ä¸ªå•è¯ã€‚</p><p id="841b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi nm translated"><span class="l nn no np bm nq nr ns nt nu di"> 4ã€‚</span> <code class="du na nb nc nd b">sparse.txt</code>è¯¥æ–‡ä»¶çš„æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªå¹¿å‘Šã€‚æ‰€ä»¥ï¼Œä»–ä»¬ä»<code class="du na nb nc nd b">advertisement ID</code>å¼€å§‹ã€‚æ¯è¡Œçš„å…¶ä½™éƒ¨åˆ†æ˜¯ä»¥é€—å·åˆ†éš”çš„<code class="du na nb nc nd b">word_index:word_freq</code>å½¢å¼çš„ç›¸åº”æè¿°çš„ç¨€ç–è¡¨ç¤ºã€‚è¡Œçš„é¡ºåºå¿…é¡»ä¸è¾“å…¥æ–‡ä»¶ä¸­å¹¿å‘Šçš„é¡ºåºç›¸åŒ¹é…ã€‚</p><p id="3b8e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">æ³¨:</strong> <code class="du na nb nc nd b">word_freq</code>è¿™é‡ŒæŒ‡çš„æ˜¯unigramåœ¨ç›¸åº”æè¿°ä¸­çš„å‡ºç°é¢‘ç‡ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ–‡æ¡£ã€‚ä¾‹å¦‚ï¼Œåœ¨å¹¿å‘Š12612628çš„æè¿°ä¸­ï¼Œå•è¯ç¼–å·11(æ ¹æ®ä¸Šé¢çš„ä¾‹å­æ˜¯â€˜abbie â€™)æ°å¥½å‡ºç°ä¸€æ¬¡(ç¼–å·ä¸æ˜¯æŒ‡ç¤ºæ€§çš„) :</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es nw"><img src="../Images/764923683b48a9904acf1302cbb84f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cUkXXD2uaCQhY06U2et3gQ.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">sparse.txtæ–‡ä»¶è¾“å‡ºæ ¼å¼</figcaption></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="e28b" class="mg kc hi bd kd mh mi mj kh mk ml mm kl io mn ip ko ir mo is kr iu mp iv ku mq bi translated">â›³ï¸è§£å†³æ–¹æ¡ˆ</h1><p id="1e97" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">æ‰€ä»¥æˆ‘ä»¬æ€»æ˜¯ä»å¯¼å…¥æ‰€éœ€çš„åº“å¼€å§‹ã€‚é‰´äºè¿™é¡¹å·¥ä½œçš„æ€§è´¨ï¼Œéœ€è¦åšåˆ°ä»¥ä¸‹å‡ ç‚¹:</p><h2 id="d406" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">å¯¼å…¥åº“</h2><ul class=""><li id="8faf" class="mr ms hi iz b ja kw jd kx jg mt jk mu jo mv js mw mx my mz bi translated"><strong class="iz hj">æ­£åˆ™è¡¨è¾¾å¼</strong></li></ul><p id="888c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ç¬¬ä¸€ä¸ªæ˜¯æ­£åˆ™è¡¨è¾¾å¼ï¼Œç®€ç§°ä¸ºReGexã€‚å¦‚æœä½ è¿˜æ²¡æœ‰ç”¨è¿‡å®ƒä»¬ï¼Œæˆ‘å¼ºçƒˆå»ºè®®ä½ æ‹¿èµ·å®ƒï¼Œåšä¸€äº›å¾ˆé…·çš„äº‹æƒ…ã€‚å†å¾€ä¸‹ï¼Œæˆ‘å·²ç»æä¾›äº†ä¸€äº›å¼€å§‹çš„ç»†èŠ‚ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="aee4" class="kb kc hi nd b fi ob oc l od oe"># Regular Expressions (ReGeX)</span><span id="b845" class="kb kc hi nd b fi of oc l od oe">import re</span></pre><ul class=""><li id="bcab" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated"><strong class="iz hj">è‡ªç„¶è¯­è¨€å·¥å…·åŒ…</strong></li></ul><p id="b393" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">NLTKæ˜¯æ„å»ºPythonç¨‹åºæ¥å¤„ç†äººç±»è¯­è¨€æ•°æ®çš„é¢†å…ˆå¹³å°ã€‚å®ƒæä¾›äº†æ˜“äºä½¿ç”¨çš„ç•Œé¢ï¼Œå¦‚WordNetï¼Œä»¥åŠä¸€å¥—ç”¨äºåˆ†ç±»ã€æ ‡è®°åŒ–ã€è¯å¹²åŒ–ã€æ ‡è®°ã€è§£æå’Œè¯­ä¹‰æ¨ç†çš„æ–‡æœ¬å¤„ç†åº“ï¼Œä»¥åŠå·¥ä¸šçº§NLPåº“çš„åŒ…è£…å™¨ã€‚</p><p id="2949" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du na nb nc nd b">nltk.probability</code>æä¾›äº†è¡¨ç¤ºå’Œå¤„ç†æ¦‚ç‡ä¿¡æ¯çš„ç±»ï¼Œæ¯”å¦‚<code class="du na nb nc nd b">FreqDist</code>ï¼Œæˆ‘ä»¬ç¨åä¼šç”¨åˆ°ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="92b4" class="kb kc hi nd b fi ob oc l od oe"># Natural Language Toolkit</span><span id="4532" class="kb kc hi nd b fi of oc l od oe">import nltk</span><span id="d682" class="kb kc hi nd b fi of oc l od oe">from nltk.probability import *</span><span id="c8d4" class="kb kc hi nd b fi of oc l od oe">from nltk.corpus import stopwords</span></pre><ul class=""><li id="8fc7" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated"><strong class="iz hj"> Itertools </strong></li></ul><p id="a3b0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Python <code class="du na nb nc nd b">itertools</code>æ¨¡å—æ˜¯å¤„ç†è¿­ä»£å™¨çš„å·¥å…·é›†åˆã€‚ç®€å•åœ°è¯´ï¼Œè¿­ä»£å™¨æ˜¯å¯ä»¥åœ¨<code class="du na nb nc nd b">for</code>å¾ªç¯ä¸­ä½¿ç”¨çš„æ•°æ®ç±»å‹ã€‚Pythonä¸­æœ€å¸¸è§çš„è¿­ä»£å™¨æ˜¯listã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="0288" class="kb kc hi nd b fi ob oc l od oe"># Functions creating iterators for efficient looping</span><span id="d922" class="kb kc hi nd b fi of oc l od oe">import itertools</span><span id="485c" class="kb kc hi nd b fi of oc l od oe">from itertools import chain</span><span id="e8ca" class="kb kc hi nd b fi of oc l od oe">from itertools import groupby</span></pre><h2 id="fd68" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">è®©æˆ‘ä»¬å†™ä¸€äº›ä»£ç ğŸ”¥</h2><p id="1a6f" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">æˆ‘ä»¬å…ˆå¯¼å…¥æ•°æ®ã€‚GitHubä¸Šæœ‰ä¸€ä¸ªåä¸º<code class="du na nb nc nd b">data.txt</code>çš„æ–‡ä»¶ä¾›ä½ å‚è€ƒã€‚æˆ‘æŠŠå®ƒä¿å­˜åœ¨æœ¬åœ°ç”µè„‘ä¸Šï¼Œå’Œæˆ‘çš„Jupyterç¬”è®°æœ¬æ–‡ä»¶æ”¾åœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹é‡Œã€‚</p><p id="a393" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">åœ¨è¯»å–æ–‡ä»¶ä¹‹å‰ï¼Œä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œå¹¶å°†å…¶å‘½åä¸º<code class="du na nb nc nd b">data</code>ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="2108" class="kb kc hi nd b fi ob oc l od oe">data = []</span></pre><p id="1a76" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ç„¶åæˆ‘ä»¬ç®€å•çš„è¯»å–<code class="du na nb nc nd b">data.txt</code>å¹¶ä¿å­˜åœ¨åˆ—è¡¨<code class="du na nb nc nd b">data</code>ä¸­ã€‚ç¡®ä¿ä½ å®šä¹‰äº†<strong class="iz hj">ç¼–ç æ ¼å¼</strong> <code class="du na nb nc nd b">utf8</code>ï¼Œå¦åˆ™ä½ å¯èƒ½ä¼šå¾—åˆ°ä¸€ä¸ªé”™è¯¯ã€‚</p><ul class=""><li id="30d1" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated">æ ·æœ¬è¯¯å·®:</li></ul><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="c888" class="kb kc hi nd b fi ob oc l od oe">UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 260893: character maps to &lt;undefined&gt;</span></pre><p id="fd73" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">å¦ä¸€ä¸ª<strong class="iz hj">è€ƒè™‘äº‹é¡¹</strong>æ˜¯ï¼Œæˆ‘ä»¬ä½¿ç”¨<code class="du na nb nc nd b">.lower()</code>å‡½æ•°ç›´æ¥å°†æ–‡æœ¬è½¬æ¢æˆlowerä»¥ä¿æŒä¸€è‡´æ€§ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="a26d" class="kb kc hi nd b fi ob oc l od oe">with open('data.txt', encoding="utf8") as f:<br/>    data = f.read().lower()</span></pre><h2 id="6ce5" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">æ ¼å¼åŒ–å’Œæ¸…ç†âœ‚ï¸ğŸ”¨ ğŸ“Œ</h2><p id="29a0" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦å¼€å§‹æ ‡è®°æ–‡æœ¬çš„è¿‡ç¨‹ã€‚å°†ä¸€ä¸ªå­—ç¬¦åºåˆ—åˆ†æˆå‡ ä¸ªéƒ¨åˆ†çš„ä»»åŠ¡ç§°ä¸ºæ ‡è®°åŒ–ã€‚</p><p id="a099" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">é¦–å…ˆï¼Œæˆ‘ä»¬å¿…é¡»ç§»é™¤æ–‡æœ¬ä¸­çš„æ‰€æœ‰å™ªéŸ³ï¼Œæ¯”å¦‚/-*#@æˆ–ä»»ä½•å…¶ä»–éå•è¯å­—ç¬¦æˆ–å¤šä½™çš„ç©ºæ ¼ï¼Œæˆ‘ä»¬ä½¿ç”¨å¼ºå¤§çš„<code class="du na nb nc nd b">ReGex</code>å·¥å…·æ¥å®Œæˆã€‚</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es og"><img src="../Images/8ab2fa98859c724ca5cb350eee10ac93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zrjjtCTZQSbnZXiMSrHJng.jpeg"/></div></div></figure><p id="ef5e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ä¸ºäº†ä½¿ç”¨ReGexè¿è¡Œæ ¼å¼åŒ–ï¼Œéœ€è¦é‡‡å–ä¸¤ä¸ªæ­¥éª¤ï¼›</p><p id="3124" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">(1)åˆ›å»ºæ¨¡å¼ï¼Œ</p><p id="9395" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">(2)ä½¿ç”¨Pythonä»£ç è¿è¡Œæ¨¡å¼å¹¶æ‰¾åˆ°åŒ¹é…ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="c853" class="kb kc hi nd b fi ob oc l od oe"># (1) create a pattern for REGEX to find and keep matching words only</span><span id="aabd" class="kb kc hi nd b fi of oc l od oe">pattern = re.compile(r"[a-zA-Z]+(?:[-'][a-zA-Z]+)?")</span><span id="c790" class="kb kc hi nd b fi of oc l od oe"># (2)tokenise the words: match the pattern to file's content <br/># and tokenize the content</span><span id="565f" class="kb kc hi nd b fi of oc l od oe">tokenised = pattern.findall(data)</span></pre><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es oh"><img src="../Images/467ab59e3a79f2cb8d4cc962944e379f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*PIKVuAWpPhpXWfbDXiKsfQ.gif"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">å›¾åƒ<a class="ae jt" href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjf4MHO4YTnAhWLyDgGHW9YDI0QjRx6BAgBEAQ&amp;url=http%3A%2F%2Fnlp.cs.tamu.edu%2F&amp;psig=AOvVaw0kkE5JmXeahMHoc6Uvi0S9&amp;ust=1579148397647336" rel="noopener ugc nofollow" target="_blank">æ¥æº</a></figcaption></figure><p id="7cac" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Regexä¸Šæœ‰å¾ˆå¤šåœ¨çº¿èµ„æºï¼Œä½†æˆ‘å‘ç°æœ€æœ‰è¶£çš„æ˜¯https://regex101.com/ã€‚å®ƒä¸ä»…å¯ä»¥å¸®åŠ©æ‚¨å°†æ–‡æœ¬ä¸æ¨¡å¼ç›¸åŒ¹é…ï¼Œè¿˜å¯ä»¥æä¾›ç®€çŸ­è€Œæœ‰ä»·å€¼çš„å†…å®¹ã€‚åœ¨å›¾1ä¸­ï¼Œæˆ‘åœ¨ä»–ä»¬çš„é¡µé¢ä¸Šæä¾›äº†ä¸€ä¸ªç®€å•çš„åŠŸèƒ½åˆ—è¡¨ã€‚</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es oi"><img src="../Images/b2a25f3088565f0816481c2465d7ee1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pwcYEMluczE-AXfHdrjbcw.jpeg"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">å›¾1ï¼Œ<code class="du na nb nc nd b"><a class="ae jt" href="https://regex101.com/" rel="noopener ugc nofollow" target="_blank">regex101</a>.com</code>æä¾›çš„åŠŸèƒ½</figcaption></figure><p id="d663" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">å¯¹Pythonæœ‰ç”¨çš„æ­£åˆ™è¡¨è¾¾å¼èµ„æº:</strong></p><ul class=""><li id="e616" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated"><a class="ae jt" href="https://www.dataquest.io/blog/regular-expressions-data-scientists/" rel="noopener ugc nofollow" target="_blank"> Pythonæ­£åˆ™è¡¨è¾¾å¼æ•°æ®ç§‘å­¦æ•™ç¨‹</a></li><li id="29c8" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated"><a class="ae jt" href="https://docs.python.org/3/library/re.html" rel="noopener ugc nofollow" target="_blank"> Python 3 reæ¨¡å—æ–‡æ¡£</a></li><li id="2afa" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated"><a class="ae jt" href="https://regex101.com/" rel="noopener ugc nofollow" target="_blank">åœ¨çº¿æ­£åˆ™è¡¨è¾¾å¼æµ‹è¯•å™¨å’Œè°ƒè¯•å™¨</a></li></ul><h2 id="2f28" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated"><strong class="ak">ç´¢å¼•æ ‡è®°åŒ–åˆ—è¡¨ğŸ“‡</strong></h2><p id="882b" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">ç°åœ¨ï¼Œæˆ‘å·²ç»æ ¹æ®æ¯ä¸ªæ‹›è˜å¹¿å‘Šä¸­çš„<code class="du na nb nc nd b">id</code>å’Œ<code class="du na nb nc nd b">title</code>å¯¹ä»¤ç‰Œè¿›è¡Œäº†ç´¢å¼•:</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="e8cc" class="kb kc hi nd b fi ob oc l od oe"># pass the length of the 'tokenised' series into a variable</span><span id="ea25" class="kb kc hi nd b fi of oc l od oe">tokenised_len = len(tokenised)</span><span id="2c39" class="kb kc hi nd b fi of oc l od oe"><br/># indexing the tokens based on the position of "id" and "title"</span><span id="e5e7" class="kb kc hi nd b fi of oc l od oe">indexes = [i for i, v in enumerate(tokenised) if v=='id' and i+1 &lt; tokenised_len and tokenised[i+1]=='title']</span></pre><p id="b917" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä»<code class="du na nb nc nd b"><strong class="iz hj">itertools</strong></code> <strong class="iz hj"> </strong> <a class="ae jt" href="https://docs.python.org/3/library/itertools.html" rel="noopener ugc nofollow" target="_blank"> recipes </a>åˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°éå†ä»¤ç‰Œåˆ—è¡¨ï¼Œå¹¶ä¸”<strong class="iz hj">åˆ›å»ºä¸€ä¸ªå­åˆ—è¡¨ï¼Œä»¥åŒ…æ‹¬ä»…ä¸ä¸€ä¸ªä½œä¸šå¹¿å‘Šç›¸å…³çš„ä»¤ç‰Œ</strong>ã€‚è¾“å‡ºå°†æ˜¯ä¸€ä¸ªæ•°æ®å­—å…¸ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="627a" class="kb kc hi nd b fi ob oc l od oe"># from itertools recipes<br/>def pairwise(iterable, fillvalue=None):<br/>    """<br/>       This function iterates through the list of tokens and <br/>       creates sub list to include tokens related to one job ad only<br/>    """<br/>    a, b = iter(iterable), iter(iterable)<br/>    next(b, None)<br/>    return itertools.zip_longest(a, b, fillvalue=fillvalue)</span><span id="5153" class="kb kc hi nd b fi of oc l od oe"><br/># pairwise based on indexes in the last block and store in the 'tokenised' as a list</span><span id="4499" class="kb kc hi nd b fi of oc l od oe">tokenised = [tokenised[i:j] for i,j in pairwise(indexes)]</span></pre><p id="414e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ä¸ºäº†åˆ›å»ºæ•°æ®å­—å…¸ï¼Œæˆ‘ä½¿ç”¨äº†Python <code class="du na nb nc nd b"><strong class="iz hj">itertools</strong></code>ã€‚<strong class="iz hj"> Jason Rigdel </strong>å¯¹Pythonä¸­çš„<code class="du na nb nc nd b"><strong class="iz hj">itertools</strong></code> <strong class="iz hj"> </strong>è¿™ä¸€è¯é¢˜åšäº†å¾ˆå¥½çš„è§£é‡Šï¼Œå¹¶æä¾›äº†ä¸€ç»„ä¾‹å­ã€‚</p><ul class=""><li id="6bdf" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated"><a class="ae jt" rel="noopener" href="/@jasonrigden/a-guide-to-python-itertools-82e5a306cdf8">Python ITER toolsæŒ‡å—</a></li></ul><p id="9d78" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ä½†æ˜¯ï¼Œè¿™ä¸ªåˆ—è¡¨åŒ…å«äº†å¾ˆå¤šåŠŸèƒ½è¯ï¼Œæ¯”å¦‚â€œtoâ€ã€â€œinâ€ã€â€œtheâ€ã€â€œisâ€ç­‰ç­‰ã€‚</p><blockquote class="lr"><p id="5308" class="ls lt hi bd lu lv lw lx ly lz ma js dx translated">è¿™äº›åŠŸèƒ½è¯é€šå¸¸<em class="oj">å¯¹æ–‡æœ¬çš„è¯­ä¹‰æ²¡æœ‰å¤ªå¤§è´¡çŒ®</em>ï¼Œé™¤äº†åœ¨æ–‡æœ¬åˆ†æä¸­å¢åŠ æ•°æ®çš„ç»´åº¦ã€‚</p><p id="466a" class="ls lt hi bd lu lv lw lx ly lz ma js dx translated">å¦å¤–ï¼Œè¯·æ³¨æ„ï¼Œæˆ‘ä»¬çš„ç›®æ ‡é€šå¸¸æ˜¯å»ºç«‹ä¸€ä¸ªé¢„æµ‹åˆ†ç±»æ¨¡å‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹æŠ¥å‘Šçš„å«ä¹‰æ¯”å¯¹è¯­æ³•æ›´æ„Ÿå…´è¶£ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€‰æ‹©åˆ é™¤é‚£äº›å•è¯ï¼Œè¿™æ˜¯ä½ çš„ä¸‹ä¸€ä¸ªä»»åŠ¡ã€‚</p></blockquote><p id="2bf1" class="pw-post-body-paragraph ix iy hi iz b ja mb ij jc jd mc im jf jg md ji jj jk me jm jn jo mf jq jr js hb bi translated">æˆ‘å°†é€šè¿‡ä¿ç•™é‚£äº›åŒ…å«3ä¸ªä»¥ä¸Šå­—ç¬¦çš„æ ‡è®°æ¥æ’é™¤æ‰€æœ‰å°‘äº4ä¸ªå­—ç¬¦çš„æ ‡è®°ï¼Œå¹¶å°†å…¶ä½™çš„æ ‡è®°æ·»åŠ åˆ°ä¸€ä¸ªåä¸º<code class="du na nb nc nd b">to_remove</code>çš„åˆ—è¡¨ä¸­ã€‚è¿™ä¸ªåˆ—è¡¨å°†è¢«æ·»åŠ åˆ°é€šç”¨è‹±è¯­<code class="du na nb nc nd b">stopwords</code>åˆ—è¡¨ä¸­ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="63d3" class="kb kc hi nd b fi ob oc l od oe">tokenised = [[word if len(word) &gt; 3 else "to_remove" for word in job] for job in tokenised]</span></pre><h2 id="5f76" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">åˆ é™¤åœç”¨è¯âœ‚ï¸</h2><p id="2ac6" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">åœç”¨è¯æºå¸¦<em class="ok">å°‘é‡è¯æ±‡å†…å®¹</em>ã€‚</p><blockquote class="lr"><p id="7e0d" class="ls lt hi bd lu lv lw lx ly lz ma js dx translated">å®ƒä»¬ç»å¸¸æ˜¯è‹±è¯­ä¸­çš„åŠŸèƒ½è¯ï¼Œä¾‹å¦‚ï¼Œå† è¯ã€ä»£è¯ã€åŠ©è¯ç­‰ç­‰ã€‚åœ¨NLPå’ŒIRä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä»è¯æ±‡è¡¨ä¸­æ’é™¤åœç”¨è¯ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬å°†é¢ä¸´<a class="ae jt" href="https://towardsdatascience.com/the-curse-of-dimensionality-f07c66128fe1" rel="noopener" target="_blank">ç»´åº¦è¯…å’’</a>ã€‚</p></blockquote><p id="6a29" class="pw-post-body-paragraph ix iy hi iz b ja mb ij jc jd mc im jf jg md ji jj jk me jm jn jo mf jq jr js hb bi translated">ä¹Ÿæœ‰ä¸€äº›ä¾‹å¤–ï¼Œæ¯”å¦‚å¥æ³•åˆ†æåƒè§£æï¼Œæˆ‘ä»¬é€‰æ‹©ä¿ç•™é‚£äº›åŠŸèƒ½è¯ã€‚ä½†æ˜¯ï¼Œæ‚¨å°†é€šè¿‡ä½¿ç”¨<strong class="iz hj"> NLTK </strong>ä¸­çš„åœç”¨è¯åˆ—è¡¨æ¥åˆ é™¤ä¸Šé¢åˆ—è¡¨ä¸­çš„æ‰€æœ‰åœç”¨è¯ï¼Œå®ƒæ˜¯:</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="ae8e" class="kb kc hi nd b fi ob oc l od oe">nltk.download('stopwords')</span><span id="5f85" class="kb kc hi nd b fi of oc l od oe">stopwords_list = stopwords.words('english')</span></pre><p id="5849" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">å¯¹äºè¿™ä¸ªä¾‹å­ï¼Œæˆ‘å·²ç»åœ¨æˆ‘çš„GitHubä¸Šæä¾›äº†<code class="du na nb nc nd b">stopwords_en.txt</code>æ–‡ä»¶ï¼Œæ‚¨å¯ä»¥ä»é‚£é‡Œä¸‹è½½ã€‚æˆ‘ä»¬é¦–å…ˆå°†ä¸Šé¢åˆ›å»ºçš„<code class="du na nb nc nd b">to_remove</code>åˆ—è¡¨æ·»åŠ åˆ°<code class="du na nb nc nd b">stopwords_en.txt</code>æ–‡ä»¶ä¸­ï¼Œè¯»å–è¯¥æ–‡ä»¶ï¼Œç„¶åå°†å®ƒä»¬ä¿å­˜ä¸º<code class="du na nb nc nd b">set()</code>ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="4422" class="kb kc hi nd b fi ob oc l od oe"># adding'to_removed' string to the list of stopwords</span><span id="71f8" class="kb kc hi nd b fi of oc l od oe">stopwords = []</span><span id="55ba" class="kb kc hi nd b fi of oc l od oe">with open('stopwords_en.txt',"a") as f:<br/>    f.write("\nto_remove") #\n to shift to next line</span><span id="005e" class="kb kc hi nd b fi of oc l od oe"><br/>with open('stopwords_en.txt') as f:<br/>    stopwords = f.read().splitlines() #reading stopwords line and create stopwords as a list</span><span id="3d72" class="kb kc hi nd b fi of oc l od oe"># convert stopwords into set</span><span id="0a38" class="kb kc hi nd b fi of oc l od oe">stopwordsset = set(stopwords)</span></pre><p id="7398" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ä½ å¯èƒ½æƒ³çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘ä»¬æŠŠ<code class="du na nb nc nd b">stopwords</code>ä¿å­˜ä¸º<code class="du na nb nc nd b">set</code>ã€‚è¿™æ˜¯ä¸ªå¥½é—®é¢˜â€¦â€¦Python<code class="du na nb nc nd b">set</code>æ¯”<code class="du na nb nc nd b">list</code>æ›´å¥½ï¼Œå› ä¸º<code class="du na nb nc nd b">set</code>åœ¨æœç´¢å¤§é‡<strong class="iz hj"> <em class="ok">å¯æ•£åˆ—</em> </strong>é¡¹ç›®æ–¹é¢æ¯”åˆ—è¡¨è¿è¡Œå¾—å¿«å¾—å¤šã€‚</p><p id="93f9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">æ¥ä¸‹æ¥ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªåä¸º<code class="du na nb nc nd b">purifier()</code>çš„å‡½æ•°ï¼Œå®ƒé€šè¿‡ç§»é™¤<code class="du na nb nc nd b">stopwords</code>æ¥å‡€åŒ–ä»¤ç‰Œï¼Œç„¶åè¿è¡Œ<code class="du na nb nc nd b">tokenised</code>åˆ—è¡¨ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="14f6" class="kb kc hi nd b fi ob oc l od oe">def purifier(tokenList,remove_token):<br/>    """<br/>        This function takes two input (list of current tokens <br/>        and list of tokens to be removed)<br/>        The function converts the list into set to improve the <br/>        performance<br/>        and return a list of sets each of which include purified <br/>        tokens and remove_token lists are removed<br/>    """<br/>    return [set(word for word in job if word not in remove_token) for job in tokenList]</span><span id="0cda" class="kb kc hi nd b fi of oc l od oe"># running the 'purifier' function</span><span id="5c77" class="kb kc hi nd b fi of oc l od oe">tokenised = purifier(tokenised,stopwordsset)</span></pre><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es ol"><img src="../Images/fa8424bd4e4259c95abd7207204e6a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*cWfW76Tdy3iWAoKeW5rRtg.png"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">ç…§ç‰‡æ¥è‡ªshutterstockå›¾ä¹¦é¦†</figcaption></figure><p id="3980" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">æ¥ä¸‹æ¥æ˜¯åˆ é™¤åœ¨ä¸€ä¸ªæ‹›è˜å¹¿å‘Šæè¿°ä¸­åªå‡ºç°ä¸€æ¬¡çš„<code class="du na nb nc nd b">words</code>ï¼Œå°†å®ƒä»¬(æ— é‡å¤)ä¿å­˜ä¸ºtxtæ–‡ä»¶(å‚è€ƒæ‰€éœ€è¾“å‡º)ã€‚ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦ä»ç”Ÿæˆçš„è¯æ±‡è¡¨ä¸­æ’é™¤è¿™äº›å•è¯ã€‚</p><p id="141a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨<code class="du na nb nc nd b"><strong class="iz hj">chain()</strong></code>åŠŸèƒ½å°†æ‰€æœ‰æ‹›è˜å¹¿å‘Šä¸­çš„æ‰€æœ‰å•è¯åˆ—æˆä¸€ä¸ªåˆ—è¡¨ã€‚åœ¨â€œ<a class="ae jt" rel="noopener" href="/@jasonrigden/a-guide-to-python-itertools-82e5a306cdf8">Python ITER tools</a>æŒ‡å—â€ä¸­ï¼Œæœ‰ä¸€ä¸ªå…³äº<code class="du na nb nc nd b">chain()</code>å‡½æ•°å¦‚ä½•å·¥ä½œçš„å¾ˆå¥½çš„è§£é‡Šã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="5b2a" class="kb kc hi nd b fi ob oc l od oe">stop_wrds_removed_words = list(chain.from_iterable([word for word in job] for job in tokenised))</span></pre><p id="73fa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">å°†å•è¯åˆ—è¡¨è½¬æ¢ä¸ºé›†åˆä»¥åˆ é™¤é‡å¤é¡¹å¹¶åˆ›å»ºè¯æ±‡é›†åˆ</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="37b6" class="kb kc hi nd b fi ob oc l od oe">stop_wrds_removed_vocab = set(stop_wrds_removed_words)</span></pre><p id="b031" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">æ¥ä¸‹æ¥æ˜¯é€šè¿‡<code class="du na nb nc nd b"><strong class="iz hj">FreqDisrt()</strong></code>å‡½æ•°ä¸­çš„å•è¯æ¥ç»Ÿè®¡ä»¤ç‰Œçš„æ•°é‡ã€‚</p><blockquote class="om on oo"><p id="908f" class="ix iy ok iz b ja jb ij jc jd je im jf op jh ji jj oq jl jm jn or jp jq jr js hb bi translated">FreqDistç±»ç”¨äºå¯¹â€œé¢‘ç‡åˆ†å¸ƒâ€è¿›è¡Œç¼–ç ï¼Œå®ƒè®¡ç®—å®éªŒçš„æ¯ä¸ªç»“æœå‡ºç°çš„æ¬¡æ•°ã€‚å®ƒæ˜¯<code class="du na nb nc nd b">nltk.probability</code>æ¨¡å—ä¸‹çš„ä¸€ä¸ªç±»ã€‚</p><p id="3cf8" class="ix iy ok iz b ja jb ij jc jd je im jf op jh ji jj oq jl jm jn or jp jq jr js hb bi translated">æ ¹æ®<a class="ae jt" href="https://devopedia.org/text-corpus-for-nlp" rel="noopener ugc nofollow" target="_blank"> developedia </a>çš„è¯´æ³•ï¼Œé€šå¸¸ï¼Œæ¯ä¸ªæ–‡æœ¬è¯­æ–™åº“éƒ½æ˜¯æ–‡æœ¬æºçš„é›†åˆã€‚å¯¹äºå„ç§NLPä»»åŠ¡ï¼Œæœ‰å‡ åä¸ªè¿™æ ·çš„è¯­æ–™åº“ã€‚æœ¬æ–‡å¿½ç•¥è¯­éŸ³è¯­æ–™åº“ï¼Œåªè€ƒè™‘æ–‡æœ¬å½¢å¼çš„è¯­æ–™åº“ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæ–‡æœ¬è¯­æ–™åº“æŒ‡çš„æ˜¯æ‰€æœ‰å·¥ä½œå¹¿å‘Šçš„ç»„åˆ(â€¦è€Œä¸æ˜¯æ¯ä¸ªå·¥ä½œå•ç‹¬)ã€‚</p></blockquote><p id="2eae" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ä¸‹é¢çš„lineå‡½æ•°è®¡ç®—ä¸€ä¸ªå•è¯åœ¨æ•´ä¸ª<code class="du na nb nc nd b">corpus</code>ä¸­å‡ºç°çš„æ¬¡æ•°ï¼Œè€Œä¸ç®¡å®ƒåœ¨å“ªä¸ªadä¸­ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="8077" class="kb kc hi nd b fi ob oc l od oe">fd = FreqDist(stop_wrds_removed_words)</span></pre></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h2 id="65b0" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">ä½é¢‘ä»¤ç‰Œ</h2><p id="7651" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">ä¸ºäº†æ‰¾åˆ°ä¸å¤ªé¢‘ç¹çš„ä»¤ç‰Œï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªåªå‡ºç°è¿‡ä¸€æ¬¡çš„ä»¤ç‰Œåˆ—è¡¨ï¼Œå¹¶å°†è¯¥åˆ—è¡¨è½¬æ¢ä¸º<code class="du na nb nc nd b">set</code>ä»¥æé«˜æ€§èƒ½ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="311d" class="kb kc hi nd b fi ob oc l od oe">once_only = set([k for k, v in fd.items() if v == 1])</span><span id="e95b" class="kb kc hi nd b fi of oc l od oe"># sort the set into alphabetical order</span><span id="bde5" class="kb kc hi nd b fi of oc l od oe">once_only = sorted(once_only)</span><span id="4175" class="kb kc hi nd b fi of oc l od oe"><br/>set(once_only)</span></pre><p id="319d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ä¸ºäº†åˆ›å»º<code class="du na nb nc nd b">lowFreq.txt</code>æ–‡ä»¶ï¼Œæˆ‘å·²ç»å°†åœ¨ä¸€ä¸ªæ‹›è˜å¹¿å‘Šæè¿°ä¸­å‡ºç°â€œä»…ä¸€æ¬¡â€çš„å•è¯çš„æ’åº<code class="du na nb nc nd b">set</code>ä¿å­˜åˆ°ä¸€ä¸ªåŒåæ–‡ä»¶ä¸­ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="03a9" class="kb kc hi nd b fi ob oc l od oe">out_file = open("lowFreq.txt", 'w')<br/>for d in once_only:<br/>    out_file.write(''.join(d) + '\n')<br/>out_file.close()</span></pre><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es os"><img src="../Images/d4ce25e2dfa65ae46764a298fa807449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rm8HmCWd017_LbUSKgh3Tg.png"/></div></div></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h2 id="3c40" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">é«˜é¢‘ä»¤ç‰Œ</h2><p id="db0a" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘é‡å¤ä¸Šé¢ç›¸åŒçš„æ­¥éª¤ï¼Œä½†æ˜¯ï¼Œè¿™ä¸€æ¬¡çš„ç›®çš„æ˜¯æ‰¾åˆ°é«˜é¢‘è¯å¹¶æŠŠå®ƒä»¬ä¿å­˜åœ¨åä¸º<code class="du na nb nc nd b">highFreq.txt</code>çš„æ–‡ä»¶ä¸­ã€‚<br/>æˆ‘é¦–å…ˆé€šè¿‡è¿è¡Œæˆ‘ä»¬ä¹‹å‰å®šä¹‰çš„<code class="du na nb nc nd b">purifier()</code>å‡½æ•°ï¼Œä»ä»¤ç‰Œçš„<code class="du na nb nc nd b">list</code>ä¸­ç§»é™¤<code class="du na nb nc nd b">lowFreq</code>ä»¤ç‰Œã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="08d7" class="kb kc hi nd b fi ob oc l od oe">tokenised = purifier(tokenised,once_only)</span></pre><p id="eaa6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ä¸‹ä¸€æ­¥æ˜¯åœ¨ç§»é™¤äº†<code class="du na nb nc nd b">once_only</code>ä¸ªå•è¯ä¹‹ååˆ›å»ºä¸€ä¸ªæ–°çš„<code class="du na nb nc nd b">list</code>ä¸ªå•è¯ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="23da" class="kb kc hi nd b fi ob oc l od oe">LowFreqRemoved_Words = list(chain.from_iterable([word for word in job] for job in tokenised))</span><span id="46e0" class="kb kc hi nd b fi of oc l od oe"><br/>LowFreqRemoved_vocab = set(LowFreqRemoved_Words)</span><span id="db34" class="kb kc hi nd b fi of oc l od oe"><br/>LowFreqRemoved_fd = FreqDist(LowFreqRemoved_Words)</span></pre><p id="e0c7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">å¯¹äºé«˜é¢‘è¯ï¼Œæˆ‘é€‰æ‹©äº†100ä¸ªé˜ˆå€¼ã€‚ä½ å¯ä»¥æ ¹æ®ä½ çš„å·¥ä½œç¯å¢ƒé€‰æ‹©ä»»ä½•é—¨æ§›ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="6c42" class="kb kc hi nd b fi ob oc l od oe">highFreq = set([k for k, v in LowFreqRemoved_fd.items() if v &gt; 100])</span></pre><p id="cca3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ç°åœ¨ï¼Œå°†åœ¨100å¤šä¸ªæ‹›è˜å¹¿å‘Šæè¿°ä¸­å‡ºç°çš„é«˜é¢‘è¯çš„æ’åºåˆ—è¡¨ä¿å­˜åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="e8ad" class="kb kc hi nd b fi ob oc l od oe">out_file = open("highFreq.txt", 'w')<br/>for d in highFreq:<br/>    out_file.write(''.join(d) + '\n')<br/>out_file.close()</span></pre><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es ot"><img src="../Images/174bc5c558a8ce9268166aa2f6d7e370.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6GLm05BN8s3TMBUMjQO1zw.png"/></div></div></figure><p id="91ff" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">æˆ‘ä»¬å†æ¬¡è¿è¡Œ<code class="du na nb nc nd b">purifier()</code>å‡½æ•°æ¥åˆ é™¤<code class="du na nb nc nd b">highFreq</code>æ•°æ®é›†å¹¶åˆ›å»ºä¸€ä¸ªæ–°çš„<code class="du na nb nc nd b">list</code>ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="04de" class="kb kc hi nd b fi ob oc l od oe">tokenised = purifier(tokenised,highFreq)</span><span id="6d74" class="kb kc hi nd b fi of oc l od oe"><br/>HighFreqRemoved_words = list(chain.from_iterable([word for word in job] for job in tokenised))</span><span id="e2d0" class="kb kc hi nd b fi of oc l od oe">HighFreqRemoved_vocab = set(HighFreqRemoved_words)</span></pre><h2 id="7a3d" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">æ³¨æ„</h2><p id="1031" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">ä½ å¯èƒ½æƒ³çŸ¥é“åœ¨æˆ‘çš„ä»£ç ä¸­<code class="du na nb nc nd b">words</code>å’Œ<code class="du na nb nc nd b">vocab</code>åˆ—è¡¨æœ‰ä»€ä¹ˆä¸åŒï¼Œä¸ºä»€ä¹ˆæ¯æ¬¡æˆ‘åˆ›å»ºä¸€ä¸ª<code class="du na nb nc nd b">words</code>åˆ—è¡¨ï¼Œç„¶åä¸€ä¸ª<code class="du na nb nc nd b">vocab</code>ä¹Ÿè¢«åˆ›å»ºã€‚åŸå› è¿˜è¦è¿½æº¯åˆ°Pythonä¸­<code class="du na nb nc nd b">list</code>å’Œ<code class="du na nb nc nd b">set</code>çš„åŒºåˆ«ã€‚ç„¶è€Œåº•çº¿æ˜¯åœ¨<code class="du na nb nc nd b">vocab</code>ä¸­æ¯ä¸ªå•è¯åªè¢«åˆ—å‡ºä¸€æ¬¡ï¼Œè€Œ<code class="du na nb nc nd b">words</code>å¯èƒ½æœ‰é‡å¤ã€‚</p><p id="8594" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">æ¥ä¸‹æ¥æ˜¯ä¸€ä¸ªç®€å•çš„æ£€æŸ¥ç‚¹ï¼Œç”¨äºæŸ¥çœ‹ä»¤ç‰Œçš„æçº¯è¿›åº¦:</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="71a6" class="kb kc hi nd b fi ob oc l od oe">print(f"Length of words: {len(stop_wrds_removed_words)}")</span><span id="2813" class="kb kc hi nd b fi of oc l od oe">print(f"Length of vocab: {len(stop_wrds_removed_vocab)}")</span><span id="4104" class="kb kc hi nd b fi of oc l od oe">print(f"Length of LowFreqRemoved_Words: {len(LowFreqRemoved_Words)}")</span><span id="00d3" class="kb kc hi nd b fi of oc l od oe">print(f"Length of LowFreqRemoved_vocab: {len(LowFreqRemoved_vocab)}")</span><span id="7726" class="kb kc hi nd b fi of oc l od oe">print(f"Length of HighFreqRemoved_words: {len(HighFreqRemoved_words)}")</span><span id="d237" class="kb kc hi nd b fi of oc l od oe">print(f"Length of HighFreqRemoved_vocab: {len(HighFreqRemoved_vocab)}")</span></pre><p id="bd84" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">â€¦è¿™ä¸ºæˆ‘ä»¬æä¾›äº†ä»¥ä¸‹è¾“å‡º:</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="8e7e" class="kb kc hi nd b fi ob oc l od oe">Length of words: 474345<br/>Length of vocab: 18619<br/>Length of LowFreqRemoved_Words: 465779<br/>Length of LowFreqRemoved_vocab: 10053<br/>Length of HighFreqRemoved_words: 126491<br/>Length of HighFreqRemoved_vocab: 9103</span></pre></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="1a9a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">æ¥ä¸‹æ¥æ˜¯åˆ›å»ºä¸€ä¸ªåä¸º<code class="du na nb nc nd b">vocab.txt</code>çš„æ‰€æœ‰è¯æ±‡çš„æ–‡ä»¶ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="b9a9" class="kb kc hi nd b fi ob oc l od oe">HighFreqRemoved_vocab = list(HighFreqRemoved_vocab)</span><span id="b4aa" class="kb kc hi nd b fi of oc l od oe"><br/># list of final vocabs</span><span id="78a1" class="kb kc hi nd b fi of oc l od oe">vocab = {HighFreqRemoved_vocab[i]:i for i in range(0,len(HighFreqRemoved_vocab))}</span></pre><p id="429f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">æ„å»ºä¸€ä¸ªå‡½æ•°æ¥åˆ›å»º<code class="du na nb nc nd b">vocab.txt</code>æ–‡ä»¶ï¼Œæœ€åé€šè¿‡è°ƒç”¨ä»¥ä¸‹å‡½æ•°æ¥æ„å»ºå’Œæ’åºè¯¥æ–‡ä»¶:</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="d5a1" class="kb kc hi nd b fi ob oc l od oe">def vaocab_output(file):<br/>    with open (file, "a") as f:<br/>        for key in sorted(vocab.keys()):<br/>            f.write("%s:%s\n" % (key, vocab[key]))</span><span id="2c38" class="kb kc hi nd b fi of oc l od oe"># calling the function to build the file</span><span id="9fb9" class="kb kc hi nd b fi of oc l od oe">vaocab_output("vocab.txt")</span></pre><p id="f488" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du na nb nc nd b">vocab.txt</code>è¾“å‡º:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es ou"><img src="../Images/10261f935ac2fbfe2f500c1a974276b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0WliTu2MnmQ4TX-acSSkA.png"/></div></div></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="79bc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">åˆ°ç›®å‰ä¸ºæ­¢ï¼Œå‡ºäºç»ƒä¹ çš„ç›®çš„ï¼Œæˆ‘å°½é‡ä¿æŒä»£ç ç®€å•ã€‚ç„¶è€Œï¼Œå¯¹äºè¿™ä¸€æ­¥ï¼Œæˆ‘åˆ›å»ºäº†ä¸€æ®µæ›´å¤æ‚ä½†æ›´æœ‰æ•ˆçš„ä»£ç ã€‚è¯·åœ¨è¯„è®ºä¸­ç•™ä¸‹ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä¿è¯ä»–ä»¬ä¼šå¾—åˆ°å›ç­”ã€‚</p><p id="da56" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">æœ€åçš„æ´»åŠ¨æ˜¯ä»¥é€—å·åˆ†éš”çš„<code class="du na nb nc nd b">word_index:word_freq</code>çš„å½¢å¼ç¨€ç–è¡¨ç¤ºç›¸åº”çš„æè¿°å¹¶åˆ›å»ºæ–‡ä»¶<code class="du na nb nc nd b">sparse.txt</code>ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="9536" class="kb kc hi nd b fi ob oc l od oe">data = {}<br/>id = None<br/>with open('data.txt', 'r',encoding="utf8") as f:<br/>    for i, line in enumerate(f): # create the iteration in the range of imported file's length<br/>        line = line.lower() <br/>        line = line.strip()<br/>        if not line:<br/>            continue<br/>        section = line.split(':')[0] # define 'section' as a method to manupilate each line based on how the line begins<br/>        content = ':'.join(line.split(':')[1:]).strip() # define 'content' a method to capture tokens<br/>        if section == 'id': # id section:<br/>            if id: # Error handle if theres some bad formatting: multiple ids<br/>                raise ValueError('unable to parse file at line %d, multiple ids' % i)<br/>            id = content[1:] # capture the job id<br/>            if id in data.keys():# Error handle if theres some bad formatting: duplicates<br/>                raise ValueError('unable to parse file at line %d, duplicate id' % i)<br/>        elif section == 'description': #capture job description per each job ad<br/>            if not id:# Error handle if theres some bad formatting: missing id<br/>                raise ValueError('unable to parse file at line %d, missing id' % i)<br/>            content = pattern.findall(line)<br/>            content = [value for value in content if len(value) &gt; 3] # remove short character token<br/>            content = [value for value in content if value not in stopwordsset] # remove stopwords<br/>            content = [value for value in content if value not in once_only] # remove lowFreq token<br/>            content = [value for value in content if value not in highFreq] # remove highFreq tokens<br/>            data[id] = content # creates data dictionary<br/>            id = None<br/>        elif section == 'title': # if the line start with 'title' do nothing<br/>            continue<br/>        else:<br/>            raise ValueError('unable to parse file at line %d, unexpected section name' % i)</span></pre><p id="d9a3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">æœ€åæ„å»º<code class="du na nb nc nd b">sparse.txt</code>æ–‡ä»¶ã€‚</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="4545" class="kb kc hi nd b fi ob oc l od oe">with open('sparse.txt',"w") as f:<br/>    for jobID,content in data.items(): # go through data dictionary created in the last block<br/>        fd_parse = FreqDist(content) # count number of times each token occured in the same job ad<br/>        tmp = "" # create a placeholder for word_index:word_freq<br/>        for (x,y) in fd_parse.items(): # iterate through each frequencies<br/>            tmp += f"{vocab[x]}:{y}," # build the dictionary of word_index:word_freq in the placeholder<br/>        f.write(f"#{jobID},{tmp[:-1]}\n") # write in the file line by line</span></pre><p id="ea0c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du na nb nc nd b">sparse.txt</code>çš„è¾“å‡º:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es ov"><img src="../Images/ff7c05bdc84a1a0045b5fb1c98c0d7ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*edqWIkdSj9hkE5uELDynEQ.png"/></div></div></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="f454" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">è¿›ä¸€æ­¥äº†è§£</p><ul class=""><li id="9259" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated"><a class="ae jt" href="https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments" rel="noopener ugc nofollow" target="_blank">è®¡æ•°çŸ¢é‡å™¨ï¼ŒtfidfçŸ¢é‡å™¨ï¼Œé¢„æµ‹Kaggleä¸Šçš„è¯„è®ºæ•™ç¨‹</a></li><li id="f69d" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" rel="noopener ugc nofollow" target="_blank">å°†ä¸€ç»„æ–‡æœ¬æ–‡æ¡£è½¬æ¢æˆä¸€ä¸ªä»¤ç‰Œè®¡æ•°çŸ©é˜µ</a></li></ul></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="7162" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">â€”â€”ç»“æŸâ€”</p></div></div>    
</body>
</html>