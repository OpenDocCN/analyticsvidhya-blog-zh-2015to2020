<html>
<head>
<title>Reinforcement Learning: Introduction to Monte Carlo Learning using the OpenAI Gym Toolkit</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习:使用OpenAI Gym工具包介绍蒙特卡罗学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-introduction-to-monte-carlo-learning-using-the-openai-gym-toolkit-4efef9375648?source=collection_archive---------0-----------------------#2018-11-19">https://medium.com/analytics-vidhya/reinforcement-learning-introduction-to-monte-carlo-learning-using-the-openai-gym-toolkit-4efef9375648?source=collection_archive---------0-----------------------#2018-11-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d0bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">听到“强化学习”这四个字，你首先想到的是什么？最常见的想法是——太复杂，数学太多。但是我在这里向你保证，这是一个非常迷人的研究领域——我的目标是在我的文章中将这些技术分解成易于理解的概念。</p><p id="9e31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我相信你一定听说过OpenAI和DeepMind。这是两个领先的人工智能组织，他们在该领域取得了重大进展。一个OpenAI机器人团队在Dota 2中击败了一个业余玩家团队，这是一个非常受欢迎和复杂的战斗竞技场游戏。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/139f8381792e057b45187e637c04c62c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*561HC-egCRjzGUEV.jpg"/></div></div></figure><p id="0628" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不幸的是，这是一个不去。状态实在是太多了(数以百万计)，收集DOTA 2的所有细节是一项不可能完成的任务。这就是我们进入强化学习或者更具体地说无模型学习领域的地方。</p><p id="9073" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将尝试了解蒙特卡洛学习的基础知识。当没有环境的先验信息，并且所有信息基本上都是通过经验收集的时候，就使用它。我们将使用Python中的OpenAI Gym工具包来实现这个方法。</p><p id="c730" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们开始吧！</p><p id="487d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jp">如果你是这一领域的初学者，或者需要快速复习一些基本的强化学习术语，我强烈推荐你阅读下面的文章，以便从这篇文章中获得最大的收获:</em></p><ol class=""><li id="c380" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated"><a class="ae jz" href="https://www.analyticsvidhya.com/blog/2017/01/introduction-to-reinforcement-learning-implementation/" rel="noopener ugc nofollow" target="_blank"> <em class="jp">强化学习简易入门&amp;其实现</em> </a></li><li id="d48c" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc jv jw jx jy bi translated"><a class="ae jz" href="https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/" rel="noopener ugc nofollow" target="_blank"> <em class="jp">螺母&amp;螺栓的强化学习:基于模型的规划使用动态编程</em> </a></li><li id="18d3" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc jv jw jx jy bi translated"><a class="ae jz" href="https://www.analyticsvidhya.com/blog/2018/09/reinforcement-multi-armed-bandit-scratch-python/" rel="noopener ugc nofollow" target="_blank"> <em class="jp">强化学习指南:用Python从零开始解决多臂土匪问题</em> </a></li></ol><h1 id="b018" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">目录</h1><ol class=""><li id="2956" class="jq jr hi ih b ii ld im le iq lf iu lg iy lh jc jv jw jx jy bi translated">基于模型的学习与无模型的学习</li><li id="9b15" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc jv jw jx jy bi translated">蒙特卡罗方法——一个例子</li><li id="e0b8" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc jv jw jx jy bi translated">蒙特卡洛强化学习<br/> 1。蒙特卡洛预测<br/> 2。蒙特卡洛控制</li><li id="b719" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc jv jw jx jy bi translated">使用OpenAI Gym在Python中实现</li></ol><h1 id="de62" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">基于模型的学习与无模型的学习</h1><p id="6f4a" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">我们知道，动态编程用于解决环境的底层模型事先已知的问题(或者更准确地说，基于模型的学习)。强化学习就是在玩游戏中从经验中学习。然而，在所有的动态编程算法中，我们都没有真正地玩游戏/体验环境。我们有一个完整的环境模型，包括所有的状态转移概率。</p><p id="b319" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，正如我们在引言中看到的，在大多数现实生活情况下，从一个状态到另一个状态的转移概率(或所谓的环境模型)是事先不知道的。任务甚至不必遵循马尔可夫性质。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ll"><img src="../Images/5484db86e1123e4b5e0f54aa9c2ec8a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/0*Bwo3vJ6Jknhmz6HI.jpg"/></div></figure><p id="0db3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">比方说，我们想训练一个机器人学习如何下棋。考虑将象棋环境转换为MDP。</p><p id="e38d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，根据棋子的定位，这个环境将有许多状态(超过1050个)，以及大量可能的动作。这种环境的模型几乎无法设计！</p><p id="f96e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个潜在的解决方案可能是重复地玩一盘完整的棋，在每局结束时，赢了得到正的奖励，输了得到负的奖励。这叫从经验中学习。</p><h1 id="7884" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">蒙特卡罗方法——一个例子</h1><p id="eb0b" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">任何通过产生合适的随机数并观察服从某种性质的那部分数来解决问题的方法，都可以归类为蒙特卡罗方法。</p><p id="78be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们做一个有趣的练习，用纸和笔找出圆周率的值。我们画一个单位长度的正方形，用单位长度半径画一个四分之一圆。现在，我们有一个助手机器人C3PO。它的任务是在正方形上随机放置尽可能多的点3000次，结果如下图所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lm"><img src="../Images/7a8276e716980644cabddc7a5a59fa1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*E6tW7oLCquYr36Us.png"/></div></figure><p id="c6df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">C3PO每次在一个圆里放一个点都需要计数。因此，圆周率的值由下式给出:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ln"><img src="../Images/0a9a50b527b6886c484a4ad2041b9395.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/0*8wRoYfFl8CVzRac3.jpg"/></div></figure><p id="ab40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中N是一个点被放入圆中的次数。正如你所看到的，我们没有做任何事情，除了数落在圆内的随机点，然后用一个比率来近似圆周率的值。</p><h1 id="21f5" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">蒙特卡洛强化学习</h1><p id="5682" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">用于强化学习的蒙特卡罗方法直接从经验片断中学习，而不需要任何MDP跃迁的先验知识。在这里，随机成分是回报或奖励。</p><p id="cb63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jp">有一点需要注意的是，它只能应用于偶发的MDP</em>。在这一点上，有理由问为什么。原因是这一集必须在之前结束<em class="jp">，我们才能计算任何回报。在这里，我们不是在每一个动作之后做一个更新，而是在每一集之后。它使用了最简单的想法——该值是每个状态的所有样本轨迹的平均回报。</em></p><p id="5401" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回想一下这篇<a class="ae jz" href="https://www.analyticsvidhya.com/blog/2018/09/reinforcement-multi-armed-bandit-scratch-python/" rel="noopener ugc nofollow" target="_blank">文章</a>中讨论的多武装匪徒的想法，每个州都是一个独立的多武装匪徒问题，这个想法是让所有多武装匪徒同时表现最佳。</p><p id="03fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与动态规划类似，还有一个策略评估(为给定的随机策略寻找值函数)和策略改进步骤(寻找最优策略)。我们将在接下来的两节中介绍这两个步骤。</p><h1 id="0b9a" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">蒙特卡洛政策评估</h1><p id="dce5" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">同样，这里的目标是从政策pi下的经验中学习价值函数vpi(s)。回想一下，回报是总折扣奖励:</p><p id="9411" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jp"> S1，A1，R2，…Sk ~ pi </em></p><p id="bbee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还记得价值函数是预期收益:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lo"><img src="../Images/a5b2ccaf80c84006de6499ab4d9d4a3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/0*owhAtiupDUIaLeKm.jpg"/></div></figure><p id="ef5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们知道，只需将样本相加，然后除以样本总数，就可以估算出任何期望值:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lp"><img src="../Images/823f8adf93a286f953ba7ab451dd75fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/0*EBDE43o5AM913nwN.png"/></div></figure><ul class=""><li id="d563" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc lq jw jx jy bi translated">i —情节索引</li><li id="1b13" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc lq jw jx jy bi translated">s——状态指数</li></ul><p id="254e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">问题是我们如何获得这些样本回报？为此，我们需要播放一系列剧集并生成它们。</p><p id="aa47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于我们播放的每一集，我们都会有一系列的状态和奖励。而从这些回报中，我们可以根据定义计算出回报，这只是所有未来回报的总和。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/aa37065a5174c1efabd4048027e954ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*z5LR1ntvxp-ktRwL.png"/></div></figure><p id="8367" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jp">第一次访问蒙特卡洛:</em> </strong> <em class="jp"> </em>平均回报只针对第一次s被访问的一集。</p><p id="9660" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是该算法工作原理的分步视图:</p><ol class=""><li id="7d55" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">初始化策略、状态值函数</li><li id="9380" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc jv jw jx jy bi translated">首先根据当前策略<br/> 1生成一集。记录该事件中遇到的状态</li><li id="ae90" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc jv jw jx jy bi translated">在2.1 <br/> 1中选择一个状态。将此状态<br/> 2首次出现后收到的返回添加到列表中。所有回报的平均值<br/> 3。将状态值设置为计算出的平均值</li><li id="6dcd" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc jv jw jx jy bi translated">重复步骤3</li><li id="e9c1" class="jq jr hi ih b ii ka im kb iq kc iu kd iy ke jc jv jw jx jy bi translated">重复2–4直到满意为止</li></ol><p id="bf94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jp">每次访问蒙特卡洛:</em> </strong> <em class="jp"> </em>每集s每次被访问的平均回报。</p><p id="8ab8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于该算法，我们只需将步骤#3.1更改为“将每次出现该状态后收到的返回添加到列表中”。</p><p id="0d48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑一个简单的例子来进一步理解这个概念。假设在一个环境中，我们有两种状态——A和b。假设我们观察到两个样本集:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/a178a787e3a5edc0ad365b1da107a408.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/0*3cf9MufrTKpmzBwz.png"/></div></figure><p id="2e2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">A+3 =&gt; A表示从状态A到状态A的转换，奖励为+3。让我们用两种方法找出价值函数:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/99b12ee940c9c9f4a98b427934c7b4df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/0*Sf2ZaJ22XQtJYWUu.jpg"/></div></figure><h1 id="1695" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">增量平均值</h1><p id="08ec" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">将均值回报转换为增量更新是很方便的，这样均值可以随着每集更新，我们可以了解每集取得的进展。我们在解决多臂强盗问题时已经学到了这一点。</p><p id="ccc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在剧集结束后逐步更新v(s)。对于每个状态St，返回Gt:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/9b0e963865df762a01fc25623f5f4619.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/0*zR8rY-6KV5YsFDOt.png"/></div></figure><p id="ac3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在非平稳问题中，跟踪运行均值(即忘记旧事件)可能是有用的:</p><p id="c7ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">V(St)←V(St)+α(Gt V(St))</p><h1 id="43d0" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">蒙特卡洛控制</h1><p id="b78b" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">类似于动态规划，一旦我们有了随机策略的价值函数，剩下的重要任务就是使用蒙特卡罗找到最优策略。</p><p id="8f57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回想一下，DP中的政策改进公式需要环境模型，如下式所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/76428e7040b1abee3103056f4b142e29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/0*5WPPr1Tz5k3VcsYg.jpg"/></div></figure><p id="709c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个等式通过寻找使回报总和最大化的行动来找出最优策略。然而，这里的一个主要警告是，它使用转移概率，这在无模型学习的情况下是未知的。</p><p id="5445" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于我们不知道状态转移概率<em class="jp"> p(s '，r/s，a) </em>，我们不能像DP一样进行前瞻搜索。因此，所有信息都是通过玩游戏或探索环境的经验获得的。</p><p id="adf2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过使策略相对于当前值函数变得贪婪来完成策略改进。在这种情况下，我们有一个动作值函数，因此不需要模型来构建贪婪策略。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/6a71528f2e29ccbb7dce034d2fdd7899.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/0*GjEnbxud9w2CCugf.jpg"/></div></figure><p id="0679" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个贪婪的策略(就像上面提到的)将总是倾向于某个特定的行为，如果大多数行为没有被适当地探索的话。对此有两种解决方案:</p><p id="7b67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">蒙特卡洛探索开始</strong></p><p id="26e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在该算法中，所有状态动作对都有非零概率成为起始对。这将确保播放的每一集将代理带到新的状态，因此，有更多的环境探索。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lx"><img src="../Images/3c1672ebeba094cf753feab9687b1893.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xYJdojIYkz2HihLA.jpg"/></div></div></figure><p id="e055" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">蒙特卡罗与ε-Soft</strong></p><p id="f95a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果一个环境(例如，一盘棋)只有一个起点，该怎么办？在这种情况下，探索起点并不是正确的选择。回想一下，在多臂土匪问题中，我们讨论了<a class="ae jz" href="https://www.analyticsvidhya.com/blog/2018/09/reinforcement-multi-armed-bandit-scratch-python/" rel="noopener ugc nofollow" target="_blank">ε贪婪方法</a>。</p><p id="a6e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">确保持续探索的最简单想法所有行动都以非零概率尝试1-ε选择最大化行动价值函数的行动，以概率ε随机选择行动。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ly"><img src="../Images/a8f681f47ba0835c2f89ba11116b10a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*neEL22zGbjnvIez7.jpg"/></div></div></figure><p id="7742" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们已经了解了蒙特卡罗控制和预测的基础，让我们用Python实现这个算法。我们将从流行的OpenAI健身房工具包中导入冰冻的湖泊环境。</p><h1 id="c01d" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">用Python实现蒙特卡罗</h1><h1 id="810e" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">冰冻湖泊环境</h1><p id="0794" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">代理控制网格世界中角色的移动。网格的一些瓦片是可行走的，其他的导致代理人掉进水里。此外，代理的移动方向是不确定的，并且仅部分取决于所选择的方向。代理人因找到一条通往目标方块的可行走路径而获得奖励。</p><p id="eedd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用如下所示的网格来描述表面:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/99241999f2b882d04da178491a339721.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*R1BuAFbeKPyjMdtF.png"/></div></figure><p id="8085" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(S:起点，安全)，(F:冰面，安全)，(H:洞，摔到你的末日)，(G:球门)</p><p id="1e82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个想法是通过只在冰冻的表面上行走并避开所有的洞，从起点到达目标。OpenAI Gym的安装细节和文档可通过此<a class="ae jz" href="https://gym.openai.com/docs/" rel="noopener ugc nofollow" target="_blank">链接</a>获得。我们开始吧！</p><p id="03f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">­­</p><p id="84d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们将定义几个辅助函数来设置蒙特卡罗算法。</p><p id="3cb8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">创造环境</strong></p><pre class="je jf jg jh fd lz ma mb mc aw md bi"><span id="b15d" class="me kg hi ma b fi mf mg l mh mi">import gym<br/>import numpy as np<br/>import operator<br/>from IPython.display import clear_output<br/>from time import sleep<br/>import random<br/>import itertools<br/>import tqdm<br/><br/>tqdm.monitor_interval = 0</span></pre><p id="8bd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">随机策略功能</strong></p><pre class="je jf jg jh fd lz ma mb mc aw md bi"><span id="a778" class="me kg hi ma b fi mf mg l mh mi">def create_random_policy(env):<br/>     policy = {}<br/>     for key in range(0, env.observation_space.n):<br/>          current_end = 0<br/>          p = {}<br/>          for action in range(0, env.action_space.n):<br/>               p[action] = 1 / env.action_space.n<br/>          policy[key] = p<br/>     return policy</span></pre><p id="406b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">用于存储状态动作值的字典</strong></p><pre class="je jf jg jh fd lz ma mb mc aw md bi"><span id="1827" class="me kg hi ma b fi mf mg l mh mi">def create_state_action_dictionary(env, policy):<br/>    Q = {}<br/>    for key in policy.keys():<br/>         Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}<br/>    return Q</span></pre><p id="0934" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">播放剧集的功能</strong></p><pre class="je jf jg jh fd lz ma mb mc aw md bi"><span id="61b6" class="me kg hi ma b fi mf mg l mh mi">def run_game(env, policy, display=True):<br/>     env.reset()<br/>     episode = []<br/>     finished = False<br/><br/>     while not finished:<br/>          s = env.env.s<br/>          if display:<br/>               clear_output(True)<br/>               env.render()<br/>               sleep(1)<br/><br/>          timestep = []<br/>          timestep.append(s)<br/>           n = random.uniform(0, sum(policy[s].values()))<br/>           top_range = 0<br/>           for prob in policy[s].items():<br/>                 top_range += prob[1]<br/>                 if n &lt; top_range:<br/>                       action = prob[0]<br/>                       break <br/>           state, reward, finished, info = env.step(action)<br/>           timestep.append(action)<br/>           timestep.append(reward)<br/><br/>           episode.append(timestep)<br/><br/>     if display:<br/>          clear_output(True)<br/>          env.render()<br/>          sleep(1)<br/>     return episode</span></pre><p id="4804" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">测试策略和打印胜率的功能</strong></p><pre class="je jf jg jh fd lz ma mb mc aw md bi"><span id="3049" class="me kg hi ma b fi mf mg l mh mi">def test_policy(policy, env):<br/>      wins = 0<br/>      r = 100<br/>      for i in range(r):<br/>            w = run_game(env, policy, display=False)[-1][-1]<br/>            if w == 1:<br/>                  wins += 1<br/>      return wins / r</span></pre><p id="a22a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">首次访问蒙特卡罗预测与控制</strong></p><pre class="je jf jg jh fd lz ma mb mc aw md bi"><span id="1e69" class="me kg hi ma b fi mf mg l mh mi">def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):<br/>    if not policy:<br/>        policy = create_random_policy(env)  # Create an empty dictionary to store state action values    <br/>    Q = create_state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair<br/>    returns = {} # 3.<br/>    <br/>    for _ in range(episodes): # Looping through episodes<br/>        G = 0 # Store cumulative reward in G (initialized at 0)<br/>        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively <br/>        <br/>        # for loop through reversed indices of episode array. <br/>        # The logic behind it being reversed is that the eventual reward would be at the end. <br/>        # So we have to go back from the last timestep to the first one propagating result from the future.<br/>        <br/>        for i in reversed(range(0, len(episode))):   <br/>            s_t, a_t, r_t = episode[i] <br/>            state_action = (s_t, a_t)<br/>            G += r_t # Increment total reward by reward on current timestep<br/>            <br/>            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # <br/>                if returns.get(state_action):<br/>                    returns[state_action].append(G)<br/>                else:<br/>                    returns[state_action] = [G]   <br/>                    <br/>                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes<br/>                <br/>                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value<br/>                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]<br/>                max_Q = random.choice(indices)<br/>                <br/>                A_star = max_Q # 14.<br/>                <br/>                for a in policy[s_t].items(): # Update action probability for s_t in policy<br/>                    if a[0] == A_star:<br/>                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))<br/>                    else:<br/>                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))<br/><br/>    return policy</span></pre><p id="70d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，是时候运行此算法来解决一个8×8的冰湖环境，并检查奖励:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mj"><img src="../Images/ed50f09cacefc4c5f849a274d2f1b71f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/0*Lhb30Z9njghzKOuw.jpg"/></div></figure><p id="7691" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">运行50，000集，我们得到0.9分。并且随着剧集越来越多，最终达到最优策略。</p><h1 id="0b2d" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">结束注释</h1><p id="e9da" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">蒙特卡洛学习的故事并没有到此结束。在此之下还有另一套算法，称为<strong class="ih hj">非策略蒙特卡罗方法</strong>。偏离策略方法试图使用从另一个策略生成的回报来学习最优策略。</p><p id="312b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文中讨论的方法是关于政策方法的，基本上就像边做边学。而非政策方法类似于边看别人做工作边学习。我将在后续文章中介绍策略方法。</p><p id="14d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你对这篇文章有任何问题或建议，欢迎在下面的评论区联系我。</p></div><div class="ab cl mk ml gp mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="hb hc hd he hf"><p id="e80e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jp">原载于2018年11月19日</em><a class="ae jz" href="https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/" rel="noopener ugc nofollow" target="_blank"><em class="jp">www.analyticsvidhya.com</em></a><em class="jp">。</em></p></div></div>    
</body>
</html>