# 如何训练神经网络

> 原文：<https://medium.com/analytics-vidhya/how-to-train-neural-networks-3ec2208ae953?source=collection_archive---------16----------------------->

在这篇文章中，我将讲述任何深度学习模型都要遵循的通用蓝图。在这里，我不会深入到深度学习的概念，但这是开发神经网络的基本步骤。根据要求，可以在下面的列表中添加或删除一些步骤。

**1。数据预处理**

我们获得的用于建模的数据大部分时间都是非结构化的和原始的，其中我们有很多我们的案例不需要的数据。所以我们需要保留必要的数据，而忽略其他的

**2。重量初始化**

神经网络建模的第一步是权重初始化，这是一个非常重要的步骤，因为如果权重初始化不正确，就不可能收敛到最小值，但如果方法正确，就可以在最短的时间内实现优化。有几种技术，如零初始化，随机初始化，HE 初始化，glorant 初始化，Xavier 等。

**3。选择正确的激活功能**

激活函数被认为是一个门，可以简单地开或关，也可以转换神经元的输入并给出输出。根据使用情况，您可以选择几种类型的激活功能。一些激活函数大致分为线性和非线性。线性激活函数的问题是不能应用反向传播。如果一个线性变换有多层，它仍然相当于一层，因为线性的函数仍然是线性函数。我们有非线性激活，如 Sigmoid，tanh，ReLu，它解决了线性激活函数的问题。

![](img/667a92848f782378c09834016dd7b679.png)

图片提供:[https://www.google.com/url?sa=i&URL = https % 3A % 2F % 2fwp . wwu . edu % 2f machine learning % 2f 2017% 2f 02% 2f 12% 2f 深度神经网络% 2F&psig = aovvaw 1 Kut 6 urns KH 9 o 6 rn 6 thk 75&ust = 1601991340554000&source = images&CD = vfe&ved = ved](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwp.wwu.edu%2Fmachinelearning%2F2017%2F02%2F12%2Fdeep-neural-networks%2F&psig=AOvVaw1KUT6UrNsKH9O6rN6THk75&ust=1601991340554000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCJDJp__InewCFQAAAAAdAAAAABAD)

**4。批量标准化**

归一化是将所有要素放在一个单一的范围内，例如，可能有值为 1-100 的要素，也可能有值为 0-1 的要素。我们需要将数据标准化为 0–1，这样学习速度会更快。如果输入层可以从归一化中受益，为什么隐藏层不能受益，所以我们也对隐藏层添加批量归一化，特别是对于更接近深度神经网络中的输出层的后面的层，以便收敛变得更容易。标准化使非活化变得更高。

**5。如有需要，添加脱落层**

基本上，添加漏失层是为了避免过度拟合，如果您怀疑模型过度拟合，可以使用该层。丢弃只是在特定层中随机丢弃一些神经元。虽然如果我们加上了丢失，它需要更长的时间来收敛，但是每个时期将需要更短的时间。

**6。选择一个好的优化器**

优化器是改变神经网络属性的方法。例如权重和学习率，以便减少损失。有很多优化器可以选择。例如梯度下降、随机梯度下降、小批量梯度下降、动量、Adagrad、AdaDelta、Adam。Adam 优化器是最近出现的，是迄今为止最好的优化器，它花费的时间更少，对训练任何神经网络都更有效。

**7。超参数调谐**

超参数是所有的训练变量在开始训练前都用预先设定的值手动设定。

一些常见的超参数如下:

*   学习率
*   动力
*   亚当超参数
*   隐藏层数
*   不同层的隐藏单元数量
*   学习率衰减
*   小批量

在所有的超参数中，学习率/sep 大小是最重要的超参数，它告诉我们在梯度中移动多远。如果学习率小，我们会有更多的可靠性，但是收敛需要很多时间。有几种方法来找到学习率，其中一些是，试错法，网格搜索，随机搜索，贝叶斯优化。

**8。损失函数**

最终，ML 或深度学习模型中重要的是减少损失函数。所以在分类任务中，我们需要减少，在回归任务中，我们需要将损失最小化。在分类任务的情况下，我们通常减少日志损失，在多类分类中是多类日志损失。在回归任务的情况下，这将是一个均方损失。

**9。监控梯度**

关于梯度有两种问题，一种是爆炸梯度和消失梯度下降。该模型是不稳定的，导致每次更新的损失变化很大。我们可以说它受到了爆炸梯度的影响。消失梯度问题是，在某些情况下，梯度会变得非常小，从而有效地防止权重改变其值。在最坏的情况下，这可能会完全停止神经网络的进一步训练。我们需要通过使用像梯度裁剪这样的技术来监控梯度。

**10 可视化**

曲线图很好地展示了模型的性能。很容易理解从模型中流出的数据，并对需要对影响模型的参数或超参数进行的更改做出明智的决策。

本文不应被视为构建深度学习模型的指南，而是本文只是如何构建深度学习模型的一种方式。

快乐学习