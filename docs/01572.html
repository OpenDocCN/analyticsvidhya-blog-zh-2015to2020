<html>
<head>
<title>Understanding Principle Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解主成分分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-principle-component-analysis-3b310ad36163?source=collection_archive---------20-----------------------#2019-10-31">https://medium.com/analytics-vidhya/understanding-principle-component-analysis-3b310ad36163?source=collection_archive---------20-----------------------#2019-10-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/8fbb24ca6aff7ac18d894bcdb2ce5080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZJKbvooJocRo-qlit-ohOA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">法国普罗旺斯凡尔登峡谷</figcaption></figure><p id="313d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">主成分分析(PCA)广泛应用于机器学习和数据科学。PCA在低维空间中找到模型数据的表示，而不会丢失大量信息。这种数据压缩过程可用于数据可视化和分析，以及加速机器学习算法和流水线。</p><p id="2561" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们了解PCA是如何工作的，我们有了模型的数据:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es js"><img src="../Images/0a8cfb3d93cf2f6fe719d659f34396f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*_AFXvJ0ZNYbsb6S6WmfBSQ.png"/></div></figure><p id="d367" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在哪里</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es jx"><img src="../Images/39113f4134750f07e0f93c74d0262023.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*j5V86zRuuYvaEc5HXGnQ5g.png"/></div></figure><p id="6c8e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">该模型有k个数据点，每一个都是一个l维向量，我们希望在m维的低维空间中找到数据的表示<l./></p><p id="64d1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">We start by normalising the data across the l dimensions, for every dimension j we compute the mean across the data points:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es jy"><img src="../Images/32716620c7ef700a26c123943f8204d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*8pVrtPp8608YQ7e5merlYA.png"/></div></figure><p id="5a9b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">And for every dimension j and data point i instead of taking:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es jz"><img src="../Images/bc961c7a08fffb6652b00a7d0a7fb4ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*fzXimtTZgutcnH9zdg_Stg.png"/></div></figure><p id="7bae" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">we take the normalised value:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es ka"><img src="../Images/87c04b5b2e9c0932e99626a0b7b4702e.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*Cktl_vE4hUdqn0c9rV3WEg.png"/></div></figure><p id="d178" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">For simplicity we continue using the original notation:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es js"><img src="../Images/0a8cfb3d93cf2f6fe719d659f34396f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*_AFXvJ0ZNYbsb6S6WmfBSQ.png"/></div></figure><p id="e5c9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">for the normalised data points.</p><p id="d1de" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">We define the data matrix X, the columns are our data points and the rows hold the l dimensions of the data:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kb"><img src="../Images/578fc604f9762cacc1b05542d38636c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*BvtC2BEd116rd-Z2X6wVUw.png"/></div></figure><p id="fc6e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">For every pair of dimensions i,j =1,2,…,l we compute the covariance:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kc"><img src="../Images/16eb8723b846c82bac59d8b364000729.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*t79MC0BaceUd-I3vdz9d8w.png"/></div></figure><p id="2d3e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">and define the data covariance matrix COV(X):</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kd"><img src="../Images/e997c6310a0f203d38d06e47ee66c1ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S4g2dGj9zMRbhY-DJ2YB6Q.png"/></div></div></figure><p id="35c9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">For example entry (1,2) is computed by:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es ke"><img src="../Images/b3775969e8d0a1e21dd907dfa444331a.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*tLnTzozSBsiChBm_wqtoGA.png"/></div></figure><p id="d822" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">using the mean values of the first and second dimensions:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kf"><img src="../Images/273f364f8e496b24f851eb4ddd2c7bd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*h0cjzm37JrkklJ5NKZBOAw.png"/></div></div></figure><p id="6136" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Notice that COV(X) is symmetric since for every i,j=1,2,…,l we have:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kg"><img src="../Images/b99ea8e90afca0d08a2aa60be0bfee35.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*VD3EhPHnipQ0WDmywz-Trw.png"/></div></figure><p id="b8cc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Next we compute the eigenvalues:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es ka"><img src="../Images/e0425a8b857b9204b1fec4b66693240e.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*eRmrtWx1j_oU8IQs2Q8XPA.png"/></div></figure><p id="a651" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">and eigenvectors of the covariance matrix:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kh"><img src="../Images/eeebb1305ba4f9a091ab59fea4d2f84d.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*BfS_FYyxHEJ_-wsTt77vzQ.png"/></div></figure><p id="13ac" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">where</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ki"><img src="../Images/a3567027e9f0e29d75341ad61fe26f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*D1IcaG38FytKeVFt_quEVA.png"/></div></div></figure><p id="0b24" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">，这些特征向量被称为数据的主分量，代表数据中显著方差的方向</strong>，主分量e1在方差最大的方向，主分量e2在第二大的方向，依此类推。</p><p id="50ce" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">例如，假设我们的数据具有以下形式:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kj"><img src="../Images/876d3b736dcb22890d88a43627507dff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*tmzsRXhboHXiHodI8Vcg8g.png"/></div></figure><p id="1765" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">那么主要分量e1、e2将是:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kk"><img src="../Images/51aea71efe2c886f51f5d4345e796a7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*ARvNpSUgnghOfisa0D9x_g.png"/></div></figure><p id="9eed" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">可以看出，e1捕获数据中最大变化量的方向，e2捕获第二大变化量的方向。</p><p id="b87a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">协方差矩阵的特征值也包含关于数据的有价值的信息，它们代表数据在主分量方向上的方差。</p><p id="a627" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在下图中，我们在主分量e1、e2的方向上有一对向量v1、v2。相应地，可以看出，v1、v2的幅度捕捉到了数据的扩展。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kj"><img src="../Images/5b09f2b5c75fcce9fb573210bdb18a49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*i8NPj-TbUtJ3IjxE9oaXxw.png"/></div></figure><p id="feef" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在我们有了协方差矩阵的特征值和特征向量，我们可以选择一个较低的维数m &lt; 1，并定义特征向量矩阵E。E的行是捕获数据中最大方差的m个特征向量，通过取对应于m个最大特征值的特征向量来选择这些特征向量。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kl"><img src="../Images/55a04b7408d93a2f97b281443b417c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*_FYZ9XtO9KbOf5Bz5hmIig.png"/></div></figure><p id="8f0d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将X乘以E，并将我们的数据从l维空间投影到由m个主分量构成的较低的m维空间</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es km"><img src="../Images/2351e9e8c40fcc0a91ee441b78fd8245.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EoncsUght2_aMokbEqA9Fg.png"/></div></div></figure><p id="8553" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了便于记记，我们写下</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kn"><img src="../Images/2c68aadb343a395139c47a0a191fca07.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*Rw_4f1VHcN-jEPVzCFhJZA.png"/></div></figure><p id="7822" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">通过忠实于我们最初的符号，我们可以把EX写成</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es ko"><img src="../Images/7941806bc9a17f40a2c83fe5bef7b28f.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*YMCgTR2zUNIaJFcR1M17FQ.png"/></div></figure><p id="c49f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们的多维数据集是</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/19dabe325ef720438a8360dd0a7745bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*hBy-OdMt6pxYtpMXBf8pOA.png"/></div></figure><p id="58c0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在哪里</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/4c1837203d5cd644a1d2e83b9f664df8.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*F4gHxMZ2PtzrJoLE_TjMQg.png"/></div></figure><p id="aabe" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们完事了。</p><p id="5132" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">请注意，对于每个i=1，2，…，k，矩阵EX(1)中的列I包含数据点xi在m个主分量方向上的投影，我们在矩阵EX(2)中的符号捕捉到了这一点，k个m维数据点是列，m个主分量在行中表示。</p></div></div>    
</body>
</html>