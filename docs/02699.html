<html>
<head>
<title>A Brief History of Sentence Representation in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中句子表征的简史</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-brief-history-of-sentence-representation-in-nlp-a50492481d93?source=collection_archive---------9-----------------------#2019-12-29">https://medium.com/analytics-vidhya/a-brief-history-of-sentence-representation-in-nlp-a50492481d93?source=collection_archive---------9-----------------------#2019-12-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ced0f9a3daaa6999ef9eda393566aec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*cOsLU-tXyR20A5KMXQKhmg.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">插图来自dribbble，版权归格莱布Kuznetsov✈所有，<a class="ae iu" href="https://dribbble.com/shots/4787574-Organic-Artificial-Intelligence-design?utm_source=Clipboard_Shot&amp;utm_campaign=glebich&amp;utm_content=Organic%20Artificial%20Intelligence%20design&amp;utm_medium=Social_Share" rel="noopener ugc nofollow" target="_blank">链接</a>。</figcaption></figure><p id="c3a0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇<a class="ae iu" href="https://hubertwang.me/machinelearning/a-brief-history-of-sentence-representation" rel="noopener ugc nofollow" target="_blank">博文</a>是总结自然语言处理领域句子表示的简史。</p><p id="22a9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">句子嵌入/句子表示</strong>:这些是句子在n维向量空间中的表示，以便语义相似或语义相关的句子根据训练方法变得更接近。</p><h1 id="8f3f" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">1.句子表征的用法</h1><p id="edd7" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">工业中的句子表达之道；</p><ol class=""><li id="22b1" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lb lc ld le bi translated">相同的嵌入可以跨域使用；</li><li id="2cd1" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lb lc ld le bi translated">微调预训练嵌入以用于产品。</li></ol><p id="0400" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">具体来说，您可以决定:</p><ol class=""><li id="7bf8" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lb lc ld le bi translated">直接使用嵌入，训练分类器，不关心嵌入；</li><li id="e6bd" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lb lc ld le bi translated">或者重新训练嵌入模型和分类器。例如伯特</li></ol><p id="0c17" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">BERT无处不在:BERT已经广泛应用于各种工业应用中。BERT作为一个突破可能是单词/句子表示的巅峰，因此Neurips 2019上发表的几乎所有论文都是基于BERT，而不是提出新的方法。</p><h1 id="a832" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">2.句子表征的历史</h1><h1 id="c246" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">2.1传统的句子表示法</h1><p id="e465" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated"><strong class="ix hj"> One-hot Bag of Words (BoW) </strong>:每个单词都链接到一个向量索引，并根据它是否出现在给定的句子中而标记为0或1[1]。例如:</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lk"><img src="../Images/4bde7741c400ac33e17a0eb6d6cf8162.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cWFuu0xvfE4EOzta.png"/></div></div></figure><p id="18ac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du lp lq lr ls b">banana mango</code>编码为<code class="du lp lq lr ls b">000001000100</code>；</p><p id="ab33" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du lp lq lr ls b">mango banana</code>编码为<code class="du lp lq lr ls b">000001000100</code>。</p><ul class=""><li id="fe70" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lt lc ld le bi translated">亲:简单快捷；</li><li id="fa72" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lt lc ld le bi translated">弊:没有语序；没有语义信息；重要词和非重要词没有区别。</li></ul><p id="1ec8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">词频-逆文档频率(TF-IDF) </strong>:是一种表示一个词在语料库中有多重要的方法。它根据给定单词出现的上下文为其提供权重。[2]中关于TF-IDF的更多内容。下面的图片来自[2]。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/33952a10b8805e92c7e33658af94acd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/0*8zfEU_VwOsouLF1g.png"/></div></div></figure><ul class=""><li id="eaa7" class="kw kx hi ix b iy iz jc jd jg ky jk kz jo la js lt lc ld le bi translated">亲:简单快捷；考虑单词的重要性；</li><li id="7c32" class="kw kx hi ix b iy lf jc lg jg lh jk li jo lj js lt lc ld le bi translated">弊:没有语序信息；没有语义信息。</li></ul><h1 id="25fd" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">2.2神经嵌入</h1><p id="6911" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">语言模型。语言模型描述了文本在语言中存在的概率。例如，句子“我喜欢吃香蕉”比“我喜欢吃回旋”更有可能。我们通过对n个单词的窗口进行切片并预测文本中的下一个单词是什么来训练语言模型。</p><p id="13a9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">word 2 vec/doc 2 vec</strong>:word 2 vec[3]—包含2个模型，其中第一个模型(连续单词包，CBoW)使用前面的单词预测下一个单词，另一个模型(Skip-Gram)使用1个单词预测所有周围的单词。Doc2Vec [4] —创建文档的数字表示，而不管其长度。基于Word2Vec模型，他们在CBoW的输入中添加了另一个名为段落ID的向量。新的向量作为记忆，记住当前上下文或段落主题中缺少的内容[5]。推荐阅读[5]了解更多信息。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/3476d9f9a47ab46ab011ba0109e320b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/0*wZLYyttDrdtrKJva.png"/></div></figure><p id="4a07" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">除了Word2Vec，还有多种其他无监督的学习句子表示的方法。下面列出。</p><p id="61fa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> Autoencoder </strong>:是一个无监督的深度学习模型，试图将其输入复制到其输出。自动编码器的诀窍在于中间隐藏层的维度低于输入数据的维度。因此，神经网络必须以智能和紧凑的方式表示输入，以便成功地重建它[6]。</p><p id="2367" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> RNN / LSTM </strong>:角色级别的LSTM(绝对是我最喜欢的LSTM文章，作者安德烈·卡帕西)【7】和双向RNNs【8】。</p><p id="a7c4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">跳过思维:同样的直觉，语言模型从前一个预测下一个。然而，它不是预测下一个单词或下一个字符，而是预测上一个和下一个句子。这给了模型更多的句子上下文，因此我们可以建立更好的句子表示。[6]在[9]中有更多关于Skip-thinks的内容。</p><p id="ddcf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">注意力和变压器</strong>:注意力取两个句子，把它们变成一个矩阵，其中一个句子的单词形成列，另一个句子的单词形成行，然后它进行匹配，识别相关的上下文。这在机器翻译中非常有用。自我注意:把同一个句子列和行，我们可以学习句子的一部分如何与另一部分相关。一个很好的用例是帮助理解“它”指的是什么，即把代词和先行词联系起来[10]。</p><p id="3a70" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然注意力最初是与其他算法(如RNNs或CNN)一起使用的，但已经发现它本身表现得非常好。结合前馈层，注意力单元可以简单地堆叠起来，形成编码器。此外，与LSTM相比，注意力的“聚光灯”有助于聚焦于实际有用的数据。</p><p id="e3ca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意力是你所需要的(Vaswani等人，2017) [11]绝对是提议Transformer允许我们在没有递归单元的情况下进行Seq2Seq的影响者。它基于<strong class="ix hj"> K </strong> ey、<strong class="ix hj">V</strong>value、<strong class="ix hj"> Q </strong> uery和编码器/解码器堆栈。这在[12]中有详细解释。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/19bf4d31f3cdc5778cbaecec9d4cc61f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YRuwV_eIXRfSW8vv.png"/></div></div></figure><p id="4074" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> BERT </strong>:从名字中可以看出它的想法——<strong class="ix hj">B</strong>I directional<strong class="ix hj">E</strong>n coder<strong class="ix hj">R</strong>代表来自<strong class="ix hj">T</strong>transformers，这表明关键的技术创新是应用变压器的双向训练。预训练的BERT模型可以通过一个额外的输出层进行微调，从而为各种任务创建最先进的模型[13]。更多关于BERT的解释可以在这里找到[14]。也用于可视化伯特[15]。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/1144a819cbd6051e171709d944f77bbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5D99iMezqhcWpzcx.png"/></div></div></figure><p id="22d8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[1] <a class="ae iu" href="https://towardsdatascience.com/beyond-word-embeddings-part-2-word-vectors-nlp-modeling-from-bow-to-bert-4ebd4711d0ec" rel="noopener" target="_blank">亚伦(Ari)博恩施泰因，超越文字嵌入第2部分，中等</a></p><p id="d1fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[2] <a class="ae iu" href="https://skymind.ai/wiki/bagofwords-tf-idf" rel="noopener ugc nofollow" target="_blank">克里斯·尼科尔森，词汇入门指南&amp; TF-IDF，Skymind </a></p><p id="dfa2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[3] <a class="ae iu" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank"> T .米科洛夫等人向量空间中单词表示的有效估计</a></p><p id="f682" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[4] <a class="ae iu" href="https://arxiv.org/abs/1405.4053" rel="noopener ugc nofollow" target="_blank"> Quoc诉Le和T. Mikolov，句子和文件的分布式表示</a></p><p id="ebcc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[5] <a class="ae iu" rel="noopener" href="/wisio/a-gentle-introduction-to-doc2vec-db3e8c0cce5e"> Gidi Shperber，对Doc2Vec的温和介绍，WISIO </a></p><p id="e859" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[6] <a class="ae iu" href="https://blog.myyellowroad.com/unsupervised-sentence-representation-with-deep-learning-104b90079a93" rel="noopener ugc nofollow" target="_blank"> Yonatan Hadar，深度学习的无监督句子表示，YellowRoad </a></p><p id="e05a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[7] <a class="ae iu" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank"> Andej Karpathy，递归神经网络的不合理有效性</a></p><p id="88ff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[8] <a class="ae iu" href="https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks" rel="noopener ugc nofollow" target="_blank">双向递归神经网络，维基百科</a></p><p id="f41a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[9] <a class="ae iu" rel="noopener" href="/@sanyamagarwal/my-thoughts-on-skip-thoughts-a3e773605efa"> Sanyam Agarwal，我对Skip-Thoughts的想法</a></p><p id="92f2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[10] <a class="ae iu" href="https://skymind.ai/wiki/attention-mechanism-memory-network" rel="noopener ugc nofollow" target="_blank">克里斯·尼科尔森，注意力机制和记忆网络初学者指南，Skymind </a></p><p id="cc99" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[11] <a class="ae iu" href="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">瓦斯瓦尼等著《注意力就是你所需要的一切》，NIPS 2017 </a></p><p id="059c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[12] <a class="ae iu" rel="noopener" href="/@bgg/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d">大春(Bgg)苏，Seq2seq注意自我注意:第二部</a></p><p id="cec4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[13]j .德夫林等人、BERT:语言理解深度双向转换器的预训练</p><p id="5858" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[14] <a class="ae iu" href="https://www.lyrn.ai/2018/11/07/explained-bert-state-of-the-art-language-model-for-nlp/" rel="noopener ugc nofollow" target="_blank">拉尼·霍雷夫，伯特——自然语言处理语言模型的现状，里纳伊</a></p><p id="4dfb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">【15】<a class="ae iu" href="https://github.com/jessevig/bertviz" rel="noopener ugc nofollow" target="_blank">杰西·维格(Jesse Vig)，在变形金刚模型中可视化注意力的工具(伯特、GPT-2、艾伯特、XLNet、罗伯塔、CTRL等。)</a></p><p id="e740" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">喜欢这篇文章？在我的博客上阅读更多:<a class="ae iu" href="https://hubertwang.me/" rel="noopener ugc nofollow" target="_blank">https://hubertwang.me/</a></p></div></div>    
</body>
</html>