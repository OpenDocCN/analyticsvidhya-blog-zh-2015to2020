<html>
<head>
<title>Linear Regression: Hypothesis Function, Cost Function, and Gradient Descent. (Part:2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归:假设函数、成本函数和梯度下降。(第二部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-hypothesis-function-cost-function-and-gradient-descent-part-2-730b13959b3c?source=collection_archive---------2-----------------------#2020-02-16">https://medium.com/analytics-vidhya/linear-regression-hypothesis-function-cost-function-and-gradient-descent-part-2-730b13959b3c?source=collection_archive---------2-----------------------#2020-02-16</a></blockquote><div><div class="ds hc hd he hf hg"/><div class="hh hi hj hk hl"><h2 id="50f9" class="hm hn ho bd b fp hp hq hr hs ht hu dx hv translated" aria-label="kicker paragraph">内部人工智能</h2><div class=""/><div class=""><h2 id="f792" class="pw-subtitle-paragraph iu hx ho bd b iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl dx translated">最著名的监督学习技术背后的数学和理论</h2></div><p id="9745" class="pw-post-body-paragraph jm jn ho jo b jp jq iy jr js jt jb ju jv jw jx jy jz ka kb kc kd ke kf kg kh hh bi translated">【注:在阅读本文之前，我鼓励你先阅读本文的第一部分(<a class="ae ki" rel="noopener" href="/@m.mahyarali/linear-regression-hypothesis-function-cost-function-and-gradient-descent-part-1-6cd865552923"> <strong class="jo hy"> <em class="kj">链接</em> </strong> </a>)，以便更好地理解假设函数、成本函数。]</p></div></div>    
</body>
</html>