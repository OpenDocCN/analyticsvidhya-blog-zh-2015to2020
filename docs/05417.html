<html>
<head>
<title>NLP using RNN — Can you be the next Shakespeare?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用RNN的NLP你能成为下一个莎士比亚吗？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/nlp-using-rnn-can-you-be-the-next-shakespeare-27abf9af523?source=collection_archive---------10-----------------------#2020-04-20">https://medium.com/analytics-vidhya/nlp-using-rnn-can-you-be-the-next-shakespeare-27abf9af523?source=collection_archive---------10-----------------------#2020-04-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3b95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">合著者:<a class="jd je ge" href="https://medium.com/u/9e828c14da26?source=post_page-----27abf9af523--------------------------------" rel="noopener" target="_blank">文卡特什·钱德拉</a></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jf"><img src="../Images/3116e06f7d1f8da98fa6f4272f704748.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*8jwB6vtOlWKXuMZQyITSYQ.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">iOS设备上的预测键盘</figcaption></figure><p id="367f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你有没有想过像预测键盘这样的工具如何在你的智能手机上工作？在本文中，我们探讨了使用先验信息生成文本的思想。具体来说，我们将在Google Colab上使用递归神经网络(RNNs)和自然语言处理(NLP)从16世纪的文学作品中生成段落。这个想法很简单，我们将尝试给一个模型一个样本莎士比亚戏剧生成假的部分，同时保持相同的方言。虽然预测键盘可以为可能包含多个单词的不完整句子生成最佳的“一个单词”，但我们将通过使用单个单词来生成莎士比亚戏剧的一部分，从而使这一过程变得更加困难。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jr"><img src="../Images/f7baa9d545d02baef1cff73fe788f2d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*2WeaNWA6I89Qy-8oYBUyvA.jpeg"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">威廉·莎士比亚的肖像(或者是？)</figcaption></figure><h1 id="c0a0" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">了解NLP和RNNs </strong></h1><p id="e452" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">我们先来刷新一下NLP的RNNs概念。rnn广泛用于预测。对RNNs的数据集约束是它应该是时间序列的形式。NLP是人工智能中的一个领域，它赋予机器阅读、理解和发现文本数据模式的能力。</p><p id="4ba6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以这样想——我们可以将文本中的字母转换成数字，并将其输入RNN模型，以生成下一个可能的结果(听起来像预测，对吗？)</p><p id="d5b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">RNN变奏曲</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es kv"><img src="../Images/969ad009436d325d5ac96378c0f6efe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*djRdWpOisye9dIL_ZPKzBw.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">表示不同rnn内部机制的图表</figcaption></figure><p id="6bf6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">rnn拥有一种循环机制，充当允许信息从一个步骤流向下一个步骤的路径。这个信息是隐藏状态，它是先前输入的表示。</p><p id="5f73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">rnn有许多不同的变体，最常见的是LSTMs(长短期记忆)。在本文中，我们将利用一个不太为人所知的变体，称为门控循环单位(GRUs)。简单RNNs和GRUs之间的关键区别在于，后者支持隐藏状态的门控。如前所述，隐藏状态允许我们输入先前时间步长的信息。因此，rnn和gru的不同之处在于传递信息的方式。区别在于何时应该更新隐藏状态以及何时应该重置隐藏状态的专用机制。</p><p id="a483" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果前面的段落超出了你的理解，不要担心！LSTMs和GRUs一开始有点难掌握。总而言之，gru与LSTMs非常相似。唯一的区别是gru没有单元状态，它使用隐藏状态来传递信息。事实上，GRU有两个门:一个<strong class="ih hj">更新门</strong>和一个<strong class="ih hj">复位门</strong>。<strong class="ih hj">更新门</strong>的作用类似于LSTM的遗忘和输入门。它决定丢弃什么信息和添加什么新信息。<strong class="ih hj">复位门</strong>是另一个用于决定忘记多少过去信息的门。详细解释可以看这个<a class="ae kw" href="https://www.youtube.com/watch?v=8HyCNIVRbSU" rel="noopener ugc nofollow" target="_blank">视频</a>了解过程。</p><p id="0a4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在哪一个对我们有利？一个简单的RNN，LSTM，GRU？像生活中的所有事情一样，没有什么是明确的。一切都取决于用例、数据量和性能。所以，决定权在每个人身上！</p><h1 id="5eae" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">使用GRUs生成莎士比亚戏剧</strong></h1><p id="6048" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">我们现在将使用戏剧《罗密欧与朱丽叶》中的文本来生成一些模仿16世纪文学的“假段落”。为此，我们从<a class="ae kw" href="https://www.gutenberg.org/" rel="noopener ugc nofollow" target="_blank">https://www.gutenberg.org/</a>提取了一定数量的数据。</p><p id="9a85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据集链接—<a class="ae kw" href="https://www.gutenberg.org/ebooks/1112" rel="noopener ugc nofollow" target="_blank">https://www.gutenberg.org/ebooks/1112</a></p><p id="5c8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以从。包含内容和确认部分的txt文件。这将有助于产生更好的模型。</p><p id="876e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将开发一个模型，使用先前的字符序列来预测下一个最高概率的字符。我们必须谨慎使用多少字符。一方面，使用非常长的序列将需要大量的训练时间，并且很可能过度适应与更远的字符不相关的字符序列。另一方面，序列太短会使我们的模型不适合。因此，我们从现有的数据长度中建立直觉。根据正常短语的长度，我们将使用一个单词来预测接下来的180个字符。</p><p id="5b64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是时候行动了！遵循以下步骤:</p><p id="ff76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意:下面提供了Google colab链接</strong></p><p id="d3c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第一步:</strong>数据导入&amp;基本功能</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="8620" class="lc jt hi ky b fi ld le l lf lg">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import tensorflow as tf</span></pre><p id="945e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">导入数据集(Google colab示例)</strong></p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="b6e0" class="lc jt hi ky b fi ld le l lf lg">input_text = uploaded[‘romeo_juliet.txt’].decode(“utf-8”)</span></pre><p id="44b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第二步:</strong>数据预处理</p><p id="20b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">创建一组独特的字符:</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="d84f" class="lc jt hi ky b fi ld le l lf lg">letter_corpus = sorted(set(input_text))<br/>letter_corpus[:10]</span></pre><p id="a19e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将字母编码成数字</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="a668" class="lc jt hi ky b fi ld le l lf lg">char_to_ind = {u:i for i, u in enumerate(letter_corpus)}<br/>ind_to_char = np.array(letter_corpus)<br/>encoded_text = np.array([char_to_ind[c] for c in input_text])</span></pre><p id="3ebd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第三步:</strong>检查顺序</p><p id="42a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个句子的长度是43，如果我们捕获三个句子，模型应该能够拾取模式并学习它们。</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="afc6" class="lc jt hi ky b fi ld le l lf lg">part_stanza = “””Chor. Two households, both alike in dignity,<br/>In fair Verona, where we lay our scene,<br/>From ancient grudge break to new mutiny,<br/>Where civil blood makes civil hands unclean”””</span><span id="3cea" class="lc jt hi ky b fi lh le l lf lg">len(part_stanza)</span><span id="9fe9" class="lc jt hi ky b fi lh le l lf lg"><strong class="ky hj">#Output - 181</strong></span></pre><p id="b3ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第四步:</strong>训练序列</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="ccf8" class="lc jt hi ky b fi ld le l lf lg">seq_len = 180<br/>total_num_seq = len(input_text)//(seq_len+1)</span><span id="8142" class="lc jt hi ky b fi lh le l lf lg"><strong class="ky hj">#We obtain 972 sequences</strong></span><span id="d037" class="lc jt hi ky b fi lh le l lf lg">char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)<br/>sequences = char_dataset.batch(seq_len+1, drop_remainder=True)</span><span id="04e9" class="lc jt hi ky b fi lh le l lf lg"><strong class="ky hj">#drop_remainder=True</strong> ensures that the last batch gets dropped if it #has less number of words</span></pre><p id="88f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将序列映射到数据集</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="591b" class="lc jt hi ky b fi ld le l lf lg">dataset = sequences.map(create_seq_targets)</span></pre><p id="edc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">第五步:</strong>创建批次</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="ecbe" class="lc jt hi ky b fi ld le l lf lg">batch_size = 1</span><span id="a177" class="lc jt hi ky b fi lh le l lf lg">buffer_size = 10000</span><span id="d469" class="lc jt hi ky b fi lh le l lf lg">dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)</span></pre><p id="8c1d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">创建网络的时间到了</strong></p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="bc49" class="lc jt hi ky b fi ld le l lf lg">vocab_size = len(letter_corpus)<br/>embed_dim = 64<br/>rnn_neurons = 1026</span></pre><p id="cc99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">导入张量流模型、层和损失函数</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="be4e" class="lc jt hi ky b fi ld le l lf lg">from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import LSTM,Dense,Embedding,Dropout,GRU<br/>from tensorflow.keras.losses import sparse_categorical_crossentropy</span></pre><p id="24f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们定义损失函数</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="9464" class="lc jt hi ky b fi ld le l lf lg">def sparse_cat_loss(y_true,y_pred):<br/> return sparse_categorical_crossentropy(y_true, y_pred,   <br/> from_logits=True)</span></pre><p id="da56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">创建模型</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="c128" class="lc jt hi ky b fi ld le l lf lg">def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):</span><span id="0124" class="lc jt hi ky b fi lh le l lf lg"> model = Sequential()</span><span id="4f25" class="lc jt hi ky b fi lh le l lf lg"> model.add(Embedding(vocab_size, embed_dim,batch_input_shape=<br/> [batch_size, None]))<br/>      model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer=’glorot_uniform’))</span><span id="5767" class="lc jt hi ky b fi lh le l lf lg"># Final Dense Layer to Predict</span><span id="7ccf" class="lc jt hi ky b fi lh le l lf lg">model.add(Dense(vocab_size))</span><span id="19e9" class="lc jt hi ky b fi lh le l lf lg">model.compile(optimizer=’adam’, loss=sparse_cat_loss, metrics = ['accuracy'])</span><span id="de67" class="lc jt hi ky b fi lh le l lf lg">return model</span><span id="91f6" class="lc jt hi ky b fi lh le l lf lg">model = create_model( vocab_size = vocab_size, embed_dim=embed_dim, rnn_neurons=rnn_neurons, batch_size=batch_size)</span></pre><p id="a840" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的模型现在看起来像这样:</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="08f5" class="lc jt hi ky b fi ld le l lf lg">model.summary()</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es li"><img src="../Images/e037434c6ae6ba2c14a26379374f094e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ldoWPgxffqzEZQxlufsEA.png"/></div></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">模型架构</figcaption></figure><h1 id="8006" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">该训练了</h1><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ln"><img src="../Images/ef9a005049f5b4f92ad42636c0310a02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L8JnhkQ4wsHj3933GQqQ7A.png"/></div></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">穆罕默德·阿里在行动，就像我们的模型！</figcaption></figure><p id="745f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将纪元设置为30</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="3cd7" class="lc jt hi ky b fi ld le l lf lg">epochs = 30</span></pre><p id="d687" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练模型。请注意，这将需要一些时间。我们花了大约40分钟来训练数据集</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="33ef" class="lc jt hi ky b fi ld le l lf lg">model.fit(dataset,epochs=epochs)</span></pre><h1 id="8901" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">模型评估</strong></h1><p id="7c4e" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">我们在下面的代码中保存了模型历史并绘制了报告指标</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="9a30" class="lc jt hi ky b fi ld le l lf lg">losses = pd.DataFrame(model.history.history)<br/><br/>losses[[‘loss’,’accuracy’]].plot()</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lo"><img src="../Images/aa633a413dcb8f0591cac5a3cdf1767e.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*_hFOpxx3NRi3aC-lB-IZwA.jpeg"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">GRU模特培训结果</figcaption></figure><p id="61cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意损失是如何减少到第20个纪元，然后猛增的。在第18个历元上获得的最高精度是86.03%。因此，我们已经为我们的模型训练了18个纪元。</p><h1 id="fd95" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">生成文本</strong></h1><p id="690a" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">我们定义了一个函数(没有固定种子)来使用序列1生成文本。如果我们用两个单词训练模型，那么我们的模型会更强大，但是训练时间会增加。</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="e503" class="lc jt hi ky b fi ld le l lf lg">def generate_text(model, start_seed,gen_size=100,temp=1.0):</span><span id="e391" class="lc jt hi ky b fi lh le l lf lg">num_generate = gen_size</span><span id="688f" class="lc jt hi ky b fi lh le l lf lg">input_eval = [char_to_ind[s] for s in start_seed]</span><span id="f150" class="lc jt hi ky b fi lh le l lf lg">input_eval = tf.expand_dims(input_eval, 0)</span><span id="63c3" class="lc jt hi ky b fi lh le l lf lg">text_generated = []</span><span id="cef6" class="lc jt hi ky b fi lh le l lf lg">temperature = temp</span><span id="f562" class="lc jt hi ky b fi lh le l lf lg">model.reset_states()</span><span id="cd49" class="lc jt hi ky b fi lh le l lf lg">for i in range(num_generate):</span><span id="19bb" class="lc jt hi ky b fi lh le l lf lg"><strong class="ky hj"> # Generate Predictions</strong></span><span id="21ba" class="lc jt hi ky b fi lh le l lf lg"> predictions = model(input_eval)</span><span id="22a7" class="lc jt hi ky b fi lh le l lf lg"><strong class="ky hj"> # Remove the batch shape dimension</strong></span><span id="77cf" class="lc jt hi ky b fi lh le l lf lg"> predictions = tf.squeeze(predictions, 0)</span><span id="867a" class="lc jt hi ky b fi lh le l lf lg"><strong class="ky hj"> # Use a cateogircal disitribution to select the next character</strong></span><span id="3580" class="lc jt hi ky b fi lh le l lf lg"> predictions = predictions / temperature</span><span id="fa69" class="lc jt hi ky b fi lh le l lf lg"> predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()</span><span id="60bb" class="lc jt hi ky b fi lh le l lf lg"><strong class="ky hj"> # Pass the predicted charracter for the next input</strong></span><span id="6487" class="lc jt hi ky b fi lh le l lf lg"> input_eval = tf.expand_dims([predicted_id], 0)</span><span id="183b" class="lc jt hi ky b fi lh le l lf lg"><strong class="ky hj"> # Transform back to character letter</strong></span><span id="2350" class="lc jt hi ky b fi lh le l lf lg"> text_generated.append(ind_to_char[predicted_id])</span><span id="542b" class="lc jt hi ky b fi lh le l lf lg">return (start_seed + ‘’.join(text_generated))</span></pre><p id="74fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于生成，您可以使用下面的代码，其中您只需要指定起始单词和连续单词的数量。</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="be9d" class="lc jt hi ky b fi ld le l lf lg">print(generate_text(model,”But”,gen_size=1000))</span></pre><h1 id="72ef" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">输出</strong></h1><p id="aeba" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">我们获得了单词“flower”的如下输出</p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="74df" class="lc jt hi ky b fi ld le l lf lg">flowers, with power,<br/>thitiest ince to speak.</span><span id="cff4" class="lc jt hi ky b fi lh le l lf lg">Enter Lady, packed turnare didomaid,<br/> O hands the fair creastrim! bystalt me this sare!<br/> Out you? O, pies, peach, ar, and outsides.</span><span id="e664" class="lc jt hi ky b fi lh le l lf lg">Enter Julie.</span><span id="30ae" class="lc jt hi ky b fi lh le l lf lg">Cep.’ Hath you, Caup with such scater, ose must reports! colal, with so smally,<br/> ‘Year ‘ads-noods withal.</span><span id="1646" class="lc jt hi ky b fi lh le l lf lg">Cap. Ay thou hast thou been shopy sender hase.</span><span id="3129" class="lc jt hi ky b fi lh le l lf lg">Ey’ WAtch make close curtain, with the humour times.<br/> O, good night, s oppriwite up, in displayd-by so night raught<br/> Shall back that hous shalt confurers to away in this?</span><span id="4d38" class="lc jt hi ky b fi lh le l lf lg">Jul. He case us borny, my chall, is fould wish permission.<br/> Give me thy shrew, so bir, sighs all,<br/> Apphrel thee but but my lips?</span><span id="aa32" class="lc jt hi ky b fi lh le l lf lg">Jul. Ay, noinot makes, I cave me not doth she country.</span><span id="3add" class="lc jt hi ky b fi lh le l lf lg">Man. The sorisim! O O, Capulet,<br/> Mush fairence the murte-baggage of Montaghous.<br/> Where viewild you but ny yo,<br/> Her simps to the-</span><span id="1937" class="lc jt hi ky b fi lh le l lf lg">Ben.</span></pre><p id="c7e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意我们的模型是如何初始化Juliet和Ben的名字的。此外，当句子以标点符号结尾时，它会选择模式，并模仿16世纪的散文，如<em class="lp"> Ey，你，你</em>等。</p><p id="1bd8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输出为单词“But”</strong></p><pre class="jg jh ji jj fd kx ky kz la aw lb bi"><span id="1bb7" class="lc jt hi ky b fi ld le l lf lg">But,<br/> Who say wethith is the day purg’d which your bight!<br/> Are providunity nurse, that mark much soul-</span><span id="1973" class="lc jt hi ky b fi lh le l lf lg">D.ASCup on phforight or verfain, is doth compirications comes and curnais,<br/> How?<br/> Allotions back,<br/> Where I sear and kindroud.<br/> A plaguage of gracksten, creptain!<br/> Show her plamangled us, now, sir?</span><span id="431c" class="lc jt hi ky b fi lh le l lf lg">Wife. Spaker, you, sir.</span><span id="960a" class="lc jt hi ky b fi lh le l lf lg">Cap. What [and] Buy Halth will’dinging, non, and pular our soul<br/> And lovely dreamerly eress murkdeds<br/> Whose she beshes eyes will be in thy brace!</span><span id="6d8e" class="lc jt hi ky b fi lh le l lf lg">Enter Appraide his banished.</span><span id="944e" class="lc jt hi ky b fi lh le l lf lg">Ben. I can you like whose’s keaus.</span><span id="72d1" class="lc jt hi ky b fi lh le l lf lg">Speak. ’Tis answer ‘I’ shall up, corpudin!<br/> She [and by] Well, sight as as a know this may be the hight comes Refuchis works corns.</span><span id="6d99" class="lc jt hi ky b fi lh le l lf lg">Par. So am I conduct and Montague, Commend.</span><span id="5f0b" class="lc jt hi ky b fi lh le l lf lg">Extut may cell till me comes she wret?<br/> Marry, the maid shrifid- grovimeo,<br/> Whoce arm Louren lover’d right.<br/> Th</span></pre><p id="9fb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个模型太棒了，它设置了语音让妻子用先生称呼男人！</p><h1 id="2f5b" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">结论</strong></h1><p id="789c" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">我们看到我们的模型模仿了剧本《罗密欧与朱丽叶》的写作方式。注意涉及字符的句子的开头。此外，整个方言被复制。</p><p id="3579" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了用《罗密欧与朱丽叶》训练模型之外，我们还打算用类似的方法训练其他文本，如《傲慢与偏见》和埃德蒙兹的汽车评论。虽然前者的模型训练显示了希望，但后者没有达到预期。具体来说，对于评论，模型的表现并不理想，因为它找不到模式。这很可能与评论的撰写方式有关。大多数人有不同的写作风格，这使得模型很难模仿散文。</p><p id="9a1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在未来，当我们查看推文时，探索这样一种方法，以及我们如何用假推文实现这样一个模型，将会很有趣。但是为什么只有推特？理想情况下，我们还可以看看虚假的在线文章，甚至虚假的WhatsApp新闻(尤其是在选举期间)。</p><p id="7b07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以在下面找到代码的链接:</p><div class="lq lr ez fb ls lt"><a href="https://github.com/chandravenky/Generating-Text-using-RNN" rel="noopener  ugc nofollow" target="_blank"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hj fi z dy ly ea eb lz ed ef hh bi translated">钱德拉文基/生成文本-使用-RNN</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">使用GRU为罗密欧与朱丽叶生成文本。由Shaan和Venkatesh创作的RNN支持的NLP模型能写一个…</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">github.com</p></div></div><div class="mc l"><div class="md l me mf mg mc mh jl lt"/></div></div></a></div><p id="619d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请在LinkedIn上与我们联系，了解更多故事。</p><div class="lq lr ez fb ls lt"><a href="https://www.linkedin.com/in/shaan-kohli/?originalSubdomain=ca" rel="noopener  ugc nofollow" target="_blank"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hj fi z dy ly ea eb lz ed ef hh bi translated">加拿大蒙特利尔德绍特尔管理学院</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">查看Shaan Kohli在世界上最大的职业社区LinkedIn上的个人资料。Shaan的教育列在…</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">www.linkedin.com</p></div></div><div class="mc l"><div class="mi l me mf mg mc mh jl lt"/></div></div></a></div><div class="lq lr ez fb ls lt"><a href="https://www.linkedin.com/in/venkateshchandra/" rel="noopener  ugc nofollow" target="_blank"><div class="lu ab dw"><div class="lv ab lw cl cj lx"><h2 class="bd hj fi z dy ly ea eb lz ed ef hh bi translated">Venkatesh Chandra - Aon Canada |解决方案架构师-分析顶点项目- McGill…</h2><div class="ma l"><h3 class="bd b fi z dy ly ea eb lz ed ef dx translated">我有4年的数据科学家经验。我曾与财富100强公司合作进行外部分析…</h3></div><div class="mb l"><p class="bd b fp z dy ly ea eb lz ed ef dx translated">www.linkedin.com</p></div></div><div class="mc l"><div class="mj l me mf mg mc mh jl lt"/></div></div></a></div></div></div>    
</body>
</html>