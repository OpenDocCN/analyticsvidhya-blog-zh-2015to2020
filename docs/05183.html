<html>
<head>
<title>Reinforcement Learning — Machines learning by interacting with the world</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习——机器通过与世界互动来学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-machines-learning-by-interacting-with-the-world-64e5862dbf19?source=collection_archive---------22-----------------------#2020-04-13">https://medium.com/analytics-vidhya/reinforcement-learning-machines-learning-by-interacting-with-the-world-64e5862dbf19?source=collection_archive---------22-----------------------#2020-04-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7663" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在过去的几年里，人工智能领域一直在蓬勃发展。它取得了令人印象深刻的进步，并使计算机能够在各个领域不断挑战人类的表现。我们很多人都熟悉AlphaGo，它是第一个在<strong class="ih hj">围棋</strong>比赛中毫无障碍地击败职业选手的计算机程序。它的继任者<a class="ae jd" href="https://en.wikipedia.org/wiki/AlphaZero" rel="noopener ugc nofollow" target="_blank"> AlphaZero </a>目前被认为是世界上最好的围棋选手，也可能是国际象棋选手。</p><p id="3ca4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是强化学习(RL)不仅仅擅长游戏。它在<a class="ae jd" href="https://arxiv.org/ftp/arxiv/papers/1907/1907.04373.pdf" rel="noopener ugc nofollow" target="_blank">金融</a><a class="ae jd" href="https://arxiv.org/pdf/1906.05799.pdf" rel="noopener ugc nofollow" target="_blank">网络安全</a><a class="ae jd" href="https://arxiv.org/pdf/1903.04411.pdf" rel="noopener ugc nofollow" target="_blank">甚至教机器画画</a>都有应用。这篇文章是解释强化学习重要概念的系列文章的第一篇。</p><p id="b7bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我写这一系列文章是为了在阅读这本书的过程中巩固我对RL概念的理解:<a class="ae jd" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">强化学习:简介</strong> </a>作者:安德鲁·巴尔托和理查德·萨顿。我将写下我对各种概念的理解以及相关的编程任务。</p><h1 id="b06c" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">什么是强化学习？</h1><p id="1b3a" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">假设，我们有一个代理人在一个代理人完全不知道其动态的环境中。代理可以通过采取一定数量的动作与环境进行交互，而环境反过来为该动作返回<strong class="ih hj">奖励</strong>。代理人应该最大化在与环境相互作用的<strong class="ih hj">事件</strong>中累积的总报酬。例如，玩游戏的机器人，或者在顾客离开后因清理桌子而获得奖励的餐馆机器人。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/ecf5f2a5a858f18fe54074957e6a2713.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*NL2XY7OfY3qftXWCLL5S0g.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">一个代理人采取行动与环境互动，并得到回报。<a class="ae jd" href="https://lilianweng.github.io/lil-log/assets/images/RL_illustration.png" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="3dd8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目标是确保代理从试验和收到的反馈中学习策略，以最大化回报。</p><h1 id="a01c" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">关键概念</h1><p id="dc07" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在继续之前，让我们定义一些关键概念。<strong class="ih hj">代理</strong>在<strong class="ih hj">环境</strong>中行动。环境对代理交互的反应由环境的<strong class="ih hj">模型</strong>定义。在任何给定的时间点，代理都处于<strong class="ih hj">状态(s∈S </strong>)并且可以从一组<strong class="ih hj">动作(a∈A)中采取任何<strong class="ih hj">动作</strong>。</strong>在采取行动时，代理从状态<strong class="ih hj"> s </strong>转换到<strong class="ih hj">s’</strong>。从<strong class="ih hj"> s </strong>转换到<strong class="ih hj">s’</strong>的概率由<strong class="ih hj">转换函数给出。</strong>环境从一组<strong class="ih hj">奖励中奖励代理人(r∈R)。</strong>代理在一个状态下采取行动的策略称为<strong class="ih hj">策略π(s)。在设计RL代理时，代理可能熟悉也可能不熟悉环境的模型。于是，就出现了两种不同的情况:<br/> <strong class="ih hj"> 1。基于模型的RL: </strong>智能体熟悉环境的完整模型，或者在与环境的交互过程中了解它。这里，如果已知完整的模型，可以使用<a class="ae jd" href="https://en.wikipedia.org/wiki/Dynamic_programming" rel="noopener ugc nofollow" target="_blank">动态编程</a>找到最优解。<br/> <strong class="ih hj"> 2。</strong> <strong class="ih hj">模型自由RL: </strong>智能体在没有任何模型知识的情况下学习一种与环境交互的策略，并不试图学习模型的环境。</strong></p><p id="ab75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理人的目标是采取行动，使总报酬最大化。每个状态都与一个<strong class="ih hj">值</strong> <strong class="ih hj">函数V(s) </strong>相关联，该函数预测通过采取相应的策略，我们在该状态下能够获得的预期未来奖励金额。换句话说，价值函数量化了一个状态有多好。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es kt"><img src="../Images/8538327a5f8d4918c6d5a21aa5b4314d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BsN4a2N1EDmgG19wWDd9CQ.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">RL方法以及每种方法的算法名称。<a class="ae jd" href="https://spinningup.openai.com/en/latest/_images/rl_algorithms_9_15.svg" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="a870" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理人和环境之间的一系列相互作用被称为<strong class="ih hj">事件</strong>(也称为“轨迹”或“试验”)。一集由任意时刻的状态、动作和奖励组成，<strong class="ih hj"> t = 1，2，…，T. </strong>在时刻<strong class="ih hj"> t，</strong>观察到的状态、采取的动作和奖励分别用<strong class="ih hj"> Sₜ、Aₜ和Rₜ </strong>表示。因此，一集由以下部分组成:<strong class="ih hj"> Sₜ = S₁、A₁、R₁、S₂、A₂、…、Sₜ.<br/>其他一些常用的关键术语:<br/> 1。On-policy </strong>:使用来自<strong class="ih hj">目标策略</strong>的确定性结果或样本来训练算法。目标策略是当代理将被付诸行动而不是被训练时将要使用的策略。<br/> <strong class="ih hj"> 2 </strong>。<strong class="ih hj">非策略</strong>:由不同行为策略而非目标策略产生的过渡或情节分布的培训。</p><h1 id="af5a" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">什么是环境模型？</h1><p id="9019" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">假设我们正在训练一个机器人步行到一个很远的地方。将其公式化为一个非常基本的任务，假设我们希望代理控制机器人的各个部分，以方便以直线姿势行走。机器人偏离垂直轴的角度不应超过20°。机器人在角度标准内停留的每一个时间步长都会得到奖励，并且它越接近目的地，奖励就越高。这里，环境模型对机器人采取的每个动作做出反应，同时结合重力、动量等的结果。然后将下一个状态返回给代理。因此，决定行为者下一个状态的所有因素，以及它所获得的回报，都是环境模型的一部分。<br/>一个模型有两个主要部分，过渡函数<strong class="ih hj"> P </strong>和奖励函数<strong class="ih hj"> R </strong>。</p><p id="dac0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设当我们处于状态s时，我们决定采取行动a以到达下一个状态s '并获得奖励r。这被称为一个<strong class="ih hj">转换</strong>步骤，由一个元组(s，a，s '，r)表示。如果我们处于状态<strong class="ih hj"> s </strong>，采取行动<strong class="ih hj"> a </strong>到达下一个状态<strong class="ih hj">s’</strong>并获得奖励<strong class="ih hj"> r </strong>。这是一个单一的<strong class="ih hj">转换:(s，a，s '，r) </strong>。</p><p id="4796" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">转移函数P </strong>记录采取行动a后从状态s转移到s’的概率，同时获得奖励r</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ky"><img src="../Images/767ed55b2889451cc86a0c1b6e85d146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*ScL8K4T1HY3cru-ZngVw6g.png"/></div></figure><p id="1616" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由此，我们可以确定状态转移函数:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es kz"><img src="../Images/bd8c44c97e97342879657c374ec29537.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AVhx7dkdPxHq1OY0wZVrXw.png"/></div></div></figure><p id="7905" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">奖励函数R </strong>是在状态<strong class="ih hj"> s </strong>采取行动<strong class="ih hj"> a </strong>时获得奖励<strong class="ih hj"> r </strong>的期望。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es la"><img src="../Images/66534fe6e5b87bb1611f8bfbb809b631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*XcwLZwFAp55hvQJoVXgByg.png"/></div></figure><h1 id="0ae6" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">策略—代理的策略</h1><p id="60a5" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">策略<strong class="ih hj"> (π) </strong>决定了代理的行为，即代理在状态<strong class="ih hj"> s </strong>采取的动作<strong class="ih hj"> a </strong>。策略可以是:<br/> <strong class="ih hj"> 1。确定性:</strong>对于每一个状态，都有一个定义好的动作，代理将在该状态下采取该动作。<strong class="ih hj"> π(s) = a <br/> 2。随机的:</strong>该策略返回在状态<strong class="ih hj"> s. </strong>中对所有可能的动作采取每个动作的概率(看起来神经网络在这里可能有用？).<strong class="ih hj"> π(a|s) = ℙ[A=a|S=s].</strong></p><h1 id="c3e5" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">价值函数——我的状态有多好</h1><p id="7b0f" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">对于每一个状态，都有一个价值函数决定了未来可以获得的总回报。未来回报，也称为<strong class="ih hj">回报</strong>，是未来折现回报的总和。返回由<strong class="ih hj"> Gₜ.表示</strong></p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lb"><img src="../Images/bf8274e3fbefe4839f66aeda959a8426.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*1XgWKGuBlj17Ii0gkPaFXw.png"/></div></figure><p id="4704" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">贴现因子<strong class="ih hj"> γ∈[0，1] </strong>惩罚未来的奖励，因为:</p><ul class=""><li id="53a8" class="lc ld hi ih b ii ij im in iq le iu lf iy lg jc lh li lj lk bi translated">未来的回报可能具有较高的不确定性；即股票市场。</li><li id="b03e" class="lc ld hi ih b ii ll im lm iq ln iu lo iy lp jc lh li lj lk bi translated">未来的回报并不能带来直接的好处。</li><li id="e069" class="lc ld hi ih b ii ll im lm iq ln iu lo iy lp jc lh li lj lk bi translated">贴现提供了数学上的便利；也就是说，我们不需要永远跟踪未来的步骤来计算回报。</li></ul><p id="aefc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">状态值</strong>是我们在时间<strong class="ih hj"> t. </strong>处于状态<strong class="ih hj"> s </strong>时可以获得的预期收益</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lq"><img src="../Images/e3b3bf4d7d69cf43adeaf6810580a4ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*ZPU7zpywTRF0cwHu7yzaYg.png"/></div></figure><p id="2c8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类似地，我们也有一个<strong class="ih hj">动作值</strong>。它是在时间<strong class="ih hj"> t </strong>在状态<strong class="ih hj"> s </strong>采取行动<strong class="ih hj"> a </strong>的预期收益。它也被称为<strong class="ih hj"> Q值。</strong></p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lr"><img src="../Images/e6f94e7d11a8d9af07bf0c4ef088f233.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*kTFMeA-Fh1-FcT6sPgZHrw.png"/></div></figure><p id="264c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有一种方法可以让我们从Q(s，a) 中确定<strong class="ih hj"> V(s)。如果我们取一个状态下所有可能行动的行动值，乘以在该状态下采取那个行动的概率，会怎么样？这正是我们所做的:</strong></p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ls"><img src="../Images/51c6b778d0998c25354a339160d1d57f.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*Q7RP0ueW0KmPR4ZcqVCPIA.png"/></div></figure><p id="3bfb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一个很酷的是<strong class="ih hj">优势功能。</strong>它是状态<strong class="ih hj"> s </strong>下一个动作<strong class="ih hj"> a </strong>的<strong class="ih hj"> Q值</strong>与状态<strong class="ih hj">s</strong>的值之差你可以这样想:我知道在我目前的状态下，我可以期待一定的回报。现在，如果我采取一项行动，我会处于多好的位置呢？</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lt"><img src="../Images/f8c3ddfd6f4df002b061607dfdd43836.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*Qh_lEho7-yl5Dr8kIgWNYA.png"/></div></figure><h1 id="1aa2" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak">最优值和策略</strong></h1><p id="002e" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">既然我们在讨论学习一个策略来使我们的回报最大化，那么肯定有某种<strong class="ih hj">【最优】</strong>形式。对吗？确实有。</p><p id="a70d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">最优价值函数</strong>是返回最大回报的策略<strong class="ih hj"> π </strong>关联的价值函数。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lu"><img src="../Images/3d10b09729982db233417ef43bb7444f.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*ilI7B5aK4D4JHOGfSr4RjA.png"/></div></figure><p id="6a21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类似地，</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lv"><img src="../Images/1a0ba58c1c80dbedc6959b07070bef46.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*SI9jRSyscce6qnxA8ucFQQ.png"/></div></figure><p id="98ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">显然，最优策略是代理人试图学习的。在每个州采取尽可能好的行动以真正实现回报最大化的政策。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lw"><img src="../Images/ba4800b054b77ade1845194890c701e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*St3j1buPGp3iLtjftJVc9Q.png"/></div></figure><p id="9ae0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="lx">结尾注释:</em> </strong></p><p id="a9df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是本教程的内容。下一集再见！</p><p id="67c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="lx">引用:</em> </strong></p><div class="ly lz ez fb ma mb"><a rel="noopener follow" target="_blank" href="/@ODSC/best-deep-reinforcement-learning-research-of-2019-so-far-e8e83a08c449"><div class="mc ab dw"><div class="md ab me cl cj mf"><h2 class="bd hj fi z dy mg ea eb mh ed ef hh bi translated">2019年迄今为止最佳深度强化学习研究</h2><div class="mi l"><h3 class="bd b fi z dy mg ea eb mh ed ef dx translated">在这篇文章中，我对2019年迄今为止的所有深度强化学习研究进行了非正式调查，并…</h3></div><div class="mj l"><p class="bd b fp z dy mg ea eb mh ed ef dx translated">medium.com</p></div></div><div class="mk l"><div class="ml l mm mn mo mk mp kn mb"/></div></div></a></div><p id="cca2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#key-concepts" rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning . html # key-concepts</a></p></div></div>    
</body>
</html>