<html>
<head>
<title>Elbow method of K-means clustering using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python实现K-means聚类的肘方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/elbow-method-of-k-means-clustering-algorithm-a0c916adc540?source=collection_archive---------1-----------------------#2019-11-23">https://medium.com/analytics-vidhya/elbow-method-of-k-means-clustering-algorithm-a0c916adc540?source=collection_archive---------1-----------------------#2019-11-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/fc5ee88b041010b201437640ee6f4684.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-OQjkvrYgnhA7B3S"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">约翰-马克·史密斯在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="36e1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">K-means聚类是一种无监督学习算法，旨在将n个观察值划分为k个聚类，其中每个观察值属于质心最近的聚类。该算法旨在最小化观测值与其所属聚类质心之间的平方欧氏距离。本文提供了算法的详细代码:- <a class="ae iu" rel="noopener" href="/@joel_34096/k-means-clustering-using-python-from-scratch-7ccdace7789"> K-means从零开始使用Python进行聚类。</a></p><p id="6d62" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇文章中，我们将看到K-means聚类算法的肘方法。肘方法通过用“k”值的范围来拟合模型来帮助选择“k”(聚类数)的最佳值。这里我们将使用二维数据集，但肘方法适用于任何多元数据集。</p><p id="7bd5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们从理解K-means聚类的成本函数开始。我们将K均值聚类的代价函数定义为“ε”,它是数据点和该数据点所属的聚类的相应质心之间的距离的平方和。我们期望成本函数随着迭代次数的增加而减少。让我们通过绘制ε的平方根与迭代次数的关系来验证这一点。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jt"><img src="../Images/e9c8f9ddf69d8a6e4a8e8e9ab96b403f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xg28wTtnZzeuwyO7XJ7ROQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">该图是针对k = 5的二维数据集绘制的。</figcaption></figure><p id="6e90" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为‘ε’的期望值随着迭代次数而减小。我们还观察到，在第8次迭代之后，ε的值少量减少，这表明算法几乎已经收敛，我们可以停止8次迭代，以节省计算能力和时间。算法收敛所需的迭代次数取决于数据集的维数和“k”值。数据集的维数越高，k值越高，算法收敛所需的迭代次数就越多。</p><p id="071e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们用从2到5的‘k’值范围来拟合模型。对于“k”的每个值，算法运行15次迭代，并且计算第15次迭代结束时的成本函数“ε”。下面提供了代码。</p><figure class="ju jv jw jx fd ij"><div class="bz dy l di"><div class="jy jz l"/></div></figure><p id="cf2b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们绘制x轴上的“k值”和y轴上的“ε值”时，在“k”的最佳值处有一个弯头形成。让我们通过绘制“k值”对“ε值”的图表来检验这一点。</p><figure class="ju jv jw jx fd ij"><div class="bz dy l di"><div class="jy jz l"/></div></figure><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ka"><img src="../Images/80548b377a029e43b1a29e4450589835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jundDj7uTVj88L3ZSIWPmg.png"/></div></div></figure><p id="3ded" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上图中，我们观察到在k = 3时有一个弯头形成。因此k的最佳值是3。因此，我们将数据集分为3类。</p></div></div>    
</body>
</html>