<html>
<head>
<title>Introduction To Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-of-linear-regression-692da653072a?source=collection_archive---------28-----------------------#2020-04-14">https://medium.com/analytics-vidhya/introduction-of-linear-regression-692da653072a?source=collection_archive---------28-----------------------#2020-04-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="7bf8" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">通往梦想的道路很少是直线的。你必须曲折地走向幸福。</p></blockquote><p id="f6ec" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">在本文中，我们将学习线性回归和线性回归的类型。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jk"><img src="../Images/c96bd83f281e61db2c7d6e32b9ab571d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rs6j7UscTNWIuLk0PZQLdg.jpeg"/></div></div></figure><h1 id="5c48" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">线性回归</strong></h1><p id="b0fd" class="pw-post-body-paragraph ii ij hi il b im ku io ip iq kv is it jh kw iw ix ji kx ja jb jj ky je jf jg hb bi translated"><em class="ik"> - &gt;回归的思想是获取连续的数据，并找出一条最适合该数据的直线。</em></p><h2 id="501a" class="kz jx hi bd jy la lb lc kc ld le lf kg jh lg lh kk ji li lj ko jj lk ll ks lm bi translated">1.假设的表示</h2><ul class=""><li id="b139" class="ln lo hi il b im ku iq kv jh lp ji lq jj lr jg ls lt lu lv bi translated">y = b + m x</li><li id="e659" class="ln lo hi il b im lw iq lx jh ly ji lz jj ma jg ls lt lu lv bi translated">也可以写成:- h(x) = theta(0)+ theta(1)x</li><li id="977e" class="ln lo hi il b im lw iq lx jh ly ji lz jj ma jg ls lt lu lv bi translated">基本上回归的目的是找出m是什么。</li></ul><h1 id="f108" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">最适合的是什么？</h1><p id="ffee" class="pw-post-body-paragraph ii ij hi il b im ku io ip iq kv is it jh kw iw ix ji kx ja jb jj ky je jf jg hb bi translated">根据参数的数量，它可以是任何形状(轴上的一个点、二维中的一条线、三维中的一个平面或高维中的超平面)。</p><h2 id="0c97" class="kz jx hi bd jy la lb lc kc ld le lf kg jh lg lh kk ji li lj ko jj lk ll ks lm bi translated">2.价值函数</h2><p id="ce03" class="pw-post-body-paragraph ii ij hi il b im ku io ip iq kv is it jh kw iw ix ji kx ja jb jj ky je jf jg hb bi translated">这将计算出如何将最佳直线拟合到我们的数据，或者简单地说:成本函数是通过确保形状和每个点的实际观察值之间的所有距离之和尽可能相同来完成的。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es mb"><img src="../Images/76667cf0e1bcb11b140f3e6596897319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jb0ffgItyZTM6V4zGqx7OA.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated"><strong class="bd jy">成本函数的数学表示</strong></figcaption></figure><blockquote class="if ig ih"><p id="189d" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">注意:我只展示了简单线性回归的方法。你可以把它推广到多元线性回归。如果你不知道什么是一元和多元回归？那就别急，继续看。</p></blockquote><h1 id="9f68" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">线性回归的类型</strong></h1><h2 id="194a" class="kz jx hi bd jy la lb lc kc ld le lf kg jh lg lh kk ji li lj ko jj lk ll ks lm bi translated"><strong class="ak">答。简单线性回归</strong></h2><p id="92b6" class="pw-post-body-paragraph ii ij hi il b im ku io ip iq kv is it jh kw iw ix ji kx ja jb jj ky je jf jg hb bi translated">这种方法通过拟合最佳线性关系，使用单个自变量来预测因变量。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es mg"><img src="../Images/4572460bdfc0782c1d26e9db2634c1c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R9C3aJIiveoZJf1Sb4_R2Q.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated"><strong class="bd jy">线性回归的图形表示</strong></figcaption></figure><h2 id="f56c" class="kz jx hi bd jy la lb lc kc ld le lf kg jh lg lh kk ji li lj ko jj lk ll ks lm bi translated">b)。多元线性回归</h2><p id="c652" class="pw-post-body-paragraph ii ij hi il b im ku io ip iq kv is it jh kw iw ix ji kx ja jb jj ky je jf jg hb bi translated">这种方法通过拟合最佳线性关系，使用多个自变量来预测因变量。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es mh"><img src="../Images/285285d499bb7b374290ef7fb7ef49a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*YG7tFQ5D-aqQIvYlsip6Jg.png"/></div><figcaption class="mc md et er es me mf bd b be z dx translated"><strong class="bd jy">多元线性回归</strong></figcaption></figure><p id="432a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">在多元回归的情况下，可以通过使用下式最小化成本函数，以与简单线性回归相同的方式找到参数:</p><h2 id="178d" class="kz jx hi bd jy la lb lc kc ld le lf kg jh lg lh kk ji li lj ko jj lk ll ks lm bi translated">3.梯度下降:</h2><p id="c736" class="pw-post-body-paragraph ii ij hi il b im ku io ip iq kv is it jh kw iw ix ji kx ja jb jj ky je jf jg hb bi translated">给定由参数训练集定义的函数，梯度下降从初始训练集值开始，并向使函数最小化的一组值移动。这种最小化是通过使用微积分完成的，在函数梯度的负方向上采取步骤。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es mi"><img src="../Images/7f67e3db614a0f77b01b74ecf4e3c068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*rwrpJa9DpH3uZvYN3of0Fg.jpeg"/></div></figure><p id="8458" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><em class="ik">注意:Alpha基本上控制了我们在下坡时的坡度</em></p><h2 id="e877" class="kz jx hi bd jy la lb lc kc ld le lf kg jh lg lh kk ji li lj ko jj lk ll ks lm bi translated"><strong class="ak">梯度下降的一些重要性质:</strong></h2><ol class=""><li id="0c07" class="ln lo hi il b im ku iq kv jh lp ji lq jj lr jg mj lt lu lv bi translated">如果你在某个地方稍微改变你的设置，那么你会达到另一个局部最小值。</li><li id="e893" class="ln lo hi il b im lw iq lx jh ly ji lz jj ma jg mj lt lu lv bi translated">如果θ(0)和θ(1)被初始化为局部最小值，那么一次迭代不会改变它们的值。</li><li id="4c22" class="ln lo hi il b im lw iq lx jh ly ji lz jj ma jg mj lt lu lv bi translated">不同的特征具有相似的值范围，那么梯度下降可以更快地收敛。</li></ol><h2 id="3d01" class="kz jx hi bd jy la lb lc kc ld le lf kg jh lg lh kk ji li lj ko jj lk ll ks lm bi translated">结论</h2><blockquote class="if ig ih"><p id="bf19" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">在这篇文章中，我们成功地学习了线性回归的假设，线性回归的成本函数，线性回归的类型和线性回归的梯度下降。请继续关注更多更新。如果你喜欢这篇文章，并从中学到了一些东西，请留下掌声。</p><p id="8893" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><em class="hi">感谢您的阅读:)</em></p></blockquote></div></div>    
</body>
</html>