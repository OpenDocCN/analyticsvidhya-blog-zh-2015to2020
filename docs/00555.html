<html>
<head>
<title>A math-first explanation of Word2Vec</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2Vec的数学优先解释</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/maths-behind-word2vec-explained-38d74f32726b?source=collection_archive---------0-----------------------#2019-07-30">https://medium.com/analytics-vidhya/maths-behind-word2vec-explained-38d74f32726b?source=collection_archive---------0-----------------------#2019-07-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/56914c0d32d3c10308881684a594f3a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*wwDrMqTX_eW7Vr9UxFPAOQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">演职员表</strong>:https://thegradient.pub/nlp-imagenet/<a class="ae iv" href="https://thegradient.pub/nlp-imagenet/" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><h1 id="49f1" class="iw ix hi bd iu iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">简介</strong></h1><blockquote class="jt ju jv"><p id="a2a1" class="jw jx jy jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku hb bi translated">Word2Vec已经成为自然语言处理中各种ask的垫脚石。</p></blockquote><p id="32a0" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">当我开始学习Word2Vec算法时，我发现很多资源只关注实现，而没有涉及太多细节。</p><p id="816e" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">然而，当解决一个现实世界的项目时，理解底层算法和它所基于的数学假设以及实现代码是有帮助的，这就是我们将在这个博客中讨论的。</p><p id="fe40" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">在这篇博客中，我们将以数学为先深入Word2Vec，揭开这个美丽算法内部工作的一些最复杂的细节。</p><p id="2c99" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated"><strong class="jz hj">注:</strong>本博客受到了<a class="ae iv" href="http://web.stanford.edu/class/cs224n/" rel="noopener ugc nofollow" target="_blank"> <strong class="jz hj">这门</strong> </a>令人惊叹的斯坦福NLP课程的高度激励。</p><h1 id="282c" class="iw ix hi bd iu iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">什么是Word2Vec？</strong></h1><p id="556d" class="pw-post-body-paragraph jw jx hi jz b ka ky kc kd ke kz kg kh kv la kk kl kw lb ko kp kx lc ks kt ku hb bi translated">在深入Word2Vec算法之前，我们首先需要了解什么是单词嵌入，为什么它很重要？</p><blockquote class="jt ju jv"><p id="2be3" class="jw jx jy jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku hb bi translated">单词嵌入是一种将一段文本转换成我们的机器可以阅读的数字格式的方法。</p></blockquote><p id="1c86" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">在Word2Vec出现之前，有几种算法用于以数字的形式表示文本。最常见的算法之一是计数矢量器，它为我们提供了文本的本地化表示。让我们考虑下面的例子:</p><p id="91d6" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">假设我们的词汇表中有5个单词，我们想用一个向量来表示“快乐”和“愉快的”。这将是计数矢量器的输出:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/ef140a9d0b30c1e675a7579a0cfb0967.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4aX9Q_GY-M2RNzgaYWdrtA.png"/></div></div></figure><p id="98d3" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">在上表中，如果一个单词出现在文本中，我们将得到1，否则值将为零。我们也可以对整个句子这样做，方法是输入特定文档中单词的计数，并将rest保持为零。但是这种方法有两个问题。</p><p id="8d18" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">首先，虽然happy和欢快是相似的，但是这两个向量之间没有相似性的概念，因为它们是正交向量。</p><p id="e1e2" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">第二，当我们有一个大型语料库时，这些向量在本质上非常稀疏，当我们的词汇中有数百万个单词时，这些向量变得非常低效。有一些改进的算法，如TF-IDF，但它仍然面临上述两个问题。</p><blockquote class="jt ju jv"><p id="ef5a" class="jw jx jy jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku hb bi translated">Word2Vec算法解决了这些缺点。它基于分布语义学的思想，这意味着我们可以通过理解一个单词的同伴(上下文)来理解一个单词的意思。</p></blockquote><p id="dd5f" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">因此，为了学习单词的表示，我们将尝试学习特定单词出现的上下文，并希望这个单词的向量与上下文单词的向量相似。这将是一个分布式的表示，因为现在，这个词的意义分布在所有的维度上。这将解决我们上面讨论的两个问题。</p><p id="403d" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">现在的问题是我们如何做到这一点？</p><h2 id="2d76" class="li ix hi bd iu lj lk ll jb lm ln lo jf kv lp lq jj kw lr ls jn kx lt lu jr lv bi translated"><strong class="ak">跳格算法</strong></h2><p id="0f07" class="pw-post-body-paragraph jw jx hi jz b ka ky kc kd ke kz kg kh kv la kk kl kw lb ko kp kx lc ks kt ku hb bi translated">该算法背后的主要思想是，首先，我们将为词汇表中的每个单词随机初始化向量。</p><p id="0726" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">之后，我们将遍历每个位置<strong class="jz hj"> <em class="jy"> t </em> </strong>，我们将该位置的中心词定义为<strong class="jz hj"> <em class="jy"> c </em> </strong>，其上下文词定义为<strong class="jz hj"> <em class="jy"> o </em> </strong>。为了识别上下文单词，我们将定义一个大小为<strong class="jz hj"> <em class="jy"> m </em> </strong>的窗口，这意味着我们的模型将把位置<strong class="jz hj"> <em class="jy"> t-m到t+m </em> </strong>的单词视为上下文。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es if"><img src="../Images/8d67330be8ddf1dcfc7e2f50224d60bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*rD_qLU-8xkuccTyfG_2EaA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">学分:<a class="ae iv" href="http://web.stanford.edu/class/cs224n/" rel="noopener ugc nofollow" target="_blank">斯坦福CS224N </a></figcaption></figure><p id="1ff8" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">一旦我们在位置t有了所有的上下文单词，我们将尝试最大化给定中心单词的上下文单词的可能性，即，我们将计算给定中心单词的我们的模型预测上下文单词的概率，并且我们将尝试最大化该概率。这种可能性可以用以下公式表示:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/fb6f198cf7b6d1f0a8be71808c8d5be1.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*Pb20h56HRMORLu5VtH3ckQ.png"/></div></figure><p id="ceeb" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">为了使这个方程的形式易于求导，并使之成为一个最小化问题，我们将只取方程的对数，并乘以-1来计算-ve对数似然。</p><p id="c6bf" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">注意，现在乘法将变成求和，因为我们已经取了对数。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es if"><img src="../Images/fe9d6d4db90bc68b30c984ff1a93e798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*IC2S6GD1YfRNEAuHMs8KqA.png"/></div></figure><p id="2d0c" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">现在的问题是，给定中心词，我们如何计算上下文词的概率？</p><p id="06a5" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">为此，我们将用两组向量来表示每个单词，<strong class="jz hj"> <em class="jy"> Uw </em> </strong>和<strong class="jz hj"> <em class="jy"> Vw </em> </strong>。当<strong class="jz hj"> w </strong>是上下文词时，我们将使用<strong class="jz hj"> <em class="jy"> Uw </em> </strong>，当<strong class="jz hj"> w </strong>是中心词时，我们将使用<strong class="jz hj"> <em class="jy"> Vw </em> </strong>。使用这两个向量，我们的中心词<strong class="jz hj"> o </strong>和上下文词<strong class="jz hj"> c </strong>的概率方程将如下所示:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/ae72d89d98a04884ab2be41d3deb1a64.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*mR3U3Hvjjv3BJkEpW47oXA.png"/></div></figure><p id="76c2" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">正如我们所看到的，这是softmax函数的一个例子。在分子中，我们有单词<strong class="jz hj"> o </strong>和<strong class="jz hj"> c </strong>的点积，它捕捉了这两个向量之间的相似性。相似度越高，概率就越高。分母为我们提供了一种方法来归一化整个词汇表中的概率值，使其总和为1。</p><p id="1ec0" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">现在我们已经有了损失函数，接下来是训练模型的部分。如果我们看看上面的似然方程，我们可以看到方程θ中只有一个模型参数。这是一个长向量，包含所有单词的长度为<strong class="jz hj"> <em class="jy"> d </em> </strong> <em class="jy"> </em>的<strong class="jz hj"> v </strong>和<strong class="jz hj"> u </strong>向量。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/75e2c3860b09f04da15ceaacce37ecdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*4Xsw-qSrnSgSZkN12wgeqQ.png"/></div></figure><p id="1f68" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">我们将使用梯度下降算法下山，并慢慢地改变所有的权重，以最大化我们的可能性。我们将通过对我们的损失函数相对于U和v求导来获得我们必须移动以改变权重的方向。如果您不熟悉链规则和梯度下降算法，请分别参考<a class="ae iv" href="https://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jz hj">此</strong> </a>和<a class="ae iv" rel="noopener" href="/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c"> <strong class="jz hj">此</strong> </a>链接。</p><p id="68a8" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">先说<strong class="jz hj"><em class="jy">J(θ)</em></strong>w . r . t<strong class="jz hj"><em class="jy">Vc:</em></strong>的导数</p><p id="71a8" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">由于<strong class="jz hj"> <em class="jy"> J(θ) </em> </strong>是一个比值，取对数就要取上面带负号的分母。所以，我们可以这样表示我们的导数:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/70938b186b2eb0744c6d6300948ed67f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8dTOLGC1zsdlCDHolhjQdw.png"/></div></div></figure><p id="ab87" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">让我们把方程分成两部分，分别求解。</p><p id="e6ab" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">对于第一部分，由于log(exp(x)等于x，我们可以将其写成:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/1447e478248259b58b0a0554daa640d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ggTtJHee2nFJkuKSY-ZLGQ.png"/></div></div></figure><p id="3da3" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">如果不清楚它是如何变成<strong class="jz hj"> <em class="jy"> Uo </em> </strong>的，试着按元素求导。如果我们对w.r.t <strong class="jz hj"> <em class="jy"> Vc1 </em> </strong>求导，结果将是<strong class="jz hj"> <em class="jy"> Uo1 </em> </strong>，因为只有这一项才会有<strong class="jz hj"> <em class="jy"> Vc1 </em> </strong>。对所有元素都这样做，我们会得到一个向量<strong class="jz hj"> <em class="jy"> Uo1 </em> </strong>到<strong class="jz hj"> <em class="jy"> Uon </em> </strong>相当于<strong class="jz hj"> <em class="jy"> Uo。</em>T57】</strong></p><p id="7990" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">现在让我们进入等式的第二部分。对log(x)求导，并在总和内移动导数，我们将得到这一项:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es if"><img src="../Images/185a4c34475fff1f65f235870aa207e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*5Jkq_1HHg63ly59UoCRy9A.png"/></div></figure><p id="94a0" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">对exp(x)项求导，并重新排列求和符号，将得到该项:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es if"><img src="../Images/9e690be6d055f43ecc01fb716f27c44b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*NfVboIy9_74Vaxeqmdl0VQ.png"/></div></figure><p id="70e1" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">如果我们仔细观察上面的项，可以发现求和中的项与我们上面定义的概率项相同，因此可以写成:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es if"><img src="../Images/5300898cf295764874b2847d49a65fc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*7kwcPyI_zb0ayrRNVU5r_Q.png"/></div></figure><p id="9e9f" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">综合起来，我们可以写出:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/62ebbff2f1683fb0bbd588ef3651c9fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nnw2pApQCnGXtw-FCxQ3NA.png"/></div></div></figure><p id="5085" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">这个等式有一点非常有趣。第一部分是上下文单词的当前表示。</p><p id="5043" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">第二部分是根据我们的模型对上下文单词应该看起来像什么的预期，因为我们采用所有上下文单词的当前表示，并将它们与它们在当前模型中的概率相乘。</p><p id="3381" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">所以基本上，我们减去实际的和预期的表示，得到我应该移动的方向，改变我的权重向量<strong class="jz hj"> <em class="jy"> Vc </em> </strong>，这样我可以最大化可能性。</p><p id="f019" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">以同样的方式向前移动，我们还可以计算出<strong class="jz hj">T5【J(θ)</strong>wrt对<strong class="jz hj"> <em class="jy"> Uw的导数。</em> </strong>对于<strong class="jz hj"> <em class="jy"> Uw，</em> </strong>会有两种情况，一种是w是上下文词，一种是w不是上下文词。这将是两种情况下衍生产品的输出:</p><h2 id="dbf4" class="li ix hi bd iu lj lk ll jb lm ln lo jf kv lp lq jj kw lr ls jn kx lt lu jr lv bi translated">W ≠ O:</h2><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es if"><img src="../Images/4af698e09d63c410e4c74552e1199b67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*OMrYiAdUUaViV52Ze58Uww.png"/></div></figure><h2 id="fa74" class="li ix hi bd iu lj lk ll jb lm ln lo jf kv lp lq jj kw lr ls jn kx lt lu jr lv bi translated"><strong class="ak"> W = O: </strong></h2><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/7928de23e556acae8c851ab8c3b2d6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*_2TfE7YQRyZFNNE46VU2Qw.png"/></div></figure><p id="5622" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">一旦我们有了这两个导数，我们就可以在SGD方程中使用它们来更新权重。</p><p id="5339" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">然而，这种方法有一个问题。正如我们所见，在分母中，我们必须取所有单词点积的指数，当我们有大量词汇时，这是非常耗时的。我们将需要训练数百万的重量，这是不可行的。</p><p id="c76b" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">所以，为了增加训练时间，一种叫做<strong class="jz hj"><em class="jy"/></strong>的新方法被采用。在这种方法中，我们将在一个步骤中只更新一小部分权重。我们将选择几个单词，即不在上下文窗口中的单词，并且我们将以这样的方式改变我们的权重，即最大化真实上下文单词的概率，并且最小化出现在中心单词周围的随机单词的概率。这改变了损失函数，现在我们试图最大化下面的等式:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es if"><img src="../Images/6391f7794aed668c8d194fef75a1d64c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*QpN7RzDfaMsfgCBrTUxAWQ.png"/></div></figure><p id="4b7a" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">其中K是负样本的数量。</p><p id="4db1" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">为了选择随机单词，我们使用一元分布，其中更频繁的单词更有可能被选择。</p><p id="1a71" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">为了最大化上述项，我们再次需要对损失函数相对于权重进行求导，在这种情况下，它将是<strong class="jz hj"> <em class="jy"> Uw、Uk和Vc。</em> </strong>按照与上面类似的方式做这件事，我们会得到以下三个等式:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/182b38455510023c5ebbaeef1516c698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UVF0043gsdZd858blgF-wg.png"/></div></div></figure><p id="e107" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">计算出所有导数后，现在我们可以一点一点地更新我们的权重向量，并获得一个向量表示，它将把出现在相似上下文中的单词指向相同的方向。</p><p id="e551" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">Word2Vec算法还有各种更高级的变体，如斯坦福开发的<a class="ae iv" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jz hj"><em class="jy"/></strong></a><strong class="jz hj"><em class="jy"/></strong>、脸书开发的<a class="ae iv" href="https://fasttext.cc/" rel="noopener ugc nofollow" target="_blank"><strong class="jz hj"><em class="jy">fast text</em></strong></a><strong class="jz hj"><em class="jy"/></strong>等。</p><p id="ef57" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">我希望这篇文章能让您很好地理解Word2Vec模型是如何被训练的，以及到底发生了什么。在下一篇文章中，我们将使用Word2Vec、主题建模和TF-IDF来识别相似的句子，并将比较每个模型的结果。</p><p id="31a0" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh kv kj kk kl kw kn ko kp kx kr ks kt ku hb bi translated">谢谢大家！！</p></div></div>    
</body>
</html>