<html>
<head>
<title>Azure Spark ETL in Azure Synapse Analytics (Workspace) with sample datasets and machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Azure Synapse Analytics(Workspace)中的Azure Spark ETL，包含样本数据集和机器学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/azure-spark-etl-in-azure-synapse-analytics-workspace-with-sample-datasets-and-machine-learning-5b95676e3d0a?source=collection_archive---------13-----------------------#2020-10-01">https://medium.com/analytics-vidhya/azure-spark-etl-in-azure-synapse-analytics-workspace-with-sample-datasets-and-machine-learning-5b95676e3d0a?source=collection_archive---------13-----------------------#2020-10-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="b1bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">能够使用样本数据集处理数据，以学习使用Azure synapse analytics workspace spark进行大规模提取、转换和加载。</p><p id="6430" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我可以用pyspark做ETL，然后用Spark SQL做聚合，然后用scala做建模，所有这些都在一个笔记本里。</p><p id="32eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本教程只展示了如何使用spark进行回归建模的模型训练。</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="2889" class="jm jn hi ji b fi jo jp l jq jr">note: the data set and use case are just imaginary.</span></pre><ul class=""><li id="9c6d" class="js jt hi ih b ii ij im in iq ju iu jv iy jw jc jx jy jz ka bi translated">Azure帐户</li><li id="aa42" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc jx jy jz ka bi translated">创建Azure Synapse分析工作区</li><li id="2b27" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc jx jy jz ka bi translated">创建火花线轴</li><li id="c567" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc jx jy jz ka bi translated">我使用的是中型实例</li><li id="0f16" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc jx jy jz ka bi translated">没有库被上传</li><li id="60d4" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc jx jy jz ka bi translated">Spark版本:2.4.4</li></ul><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="39a4" class="jm jn hi ji b fi jo jp l jq jr">from azureml.opendatasets import NycTlcYellow<br/><br/>data = NycTlcYellow()<br/>data_df = data.to_spark_dataframe()<br/># Display 10 rows<br/>display(data_df.limit(10))</span></pre><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kg"><img src="../Images/d221f388c8c3fb90a5ef389c5b1fdc90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2KQQOpaJAfMat4FD.jpg"/></div></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="ad33" class="jm jn hi ji b fi jo jp l jq jr">display(data_df)</span></pre><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ko"><img src="../Images/9d49a62031b0ab765553b9ef6cd750fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hSVNXNhVE3GlfKSG.jpg"/></div></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="19fa" class="jm jn hi ji b fi jo jp l jq jr">from pyspark.sql.functions import * <br/>from pyspark.sql import *</span><span id="891b" class="jm jn hi ji b fi kp jp l jq jr">df1 = data_df.withColumn("Date", (col("tpepPickupDateTime").cast("date"))) <br/>display(df1)</span></pre><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kq"><img src="../Images/746c740e8e9e2774d0d561c39815de63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lt2aQjSw5jileSuX.jpg"/></div></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="d8dc" class="jm jn hi ji b fi jo jp l jq jr">df1.dropDuplicates("key","pickup_datetime","pickup_longitude","pickup_latitude","dropoff_longitude","dropoff_latitude")<br/>df1.printSchema</span></pre><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kr"><img src="../Images/fcffd660c3b0c4be0caed99b2af15446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*iMZyPMgt0iuCNzOy.jpg"/></div></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="a255" class="jm jn hi ji b fi jo jp l jq jr">df2 = df1.withColumn("year", year(col("date"))) .withColumn("month", month(col("date"))) .withColumn("day", dayofmonth(col("date"))) .withColumn("hour", hour(col("date"))) <br/>display(df2)</span></pre><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ks"><img src="../Images/fb833df29fcf92ccdf802e56e959205f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VTIJoC1Owubv-jMY.jpg"/></div></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="7613" class="jm jn hi ji b fi jo jp l jq jr">df2.groupBy("year","month").agg(sum("fareAmount").alias("Total"),count("vendorID").alias("Count")).sort(asc("year"), asc("month")).show()</span><span id="5ec7" class="jm jn hi ji b fi kp jp l jq jr">dfgrouped = df2.groupBy("year","month").agg(sum("fareAmount").alias("Total"),count("vendorID").alias("Count")).sort(asc("year"), asc("month")) display(dfgrouped)</span></pre><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kt"><img src="../Images/09b852b876b2064ebe8bc45704734cb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*hV7HBt01bkkVs3w-.jpg"/></div></div></figure><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ku"><img src="../Images/a9c709dcfa52e57d6f48e05952ab9fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2KPlzdy9tO4Ct7yw.jpg"/></div></div></figure><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kv"><img src="../Images/5681012e6dc916c118b74e7a2f210cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*E6Y02GwKzpCYUxcm.jpg"/></div></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="2a36" class="jm jn hi ji b fi jo jp l jq jr">df2.createOrReplaceTempView("nycyellow")</span><span id="609b" class="jm jn hi ji b fi kp jp l jq jr">%%sql<br/>select  year(cast(tpepPickupDateTime  as timestamp)) as tsYear,<br/>        day(cast(tpepPickupDateTime  as timestamp)) as tsDay, <br/>        hour(cast(tpepPickupDateTime  as timestamp)) as tsHour,<br/>        avg(totalAmount) as avgTotal, avg(fareAmount) as avgFare<br/>from nycyellow<br/>group by  tsYear,tsDay, tsHour<br/>order by  tsYear,tsDay, tsHour</span></pre><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kw"><img src="../Images/788ee5dec64d9616719f46f7bf1b296a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OtvaomofoyGFzSNJ.jpg"/></div></div></figure><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kx"><img src="../Images/7b8083f8653a1af2952f96ebeacbe3d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lzthDDwslslC8GZZ.jpg"/></div></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="3057" class="jm jn hi ji b fi jo jp l jq jr">%%sql<br/>CREATE TABLE dailyaggr<br/>  COMMENT 'This table is created with existing data'<br/>  AS select  year(cast(tpepPickupDateTime  as timestamp)) as tsYear,<br/>        month(cast(tpepPickupDateTime  as timestamp)) as tsmonth,<br/>        day(cast(tpepPickupDateTime  as timestamp)) as tsDay, <br/>        hour(cast(tpepPickupDateTime  as timestamp)) as tsHour,<br/>        avg(totalAmount) as avgTotal, avg(fareAmount) as avgFare<br/>from nycyellow<br/>group by  tsYear, tsmonth,tsDay, tsHour<br/>order by  tsYear, tsmonth,tsDay, tsHour</span></pre><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ky"><img src="../Images/b297b8ad52ddb95d650fe3cf78b3daa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OZugKEDCX6SAIr-S.jpg"/></div></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="899d" class="jm jn hi ji b fi jo jp l jq jr">from pyspark.ml.regression import LinearRegression</span></pre><ul class=""><li id="31d5" class="js jt hi ih b ii ij im in iq ju iu jv iy jw jc jx jy jz ka bi translated">在这里，我也从scala切换到回归建模</li><li id="c989" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc jx jy jz ka bi translated">Majic命令是%%spark，用于scala编程</li></ul><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="762f" class="jm jn hi ji b fi jo jp l jq jr">%%spark<br/>import org.apache.spark.ml.feature.VectorAssembler <br/>import org.apache.spark.ml.linalg.Vectors <br/>val dailyaggr = spark.sql("SELECT tsYear, tsMonth, tsDay, tsHour, avgTotal FROM dailyaggr")<br/>val featureCols=Array("tsYear","tsMonth","tsDay","tsHour") <br/>val assembler: org.apache.spark.ml.feature.VectorAssembler= new VectorAssembler().setInputCols(featureCols).setOutputCol("features") <br/>val assembledDF = assembler.setHandleInvalid("skip").transform(dailyaggr) <br/>val assembledFinalDF = assembledDF.select("avgTotal","features")</span></pre><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kz"><img src="../Images/3d935b5d87791b90c7a02a0a17acbcd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*F06n8V_Y5NzFqztp.jpg"/></div></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="1c7d" class="jm jn hi ji b fi jo jp l jq jr">%%spark<br/>import org.apache.spark.ml.feature.Normalizer <br/>val normalizedDF = new Normalizer().setInputCol("features").setOutputCol("normalizedFeatures").transform(assembledFinalDF)</span><span id="f81a" class="jm jn hi ji b fi kp jp l jq jr">%%spark<br/>val normalizedDF1 = normalizedDF.na.drop()</span><span id="5929" class="jm jn hi ji b fi kp jp l jq jr">%%spark<br/>val Array(trainingDS, testDS) = normalizedDF1.randomSplit(Array(0.7, 0.3))</span></pre><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es la"><img src="../Images/53c60547bebd053e8e5e06c54eef1a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TCK1Hei2cqdZUkEl.jpg"/></div></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="f4e3" class="jm jn hi ji b fi jo jp l jq jr">%%spark<br/>import org.apache.spark.ml.regression.LinearRegression<br/>// Create a LinearRegression instance. This instance is an Estimator. <br/>val lr = new LinearRegression().setLabelCol("avgTotal").setMaxIter(100)<br/>// Print out the parameters, documentation, and any default values. println(s"Linear Regression parameters:\n ${lr.explainParams()}\n") <br/>// Learn a Linear Regression model. This uses the parameters stored in lr.<br/>val lrModel = lr.fit(trainingDS)<br/>// Make predictions on test data using the Transformer.transform() method.<br/>// LinearRegression.transform will only use the 'features' column. <br/>val lrPredictions = lrModel.transform(testDS)</span><span id="8939" class="jm jn hi ji b fi kp jp l jq jr">%%spark<br/>import org.apache.spark.sql.functions._ <br/>import org.apache.spark.sql.types._ <br/>println("\nPredictions : " ) <br/>lrPredictions.select($"avgTotal".cast(IntegerType),$"prediction".cast(IntegerType)).orderBy(abs($"prediction"-$"avgTotal")).distinct.show(15)</span></pre><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kx"><img src="../Images/0b112a2f32e69b8c8f8a68dee901c92d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*chaaQD_NO5Mrd9jx.jpg"/></div></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="0eeb" class="jm jn hi ji b fi jo jp l jq jr">%%spark<br/>import org.apache.spark.ml.evaluation.RegressionEvaluator <br/><br/>val evaluator_r2 = new RegressionEvaluator().setPredictionCol("prediction").setLabelCol("avgTotal").setMetricName("r2") <br/>//As the name implies, isLargerBetter returns if a larger value is better or smaller for evaluation. <br/>val isLargerBetter : Boolean = evaluator_r2.isLargerBetter <br/>println("Coefficient of determination = " + evaluator_r2.evaluate(lrPredictions))</span></pre><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lb"><img src="../Images/cd4078adf39935d7a0d0f2ae08952a1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*t6Jb25GiYrsq5DgX.jpg"/></div></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="cd8f" class="jm jn hi ji b fi jo jp l jq jr">%%spark<br/>//Evaluate the results. Calculate Root Mean Square Error <br/>val evaluator_rmse = new RegressionEvaluator().setPredictionCol("prediction").setLabelCol("avgTotal").setMetricName("rmse") <br/>//As the name implies, isLargerBetter returns if a larger value is better for evaluation. <br/>val isLargerBetter1 : Boolean = evaluator_rmse.isLargerBetter <br/>println("Root Mean Square Error = " + evaluator_rmse.evaluate(lrPredictions))</span></pre><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kw"><img src="../Images/b542197f838076b9101b6e7a111e770d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*b1kfgoZQ8G2_2ySE.jpg"/></div></div></figure><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lc"><img src="../Images/e7ce3c5fab26135bf9ef349b6b0a200d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oZ9-2NWECNkagcvD.jpg"/></div></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="85cc" class="jm jn hi ji b fi jo jp l jq jr">%%spark<br/>import com.microsoft.spark.sqlanalytics.utils.Constants<br/>import org.apache.spark.sql.SqlAnalyticsConnector._</span></pre><ul class=""><li id="c63e" class="js jt hi ih b ii ij im in iq ju iu jv iy jw jc jx jy jz ka bi translated">写入Azure Synapse Analytics (SQL DW)</li><li id="c31f" class="js jt hi ih b ii kb im kc iq kd iu ke iy kf jc jx jy jz ka bi translated">不需要创建表格。</li></ul><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="8705" class="jm jn hi ji b fi jo jp l jq jr">%%spark<br/>dailyaggr.repartition(2).write.sqlanalytics("accsynapsepools.wwi.dailyaggr", Constants.INTERNAL)</span><span id="4a6b" class="jm jn hi ji b fi kp jp l jq jr">DROP TABLE [wwi].[dailyaggr]<br/>GO<br/><br/>SET ANSI_NULLS ON<br/>GO<br/>SET QUOTED_IDENTIFIER ON<br/>GO<br/><br/>CREATE TABLE [wwi].[dailyaggr]<br/>( <br/>	[tsYear] [int]  NULL,<br/>	[tsMonth] [int]  NULL,<br/>	[tsDay] [int]  NULL,<br/>	[tsHour] [int]  NULL,<br/>	[avgTotal] [float]  NULL<br/>)<br/>WITH<br/>(<br/>	DISTRIBUTION = ROUND_ROBIN,<br/>	CLUSTERED COLUMNSTORE INDEX<br/>)<br/>GO</span></pre><figure class="jd je jf jg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ld"><img src="../Images/473c470dedb67cf2f3acea81a4bddd09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zGcsGRFqyWIK05io.jpg"/></div></div></figure><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="9a75" class="jm jn hi ji b fi jo jp l jq jr">%%spark<br/>val dailyaggrdf = spark.read.sqlanalytics("accsynapsepools.wwi.dailyaggr")r")</span><span id="34ab" class="jm jn hi ji b fi kp jp l jq jr">%%spark<br/>display(dailyaggrdf)</span><span id="4785" class="jm jn hi ji b fi kp jp l jq jr">%%spark<br/>dailyaggrdf.count()</span></pre><p id="9d15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">了解更多详情。</p><p id="014a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae le" href="https://github.com/balakreshnan/synapseAnalytics/blob/master/sparketl.md" rel="noopener ugc nofollow" target="_blank">https://github . com/balakreshnan/synapse analytics/blob/master/spark ETL . MD</a></p></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><p id="3327" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lm">最初发表于</em><a class="ae le" href="https://github.com/balakreshnan/synapseAnalytics/blob/master/sparketl.md" rel="noopener ugc nofollow" target="_blank">T5【https://github.com】</a><em class="lm">。</em></p></div></div>    
</body>
</html>