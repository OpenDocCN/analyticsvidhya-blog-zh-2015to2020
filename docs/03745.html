<html>
<head>
<title>PCA vs LDA vs T-SNE — Let’s Understand the difference between them!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PCA vs LDA vs T-SNE——让我们来理解它们之间的区别！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pca-vs-lda-vs-t-sne-lets-understand-the-difference-between-them-22fa6b9be9d0?source=collection_archive---------0-----------------------#2020-02-17">https://medium.com/analytics-vidhya/pca-vs-lda-vs-t-sne-lets-understand-the-difference-between-them-22fa6b9be9d0?source=collection_archive---------0-----------------------#2020-02-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/e113b94a6790c965aeebae2a284c2e48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wmg4hx-FcJZ8lG1Wo45N6w.png"/></div></div></figure><div class=""/></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><blockquote class="ix iy iz"><p id="54d9" class="ja jb jc jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy hb bi translated">每当我们提到高维数据或者如何可视化具有数百个甚至更多属性的数据时，我们都会看到这些方法。</p><p id="e2a6" class="ja jb jc jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy hb bi translated">我们经常采用的解决方案是降维技术，它可以帮助我们解决这个问题。但是，这里出现的主要问题是何时使用哪一个？它们之间的基本区别是什么，这些技术的直觉是什么？</p><p id="87cd" class="ja jb jc jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy hb bi translated">在本文中，我们将探讨上述问题的解决方案。</p></blockquote><p id="18a4" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu"> <em class="jc">我们开始吧！</em>T3】</strong></p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="b66c" class="kc kd ht bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">主成分分析</strong></h1><p id="7431" class="pw-post-body-paragraph ja jb ht jd b je la jg jh ji lb jk jl jz lc jo jp ka ld js jt kb le jw jx jy hb bi translated">PCA是一种<em class="jc">无监督的机器学习方法</em>，用于降维。主成分分析(PCA)的主要思想是降低由许多彼此相关的变量组成的数据集的维度，这些变量或多或少，同时最大程度地保留数据集中存在的变化。</p><p id="2d9a" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">这是通过将变量转换成一组新的变量来实现的，该组新的变量是来自原始数据集的变量或属性的组合，其方式是保留最大变化。这种属性组合被称为<strong class="jd hu"><em class="jc">【PCs】</em></strong>，具有最大捕获方差的成分被称为<strong class="jd hu"> <em class="jc">主导主成分</em> </strong>。方差保留的顺序随着我们向下移动而降低，即PC1 &gt; PC2 &gt; PC3 &gt; …等等。</p><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lf"><img src="../Images/9ab6574eeb0513e8bdc17a1b62d54064.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rRzIcsna0NWK-Ugs9uM7WQ.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">将2D数据转换为1D数据(PC1包含最大方差)</figcaption></figure><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lo"><img src="../Images/a7d1434bc336281462da4b56f1fd46a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gtew7GFsP64QTuGuiDixpg.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">将3D数据转换到2D/1D ( PC1 &gt; PC2 &gt; PC3)</figcaption></figure><p id="79d1" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">一旦我们把数据转换成主成分，我们就可以选择去掉那些没有方差的变量。这提供了一种方法来减少维度，并专注于方差较大的维度。</p><p id="6cb7" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu">我们为什么要使用PCA？</strong></p><p id="e5b7" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">实际上使用PCA有两个原因:</p><ol class=""><li id="b8aa" class="lp lq ht jd b je jf ji jj jz lr ka ls kb lt jy lu lv lw lx bi translated"><strong class="jd hu"> <em class="jc">降维:</em> </strong>分布在大量列上的信息被转换成主成分(PC)，使得前几个PC可以解释总信息(方差)的相当大的块。这些PC可以用作机器学习模型中的解释变量。</li><li id="b7ac" class="lp lq ht jd b je ly ji lz jz ma ka mb kb mc jy lu lv lw lx bi translated"><strong class="jd hu"> <em class="jc">可视化类:</em> </strong>对于3维以上的数据(特征)，可视化类(或聚类)的分离是很困难的。对于前两台电脑本身，通常可以看到明显的区别。</li></ol><p id="ed60" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu">PCA是一种特征选择技术吗？</strong></p><p id="255c" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">它不是一种特征选择技术。相反，它是一种特征组合技术。因为每台PC都是原始数据集中所有列的加权相加组合。</p><h1 id="03a6" class="kc kd ht bd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz bi translated">主成分分析方法</h1><p id="565f" class="pw-post-body-paragraph ja jb ht jd b je la jg jh ji lb jk jl jz lc jo jp ka ld js jt kb le jw jx jy hb bi translated"><strong class="jd hu">第一步:标准化每一列<br/> </strong>如果有不同数量级的值，则对它们进行缩放/标准化。将分类变量转换为虚拟数值变量，因为PCA只对数值数据起作用。</p><p id="c018" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu">第二步:计算协方差矩阵<br/> </strong>从分析特征的协方差矩阵开始。</p><p id="4524" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu"> <em class="jc">为什么是协方差矩阵？<br/> </em> </strong>协方差衡量的是两个变量之间的相互关系，即两个变量相对于彼此是否在同一个方向上运动。当协方差为正时，这意味着，如果一个变量增加，另一个也增加。当协方差为负时，情况正好相反。</p><figure class="lg lh li lj fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/cdf2f6a9fec70bc08dfe203f3a169272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*f-fEXTeH8EHhxlcMPNpB1A.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">X和Y的协方差</figcaption></figure><p id="ab62" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">协方差矩阵计算所有可能的列组合的协方差。结果，它变成了一个行数和列数相同的正方形矩阵。</p><p id="5cc1" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">矩阵对一般向量的作用可以被认为是<em class="jc">拉伸和旋转的组合。</em></p><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mj"><img src="../Images/3ac1e76d546a0bd1a95e9e391d4d957d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_FrnREt7BDNQLCjHZ-BqRw.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">具有拉伸和旋转的协方差矩阵</figcaption></figure><p id="3330" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu">步骤3:计算特征值和特征向量</strong></p><p id="c495" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">对于一个给定的矩阵，存在一个特殊的方向，沿着这个方向效应只是拉伸(没有旋转)，这些特殊的方向叫做<strong class="jd hu"> <em class="jc">特征向量</em> </strong>或<strong class="jd hu"> <em class="jc">特征方向</em> </strong>。</p><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mk"><img src="../Images/9910f4367e358d7a6818be45ae16aff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YtIE59GmXdlZkE4tuHtA0A.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">只有拉伸的协方差矩阵</figcaption></figure><p id="c5d0" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">矩阵a的特征向量和特征值被定义为非零的x和<strong class="jd hu"> ⲗ </strong>值，</p><p id="aecb" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu"> AX = ⲗX ( </strong> A只是拉伸<strong class="jd hu"> ) </strong></p><p id="0d3a" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">对于一个n维方阵，有‘n’个特征向量和‘n’个特征值。</p><p id="e737" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">特征向量是主分量方向，特征值是沿这些方向的方差的大小。</p><p id="7117" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">在上面的例子中，[-0.49，0.87]是主分量(特征向量)，5.51是拉伸的幅度(特征值)</p><p id="671e" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu">步骤4:导出主成分特征</strong></p><p id="9bb7" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">通过取特征向量和标准化列的点积，导出主成分特征。</p><h1 id="8d9c" class="kc kd ht bd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz bi translated"><strong class="ak">线性判别分析</strong></h1><p id="eb14" class="pw-post-body-paragraph ja jb ht jd b je la jg jh ji lb jk jl jz lc jo jp ka ld js jt kb le jw jx jy hb bi translated">LDA是一种有监督的机器学习方法，用于区分两个组/类。线性判别分析(LDA)的主要思想是最大化两组之间的可分性，以便我们可以做出最佳决策来对它们进行分类。<strong class="jd hu"> <em class="jc"> LDA类似于PCA，有助于降维，但它侧重于通过创建新的线性轴并将数据点投影到该轴来最大化已知类别之间的可分性。</em> </strong></p><p id="7fcd" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">LDA并不致力于寻找主成分，它基本上观察哪种类型的点/特征/子空间提供更多的区分来分离数据。</p><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ml"><img src="../Images/41d87d741947e6084ba12993cd6b59b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ELmUt0gXZPm3s5bfcr5KOg.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">LDA与PCA</figcaption></figure><p id="a74a" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">LDA的目标是找到一条使类分离最大化的线。因此，要做到这一点，我们需要定义一个良好的分离措施。</p><p id="fce9" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu">平均向量</strong></p><p id="ed2d" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">均值向量用于计算每类数据点的均值。</p><figure class="lg lh li lj fd hk er es paragraph-image"><div class="er es mm"><img src="../Images/9344219f7037b57d62f3dc31602e4ac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*MX-tmlZBaHxJ_CpoUIlpCw.jpeg"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">X和Y特征空间中每个类的平均向量。</figcaption></figure><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mn"><img src="../Images/d5189178932220854a2ddac46a471b51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v5mrCnlBl02GjWDSD-k3eQ.png"/></div></div></figure><p id="4546" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu">分离的驱动力</strong></p><p id="5a02" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">目标是找到给出最大间隔的最佳组<strong class="jd hu"> w，</strong>，即两个平均值之间的距离最大。</p><p id="5c07" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">因此，目标函数将是</p><p id="eada" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu"> L1常态:</strong></p><figure class="lg lh li lj fd hk er es paragraph-image"><div class="er es mo"><img src="../Images/ac720243e8dc02e501545d21d607aca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*4JD1xYLW78VrmFJ10fT7SA.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">L1范数目标函数</figcaption></figure><p id="921f" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">但是，投影平均值之间的距离不是一个很好的度量，因为它没有考虑类内的标准偏差。</p><h1 id="8f30" class="kc kd ht bd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz bi translated">如何定义哪个阶级更好？</h1><p id="57f0" class="pw-post-body-paragraph ja jb ht jd b je la jg jh ji lb jk jl jz lc jo jp ka ld js jt kb le jw jx jy hb bi translated">类内不变性最小而其他类间可变性最大的数据被认为是好的。</p><p id="b84f" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">由<strong class="jd hu"> Fisher </strong>提出的解决方案是最大化一个代表均值之间差异的函数，通过对<strong class="jd hu"> <em class="jc">类内(intra-class) </em> </strong>可变性或所谓的分散性的测量来标准化。</p><p id="9533" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">注意:<strong class="jd hu"> <em class="jc">散点=方差</em> </strong></p><p id="4e9d" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">对于每一类，我们将离差定义为:(<em class="jc">投影样本与其类均值的平方差之和</em>)。</p><figure class="lg lh li lj fd hk er es paragraph-image"><div class="er es mp"><img src="../Images/e0963d013a46c4822540b13863d33e32.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/format:webp/1*JY3W3AJ7AIcN4gRsB5_qrg.png"/></div></figure><p id="375c" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu"> Si </strong>在将类<strong class="jd hu"> <em class="jc"> ωi </em> </strong>投影到Y空间后测量类内的可变性。</p><p id="208a" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu"><em class="jc">S1+S2</em></strong><em class="jc"/>度量投影后手头两个类内的变异性，因此称为投影样本的<strong class="jd hu"> <em class="jc">类内散度</em> </strong>。</p><p id="d2b3" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">因此，Fisher线性判别式被定义为使标准函数最大化的线性函数:(由投影样本的类内分散标准化的投影平均值之间的距离。</p><p id="0edf" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu">目标是最大化J(w) ( L2范数):</strong></p><figure class="lg lh li lj fd hk er es paragraph-image"><div class="er es mq"><img src="../Images/bf73fccd9d5d795507cb16d96fbe9f02.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*WllMkAYfkGoOpP5HkI9dDw.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">L2范数目标函数</figcaption></figure><p id="1eb3" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu">PCA与LDA的相似之处:</strong></p><ol class=""><li id="7a87" class="lp lq ht jd b je jf ji jj jz lr ka ls kb lt jy lu lv lw lx bi translated">两者都按照重要性的顺序排列新坐标轴。</li></ol><ul class=""><li id="4cf6" class="lp lq ht jd b je jf ji jj jz lr ka ls kb lt jy mr lv lw lx bi translated">PC1(PCA创建的第一个新轴)占数据变化的最大部分，PC2(第二个新轴)做的第二好，依此类推…</li><li id="2bfe" class="lp lq ht jd b je ly ji lz jz ma ka mb kb mc jy mr lv lw lx bi translated">LD1(LDA创建的第一个新轴)占数据中最大的变化，LD2(第二个新轴)做了第二好的工作，等等…</li></ul><p id="c8eb" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">2.B <em class="jc">这两种算法都告诉我们哪个属性或特征在创建新轴的过程中贡献更大。</em></p><p id="cbeb" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">3.LDA就像PCA——两者都试图降低维数。</p><ul class=""><li id="e70f" class="lp lq ht jd b je jf ji jj jz lr ka ls kb lt jy mr lv lw lx bi translated">PCA寻找方差最大的属性。</li><li id="a968" class="lp lq ht jd b je ly ji lz jz ma ka mb kb mc jy mr lv lw lx bi translated">LDA试图最大化已知类别的分离。</li></ul><h1 id="a35e" class="kc kd ht bd ke kf md kh ki kj me kl km kn mf kp kq kr mg kt ku kv mh kx ky kz bi translated">T分布随机邻居嵌入(T-SNE)</h1><p id="c554" class="pw-post-body-paragraph ja jb ht jd b je la jg jh ji lb jk jl jz lc jo jp ka ld js jt kb le jw jx jy hb bi translated">T-SNE是一种uns <em class="jc">监督机器学习方法</em>，用于在低维中可视化高维数据。T-SNE用于设计/实现，并且可以将任意数量的特征空间降入二维特征空间。</p><p id="008c" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">PCA和LDA都用于可视化和降维，但<strong class="jd hu"> <em class="jc"> T-SNE仅专门用于可视化目的</em> </strong>。它非常适合于高维数据集的可视化。</p><p id="2996" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">与PCA和LDA不同，T-SNE是一种<em class="jc">非线性数据可视化工具。</em>这意味着它不会形成一条直线来分隔类或计算方差，也不会使用任何范数或距离度量来计算点之间的距离。</p><p id="f836" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu">T-SNE工作概述:</strong></p><ul class=""><li id="4a8b" class="lp lq ht jd b je jf ji jj jz lr ka ls kb lt jy mr lv lw lx bi translated">该算法首先计算高维空间中的点的相似概率，然后计算对应的低维空间中的点的相似概率。点的相似性计算为条件概率，即如果在以A为中心的<strong class="jd hu"> <em class="jc">【正态分布】</em> </strong>高斯分布下按比例挑选邻居，则A点会选择B点作为其邻居。它<strong class="jd hu"> <em class="jc">使用T分布</em> </strong>中的T检验。</li><li id="230c" class="lp lq ht jd b je ly ji lz jz ma ka mb kb mc jy mr lv lw lx bi translated">然后，它试图最小化高维空间和低维空间中这些条件概率(或相似性)之间的差异，以便在低维空间中完美地表示数据点。</li><li id="6b38" class="lp lq ht jd b je ly ji lz jz ma ka mb kb mc jy mr lv lw lx bi translated">为了测量条件概率差之和的最小化，t-SNE使用梯度下降法最小化所有数据点的<a class="ae ms" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> <strong class="jd hu">库尔贝克-莱布勒散度</strong> </a>之和。</li></ul><p id="2c6e" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu">注:</strong> Kullback-Leibler散度或KL散度是一个概率分布如何偏离第二个预期概率分布的度量。</p><figure class="lg lh li lj fd hk er es paragraph-image"><div class="er es mt"><img src="../Images/31867576e8656ac57f3d1dd60573fabc.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*AysdoYS2cw95AnrlMkZ6Kg.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">P II Q:表示P偏离Q的程度</figcaption></figure><p id="c4d3" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu">与香农熵的关系</strong></p><p id="1a26" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">香农熵是从N个同等可能性中识别X所需的比特数，<strong class="jd hu">减去</strong>均匀分布与真实分布的KL散度。</p><figure class="lg lh li lj fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mu"><img src="../Images/4c94a14c7d5f290063dcea01950b4bc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2PLKvjgg1QgMMjVe5LH-NQ.jpeg"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">香农熵~ KL散度关系</figcaption></figure><p id="2533" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">T-SNE给人的印象是，它通过将数据带到二维空间来对数据进行分类，但实际上，它并没有降低维度。它是一个可视化工具，告诉你每个类是如何分布的，它们之间有没有重叠。</p><p id="6c68" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">在超维空间中，欧几里德距离变得无用。高维度中的类之间的相似性对应于低维度中的类之间的短距离。(各班数据点变得很接近)。</p><p id="73b5" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">T-SNE使用<em class="jc">梯度下降</em>方法最小化所有数据点上的KL散度之和。</p><figure class="lg lh li lj fd hk er es paragraph-image"><div class="er es mv"><img src="../Images/433cfc200e78d7cd48499b43ac0e9672.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*egeAu14SDYrkV4PIEvqwKw.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">KL散度目标函数</figcaption></figure><blockquote class="ix iy iz"><p id="fec3" class="ja jb jc jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy hb bi translated">在高维空间中:</p></blockquote><figure class="lg lh li lj fd hk er es paragraph-image"><div class="er es mw"><img src="../Images/588a0f59fba1b5656d9ef28541b003fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*f9lKkDGMc2CL_N_SVHNsLA.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">高维条件概率</figcaption></figure><blockquote class="ix iy iz"><p id="9f04" class="ja jb jc jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy hb bi translated">在低维空间中:</p></blockquote><figure class="lg lh li lj fd hk er es paragraph-image"><div class="er es mx"><img src="../Images/6e1e2e9a60b8ade73fc6a628ede36576.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*wXPIW-3loBsOyOR7n6CBOQ.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">低维条件概率，此处方差为1/2 (0.5) —固定值</figcaption></figure><p id="66ec" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated">T-SNE将数据点从<strong class="jd hu"> P j|i映射到Q j|i </strong> <em class="jc">(将数据点从高维映射到低维)</em></p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><p id="2481" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu"> <em class="jc">这就是上面提到的三种技术的基本区别。</em>T9】</strong></p><p id="2ad8" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><strong class="jd hu"> <em class="jc">感谢阅读。如果你觉得这篇文章有用，请分享:)</em> </strong></p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="19df" class="kc kd ht bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><em class="my">参考文献:</em></h1><p id="2967" class="pw-post-body-paragraph ja jb ht jd b je la jg jh ji lb jk jl jz lc jo jp ka ld js jt kb le jw jx jy hb bi translated"><a class="ae ms" href="https://www.youtube.com/watch?v=1cDSlY5Q-Sw" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=1cDSlY5Q-Sw</a></p><p id="1e44" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><a class="ae ms" href="https://www.dezyre.com/data-science-in-python-tutorial/principal-component-analysis-tutorial" rel="noopener ugc nofollow" target="_blank">https://www . dezyre . com/data-science-in-python-tutorial/principal-component-analysis-tutorial</a></p><p id="32b4" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><a class="ae ms" href="https://www.machinelearningplus.com/machine-learning/principal-components-analysis-pca-better-explained/" rel="noopener ugc nofollow" target="_blank">https://www . machine learning plus . com/machine-learning/principal-components-analysis-PCA-better-explained/</a></p><p id="6a34" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><a class="ae ms" href="http://setosa.io/ev/principal-component-analysis/" rel="noopener ugc nofollow" target="_blank">http://setosa.io/ev/principal-component-analysis/</a></p><p id="1020" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><a class="ae ms" href="https://www.youtube.com/watch?v=azXCzI57Yfc" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=azXCzI57Yfc</a></p><div class="hh hi ez fb hj mz"><a href="https://sebastianraschka.com/Articles/2014_python_lda.html" rel="noopener  ugc nofollow" target="_blank"><div class="na ab dw"><div class="nb ab nc cl cj nd"><h2 class="bd hu fi z dy ne ea eb nf ed ef hs bi translated">线性判别分析</h2><div class="ng l"><h3 class="bd b fi z dy ne ea eb nf ed ef dx translated">线性判别分析(LDA)是数据预处理中最常用的降维技术</h3></div><div class="nh l"><p class="bd b fp z dy ne ea eb nf ed ef dx translated">sebastianraschka.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn hp mz"/></div></div></a></div><p id="f176" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><a class="ae ms" href="https://www.datacamp.com/community/tutorials/introduction-t-sne" rel="noopener ugc nofollow" target="_blank">https://www . data camp . com/community/tutorials/introduction-t-SNE</a></p><p id="61a6" class="pw-post-body-paragraph ja jb ht jd b je jf jg jh ji jj jk jl jz jn jo jp ka jr js jt kb jv jw jx jy hb bi translated"><a class="ae ms" href="http://www.sci.utah.edu/~shireen/pdfs/tutorials/Elhabian_LDA09.pdf" rel="noopener ugc nofollow" target="_blank">http://www . sci . Utah . edu/~ shire en/pdf/tutorials/El habian _ LDA 09 . pdf</a></p></div></div>    
</body>
</html>