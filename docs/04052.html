<html>
<head>
<title>A better visualization of L1 and L2 Regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">L1和L2正则化的更好的可视化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/effects-of-l1-and-l2-regularization-explained-5a916ecf4f06?source=collection_archive---------7-----------------------#2020-03-03">https://medium.com/analytics-vidhya/effects-of-l1-and-l2-regularization-explained-5a916ecf4f06?source=collection_archive---------7-----------------------#2020-03-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/fd4a35901df1ce09f5e8970ed69b5cb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XdWBFrUiSob8N1n6Xpd0FQ.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">马尔科·博诺莫在<a class="ae hv" href="https://unsplash.com/s/photos/gradient-descend?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/><div class=""><h2 id="ae1f" class="pw-subtitle-paragraph iv hx hy bd b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm dx translated">L1正则化为什么把权重缩小到0的直观解释。</h2></div><p id="4b6c" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">正则化是一种防止模型过度拟合的流行方法。想法很简单:我想保持我的模型权重小，所以我会为拥有大的权重增加一个惩罚。两种最常见的正则化方法是拉索(或L1)正则化和岭(或L2)正则化。他们用绝对重量(L1)或重量的平方(L2)来惩罚模型。这就引出了一个问题:那么我应该选择哪一个呢？还有Lasso为什么要进行特征选择？</p><h1 id="67eb" class="kj kk hy bd kl km kn ko kp kq kr ks kt je ku jf kv jh kw ji kx jk ky jl kz la bi translated">老办法</h1><p id="39a0" class="pw-post-body-paragraph jn jo hy jp b jq lb iz js jt lc jc jv jw ld jy jz ka le kc kd ke lf kg kh ki hb bi translated">在正则化的所有最佳解释中，你经常会听到“L1正则化倾向于将不重要特征的系数缩小到0，但L2不会”，如这里的<a class="ae hv" href="https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c" rel="noopener" target="_blank">和这里的</a><a class="ae hv" href="https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261" rel="noopener" target="_blank">和</a>所示。视觉解释通常由图表组成，就像这张非常受欢迎的图片，来自Hastie、Tibshirani和Friedman的《统计学习的<em class="lg">元素》</em>:</p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es lh"><img src="../Images/61df1fa43976943e0b2792280a49bd58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/0*MuVvxE8pkBLtPzrx"/></div></figure><p id="4ed6" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在Bishop的<em class="lg">模式识别和机器学习</em>中也可以看到:</p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es lm"><img src="../Images/d1cadf6e2d65de5ea9595783c4cd7dad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/0*JbWKTQY9GB66y6hR"/></div></figure><p id="7803" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我发现这些图表不直观，所以做了一个更简单的，感觉更容易理解。</p><h1 id="e389" class="kj kk hy bd kl km kn ko kp kq kr ks kt je ku jf kv jh kw ji kx jk ky jl kz la bi translated">新的方式</h1><p id="ed1c" class="pw-post-body-paragraph jn jo hy jp b jq lb iz js jt lc jc jv jw ld jy jz ka le kc kd ke lf kg kh ki hb bi translated">这是我的观点，一步一步的可视化。首先，上面的图像实际上是三维的，不能很好地翻译到书或屏幕上。相反，让我们回到线性数据集的基础。</p><p id="4633" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">首先，我们创建一个非常简单的数据集，只有一个权重:<em class="lg"> y=w*x </em>。我们的线性模型将尝试学习权重<em class="lg"> w </em>。</p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es ln"><img src="../Images/cb649823a9a86624c2f3ed6d7e4eb678.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/0*l6VjHq1gbkB9qc0T.png"/></div></figure><p id="ab09" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">假装我们不知道<em class="lg"> w </em>的正确值，我们随机选择<em class="lg"> w </em>的值。然后，我们计算不同的<em class="lg"> w </em>值的损失(均方误差)。在<em class="lg"> w=0.5 </em>时损耗为0，这是我们之前定义的<em class="lg"> w </em>的正确值。随着我们远离<em class="lg"> w=0.5 </em>，损耗会增加。</p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es lo"><img src="../Images/257d4b6904cb8ed88f83f3ea234181bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*YaHr8EnfctKwj0dL.png"/></div></figure><p id="8698" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在我们绘制正则化损失函数。当<em class="lg"> w </em>为0时，L1损耗为0，并且随着远离<em class="lg"> w=0 </em>而线性增加。随着远离<em class="lg"> w=0 </em>，L2损耗非线性增加。</p><div class="li lj lk ll fd ab cb"><figure class="lp hk lq lr ls lt lu paragraph-image"><img src="../Images/34ca91b32a43f6e54d1aa8f817755d17.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*GJZxy56d3f8he6P9.png"/></figure><figure class="lp hk lq lr ls lt lu paragraph-image"><img src="../Images/59e5183bbc85dd34d7758a29cc15117f.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*1YSu88tdl1dErnV1.png"/></figure></div><p id="260f" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在是有趣的部分。正则化损失的计算方法是将损失项加到正则化项上。对上面的每个损失都这样做，我们会得到下面的蓝色(L1正则化损失)和红色(L2正则化损失)曲线。</p><p id="27e9" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在L1正则化损失(蓝线)的情况下，使损失最小化的<em class="lg"> w </em>的值在<em class="lg"> w=0 </em>。对于L2规则化损失(红线)，使损失最小化的<em class="lg"> w </em>的值低于实际值(0.5)，但不完全达到0。</p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es lo"><img src="../Images/34eadb34c00e71b2115c1119aac1b7ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/0*RCNM57z81K0Sg9Zs.png"/></div></figure><p id="893d" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">就这样，对于相同的lambda值，L1正则化将特征权重缩小到0！</p><p id="194a" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">对此的另一种思考方式是在使用梯度下降来最小化损失函数的背景下。我们可以沿着损失函数的梯度到损失最小的点。然后，正则化将一个梯度添加到非正则化损失的梯度上。L1正则化在除0之外的每个值处将固定梯度添加到损失，而由L2正则化添加的梯度随着我们接近0而减小。因此，当<em class="lg"> w </em>的值非常接近0时，使用L1正则化的梯度下降继续将<em class="lg"> w </em>推向0，而L2上的梯度下降随着越接近0而减弱。</p><p id="c9cd" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><em class="lg">用来创建这些可视化效果的笔记本和代码可以在我的</em><a class="ae hv" href="https://github.com/nickyeolk/regularization_visualized" rel="noopener ugc nofollow" target="_blank"><em class="lg">github repo</em></a><em class="lg">中找到！也可以在Twitter</em><a class="ae hv" href="https://twitter.com/ninjanugget" rel="noopener ugc nofollow" target="_blank"><em class="lg">@ ninjanaugget</em></a>T22】或者在<a class="ae hv" href="https://www.linkedin.com/in/nickyeolk/" rel="noopener ugc nofollow" target="_blank"><em class="lg">LinkedIn</em></a>上关注我</p></div></div>    
</body>
</html>