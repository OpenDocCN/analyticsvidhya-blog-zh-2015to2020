<html>
<head>
<title>VAE Generative Modelling for Face Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于人脸识别的VAE生成模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/vae-generative-modelling-for-face-recognition-71e8ba16950c?source=collection_archive---------21-----------------------#2020-04-13">https://medium.com/analytics-vidhya/vae-generative-modelling-for-face-recognition-71e8ba16950c?source=collection_archive---------21-----------------------#2020-04-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e07f3c784ee4e5b5ae4443b224f680f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LHeJs_EbWiSUH2F5"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">凯利·西克玛在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="df55" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">简介:</strong></h1><p id="cce0" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">生成模型是最近发展起来的另一种有趣的深度学习机制，用于无监督学习问题，如密度估计、图像样本生成、去偏差等。这种类型的建模通过对训练集中的所有特征给予不偏不倚的重要性，将面部识别带到了下一个级别，并且可以从中获得最大的准确性。在本文中，我们将简要了解人脸识别应用的VAE生成模型所涉及的过程。那么，让我们开始吧…</p><h1 id="1f89" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">背景:</strong></h1><p id="2ded" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">生成模型通常用于根据训练数据生成新的数据实例，而判别模型有助于区分数据实例。与判别模型相比，生成模型很难建模，并且处理更困难的任务。它捕捉图像的许多相关性，如“船之类的东西会出现在靠近水的地方”和“驾驶汽车与道路和信号相关的东西”。生成模型有不同的类型，即自动编码器、变分自动编码器和生成对抗网络。</p><p id="0a8e" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在我们深入研究VAE之前，让我们简要地看看什么是潜在变量。</p><p id="b1ad" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">什么是潜在变量？*在统计学中，这些变量不是直接观察到的，而是通过数学模型推断出来的。在我们的生成模型中，我们通过将输入值训练成由潜在空间给出的压缩格式来创建变量的抽象。列表中的每个变量都包含图像的一个独特特征。</p><h1 id="3e0d" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak"> VAE及其建筑:</strong></h1><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/5d1cee0bbf76e6599b6540bbe01656ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qzuvL2BuIV8tAwGgCRe9sQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图一。VAE建筑</figcaption></figure><p id="a240" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">一般来说，VAE架构包括一个编码器网络，用于使用我们的ConvNets训练输入图像，该ConvNets输出压缩的潜在空间，解码器试图从该压缩的潜在空间解码回相应的图像。与传统的自动编码器相比，VAE对每个潜在向量都有一个均值和标准差的随机层，如图所示。这意味着(管理部门)和标准偏差。(sigma)描述潜在变量的概率分布。因此编码器和解码器不再是确定性的，而是本质上是概率性的。</p><p id="48c3" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">比方说，编码器生成p(z|x)的概率分布，解码器生成q(x|z)的概率分布。现在我们在这个潜在空间上增加一个先验，这将是一个高斯单位。这将强制编码器在高斯中心周围平滑地分布编码，如果它试图在曲线的特定区域分组，则会受到惩罚。通过这样做，我们可以避免编码过度适应特定的特征表示。现在带先验的pdf变成p(x|z || p(z))，p(z)就是我们带单位高斯的先验。</p><p id="7a06" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"> VAEs损失:</strong></p><p id="86a2" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">由于我们已将均值和标准差限制为遵循单位高斯分布，因此现在有了一个可学习的参数，解码器重构输出时也应该学习损失函数。因此，我们通常会有两个损失函数。</p><ol class=""><li id="bd51" class="lb lc hi jv b jw kr ka ks ke ld ki le km lf kq lg lh li lj bi translated"><strong class="jv hj">潜在损失:</strong>它衡量潜在变量与单位高斯的匹配程度。它由一个著名的散度函数定义，称为<a class="ae iu" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj"> kullback-leibler </strong> </a>散度函数，它显示了一个概率分布如何不同于另一个。为了与高斯单位进行比较，其形式如下:</li></ol><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/b6e042cf38b993eda9b6a59f6b47b109.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*TIMIb3J0TPLqpzK0nOINcg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">KL损失函数</figcaption></figure><p id="7016" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">2.<strong class="jv hj">重建损失:</strong>它衡量输入图像与L1范数提供的解码器输出重建图像的匹配程度。最终的完整损失函数由下式给出，其中c为加权系数。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/a8f3b17c2cad4fced624147d6e649097.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*-Cxhpvy6wG2v1gXEtrmo3A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">VAE总损失函数</figcaption></figure><p id="1fd3" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">下面是这个函数的代码片段:</p><pre class="kx ky kz la fd lm ln lo lp aw lq bi"><span id="b181" class="lr iw hi ln b fi ls lt l lu lv">def vae_loss_function(x, x_recon, mu, logsigma, kl_weight=0.0005):<br/>  <br/>  latent_loss = 0.5 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) - 1.0 - logsigma, axis=1)<br/>  <br/>  reconstruction_loss = tf.reduce_mean(tf.abs(x-x_recon), axis=(1,2,3))<br/>  <br/>  vae_loss = kl_weight * latent_loss + reconstruction_loss<br/>  <br/>  return vae_loss</span></pre><p id="7257" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在我们有了这个损失函数，我们还需要另一个度量来处理反向传播。这是因为我们目前拥有的z仍然具有随机抽样的随机性。因此，为了避免这种情况，我们将使用εϵ∼n(0,(i来重新参数化)，这是标准的常态。这使得z具有确定性，因此可以在反向传播过程中进行区分，如图所示。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/114e9c169e689b06f2f35ae14eabeead.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*MYFUMjgci-oShB6DproOkg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">重新参数化潜在空间</figcaption></figure><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/71d028c4c7d8cc32c6e9ddfa2d3c3fdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*2iVLcuJGB0g8nWlAJaiXig.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">重新参数化后的z方程</figcaption></figure><ul class=""><li id="ed5b" class="lb lc hi jv b jw kr ka ks ke ld ki le km lf kq ly lh li lj bi translated">σ是协方差矩阵</li></ul><p id="5012" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">潜在向量是从高斯采样的固定平均值和标准差以及来自标准正态的随机ε的组合。</p></div><div class="ab cl lz ma gp mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="hb hc hd he hf"><p id="db77" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在我们可以反向探测并训练我们的网络，我们将看看如何在面部识别过程中<strong class="jv hj">消除</strong>网络的偏差，因为这将大大提高对具有罕见特征的面部的测试准确性。因此，在面部识别CelebA标记的数据集中，我们将有不同种类的脸，其中包括一些罕见的特征，如太阳镜、帽子等。主要思想是使用通过VAE学习的潜在变量在训练期间自适应地重新采样CelebA数据。准确地说，我们将根据图像的潜在特征在数据集中出现的频率来更新图像在训练过程中被使用的概率。因此，具有稀有特征的面部图像将更有可能被采样，而过度表现的特征将在采样期间减少。我们称这种架构为DB-VAE(去偏变分自动编码器)。</p><p id="6832" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj"> DB-VAE建筑:</strong></p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/54198f2b7205b40f4ebf4eeb18f19e5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-tLP1ApWe52b7dBYPf6iTw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">VAE模式图</figcaption></figure><p id="3429" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在这个DB-VAE中，我们还可以看到一个Zo，它给出了有脸或无脸的编码器分类，这是DB-VAE和VAE之间的另一个区别。对于有人脸的标签，我们将更新去偏概率，对于没有人脸的标签，我们将只有Zo类预测。</p><h2 id="fbc1" class="lr iw hi bd ix mh mi mj jb mk ml mm jf ke mn mo jj ki mp mq jn km mr ms jr mt bi translated"><strong class="ak"> DB-VAE损失函数:</strong></h2><ol class=""><li id="0d86" class="lb lc hi jv b jw jx ka kb ke mu ki mv km mw kq lg lh li lj bi translated">VAE损失:如前所述，它包括潜在损失和重建损失。</li><li id="e867" class="lb lc hi jv b jw mx ka my ke mz ki na km nb kq lg lh li lj bi translated"><strong class="jv hj">分类损失:</strong>这是二进制分类问题的标准交叉熵损失。</li></ol><p id="c852" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">一般来说，对于非人脸标签只会有分类。下面的等式中，I(y)对于人脸标注为1，对于非人脸标注为0。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es nc"><img src="../Images/0b53bbb3666dcce1d3107e4f220b9b27.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*Q6MmtW71vtj0-jkU03BgmQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">DB-VAE的总损失函数</figcaption></figure><p id="233a" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">下面是这个损失函数的代码片段:</p><pre class="kx ky kz la fd lm ln lo lp aw lq bi"><span id="320b" class="lr iw hi ln b fi ls lt l lu lv">def db_vae_loss_function(x, x_pred, y, y_logit, mu, logsigma):</span><span id="8b6d" class="lr iw hi ln b fi nd lt l lu lv">vae_loss = vae_loss_function(x, x_pred, mu, logsigma)</span><span id="cfcc" class="lr iw hi ln b fi nd lt l lu lv">classification_loss=tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_logit)</span><span id="2780" class="lr iw hi ln b fi nd lt l lu lv">face_indicator = tf.cast(tf.equal(y, 1), tf.float32)</span><span id="2d21" class="lr iw hi ln b fi nd lt l lu lv">total_loss = tf.reduce_mean(classification_loss +face_indicator*vae_loss)</span><span id="9621" class="lr iw hi ln b fi nd lt l lu lv">return total_loss, classification_loss</span></pre><p id="212e" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">下面是根据数据集中的要素表示可用性为每个训练图像生成样本概率的代码片段。</p><pre class="kx ky kz la fd lm ln lo lp aw lq bi"><span id="2428" class="lr iw hi ln b fi ls lt l lu lv">def get_latent_mu(images, dbvae, batch_size=1024):<br/>  N = images.shape[0]<br/>  mu = np.zeros((N, latent_dim))<br/>  for start_ind in range(0, N, batch_size):<br/>    end_ind = min(start_ind+batch_size, N+1)<br/>    batch = (images[start_ind:end_ind]).astype(np.float32)/255.<br/>    _, batch_mu, _ = dbvae.encode(batch)<br/>    mu[start_ind:end_ind] = batch_mu<br/>  return mu</span><span id="a8ec" class="lr iw hi ln b fi nd lt l lu lv">def get_training_sample_probabilities(images, dbvae, bins=10, smoothing_fac=0.001): <br/>    print("Recomputing the sampling probabilities")<br/>    <br/>    mu = get_latent_mu(images, dbvae)</span><span id="fc84" class="lr iw hi ln b fi nd lt l lu lv"># sampling probabilities for the images<br/>    training_sample_p = np.zeros(mu.shape[0])<br/>    <br/>    # consider the distribution for each latent variable <br/>    for i in range(latent_dim):<br/>      <br/>        latent_distribution = mu[:,i]<br/>        # generate a histogram of the latent distribution<br/>        hist_density, bin_edges =  np.histogram(latent_distribution, density=True, bins=bins)</span><span id="9392" class="lr iw hi ln b fi nd lt l lu lv"># find which latent bin every data sample falls in <br/>        bin_edges[0] = -float('inf')<br/>        bin_edges[-1] = float('inf')<br/>        <br/>  <br/>        bin_idx = np.digitize(latent_distribution, bin_edges)</span><span id="8878" class="lr iw hi ln b fi nd lt l lu lv"># smooth the density function<br/>        hist_smoothed_density = hist_density + smoothing_fac<br/>        hist_smoothed_density = hist_smoothed_density /  np.sum(hist_smoothed_density)</span><span id="c99a" class="lr iw hi ln b fi nd lt l lu lv"># invert the density function <br/>        p = 1.0/(hist_smoothed_density[bin_idx-1])<br/>        <br/>        # normalize all probabilities<br/>        p = p / np.sum(p)<br/>       <br/>        training_sample_p = np.maximum(p, training_sample_p)<br/>        <br/>        <br/>    # final normalization<br/>    training_sample_p /= np.sum(training_sample_p)</span><span id="fb3b" class="lr iw hi ln b fi nd lt l lu lv">return training_sample_p</span></pre><p id="2819" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">dbvae.encode函数使用编码器CNN来训练网络，以便将输入图像压缩到潜在分布中，从而为每个图像生成训练样本概率值。我们将压缩分布转换为直方图，并基于其针对每个潜在变量(特征)的密度值来更新概率值。理想情况下，因为我们将密度反转1.0，密度越小，概率越大。换句话说，代表性不足的特征可能会得到更多的概率值。</p><p id="b8dc" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">然后，我们可以使用这个概率值对每个图像进行训练，并在我们的CNN编码器中继续下面的步骤。我们可以看到，在测试阶段，精确度有了相当大的提高。下面的条形图显示了在基于肤色的几个类别中，与我们的标准CNN网络相比，DB-VAE如何提高面部识别的性能。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/ab084a5820cc164adb93b8b5c677f1c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*b1RAepshOg98pYQ8Lcic_w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">正常CNN精确度与DB-VAE精确度的比较</figcaption></figure><p id="36d3" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">结论:</strong></p><p id="fac7" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们已经看到了一种使用这种生成模型的方法，通过调整训练时使用每个图像的概率来提高面部识别的准确性。我们还可以使用该模型，基于输入图像的分布，在解码器之外生成一些输入图像的假样本。生成模型在一个不断发展的领域，这些天有令人印象深刻的用例。期待更多这方面的出版。</p><p id="e6c3" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><strong class="jv hj">参考文献:</strong></p><p id="9dc2" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">麻省理工学院生成模型文件</p></div></div>    
</body>
</html>