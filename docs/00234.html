<html>
<head>
<title>[Paper Summary] Rethinking ImageNet Pre-training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">[论文摘要]反思ImageNet职前培训</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/paper-summary-rethinking-imagenet-pre-training-d661ab7406b7?source=collection_archive---------0-----------------------#2019-01-04">https://medium.com/analytics-vidhya/paper-summary-rethinking-imagenet-pre-training-d661ab7406b7?source=collection_archive---------0-----------------------#2019-01-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="cf4a" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">预训练总是一种优越的训练方法吗？</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/ee47121dabdb8ec61e6a7cc8bd33f52c.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/1*ZMTNci6SljaxsVaTTpqGMg.gif"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">Gif来自这个<a class="ae jj" href="https://giphy.com/gifs/andrea-8lQyyys3SGBoUUxrUp" rel="noopener ugc nofollow" target="_blank">网站</a></figcaption></figure></div><div class="ab cl jk jl gp jm" role="separator"><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp"/></div><div class="hb hc hd he hf"><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="jr js l"/></div></figure></div><div class="ab cl jk jl gp jm" role="separator"><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp"/></div><div class="hb hc hd he hf"><p id="d3ab" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><strong class="jv hj">摘要</strong></p><p id="572a" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">这篇论文的作者发现，即使网络是随机初始化的，最终的性能与预训练(迁移学习)模型没有太大的不同。(特别是来自ImageNet数据的预训练模型。).</p><p id="99a9" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">现在，当我们谈论收敛速度时，预训练模型具有更快的收敛时间。但总的来说，这些发现挑战了“预先训练的模型是最好的方法”这一普遍观念。</p></div><div class="ab cl jk jl gp jm" role="separator"><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp"/></div><div class="hb hc hd he hf"><p id="3436" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><strong class="jv hj">简介</strong></p><p id="5829" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">近年来，越来越多的研究人员使用迁移学习技术，这种技术采用预先训练好的网络，并针对不同的任务进行微调。</p><p id="b3b3" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">本文对上述范式提出了挑战，并表明我们仍然可以从随机初始化方案中获得非常有竞争力的性能。(关键是使用一个归一化方案和更长的训练时间。).</p><ol class=""><li id="e450" class="kp kq hi jv b jw jx jz ka kc kr kg ks kk kt ko ku kv kw kx bi translated">ImageNet预训练加速收敛</li><li id="91ee" class="kp kq hi jv b jw ky jz kz kc la kg lb kk lc ko ku kv kw kx bi translated">ImageNet预训练不会自动给出正则化。</li><li id="b14c" class="kp kq hi jv b jw ky jz kz kc la kg lb kk lc ko ku kv kw kx bi translated">ImageNet预培训并未显示出对某些任务的益处。</li></ol></div><div class="ab cl jk jl gp jm" role="separator"><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp"/></div><div class="hb hc hd he hf"><p id="7c50" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><strong class="jv hj">方法论</strong></p><p id="521c" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">作者使用了标准化方案，如组/同步批处理标准化，并发现这两种方案都能在随机初始化的网络上实现有竞争力的性能。此外，作者增加了从零开始训练的网络的历元数。(训练时间较长)。</p></div><div class="ab cl jk jl gp jm" role="separator"><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp"/></div><div class="hb hc hd he hf"><p id="2a7d" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><strong class="jv hj">结果</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ld"><img src="../Images/1e421b820911a5804b4ffd3539a3a2bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*Lbn4KiaYXFblNFtpQOGxWQ.png"/></div></figure><p id="7a4d" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">上图说明了一切，对于区域建议任务，预训练的网络确实收敛得更快，但经过从头开始训练的网络会及时赶上。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es le"><img src="../Images/4f1679422f7634212059025a1646aa50.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*kNK6pOqgU7sijZnLhrboTA.png"/></div></figure><p id="2e21" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">同样，即使当不同的增强时，网络之间也没有太大的差异。</p></div><div class="ab cl jk jl gp jm" role="separator"><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp"/></div><div class="hb hc hd he hf"><p id="2a20" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><strong class="jv hj">进一步讨论</strong></p><ol class=""><li id="528e" class="kp kq hi jv b jw jx jz ka kc kr kg ks kk kt ko ku kv kw kx bi translated">从头开始训练是可能的，但需要时间</li><li id="9c51" class="kp kq hi jv b jw ky jz kz kc la kg lb kk lc ko ku kv kw kx bi translated">只需要好的规范化方案</li><li id="2032" class="kp kq hi jv b jw ky jz kz kc la kg lb kk lc ko ku kv kw kx bi translated">ImageNet预培训对某些任务很有用。</li></ol><p id="720f" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated">当我们没有足够的数据来完成目标任务时，ImageNet预培训<em class="lf">可能是一个不错的选择。否则，从头开始训练网络可能是个好主意。</em></p></div><div class="ab cl jk jl gp jm" role="separator"><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp jq"/><span class="jn bw bk jo jp"/></div><div class="hb hc hd he hf"><p id="95b3" class="pw-post-body-paragraph jt ju hi jv b jw jx ij jy jz ka im kb kc kd ke kf kg kh ki kj kk kl km kn ko hb bi translated"><strong class="jv hj">参考</strong></p><ol class=""><li id="2cf4" class="kp kq hi jv b jw jx jz ka kc kr kg ks kk kt ko ku kv kw kx bi translated">何，k .，吉希克，r .，&amp;多拉尔，P. (2018)。反思ImageNet预培训。arXiv.org。检索于2019年1月4日，来自<a class="ae jj" href="https://arxiv.org/abs/1811.08883" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1811.08883</a></li></ol></div></div>    
</body>
</html>