<html>
<head>
<title>Principal Component Analysis(PCA) — Dive deep</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析(PCA) —深入研究</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/principal-component-analysis-pca-dive-deep-411db0f9ee10?source=collection_archive---------4-----------------------#2019-10-12">https://medium.com/analytics-vidhya/principal-component-analysis-pca-dive-deep-411db0f9ee10?source=collection_archive---------4-----------------------#2019-10-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="c50f" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">降维的基本术语和直觉。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/9dbdf833781c3ac48c001b8321250499.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vcYw2npFGGqfxJhUpJG_kw.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://pngtree.com/rabbit%E6%8D%B7_8236194?type=2" rel="noopener ugc nofollow" target="_blank">兔子</a><strong class="bd jo">T3<a class="ae jn" href="https://pngtree.com/freebackground/creative-hand-painted-lamp-business-vector-banner-material_270245.html" rel="noopener ugc nofollow" target="_blank">桃树</a>上的照片</strong></figcaption></figure><p id="9041" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">这篇文章的目的是让读者感受到降维技术<strong class="jr hj">主成分分析</strong>背后的几何和数学直觉。所以，我们继续吧。</p><p id="5e58" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">此商品的物流如下:</p><ol class=""><li id="f177" class="kl km hi jr b js jt jv jw jy kn kc ko kg kp kk kq kr ks kt bi translated"><em class="ku">协方差矩阵，</em></li><li id="a7dc" class="kl km hi jr b js kv jv kw jy kx kc ky kg kz kk kq kr ks kt bi translated"><em class="ku">几何解释，</em></li><li id="dd01" class="kl km hi jr b js kv jv kw jy kx kc ky kg kz kk kq kr ks kt bi translated"><em class="ku">目标函数、约束优化问题和</em></li><li id="259e" class="kl km hi jr b js kv jv kw jy kx kc ky kg kz kk kq kr ks kt bi translated"><em class="ku">优化问题的解决方案</em>。</li></ol><p id="f439" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">首先，让我们熟悉一些基本术语，</p><p id="b09f" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated"><a class="ae jn" href="https://en.wikipedia.org/wiki/Variance" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hj">方差</strong> </a> <strong class="jr hj"> : </strong>方差是一个数据集展开程度的度量。它在数学上被定义为一个<a class="ae jn" href="https://www.statisticshowto.datasciencecentral.com/random-variable/" rel="noopener ugc nofollow" target="_blank">随机变量</a> (X)的平均值的方差的平均值。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es la"><img src="../Images/26f57c24389420043da3b0db557b1b50.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/1*NdpQyYlFMze5SYApp7k0Ww.gif"/></div></figure><p id="8e52" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated"><a class="ae jn" href="https://en.wikipedia.org/wiki/Covariance" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hj">协方差</strong> </a> <strong class="jr hj"> : </strong>协方差是两个随机变量之间关系的度量。该指标评估变量一起变化的程度。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lb"><img src="../Images/9ef7dcabc6f6fddc06544191864eb292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/1*3hBNWdXr5l6Bcs3BtxprCw.gif"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lc"><img src="../Images/f5ad880df36208a97eb1f766acb00f45.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/0*Hr7OMkYlcBaRZhdD.gif"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jo">负的CoVar-随着X增加Y减少|零-无关系|正的CoVar-随着X增加Y增加</strong></figcaption></figure><blockquote class="ld le lf"><p id="4c6a" class="jp jq ku jr b js jt ij ju jv jw im jx lg jz ka kb lh kd ke kf li kh ki kj kk hb bi translated"><strong class="jr hj"> <em class="hi"> 1。协方差矩阵(C) : </em> </strong> <em class="hi">以矩阵的形式表示给定数据集(X)的维度之间的协方差。</em></p></blockquote><p id="d7fa" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">例如，考虑一个三维数据集(X ),那么它的协方差矩阵(C)是，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lj"><img src="../Images/7a1553bf8fbe5264fec39433e7c80961.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/0*vzQn0MsH6sMyFLur"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jo">这里，n(=4) — #个数据点和d(=3) — #维度。</strong></figcaption></figure><p id="5651" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">这里，沿对角线的协方差表示每个维度的方差。<br/>当C(i，j) = C(j，I)时，协方差矩阵(C)是对称的。<br/>我们可以把协方差矩阵写成矩阵符号如下，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lk"><img src="../Images/7aff32376d066cc0e36a24f09a485f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/0*xZR8iFZsqI6HFDuU"/></div></figure><p id="e6bb" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">如果像<a class="ae jn" href="https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization)" rel="noopener ugc nofollow" target="_blank">列标准化</a>这样的标准形式应用于数据集，那么所有维度的平均值和方差将为0和1。因此，我们可以将协方差矩阵写成:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ll"><img src="../Images/f55b8da64bfcfb5dc083ef311328de75.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/0*yEhAHWeGri1Ki5fc"/></div></figure><blockquote class="ld le lf"><p id="3b7f" class="jp jq ku jr b js jt ij ju jv jw im jx lg jz ka kb lh kd ke kf li kh ki kj kk hb bi translated"><strong class="jr hj"> <em class="hi">主成分分析:</em> </strong></p></blockquote><p id="f4ab" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">现实世界的数据太杂乱，高度多维，我们执行数据分析，对这些数据进行绘图，以找到其中隐藏的模式，并使用这些模式来训练机器学习模型。但是随着维数的增加，可视化和计算的难度也在增加。</p><p id="be6d" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">因此，我们需要以某种方式减少#维，以便最大限度地保留信息。我们可以想到的一些方法是，<br/> *删除不必要/多余的维度，<br/> *只包含最重要的维度。</p><p id="40c7" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">主成分分析(PCA)就是这样一种最简单的降维技术，它保留了沿称为主轴的每个正交维度的最大方差/扩散。<br/>常用于使数据在低维空间中易于探索和可视化。</p><p id="98b3" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">这些新的正交维度基于方差和它们一起排列，即更重要的主轴首先是具有最大方差/扩展的主轴。</p><blockquote class="ld le lf"><p id="23f3" class="jp jq ku jr b js jt ij ju jv jw im jx lg jz ka kb lh kd ke kf li kh ki kj kk hb bi translated"><strong class="jr hj"> <em class="hi"> 2。几何解释:</em> </strong> <em class="hi">这里我们来通过一个几何解释来说明维度是如何减少的。</em></p></blockquote><p id="db0d" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated"><strong class="jr hj">情况— 1:一个轴上的方差大于其他轴上的方差。<br/> </strong>让我们考虑一个具有两个维度/特征的数据集(X)，比如一个人的身高和体重，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lm"><img src="../Images/a7daf6184dc2ec54a1b62851177af804.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/0*qjo0xmZrd8XVXHXg"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jo"> n — #个数据点，具有d — 2维。</strong></figcaption></figure><p id="be44" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">当我们绘制该数据集的散点图时，我们可以观察到F1(2–8)的方差大于F2(4–6)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ln"><img src="../Images/38103017fa7d75f12f5627e5c0dc351f.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*SBSP8lETCGSv6n9vJNa5SQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">数据的散点图</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lo"><img src="../Images/4e8f88135fc573c4e34a7161c6ce1494.png" data-original-src="https://miro.medium.com/v2/resize:fit:220/0*qQO_lrOz9xINUQKy"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><strong class="bd jo">维数减少的数据集</strong></figcaption></figure><p id="2107" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">因此，我们可以跳过F2，仅将特征F1包括在我们的数据集中，保留具有最大信息的方向。</p><p id="3880" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated"><strong class="jr hj">情况— 2:沿所有轴的方差相等。<br/> </strong>让我们考虑一个数据集，它在两个轴上都有相等的方差，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/4e18688bc643fe2e7eaacc6bb946acf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gTH-pRM7o8oWAvQZs1yQeg.png"/></div></div></figure><p id="71bb" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">沿所有维度的方差(<strong class="jr hj"> F1，F2 </strong>)几乎相同。<br/>所以，现在我们确实找到了新的维度(<strong class="jr hj"> F1 '，F2' </strong>)，即向量，使得方差沿着其中一个最大化，如上图所示，使用轴旋转技术。</p><ol class=""><li id="eb46" class="kl km hi jr b js jt jv jw jy kn kc ko kg kp kk kq kr ks kt bi translated">将数据点X(i)的<a class="ae jn" href="https://en.wikipedia.org/wiki/Vector_projection" rel="noopener ugc nofollow" target="_blank">投影</a>到<strong class="jr hj">Fi’</strong>上，得到<strong class="jr hj"> Fi </strong>维空间在<strong class="jr hj">Fi’</strong>维空间的对应坐标，</li><li id="199a" class="kl km hi jr b js kv jv kw jy kx kc ky kg kz kk kq kr ks kt bi translated">沿<strong class="jr hj">F1’</strong>的方差大于沿<strong class="jr hj">F2’</strong>的方差，</li><li id="0ae8" class="kl km hi jr b js kv jv kw jy kx kc ky kg kz kk kq kr ks kt bi translated">我们可以跳过<strong class="jr hj">F2’</strong>，只包含<strong class="jr hj">F1’</strong>。</li></ol><p id="c9c4" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">—此处<strong class="jr hj">F1’</strong>为第一主轴，<strong class="jr hj">F2’</strong>为第二主轴。</p><p id="7def" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">既然我们已经获得了不同维空间中的最大方差轴，我们可以跳过该空间中方差较小的维，并在相同的维中操作。</p><blockquote class="ld le lf"><p id="7d3d" class="jp jq ku jr b js jt ij ju jv jw im jx lg jz ka kb lh kd ke kf li kh ki kj kk hb bi translated"><strong class="jr hj"> <em class="hi"> 3。目标函数:</em> </strong> <em class="hi">既然我们已经看到了PCA的几何解释，现在我们来看看它的数学视角。</em></p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lq"><img src="../Images/17e2716288981ffab80a28eac6173d75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*ktWpVk6DVj0fGY26Ogsn-A.png"/></div></figure><p id="3f6b" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">考虑前面的几何解释的例子，从上面的<br/>图(a)我们可以看到沿着<strong class="jr hj">F1’</strong>的扩散是最大的，我们只需要最大扩散的那个方向。<br/>所以，我们需要沿着那个方向找一个单位向量(<strong class="jr hj">u1</strong>),<br/>即大小为<strong class="jr hj"> ||u1||=1 </strong>。</p><p id="17d6" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">让原始数据集成为，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lr"><img src="../Images/91eccf0b690213b536c92b9b58f352c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:190/0*BYPA-LoBU_XoSaFn"/></div></figure><p id="5dd4" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">从上图(b)中我们可以看到<strong class="jr hj"> x'(i) </strong>是<strong class="jr hj"> x(i) </strong>在<strong class="jr hj"> u1 </strong>上的投影。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ls"><img src="../Images/3e0821b5c2979b4dac447683d8536d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/0*U3ZA7txqDkj-Bqg9"/></div></figure><p id="02f5" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">现在低维空间中的新投影数据集是，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lt"><img src="../Images/471dd65bee659cdb037761fe5eb88afc.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/0*aus0MwyUkDSZCUXn"/></div></figure><p id="a3d9" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated"><strong class="jr hj">目标:</strong>最后我们要找到<strong class="jr hj">‘u1’</strong>使得<strong class="jr hj"> x(i) </strong>在单位向量(<strong class="jr hj"> u1 </strong>)上的投影方差最大，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lu"><img src="../Images/e5aef8973a4939180725373fecf79fc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/0*g-9knii9lGeEYYb4"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lv"><img src="../Images/cb7ad590409b6cde7fc63c9e3fc01f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*kqCsHurATpcV-C8i.gif"/></div></figure><blockquote class="ld le lf"><p id="efbb" class="jp jq ku jr b js jt ij ju jv jw im jx lg jz ka kb lh kd ke kf li kh ki kj kk hb bi translated">在进行主成分分析之前最好要小心，我们应该始终归一化我们的数据，因为如果我们使用不同尺度的数据(这里的特征),我们将得到误导性的主成分。<br/>典型的标准化形式是 <a class="ae jn" href="https://en.wikipedia.org/wiki/Feature_scaling#Standardization_(Z-score_Normalization)" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hj">列标准化</strong> </a> <strong class="jr hj">。为了简单起见，我假设我们已经标准化了数据。</strong></p></blockquote><p id="543d" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">由于我们已经对数据集(<strong class="jr hj"> D </strong>)特征应用了列标准化，因此每个特征的平均向量值将为零。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lw"><img src="../Images/68417f1b7c9b936e5e76d2fd7762bd74.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/0*JbjYjKcK3zzjHBez"/></div></div></figure><p id="b6e0" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">因此，用于寻找最大扩展主成分的数学目标函数(<strong class="jr hj"> ui的</strong>)可以写成:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lx"><img src="../Images/36fc53bf45065d04911e3d3c83f67b3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/0*dYy2_GAYBcGDadEX"/></div></figure><p id="a3d3" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">我们可以用矩阵符号写上面的优化问题，使用我们在开始时学过的<br/>协方差矩阵(<strong class="jr hj"> C </strong>),</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/973cebb96f3b05ac0a90ad21cd840581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/0*4AR0V9IJboTKbYIe"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lz"><img src="../Images/c14007929b62052fee11cdeb2c09f36a.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/0*wfBp_mhN5VCAYeuo"/></div></figure><p id="22b0" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">上述类型的问题被称为<a class="ae jn" href="https://en.wikipedia.org/wiki/Constrained_optimization#General_form" rel="noopener ugc nofollow" target="_blank">约束优化</a>问题，其中每个主成分(<strong class="jr hj"> ui </strong>)的约束是单位向量。</p><p id="9732" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">由于分量彼此正交，任何两个分量之间的<a class="ae jn" href="https://en.wikipedia.org/wiki/Dot_product#Properties" rel="noopener ugc nofollow" target="_blank">点积</a>为零。</p><blockquote class="ld le lf"><p id="858e" class="jp jq ku jr b js jt ij ju jv jw im jx lg jz ka kb lh kd ke kf li kh ki kj kk hb bi translated"><strong class="jr hj"> <em class="hi"> 4。求解约束优化问题:</em> </strong></p></blockquote><p id="dc31" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">既然我们已经讨论了优化函数的数学目标，现在我们将讨论它的求解部分。</p><p id="8824" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">我们可以用我们老的友好技术<a class="ae jn" href="https://en.wikipedia.org/wiki/Lagrange_multiplier" rel="noopener ugc nofollow" target="_blank">拉格朗日乘子</a>来解决这个约束优化问题，并把它变成拉格朗日形式，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ma"><img src="../Images/47fd5515ae2bc3e957f6326c127c5cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/0*8P0XrYusFt_mVm2t"/></div></figure><p id="68d4" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">上面的方程(<strong class="jr hj"> 1 </strong>)是<a class="ae jn" href="https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors" rel="noopener ugc nofollow" target="_blank">特征值&amp;特征向量</a>的定义，其中<strong class="jr hj">‘u1’</strong>是特征向量，<strong class="jr hj">‘λ’</strong>是<strong class="jr hj">‘C’</strong>对应的特征值，即<strong class="jr hj">‘X’的协方差矩阵。<br/> </strong>同样，我们可以求出协方差矩阵<strong class="jr hj">‘C’</strong>(<strong class="jr hj">d</strong>)的其他特征向量(<strong class="jr hj"> ui </strong>)和特征值(<strong class="jr hj"> λi </strong>)。因此，主成分分析得出特征值和特征向量。</p><p id="e4f3" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated"><strong class="jr hj">属性:</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mb"><img src="../Images/2014be7a7cf3b4b339b1f7e30fb3b3c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/0*uBXVWSNQYYWxm1Fv"/></div></figure><p id="6c1a" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">现在，选择#维，使得在其他维空间中捕获最大方差是执行PCA的关键。这可以通过我们在求解优化问题时得到的特征值(<strong class="jr hj"> λi </strong>)来实现。<br/>从这个特征值我们可以生成累积方差比率为，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mc"><img src="../Images/92413a3e18cfdc3ca2c78504506a5156.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/0*-QK6JYbCrtJAyIw_"/></div></figure><p id="d07d" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated"><strong class="jr hj">方差比</strong>告诉我们不同#分量在新维度空间中保留的信息百分比。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es md"><img src="../Images/b4ce8ccc082105191311729e82fe0e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*41EDwBc8-yCzKK0atLrcsQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">用不同的#维数和相应的方差比为MNIST数据集绘图。</figcaption></figure><p id="9345" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">—这是通过解决我们在上一节中看到的约束优化问题获得的不同#维的典型PCA累积方差比图。<br/> —我们可以根据新维度空间中要保留的最大或所需信息量来选择# dimensions。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es me"><img src="../Images/a37e49ab4e4ba7ab891436a4214036e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AEEZUn-Kne0iCW4y.gif"/></div></div></figure><p id="3ce5" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">一旦我们选择了#维，我们现在可以将原始数据集(<strong class="jr hj"> D </strong>)点投影到这些顶部#特征向量上，并获得具有减少的维的新数据集(<strong class="jr hj">D’</strong>)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mf"><img src="../Images/66339404fe14beba5829aae34239609d.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/0*DAMw7Ro1SGmYa0SZ"/></div></figure><p id="c7f0" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">通常，PCA用于在低维空间(<strong class="jr hj"> 2D </strong>)中可视化高维数据(<strong class="jr hj">和</strong>)，以便我们了解数据点在高维空间中是如何分布的。</p><blockquote class="ld le lf"><p id="a167" class="jp jq ku jr b js jt ij ju jv jw im jx lg jz ka kb lh kd ke kf li kh ki kj kk hb bi translated"><strong class="jr hj"> <em class="hi">局限性:</em> </strong></p></blockquote><ol class=""><li id="b9cd" class="kl km hi jr b js jt jv jw jy kn kc ko kg kp kk kq kr ks kt bi translated"><strong class="jr hj">线性</strong> : PCA假设主成分是原始特征的线性组合。如果这个假设不成立，那么它会给我们误导性的结果。</li><li id="c646" class="kl km hi jr b js kv jv kw jy kx kc ky kg kz kk kq kr ks kt bi translated"><strong class="jr hj">较大的方差意味着更多的结构:</strong> PCA <strong class="jr hj"> </strong>通过将方差作为特定维度重要性的度量，试图保留数据的全局结构而不是局部结构。如下图'<strong class="jr hj"> A </strong>'所示，当数据没有适当展开时，有时可能会导致一些信息丢失。</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mg"><img src="../Images/7a3c232fa6272a718d34bf58a45410c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*AYweHvYyhytzg_HICISewA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://arxiv.org/pdf/1404.1100.pdf" rel="noopener ugc nofollow" target="_blank">主成分分析教程</a></figcaption></figure><p id="ac6f" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">红色箭头表示主轴，它没有按照假设获取完整的方差。</p><p id="1a2d" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">3.<strong class="jr hj">正交性:</strong> PCA也是假设主成分互相正交。从上面的图'<strong class="jr hj"> B </strong> ' <strong class="jr hj"> </strong>可以看出，由于主轴是正交的，所以有一些信息丢失。</p><p id="af98" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">由于PCA是一种相当古老的技术，当上述假设失败时，有相当多的工作在改进它。根据不同的用例，我们可能想要使用<a class="ae jn" href="https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/" rel="noopener ugc nofollow" target="_blank">中的一种更高级的技术</a>。</p><blockquote class="ld le lf"><p id="afdb" class="jp jq ku jr b js jt ij ju jv jw im jx lg jz ka kb lh kd ke kf li kh ki kj kk hb bi translated"><strong class="jr hj"> <em class="hi">结论</em> : </strong></p></blockquote><p id="7507" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated"><strong class="jr hj">主成分分析</strong> ( <strong class="jr hj"> PCA </strong> ) <strong class="jr hj"> </strong>是一种<strong class="jr hj"> <em class="ku">无监督</em> </strong>技术，用于预处理和降低高维数据的维度，同时保留原始数据集固有的原始结构和关系，以便机器学习模型仍然可以从中学习并用于做出准确的预测。</p><p id="51a1" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">非常好看的直观解释，也是我最喜欢的，可以去翻翻，<br/> <a class="ae jn" href="http://setosa.io/ev/principal-component-analysis/" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hj">主成分分析</strong> ( <strong class="jr hj"> PCA </strong> ) </a>。</p><p id="9214" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">这使我们结束了关于基本降维技术背后的直觉的讨论，<strong class="jr hj">主成分分析</strong> ( <strong class="jr hj"> PCA </strong>)。<br/>如果你发现帖子中有什么错误或者有什么补充的话就在评论里讨论吧:P <strong class="jr hj">。</strong></p><p id="c652" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">参考资料:<br/>【1】<a class="ae jn" href="https://www.appliedaicourse.com" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com</a><br/>【2】<a class="ae jn" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Principal_component_analysis</a><br/>【3】<a class="ae jn" href="https://www.statisticshowto.datasciencecentral.com/" rel="noopener ugc nofollow" target="_blank">https://www.statisticshowto.datasciencecentral.com/</a><br/>【4】<a class="ae jn" href="http://setosa.io/ev/eigenvectors-and-eigenvalues/" rel="noopener ugc nofollow" target="_blank">http://setosa.io/ev/eigenvectors-and-eigenvalues/</a><br/>【5】<a class="ae jn" href="https://arxiv.org/pdf/1404.1100.pdf?utm_campaign=buffer&amp;utm_content=bufferb37df&amp;utm_medium=social&amp;utm_source=facebook.com" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1404.1100.pdf</a></p><p id="9c46" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated"><em class="ku">你也可以在</em><strong class="jr hj"><em class="ku"/></strong><a class="ae jn" href="https://www.linkedin.com/in/sanjayulsha/" rel="noopener ugc nofollow" target="_blank"><strong class="jr hj"><em class="ku">LinkedIn</em></strong></a><strong class="jr hj"><em class="ku"/></strong><em class="ku">和</em><a class="ae jn" href="https://github.com/sanjay235" rel="noopener ugc nofollow" target="_blank"><strong class="jr hj"><em class="ku">GitHub</em></strong></a><em class="ku">上找到并与我联系。</em></p><blockquote class="ld le lf"><p id="049b" class="jp jq ku jr b js jt ij ju jv jw im jx lg jz ka kb lh kd ke kf li kh ki kj kk hb bi translated"><strong class="jr hj"> <em class="hi">看看我以前的文章:<br/> </em> </strong> <em class="hi">给他们看一下，非常感谢你的反馈。</em></p></blockquote><p id="242b" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">[1]<a class="ae jn" rel="noopener" href="/p/sketch-to-color-anime-translation-using-generative-adversarial-networks-gans-8f4f69594aeb?source=email-df6131035697--writer.postDistributed&amp;sk=15df1b03e8ffc60eaa82bb34114c242a"><strong class="jr hj">【sketch 2 color anime翻译使用生成对抗网络(GANs) </strong> </a> <strong class="jr hj">。<br/></strong>【2】关于<a class="ae jn" href="https://sanjay235.github.io/StochasticGradientDescent" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hj">随机梯度下降</strong> </a> <strong class="jr hj">的一种数学直觉。</strong></p></div></div>    
</body>
</html>