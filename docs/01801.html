<html>
<head>
<title>Reinforcement learning with Skinner</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">斯金纳的强化学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-with-skinner-deacef0a281a?source=collection_archive---------12-----------------------#2019-11-14">https://medium.com/analytics-vidhya/reinforcement-learning-with-skinner-deacef0a281a?source=collection_archive---------12-----------------------#2019-11-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/550f3afe7c0653e7ef8cdfcccbf15fe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VuKODkpO0lgN__9C"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">姆拉登·米利诺维奇在<a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/><div class=""><h2 id="60f7" class="pw-subtitle-paragraph iv hx hy bd b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm dx translated">用神经科学的例子友好地介绍强化学习的问题</h2></div><p id="434e" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">强化学习最近因诸如<a class="ae hv" href="https://www.nature.com/articles/nature16961" rel="noopener ugc nofollow" target="_blank"> AlphaGo </a>的成就而进入聚光灯下，并且被认为是我们在人工一般智能方面最好的尝试之一——或者至少是更一般的智能。在这篇文章中，我将追溯它的一些历史，追溯到斯金纳对<strong class="jp hz">操作性条件反射</strong>的研究。</p><blockquote class="kk"><p id="9233" class="kl km hy bd kn ko kp kq kr ks kt ki dx translated">真正的问题不是机器是否思考，而是人类是否思考——b . f .斯金纳</p></blockquote><figure class="kw kx ky kz la hk er es paragraph-image"><div class="er es kv"><img src="../Images/8873b45ec128e5abcd63704a4182ca30.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*Jut7WQXNT3l_uO7khw6IvQ.jpeg"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">B.f .斯金纳与他创造的操作性条件反射室一起工作。图片取自<a class="ae hv" href="https://www.diarioliberdade.org/mundo/361-linguaeducacom/50965-o-modelo-did%C3%A1tico-do-ensino-programado,-segundo-b-f-skinner.html" rel="noopener ugc nofollow" target="_blank">此处</a>。</figcaption></figure><p id="b6ad" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">斯金纳想了解动物如何发展适应行为，学习背后的规则是什么。那个时候，许多其他科学家对条件反射感兴趣，比如伊凡·巴甫洛夫——他因证明在给一个铃铛和一个牛肉配对后，狗对铃声垂涎三尺而闻名。</p><p id="e945" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">斯金纳和他的同行之间的主要区别是他在做非常受控的实验时的彻底性。斯金纳开发了一个小室——现在称为<em class="kj">操作性条件反射室，</em>或简称为<em class="kj">斯金纳箱——</em>，在这个小室里，动物，通常是老鼠和鸽子，可以与外界的声音、气味和光线隔离，并在每次实验中受到精确的刺激。马文·明斯基开玩笑地将斯金纳的一丝不苟与巴甫洛夫在一个满是笼子里的狗的实验室里进行的实验相比较，在照顾和控制方面相去甚远。</p><figure class="lc ld le lf fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lb"><img src="../Images/2703821fb55f5fca70f4d09c5d81b9b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mzu_ukMZ6Z65ZH0MCn-l9Q.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">老鼠在调节室里。有两盏灯可以用来刺激动物，还有两个杠杆动物可以用来做出反应。蔗糖溶液由自动化系统控制。图片来自Malkki等人，2010年</figcaption></figure><h2 id="a63f" class="lg lh hy bd li lj lk ll lm ln lo lp lq jw lr ls lt ka lu lv lw ke lx ly lz ma bi translated">奖励和重复</h2><p id="22c6" class="pw-post-body-paragraph jn jo hy jp b jq mb iz js jt mc jc jv jw md jy jz ka me kc kd ke mf kg kh ki hb bi translated">动物将接受特定的刺激，如光、声音或气味，来自刺激的信息可用于获得一些食物或水(一种<em class="kj">强化物</em>)。但是老鼠需要执行一些特定的行动来得到强化物的奖励，在一小组可能采取的行动中做出正确的选择。可能会有一个歧视性的任务，其中一个灯会亮，如果灯是绿色的，动物会因为按下下面的控制杆而得到奖励。另一方面，如果灯是红色的，动物会因为按下对侧控制杆而得到奖励。</p><figure class="lc ld le lf fd hk er es paragraph-image"><div class="er es mg"><img src="../Images/3dc911212dcff30e71ffb72d0235ea3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*05sQ3H-byUXFp985odTLQw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">操作性条件反射任务的t型迷宫。图片来自史密斯、凯尔·s和安·m·格雷贝尔，2013年</figcaption></figure><p id="768c" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">经过一些尝试和错误之后，动物们开始以这样一种方式来增加它们的奖励率，就好像理解了指导它们奖励的规则，<strong class="jp hz">就好像</strong>理解了红色意味着“另一根杠杆”。此外，如果动物得到更高的奖励，它们(通常)会学得更快。受<em class="kj">斯金纳盒子的启发，大量受控空间被创造出来。</em>以这里显示的<em class="kj"> T型迷宫</em>为例，有一个起始位置和一个决策点。根据播放的声音(<em class="kj">音调提示</em>),左臂或右臂将包含其各自的<em class="kj">增强器</em>。动物最终学会了根据声音去右臂，增加了它的奖励率。</p><blockquote class="kk"><p id="5fb1" class="kl km hy bd kn ko kp kq kr ks kt ki dx translated">在特定情况下产生令人满意效果的反应在该情况下更有可能再次出现，而产生令人不安效果的反应在该情况下不太可能再次出现——桑代克效果定律</p></blockquote><p id="9aae" class="pw-post-body-paragraph jn jo hy jp b jq mh iz js jt mi jc jv jw mj jy jz ka mk kc kd ke ml kg kh ki hb bi translated">对<em class="kj">操作性条件作用</em>的研究仍然非常活跃，有许多发展中的分支，如习惯化的动力学，例如，一种行为失去灵活性需要多少训练——变得对贬值有抵抗力——以及涉及哪些潜在过程。这篇文章没有过多地涉及我们大脑可能使用的算法，而是更侧重于描述这个问题。研究人工智能的人特别感兴趣的是顺序任务，其中在获得回报之前需要采取许多行动。</p><h2 id="fe9f" class="lg lh hy bd li lj lk ll lm ln lo lp lq jw lr ls lt ka lu lv lw ke lx ly lz ma bi translated">顺序任务</h2><p id="b6c2" class="pw-post-body-paragraph jn jo hy jp b jq mb iz js jt mc jc jv jw md jy jz ka me kc kd ke mf kg kh ki hb bi translated">顺序任务的一个很好的例子是迷宫。还有许多其他情况，其中每一步的偶然性都取决于前一步。但是在迷宫中，顺序方面是空间分布的，所以它尽可能的清晰。想象动物跑着探索迷宫，直到它找到奖励(并被移出迷宫重新开始)。严格遵循效果法则，动物们会尝试在迷宫周围重复同样的准随机罐子，直到它们以同样的方式再次找到奖励，但这显然是低效的。事实上，从实验中可以清楚地看出，动物在训练中变得更有效率，直到试验中它们不犯任何“错误”直接获得奖励。</p><figure class="lc ld le lf fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mm"><img src="../Images/dda0219ed182d96422c13752303ea4bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4k56byIu7VYy_mIJy-Nsrw.png"/></div></div></figure><p id="398a" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">动物要应对的问题是<em class="kj">信用分配</em>问题，即如何强化那些真正有助于带来回报的行为，而不强化那些恰好在接近回报时发生的行为？事实上，有许多记录在案的例子，鸽子和猫在按下杠杆之前会重复做一些完全不必要的动作(如Guthrie 1946)，在这些例子中，信用分配显然不是最优的。这是一个大问题，这个方向上的每一个进步都是对我们创造的强化学习系统的潜在巨大改进。</p><p id="a522" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">明确地说，这不是一个边缘问题:这是强化学习解决的核心问题。在这种情况下，行动与回报相去甚远，甚至连“完美的回应”都没有很好的定义。与监督学习相比，监督学习在每一步都指定并显示“正确”的答案。额外的困难正是强化学习如此广泛的原因，也是我们对人工智能的最佳尝试。</p><h2 id="7c1c" class="lg lh hy bd li lj lk ll lm ln lo lp lq jw lr ls lt ka lu lv lw ke lx ly lz ma bi translated">强化学习形式主义——一个草图</h2><p id="0970" class="pw-post-body-paragraph jn jo hy jp b jq mb iz js jt mc jc jv jw md jy jz ka me kc kd ke mf kg kh ki hb bi translated">RL类似于剥皮盒。一个<strong class="jp hz">代理</strong>可以访问指定的一组<strong class="jp hz">状态</strong>中的一个状态(在前面的例子中，这可能是特定的左绿灯亮了)，并且可以选择一些<strong class="jp hz">动作</strong>(按下左杆、右杆、不按下……)。然后，在环境中行动之后，<strong class="jp hz">代理</strong>接收到<strong class="jp hz">奖励</strong>(例如，食物，什么都没有……)，并感知自己处于新的<strong class="jp hz">状态</strong>。</p><blockquote class="kk"><p id="d7e3" class="kl km hy bd kn ko kp kq kr ks kt ki dx translated">学习就是:提高报酬率</p></blockquote><figure class="kw kx ky kz la hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mn"><img src="../Images/3ab09a0a2bc693f5e87398cf5f46c8d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ecjiU3otsee_DmQLTPhvlw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图片来自萨顿&amp;巴尔托事务所，2018年</figcaption></figure><p id="bbeb" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">为了增加任务中的奖励数量，代理人必须考虑“在每个状态下哪一个是最佳行动”。这就产生了一个<strong class="jp hz">最优策略</strong>——一个决定行动的程序——<strong class="jp hz">，</strong>达到最大期望报酬率。当满足以下两个标准中的任何一个时,<strong class="jp hz">最优策略</strong>的存在在数学上是明确定义的:</p><ol class=""><li id="cc51" class="mo mp hy jp b jq jr jt ju jw mq ka mr ke ms ki mt mu mv mw bi translated">任务是有限的，或者</li><li id="56e2" class="mo mp hy jp b jq mx jt my jw mz ka na ke nb ki mt mu mv mw bi translated">未来晚些时候的奖励没有更接近现在的奖励有价值(有一个<em class="kj">贴现率</em>)。</li></ol><p id="3f60" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在操作性条件反射的情况下，任务显然是有限的，但这并不意味着缺乏折扣。事实上，有一个非常当代的讨论围绕着<em class="kj">延迟贴现</em>及其对人类生活的影响，例如，它与药物滥用的关系(比克尔和马希，2001)。</p><p id="8cf6" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">另一方面，对于执行连续任务的人工智能体训练(像玩《我的世界》，它不是有限的)，重要的是要有一个小的折扣因子，以确保存在一个最优策略供智能体学习。</p><h2 id="14d0" class="lg lh hy bd li lj lk ll lm ln lo lp lq jw lr ls lt ka lu lv lw ke lx ly lz ma bi translated">使用该算法</h2><p id="3328" class="pw-post-body-paragraph jn jo hy jp b jq mb iz js jt mc jc jv jw md jy jz ka me kc kd ke mf kg kh ki hb bi translated">好的，所以我们不能结束强化学习的介绍，而不去探究一下让它在计算机中成为可能的方程式。我将绕过形式主义，而是给出一个小而直观的算法推导，该算法可用于寻找最优策略。我这样做是为了说明我们从理论到算法有多快。在进入图像之前，我们只需要对行动的价值建立一个小小的直觉:</p><ol class=""><li id="5724" class="mo mp hy jp b jq jr jt ju jw mq ka mr ke ms ki mt mu mv mw bi translated">最佳行动是使未来预期报酬最大化的行动。</li><li id="33f4" class="mo mp hy jp b jq mx jt my jw mz ka na ke nb ki mt mu mv mw bi translated">如果我们知道采取每一个行动的未来预期回报，那么我们总是可以选择最好的行动。</li><li id="e5c6" class="mo mp hy jp b jq mx jt my jw mz ka na ke nb ki mt mu mv mw bi translated">如果我们总能知道最佳行动，那么我们就达到了最优政策。</li></ol><p id="2b5b" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里的想法是找到这个<em class="kj">值</em>函数，它输出在一个状态中采取行动的预期返回值。你会看到我们从(1)中的价值函数定义开始，到(5)中的算法结束。</p><figure class="lc ld le lf fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nc"><img src="../Images/10d160b3cd56a4dc37b5cfaf529070e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YAHhTFNfgqW4zHOlnBmy0Q.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">一些简化的SARSA算法。最后一个等式可以在与环境交互的循环中在线使用。这里贴现率设为1，为简单起见，Q函数称为V。</figcaption></figure><p id="40a3" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">有了这个算法，就有可能通过与环境的交互来迭代学习。在每一步中，代理观察其状态<em class="kj"> s </em>并采取动作<em class="kj"> a </em>，根据收到的奖励和下一个状态-动作对更新其<em class="kj">值</em>。还记得迷宫问题吗？因为它有一组离散和有限的状态(决策点，分叉)和动作(例如向左，向右)，我们可以使用类似下面的<em class="kj"> Q表</em>来解决它。</p><figure class="lc ld le lf fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nd"><img src="../Images/627394effd05abee1c115bebaf699743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k5ymVs4ta_tpiUhkBZtSqQ.png"/></div></div></figure><p id="6119" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">使用此<em class="kj"> Q-table </em>，代理将在第三个分叉处左转，因为动作1的值最高。该表将在每一步进行更新，最终收敛到一个最优策略。在这里，我展示了一个非常简单的代理，它使用这个算法学习如何握住杆子！</p><figure class="lc ld le lf fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ne"><img src="../Images/45404b23380b5d1f084471c345d2b288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*uiubXtg3nXEKW3275csbjw.gif"/></div></div></figure><p id="03a3" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">你可以看到，它需要大量的重复来执行可以接受的，但这是因为有很多改进可以为这个设置。</p><figure class="lc ld le lf fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nf"><img src="../Images/988fa3b4702886b278072da7da82dcb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LPz_MAoaKeP9MpF6HqnS1w.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">离散化CartPole的截断Q表。对于每个维度的20个箱的不太精细的离散化，存在20⁴ = 160，000个状态的庞大总数。在这些情况下，函数逼近器是可行的方法。</figcaption></figure><p id="3102" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">因为状态是连续的，我们可以在离散化的基础上改进很多，使用连续函数逼近器，比如神经网络！尽管如此，问题的表述仍然是一样的，介绍这个问题是这篇博文的中心目的。</p><h2 id="b311" class="lg lh hy bd li lj lk ll lm ln lo lp lq jw lr ls lt ka lu lv lw ke lx ly lz ma bi translated">结论</h2><p id="46ff" class="pw-post-body-paragraph jn jo hy jp b jq mb iz js jt mc jc jv jw md jy jz ka me kc kd ke mf kg kh ki hb bi translated">这篇文章是对强化学习和操作性条件反射的非常简短的介绍，我打算写后续文章，更深入地研究这两者背后的理论和数学，展示越来越好和越来越复杂的算法，并将它们与神经科学联系起来。</p><p id="93c4" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">希望能让你和我一样对强化学习感兴趣！我相信(正如许多人一样)神经科学可以为人工智能领域提供很多东西，尤其是高水平的洞察力。请评论反馈，感谢阅读！</p><h2 id="2f25" class="lg lh hy bd li lj lk ll lm ln lo lp lq jw lr ls lt ka lu lv lw ke lx ly lz ma bi translated">进一步阅读</h2><p id="2c89" class="pw-post-body-paragraph jn jo hy jp b jq mb iz js jt mc jc jv jw md jy jz ka me kc kd ke mf kg kh ki hb bi translated"><a class="ae hv" href="https://neptune.ai/blog/best-reinforcement-learning-tutorials-examples-projects-and-courses" rel="noopener ugc nofollow" target="_blank">由<em class="kj"> neptune.ai </em>撰写的这篇文章</a>有很多有趣的资源，可以深入到强化学习领域，从教程到完整的课程。</p><p id="0d89" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如果你想了解更多关于神经科学的知识，试试medium上的尖峰信号。</p><h2 id="4931" class="lg lh hy bd li lj lk ll lm ln lo lp lq jw lr ls lt ka lu lv lw ke lx ly lz ma bi translated">参考</h2><p id="457c" class="pw-post-body-paragraph jn jo hy jp b jq mb iz js jt mc jc jv jw md jy jz ka me kc kd ke mf kg kh ki hb bi translated">比克尔，W. K .，，马希，洛杉矶(2001年)。对药物依赖的行为经济学理解:延迟贴现过程。<em class="kj">瘾</em>，<em class="kj"> 96 </em> (1)，73–86。</p><p id="08b6" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">Dam，g .，Kording，k .，，Wei，K. (2013)。运动强化学习中的学分分配。<em class="kj"> PLoS One </em>，<em class="kj"> 8 </em> (2)，e55352。</p><p id="29ab" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">格思里，E. R .，&amp;霍顿，G. P. (1946)。拼图盒里的猫。</p><p id="f4fc" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">马尔基，H. A .，东加，L. A .，德格鲁特，S. E .，巴塔格利亚，F. P .，&amp;彭纳茨，C. M. (2010)。小鼠的食欲操作条件反射:训练阶段的遗传性和分离性。<em class="kj">行为神经科学前沿</em>，<em class="kj"> 4 </em>，171</p><p id="8334" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">K. S .史密斯和A. M .格雷贝尔(2013年)。反映皮层和纹状体动力学的习惯性行为的双重操作者观点。<em class="kj">神经元</em>，<em class="kj"> 79 </em> (2)，361–374。</p><p id="4b94" class="pw-post-body-paragraph jn jo hy jp b jq jr iz js jt ju jc jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">萨顿和巴尔托(2018年)。强化学习:简介。麻省理工出版社。</p></div></div>    
</body>
</html>