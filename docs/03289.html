<html>
<head>
<title>High School Math Performance Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高中数学成绩预测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/high-school-math-performance-prediction-a10a990be886?source=collection_archive---------8-----------------------#2020-01-24">https://medium.com/analytics-vidhya/high-school-math-performance-prediction-a10a990be886?source=collection_archive---------8-----------------------#2020-01-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/bd30c602f8375efc723d6f4fe6d3da04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LDE-6V62-0tGMLZc"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">安托万·道特里在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="8817" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">简介</strong></p><p id="62f9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在数学教育领域，大学和教育工作者面临的一个主要问题是，学生在数学方面没有达到令人满意的水平，也没有以令人满意的速度取得成功。大学和教育工作者抱怨学生的高失败率、退学率和退学率。这对学生来说是一个问题，因为数学成绩差阻碍了他们追求学位和职业。这对大学和教育工作者来说是一个问题，因为这意味着大学或教育工作者没有成功地教授学生，没有留住学生，没有满足学生的需求——这些问题损害了大学和教育工作者的盈利能力和吸引力。</p><p id="16d6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们能够获得一些关于哪些因素最有助于或损害学生数学成绩的见解，我们就有可能解决上述问题。如果我们能够产生预测模型，预测学生是否会通过或失败，预测学生在数学评估中的数字分数，预测学生的整体实力和前途，那么大学和教育工作者将能够使用这些模型更好地将学生置于适当的能力水平，更好地选择录取学生，并更好地了解可以改善的因素，以帮助学生取得成功。</p><p id="7205" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们将对代表两所葡萄牙高中学生数学成绩的数据集进行数据科学和机器学习。</p><p id="8a01" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在之前的一篇文章中，可以在<a class="ae iu" rel="noopener" href="/analytics-vidhya/high-school-math-performance-523d5839d7d7">高中数学成绩回归</a>中找到，我对数据集应用了回归方法来预测G3的值。在本文中，我想将G3分数分为五个等级，并尝试根据学生的G3分数将他们归入五个等级中的一个。这就变成了一个5类分类问题，我们可以把机器学习的分类方法应用到这个问题上。</p><p id="5a11" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">数据准备</strong></p><p id="763a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据文件用分号而不是逗号分隔。我用逗号代替了分号。然后，将所有内容复制并粘贴到记事本中。然后，使用以下链接中的步骤转换为csv文件:</p><p id="4485" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://knowledgebase.constantcontact.com/articles/KnowledgeBase/6269-convert-a-text-file-to-an-excel-file?lang=en_US" rel="noopener ugc nofollow" target="_blank">https://knowledge base . constant contact . com/articles/knowledge base/6269-convert-a-text-file-to-a-excel-file？lang=en_US </a></p><p id="a326" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我有一个很好的csv文件。</p><p id="0416" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">共有30个属性，包括学生年龄、父母的教育程度、父母的工作、每周学习时间、缺席次数、以往课程失败次数等。有一年级、二年级和三年级；这些由G1、G2和G3表示。分数从0到20不等。G1和G2可以用作输入特征，而G3将是主要的目标输出。</p><p id="ed6e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一些属性是顺序的，一些是二进制的是-否，一些是数字的，一些是名义的。我们确实需要做一些数据预处理。对于二进制的是-否属性，我将使用0和1对它们进行编码。属性famrel、freetime、goout、Dalc、Walc和health是有序的；这些值的范围从1到5。属性Medu、Fedu、traveltime、studytime、failures也是有序的；值的范围从0到4或1到4。属性“缺席”是一个计数属性；值的范围从0到93。属性sex、school、address、Pstatus、Mjob、Fjob、guardian、famsize、reason是名义上的。对于名义属性，我们可以使用一键编码。年龄、G1、G2和G3属性可视为区间属性。</p><p id="7bf4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我一次性编码了每个名义属性，一次一个。我每次都将数据帧导出为csv文件，并在导出过程中重新标记列。最后，我对列进行了重新排序。</p><p id="53ea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是python代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="6080" class="kc kd hi jy b fi ke kf l kg kh">import numpy as np</span><span id="ff37" class="kc kd hi jy b fi ki kf l kg kh">import pandas as pd</span><span id="4d7d" class="kc kd hi jy b fi ki kf l kg kh">dataset = pd.read_csv(‘C:\\Users\\ricky\\Downloads\\studentmath.csv’)</span><span id="1a7f" class="kc kd hi jy b fi ki kf l kg kh">X = dataset.iloc[:,:-1].values</span><span id="d00f" class="kc kd hi jy b fi ki kf l kg kh">Y = dataset.iloc[:,32].values</span><span id="c6ae" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.preprocessing import LabelEncoder, OneHotEncoder</span><span id="e865" class="kc kd hi jy b fi ki kf l kg kh">labelencoder_X = LabelEncoder()</span><span id="44f0" class="kc kd hi jy b fi ki kf l kg kh"># Encoding binary yes-no attributes</span><span id="597b" class="kc kd hi jy b fi ki kf l kg kh">X[:,15] = labelencoder_X.fit_transform(X[:,15])</span><span id="9407" class="kc kd hi jy b fi ki kf l kg kh">X[:,16] = labelencoder_X.fit_transform(X[:,16])</span><span id="7a53" class="kc kd hi jy b fi ki kf l kg kh">X[:,17] = labelencoder_X.fit_transform(X[:,17])</span><span id="f2b7" class="kc kd hi jy b fi ki kf l kg kh">X[:,18] = labelencoder_X.fit_transform(X[:,18])</span><span id="18b8" class="kc kd hi jy b fi ki kf l kg kh">X[:,19] = labelencoder_X.fit_transform(X[:,19])</span><span id="229c" class="kc kd hi jy b fi ki kf l kg kh">X[:,20] = labelencoder_X.fit_transform(X[:,20])</span><span id="cf88" class="kc kd hi jy b fi ki kf l kg kh">X[:,21] = labelencoder_X.fit_transform(X[:,21])</span><span id="d71b" class="kc kd hi jy b fi ki kf l kg kh">X[:,22] = labelencoder_X.fit_transform(X[:,22])</span><span id="f5f8" class="kc kd hi jy b fi ki kf l kg kh"># Encoding nominal attributes</span><span id="6d9c" class="kc kd hi jy b fi ki kf l kg kh">X[:,0] = labelencoder_X.fit_transform(X[:,0])</span><span id="1961" class="kc kd hi jy b fi ki kf l kg kh">X[:,1] = labelencoder_X.fit_transform(X[:,1])</span><span id="60fa" class="kc kd hi jy b fi ki kf l kg kh">X[:,3] = labelencoder_X.fit_transform(X[:,3])</span><span id="c4cf" class="kc kd hi jy b fi ki kf l kg kh">X[:,4] = labelencoder_X.fit_transform(X[:,4])</span><span id="16f4" class="kc kd hi jy b fi ki kf l kg kh">X[:,5] = labelencoder_X.fit_transform(X[:,5])</span><span id="551a" class="kc kd hi jy b fi ki kf l kg kh">X[:,8] = labelencoder_X.fit_transform(X[:,8])</span><span id="b54b" class="kc kd hi jy b fi ki kf l kg kh">X[:,9] = labelencoder_X.fit_transform(X[:,9])</span><span id="e089" class="kc kd hi jy b fi ki kf l kg kh">X[:,10] = labelencoder_X.fit_transform(X[:,10])</span><span id="a66e" class="kc kd hi jy b fi ki kf l kg kh">X[:,11] = labelencoder_X.fit_transform(X[:,11])</span><span id="d08b" class="kc kd hi jy b fi ki kf l kg kh">onehotencoder = OneHotEncoder(categorical_features = [0])</span><span id="4d84" class="kc kd hi jy b fi ki kf l kg kh">X = onehotencoder.fit_transform(X).toarray()</span><span id="672c" class="kc kd hi jy b fi ki kf l kg kh">from pandas import DataFrame</span><span id="c52f" class="kc kd hi jy b fi ki kf l kg kh">df = DataFrame(X)</span><span id="1617" class="kc kd hi jy b fi ki kf l kg kh">export_csv = df.to_csv (r’C:\Users\Ricky\Downloads\highschoolmath.csv’, index = None, header=True)</span></pre><p id="4bc3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此时，数据集的最后一列由G3的整数组成。分数16-20将形成等级1，分数14-15将形成等级2，分数12-13将形成等级3，分数10-11将形成等级4，分数0-9将形成等级5。我们可以通过将每个分数转换为其中一个类别来创建类别1–5的最终列。</p><p id="f82f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是实现这一点的python代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="6cb1" class="kc kd hi jy b fi ke kf l kg kh">#Defining a function that converts G3 to one of the five classes</span><span id="652a" class="kc kd hi jy b fi ki kf l kg kh">def filter_class(score):</span><span id="0f9b" class="kc kd hi jy b fi ki kf l kg kh">if score&lt;10:</span><span id="d669" class="kc kd hi jy b fi ki kf l kg kh">return 5</span><span id="2db8" class="kc kd hi jy b fi ki kf l kg kh">elif score&lt;12:</span><span id="5ef2" class="kc kd hi jy b fi ki kf l kg kh">return 4</span><span id="c64d" class="kc kd hi jy b fi ki kf l kg kh">elif score&lt;14:</span><span id="d73b" class="kc kd hi jy b fi ki kf l kg kh">return 3</span><span id="f7e9" class="kc kd hi jy b fi ki kf l kg kh">elif score&lt;16:</span><span id="5773" class="kc kd hi jy b fi ki kf l kg kh">return 2</span><span id="9598" class="kc kd hi jy b fi ki kf l kg kh">else:</span><span id="08ab" class="kc kd hi jy b fi ki kf l kg kh">return 1</span><span id="e83c" class="kc kd hi jy b fi ki kf l kg kh">#defining a new column called 'class' and dropping column 'G3'</span><span id="8e45" class="kc kd hi jy b fi ki kf l kg kh">dataset_trap['class'] = dataset_trap['G3'].apply(filter_class)</span><span id="0633" class="kc kd hi jy b fi ki kf l kg kh">dataset_trap = dataset_trap.drop(['G3'], axis=1)</span></pre><p id="1a8b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们的数据集已经准备好应用分类方法了。</p><p id="4d94" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">逻辑回归</strong></p><p id="9c2d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们用dataset_trap定义X和Y。然后，我们将数据集分为训练集和测试集，对X_train和X_test应用特征缩放，对训练集进行逻辑回归拟合，并预测测试集结果。</p><p id="d74c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是python代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="a7db" class="kc kd hi jy b fi ke kf l kg kh">#Define X and Y using dataset_trap</span><span id="ae5c" class="kc kd hi jy b fi ki kf l kg kh">X = dataset_trap.iloc[:,:-1].values</span><span id="0908" class="kc kd hi jy b fi ki kf l kg kh">Y = dataset_trap.iloc[:,-1].values</span><span id="a271" class="kc kd hi jy b fi ki kf l kg kh">#Splitting the dataset into the Training set and Test set</span><span id="8098" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.model_selection import train_test_split</span><span id="4c9a" class="kc kd hi jy b fi ki kf l kg kh">X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state = 0)</span><span id="41df" class="kc kd hi jy b fi ki kf l kg kh">#Feature Scaling</span><span id="b00e" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.preprocessing import StandardScaler</span><span id="a558" class="kc kd hi jy b fi ki kf l kg kh">sc_X = StandardScaler()</span><span id="ac9f" class="kc kd hi jy b fi ki kf l kg kh">X_train = sc_X.fit_transform(X_train)</span><span id="6aa1" class="kc kd hi jy b fi ki kf l kg kh">X_test = sc_X.fit_transform(X_test)</span><span id="05f5" class="kc kd hi jy b fi ki kf l kg kh">#Fitting Logistic Regression to the Training set</span><span id="3bec" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.linear_model import LogisticRegression</span><span id="826b" class="kc kd hi jy b fi ki kf l kg kh">classifier = LogisticRegression()</span><span id="74b0" class="kc kd hi jy b fi ki kf l kg kh">classifier.fit(X_train,Y_train)</span><span id="4832" class="kc kd hi jy b fi ki kf l kg kh">#Predicting the Test set results</span><span id="5589" class="kc kd hi jy b fi ki kf l kg kh">Y_pred = classifier.predict(X_test)</span></pre><p id="62b8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们有了测试集Y值的预测Y值。通过查看混淆矩阵，我们可以看到我们的模型有多准确:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kj"><img src="../Images/16ce86501c9a4c72e4db33ccca09008b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*Efz8jbOw6JDPgds6kKhjxg.png"/></div></figure><p id="b18e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">混淆矩阵对角线上的数字表示正确分类的数量。因此，为了发现我们的模型有多精确，我们将添加对角线条目并除以测试集结果的总数，即79。</p><p id="413b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是创建混淆矩阵的python代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="544c" class="kc kd hi jy b fi ke kf l kg kh">#Making the confusion matrix</span><span id="ef47" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.metrics import confusion_matrix</span><span id="8ad3" class="kc kd hi jy b fi ki kf l kg kh">cm = confusion_matrix(Y_test, Y_pred)</span><span id="959f" class="kc kd hi jy b fi ki kf l kg kh">cm.trace()/79</span></pre><p id="4bde" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">模型的准确性可以通过正确预测的数量除以测试集结果的总数来衡量。在这种情况下，准确率为50%。这不是很令人印象深刻。</p><p id="d690" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> K个最近邻居</strong></p><p id="aed4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用欧几里德距离和k=5个邻居在我们的训练集上训练k-最近邻居模型。python代码与逻辑回归代码非常相似，只是我们用k近邻模型替换了逻辑回归模型。以下是完整的python代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="e9db" class="kc kd hi jy b fi ke kf l kg kh">#Importing the libraries</span><span id="4523" class="kc kd hi jy b fi ki kf l kg kh">import numpy as np</span><span id="08be" class="kc kd hi jy b fi ki kf l kg kh">import pandas as pd</span><span id="0718" class="kc kd hi jy b fi ki kf l kg kh">import matplotlib.pyplot as plt</span><span id="c338" class="kc kd hi jy b fi ki kf l kg kh">#Importing the dataset</span><span id="4c3b" class="kc kd hi jy b fi ki kf l kg kh">dataset = pd.read_csv("studentmathdummified.csv")</span><span id="955c" class="kc kd hi jy b fi ki kf l kg kh">#Avoiding the dummy variable trap</span><span id="42d6" class="kc kd hi jy b fi ki kf l kg kh">#Dropping GP, Male, urban,LE3, Apart,mother_at_home, father_at_home, reason_course, guardian_other</span><span id="bac3" class="kc kd hi jy b fi ki kf l kg kh">dataset_trap = dataset.drop(dataset.columns[[0,2,4,6,8,10,15,20,26]],axis=1)</span><span id="2305" class="kc kd hi jy b fi ki kf l kg kh">#Defining a function that converts G3 to one of the five classes</span><span id="40da" class="kc kd hi jy b fi ki kf l kg kh">def filter_class(score):</span><span id="92fa" class="kc kd hi jy b fi ki kf l kg kh">if score&lt;10:</span><span id="f52e" class="kc kd hi jy b fi ki kf l kg kh">return 5</span><span id="6abd" class="kc kd hi jy b fi ki kf l kg kh">elif score&lt;12:</span><span id="f1b2" class="kc kd hi jy b fi ki kf l kg kh">return 4</span><span id="7375" class="kc kd hi jy b fi ki kf l kg kh">elif score&lt;14:</span><span id="0116" class="kc kd hi jy b fi ki kf l kg kh">return 3</span><span id="aa72" class="kc kd hi jy b fi ki kf l kg kh">elif score&lt;16:</span><span id="3022" class="kc kd hi jy b fi ki kf l kg kh">return 2</span><span id="961f" class="kc kd hi jy b fi ki kf l kg kh">else:</span><span id="8b81" class="kc kd hi jy b fi ki kf l kg kh">return 1</span><span id="953b" class="kc kd hi jy b fi ki kf l kg kh">#defining a new column called 'class' and dropping column 'G3'</span><span id="a6ac" class="kc kd hi jy b fi ki kf l kg kh">dataset_trap['class'] = dataset_trap['G3'].apply(filter_class)</span><span id="6921" class="kc kd hi jy b fi ki kf l kg kh">dataset_trap = dataset_trap.drop(['G3'], axis=1)</span><span id="a300" class="kc kd hi jy b fi ki kf l kg kh">#Define X and Y using dataset_trap</span><span id="2d0b" class="kc kd hi jy b fi ki kf l kg kh">X = dataset_trap.iloc[:,:-1].values</span><span id="895a" class="kc kd hi jy b fi ki kf l kg kh">Y = dataset_trap.iloc[:,-1].values</span><span id="e08a" class="kc kd hi jy b fi ki kf l kg kh">#Splitting the dataset into the Training set and Test set</span><span id="c28e" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.model_selection import train_test_split</span><span id="9af2" class="kc kd hi jy b fi ki kf l kg kh">X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state = 0)</span><span id="cf29" class="kc kd hi jy b fi ki kf l kg kh">#Feature Scaling</span><span id="a69f" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.preprocessing import StandardScaler</span><span id="9027" class="kc kd hi jy b fi ki kf l kg kh">sc_X = StandardScaler()</span><span id="e302" class="kc kd hi jy b fi ki kf l kg kh">X_train = sc_X.fit_transform(X_train)</span><span id="7cc5" class="kc kd hi jy b fi ki kf l kg kh">X_test = sc_X.fit_transform(X_test)</span><span id="4308" class="kc kd hi jy b fi ki kf l kg kh">#Fitting K nearest neighbors to the Training set</span><span id="195e" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.neighbors import KNeighborsClassifier</span><span id="0c6b" class="kc kd hi jy b fi ki kf l kg kh">classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p =2)</span><span id="b971" class="kc kd hi jy b fi ki kf l kg kh">classifier.fit(X_train,Y_train)</span><span id="f94f" class="kc kd hi jy b fi ki kf l kg kh">#Predicting the Test set results</span><span id="5851" class="kc kd hi jy b fi ki kf l kg kh">Y_pred = classifier.predict(X_test)</span><span id="3556" class="kc kd hi jy b fi ki kf l kg kh">#Making the confusion matrix</span><span id="1b48" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.metrics import confusion_matrix</span><span id="ee01" class="kc kd hi jy b fi ki kf l kg kh">cm = confusion_matrix(Y_test, Y_pred)</span><span id="a75f" class="kc kd hi jy b fi ki kf l kg kh">cm.trace()/79</span></pre><p id="0048" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们模型的准确率是28%，真的很差。</p><p id="34a5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">支持向量机</strong></p><p id="821c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">支持向量机被设计成当只有两个类别时执行分类。然而，当有两个以上的类时，有一种方法可以使用svm，就像我们有五个类的情况一样。一种方法是使用所谓的一对一方案。在一对一方案中，我们为每一对可能的类建立一个svm模型。对于K级分类，有K个选择2对。因此，在我们的例子中，有10对类。我们可以应用构建10个支持向量分类器，并通过将所有10个支持向量分类器应用于测试点并选择测试点被分类为该类的次数最高的类来预测给定测试点的类。python代码与之前的分类器相同，只是我们用支持向量分类器替换了分类器:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="4228" class="kc kd hi jy b fi ke kf l kg kh">#Fitting support vector classifier to the Training set using one-versus-one</span><span id="3671" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.svm import SVC</span><span id="8219" class="kc kd hi jy b fi ki kf l kg kh">classifier = SVC(kernel='linear')</span><span id="a6e4" class="kc kd hi jy b fi ki kf l kg kh">classifier.fit(X_train,Y_train)</span></pre><p id="a118" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们模型的准确率是62%。</p><p id="28d4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当有两个以上的类时，使用svm的另一种方法是使用一对所有的方案。在这个方案中，K个模型是通过将每个类与其他类配对来构建的。在我们的例子中，制造了5个模型。为了预测给定测试点的类别，我们将所有5个模型应用于该测试点，并选择该测试点与该类别的相应模型的最大边缘超平面的垂直距离最大的类别。换句话说，我们选择相应的模型最有把握将测试点分类为该类的类。以下是python代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="0b67" class="kc kd hi jy b fi ke kf l kg kh">#Fitting support vector classifier to the Training set using one-versus-rest</span><span id="d6a9" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.svm import LinearSVC</span><span id="5dd9" class="kc kd hi jy b fi ki kf l kg kh">classifier = LinearSVC()</span><span id="0d40" class="kc kd hi jy b fi ki kf l kg kh">classifier.fit(X_train,Y_train)</span></pre><p id="7b3d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们模型的准确率是56%。</p><p id="0171" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我尝试使用rbf内核，获得了44%的准确率。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="a9f2" class="kc kd hi jy b fi ke kf l kg kh">#Fitting support vector classifier to the Training set using one-versus-one and rbf kernel</span><span id="c9b9" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.svm import SVC</span><span id="269e" class="kc kd hi jy b fi ki kf l kg kh">classifier = SVC(kernel='rbf', random_state=0)</span><span id="2775" class="kc kd hi jy b fi ki kf l kg kh">classifier.fit(X_train,Y_train)</span></pre><p id="7dd5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">默认的正则化参数C是1。我将正则化参数提高到4，得到了45.5%的准确率。将正则化参数提高到10给出48%的准确度。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="c12f" class="kc kd hi jy b fi ke kf l kg kh">#Fitting support vector classifier to the Training set using one-versus-one and rbf kernel and C=10</span><span id="ffab" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.svm import SVC</span><span id="c227" class="kc kd hi jy b fi ki kf l kg kh">classifier = SVC(kernel='rbf', C=10, random_state=0)</span><span id="44d8" class="kc kd hi jy b fi ki kf l kg kh">classifier.fit(X_train,Y_train)</span></pre><p id="fbc4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">增加C的值意味着我们对有多少点被错误分类更加严格；它对应于具有较小边界的分离超平面。换句话说，通过增加C，我们减少了违反保证金的余地。降低C的值对应于对错误分类更加宽容；它对应于具有更大间隔的分离超平面。随着C值的增加，我们看到精度上升。</p><p id="53b6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我还尝试了sigmoid内核，C=30，得到了62%的准确率。将C的值降低到小于30或者将C的值提高到大于30会降低精确度。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="3c71" class="kc kd hi jy b fi ke kf l kg kh">#Fitting support vector classifier to the Training set using one-versus-one and sigmoid kernel and C=30</span><span id="be20" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.svm import SVC</span><span id="0618" class="kc kd hi jy b fi ki kf l kg kh">classifier = SVC(kernel='sigmoid', C=30, random_state=0)</span><span id="00a0" class="kc kd hi jy b fi ki kf l kg kh">classifier.fit(X_train,Y_train)</span></pre><p id="e4f1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">决策树</strong></p><p id="aa95" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这个方法中，我们要种一棵树。分裂标准被选择为熵，并且特征缩放是不必要的。当我将决策树分类器应用于测试集时，我获得了72%的准确率。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="3368" class="kc kd hi jy b fi ke kf l kg kh">#Fitting decision tree classifier to the Training set</span><span id="12e2" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.tree import DecisionTreeClassifier</span><span id="b9d8" class="kc kd hi jy b fi ki kf l kg kh">classifier = DecisionTreeClassifier(criterion=’entropy’)</span><span id="acb1" class="kc kd hi jy b fi ki kf l kg kh">classifier.fit(X_train,Y_train)</span></pre><p id="f100" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我将分割内部节点所需的最小样本数设置为10，将叶节点所需的最小样本数设置为5。这将准确率提高到了77%。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="df05" class="kc kd hi jy b fi ke kf l kg kh">#Fitting decision tree classifier to the Training set with minimum number of samples required to split 10 and minimum number of leaf samples 5</span><span id="8b32" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.tree import DecisionTreeClassifier</span><span id="ad8e" class="kc kd hi jy b fi ki kf l kg kh">classifier = DecisionTreeClassifier(criterion='entropy', min_samples_split=10, min_samples_leaf=5)</span><span id="60f3" class="kc kd hi jy b fi ki kf l kg kh">classifier.fit(X_train,Y_train)</span></pre><p id="6848" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">随机森林</strong></p><p id="6059" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">用这种方法，我们要种一堆树。分裂标准被选择为熵，并且不使用特征缩放。当我使用10棵树将随机森林分类器应用于测试集时，我得到了62%的准确率:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="e4e0" class="kc kd hi jy b fi ke kf l kg kh">#Fitting random forest classifier to the Training set</span><span id="3e5d" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.ensemble import RandomForestClassifier</span><span id="edd0" class="kc kd hi jy b fi ki kf l kg kh">classifier = RandomForestClassifier(criterion='entropy', n_estimators=10)</span><span id="be71" class="kc kd hi jy b fi ki kf l kg kh">classifier.fit(X_train,Y_train)</span></pre><p id="4441" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我将分割内部节点所需的最小样本数设置为10，将叶节点所需的最小样本数设置为5。我还把树的数量增加到了100棵。这将准确率提高到了74%:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="7bea" class="kc kd hi jy b fi ke kf l kg kh">#Fitting random forest classifier to the Training set with 100 trees and minimum samples required to split 10 and minimum samples at a leaf 5</span><span id="5ad7" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.ensemble import RandomForestClassifier</span><span id="3c30" class="kc kd hi jy b fi ki kf l kg kh">classifier = RandomForestClassifier(criterion='entropy', n_estimators=100, min_samples_split=10, min_samples_leaf=5)</span><span id="017b" class="kc kd hi jy b fi ki kf l kg kh">classifier.fit(X_train,Y_train)</span></pre><p id="f623" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了进一步提高准确性，我将max_features设置为None:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="dc07" class="kc kd hi jy b fi ke kf l kg kh">#Fitting random forest classifier to the Training set with 100 trees, min_samples_split=10, min_samples_leaf=5, and max_features=None</span><span id="11a2" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.ensemble import RandomForestClassifier</span><span id="2d40" class="kc kd hi jy b fi ki kf l kg kh">classifier = RandomForestClassifier(criterion='entropy', n_estimators=100, max_features=None, min_samples_split=10, min_samples_leaf=5)</span><span id="acac" class="kc kd hi jy b fi ki kf l kg kh">classifier.fit(X_train,Y_train)</span></pre><p id="5991" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我得到了78%的准确率。</p><p id="1a08" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">型号选择</strong></p><p id="1dd5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了确定哪个模型是最好的，我们将对每个模型执行k重交叉验证(k=10 ),并选择具有最佳准确性的模型。</p><p id="653b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于逻辑回归，我得到了57%的准确率。</p><p id="4fd5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于k=5的k近邻，我得到了43%的准确率。</p><p id="6a82" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于支持向量分类器，一对一，我得到了64%的准确率。</p><p id="ed60" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于支持向量分类器，一对休息，我得到了57%的准确率。</p><p id="f217" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于支持向量分类器，一对一与rbf核，我得到了53%的准确率。</p><p id="e8ea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于支持向量分类器，一对一与rbf核和C=10，我得到了57%的准确率。</p><p id="e259" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于支持向量分类器，一对一的sigmoid核和C=30，我得到了60%的准确率。</p><p id="25e6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于单个决策树，我得到了66%的准确率。</p><p id="9dc7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于min_samples_split=10和min_samples_leaf=5的单个决策树，我得到了69%的准确率。</p><p id="75d7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于有10棵树的随机森林，我得到了64%的准确率。</p><p id="9241" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于有100棵树的随机森林，min_samples_split=10，min_samples_leaf=5，我得到了70%的准确率。</p><p id="b3eb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于有100棵树的随机森林，min_samples_split=10，min_samples_leaf=5，max_features=None，我得到了76%的准确率。</p><p id="e11d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是应用k重交叉验证的python代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="8b78" class="kc kd hi jy b fi ke kf l kg kh">#Applying k-fold cross validation</span><span id="a0bf" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.model_selection import cross_val_score</span><span id="c0ef" class="kc kd hi jy b fi ki kf l kg kh">accuracies = cross_val_score(estimator=classifier,X=X_train, y=Y_train, cv=10)</span><span id="c477" class="kc kd hi jy b fi ki kf l kg kh">accuracies.mean()</span></pre><p id="5567" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">比较每个模型的精度，我们看到具有100棵树的随机森林，min_samples_split=10，min_samples_leaf=5，max_features=None具有最高的精度。我们可能想知道熵是否是分裂的最佳标准，100棵树是否是使用的最佳树数；我们可能还想知道max_features的最佳值是多少。我在熵和基尼系数中进行了网格搜索，在10，100，500中进行了n估计，在“自动”、“无”、“log2”和“1”中进行了最大特征搜索。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="2a7b" class="kc kd hi jy b fi ke kf l kg kh">#grid search</span><span id="a640" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.model_selection import GridSearchCV</span><span id="21d6" class="kc kd hi jy b fi ki kf l kg kh">parameters = [{'criterion':['entropy'],'n_estimators':[10,100,500],'max_features':['auto',None,'log2',1]},{'criterion':['gini'],'n_estimators':[10,100,500],'max_features':['auto',None,'log2',1]}]</span><span id="ec28" class="kc kd hi jy b fi ki kf l kg kh">grid_search=GridSearchCV(estimator = classifier, param_grid=parameters, scoring='accuracy',cv=10)</span><span id="248a" class="kc kd hi jy b fi ki kf l kg kh">grid_search=grid_search.fit(X_train, Y_train)</span><span id="0b57" class="kc kd hi jy b fi ki kf l kg kh">best_accuracy=grid_search.best_score_</span><span id="8224" class="kc kd hi jy b fi ki kf l kg kh">best_parameters=grid_search.best_params_</span></pre><p id="31c6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">结果是最佳精度为76.9%，最佳参数标准=“基尼”，最大特征=无，n估计值=500。</p><p id="c49f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于标准='gini '的随机森林，500棵树，min_samples_split=10，min_samples_leaf=5，max_features=None，我得到了77.5%的准确率。我将min_samples_split增加到50，得到了78.2%的准确率。</p><p id="43e0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">结论</strong></p><p id="cf34" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们将逻辑回归、k-最近邻、支持向量分类器、决策树和随机森林应用于5类分类问题，以预测每个学生的第三年分数将属于5类中的哪一类。我们发现，在我们检查的模型中，性能最好的模型是随机森林分类器，其标准='gini '，500棵树，min_samples_split=50，min_samples_leaf=5，max_features=None。达到的准确率为78.2%。</p><p id="ce3b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们以前的论文中对数据集的回归分析中，我们发现一些最重要的属性是第一年和第二年的成绩、家庭关系的质量、年龄和缺勤次数。500棵树的随机森林回归证明是表现最好的模型之一，准确率为87–88%(R平方)。我们还发现，第三年的成绩与第一年和第二年的成绩之间有很强的线性关系。</p><p id="83ef" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">G1、G2、家庭关系质量、年龄和缺课次数这些属性在每个学校、每个时间段和每个国家是否总是重要的，这是一个公开的问题。在这里收集到的见解可以推广到我们考虑的两所葡萄牙高中以外吗？除了我们考虑的因素之外，还有哪些因素对数学成绩有重要影响？这些是值得探究的开放性问题，以进一步理解和解决数学成绩差的问题。</p><p id="757e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该数据集可在以下位置找到:</p><p id="5422" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">https://archive.ics.uci.edu/ml/datasets/student+performance<a class="ae iu" href="https://archive.ics.uci.edu/ml/datasets/student+performance" rel="noopener ugc nofollow" target="_blank"/></p><p id="aa43" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">页（page的缩写）科尔特斯和a .席尔瓦。使用数据挖掘预测中学生成绩。在a .布里托和j .特谢拉编辑的。，第五届未来商业技术会议论文集(FUBUTEC 2008)第5–12页，葡萄牙波尔图，2008年4月，EUROSIS，ISBN 978–9077381–39–7。<br/> <a class="ae iu" href="http://www3.dsi.uminho.pt/pcortez/student.pdf" rel="noopener ugc nofollow" target="_blank">【网页链接】</a></p></div></div>    
</body>
</html>