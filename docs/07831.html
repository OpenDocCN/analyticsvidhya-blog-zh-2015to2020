<html>
<head>
<title>FPN(feature pyramid networks)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">FPN(要素金字塔网络)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fpn-feature-pyramid-networks-77d8be41817c?source=collection_archive---------6-----------------------#2020-07-09">https://medium.com/analytics-vidhya/fpn-feature-pyramid-networks-77d8be41817c?source=collection_archive---------6-----------------------#2020-07-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="d91a" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">在几乎任何架构上获得免费的精度提升</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/574dc1d230d782419d9971f2e68076bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0b5l_AhQaKjSvqY9afqzfg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">资料来源:FPN报纸</figcaption></figure><p id="bdd0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我已经计划阅读主要的物体探测论文(虽然我已经浏览了大部分，现在我将详细阅读它们，好到足以写一篇关于它们的博客)。这些论文与基于深度学习的对象检测相关。随时给建议或询问疑惑会尽我所能帮助大家。任何从这个领域开始的人都可以跳过许多这样的论文。当我看完所有的论文后，我也会写下它们的优先顺序/重要性。我写这篇博客是考虑到和我相似的读者仍在学习。虽然我会通过从各种来源(包括博客、代码和视频)深入理解论文来尽力写出论文的关键，但如果您发现任何错误，请随时在博客上指出或添加评论。我已经提到了我将在博客结尾涉及的论文列表。</p><p id="2f25" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们开始吧:)</p></div><div class="ab cl kj kk gp kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="hb hc hd he hf"><p id="e880" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">亚字幕是正确的，FPN是一个非常简单的方法，可以用于几乎任何模型，以改善结果。我们将很快进入论文的技术细节，但是对于这个博客，有一些先决条件。下面的快速RCNN，更快的RCNN，锚盒，SSD的知识就派上用场了，你应该有个高层次的想法。我也有所有这些论文的博客，你可以查看它们(博客末尾的链接)。如果你很好地理解所有的先决条件，FPN是一个相对简单的。</p><p id="8283" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">影像金字塔(多种比例的多幅影像)通常在预测时使用，以改善结果。但是使用现代深度学习架构计算结果通常在计算和时间方面都是一个昂贵的过程。</p><p id="813f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><em class="kq"> FPN是基于利用深度CNN固有的多尺度金字塔式层级。它类似于RCNN和快速RCNN之间的差异，RCNN是一种基于区域的对象检测器，其中我们首先使用诸如选择性搜索的算法来找到ROI，然后从图像中裁剪这些ROI(大约2000个),并将它们馈送到CNN以获得结果，在快速RCNN中，CNN的初始层被共享用于完整图像，ROI裁剪在提取的特征图上完成，因此节省了大量时间。在FPN的情况下，研究是基于开发内部多尺度的性质，和形象金字塔是以某种方式实施的内部架构和共享网络的大部分。我们现在将进入技术细节。</em></p><p id="0a00" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">CNN基于分层结构，其中特征图的分辨率在每一层之后降低，但是每一个更深的层捕获的语义比前一层更强。由于下采样，语义更强的特征在空间上更粗糙。FPN创建了一种架构，其中语义更强的特征与来自先前层的特征合并(先前层被二次抽样的次数更少，因此具有更准确的本地化信息)。</p><p id="db6e" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">该架构由两条路径组成:</p><ol class=""><li id="2365" class="kr ks hi jp b jq jr jt ju jw kt ka ku ke kv ki kw kx ky kz bi translated">自下而上路径(正常前馈CNN)</li><li id="0fc4" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated">自顶向下途径(用于合并功能的新架构)</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lf"><img src="../Images/e966e3b526b93125b9cd285f67ac7c1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GYtJv3Kv1qbBh4LBGVkV_Q.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">FPN建筑资料来源:FPN报纸</figcaption></figure><h1 id="19c2" class="lg lh hi bd li lj lk ll lm ln lo lp lq io lr ip ls ir lt is lu iu lv iv lw lx bi translated">自下而上的路径(上图中的左金字塔)</h1><ul class=""><li id="1f6f" class="kr ks hi jp b jq ly jt lz jw ma ka mb ke mc ki md kx ky kz bi translated">这是正常的前馈CNN架构。在本文中，作者使用Resnet体系结构进行性能评估。我们首先将这些层命名为C2、C3、C4、C5，它们是resnet架构中的Conv 2、3、4和5层。在应用C2后，特征地图的大小为imagesize/4，并且在每一层后，该空间维度以因子2缩减采样。</li></ul><h1 id="968a" class="lg lh hi bd li lj lk ll lm ln lo lp lq io lr ip ls ir lt is lu iu lv iv lw lx bi translated">自上而下的路径(上图中的右金字塔)</h1><p id="8051" class="pw-post-body-paragraph jn jo hi jp b jq ly ij js jt lz im jv jw me jy jz ka mf kc kd ke mg kg kh ki hb bi translated">在这个路径中，使用横向连接将较深的特征与较低的特征合并。由于自底向上路径中的层的通道数量不相同，因此首先应用1*1卷积，以获得每层的固定数量的通道(在本文中该维度保持为256)。空间大小也不同，因此我们对更深的特征进行上采样(2x ),使得该特征的空间大小与自下而上路径中的前一层的更高分辨率的特征地图相匹配。现在，两个特征图的维度是相同的，并且它们通过逐元素添加来合并。</p><p id="27f7" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们可以用一个例子来理解这一点。假设我们的图像尺寸为512*512，现在每个卷积层(C2、C3、C4、C5)后的特征图尺寸将为[(128*128)，(64*64)，(32*32)，(16*16)]。每层输出通道数为[256，512，1024，2048]。现在，我们对C2、C3、C4和C5的输出应用1*1卷积(输出通道数= 256 ),以获得相等的通道数。我们将用与S2、S3、S4相同数量的输出通道来调用这些中间特征，S5对应于C2、C3、C4、C5。现在S5被上采样为32*32，并使用元素加法与S4合并。现在，这个输出将被上采样为64*64，并将与S3等合并。我们将此阶段的输出称为T2、T3、T4和T5。</p><p id="7966" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">为了减少由于上采样引起的混叠效应，对T2、T3、T4、T5应用3×3卷积，以获得我们的最终特征图P2、P3、P4、P5，对应于C2、C3、C4、C5。这些特征用于生成最终的分类和回归得分(bbox)。磁头的参数可以共享，单独的磁头没有额外的好处。</p><p id="bcb6" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这就是FPN的理论。但是我们将会看到FPN是如何实现更快的RCNN和快速RCNN的。</p><h1 id="5e41" class="lg lh hi bd li lj lk ll lm ln lo lp lq io lr ip ls ir lt is lu iu lv iv lw lx bi translated">FPN为更快的RCNN</h1><p id="16e9" class="pw-post-body-paragraph jn jo hi jp b jq ly ij js jt lz im jv jw me jy jz ka mf kc kd ke mg kg kh ki hb bi translated">快速RCNN采用区域建议网络。RPN用于生成边界框提议，这些提议稍后用于生成最终预测。RPN是在最后一层(C5)的提取特征上实现的小型网络。将3*3卷积应用于该提取的特征，然后是两个类似的1*1卷积层(一个用于分类，另一个用于回归)。</p><p id="79a4" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">RPN在这里通过简单地用FPN代替单一比例的特征地图来适应。因此，现在RPN是为P2-P5而不是C5单独实施的。对于RPN的训练，使用多尺度的锚箱。但是由于多尺度现在是所提取的特征中固有的，所以没有必要在任何级别上具有多尺度锚定框。相反，单个比例锚定框被指定给每个级别。对于{P2、P3、P4、P5和P6}，本文中使用的锚盒大小为{32，64，128，256，512 }。此处引入P6，以便可以使用大型锚箱。P6是步长为2的P5的二次抽样。使用纵横比为{1:1，1:2，2:1}的锚盒。</p><p id="e013" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这些锚盒与地面真实盒相匹配，并且模型被端到端地训练。</p><h1 id="ca55" class="lg lh hi bd li lj lk ll lm ln lo lp lq io lr ip ls ir lt is lu iu lv iv lw lx bi translated">快速RCNN的FPN</h1><p id="2b1f" class="pw-post-body-paragraph jn jo hi jp b jq ly ij js jt lz im jv jw me jy jz ka mf kc kd ke mg kg kh ki hb bi translated">在快速RCNN中实现FPN非常简单。快速RCNN使用区域提议技术(如选择性搜索)来生成ROI，并在单尺度特征图上使用ROI池来获得最终结果。通过应用FPN，我们将有不同比例的多个特征图，我们需要一个策略来分配给定的ROI到特征图(现在我们有多个特征图，哪个特征图用于给定的ROI？).</p><p id="3a1a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">使用的特征图通过以下方式计算:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mh"><img src="../Images/b6e6af91c96299eea51bbb3d1b7a21a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fZm-fXEOdKyGRqZpJ70BBg.png"/></div></div></figure><p id="f8e4" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里224是imagenet数据集中图像的训练大小(使用的resnet保留在imagenet上)。k0是尺寸为224的ROI被分配到的特征图，w和h是ROI的宽度和高度。头部具有用于每个特征图的共享参数。</p><h1 id="4156" class="lg lh hi bd li lj lk ll lm ln lo lp lq io lr ip ls ir lt is lu iu lv iv lw lx bi translated">参考</h1><ol class=""><li id="713c" class="kr ks hi jp b jq ly jt lz jw ma ka mb ke mc ki kw kx ky kz bi translated"><a class="ae mi" href="https://github.com/potterhsu/easy-fpn.pytorch" rel="noopener ugc nofollow" target="_blank">https://github.com/potterhsu/easy-fpn.pytorch</a></li><li id="92ea" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1612.03144.pdf" rel="noopener ugc nofollow" target="_blank">用于目标检测的特征金字塔网络。</a></li></ol><p id="0b2b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">和平…</p></div><div class="ab cl kj kk gp kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="hb hc hd he hf"><h1 id="7745" class="lg lh hi bd li lj mj ll lm ln mk lp lq io ml ip ls ir mm is lu iu mn iv lw lx bi translated">论文列表:</h1><ol class=""><li id="e987" class="kr ks hi jp b jq ly jt lz jw ma ka mb ke mc ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1312.6229.pdf" rel="noopener ugc nofollow" target="_blank"> OverFeat:使用卷积网络的综合识别、定位和检测</a>。[ <a class="ae mi" href="https://towardsdatascience.com/overfeat-review-1312-6229-4fd925f3739f" rel="noopener" target="_blank">链接到博客</a></li><li id="6534" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1311.2524.pdf" rel="noopener ugc nofollow" target="_blank">丰富的特征层次，用于精确的对象检测和语义分割(RCNN)。</a> [ <a class="ae mi" rel="noopener" href="/@sanchittanwar75/rcnn-review-1311-2524-898c3148789a">链接到博客</a></li><li id="8dde" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1406.4729.pdf" rel="noopener ugc nofollow" target="_blank">用于视觉识别的深度卷积网络中的空间金字塔池(SPPNet)。</a> [ <a class="ae mi" rel="noopener" href="/@sanchittanwar75/review-spatial-pyramid-pooling-1406-4729-bfc142988dd2">链接到博客</a></li><li id="1c48" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1504.08083.pdf" rel="noopener ugc nofollow" target="_blank">快速R-CNN </a> [ <a class="ae mi" rel="noopener" href="/@sanchittanwar75/fast-rcnn-1504-08083-d9a968a82a70">链接到博客</a></li><li id="2cc7" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1506.01497.pdf" rel="noopener ugc nofollow" target="_blank">更快的R-CNN:用区域提议网络实现实时目标检测。</a> [ <a class="ae mi" href="https://towardsdatascience.com/faster-rcnn-1506-01497-5c8991b0b6d3" rel="noopener" target="_blank">链接到博客</a></li><li id="e372" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">你只看一次:统一的，实时的物体检测。</a> [ <a class="ae mi" rel="noopener" href="/@sanchittanwar75/yolo-1506-02640-dbe968e87b46">链接到博客</a></li><li id="8d74" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank"> SSD:单次多盒探测器</a>。[博客链接]</li><li id="7795" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1612.03144.pdf" rel="noopener ugc nofollow" target="_blank">用于目标检测的特征金字塔网络。</a> ←你完成了这篇博客。</li><li id="c412" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1701.06659.pdf" rel="noopener ugc nofollow" target="_blank"> DSSD:解卷积单粒子探测器</a>。[博客链接]</li><li id="f49f" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1708.02002.pdf" rel="noopener ugc nofollow" target="_blank">密集物体检测的焦点丢失(视网膜网)。</a>【博客链接】</li><li id="c274" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated">YOLOv3:一种渐进的改进。[博客链接]</li><li id="1704" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1805.09300v3.pdf" rel="noopener ugc nofollow" target="_blank">狙击手:高效多尺度训练</a>。[博客链接]</li><li id="3a92" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1904.04514.pdf" rel="noopener ugc nofollow" target="_blank">标注像素和区域的高分辨率表示。</a>【博客链接】</li><li id="bf3c" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1904.01355v5.pdf" rel="noopener ugc nofollow" target="_blank"> FCOS:全卷积一级目标检测</a>。[博客链接]</li><li id="1eff" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1904.07850.pdf" rel="noopener ugc nofollow" target="_blank">物为点</a>。[博客链接]</li><li id="e9d6" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated">CornerNet-Lite:高效的基于关键点的对象检测。【博客链接】</li><li id="31d8" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1904.08189v3.pdf" rel="noopener ugc nofollow" target="_blank"> CenterNet:用于对象检测的关键点三元组</a>。[博客链接]</li><li id="2c94" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1909.03625v1.pdf" rel="noopener ugc nofollow" target="_blank"> CBNet:一种用于目标检测的新型复合主干网络体系结构。</a>【博客链接】</li><li id="601f" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/pdf/1911.09070v2.pdf" rel="noopener ugc nofollow" target="_blank"> EfficientDet:可扩展且高效的对象检测</a>。[博客链接]</li><li id="d292" class="kr ks hi jp b jq la jt lb jw lc ka ld ke le ki kw kx ky kz bi translated"><a class="ae mi" href="https://arxiv.org/abs/2004.10934" rel="noopener ugc nofollow" target="_blank"> YOLOv4:物体检测的最佳速度和精度</a>。[博客链接]</li></ol></div></div>    
</body>
</html>