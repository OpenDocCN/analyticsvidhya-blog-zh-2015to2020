# 卷积和池的简短介绍:深度学习 101！

> 原文：<https://medium.com/analytics-vidhya/deep-learning-methods-1700548a3093?source=collection_archive---------0----------------------->

在 Stetson 大学计算机科学助理教授 Joshua Eckroth 的教程中学习卷积和池的概念。

![](img/b4ed6228297afaba852d272b18e73ee3.png)

深度学习是一个广阔的领域，这些天来引起了人们极大的兴趣。它在研究中广泛使用，但在过去几年中，它在行业中慢慢获得了市场渗透。但是深度学习的本质是什么呢？

深度学习指的是有很多层的神经网络。这仍然是一个相当时髦的词，但它背后的技术是真实的，相当复杂。正如下面的谷歌趋势图所示，随着机器学习和人工智能的出现，这个词越来越受欢迎:

![](img/1ac55176ea2a3df2f9c3fd759c442b7c.png)

深度学习的主要优势在于，将更多数据与计算能力相结合通常会产生更准确的结果，而无需工程任务所需的大量工作。

在本文中，我们将快速了解卷积和池的概念。*本文假设你对基本的深度学习术语有基本的了解。*

# 深度学习方法

深度学习指的是可以在特定应用中使用的几种方法。这些方法包括卷积层和池。更简单和更快速的激活函数，如 ReLU，如果是正数，则返回神经元的加权和，如果是负数，则返回零。

正则化技术，例如 dropout，在权重更新基期间随机忽略权重，以防止过拟合。GPU 用于更快的训练，速度快 50 倍。这是因为它们针对广泛用于神经网络和语音识别等应用的存储单元的矩阵计算进行了优化。

几个因素导致了深度学习在过去五年中的急剧增长。大型公共数据集，如 ImageNet，拥有数百万张包含 1000 个类别的标记图像，以及 Mozilla 的 Common Voice 项目，包含语音样本，现在都可以使用。这样的数据集已经满足了深度学习的基本要求——大量的训练数据。GPU 已经过渡到深度学习和集群，同时也专注于游戏。这有助于使大规模深度学习成为可能。

每个人都可以使用开源的、正在快速改进的高级软件框架。其中包括 TensorFlow、Keras、Torch 和 Caffe。实现最先进结果的深度架构，如 Inception-v3，正用于 ImageNet 数据集。这个网络实际上有大约 2400 万个参数，一个大型的研究人员和软件工程师社区正在快速地将研究原型转化为任何人都可以下载、评估和扩展的开源软件。

# 卷积和汇集

仔细看看两个基本的深度学习技术，即卷积和池化。在本节中，我们使用图像来理解这些概念。然而，你将要学习的东西也可以应用于其他数据，比如音频信号。

## 盘旋

看看下面的照片，开始放大观察像素:

![](img/c32fd7be82e82e66bb94e8149ecbee7a.png)

每个通道都会发生卷积。输入图像通常由三个通道组成；红色、绿色和蓝色。下一步是分离这三种颜色。下图对此进行了描述:

![](img/21aebfa4588795da6962f19a54ce7320.png)

卷积是一个核。在此图像中，应用了 3 x 3 内核。每个内核都包含许多权重。内核在图像上滑动，并计算内核上像素的加权和，每个像素乘以其对应的内核权重:

![](img/ac268130826750e5389131dee21e07db.png)

还添加了一个偏置项。内核滑过的每个位置都会产生一个数字，即加权和。内核的权重从任意随机值开始，并在训练阶段发生变化。下图显示了三个不同权重的内核示例:

![](img/c246ad249b4cadb242416204fd47e78e.png)

您可以看到图像如何根据权重进行不同的变换。最右边的图像突出显示了边缘，这通常有助于识别对象。stride 帮助您理解内核如何在映像中滑动。下图是 1 x 1 步幅的示例:

![](img/4863e904f4db192e2b87b2d8ae995695.png)

内核向右移动一个像素，然后向下移动。在整个过程中，内核的中心将击中图像的每个像素，同时与其他内核重叠。还观察到内核的中心遗漏了一些像素。下图描绘了一个 2 x 2 的步幅:

![](img/280e05c9b7e19407c3b62217cf87bc35.png)

在某些情况下，观察到没有重叠发生。为了证明这一点，下图包含一个 3 x 3 步幅:

![](img/0f867430b0c3e00602bed3bb078881d4.png)

在这种情况下，不会发生重叠，因为内核的大小与步幅相同。

然而，图像的边界需要不同的处理。要实现这一点，您可以使用填充。这有助于避免跨越边界扩展内核。填充由额外的像素组成，这些像素始终为零。它们对加权和没有贡献。填充允许内核的权重覆盖图像的每个区域，同时仍然让内核假设步幅为 1。内核为它覆盖的每个区域产生一个输出。

因此，如果步幅大于 1，则输出会比原始像素少。换句话说，卷积有助于降低图像的维度。此处显示的公式告诉我们卷积输出的维数:

![](img/f56797aaf29dea85b54c46a8db2489a2.png)

通常使用正方形图像。为了简单起见，使用了内核和步长。这有助于只关注一个维度，即宽度和高度。在下图中，描绘了步幅为(3，3)的 3 x 3 内核:

![](img/6853223931e3fb4e0cdd09a95e7dfa9d.png)

前面的计算给出了 85 宽度和 85 高度的结果。该图像的宽度和高度已经从最初的 256 有效地减少了三分之一。您可以使用步长 1 让卷积触及每个像素，而不是使用大的步长。这将有助于获得更实际的结果。您还需要确保有足够的填充。

但是，当您在网络中移动时，减少图像尺寸是有益的。这有助于网络训练更快，因为参数会更少。参数越少，过度拟合的可能性就越小。

## 联营

您可能经常在卷积维度之间使用最大或平均池，而不是改变步长。Pooling 查看一个区域，让您假设它是 2 x 2，并且只保留最大值或平均值。下图描述了一个 2 x 2 矩阵，该矩阵描述了池化:

![](img/8c60a186d3ebbb18bcd3af6c800b7ce6.png)

池区域总是具有与池大小相同的步幅。这有助于避免重叠。这里有一个相对浅显的**卷积神经网络** ( **CNN** )表示法:

![](img/56adcdd9de2a93bc4b6d6e7eaac926d0.png)

来源:cs231.github.io，麻省理工学院许可

您可以观察到，在最终到达传统的全连接网络之前，输入图像经历了各种卷积和池层以及它们之间的 ReLU 激活。尽管图中没有描绘，但是完全连接的网络最终预测了类别。

在这个例子中，和大多数 CNN 一样，每一层都有多个卷积。在这里，您将观察到 10 个，它们被描述为行。这 10 个卷积中的每一个在每一列中都有它们自己的核，因此可以在每个分辨率下学习不同的卷积。右边完全连接的层将决定哪些回旋最好地识别汽车或卡车，等等。

*如果你发现了这篇文章，可以探索一下* *约书亚·埃克罗斯博士的* [*Python 人工智能项目初学者*](https://www.amazon.com/Python-Artificial-Intelligence-Projects-Beginners/dp/1789539463) *通过实现真实世界的人工智能项目来构建智能应用。这本* [*书*](https://www.packtpub.com/big-data-and-business-intelligence/python-artificial-intelligence-projects-beginners) *用 Python 演示了 AI 项目，涵盖了构成人工智能世界的现代技术。*

约书亚·埃克罗斯在斯特森大学教授大数据挖掘和分析、人工智能和软件工程。他还拥有人工智能和认知科学的博士学位，专注于溯因推理和元推理。