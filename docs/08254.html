<html>
<head>
<title>Decision Tree Algorithm..</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树算法..</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decision-tree-algorithm-fa4b4d8d5bd7?source=collection_archive---------18-----------------------#2020-07-22">https://medium.com/analytics-vidhya/decision-tree-algorithm-fa4b4d8d5bd7?source=collection_archive---------18-----------------------#2020-07-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="e376" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树是流行的机器学习算法之一，是理解使用树的集成技术的敲门砖。</p><p id="4cfb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，决策树算法是许多与数据科学领域相关的采访中的热门话题。</p><p id="542f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">理解决策树..</strong></p><p id="2c92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树更像是一种管理工具，许多专业人员使用它来做出有关资源成本的决策，这些决策是基于所应用的过滤器做出的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/da67752fdbca78521d773da6ea2c78c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*rRn1sowaBY1QZb1mKrnwbA.png"/></div></figure><p id="4174" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树最好的部分是它是一个非参数工具，这意味着没有关于错误或数据分布的潜在假设。它基本上意味着模型是基于观察到的数据构建的。</p><p id="234c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们善于解决手边的任何类型的问题(分类或回归)。决策树算法简称为<strong class="ih hj"/><strong class="ih hj">(分类和回归树)</strong>。</p><p id="6539" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">决策树常用术语:</strong></p><ol class=""><li id="1c8c" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated"><strong class="ih hj">根节点:</strong>它代表整个群体或样本，并进一步分成两个或多个同类集合。</li><li id="2da5" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated"><strong class="ih hj">拆分:</strong>是将一个节点分成两个或两个以上子节点的过程。</li><li id="17c5" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated"><strong class="ih hj">决策节点:</strong>当一个子节点分裂成更多的子节点时，则称为决策节点。</li><li id="8c67" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated"><strong class="ih hj">叶/端节点:</strong>不分裂的节点称为叶或端节点。</li><li id="40a5" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated"><strong class="ih hj"> Max_Depth: </strong>一棵树从根到叶节点的完整旅程。</li><li id="1a62" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated"><strong class="ih hj">分支/子树:</strong>整个树的一个子部分称为分支或子树。</li><li id="8ea7" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated"><strong class="ih hj">父节点和子节点:</strong>被划分为子节点的节点称为子节点的父节点，而子节点是父节点的子节点。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jz"><img src="../Images/70771c46863b20fa08ee5fed0e5c1ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*eHdZyTS0FewEyp2AcK5hRw.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">演示决策树的经典例子</figcaption></figure><p id="40e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树是如何工作的！</p><ol class=""><li id="b627" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">首先，我们将遍历所有可能的分裂，计算每个分裂的纯度并挑选分裂。</li><li id="ac9a" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">对所有标签的纯度进行比较，选择最好的。这使得根节点成为最佳预测器。</li><li id="f6b0" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">该算法本质上是递归的，因为形成的组可以被细分，并且重复该过程直到树完全生长。</li></ol><p id="a5ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">主要决策领域:</strong></p><ol class=""><li id="1908" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">确定最佳分割:</li></ol><p id="af21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">具有同类分布节点是优选的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es ke"><img src="../Images/3b5cdde39d0f969f58aa0f90ba38fe3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bey3hZIb5YsMiZTE50ouMQ.png"/></div></div></figure><p id="c446" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.节点杂质的测量:下面是杂质的测量</p><p id="756f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(一)。基尼指数</p><p id="b346" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(二)。熵</p><p id="78cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(三)。错误分类误差</p><p id="50cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">举例理解各术语:</strong></p><p id="3cc2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来看一个数据集——天气，下面是数据标题的快照:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kj"><img src="../Images/2df6f8ec2ea20604b950b31916c50b75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*l8QterV3RnM5bbS3bH8LsA.png"/></div></figure><p id="2c6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在根据上面写的算法和要考虑的决策点，我们需要具有最大可能信息分割的特征。</p><p id="1b27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注:</strong>在根节点处，杂质水平最大，信息增益可以忽略不计。随着我们沿着树往下走，熵随着信息增益的最大化而减少。因此，我们选择增益最大的特性。</p><p id="ac85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，使用<strong class="ih hj">熵</strong>计算天气数据集的杂质测度:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kk"><img src="../Images/0c353d29d2ccacaf1dca493a07957c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*HBuWCgKO9enFMpRPcRXuiw.png"/></div></figure><p id="f19b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于每个特征，我们将计算熵，例如，outlook和windy计算如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es kl"><img src="../Images/21e6b6eacaa2a9ea7c8f5584f2e30eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aWSGnuhvsZb23wgVVGJt5Q.png"/></div></div></figure><p id="0acf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在对所有特征进行计算之后，将为根节点选择具有最大杂质度量(熵)的特定特征。</p><p id="cafb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是所有功能的摘要:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es km"><img src="../Images/9c61bad1a7952923a637642302693cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mfjMM6m9GATAmWNw2mFwKA.png"/></div></div></figure><p id="793d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以我们的根节点是<strong class="ih hj">展望。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es kn"><img src="../Images/d9c10df21cc45dcd1a6010944fbf8a33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3tIhPGfcfd3gn1rtr9L4Ew.png"/></div></div></figure><p id="5f57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对子树重复同样的操作，直到长成完整的树。下面是最终的决策树:-</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es ko"><img src="../Images/4f07010595284cf534587c3e8963976b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*azaAeAQ1k6GXWFOrd0o1OQ.png"/></div></div></figure><p id="1363" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">基尼指数:</strong></p><p id="9eba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在机器学习sci-kit learn中，基尼指数被用作评估杂质的默认方法。然而，当使用熵或基尼系数时，在结果中的结果几乎没有任何区别，但是因为有两种不同的度量，所以我们应该知道它们。</p><p id="eaa6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">二元目标变量的基尼指数为:-</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kp"><img src="../Images/1345ffe107468eea216dc8eb1be74bd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/0*VxweeuXCW6TNG5B_.png"/></div></figure><p id="c837" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基尼系数通过由分裂产生的两个群体中的阶级混合程度，给出了分裂有多好的概念。完美的分离导致基尼系数为0，而最糟糕的分离导致50/50的等级。</p><p id="a1aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们为每一行计算它，并在二叉树中相应地分割数据。我们递归地重复这个过程。</p><p id="b088" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于二元目标变量，最大基尼指数值:</p><p id="6dd0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">= 1—(1/2)—(1/2)<br/>= 1–2 *(1/2)<br/>= 1-2 *(1/4)<br/>= 1–0.5<br/>= 0.5</p><p id="1e26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好地理解，用图展示熵和基尼系数:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kq"><img src="../Images/6104c4d66bb20ca5b1cb61cffb33d71b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*WT0lqIBscszI0TqXy8-ccg.png"/></div></figure><h1 id="fe7e" class="kr ks hi bd kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo bi translated">信息增益:</h1><p id="e4fc" class="pw-post-body-paragraph if ig hi ih b ii lp ik il im lq io ip iq lr is it iu ls iw ix iy lt ja jb jc hb bi translated">较少不纯的节点需要较少的信息来描述它。并且，更不纯的节点需要更多的信息。信息增益是一种度量，用来定义一个被称为熵的系统中的无序程度。如果样本完全同质，则熵为零，如果样本被等分(50% — 50%)，则熵为1。它选择与父节点和其他分裂相比具有较低熵的分裂。熵越小越好。</p><p id="bd75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意:-超参数调整是任何决策树算法中非常关键的一步。</strong></p><p id="fba6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">需要调整的主要超参数:</p><ol class=""><li id="f023" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated"><strong class="ih hj"> max_depth: </strong>定义了一棵树的总深度，一般应该进行调优，否则会导致模型过拟合。</li><li id="f812" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated"><strong class="ih hj"> min_samples_leaf: </strong>叶节点所需的最小样本。如果min_samples_leaf留在分支中，将考虑任何深度的分割点。</li><li id="8012" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated"><strong class="ih hj"> max_leaf_nodes: </strong>用max个叶节点来生长一棵树，以获得最佳结果。</li></ol><p id="f8cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">调整这些超参数的主要原因是，如果我们不控制树的生长，那么最后所有的叶节点将具有1个具有大深度的样本(在数据集中的大特征的情况下),这可能在很大程度上导致过拟合，并因此降低模型的准确性，同时增加模型的复杂性。</p><p id="53aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">决策树的优势:</strong></p><ul class=""><li id="bfe6" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc lu jr js jt bi translated">决策树很容易解释。</li><li id="fd87" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc lu jr js jt bi translated">构建决策树需要用户准备更少的数据，因为不需要标准化或缩放数据。</li></ul><p id="1589" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">决策树的缺点:</strong></p><ul class=""><li id="076c" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc lu jr js jt bi translated">通常，决策树倾向于过度拟合数据，这导致模型复杂性的增加以及模型中方差的增加。</li><li id="a947" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc lu jr js jt bi translated">决策树也被称为贪婪算法，因为数据集中的小变化会对整个模型产生大的影响。</li></ul><p id="5502" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，决策树是对所有机器学习专业人员总是有用的基础模型，因为它也有助于可视化数据集分布，并告诉我们数据集中的最佳特征。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/24bfb1403deb6225bb401e18a1863a9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*2g1KoyaECpb3-peKixbReg.png"/></div></figure></div></div>    
</body>
</html>