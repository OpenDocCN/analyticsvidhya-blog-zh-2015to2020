<html>
<head>
<title>Cost Complexity Pruning in Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树中的成本复杂性修剪</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/cost-complexity-pruning-in-decision-trees-f82b14a7fe91?source=collection_archive---------14-----------------------#2020-09-19">https://medium.com/analytics-vidhya/cost-complexity-pruning-in-decision-trees-f82b14a7fe91?source=collection_archive---------14-----------------------#2020-09-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="1d4a" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">理解决策树中的过拟合问题，并使用Python中的Scikit-Learn通过最小成本复杂度剪枝解决该问题</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/2580fbf7595d2b8cedf8198719dcf804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IBixgP3ehY66gDXVHwXCUA.jpeg"/></div></div></figure><p id="3c89" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi kf translated">决策树是数据科学家工具箱中最直观、最有效的工具之一。它有一个倒置的树状结构，曾经只用于决策分析，但现在也是一个出色的机器学习算法，尤其是当我们手中有一个分类问题时。</p><p id="4840" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">他们以捕捉数据模式的能力而闻名。但是，凡事过度都是有害的，对吗？决策树臭名昭著，因为它们太依赖于被训练的数据。<br/>因此，你的树在部署上给出的结果很差，因为它不能处理一组新的值。</p><p id="253e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">但是，你不用担心。就像一个熟练的机械师在他的工具箱里有各种尺寸的扳手一样，一个熟练的数据科学家也有他的一套技术来处理任何类型的问题。</p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><p id="b0b8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">修剪是用来克服过度拟合问题的技术之一。从字面上来看，修剪是一种有选择地去除树(或植物)的某些部分，如树枝、芽或根，以改善树的结构，促进健康生长的做法。这也正是修剪对我们的决策树所做的。它使它变得多才多艺，因此如果我们向它输入任何新类型的数据，它都能适应，从而解决了过度拟合的问题。</p><p id="7c36" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">它减少了决策树的大小，这可能会稍微增加你的训练误差，但大大减少了你的测试误差，从而使它更具适应性。</p><p id="1b48" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">最小成本复杂度剪枝</strong>是决策树剪枝的一种。</p><p id="70d2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">该算法由称为复杂度参数的<strong class="jl hj"> α </strong> (≥0 <strong class="jl hj"> ) </strong>参数化。</p><p id="a564" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">复杂度参数用于定义给定树T的成本复杂度度量Rα(T):<strong class="jl hj">Rα(T)= R(T)+α|T|<br/></strong>其中| T |是T中的终端节点的数量，R(T)传统上被定义为终端节点的总误分类率。</p><p id="5934" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在其0.22版本中，Scikit-learn引入了这个名为ccp阿尔法的参数(是的！它是<strong class="jl hj">成本复杂性修剪- Alpha </strong>的缩写，可用于执行相同的操作。</p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><p id="456e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们将使用<strong class="jl hj"> Iris数据集</strong>来拟合决策树。你可以点击下载数据集<a class="ae kv" href="https://github.com/iasarthak/My-Data-Logs/blob/master/Concepts/Iris.csv" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="d9e5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">首先，让我们导入基本库和数据集</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kw"><img src="../Images/44c2679725713376bb1f43d521835b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*TdQXH0N70iE6d19GH0HZiw.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">导入库和数据集</figcaption></figure><p id="fcbf" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">数据集看起来像这样-</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lb"><img src="../Images/f90def8f1c97c90c67c353feb3a8a536.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*J8CeiYrkcQXq2-aV7pO0Lg.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">我们数据集的一个片段</figcaption></figure><p id="f056" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们的目的是根据萼片的长度和宽度来预测花的种类。</p><p id="cec4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们将把数据集分成两部分——训练和测试，这样我们就可以看到我们的模型在看不见的数据上的表现。<br/>(我们将使用<strong class="jl hj"> <em class="lc"> sklearn的<strong class="jl hj"> train_test_split </strong>函数。模型_选择</em> </strong>来分割数据集)</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ld"><img src="../Images/acc9336fb3d87ddf6a49e11d887a1a4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UI8LUGwZuq04uSBJpTSxCQ.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">将数据集拆分成训练和测试</figcaption></figure><p id="80af" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，让我们为训练部分安装一个决策树，并对测试和训练进行预测。<br/>(为此，我们将使用<strong class="jl hj"> <em class="lc"> sklearn.tree </em> </strong>中的<strong class="jl hj">决策树分类器</strong>)</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es le"><img src="../Images/048c7932eb5cc5e42efda0870f80c2b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*gLOrV8ZWo2OqusdyPj_dZw.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">决策树的实现</figcaption></figure><p id="fbc2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">默认情况下，决策树函数不执行任何修剪，而是允许树尽可能地增长。我们在列车和测试部分分别得到了<strong class="jl hj"> 0.95 </strong>和<strong class="jl hj"> 0.63 </strong>的精度分数，如下所示。我们可以说我们的模型过拟合，即记忆训练部分，但不能在测试部分表现得同样好。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lf"><img src="../Images/cf833bf4f4d9d1f6816bf62be2db1d90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sWFgS-KSUD1PXXUvEfoEVw.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">训练和测试精度分别为0.95和0.63</figcaption></figure><p id="c8e2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">sklearn中的决策树有一个函数叫做cost_complexity_pruning_path，它给出了剪枝时子树的有效alphas，也给出了相应的杂质。换句话说，我们可以用这些α值来修剪我们的决策树-</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lg"><img src="../Images/bd38828d182387daa81361143d6b8a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*VL7TqhI7MPcDsI8XCY7vjg.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">成本复杂性修剪路径</figcaption></figure><p id="5d94" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们将使用这些设置阿尔法的值，并将其传递给决策树分类器的<strong class="jl hj"> ccp阿尔法</strong>参数。通过循环遍历<strong class="jl hj">阿尔法</strong>数组，我们将找到数据集的训练和测试部分的准确性。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lh"><img src="../Images/ce45f6355f72d4e3cd8ccae5148586bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*uA0d1pnKK_sZBjeu81yvNw.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">循环遍历alphas并为相应的训练和测试精度绘制线图的代码，</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es li"><img src="../Images/4de10624fb79f7a5a54ce5b3497ec415.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SN-XnmfFIcU5rkqOcv5kBg.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">精度v/sα</figcaption></figure><p id="f009" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">从上面的图中我们可以看到，在alpha=0.01到0.02之间，我们得到了最大的测试精度。虽然我们的训练精度已经降低到0.8，但我们的模型现在更加一般化，它将在看不见的数据上表现得更好。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lj"><img src="../Images/7a7a4687f4e7e44965d31473d247a229.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bRkKBmbqToRKiaEtKX3z4g.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">α=0.02时的训练和测试精度</figcaption></figure><p id="c2e9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们还可以使用K-fold交叉验证来测试我们的模型，而不是使用训练测试分割。这将让我们更好地了解我们的模型在看不见的数据上的表现。</p><p id="9bc3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果你想了解成本复杂性修剪背后的数学原理，点击<a class="ae kv" href="https://online.stat.psu.edu/stat508/lesson/11/11.8/11.8.2" rel="noopener ugc nofollow" target="_blank">这里</a>。点击<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" rel="noopener ugc nofollow" target="_blank">此处</a>查看决策树的scikit-learn文档。</p></div><div class="ab cl ko kp gp kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="hb hc hd he hf"><p id="cbfd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">你可以在<a class="ae kv" href="https://github.com/iasarthak/My-Data-Logs/blob/master/Concepts/Cost%20Complexity%20Pruning.ipynb" rel="noopener ugc nofollow" target="_blank">我的GitHub </a>上找到笔记本，仔细看看我都做了些什么。<br/>另外，在<a class="ae kv" href="https://www.linkedin.com/in/iasarthak/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与我联系，让我们讨论数据！</p><p id="e247" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">感谢你坚持到最后！^_^ </p><p id="c0b5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">另外，请给我一个中等的。这将激励我经常为你创造内容。干杯！</p></div></div>    
</body>
</html>