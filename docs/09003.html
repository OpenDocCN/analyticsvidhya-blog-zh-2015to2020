<html>
<head>
<title>Word2Vec, GLOVE, FastText and Baseline Word Embeddings step by step</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Word2Vec、GLOVE、FastText和基线单词逐步嵌入</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/word2vec-glove-fasttext-and-baseline-word-embeddings-step-by-step-d0489c15d10b?source=collection_archive---------1-----------------------#2020-08-22">https://medium.com/analytics-vidhya/word2vec-glove-fasttext-and-baseline-word-embeddings-step-by-step-d0489c15d10b?source=collection_archive---------1-----------------------#2020-08-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="eae3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在之前的讨论中，我们已经逐步了解了标记化的基础。如果你没有看过我的前一篇文章，我强烈推荐你看一下那篇文章，因为首先要理解嵌入，我们需要理解标记化器，这篇文章是前一篇文章的延续。我在下面提供了我关于Tokenizers的帖子的链接。我用一个简单的例子一步一步地解释了这些概念</p><div class="jk jl ez fb jm jn"><a rel="noopener follow" target="_blank" href="/@akash97715/understanding-nlp-keras-tokenizer-class-arguments-with-example-551c100f0cbd"><div class="jo ab dw"><div class="jp ab jq cl cj jr"><h2 class="bd hj fi z dy js ea eb jt ed ef hh bi translated">通过示例了解NLP Keras标记器类参数</h2><div class="ju l"><h3 class="bd b fi z dy js ea eb jt ed ef dx translated">众所周知，准备输入是完成图像和文本深度学习管道中非常重要的一步…</h3></div><div class="jv l"><p class="bd b fp z dy js ea eb jt ed ef dx translated">medium.com</p></div></div><div class="jw l"><div class="jx l jy jz ka jw kb kc jn"/></div></div></a></div><p id="aec6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">还有很多像countvectorizer和TF-IDF这样的方法。但在这两种情况下，单词的上下文都没有得到维护，这导致了非常低的准确性，并且再次基于我们需要选择的不同场景。Countvectorizer和TF-IDF不在讨论范围之内。谈到嵌入，首先我们试图理解嵌入这个词的真正含义。正如我们所知，英语中有超过171，476个单词，每个单词都有不同的意思。如果我们希望根据每个单词的含义在维度中表示171，476个甚至更多的单词，那么这将导致超过30-40万个维度，因为我们之前已经讨论过每个单词都有不同的含义，需要注意的一点是，单词的含义很有可能也会根据上下文发生变化。为了更好地理解基于上下文的含义，我们将看下面的例子</p><p id="e915" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一天一个苹果，医生远离我。由于新冠肺炎·疫情，苹果公司的股票价格正在下跌。我</p><p id="c1f9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在上面的例子中，苹果的意思根据两个不同的上下文而变化。所以如果我们看看不同句子中不同单词的上下文含义，那么互联网上有超过1000亿个单词。因此，为了理解互联网上每一个单词的真实含义，谷歌和facebook开发了许多模型。word2vec和glove是Google开发的，fastText模型是脸书开发的。Word2Vec是针对300万个单词和短语的词汇向量进行训练的，他们从谷歌新闻数据集中训练了大约1000亿个单词，与GLOVE和fastText的情况类似。在这里，嵌入是指所有的单词都根据意思保持的维度，最重要的是根据不同的上下文，我再次重复，根据不同的上下文。</p><p id="810f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这篇文章中，我们将尝试使用python的<strong class="io hj"> gensim </strong>库来理解word2vec、glove、fastText和<strong class="io hj"> Word2Vec的基本编程实现</strong>背后的直觉。手套和快速文本的程序化实现我们会看看其他的帖子。</p><p id="1757" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将试着逐一了解Word2Vec、GLOVE和fastText背后的基本直觉。首先将从Word2vec开始</p><p id="82c7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> Word2Vec: </strong>它背后的主要思想是，你在每个单词上训练一个关于上下文的模型，所以相似的单词会有相似的数字表示。就像一个正常的前馈密集连接神经网络(NN)一样，其中您有一组自变量和一个您试图预测的目标因变量，您首先将您的句子分成单词(标记化)并根据窗口大小创建许多单词对。因此，其中一个组合可以是一对单词，如(' cat '，' purr ')，其中cat是独立变量(X)，而' purr '是我们要预测的目标因变量(Y)。</p><p id="d481" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们通过用随机权重初始化的嵌入层将“猫”输入到神经网络中，并通过softmax层传递它，最终目的是预测“咕噜声”。诸如SGD的优化方法最小化损失函数“(目标单词|上下文单词)”，其寻求最小化给定上下文单词时预测目标单词的损失。如果我们用足够多的历元来这样做，嵌入层中的权重将最终表示单词向量的词汇，即单词在这个几何向量空间中的“坐标”。</p><p id="a103" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> GLOVE: </strong> GLOVE的工作方式与Word2Vec类似。虽然你可以在上面看到Word2Vec是一个预测给定单词上下文的“预测”模型，但GLOVE通过构建一个共现矩阵(单词X上下文)来学习，该矩阵基本上计算一个单词在上下文中出现的频率。因为这将是一个巨大的矩阵，我们分解这个矩阵以获得一个低维的表示。有很多细节，但这是粗略的想法。</p><p id="d28f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> FastText: </strong> FastText与以上2种嵌入有很大不同。虽然Word2Vec和GLOVE将每个单词作为训练的最小单位，但FastText使用n-gram字符作为最小单位。例如，单词vector“apple”可以分解成单独的单词vector单元，如“ap”、“app”、“ple”。使用FastText的最大好处是，它可以为罕见的单词生成更好的单词嵌入，甚至是在训练期间看不到的单词，因为n元字符向量与其他单词共享。这是Word2Vec和GLOVE无法实现的。</p><p id="487d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> Baseline: </strong> Baseline不使用这三种嵌入中的任何一种，或者我可以直接说，标记化的单词被传递到keras嵌入层，但是对于这三种嵌入类型，我们需要将我们的数据集传递到这些预训练的嵌入层，并且这三种嵌入的输出需要在keras嵌入层上传递。keras嵌入层的实现不在本教程的讨论范围之内，我们将在以后的文章中看到，但是流程是怎样的我们需要理解。</p><p id="f071" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们已经学习了Word2Vec、GLOVE和FastText的基础知识，并得出结论，以上3个都是单词嵌入，可以根据不同的用例使用，或者我们可以在用例中使用这3个预先训练好的单词，这样可以提高用例的准确性。现在我们将一步一步地看到word2vec的编程实现</p><p id="e443" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">正如我上面提到的，我们将使用python的gensim库来导入word2vec预训练的嵌入。这可以通过执行下面的代码来完成。我使用google colab来执行我所有帖子中的所有代码。</p><p id="d04b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">从gensim.models导入Word2Vec </strong></p><p id="4308" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在我们将选取一个非常简单的段落，我们需要在其上应用单词嵌入。我在我的帖子里放了一小段，这样就容易理解，如果我们能理解如何在小段中使用嵌入，那么显然我们可以在巨大的数据集上重复同样的步骤。</p><p id="8d07" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将采取“段=足球是一个家庭的团队运动，涉及，在不同程度上，踢球得分。非限定性的，football这个词通常指的是使用这个词的地方最流行的足球形式。俗称足球的运动有协会足球(有些国家称为英式足球)；gridiron足球(特指美式足球或加拿大足球)；澳大利亚规则足球；橄榄球(橄榄球联盟或橄榄球联盟)；还有盖尔足球。这些不同形式的足球在不同程度上有着共同的起源，被称为足球代码。”</p><blockquote class="kd"><p id="1d30" class="ke kf hi bd kg kh ki kj kk kl km jj dx translated">我们可以看到，在上面的段落中，我们有许多停用词和特殊字符，所以我们需要首先删除它们。我们删除是因为我们已经知道，这些都不会给我们的语料库添加任何信息。为了完成这项任务，我们不需要太担心。我们有python中的“NLTK”包，它将删除停用词，还有<strong class="ak">“正则表达式”</strong>包，它将删除特殊字符。详情请参考下面的片段</p></blockquote><figure class="ko kp kq kr ks kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es kn"><img src="../Images/bef0022b7cf91c82fd151bb674bdf6d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CezrgbpaIMdSO_u9jUsflA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">导入NLTK、RE和段落初始化的代码</figcaption></figure><p id="f2aa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们将使用下面的代码删除段落中的所有特殊字符，并将干净的段落存储在文本变量中</p><figure class="le lf lg lh fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ld"><img src="../Images/b964862aef292660b4511720337e3e63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IezI7mz1SxsgV1s7tYQABg.png"/></div></div></figure><p id="e8e2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">应用文本清理后，我们将查看清理前后的段落长度。注意，在清除了我们存储在文本变量中的文本之后。很明显，我们可以看到之前的长度是598，现在清洗后缩短到593</p><figure class="le lf lg lh fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es li"><img src="../Images/db83cca2a7bf35f65a84a9a5e459a569.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-KCAUQRbE4h3KXjwA45GNg.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">清洗前后的单词长度</figcaption></figure><p id="1c79" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在我们将使用下面的代码将单词转换成句子并存储在列表中。我们可以清楚地看到,“sent_tokenize”方法将593个单词转换成了4个句子，并将其存储在list中，基本上我们得到了输出的句子列表。“sent_tokenize”已使用。作为在句子中分割单词的标记</p><figure class="le lf lg lh fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lj"><img src="../Images/f6b9718f9cda35b60d6322df6469da4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j_maQfj2oKn7p-zi83bosA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">单词到句子的转换</figcaption></figure><p id="21b5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在我们将使用下面的代码把这个句子列表转换成单词列表。句子列表被转换成单词列表并存储在另一个列表中。</p><figure class="le lf lg lh fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es lk"><img src="../Images/e548724063c38d4ad8afc57d97952576.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C0siEAy6jwBuLc4BnL1AYA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">句子列表到单词列表</figcaption></figure><p id="e405" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们得到了单词列表，现在我们将使用下面的代码片段从单词列表中删除所有停用词，如is、am、are等。我们可以比较前面代码和下面代码的输出片段，我们会清楚地看到不同之处，像' is '，' a '等停用词都从句子中删除了</p><figure class="le lf lg lh fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ll"><img src="../Images/df34a8eed52b00463bd074e8464a142c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ip0QqFQXt4dvkjMPRGNZ0g.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">停用词移除</figcaption></figure><p id="2f6c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在我们可以在上面准备好的单词上应用word2vec嵌入了。Word2vec是我们已经从python的gensim库中导入的一个类。现在，我们将把预处理过的单词传递给word2vec类，并在将单词传递给word2vec类时指定一些属性。下面列出了一些重要的属性</p><ol class=""><li id="9b9d" class="lm ln hi io b ip iq it iu ix lo jb lp jf lq jj lr ls lt lu bi translated"><strong class="io hj"> min_counts:指定相似词出现的最小次数。一般来说，我们通常指定为2和3，这意味着word2vec将保留相同的单词2或3次，如果该单词出现超过3次，它将从我们将作为输入传递的单词列表中删除</strong></li><li id="9dd9" class="lm ln hi io b ip lv it lw ix lx jb ly jf lz jj lr ls lt lu bi translated"><strong class="io hj">尺寸:这也是我们需要牢记的最重要的属性之一。Size指定我们想要为一个单词指定的维度空间。简单地说，它意味着我们之前讨论过的那个词的封闭的相似维度。默认情况下，其值为100。我们可以改变这个值。这里100表示它将为单词分配100个向量。我们将在下面的片段中看到细节。</strong></li></ol><p id="3f14" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在下面的代码片段中，我们从Word2Vec类实例创建了一个模型对象，并且我们将min_count赋值为1，因为我们的数据集非常小，我的意思是它只有几个单词。大小我们已经指定为10，所以10个向量即维度将被分配给Word2Vec类中所有传递的单词。我们将在创建的模型对象上使用方法“wv ”,并从下面的单词列表中传递任何单词来检查维度或向量的数量，在我们的例子中是10</p><figure class="le lf lg lh fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ma"><img src="../Images/900a70974cf561d9d6422e028a8f59a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fs-ZFeGKPEp0kkw9Mwijpw.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">Word2vec培训</figcaption></figure><figure class="le lf lg lh fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es mb"><img src="../Images/09c2565a3375c28c455bf1a5b71fcba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ahrTBg3KrJnwmT49vj6pXQ.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">我们使用了单词列表中的单词规则，得到了预期的10个向量的大小</figcaption></figure><p id="916f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在上面的帖子中，我们已经成功地将word2vec预训练单词嵌入应用到我们的小数据集。如果我们理解了这些概念，那么我确信我们能够在更大的数据集上应用同样的概念。为了进行更多的单词嵌入练习，我建议从<strong class="io hj"> UCI机器学习库</strong>获取任何大型数据集，并在该数据集上应用相同的讨论过的概念。在下一篇博客中，我们将尝试理解Keras嵌入层以及更多。如果任何人对我们在本帖中讨论过的话题有任何疑问，请在下面随意评论，我将非常乐意解答您的疑问。</p><h1 id="bd4e" class="mc md hi bd me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my mz bi translated">敬请关注</h1></div></div>    
</body>
</html>