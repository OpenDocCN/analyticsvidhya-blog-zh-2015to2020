<html>
<head>
<title>Learning Attention Mechanism from scratch!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始学习注意力机制！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/learning-attention-mechanism-from-scratch-f08706aaf6b6?source=collection_archive---------1-----------------------#2019-08-05">https://medium.com/analytics-vidhya/learning-attention-mechanism-from-scratch-f08706aaf6b6?source=collection_archive---------1-----------------------#2019-08-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="012f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为<a class="ae jd" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>，毫不夸张！</p><blockquote class="je"><p id="78ff" class="jf jg hi bd jh ji jj jk jl jm jn jc dx translated">“人类感知的一个重要特性是，人们不倾向于一次性完整地处理整个场景。相反，人类选择性地将注意力集中在视觉空间的部分，以在需要的时间和地点获取信息，并随着时间的推移结合来自不同注视的信息，建立场景的内部表示，指导未来的眼球运动和决策”<a class="ae jd" href="https://papers.nips.cc/paper/5542-recurrent-models-of-visual-attention.pdf" rel="noopener ugc nofollow" target="_blank">视觉注意力循环模型</a>，2014</p></blockquote><figure class="jp jq jr js jt ju er es paragraph-image"><div class="er es jo"><img src="../Images/1416aa45e7455b30ccc289c160f74e62.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/1*Z0QWDufMs059RQ9_uJWC6g.gif"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">Gif礼遇:<a class="ae jd" href="https://www.google.co.in/url?sa=i&amp;source=images&amp;cd=&amp;ved=2ahUKEwi8-dKwqOzjAhWDYysKHTloC_EQjRx6BAgBEAU&amp;url=https%3A%2F%2Ftenor.com%2Fview%2Fattention-benedict-cumberbatch-paying-attention-cumberbatch-benedict-gif-5957401&amp;psig=AOvVaw1wuj2FNihn_hG-lxeIdXBA&amp;ust=1565114581105181" rel="noopener ugc nofollow" target="_blank">谷歌</a></figcaption></figure><p id="5ac4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将向你展示注意力是如何实现的。主要的焦点是从一个更大的模型中分离出注意力。这是因为当我们在现实世界模型中实现注意力时，很多注意力都集中在管道数据和处理大量向量上，而不是注意力的概念本身。</p><p id="6e31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kb">我将实现注意力评分以及计算注意力上下文向量。</em>T9】</strong></p><figure class="kd ke kf kg fd ju er es paragraph-image"><div class="er es kc"><img src="../Images/0bcde85bc18477cc8c96dd76520a87d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*7Aye1P2QcS2wMhuSzQzZbA.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated"><a class="ae jd" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">(左)缩放点积注意。(右)我们将在下面计算的多头注意力</a></figcaption></figure><h1 id="d14f" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">注意力评分:</h1><p id="5d87" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">让我们先来看看我们将给予评分函数的输入。我们将假设我们处于解码阶段的第一步。计分函数的第一个输入是解码器的<em class="kb">隐藏状态(假设一个玩具RNN有三个隐藏节点——在现实生活中不可用，但更容易说明)</em></p><pre class="kd ke kf kg fd lk ll lm ln aw lo bi"><span id="6862" class="lp ki hi ll b fi lq lr l ls lt">dec_hidden_state = [5,1,20]</span></pre><p id="6b11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们想象一下这个向量:</p><pre class="kd ke kf kg fd lk ll lm ln aw lo bi"><span id="b2f7" class="lp ki hi ll b fi lq lr l ls lt">%matplotlib inline<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns</span></pre><p id="b4e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们想象一下我们的解码器隐藏状态:</p><pre class="kd ke kf kg fd lk ll lm ln aw lo bi"><span id="d81c" class="lp ki hi ll b fi lq lr l ls lt">plt.figure(figsize=(1.5, 4.5))<br/>sns.heatmap(np.transpose(np.matrix(dec_hidden_state)), annot=True, cmap=sns.light_palette(“purple”, as_cmap=True), linewidths=1)</span></pre><p id="5f57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你会得到这样的结果:</p><figure class="kd ke kf kg fd ju er es paragraph-image"><div class="er es lu"><img src="../Images/1383eee520e89a4af0c361accee23010.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*fRjgU59Uen78hj6xGH9sKg.png"/></div></figure><p id="82f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的第一个评分函数将对单个注释(编码器隐藏状态)进行评分，如下所示:</p><pre class="kd ke kf kg fd lk ll lm ln aw lo bi"><span id="d19c" class="lp ki hi ll b fi lq lr l ls lt">annotation = [3,12,45] #e.g. Encoder hidden state</span></pre><p id="9ca2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们想象一下这个注释:</p><pre class="kd ke kf kg fd lk ll lm ln aw lo bi"><span id="49c4" class="lp ki hi ll b fi lq lr l ls lt">plt.figure(figsize=(1.5, 4.5))<br/>sns.heatmap(np.transpose(np.matrix(annotation)), annot=True, cmap=sns.light_palette(“orange”, as_cmap=True), linewidths=1)</span></pre><figure class="kd ke kf kg fd ju er es paragraph-image"><div class="er es lu"><img src="../Images/ef5e340ea773e86d38936f988cb150f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*_PmzwQchtbBvfPQv6_wy5w.png"/></div></figure><h1 id="3ff9" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">实施:对单个注释评分</h1><p id="9496" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">让我们计算单个注释的点积。</p><blockquote class="lv lw lx"><p id="d606" class="if ig kb ih b ii ij ik il im in io ip ly ir is it lz iv iw ix ma iz ja jb jc hb bi translated">N <!-- --> umPy的点()是这个操作的一个很好的候选</p></blockquote><pre class="kd ke kf kg fd lk ll lm ln aw lo bi"><span id="3531" class="lp ki hi ll b fi lq lr l ls lt">def single_dot_attention_score(dec_hidden_state, enc_hidden_state):<br/> #return the dot product of the two vectors<br/> return np.dot(dec_hidden_state, enc_hidden_state)<br/> <br/>single_dot_attention_score(dec_hidden_state, annotation)</span></pre><p id="95bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结果:927</p><h1 id="7ebe" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">注释矩阵</h1><p id="488b" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">现在让我们来看一下一次给所有的注释打分。为此，这里是我们的注释矩阵:</p><pre class="kd ke kf kg fd lk ll lm ln aw lo bi"><span id="67de" class="lp ki hi ll b fi lq lr l ls lt">annotations = np.transpose([[3,12,45], [59,2,5], [1,43,5], [4,3,45.3]])</span></pre><p id="5db1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">并且可以这样可视化(每一列都是一个编码器时间步长的隐藏状态):</p><pre class="kd ke kf kg fd lk ll lm ln aw lo bi"><span id="722c" class="lp ki hi ll b fi lq lr l ls lt"><br/>ax = sns.heatmap(annotations, annot=True, cmap=sns.light_palette(“orange”, as_cmap=True), linewidths=1)</span></pre><figure class="kd ke kf kg fd ju er es paragraph-image"><div class="er es mb"><img src="../Images/2d425ae21d9cbbee20e2113c8a54adb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*F6Iyk1FhQKokK_vG2lydOQ.png"/></div></figure><h1 id="1526" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">实现:一次给所有注释打分</h1><p id="6123" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">让我们用矩阵乘法一步算出所有注解的分数。让我们继续使用点评分法，但要做到这一点，我们必须转置dec_hidden_state并将其与注释相乘。</p><pre class="kd ke kf kg fd lk ll lm ln aw lo bi"><span id="c7fc" class="lp ki hi ll b fi lq lr l ls lt">def dot_attention_score(dec_hidden_state, annotations):<br/> # return the product of dec_hidden_state transpose and enc_hidden_states<br/> return np.matmul(np.transpose(dec_hidden_state), annotations)<br/> <br/>attention_weights_raw = dot_attention_score(dec_hidden_state, annotations)<br/>attention_weights_raw</span></pre><p id="01c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有了分数，让我们应用softmax:</p><pre class="kd ke kf kg fd lk ll lm ln aw lo bi"><span id="21f2" class="lp ki hi ll b fi lq lr l ls lt">def softmax(x):<br/> x = np.array(x, dtype=np.float128)<br/> e_x = np.exp(x)<br/> return e_x / e_x.sum(axis=0)</span><span id="c58f" class="lp ki hi ll b fi mc lr l ls lt">attention_weights = softmax(attention_weights_raw)<br/>attention_weights</span></pre><h1 id="c14a" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">将分数应用回注释</h1><p id="1adc" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">现在我们已经有了分数，让我们将每个注释乘以它的分数，以更接近注意力上下文向量。这是这个公式的乘法部分(我们将在后面的单元格中处理求和部分)</p><pre class="kd ke kf kg fd lk ll lm ln aw lo bi"><span id="25d9" class="lp ki hi ll b fi lq lr l ls lt">def apply_attention_scores(attention_weights, annotations):<br/> # Multiple the annotations by their weights<br/> return attention_weights * annotations</span><span id="5cb7" class="lp ki hi ll b fi mc lr l ls lt">applied_attention = apply_attention_scores(attention_weights, annotations)<br/>applied_attention</span></pre><p id="7792" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们想象一下上下文向量的样子，因为我们已经对它应用了注意力分数:</p><pre class="kd ke kf kg fd lk ll lm ln aw lo bi"><span id="c20a" class="lp ki hi ll b fi lq lr l ls lt"># Let’s visualize our annotations after applying attention to them<br/>ax = sns.heatmap(applied_attention, annot=True, cmap=sns.light_palette(“orange”, as_cmap=True), linewidths=1)</span></pre><figure class="kd ke kf kg fd ju er es paragraph-image"><div class="er es mb"><img src="../Images/c06950068504e28938661e6ad5a771d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*OdcPhUoE_-74B-Sv5dwOWQ.png"/></div></figure><p id="6813" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将这与之前可视化的原始注释进行对比，我们可以看到第二个和第三个注释(列)几乎被删除了。第一个注释保留了它的一些值，第四个注释是最明显的。</p><h1 id="8f88" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">计算注意力上下文向量</h1><p id="906d" class="pw-post-body-paragraph if ig hi ih b ii lf ik il im lg io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">现在，要生成我们的注意力上下文向量，剩下的就是将四列相加，以生成单个注意力上下文向量</p><pre class="kd ke kf kg fd lk ll lm ln aw lo bi"><span id="9c7b" class="lp ki hi ll b fi lq lr l ls lt">def calculate_attention_vector(applied_attention):<br/> return np.sum(applied_attention, axis=1)</span><span id="41ce" class="lp ki hi ll b fi mc lr l ls lt">attention_vector = calculate_attention_vector(applied_attention)<br/>attention_vector</span><span id="8457" class="lp ki hi ll b fi mc lr l ls lt"># Let’s visualize the attention context vector<br/>plt.figure(figsize=(1.5, 4.5))<br/>sns.heatmap(np.transpose(np.matrix(attention_vector)), annot=True, cmap=sns.light_palette(“Blue”, as_cmap=True), linewidths=1)</span></pre><figure class="kd ke kf kg fd ju er es paragraph-image"><div class="er es lu"><img src="../Images/609628c75752e919272aa09e5b5678f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*eEQU3UGwsmAJE2-me2yOnA.png"/></div></figure><p id="c741" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有了上下文向量，我们可以将它与隐藏状态连接起来，并通过隐藏层传递它，以提供这个解码时间步骤的结果。</p><p id="553e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，在这篇博文中，我们学习了注意力评分的所有内容，对单个&amp;所有注释评分，注释矩阵，将分数应用到注释上。我希望，将注意力的实现从一个更大的模型中分离出来，能让注意力的概念变得更加清晰。</p><p id="bc0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">如果想一次把代码全部检查出来，请参考:</strong> <a class="ae jd" href="https://github.com/Garima13a/Attention-Mechanism-Basics" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">注意基础知识</strong> </a></p></div></div>    
</body>
</html>