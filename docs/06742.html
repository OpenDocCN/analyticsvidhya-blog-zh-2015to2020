<html>
<head>
<title>Advanced House Price Prediction Kaggle Competition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高级房价预测卡格尔竞赛</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/advanced-house-price-prediction-kaggle-competition-adefb458d201?source=collection_archive---------8-----------------------#2020-06-01">https://medium.com/analytics-vidhya/advanced-house-price-prediction-kaggle-competition-adefb458d201?source=collection_archive---------8-----------------------#2020-06-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="dcd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">阅读这篇文章的每个人都可能听说过Kaggle，它有广泛的数据集和有大奖的比赛。我也是Kaggle竞赛的新手，没有任何使用它的经验，所以我想试一试，于是我前往Kaggle，在https://www . ka ggle . com/c/house-prices-advanced-regression-techniques上试手</p><p id="e2a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我想让你知道这是我的第一个媒体帖子，所以请原谅我的任何错误。如果你不能理解一些步骤或有任何关于代码的问题，我会在帖子底部提供我的邮件id，你可以通过邮件联系我。</p><p id="9acf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是代码的GitHub链接</p><div class="je jf ez fb jg jh"><a href="https://github.com/kpanwala/Advanced-House-Price-Prediction" rel="noopener  ugc nofollow" target="_blank"><div class="ji ab dw"><div class="jj ab jk cl cj jl"><h2 class="bd hj fi z dy jm ea eb jn ed ef hh bi translated">kpanwala/高级房价预测</h2><div class="jo l"><h3 class="bd b fi z dy jm ea eb jn ed ef dx translated">使用回归技术预测房屋的销售价格-kpanwala/高级房价预测</h3></div><div class="jp l"><p class="bd b fp z dy jm ea eb jn ed ef dx translated">github.com</p></div></div><div class="jq l"><div class="jr l js jt ju jq jv jw jh"/></div></div></a></div><p id="022e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我用过<strong class="ih hj"> Google colab </strong>你可以用任何ide，比如<strong class="ih hj"> Jupyter Notebook。</strong></p><h2 id="64c7" class="jx jy hi bd jz ka kb kc kd ke kf kg kh iq ki kj kk iu kl km kn iy ko kp kq kr bi translated">资料组</h2><figure class="kt ku kv kw fd kx er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ks"><img src="../Images/2a5cdfb353a7d69ccf06ec98d375f34a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nSfwAhI3PFWKPqb4QKKfpg.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">以上是本文中使用的数据集</figcaption></figure><p id="c3b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如图所示，使用了包含4个文件的<strong class="ih hj">房价:高级回归技术</strong>数据集</p><ol class=""><li id="31ae" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">train.csv(里面有你的训练数据)。</li><li id="df42" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">test.csv(包含您的测试数据)。</li><li id="5c8d" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">data_description.txt(包含数据属性的描述，如特定属性拥有哪些不同的类别)。</li><li id="6faa" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">sample_submission.csv(这是一个示例提交文件，让您知道您预测的文件应该具有以下格式)。</li></ol><p id="15e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">压缩文件后，这里是火车数据</p><figure class="kt ku kv kw fd kx"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="557e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练数据中有80列，测试数据中有79列。我们需要使用回归技术预测<strong class="ih hj">销售价格</strong>，并在sample_submission.csv中提交预测值，然后上传到kaggle上。</p><p id="e4f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解决竞争，我发现了3个阶段:</p><ol class=""><li id="4712" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated"><strong class="ih hj">数据预处理。</strong></li><li id="347b" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated"><strong class="ih hj">特征选择</strong>(实际上，特征选择属于数据预处理部分，但只是为了按照步骤执行，我将它作为一个单独的步骤提及)。</li><li id="5242" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated"><strong class="ih hj">回归的算法选择。</strong></li></ol><h1 id="037c" class="lx jy hi bd jz ly lz ma kd mb mc md kh me mf mg kk mh mi mj kn mk ml mm kq mn bi translated"><strong class="ak">数据预处理</strong></h1><p id="b5a5" class="pw-post-body-paragraph if ig hi ih b ii mo ik il im mp io ip iq mq is it iu mr iw ix iy ms ja jb jc hb bi translated">让我们从数据预处理开始，因为我们需要了解数据及其类型。我们可以通过<strong class="ih hj">获得关于我们数据的信息。info() </strong>函数。</p><p id="6ea4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还需要知道我们的数据集中有多少空值，所以我使用了下面的代码，在  <em class="mt"> </em> <strong class="ih hj"> <em class="mt">中有<strong class="ih hj"> <em class="mt"> 19列</em> </strong> <em class="mt"> </em> <strong class="ih hj"> <em class="mt">，在</em> </strong> <em class="mt"> </em> <strong class="ih hj"> <em class="mt">中有</em> </strong> <em class="mt">和</em> <strong class="ih hj"> <em class="mt"> 33列</em><em class="mt"/><strong class="ih hj"><em class="mt"/></strong></strong></em></strong></p><pre class="kt ku kv kw fd mu mv mw mx aw my bi"><span id="2bdf" class="jx jy hi mv b fi mz na l nb nc"><em class="mt">## Count of top 20 Fields of train data having null values</em><br/>train_data.isnull().sum().sort_values(ascending=<strong class="mv hj">False</strong>).iloc[:20]</span><span id="e23c" class="jx jy hi mv b fi nd na l nb nc"><em class="mt">## Count of top 35 Fields of test data having null values</em><br/>test_data.isnull().sum().sort_values(ascending=<strong class="mv hj">False</strong>).iloc[:35]</span></pre><figure class="kt ku kv kw fd kx"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="b10e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，对于空值，我根据自己的经验遵循一些规则:</p><ol class=""><li id="309a" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc lm ln lo lp bi translated">如果您有大量数据，那么您可能会删除具有空值的行，但是请记住，如果某个要预测的类出现得很少，那么不要删除该行，而是尝试替换空值，如步骤4中所定义的。这一步不常用，因为它会导致一些重要数据的删除。</li><li id="188a" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">如果您看到空值属性(列),并且如果某些属性的空值比例大于实际数据，那么您可以确定一个比率，并删除那些空值与总数据的比率大于预定比率的属性(例如，如果您确定比率为0.7，并且您有1000条记录，其中800条记录包含空值，则您可以删除该属性)。(参考下面的3.ipynb)</li><li id="dda8" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">你也可以制作一个分类器来预测那些空值，<a class="ae jd" href="https://towardsdatascience.com/k-nearest-neighbor-python-2fccc47d2a55" rel="noopener" target="_blank"><strong class="ih hj"><em class="mt"/></strong></a>是KNN广泛使用的技术。但这是一个非常累人的过程。</li><li id="ed89" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc lm ln lo lp bi translated">你可能会想如何替换空值，为此我们研究属性的数据类型，如果属性是int或float，那么我们可以用属性的平均值替换空值。</li></ol><pre class="kt ku kv kw fd mu mv mw mx aw my bi"><span id="b7fd" class="jx jy hi mv b fi mz na l nb nc">dataset[col].fillna(dataset[col].mean(),inplace=<strong class="mv hj">True</strong>)</span></pre><p id="ee11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">else如果是宾语，那么我们可以用最常用的来代替。现在这可以用下面的许多方法来完成，我列举了两种方法。两者输出相同。</p><p id="b2c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">I)来自<strong class="ih hj"> sklearn_pandas。分类计算器</strong></p><div class="je jf ez fb jg jh"><a href="https://github.com/scikit-learn-contrib/sklearn-pandas#categoricalimputer" rel="noopener  ugc nofollow" target="_blank"><div class="ji ab dw"><div class="jj ab jk cl cj jl"><h2 class="bd hj fi z dy jm ea eb jn ed ef hh bi translated">sci kit-learn-contrib/sk learn-pandas</h2><div class="jo l"><h3 class="bd b fi z dy jm ea eb jn ed ef dx translated">这个模块提供了Scikit-Learn的机器学习方法和pandas风格的数据框架之间的桥梁。在…</h3></div><div class="jp l"><p class="bd b fp z dy jm ea eb jn ed ef dx translated">github.com</p></div></div><div class="jq l"><div class="ne l js jt ju jq jv jw jh"/></div></div></a></div><p id="8802" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ii)替换为<strong class="ih hj">模式</strong>(参见下面的4.ipynb)</p><pre class="kt ku kv kw fd mu mv mw mx aw my bi"><span id="af12" class="jx jy hi mv b fi mz na l nb nc">dataset[col].fillna(dataset[col].mode()[0],inplace=<strong class="mv hj">True</strong>)</span></pre><figure class="kt ku kv kw fd kx"><div class="bz dy l di"><div class="lv lw l"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated">第二点的片段</figcaption></figure><figure class="kt ku kv kw fd kx"><div class="bz dy l di"><div class="lv lw l"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated">第4rd点的片段</figcaption></figure><p id="9988" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以使用下面的代码得到空值的总数。</p><pre class="kt ku kv kw fd mu mv mw mx aw my bi"><span id="c1b0" class="jx jy hi mv b fi mz na l nb nc"><em class="mt">## Top 5 Fields of test data having null values</em><br/>test_data.isnull().sum().sort_values(ascending=<strong class="mv hj">False</strong>).iloc[:5]</span><span id="80b9" class="jx jy hi mv b fi nd na l nb nc"><em class="mt">## Top 5 Fields of test data having null values</em><br/>train_data.isnull().sum().sort_values(ascending=<strong class="mv hj">False</strong>).iloc[:5]<br/></span></pre><p id="caca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我发现有很多属性在训练数据中有3个类，而在测试数据中或多或少有3个类。这可能会导致不平衡的分类，因为我们最终会将类分成一个热编码器形式。<strong class="ih hj"> category_onehot_multcols </strong>函数用于将作为参数提供的数据集中出现的所有分类变量的<a class="ae jd" href="https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/" rel="noopener ugc nofollow" target="_blank">变成一个热编码器</a>的形式。另外<a class="ae jd" href="https://www.geeksforgeeks.org/ml-dummy-variable-trap-in-regression-models/" rel="noopener ugc nofollow" target="_blank">虚拟变量陷阱</a>也被处理。</p><pre class="kt ku kv kw fd mu mv mw mx aw my bi"><span id="f61e" class="jx jy hi mv b fi mz na l nb nc">total= pd.concat([train_data.drop(['SalePrice'],axis=1),test_data]<br/>,axis=0)</span><span id="8fa3" class="jx jy hi mv b fi nd na l nb nc"><br/>## func to convert into one hot encoder form<br/><strong class="mv hj">def</strong> category_onehot_multcols(multcolumns):<br/>    df_final=total<br/>    i=0<br/>    <strong class="mv hj">for</strong> fields <strong class="mv hj">in</strong> multcolumns:<br/>        <br/>        print(fields)<br/>        df1=pd.get_dummies(total[fields],drop_first=<strong class="mv hj">True</strong>)<br/>        <br/>        total.drop([fields],axis=1,inplace=<strong class="mv hj">True</strong>)<br/>        <strong class="mv hj">if</strong> i==0:<br/>            df_final=df1.copy()<br/>        <strong class="mv hj">else</strong>:<br/>            df_final=pd.concat([df_final,df1],axis=1)<br/>        i=i+1<br/>       <br/>    df_final=pd.concat([total,df_final],axis=1) <br/>    <strong class="mv hj">return</strong> df_final</span><span id="93b6" class="jx jy hi mv b fi nd na l nb nc">## passing our total Dataframe for one hot encoder<br/>total=category_onehot_multcols(column)</span><span id="66de" class="jx jy hi mv b fi nd na l nb nc">save_cols=total.columns</span></pre><p id="06c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，当所有分类值都转换为int形式时，我们可以执行最重要的步骤，您应该始终记住这一步，即使用<a class="ae jd" href="https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02" rel="noopener" target="_blank"><strong class="ih hj">standard scaler</strong></a><strong class="ih hj"/>from<strong class="ih hj"/><strong class="ih hj">sk learn . preprocessing</strong><strong class="ih hj">import</strong><strong class="ih hj">standard scaler缩放数据。它有助于在-1到1之间转换数据，我尝试了使用和不使用<strong class="ih hj">标准缩放器</strong>的预测，发现结果差异很大，所以我建议你在回归问题中强制使用它。</strong></p><pre class="kt ku kv kw fd mu mv mw mx aw my bi"><span id="5548" class="jx jy hi mv b fi mz na l nb nc"><strong class="mv hj">from</strong> <strong class="mv hj">sklearn.preprocessing</strong> <strong class="mv hj">import</strong> StandardScaler<br/>x = total.values<br/>x = StandardScaler().fit_transform(x)</span><span id="af05" class="jx jy hi mv b fi nd na l nb nc">x=pd.DataFrame(x,columns=save_cols)<br/></span><span id="7b0f" class="jx jy hi mv b fi nd na l nb nc">## naming columns to ease the operations<br/>cols=[]<br/>for i in range(0,233):<br/>    name = "col"+str(i)<br/>    cols.append(str(name))</span><span id="e17d" class="jx jy hi mv b fi nd na l nb nc">x=pd.DataFrame(x,columns=cols)</span><span id="824e" class="jx jy hi mv b fi nd na l nb nc"># splitting scaled total data into train and test data<br/>train = x.iloc[:1460]<br/>test = x.iloc[1460:]</span><span id="560e" class="jx jy hi mv b fi nd na l nb nc"># getting predicting values i.e SalePrice into "y" and scaling it .. <br/># separately <br/>y = train_data['SalePrice'].values<br/>sc=StandardScaler()<br/>y = pd.DataFrame(sc.fit_transform(y.reshape(-1,1)))<br/>y.columns=['SalePrice']<br/>train = pd.concat([train,y],axis=1)</span></pre><h1 id="3e4b" class="lx jy hi bd jz ly lz ma kd mb mc md kh me mf mg kk mh mi mj kn mk ml mm kq mn bi translated"><strong class="ak">功能选择</strong></h1><p id="116e" class="pw-post-body-paragraph if ig hi ih b ii mo ik il im mp io ip iq mq is it iu mr iw ix iy ms ja jb jc hb bi translated">我使用关联矩阵来查看属性与销售价格(待预测)的关联程度。</p><p id="93c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/Correlation_coefficient" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">相关系数</strong> </a>在统计学中用来衡量两个变量之间的关系有多强。相关系数有几种类型:皮尔逊相关(也叫皮尔逊相关<em class="mt"> R </em>)是线性回归中常用的一种<strong class="ih hj">相关系数</strong>。使用相关系数有一个限制，因为它们<em class="mt">仅用于连续值，而不用于类</em>，所以我们实际上创建了一个热编码器表单，它将其转换为连续形式，以便我们可以对其应用相关系数。</p><p id="829a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相关系数公式用于确定数据之间的关系有多强。这些公式返回一个介于-1和1之间的值，其中:</p><ul class=""><li id="e8b6" class="lh li hi ih b ii ij im in iq lj iu lk iy ll jc nf ln lo lp bi translated">1表示强正相关。</li><li id="6eaa" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc nf ln lo lp bi translated">-1表示强烈的负面关系。</li><li id="1242" class="lh li hi ih b ii lq im lr iq ls iu lt iy lu jc nf ln lo lp bi translated">结果为零表示完全没有关系。</li></ul><p id="9356" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以通过以下代码获得相关系数:</p><pre class="kt ku kv kw fd mu mv mw mx aw my bi"><span id="9688" class="jx jy hi mv b fi mz na l nb nc">train.corr().reset_index()[['index','SalePrice']]</span></pre><figure class="kt ku kv kw fd kx"><div class="bz dy l di"><div class="lv lw l"/></div></figure><p id="d201" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我取了任何系数大于+0.15小于-0.15的属性。这是从0.6到0.15的实验，因为它显示了改进的结果。您还可以看到，我将与销售价格更相关的列存储到了<em class="mt">列</em>中，我将进一步使用这些列<em class="mt"> </em>。</p><p id="f9aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在是为我们的问题陈述选择回归算法的部分，你几乎完成了这一步，因为数据预处理是一个非常令人厌倦的过程<strong class="ih hj"> <em class="mt">根据一些调查数据，科学家花费他们90%的时间来预处理数据。</em> </strong></p><p id="c032" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我使用的算法列表:</p><blockquote class="ng nh ni"><p id="31f6" class="if ig mt ih b ii ij ik il im in io ip nj ir is it nk iv iw ix nl iz ja jb jc hb bi translated"><em class="hi"> 1。</em> <strong class="ih hj"> <em class="hi">人工神经网络</em> </strong>(使用相关性，通过基本数据预处理，1000个历元，得到0.21339左右的误差分数4800秩)。在玩了760个时期的参数后，我得出Ann的分数为0.18528，相当不错。我注意到在每一层都添加漏失层是不可行的，因为它的误差增加了，所以我只添加了一个漏失层，也就是第一层。</p></blockquote><pre class="kt ku kv kw fd mu mv mw mx aw my bi"><span id="179b" class="jx jy hi mv b fi mz na l nb nc">import tensorflow as tf<br/>from tensorflow import keras<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense<br/>from tensorflow.keras.layers import LeakyReLU,PReLU,ELU<br/>from tensorflow.keras.layers import Dropout</span><span id="1f49" class="jx jy hi mv b fi nd na l nb nc"># Initialising the ANN<br/>classifier = Sequential()</span><span id="3942" class="jx jy hi mv b fi nd na l nb nc"># Adding dropout layer<br/>classifier.add(Dropout(0.2))</span><span id="3298" class="jx jy hi mv b fi nd na l nb nc"># Adding the input layer and the first hidden layer<br/>classifier.add(Dense(50, kernel_initializer = 'he_uniform', activation='relu',input_dim = 146))</span><span id="90b0" class="jx jy hi mv b fi nd na l nb nc"># Adding the second hidden layer<br/>classifier.add(Dense(25, kernel_initializer = 'he_uniform', activation='relu'))</span><span id="6b38" class="jx jy hi mv b fi nd na l nb nc"># Adding the third hidden layer<br/>classifier.add(Dense(50, kernel_initializer = 'he_uniform', activation='relu'))</span><span id="fa71" class="jx jy hi mv b fi nd na l nb nc"># Adding the output layer<br/>classifier.add(Dense(1, kernel_initializer = 'he_uniform', use_bias=True))</span><span id="e23f" class="jx jy hi mv b fi nd na l nb nc"># Compiling the ANN<br/>classifier.compile(loss=root_mean_squared_error, optimizer='Adamax')</span><span id="aa98" class="jx jy hi mv b fi nd na l nb nc"># Fitting the ANN to the Training set<br/>model_history=classifier.fit(train.values, y.values,validation_split=0.20, batch_size = 10, epochs = 760)</span></pre><blockquote class="ng nh ni"><p id="f4e6" class="if ig mt ih b ii ij ik il im in io ip nj ir is it nk iv iw ix nl iz ja jb jc hb bi translated">2.接下来我使用了<a class="ae jd" href="https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab" rel="noopener" target="_blank"> <strong class="ih hj"> <em class="hi">梯度推进算法</em> </strong> </a>但是得到了0.16394的分数在3400左右排名但是改变参数后分数几乎保持不变。</p></blockquote><pre class="kt ku kv kw fd mu mv mw mx aw my bi"><span id="6bba" class="jx jy hi mv b fi mz na l nb nc">from sklearn.ensemble import GradientBoostingRegressor<br/>from sklearn.metrics import mean_squared_error<br/>from sklearn.metrics import mean_absolute_error</span><span id="c24c" class="jx jy hi mv b fi nd na l nb nc">regressor = GradientBoostingRegressor(<br/>    max_depth=10,<br/>    n_estimators=500,<br/>    learning_rate=1.0<br/>)</span><span id="ea3c" class="jx jy hi mv b fi nd na l nb nc">regressor.fit(X_train, y_train)</span><span id="c1ce" class="jx jy hi mv b fi nd na l nb nc">errors = [root_mean_squared_error(y_train, y_pred) for y_pred in regressor.staged_predict(X_train)]<br/>best_n_estimators = np.argmin(errors)</span><span id="85a5" class="jx jy hi mv b fi nd na l nb nc">best_regressor = GradientBoostingRegressor(<br/>    max_depth=2,<br/>    n_estimators=best_n_estimators,<br/>    learning_rate=1.0<br/>)<br/>best_regressor.fit(X_train, y_train)</span><span id="91ac" class="jx jy hi mv b fi nd na l nb nc">y_pred = best_regressor.predict(X_test)</span></pre><blockquote class="ng nh ni"><p id="41d6" class="if ig mt ih b ii ij ik il im in io ip nj ir is it nk iv iw ix nl iz ja jb jc hb bi translated">3.接下来我使用了<a class="ae jd" href="https://www.geeksforgeeks.org/random-forest-regression-in-python/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="hi">随机森林回归器</em> </strong> </a> <em class="hi"> </em>，我获得了0.16282的新分数，这是一个微小的进步。</p></blockquote><pre class="kt ku kv kw fd mu mv mw mx aw my bi"><span id="6d10" class="jx jy hi mv b fi mz na l nb nc">from sklearn.ensemble import RandomForestRegressor <br/> <br/> # create regressor object <br/>regressor = RandomForestRegressor(n_estimators = 500, random_state = 0) <br/> <br/># fit the regressor with x and y data <br/>regressor.fit(X_train, y_train)</span><span id="0cb9" class="jx jy hi mv b fi nd na l nb nc">y_pred = regressor.predict(X_test)</span></pre><blockquote class="ng nh ni"><p id="03b1" class="if ig mt ih b ii ij ik il im in io ip nj ir is it nk iv iw ix nl iz ja jb jc hb bi translated">4.接下来我尝试了<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="hi"> SVR(支持向量回归机)</em> </strong> </a>和<a class="ae jd" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">PCA</strong></a><strong class="ih hj"/>用2，5，10，20个分量，但徒劳无功，它们都是表现最差的模型。</p></blockquote><pre class="kt ku kv kw fd mu mv mw mx aw my bi"><span id="1236" class="jx jy hi mv b fi mz na l nb nc">from sklearn.decomposition import PCA</span><span id="e7b6" class="jx jy hi mv b fi nd na l nb nc">pca = PCA(n_components=20)</span><span id="75b0" class="jx jy hi mv b fi nd na l nb nc">principalComponents = pca.fit_transform(train[columns])</span><span id="b74e" class="jx jy hi mv b fi nd na l nb nc">Df = pd.DataFrame(data = principalComponents, columns = [‘pc1’, ‘pc2’,’pc3',’pc4',’pc5',’pc6',’pc7',’pc8',’pc9',’pc10',’pc11', ‘pc12’,’pc13',’pc14',’pc15',’pc16',’pc17',’pc18',’pc19',’pc20'])</span><span id="350d" class="jx jy hi mv b fi nd na l nb nc">print(‘Explained variation per principal component: {}’.format(pca.explained_variance_ratio_))</span></pre><blockquote class="ng nh ni"><p id="cd16" class="if ig mt ih b ii ij ik il im in io ip nj ir is it nk iv iw ix nl iz ja jb jc hb bi translated">5.接下来我尝试了<a class="ae jd" href="https://www.datacamp.com/community/tutorials/xgboost-in-python" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> XGBoost回归</strong> </a>，我用500个估计量获得了0.14847的分数，这是从随机森林回归量的一个巨大飞跃。我得到了大约2800分，这也是一个巨大的飞跃。</p></blockquote><pre class="kt ku kv kw fd mu mv mw mx aw my bi"><span id="ce0b" class="jx jy hi mv b fi mz na l nb nc"><strong class="mv hj">import</strong> <strong class="mv hj">xgboost</strong> <strong class="mv hj">as</strong> <strong class="mv hj">xgb</strong><br/><br/>xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 1000)<br/><br/>xg_reg.fit(train[columns],y)<br/><br/>y_pred = xg_reg.predict(test[columns])</span></pre><p id="b618" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大约20次提交后，我得到的分数是<em class="mt"> 0.13933 </em>和<em class="mt">2465</em>的等级，有500个评估者，相关值截止值为(+/-) 0.15。我尝试了各种估计值，你也可以尝试不同的值。你也可以尝试许多其他的回归技术，如多项式回归、逻辑回归、线性回归等。</p><p id="c12d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们需要提交具有sample_submission.csv相同格式的提交，这里是它的代码。</p><pre class="kt ku kv kw fd mu mv mw mx aw my bi"><span id="2b78" class="jx jy hi mv b fi mz na l nb nc">## as we have transformed the prediction price between -1 to 1 we <br/>## need to inversely transform it back to original values.<br/>pred=pd.DataFrame(sc.inverse_transform(y_pred))</span><span id="5dc3" class="jx jy hi mv b fi nd na l nb nc">sub_df=pd.read_csv('sample_submission.csv')<br/>datasets=pd.concat([sub_df['Id'],pred],axis=1)<br/>datasets.columns=['Id','SalePrice']<br/><br/>datasets.isnull()<br/>datasets.to_csv('sample_submission.csv',index=<strong class="mv hj">False</strong>)<br/><br/>datasets.head()</span></pre></div><div class="ab cl nm nn gp no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="hb hc hd he hf"><h1 id="fb22" class="lx jy hi bd jz ly nt ma kd mb nu md kh me nv mg kk mh nw mj kn mk nx mm kq mn bi translated"><strong class="ak">非常感谢</strong>阅读这篇文章，如果你欣赏这篇文章背后的努力，那么一定要为它鼓掌。</h1><p id="a6ff" class="pw-post-body-paragraph if ig hi ih b ii mo ik il im mp io ip iq mq is it iu mr iw ix iy ms ja jb jc hb bi translated">如果有疑问或需要改进的地方，这里是我的邮箱id:<strong class="ih hj"><em class="mt">kpanwala33@gmail.com</em></strong>你可以发邮件给我，我会尽快回复你。</p><p id="8c45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<strong class="ih hj">推特</strong>:<a class="ae jd" href="https://twitter.com/PanwalaKalp" rel="noopener ugc nofollow" target="_blank"><em class="mt">https://twitter.com/PanwalaKalp</em></a>上关注我</p><p id="69b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<strong class="ih hj">LinkedIn</strong>:<a class="ae jd" href="https://www.linkedin.com/in/kalp-panwala-72284018a/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/kalp-panwala-72284018a</a>上联系我</p><p id="6ffb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">跟随我上<strong class="ih hj">Github</strong>:<a class="ae jd" href="https://github.com/kpanwala" rel="noopener ugc nofollow" target="_blank">https://github.com/kpanwala</a></p></div></div>    
</body>
</html>