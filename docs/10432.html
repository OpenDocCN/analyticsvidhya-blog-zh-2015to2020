<html>
<head>
<title>Linear Regression with Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-with-tensorflow-161f35a6ef3f?source=collection_archive---------18-----------------------#2020-10-18">https://medium.com/analytics-vidhya/linear-regression-with-tensorflow-161f35a6ef3f?source=collection_archive---------18-----------------------#2020-10-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="863e" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph"><strong class="ak">万物张量流</strong></h2><div class=""/><figure class="ev ex ip iq ir is er es paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="er es io"><img src="../Images/26c177775fc8a9717bf56c34938e5629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*edhGew6oBeJcdUkU"/></div></div><figcaption class="iz ja et er es jb jc bd b be z dx translated">照片由<a class="ae jd" href="https://unsplash.com/@raheemsphoto?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">埃纳耶特·拉希姆</a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="e48a" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">所以，你知道数学，你知道如何回归你的数据，你也知道如何用 Scikit-Learn 或 Statsmodels 在 python 中实现它们，这对于解决任何回归问题可能都是绰绰有余的。但是，不如我们更进一步，使用“非常神奇”的张量流，以非常传统的方式执行线性回归？</p><p id="6622" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">我的名字是 Pranab，这是我(希望)关于数据科学、机器学习和统计的许多文章中的第一篇。通过这篇文章，我想解释我用<a class="ae jd" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>执行线性回归的方法。我将分享每一行代码，同时描述它们背后的逻辑解释。</p><p id="8595" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">我们将使用研究生入学数据集，可以在这里下载<a class="ae jd" href="https://www.kaggle.com/mohansacharya/graduate-admissions" rel="noopener ugc nofollow" target="_blank"/>。我们将建立一个简单的线性回归模型，只有一个预测变量</p><blockquote class="kc kd ke"><p id="7b6f" class="je jf kf jg b jh ji jj jk jl jm jn jo kg jq jr js kh ju jv jw ki jy jz ka kb hb bi translated">这里的目标不是建立一个性能最佳的模型，而是尝试使用 tensorflow 来解释这个模型</p></blockquote></div><div class="ab cl kj kk gp kl" role="separator"><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko kp"/><span class="km bw bk kn ko"/></div><div class="hb hc hd he hf"><p id="8498" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated"><strong class="jg hs">首先要做的事情</strong></p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="c017" class="kz la hi kv b fi lb lc l ld le">import numpy as np<br/>import pandas as pd<br/>import tensorflow.compat.v1 as tf<br/>tf.disable_v2_behavior()</span></pre><p id="8d89" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">因为我们将在这里使用 tensorflow v1，所以我们在第 4 行禁用 v2</p><p id="4965" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated"><strong class="jg hs">我们来看看数据</strong></p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="883a" class="kz la hi kv b fi lb lc l ld le">data = pd.read_csv('Admission_Predict.csv')<br/>data</span></pre><figure class="kq kr ks kt fd is er es paragraph-image"><div class="er es lf"><img src="../Images/373e37047dbe319d5ba541e89f12593b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*FeZN9LDacm8Z4V5qCM2ang.png"/></div><figcaption class="iz ja et er es jb jc bd b be z dx translated">数据</figcaption></figure><p id="a198" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">在接下来的步骤中，我们将建立一个简单的线性回归模型，以'<em class="kf"> GRE 分数'</em>为预测因子，预测'<em class="kf">录取机会'</em>变量。在简单的数学中，我们想建立等式</p><p id="cafe" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated"><strong class="jg hs"> Y = WX + B </strong>，其中</p><p id="9894" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">Y =目标变量<br/> X =预测变量<br/> W =权重<br/> B =偏差项</p><p id="a8df" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated"><strong class="jg hs">声明占位符</strong></p><p id="2c55" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">在 tensorflow 中，我们从定义占位符和变量开始。<br/> <strong class="jg hs">占位符</strong>是 tensorflow 声明用于训练模型的特征的方式。在这种情况下，<strong class="jg hs"> X </strong>和<strong class="jg hs"> Y </strong>是占位符，或者换句话说，分别来自数据集的“<em class="kf"> GRE 分数</em>”和“<em class="kf">录取几率</em>”。在训练模型时，我们在梯度下降的每一步中输入这两个特征的值。稍后我会详细讨论这一点。</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="4237" class="kz la hi kv b fi lb lc l ld le">num_features = 1<br/>x = tf.placeholder(dtype=float, shape=(None, num_features))<br/>y = tf.placeholder(dtype=float)</span></pre><p id="4322" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">shape 属性定义提供给 x 的输入矩阵的形状。在这种情况下，由于我们正在训练单变量线性回归，因此 num_features 设置为 1</p><p id="2d3c" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated"><strong class="jg hs">声明变量</strong></p><p id="79ac" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">tensorflow 中的变量是经过训练的变量。在我们的例子中，<strong class="jg hs"> W </strong>和<strong class="jg hs"> B </strong>。<br/> <em class="kf">这些是 tensorflow 变量，确保不要将它们与 python 变量混淆<br/> </em>变量需要用某个值进行初始化。最常见的做法是给它们分配随机值</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="29da" class="kz la hi kv b fi lb lc l ld le">W = tf.Variable([tf.random_normal(shape=[num_features,1])])<br/>B = tf.Variable(0.05)</span></pre><p id="ec44" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated"><strong class="jg hs">建立关系</strong></p><p id="fe58" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">一旦我们完成了变量和占位符的声明，我们需要建立它们之间的关系，即<strong class="jg hs"> <em class="kf"> Y = WX + B. <br/> </em> </strong>之后，我们定义误差函数(损失函数)和我们用来减少损失的优化器</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="8e2c" class="kz la hi kv b fi lb lc l ld le">#perform matrix multiplication and predicted y<br/>xW = tf.matmul(x, W)<br/>ymodel = tf.add(xW, B)</span><span id="579c" class="kz la hi kv b fi lg lc l ld le">#calculate error aka loss function<br/>error = tf.reduce_sum(tf.square(y-ymodel))<br/>       <br/>#define optimizer. In this case gradient descent<br/>optimizer = tf.train.GradientDescentOptimizer(0.00001)<br/>train = optimizer.minimize(error)</span></pre><p id="107e" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">我们使用 global_variables_initializer 对象来初始化所有变量</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="2e5a" class="kz la hi kv b fi lb lc l ld le">init = tf.global_variables_initializer()  <br/>sess.run(init)</span></pre><p id="6b96" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated"><strong class="jg hs">进给数据和训练</strong></p><p id="6ee1" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">我们首先使用 Sklearn 的 StandardScaler 方法标准化我们的数据，并在嵌套 for 循环的帮助下迭代每个历元的每个示例(也称为<a class="ae jd" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a>),并将它们馈送到我们的占位符‘x’和‘y’</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="0ed3" class="kz la hi kv b fi lb lc l ld le">scaler = StandardScaler()<br/>x_data = scaler.fit_transform(data['GRE Score'])<br/>y_data = data.iloc[:,8]<br/>training_epochs = 500</span><span id="b0b0" class="kz la hi kv b fi lg lc l ld le">for epoch in range(training_epochs): <br/>    for (_x, _y) in zip(x_data, y_data): <br/>        sess.run(train, feed_dict = {x : _x.reshape(1,num_features),     y : _y})</span></pre><p id="63a5" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">让我们也打印出每 10 个时期的损失和变量值</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="bdab" class="kz la hi kv b fi lb lc l ld le">    if (epoch + 1) % 10 == 0: <br/>        c = sess.run(error, feed_dict = {x : x_data, y : y_data}) <br/>        print("Epoch", (epoch + 1), ": cost =", c/x_data.shape[0], "W =", sess.run(W), "b =", sess.run(B))</span></pre><p id="6874" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">运行上面的代码片段后，模型将自我训练，并且应该显示每 10 个时期的损失。下面显示了<strong class="jg hs">损失与时期</strong>的关系图，我们可以清楚地看到收敛</p><figure class="kq kr ks kt fd is er es paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="er es lh"><img src="../Images/11ffb0d8648cce535c5193ec5510857c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3WI1zObN5_SFNpyIzBsJsw.png"/></div></div><figcaption class="iz ja et er es jb jc bd b be z dx translated">损失与时代</figcaption></figure><p id="b2f2" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">为了对新数据进行预测，我们首先保存训练变量<strong class="jg hs"> W </strong>和<strong class="jg hs"> B </strong>，并将我们的测试数据输入等式<strong class="jg hs"> Y = WX + B. </strong></p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="7d71" class="kz la hi kv b fi lb lc l ld le">weight = sess.run(W) <br/>bias = sess.run(B)</span><span id="e363" class="kz la hi kv b fi lg lc l ld le">def predict(x, w, b):<br/>    predictions = np.dot(x,w) +b<br/>    return predictions</span><span id="351f" class="kz la hi kv b fi lg lc l ld le">y_hat = predict(x_data, weight, bias)</span></pre><p id="f9b6" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">我们可以用下图所示的散点图交叉检查我们的预测值和原始值</p><figure class="kq kr ks kt fd is er es paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="er es lh"><img src="../Images/30f5de813bd0d64741cc9a5ef265c639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lQoPRKP4sNu9XWEWSz5zcQ.png"/></div></div><figcaption class="iz ja et er es jb jc bd b be z dx translated">最佳拟合线</figcaption></figure><p id="8a39" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">这就把我们带到了文章的结尾。您可以在这个 github 链接中的<a class="ae jd" href="https://github.com/Pranab1011/tensorflow_for_everything/blob/main/Regression_with_tensorflow.ipynb" rel="noopener ugc nofollow" target="_blank">tensor flow 中找到一个通用代码来执行简单和多重线性回归</a></p><p id="b93c" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">在我的下一篇文章中，我将讨论在 tensorflow 中使用逻辑回归执行分类任务。</p><p id="4c29" class="pw-post-body-paragraph je jf hi jg b jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb hb bi translated">在那之前，谢谢你，注意安全</p><blockquote class="kc kd ke"><p id="2ba3" class="je jf kf jg b jh ji jj jk jl jm jn jo kg jq jr js kh ju jv jw ki jy jz ka kb hb bi translated"><em class="hi"/>我们的建筑也是一个动词<em class="hi">——马修·麦康纳</em></p></blockquote></div></div>    
</body>
</html>