<html>
<head>
<title>Simple and Multiple Linear Regression Maths, Calculating Intercept, coefficients and Implementation Using Sklearn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单和多元线性回归数学，计算截距，系数和使用Sklearn实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/simple-and-multiple-linear-regression-maths-calculating-intercept-coefficients-and-9b05756391b5?source=collection_archive---------5-----------------------#2020-04-08">https://medium.com/analytics-vidhya/simple-and-multiple-linear-regression-maths-calculating-intercept-coefficients-and-9b05756391b5?source=collection_archive---------5-----------------------#2020-04-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/5b0cc172620100f963eabe6c22216ed2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*md7XN9nXzyjcOXmL7LRzpw.png"/></div></div></figure><p id="d920" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性回归是概率/统计领域最古老的方法之一。它的工作原理是拟合因变量和自变量之间的最佳拟合线。让我们熟悉一些常用术语。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jo"><img src="../Images/2e177ebddfdb6bc97421d811a808292f.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/1*zV-BfjwHOOm-WLwUpKF1DA.gif"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">最佳拟合线</figcaption></figure><p id="69cc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">截距(b0): </strong>截距是最佳拟合直线在平面上与y轴相交的地方。</p><p id="912c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">斜率(b1): </strong>斜率是y值如何随着x轴上相应的单位变化而变化的度量(单位=1值偏移)</p><p id="a4c3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们使用一个样本数据集，这样我们就可以理解简单线性回归背后的数学原理。</p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="d5b1" class="kc kd hi jy b fi ke kf l kg kh">x=[1,2,4,3,5] #independent variable<br/>y=[1,3,3,2,5] #dependent variable</span></pre><h2 id="231b" class="kc kd hi bd ki kj kk kl km kn ko kp kq jb kr ks kt jf ku kv kw jj kx ky kz la bi translated"><strong class="ak"> Y=b0+b1*(x) </strong>，其中:</h2><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="27c8" class="kc kd hi jy b fi ke kf l kg kh">b0= interept<br/>b1= coefficint of the independent variable<br/>x= independent variable<br/>Y=target variable</span></pre><p id="4808" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">b0 =平均值(y)-B1 *平均值(x)</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lb"><img src="../Images/46c5218abe7dcad57a2e87880affa172.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VnbA-CB_FF5WvuE0gPy5ug.jpeg"/></div></div></figure><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="10c3" class="kc kd hi jy b fi ke kf l kg kh">consider <strong class="jy hj">xi-mean(x)</strong> as <strong class="jy hj">a</strong>,<strong class="jy hj">yi-mean(y)</strong> as<strong class="jy hj"> b</strong> <br/>So in the numerator we are going to have <strong class="jy hj">a*b</strong> for all n values<br/>And in the denominator we will have <strong class="jy hj">a*a</strong> for all n values</span></pre><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/fedc9d2b8def6b14ec6545582fb7ccd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*qw2eWf0_NzBFl5CFKChnAA.png"/></div></figure><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="cf0c" class="kc kd hi jy b fi ke kf l kg kh">So according to the formula for b1, the value of b1=8/10 is 0.8<br/>b0=mean(y)-b1*(mean(x))<br/>b0=2.8-0.8*(3)<br/>b0=0.4</span></pre><p id="daaf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们有了X和Y值的线性回归方程。</p><p id="05ad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Y=0.4+0.8(X)</p><p id="bb97" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们替换X的一些值来检查我们的预测值。</p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="0e54" class="kc kd hi jy b fi ke kf l kg kh">##when x=1<br/>Y=04+0.8(1)--&gt;<strong class="jy hj">1.2</strong><br/>##when x=2<br/>Y=0.4+0.8(2)--&gt;<strong class="jy hj">2</strong><br/>##when x=4<br/>Y=0.4+0.8(4)--&gt;<strong class="jy hj">3.6</strong><br/>##when x=3<br/>Y=0.4+0.8(3)--&gt;<strong class="jy hj">2.8</strong><br/>##when x=5<br/>Y=0.4+0.8(5)--&gt;<strong class="jy hj">4.4</strong></span><span id="e731" class="kc kd hi jy b fi ld kf l kg kh"><strong class="jy hj">0.692</strong>(RMSE metric)</span></pre><p id="6b96" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我创建了一个Python程序来计算简单线性回归的截距、斜率和预测。</p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="6f55" class="kc kd hi jy b fi ke kf l kg kh">def slope(x1,y1):<br/>    a11=[]<br/>    b11=[]<br/>    c11=[]<br/>    d11=[]<br/>    a1=[]<br/>    b1=[]<br/>    c1=[]<br/>    d1=[]<br/>    mean_x=sum(x1)/len(x1)<br/>    mean_y=sum(y1)/len(y1)<br/>    for i in x1:<br/>        a1=i-mean_x<br/>        a11.append(round(a1,2))<br/>        <br/>    for j in y1:<br/>        b1=j-mean_y<br/>        b11.append(round(b1,2))<br/>        <br/>    for i,j in zip(a11,b11):<br/>        c1=i*j<br/>        c11.append(round(c1,2))<br/>        <br/>    for k in a11:<br/>        d1=k*k<br/>        d11.append(round(d1,2))<br/>        <br/>    sflope_l=sum(c11)/sum(d11)<br/>    return sflope_l</span><span id="91d8" class="kc kd hi jy b fi ld kf l kg kh">def intercept(x2,y2):<br/>    mean_x1=sum(x2)/len(x2)<br/>    mean_y1=sum(y2)/len(y2)<br/>    <br/>    intercept=mean_y1-b1*mean_x1<br/>    return round(intercept,2)</span><span id="7615" class="kc kd hi jy b fi ld kf l kg kh">def prediction(b0,b1,x):<br/>    pred=b0+b1*x<br/>    return round(pred,2)</span><span id="cf6d" class="kc kd hi jy b fi ld kf l kg kh">prediction(intercept(x,y),slope(x,y),4)<br/>#returns <strong class="jy hj">3.6</strong></span></pre><p id="9156" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">既然我们知道简单线性回归是如何工作的，那么实现多元线性回归将会非常容易。公式有一些小的变化，我们准备好了。假设我们有两个独立特性<strong class="is hj">年龄</strong>和<strong class="is hj">经验</strong>和一个依赖特性<strong class="is hj">薪水</strong>。</p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="780c" class="kc kd hi jy b fi ke kf l kg kh">Y=b0+b1*x1+b2*x2<br/>where:<br/>b1=Age coefficient<br/>b2=Experience coefficient<br/>#use the same b1 formula(given above) to calculate the coefficients of Age and Experience</span></pre><p id="8c8a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为多元线性回归的计算可能很复杂并且需要很长时间。对于截距和系数的计算，我将使用sklearn线性回归模型。</p><pre class="jp jq jr js fd jx jy jz ka aw kb bi"><span id="224e" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">#implementing multiple Linear Regression Using Sklearn<br/></strong>import pandas as pd<br/>import numpy as np<br/>from sklearn.linear_model import LinearRegression as LR</span><span id="72e5" class="kc kd hi jy b fi ld kf l kg kh">Age=[21,25,28,30,35]<br/>Experience=[1,4,7,9,14]<br/>Salary=[2000,4000,8000,10000,20000]</span><span id="01e0" class="kc kd hi jy b fi ld kf l kg kh">data=pd.DataFrame({<br/>    'Ages':Age,<br/>    'Experiences':Experience,<br/>    'Salary':Salary<br/>})</span><span id="53d5" class="kc kd hi jy b fi ld kf l kg kh">X=data.iloc[:,data.columns!='Salary']<br/>Y=data.Salary</span><span id="c929" class="kc kd hi jy b fi ld kf l kg kh">model=LR()<br/>model.fit(X,Y)</span><span id="1de0" class="kc kd hi jy b fi ld kf l kg kh">model.intercept_<br/><strong class="jy hj">70962.26415094335</strong></span><span id="b002" class="kc kd hi jy b fi ld kf l kg kh">model.coef_<br/><strong class="jy hj">-3528.30188679,5132.0754717</strong></span><span id="5fc0" class="kc kd hi jy b fi ld kf l kg kh">def predictions_multiple(x,y):<br/>    pred=70962.26415094335+(-3528.30188679*x)+(5132.0754717*y)<br/>    return round(pred,0)</span><span id="b33d" class="kc kd hi jy b fi ld kf l kg kh">predictions_multiple(21,1)<br/><strong class="jy hj">#2000.0</strong></span><span id="1e39" class="kc kd hi jy b fi ld kf l kg kh">predictions_multiple(25,4)<br/><strong class="jy hj">#3283.0</strong></span><span id="d7d2" class="kc kd hi jy b fi ld kf l kg kh">predictions_multiple(28,7)<br/><strong class="jy hj">#8094.0</strong></span><span id="5cd8" class="kc kd hi jy b fi ld kf l kg kh">predictions_multiple(30,9)<br/><strong class="jy hj">#11302.0</strong></span><span id="7e68" class="kc kd hi jy b fi ld kf l kg kh">predictions_multiple(35,14)<br/><strong class="jy hj">#19321.0</strong></span></pre><p id="38e1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">好了，伙计们，我们现在对线性回归背后的数学有了一个很好的了解。这是一个非常有效的模型，易于实现和理解。为了使预测更加准确，需要执行一些预处理技术来减少误差，并帮助模型学习从输入到输出的基本映射函数。以下是为线性回归准备数据的一些提示:</p><ul class=""><li id="2ed0" class="le lf hi is b it iu ix iy jb lg jf lh jj li jn lj lk ll lm bi translated"><strong class="is hj">线性假设:</strong>模型受益于从属&amp;独立特征之间的线性关系。我们可以使用<strong class="is hj">对数变换对数据进行线性变换。</strong></li><li id="9b07" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated"><strong class="is hj">移除共线性</strong>:如果存在多重共线性，线性回归将过度拟合模型。考虑移除最独立的相关特征。</li><li id="70da" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated"><strong class="is hj">去除噪声:</strong>去除数据中的异常值，适当清理数据。从输出变量中移除异常值会有所帮助。</li><li id="cf20" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated">重新调整输入:模型通常会做出更可靠的预测，如果我们<strong class="is hj">标准化</strong>或者<strong class="is hj">规范化</strong>数据的话。</li><li id="7f1a" class="le lf hi is b it ln ix lo jb lp jf lq jj lr jn lj lk ll lm bi translated"><strong class="is hj">高斯分布:</strong>模型受益于高斯分布，尝试使用<strong class="is hj">对数</strong>或<strong class="is hj"> box-cox </strong>变换。</li></ul><p id="83cb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你觉得这篇文章有帮助或者你认为有改进的空间，请在下面的评论区告诉我。如果您是数据科学领域的新手，请访问我的Github，我有一些很棒的教程和概念可以帮助您入门。</p><p id="d2c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">github:<a class="ae ls" href="https://github.com/nitin689" rel="noopener ugc nofollow" target="_blank">https://github.com/nitin689</a></p><p id="2ba5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">练习线性回归的一些问题:<a class="ae ls" href="https://mathbitsnotebook.com/Algebra1/StatisticsReg/ST2LinRegPractice.html" rel="noopener ugc nofollow" target="_blank">https://mathbitsnotebook . com/代数1/statistics reg/ST 2 linregpractice . html</a></p></div></div>    
</body>
</html>