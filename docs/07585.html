<html>
<head>
<title>CPU / GPU/ TPU — ML perspective</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CPU / GPU/ TPU — ML视角</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/cpu-gpu-tpu-ml-perspective-1f049cd4d43d?source=collection_archive---------14-----------------------#2020-06-30">https://medium.com/analytics-vidhya/cpu-gpu-tpu-ml-perspective-1f049cd4d43d?source=collection_archive---------14-----------------------#2020-06-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/7668bf0cc1315c4b7b5779e22b23f78d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*FOQIzNfA3COKyjTrIqeVZA.jpeg"/></div></figure><p id="627c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">作为一个一直试图即兴发挥学习模型性能的机器学习爱好者，我们都已经达到了性能上限，并开始经历不同程度的处理滞后。</p><p id="6037" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">过去需要几分钟完成较小训练数据集的任务现在开始需要几个小时来训练大数据集。为了解决这些问题，我们必须相应地升级硬件，为此，我们需要了解不同处理单元之间的差异。</p><p id="9f3c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从中央处理器(CPU)开始，它本质上是计算设备的大脑，通过执行控制、逻辑和输入/输出(I/O)操作来执行程序指令。</p><blockquote class="jk"><p id="25a1" class="jl jm hi bd jn jo jp jq jr js jt jj dx translated">CPU用于通用编程问题。</p></blockquote><p id="d328" class="pw-post-body-paragraph im in hi io b ip ju ir is it jv iv iw ix jw iz ja jb jx jd je jf jy jh ji jj hb bi translated">一种被设计用来以通用方式解决每一个计算问题的处理器。内存和缓存被设计为对于任何一般的编程问题都是最优的，并且可以处理不同的编程语言，比如(C、Java、Python)。</p><blockquote class="jz ka kb"><p id="f1fb" class="im in kc io b ip iq ir is it iu iv iw kd iy iz ja ke jc jd je kf jg jh ji jj hb bi translated">CPU一次处理的最小数据单位是一个标量，它是1x1维的数据。</p></blockquote><figure class="kh ki kj kk fd ij er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kg"><img src="../Images/166cb230559dae38b8f94f48f1da6e7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zoab4dRN3dJjqvvdljuaEw.jpeg"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated"><a class="ae kt" href="https://arabhardware.net/wp-content/uploads/2019/06/Scalar-vs-Vector-vs-Tensor.jpg" rel="noopener ugc nofollow" target="_blank">图像来源</a></figcaption></figure><p id="a6d3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在谈论GPU，图形处理单元是许多阅读本文的游戏玩家熟悉的名字。最初主要设计为计算机游戏的专用图形渲染工具，GPU后来得到了增强，以加速照片/视频编辑、动画、研究和其他分析软件等需要用大量数据绘制图形结果的其他软件。</p><blockquote class="jk"><p id="3b2a" class="jl jm hi bd jn jo jp jq jr js jt jj dx translated">CPU最擅长顺序处理单个更复杂的计算，而GPU更擅长并行处理多个但更简单的计算。</p></blockquote><p id="d7d2" class="pw-post-body-paragraph im in hi io b ip ju ir is it jv iv iw ix jw iz ja jb jx jd je jf jy jh ji jj hb bi translated">一般来说，GPU是快速机器学习的更安全的赌注，因为在其核心，数据科学模型训练由简单的矩阵数学计算组成，如果计算可以并行执行，其速度可以大大提高，因此GPU在单个处理器中有数千个ALU，这意味着你可以同时执行数千次乘法和加法。</p><blockquote class="jz ka kb"><p id="f317" class="im in kc io b ip iq ir is it iu iv iw kd iy iz ja ke jc jd je kf jg jh ji jj hb bi translated">它们每个周期可以处理数万次运算，数据的维数一般为1×N个数据单元。</p></blockquote><p id="134f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">但是内存访问的问题仍然存在，因为GPU在成千上万的alu上执行并行计算，所以它也在内存访问上花费了更多的精力。为了克服这个问题，谷歌设计了TPU。</p><p id="dc89" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">张量处理单元(TPU)是一个ASIC处理器，它是一个专用集成芯片，旨在加速使用Tensorflow框架开发的深度学习任务。</p><blockquote class="jk"><p id="5548" class="jl jm hi bd jn jo jp jq jr js jt jj dx translated">它不是一个通用处理器。它只能在上面处理张量流模型。</p></blockquote><p id="47b1" class="pw-post-body-paragraph im in hi io b ip ju ir is it jv iv iw ix jw iz ja jb jx jd je jf jy jh ji jj hb bi translated">TPU解决了内存访问问题。首先，它将参数从存储器加载到乘法器和加法器的矩阵中。之后，TPU从内存中加载数据。每次执行乘法时，结果将传递给下一个乘法器，同时进行求和。因此，输出将是数据和参数之间所有乘法结果的总和。在大规模计算和数据传递的整个过程中，根本不需要任何内存访问。这就是为什么TPU可以在神经网络上实现高计算吞吐量的原因。</p><p id="427f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">没有为TPU开发可用于通用编程的编译程序，因此，在TPU上进行通用编程需要很大的努力。</p><blockquote class="jz ka kb"><p id="50c2" class="im in kc io b ip iq ir is it iu iv iw kd iy iz ja ke jc jd je kf jg jh ji jj hb bi translated">TPU每周期可处理128000次运算，数据的维数为NxN数据单元。</p></blockquote><p id="f446" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">参考资料:</p><ol class=""><li id="4d8f" class="ku kv hi io b ip iq it iu ix kw jb kx jf ky jj kz la lb lc bi translated"><a class="ae kt" href="https://blogs.oracle.com/datascience/cpu-vs-gpu-in-machine-learning" rel="noopener ugc nofollow" target="_blank"> <em class="kc">机器学习中CPU vs GPU</em></a></li><li id="e23b" class="ku kv hi io b ip ld it le ix lf jb lg jf lh jj kz la lb lc bi translated"><a class="ae kt" href="https://cloud.google.com/blog/products/ai-machine-learning/what-makes-tpus-fine-tuned-for-deep-learning" rel="noopener ugc nofollow" target="_blank"> <em class="kc">是什么让TPU针对深度学习进行微调？</em> </a></li><li id="6c4c" class="ku kv hi io b ip ld it le ix lf jb lg jf lh jj kz la lb lc bi translated"><a class="ae kt" href="https://candid.technology/tpu-vs-gpu-vs-cpu-comparison/" rel="noopener ugc nofollow" target="_blank"> <em class="kc"> TPU vs GPU vs CPU </em> </a></li></ol></div></div>    
</body>
</html>