<html>
<head>
<title>How To Handle Imbalanced Classes In Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中如何处理不平衡类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-handle-imbalanced-classes-in-machine-learning-ea4ad2dbde93?source=collection_archive---------11-----------------------#2019-10-02">https://medium.com/analytics-vidhya/how-to-handle-imbalanced-classes-in-machine-learning-ea4ad2dbde93?source=collection_archive---------11-----------------------#2019-10-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2235" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们在印度肝病患者数据集上应用不同的方法，并比较它们的性能。</p><p id="aec8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本帖中，我们将探索scikit中可用的不同分类模型——学习将患者记录分为两类</p><ol class=""><li id="7264" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">肝病患者</li><li id="0b70" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">非肝病患者</li></ol><p id="c15f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原始数据集的链接是<a class="ae jr" href="https://archive.ics.uci.edu/ml/datasets/ILPD+(Indian+Liver+Patient+Dataset)" rel="noopener ugc nofollow" target="_blank">https://archive . ics . UCI . edu/ml/datasets/ILPD+(Indian+Liver+Patient+Dataset)</a></p><p id="0848" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有416份患者记录，只有167份非患者记录，我们稍后会看到这如何影响我们的模型，然后<strong class="ih hj">最后讨论一种最流行也是最直观的方法来处理这样一个倾斜的数据集</strong>，请继续阅读。</p><p id="b2d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为已经有很多帖子可以理解什么是分类问题，它与回归有什么不同，所以我在这里只关注手头的问题，而不进入基本机器学习的细节。</p><p id="dcf7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该数据集中的10个属性指示肝脏的状况，并且我们必须基于这些来预测该人是患者还是健康人。</p><p id="0da9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，让我们从导入和探索我们的数据集开始:-</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="cdd2" class="kb kc hi jx b fi kd ke l kf kg">data=pd.read_csv(‘data.csv’,names=[‘Age’,’Gender’,’TB’,’DB’,’Alkphos’,’Sgpt’,’Sgot’,’TP’,’ALG’,’A/G’,’Class’])<br/>data.head()</span></pre><figure class="js jt ju jv fd ki er es paragraph-image"><div class="er es kh"><img src="../Images/43e6369f3c9faa07c55cbd851fc9201f.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*eLmy7Z6tiPEz7WsGx8SIbQ.jpeg"/></div></figure><p id="962f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所看到的，数据看起来已经非常干净了，这对我们来说很好，我们可以直接进入有趣的部分，而不必花很多时间清理数据，但在继续之前，我们仍然需要解决一些小问题，让我们完成它们。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="8b10" class="kb kc hi jx b fi kd ke l kf kg">data.isnull().any()</span></pre><p id="c106" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这一行代码将告诉我们在任何列中是否有任何丢失的值，运行后我们现在知道在<strong class="ih hj"> A/G </strong>列中有丢失的值，让我们对它们做些什么。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="d11a" class="kb kc hi jx b fi kd ke l kf kg">nan_rows = data[data[‘A/G’].isnull()]<br/>print(nan_rows)</span></pre><figure class="js jt ju jv fd ki er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kl"><img src="../Images/f9fae634d3b98eed6421ff4bdfaa440d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*tY3mviLwUIsiCGuiO_B8dg.jpeg"/></div></div></figure><p id="3da8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，只有4行缺少值。现在，在这里，我们可以并且理想地应该进行缺失值插补(如果你不知道这意味着什么，你可以在towardsdatascience.com上阅读所有相关内容)，但是在这里，由于我们的目的是显示不平衡类对你的模型的影响，我们将简单地删除这些行并继续。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="6631" class="kb kc hi jx b fi kd ke l kf kg">data=data.dropna()</span></pre><p id="bef8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有的列都已经是数字格式，除了“性别”是分类的，让我们把它转换成数字，然后开始建立分类模型。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="a025" class="kb kc hi jx b fi kd ke l kf kg">X=data.iloc[:,0:10]<br/>y=data.iloc[:,-1]<br/>X.Gender = X.Gender.map( {‘Male’:0 , ‘Female’:1} )</span><span id="6118" class="kb kc hi jx b fi kq ke l kf kg">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)</span><span id="6616" class="kb kc hi jx b fi kq ke l kf kg">from sklearn.linear_model import LogisticRegression<br/>classifier = LogisticRegression()<br/>classifier.fit(X_train, y_train)</span><span id="56d6" class="kb kc hi jx b fi kq ke l kf kg">y_pred = classifier.predict(X_test)<br/>from sklearn.metrics import accuracy_score<br/>print("Accuracy :", accuracy_score(y_pred, y_test))</span></pre><p id="4b70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在建立这个简单的逻辑回归模型后，我们得到了大约71%的测试准确率，还不错吧？</p><p id="8ccf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们也试试其他的模型。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="7b83" class="kb kc hi jx b fi kd ke l kf kg">from sklearn.tree import DecisionTreeClassifier<br/>clf = DecisionTreeClassifier(criterion=’gini’)<br/>clf.fit(X_train, y_train)<br/>print(‘Accuracy :’, accuracy_score(clf.predict(X_test), y_test))</span><span id="a69f" class="kb kc hi jx b fi kq ke l kf kg">from sklearn.linear_model import Perceptron<br/>clf2 = Perceptron()<br/>clf2.fit(X_train, y_train)<br/>print('Accuracy :', accuracy_score(clf2.predict(X_test), y_test))</span><span id="7b40" class="kb kc hi jx b fi kq ke l kf kg">from sklearn.ensemble import RandomForestClassifier<br/>clf3 = RandomForestClassifier()<br/>clf3.fit(X_train, y_train)<br/>print('Accuracy :', accuracy_score(clf3.predict(X_test), y_test))</span></pre><p id="da46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">决策树</strong>、<strong class="ih hj">感知器</strong>和<strong class="ih hj">随机森林</strong>分别给我们60%、43%和65%的准确率，不算太好，那么现在我们来看看这里可能出了什么问题，如何处理。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="cfbc" class="kb kc hi jx b fi kd ke l kf kg">data[‘Class’].value_counts()</span></pre><p id="d9c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这行代码将向我们返回类以及每个类中的记录数。这里，类1的记录数为414，类2的记录数为165，表明类1的记录数是类2的两倍多，这是我们的模型在如此小而简单的数据集上表现如此差的原因吗？让我们来找出原因。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="cbd7" class="kb kc hi jx b fi kd ke l kf kg">df_majority = data[data.Class==1]<br/>df_minority = data[data.Class==2]<br/>from sklearn.utils import resample<br/>df_minority_upsampled = resample(df_minority, <br/>                                 replace=True,<br/>                                 n_samples=414,  <br/>                                 random_state=123)</span></pre><p id="bf74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，神奇的事情发生了，我们对数据集进行了重新采样。在这个特殊的例子中，我们使用<strong class="ih hj">上采样</strong>来匹配少数类中的记录数和多数类中的记录数。我们还可以使用<strong class="ih hj">下采样、</strong>，但是我们没有在这里这样做，因为这个数据集已经很小了，进一步减少记录的数量并不是一个好主意。它只在我们处理一个巨大的数据集时使用，其中少数类中的记录数仍然是数千(或更多)。</p><p id="a258" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们看看这是如何影响我们模型的性能的。</p><pre class="js jt ju jv fd jw jx jy jz aw ka bi"><span id="3cb1" class="kb kc hi jx b fi kd ke l kf kg">df_upsampled = pd.concat([df_majority, df_minority_upsampled])]<br/>X=df_upsampled.iloc[:,0:10]<br/>y=df_upsampled.iloc[:,-1]</span><span id="c011" class="kb kc hi jx b fi kq ke l kf kg">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)</span><span id="f98b" class="kb kc hi jx b fi kq ke l kf kg">from sklearn.linear_model import LogisticRegression<br/>classifier = LogisticRegression()<br/>classifier.fit(X_train, y_train)<br/>y_pred = classifier.predict(X_test)<br/>from sklearn.metrics import accuracy_score<br/>print("Accuracy :", accuracy_score(y_pred, y_test))</span><span id="98b3" class="kb kc hi jx b fi kq ke l kf kg">from sklearn.tree import DecisionTreeClassifier<br/>clf = DecisionTreeClassifier(criterion='gini')<br/>clf.fit(X_train, y_train)<br/>print('Accuracy :', accuracy_score(clf.predict(X_test), y_test))</span><span id="a94c" class="kb kc hi jx b fi kq ke l kf kg">from sklearn.linear_model import Perceptron<br/>clf2 = Perceptron()<br/>clf2.fit(X_train, y_train)<br/>print('Accuracy :', accuracy_score(clf2.predict(X_test), y_test))</span><span id="0cbe" class="kb kc hi jx b fi kq ke l kf kg">from sklearn.ensemble import RandomForestClassifier<br/>clf3 = RandomForestClassifier()<br/>clf3.fit(X_train, y_train)<br/>print('Accuracy :', accuracy_score(clf3.predict(X_test), y_test))</span></pre><p id="b7aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">运行所有这些模型后，我们得到的精度如下</p><p id="a386" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归的71%</p><p id="3987" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树占86%</p><p id="07e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感知器67%</p><p id="81b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">85%用于随机森林</p><p id="856c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们看到的，除了逻辑回归，每个模型都比以前表现得更好，只是通过平衡目标类。现在想象一下，如果我们进行适当的特征提取并去除对预测过程没有太大贡献的特征，可以做多少事情。我会让你去尝试，因为我只是想展示不平衡的课程对你的模型性能有多大的影响，以及你如何改善它。</p></div></div>    
</body>
</html>