# 自然语言处理(NLP)，主题演讲和 R，Python 包

> 原文：<https://medium.com/analytics-vidhya/natural-language-processing-nlp-keynotes-and-r-python-packages-3a548f4dac1e?source=collection_archive---------2----------------------->

这是人工智能的一部分，其中任何智能系统都使用自然语言如英语进行交流，并尝试教计算机做人类自然会做的事情。它直接从语言输入中学习，而不依赖于预先确定的公式。你可以从网上读到很多关于这个话题的内容。

这种系统的输入可以是语音或输入文本，然后它将进入 NLP 引擎，该引擎提取数据的含义，做出一些决策，然后产生一些动作，这将使它成为一个完整的人工智能解决方案。

# 2 —你必须知道的关键概念

在这里，我将离开理论上的 NLP 概念，主要关注在日常 NLP 系统编程中使用的关键字。在学习这些之前，理论概念是必须的，但是这超出了本文的范围。

## 2.1 —语料库/语料库

我们将要分析的文本或文本集合，比如蜘蛛侠漫画或关于纳伦德拉·莫迪的文章

## 2.2 —词典

一个有意义的词，但在不同的上下文中可能会有所不同，就像一个词“程序”对于程序经理、开发人员、学校、孩子和政治家来说可能是不同的。理想情况下，我们需要开发我们正在工作的领域的词汇，这将取决于多种因素，如国家，时间，背景等。这就像一个词“一楼”在美国有不同的含义，即地面上的楼层，但在印度是地上的楼层

## 2.3 —令牌/令牌化

给定一个字符序列和一个已定义的文档单元，标记化就是将它分割成称为标记的小块，也许同时丢弃某些字符，如标点符号。

## 2.4 —停用词

通常是任何语言中的常用词，你不想在你的分析中考虑这些词，并且这些词对分析没有太大的帮助，分析这些词是浪费资源

## 2.5 —词干

识别并删除除了一个具有相同含义的词之外的词，所有这些选项只有一个词不会产生太大影响，同时考虑到所有选项都可能是性能和记忆命中，如民主、民主和民主化。主要是从结尾开始的词语删减。

## 2.6 —词汇化

几乎和词干分析一样，但它处理的是词汇意义。因此，删除所有具有相同字典含义的单词，即“好”和“更好”,至少在大多数情况下可以互相替换

## 2.7 —标记

根据词性对语料库中的单词进行标记，这里的标记有其字面意义

## 2.8 —分块

标记告诉你单词是名词、动词、形容词等，但它没有给你任何关于句子或句子中短语的结构的线索，所以将(连接的项目或单词)组合在一起，以便它们可以作为单个概念存储或处理。例如“总理纳伦德拉·莫迪”

## 2.9 —裂缝

从组块中移除不需要的单词被称为组块，你可以使用一些正则表达式来移除这样的单词。

## 2.10 —命名实体识别

识别句子中的命名物体和地点。这些可能包括人员、地点和组织等

## 2.11 —语义

这些想法汇集在一起，以一系列句子的形式形成话语或文本的“意义”。一篇文章的意义叫做它的语义。

## 2.12 — tf-idf

词频—逆词频，旨在衡量某个词对文档集合(或语料库)中的某个文档有多重要，通常在信息检索、文本挖掘和用户建模的搜索中用作加权因子

## 2.13 —条件概率

P(A|B) = P( A ∩ B) / P (B)

已知 B 发生的事件 A 的概率=事件 A 和 B 都发生的概率/事件 B 发生的概率

## 2.14 —贝叶斯定理

事件发生的概率，基于可能与该事件相关的条件的先验知识

P(A|B) = P(B|A) P(A)/ P(B)

## 2.15 —朴素贝叶斯分类器

基于贝叶斯定理和非常流行的文本分类，它就像每个被分类的特征都独立于任何其他特征的值。

后验概率=(条件概率*先验概率)/证据

## 2.16-N 克

n 个单词的序列，如一元、二元、三元等，用于创建单词序列的统计模型

## 2.17 —回退

在计算文本中一个单词的概率时，后退意味着当你遇到一个概率=0 的单词时，你回到 n-1 gram 级别来计算概率

## 2.18 —平滑算法

假设我们已经计算了一些 n 元概率，现在我们正在分析一些文本。当我们遇到一个没见过的单词时会发生什么？我们怎么知道给它分配什么概率呢？我们用平滑算法给它一个概率。

# 3——概率如何让你的生活变得轻松(或艰难)

```
> SentanceStart <- c('You are looking very', "You are looking very",
 "I love India", "I love India")
> SentanceEnd <- c('Handsome', "Pretty", "alot", "much")
> frequency <- c(10, 234, 43,83)
> analysis <- data.frame(SentanceStart,SentanceEnd, frequency)
> analysis

        SentanceStart SentanceEnd frequency
1 You are looking very    Handsome        10
2 You are looking very      Pretty       234
3         I love India        alot        43
4         I love India        much        83
```

现在你想知道在“你看起来很漂亮”之后变得“漂亮”的概率，这是一种概率问题，记住条件概率，即 P(“漂亮”|“你看起来很漂亮”)

这是 NLP 中最简单的概率例子，但是相信我"**这个夜晚是黑暗的，充满了恐怖**"

# 4-在进行任何 NLP 分析之前

好好清理你的数据，考虑以下关键点

1.  删除标点符号
2.  删除控制文本
3.  删除号码
4.  删除非 ASCII 字符
5.  将所有文本转换为小写
6.  删除重复数据
7.  删除不良词语(没有必要)
8.  停止单词过滤
9.  做词干分析、词汇化等(没有必要，因为它可能会删除数据的本质)
10.  将所有干净的数据存储在另一个地方，如 csv，并用于所有的分析目的，这将是主要的性能提升
11.  创建测试和训练数据(这样机器学习就能工作)

# 5 —如何实施

做完这些之后，你就可以进行 NLP 分析了(一切顺利)。这些只是我用过或听说过的几个包，但这只是一桶中的几滴(不是海洋！！！)

## 5.1 — R 编程

TM:R 的文本挖掘包，它几乎拥有 R 支持的所有功能，这是几乎所有其他包的基础包

OpenNLP : OpenNLP 广泛用于 NLP 中最常见的任务，如标记化、词性标注、命名实体识别(NER)、分块等等

RWeka:R 中的 R Weka 包提供了到 Weka 的接口。Weka 是一个由机器学习小组开发的开源软件

Tidytext:使用“dplyr”、“ggplot2”和其他整洁工具进行文字处理和情感分析的文本挖掘。tidy 的输出总是一个带有可任意使用的行名的 data.frame。

## 5.2 — Python 编程

NLTK:自然语言工具包，执行所有可以从 NLP 包中预期的活动，几乎是所有其他包的基础。知道这一点是必须的，但我们可以使用写在它上面的任何其他内容

TextBlob:构建在 NLTK 之上，更容易访问。

SpaCy 是一个新的 NLP 库，旨在快速、简化和生产就绪。

## 5.3 — Java 编程

斯坦福的 CoreNLP 是一个 Java 库，几乎是一切的起点。仍然是大多数 NLP 概念的基础。在 Python 和 R 中都有包装器，命名为“CoreNLP”

我尽了最大努力做到正确，但是请让我知道你的反馈/建议。