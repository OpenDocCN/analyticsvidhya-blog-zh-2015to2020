<html>
<head>
<title>Variational Autoencoders: A generative model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变分自动编码器:一个生成模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/variational-autoencoders-a-generative-model-df3b420c774a?source=collection_archive---------20-----------------------#2020-04-12">https://medium.com/analytics-vidhya/variational-autoencoders-a-generative-model-df3b420c774a?source=collection_archive---------20-----------------------#2020-04-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/dfa605025ce4a28f054b5337a1766976.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4p2NWahKF7kZnI29Wt4-7g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源—<a class="ae iu" href="https://www.jeremyjordan.me/variational-autoencoders/" rel="noopener ugc nofollow" target="_blank">https://www.jeremyjordan.me/variational-autoencoders/</a></figcaption></figure><p id="9eb9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated"><span class="l ju jv jw bm jx jy jz ka kb di">在我的上一篇文章中，我谈到了不同类型的自动编码器，它们的损失函数。在这篇文章中，我将谈论变分自动编码器。它们作为<strong class="ix hj">强大的生成模型</strong>。</span></p><p id="9db0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">同样，只学习输入特征而不学习有用特征的好处是什么？变分自动编码器的好处是，它们作为一个生成模型，这意味着它们可以用来生成具有轻微变化的相似特征的特征。例如，具有人脸的图像可以通过输入图像的面部特征的微小变化来生成。这在用于克服机器学习问题中的小数据集量问题的数据扩充步骤中非常有用。</p><h1 id="28e0" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">可变自动编码器</h1><p id="3e9b" class="pw-post-body-paragraph iv iw hi ix b iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo le jq jr js hb bi translated">变型自动编码器在某种程度上不同于普通的自动编码器，它们通过设计代表其潜在空间<em class="lf">，</em>连续，允许简单的随机采样和插值，因此有助于数据生成任务。</p><p id="b5c4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">他们不将编码输出表示为单个向量，而是将编码输出表示为两个向量:一个均值向量<strong class="ix hj">、μ </strong>，另一个标准差向量<strong class="ix hj">、σ </strong>。</p><p id="d837" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，通过这两个向量对解码器的输入向量进行采样，一个是均值<strong class="ix hj"> μ </strong>，另一个是标准差<strong class="ix hj"> σ </strong>。对于从编码器输出生成的任何采样，期望解码器学习这些特征，并且能够生成与输入图像相似的特征，但具有所需的变化。</p><p id="9947" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">变分自动编码器通过在其编码器和解码器中输出概率来生成概率分布，并试图通过最小化KL散度损失来最小化这两个概率分布之间的差异，KL散度损失是两个概率分布之间差异的度量。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/0a417bf76b314909cfcd1a94020fdf11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nLLC3nrS42oCdzz4hRDmGA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源—<a class="ae iu" href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html" rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/lil-log/2018/08/12/from-auto encoder-to-beta-vae . html</a></figcaption></figure><h1 id="4767" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">损失函数</h1><p id="3c42" class="pw-post-body-paragraph iv iw hi ix b iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo le jq jr js hb bi translated">假设输入特征是<strong class="ix hj"> x </strong>，编码输出是z，那么条件概率可以写成</p><blockquote class="ll lm ln"><p id="2ec2" class="iv iw lf ix b iy iz ja jb jc jd je jf lo jh ji jj lp jl jm jn lq jp jq jr js hb bi translated"><strong class="ix hj">p(z | x)=(p(x | z)* p(z))/p(x)</strong></p></blockquote><p id="88d1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还有另一个类似于p(z|x)的概率分布，比如说q(z|x)。如果它可以被定义为与p(z|x)具有相似的分布。那么这两个概率分布之间的差异可以通过KL发散损失来测量和最小化。</p><blockquote class="ll lm ln"><p id="6b93" class="iv iw lf ix b iy iz ja jb jc jd je jf lo jh ji jj lp jl jm jn lq jp jq jr js hb bi translated"><strong class="ix hj"> minKL(q(z|x)||p(z|x)) </strong></p></blockquote><p id="30d6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上述损失函数可以最小化为-</p><blockquote class="ll lm ln"><p id="78f8" class="iv iw lf ix b iy iz ja jb jc jd je jf lo jh ji jj lp jl jm jn lq jp jq jr js hb bi translated"><strong class="ix hj">E(log p(x | z))—KL(q(z | x)| | p(z))</strong></p></blockquote><p id="2811" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这次都是我这边的。你可以通过下面的链接阅读我之前关于自动编码器的文章</p><div class="lr ls ez fb lt lu"><a rel="noopener follow" target="_blank" href="/@taunkdhaval08/auto-encoders-a-formal-introduction-524ba5a60cd6"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hj fi z dy lz ea eb ma ed ef hh bi translated">自动编码器:正式介绍</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">自动编码器是神经网络，应该学会复制模型的输入。我知道你一定在想…</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">medium.com</p></div></div><div class="md l"><div class="me l mf mg mh md mi io lu"/></div></div></a></div><p id="f4ff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">更多关于KL散度的信息可以在这里阅读—</p><div class="lr ls ez fb lt lu"><a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab dw"><div class="lw ab lx cl cj ly"><h2 class="bd hj fi z dy lz ea eb ma ed ef hh bi translated">库尔贝克-莱布勒散度</h2><div class="mb l"><h3 class="bd b fi z dy lz ea eb ma ed ef dx translated">在数理统计中，Kullback-Leibler散度(也称为相对熵)是衡量一个人如何…</h3></div><div class="mc l"><p class="bd b fp z dy lz ea eb ma ed ef dx translated">en.wikipedia.org</p></div></div><div class="md l"><div class="mj l mf mg mh md mi io lu"/></div></div></a></div></div></div>    
</body>
</html>