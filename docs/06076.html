<html>
<head>
<title>Q-Learning, Expected Sarsa and comparison of TD learning algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">q学习、期望Sarsa和TD学习算法的比较</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/q-learning-expected-sarsa-and-comparison-of-td-learning-algorithms-e4612064de97?source=collection_archive---------2-----------------------#2020-05-11">https://medium.com/analytics-vidhya/q-learning-expected-sarsa-and-comparison-of-td-learning-algorithms-e4612064de97?source=collection_archive---------2-----------------------#2020-05-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="de54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文延续了上一篇文章中的内容，在那篇文章中，我谈到了TD学习并演示了SARSA算法的工作原理。在此基础上，让我们来谈谈另外两个基于TD的算法:<strong class="ih hj"> Q-Learning </strong>和<strong class="ih hj"> Expected Sarsa </strong></p><h1 id="0b25" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">q学习</h1><p id="7a16" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">RL的早期突破之一是开发了称为Q-Learning的非策略TD控制算法，定义如下:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/57051bfe480e8a0d578ff447a219d96b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XoNXosrmdavGbJqhGrPZ8Q.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">q-学习更新方程。(来源:强化学习:萨顿和巴尔托的介绍)</figcaption></figure><p id="b65d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与Sarsa算法不同，Q学习更新方程对下一个状态使用贪婪动作选择来计算TD误差。这有助于该算法的非策略性质，因为不管所遵循的策略(ϵ-greedy等。)，TD误差将总是使用下一状态的最大Q值对。</p><p id="e6a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是Q学习算法。除了更新方程式的微小变化，其他都和Sarsa一样。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kx"><img src="../Images/3612b1fa82e45395e5a40d1bdd1e49a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7AWfjw8YDfoRqnIO71DjiA.png"/></div></div></figure><h1 id="b6fd" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">Python中的q-学习</h1><p id="96a7" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">使用与上一篇文章相同的Gridworld环境，我实现了Q-Learning算法。我做的一个小改动是，现在动作选择策略是<strong class="ih hj"> ϵ-greedy </strong>，而不是使用固定的<strong class="ih hj"> ϵ </strong>。以下是Q-Learning代理:</p><figure class="ki kj kk kl fd km"><div class="bz dy l di"><div class="ky kz l"/></div></figure><p id="271d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在展示Q学习算法的一些结果之前，让我们看另一个算法。然后，我将执行所有3个算法，并比较它们。</p><h1 id="1ee8" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">预期Sarsa</h1><p id="5f21" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">它不是在下一个状态-动作对上取最大值，而是使用期望值，考虑每个动作在当前策略下的可能性。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es la"><img src="../Images/88fd15ea4848eaa02bcb22e53059c748.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PktYu4Xy0SDH43jgDMyokA.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">预期的Sarsa更新等式。</figcaption></figure><p id="133f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给定下一个状态<strong class="ih hj"> Sₜ₊₁ </strong>，该算法确定性地在与预期中Sarsa移动<em class="lb">相同的方向上移动，因此，它被称为<strong class="ih hj">预期Sarsa </strong>。它在计算上比Sarsa更复杂，但反过来，它消除了由于随机选择<strong class="ih hj"> Aₜ₊₁ </strong>而产生的方差。</em></p><figure class="ki kj kk kl fd km"><div class="bz dy l di"><div class="ky kz l"/></div></figure><h1 id="db26" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">比较Sarsa、Q-Learning和Expected Sarsa</h1><p id="0761" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">我对Sarsa实现做了一点小小的改动，使用了ϵ-greedy策略，然后实现了所有3种算法，并使用它们的训练进度和平均分数对它们进行了比较。以下是所有3种算法中使用的参数:</p><ul class=""><li id="5e97" class="lc ld hi ih b ii ij im in iq le iu lf iy lg jc lh li lj lk bi translated"><strong class="ih hj">剧集数量</strong> :100</li><li id="364e" class="lc ld hi ih b ii ll im lm iq ln iu lo iy lp jc lh li lj lk bi translated"><strong class="ih hj"> random_seed </strong> : 10(这使得在测试各种参数时随机动作一致)</li><li id="5efa" class="lc ld hi ih b ii ll im lm iq ln iu lo iy lp jc lh li lj lk bi translated"><strong class="ih hj">伽玛</strong>(折扣系数):0.99</li><li id="744c" class="lc ld hi ih b ii ll im lm iq ln iu lo iy lp jc lh li lj lk bi translated"><strong class="ih hj"> alpha </strong>(更新大小):0.6</li><li id="6706" class="lc ld hi ih b ii ll im lm iq ln iu lo iy lp jc lh li lj lk bi translated"><strong class="ih hj">初始ε</strong>:1</li><li id="8fa4" class="lc ld hi ih b ii ll im lm iq ln iu lo iy lp jc lh li lj lk bi translated"><strong class="ih hj">ε衰减因子</strong> : 0.9(每一集之后，ε减少到前一个ε的0.9倍)</li><li id="2d4d" class="lc ld hi ih b ii ll im lm iq ln iu lo iy lp jc lh li lj lk bi translated"><strong class="ih hj">ε最小值</strong>:0.1(ε永远不会减少到小于0.1，以便即使在后面的情节中也有利于最小探索)</li></ul><p id="ba89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是python脚本，其中所有3种算法都在Gridworld环境中执行并进行比较:</p><figure class="ki kj kk kl fd km"><div class="bz dy l di"><div class="ky kz l"/></div></figure><h2 id="ad18" class="lq jf hi bd jg lr ls lt jk lu lv lw jo iq lx ly js iu lz ma jw iy mb mc ka md bi translated">结果呢</h2><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es me"><img src="../Images/30a463c3f7cf51cb1db9bf470e5212d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PfiVYJT8hnLVYcxj1NIvaA.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">请注意，epsilon值后面的额外字符是一些问题，因为在训练时打印在同一行上。系统基本上会覆盖每一行，在后面的剧集中，分数会降低，平均字符数也会减少，最终会产生一些额外的字符</figcaption></figure><p id="3299" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我用10种不同的种子值进行了测试，在所有测试中，我观察到以下情况:</p><ul class=""><li id="b335" class="lc ld hi ih b ii ij im in iq le iu lf iy lg jc lh li lj lk bi translated">Expected Sarsa在10集的6集中给出了最高的平均分，在其他4集中，在平均分方面处于第二位。</li><li id="c569" class="lc ld hi ih b ii ll im lm iq ln iu lo iy lp jc lh li lj lk bi translated">Q-Learning在平均分数方面排名第二，并且始终是最快到达最佳路径的。</li><li id="54f1" class="lc ld hi ih b ii ll im lm iq ln iu lo iy lp jc lh li lj lk bi translated">在我的测试中，与其他两个相比，Sarsa既不是最快的，也不是最好的。此外，在10次测试中，有2次甚至找不到最佳路径。</li></ul><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mf"><img src="../Images/fdbb9486ddc1bafb6642b31f9e69681e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jx-qmxdXvE8_mF2-C6_QhA.png"/></div></div></figure><p id="a598" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这张图表讲述了一个非常有趣的故事。它告诉我们，即使预期的Sarsa有一个糟糕的开始(开始时绿线的急剧下降)，它也是最快稳定下来的。我们看到，大约在10ᵗʰ事件和20ᵗʰ事件之间，预期的Sarsa开始给出稳定的结果，而其他两个事件仍在经历一些下降。后来，在Q-Learning或Sarsa的台词中也有2-3个下降的例子。</p><p id="3f9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种稳定性可能是由于在计算TD误差时，预期的Sarsa用动作发生的概率来加权每个状态-动作对的Q值。</p><p id="0d8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看起来Expected Sarsa是上述3种算法中最好的算法，但是请记住，在某些情况下，Q-learning和Sarsa都处于领先地位。此外，它也严重依赖于参数的选择。</p><p id="fd5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这一次到此为止。下一集见。继续摇摆！</p></div></div>    
</body>
</html>