<html>
<head>
<title>Personalized Cancer Diagnosis using Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用机器学习的个性化癌症诊断</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/personalized-cancer-diagnosis-3d6f09a6b8c9?source=collection_archive---------1-----------------------#2018-04-03">https://medium.com/analytics-vidhya/personalized-cancer-diagnosis-3d6f09a6b8c9?source=collection_archive---------1-----------------------#2018-04-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="4ff2" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">探索性数据分析和不同的机器学习模型应用于诊断癌性肿瘤。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/0e2faaa43e8684237e88f7964a5596ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1b88wdy0KFYH2ef5"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">照片由<a class="ae jn" href="https://unsplash.com/@drew_hays?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">德鲁·海斯</a>在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="07b0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果你对代码更感兴趣，你可以直接进入这个<a class="ae jn" href="https://github.com/tulasiram58827/Cancer-Diagnosis" rel="noopener ugc nofollow" target="_blank">库</a></p><h1 id="0033" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">癌症基本上是什么？</h1><p id="6e1d" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">我们的身体由数万亿个细胞组成，这些细胞不断死亡和再生。正常情况下，一个细胞分裂，并利用一种叫做DNA的遗传蓝图进行自我复制。偶尔，DNA蓝图有时会被破坏，因此细胞不听身体信号，继续分裂形成肿瘤。</p><h1 id="7bfe" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">我们需要解决的业务问题是什么？</h1><p id="7c94" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">让我们简单讨论一下数据和业务问题，因为理解我们正在解决的业务问题是非常重要的。</p><p id="62a5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">当一名患者似乎患有癌症时，我们从患者身上提取肿瘤样本，并对DNA进行基因测序。一旦测序，一个肿瘤可以有数千种基因突变。简而言之,“突变”是导致癌症的基因的微小变化。更重要的一点是，对于每一个基因，都有与之相关的变异。现在在基因及其变异的帮助下，我们必须分类它属于哪一类(我们总共有9类)。只有部分类属于巨蟹座。</p><p id="6a6e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们更清楚地了解一下工作流程。</p><p id="5bc7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">来源:<a class="ae jn" href="https://www.kaggle.com/c/msk-redefining-cancer-treatment/discussion/35336#198462" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/MSK-重新定义-癌症-治疗/讨论/35336#198462 </a></p><ol class=""><li id="cca5" class="lh li hi jq b jr js ju jv jx lj kb lk kf ll kj lm ln lo lp bi translated">分子病理学家选择他/她想要分析的感兴趣的遗传变异的列表。</li><li id="c98a" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated">分子病理学家在医学文献中寻找与感兴趣的遗传变异有关的证据。</li><li id="1676" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated">最后，分子病理学家花费大量时间分析与每个变异相关的证据，以对它们进行分类。</li></ol><p id="1c3b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这里，通过步骤1和2可以很容易地完成，用更少的时间。但是第三步非常耗时。我们的目标是用机器学习模型取代第三步。</p><p id="331f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，问题陈述是基于来自基于文本的临床文献或研究论文的证据对遗传变异进行分类。</p><p id="1295" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">正如我们所说，我们必须对基因变异进行分类——这意味着这是一个分类问题。因为有9个类别，所以这是一个多类别分类问题。</p><p id="9a92" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">了解问题的业务限制是最重要的。如果我们不知道业务约束，我们训练的模型就不能投入生产。</p><h2 id="3d24" class="lv kl hi bd km lw lx ly kq lz ma mb ku jx mc md kw kb me mf ky kf mg mh la mi bi translated"><strong class="ak">该问题的业务约束:</strong></h2><ol class=""><li id="9255" class="lh li hi jq b jr lc ju ld jx mj kb mk kf ml kj lm ln lo lp bi translated">算法的可解释性是必须的，因为癌症专家应该理解为什么模型被赋予特定的类别，以便他可以向患者解释。</li><li id="7ef9" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated">没有低延迟要求，这意味着患者可以等待结果。由于没有低延迟要求，我们可以应用复杂的机器学习模型。</li><li id="4a4e" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated">错误是非常昂贵的。</li><li id="4931" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated">需要的是属于某个类别的概率，而不是它属于某个特定的类别。</li></ol><p id="3484" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">将现实世界/商业问题映射到机器学习问题:</strong></p><ol class=""><li id="b97a" class="lh li hi jq b jr js ju jv jx lj kb lk kf ll kj lm ln lo lp bi translated">正如我们已经提到的，有9个类别要分类。因此，这是一个多类分类问题。</li><li id="fed2" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated">性能度量:-多类对数损失、混淆矩阵(选择对数损失是因为它实际上使用了概率，这是我们的业务约束)</li></ol><p id="7a4a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">机器学习目标:</strong>预测每个数据点属于9类中每一类的概率。</p><p id="8511" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">机器学习约束:</strong>这些与我之前提到的业务约束相同。</p><p id="e85b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在读取数据后，我们对数据进行预处理，如去除停用词、转换为小写和去除标点符号等。预处理是一个非常重要的阶段。</p><p id="156a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">分割数据:</strong>由于数据本质上不是时间性的，这意味着它不随时间而变化，我们可以随机分割数据用于训练、交叉验证和测试。然后，在分割数据之后，还发现训练和测试数据具有几乎相似的分布，并且从分布来看，很明显数据是不平衡的。</p><div class="iy iz ja jb fd ab cb"><figure class="mm jc mn mo mp mq mr paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/bc6def948dc39af36c580a09618b5a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Tz4-ztm0-HTnIrRvvbSiPw.png"/></div></figure><figure class="mm jc mn mo mp mq mr paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/792dc39c0724c283636703e8e47fc0a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Ep1KnKcDAyVSTeXq6Y2FyA.png"/></div></figure><figure class="mm jc mn mo mp mq mr paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/72d3d45817cfb98ba7964406897d2501.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*jAYl1_FUmwSHUp62vV_LFQ.png"/></div></figure></div><p id="ef6c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">众所周知，对数损失的范围从0到无穷大。所以我们首先定义一个随机模型，如果我们的ML模型的对数损失小于我们的随机模型，那么我们可以认为我们的ML模型是好的。在给我们的随机模型提供数据后，它给出了大约2.5的对数损失。</p><p id="b980" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们甚至检查了精度和召回矩阵，其中对角元素(所有类别的精度和召回)由于随机模型而显得非常低。精度和召回矩阵附后。从上面的分布也可以清楚地看出，1、2、4、7类是多数类。</p><div class="iy iz ja jb fd ab cb"><figure class="mm jc ms mo mp mq mr paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/0f964e391afebe1d612c27cdadf42834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*dSoHfKBEfsv3VHJNepOLPg.png"/></div></figure><figure class="mm jc ms mo mp mq mr paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/0d12c3c732db9179b7ff5f45361cc6b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*XuVY8WCOjHMfWy0f-2D5Ig.png"/></div></figure></div><h2 id="595a" class="lv kl hi bd km lw lx ly kq lz ma mb ku jx mc md kw kb me mf ky kf mg mh la mi bi translated"><strong class="ak">单变量分析:</strong></h2><p id="e6b4" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">我们采用每个特征，并通过各种方式检查它是否对类别标签中的预测有用，以便我们可以使用该特征。如果它没有用，我们可以简单地删除该功能</p><ol class=""><li id="085c" class="lh li hi jq b jr js ju jv jx lj kb lk kf ll kj lm ln lo lp bi translated"><strong class="jq hj">基因特征</strong></li></ol><p id="02df" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们知道，基因是一个分类特征。从中我们观察到有235种独特的基因，其中前50个最常见的基因几乎贡献了75%的数据。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mt"><img src="../Images/0a1d6c88f8d99c35ff3f0456beb10b15.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*eVOrzPcM2l8_MkfoxU041w.png"/></div></div></figure><p id="2bc9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们通过一次热编码和反应编码将基因导入载体。然后，我们建立了一个简单的逻辑回归模型与校准分类器，并适用于基因特征和类别标签。我们发现训练、CV、测试对数损失值大致相同，还发现对数损失值小于2.5(随机分类器值)。因此，我们可以说基因是我们分类的一个重要特征。我们还可以得出结论，基因是稳定的特征，因为CV和测试误差大致等于训练误差。</p><p id="6b09" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> 2。变化特征</strong></p><p id="7964" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这里变异也是一个分类特征，我们观察到2124个变异中有1927个出现在训练数据中，这意味着大多数变异出现一次或两次。</p><p id="09a6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">变化的CDF看起来如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mu"><img src="../Images/e578eccb483d1a91dfee20bdc9e30bc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*rdZ3GkGJohVkoSz2uXZViQ.png"/></div></div></figure><p id="dd10" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">累积分布是直线，这意味着大多数变化在训练数据中出现一次或两次。我们通过一个热编码和响应编码来表征到向量的变化。正如我们之前对基因特征所做的那样，我们建立了一个简单的LR模型，并对其应用数据，发现Train，CV，Test的对数损失值小于随机模型。</p><p id="6f33" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">但训练对数损失与CV、测试对数损失的差异明显大于基因特征，这意味着变异特征是不稳定的。但是由于对数损失小于随机模型，我们仍然使用变化特征，但是要小心，因为它不稳定。</p><p id="7173" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> 3。文本特征</strong></p><p id="e5bd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在文本数据中，总共有53，000个独特的单词存在于训练数据中。我们还观察到，大多数单词出现的次数很少，这在文本数据中很常见。我们通过BOW(一种热编码)和响应编码将文本数据转换成矢量。</p><p id="8813" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">正如我们在以前的案例中所做的那样，我们将其应用于简单模型LR，并且发现Train、CV、Test的对数损失值小于随机模型。从变异系数、测试数据的分布来看，测试特征是一个稳定的特征。</p><p id="8a81" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在通过两种方式组合所有的特征</p><ol class=""><li id="a009" class="lh li hi jq b jr js ju jv jx lj kb lk kf ll kj lm ln lo lp bi translated">一次热编码:发现通过一次热编码，维数是55，517，这是因为文本数据。</li><li id="2eeb" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated">反应编码:发现反应编码的维数为27(每个特征对应9个维度)。</li></ol><h1 id="69fa" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">基线模型</h1><p id="9c3f" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated"><strong class="jq hj">方法一:</strong></p><ol class=""><li id="1282" class="lh li hi jq b jr js ju jv jx lj kb lk kf ll kj lm ln lo lp bi translated"><strong class="jq hj">朴素贝叶斯:</strong></li></ol><p id="fdde" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们知道，对于文本数据，NB模型是一个基线模型。现在，我们将训练数据应用于模型，并使用CV数据来寻找最佳超参数(alpha)</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mv"><img src="../Images/ecaf94874479fea150fcd241fb2e9165.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*uBcREPgROc_Hiotq-AY2sg.png"/></div></figure><p id="d03b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">用最好的阿尔法我们符合模型。然后将测试数据应用于模型，我们发现对数损失值为1.27，远小于随机模型。在这里，我们还发现错误分类的案例总数为39.8%。我们还检查了每个数据的每个类别的概率，并解释了每个点。这是为了检查它为什么随机预测特定的类。我们得出结论，对于错误分类的点，该点属于预测类的概率非常低。从精度和召回率矩阵中发现，来自类别2的大多数点被预测为7。类似地，来自类1的大多数点被预测为4。</p><p id="8da9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">方法2: </strong></p><p id="e861" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> 2。k个最近邻居:</strong></p><p id="753c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">正如我们所知，k-NN模型是不可解释的(这是我们的业务限制)，但我们仍然使用这个模型只是为了找出日志损失值。由于k-NN遭受维数灾难，我们使用响应编码代替一位热编码。将数据应用于模型后，我们获得了最佳超参数(k)</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mw"><img src="../Images/c71043c21a7b5ea593c4c28d9fd1f761.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*SC5VnZZ3rXpEeoxHlJ4GnQ.png"/></div></figure><p id="90c3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">使用最佳k值，我们拟合该模型，并将测试数据应用于该模型。对数损失值为1.002，小于NB模型。但是错误分类点的数量是39.47%(几乎等于NB模型)。在k-nn模型中，发现来自类别2的大多数点预测为7。类似地，来自类别1的大多数点预测为4。</p><p id="3ddb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">方法三:</strong></p><p id="3be0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> 3。逻辑回归:</strong></p><p id="7d00" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">正如我们已经看到的，LR模型对单变量分析非常有效。因此，我们通过获取不平衡数据和平衡数据对LR进行了深入分析。</p><ol class=""><li id="09ec" class="lh li hi jq b jr js ju jv jx lj kb lk kf ll kj lm ln lo lp bi translated"><strong class="jq hj">使用类平衡</strong>:我们也知道LR可以很好地处理高维数据，并且它也是可解释的。因此，我们对较低类别的点进行过采样，并将训练数据应用于模型，并使用CV数据来寻找最佳超参数(λ)</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mw"><img src="../Images/4fba22fb5e416f71171a425d04b6b39f.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*pkTCZWh0IUd3ev0yxISOsg.png"/></div></figure><p id="4272" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们用最佳λ拟合模型，并将测试数据应用于模型。对数损失值为1.048(接近k-nn)。但是错误分类点的数量为34.77%(小于NB和K-nn)。因为LR是可解释的，并且误分类点比其他模型(k-NN和NB)少，所以它比k-NN和NB好。</p><p id="31f4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果没有类平衡，日志丢失和错误分类点会增加。因此，我们使用类平衡。</p><p id="f00e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">方法4: </strong></p><p id="d22b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> 4。SVM: </strong></p><p id="26cc" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们使用线性SVM(具有类平衡)，因为它是可解释的，并且对于高维数据非常有效。RBF核SVM是不可解释的，所以我们不能使用它。现在，我们将训练数据应用于模型，并使用CV数据来寻找最佳超参数(C)</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mw"><img src="../Images/74971ad3518105d18f2369a5a49ebfa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*cCm-p3MNJ_89ZBmVAi_P9Q.png"/></div></figure><p id="8f17" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">使用最佳C我们拟合模型，并将测试数据应用于模型。现在，对数损失值是1.06(接近LR)，比随机模型小得多。这里，错误分类案例的总数是36.47%(超过LR)。因为我们使用了类平衡，所以对于次要类，我们获得了很好的性能。</p><p id="3728" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">接近5: </strong></p><p id="6534" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> 5。随机森林:</strong></p><p id="5f6a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> 5.1)一键编码:</strong>通常情况下，决策树适用于低维数据。这也是可以解释的。通过改变随机森林分类器中基学习器的数量和最大深度，我们得到最佳基学习器=2000和最大深度=10。然后，我们用最佳超参数拟合模型，并将测试数据应用于该模型。得到的对数损失值为1.097(接近LR)，错误分类点的总数为36.84%(大于LR)。</p><p id="8ea2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> 5.2)响应编码:</strong>通过改变随机森林分类器中基学习器的数量和最大深度，我们发现最佳基学习器=100，最大深度=5。然后，我们用最佳超参数拟合模型，发现训练对数损失为0.052，CV对数损失为1.325，这表明即使使用最佳超参数，模型也是过度拟合的。这就是我们不使用RF+响应编码的原因。</p><p id="c1ef" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">方法6: </strong></p><p id="c1e6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> 6。堆积分类器:</strong></p><p id="5219" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们堆叠了三个分类器——LR、SVM、NB，并保留LR作为元分类器。现在，我们将训练数据应用于模型，并使用CV数据来寻找最佳超参数。使用最佳超参数，我们拟合模型，将测试数据应用于模型。对数损失值为1.08，远小于随机模型。在这里，我们还发现错误分类的案例总数为36.2%。在这里，即使我们使用复杂的模型，我们得到的结果几乎类似于LR。此外，我们知道堆叠分类器是不可解释的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mx"><img src="../Images/0be829cc888f9fb201b8cd6c0b0c6400.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vE-9pPg14IAPFSfd_vN_YA.png"/></div></div></figure><p id="c176" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">从上表中可以清楚地看出，RF(响应编码)在train和CV对数损失中有一个剧烈的变化(近20倍)。这意味着模型是过度拟合的，因此，我们移除该模型。在堆积分类器(集合)中，测井损失值几乎与LR+平衡相同。从上表可以看出，LR+平衡适合我们的业务或现实世界的限制，如可解释性和比任何其他模型更好的测井损耗值。</p><p id="96f6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">方法7: </strong></p><p id="fa41" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这之后，现在我还用TF-IDF矢量器(用于特性的一键编码)而不是CountVectorizer进行了检查，结果相当不错。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es my"><img src="../Images/e538869069a035674f50d80dfac36f90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3uEKYGuu_fWbZnvYavxL7g.jpeg"/></div></div></figure><p id="7daa" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们可以看到，与CountVectorizer相比，TF-IDF矢量器的结果稍好一些。在这里，我们还可以看到，与其他算法相比，逻辑回归工作得很好，而带有响应编码的随机森林过拟合，因为在训练和CV损失方面存在巨大差异。</p><p id="8f50" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们再看一个有趣的分析:</p><p id="cae0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">方法8: </strong></p><p id="fa15" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我根据每个词的TF-IDF得分，从排除基因和变异的文本数据中选择了前1000个词。现在，我从文本数据中删除不在前1000个单词中的单词。有趣的是，与上面的分析结果相比，我得到了很好的结果。让我们看一次结果。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mz"><img src="../Images/52e0885c4906ed503c0243e2d9aad684.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C38Ys9MGaVCVSgo6im1ahw.jpeg"/></div></div></figure><p id="2411" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们可以看到结果有了很大的改善。事实上，与其他算法相比，逻辑回归表现良好，而带有响应编码的随机森林像往常一样过度拟合。但是结果有所改善。</p><p id="622e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们可以看到，在所有上述分析中，逻辑回归优于所有算法。因此，我试着对单词和双词应用逻辑回归，而不是只使用单词。然而，结果并不丰硕。</p><p id="ab55" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">接近9: </strong></p><p id="d39e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">逻辑回归(一元和二元模型)</strong></p><p id="2cc7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">列车日志损失:0.83</p><p id="66df" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">CV对数损失:1.17</p><p id="90eb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">测试日志损失:1.19</p><p id="e05c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">错误分类点的百分比:40.7</p><p id="4c18" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最后，在尝试了不同的方法后，原木损失值最终降低到小于1。这是用4克进行的逻辑回归，从1-4克中提取出前2000个特征，然后用class = balanced进行逻辑回归。</p><p id="83f4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">方法10: </strong></p><p id="8166" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">逻辑回归(1-4克)前2000名TF-IDF特征:</p><p id="4f63" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">列车日志损失:0.439</p><p id="8abb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">CV对数损失:0.957</p><p id="3e2d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">测试日志损失:0.982。</p><p id="3f4c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我会继续用新分析更新这个博客。</p><p id="31d9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">有关详细的代码分析，请查看此<a class="ae jn" href="https://github.com/tulasiram58827/Cancer-Diagnosis" rel="noopener ugc nofollow" target="_blank">报告</a>。</p><p id="91c6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如有任何疑问，请联系tulasiram11729@gmail.com</p></div><div class="ab cl na nb gp nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="hb hc hd he hf"><p id="27d2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">感谢您的阅读。请在下面分享您的意见和反馈。</p></div></div>    
</body>
</html>