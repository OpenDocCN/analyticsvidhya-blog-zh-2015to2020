# 最小二乘法线性回归

> 原文：<https://medium.com/analytics-vidhya/linear-regression-using-least-squared-method-88f85b1b3050?source=collection_archive---------19----------------------->

在处理数据的时候，首先要做的就是用回归分析来检验你所拥有的变量之间是否有关系。散点图是可视化变量之间关系的最佳方式。从左到右向上的圆点表示一种积极的关系。这意味着 x 变量的增加将导致 y 变量的增加。从左到右向下的点表示反比关系。

现在重要的是首先检查你的数据中是否有任何可能的关系，散点图将帮助我们进行调查。我们需要导入`**matplotlib.pyplot as plt**`

`plt.scatter( x, y, color=’blue’) // putting data to your scatter diagram`

`plt.show() // This will plot and give you a pop up of a scatter diagram`

本文假设您正在处理简单的线性回归。有了它，你就可以分析你的数据中是否有任何关系。如果这些点靠得很近，这意味着有很强的关系，而分散的点则意味着很弱的关系。如果您得到的点形成了一条水平线，这意味着您的数据中没有关系。

**假设你已经从 sklearn.linear_model 库导入了线性回归，**第一步是获取对象的实例:

`Regression = LinearRegression ()`

现在，你可以像一个物体一样与它互动。其中有一个叫做 **fit** 的方法，分别取 x 和 y 的参数。直观地说，它会将你所有的点(点)绘制成散点图，然后在散点图中找到最符合**点的线。也就是找到一条尽可能接近所有其他点的线，使我们的预测更加准确。**

**拟合**方法使用一种叫做**最小二乘法**的方法来达到这个目的。既然这条线不会是完美的，那就穿过所有的点。它会在某些点上方和其他点下方通过。所有不在直线上的点称为“误差”。线上的点具有正误差，线上的点具有零误差，线下的点具有负误差。所有这些误差的总和必须为零。目标是使误差尽可能小，即接近直线。我们可以通过一种统计方法——最小二乘法来实现这一点，这种方法将计算最小的误差平方和(线外所有点的平方)

**拆分您的数据，我假设您从 sklearn.model_selection 导入了 train_test_split 方法**

```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)
```

train_test_split 方法有四个参数，第一个参数“X”将获取我们用于训练的数据，在这种情况下，它将使用 70%的数据进行训练，并将 X 变量存储在 X_train 中，将 y 变量存储在 y_train(target)中。剩下的 30%将用于测试，它将测试数据中的 X 变量存储在 X_test 中，测试数据中的 y 变量存储在 y_test 中。

根据数据的偏差程度，这将决定您希望分割数据的百分比。另一方面，均衡数据总是最佳的，也就是说；如果你在进行情绪分析，确保你的数据有 50%的正面情绪和 50%的负面情绪。

**随机状态**，重要的是要包括使你的数据一致，现在这个数字完全取决于你，它可以是 4，33，或者任何数字，它将总是产生相同的结果。如果没有包含，每次运行脚本时，它会选择不同的索引，而当有一个数字时，它总是选择相同的索引，也就是说，如果它第一次选择索引 3、5、6 和 9 进行测试，下一次编译时它可能会选择索引 1、2、7 和 10。

现在我们已经有了所有的基础，我们可以使用上面提到的 fit 方法和我们从 train_test_split 方法中得到的数据

`regressor.fit(X_train, y_train)`

这种方法只是计算最接近所有点的直线的斜率和 y 截距。该公式将采用以下形式:

Y = a + bx

现在，我们的算法已经生成了公式，我们可以从预测开始，在纸面上，您可以用一个值来代替 X，在编程术语中，将在我们的回归实例中使用一种称为 predict 的方法，我们可以使用 X_test 数据来查看我们的公式(模型)有多准确

`y_prediction = regressor.predict(X_test)`

要查看结果，您可以打印结果并检查我们的模型有多准确。快乐编码