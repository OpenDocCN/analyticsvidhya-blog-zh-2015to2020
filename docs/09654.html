<html>
<head>
<title>30 Minutes to Understand K-Nearest Neighbours (KNN) in One Article</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 30 分钟理解一篇文章中的 K 近邻(KNN)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95-knn-a1c3571562c1?source=collection_archive---------7-----------------------#2020-09-15">https://medium.com/analytics-vidhya/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95-knn-a1c3571562c1?source=collection_archive---------7-----------------------#2020-09-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="71cb" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">从 KNN 出发潜入 ML</h2></div><h2 id="4cb7" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">内容</h2><ul class=""><li id="fb42" class="jv jw hi jx b jy jz ka kb ji kc jm kd jq ke kf kg kh ki kj bi translated">什么是 K-最近邻？</li><li id="0e9b" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">KNN 是如何运作的</li><li id="9ea6" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">如何应对 k</li><li id="6421" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">Python 中的 KNN:Scikit-Learn &amp; Scratch</li><li id="f9c1" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">KNN 的利弊</li><li id="d4ba" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">改进</li><li id="0160" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">KNN 面试问题和其他材料</li><li id="26e5" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">摘要</li></ul><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es kp"><img src="../Images/d59fbd6510518dea8d86f1982d1cb6db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IA0Aq2fymtcTXKYuGxSYew.jpeg"/></div></div></figure><h1 id="af07" class="lb iy hi bd iz lc ld le jd lf lg lh jh io li ip jl ir lj is jp iu lk iv jt ll bi translated">什么是 K-最近邻</h1><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lm"><img src="../Images/6dac9c0c474fdbb464e43b0d3d78585f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dNwOcofpryp8R4Og.png"/></div></div></figure><p id="7040" class="pw-post-body-paragraph ln lo hi jx b jy lp ij lq ka lr im ls ji lt lu lv jm lw lx ly jq lz ma mb kf hb bi translated">我们都已经熟悉了用 X 来代表<strong class="jx hj">特征</strong>，用 y 来代表<strong class="jx hj">目标</strong>。KNN 是一个<strong class="jx hj">监督学习算法</strong>，即我们有一个目标结果的数据集(X，y)，我们想发现它们之间的关系，这意味着我们想找到一个方程 h(x)，这样 X 就可以用来预测 y</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div class="er es mc"><img src="../Images/8d0072fd95919d6ec70e39fd79c706ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*WgmTbWoVAn2bPotEr6HUxw.png"/></div></figure><p id="280a" class="pw-post-body-paragraph ln lo hi jx b jy lp ij lq ka lr im ls ji lt lu lv jm lw lx ly jq lz ma mb kf hb bi translated">KNN 分类器也是一个基于<strong class="jx hj">非参数</strong>和<strong class="jx hj">实例的</strong>算法。</p><ul class=""><li id="8ffd" class="jv jw hi jx b jy lp ka lr ji md jm me jq mf kf kg kh ki kj bi translated"><strong class="jx hj">非参数化</strong>意味着它不对函数 h(x)做任何显式的假设，从而避免了对数据分布进行不正确建模的风险。比如我们的数据是高度非高斯的，但是我们选择的学习模型是高斯分布的(<a class="ae mg" href="https://zg104.github.io/Naive_bayes#362" rel="noopener ugc nofollow" target="_blank"> LDA </a>)，结果会很不理想。</li><li id="6366" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated"><strong class="jx hj">基于实例的</strong>意味着我们的算法没有真正的学习模型。相反，它选择记住经过训练的实例，并将其用作预测阶段的“知识”。具体来说，这意味着只有当对我们的数据库进行查询时(即，当我们要求它预测给定输入的标签时)，算法才会使用经过训练的实例来给出结果。</li></ul><p id="a9df" class="pw-post-body-paragraph ln lo hi jx b jy lp ij lq ka lr im ls ji lt lu lv jm lw lx ly jq lz ma mb kf hb bi translated">值得注意的是，KNN 的最小训练阶段不仅需要“内存成本”(因为我们在测试过程中必须存储庞大的数据集)，还需要计算成本(因为对给定的观察值进行分类需要整个数据组)。其实这是不可取的。</p><h1 id="41d8" class="lb iy hi bd iz lc ld le jd lf lg lh jh io li ip jl ir lj is jp iu lk iv jt ll bi translated">KNN 是如何运作的</h1><p id="ce18" class="pw-post-body-paragraph ln lo hi jx b jy jz ij lq ka kb im ls ji mh lu lv jm mi lx ly jq mj ma mb kf hb bi translated">在进行分类时，K 近邻算法本质上归结为在给定的“看不见的”K 个最相似的实例中，基于“少数服从多数”的投票(所以也叫投票算法)。</p><p id="7af4" class="pw-post-body-paragraph ln lo hi jx b jy lp ij lq ka lr im ls ji lt lu lv jm lw lx ly jq lz ma mb kf hb bi translated">如何定义相似度？我们基于两个数据点之间的距离度量来定义相似性。欧几里德距离定义如下:</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div class="er es mk"><img src="../Images/579046d20398d85e92c99eedb7368224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*FeBsVApXDixuOqlEPZiYqQ.png"/></div></figure><p id="9676" class="pw-post-body-paragraph ln lo hi jx b jy lp ij lq ka lr im ls ji lt lu lv jm lw lx ly jq lz ma mb kf hb bi translated">但是还有其他的选择比如<strong class="jx hj">曼哈顿</strong>、<strong class="jx hj">切比雪夫</strong>和<strong class="jx hj">海明距离</strong>。</p><p id="9eeb" class="pw-post-body-paragraph ln lo hi jx b jy lp ij lq ka lr im ls ji lt lu lv jm lw lx ly jq lz ma mb kf hb bi translated">给定一个正整数<em class="ml"> K </em>，一个未知的观测值<em class="ml"> x </em>，一个特定的测量方法<em class="ml"> d </em>，KNN 通过以下步骤进行分类</p><ul class=""><li id="1a53" class="jv jw hi jx b jy lp ka lr ji md jm me jq mf kf kg kh ki kj bi translated">该算法遍历整个数据集，以计算<em class="ml"> x </em>与每个训练集中的观察值之间的距离。我们将训练数据集中最接近<em class="ml"> x </em>的<em class="ml"> K </em>点称为 a。注意，为了防止平局，<em class="ml"> K </em>通常是奇数。</li><li id="abd9" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">接下来，它估计每个类别的条件概率，即给定类别标签的点在集合<em class="ml"> A </em>中所占的比例。(注意 I(x)是一个指标函数，当其参数 x 为真时返回 1，否则为 0)</li></ul><figure class="kq kr ks kt fd ku er es paragraph-image"><div class="er es mm"><img src="../Images/a2a52a0d362989b0e99d56a5e169563a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*MKUAPS96g_6i8yIW1dsksw.png"/></div></figure><p id="6945" class="pw-post-body-paragraph ln lo hi jx b jy lp ij lq ka lr im ls ji lt lu lv jm lw lx ly jq lz ma mb kf hb bi translated">最后，我们输入的输入按照最大概率被分配到相应的类别。</p><blockquote class="mn mo mp"><p id="93b1" class="ln lo ml jx b jy lp ij lq ka lr im ls mq lt lu lv mr lw lx ly ms lz ma mb kf hb bi translated"><em class="hi"> KNN 在记忆的训练观察中搜索与新实例最相似的 K 个实例，并将它们最常见的类别分配给新实例。</em></p></blockquote><p id="7e08" class="pw-post-body-paragraph ln lo hi jx b jy lp ij lq ka lr im ls ji lt lu lv jm lw lx ly jq lz ma mb kf hb bi translated">具体算法如下:</p><figure class="kq kr ks kt fd ku"><div class="bz dy l di"><div class="mt mu l"/></div></figure><h1 id="9281" class="lb iy hi bd iz lc ld le jd lf lg lh jh io li ip jl ir lj is jp iu lk iv jt ll bi translated">如何应对 k</h1><p id="e3e3" class="pw-post-body-paragraph ln lo hi jx b jy jz ij lq ka kb im ls ji mh lu lv jm mi lx ly jq mj ma mb kf hb bi translated">那么现在，你应该知道 KNN 算法是如何工作的了，但是<em class="ml"> K </em>的选择还是很关键的。与其他机器学习算法类似，<em class="ml"> K </em>是一个超参数，用于控制决策边界的形状。</p><p id="c98f" class="pw-post-body-paragraph ln lo hi jx b jy lp ij lq ka lr im ls ji lt lu lv jm lw lx ly jq lz ma mb kf hb bi translated">当 K 较小时，我们将“眼睛”局限于给定的预测区域，对分类器的整体分布“视而不见”。它将具有<strong class="jx hj"> </strong>低偏差但高方差(过拟合)的性质。从图形上看，决策边界会更加不均匀。</p><p id="6362" class="pw-post-body-paragraph ln lo hi jx b jy lp ij lq ka lr im ls ji lt lu lv jm lw lx ly jq lz ma mb kf hb bi translated">另一方面，更大的<em class="ml"> K </em>意味着每种预测情境下的平均“投票者”更多(即我们更侧重于全局考虑，每次考虑的数据点更多，模型的泛化能力更强)，因此，对异常值的适应性更高。更大的<em class="ml"> K </em>将具有更平滑的决策边界，这意味着方差更小，但偏差增加(欠拟合)。</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es mv"><img src="../Images/ac9c8fe611d88721f35ad883d896629f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*n4LQN1RJ2OQeah9X.png"/></div></div></figure><p id="b4c4" class="pw-post-body-paragraph ln lo hi jx b jy lp ij lq ka lr im ls ji lt lu lv jm lw lx ly jq lz ma mb kf hb bi translated">当<em class="ml"> k </em>为 1 时，也就是说，在计算出新的测试点与所有训练集的点之间的距离以及排序后，只取最近的<strong class="jx hj">单个</strong>点进行分析。如果这个点属于某个类别，那么我们就断言这个测试点属于这个类别。显然，这太绝对了。我们应该允许更多的点参与比较，以使投票结果更有说服力！</p><figure class="kq kr ks kt fd ku er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es mv"><img src="../Images/52e1b9170ca7f5a941aaf9f4fe7b37d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ZWQ4AAJWl_Yx8jKU.png"/></div></div></figure><p id="b5c3" class="pw-post-body-paragraph ln lo hi jx b jy lp ij lq ka lr im ls ji lt lu lv jm lw lx ly jq lz ma mb kf hb bi translated">当我们将<em class="ml"> k </em>增加到 20 时，我们发现整个决策边界的形状发生了很大的变化，锯齿状的形状减少了，投票点的数量从 1 个增加到 20 个，为了避免过拟合，使模型更加一般化。</p><ul class=""><li id="5cf6" class="jv jw hi jx b jy lp ka lr ji md jm me jq mf kf kg kh ki kj bi translated">k 越小，越容易过拟合；k 越大，越容易欠填。</li><li id="0fb6" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">k 是一个超参数，需要通过交叉验证进行调整。</li></ul><h1 id="067e" class="lb iy hi bd iz lc ld le jd lf lg lh jh io li ip jl ir lj is jp iu lk iv jt ll bi translated">Python 中的 KNN:sk learn &amp; Scratch</h1><figure class="kq kr ks kt fd ku"><div class="bz dy l di"><div class="mw mu l"/></div></figure><h1 id="f3b1" class="lb iy hi bd iz lc ld le jd lf lg lh jh io li ip jl ir lj is jp iu lk iv jt ll bi translated">KNN 的利弊</h1><blockquote class="mn mo mp"><p id="3708" class="ln lo ml jx b jy lp ij lq ka lr im ls mq lt lu lv mr lw lx ly ms lz ma mb kf hb bi translated">赞成的意见</p></blockquote><ul class=""><li id="ef66" class="jv jw hi jx b jy lp ka lr ji md jm me jq mf kf kg kh ki kj bi translated">容易理解</li><li id="7788" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">没有关于数据分布的初步假设</li><li id="1397" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">适用于多类别情况</li></ul><blockquote class="mn mo mp"><p id="6fd8" class="ln lo ml jx b jy lp ij lq ka lr im ls mq lt lu lv mr lw lx ly ms lz ma mb kf hb bi translated">骗局</p></blockquote><ul class=""><li id="2d4a" class="jv jw hi jx b jy lp ka lr ji md jm me jq mf kf kg kh ki kj bi translated">处理大型数据集时的低效计算。</li><li id="4947" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">偏斜数据的效果很差。如果一类数据出现的频率极高，就会严重压制其他类型。投票的时候，我们会忽略他们的存在。</li><li id="7e03" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">处理高维数据时性能不好。在这种情况下，距离度量变得模糊。</li></ul><h1 id="483c" class="lb iy hi bd iz lc ld le jd lf lg lh jh io li ip jl ir lj is jp iu lk iv jt ll bi translated">丰富</h1><ul class=""><li id="84b3" class="jv jw hi jx b jy jz ka kb ji kc jm kd jq ke kf kg kh ki kj bi translated">改善部分分布的一个简单有效的方法是使用“加权投票”。每个<em class="ml"> K </em>邻居的类别乘以与从该点到给定测试点的距离的倒数成比例的权重。这确保了较近的邻居比较远的邻居对最终投票的贡献更大。</li><li id="6ae5" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">改变测量距离的方法。</li><li id="699f" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">将数据归一化，使距离的概念更加明显</li><li id="e6df" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">降维</li><li id="09cc" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated">近似最近邻(例如，使用 k-d 树来存储训练观察值)可以用来减少测试时间。然而，这种方法通常在高维(20+)中表现不佳。如果维数太高，尝试使用区域敏感哈希(LHS)</li></ul><h1 id="4381" class="lb iy hi bd iz lc ld le jd lf lg lh jh io li ip jl ir lj is jp iu lk iv jt ll bi translated">KNN 面试问题和其他材料</h1><blockquote class="mn mo mp"><p id="c7a9" class="ln lo ml jx b jy lp ij lq ka lr im ls mq lt lu lv mr lw lx ly ms lz ma mb kf hb bi translated">面试问题</p></blockquote><ul class=""><li id="7c95" class="jv jw hi jx b jy lp ka lr ji md jm me jq mf kf kg kh ki kj bi translated"><a class="ae mg" href="https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-k-nearest-neighbors-algorithm/" rel="noopener ugc nofollow" target="_blank"> 30 个问题测试一位数据科学家的 K 近邻(kNN)算法</a></li><li id="2a44" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated"><a class="ae mg" rel="noopener" href="/@cornell_data/interview-case-study-1-sampling-methods-and-parameter-changes-4799c580aa42">访谈案例研究# 1:KNN 参数优化统计</a></li><li id="84d8" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated"><a class="ae mg" href="https://www.cnblogs.com/xueyunqing/p/10281656.html" rel="noopener ugc nofollow" target="_blank"> KNN 面试问题(中文版)</a></li></ul><blockquote class="mn mo mp"><p id="4948" class="ln lo ml jx b jy lp ij lq ka lr im ls mq lt lu lv mr lw lx ly ms lz ma mb kf hb bi translated">其他材料</p></blockquote><ul class=""><li id="380a" class="jv jw hi jx b jy lp ka lr ji md jm me jq mf kf kg kh ki kj bi translated"><a class="ae mg" href="https://cs231n.github.io/classification/#nn" rel="noopener ugc nofollow" target="_blank">斯坦福斯<strong class="jx hj">cs 231n</strong>KNN 笔记</a></li><li id="9c88" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated"><a class="ae mg" href="https://scikit-learn.org/stable/modules/neighbors.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn 的 KNN 文档</a></li><li id="f722" class="jv jw hi jx b jy kk ka kl ji km jm kn jq ko kf kg kh ki kj bi translated"><a class="ae mg" href="https://www.cnblogs.com/pinard/p/6061661.html" rel="noopener ugc nofollow" target="_blank"> KNN 刘建平总结</a></li></ul><figure class="kq kr ks kt fd ku"><div class="bz dy l di"><div class="mx mu l"/></div><figcaption class="my mz et er es na nb bd b be z dx translated">stat quest:K-最近邻，解释清楚</figcaption></figure><h1 id="f156" class="lb iy hi bd iz lc ld le jd lf lg lh jh io li ip jl ir lj is jp iu lk iv jt ll bi translated">摘要</h1><p id="68c3" class="pw-post-body-paragraph ln lo hi jx b jy jz ij lq ka kb im ls ji mh lu lv jm mi lx ly jq mj ma mb kf hb bi nc translated"><span class="l nd ne nf bm ng nh ni nj nk di">K</span>——最近邻法(KNN)是一种非常基础的机器学习方法，在我们的日常生活中也是不由自主的应用。比如判断一个人的性格，我们只需要观察和他交往最密切的那几个人的性格。这里用到了 KNN 的想法。KNN 方法可以进行分类和回归，这与决策树算法相同。</p><p id="7754" class="pw-post-body-paragraph ln lo hi jx b jy lp ij lq ka lr im ls ji lt lu lv jm lw lx ly jq lz ma mb kf hb bi translated">KNN 回归和分类的主要区别在于进行预测时的决策方法不同。KNN 在进行分类预测时，一般选择多数投票法，即将训练集中最接近预测样本特征的<em class="ml"> K </em>个样本预测为类别数最多的类别。KNN 进行回归时，一般选择平均法，即取最近的<em class="ml"> K </em>个样本的样本输出的平均值作为回归预测值。由于两者之间的细微差别，虽然本文主要解释了 KNN 分类法，但这一思想也适用于 KNN 回归法。由于 scikit-learn 只使用了蛮力、KDTree 和 BallTree，所以本文只讨论这些算法的实现原理。其余的实现方法，如 BBF 树、MVP 树等。，这里不讨论。</p></div><div class="ab cl nl nm gp nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="hb hc hd he hf"><p id="1ab8" class="pw-post-body-paragraph ln lo hi jx b jy lp ij lq ka lr im ls ji lt lu lv jm lw lx ly jq lz ma mb kf hb bi translated"><em class="ml">原发布于</em><a class="ae mg" href="https://zg104.github.io/KNN_tutorial" rel="noopener ugc nofollow" target="_blank"><em class="ml">https://ZG 104 . github . io</em></a><em class="ml">。</em></p></div></div>    
</body>
</html>