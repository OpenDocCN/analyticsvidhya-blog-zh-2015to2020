<html>
<head>
<title>ML04: From ML to DL to NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML04:从ML到DL再到NLP</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ml04-ce0b172deb2b?source=collection_archive---------13-----------------------#2020-10-29">https://medium.com/analytics-vidhya/ml04-ce0b172deb2b?source=collection_archive---------13-----------------------#2020-10-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="50a5" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">简明概念图</h2></div><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="2f97" class="jg jh hi jc b fi ji jj l jk jl">Read time: 20 min</span><span id="2b2e" class="jg jh hi jc b fi jm jj l jk jl">This article is a part my mid-term report of the course <em class="jn">PyTorch and Machine Learning </em>in NCCU. Original report: <a class="ae jo" href="https://bit.ly/2UZftXq" rel="noopener ugc nofollow" target="_blank">https://bit.ly/2UZftXq</a></span></pre><p id="55f8" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">这篇文章就像一张从ML到ANN再到NLP的简明概念图。我不会太关注ML、DL和NLP背后复杂的数学。相反，我试着浏览所有的概念，把细节留给读者。</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><blockquote class="ks kt ku"><p id="4e0f" class="jp jq jn jr b js jt ij ju jv jw im jx kv jz ka kb kw kd ke kf kx kh ki kj kk hb bi translated"><strong class="jr hj"> <em class="hi">概述</em><br/><em class="hi">(1)</em></strong><a class="ae jo" href="#ec74" rel="noopener ugc nofollow"><strong class="jr hj"><em class="hi">机器学习基础知识</em></strong></a><strong class="jr hj"><br/></strong>1–1监督学习<br/>1–2无监督学习<br/>1–3强化学习<br/>1–4模型评估<br/>1–5数据拆分<br/>交叉验证<br/>1–6数据预处理和特征工程<br/>1–7过拟合和欠拟合【t27   <a class="ae jo" href="#9e74" rel="noopener ugc nofollow"> <strong class="jr hj"> <em class="hi">神经网络基础知识</em></strong></a><br/>NN的2-1可视化<br/>2–2激活函数<br/>2–3损失函数<br/>2–4优化器<br/>2–5批量学习<br/>2–6批量归一化<br/>2–7丢失<br/>2–8超参数<br/>2–9数据拆分&amp;交叉验证<br/> <strong class="jr hj"> <em class="hi"> </em></strong><a class="ae jo" href="#8ab8" rel="noopener ugc nofollow"><strong class="jr hj"><em class="hi">NLP中的神经网络</em> </strong> </a> <br/> 4-1数据预处理<br/>4–2弓逼近<br/>4–3 CNN<br/>4–4 RNN<br/>4–5 LSTM<br/><strong class="jr hj"><em class="hi">(5)</em></strong><a class="ae jo" href="#428f" rel="noopener ugc nofollow"><strong class="jr hj"><em class="hi">参考文献</em></strong></a></p></blockquote></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h1 id="ec74" class="ky jh hi bd kz la lb lc ld le lf lg lh io li ip lj ir lk is ll iu lm iv ln lo bi translated">(1) <strong class="ak">机器学习基础知识</strong></h1><h2 id="e880" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">1-1监督学习</h2><p id="63eb" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">回归问题<br/>分类问题<br/>图像分割<br/>语音分割<br/>语言分割</p><h2 id="aa44" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">1–2无监督学习</h2><p id="8207" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">聚类<br/>降维(如奇异值分解、主成分分析)</p><h2 id="3c2c" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">1–3强化学习</h2><h2 id="fe0c" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">1–4模型评估</h2><p id="7b23" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">对于数字目标，我们有:<br/> MSE <br/> RMSE <br/> MAPE <br/>对于分类目标，我们有:<br/>准确度<br/>精度<br/>回忆<br/>F1-分数</p><h2 id="2853" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">1–5数据分割和交叉验证</h2><p id="3c66" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated"><strong class="jr hj">三路数据拆分</strong>:将数据集拆分成三部分——训练、验证和测试数据集。它比双向数据拆分更严格，性能也更好-只将数据集拆分为训练数据集和测试数据集。<em class="jn">三路数据拆分</em>也叫“拆分数据机器学习验证”，<em class="jn">这个名词奇怪的是没有统一的名字</em>。</p><div class="mh mi ez fb mj mk"><a href="https://stats.stackexchange.com/questions/303836/whats-the-more-common-name-of-three-way-data-splits" rel="noopener  ugc nofollow" target="_blank"><div class="ml ab dw"><div class="mm ab mn cl cj mo"><h2 class="bd hj fi z dy mp ea eb mq ed ef hh bi translated">“三向数据拆分”更常见的名称是什么？</h2><div class="mr l"><h3 class="bd b fi z dy mp ea eb mq ed ef dx translated">begingroup$我正在使用Libsvm训练模型，但结果总是不太好。现在我正在读这个讲座的…</h3></div><div class="ms l"><p class="bd b fp z dy mp ea eb mq ed ef dx translated">stats.stackexchange.com</p></div></div><div class="mt l"><div class="mu l mv mw mx mt my mz mk"/></div></div></a></div><p id="915e" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated"><strong class="jr hj"> K重交叉验证</strong>:防止过拟合，使模型更加稳定。</p><figure class="ix iy iz ja fd nb er es paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="er es na"><img src="../Images/35195b6201842e55dd80b154da85083b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SHRHmZD1hi-STbnh54f4zw.png"/></div></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图1:三路数据分割[2]</figcaption></figure><figure class="ix iy iz ja fd nb er es paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="er es nl"><img src="../Images/f4360b4334f5844b3b2e101422052b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y0Iya40t21k9D14y3fSKSw.png"/></div></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图2:四重交叉验证和三重数据分割[2]</figcaption></figure><h2 id="d8c7" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">1–6数据预处理和特征工程</h2><p id="341d" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated"><strong class="jr hj">矢量化</strong>:文本、声音、图像、视频等格式数据的必经过程。<br/> <strong class="jr hj">处理缺失值</strong>:删除或输入。一个月前，我在我的媒体博客[1]上写了一篇关于缺失值插补的非常详细的文章，结论是:</p><blockquote class="ks kt ku"><p id="7389" class="jp jq jn jr b js jt ij ju jv jw im jx kv jz ka kb kw kd ke kf kx kh ki kj kk hb bi translated">1.一般而言，缺失值插补的复杂方法(随机森林、贝叶斯线性回归等)不会比简单方法(如仅插补平均值或中位数)表现更差，这与一些著名和流行的ML书籍相反。<br/> 2。理论上，随机森林拥有比kNN更好的速度和相似的准确性，这与一些著名和流行的ML书籍相矛盾。<br/> 3。贝叶斯线性回归(Python中的BayesianRidge)和随机森林模型(Python中的ExtraTreesRegressor)可能比其他模型具有更好的准确性。</p></blockquote><h2 id="7bc6" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">1–7过度配合和欠配合</h2><p id="9a80" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">获取更多数据<br/>缩小网络规模(即降低ML模型的复杂性)<br/>应用权重正则化<br/>剔除(仅适用于ANN模型，不适用于SVM、RF等)<br/>欠拟合</p><h2 id="9cd5" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">ML项目的1–8个工作流程</h2><p id="6a70" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">问题定义和数据集创建<br/>成功的衡量标准<br/>评估协议<br/>数据准备<br/>基线模型<br/>足够大以至于过拟合<br/>应用正则化<br/>学习率选取策略</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h1 id="9e74" class="ky jh hi bd kz la lb lc ld le lf lg lh io li ip lj ir lk is ll iu lm iv ln lo bi translated"><strong class="ak"> (2)神经网络基础知识</strong></h1><h2 id="1f37" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">神经网络的2–1可视化</h2><figure class="ix iy iz ja fd nb er es paragraph-image"><div class="er es nm"><img src="../Images/acc6dbd3958989d4fda7e3287bc6e391.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*e1zMa0oT4mxDUYes-ykZbA.png"/></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图3:感知机的可视化[3]</figcaption></figure><figure class="ix iy iz ja fd nb er es paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="er es nn"><img src="../Images/6f7fa6af1ae2b22aa12fbf9a4a5710e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ub-ifcgdi9xgryqvo0_GRA.png"/></div></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图4:神经网络的可视化[3]</figcaption></figure><figure class="ix iy iz ja fd nb er es paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="er es no"><img src="../Images/ab37aaaf7a3af31a1503aff786fe3e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gh2bqnpeix3Bh12KB7hEoQ.jpeg"/></div></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图5:低级操作和DL算法[2]</figcaption></figure><p id="04c6" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">我们可以看到，<a class="ae jo" href="https://becominghuman.ai/making-a-simple-neural-network-2ea1de81ec20" rel="noopener ugc nofollow" target="_blank">NN</a>—<strong class="jr hj">权重</strong>、<strong class="jr hj">激活函数</strong>(在感知器中)、<strong class="jr hj">损失函数</strong>、<strong class="jr hj">优化器</strong>、<strong class="jr hj">权重更新</strong>几个主要概念。那么，让我们来探讨一下这些概念。</p><h2 id="07fa" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">2–2个激活功能</h2><p id="949d" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">乙状结肠<br/> Tanh <br/> ReLU <br/> PReLU(漏ReLU是PReLU的一种):消除ReLU中的“垂死ReLU”。<br/> Softmax:用于分类。</p><figure class="ix iy iz ja fd nb er es paragraph-image"><div class="er es np"><img src="../Images/1a647cf1e4b7952ebfe5f6a99b2cd072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*Whw11mpkuATM1Rj5mNzf0A.png"/></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图PReLU和泄漏ReLU之间的关系[4]</figcaption></figure><figure class="ix iy iz ja fd nb er es paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="er es nq"><img src="../Images/6c348fd45bc24d13714086feb6cf46eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sbGu7unpiGKS-X7za5Z3UA.jpeg"/></div></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图7:常见激活函数图[5]</figcaption></figure><figure class="ix iy iz ja fd nb er es paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="er es nr"><img src="../Images/1ee5a9bc8f1b03524c8b1fca13b3d251.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7MesxlN0K5eZSBiZq4Dk8w.png"/></div></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图8:饱和与非饱和活化剂[6]</figcaption></figure><h2 id="3c14" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">2–3个损失函数</h2><p id="4385" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">L1损耗<br/> MSE损耗<br/>交叉熵损耗:用于分类<br/> NLL损耗<br/> NLL损耗2d</p><h2 id="9032" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">2–4个优化器</h2><p id="8d40" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">SGD:随机梯度下降<br/>动量<br/>阿达格拉德<br/> RMSprop (=阿达格拉德+动量)<br/>亚当(=高级RMSprop)</p><figure class="ix iy iz ja fd nb er es paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="er es ns"><img src="../Images/8e5a3d9eb9e0166c27cd587b6d2bb6f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*28FQxDaWsGIsN87_p2u8gA.png"/></div></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图9:优化器比较— SGD，Momentum，AdaGrad，Adam [7]</figcaption></figure><figure class="ix iy iz ja fd nb er es paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="er es nt"><img src="../Images/3a9c0419b1e9d4cc47db335553fc672d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rP-q_nkYu_ppZNhDlu4Rg.png"/></div></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图10:MNIST的优化者比较:SGD，Momentum，AdaGrad，Adam [7]</figcaption></figure><p id="d29a" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">一般来说，<strong class="jr hj">亚当</strong> &gt;阿达格拉德&gt;气势&gt; SGD ( &gt;代表“胜过”)，但在前面的MNIST案例中，阿达格拉德&gt; <strong class="jr hj">亚当</strong> &gt;气势&gt; SGD。对于大多数用例，一个<strong class="jr hj"> Adam </strong>或<strong class="jr hj"> RMSprop </strong>优化算法<em class="jn">工作得更好</em>。</p><h2 id="e376" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">2–5批次学习</h2><p id="3158" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">小批量:接近<em class="jn">自举</em>的概念。</p><h2 id="eb29" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">2–6批次标准化</h2><p id="c9eb" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">归一化是神经网络的一个基本步骤。</p><h2 id="6d1a" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">2-7岁辍学</h2><p id="84fb" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">这对避免过度拟合很重要。</p><h2 id="8b2a" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">2–8个超参数</h2><p id="3cf9" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">调谐参数如:<br/>每层感知器的数量<br/>批量<br/>学习率<br/>权重衰减</p><h2 id="299e" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">2–9数据分割和交叉验证</h2><p id="07c0" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">最好采用三向数据分割和k倍交叉验证。</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h1 id="4507" class="ky jh hi bd kz la lb lc ld le lf lg lh io li ip lj ir lk is ll iu lm iv ln lo bi translated"><strong class="ak"> (3)神经网络模型</strong></h1><h2 id="ca7a" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">3–1感知器</h2><p id="29fa" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">神经元是神经网络的最小单位。感知器是单层神经网络。</p><h2 id="4403" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">3–2 FNN</h2><p id="618b" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">前馈神经网络(FNN)，一种人工神经网络，其中节点之间的连接不形成循环。</p><h2 id="3697" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">3–3 MLP</h2><p id="05c6" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">多层感知器(MLP)是一类前馈神经网络。</p><h2 id="28b9" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">3–4美国有线电视新闻网</h2><p id="ac9b" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">CNN，卷积神经网络，是一种FNN。<em class="jn">全连接层</em>(或线性层)<strong class="jr hj"> <em class="jn">太复杂</em></strong><strong class="jr hj"><em class="jn">丢失所有空间信息</em> </strong>，而CNN避免了上述问题，并利用卷积层和池层来产生计算机视觉中出色的真实世界结果。[2]</p><p id="96e7" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">CNN在计算机视觉方面有两个主要优点:[8] <br/>平移不变量<br/>模式的空间层次</p><p id="85cf" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">大众CNN的网络架构:[7][9]<br/>LeNet<br/>AlexNet<br/>ResNet<br/>GoogLeNet<br/>VGGNet<br/>ImageNet</p><p id="f26d" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">而且，CNN的其他主要概念:[6]<br/>Conv2d(Conv2d)<br/>Pooling(MaxPooling2D)<br/>非线性激活器— ReLU <br/>迁移学习<br/>预卷积特性</p><figure class="ix iy iz ja fd nb er es paragraph-image"><div class="er es np"><img src="../Images/e58ebcd0d326813339012c9ec7a6c0f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*yDBqu1JPkuxSS4fiTcS_OQ.png"/></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图11:完全连接的层[2]</figcaption></figure><figure class="ix iy iz ja fd nb er es paragraph-image"><div class="er es nu"><img src="../Images/bc388b2df80cac1c4e1acf2e26ae5c14.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*IT2uleGGVdxXzgosHf2Fag.png"/></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图12:CNN的简化版本[2]</figcaption></figure><figure class="ix iy iz ja fd nb er es paragraph-image"><div class="er es nv"><img src="../Images/e3ae3aa27a90325632bf272d94f74493.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*KNeHCRh32fL0qNlXb6f5pA.png"/></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图13:卷积如何工作[8]</figcaption></figure><p id="3e36" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">关于CNN的更多详细信息，请查看:</p><div class="mh mi ez fb mj mk"><a rel="noopener follow" target="_blank" href="/@taposhdr/medical-image-analysis-with-deep-learning-ii-166532e964e6"><div class="ml ab dw"><div class="mm ab mn cl cj mo"><h2 class="bd hj fi z dy mp ea eb mq ed ef hh bi translated">基于深度学习的医学图像分析——ⅱ</h2><div class="mr l"><h3 class="bd b fi z dy mp ea eb mq ed ef dx translated">在上一篇文章中，我们介绍了使用OpenCV进行图像处理的一些基础知识和DICOM图像的基础知识。在这个…</h3></div><div class="ms l"><p class="bd b fp z dy mp ea eb mq ed ef dx translated">medium.com</p></div></div><div class="mt l"><div class="nw l mv mw mx mt my mz mk"/></div></div></a></div></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h1 id="8ab8" class="ky jh hi bd kz la lb lc ld le lf lg lh io li ip lj ir lk is ll iu lm iv ln lo bi translated"><strong class="ak">(4)NLP中的神经网络</strong></h1><p id="5b70" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">NLP(自然语言处理)在ANN(人工神经网络)可行之前就已经发展起来了，尽管直到ANN被加入NLP之后，它才繁荣起来。2009年出版的经典NLP书籍“使用Python的自然语言处理”[11]仅阐述了统计语言建模，而没有提及任何ANN方法。</p><h2 id="f6e9" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">4–1数据预处理</h2><p id="71e7" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">在进入神经网络之前将文本转换为矩阵:<br/>使用收缩字典<br/>标记化<br/>删除停用词<br/>词干</p><h2 id="48ad" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">4–2个船首引桥</h2><p id="f446" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">然后，我们可以将文本视为单词包(BOW)并进行<strong class="jr hj">矢量化</strong>，或者是<em class="jn">一键编码</em>或者是<em class="jn">单词嵌入</em>。</p><blockquote class="ks kt ku"><p id="4244" class="jp jq jn jr b js jt ij ju jv jw im jx kv jz ka kb kw kd ke kf kx kh ki kj kk hb bi translated"><strong class="jr hj"><em class="hi"/></strong><em class="hi">:一种传统的NLP方法，通常与TF-IDF一起使用。这里的数据过于稀疏，面临着</em>维数灾难<em class="hi">问题，因此</em>很少用于深度学习<em class="hi">。还有，它经常附带n-gram模型。<br/> </em> <strong class="jr hj"> <em class="hi">字嵌入</em> </strong> <em class="hi">:将数据转换成密集矩阵。Word2vec是一个受欢迎的衡量标准。</em></p></blockquote><p id="3d93" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">但是，弓接近<strong class="jr hj"> <em class="jn">就失去了文字</em> </strong>的顺序性。所以，我们转向RNN，来充分利用文本的顺序性。[2]</p><h2 id="6f21" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">4–3 CNN</h2><p id="4d61" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">CNN通过从图像中学习特征来解决计算机视觉中的问题。在图像中，CNN通过在高度和宽度上进行卷积来工作。同样，时间可以被视为卷积特征。一维CNN有时比rnn执行得更好，并且计算成本更低。CNN在NLP中的另一个用法是<em class="jn">文本分类</em>。[2]</p><h2 id="3c31" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">4–4 RNN</h2><p id="b2d4" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">递归神经网络(RNN)，它不是FNN，旨在解决序列数据。RNN可以解决自然语言理解、文档分类、情感分类等问题。RNN使用通过时间的反向传播(BPTT)而不是反向传播(BP)。[11]</p><figure class="ix iy iz ja fd nb er es paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="er es nx"><img src="../Images/132decdb67fb114283dba64a71632aef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LNMyQutCQyTuFY6nvGwi9w.png"/></div></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图14:简单的RNN [8]</figcaption></figure><p id="77e7" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">实际上，简单版本的RNN发现<strong class="jr hj"> <em class="jn">很难记住在</em> </strong>序列的前几部分发生的上下文。LSTMs和RNN的其他不同变体通过在LSTM内部添加不同的神经网络来解决这个问题，这些神经网络随后决定要记住多少或哪些数据。[2]</p><h2 id="987b" class="jg jh hi bd kz lp lq lr ld ls lt lu lh jy lv lw lj kc lx ly ll kg lz ma ln mb bi translated">4–5 LSTM</h2><p id="44db" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi translated">长短期记忆网络(LSTM)是一种RNN，能够学习长期依赖。简单的RNN在处理大序列 时会出现<em class="jn">渐变消失</em>和<em class="jn">渐变爆炸</em> <strong class="jr hj"> <em class="jn">的问题。LSTMs被设计成<strong class="jr hj"> <em class="jn">避免长期依赖问题</em> </strong>通过一种自然地长时间记忆信息的设计。[2]</em></strong></p><p id="a01b" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">LSTM有5个部分——单元状态、隐藏状态、输入门、遗忘门和输出门。[12]</p><figure class="ix iy iz ja fd nb er es paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="er es nx"><img src="../Images/b3f89fb28388ce6b81c99c53a75e3cb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F022XQtCqWSHTaQ_iltBWA.png"/></div></div><figcaption class="nh ni et er es nj nk bd b be z dx translated">图15:LSTM的解剖图[8]</figcaption></figure></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h1 id="428f" class="ky jh hi bd kz la lb lc ld le lf lg lh io li ip lj ir lk is ll iu lm iv ln lo bi translated"><strong class="ak"> (5)参考文献</strong></h1><p id="fe34" class="pw-post-body-paragraph jp jq hi jr b js mc ij ju jv md im jx jy me ka kb kc mf ke kf kg mg ki kj kk hb bi">[1] Kuo, M. (2020)。ML02: 初探遺失值(missing value)處理。取自 <a class="ae jo" href="https://merscliche.medium.com/ml02-na-f2072615158e" rel="noopener">https://merscliche.medium.com/ml02-na-f2072615158e</a><br/>[2] Subramanian, V. (2018). Deep Learning with PyTorch. Birmingham, UK: Packt Publishing.<br/>[3] Bre, F. et al. (2020). An efficient metamodel-based method to carry out multi-objective building performance optimizations. Energy and Buildings, 206, (unknown).<br/>[4] Guo, H. (2017). How do I implement the PReLU on Tensorflow?. Retrieved from <a class="ae jo" href="https://www.quora.com/How-do-I-implement-the-PReLU-on-Tensorflow" rel="noopener ugc nofollow" target="_blank">https://www.quora.com/How-do-I-implement-the-PReLU-on-Tensorflow</a><br/>[5] Endicott, S. (2017). Game Applications of Deep Neural Networks. Retrieved from <a class="ae jo" href="https://bit.ly/2G8nUIQ" rel="noopener ugc nofollow" target="_blank">https://bit.ly/2G8nUIQ</a><br/>[6] Taposh Dutta-Roy (2017). Medical Image Analysis with Deep Learning — II. Retrieved from<br/><a class="ae jo" rel="noopener" href="/@taposhdr/medical-image-analysis-with-deep-learning-ii-166532e964e6">https://medium.com/@taposhdr/medical-image-analysis-with-deep-learning-ii-166532e964e6</a><br/>[7] 斎藤康毅 (2016). ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装 (中譯：Deep Learning：用Python進行深度學習的基礎理論實作). Japan, JP: O’Reilly Japan.<br/>[8] Chollet, F. (2018). Deep learning with Python. New York, NY: Manning Publications.<br/>[9] 邢夢來等人 (2018)。PyTorch 深度學習與自然語言處理。新北市，台灣：博碩文化。<br/>[10] Bird, S. et al. (2009). Natural Language Processing with Python. California, CA: O’Reilly Media.<br/>[11] Rao, D., &amp; McMahan, B. (2019). Natural Language Processing with PyTorch. California, CA: O’Reilly Media.<br/>[12] Ganegedara, T. (2018). Natural Language Processing with TensorFlow. Birmingham, UK: Packt Publishing.</p></div></div>    
</body>
</html>