<html>
<head>
<title>A Beginner’s Guide to Reinforcement Learning and its Basic Implementation from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习初学者指南及其从零开始的基本实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-beginners-guide-to-reinforcement-learning-and-its-basic-implementation-from-scratch-2c0b5444cc49?source=collection_archive---------1-----------------------#2020-11-14">https://medium.com/analytics-vidhya/a-beginners-guide-to-reinforcement-learning-and-its-basic-implementation-from-scratch-2c0b5444cc49?source=collection_archive---------1-----------------------#2020-11-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="51fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">现实世界类比强化学习的基础，以及从头开始使用 Python 训练自动驾驶出租车在正确的目的地接送乘客的教程。</em></p><p id="07e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你们大多数人可能都听说过人工智能自己学习玩电脑游戏，一个非常受欢迎的例子是<strong class="ih hj"><em class="jd">【deep mind】</em></strong>，其中<strong class="ih hj"> <em class="jd"> </em> </strong>在 2016 年击败韩国围棋世界冠军<strong class="ih hj"> <em class="jd"> AlphaGo </em> </strong>时轰动了新闻并震惊了世界。</p><blockquote class="je jf jg"><p id="f791" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">那么这个重大突破背后的秘密是什么呢？沉住气！几分钟后你就会明白了。</p></blockquote><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es jk"><img src="../Images/3bbde365c45cff113907565e4c86804f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/0*qsbGpH7NvDjeLHwN.jpg"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">强化学习的一个类比</figcaption></figure><p id="497a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑一下教一只狗新的狗技巧的类比。在这个场景中，我们模拟了一种情况，狗试图以不同的方式做出反应。如果狗的反应是我们想要的，我们会奖励它狗粮。否则，我们会以这样或那样的方式传达出它的反应不是我们想要的。</p><p id="2be9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，每一次狗暴露在同样的情况下，狗会以更大的热情执行类似的动作，期待更多的食物。这基本上是从积极的经历中学习该做什么。同样，当面对负面经历时，它会倾向于学习什么不该做。</p><blockquote class="je jf jg"><p id="f973" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">这正是强化学习在更广泛意义上的工作方式！</p></blockquote><ul class=""><li id="356c" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated">狗是暴露在<strong class="ih hj"> <em class="jd">环境</em> </strong>中的<strong class="ih hj"> <em class="jd">特工</em> </strong>。<strong class="ih hj"> </strong> <em class="jd">环境</em>可能是想到了你的房子，有了你。</li><li id="4303" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">遇到的情况类似于<strong class="ih hj"> <em class="jd">状态</em> </strong>。一个<em class="jd">状态</em>的例子可能是你的狗站着，你在你的起居室里用某种特定的语气使用一个特定的词。</li><li id="ff82" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">我们的<em class="jd">代理</em>通过执行<strong class="ih hj"> <em class="jd">动作</em> </strong>做出反应，从一种状态转换到另一种状态。比如你的狗从<em class="jd">站着到跑着去抓棍子</em>。</li><li id="7fd1" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">过渡结束后，它可能会收到<strong class="ih hj"> <em class="jd">奖励</em> </strong>或<strong class="ih hj"> <em class="jd">惩罚</em> </strong> <em class="jd"> </em>作为回报。你请客作为<em class="jd">奖励</em>或者干脆说<em class="jd">不</em>作为<em class="jd">惩罚</em>。</li><li id="b48a" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><strong class="ih hj"> <em class="jd">策略</em> </strong>是在给定一个<em class="jd">状态</em>的情况下，选择一个<em class="jd">动作</em>的策略，期望得到更好的结果。</li></ul><blockquote class="je jf jg"><p id="8fa6" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">现在，将所有这些放在一起…</p></blockquote><p id="3e2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个<strong class="ih hj"> <em class="jd">强化学习(RL) </em> </strong>任务是关于训练一个<strong class="ih hj"> <em class="jd">智能体</em> </strong>与其<strong class="ih hj"> <em class="jd">环境</em> </strong>交互。代理通过执行<strong class="ih hj"> <em class="jd">动作</em> </strong>在环境的不同场景之间转换，称为<strong class="ih hj"> <em class="jd">状态</em> </strong>。作为回报，行动产生<strong class="ih hj"> <em class="jd">奖励</em> </strong>，可以是正数、负数或零。代理的唯一目的是在一集<strong class="ih hj"><em class="jd"/></strong>中最大化累积<strong class="ih hj"> <em class="jd">奖励</em> </strong>的概念，这是在初始状态和结束状态之间发生的所有事情，其中我们决定与我们想要完成的任务一致的奖励。</p><p id="704e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们<strong class="ih hj"> <em class="jd">通过提供正奖励来强化</em> </strong>主体执行某些行为，通过提供负奖励来远离他人。这就是一个代理如何学习制定一个策略或<strong class="ih hj"> <em class="jd">政策</em> </strong>。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es kk"><img src="../Images/296e648a083d8c498d5d51dddaa7a457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*ITDZP-8uimj7m2zG.gif"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">强化学习过程的图解</figcaption></figure><p id="2469" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="jd"/></strong>强化学习是<em class="jd"> </em>三种基本<strong class="ih hj"> <em class="jd">机器学习</em> </strong>范式之一，与<strong class="ih hj"> <em class="jd">有监督</em> </strong>和<strong class="ih hj"> <em class="jd">无监督学习并列。</em> </strong>它的<strong class="ih hj"> <em class="jd"> </em> </strong>论及<strong class="ih hj"> <em class="jd">开采</em> </strong>或<strong class="ih hj"> <em class="jd">勘探。</em> </strong></p><blockquote class="je jf jg"><p id="3366" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">一些需要注意的重要事项…</p></blockquote><ol class=""><li id="b388" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kl kc kd ke bi translated"><strong class="ih hj"> <em class="jd">贪婪并不总是奏效</em> </strong> <br/>有些事情很容易为了即时的满足而去做，有些事情可以提供长期的回报。我们的目标是不要贪婪地只寻求快速的直接回报，而是在整个训练中优化以获得最大回报。</li><li id="421c" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kl kc kd ke bi translated"><strong class="ih hj"> <em class="jd">序列在强化学习中至关重要</em> </strong> <br/>奖励代理不仅仅取决于当前状态，而是整个历史状态。与监督和非监督学习不同，时间在这里很重要。</li></ol><p id="79b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在某种程度上，<strong class="ih hj"> <em class="jd">强化学习</em> </strong>是利用经验做出最优决策的科学。分解一下，这个过程包括以下简单的步骤:</p><ol class=""><li id="18fb" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kl kc kd ke bi translated"><em class="jd">对环境的观察</em></li><li id="41f6" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kl kc kd ke bi translated"><em class="jd">使用某种策略决定如何行动</em></li><li id="ccfd" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kl kc kd ke bi translated"><em class="jd">随机应变</em></li><li id="9b9c" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kl kc kd ke bi translated"><em class="jd">接受奖励或处罚</em></li><li id="de5c" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kl kc kd ke bi translated"><em class="jd">从经验中学习并完善我们的战略</em></li><li id="cfa2" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kl kc kd ke bi translated"><em class="jd">迭代直到找到最优策略</em></li></ol><blockquote class="je jf jg"><p id="b0fa" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">回到<strong class="ih hj"> AlphaGo </strong> …</p></blockquote><p id="a71e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd"> AlphaGo </em>是一个 RL 代理的经典例子，代理已经学会了如何玩围棋来最大化它的回报。在本教程中，让我们通过实际开发一个代理来学习自动玩游戏来理解强化学习。</p><blockquote class="je jf jg"><p id="d4d1" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">强化学习不仅仅局限于游戏！</p><p id="01d6" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">它用于管理股票投资组合和财务，用于制造人形机器人，用于制造和库存管理，用于开发通用人工智能代理等…</p></blockquote><h1 id="b784" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">从头开始设计无人驾驶出租车</h1><p id="4607" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">我们来设计一个 S <strong class="ih hj"> <em class="jd">自动驾驶智能出租车</em> </strong>的模拟。主要目标是在一个简化的环境中演示如何使用 RL 技术来开发一种有效且安全的方法来解决这个问题。</p><p id="ae00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">智能出租车的工作是在一个地方搭载乘客，在另一个地方放下他们。我们希望智能出租车注意的事情:</p><ul class=""><li id="f462" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated"><em class="jd">在正确的位置让乘客下车。</em></li><li id="a326" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><em class="jd">尽可能缩短下车时间，以节省乘客的时间。</em></li><li id="200f" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><em class="jd">注意乘客的安全和交通规则。</em></li></ul><p id="c84a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在对这个问题的 RL 解决方案建模时，这里需要考虑不同的方面:<em class="jd">奖励、状态和动作。</em></p><h2 id="c42e" class="lp kn hi bd ko lq lr ls ks lt lu lv kw iq lw lx la iu ly lz le iy ma mb li mc bi translated">1.奖励</h2><p id="2f8d" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">由于<strong class="ih hj"> <em class="jd">智能体</em> </strong> <em class="jd">(假想驾驶员)</em>是奖励激励型的，并且将通过在环境中的试错经验来学习如何控制驾驶室，我们需要相应地决定<strong class="ih hj"> <em class="jd">奖励</em> </strong>和/或<strong class="ih hj"> <em class="jd">惩罚</em> </strong>及其幅度。这里有几点需要考虑:</p><ul class=""><li id="c88d" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated"><em class="jd">代理应该因为成功卸货而获得高额的正奖励，因为这种行为是非常可取的。</em></li><li id="24b3" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">如果旅行社试图让乘客在错误的地点下车，应该受到处罚。</li><li id="bc94" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">在每个时间步之后，代理应该因为没有到达目的地而得到一点负面的奖励。</li><li id="9997" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><em class="jd">“稍微”否定，因为我们更希望我们的代理晚一点到达，而不是错误地试图尽快到达目的地。</em></li></ul><h2 id="c06e" class="lp kn hi bd ko lq lr ls ks lt lu lv kw iq lw lx la iu ly lz le iy ma mb li mc bi translated">2.状态矢量空间</h2><p id="beae" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">在 RL 中，代理遇到一个<strong class="ih hj"> <em class="jd">状态</em> </strong>，然后根据它所处的状态采取<strong class="ih hj"> <em class="jd">动作</em> </strong>。</p><p id="0423" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">状态空间</em> </strong>是我们的出租车可能存在的所有可能情况的集合。状态应该包含代理做出正确动作所需的有用信息。</p><p id="a4b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们有一个智能出租车的培训区，我们在那里教它将停车场中的人运送到四个不同的位置(R，G，Y，B):</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es md"><img src="../Images/886b066a113f0cf91ad1597f842f87e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/0*c6tyKItRB5ZvfGsG.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">停车场中智能出租车的示意图</figcaption></figure><p id="22bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">我们假设智能出租车是这个停车场唯一的车辆。我们可以把停车场分成一个 5x5 的格子，这样就有 25 个可能的出租车位置。这 25 个位置是我们状态空间的一部分。我们出租车的当前位置状态是(3，1)。</em></p><p id="1e51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有 4 个位置可以让乘客上下车:<strong class="ih hj"><em class="jd">(row，col)坐标中的 R、G、Y、B </em> </strong>或<code class="du me mf mg mh b">[(0,0), (0,4), (4,0), (4,3)]</code>。我们的乘客在位置<strong class="ih hj"> <em class="jd"> Y </em> </strong>，他们希望去位置<strong class="ih hj"> <em class="jd"> R </em> </strong>。</p><p id="331c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们有 1 个额外的乘客在出租车内的状态，我们可以采用乘客位置和目的地位置的所有组合来得出我们的出租车环境的状态总数；共有 4 个目的地和 5 个<em class="jd"> (4+1) </em>客运点。</p><p id="4c9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们的滑行环境有<em class="jd">5×5×4 = 500</em>个可能的状态。<em class="jd">(出租车位置— 5×5，乘客位置— 5，目的地位置— 4) </em></p><h2 id="759c" class="lp kn hi bd ko lq lr ls ks lt lu lv kw iq lw lx la iu ly lz le iy ma mb li mc bi translated">3.行为空间</h2><p id="ffe8" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">动作空间</em> </strong>是我们的 agent 在给定状态下可以采取的所有动作的集合。代理遇到<em class="jd"> 500 个状态</em>中的一个并采取行动。在我们的例子中，行动可以是向一个方向移动或决定<em class="jd">搭载/放下一名乘客。</em></p><p id="db7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">换句话说，我们有<em class="jd">六种可能的行动</em>:</p><ol class=""><li id="6e0d" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kl kc kd ke bi translated"><code class="du me mf mg mh b">south</code></li><li id="bb57" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kl kc kd ke bi translated"><code class="du me mf mg mh b">north</code></li><li id="9eb9" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kl kc kd ke bi translated"><code class="du me mf mg mh b">east</code></li><li id="5e2a" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kl kc kd ke bi translated"><code class="du me mf mg mh b">west</code></li><li id="3be3" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kl kc kd ke bi translated"><code class="du me mf mg mh b">pickup</code></li><li id="58b4" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kl kc kd ke bi translated"><code class="du me mf mg mh b">dropoff</code></li></ol><p id="13ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，由于墙壁的原因，出租车在某些状态下无法执行某些操作。在环境的代码中，我们将为每一次撞墙提供一个<em class="jd"> -1 的惩罚</em>，这将使出租车考虑绕过墙。</p><h1 id="ad32" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">用 Python 实现</h1><p id="7ad2" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated"><em class="jd">幸运的是，</em> <a class="ae mi" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> <em class="jd"> OpenAI 健身房</em> </a> <em class="jd">已经为我们搭建好了这个确切的环境。</em></p><p id="3fcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae mi" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> <em class="jd"> OpenAI Gym </em> </a>提供了不同的游戏环境，我们可以插入我们的代码并测试一个代理。该库负责 API 提供我们的代理需要的所有信息，比如可能的动作、分数和当前状态。我们只需要关注代理的算法部分。</p><p id="3d58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用名为<code class="du me mf mg mh b">Taxi-V2</code>的健身房环境，上面解释的所有细节都来自这里。</p><h2 id="2310" class="lp kn hi bd ko lq lr ls ks lt lu lv kw iq lw lx la iu ly lz le iy ma mb li mc bi translated">健身房的界面</h2><p id="ddef" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated"><em class="jd">我们需要先安装</em> <code class="du me mf mg mh b">gym</code> <em class="jd">。执行下面的代码应该可以工作:</em></p><pre class="jl jm jn jo fd mj mh mk ml aw mm bi"><span id="8398" class="lp kn hi mh b fi mn mo l mp mq">!pip install cmake 'gym[atari]' scipy</span></pre><p id="1514" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">安装完成后，我们可以加载游戏环境并渲染它的外观:</em></p><pre class="jl jm jn jo fd mj mh mk ml aw mm bi"><span id="8c28" class="lp kn hi mh b fi mn mo l mp mq">import gym  <br/>env = gym.make("Taxi-v2").env  <br/>env.render()</span></pre><p id="87a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">核心健身房界面是<code class="du me mf mg mh b">env</code>，也就是<strong class="ih hj"> <em class="jd">统一环境界面</em> </strong>。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es mr"><img src="../Images/4558eddfc73aafe6545135694878793a.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/format:webp/1*-DOs5Tv33YV0v144uCidIg.png"/></div></figure><p id="569f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是我们将在代码中遇到的<code class="du me mf mg mh b">env</code>方法。</p><p id="e49e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du me mf mg mh b">env.reset</code> <em class="jd">:重置环境，返回随机初始状态。</em></p><p id="c1f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du me mf mg mh b">env.step(action)</code> <em class="jd">:在环境中移动一个时间步长。返回</em></p><ul class=""><li id="edd6" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated"><strong class="ih hj"> <em class="jd">观察</em> </strong> <em class="jd">:对环境的观察</em></li><li id="35f4" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><strong class="ih hj"> <em class="jd">奖励</em> </strong> <em class="jd">:你的行为是否有益</em></li><li id="d3b5" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><strong class="ih hj"> <em class="jd">完成</em> </strong> <em class="jd">:表示我们是否成功接送了一名乘客，也称为一集</em></li><li id="d65d" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><strong class="ih hj"> <em class="jd">信息</em> </strong> <em class="jd">:用于调试目的的性能和延迟等附加信息</em></li><li id="20bb" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><code class="du me mf mg mh b">env.render</code> <em class="jd">:渲染环境的一帧(有助于可视化环境)</em></li></ul><p id="686e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是我们重组后的问题陈述<em class="jd">(摘自</em> <a class="ae mi" href="https://gym.openai.com/docs/" rel="noopener ugc nofollow" target="_blank"> <em class="jd">健身房文档</em> </a> <em class="jd"> ) </em>:</p><blockquote class="je jf jg"><p id="7843" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">有 4 个地点(用不同的字母标记)，我们的工作是在一个地点接乘客，在另一个地点让他下车。我们获得+20 分的成功下降，每一个时间步失去 1 分。非法上下车行为还会被扣 10 分。</p></blockquote><p id="f4e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">让我们深入了解环境- </em></p><pre class="jl jm jn jo fd mj mh mk ml aw mm bi"><span id="d58e" class="lp kn hi mh b fi mn mo l mp mq">env.reset() # reset environment to a new, random state <br/>env.render()  <br/>print("Action Space {}".format(env.action_space)) <br/>print("State Space {}".format(env.observation_space))</span></pre><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es ms"><img src="../Images/c4b09a0f44a8b41808b397ff452cce0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*dLwD689DiC_B1zIDD68qzQ.png"/></div></figure><pre class="jl jm jn jo fd mj mh mk ml aw mm bi"><span id="0687" class="lp kn hi mh b fi mn mo l mp mq">Action Space Discrete(6)<br/>State Space Discrete(500)</span></pre><ul class=""><li id="8fbd" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated"><strong class="ih hj"> <em class="jd">实心方块</em> </strong>代表出租车，也就是<strong class="ih hj"> <em class="jd">黄色</em> </strong> <em class="jd">不带乘客</em><strong class="ih hj"><em class="jd">绿色</em> </strong>带<em class="jd">乘客</em>。</li><li id="0b2f" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><strong class="ih hj"> <em class="jd">管道(' |') </em> </strong>代表出租车无法穿越的墙壁。</li><li id="d90c" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><strong class="ih hj"> <em class="jd"> R、G、Y、B </em> </strong>是可能的<em class="jd">取货/目的地</em>位置。<strong class="ih hj"> <em class="jd">蓝色字母</em> </strong>代表当前乘客<em class="jd">上车地点</em>,<em class="jd"/><strong class="ih hj"><em class="jd">紫色字母</em> </strong>为<em class="jd">目的地</em>。</li></ul><p id="32b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有一个<em class="jd">大小为 6 </em>的<strong class="ih hj"> <em class="jd">动作空间</em> </strong>和一个<em class="jd">大小为 500 </em>的<strong class="ih hj"> <em class="jd">状态空间</em> </strong>。RL 学习从<em class="jd">可能的动作 0–5</em>中选择一个动作号，其中:</p><ul class=""><li id="a354" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated"><code class="du me mf mg mh b">0-south</code></li><li id="328b" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><code class="du me mf mg mh b">1-north</code></li><li id="8629" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><code class="du me mf mg mh b">2-east</code></li><li id="94c1" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><code class="du me mf mg mh b">3-west</code></li><li id="e0c2" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><code class="du me mf mg mh b">4-pickup</code></li><li id="a2ce" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><code class="du me mf mg mh b">5-dropoff</code></li></ul><p id="cc70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RL 将学习<strong class="ih hj"> <em class="jd">状态</em> </strong>到<em class="jd">最优</em> <strong class="ih hj"> <em class="jd">动作</em> </strong>的映射，以通过<strong class="ih hj"> <em class="jd">探索</em> </strong>在该状态下执行，即代理探索环境并基于环境中定义的奖励采取行动。</p><p id="0d0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个状态的最优动作是具有<strong class="ih hj"> <em class="jd">最高累积长期奖励</em> </strong> <em class="jd">的动作。</em></p><blockquote class="je jf jg"><p id="7027" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">回到我们的插图…</p></blockquote><p id="bbd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们以我们的插图为例，对其状态进行编码，并将其交给环境在<code class="du me mf mg mh b">gym</code>中进行渲染。回想一下，我们的出租车在<em class="jd">第 3 行第 1 列</em>，我们的乘客在<em class="jd">位置 2 </em>，目的地是<em class="jd">位置 0 </em>。使用<code class="du me mf mg mh b">Taxi-V2</code>状态编码方法，我们可以做到以下几点:</p><pre class="jl jm jn jo fd mj mh mk ml aw mm bi"><span id="3b39" class="lp kn hi mh b fi mn mo l mp mq">state = env.encode(3, 1, 2, 0) <br/>#(taxi row, taxi column, passenger index, destination index)<br/>print("State:", state)</span><span id="b6c8" class="lp kn hi mh b fi mt mo l mp mq">env.s = state<br/>env.render()</span></pre><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es ms"><img src="../Images/c4b09a0f44a8b41808b397ff452cce0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*dLwD689DiC_B1zIDD68qzQ.png"/></div></figure><pre class="jl jm jn jo fd mj mh mk ml aw mm bi"><span id="4268" class="lp kn hi mh b fi mn mo l mp mq">State: 328</span></pre><p id="3a3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用插图的坐标来生成一个数字，该数字对应于在<em class="jd"> 0 和 499 </em>之间的一个状态，对于我们的插图的状态来说，这个数字就是<strong class="ih hj"> <em class="jd"> 328 </em> </strong>。</p><p id="64bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们可以使用该编码数字通过<code class="du me mf mg mh b">env.env.s</code>手动设置环境状态。你可以摆弄这些数字，你会看到出租车、乘客和目的地在移动。</p><h2 id="be19" class="lp kn hi bd ko lq lr ls ks lt lu lv kw iq lw lx la iu ly lz le iy ma mb li mc bi translated">奖励表</h2><p id="d971" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">在创建出租车环境的时候，有一个已经创建好的初始奖励表，叫做<code class="du me mf mg mh b">P</code>。我们可以把它想象成一个有<em class="jd">个状态</em>作为<em class="jd">行</em>和<em class="jd">个动作</em>作为<em class="jd">列</em>的矩阵，即<strong class="ih hj"> <em class="jd">个状态×动作</em> </strong>矩阵。</p><p id="a80a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为每个州都在这个矩阵中，所以我们可以看到分配给我们示例的州的默认奖励值:</p><pre class="jl jm jn jo fd mj mh mk ml aw mm bi"><span id="5c18" class="lp kn hi mh b fi mn mo l mp mq">env.P[328]</span></pre><p id="ea87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">输出:</em></p><pre class="jl jm jn jo fd mj mh mk ml aw mm bi"><span id="0b74" class="lp kn hi mh b fi mn mo l mp mq">{0: [(1.0, 428, -1, False)],<br/> 1: [(1.0, 228, -1, False)],<br/> 2: [(1.0, 348, -1, False)],<br/> 3: [(1.0, 328, -1, False)],<br/> 4: [(1.0, 328, -10, False)],<br/> 5: [(1.0, 328, -10, False)]}</span></pre><p id="3934" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这本字典的结构是<code class="du me mf mg mh b">{action: [(probability, nextstate, reward, done)]}</code>。</p><p id="91d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">需要注意一些事情:</p><ul class=""><li id="c795" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated"><em class="jd">0–5</em>对应于出租车在当前状态下可以执行的动作<em class="jd">(南、北、东、西、上客、下车)</em>。</li><li id="cf62" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">在这个 env 中，<code class="du me mf mg mh b">probability</code>始终是<em class="jd"> 1.0 </em>。</li><li id="3cd0" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><code class="du me mf mg mh b">nextstate</code>是如果我们在字典的这个索引处采取行动，我们将处于的状态。</li><li id="07f1" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">所有的移动动作都有一个<em class="jd"> -1 奖励</em>并且<em class="jd">取放动作</em>在该特定状态下有<em class="jd"> -10 奖励</em>。如果我们处于这样一种状态，出租车上有一名乘客，并且在正确的目的地之上，我们将在下车动作(5)处看到 20 的<em class="jd">奖励</em></li><li id="95d3" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><code class="du me mf mg mh b">done</code>用于告诉我们何时在正确的位置成功放下乘客。每一个<em class="jd">的成功落幕</em>就是一个<strong class="ih hj"> <em class="jd">插曲的结束。</em> </strong></li></ul><p id="ab50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，如果我们的代理选择在此状态下探索行动二(2 ),它将会向东撞墙。源代码已经使实际移动出租车越过一堵墙成为不可能，所以如果出租车选择了那个动作，它将只是继续累积<em class="jd"> -1 惩罚</em>，这影响了<strong class="ih hj"> <em class="jd">长期奖励</em> </strong> <em class="jd">。</em></p><h2 id="a391" class="lp kn hi bd ko lq lr ls ks lt lu lv kw iq lw lx la iu ly lz le iy ma mb li mc bi translated">进入强化学习</h2><p id="d6a5" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">我们将使用一个简单的 RL 算法，称为<em class="jd"> Q-learning </em>，它将为我们的代理提供一些内存。</p><blockquote class="je jf jg"><p id="5622" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">关于 Q-Learning 的更多信息，请参考我的博客</p><p id="7f40" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated"><a class="ae mi" rel="noopener" href="/@tp6145/introduction-to-q-learning-for-the-self-driving-cab-problem-ee1dbf959b99">自动驾驶驾驶室问题 Q-Learning 介绍</a></p></blockquote><h1 id="ae09" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">在 Python 中实现 Q 学习</h1><h2 id="5edb" class="lp kn hi bd ko lq lr ls ks lt lu lv kw iq lw lx la iu ly lz le iy ma mb li mc bi translated">培训代理</h2><p id="bb04" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated"><em class="jd">首先，我们将把 Q 表初始化成一个 500×6500×6 的零矩阵:</em></p><pre class="jl jm jn jo fd mj mh mk ml aw mm bi"><span id="e31d" class="lp kn hi mh b fi mn mo l mp mq">import numpy as np<br/>q_table = np.zeros([env.observation_space.n, env.action_space.n])</span></pre><p id="212e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在可以创建训练算法，该算法将随着代理在数千次事件中探索环境而更新该 Q 表。</p><p id="48a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<code class="du me mf mg mh b">while not done</code>的第一部分，我们决定是<em class="jd">选择一个随机动作</em>还是<em class="jd">利用已经计算好的 Q 值</em>。这可以简单地通过使用<code class="du me mf mg mh b">epsilon</code>值并将其与<code class="du me mf mg mh b">random.uniform(0, 1)</code>函数进行比较来完成，后者返回 0 到 1 之间的任意数字。</p><p id="b9d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在环境中执行选择的动作，从执行动作中获得<code class="du me mf mg mh b">next_state</code>和<code class="du me mf mg mh b">reward</code>。之后，我们计算对应于<code class="du me mf mg mh b">next_state</code>的动作的最大 Q 值，这样，我们可以很容易地将我们的 Q 值更新为<code class="du me mf mg mh b">new_q_value</code>:</p><figure class="jl jm jn jo fd jp"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="c44a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">输出:</em></p><pre class="jl jm jn jo fd mj mh mk ml aw mm bi"><span id="6ef0" class="lp kn hi mh b fi mn mo l mp mq">Episode: 100000<br/>Training finished.</span><span id="40b5" class="lp kn hi mh b fi mt mo l mp mq">Wall time: 30.6 s</span></pre><p id="dcc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在 Q 表已经建立了 100，000 集，让我们看看在我们的图示状态下 Q 值是什么:</p><pre class="jl jm jn jo fd mj mh mk ml aw mm bi"><span id="7ebb" class="lp kn hi mh b fi mn mo l mp mq">q_table[328]</span></pre><p id="d2df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">输出:</em></p><pre class="jl jm jn jo fd mj mh mk ml aw mm bi"><span id="e269" class="lp kn hi mh b fi mn mo l mp mq">array([ -2.30108105,  -1.97092096,  -2.30357004,  -2.20591839,<br/>       -10.3607344 ,  -8.5583017 ])</span></pre><p id="c59c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最大 Q 值是<em class="jd">“北”(-1.971) </em>，所以看起来 Q-learning 已经有效地学习了在我们的示例状态下要采取的最佳行动！</p><h2 id="80ef" class="lp kn hi bd ko lq lr ls ks lt lu lv kw iq lw lx la iu ly lz le iy ma mb li mc bi translated">评估代理</h2><blockquote class="je jf jg"><p id="3ec8" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">是时候评估我们代理人的表现了…</p></blockquote><figure class="jl jm jn jo fd jp"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="1f53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">输出:</em></p><pre class="jl jm jn jo fd mj mh mk ml aw mm bi"><span id="b4c4" class="lp kn hi mh b fi mn mo l mp mq">Results after 100 episodes:<br/>Average timesteps per episode: 12.3<br/>Average penalties per episode: 0.0</span></pre><p id="9573" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，该代理的绩效显著提高，并且没有受到处罚，这意味着它对 100 名不同的乘客执行了正确的上下车操作。</p><h2 id="ca09" class="lp kn hi bd ko lq lr ls ks lt lu lv kw iq lw lx la iu ly lz le iy ma mb li mc bi translated">比较我们的 Q 学习代理和无强化学习</h2><p id="dc9c" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">让我们看看我们的<em class="jd"> Q-learning 解决方案</em>与仅仅随机移动的代理<em class="jd">相比有多好。</em></p><blockquote class="je jf jg"><p id="785e" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">对于没有强化学习的实施<strong class="ih hj">参考我的博客——</strong></p><p id="ce07" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated"><a class="ae mi" rel="noopener" href="/@tp6145/solving-the-self-driving-cab-problem-without-reinforcement-learning-5b07ac3c53ba">不用强化学习解决自动驾驶驾驶室问题</a></p></blockquote><p id="bee4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用 Q-learning，代理最初在探索过程中会犯错误，但是一旦它探索得足够多，它就可以明智地采取行动，做出明智的举动，使回报最大化。</p><p id="151c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们根据以下标准评估我们的代理，</p><ul class=""><li id="6d3f" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated"><strong class="ih hj"><em class="jd"/></strong><em class="jd">(越低越好)</em></li><li id="7245" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><strong class="ih hj"><em class="jd"/></strong><em class="jd">(越低越好)</em></li><li id="4230" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><strong class="ih hj"><em class="jd"/></strong><em class="jd">(越高越好)</em></li></ul><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es mw"><img src="../Images/769cd5d47ac0897652d56eee8bb52099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*ag4Qw9-d3h2gi4Z2t3PuDA.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">我们两个代理的比较(有和没有 RL)</figcaption></figure><blockquote class="je jf jg"><p id="6672" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">看来我们的 Q-Learning 代理已经搞定了！</p></blockquote><h2 id="3883" class="lp kn hi bd ko lq lr ls ks lt lu lv kw iq lw lx la iu ly lz le iy ma mb li mc bi translated">调整超参数</h2><p id="ffa9" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated"><strong class="ih hj"><em class="jd">α、γ和ε</em></strong>的值大多是基于直觉和<em class="jd">试凑</em>，但是有更好的方法来得出好的值。</p><p id="c36c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理想情况下，这三者应该随着时间的推移而减少，因为随着代理继续学习，它实际上建立了更有弹性的先验。因此，研究和试验超参数是极其重要的。</p><h1 id="1720" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">结论和未来</h1><p id="698c" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">好吧！我们从借助真实世界的类比来理解强化学习开始。然后我们一头扎进强化学习的基础，用 python 中的<em class="jd"> OpenAI 的 Gym </em>为我们设计了一个<em class="jd">自动驾驶出租车</em>作为<em class="jd">强化学习问题</em>，为我们提供了一个相关的环境，我们可以在那里开发我们的代理并对其进行评估。</p><p id="26ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我们观察到我们的代理在不使用任何算法玩游戏的情况下有多糟糕，所以我们继续从头开始实现<em class="jd"> Q-Learning 算法</em>。在<em class="jd"> Q-Learning </em>之后，代理人的表现显著提高。</p><blockquote class="je jf jg"><p id="eae7" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">q 学习是最简单的强化学习算法之一。然而，Q-Learning 的问题是，一旦环境中的状态数量非常多，就很难用 Q-table 来实现它们，因为它的大小会变得非常非常大。</p><p id="37ff" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">最先进的技术使用深度神经网络代替 Q 表(深度强化学习)。神经网络将状态信息和动作输入到输入层，并随着时间的推移学习输出正确的动作。</p></blockquote><p id="b127" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你想继续这个项目，让它变得更好，这里有一些你可以添加的东西—</p><ul class=""><li id="9e17" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated"><em class="jd">将这段代码转换成可以使用多种环境的功能模块</em></li><li id="d8d4" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><em class="jd">使用剧集衰减来调整 alpha、gamma 和/或 epsilon</em></li><li id="de44" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated"><em class="jd">实施网格搜索以发现最佳超参数</em></li></ul><blockquote class="mx"><p id="fe0b" class="my mz hi bd na nb nc nd ne nf ng jc dx translated">“事实证明，强化学习是一种机器学习，它对数据的渴望甚至大于监督学习。强化学习算法真的很难获得足够的数据。”</p><p id="4a71" class="my mz hi bd na nb nh ni nj nk nl jc dx translated">—吴恩达</p></blockquote><p id="e373" class="pw-post-body-paragraph if ig hi ih b ii nm ik il im nn io ip iq no is it iu np iw ix iy nq ja jb jc hb bi translated"><em class="jd">进一步参考我的其他博客:</em></p><blockquote class="je jf jg"><p id="60f7" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated"><a class="ae mi" rel="noopener" href="/@tp6145/solving-the-self-driving-cab-problem-without-reinforcement-learning-5b07ac3c53ba">无需强化学习解决自动驾驶驾驶室问题</a></p><p id="4c73" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated"><a class="ae mi" rel="noopener" href="/@tp6145/introduction-to-q-learning-for-the-self-driving-cab-problem-ee1dbf959b99">自动驾驶驾驶室问题 Q-Learning 介绍</a></p></blockquote></div></div>    
</body>
</html>