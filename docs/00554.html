<html>
<head>
<title>Guide to OpenAI’s GPT-2 and How to Use it in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenAI的GPT-2以及如何在Python中使用它的指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/guide-to-openais-gpt-2-and-how-to-use-it-in-python-72d37d7dd64c?source=collection_archive---------1-----------------------#2019-07-29">https://medium.com/analytics-vidhya/guide-to-openais-gpt-2-and-how-to-use-it-in-python-72d37d7dd64c?source=collection_archive---------1-----------------------#2019-07-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0efd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">了解如何使用世界上最先进的NLP框架构建自己的文本生成器！</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/13c59f772823c8dad5c28501d32ea00e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8yuF2NEiQriHirXy.jpg"/></div></div></figure><blockquote class="jq jr js"><p id="6605" class="if ig jd ih b ii ij ik il im in io ip jt ir is it ju iv iw ix jv iz ja jb jc hb bi translated"><em class="hi">“世界上最好的经济体与鼓励和积极反馈的文化直接相关。”</em></p></blockquote><p id="39f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你能猜到是谁说的吗？不是总统或总理。肯定不是像拉古拉姆·拉扬这样的顶尖经济学家。都是出于猜测？</p><p id="5433" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个报价是机器生成的！没错——一个在OpenAI的GPT-2框架上训练的<a class="ae jw" href="http://courses.analyticsvidhya.com/courses/natural-language-processing-nlp/?utm_source=blog&amp;utm_medium=openai-gpt2-text-generator-python" rel="noopener ugc nofollow" target="_blank">自然语言处理(NLP) </a>模型提出了这个非常真实的引用。现在机器学习的状态完全在另一个层面上，不是吗？</p><p id="ee29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在NLP真正的黄金时代，OpenAI的GPT-2重塑了我们处理文本数据的方式。乌尔菲特和谷歌的伯特为自然语言处理爱好者打开了方便之门，而GPT-2则打破了这一限制，使自然语言处理任务变得更加容易——主要是文本生成。</p><p id="78a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将在本文中使用GPT-2来构建我们自己的文本生成器。</p><p id="8dc6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">激动吗？那我们就进入文章吧。我们将首先理解GPT-2背后的直觉，然后直接进入Python来构建我们的文本生成模型。</p><p id="f442" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">如果你是一个狂热的NLP追随者，你会喜欢下面关于NLP最新发展的指南和教程:</em></p><ul class=""><li id="a576" class="jx jy hi ih b ii ij im in iq jz iu ka iy kb jc kc kd ke kf bi translated"><a class="ae jw" href="https://www.analyticsvidhya.com/blog/2019/03/pretrained-models-get-started-nlp/?utm_source=blog&amp;utm_medium=openai-gpt2-text-generator-python" rel="noopener ugc nofollow" target="_blank"><em class="jd"/></a>8个优秀的预训练模型让你开始自然语言处理(NLP)</li><li id="46cb" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated"><a class="ae jw" href="https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/?utm_source=blog&amp;utm_medium=openai-gpt2-text-generator-python" rel="noopener ugc nofollow" target="_blank"> <em class="jd">变形金刚在NLP中是如何工作的？最新最先进车型指南</em> </a></li><li id="5536" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated"><a class="ae jw" href="https://www.analyticsvidhya.com/blog/2019/07/pytorch-transformers-nlp-python/?utm_source=blog&amp;utm_medium=openai-gpt2-text-generator-python" rel="noopener ugc nofollow" target="_blank"><em class="jd">py torch-Transformers简介:一个不可思议的最新NLP库(带Python代码)</em> </a></li><li id="46ff" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated"><a class="ae jw" href="https://www.analyticsvidhya.com/blog/2019/02/stanfordnlp-nlp-library-python/?utm_source=blog&amp;utm_medium=openai-gpt2-text-generator-python" rel="noopener ugc nofollow" target="_blank"><em class="jd">Stanford NLP简介:一个令人难以置信的53种语言的一流NLP库(带Python代码)</em> </a></li></ul><h1 id="7286" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">OpenAI的GPT-2框架有什么新内容？</h1><p id="4097" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">在过去的几年里，自然语言处理(NLP)以惊人的速度发展。机器现在能够理解句子背后的上下文——想想看，这真是一个巨大的成就。</p><p id="b5de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由OpenAI开发的GPT-2是一个预先训练好的语言模型，我们可以用它来完成各种NLP任务，例如:</p><ul class=""><li id="5409" class="jx jy hi ih b ii ij im in iq jz iu ka iy kb jc kc kd ke kf bi translated">文本生成</li><li id="8e7d" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">语言翻译</li><li id="d7a6" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">建立问答系统，等等。</li></ul><p id="2e39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">语言建模是现代自然语言处理的重要任务之一。<strong class="ih hj">语言模型是预测文档中下一个单词或字符的概率模型。</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lo"><img src="../Images/53e21119f9c20d06a31796350e150846.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*E1eAECEHXvqAnUZO.jpg"/></div></div></figure><p id="c30e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GPT-2是GPT的继承者，是OpenAI的原始NLP框架。<strong class="ih hj">完整的GPT-2模型有15亿个参数，几乎是GPT参数的10倍。正如你可能已经猜到的那样，GPT-2给出了最先进的结果(当我们进入Python时，很快就会看到)。</strong></p><p id="8c76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">预先训练的模型包含从Reddit的出站链接收集的800万个网页的数据。让我们花一分钟来了解GPT-2是如何工作的。</p><h1 id="b121" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">建筑</h1><p id="6d06" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">GPT-2的架构是基于非常著名的变形金刚概念，这是由谷歌在他们的论文“注意力是你所需要的一切”中提出的。转换器提供了一种基于编码器-解码器的机制来检测输入-输出依赖性。</p><p id="2a6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在每一步，当生成下一个输出时，模型都将以前生成的符号作为附加输入。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lp"><img src="../Images/ba0885b322fd5c7162c44e70b0dee931.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/0*IUA2maPec-K3Q9ki.png"/></div></figure><p id="e190" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GPT-2除了有更多的参数和变形层之外，只有少量的架构修改:</p><ul class=""><li id="fcf2" class="jx jy hi ih b ii ij im in iq jz iu ka iy kb jc kc kd ke kf bi translated">该模型使用更大的上下文和词汇规模</li><li id="dba5" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">在最后的自我关注块之后，添加了附加的归一化层</li><li id="d897" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated">类似于“构建块”类型的残差单元，层归一化被移动到每个子块的输入。它在权重层之前应用了批规范化，这不同于原始类型“瓶颈”</li></ul><blockquote class="jq jr js"><p id="51bf" class="if ig jd ih b ii ij ik il im in io ip jt ir is it ju iv iw ix jv iz ja jb jc hb bi translated"><em class="hi">“GPT-2在各种特定领域的语言建模任务上取得了一流的成绩。我们的模型没有针对这些任务的任何特定数据进行训练，只是作为最终测试对其进行评估；这就是所谓的“零射击”设置。在特定领域数据集(如维基百科、新闻、书籍)上进行评估时，GPT-2的表现优于在这些数据集上训练的模型。”—开放AI团队。</em></p></blockquote><p id="442d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">具有不同参数的四个模型被训练以适应不同的场景:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lq"><img src="../Images/7bd63a83356a4ac24d7fecaf326bfed9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*h6_Z7gZrpQebLSU2.png"/></div></div></figure><p id="dddb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GPT-2有能力基于小的输入句子生成一整篇文章。这与早期的NLP模型形成了鲜明的对比，早期的NLP模型只能生成下一个单词，或者找到句子中缺少的单词。本质上，我们在一个全新的联盟中交易。</p><p id="39ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是GPT-2如何与其他类似的NLP模型相抗衡的:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lr"><img src="../Images/0c2ce804b1b46fedd1e765d4635dc79e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NM0H3xh_6HKBysURXHM_wg.png"/></div></div></figure><h1 id="20ba" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">如何为新GPT协议设置环境</h1><p id="f506" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">我们将使用一个有3.45亿个参数的中型模型。可以从官方<a class="ae jw" href="https://github.com/openai/gpt-2" rel="noopener ugc nofollow" target="_blank"> OpenAI GitHub资源库</a>下载预训练好的模型。</p><p id="640e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们需要通过键入下面的语句来克隆存储库(为了更快的计算，我建议使用Colab笔记本而不是您的本地机器):</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="6994" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，我们需要更改我们的目录。为此，我们将使用<strong class="ih hj">操作系统</strong>的<strong class="ih hj"> chdir() </strong>:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="4e8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，选择我们想要使用的模型。在这种情况下，我们将使用一个具有3.45亿个参数的中型模型。</p><p id="1be1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个模型需要有GPU支持的TensorFlow才能运行得更快。因此，让我们继续在笔记本中安装TensorFlow:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="be2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在进入建模部分之前，我们希望满足一些基本要求。在克隆的文件夹中，您会发现一个文件— <strong class="ih hj"> requirements.txt </strong>。它包含以下四个库，这四个库是该模型工作所必需的:</p><p id="7f51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">仅使用一行代码安装所有这些库:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="0149" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就是这样——我们已经适应了我们的环境。在我们进入文本生成器之前的最后一步——下载medum大小的预训练模型！同样，我们只用一行代码就可以做到:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="61ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据您的互联网带宽，可能需要一些时间。完成后，我们需要用下面的代码进行编码:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ls lt l"/></div></figure><h1 id="a970" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">用Python实现GPT-2来构建我们自己的文本生成器</h1><p id="1a87" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">你准备好了吗？因为这是你期待已久的时刻。是时候使用GPT-2在Python中构建我们自己的高级文本生成器了！我们开始吧。</p><p id="ebed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，使用chdir()进入src文件夹，就像我们之前做的那样:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="8f7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，导入所需的库:</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="01f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">注意:</em> <strong class="ih hj"> <em class="jd">型号</em> </strong> <em class="jd">，</em> <strong class="ih hj"> <em class="jd">样本</em> </strong> <em class="jd">和</em> <strong class="ih hj"> <em class="jd">编码器</em> </strong> <em class="jd">是存在于主GPT-2文件夹的</em><strong class="ih hj"><em class="jd">src</em></strong><em class="jd">子文件夹中的Python文件:</em></p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="889b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们一个一个地来理解我们刚刚看到的参数:</p><ul class=""><li id="f0ba" class="jx jy hi ih b ii ij im in iq jz iu ka iy kb jc kc kd ke kf bi translated"><strong class="ih hj">型号名称:</strong>表示我们正在使用的型号。在我们的例子中，我们使用的是具有3.45亿个参数或权重的GPT-2模型</li><li id="6fe8" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated"><strong class="ih hj">种子:</strong>随机数生成器的整数种子，修复种子以重现结果</li><li id="1269" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated"><strong class="ih hj"> nsamples: </strong>这表示在我们的输出中生成的样本文本的数量</li><li id="fe2c" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated"><strong class="ih hj"> batch_size: </strong>这只影响速度/内存。这也必须划分n个样本</li></ul><p id="b94f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">注意:要生成一个以上的样本，您需要更改nsamples和batch_size的值，并且还必须保持它们相等。</em></p><ul class=""><li id="bbef" class="jx jy hi ih b ii ij im in iq jz iu ka iy kb jc kc kd ke kf bi translated"><strong class="ih hj"> length: </strong>表示生成的文本中记号的个数。如果长度为0，那么记号的数量由模型超参数决定</li><li id="df4b" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated"><strong class="ih hj">温度:</strong>这控制着玻尔兹曼分布的随机性。较低的温度导致较少的随机完井。随着温度接近零度，模型将变得确定和重复。更高的温度导致更多的随机完井</li><li id="33c3" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated"><strong class="ih hj"> top_k: </strong>该参数控制多样性。如果top_k的值设置为1，这意味着每一步只考虑1个字(令牌)。如果top_k设置为40，这意味着每步考虑40个单词。0(默认值)是一个特殊设置，表示没有限制。<em class="jd"> top_k = 40通常是一个好值</em></li><li id="d45d" class="jx jy hi ih b ii kg im kh iq ki iu kj iy kk jc kc kd ke kf bi translated"><strong class="ih hj"> models_dir: </strong>表示包含模型子文件夹的父文件夹的路径(包含&lt; model_name &gt;文件夹)</li></ul><p id="cac9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，见证最先进的语言模型所产生的结果的时候到了。让我们运行这个函数并生成一些文本(做好震惊的准备):</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="ls lt l"/></div></figure><p id="45bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在将要求您输入一个字符串。这是我的想法:</p><blockquote class="jq jr js"><p id="a4d1" class="if ig jd ih b ii ij ik il im in io ip jt ir is it ju iv iw ix jv iz ja jb jc hb bi translated"><em class="hi">我去一个休息室庆祝我的生日和</em></p></blockquote><p id="79cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我的GPT-2文本生成器得出的结果:</p><blockquote class="jq jr js"><p id="be57" class="if ig jd ih b ii ij ik il im in io ip jt ir is it ju iv iw ix jv iz ja jb jc hb bi translated">我打电话给唐娜，告诉她我刚刚收养了她。她认为我的披露是一种捐赠，但我不确定唐娜是否符合标准。唐娜是一个真正甜美、有才华的女人，她把自己的生活当成了一个爱情故事。我知道她感谢我，因为我在照片上看到了她，她理解我的愤怒。这绝对不是礼物。我很感激我让她关心孩子，她在我身上看到了一些东西。我也没有太多的选择，只能让她知道她的新订婚，虽然这并不意味着我不关心，我非常感谢她为这个国家所做的一切。当我看到它的时候，我说:“你为什么没有变得像贝蒂或琳达一样？”“这是我们国家的孩子，我不能轻易做出这个决定。”“但不要告诉我你太心急了。”唐娜哭着拥抱了我。她从不挤奶，否则我会为她感到难过，但有时他们会立刻意识到这对她有多重要。她公开道歉，并在社会面前提出了明显的判断错误，这是令人震惊的，没有批准我的出生证明的请求。唐娜非常情绪化。我忘了她是个童子军。她实际上什么也没做，她基本上是自己的代理主人。2017年8月11日中午12:11匿名说… </p></blockquote><p id="9fa7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">难以置信！第一次看到这个结果我就无语了。令人难以置信的细节和令人印象深刻的语法——几乎不可能说它完全是由机器生成的。印象深刻，对吧？</p><p id="99b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">继续操作输入字符串，并在下面的评论部分分享你的结果。</p><h1 id="d4f6" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">关于GPT新协议潜在滥用的说明</h1><p id="092c" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">GPT-2因其可能被恶意使用而成为新闻。你可以想象这个NLP框架有多强大。它可以很容易地被用来产生假新闻，或者坦率地说，任何虚假文本，而人类无法意识到这种差异。</p><p id="5b24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑到这些事情，OpenAI没有发布完整的模型。相反，他们发布了一个小得多的模型。原始模型是在40 GB的互联网数据上训练的，有15亿个参数。OpenAI发布的两个样本模型分别有1.17亿和3.45亿个参数。</p><h1 id="6144" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">结束注释</h1><p id="1c33" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">在本文中，我们使用了具有3 . 45亿个参数的中型模型。如果这些较小的模型能够产生如此令人印象深刻的结果，想象一下15亿个参数的完整模型会产生什么。既害怕又兴奋。</p><p id="e673" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">NLP的下一步是什么？我觉得我们不用等太久就能知道了。</p><p id="b092" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与此同时，尝试这个GPT新协议框架，并让我知道你的经验如下。除此之外，我还鼓励您仅出于研究和获取知识的目的使用该模型。继续学习！</p></div><div class="ab cl lu lv gp lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="hb hc hd he hf"><p id="e018" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">原载于2019年7月29日</em><a class="ae jw" href="https://www.analyticsvidhya.com/blog/2019/07/openai-gpt2-text-generator-python/" rel="noopener ugc nofollow" target="_blank"><em class="jd">【https://www.analyticsvidhya.com】</em></a><em class="jd">。</em></p></div></div>    
</body>
</html>