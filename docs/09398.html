<html>
<head>
<title>Linear Regression from scratch and basic intuition!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始线性回归和基本直觉！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-from-scratch-and-basic-intuition-e2564172bdc3?source=collection_archive---------19-----------------------#2020-09-04">https://medium.com/analytics-vidhya/linear-regression-from-scratch-and-basic-intuition-e2564172bdc3?source=collection_archive---------19-----------------------#2020-09-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="377a" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">线性回归基本上是机器学习领域的序幕。理解线性回归并不难，但却是非常重要的一步。线性回归是最容易实现的机器学习算法之一。</p></blockquote><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es jh"><img src="../Images/8dd843e43ec3dc125bf3f17bf59566ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*kXFr9TNU0pdcNCt5aOn_uA.jpeg"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">也许渐变只是一种错觉！:ROFL:！</figcaption></figure><p id="deb7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">线性回归是借助直线找到两个变量之间关系的过程。我们的目标是找到以最小损失拟合这些点的线。这条线称为回归线。创建这条线性回归线的过程将是一个非常简单的过程。如果你到现在还不能理解任何事情，不要担心。我们将一步一步地介绍它们。</p><p id="cbb5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">线性回归定义:</strong></p><p id="ee9f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">正如我们前面看到的，线性回归是一种寻找两个变量(X和y)之间关系的方法，其中X是自变量，y是因变量。</p><p id="96b9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">y<em class="ik">= w1 * X+w0</em>T5】</strong></p><p id="27fd" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><em class="ik">其中，</em></p><p id="ea31" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><em class="ik"> y—因变量</em></p><p id="bcd2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><em class="ik"> X —独立变量</em></p><p id="ea4f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><em class="ik"> w0—偏置</em></p><p id="8989" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><em class="ik">w1——比例因子或系数</em></p><p id="d52d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">偏差因子(w0)给出了模型的自由度。</p><p id="19f5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">我们的工作是找到m和b的值，使得损失最小。求m和b值的两种方法是<strong class="il hj">普通最小二乘法</strong>和<strong class="il hj">梯度下降法。</strong></p><p id="2a5f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">我们将使用梯度下降法来实现梯度下降。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es jw"><img src="../Images/cb53812ed7b75fa6635eb131918b3487.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/0*hGicp3cySKx85lZP.jpg"/></div></figure><p id="505e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">j是成本函数。它找出预测值和实际值之间的差异。</p><p id="ecf7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">我们的目标是找到一条成本最低的线路。(即最小化成本)。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es jx"><img src="../Images/90d75a1f55872b8c2c981fe662dcc6c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/0*fnqXxsWxfU-EprBu.jpg"/></div></figure><p id="8a37" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">让我们进入编码部分！！！</p><p id="bf5c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">让我们在sklearn库的帮助下创建一个玩具数据集。这是我们唯一会用到sklearn的部分。整个线性回归的实现只有在numpy的帮助下才能完成。</p><pre class="ji jj jk jl fd jy jz ka kb aw kc bi"><span id="0747" class="kd ke hi jz b fi kf kg l kh ki"><strong class="jz hj">#LINEAR REGRESSION FROM SCRATCH</strong></span><span id="1edd" class="kd ke hi jz b fi kj kg l kh ki">from sklearn.datasets.samples_generator import make_regression</span><span id="9677" class="kd ke hi jz b fi kj kg l kh ki">X, y = make_regression(n_samples=200, n_features=1, n_informative=1, noise=6, bias=30, random_state=200)<br/>m = 200</span><span id="54ba" class="kd ke hi jz b fi kj kg l kh ki">from matplotlib import pyplot as plt<br/>plt.scatter(X,y, c = "red",alpha=.5, marker = 'o')<br/>plt.xlabel("X")<br/>plt.ylabel("Y")<br/>plt.show()</span></pre><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es kk"><img src="../Images/8a17c3c002fea386249f4db65be53acc.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*2evyE4snBt_KDdH33fSi7g.png"/></div></figure><p id="d18c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><em class="ik"> h( ) </em> — h函数是假设函数。它返回w1*x1 + w0。</p><p id="593d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><em class="ik"> cost( ) </em> —成本函数计算预测值和实际值之间的MSE(均方误差)</p><p id="de92" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><em class="ik"> grad( ) </em> — grad函数计算成本函数w.r.t对w0和w1的一阶导数。</p><p id="3676" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><em class="ik">下降()</em> —下降功能负责梯度下降操作。(即)它负责权重更新过程，并试图找到每个点的损失最小的点。</p><p id="4abc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><em class="ik"> lr </em> —学习率</p><p id="8145" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><em class="ik">graph()&amp;formula()</em>—用于绘图目的。</p><pre class="ji jj jk jl fd jy jz ka kb aw kc bi"><span id="2ab3" class="kd ke hi jz b fi kf kg l kh ki">import numpy as np</span><span id="5ef9" class="kd ke hi jz b fi kj kg l kh ki"><strong class="jz hj">def h(X,w):</strong><br/>    return (w[1]*np.array(X[:,0])+w[0])</span><span id="eb2b" class="kd ke hi jz b fi kj kg l kh ki"><strong class="jz hj">def cost(w,X,y):</strong><br/>    return (.5/m) * np.sum(np.square(h(X,w)-np.array(y)))</span><span id="0c7d" class="kd ke hi jz b fi kj kg l kh ki"><strong class="jz hj">def grad(w,X,y):</strong><br/>    g = [0]*2<br/>    g[0] = (1/m) * np.sum(h(X,w)-np.array(y))<br/>    g[1] = (1/m) * np.sum((h(X,w)-np.array(y))*np.array(X[:,0]))<br/>    return g</span><span id="7a10" class="kd ke hi jz b fi kj kg l kh ki"><strong class="jz hj">def descent(w_new, w_prev, lr):</strong><br/>    print(w_prev)<br/>    print(cost(w_prev,X,y))<br/>    j=0<br/>    while True:<br/>        w_prev = w_new<br/>        w0 = w_prev[0] - lr*grad(w_prev,X,y)[0]<br/>        w1 = w_prev[1] - lr*grad(w_prev,X,y)[1]<br/>        w_new = [w0, w1]<br/>        print(w_new)<br/>        print(cost(w_new,X,y))<br/>        if (w_new[0]-w_prev[0])**2 + <br/>                              (w_new[1]-w_prev[1])**2 &lt;= pow(10,-6):<br/>            return w_new<br/>        if j&gt;500: <br/>            return w_new<br/>        j+=1</span><span id="7bfe" class="kd ke hi jz b fi kj kg l kh ki">w = [0,-1]<br/>w = descent(w, w , 0.01)</span><span id="7261" class="kd ke hi jz b fi kj kg l kh ki"><strong class="jz hj">def graph(formula, x_range):</strong>  <br/>    x = np.array(x_range)  <br/>    y = formula(x)  <br/>    plt.plot(x, y, color="blue")  <br/>    <br/><strong class="jz hj">def my_formula(x):</strong><br/>    return w[0]+w[1]*x</span><span id="44e4" class="kd ke hi jz b fi kj kg l kh ki">plt.scatter(X,y, c = "red",alpha=.5, marker = 'o')<br/>graph(my_formula, range(-2,3))<br/>plt.xlabel('X')<br/>plt.ylabel('Y')<br/>plt.show()</span></pre><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es kk"><img src="../Images/bc91cd9e12d2fdb332d5372b19f338b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*AQ76Ws36F_XfOOd0wBiCAg.png"/></div></figure><p id="82d6" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">就是这样。简单的线性回归就搞定了！</p><p id="49a8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">虽然精度不会很大，但这是一个很好的开始模型。</p><blockquote class="if ig ih"><p id="8b67" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">这个博客到此为止。我希望你们能学到一些有用的东西。请跟随我的叙述。欢迎在评论中提出关于博客的问题，并通过鼓掌表示感谢。还有，通过我的<a class="ae kl" href="https://www.linkedin.com/in/venkatesha-prasad-sridar/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>账号和我联系。感谢阅读。</p></blockquote></div></div>    
</body>
</html>