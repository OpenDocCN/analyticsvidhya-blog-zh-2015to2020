<html>
<head>
<title>Replacing YoloV3 Backbone with ChexNet for Pneumonia Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用ChexNet替换YoloV3主干用于肺炎检测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/replacing-yolov3-backbone-with-chexnet-for-pneumonia-detection-a29434a698b7?source=collection_archive---------3-----------------------#2020-10-04">https://medium.com/analytics-vidhya/replacing-yolov3-backbone-with-chexnet-for-pneumonia-detection-a29434a698b7?source=collection_archive---------3-----------------------#2020-10-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="039d" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">构建深度学习模型，自动检测和定位<strong class="ak">胸片(CXR) </strong>上潜在的肺炎肺部阴影。</h2></div><h1 id="6f93" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">商业问题</h1><p id="410a" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi kl translated">在全球范围内，5岁以下儿童的死亡中，肺炎占了15%以上。2015年，有92万名5岁以下儿童死于该疾病。在美国，2015年，肺炎导致超过500，000次急诊和超过50，000例死亡，使这种疾病成为该国十大死亡原因之一。</p><p id="47de" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated">虽然常见，但准确诊断肺炎是一项艰巨的任务。它需要由训练有素的专家对<strong class="jr hj">胸片(CXR) </strong>进行审查，并通过临床病史、生命体征和实验室检查进行确认。肺炎通常表现为CXR 上一个或多个区域<strong class="jr hj">的不透明增加。然而，在CXR上肺炎的诊断是复杂的，因为肺部有许多其他情况，如液体超载(肺水肿)、出血、容量损失(肺不张或虚脱)、肺癌、或放疗后或手术后的变化。在肺外，胸膜腔内的液体(胸腔积液)在CXR上也表现为阴影增加。如果可以的话，在不同时间点对患者的CXRs进行比较，并与临床症状和病史相关联，有助于做出诊断。</strong></p><p id="a1c3" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated">CXRs是最常进行的诊断成像研究。许多因素，如患者的体位和吸气深度，会改变CXR的外观，使解释更加复杂。此外，临床医生每次轮班都要阅读大量图像。</p><p id="a18b" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated">我们的目标是建立一个深度学习模型，以检测医学图像中肺炎的视觉信号。具体来说，我们的模型需要自动定位胸片上的肺部阴影，从而帮助临床医生以高准确度和高效率检测和诊断肺炎。</p><p id="9030" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated">为了实现上述目标，我们将YoloV3主干模型(即DarkNet53)替换为ChexNet(在胸部x光片上训练),并使用ChexNet主干重新训练整个YoloV3模型。</p><blockquote class="kz la lb"><p id="5cb4" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">先决条件:-读者必须了解YoloV3模型的架构和功能及其基本概念，即:锚框，边界框，非最大抑制。</p></blockquote><h2 id="07e7" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">接下来讨论什么！！</h2><p id="6851" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">因为这是一篇详细的文章，所以最好列出这篇文章中涉及的要点(如果你想忽略这个理论，可以直接跳到第7点)。</p><ol class=""><li id="8caa" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk lz ma mb mc bi translated"><strong class="jr hj">深度学习/计算机视觉的使用:- </strong>讨论了如何将这个问题提出来作为深度学习或计算机视觉问题。</li><li id="3507" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><strong class="jr hj">数据来源:- </strong>提供了数据来源和对每个文件内容的深入解释。</li><li id="1a6f" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><strong class="jr hj">评估指标:- </strong>解释了评估指标&amp;对于给定问题的重要性。</li><li id="fef8" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><strong class="jr hj">探索性数据分析:- </strong>对数据进行分析，直观了解其广度&amp;深度。</li><li id="4a73" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><strong class="jr hj">现有方法:- </strong>解释了此问题的现有解决方案。</li><li id="956b" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><strong class="jr hj">第一次切割方法</strong>:-解释了我解决这个问题的方法。</li><li id="5763" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><strong class="jr hj">训练ChexNet(二值图像分类模型):- </strong>讲解建模&amp;训练ChexNet (DensNet121)二值图像分类模型，将胸片分类为肺炎阳性或阴性。</li><li id="a0d0" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><strong class="jr hj">使用ChexNet模型替换YoloV3主干并对其进行重新训练:- </strong>提供了关于如何使用ChexNet (DensNet121在胸片上训练)替换YoloV3主干(DarkNet53)并对其进行重新训练以进行肺炎混浊度检测的深入解释。</li><li id="830c" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated">未来的工作:- 讨论了我们未来可以做些什么来更好地解决这个问题。</li><li id="696f" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><strong class="jr hj"> Github Repo:- </strong>提供了到Github存储库的链接来访问全部代码。</li></ol></div><div class="ab cl mi mj gp mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="hb hc hd he hf"><h1 id="cd2f" class="ix iy hi bd iz ja mp jc jd je mq jg jh io mr ip jj ir ms is jl iu mt iv jn jo bi translated"><strong class="ak">深度学习/计算机视觉的使用</strong></h1><p id="e88e" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">随着成功的实验结果和广泛的应用，深度学习(DL)有可能改变医疗保健的未来。人工智能(AI)的使用已经变得越来越流行，并且现在被用于例如癌症诊断和治疗。计算机视觉的深度学习可以实现更精确的医学成像和诊断，本案例研究是其中的一部分</p></div><div class="ab cl mi mj gp mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="hb hc hd he hf"><h1 id="8f15" class="ix iy hi bd iz ja mp jc jd je mq jg jh io mr ip jj ir ms is jl iu mt iv jn jo bi translated"><strong class="ak">数据来源</strong></h1><h2 id="a4a1" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">我需要什么文件？</h2><p id="7bde" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">我们将需要当前阶段的图像—提供为<code class="du mu mv mw mx b">stage_2_train_images.zip</code>和<code class="du mu mv mw mx b">stage_2_test_images.zip</code>。我们还需要训练数据——<code class="du mu mv mw mx b">stage_2_train_labels.csv</code>——和样本提交，它提供了测试集的id，以及我们提交的样本应该是什么样子。文件<code class="du mu mv mw mx b">stage_2_detailed_class_info.csv</code>包含关于训练集中的正类和负类的详细信息，并且可以用于建立更细微的模型。</p><h2 id="ab2c" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">我应该期待什么样的数据格式？</h2><p id="ae8f" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">训练数据以一组<code class="du mu mv mw mx b">patientIds</code>和<code class="du mu mv mw mx b">bounding boxes</code>的形式提供。边界框定义如下:<code class="du mu mv mw mx b">x-min</code> <code class="du mu mv mw mx b">y-min</code> <code class="du mu mv mw mx b">width</code> <code class="du mu mv mw mx b">height</code>还有一个二元目标列，<code class="du mu mv mw mx b">Target</code>表示肺炎或非肺炎。每个<code class="du mu mv mw mx b">patientId</code>可能有多行。</p><h2 id="92b3" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">文件描述(<a class="ae my" href="https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/data" rel="noopener ugc nofollow" target="_blank">下载</a>)</h2><p id="aee5" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated"><strong class="jr hj"><em class="lc">stage _ 2 _ train _ images . zip</em></strong></p><ul class=""><li id="d7e0" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">由DICOM格式的26684幅胸部x光图像组成，用于模型训练。</li><li id="ad80" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">DICOM文件是以医学数字成像和<br/>通信(DICOM)格式保存的图像。它包含来自医学扫描的图像，如超声波或核磁共振成像。DICOM文件还可能包含患者的识别数据，以便将图像与特定的个人相关联。</li></ul><p id="0c9c" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated"><strong class="jr hj"><em class="lc">stage _ 2 _ test _ images . zip</em></strong></p><ul class=""><li id="8206" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">由3000张DICOM格式的胸部x光图像组成。</li></ul><p id="2d50" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated"><strong class="jr hj"><em class="lc">stage _ 2 _ train _ labels . CSV(30227 x 6)</em></strong></p><ul class=""><li id="9f21" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated"><code class="du mu mv mw mx b"><strong class="jr hj">patientId</strong></code> —一个患者Id。每个患者Id对应一个<br/>唯一图像。</li><li id="e94a" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated"><code class="du mu mv mw mx b"><strong class="jr hj">x</strong> </code> —边界框的左上角x坐标。</li><li id="d034" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated"><code class="du mu mv mw mx b"><strong class="jr hj">y</strong> </code> —边界框的左上角y坐标。</li><li id="9e10" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated"><code class="du mu mv mw mx b"><strong class="jr hj">width</strong></code> —边界框的宽度。</li><li id="3f4d" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated"><code class="du mu mv mw mx b"><strong class="jr hj">height</strong></code> —边界框的高度。</li><li id="8093" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated"><code class="du mu mv mw mx b"><strong class="jr hj">Target</strong></code> —二元目标，表示该样本<br/>是否有肺炎的迹象。</li></ul><p id="c629" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated"><strong class="jr hj"><em class="lc">stage _ 2 _ detailed _ class _ info . CSV(30227 x 2)</em></strong></p><ul class=""><li id="7337" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated"><code class="du mu mv mw mx b"><strong class="jr hj">patientId</strong></code> —一个患者Id。每个患者Id对应一个<br/>唯一图像。</li><li id="4a0c" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated"><code class="du mu mv mw mx b"><strong class="jr hj">class</strong></code> —告知患者肺部状况<br/>(即:-正常，无肺部阴影/不正常，肺部阴影)</li></ul><p id="e52f" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated"><strong class="jr hj"><em class="lc">【stage _ 2 _ sample _ submission . CSV(3000 x 2)</em></strong></p><ul class=""><li id="8a47" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated"><code class="du mu mv mw mx b"><strong class="jr hj">patientId</strong></code> —一个患者Id。每个患者Id对应一个<br/>唯一图像。</li><li id="4cd0" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated"><code class="du mu mv mw mx b"><strong class="jr hj">PredictionString</strong> </code> —由预测置信度<br/> &amp;包围盒信息组成。</li></ul></div><div class="ab cl mi mj gp mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="hb hc hd he hf"><h1 id="2944" class="ix iy hi bd iz ja mp jc jd je mq jg jh io mr ip jj ir ms is jl iu mt iv jn jo bi translated"><strong class="ak">评估指标</strong></h1><ul class=""><li id="1cf4" class="lu lv hi jr b js jt jv jw jy na kc nb kg nc kk mz ma mb mc bi translated">并集上的交集(IoU):- <br/>并集上的交集是两个边界框(或者，在更一般的情况下<br/>两个对象)之间重叠的<br/>大小的度量。它计算两个对象<br/>之间重叠的大小，除以两个对象<br/>合并的总面积。<br/>它可以被形象化为以下内容:</li></ul><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nd"><img src="../Images/4960b1ab3c168b0115c7884ee7a096ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4YVUS-f0hv1qRqWobHQoEA.jpeg"/></div></div><figcaption class="np nq et er es nr ns bd b be z dx translated">并集上的交集</figcaption></figure><ul class=""><li id="653a" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">可视化中的两个框重叠，但是<br/>重叠的区域与两个<br/>对象一起占据的区域相比是微不足道的。欠条会很低——在欠条阈值较高的情况下，可能不会将<br/>算作“命中”。</li></ul></div><div class="ab cl mi mj gp mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="hb hc hd he hf"><h1 id="398f" class="ix iy hi bd iz ja mp jc jd je mq jg jh io mr ip jj ir ms is jl iu mt iv jn jo bi translated"><strong class="ak">探索性数据分析</strong></h1><h2 id="eeda" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">不透明和不透明的CXR图像样本</h2><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nt"><img src="../Images/6078594ca4b8c5ee22fd1c3aa366fc88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oussuOtZqRoEReZDHHVRSQ.jpeg"/></div></div><figcaption class="np nq et er es nr ns bd b be z dx translated">有和没有不透明的CXR图像</figcaption></figure><blockquote class="kz la lb"><p id="b276" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated"><strong class="jr hj">观察:- </strong></p><p id="2527" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">在训练数据集中，我们有26684个独特患者的数据。</p><p id="431e" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">不透明度的最小值是1。</p><p id="4a44" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">不透明度的最大数量是4。</p></blockquote><h2 id="303a" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">阶级不平衡</h2><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nu"><img src="../Images/76a881b535c1f6224ce5c4f9691d6172.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*baVmSJzcK8OQSHyimiIy7Q.jpeg"/></div></div><figcaption class="np nq et er es nr ns bd b be z dx translated">阶级不平衡</figcaption></figure><blockquote class="kz la lb"><p id="f8be" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated"><strong class="jr hj">观察:- </strong></p><p id="b1c1" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">从图中可以明显看出数据是高度不平衡的。</p><p id="7062" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">大约有5000个积极点和20000个消极点。</p><p id="1df0" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">不平衡比例几乎是4:1(负:正)</p><p id="f4e2" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">不平衡背后的原因是有将近11000个数据点被分类为无肺部阴影/不正常，但是这些点也被认为是阴性的。</p></blockquote><h2 id="6568" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">边界框X坐标</h2><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nt"><img src="../Images/3b8c865ed7e5ddebebd4981cc87f6818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DPA4ntF--VO4iQip85VSOA.jpeg"/></div></div><figcaption class="np nq et er es nr ns bd b be z dx translated">边界框X坐标</figcaption></figure><blockquote class="kz la lb"><p id="8c31" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated"><strong class="jr hj">观察:- </strong></p><p id="b528" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们在x坐标的PDF中得到的双钟曲线是由于肺的位置(因为不透明度边界框仅在肺上)。</p><p id="d367" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">与没有肺的区域相比，穿过肺的x坐标具有较高的值。</p><p id="807f" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的x坐标范围是从0到800。</p><p id="8dd1" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">几乎99%的x坐标值小于750。</p><p id="9d51" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的IQR(四分位范围)是从200(第25百分位)到600(第75百分位)。</p><p id="935f" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的中间值是300(大约)</p></blockquote><h2 id="23e0" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">边界框y坐标</h2><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nt"><img src="../Images/462285b899330467088cc81428bff66c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kxayhsb0rGAjTNPSIoQIlQ.jpeg"/></div></div><figcaption class="np nq et er es nr ns bd b be z dx translated">边界框y坐标</figcaption></figure><blockquote class="kz la lb"><p id="40ba" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated"><strong class="jr hj">观察:- </strong></p><p id="08fe" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们可以在曲线中间观察到的下降是由于胸部的中心，因为在中心没有肺(边界框只在肺上)。</p><p id="ccef" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的最小值和最大值分别是0和800。</p><p id="6d96" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">几乎99%的值都在100到700之间。</p><p id="4946" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的IQR(四分位数范围)是从250(第25百分位)到500(第75百分位)。</p><p id="e3bf" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的中间值是350(大约)。</p><p id="c092" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">某些异常值也高于800。</p></blockquote><h2 id="ce29" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">边界框宽度</h2><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nt"><img src="../Images/849188571ed4fccb272559d336574ac8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KYhBV4fdGDNe7Yo3I5-lDw.jpeg"/></div></div><figcaption class="np nq et er es nr ns bd b be z dx translated">边界框宽度</figcaption></figure><blockquote class="kz la lb"><p id="d6d5" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated"><strong class="jr hj">观察:- </strong></p><p id="d74b" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的边界框宽度的PDF是近似正态分布的。</p><p id="003c" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的最小和最大值是50和400，某些异常值在400以上。</p><p id="893c" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">几乎99%的值都小于350。</p><p id="759f" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的IQR(四分位距)分别是从175到275。</p><p id="767d" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的中间值是225(大约)。</p></blockquote><h2 id="5428" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">边界框高度</h2><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nt"><img src="../Images/52b302c5b6bc8112be8dd54cfbdc1232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*04XY9pkfvp6LB4k9TogLDg.jpeg"/></div></div><figcaption class="np nq et er es nr ns bd b be z dx translated">边界框高度</figcaption></figure><blockquote class="kz la lb"><p id="934f" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated"><strong class="jr hj">观察:- </strong></p><p id="ee63" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的边界框高度的PDF是正倾斜的。</p><p id="9f57" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的最小和最大值是0和800，某些异常值在800以上。</p><p id="f04e" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">几乎99%的值都小于700。</p><p id="e4fe" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的IQR(四分位距)分别是从200到450。</p><p id="a6a3" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的中间值是300(大约)。</p></blockquote><h2 id="c957" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">边界框区域</h2><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nt"><img src="../Images/6e608c90aaaca5d6c99b7a359bceb49f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_DtgxzEsMwrOtAV22XYsNg.jpeg"/></div></div><figcaption class="np nq et er es nr ns bd b be z dx translated">边界框区域</figcaption></figure><blockquote class="kz la lb"><p id="ea0e" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">观察:-</p><p id="e4fd" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的包围盒包围盒区域的PDF是正倾斜的。</p><p id="8fc5" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的一般最小值和最大值分别是0和200000像素。</p><p id="1492" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">几乎99%的价值低于250000。</p><p id="70d2" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的IQR(四分位距)分别是从25000到100000。</p><p id="5756" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">200000以上有一定的离群值。</p></blockquote><h2 id="789e" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">边界框纵横比</h2><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nt"><img src="../Images/9041d341bcf595b675bbca3356721f17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_K_H0Z_M3B-DMirQGtu3HA.jpeg"/></div></div><figcaption class="np nq et er es nr ns bd b be z dx translated">边界框纵横比</figcaption></figure><blockquote class="kz la lb"><p id="973e" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">观察:-</p><p id="913c" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的包围盒包围盒区域的PDF是正倾斜的。</p><p id="6319" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的一般最小值和最大值分别是0和2。</p><p id="bbb9" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">几乎99%的值都小于2。</p><p id="8328" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们得到的IQR(四分位距)分别是从0.5到1。</p><p id="9718" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">1.75以上有一定的离群值。</p></blockquote><h2 id="fe2d" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">患者年龄</h2><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nt"><img src="../Images/ab897ac6f6d3e847feff5b41d1bbe225.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QYTagnkUE1sUEP1SKS5uNg.jpeg"/></div></div><figcaption class="np nq et er es nr ns bd b be z dx translated">患者年龄</figcaption></figure><blockquote class="kz la lb"><p id="654c" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">观察:-</p><p id="1669" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">病人的年龄范围从15岁到95岁。</p><p id="27f2" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">在140年和160年之间有一些值肯定是异常值。</p><p id="6925" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">30岁至95岁的年龄组极易患这种疾病。</p><p id="ca2e" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">IQR范围分别为30至60岁。</p><p id="1869" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">患者的平均年龄为50岁。</p></blockquote><h2 id="3e0a" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">按性别和目标分列的年龄分布</h2><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nv"><img src="../Images/125a129d19f846c0a9cd4cd3c207e3f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SX79d599Cqy9pTZlHC0fsg.jpeg"/></div></div><figcaption class="np nq et er es nr ns bd b be z dx translated">按性别和目标分列的年龄分布</figcaption></figure><blockquote class="kz la lb"><p id="2885" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated"><strong class="jr hj">观察</strong> :-</p><p id="1d96" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">对于Target=0和Target=1，两种性别的分布完全重叠。</p><p id="566c" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">这两个关系看起来几乎相同，这解释了Target = 0和Target = 1在年龄方面遵循相同趋势的事实。</p><p id="73de" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">它解释了疾病和遗传之间没有任何特定关系的事实。</p><p id="d967" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">甚至当我们将这个基于目标和目标的图与上面解释的基于总年龄的图进行比较时，我们可以很容易地观察到，无论目标和目标如何，两者都遵循相同的趋势。</p></blockquote><h2 id="0a77" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">边界框质心图(检测边界框的异常值)</h2><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nw"><img src="../Images/05e91ffe36a3e7111ca49ce8d3fe1582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v37LBFwf_lRKCQUhi9Y7TA.jpeg"/></div></div><figcaption class="np nq et er es nr ns bd b be z dx translated">边界框质心图(检测边界框的异常值)</figcaption></figure><blockquote class="kz la lb"><p id="5388" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated"><strong class="jr hj">观察:- </strong></p><p id="8c65" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们可以观察到两个肺的中心区域是高度密集的，具有最大数量的包围盒质心。</p><p id="f5c5" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">一旦我们远离中心，包围盒的密度就逐渐减小。</p><p id="cb9c" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">我们在肺部边缘看到的红叉是异常值。</p><p id="7b29" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">通过去除这些异常值，我们可以消除异常值的影响。</p></blockquote><h2 id="5fda" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">长宽比与面积</h2><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nx"><img src="../Images/ccc6c90c110f41c59c0477be8027e736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ss2dMfDy8_GBfEd2RgbVGg.jpeg"/></div></div><figcaption class="np nq et er es nr ns bd b be z dx translated">长宽比与面积</figcaption></figure><blockquote class="kz la lb"><p id="b7d7" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated"><strong class="jr hj">观察:- </strong></p><p id="ec34" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">该图描绘了边界纵横比和面积之间的反比关系。</p><p id="a0df" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">纵横比一增加，面积就减小，反之亦然。</p><p id="10d3" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">具有最大高度的边界框具有较小的宽度&amp;反之亦然。</p><p id="768a" class="jp jq lc jr b js ku ij ju jv kv im jx ld kw ka kb le kx ke kf lf ky ki kj kk hb bi translated">某些边界框具有非常高的椭圆率。</p></blockquote></div><div class="ab cl mi mj gp mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="hb hc hd he hf"><h1 id="1833" class="ix iy hi bd iz ja mp jc jd je mq jg jh io mr ip jj ir ms is jl iu mt iv jn jo bi translated"><strong class="ak">现有方法</strong></h1><h2 id="fc2b" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated"><a class="ae my" href="https://arxiv.org/abs/1811.08939" rel="noopener ugc nofollow" target="_blank">胸片中的肺炎检测深度放射学<br/>团队</a></h2><ul class=""><li id="893b" class="lu lv hi jr b js jt jv jw jy na kc nb kg nc kk mz ma mb mc bi translated">CoupleNet构成了检测算法的基础。虽然<br/>回顾了几种开源对象检测架构<br/>，但是从实验中发现CoupleNet <br/>产生了最强的结果。</li><li id="5349" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">在CoupleNet中，图像通过基础网络传递，基础网络<br/>将图像输入区域建议网络(RPN)以生成<br/>建议。每份提案都被传递给两个分支机构。第一个<br/>使用位置敏感感兴趣区域(PSRoI)池来<br/>捕获本地信息。第二个分支提取全局<br/>上下文信息。这两个分支的输出被合并<br/>以基于局部和全局<br/>信息生成预测。CoupleNet架构图示如下<br/>所示:-</li></ul><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es ny"><img src="../Images/d4e550d2227c6c553ae60f5f5df012c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BQHN_JhdbBhsuZmLgxC2Bw.png"/></div></div><figcaption class="np nq et er es nr ns bd b be z dx translated">耦合网络体系结构</figcaption></figure><ul class=""><li id="1cdc" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">他们使用多任务损失端到端地训练他们的模型，并且<br/>在由训练集的<br/> 10%分层样本组成的验证集上评估超参数。</li><li id="73b6" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">在训练过程中，他们使用随机梯度下降(SGD) <br/>，初始学习率为0.001。这个学习率在10个时期<br/>下降了90%,并且他们的模型在<br/>总共训练了14个时期。</li><li id="3d35" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">他们将前景IoU阈值设置为0.30，因此ROI<br/>包含边界<br/>框坐标和类别分数的一小部分真实情况。</li><li id="0973" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">在训练期间，他们通过随机水平<br/>翻转以及批量内随机重新缩放来扩充数据。</li><li id="5c16" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">最终预测由在整个训练集上训练的四个模型<br/>的集合生成。每个模型都产生<br/>独特的预测，其中，他们只考虑置信阈值为0.50或以上的边界<br/>框。</li><li id="b992" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">三个因素在他们的最终竞赛得分中发挥了重要作用，最终在RSNA <br/>肺炎检测挑战赛中产生了一个获胜的解决方案。首先，选择一个具有全球和本地上下文的架构<br/>，比如CoupleNet，为生成准确的结果提供了额外的<br/>上下文。第二，在培训时使用合理的<br/>前景和背景提案阈值<br/>调整我们的网络，使其在竞争中表现出色。</li></ul><h2 id="ccb4" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated"><a class="ae my" href="https://arxiv.org/abs/1711.05225" rel="noopener ugc nofollow" target="_blank"> CheXNet:通过Pranav Rajpurkar &amp;其他</a>的深度学习对胸部x光片进行放射科医师级别的肺炎检测<br/></h2><ul class=""><li id="61a4" class="lu lv hi jr b js jt jv jw jy na kc nb kg nc kk mz ma mb mc bi translated">ChexNet是一个121层的卷积神经网络，它输入一个<br/>胸部x光图像，并输出肺炎的概率<br/>以及一个热图，该热图定位图像中最能表明肺炎的区域<br/>。</li><li id="22c2" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">他们使用密集连接和批量标准化来使<br/>这样一个深度网络的优化变得容易处理。</li><li id="dd8c" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">肺炎检测任务是二进制分类问题，<br/>其中输入是正面胸部X射线图像X，<br/>输出是二进制标签y，分别指示ε(0，1)肺炎的不存在或<br/>存在。</li><li id="a063" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">CheXNet是在ChestX-ray 14数据集上训练的121层密集卷积网络(DenseNet) <br/>。</li><li id="5776" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">DenseNets改善了通过网络的信息流和梯度，使得非常深的网络的优化变得容易处理。他们用一个有单一输出的<br/>取代了最后一个全连接层，之后他们应用了一个sigmoid <br/>非线性。</li><li id="a524" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">网络的权重用来自ImageNet上预先训练的<br/>模型的权重初始化。使用具有标准参数(β 1 = 0.9和<br/> β 2 = 0.999)的Adam对网络进行端到端训练<br/>。他们用16个小批量来训练这个模型。他们使用0.001的初始学习率，每次验证损失在一个<br/>时期后达到稳定状态时，该学习率就会衰减<br/>10倍，然后选择验证损失最低的模型。</li><li id="05cf" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">他们使用了由Wang等人<br/> (2017)发布的ChestX-ray14数据集，该数据集包含112，120张正面X射线图像，涉及<br/> 30，805名独特的患者。</li><li id="b401" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">使用放射学报告上的自动提取方法，用多达14种不同的胸部病理学<br/>标签对每张图像进行注释。<br/>将肺炎作为注释的<br/>病理之一的图像标记为阳性样本，将所有其他图像标记为<br/>阴性样本。对于肺炎检测任务，他们<br/>将数据集随机分为训练(28744名患者，98637张<br/>图像)、验证(1672名患者，6351张图像)和测试(389名<br/>患者，420张图像)。<br/>组之间没有患者重叠。在将图像输入网络之前，他们<br/>将图像缩小到224x224，并根据ImageNet训练<br/>集中图像的<br/>平均值和标准偏差进行标准化。他们还通过随机水平翻转来增加训练数据。</li><li id="8a58" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">CheXNet在ChestX-ray14数据集中的所有14种<br/>病理上的表现优于已发表的最佳结果。在检测肿块、<br/>结节、肺炎和肺气肿时，CheXNet比之前的最新结果多出<br/> &gt; 0.05 AUROC，如下表<br/>所示:</li></ul><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es nz"><img src="../Images/24ecc5155ac97309eefb1c3c13fdd52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qj2zC4J-e3WLmCxvZZQBAg.png"/></div></div></figure><ul class=""><li id="66b5" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">CheXNet使用类别激活<br/>图对其识别的病理进行定位，该图突出显示了对特定病理分类最重要的X射线区域。每张图像的<br/>标题由一名实习<br/>放射科医生提供。</li></ul><h2 id="28e1" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated"><a class="ae my" href="https://www.kaggle.com/jonnedtc/cnn-segmentation-connected-components" rel="noopener ugc nofollow" target="_blank"> CNN分割Jonne的连通分量内核</a></h2><ul class=""><li id="f64c" class="lu lv hi jr b js jt jv jw jy na kc nb kg nc kk mz ma mb mc bi translated">卷积神经网络用于分割图像，<br/>直接使用边界框作为掩模。</li><li id="541b" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">连接组件用于分离<br/>预测肺炎的多个区域。</li><li id="50ab" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">围绕每个连接的<br/>组件简单地绘制边界框。</li><li id="4579" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">该网络由多个具有<br/>卷积的残差块和具有最大池的下采样块组成。</li><li id="ca4e" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">在网络的末端，单个上采样层将<br/>输出转换为与输入相同的形状。</li><li id="1c09" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">由于网络的输入是256×256(而不是原来的<br/>1024×1024 ),并且网络在没有任何有意义的上采样的情况下多次<br/>下采样(最后的上采样只是<br/>以匹配256×256掩码),所以最终的预测非常粗略。如果<br/>网络向下采样4次，最终边界框可以<br/>仅改变至少16个像素。</li><li id="f153" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">数据集太大，内存容纳不下，使用生成器<br/>动态加载数据。</li><li id="d891" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">生成器接受一些文件名、batch_size和其他<br/>参数。</li><li id="590f" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">生成器随机输出一批NumPy图像和<br/> NumPy遮罩。</li><li id="0b37" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">模型损失、准确性和iou得分图如下</li></ul><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es oa"><img src="../Images/8c5f92dcacd1236ff74e1bd0a4cd0b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DtTSaKywIOiMvkCySOE0LQ.png"/></div></div></figure></div><div class="ab cl mi mj gp mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="hb hc hd he hf"><h1 id="7bae" class="ix iy hi bd iz ja mp jc jd je mq jg jh io mr ip jj ir ms is jl iu mt iv jn jo bi translated"><strong class="ak">第一次切割方法</strong></h1><ol class=""><li id="ba46" class="lu lv hi jr b js jt jv jw jy na kc nb kg nc kk lz ma mb mc bi translated">由于数据是高度不平衡的(几乎4:1的负比正比)，我们对少数类图像(正类)执行图像增强，以便两个类之间的比率变得几乎相等。</li><li id="1d81" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated">我们采用预训练的ChexNet多类分类模型(因为ChexNet被训练用于14个类),并通过替换其最终密集层转换成二元分类模型(以对肺炎进行阳性和阴性分类)。</li><li id="71c3" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated">我们冻结了ChexNet(二元分类器)的最初几层。</li><li id="9dc9" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated">然后，我们使用自己的扩充数据重新训练ChexNet(二元分类器)。</li><li id="5e95" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated">我们保存了经过训练的ChexNet(二元分类器)模型的权重。</li><li id="b529" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated">我们采用了用作YoloV3主干的DarkNet53架构，并将其与我们通过适当分析层输入和输出维度训练的ChexNet(二进制分类器)模型进行了比较。</li><li id="fc72" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated">然后我们将YoloV3中的DarkNet53模型替换为我们通过连接ChexNet作为YoloV3主干而训练的ChexNet模型。</li><li id="81d6" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated">最后，我们以ChexNet为骨干，使用Yolo-loss、锚盒和非最大值抑制对整个YoloV3进行重新训练，得到最终的不透明度包围盒预测。</li></ol></div><div class="ab cl mi mj gp mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="hb hc hd he hf"><h1 id="baa2" class="ix iy hi bd iz ja mp jc jd je mq jg jh io mr ip jj ir ms is jl iu mt iv jn jo bi translated"><strong class="ak">训练ChexNet(二值图像分类模型)</strong></h1><h2 id="2e18" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">数据预处理</h2><p id="4786" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated"><strong class="jr hj"> DICOM到JPG的转换:- </strong></p><ul class=""><li id="a7d9" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">由于图像存在于* <strong class="jr hj">。DCM</strong>(DICOM-医学数字成像和通信)格式，我们必须转换成jpg或png格式。</li><li id="736b" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">去表演。dcm到。jpg转换我们使用了以下函数:-</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="67cf" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">该函数返回一个包含图像目录和目标值(类别标签)的数据帧，这对<code class="du mu mv mw mx b">ImageDataGenerator</code>很有用。</li></ul><p id="c7b0" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated"><strong class="jr hj">图像增强:- </strong></p><ul class=""><li id="d561" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">由于图像的形式非常不均衡，我们使用来自<code class="du mu mv mw mx b">albumentation</code>库的<code class="du mu mv mw mx b">transform</code>对象对少数族裔图像进行了图像增强，如下所示</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="3135" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">上面的函数执行扩充并返回一个包含图像目录和目标(类标签)的<code class="du mu mv mw mx b">DataFrame</code>，它将在<code class="du mu mv mw mx b">ImageDataGenerator</code>中使用。</li><li id="0fd8" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">最后，我们将扩充的数据与原始可用数据相结合。</li></ul><p id="82db" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated"><strong class="jr hj">创建</strong> <a class="ae my" href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hj">图像数据生成器</strong> </a> <strong class="jr hj">用于模型训练:- </strong></p><ul class=""><li id="7df0" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">使用<code class="du mu mv mw mx b">ImageDataGenerator</code>的原因是我们的数据集很大，无法放入ram，ImageDatagenerator让我们很容易，因为它在模型训练时读取图像，因此很有效。</li><li id="6b69" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">之前，我们将原始数据和增强数据结合起来，创建了一个包含图像目录和两个原始图像的目标(类标签)的<code class="du mu mv mw mx b">DataFrame</code>增强图像，它在<code class="du mu mv mw mx b">ImageDataGenerator</code>的<code class="du mu mv mw mx b">flow_from_dataframe</code>方法中使用，如下所示</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><h2 id="6a12" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">创建模型</h2><ul class=""><li id="a866" class="lu lv hi jr b js jt jv jw jy na kc nb kg nc kk mz ma mb mc bi translated">我们首先使用<code class="du mu mv mw mx b">DenseNet121</code> <code class="du mu mv mw mx b">tensorflow</code>创建ChexNet基本模型(14类)，以便加载<a class="ae my" href="https://github.com/brucechou1983/CheXNet-Keras" rel="noopener ugc nofollow" target="_blank">可用权重</a>(因为模型权重可用于14类分类模型，而不可用于我们的二元分类器):-</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="eebc" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">我们冻结了<code class="du mu mv mw mx b">base_model</code>的最初几层，因为我们拥有的初始权重也来自使用胸片数据训练的模型:-</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="68c6" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">然后使用上面的<code class="du mu mv mw mx b">base_model</code>我们创建了我们的ChexNet(二进制分类器)，用一个单元<code class="du mu mv mw mx b">Dense</code>层替换最后的密集层</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="8d7b" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">我们使用<code class="du mu mv mw mx b">accuracy</code>和<code class="du mu mv mw mx b">f1_score</code>作为带有<code class="du mu mv mw mx b">adam</code>优化器和<code class="du mu mv mw mx b">binary_crossentropy</code>损失的指标。</li><li id="1f4c" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">ChexNet(二进制分类器)模型总结如下(它很长，但如果不包括它，我们将无法获得要点):-</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="af49" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">您可能已经观察到模型的输入维度为<code class="du mu mv mw mx b">416x416</code>，这是因为YoloV3的输入维度也是<code class="du mu mv mw mx b">416x416</code>，我们将使用这个模型作为YoloV3的主干。</li></ul><h2 id="83ac" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">模型性能</h2><p id="4f71" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated"><strong class="jr hj">精度与历元:- </strong></p><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es od"><img src="../Images/e6597f0e5e877f0b426ad47366a90b20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aZBm_uU-7zC_PY7XKfbLzQ.jpeg"/></div></div></figure><p id="e8f8" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated"><strong class="jr hj">F1 _分数vs纪元:- </strong></p><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es od"><img src="../Images/9f2c26b85e858d6f5fdb19304f12f448.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HMek0fCq_cbZbEaG5KGTEA.jpeg"/></div></div></figure><p id="7ea4" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated"><strong class="jr hj">损失对比纪元图:- </strong></p><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es od"><img src="../Images/e71d3a4813b14637d29be564ec5f01bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BAIwXfCLtlnqd-oOhEzYig.jpeg"/></div></div></figure><ul class=""><li id="bd7b" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">我们得到的最好的模型性能是第3历元的<strong class="jr hj"> val_loss: 0.2554，val_accuracy: 0.8832，val_f1_m: 0.8658 </strong>。</li></ul></div><div class="ab cl mi mj gp mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="hb hc hd he hf"><h1 id="1762" class="ix iy hi bd iz ja mp jc jd je mq jg jh io mr ip jj ir ms is jl iu mt iv jn jo bi translated"><strong class="ak">用ChexNet模型替换YoloV3主干网并对其进行重新训练</strong></h1><h2 id="c113" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">数据预处理</h2><p id="b7c3" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated"><strong class="jr hj">将数据格式化为</strong><a class="ae my" href="http://host.robots.ox.ac.uk/pascal/VOC/" rel="noopener ugc nofollow" target="_blank"><strong class="jr hj">Pascal VOC</strong></a><strong class="jr hj">格式&amp;创建TFRecords:- </strong></p><ul class=""><li id="1804" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">由于我们已经利用了张子豪的YoloV3实现(<a class="ae my" href="https://github.com/zzh8829/yolov3-tf2" rel="noopener ugc nofollow" target="_blank"> GitHub </a>)，我们必须将我们的数据集转换成<a class="ae my" href="http://host.robots.ox.ac.uk/pascal/VOC/" rel="noopener ugc nofollow" target="_blank"> Pascal VOC </a>对于每个CXR图像，我们必须创建以下格式的XML注释文件:-</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="60d3" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">为了为每个CXR图像创建上述注释文件，我们实现了以下函数:-</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="e89a" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">我们必须创建一个. names文件，在新的一行中包含每个类的名称，因为在我们的例子中，我们只有一个类“opacity”。</li><li id="37e9" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">我们必须创建两个文件文本文件，第一个包含训练CXR图像的名称，第二个包含验证CXR图像的名称(名称应该没有扩展名)</li><li id="d4f6" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">我们必须遵循指定的目录结构<a class="ae my" href="http://host.robots.ox.ac.uk/pascal/VOC/" rel="noopener ugc nofollow" target="_blank"> Pascal VOC </a>格式。</li><li id="cbee" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">最后，我们必须使用张子豪(<a class="ae my" href="https://github.com/zzh8829/yolov3-tf2" rel="noopener ugc nofollow" target="_blank"> GitHub </a>)提供的脚本将数据集转换成训练和验证<a class="ae my" href="https://www.tensorflow.org/tutorials/load_data/tfrecord" rel="noopener ugc nofollow" target="_blank"> TFRecords </a></li></ul><p id="2f82" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated"><strong class="jr hj">生成锚盒:- </strong></p><ul class=""><li id="aaf5" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">我们已经使用使用了k-means算法的<a class="ae my" href="https://github.com/AlexeyAB/darknet/blob/master/scripts/gen_anchors.py" rel="noopener ugc nofollow" target="_blank">这个</a>脚本为我们的数据生成了锚定框，但是要使用该脚本，我们必须为我们的每个CXR图像创建文本文件，该文件包含边界框信息，其格式为每个边界框信息应该是(<strong class="jr hj">object_class x _ mid y _ mid width height)</strong>其中object _ class是对象的类索引，x_mid &amp; y_mid是边界框的中心坐标，所有这些值都应该标准化。</li><li id="a175" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">为了将我们的数据转换成上述格式，我们实现了以下函数:-</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="26a3" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">在上述转换之后，我们使用<a class="ae my" href="https://github.com/AlexeyAB/darknet/blob/master/scripts/gen_anchors.py" rel="noopener ugc nofollow" target="_blank">脚本</a>来计算锚盒，我们得到的锚盒是:-</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="7136" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">总共有9个锚盒，YoloV3在3个尺度上执行检测，因此在每个尺度上使用3个锚盒。</li></ul><h2 id="ce91" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">在YoloV3中用ChexNet替换DarkNet53</h2><ul class=""><li id="7be9" class="lu lv hi jr b js jt jv jw jy na kc nb kg nc kk mz ma mb mc bi translated">让我们首先观察输入形状为<code class="du mu mv mw mx b">416x416</code>的DarkNet53模型的概要</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="34e8" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">让我们观察暗网53的输出形状</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="8b46" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">正如我们可以从DarkNet53返回的输出形状中观察到的，我们得到了三种形状的输出，即(52，52，256)，(26，26，512)和(13，13，1024)，所有这三种输出都被传递给YoloV3前端模型。</li><li id="8c2f" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">让我们将它与以下三层ChexNet返回的输出进行比较</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="480a" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">我们可以观察到上面三层<strong class="jr hj"> ChexNet </strong>的输出与<strong class="jr hj"> DarkNet的输出完美匹配。</strong></li><li id="c640" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">我们要做的是修改ChexNet的输出，使其返回上述三层的输出，这就是我们在以下函数中所做的</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="eb0b" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated"><code class="du mu mv mw mx b">chexnet_weights</code>是我们训练ChexNet二元分类模型后得到的权重。</li><li id="fbf3" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">为了得到特定层的输出，我们使用了tf.keras.Model API的<code class="du mu mv mw mx b"><a class="ae my" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model#get_layer" rel="noopener ugc nofollow" target="_blank">get_layer()</a></code>方法。</li><li id="0e54" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">为了将上述ChexNet主干与YoloV3连接起来，我们修改了张子豪(<a class="ae my" href="https://github.com/zzh8829/yolov3-tf2" rel="noopener ugc nofollow" target="_blank"> GitHub </a>)的如下函数</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="6106" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">我们传递了一个带有值<code class="du mu mv mw mx b">darknet</code>和<code class="du mu mv mw mx b">chexnet</code>的参数<code class="du mu mv mw mx b">backbone</code>，以保持它的模块化，并相应地改变YoloV3的主干。</li><li id="a5ca" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">要获得带有ChexNet主干的YoloV3模型，我们必须做的是用<code class="du mu mv mw mx b">backbone=’chexnet’</code>调用上面的<code class="du mu mv mw mx b">YoloV3</code>函数。</li><li id="87ca" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">这就是我们如何用ChexNet替换YoloV3主干DrakNet的。</li></ul><h2 id="9b9e" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">比较YoloV3_DarNet和YoloV3_ChexNet</h2><ul class=""><li id="ebf7" class="lu lv hi jr b js jt jv jw jy na kc nb kg nc kk mz ma mb mc bi translated">让我们观察一下YoloV3_DarkNet模型的概要:-</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="9dcb" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">现在让我们观察一下YoloV3_ChexNet模型的概要:-</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="3fc6" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">比较YoloV3_DarkNet和YoloV3_ChexNet的参数:-</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="9f28" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">正如我们所观察到的，yolov3_chexnet中的参数数量几乎是yolov3_darknet的一半，因此yolov3_chexnet非常轻便，训练速度也很快。</li><li id="33c2" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">另一件事是DarNet53是在ImageNet数据集上训练的，这就是为什么它在检测容易看到和区分的对象时工作得非常好，但是使用它来检测不可区分的肺炎阴影等问题是不可行的，因此在胸部x光照片上训练的ChexNet将是这个问题的完美选择。</li></ul><h2 id="d38b" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">培训YoloV3_ChexNet</h2><ul class=""><li id="56c8" class="lu lv hi jr b js jt jv jw jy na kc nb kg nc kk mz ma mb mc bi translated">我们只使用正面图像训练YoloV3_ChexNet，其背后的原因是YoloV3和大小为<code class="du mu mv mw mx b">416x416</code>的图像被细分为<code class="du mu mv mw mx b">13x13</code> <code class="du mu mv mw mx b">(as 416/32 = 13)</code>的网格，并对<code class="du mu mv mw mx b">13x13</code>网格的每个网格单元进行预测，如果锚盒的数量为<code class="du mu mv mw mx b">3</code>，则<code class="du mu mv mw mx b">13x13</code>网格中的每个网格单元也与<code class="du mu mv mw mx b">3</code>锚盒相关联，这使得总维度等于<code class="du mu mv mw mx b">13x13x3=507</code>。这意味着对于单个图像，我们进行<code class="du mu mv mw mx b">507</code>预测，并且即使图像是正面的并且其中存在<code class="du mu mv mw mx b">2</code>不透明度，我们也将只有<code class="du mu mv mw mx b">2</code>正面预测和<code class="du mu mv mw mx b">507–2 = 505</code>负面预测，因此已经存在大多数负面预测。如果我们再次将负面图像添加到模型中进行训练，这将使我们的模型偏向于负面类。这就是我们只使用正面图像来训练模型的原因。</li><li id="e335" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">转换后的训练特征和标签的形状如下</li></ul><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es oe"><img src="../Images/1e9035c7d498eb4ea65a219c46fd0da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3EnozpUFlCkX_Ihrqpm4HQ.png"/></div></div></figure><ul class=""><li id="9bd8" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">其中8是样本批次大小，416x416x3是输入图像尺寸。</li><li id="efdc" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">在标签中，8是批量大小，13x13是网格大小，每个网格单元与3个锚点相关联，对于每个锚点，我们有一个6维数组，用于边界框、对象和格式为<strong class="jr hj"> (x_min，y_min，x_max，y_max，objectness_score，class) </strong>的类信息，在我们的示例中，只有一个类，如果有更多的类，则该数组的大小将随着类的增加而增加</li><li id="8f01" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">我们使用为<code class="du mu mv mw mx b">epochs=30</code>、<code class="du mu mv mw mx b">batch-size=16</code>、<code class="du mu mv mw mx b">optimizer=adam</code>和<code class="du mu mv mw mx b">learning_rate=1e-3</code>和<code class="du mu mv mw mx b">loss=YoloLoss</code>生成的TFRecods来训练YoloV3ChexNet</li></ul><p id="1216" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated"><strong class="jr hj">车型性能:- </strong></p><ul class=""><li id="0f71" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">纪元与损失图:-</li></ul><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es of"><img src="../Images/94c13166993d0df11a8f35a3f1029360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OXPejz9eDMJ55HZgB-ja_g.jpeg"/></div></div></figure><ul class=""><li id="29ac" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">Epoch vs yolo_output_0_loss</li></ul><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es og"><img src="../Images/b2b51b639f8d0a335d9c70184997b785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IClgzAffWo0BB2lhpZ07bA.jpeg"/></div></div></figure><ul class=""><li id="1368" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">Epoch vs yolo_output_1_loss</li></ul><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es oh"><img src="../Images/e4ee4969e1fa429a8c73c4f697258d43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ypv782I93HJwHfC0CBPr1Q.jpeg"/></div></div></figure><ul class=""><li id="d427" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">Epoch vs yolo_output_2_loss:-</li></ul><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es og"><img src="../Images/f56cabb7dd451fd1dfccd99f63d3e052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xsMruvh1lVeDLh9VpCjljQ.jpeg"/></div></div></figure><h2 id="5566" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">推理YoloV3_ChexNet</h2><ul class=""><li id="82a4" class="lu lv hi jr b js jt jv jw jy na kc nb kg nc kk mz ma mb mc bi translated">在推断时，通过添加lambda层来修改模型的最终层，因为存在Yolo _ boxes和非最大抑制的额外计算。</li><li id="b9d4" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">在推理时，模型摘要如下所示</li></ul><figure class="ne nf ng nh fd ni"><div class="bz dy l di"><div class="ob oc l"/></div></figure><ul class=""><li id="4eb4" class="lu lv hi jr b js ku jv kv jy lw kc lx kg ly kk mz ma mb mc bi translated">几个预测:-</li></ul><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es oi"><img src="../Images/daaef235deee786faa09aa591ea48b26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QXBJJy05y5LvOOlIZkirXQ.jpeg"/></div></div></figure><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es oj"><img src="../Images/08c0758301583f26b97acc9fadf61c90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zsWArkEklJ9HNhjF3B6Uog.jpeg"/></div></div></figure><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es ok"><img src="../Images/8fa8288cef4b46faf5e8e44e19720eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v6-GWyqqRAyYSV91wforzA.jpeg"/></div></div></figure><figure class="ne nf ng nh fd ni er es paragraph-image"><div role="button" tabindex="0" class="nj nk di nl bf nm"><div class="er es ol"><img src="../Images/31e2086546f5fa6b7d490c3869c1f644.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gHcgXpiWwWm7v-mAI-qdYA.jpeg"/></div></div></figure><h2 id="7e63" class="lg iy hi bd iz lh li lj jd lk ll lm jh jy ln lo jj kc lp lq jl kg lr ls jn lt bi translated">摘要:-</h2><ul class=""><li id="9ea0" class="lu lv hi jr b js jt jv jw jy na kc nb kg nc kk mz ma mb mc bi translated"><strong class="jr hj">yolo v3 _ darknet</strong>共有<strong class="jr hj">61576342</strong>个参数，<strong class="jr hj"> yoloV3_chexnet </strong>共有<strong class="jr hj">27993206</strong>个参数。</li><li id="9d4f" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">在<strong class="jr hj"> yoloV3_darknet </strong>的对比中，<strong class="jr hj"> yoloV3_chexnet </strong>的参数数量几乎是<strong class="jr hj"> yoloV3_chexnet </strong> light &amp;快速训练的一半。</li><li id="3aa0" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">我们设置<strong class="jr hj">yolo _ iou _ threshold = 0.5</strong>&amp;<strong class="jr hj">yolo _ score _ threshold = 0.2</strong>，因为在与正常物体的比较中，不透明度很难识别。</li><li id="e401" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">我们使用<strong class="jr hj"> k-means </strong>计算定制锚盒。</li><li id="4c77" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">我们得到的最终验证损失是<strong class="jr hj">val _ loss:7.4992—val _ yolo _ output _ 0 _ loss:6.0457—val _ yolo _ output _ 1 _ loss:9.3626 e-04—val _ yolo _ output _ 2 _ loss:0.0028</strong>。</li><li id="5812" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">由于数据集相当复杂，而且不透明度边界框人眼无法分辨，我们用<strong class="jr hj"> yoloV3_chexnet </strong>得到了相当不错的结果(如果没有达到标准)</li></ul></div><div class="ab cl mi mj gp mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="hb hc hd he hf"><h1 id="190f" class="ix iy hi bd iz ja mp jc jd je mq jg jh io mr ip jj ir ms is jl iu mt iv jn jo bi translated"><strong class="ak">未来工作</strong></h1><ul class=""><li id="65e5" class="lu lv hi jr b js jt jv jw jy na kc nb kg nc kk mz ma mb mc bi translated">我们可以为其他医学图像训练这个模型，也可以将其用于其他特定的疾病诊断</li><li id="e531" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">该模型可以被转换成完全成熟的应用程序或API，其可以被医疗从业者用作医疗诊断中的初始筛选工具。</li><li id="5264" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk mz ma mb mc bi translated">由于我们已经根据医疗保健中的特定应用将YoloV3主干替换为ChexNet，并获得了最佳结果，因此我们也可以将其替换为最适合其他应用(如空间研究、海洋应用)的其他型号。</li></ul></div><div class="ab cl mi mj gp mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="hb hc hd he hf"><h1 id="05a7" class="ix iy hi bd iz ja mp jc jd je mq jg jh io mr ip jj ir ms is jl iu mt iv jn jo bi translated"><strong class="ak"> Github回购</strong></h1><ul class=""><li id="bcca" class="lu lv hi jr b js jt jv jw jy na kc nb kg nc kk mz ma mb mc bi translated">如果您对这个案例研究感兴趣，或者想进一步改进它，那么包含全部代码的Jupyter Notebook可以在我下面的repo中找到</li></ul><div class="om on ez fb oo op"><a href="https://github.com/junaidnasirkhan/Replacing-YoloV3-Backbone-with-ChexNet-for-Pneumonia-Detection" rel="noopener  ugc nofollow" target="_blank"><div class="oq ab dw"><div class="or ab os cl cj ot"><h2 class="bd hj fi z dy ou ea eb ov ed ef hh bi translated">junaidnasirkhan/用ChexNet替换-yolov 3-Backbone-用于肺炎检测</h2><div class="ow l"><h3 class="bd b fi z dy ou ea eb ov ed ef dx translated">构建深度学习模型，自动检测和定位胸部潜在的肺炎肺部阴影…</h3></div><div class="ox l"><p class="bd b fp z dy ou ea eb ov ed ef dx translated">github.com</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd nn op"/></div></div></a></div></div><div class="ab cl mi mj gp mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="hb hc hd he hf"><h1 id="4cd8" class="ix iy hi bd iz ja mp jc jd je mq jg jh io mr ip jj ir ms is jl iu mt iv jn jo bi translated">参考</h1><ol class=""><li id="1fc2" class="lu lv hi jr b js jt jv jw jy na kc nb kg nc kk lz ma mb mc bi translated"><a class="ae my" href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" rel="noopener ugc nofollow" target="_blank">https://pjreddie.com/media/files/papers/YOLOv3.pdf</a></li><li id="ccec" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><a class="ae my" href="https://arxiv.org/abs/1811.08939" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1811.08939</a></li><li id="f493" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><a class="ae my" href="https://arxiv.org/abs/1711.05225" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1711.05225</a></li><li id="61f8" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><a class="ae my" href="https://www.kaggle.com/c/rsna-pneumonia-detection-challenge/overview" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/rsna-肺炎-检测-挑战/概述</a></li><li id="1bb9" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><a class="ae my" href="https://github.com/zzh8829/yolov3-tf2/blob/master/docs/training_voc.md" rel="noopener ugc nofollow" target="_blank">https://github . com/zzh 8829/yolo v3-tf2/blob/master/docs/training _ VOC . MD</a></li><li id="26bf" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><a class="ae my" href="https://github.com/brucechou1983/CheXNet-Keras" rel="noopener ugc nofollow" target="_blank">https://github.com/brucechou1983/CheXNet-Keras</a></li><li id="438e" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><a class="ae my" href="https://github.com/AlexeyAB/darknet/blob/master/scripts/gen_anchors.py" rel="noopener ugc nofollow" target="_blank">https://github . com/AlexeyAB/darknet/blob/master/scripts/gen _ anchors . py</a></li><li id="f8e6" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated"><a class="ae my" href="https://www.kaggle.com/jonnedtc/cnn-segmentation-connected-components" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/jonnedtc/CNN-segmentation-connected-components</a></li><li id="7fca" class="lu lv hi jr b js md jv me jy mf kc mg kg mh kk lz ma mb mc bi translated">【https://www.appliedaicourse.com/ T4】</li></ol></div><div class="ab cl mi mj gp mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="hb hc hd he hf"><p id="04d2" class="pw-post-body-paragraph jp jq hi jr b js ku ij ju jv kv im jx jy kw ka kb kc kx ke kf kg ky ki kj kk hb bi translated"><em class="lc">与我连线</em><a class="ae my" href="https://www.linkedin.com/in/junaidnasirkhan/" rel="noopener ugc nofollow" target="_blank"><strong class="jr hj"><em class="lc">LinkedIn</em></strong></a><em class="lc">或</em><a class="ae my" href="https://github.com/junaidnasirkhan/" rel="noopener ugc nofollow" target="_blank"><strong class="jr hj"><em class="lc">GitHub</em></strong></a></p></div></div>    
</body>
</html>