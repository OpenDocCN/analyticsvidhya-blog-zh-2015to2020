<html>
<head>
<title>Baum-Welch algorithm for training a Hidden Markov Model — Part 2 of the HMM series</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">训练隐马尔可夫模型的Baum-Welch算法——隐马尔可夫模型系列之二</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86?source=collection_archive---------0-----------------------#2019-07-21">https://medium.com/analytics-vidhya/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86?source=collection_archive---------0-----------------------#2019-07-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="10bb" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><em class="ix">如何训练一个隐马尔可夫模型，并用于滤波平滑？</em></h2></div><p id="05ba" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">第一部分:<a class="ae ju" rel="noopener" href="/@rmwkwok/hidden-markov-model-part-1-of-the-hmm-series-3f7fea28a08">隐马尔可夫模型的体系结构</a> <br/>第二部分:<a class="ae ju" rel="noopener" href="/@rmwkwok/baum-welch-algorithm-for-training-a-hidden-markov-model-part-2-of-the-hmm-series-d0e393b4fb86">训练HMM的算法:Baum-Welch算法</a> <br/>第三部分:<a class="ae ju" rel="noopener" href="/@rmwkwok/viterbi-algorithm-for-prediction-with-hmm-part-3-of-the-hmm-series-6466ce2f5dc6">用训练好的HMM进行预测的算法:Viterbi算法</a></p><p id="14e2" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">在<a class="ae ju" rel="noopener" href="/@rmwkwok/hidden-markov-model-part-1-of-the-hmm-series-3f7fea28a08">上一篇文章</a>中，我谈到了隐马尔可夫模型(HMM)的架构和参数化，以及我将在这里使用的变量的含义。在本文中，我们将讨论在使用HMM进行<a class="ae ju" rel="noopener" href="/@rmwkwok/viterbi-algorithm-for-prediction-with-hmm-part-3-of-the-hmm-series-6466ce2f5dc6">预测</a>之前，训练HMM的算法。</p><h1 id="8041" class="jv jw hi bd jx jy jz ka kb kc kd ke kf io kg ip kh ir ki is kj iu kk iv kl km bi translated">鲍姆-韦尔奇算法</h1><p id="5cae" class="pw-post-body-paragraph iy iz hi ja b jb kn ij jd je ko im jg jh kp jj jk jl kq jn jo jp kr jr js jt hb bi translated">Baum-Welch算法也称为前向-后向算法，是一种动态规划方法，也是期望最大化算法的特例。其目的是调整隐马尔可夫模型的参数，即状态转移矩阵<strong class="ja hj"> <em class="ks"> A </em> </strong>，发射矩阵<strong class="ja hj"> <em class="ks"> B </em> </strong>，初始状态分布<strong class="ja hj"> π₀ </strong>，使模型最大限度地与观测数据相似。</p><p id="29cd" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">该算法有几个阶段，包括初始阶段、前进阶段、后退阶段和更新阶段。前向和后向阶段形成EM算法的E步骤，而更新阶段本身是M步骤。</p><h1 id="0f84" class="jv jw hi bd jx jy jz ka kb kc kd ke kf io kg ip kh ir ki is kj iu kk iv kl km bi translated">初始相位</h1><p id="898d" class="pw-post-body-paragraph iy iz hi ja b jb kn ij jd je ko im jg jh kp jj jk jl kq jn jo jp kr jr js jt hb bi translated">在初始阶段，参数矩阵<strong class="ja hj"><em class="ks">a</em></strong><strong class="ja hj"><em class="ks">b</em></strong><strong class="ja hj">【π₀】</strong>的内容被初始化，如果没有关于它们的先验知识，这可以随机进行。</p><h1 id="18c7" class="jv jw hi bd jx jy jz ka kb kc kd ke kf io kg ip kh ir ki is kj iu kk iv kl km bi translated">正向阶段</h1><p id="169f" class="pw-post-body-paragraph iy iz hi ja b jb kn ij jd je ko im jg jh kp jj jk jl kq jn jo jp kr jr js jt hb bi translated">在前向阶段，计算以下递归阿尔法函数。对于函数的偏差，我会强烈推荐<a class="ae ju" href="https://www.youtube.com/watch?v=M7afek1nEKM" rel="noopener ugc nofollow" target="_blank">这个YouTube视频</a>，因为演讲者把它呈现得很清楚，解释得很好。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/11056fbf4420d8496ffe1e5214e045f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xzb1Rceaun9FauLFOxqY5w.png"/></div></div></figure><p id="5203" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这里有几点需要说明:</p><ol class=""><li id="d914" class="lf lg hi ja b jb jc je jf jh lh jl li jp lj jt lk ll lm ln bi translated">阿尔法函数被定义为直到时间<em class="ks"> k </em>的观测数据和时间<em class="ks"> k </em>的状态的联合概率</li><li id="3dd8" class="lf lg hi ja b jb lo je lp jh lq jl lr jp ls jt lk ll lm ln bi translated">它是一个递归函数，因为alpha函数出现在等式右侧(R.H.S .)的第一项中，这意味着前一个alpha在下一个的计算中重复使用。这也是为什么它被称为前进阶段。</li><li id="7cfe" class="lf lg hi ja b jb lo je lp jh lq jl lr jp ls jt lk ll lm ln bi translated">R.H.S .的第二项是来自<strong class="ja hj">T5【A】</strong>的状态转移概率，而最后一项是来自<strong class="ja hj"> <em class="ks"> B </em> </strong>的发射概率。</li><li id="fdb2" class="lf lg hi ja b jb lo je lp jh lq jl lr jp ls jt lk ll lm ln bi translated">在时间<em class="ks"> k </em> -1，对所有可能的状态求和。</li></ol><p id="2d0c" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">需要指出的是，每个alpha都包含了到时间<em class="ks"> k </em>为止的观测数据的信息，为了得到下一个alpha，我们只需要重新使用当前alpha，并添加关于下一个状态和下一个观测变量的转换的信息。这种递归行为通过将我们从每次查看过去的观察数据中解放出来，节省了获取下一个alpha的计算。</p><p id="bdc2" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">顺便说一下，我们需要下面的起始alpha来开始递归。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lt"><img src="../Images/9c6f2a9a417c3ffa1b05b23eb140fdf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*noItQJqxWJ-i3iNOgnDWqw.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">起始α是发射和初始状态概率的乘积</figcaption></figure><h1 id="71cd" class="jv jw hi bd jx jy jz ka kb kc kd ke kf io kg ip kh ir ki is kj iu kk iv kl km bi translated">反向相位</h1><p id="c9bd" class="pw-post-body-paragraph iy iz hi ja b jb kn ij jd je ko im jg jh kp jj jk jl kq jn jo jp kr jr js jt hb bi translated">下面公式的偏差请参考<a class="ae ju" href="https://www.youtube.com/watch?v=jwYuki9GgJo" rel="noopener ugc nofollow" target="_blank">这段YouTube视频</a>。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ly"><img src="../Images/5ac66685aca828493c9925a335e2554c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YOWA4Pj_Fe2YjSViPP4HRA.png"/></div></div></figure><p id="aa88" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这里也可以提出类似的观点:</p><ol class=""><li id="b4c5" class="lf lg hi ja b jb jc je jf jh lh jl li jp lj jt lk ll lm ln bi translated">贝塔函数被定义为在给定时间<em class="ks"> k </em>的状态下，从时间<em class="ks"> k+ </em> 1观察到的数据的条件概率</li><li id="da8e" class="lf lg hi ja b jb lo je lp jh lq jl lr jp ls jt lk ll lm ln bi translated">这是一个递归函数，因为β函数出现在等式右侧的第一项，这意味着下一个β函数将在当前β函数的计算中重复使用。这也是为什么称之为逆相。</li><li id="7e04" class="lf lg hi ja b jb lo je lp jh lq jl lr jp ls jt lk ll lm ln bi translated">R.H.S .的第二项是来自<strong class="ja hj"> <em class="ks"> A </em> </strong>的状态转移概率，而最后一项是来自<strong class="ja hj"> <em class="ks"> B </em> </strong>的发射概率。</li><li id="5c18" class="lf lg hi ja b jb lo je lp jh lq jl lr jp ls jt lk ll lm ln bi translated">在时间<em class="ks"> k </em> +1，对所有可能的状态求和。</li></ol><p id="f51d" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">同样，我们需要结束β来开始递归。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lz"><img src="../Images/a6fb4f56d23cbccdad83e77fb8b2eba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*Pk-bcF3Mnf_INvnTCvV7JQ.png"/></div></figure><p id="fbb4" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="ja hj">坚持住！为什么是阿尔法和贝塔函数？</strong></p><p id="3e0f" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">好问题！</p><p id="4434" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">首先，如前所述，它们都是递归函数，这意味着我们可以重用前一个答案作为下一个答案的输入。这就是动态编程的意义所在——您可以通过重用旧的结果来节省时间！</p><p id="e3bd" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">其次，正向阶段的公式非常有用。假设你有一组训练有素的跃迁和发射参数，并且假设你的问题是实时地从观测数据中找出神秘的隐藏真相。那么你实际上可以这样做！当你得到一个数据点(数据点<em class="ks"> p </em>)时，你可以把它放入公式中，这个公式将给出相关隐藏状态的概率分布，从中你可以选择最有可能的一个作为你的答案。故事并没有就此结束，当你得到下一个数据点(数据点<em class="ks"> q </em>)时，你再把它放入公式中，它会给你另一个概率分布，让你挑选最佳选择，但这不仅是基于数据点<em class="ks"> q </em>和跃迁和发射参数，也是基于数据点<em class="ks"> p </em>。这样使用的公式叫做<strong class="ja hj"><em class="ks"/></strong>。</p><p id="c44b" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">第三，继续上面的讨论，假设你已经收集了许多数据点，因为你知道数据点越早，你的答案选择所基于的观察数据就越少。因此，你想通过某种方式将后期数据中的信息“注入”到早期数据中来改善这种情况。这就是逆向公式发挥作用的地方。这样使用的公式叫做<strong class="ja hj"> <em class="ks">平滑</em> </strong>。</p><p id="903b" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">第四，这是关于最后两段的合并。在α和β公式的帮助下，在给定整个观察数据序列的情况下，可以确定状态变量在任何时间<em class="ks"> k </em>的概率分布。这也可以从数学上理解。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ma"><img src="../Images/a9eca5c2ea3f301649029016e43479bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m2o5S6KFFINrWHw9KPjGzA.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">分母项是一个归一化常数，通常像这样去掉，因为它不依赖于状态，因此在比较任意时刻k不同状态的概率时，它并不重要。</figcaption></figure><p id="966f" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">最后，alpha和beta函数的结果在更新阶段非常有用。</p><h1 id="1fdb" class="jv jw hi bd jx jy jz ka kb kc kd ke kf io kg ip kh ir ki is kj iu kk iv kl km bi translated">更新阶段</h1><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mb"><img src="../Images/96dd87d4222780f3c3b150216c7c6360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0WPvmVsDOzNJTccpHTjAuA.png"/></div></div></figure><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mc"><img src="../Images/5cb18e578ae84f47086da874c97a81b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nd1ccDicqs33PHLhp9kwXg.png"/></div></div></figure><p id="3a82" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">对于以上公式的偏差，如果你看过我建议的正向和反向公式的YouTube视频，并且能理解，那么大概你自己推导这两个就没问题了。</p><p id="4865" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这里的第一个公式只是重复我们在上面看到的，概括一下，它告诉我们在给定所有观测数据的情况下，一个状态在时间<em class="ks"> k </em>的概率分布。然而，第二个公式告诉我们一点不同的东西，这是给定数据的两个连续状态的联合概率。他们利用阿尔法函数，贝塔函数，过渡和发射已经可用。这两个公式进一步用于最终进行更新。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es md"><img src="../Images/52d7277dffbd58318d6c67d654260316.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*ct1ntzmAl5YmNCFHtbEeAw.png"/></div></figure><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es me"><img src="../Images/63038f3c33130bdc6612ff136fb2a1a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fx87kNzEru_nCu9_ILWzHw.png"/></div></div></figure><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mf"><img src="../Images/1dbbc40ac1c135f3eef5202c78f8f5b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KAbf1GsC2Jak3TN85fhw1w.png"/></div></div></figure><p id="dc9d" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这里没有显示偏差步骤，因为数学不是本文的目的，但是显示公式本身将有助于我们了解它们是如何通过这些步骤被重用的。</p><p id="5433" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">提到了Baum-Welch算法是EM算法的一个例子。这里我将简单解释一下原因。α和β函数形成e步骤，因为它们在给定观察数据和参数矩阵<strong class="ja hj"> <em class="ks">、A </em> </strong>、<strong class="ja hj"> <em class="ks"> B </em> </strong>、<strong class="ja hj"> π₀ </strong>的情况下预测期望的隐藏状态。更新阶段是M步，因为最后三个更新公式是这样导出的，即在给定观测数据的情况下，L.H.S .参数将最好地拟合预期的隐藏状态。</p><h1 id="a08e" class="jv jw hi bd jx jy jz ka kb kc kd ke kf io kg ip kh ir ki is kj iu kk iv kl km bi translated">摘要</h1><p id="7113" class="pw-post-body-paragraph iy iz hi ja b jb kn ij jd je ko im jg jh kp jj jk jl kq jn jo jp kr jr js jt hb bi translated">Baum-Welch算法是EM算法的一个例子，在E步骤中，给定观察数据和调整前的参数矩阵集，向前和向后公式告诉我们期望的隐藏状态。然后，M步更新公式调整参数矩阵，以最佳拟合观察到的数据和预期的隐藏状态。然后这两个步骤被反复迭代，直到参数收敛，或者直到模型达到某个精度要求。</p><p id="4823" class="pw-post-body-paragraph iy iz hi ja b jb jc ij jd je jf im jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">像任何机器学习算法一样，该算法可能会过度拟合数据，因为根据定义，M-step鼓励模型尽可能好地接近观察到的数据。此外，虽然我们没有过多地讨论初始阶段，但它确实会影响模型的最终性能(因为这是一个使模型陷入局部最优的问题)，因此人们可能希望尝试不同的参数初始化方式，看看哪种方式更好。</p></div></div>    
</body>
</html>