<html>
<head>
<title>Camelyon16: Detecting Breast Cancer Metastases in Lymph Node Biopsies</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Camelyon16:在淋巴结活检中检测乳腺癌转移</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/camelyon16-detecting-breast-cancer-metastases-in-lymph-node-biopsies-8de6ae6cd4f4?source=collection_archive---------9-----------------------#2020-03-18">https://medium.com/analytics-vidhya/camelyon16-detecting-breast-cancer-metastases-in-lymph-node-biopsies-8de6ae6cd4f4?source=collection_archive---------9-----------------------#2020-03-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="bdb5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在过去的几个月里，我一直在使用Kaggle上的数据集进行癌症检测项目。[0]我在下面总结了我的工作——欢迎提出任何想法或建议！</p><h1 id="90ca" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">有什么问题？</h1><p id="2817" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">癌症可以转移或扩散到身体的其他部位，而不是它的发源地。特别是，根据卡米里恩网站，“腋下的淋巴结是乳腺癌最容易扩散的地方。”[1]如果乳腺癌已经转移到淋巴结，预后较差。然而，识别淋巴结转移是一件“乏味、耗时且容易被误解”的事情。[1]因此，Kaggle挑战和这个项目的目标是创建一个可以在淋巴结图像中自动检测乳腺癌转移的模型。这种模型可以减少病理学家的工作量，并提高诊断精度和召回率。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/a84b5dd1df88136749d7d96a019bc1a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/0*9Sic_9wCMqsGlgtj.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">乳腺癌可以转移到腋下的淋巴结。</figcaption></figure><h1 id="c580" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">有哪些数据可用？</h1><p id="25c4" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">Camelyon16数据集由400个淋巴结的苏木精-伊红(H&amp;E)全切片图像组成，并标记了转移区域。Kaggle challenge进一步将数据分解为96×96像素的图像，这些图像来自原始的整片图像。如果在中心32×32像素区域中有至少一个癌症组织像素，则每个图像被标记为阳性；否则，它被标记为负面。有220，000张训练图像和570，000张评估图像，其中大约60/40的图像分为正反两部分。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es kw"><img src="../Images/e09626b85cabf4e46df9b063a25a77af.png" data-original-src="https://miro.medium.com/v2/resize:fit:192/format:webp/1*zaIR6iTg6m9XtpyB4hyY5g.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">阳性数据点(包含癌组织)</figcaption></figure><h1 id="622a" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">问题设置—培训/开发集和指标</h1><p id="5f44" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">我决定分出10%的训练数据作为开发集，我可以用它来比较不同模型的功效。我还决定使用AUC(ROC曲线下的面积)作为我的主要发展指标，它代表了模型将随机选择的正面例子排在随机选择的负面例子之上的概率。在处理类别不平衡时，AUC是一个比准确性更好的指标-举个简单的例子，如果阴性/阳性分裂是99/1，那么分类器可以通过简单地预测阴性来实现高准确性。</p><h1 id="e0a6" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">数据扩充</h1><p id="773c" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">一种综合增加计算机视觉任务中训练数据量的常用技术是对数据进行变换。我决定通过应用-90到90度之间的随机旋转，以及以0.5的概率水平和/或垂直翻转图像，来创建每个训练数据图像的副本。请注意，亮度衰减、裁剪和剪切等其他变换也是可能的，但我选择不应用这些变换，因为它们可能会引入与结果相关的伪像，例如剪切可能会在细胞形状中引入形态异常。</p><pre class="kh ki kj kk fd kx ky kz la aw lb bi"><span id="aca9" class="lc je hi ky b fi ld le l lf lg">from imgaug import augmenters as iaa</span><span id="d44b" class="lc je hi ky b fi lh le l lf lg">seq = iaa.Sequential([<br/>   iaa.Affine(rotate=(-90, 90)), # Rotate image between -90 and 90 degrees<br/>   iaa.Fliplr(0.5), # horizontally flip 50% of all images<br/>   iaa.Flipud(0.5), # vertically flip 50% of all images<br/>])</span></pre><p id="96df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我只增加了训练数据，而没有增加开发数据——原因是我们希望开发和测试数据尽可能接近我们关心的实际数据，以便我们对它们的度量读数反映“真实”的、未增强的淋巴结图像的真实性能。</p><p id="1878" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还要注意，在将数据分成训练集和开发集之后进行扩充是很重要的；否则，您可能会以训练数据泄漏而告终，其中一个增强版本的训练映像会出现在您的开发集中。</p><p id="a400" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在数据扩充之后，我们最终得到397k训练数据图像和21k开发集图像。</p><h1 id="59a3" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">基线模型—逻辑回归</h1><p id="889b" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">作为基线模型，我创建了一个逻辑回归，将96x96x3图像中的每个像素作为输入。我用Keras/Tensorflow来做。</p><pre class="kh ki kj kk fd kx ky kz la aw lb bi"><span id="b966" class="lc je hi ky b fi ld le l lf lg">lr_model = Sequential()<br/>lr_model.add(Flatten(input_shape=(96, 96, 3)))<br/>lr_model.add(Dense(1))<br/>lr_model.add(Activation('sigmoid'))</span><span id="4069" class="lc je hi ky b fi lh le l lf lg">lr_model.compile(<br/>  optimizer='adam',<br/>  loss='binary_crossentropy',<br/>  metrics=['accuracy', 'AUC']<br/>)</span><span id="f9df" class="lc je hi ky b fi lh le l lf lg">lr_model.fit(<br/>    x=train_x,<br/>    y=train_y,<br/>    validation_data=(validation_x, validation_y),<br/>    epochs=1,<br/>    batch_size=32,<br/>    shuffle=True,<br/>)</span></pre><p id="f24d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基线模型达到了以下指标:</p><ul class=""><li id="cc1a" class="li lj hi ih b ii ij im in iq lk iu ll iy lm jc ln lo lp lq bi translated">培训损失:0.8519</li><li id="77c6" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">训练精度:0.5782</li><li id="8fee" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">训练AUC: 0.5975</li><li id="37b2" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">发展损失:1.4036</li><li id="0f2d" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">dev精度:0.4034</li><li id="f6b3" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">开发AUC: 0.7228</li></ul><p id="c706" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">显然，还有很大的改进空间！我们应该优先考虑哪些后续步骤？正如吴恩达在他关于深度学习的课程中提到的[2]，我们应该看看贝叶斯误差(最低可能实现的误差)，训练误差和dev误差。这里，我们将AUC作为主要指标，但我们可以计算出误差为1-AUC。我们也可以假设贝叶斯误差约为0，对应于Kaggle排行榜上取得的最好成绩(~1)。由于训练误差和开发误差都明显高于贝叶斯误差，我们可以得出结论，我们有一个偏差问题-最有可能的是，模型不够复杂，无法捕捉数据中的相互作用。这意味着我们应该训练一个更大的模型。</p><h1 id="c36e" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">迁移学习——NASNet模型</h1><p id="1d97" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">计算机视觉任务的一个常见出发点是使用另一个视觉任务的预训练模型，并针对手头的任务进行微调。这被称为迁移学习。</p><p id="9759" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们试试NASNetMobile模型，它可以通过Tensorflow/Keras库获得，并在ImageNet上展示了最先进的性能。由于NASNetMobile将224x224px像素的图像作为输入，那么这个模型如何适用于96×96像素的输入图像呢？答案是，我们只使用模型中的卷积层和池层，它们对输入维度没有要求；我们消除了顶部的全连接层，因为它期望从conv/池层获得一定大小的输出量。</p><p id="b559" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为最初的方法，我决定将输出音量变平，然后用一个神经元将其输入到一个密集的层中。我还决定解冻NASNet模型参数，因为训练数据集似乎足够大，可以训练这样大小的模型。</p><pre class="kh ki kj kk fd kx ky kz la aw lb bi"><span id="971c" class="lc je hi ky b fi ld le l lf lg">inputs = Input((96, 96, 3))<br/>base_model = NASNetMobile(include_top=False, input_tensor=inputs, weights=’imagenet’)<br/>x = base_model(inputs)<br/>x = Flatten()(x)<br/>x = Dense(1, activation=”sigmoid”)(x)<br/>model = Model(inputs, x)</span><span id="6ac9" class="lc je hi ky b fi lh le l lf lg">model.compile(<br/>  optimizer=Adam(1e-4),<br/>  loss=’binary_crossentropy’,<br/>  metrics=[‘accuracy’, ‘AUC’])</span><span id="c782" class="lc je hi ky b fi lh le l lf lg">model.fit(<br/>  x=train_x,<br/>  y=train_y,<br/>  validation_data=(validation_x, validation_y),<br/>  epochs=6,<br/>  batch_size=32,<br/>  shuffle=True,<br/>  callbacks=[model_checkpoint],<br/>)</span></pre><p id="c136" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这产生了以下指标:</p><ul class=""><li id="8a05" class="li lj hi ih b ii ij im in iq lk iu ll iy lm jc ln lo lp lq bi translated">列车损失:0.053</li><li id="8526" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">训练AUC: 0.9977</li><li id="3a8f" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">发展损失:0.097</li><li id="a3a3" class="li lj hi ih b ii lr im ls iq lt iu lu iy lv jc ln lo lp lq bi translated">开发AUC: 0.9920</li></ul><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lw"><img src="../Images/fb9b6b45a0afc6ae024117df1c9b60ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*oonr9ya7XXkEB2PAfykyPQ.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">整个训练时期的训练和开发损失</figcaption></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lx"><img src="../Images/999c633a8a8e60458ae68474d3c1b491.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*aYAMy877SRjY5smi5Rmuog.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">在培训时期培训和开发AUC</figcaption></figure><p id="db97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们已经消除了大部分偏差问题—贝叶斯误差和训练误差之间的差距是1–0.9977 = 0.0023。训练误差和开发误差之间的差距是0.9977–0.9920 = 0.057，这告诉我们，我们需要专注于解决方差问题。我们可以通过正则化模型来做到这一点，通过辍学或L2正则化。</p><h1 id="5295" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">能不能加快训练速度？</h1><p id="884b" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">训练NASNet模型需要很长时间，每个时期大约1300秒，即使在GPU上训练也是如此。由于我是在Google Cloud上训练的，所以我可以使用四个GPU，并想知道我是否可以使用它们来加快训练速度。</p><p id="a99e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我尝试在TensorFlow中使用MirroredStrategy来实现这一点:</p><pre class="kh ki kj kk fd kx ky kz la aw lb bi"><span id="85e7" class="lc je hi ky b fi ld le l lf lg">with tf.distribute.MirroredStrategy().scope():<br/>   # Create, compile, and fit model<br/>   # ...</span></pre><p id="ad66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MirroredStrategy将模型的所有变量复制到每个GPU，并将正向/反向传递计算分批分配给所有GPU。然后，它使用all-reduce合并来自每个GPU的渐变，然后将结果应用到每个GPU的模型副本。本质上，它是划分批处理并将每个块分配给一个GPU。</p><p id="23bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">令人惊讶的是，使用MirroredStrategy最初使训练变得更慢——2200秒比1300秒！我将此归因于32的批处理大小相对较小，因此分割批处理的开销可能超过了使用多个GPU节省的时间。</p><p id="7685" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我使用了更大的批量1024。这导致了6倍的训练加速，从每历元1300秒到218秒。然而，虽然这导致了与之前类似的培训损失(0.046)，但是dev损失却激增至2.34。</p><p id="88f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">事实证明，增加批量大小会显著增加泛化差距——训练和开发性能之间的差距。原因是较大的批量往往会导致“急剧最小化”，这不能很好地推广到新数据，而较小的批量会引入一些噪声，使它们能够避开这些“急剧最小化”，而有利于推广得更好的“平坦最小化”。[3]不幸的是，这意味着更大的批量，以及并行化带来的相应加速是不可能的，除非采用技术来改善大批量的泛化能力。*</p><p id="14f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">*其他人之前观察到，增加学习率可以消除大批量上的泛化差距；然而，我发现这样做并没有带来任何收获。</p><h1 id="de0b" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">前期培训有帮助吗？</h1><p id="02b5" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">我们在上面看到，从简单的逻辑回归模型转移到预先训练的NASNet模型显著提高了AUC。然而，这是因为NASNet是预训练的，还是仅仅因为NASNet的架构更复杂，因此更能够学习训练数据中的交互？换句话说，前期培训真的有帮助吗？</p><p id="e14c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了回答这个问题，我运行了一个相同的运行，使用随机初始化的权重，而不是预先训练的ImageNet权重。如下图所示，模型需要更长的时间来学习并达到良好的性能——即使在15个时期后，模型仍然没有仅在6个时期后预训练模型的性能好！</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ly"><img src="../Images/3c43c18d517c9fa29f5e210f625c5cf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*8vVTRdLEH6RBfA_MKTCV7w.png"/></div></figure><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es lw"><img src="../Images/095090a7bfb4e7a6f35da84f26d0033f.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*kUEcTrpO_N1v6zOCAnHJ5g.png"/></div></figure><p id="ca3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于我只训练了15个时期的模型，我不确定它最终是否会赶上预训练的版本——但我们可以合理地得出结论，随机初始化的模型需要更长的时间来匹配预训练模型的性能，如果它曾经达到过的话。</p><h1 id="207d" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">参考</h1><p id="ae82" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">[0]<a class="ae lz" href="https://www.kaggle.com/c/histopathologic-cancer-detection/overview" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/organisationic-cancer-detection/overview</a></p><p id="f326" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[1]https://camelyon16.grand-challenge.org/Background/<a class="ae lz" href="https://camelyon16.grand-challenge.org/Background/" rel="noopener ugc nofollow" target="_blank"/></p><p id="4742" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]<a class="ae lz" href="https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/neural-networks-deep-learning</a>/</p><p id="18fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3]<a class="ae lz" href="https://openreview.net/pdf?id=H1oyRlYgg" rel="noopener ugc nofollow" target="_blank">https://openreview.net/pdf?id=H1oyRlYgg</a></p></div></div>    
</body>
</html>