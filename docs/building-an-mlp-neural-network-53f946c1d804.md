# 构建 MLP 神经网络

> 原文：<https://medium.com/analytics-vidhya/building-an-mlp-neural-network-53f946c1d804?source=collection_archive---------24----------------------->

![](img/18fcbc39db6ff7318c97967d3291e30b.png)

照片由 [Maximalfocus](https://unsplash.com/@maximalfocus?utm_source=medium&utm_medium=referral) 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄

# 建立 MLP 神经网络的步骤包括:

# 1.预处理数据:

*   根据目标的需要，执行数据清理，如重复数据删除，删除不必要的元素，如 URL 等。(如果处理文本数据)
*   将文本数据转换成数字向量。(如果处理文本数据)
*   不要忘记标准化数据。

# 2.选择架构:

根据目标的需要:

*   选择要构建的 MLP 的适当层数。
*   在每层中选择适当数量的神经元。
*   可以通过对不同数量的层/神经元执行超参数调谐来选择适当数量的层/神经元。

# 3.重量初始化:

选择适当的(使用超参数调整)随机权重初始化方案，例如:

*   从所有权重= 0 开始(很少使用)
*   统一初始化(适用于 Sigmoid 激活功能)
*   Xavier/Glorot 初始化:

> -制服
> 
> -正常

*   He 初始化

> -制服
> 
> -正常

*   更多权重初始化方案，查看 Keras 文档[此处](https://keras.io/api/layers/initializers/)。

# 4.选择激活功能:

选择适当的(使用超参数调谐)激活功能，例如:

*   Sigmoid(由于**“消失梯度”**问题，应避免使用)
*   Tanh(由于**“消失梯度**”问题，应避免使用)
*   ReLu(回归任务的首选)
*   Softmax(分类任务的首选)
*   更多激活功能，查看 Keras 文档[此处](https://keras.io/api/layers/activations/)。

# 5.选择优化器:

*   由于曲线中的“鞍点”问题，我们通常避免在深度学习(不像机器学习)中使用 SGD 进行优化，SGD 无法正确处理这些问题。

> 鞍点是曲线上斜率为 0 的点，通常不是曲线上的局部极值。

*   因此，我们选择其他优化技术，例如:

> 亚当
> 
> 阿达德尔塔
> 
> 阿达格勒
> 
> -阿达马克斯

*   更多优化，查看 Keras 文档[这里](https://keras.io/api/optimizers/)。

# 6.使用批处理规范化(可选):

*   通常，批量标准化用于非常深的分层 MLP 的情况。
*   MLP 初始层中权重值的微小变化可能会导致深层的巨大变化。
*   因此，在小变化导致大变化之前，我们批量标准化 MLP 中的深层。

# 7.使用辍学(可选):

*   辍学是指在一个层中放弃一定数量的神经元，这导致了 MLP 的正则化。
*   一定数量的神经元随机关闭，因此它们的输出不用于模型建立。
*   正则化用于“偏差-方差权衡”，指的是“过拟合”或“欠拟合”模型。

# 8.选择损失函数:

用于计算模型在训练期间应该寻求最小化的数量的一些损失函数是:

*   交叉熵(分类任务首选)
*   均方误差(回归任务的首选)
*   绝对平均误差
*   更多损失函数，查看 Keras 文档[此处](https://keras.io/api/losses/)。

# 9.梯度监控和削波(如果需要):

*   一些激活函数导致“消失梯度”问题和“爆炸梯度”问题。
*   消失梯度问题指的是梯度变得如此之小，这是由于梯度的许多小值乘积，以至于它们对于优化目的变得无效，因为权重从不收敛。

> 使用 ReLu 激活函数解决渐变消失问题。

*   爆炸梯度问题指的是梯度变得非常大，这是由于大量的大值梯度乘积，由于权重从不收敛，它们对于优化目的变得无效。

> 使用 L2 范数裁剪解决爆炸梯度问题。

*   这两个问题经常发生在深层神经网络中的 Sigmoid 和 Tanh 激活函数。

# 10.模型评估:

*   找出所建模型的精度和损耗。
*   绘制“测试损失与历元数”图，这可以提供关于模型每个历元的错误率的见解。

[代码实现，跟着我上 GitHub](https://github.com/deveshSingh06/) 。