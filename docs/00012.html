<html>
<head>
<title>How to finish in the top 10 percentile in Bike Sharing Demand Competition In Kaggle? (part -2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何在Kaggle的自行车共享需求竞赛中名列前10%？(第二部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-finish-top-10-percentile-in-bike-sharing-demand-competition-in-kaggle-part-2-29e854aaab7d?source=collection_archive---------1-----------------------#2017-05-03">https://medium.com/analytics-vidhya/how-to-finish-top-10-percentile-in-bike-sharing-demand-competition-in-kaggle-part-2-29e854aaab7d?source=collection_archive---------1-----------------------#2017-05-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2d735977911229bc31b9c02d36295c3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8xkwV6r-i6tHPZz0jZt0Vw.jpeg"/></div></div></figure><p id="1022" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这篇博文是我们之前的<a class="ae jo" rel="noopener" href="/@viveksrinivasan/how-to-finish-top-10-percentile-in-bike-sharing-demand-competition-in-kaggle-part-1-c816ea9c51e1"> <em class="jp">部分</em> </a>的续篇，在这里我们将数据可视化到了一个更高的程度，并掌握了它。在这篇文章中，我们将看到我们如何利用机器学习算法，如<code class="du jq jr js jt b">Linear Regression</code>、<code class="du jq jr js jt b">Random Forest</code>和<code class="du jq jr js jt b">Gradient Boost </code>，进入Kaggle排行榜的前10%。模型构建部分分为以下主题</p><ul class=""><li id="3655" class="ju jv hi is b it iu ix iy jb jw jf jx jj jy jn jz ka kb kc bi translated"><code class="du jq jr js jt b">windspeed</code>中的缺失值分析。</li><li id="1829" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">RMSLE计分员。</li><li id="0bd1" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">线性回归。</li><li id="01b5" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">正规化。</li><li id="9bbf" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">集合模型。</li><li id="c0b7" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">进一步的改进。</li></ul><blockquote class="ki kj kk"><p id="2b75" class="iq ir jp is b it iu iv iw ix iy iz ja kl jc jd je km jg jh ji kn jk jl jm jn hb bi translated">只有关键的代码片段被显示为这个博客的一部分，完整的分析请参考Ipython笔记本中的<a class="ae jo" href="https://github.com/viveksrinivasanss/blogs/tree/master/bike_sharing_demand" rel="noopener ugc nofollow" target="_blank"> <em class="hi"> Github </em> </a>链接。</p></blockquote><h2 id="81aa" class="ko kp hi bd kq kr ks kt ku kv kw kx ky jb kz la lb jf lc ld le jj lf lg lh li bi translated">风速中的缺失值分析</h2><p id="0739" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">正如我在上一部分提到的，幸运的是我们的数据集中没有缺失值。但是<code class="du jq jr js jt b">windspeed </code>属性有很多<code class="du jq jr js jt b">0</code>条目有点可疑。下面是描述数据中的<code class="du jq jr js jt b">windspeed </code>值的<code class="du jq jr js jt b">frequency</code>的简单可视化。</p><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lo"><img src="../Images/6d48792e45170a4ce82b9ce5bc94901b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9CYqrEtMYZL81BKeI4JR-g.png"/></div></div></figure><p id="2333" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一个<strong class="is hj"> </strong> <a class="ae jo" href="https://www.kaggle.com/c/bike-sharing-demand/discussion/10431" rel="noopener ugc nofollow" target="_blank"> <em class="jp">的讨论</em> </a> <strong class="is hj"> <em class="jp"> </em> </strong>在<strong class="is hj"> <em class="jp"> </em> </strong>中kaggle给出了很多关于这个特定话题的信息。在高层次上，我们可以对<code class="du jq jr js jt b">windspeed </code>中的<code class="du jq jr js jt b">0</code>条目做出两到三个简单的推断，如下所示:</p><ul class=""><li id="6c0b" class="ju jv hi is b it iu ix iy jb jw jf jx jj jy jn jz ka kb kc bi translated">这几个点其实都可以<code class="du jq jr js jt b">0</code>。</li><li id="8125" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">它太低而无法测量，例如从0到5变化。</li><li id="f392" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">全零或部分全零只不过是<code class="du jq jr js jt b">NAs</code>。</li></ul><p id="db55" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于本文，我们将把<code class="du jq jr js jt b">0</code>条目视为缺失值，并使用一个简单的<code class="du jq jr js jt b">Random Forest Classifier</code> <em class="jp"> </em>模型来填充它们。下面是相同的代码块。</p><pre class="lp lq lr ls fd lt jt lu lv aw lw bi"><span id="c02d" class="ko kp hi jt b fi lx ly l lz ma">from sklearn.ensemble import RandomForestClassifier</span><span id="45d5" class="ko kp hi jt b fi mb ly l lz ma">wCol= ["season","weather","humidity","month","temp","year","atemp"]<br/>dataWind0 = data[data["windspeed"]==0]<br/>dataWindNot0 = data[data["windspeed"]!=0]<br/>dataWindNot0["windspeed"] = dataWindNot0["windspeed"].astype("str")</span><span id="caa0" class="ko kp hi jt b fi mb ly l lz ma">rfModel_wind = RandomForestClassifier()<br/>rfModel_wind.fit(dataWindNot0[wCol], dataWindNot0["windspeed"])<br/>wind0Values = rfModel_wind.predict(X= dataWind0[wCol])</span><span id="aacd" class="ko kp hi jt b fi mb ly l lz ma">dataWind0["windspeed"] = wind0Values<br/>data = dataWindNot0.append(dataWind0)<br/>data["windspeed"] = data["windspeed"].astype("float")<br/>data.reset_index(inplace=True)<br/>data.drop('index',inplace=True,axis=1)</span></pre><p id="d71c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们看看缺失值被估算后<code class="du jq jr js jt b">windspeed </code>值的分布情况。</p><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lo"><img src="../Images/e4a2f71e86cb3a71c93c1be10570e731.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r08djQM3m4mo7QfOx9il1w.png"/></div></div></figure><h2 id="4950" class="ko kp hi bd kq kr ks kt ku kv kw kx ky jb kz la lb jf lc ld le jj lf lg lh li bi translated">RMSLE计分器</h2><p id="6820" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">评估<code class="du jq jr js jt b">regression</code>模型的一种常见方法是通过计算<code class="du jq jr js jt b">MSE or RMSE</code>。在这场特殊的比赛中，评估我们模型的标准是<code class="du jq jr js jt b">Root Mean Square Logarithmic Error (RMSLE)</code>。<code class="du jq jr js jt b">RMSLE </code>在您想要惩罚一个低于预测的估计值大于一个高于预测的估计值时特别有用。</p><p id="df9e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们预测销售和库存需求的大多数<code class="du jq jr js jt b">Kaggle </code>竞赛特别使用<em class="jp"> RMSLE </em>作为他们的评估指标。例如像<a class="ae jo" href="https://www.kaggle.com/c/grupo-bimbo-inventory-demand" rel="noopener ugc nofollow" target="_blank"><em class="jp">Grupo-bimbo-inventory-demand</em></a>和<a class="ae jo" href="https://www.kaggle.com/c/sberbank-russian-housing-market" rel="noopener ugc nofollow" target="_blank"><em class="jp">sberbank-Russian-housing-market</em><strong class="is hj"><em class="jp"/></strong></a>这样的竞争使用<code class="du jq jr js jt b">RMSLE </code>作为度量。</p><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/ca0663b41b88d5d71553efd4a6d1cb62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9P4NEvK5qYN5Bhu0yOEzhw.png"/></div></div></figure><p id="3719" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">遗憾的是，<code class="du jq jr js jt b">sklearn </code>指标没有直接实现计算<code class="du jq jr js jt b">RMSLE</code>。因此，让我们构建一个自定义函数来执行<code class="du jq jr js jt b">RMSLE</code>计算。</p><pre class="lp lq lr ls fd lt jt lu lv aw lw bi"><span id="51cd" class="ko kp hi jt b fi lx ly l lz ma">def rmsle(y, y_):<br/>    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))<br/>    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))<br/>    calc = (log1 - log2) ** 2<br/>    return np.sqrt(np.mean(calc))</span></pre><p id="e22c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们已经通过填充缺失值准备了数据，并构建了我们的自定义<code class="du jq jr js jt b">RMSLE </code>计分器。所以我们现在可以开始我们的模型制作实验了。</p><h2 id="cdcb" class="ko kp hi bd kq kr ks kt ku kv kw kx ky jb kz la lb jf lc ld le jj lf lg lh li bi translated">线性回归</h2><p id="a5b7" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">作为第一步，让我们从一个简单的统计技术开始，如<code class="du jq jr js jt b">linear regression</code>。从一个简单的模型开始总是比一开始就尝试复杂的算法要好。因为有时特性对<code class="du jq jr js jt b">covariates</code>具有平滑的、近乎线性的依赖性。那么<code class="du jq jr js jt b">linear regression </code>将比<code class="du jq jr js jt b">random forest</code>算法更好地模拟相关性，该算法将基本上用难看的不规则阶跃函数来近似线性曲线。一个<a class="ae jo" href="https://stats.stackexchange.com/questions/174806/linear-regression-performing-better-than-random-forest-in-caret" rel="noopener ugc nofollow" target="_blank"><em class="jp">stack exchange</em></a>讨论给出了关于它的大量信息。</p><p id="17da" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面是适合我们的<code class="du jq jr js jt b">bike-sharing</code>数据集上的<code class="du jq jr js jt b">linear regression</code>的简单代码片段。代码是不言自明的，本文的重点是介绍概念而不是编码。如果你在理解它的任何部分时发现任何挑战，请随时在评论中留言。</p><pre class="lp lq lr ls fd lt jt lu lv aw lw bi"><span id="00b6" class="ko kp hi jt b fi lx ly l lz ma">from sklearn.linear_model import LinearRegression</span><span id="18be" class="ko kp hi jt b fi mb ly l lz ma"># Initialize logistic regression model<br/>lModel = LinearRegression()</span><span id="67ef" class="ko kp hi jt b fi mb ly l lz ma"># Train the model<br/>lModel.fit(X = X_train,y = np.log1p(y_train))</span><span id="4129" class="ko kp hi jt b fi mb ly l lz ma"># Make predictions<br/>preds = lModel.predict(X= X_validate)<br/>print ("RMSLE Value: ",rmsle(Y_validate,np.exp(preds)))</span></pre><p id="50df" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在提交我们的测试结果之前，我们将可视化训练和测试结果的分布。<code class="du jq jr js jt b">Kaggle </code>对每天的提交数量有限制。(在我们的例子中，是每天5次提交)。因此，可视化分布提供了一个很好的线索，表明我们根据训练集预测的测试有多接近。从图中可以看出，训练集和测试集的分布变化很大。</p><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/b90153e867f3b6bbd1510948c10ba9f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hA-P_YzdWTkrG3A_ktxUzA.png"/></div></div></figure><p id="b4fe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">测试集上的<code class="du jq jr js jt b">RMSLE </code>值在1.05左右，与<code class="du jq jr js jt b"><em class="jp">Kaggle </em></code>排行榜上的最好成绩(0.33)肯定不相上下。我们可以通过多种方式大幅提高这一分数。</p><ul class=""><li id="4a99" class="ju jv hi is b it iu ix iy jb jw jf jx jj jy jn jz ka kb kc bi translated">特征工程</li><li id="8645" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">正规化(L1和L2)</li><li id="0453" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">集合模型</li></ul><p id="3b82" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们已经从<code class="du jq jr js jt b">datetime </code>属性中创建了一些特性，如<code class="du jq jr js jt b">weekday</code>、<code class="du jq jr js jt b">month</code>、<code class="du jq jr js jt b">hour </code>。有很多方法可以实现<code class="du jq jr js jt b">feature engineering</code>步骤。作为这个博客的一部分，我没有考虑到这一点，我将把它留给用户去想象。</p><h2 id="3135" class="ko kp hi bd kq kr ks kt ku kv kw kx ky jb kz la lb jf lc ld le jj lf lg lh li bi translated">正规化</h2><p id="fd18" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated"><code class="du jq jr js jt b">Regularization </code>在以下任何一种情况下都极其有用。我们不会面临下面提到的所有情况，但是<code class="du jq jr js jt b">overfitting </code>和<code class="du jq jr js jt b">multicollinearity </code>可能会给我们带来一些问题。</p><ul class=""><li id="eaf8" class="ju jv hi is b it iu ix iy jb jw jf jx jj jy jn jz ka kb kc bi translated">过度拟合。</li><li id="6f0b" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">大量的变量。</li><li id="f408" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">观察数与变量数之比低。</li><li id="0fb3" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">多重共线性。</li></ul><p id="6c36" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du jq jr js jt b">Overfitting </code>指通过学习训练数据中的细节和噪声，在训练集上表现良好，但在新数据上概括不佳的模型。以我们的例子为例，训练数据上的<code class="du jq jr js jt b">RMSLE </code>值在<code class="du jq jr js jt b">0.98</code>附近，与测试集结果没有大的差异。到目前为止，我们的模型中没有任何<em class="jp">过度拟合</em>的问题，但是当我们拟合模型时，有时这将是一场噩梦。</p><p id="c709" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">拥有大量的变量可能会再次导致过度拟合。这是因为当我们有更多的变量时，模型变得更加复杂，有时会降低其预测和推广能力。<em class="jp"> </em> <code class="du jq jr js jt b">L1 regularization </code> <em class="jp">【套索回归】</em>在这些情况下，通过将系数减少到零，从而产生更简单的模型，就很方便了。</p><p id="185b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du jq jr js jt b">L2 regularization</code> <em class="jp"> </em> <code class="du jq jr js jt b"><em class="jp">(</em>Ridge Regression<em class="jp">)</em></code>对于第三种情况非常有帮助，在这种情况下，我们拥有的属性数量多于观察数量。但是我们现在做得很好，因为我们只有12个属性，而数据集有10886条记录。当预测变量之间存在高<code class="du jq jr js jt b">collinearity </code>时，<code class="du jq jr js jt b">Ridge Regression</code>也非常有用。这可能发生在我们的数据集中，因为我们有高度相关的变量，如气温和月-季。</p><p id="b056" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，目前看来，上述问题对我们没有太大威胁。尽管如此，没有什么能阻止我们建立简单的正则化模型，看看我们能在多大程度上提高我们的分数。</p><pre class="lp lq lr ls fd lt jt lu lv aw lw bi"><span id="2eff" class="ko kp hi jt b fi lx ly l lz ma">from sklearn.linear_model import Lasso<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn import metrics<br/>lasso_m_ = Lasso()<br/>alpha  = [0.001,0.005,0.01,0.3,0.1,0.3,0.5,0.7,1]<br/>lasso_params_ = { 'max_iter':[500],'alpha':alpha}<br/>rmsle_scorer = metrics.make_scorer(rmsle, greater_is_better=False)<br/>grid_lasso_m = GridSearchCV( lasso_m_,<br/>                          lasso_params_,<br/>                          scoring = rmsle_scorer,<br/>                          cv=5)<br/>grid_lasso_m.fit(X = X_train,y = np.log1p(y_train))<br/>preds = grid_lasso_m.predict(X= X_validate)<br/>print (grid_lasso_m.best_params_)<br/>print ("RMSLE Value: ",rmsle(Y_validate,np.exp(preds)))</span></pre><p id="6f7a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过网格搜索获得<code class="du jq jr js jt b">regularization </code>参数<code class="du jq jr js jt b">(alpha-0.005)</code>的最佳值。下图显示了不同alpha参数的<code class="du jq jr js jt b">RMSLE </code>值。<code class="du jq jr js jt b">RMSLE </code>测试集上的值约为1.04，与我们之前的相比没有提高。所以正规化并没有给我们的分数带来任何提升。但是，让我们不要失去希望，因为当一切都不顺利时，集合模型总是会为我们带来一些意想不到的东西。</p><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/beb981de0898d790585fc9fa599f2bbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vjth76gdc8G_kc_ehH0UqQ.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">套索正则化参数网格搜索</figcaption></figure><h2 id="bc5b" class="ko kp hi bd kq kr ks kt ku kv kw kx ky jb kz la lb jf lc ld le jj lf lg lh li bi translated">集合模型</h2><p id="37f3" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated"><code class="du jq jr js jt b">Ensemble models</code>只不过是一种将不同的个体<code class="du jq jr js jt b">weak learners</code>(模型)组合在一起以提高模型的稳定性和预测能力的艺术。<code class="du jq jr js jt b">Ensemble Models</code> <em class="jp"> </em>提高了模型的性能</p><ul class=""><li id="a54f" class="ju jv hi is b it iu ix iy jb jw jf jx jj jy jn jz ka kb kc bi translated">平均偏差。</li><li id="e334" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">减少差异。</li><li id="eb8e" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">避免<em class="jp">过拟合</em>。</li></ul><p id="3b05" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你仍然想知道什么是集合模型，那么这个<a class="ae jo" href="https://www.analyticsvidhya.com/blog/tag/ensemble-model/" rel="noopener ugc nofollow" target="_blank"> <em class="jp">系列文章</em> </a>可以让你开始了解它。关于集合模型的介绍已经足够了，下面是我们如何用默认参数在数据集上拟合朴素<code class="du jq jr js jt b">Random Forest </code>模型的一个片段。</p><pre class="lp lq lr ls fd lt jt lu lv aw lw bi"><span id="70c8" class="ko kp hi jt b fi lx ly l lz ma">from sklearn.ensemble import RandomForestRegressor<br/>rfModel = RandomForestRegressor(n_estimators=100)<br/>rfModel.fit(X = X_train,y = np.log1p(y_train))<br/>preds = rfModel.predict(X= X_validate)<br/>print ("RMSLE Value: ",rmsle(Y_validate,np.exp(preds)))</span></pre><p id="289a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jp"> RMSLE </em>测试集上的值大约是0.439，现在我们有了真正值得欢呼的东西。与我们之前的分数相比，有了显著的提高。<code class="du jq jr js jt b">Random forest</code>还赋予了特征作为模型拟合副产品的重要性。这对于理解特征如何在模型构建中起作用，以及在<code class="du jq jr js jt b">feature selection</code>我们有很多特征要处理，特别有用。从下图可以很明显的看出，<code class="du jq jr js jt b">hour </code>属性在模型预测中起着重要的作用，其次是<code class="du jq jr js jt b">temp</code>、<code class="du jq jr js jt b">weekday</code>、<code class="du jq jr js jt b">year</code>等。</p><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mj"><img src="../Images/3cb3bba9f5eff5481fe86d9c34ce45d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3_cXzCQLTNfpdeydXIGDg.png"/></div></div></figure><p id="0d0f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一般认为基于<code class="du jq jr js jt b">Gradient Boosting</code><em class="jp"/><code class="du jq jr js jt b">XGBoost </code>的<em class="jp"> boosting </em>算法比基于<code class="du jq jr js jt b">bagging</code>的算法性能更好。所以是时候测试助推算法如何帮助我们提高考试分数了。</p><pre class="lp lq lr ls fd lt jt lu lv aw lw bi"><span id="f1f1" class="ko kp hi jt b fi lx ly l lz ma">from sklearn.ensemble import GradientBoostingRegressor<br/>gbm = GradientBoostingRegressor(n_estimators=4000,alpha=0.01)<br/>gbm.fit(X = X_train,y = np.log1p(y_train))<br/>preds = gbm.predict(X= X_validate)<br/>print ("RMSLE Value: ",rmsle(Y_validate,np.exp(preds)))</span></pre><p id="2abc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为我们相信它确实提高了我们的分数，并且<code class="du jq jr js jt b">Gradient Boosting</code>的<code class="du jq jr js jt b">RMSLE </code>值在<code class="du jq jr js jt b">0.418</code>附近。虽然这看起来比我们的<code class="du jq jr js jt b">Random Forest</code>结果略有改善，但它将把你在<code class="du jq jr js jt b">Kaggle </code>排行榜中的位置从前15%提高到前10 %,这是一个显著的进步。</p><p id="26bb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过比较训练和测试结果，我们将再次可视化我们预测的测试结果有多接近。当我们将下面的图表与我们为线性回归创建的图表进行比较时。我们可以理解<code class="du jq jr js jt b">gradient boost </code>是如何捕捉到测试集的分布的。</p><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/76941e12f0475c3b26c3b142414fbcde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dB6qf2DsCeapnowo2caWIw.png"/></div></div></figure><p id="ea8e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们将通过向kaggle提交最佳拟合模型结果来完成循环。<code class="du jq jr js jt b">Kaggle </code>要求输出文件以特定的格式提交，其中第一个字段对应<code class="du jq jr js jt b">datetime </code>，第二个字段应该是我们预测的<code class="du jq jr js jt b">count</code>。</p><pre class="lp lq lr ls fd lt jt lu lv aw lw bi"><span id="0d26" class="ko kp hi jt b fi lx ly l lz ma">submission = pd.DataFrame({<br/>        "datetime": datetimecol,<br/>        "count": [max(0, x) for x in np.exp(predsTest)]<br/>    })<br/>submission.to_csv('bike_predictions_gbm_results.csv', index=False)</span></pre><h2 id="c7f8" class="ko kp hi bd kq kr ks kt ku kv kw kx ky jb kz la lb jf lc ld le jj lf lg lh li bi translated">进一步的改进</h2><p id="c79d" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">在<code class="du jq jr js jt b">kaggle </code>排行榜中，最好的分数在<code class="du jq jr js jt b">0.33</code>左右，这意味着模型构建活动还有很大的改进空间。下面是一些我们可以提高分数的方法。</p><ul class=""><li id="576b" class="ju jv hi is b it iu ix iy jb jw jf jx jj jy jn jz ka kb kc bi translated">通过网格搜索调整参数。</li><li id="6669" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">分别预测临时用户数和注册用户数。</li><li id="2ea4" class="ju jv hi is b it kd ix ke jb kf jf kg jj kh jn jz ka kb kc bi translated">时间序列建模。</li></ul><p id="c5a4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">改善集合模型结果的一种常见方法是通过调整参数。<code class="du jq jr js jt b">Sklearn </code>提供了一个叫做<code class="du jq jr js jt b">GridSearchCV </code>的方法，帮助我们进行彻底的网格搜索。根据系统的配置，以下代码可能会运行几个小时。</p><pre class="lp lq lr ls fd lt jt lu lv aw lw bi"><span id="f6e1" class="ko kp hi jt b fi lx ly l lz ma">from sklearn.ensemble import GradientBoostingRegressor<br/>from sklearn.model_selection import GridSearchCV</span><span id="cbc6" class="ko kp hi jt b fi mb ly l lz ma">gbm = GradientBoostingRegressor()<br/>param_grid = { <br/>    'n_estimators': [1000,2000,3000,4000],<br/>    'max_features': ["auto","sqrt","log2",0.6,0.8],<br/>    'min_samples_leaf':[30,40,50,60,70],<br/>    'min_samples_split':[150,200,250,300],<br/>    'max_depth' : [10,15,20,25],<br/>    'subsample': [0.4,0.6,0.8],<br/>    'learning_rate':[0.1,0.01,0.001]<br/>}<br/>CV_gbm=GridSearchCV(estimator=gbm,param_grid=param_grid,cv=5)<br/>CV_gbm.fit(X = X_train,y = np.log1p(y_train))<br/>print (CV_gbm.best_params_)</span><span id="7c1b" class="ko kp hi jt b fi mb ly l lz ma">preds = CV_gbm.predict(X= X_validate)<br/>print ("RMSLE Value: ",rmsle(Y_validate,np.exp(preds)))</span></pre><p id="d34d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Sklearn提供了一个叫做<code class="du jq jr js jt b">RandomizedSearchCV</code>的<code class="du jq jr js jt b">GridSearchCV </code>的替代品。它需要一个名为<code class="du jq jr js jt b">n_iter </code>的额外参数，这个参数可以灵活地选择要试验多少种参数组合。<code class="du jq jr js jt b">n_iter </code>的数量越大，解决方案的质量越好(质量和运行时间之间的权衡)。</p><p id="2121" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我们在博客的第一部分进行<code class="du jq jr js jt b">count </code>和<code class="du jq jr js jt b">hour </code>之间的<code class="du jq jr js jt b">bivariate </code>分析时。很明显，<code class="du jq jr js jt b">casual </code>和<code class="du jq jr js jt b">registered </code>用户的使用模式在<code class="du jq jr js jt b">hour</code>中有很大不同。从我们的特征重要性结果(<code class="du jq jr js jt b">Random Forest</code>)可知<code class="du jq jr js jt b">hour </code>是建模中最重要的特征。这两个结果说明，我们可以建立两个单独的模型，将<code class="du jq jr js jt b">casual </code>和<code class="du jq jr js jt b">registered </code>用户视为因变量，而不是将<code class="du jq jr js jt b">count </code>建模为因变量。一篇<a class="ae jo" href="https://www.analyticsvidhya.com/blog/2015/06/solution-kaggle-competition-bike-sharing-demand/" rel="noopener ugc nofollow" target="_blank"> <em class="jp">文章</em> </a> <strong class="is hj"> <em class="jp"> </em> </strong>出自《分析篇》维迪亚解释了我们如何在这些方面处理问题。</p><p id="8e45" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">处理这个问题的另一个完全正交的方法是对数据进行<code class="du jq jr js jt b">time series</code>分析。从下面的图表可以看出，一天中的每个小时都有一个循环趋势，一年中的每个月都有季节性趋势。</p><figure class="lp lq lr ls fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/957750c5a5cf6d177040bebe686c7555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q15UsXf-FV6V2kOjuBsi6w.png"/></div></div></figure><p id="4b91" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过<code class="du jq jr js jt b">time series</code>模型解决问题可能会也可能不会提高我们的分数。但是在这些行上做一些努力肯定是有用的，因为对模型有贡献的大多数顶级属性都是从<code class="du jq jr js jt b">datetime </code>属性派生的。但是运行时间序列模型的唯一缺点是我们没有足够的数据，因为训练集只包含来自<code class="du jq jr js jt b">2011–2012</code>的两年数据。</p><h2 id="1465" class="ko kp hi bd kq kr ks kt ku kv kw kx ky jb kz la lb jf lc ld le jj lf lg lh li bi translated">结束注释</h2><p id="cc14" class="pw-post-body-paragraph iq ir hi is b it lj iv iw ix lk iz ja jb ll jd je jf lm jh ji jj ln jl jm jn hb bi translated">因此，这篇文章只是一个开始<em class="jp"> kaggle </em>竞赛的简单介绍，同时也给我们如何处理一个新数据集提供了一个开端。要了解更多关于<code class="du jq jr js jt b">kaggle </code>竞赛的信息，了解解决问题的不同方法，请查看<code class="du jq jr js jt b">kaggle</code> <em class="jp">中的<a class="ae jo" href="https://www.kaggle.com/kernels" rel="noopener ugc nofollow" target="_blank"> <em class="jp">内核</em> </a>部分。</em></p><blockquote class="ki kj kk"><p id="e927" class="iq ir jp is b it iu iv iw ix iy iz ja kl jc jd je km jg jh ji kn jk jl jm jn hb bi translated">如果你有兴趣通过R学习R编程和机器学习，敬请查阅R   <em class="hi">中的<a class="ae jo" rel="noopener" href="/analytics-vidhya/many-ways-of-reading-data-into-r-1-fb88d4eb80a9"> <em class="hi">数据科学系列。</em></a></em></p></blockquote></div></div>    
</body>
</html>