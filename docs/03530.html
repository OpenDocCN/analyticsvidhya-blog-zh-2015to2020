<html>
<head>
<title>Implementing Logistic Regression From Scratch Using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python从头开始实现逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementing-logistic-regression-from-scratch-using-python-d684f116334f?source=collection_archive---------9-----------------------#2020-02-06">https://medium.com/analytics-vidhya/implementing-logistic-regression-from-scratch-using-python-d684f116334f?source=collection_archive---------9-----------------------#2020-02-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div class="er es hg"><img src="../Images/375f07df6625aa0c960cc5b66a7240f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*PyMg652khNRm2BEae6LcMw.png"/></div></figure><div class=""/><p id="9edb" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这篇博客中，我们将重点关注从零开始实现逻辑回归。我们不会深究其背后的数学，而是专注于自己编写代码和实现模型的过程。</p><p id="1996" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">顾名思义，逻辑回归是一种分类算法，即它用于分类任务。出于这个博客的目的，我们将只考虑二元分类。</p><p id="5f0f" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了将每个预测分配到一个类别，我们需要将预测转换为概率(即在0，1之间)。为了实现这一点，我们将使用sigmoid函数，它将每个实数值映射到0和1之间的另一个值。</p><figure class="jl jm jn jo fd hk er es paragraph-image"><div class="er es jk"><img src="../Images/25dbbf534fe6ca308a009e93b22984ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*h3rNDECVkNx7oNTRHGhYyQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">Sigmoid函数</figcaption></figure><h1 id="06a9" class="jt ju hp bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">价值函数</h1><p id="0211" class="pw-post-body-paragraph im in hp io b ip kr ir is it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj hb bi translated">我们不能使用均方误差，因为它将导致非凸函数(由于sigmoid函数引起的非线性),从而导致具有许多局部最小值的函数，这将使得难以找到最优的全局最小值</p><p id="26e5" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">相反，我们使用一个叫做交叉熵的成本函数，也就是对数损失</p><figure class="jl jm jn jo fd hk er es paragraph-image"><div class="er es kw"><img src="../Images/b16121064c7698bddc6ad85f35cf9d69.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/0*SGUFtw8vkv3nayYm"/></div></figure><p id="8b6a" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以把它合并成一个单一的矢量方程</p><figure class="jl jm jn jo fd hk er es paragraph-image"><div class="er es kx"><img src="../Images/29a93c9ae29feb3981c1501dc7a21a62.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/0*6-g5W_z5Ao807FnA"/></div></figure><p id="d327" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">其中，如果y=1，则仅保留第一部分，而对于y=0，则保留第二部分</p><p id="7f27" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">找到上述函数的梯度将导致以下成本函数导数</p><figure class="jl jm jn jo fd hk er es paragraph-image"><div class="er es ky"><img src="../Images/a884c7b83c78228a81fe04f5cc54a8ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/0*vg4kcsOoIMtEyh0G"/></div></figure><p id="565b" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">其中s(z)是模型预测，x是特征向量(模型的输入), y是实际类别标签</p><p id="f3d7" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一个观察点是成本函数的梯度与使用均方误差的线性回归的梯度相同(即线性和逻辑回归对于它们的权重具有相同的更新规则。)</p><h1 id="d6f5" class="jt ju hp bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">Python实现</h1><p id="a8a9" class="pw-post-body-paragraph im in hp io b ip kr ir is it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj hb bi translated">现在，我们已经有了足够的知识来开始构建模型架构。</p><p id="e452" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于训练和测试，我们将使用来自sklearn的乳腺癌数据集，其中我们必须发现给定的特征指示肿瘤是恶性的还是良性的</p><pre class="jl jm jn jo fd kz la lb lc aw ld bi"><span id="2611" class="le ju hp la b fi lf lg l lh li">#importing all the libraries required<br/>import sklearn.datasets<br/>import numpy as np<br/>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import MinMaxScaler</span><span id="974d" class="le ju hp la b fi lj lg l lh li">cancer=sklearn.datasets.load_breast_cancer()</span><span id="6c2c" class="le ju hp la b fi lj lg l lh li">#convering the data to a pandas dataframe object<br/>data = pd.DataFrame(cancer.data, columns = cancer.feature_names)<br/>data["label"] = cancer.target<br/>data.head()</span></pre><figure class="jl jm jn jo fd hk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lk"><img src="../Images/827cfacbbde8bede539931ed91508564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mn5ZpN-Gh_GGDDn9dSFcug.png"/></div></div></figure><p id="a4fc" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">接下来，我们将分离要素和标注，使用最小最大归一化对要素进行归一化，然后将数据集分为训练集和测试集</p><pre class="jl jm jn jo fd kz la lb lc aw ld bi"><span id="577a" class="le ju hp la b fi lf lg l lh li">X=data.iloc[:,:-1] #all rows, all columns except the last<br/>y=data.iloc[:,-1] # all rows, only the last column</span><span id="5a10" class="le ju hp la b fi lj lg l lh li">X=MinMaxScaler().fit_transform(X)<br/>X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.30, random_state = 1)</span></pre><p id="c152" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们将从头开始构建主函数。(类似于sklearn使用的fit函数)</p><pre class="jl jm jn jo fd kz la lb lc aw ld bi"><span id="96a3" class="le ju hp la b fi lf lg l lh li">def sigmoid(x):<br/> return 1/(1+np.exp(-x))</span></pre><p id="731c" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">sigmoid函数是不言自明的，它接受一个值(在我们的例子中是一个numpy数组),并对它应用sigmoid函数。</p><pre class="jl jm jn jo fd kz la lb lc aw ld bi"><span id="8e84" class="le ju hp la b fi lf lg l lh li">def logistic_regression(features,target,epoch,lr):<br/>    <br/>    weights=np.zeros(features.shape[1]) <br/>    #features.shape[1]=number of columns<br/>    <br/>    for i in range(epoch):<br/>        scores=np.dot(features,weights)<br/>        predictions=sigmoid(scores)<br/>        <br/>        gradient=np.dot(features.T,(predictions-target))<br/>        weights-=lr*gradient<br/>    <br/>    return weights</span></pre><p id="92b1" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个函数的目标是计算每个特征(列)的权重。我们传递我们的训练特征、训练标签(目标)、时期数以及用于在每个时期更新权重的学习率。</p><p id="3fad" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们用0初始化我们的权重，然后在每个时期，使用权重矩阵，计算分数(训练数据与权重的点积),并将其传递到我们的sigmoid函数中，以生成我们的预测。</p><p id="1205" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">接下来，我们计算梯度(使用前面描述的公式)，并使用它来更新我们的权重。</p><pre class="jl jm jn jo fd kz la lb lc aw ld bi"><span id="248a" class="le ju hp la b fi lf lg l lh li">weights = logistic_regression(X_train, Y_train,<br/>                     epoch = 10000, lr = 0.1)</span></pre><p id="5259" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们已经使用训练数据计算了每个特征的权重。视结果而定，可以更改周期数和学习率的值。</p><p id="9b05" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用这些权重，我们将计算测试数据的预测，并检查预测的准确性。</p><pre class="jl jm jn jo fd kz la lb lc aw ld bi"><span id="0dff" class="le ju hp la b fi lf lg l lh li">test_scores=np.dot(X_test,weights)<br/>preds=np.round(sigmoid(test_scores)) #convert score to 0 or 1</span><span id="ed7b" class="le ju hp la b fi lj lg l lh li">from sklearn.metrics import accuracy_score<br/>print("Accuracy: "+str(accuracy_score(preds, Y_test)))</span></pre><figure class="jl jm jn jo fd hk er es paragraph-image"><div class="er es lp"><img src="../Images/5f9206dae6927dc85066b22eec17d4d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*lFVjPLl8XZZO6IwIiKh6FA.png"/></div></figure><p id="e42f" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，使用我们的逻辑回归实现，我们在这个数据集上获得了94%的准确率。</p></div></div>    
</body>
</html>