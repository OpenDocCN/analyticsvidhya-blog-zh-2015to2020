<html>
<head>
<title>AI Writing Poems: Building LSTM model using PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">艾写诗:用火炬构筑模式</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ai-writing-poems-building-lstm-model-using-pytorch-d1c58a24bb64?source=collection_archive---------8-----------------------#2020-05-23">https://medium.com/analytics-vidhya/ai-writing-poems-building-lstm-model-using-pytorch-d1c58a24bb64?source=collection_archive---------8-----------------------#2020-05-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/948265820c2fc9979798748bebc50d6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NTIyHs63RZrfflgElCVQLw.png"/></div></div></figure><div class=""/><p id="ff50" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">大家好！！在本文中，我们将使用PyTorch构建一个模型来预测段落中的下一个单词。首先，我们将了解RNN和LSTM，以及他们是如何工作的。然后我们将创建我们的模型。首先，我们加载数据并对其进行预处理。然后我们将使用PyTorch来训练模型并保存它。在那之后，我们将通过给它一个起始文本来从该模型中进行预测，并使用它来生成完整的段落。</p><h1 id="a54e" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">什么是RNN？</h1><p id="b99f" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">在机器学习中，像对狗或猫的图像进行分类这样的简单问题可以通过简单地对数据集进行训练并使用分类器来解决。但是如果我们的问题更复杂，比如我们必须预测段落中的下一个单词。因此，如果我们仔细研究这个问题，我们会发现，我们人类不能仅仅通过使用我们现有的语言和语法知识来解决这个问题。在这种类型的问题中，我们使用段落的前一个单词和段落的上下文来预测下一个单词。</p><p id="dc18" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">传统的神经网络无法做到这一点，因为它们是在固定的数据集上训练的，然后用来进行预测。RNN被用来解决这类问题。RNN代表<strong class="is hu">递归神经网络</strong>。我们可以把RNN想象成一个有回路的神经网络。它将一个状态的信息传递给下一个状态。因此，信息在这个过程中持续存在，我们可以利用它来理解之前的背景，并做出准确的预测。</p><h1 id="2aeb" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">什么是LSTM？</h1><p id="3978" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">因此，如果我们通过使用RNN来解决数据序列的问题，而以前的上下文是被使用的，那么我们为什么需要LSTM呢？要回答这个问题，我们必须看看这两个例子。</p><p id="fc51" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">例一</strong>:“鸟住在<em class="kr">巢</em>。”这里很容易预测单词“nest ”,因为我们已经有了bird的上下文，RNN在这种情况下会工作得很好。</p><p id="2471" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">例2: </strong></p><p id="1827" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“我在印度长大……..所以我会说H <em class="kr"> indi </em>”所以这里预测单词“Hindi”的任务对于一个RNN人来说是困难的，因为这里语境之间的差距很大。通过看“我能说……”这一行，我们无法预测语言，我们将需要额外的印度背景。所以在这里，我们需要对我们的段落有一些长期的依赖，这样我们才能理解上下文。</p><p id="f754" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为此，我们使用LSTM( <strong class="is hu">长短期记忆</strong>)。顾名思义，它们有长时记忆和短时记忆(gate ),两者都用在连词中来做预测。如果我们谈论LSTM的建筑，它们包含4个门，即学习门、忘记门、记忆门、使用门。为了让这篇文章简单易懂，我不打算深入LSTM的建筑理论。但是也许我们会在接下来的文章中讨论它。(也许在下一部😉).</p><h1 id="ad69" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">让我们建立我们的模型。</h1><p id="b6c1" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">现在我们已经完成了理论，让我们开始有趣的部分——建立我们的模型。</p><h1 id="8466" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">加载和预处理数据</h1><p id="8808" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">我将使用来自Kaggle的诗歌数据集。它总共有15，000首诗，因此对于我们模型学习和创建模式来说足够了。现在让我们开始把它载入我们的笔记本。</p><p id="4c85" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.首先导入库。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="a0e6" class="lc jp ht ky b fi ld le l lf lg">import numpy as np<br/>import torch<br/>from torch import nn<br/>import torch.nn.functional as F</span></pre><p id="ad9e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.现在从文本文件加载数据。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="c71e" class="lc jp ht ky b fi ld le l lf lg"># open text file and read in data as `text`<br/>with open('/data/poems_data.txt', 'r') as f:<br/>    text = f.read()</span></pre><p id="9a6a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.我们可以通过打印前100个字符来验证我们的数据。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="214b" class="lc jp ht ky b fi ld le l lf lg">text[:100]</span></pre><p id="06ff" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.正如我们所知，我们的神经网络不理解文本，所以我们必须将我们的txt数据转换为整数。为此，我们可以创建令牌字典，并将字符映射到整数，反之亦然。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="9b55" class="lc jp ht ky b fi ld le l lf lg"># encode the text and map each character to an integer and vice versa</span><span id="b0b3" class="lc jp ht ky b fi lh le l lf lg"># we create two dictionaries:<br/># 1. int2char, which maps integers to characters<br/># 2. char2int, which maps characters to unique integers<br/>chars = tuple(set(text))<br/>int2char = dict(enumerate(chars))<br/>char2int = {ch: ii for ii, ch in int2char.items()}</span><span id="e101" class="lc jp ht ky b fi lh le l lf lg"># encode the text<br/>encoded = np.array([char2int[ch] for ch in text])</span></pre><p id="454f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5.一种热编码用于表示字符。比如，如果我们有三个字符a，b，c，那么我们可以这样来表示它们[1，0，0]，[0，1，0]，[0，0，1]这里我们用1来表示这个字符，其他的都是0。对于我们的用例，我们有许多字符和符号，所以我们的一个热点向量会很长。但是没关系。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="507f" class="lc jp ht ky b fi ld le l lf lg">def one_hot_encode(arr, n_labels):<br/>    <br/>    # Initialize the the encoded array<br/>    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)<br/>    <br/>    # Fill the appropriate elements with ones<br/>    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.<br/>    <br/>    # Finally reshape it to get back to the original array<br/>    one_hot = one_hot.reshape((*arr.shape, n_labels))<br/>    <br/>    return one_hot</span></pre><p id="1077" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在用这种方法测试它。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="e743" class="lc jp ht ky b fi ld le l lf lg"># check that the function works as expected<br/>test_seq = np.array([[0, 5, 1]])<br/>one_hot = one_hot_encode(test_seq, 8)</span><span id="dfba" class="lc jp ht ky b fi lh le l lf lg">print(one_hot)</span></pre><p id="c737" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">6.现在我们必须为我们的模型创建批次，这是非常关键的部分。在这种情况下，我们将选择一个批量大小，即行数，然后序列长度是一个批量中要使用的列数。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="cf43" class="lc jp ht ky b fi ld le l lf lg">def get_batches(arr, batch_size, seq_length):<br/>    '''Create a generator that returns batches of size<br/>       batch_size x seq_length from arr.<br/>       <br/>       Arguments<br/>       ---------<br/>       arr: Array you want to make batches from<br/>       batch_size: Batch size, the number of sequences per batch<br/>       seq_length: Number of encoded chars in a sequence<br/>    '''<br/>    <br/>    batch_size_total = batch_size * seq_length<br/>    # total number of batches we can make<br/>    n_batches = len(arr)//batch_size_total<br/>    <br/>    # Keep only enough characters to make full batches<br/>    arr = arr[:n_batches * batch_size_total]<br/>    # Reshape into batch_size rows<br/>    arr = arr.reshape((batch_size, -1))<br/>    <br/>    # iterate through the array, one sequence at a time<br/>    for n in range(0, arr.shape[1], seq_length):<br/>        # The features<br/>        x = arr[:, n:n+seq_length]<br/>        # The targets, shifted by one<br/>        y = np.zeros_like(x)<br/>        try:<br/>            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]<br/>        except IndexError:<br/>            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]<br/>        yield x, y</span></pre><p id="fd35" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">7 .现在我们可以检查GPU是否可用。(如果GPU不可用，请保持较低的epochs数)</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="2bc3" class="lc jp ht ky b fi ld le l lf lg"># check if GPU is available<br/>train_on_gpu = torch.cuda.is_available()<br/>if(train_on_gpu):<br/>    print('Training on GPU!')<br/>else: <br/>    print('No GPU available, training on CPU; consider making n_epochs very small.')</span></pre><p id="a652" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">8.这里我们创建了一个名为CharRNN的类。这是我们的模特班。在init方法中，我们必须为我们的模型定义层。这里我们使用两个LSTM层。我们还使用了dropout(这有助于避免过度拟合)。对于输出，我们使用简单的线性层。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="a6be" class="lc jp ht ky b fi ld le l lf lg">class CharRNN(nn.Module):<br/>    <br/>    def __init__(self, tokens, n_hidden=256, n_layers=2,<br/>                               drop_prob=0.5, lr=0.001):<br/>        super().__init__()<br/>        self.drop_prob = drop_prob<br/>        self.n_layers = n_layers<br/>        self.n_hidden = n_hidden<br/>        self.lr = lr<br/>        <br/>        # creating character dictionaries<br/>        self.chars = tokens<br/>        self.int2char = dict(enumerate(self.chars))<br/>        self.char2int = {ch: ii for ii, ch in self.int2char.items()}<br/>        <br/>        #lstm layer<br/>        self.lstm=nn.LSTM(len(self.chars),n_hidden,n_layers,<br/>                          dropout=drop_prob,batch_first=True)<br/>        <br/>        #dropout layer<br/>        self.dropout=nn.Dropout(drop_prob)<br/>        <br/>        #output layer<br/>        self.fc=nn.Linear(n_hidden,len(self.chars))</span><span id="2112" class="lc jp ht ky b fi lh le l lf lg">    <br/>    def forward(self, x, hidden):<br/>        ''' Forward pass through the network. <br/>            These inputs are x, and the hidden/cell state `hidden`. '''<br/>        ## Get the outputs and the new hidden state from the lstm<br/>        r_output, hidden = self.lstm(x, hidden)<br/>        <br/>        ## pass through a dropout layer<br/>        out = self.dropout(r_output)<br/>        <br/>        # Stack up LSTM outputs using view<br/>        # you may need to use contiguous to reshape the output<br/>        out = out.contiguous().view(-1, self.n_hidden)<br/>        <br/>        ## put x through the fully-connected layer<br/>        out = self.fc(out)<br/>        return out, hidden<br/>    <br/>    <br/>    def init_hidden(self, batch_size):<br/>        ''' Initializes hidden state '''<br/>        # Create two new tensors with sizes n_layers x batch_size x n_hidden,<br/>        # initialized to zero, for hidden state and cell state of LSTM<br/>        weight = next(self.parameters()).data<br/>        <br/>        if (train_on_gpu):<br/>            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),<br/>                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())<br/>        else:<br/>            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),<br/>                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())<br/>        <br/>        return hidden</span></pre><p id="bcdc" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">9.现在我们有了模型，是时候训练模型了。对于训练，我们必须使用优化器和损失函数。我们简单地计算每一步后的损失，然后优化器阶跃函数将其反向传播，并适当地改变权重。损失会慢慢减少，这意味着我们的模式正在变得更好。</p><p id="eb1b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们还使用两个时期之间的验证来获得验证损失，因为这样我们就可以决定我们的模型是欠拟合还是过拟合。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="0dca" class="lc jp ht ky b fi ld le l lf lg">def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):<br/>    ''' Training a network <br/>    <br/>        Arguments<br/>        ---------<br/>        <br/>        net: CharRNN network<br/>        data: text data to train the network<br/>        epochs: Number of epochs to train<br/>        batch_size: Number of mini-sequences per mini-batch, aka batch size<br/>        seq_length: Number of character steps per mini-batch<br/>        lr: learning rate<br/>        clip: gradient clipping<br/>        val_frac: Fraction of data to hold out for validation<br/>        print_every: Number of steps for printing training and validation loss<br/>    <br/>    '''<br/>    net.train()<br/>    <br/>    opt = torch.optim.Adam(net.parameters(), lr=lr)<br/>    criterion = nn.CrossEntropyLoss()<br/>    <br/>    # create training and validation data<br/>    val_idx = int(len(data)*(1-val_frac))<br/>    data, val_data = data[:val_idx], data[val_idx:]<br/>    <br/>    if(train_on_gpu):<br/>        net.cuda()<br/>    <br/>    counter = 0<br/>    n_chars = len(net.chars)<br/>    for e in range(epochs):<br/>        # initialize hidden state<br/>        h = net.init_hidden(batch_size)<br/>        <br/>        for x, y in get_batches(data, batch_size, seq_length):<br/>            counter += 1<br/>            <br/>            # One-hot encode our data and make them Torch tensors<br/>            x = one_hot_encode(x, n_chars)<br/>            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)<br/>            <br/>            if(train_on_gpu):<br/>                inputs, targets = inputs.cuda(), targets.cuda()</span><span id="8c71" class="lc jp ht ky b fi lh le l lf lg">            # Creating new variables for the hidden state, otherwise<br/>            # we'd backprop through the entire training history<br/>            h = tuple([each.data for each in h])</span><span id="5163" class="lc jp ht ky b fi lh le l lf lg">            # zero accumulated gradients<br/>            net.zero_grad()<br/>            <br/>            # get the output from the model<br/>            output, h = net(inputs, h)<br/>            <br/>            # calculate the loss and perform backprop<br/>            loss = criterion(output, targets.view(batch_size*seq_length).long())<br/>            loss.backward()<br/>            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.<br/>            nn.utils.clip_grad_norm_(net.parameters(), clip)<br/>            opt.step()<br/>            <br/>            # loss stats<br/>            if counter % print_every == 0:<br/>                # Get validation loss<br/>                val_h = net.init_hidden(batch_size)<br/>                val_losses = []<br/>                net.eval()<br/>                for x, y in get_batches(val_data, batch_size, seq_length):<br/>                    # One-hot encode our data and make them Torch tensors<br/>                    x = one_hot_encode(x, n_chars)<br/>                    x, y = torch.from_numpy(x), torch.from_numpy(y)<br/>                    <br/>                    # Creating new variables for the hidden state, otherwise<br/>                    # we'd backprop through the entire training history<br/>                    val_h = tuple([each.data for each in val_h])<br/>                    <br/>                    inputs, targets = x, y<br/>                    if(train_on_gpu):<br/>                        inputs, targets = inputs.cuda(), targets.cuda()</span><span id="17b8" class="lc jp ht ky b fi lh le l lf lg">                    output, val_h = net(inputs, val_h)<br/>                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())<br/>                <br/>                    val_losses.append(val_loss.item())<br/>                <br/>                net.train() # reset to train mode after iterationg through validation data<br/>                <br/>                print("Epoch: {}/{}...".format(e+1, epochs),<br/>                      "Step: {}...".format(counter),<br/>                      "Loss: {:.4f}...".format(loss.item()),<br/>                      "Val Loss: {:.4f}".format(np.mean(val_losses)))</span></pre><p id="cdf7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在用下面的方法训练它。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="4e8e" class="lc jp ht ky b fi ld le l lf lg"># define and print the net<br/>n_hidden = 512<br/>n_layers = 2</span><span id="5525" class="lc jp ht ky b fi lh le l lf lg">net = CharRNN(chars, n_hidden, n_layers)<br/>print(net)</span><span id="b1aa" class="lc jp ht ky b fi lh le l lf lg">batch_size = 128<br/>seq_length = 100<br/>n_epochs =  10# start small if you are just testing initial behavior</span><span id="b85e" class="lc jp ht ky b fi lh le l lf lg"># train the model<br/>train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)</span></pre><p id="05dd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">10.我们可以用下面的方法保存模型。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="36e1" class="lc jp ht ky b fi ld le l lf lg"># change the name, for saving multiple files<br/>model_name = 'poem_4_epoch.net'</span><span id="ec94" class="lc jp ht ky b fi lh le l lf lg">checkpoint = {'n_hidden': net.n_hidden,<br/>              'n_layers': net.n_layers,<br/>              'state_dict': net.state_dict(),<br/>              'tokens': net.chars}</span><span id="9351" class="lc jp ht ky b fi lh le l lf lg">with open(model_name, 'wb') as f:<br/>    torch.save(checkpoint, f)</span></pre><p id="302e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">11.既然模型已经训练好了，我们就要从中取样并预测下一个角色！为了采样，我们传入一个字符，让网络预测下一个字符。然后我们把那个字符传回来，得到另一个预测的字符。只要继续这样做，你就会生成一堆文本！这里，前k个样本只是我们的模型将预测并从中使用最相关的一个字母的数量。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="859e" class="lc jp ht ky b fi ld le l lf lg">def predict(net, char, h=None, top_k=None):<br/>        ''' Given a character, predict the next character.<br/>            Returns the predicted character and the hidden state.<br/>        '''<br/>        <br/>        # tensor inputs<br/>        x = np.array([[net.char2int[char]]])<br/>        x = one_hot_encode(x, len(net.chars))<br/>        inputs = torch.from_numpy(x)<br/>        <br/>        if(train_on_gpu):<br/>            inputs = inputs.cuda()<br/>        <br/>        # detach hidden state from history<br/>        h = tuple([each.data for each in h])<br/>        # get the output of the model<br/>        out, h = net(inputs, h)</span><span id="4c74" class="lc jp ht ky b fi lh le l lf lg">        # get the character probabilities<br/>        p = F.softmax(out, dim=1).data<br/>        if(train_on_gpu):<br/>            p = p.cpu() # move to cpu<br/>        <br/>        # get top characters<br/>        if top_k is None:<br/>            top_ch = np.arange(len(net.chars))<br/>        else:<br/>            p, top_ch = p.topk(top_k)<br/>            top_ch = top_ch.numpy().squeeze()<br/>        <br/>        # select the likely next character with some element of randomness<br/>        p = p.numpy().squeeze()<br/>        char = np.random.choice(top_ch, p=p/p.sum())<br/>        <br/>        # return the encoded value of the predicted char and the hidden state<br/>        return net.int2char[char], h</span><span id="ad84" class="lc jp ht ky b fi lh le l lf lg">def sample(net, size, prime='The', top_k=None):<br/>        <br/>    if(train_on_gpu):<br/>        net.cuda()<br/>    else:<br/>        net.cpu()<br/>    <br/>    net.eval() # eval mode<br/>    <br/>    # First off, run through the prime characters<br/>    chars = [ch for ch in prime]<br/>    h = net.init_hidden(1)<br/>    for ch in prime:<br/>        char, h = predict(net, ch, h, top_k=top_k)</span><span id="4177" class="lc jp ht ky b fi lh le l lf lg">    chars.append(char)<br/>    <br/>    # Now pass in the previous character and get a new one<br/>    for ii in range(size):<br/>        char, h = predict(net, chars[-1], h, top_k=top_k)<br/>        chars.append(char)</span><span id="9bd8" class="lc jp ht ky b fi lh le l lf lg">    return ''.join(chars)</span></pre><p id="018d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">12.现在我们可以用这个样本方法来做预测。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="40b8" class="lc jp ht ky b fi ld le l lf lg">print(sample(net, 500, prime='christmas', top_k=2))</span></pre><p id="047c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">输出看起来会像这样。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="3480" class="lc jp ht ky b fi ld le l lf lg">christmas a son of this</span><span id="ef18" class="lc jp ht ky b fi lh le l lf lg">the sun wants the street of the stars, and the way the way<br/>they went and too man and the star of the words<br/>of a body of a street and the strange shoulder of the sky</span><span id="cd9f" class="lc jp ht ky b fi lh le l lf lg">and the sun, an end on the sun and the sun and so to the stars are stars<br/>and the words of the water and the streets of the world<br/>to see them to start a posture of the streets<br/>on the street of the streets, and the sun and soul of the station<br/>and so too too the world of a sound and stranger and to the world<br/>to the sun a</span></pre><p id="2bae" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我们所看到的，我们的模型能够生成一些好的线条。内容没有太多意义，但它能够生成一些语法正确的行。如果我们能多训练它一段时间，它会表现得更好。</p><h1 id="fffe" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结论</h1><p id="0aba" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">在这篇文章中，我们了解了RNN和LSTM。我们还用PyTorch建立了我们的诗歌模型。我希望这篇文章对你有所帮助。如果您有任何疑问或建议，欢迎在下面的评论区发表，或者通过<a class="ae ks" href="mailto:yash.yn59@gmail.com" rel="noopener ugc nofollow" target="_blank">yash.yn59@gmail.com</a>联系我，我将非常乐意为您提供帮助。</p></div></div>    
</body>
</html>