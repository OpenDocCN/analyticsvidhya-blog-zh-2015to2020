# åŸºäºå¼ é‡æµçš„æ—¶é—´åºåˆ—é¢„æµ‹

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/time-series-forecasting-using-tensorflow-rkt109-ea858d8e49c6?source=collection_archive---------0----------------------->

![](img/36d2cd01d5ea1aa2fd1bbf3a29c86189.png)

**æ—¶é—´åºåˆ—**è¡¨ç¤ºå®ä½“éšæ—¶é—´çš„å˜åŒ–ã€‚ä¾‹å¦‚ï¼Œä½ å¯èƒ½å¬è¯´è¿‡å¤©æ°”é¢„æŠ¥ï¼Œæˆ‘ä»¬è¯•å›¾é¢„æµ‹æœªæ¥æŸä¸ªç‰¹å®šæ—¶é—´çš„å¤©æ°”çŠ¶å†µï¼Œå¯èƒ½æ˜¯ä¸€å¤©ä¸­çš„æŸä¸ªç‰¹å®šæ—¶é—´ã€‚æ—¶é—´åºåˆ—çš„å…¶ä»–ä¾‹å­æœ‰å…¬å¸è‚¡ç¥¨çš„æ¯æ—¥æ”¶ç›˜ä»·ã€æœˆé™é›¨é‡æ•°æ®å’Œå¹´é”€å”®é¢ç­‰ã€‚

æˆ‘æä¾›äº†æœ¬æ•™ç¨‹ä¸­ä½¿ç”¨çš„ä»£ç çš„ GitHub åº“çš„é“¾æ¥ã€‚https://github.com/aryan109/Sunspot_Prediction

# æ—¶é—´åºåˆ—çš„ç»„æˆéƒ¨åˆ†

![](img/7bbec240ed5b9ac827681484ad03ab04.png)

## è¶‹åŠ¿

è¶‹åŠ¿æ˜¾ç¤ºäº†æ•°æ®åœ¨å¾ˆé•¿ä¸€æ®µæ—¶é—´å†…å¢åŠ æˆ–å‡å°‘çš„æ€»ä½“è¶‹åŠ¿ã€‚

![](img/5523fcf00b4093ae64f109bd7e482bc5.png)

## å‘¨æœŸæ€§æ³¢åŠ¨

æ—¶é—´åºåˆ—ä¸­æœ‰ä¸€äº›æˆåˆ†ä¼šåœ¨ä¸€æ®µæ—¶é—´å†…é‡å¤å‡ºç°ã€‚ä»–ä»¬ä»¥ä¸€ç§æœ‰è§„å¾‹çš„é—´æ­‡æ€§æ–¹å¼è¡ŒåŠ¨ã€‚è¿™äº›å˜åŒ–æœ‰ä¸¤ç§ç±»å‹:-
**a)å­£èŠ‚æ€§å˜åŒ–:**è¿™äº›æ˜¯èŠ‚å¥åŠ›ï¼Œåœ¨ä¸åˆ°ä¸€å¹´çš„æ—¶é—´å†…ä»¥è§„åˆ™å’Œå‘¨æœŸæ€§çš„æ–¹å¼èµ·ä½œç”¨ã€‚
**b)å‘¨æœŸæ€§å˜åŒ–:**æ—¶é—´åºåˆ—ä¸­è‡ªèº«è¿è¡Œæ—¶é—´è·¨åº¦è¶…è¿‡ä¸€å¹´çš„å˜åŒ–ä¸ºå‘¨æœŸæ€§å˜åŒ–ã€‚

![](img/04800cfbc006c96d7d53557956193d5b.png)

æœ‰ä¸Šå‡è¶‹åŠ¿çš„å‘¨æœŸæ€§æ³¢åŠ¨

## éšæœºæˆ–ä¸è§„åˆ™è¿åŠ¨(å™ªéŸ³)

è¿™äº›éšæœºæˆ–ä¸è§„åˆ™çš„å˜åŒ–æ˜¯ä¸å¯é¢„è§çš„ã€ä¸å¯æ§çš„ã€ä¸å¯é¢„æµ‹çš„å’Œä¸ç¨³å®šçš„ã€‚

![](img/3992ea67973e6638df7f5e8848d95c0a.png)

å™ªå£°æ³¢å½¢

> ä¸€èˆ¬æ¥è¯´ï¼Œç°å®ä¸–ç•Œä¸­çš„æ—¶é—´åºåˆ—æ—¢æœ‰è¶‹åŠ¿æ€§åˆæœ‰å­£èŠ‚æ€§ï¼Œè¿˜å¸¦æœ‰ä¸€äº›ç™½å™ªå£°ã€‚

## åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†åšå¤ªé˜³é»‘å­é¢„æµ‹ã€‚

ä½ å¯èƒ½åœ¨æƒ³â€œä»€ä¹ˆï¼å­™ä¹Ÿå¾—äº†æ–‘ï¼Ÿï¼Ÿ?"

![](img/7f8b6aa30d0e4ccf7462ccfbf3ea7159.png)

å›¾åƒæ˜¾ç¤ºä¸€ä¸ªçœŸæ­£çš„å¤ªé˜³é»‘å­ã€‚

**å¤ªé˜³é»‘å­**æ˜¯å¤ªé˜³è¡¨é¢å‘ˆç°é»‘è‰²çš„åŒºåŸŸã€‚å®ƒä»¬çœ‹èµ·æ¥å¾ˆæš—ï¼Œå› ä¸ºå®ƒä»¬æ¯”å¤ªé˜³è¡¨é¢çš„å…¶ä»–éƒ¨åˆ†æ›´å†·ã€‚ç„¶è€Œï¼Œå¤ªé˜³é»‘å­çš„æ¸©åº¦ä»ç„¶å¾ˆé«˜â€”â€”å¤§çº¦ 6500 åæ°åº¦ï¼å¤ªé˜³é»‘å­è¢«ç”¨æ¥è®°å½•å¤ªé˜³å‘¨æœŸã€‚**å¤ªé˜³å‘¨æœŸ**æ˜¯å¤ªé˜³ç£åœºå¤§çº¦æ¯ 11 å¹´ç»å†çš„å‘¨æœŸã€‚

# æˆ‘ä»¬å¼€å§‹å§

![](img/be5d528a3c626474f95f4ae521d4bc6b.png)

ç”± [Max Duzij](https://unsplash.com/@max_duz?utm_source=medium&utm_medium=referral) åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„çš„ç…§ç‰‡

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸‹è½½æˆ‘ä»¬çš„æ—¶é—´åºåˆ—æ•°æ®ã€‚ä¸‹é¢æ˜¯ä»¥ CSV æ ¼å¼ä¸‹è½½æ•°æ®çš„ä»£ç ã€‚

```
!wget â€” no-check-certificate \
 [https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Sunspots.csv](https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Sunspots.csv) \
 -O /tmp/sunspots.csv
```

ä¸‹è½½çš„ CSV æ–‡ä»¶å¦‚ä¸‹æ‰€ç¤º:

![](img/d81eb213af508b7ff7d5898b3ce70afa.png)

é»‘å­. csv

å¦‚æ‚¨æ‰€è§ï¼Œè¯¥æ–‡ä»¶æ€»å…±åŒ…å« 3 åˆ—ï¼Œæˆ‘ä»¬å°†æŠŠç¬¬ 1 åˆ—å’Œç¬¬ 3 åˆ—æå–åˆ°ä¸€ä¸ª NumPy æ•°ç»„ä¸­ã€‚æˆ‘ä»¬å·²ç»é€‰æ‹©äº†ç´¢å¼•åˆ—ï¼Œè¿™å°†å¾ˆå®¹æ˜“ä½œå‡ºé¢„æµ‹ï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€è¦ç»™ç´¢å¼•ã€‚

```
import csv
time_step = []
sunspots = []with open(â€˜/tmp/sunspots.csvâ€™) as csvfile:
 reader = csv.reader(csvfile, delimiter=â€™,â€™)
 next(reader)
 for row in reader:
 sunspots.append(float(row[2]))
 time_step.append(int(row[0]))series = np.array(sunspots)
time = np.array(time_step)
print(â€˜series: {}â€™.format(series[:5]))
print(â€˜time: {}â€™.format(time[:5]))
```

è®©æˆ‘ä»¬ç”»å‡ºè¿™ä¸ªç³»åˆ—ï¼Œçœ‹çœ‹å®ƒæœ‰ä»€ä¹ˆã€‚

```
plt.figure(figsize=(10, 6))
plot_series(time, series)
```

![](img/749fbdfe9859eab12c42f3439459099e.png)

# æ­£åœ¨å‡†å¤‡æ•°æ®é›†

æˆ‘ä»¬å¿…é¡»å°†æˆ‘ä»¬çš„æ—¶é—´åºåˆ—åˆ†æˆè®­ç»ƒæœŸå’ŒéªŒè¯æœŸã€‚

```
split_time = 3000
time_train = time[:split_time]
x_train = series[:split_time]
time_valid = time[split_time:]
x_valid = series[split_time:]
```

åˆ†å‰²æ—¶é—´ä¸º 3000 æ„å‘³ç€ä» 0 åˆ° 3000 å°†ç”¨äºè®­ç»ƒï¼Œ3000 åˆ°æœ€åç”¨äºéªŒè¯ã€‚

ç°åœ¨æˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥åˆ›å»ºä¸€ä¸ªçª—å£æ•°æ®é›†ã€‚åœ¨ä¸€ä¸ª**çª—å£æ•°æ®é›†ä¸­ï¼Œ**å‰ n ä¸ªå€¼å¯ä»¥è¢«è§†ä¸º**è¾“å…¥ç‰¹å¾**ã€‚è€Œå¸¦æœ‰ä»»æ„æ—¶é—´æˆ³çš„å½“å‰å€¼å°±æ˜¯**è¾“å‡ºæ ‡ç­¾**ã€‚çª—å£æ•°æ®é›†ç”±å›ºå®šçš„**çª—å£å¤§å°**ç»„æˆã€‚

![](img/0df1dc4508e347df653c9e58789e4087.png)

```
def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
 series = tf.expand_dims(series, axis=-1)
 ds = tf.data.Dataset.from_tensor_slices(series)
 ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
 ds = ds.flat_map(lambda w: w.batch(window_size + 1))
 ds = ds.shuffle(shuffle_buffer)
 ds = ds.map(lambda w: (w[:-1], w[1:]))
 return ds.batch(batch_size).prefetch(1)
```

ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ **windowed_dataset** å‡½æ•°åˆ›å»ºè®­ç»ƒæ•°æ®é›†

```
tf.keras.backend.clear_session()
tf.random.set_seed(51)
np.random.seed(51)
shuffle_buffer_size = 1000
window_size = 64
batch_size = 256
train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
print(x_train.shape)
```

# åˆ›å»ºæ¨¡å‹

```
tf.keras.backend.clear_session()
model = tf.keras.models.Sequential([
 tf.keras.layers.Conv1D(filters=60, kernel_size=5,
 strides=1, padding=â€causalâ€,
 activation=â€reluâ€,
 input_shape=[None, 1]),
 tf.keras.layers.LSTM(60, return_sequences=True),
 tf.keras.layers.LSTM(60, return_sequences=True),
 tf.keras.layers.Dense(30, activation=â€reluâ€),
 tf.keras.layers.Dense(10, activation=â€reluâ€),
 tf.keras.layers.Dense(1),
 tf.keras.layers.Lambda(lambda x: x * 400)
])
model.summary()
```

è¯¥æ¨¡å‹çš„æ‘˜è¦å¦‚ä¸‹æ‰€ç¤º:

![](img/7ccb1b0021b1fc4255209fb4b764db86.png)

æ­£å¦‚ä½ æ‰€çœ‹åˆ°çš„ï¼Œè¿™ä¸ªæ¨¡å‹ç”±**å·ç§¯å±‚ã€LSTM å±‚**å’Œ**è‡´å¯†å±‚**ç»„æˆã€‚å¦‚æœä½ ä¸çŸ¥é“è¿™äº›å±‚åˆ°åº•æ˜¯åšä»€ä¹ˆçš„ï¼Œä¸è¦æ‹…å¿ƒï¼Œå¾ˆå¿«æˆ‘ä¼šå†™ä¸€ç¯‡æ–‡ç« æè¿°æ‰€æœ‰ä¸åŒç±»å‹å±‚çš„åŠŸèƒ½ã€‚

# ç°åœ¨è®©æˆ‘ä»¬ç¼–è¯‘å’Œè®­ç»ƒæ¨¡å‹

![](img/978eb294a4171db164c00ccf7248c4c1.png)

ç…§ç‰‡ç”± [Roozbeh Eslami](https://unsplash.com/@roozbeheslami?utm_source=medium&utm_medium=referral) åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„

```
optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)
model.compile(loss=tf.keras.losses.Huber(),
 optimizer=optimizer,
 metrics=[â€œmaeâ€])
history = model.fit(train_set,epochs=500)
```

è®­ç»ƒå¼€å§‹äº†â€¦..

![](img/91435df45ee48a2ea632de1005e13956.png)

è®­ç»ƒéœ€è¦æ—¶é—´ï¼Œæ‰€ä»¥è¯·æ”¾æ¾ã€‚

![](img/4a66bb792ab1f7ae81a1fc7df7c47a9e.png)

Max van den Oetelaar åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) æ‹æ‘„çš„ç…§ç‰‡

# è¯„ä¼°æ¨¡å‹

ç°åœ¨ï¼Œæˆ‘ä»¬å°†é€šè¿‡è§‚å¯Ÿè®­ç»ƒæŸå¤±æ¥äº†è§£æˆ‘ä»¬çš„æ¨¡å‹æœ‰å¤šå¥½

```
# â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” -
# Retrieve a list of list results on training and test data
# sets for each training epoch
# â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” -
loss=history.history[â€˜lossâ€™]
epochs=range(len(loss)) # Get number of epochs
# â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” 
# Plot training and validation loss per epoch
# â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” 
plt.plot(epochs, loss, â€˜râ€™)
plt.title(â€˜Training lossâ€™)
plt.xlabel(â€œEpochsâ€)
plt.ylabel(â€œLossâ€)
plt.legend([â€œLossâ€])
plt.figure()zoomed_loss = loss[200:]
zoomed_epochs = range(200,500)
# â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” 
# Plot training and validation loss per epoch
# â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” â€” 
plt.plot(zoomed_epochs, zoomed_loss, â€˜râ€™)
plt.title(â€˜Training lossâ€™)
plt.xlabel(â€œEpochsâ€)
plt.ylabel(â€œLossâ€)
plt.legend([â€œLossâ€])plt.figure()
```

å›¾è¡¨åº”è¯¥ä¸æ­¤ç±»ä¼¼ã€‚

![](img/02614fb7f8cfda0defcc771a766c5370.png)

ç¬¬äºŒä¸ªå›¾åªæ˜¯æ”¾å¤§äº†ç¬¬ä¸€ä¸ªå›¾çš„æœ€åä¸€éƒ¨åˆ†ã€‚é€šè¿‡è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°äº†è§£è¿‡å» 300 å¹´çš„è®­ç»ƒçŠ¶å†µã€‚

# è®©æˆ‘ä»¬é¢„æµ‹ä¸€ä¸‹

ç°åœ¨ï¼Œéšç€ä½ çš„æ¨¡å‹è¢«è®­ç»ƒï¼Œå‡†å¤‡å¥½çœ‹åˆ°æœªæ¥ã€‚

![](img/9714b4085742ccc9087773250534eb7e.png)

ç…§ç‰‡ç”± [Elena Koycheva](https://unsplash.com/@lenneek?utm_source=medium&utm_medium=referral) åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) ä¸Šæ‹æ‘„

æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å¸®åŠ©æˆ‘ä»¬åšé¢„æµ‹ã€‚

```
def model_forecast(model, series, window_size):
 ds = tf.data.Dataset.from_tensor_slices(series)
 ds = ds.window(window_size, shift=1, drop_remainder=True)
 ds = ds.flat_map(lambda w: w.batch(window_size))
 ds = ds.batch(32).prefetch(1)
 forecast = model.predict(ds)
 return forecast
```

ç°åœ¨æˆ‘ä»¬å°†ä½¿ç”¨è¿™ä¸ªå‡½æ•°æ¥åšé¢„æµ‹ã€‚

```
rnn_forecast = model_forecast(model, series[â€¦, np.newaxis], window_size)
rnn_forecast = rnn_forecast[split_time â€” window_size:-1, -1, 0]
plt.figure(figsize=(10, 6))
plot_series(time_valid, x_valid)
plot_series(time_valid, rnn_forecast)
```

![](img/219e26a92d8230b5ebe7e3b16fe1ebc9.png)

è“è‰²æ˜¯åŸå§‹ç³»åˆ—ï¼Œæ©™è‰²æ˜¯é¢„æµ‹ç³»åˆ—ï¼Œç»“æœçœ‹èµ·æ¥ç›¸å½“ä¸é”™ğŸ˜ƒã€‚

![](img/dc08000aa2f548a44fc372d177ce5f6c.png)

å¸Œæœ›ä½ æ˜ç™½å¦‚ä½•ç”¨ TensorFlow åšæ—¶é—´åºåˆ—é¢„æµ‹ã€‚

è¯·ç»™å‡ºæ‚¨çš„åé¦ˆæˆ–å»ºè®®ã€‚è°¢è°¢ä½ ğŸ™‚