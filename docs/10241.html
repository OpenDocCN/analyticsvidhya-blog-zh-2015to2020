<html>
<head>
<title>Predicting Heart Failure Using Machine Learning, Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用机器学习预测心力衰竭，第2部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/predicting-heart-failure-using-machine-learning-part-2-b343471dbde8?source=collection_archive---------13-----------------------#2020-10-10">https://medium.com/analytics-vidhya/predicting-heart-failure-using-machine-learning-part-2-b343471dbde8?source=collection_archive---------13-----------------------#2020-10-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a836" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">XGBoost参数优化的简便方法</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/e33b5c463f0e3135fafab18adb9974a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0rK-VbfRW1aoyF8C6xKwfQ.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">照片由<a class="ae jt" href="https://unsplash.com/@averey?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Robina Weermeijer </a>在<a class="ae jt" href="https://unsplash.com/s/photos/medical?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><p id="c026" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我的<a class="ae jt" rel="noopener" href="/analytics-vidhya/predicting-heart-failure-using-machine-learning-part-1-6c57ce7bee8c">上一篇文章</a>中，我使用随机森林、XGBoost、神经网络和一组模型预测了心力衰竭。在这篇文章中，我想回顾一下XGBoost参数优化，以提高模型的准确性。</p><p id="0e5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">据官方<a class="ae jt" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> XGBoost网站</a>介绍，XGBoost被定义为一个优化的分布式梯度增强库，旨在高效、灵活、可移植。它在梯度推进框架下实现机器学习算法。XGBoost提供了一种并行的树提升(也称为GBDT，GBM ),可以快速准确地解决许多数据科学问题。</p><p id="ea46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">XGBoost非常受<a class="ae jt" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>比赛参与者的欢迎，因为它可以实现非常高的模型精度。唯一的问题是为了得到好的结果，需要优化的参数数量。</p><p id="be24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">XGBoost有三种类型的参数:常规参数、助推器参数和任务参数。一般参数选择哪一个助推器你正在使用做助推器，通常树或线性模型；助推器参数取决于你选择的助推器；学习任务参数指定学习任务和相应的学习目标。所有参数的详细描述可以在<a class="ae jt" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="2b34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看所有参数超出了本文的范围。相反，我将集中精力优化以下选定的树增强器参数，以提高我们的XGBoost模型的准确性:</p><ol class=""><li id="dc78" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">有助于防止过度拟合的参数(别名用于使用sklearn命名约定的XGBoost python sklearn包装器)</li></ol><p id="be04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du kd ke kf kg b">eta</code>[默认值=0.3，范围:[0，1]，别名:<code class="du kd ke kf kg b">learning_rate</code> ]</p><ul class=""><li id="afba" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc kh ka kb kc bi translated">更新中使用的步长收缩可防止过度拟合。在每一步提升后，我们可以直接得到新特征的权重，<code class="du kd ke kf kg b">eta</code>收缩特征权重，使提升过程更加保守。</li></ul><p id="dfe0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du kd ke kf kg b">max_depth</code>[默认值=6，范围[0，∞]]</p><ul class=""><li id="3fcb" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc kh ka kb kc bi translated">树的最大深度。增加该值将使模型更加复杂，并且更有可能过度拟合。只有当tree_method设置为<code class="du kd ke kf kg b">hist</code>时，0才在<code class="du kd ke kf kg b">lossguided</code>增长策略中被接受，它表示对深度没有限制。请注意，XGBoost在训练深度树时会大量消耗内存。</li></ul><p id="1b0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du kd ke kf kg b">min_child_weight</code>[默认值=1，范围[0，∞]]</p><ul class=""><li id="0973" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc kh ka kb kc bi translated">一个孩子所需的最小体重总和。如果树划分步骤导致实例权重之和小于<code class="du kd ke kf kg b">min_child_weight</code>的叶节点，那么构建过程将放弃进一步的划分。在线性回归任务中，这只是对应于每个节点中需要的最小实例数。<code class="du kd ke kf kg b">min_child_weight</code>越大，算法就越保守。</li></ul><p id="7335" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du kd ke kf kg b">gamma</code>[默认=0，范围[0，∞]，别名:<code class="du kd ke kf kg b">min_split_loss</code> ]</p><ul class=""><li id="3621" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc kh ka kb kc bi translated">在树的叶节点上进行进一步划分所需的最小损失减少。<code class="du kd ke kf kg b">gamma</code>越大，算法就越保守。</li></ul><p id="6745" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du kd ke kf kg b">subsample</code>[默认值=1，范围:[0，1] ]</p><ul class=""><li id="9f55" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc kh ka kb kc bi translated">训练实例的子样本比率。将其设置为0.5意味着XGBoost会在生成树之前随机采样一半的训练数据。这将防止过度拟合。子采样将在每个提升迭代中发生一次。</li></ul><p id="19b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du kd ke kf kg b">colsample_bytree</code>[默认值=1，范围[0，1]]</p><ul class=""><li id="6130" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc kh ka kb kc bi translated">Colsample_bytree是构造每个树时列的子样本比率。对于每个构建的树，进行一次子采样。</li></ul><p id="d180" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du kd ke kf kg b">lambda</code>[默认值=1，别名:<code class="du kd ke kf kg b">reg_lambda</code> ]</p><ul class=""><li id="d378" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc kh ka kb kc bi translated">关于权重的L2正则化项。增加该值将使模型更加保守。</li></ul><p id="cba5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du kd ke kf kg b">alpha</code>[默认值=0，别名:<code class="du kd ke kf kg b">reg_alpha</code> ]</p><ul class=""><li id="61d9" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc kh ka kb kc bi translated">关于权重的L1正则项。增加该值将使模型更加保守。</li></ul><p id="e3ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.用于处理不平衡数据集的参数</p><p id="26bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du kd ke kf kg b">scale_pos_weight</code>[默认值=1]</p><ul class=""><li id="3772" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc kh ka kb kc bi translated">控制正负权重的平衡，对不平衡类有用。要考虑的典型值:<code class="du kd ke kf kg b">sum(negative instances) / sum(positive instances)</code>。</li></ul><p id="803f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.其他参数</p><p id="4a84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du kd ke kf kg b">n_estimators</code>[默认值=100]</p><ul class=""><li id="4ac7" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc kh ka kb kc bi translated">梯度推进树的数量。相当于助推轮数。</li></ul><p id="1ef3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上参数定义均来自官方XGBoost <a class="ae jt" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">网站</a>。</p><p id="ce58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有了这些关于tree booster参数的简短知识，让我们导入库，加载数据集，创建独立和相关变量，并将数据集分成训练集和测试集。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="ki kj l"/></div></figure><p id="4b46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们用默认参数训练我们的模型。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="ki kj l"/></div></figure><p id="3a70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如您所见，即使使用默认参数，该模型也为我们提供了可接受的结果。为了找到最佳参数，我使用了GridSearchCV，这是sklearn的model_selection包中的一个库函数。它有助于循环通过预定义的超参数，并使我们的模型适合训练集。因此，最终，我们可以从列出的超参数中选择最佳参数。我为以下每个参数尝试了三个值:learning_rate、max_depth、min_child_weight、gamma、subsample和colsample_bytree。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="ki kj l"/></div></figure><p id="85ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我用更新的参数训练我们的模型。因为我降低了学习率，所以我增加了梯度推进树的数量。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="ki kj l"/></div></figure><p id="01ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过这一简单的步骤，我成功地将验证准确性从76.67%提高到80.00%，并将验证AUC(曲线下面积)从68.81%提高到75.48%。</p><p id="a30f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有了充足的时间和计算机能力，人们可以扩大助推器参数搜索的取值范围，并使用GridSearchCV结果作为进一步参数研究的基础。例如，如果GridSearchCV learning_rate从默认的0.3减少到0.2，在下一轮搜索中，我们可以像[0.05，0.1，0.2]一样将范围进一步左移。</p><p id="b2aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我尝试优化正则化参数reg_lambda和reg_alpha。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="ki kj l"/></div></figure><p id="bf71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GridSearchCV发现reg_alpha和reg_lambda的默认值是最佳值。最后一个需要优化的参数是scale_pos_weight，我一直在增加这个参数，直到找到一个值为4的参数，这样可以得到最好的结果。然后，我用这些最终优化的参数来训练我的模型。下面是带有额外指标的代码，包括灵敏度、特异性、阳性预测值和阴性预测值。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="ki kj l"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kk"><img src="../Images/79200e3396a80b3daead29445beae092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zQkDKoTjuWj7kSX5jJ_bpA.png"/></div></div></figure><p id="4417" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结论:在GridSearchCV的帮助下，通过几个简单的步骤，我成功地将验证准确性从76.67%提高到78.33%，并将验证AUC(曲线下面积)从68.81%提高到77.09%。虽然使用特征工程可以获得最佳的精度改进，但是XGBoost参数优化也是值得的。</p><p id="b201" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢你花时间阅读这篇文章。</p><p id="21f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这艰难的时刻致以最美好的祝愿。<br/>安德鲁<br/><a class="ae jt" href="http://twitter.com/tampapath" rel="noopener ugc nofollow" target="_blank">@坦帕帕斯</a></p></div></div>    
</body>
</html>