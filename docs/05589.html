<html>
<head>
<title>9 issues I’ve encountered when setting up a Hadoop/Spark cluster for the first time</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我第一次设置Hadoop/Spark集群时遇到的9个问题</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/9-issues-ive-encountered-when-setting-up-a-hadoop-spark-cluster-for-the-first-time-87b023624a43?source=collection_archive---------17-----------------------#2020-04-26">https://medium.com/analytics-vidhya/9-issues-ive-encountered-when-setting-up-a-hadoop-spark-cluster-for-the-first-time-87b023624a43?source=collection_archive---------17-----------------------#2020-04-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7f50964d40cf18ccb0fe85bc5f3bb2ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*h9igGonn5Nieh9mP"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://unsplash.com/@bepnamanh?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">南安</a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure><p id="913f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在<a class="ae iu" rel="noopener" href="/analytics-vidhya/how-i-set-up-my-first-hadoop-spark-cluster-preparation-663893486d12">之前的文章</a>中，我们已经讨论了如何准备Hadoop/Spark集群的设置。现在，准备集群只是开始。虽然有很多关于在集群上设置Hadoop的资源，但作为一个初学者，我发现它有时会令人困惑，我花了十几个小时才把它们全部组装起来。因此，在本文中，我将记录在设置Hadoop/Spark集群时遇到的问题以及如何解决它们。如果你想自己尝试一下，请查看我跟随的教程来设置这一切，这是我全心全意推荐的。</p><p id="a9eb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，为了让您了解一些背景，我的设置是由一台笔记本电脑(它将充当名称节点和数据节点)和另外两个数据节点Raspberry Pis组成的，如下所示:</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/c5ae60957b6212641b871038233f355a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/0*PcjKuFVp6grJNkZW.png"/></div></figure><p id="fb14" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意，我将运行Spark 2.4.5和Hadoop 3.2.1。</p><p id="353f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，我们已经将hadoop下载、解包并移动到/opt/hadoop。</p><p id="6172" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们试着启动它</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="6b5e" class="kd ke hi jz b fi kf kg l kh ki">hadoop version</span></pre><p id="12f9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">哎呀！</p><h1 id="942e" class="kj ke hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">JAVA_HOME_NOT_SET</h1><p id="3f81" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">首先，请检查你是否安装了Java。如果你喜欢Spark，请记住当前稳定的Spark版本与Java 11不兼容。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="ea27" class="kd ke hi jz b fi kf kg l kh ki">sudo apt install openjdk-8-jre-headless -y</span></pre><p id="340e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，让我们看看环境变量。教程(为Raspbian构建)建议将这个环境变量设置为类似</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="4353" class="kd ke hi jz b fi kf kg l kh ki">export JAVA_HOME=$(readlink –f /usr/bin/java | sed "s:bin/java::")</span></pre><p id="d078" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而下面的<a class="ae iu" href="https://stackoverflow.com/questions/14325594/working-with-hadoop-localhost-error-java-home-is-not-set/14462600#14462600" rel="noopener ugc nofollow" target="_blank">灵感来自这里</a>，让它为我工作(在Ubuntu上):</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="31ad" class="kd ke hi jz b fi kf kg l kh ki">export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64</span></pre><p id="bd40" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还请注意我已经添加到<em class="ll">中的环境变量。bashrc </em>:</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="cc55" class="kd ke hi jz b fi kf kg l kh ki">export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-arm64<br/>export HADOOP_HOME=/opt/hadoop<br/>export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin<br/>export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span></pre><p id="2470" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，我们需要将上面的JAVA_HOME添加到<em class="ll">/opt/Hadoop/etc/Hadoop/Hadoop-env . sh .</em></p><p id="05d3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们再试一次:</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="fb8a" class="kd ke hi jz b fi kf kg l kh ki">hadoop version</span></pre><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/098ba86c924be7c5746bb3318ae271cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MHCp7o9h05RoEcmf1RQFSA.png"/></div></div></figure><p id="5b0e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作品！向前看。</p><h1 id="064a" class="kj ke hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated"><code class="du ln lo lp jz b">Permission denied (publickey,password).</code></h1><p id="3386" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">因此，一切都准备好脱离Hadoop集群。唯一的问题是跑步的时候</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="48d9" class="kd ke hi jz b fi kf kg l kh ki">start-dfs.sh &amp;&amp; start-yarn.sh</span></pre><p id="d0d3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我一直收到以下错误:</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="feda" class="kd ke hi jz b fi kf kg l kh ki">localhost: Permission denied (publickey,password).</span></pre><p id="bd5c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">事实证明，这里发生了很多事情。首先，如果我的用户与从属节点上的用户名不同，那么我的SSH验证就会失败。为了解决这个问题，在主服务器上，<a class="ae iu" href="https://stackoverflow.com/questions/51822209/hadoop-cluster-hadoop-user-ssh-communication/51830400#51830400" rel="noopener ugc nofollow" target="_blank">我必须在<strong class="ix hj"> <em class="ll">建立一个配置文件</em></strong></a>~/。ssh/config  如下:</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="7ca0" class="kd ke hi jz b fi kf kg l kh ki">Host rpi-3<br/>HostName rpi-3<br/>User ubuntu<br/>IdentityFile ~/.ssh/id_rsa</span><span id="5180" class="kd ke hi jz b fi lq kg l kh ki">Host rpi-4<br/>HostName rpi-4<br/>User ubuntu<br/>IdentityFile ~/.ssh/id_rsa</span></pre><p id="0b38" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这清楚地表明了在使用SSH连接时我想要使用的用户。</p><p id="5235" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但这还不是全部。然而更重要的是，我需要先用SSH进入本地主机，这样才能运行start-dfs.sh和start-yarn。向前看。</p><h1 id="938b" class="kj ke hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">Hadoop集群设置—Java . net . connect异常:连接被拒绝</h1><p id="3f54" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">我们列表中的下一个错误花了我一个晚上去调试。所以当我试图用<code class="du ln lo lp jz b">start-dfs.sh </code>启动集群时，我收到了一个连接被拒绝的错误。首先，不用想太多，我已经遵循了<a class="ae iu" href="https://stackoverflow.com/questions/28661285/hadoop-cluster-setup-java-net-connectexception-connection-refused/33553214#33553214" rel="noopener ugc nofollow" target="_blank">这里</a>的建议，将默认服务器设置为0.0.0.0。</p><p id="01aa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正确的答案实际上是将它设置为我的名称节点服务器的地址(在<strong class="ix hj"> core-site.xml </strong>中)my，并确保在<strong class="ix hj"> /etc/hosts </strong>中没有将它与127.0.0.1或localhost绑定的条目。Hadoop不喜欢这样，我已经被警告过了。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/0d9d25c5dc02b332e5bfb67c70db48ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*v67mp8v345xzoD0n-bh_LQ.png"/></div></figure><h1 id="6fef" class="kj ke hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">未显示的节点</h1><p id="64cc" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">在多节点设置中，一些节点缺失可能是由多种原因造成的。错误的配置、环境差异甚至防火墙都可能导致问题。这里有几个如何理解它的技巧。</p><h2 id="ef6b" class="kd ke hi bd kk ls lt lu ko lv lw lx ks jg ly lz kw jk ma mb la jo mc md le me bi translated">首先检查用户界面</h2><p id="1e62" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">在典型的Hadoop堆栈中有许多web接口。其中两个是在端口8088上公开的Yarn UI和在端口9870上公开的名称节点信息UI。在我的例子中，我在HDFS有一个不同数量的节点，这让我调查并找到我的纱线设置的问题。</p><h2 id="c89c" class="kd ke hi bd kk ls lt lu ko lv lw lx ks jg ly lz kw jk ma mb la jo mc md le me bi translated">使用jps查看服务是否正在运行</h2><p id="b110" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">Jps是一个Java工具，但是您可以使用它来查看特定机器上运行了哪些Hadoop服务。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/d88e52e87bb8fc05c5d49089d083e4d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/1*129J8izkHOKHEG94dm1paA.png"/></div></div></figure><p id="abf9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">给定节点上的DataNode是否已启动？让我们继续前进。</p><h2 id="271c" class="kd ke hi bd kk ls lt lu ko lv lw lx ks jg ly lz kw jk ma mb la jo mc md le me bi translated">深入了解hadoop日志</h2><p id="e53a" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">Hadoop日志位于logs子文件夹中，所以在我的例子中是/opt/hadoop/logs。看不到特定的数据节点？Ssh进入该机器，并分析该特定服务的日志。</p><h2 id="31f7" class="kd ke hi bd kk ls lt lu ko lv lw lx ks jg ly lz kw jk ma mb la jo mc md le me bi translated">检查配置文件</h2><p id="0b7b" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">这是最可能产生问题的地方。查看日志，记录正确的配置并应用它们。配置文件位于<em class="ll"> /opt/hadoop/etc/hadoop </em>中。从与您试图调试的服务相对应的文件开始。</p></div><div class="ab cl mg mh gp mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="hb hc hd he hf"><p id="6cdd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">好了，我们现在对Hadoop还是比较满意的。火花呢？我已经下载了Spark(没有Hadoop)并且解包了。现在怎么办？一帆风顺？没那么快。</p><h1 id="9423" class="kj ke hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">火花无法启动</h1><p id="6b48" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">如果你下载了Spark standalone，你很可能会遇到和我一样的问题，所以一定要在<code class="du ln lo lp jz b">conf/spark-env.sh</code>中添加以下内容</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="8437" class="kd ke hi jz b fi kf kg l kh ki">export SPARK_DIST_CLASSPATH=$(/path/to/hadoop/bin/hadoop classpath)</span></pre><p id="a046" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该解决方案的积分在这里<a class="ae iu" href="https://stackoverflow.com/questions/42307471/spark-without-hadoop-failed-to-launch/42689980#42689980" rel="noopener ugc nofollow" target="_blank">到</a>。</p><h1 id="8cc6" class="kj ke hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">Pyspark错误—不支持的类文件主要版本55</h1><p id="bdef" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">说真的，我在上面已经警告过你，目前稳定的Spark版本只兼容Java 8。<a class="ae iu" href="https://stackoverflow.com/questions/53583199/pyspark-error-unsupported-class-file-major-version-55/53583241#53583241" rel="noopener ugc nofollow" target="_blank">关于那个</a>的更多细节。</p><h1 id="8009" class="kj ke hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">Java . lang . numberformatexception:对于输入字符串:“0x100”</h1><p id="3efd" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">接下来，在启动<em class="ll"> spark-shell </em>时，您会看到这条无害但恼人的消息，可以通过向<strong class="ix hj">添加一个环境变量来轻松修复。bashrc </strong>。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="011e" class="kd ke hi jz b fi kf kg l kh ki">export TERM=xterm-color</span></pre><p id="d239" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">点击阅读更多信息<a class="ae iu" href="https://stackoverflow.com/questions/56457685/how-to-fix-the-sbt-crash-java-lang-numberformatexception-for-input-string-0x/56457806#56457806" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="f60c" class="kj ke hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated"><strong class="ak"> Spark UI坏了CSS </strong></h1><p id="1378" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">当启动spark-shell或提交spark作业时，在namenode上的端口4040处提供Spark上下文Web UI。在我的例子中，问题是UI有一个破损的界面，这使得使用它变得不可能。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/d3f5b04224a3838bebfcc0413361d2a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*zRv0HMOQDtaw1BjCAIUjyg.png"/></div></figure><p id="6727" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">斯达克弗洛又一次成了我的朋友。要对此进行排序，需要启动spark-shell并运行以下命令:</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="e36f" class="kd ke hi jz b fi kf kg l kh ki">sys.props.update("spark.ui.proxyBase", "")</span></pre><p id="81ab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此处归功于解决方案<a class="ae iu" href="https://stackoverflow.com/questions/47875064/spark-ui-appears-with-wrong-format-broken-css/55466574#55466574" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="07e8" class="kj ke hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">属性火花.纱线.罐子</h1><p id="0ac5" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">当在纱线簇上以簇模式运行Spark时，Spark。jar类需要被运送到没有安装Spark的其他节点。也许我花了更多的时间来整理它，但是在我的情况下，解决方案是将它上传到HDFS内部的一个位置，并让节点在需要时从那里获取它。</p><p id="a2e1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">创建归档:</p><p id="833a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du ln lo lp jz b">jar cv0f spark-libs.jar -C $SPARK_HOME/jars/ .</code></p><p id="ecb6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上传到HDFS</p><p id="2be8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du ln lo lp jz b">hdfs dfs -put spark-libs.jar /some/path/</code></p><p id="582a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，在<em class="ll"> spark-defaults.conf </em>中设置</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="523c" class="kd ke hi jz b fi kf kg l kh ki">spark.yarn.archive      hdfs://your-name-node:9000/spark-libs.jar</span></pre><p id="73f5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此处归功于解决方案和解释<a class="ae iu" href="https://stackoverflow.com/questions/41112801/property-spark-yarn-jars-how-to-deal-with-it" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="03cd" class="kj ke hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">结论</h1><p id="7962" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">以上总结了我们作为初学者在设置Hadoop和Spark时遇到的一些错误。我们将尝试一下我们的集群，对其进行测试，并在以后的文章中报告。感谢阅读，敬请关注！</p></div></div>    
</body>
</html>