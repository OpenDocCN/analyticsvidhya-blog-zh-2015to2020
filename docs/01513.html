<html>
<head>
<title>Being Lazy is Useful — Lazy Evaluation in Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">懒惰是有用的Spark中的懒惰评估</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/being-lazy-is-useful-lazy-evaluation-in-spark-1f04072a3648?source=collection_archive---------0-----------------------#2019-10-28">https://medium.com/analytics-vidhya/being-lazy-is-useful-lazy-evaluation-in-spark-1f04072a3648?source=collection_archive---------0-----------------------#2019-10-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/fdadbeb767456c9840279c97a9f1c457.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*h_wTrluEsM4OgRn1JwZkOA.jpeg"/></div></figure><p id="d05e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" href="https://www.thedailystar.net/star-weekend/almost-useful-life-hacks/news/its-hard-work-being-lazy-1708627" rel="noopener ugc nofollow" target="_blank">图像来源</a></p><p id="1574" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">假设您有一个包含数百万行的非常大的数据文件。你需要通过映射、过滤、随机分割甚至是非常基本的加减运算来进行分析。</p><p id="b943" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">现在，对于大型数据集，即使是一个基本的转换也需要执行数百万次操作。</strong></p><p id="f017" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在处理大数据时，优化这些操作至关重要，Spark以一种非常有创意的方式处理它。你只需要告诉Spark你想对数据集做什么样的转换，Spark就会维护一系列的转换。当您从Spark请求结果时，它会找出最佳路径并执行所需的转换，然后给您结果。</p><p id="1334" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，让我们举个例子。您有一个1 GB的文本文件，并为它创建了10个分区。您还执行了一些转换，最后，您请求查看第一行的外观。在这种情况下，Spark将只从第一个分区读取文件并给出结果，因为您所请求的结果不需要读取整个文件。</p><p id="7d80" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们来举几个实际例子，看看Spark是如何进行懒评的。在第一步中，我们创建了一个包含1000万个数字的列表，并创建了一个包含3个分区的RDD:</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="d38b" class="ju jv hi jq b fi jw jx l jy jz"># create a sample list<br/>my_list = [i for i in range(1,10000000)]</span><span id="8157" class="ju jv hi jq b fi ka jx l jy jz"># parallelize the data<br/>rdd_0 = sc.parallelize(my_list,3)</span><span id="b0f3" class="ju jv hi jq b fi ka jx l jy jz">rdd_0</span></pre><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es kb"><img src="../Images/97b5d5779480ea357aa1a58b55c36a07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/0*NN6CJRqpxBnTw23G.png"/></div></figure><p id="6b08" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">接下来，我们将执行一个非常基本的转换，比如给每个数字加4。请注意，此时Spark尚未开始任何转换。它只是以RDD血统的形式记录了一系列的转变。使用函数<strong class="io hj"> toDebugString </strong>可以看到RDD血统:</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="83b7" class="ju jv hi jq b fi jw jx l jy jz"># add value 4 to each number<br/>rdd_1 = rdd_0.map(lambda x : x+4)</span><span id="5c25" class="ju jv hi jq b fi ka jx l jy jz"># RDD object<br/>print(rdd_1)</span><span id="fd8e" class="ju jv hi jq b fi ka jx l jy jz"># get the RDD Lineage <br/>print(rdd_1.toDebugString())</span></pre><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es kc"><img src="../Images/bff413cb994773b48f8a3216ae9591a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*h_LgLxYDT1UqE-15.png"/></div></div></figure><p id="3b9b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以看到<em class="kh"> PythonRDD[1] </em>与<em class="kh"> ParallelCollectionRDD[0] </em>连接。现在，让我们继续添加另一个转换，将20添加到列表的所有元素中。</p><p id="0c26" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">你可能会想，如果在一个步骤中增加24，而不是多做一步会更好。但是检查RDD血统这一步之后:</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="3453" class="ju jv hi jq b fi jw jx l jy jz"># add value 20 each number<br/>rdd_2 = rdd_1.map(lambda x : x+20)</span><span id="2887" class="ju jv hi jq b fi ka jx l jy jz"># RDD Object<br/>print(rdd_2)</span><span id="86c9" class="ju jv hi jq b fi ka jx l jy jz"># get the RDD Lineage<br/>print(rdd_2.toDebugString())</span></pre><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ki"><img src="../Images/d6dc0fb3043abf0bce449c5a70b8c46c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1Bg96Cqd1d8_lW8t.png"/></div></div></figure><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es kj"><img src="../Images/2b75247d48ccaf225b2a9c7e6ab9cd2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/0*QTOeYmC-eqxoCTIW.png"/></div></figure><p id="2e2b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以看到，它已经自动跳过了那个多余的步骤，并将在一个步骤中添加24，而不是我们定义的那样。因此，Spark自动定义执行任何动作的最佳路径，并且只在需要时执行转换。</p><p id="29bd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们再举一个例子来理解懒评的过程。</p><p id="8e3d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">假设我们有一个文本文件，我们用4个分区创建了它的RDD。现在，我们定义一些转换，比如将文本数据转换成小写，对单词进行切片，给单词添加一些前缀等等。</p><p id="a102" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">但是最后，当我们执行一个操作，比如获取转换数据的第一个元素时，Spark只在第一个分区上执行转换，因为不需要查看完整的数据来执行请求的结果:</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="34da" class="ju jv hi jq b fi jw jx l jy jz"># create a RDD of the text file with Number of Partitions = 4<br/>my_text_file = sc.textFile('tokens_spark.txt',minPartitions=4)</span><span id="0789" class="ju jv hi jq b fi ka jx l jy jz"># RDD Object<br/>print(my_text_file)</span><span id="7186" class="ju jv hi jq b fi ka jx l jy jz"># convert to lower case<br/>my_text_file = my_text_file.map(lambda x : x.lower())</span><span id="7a42" class="ju jv hi jq b fi ka jx l jy jz"># Updated RDD Object<br/>print(my_text_file)</span><span id="b109" class="ju jv hi jq b fi ka jx l jy jz"># Get the RDD Lineage<br/>print(my_text_file.toDebugString())</span></pre><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es kk"><img src="../Images/754da2a104f75acee31a68951c5921a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ac4mkmJyxtVqQwBd.png"/></div></div></figure><p id="8675" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这里，我们将单词转换成小写，并切分每个单词的前两个字符(然后请求第一个单词)。</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="7a26" class="ju jv hi jq b fi jw jx l jy jz"># slice the words<br/>my_text_file = my_text_file.map(lambda x : x[:2])</span><span id="8864" class="ju jv hi jq b fi ka jx l jy jz"># RDD Object<br/>print(my_text_file)</span><span id="ec3e" class="ju jv hi jq b fi ka jx l jy jz"># Get the RDD Lineage<br/>print(my_text_file.toDebugString())</span><span id="b61e" class="ju jv hi jq b fi ka jx l jy jz"># Get the first element after all the transformations<br/>print(my_text_file.first())</span></pre><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es kl"><img src="../Images/aff0ca06b0db226a92fcbf79cb62dcd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*BGQA9m5AN6Td7ZOi.png"/></div></div></figure><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es km"><img src="../Images/83dc6db936312306f2adce9da85f63c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NbweyIOibRRXJ_LH.png"/></div></div></figure><p id="d036" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里发生了什么？我们创建了文本文件的4个分区。但是根据我们需要的结果，并不需要在所有分区上读取和执行转换，因此Spark只需要这样做。</p><p id="bee9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果要统计独一无二的单词呢？然后我们需要读取所有分区，这正是Spark要做的:</p><pre class="jl jm jn jo fd jp jq jr js aw jt bi"><span id="30ae" class="ju jv hi jq b fi jw jx l jy jz">print(my_text_file.countApproxDistinct())</span></pre><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es kn"><img src="../Images/30d96efad506723c9a7c596caae1f10e.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/0*UwF3ocHM9CwrPsLE.png"/></div></figure><figure class="jl jm jn jo fd ij er es paragraph-image"><div role="button" tabindex="0" class="kd ke di kf bf kg"><div class="er es ko"><img src="../Images/d690d11ea749bc11307c5082aed4bc16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CrjUI1ynTKxhidzk.png"/></div></div></figure><p id="028a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kh">原载于2019年10月28日</em><a class="ae jk" href="https://www.analyticsvidhya.com/blog/2019/10/pyspark-for-beginners-first-steps-big-data-analysis/" rel="noopener ugc nofollow" target="_blank"><em class="kh">【https://www.analyticsvidhya.com】</em></a><em class="kh">。</em></p></div></div>    
</body>
</html>