<html>
<head>
<title>Basics and Beyond: Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基础与超越:逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/basics-and-beyond-logistic-regression-34549d2ee800?source=collection_archive---------11-----------------------#2020-12-31">https://medium.com/analytics-vidhya/basics-and-beyond-logistic-regression-34549d2ee800?source=collection_archive---------11-----------------------#2020-12-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="ca3b" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">一点一点拆开逻辑回归算法</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/f5df8a5dfa95d7bfe26604a87000ef8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XF7y3LAH7QfDlalW"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">照片由丹·迈耶斯拍摄</figcaption></figure><p id="6319" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这篇文章将带你从最基本的开始进行逻辑回归。为了掌握机器学习，必须非常清楚基础知识。一开始可能看起来很累，但是一旦你有了完美的基础，编写代码将是一件轻而易举的事情。大多数帖子试图一口气涵盖所有内容，老实说，这有时会让人不知所措。试图理解复杂的方程，然后切换到代码，然后再回到数学，只会让你很难跟上。这个系列旨在用一种稍微不同的方法来指导你完成机器学习。我们将首先分解算法并理解它的绝对细节，然后我们将在接下来的文章中继续实现。</p><p id="044f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这篇文章是在关于线性回归的同一个系列的文章之后，所以我建议也浏览一下，因为在那篇文章中介绍和解释的一些概念在这里被直接使用:</p><div class="kj kk ez fb kl km"><a href="https://kumudlakara.medium.com/basics-and-beyond-linear-regression-c12d99a4df35" rel="noopener follow" target="_blank"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hj fi z dy kr ea eb ks ed ef hh bi translated">基础与超越:线性回归</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">这篇文章将带你从最基本的开始进行线性回归。当开始机器学习线性…</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">kumudlakara.medium.com</p></div></div><div class="kv l"><div class="kw l kx ky kz kv la jh km"/></div></div></a></div><p id="b624" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">好了，我们开始吧！</p><p id="6519" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">逻辑回归属于监督机器学习算法的范畴。监督学习广泛涵盖了两种类型的问题:</p><ol class=""><li id="8c91" class="lb lc hi jp b jq jr jt ju jw ld ka le ke lf ki lg lh li lj bi translated">回归问题</li><li id="9d0e" class="lb lc hi jp b jq lk jt ll jw lm ka ln ke lo ki lg lh li lj bi translated">分类问题(我们在这篇文章中的重点)</li></ol><p id="68a5" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">逻辑回归属于分类问题的范畴。但是这些“分类”问题到底是什么呢？简单来说，分类问题试图预测离散输出的结果。他们试图将变量分成不同的类别。这也有助于记住，当我们试图预测的目标变量是离散的(例如，在数学意义上{4，11}是离散的，而as [4，11]是连续的集合)时，这是一个分类问题。</p><p id="da57" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">分类问题的一些例子是:</p><ul class=""><li id="6532" class="lb lc hi jp b jq jr jt ju jw ld ka le ke lf ki lp lh li lj bi translated">给定产品细节，预测用户是否会购买产品</li><li id="77a3" class="lb lc hi jp b jq lk jt ll jw lm ka ln ke lo ki lp lh li lj bi translated">给定泰坦尼克号数据集，预测一个人能否幸存</li><li id="2ea8" class="lb lc hi jp b jq lk jt ll jw lm ka ln ke lo ki lp lh li lj bi translated">将电子邮件分类为垃圾邮件</li><li id="3355" class="lb lc hi jp b jq lk jt ll jw lm ka ln ke lo ki lp lh li lj bi translated">将肿瘤分类为恶性或良性</li></ul><p id="febf" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们在这些问题中试图预测的变量是<em class="lq"> y: </em></p><p id="3954" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">y = 0:缺少某些东西(例如，用户不会购买，乘客不会存活，良性肿瘤等。)</p><p id="9439" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">y = 1:存在某种情况(例如，顾客会购买，乘客会幸存，恶性肿瘤等。)</p><p id="587e" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在上面所有的例子中，我们可以看到有一点是相似的，那就是输出空间。输出不是任何连续的值，事实上它通常是一个离散的集合(把它想成是/否问题，再看一下上面的例子)。因此，我们的输出空间是离散的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lr"><img src="../Images/e46a54a4939325cb4b87ed91ae4e0852.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*wKhZ4QT3Ihq_i-FrHjDrkA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">逻辑回归的输出示例</figcaption></figure><p id="57a5" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">使用<a class="ae ls" href="https://kumudlakara.medium.com/basics-and-beyond-linear-regression-c12d99a4df35" rel="noopener">线性回归</a>解决另一种类型的监督机器学习问题，即回归问题。</p><p id="71eb" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在任何监督学习问题中，我们的目标很简单:</p><p id="6224" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><em class="lq">“给定一个训练集，我们想学习一个函数h: X →Y，使得h(x)是对Y的相应值的一个很好的预测”</em></p><p id="c3c5" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里<em class="lq"> h(x) </em>被称为假设函数，基本上是我们试图通过我们的学习算法预测的(在这种情况下是逻辑回归)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lt"><img src="../Images/1508e4f39ec0699a12b0fc541e2ecc8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*4Qojxwl9aZS0PVIbrRL1Ug.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">h将x映射到y</figcaption></figure><p id="b80b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">线性回归和逻辑回归的主要区别在于假设函数<em class="lq"> h(x) </em>。让我们从二元分类开始，然后我们可以很容易地将这种观点扩展到多类分类。</p><h1 id="9ef9" class="lu lv hi bd lw lx ly lz ma mb mc md me io mf ip mg ir mh is mi iu mj iv mk ml bi translated">假设函数</h1><p id="1274" class="pw-post-body-paragraph jn jo hi jp b jq mm ij js jt mn im jv jw mo jy jz ka mp kc kd ke mq kg kh ki hb bi translated">我们希望我们的分类器输出介于0和1之间的值，为此，我们需要使用一个特殊的假设函数来映射介于0和1之间的值。有许多这样的函数可用，但在进行逻辑回归时，sigmoid函数似乎表现得最好。让我们来看看逻辑回归的假设方程:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mr"><img src="../Images/2690016b9f8c67fe2c1879241879bec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TqepP2ua_h8URlObuusLuQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">假设方程和sigmoid函数</figcaption></figure><p id="5af2" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><em class="lq"> g(z) </em>是sigmoid函数，在我们的例子中<em class="lq">z =θ’x</em>其中<em class="lq">θ’</em>是<em class="lq"> θ </em>的转置。假设函数现在看起来像这样:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ms"><img src="../Images/6cfc13e9fef3d52a6d55febbe85f77f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*faPuamsWFTwDvFtrKZxpWQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">逻辑回归的假设函数</figcaption></figure><p id="37a0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">sigmoid函数也称为逻辑函数。sigmoid函数的图形看起来像这样:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mt"><img src="../Images/79029d07d8ec9f1ecf88a4b829120d13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HXCBO-Wx5XhuY_OwMl0Phw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">sigmoid函数的图形</figcaption></figure><p id="9e23" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">从图中我们可以看到，sigmoid函数正是我们想要的分类器。它将X轴上的所有实数值映射到Y轴上的[0，1]之间。</p><h1 id="4886" class="lu lv hi bd lw lx ly lz ma mb mc md me io mf ip mg ir mh is mi iu mj iv mk ml bi translated">但是我们的输出不是应该是离散的吗？</h1><p id="0f61" class="pw-post-body-paragraph jn jo hi jp b jq mm ij js jt mn im jv jw mo jy jz ka mp kc kd ke mq kg kh ki hb bi translated">是的，我们的“分类器”的输出仍然是离散的。这里我们说的是我们假设函数<em class="lq"> h(x) </em>的输出。你现在可能已经明白了，这意味着我们的假设函数的精确的原始输出并不是我们的分类器的输出。现在让我们来解释这个假设函数，看看它能有什么帮助。</p><p id="bbc4" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">假设函数<em class="lq"> h(x) </em>并不是我们分类器的最终输出，实际上它是给定输入<em class="lq"> x </em>的<em class="lq"> y = 1 </em>的概率。例如:如果我们考虑一封电子邮件是否被分类为垃圾邮件，那么<em class="lq"> h(x) = 0.6 </em>意味着该电子邮件有60%的可能性是垃圾邮件。</p><p id="9ef2" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">因此，该假设也可以表示为</p><p id="98ff" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><em class="lq">h(x)= P(y = 1 | x；θ) </em></p><p id="095e" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在上面的等式中，右手边是给定由<em class="lq"> θ参数化的<em class="lq"> x </em>时<em class="lq"> y = 1 </em>的概率。</em>我们必须记住<em class="lq"> θ </em>这里是假设中的参数向量。由于我们现在正在讨论二元分类，因此很明显y=0或y=1，因此:</p><p id="262f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">p(y = 0 | x<em class="lq">；θ)= 1—</em>P(y = 1 | x<em class="lq">；θ) </em></p><p id="18f7" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">假设函数实际上创建了一条线或曲线来分隔y = 0和y = 1的区域。这条线或曲线就是我们所说的决策边界。现在，让我们最终到达我们从假设中找到输出的部分。</p><p id="6ff5" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">一种方法是当<em class="lq"> h(x) </em>的输出大于或等于0.5时预测<em class="lq"> y </em> =1，当<em class="lq"> h(x) </em>小于0.5时预测<em class="lq"> y </em> =0。现在，让我们再次看看sigmoid函数:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mt"><img src="../Images/22978e708eb3f86db5b1c489a707d9ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jm63uLoR1ujppPVrB9PLFA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">Sigmoid函数</figcaption></figure><p id="5eef" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">到目前为止，我们已经得出了这个结论:</p><p id="9cbf" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><em class="lq"> y </em> = 1如果<em class="lq">h(x)</em>≥0.5→上图中黄色区域</p><p id="123f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><em class="lq">y</em>= 0 if<em class="lq">h(x)</em>&lt;0.5→上图中标注粉色的区域</p><p id="d0db" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在上面的等式中，我们可以用<em class="lq"> g(z) </em>替换<em class="lq"> h(x) </em>，记住我们最初关于假设和sigmoid函数的讨论。从图中还可以看出:</p><p id="b8c2" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">当<em class="lq"> z </em> ≥ 0时<em class="lq"> g(z) </em> ≥ 0.5(均以黄色标注)</p><p id="8d3f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><em class="lq"> g(z) </em> &lt; 0.5当<em class="lq"> z </em> &lt; 0时(均以粉红色标注)。</p><p id="df28" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">好了，这就是关于假设函数的全部内容。现在我们有了一个假设，我们可以将输入<em class="lq"> x </em>传递给它，并获得分类器的二进制输出。</p><p id="cef2" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在我们有了一个假设，我们需要评估我们的假设有多好。为此，我们需要计算我们预测的“成本”,这基本上是对我们的预测与真实值接近程度的衡量。这就是成本函数发挥作用的地方。</p><h1 id="563c" class="lu lv hi bd lw lx ly lz ma mb mc md me io mf ip mg ir mh is mi iu mj iv mk ml bi translated">价值函数</h1><p id="9b29" class="pw-post-body-paragraph jn jo hi jp b jq mm ij js jt mn im jv jw mo jy jz ka mp kc kd ke mq kg kh ki hb bi translated">如果我们的输出是<em class="lq"> h(x) </em>，而实际输出应该是<em class="lq"> y </em>，那么成本函数实质上找到了我们希望我们的模型产生的“成本”。因此，成本实际上应该与<em class="lq"> h(x) </em>和<em class="lq"> y </em>之差成比例，这变得非常直观。</p><p id="6f1a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">嗯，我们不能使用与线性回归相同的成本函数，因为逻辑回归的输出将是“波动的”,因此会导致许多局部最优。</p><p id="b861" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">线性回归的成本函数如下所示:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mu"><img src="../Images/e657c0a5fb2e87f816cbf463aeb008cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*QBu06GQqeF2uSiPBw8SkeA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">逻辑回归的成本函数</figcaption></figure><p id="8120" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们不能将这个成本函数用于逻辑回归，原因是线性回归和逻辑回归的假设不同。逻辑回归的假设涉及一个sigmoid函数，因此是一个复杂的非线性函数。如果我们把这个非线性的<em class="lq"> h(x) </em>放到上面的<em class="lq"> J(θ) </em>的方程中，我们会得到一个非凸函数。现在这是一个问题，因为一个复杂的非凸函数会有许多局部最优值，因此梯度下降将变得越来越困难。由于这个原因，我们需要一个凸函数，这样梯度下降可以找到全局最小值。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mv"><img src="../Images/41d56c2f56bb271c636144cf02f173d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hhBfdROMQ_MQ-JZtTuqVpg.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">非凸梯度下降的3d图</figcaption></figure><p id="3701" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">好了，现在让我们看看如何用逻辑回归的<em class="lq"> h(x) </em>来实现这个凸成本函数。首先，让我们用以下等式取代上一等式中的平方误差项:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mw"><img src="../Images/6413894beb42cae3fe00ad7183146f57.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*x01GF04nfU2GjNCxkz3mMA.png"/></div></figure><p id="90b1" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">所以现在我们的成本函数看起来像这样:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mx"><img src="../Images/86597adc0b67b3bd70d34d8c9aa4e0bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*4YINv9cCdcThf8yNRaR13Q.png"/></div></figure><p id="ee5e" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在让我们定义这个<em class="lq">成本:</em></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es my"><img src="../Images/38f18d4bbbed96120e90bbcc1df19d41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*loLg5aAQ_LCP1LcbzK-uZg.png"/></div></figure><p id="b4f0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这实际上是我们的逻辑回归成本函数。如果这还没有直观的意义，请不要担心。在这里，图形表示应该会有所帮助:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mz"><img src="../Images/16a5197fc9ac20cdc3df43a49cba6a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*-PqoXORkvIlWmGuTbKsikg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">当y=0时，绘制成本图(h(x)，y)</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es na"><img src="../Images/60665afe67d188fc2f771ba0d445d556.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*iVHBWFP1yLWlODgQdBpzaA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">当y=1时，绘制成本图(h(x)，y)</figcaption></figure><p id="7495" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">从上面的图表中，我们可以得出对成本函数的数学理解如下:</p><blockquote class="nb nc nd"><p id="befd" class="jn jo lq jp b jq jr ij js jt ju im jv ne jx jy jz nf kb kc kd ng kf kg kh ki hb bi translated">如果h(x) = y，则Cost(h(x)，y) = 0</p><p id="9320" class="jn jo lq jp b jq jr ij js jt ju im jv ne jx jy jz nf kb kc kd ng kf kg kh ki hb bi translated">Cost(h(x)，y) →∞如果y = 0且h(x) →1</p><p id="c12b" class="jn jo lq jp b jq jr ij js jt ju im jv ne jx jy jz nf kb kc kd ng kf kg kh ki hb bi translated">Cost(h(x)，y) →∞如果y = 1且h(x) →0</p></blockquote><p id="e3b0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这意味着，如果正确答案应该是<em class="lq"> y=0 </em>，那么如果我们的假设函数也输出0，那么我们的成本函数将是0。但是如果我们的假设接近1，那么我们的成本函数就接近无穷大(∞)。类似地，如果正确答案是1，那么如果我们的假设输出1，我们的成本函数将是0，如果<em class="lq"> h(x)=0 </em>，它将接近无穷大(∞)。这正是我们想要我们的成本函数做的。如果模型预测正确，我们希望模型产生0成本，如果预测与正确标签完全相反，我们希望模型产生最大成本(<em class="lq"> y </em>)。像这样定义成本函数保证了<em class="lq"> J(θ) </em>对于逻辑回归是凸的。</p><p id="922e" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在让我们使用上面定义的逻辑来简化我们的<em class="lq">成本</em>函数，如下所示:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nh"><img src="../Images/ab16bf4cb3d5859c9b4a4c14df3fcd56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*mKw7I8fJiP_EDVkUI9qWTQ.png"/></div></div></figure><p id="2475" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">您可以通过插入<em class="lq"> y=0 </em>并评估表达式，然后插入<em class="lq"> y=1 </em>并再次评估成本来验证成本函数的表达式。你会发现你得到了和我们之前定义的相同的成本方程。</p><p id="94fe" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在整个成本函数变成了:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ni"><img src="../Images/c4293f7d5be2976f5431eecd99de9249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3ZJpCljzbpBovSfj82SnqA.png"/></div></div></figure><p id="8ca0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">好了，现在我们有了成本函数。下一步当然是优化我们的参数，为此我们利用梯度下降。</p><h1 id="1ba6" class="lu lv hi bd lw lx ly lz ma mb mc md me io mf ip mg ir mh is mi iu mj iv mk ml bi translated">梯度下降</h1><p id="5d84" class="pw-post-body-paragraph jn jo hi jp b jq mm ij js jt mn im jv jw mo jy jz ka mp kc kd ke mq kg kh ki hb bi translated">梯度下降本身就是一个相当大的话题。要详细了解梯度下降，请务必查看同一个系列中的帖子，它将带您从最基础的开始进行梯度下降:</p><div class="kj kk ez fb kl km"><a href="https://kumudlakara.medium.com/basics-and-beyond-gradient-descent-87fa964c31dd" rel="noopener follow" target="_blank"><div class="kn ab dw"><div class="ko ab kp cl cj kq"><h2 class="bd hj fi z dy kr ea eb ks ed ef hh bi translated">基础和超越:梯度下降</h2><div class="kt l"><h3 class="bd b fi z dy kr ea eb ks ed ef dx translated">这篇文章旨在带你从梯度下降的基本概念到高级概念。从…开始时</h3></div><div class="ku l"><p class="bd b fp z dy kr ea eb ks ed ef dx translated">kumudlakara.medium.com</p></div></div><div class="kv l"><div class="nj l kx ky kz kv la jh km"/></div></div></a></div><p id="9607" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">出于我们的目的，我们将直接跳到算法:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nk"><img src="../Images/ef7009a65c199b02cdf9d8a4ff7c5139.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vX-nLVVU-NJ14HyTRnaS5g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">逻辑回归的梯度下降</figcaption></figure><p id="c4ea" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里我们可以看到，这个算法实际上和我们在线性回归的例子中看到的是一样的。然而，唯一的区别是假设函数<em class="lq"> h(x) </em>的定义，它是逻辑回归情况下的sigmoid函数。上面的等式是梯度下降的主要“更新”步骤，其中在最小化成本之后，我们试图在正确的方向上更新我们的参数。<em class="lq"> α </em>这里是学习率。</p><p id="ba6f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">嗯，大概就是这样。这就是逻辑回归的全部。到目前为止，为了便于理解，我们已经讨论了二元分类。然而，我们的方法可以很容易地扩展到多类分类问题的情况。让我们快速看一下。</p><h1 id="c357" class="lu lv hi bd lw lx ly lz ma mb mc md me io mf ip mg ir mh is mi iu mj iv mk ml bi translated">多类分类:一个对所有</h1><p id="6bea" class="pw-post-body-paragraph jn jo hi jp b jq mm ij js jt mn im jv jw mo jy jz ka mp kc kd ke mq kg kh ki hb bi translated">当我们有…嗯，“多个”类(超过2个类)时，我们使用这种方法。所以我们的<em class="lq"> y </em>不再仅仅是0或1，而是<em class="lq"> y </em>也可以取其他离散值。记住虽然y={0，1，…。，n}，y仍然是离散的。</p><p id="9bac" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里的方法实际上很简单。我们把问题分成多个(准确的说是n+1个)二元分类问题。在每一个例子中，我们都预测了<em class="lq"> y </em>是我们其中一个类的成员的概率。所以基本上我们选择一个类，把所有其他类放在一个单独的第二类中。这现在变成了一个二元分类问题，我们知道如何解决这些问题。这个迷你二进制分类问题的输出将会给我们一个概率，这个概率是我们选择的第一个类中的一个，这个类没有和其他类放在一起。现在，我们只需对每个类重复此操作，然后使用返回最高值的假设作为我们的预测，因为该假设显示了我们预测正确的最大概率。因此，我们最终选择最大化<em class="lq"> h(x) </em>或<em class="lq"> y=1 </em>的概率的类。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nl"><img src="../Images/f9b9c17cc5e0efad4f2bdf3d30ed69cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*Gh9ffVSV3TZsbKlVKLHC3A.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">二元分类与多类分类</figcaption></figure><h1 id="1e15" class="lu lv hi bd lw lx ly lz ma mb mc md me io mf ip mg ir mh is mi iu mj iv mk ml bi translated">就是这样！</h1><p id="41f7" class="pw-post-body-paragraph jn jo hi jp b jq mm ij js jt mn im jv jw mo jy jz ka mp kc kd ke mq kg kh ki hb bi translated">干得好！你现在知道什么是逻辑回归以及它是如何工作的了。如果你第一次没有理解所有的数学，不要担心。起初这看起来有点令人畏惧，但是掌握它的秘诀是不断复习基础知识，并让它们绝对清晰。好了，现在你应该能够完全理解任何实现逻辑回归的代码了。在本系列的实现文章之前，我建议您查看一些逻辑回归的实现，并尝试将代码拆开，理解一切是如何工作的。</p></div><div class="ab cl nm nn gp no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="hb hc hd he hf"><h1 id="e124" class="lu lv hi bd lw lx nt lz ma mb nu md me io nv ip mg ir nw is mi iu nx iv mk ml bi translated">参考</h1><ol class=""><li id="2148" class="lb lc hi jp b jq mm jt mn jw ny ka nz ke oa ki lg lh li lj bi translated"><a class="ae ls" href="https://www.holehouse.org/" rel="noopener ugc nofollow" target="_blank">http://www.holehouse.org/</a></li><li id="0552" class="lb lc hi jp b jq lk jt ll jw lm ka ln ke lo ki lg lh li lj bi translated">Marc Peter Deisenroth，A. Aldo Faisal，Cheng Soon Ong </li><li id="42d1" class="lb lc hi jp b jq lk jt ll jw lm ka ln ke lo ki lg lh li lj bi translated">https://www.coursera.org/learn/machine-learning/home/<a class="ae ls" href="https://www.coursera.org/learn/machine-learning/home/" rel="noopener ugc nofollow" target="_blank"/></li></ol></div></div>    
</body>
</html>