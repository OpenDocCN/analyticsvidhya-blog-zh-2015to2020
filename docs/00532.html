<html>
<head>
<title>10 Powerful Applications of Linear Algebra in Data Science</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性代数在数据科学中的10个强大应用</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/10-powerful-applications-of-linear-algebra-in-data-science-c9c0b6018074?source=collection_archive---------3-----------------------#2019-07-23">https://medium.com/analytics-vidhya/10-powerful-applications-of-linear-algebra-in-data-science-c9c0b6018074?source=collection_archive---------3-----------------------#2019-07-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="71ca" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">拥有多种资源</h2></div><h1 id="fe10" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">概观</h1><ul class=""><li id="71f2" class="jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">线性代数为各种各样的数据科学算法和应用提供了动力</li><li id="9dfd" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">在这里，我们提出10个这样的应用，线性代数将帮助你成为一个更好的数据科学家</li><li id="f832" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">我们将这些应用分为不同的领域——基础机器学习、降维、自然语言处理和计算机视觉</li></ul><h1 id="54d3" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">介绍</h1><p id="f688" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">如果数据科学是蝙蝠侠，线性代数就是罗宾。这个忠实的伙伴经常被忽视。但在现实中，它推动了数据科学的主要领域，包括自然语言处理和计算机视觉的热门领域。</p><p id="9811" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">我个人看到许多数据科学爱好者跳过这个主题，因为他们发现数学太难理解了。当用于数据科学的编程语言提供了大量处理数据的包时，人们就不会太在意线性代数了。</p><p id="e8be" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">那是一个错误。线性代数是我们如此熟悉的所有强大的机器学习算法的背后。这是数据科学家技能组合中至关重要的一环。正如我们很快会看到的，你应该考虑线性代数作为一个必须知道的数据科学的主题。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es le"><img src="../Images/606e94365c98a81ba79d0670148cebaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uL41IHtWh-1pngu1.png"/></div></div></figure><p id="668f" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">相信我，线性代数真的是无孔不入的！它将开启你以前想象不到的工作和操作数据的可能性。</p><p id="9f37" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">在本文中，我详细解释了线性代数在数据科学中的十个令人敬畏的应用。我将应用程序大致分为四个领域，供您参考:</p><p id="2832" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">我还为每个应用程序提供了资源，因此您可以更深入地了解吸引您注意力的应用程序。</p><p id="5ac9" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated"><em class="lq">注:在你继续阅读之前，我推荐你浏览一下这篇出色的文章——</em><a class="ae lr" href="https://www.analyticsvidhya.com/blog/2017/05/comprehensive-guide-to-linear-algebra/?utm_source=blog&amp;utm_medium=10-applications-linear-algebra-data-science" rel="noopener ugc nofollow" target="_blank"><em class="lq"/></a><em class="lq">【数据科学线性代数】。它不是理解我们将在这里讨论的内容的强制要求，但它对你的初学技能是一篇有价值的文章。</em></p><h1 id="735a" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">目录</h1><ul class=""><li id="abae" class="jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">为什么要学习线性代数？</li></ul><h2 id="6d26" class="ls iy hi bd iz lt lu lv jd lw lx ly jh jw lz ma jj jy mb mc jl ka md me jn mf bi translated">机器学习中的线性代数</h2><ul class=""><li id="2157" class="jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">损失函数</li><li id="19b2" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">正规化</li><li id="a52e" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">协方差矩阵</li><li id="8925" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">支持向量机分类</li></ul><h2 id="d293" class="ls iy hi bd iz lt lu lv jd lw lx ly jh jw lz ma jj jy mb mc jl ka md me jn mf bi translated">降维中的线性代数</h2><ul class=""><li id="f645" class="jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">主成分分析</li><li id="eabb" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">奇异值分解</li></ul><h2 id="d557" class="ls iy hi bd iz lt lu lv jd lw lx ly jh jw lz ma jj jy mb mc jl ka md me jn mf bi translated">自然语言处理中的线性代数</h2><ul class=""><li id="139e" class="jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">单词嵌入</li><li id="a99e" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">潜在语义分析</li></ul><h2 id="5ef4" class="ls iy hi bd iz lt lu lv jd lw lx ly jh jw lz ma jj jy mb mc jl ka md me jn mf bi translated">计算机视觉中的线性代数</h2><ul class=""><li id="289e" class="jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">作为张量的图像表示</li><li id="8def" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">卷积和图像处理</li></ul><h1 id="2ed4" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">为什么要学习线性代数？</h1><p id="0fb1" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">这个问题我遇到过太多次了。当您可以简单地用Python导入一个包并构建您的模型时，为什么要花时间学习线性代数呢？这是一个公平的问题。因此，让我提出我对此的观点。</p><p id="c435" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">我认为线性代数是数据科学的基础之一。没有坚实的基础，你不可能建造摩天大楼，是吗？想想这个场景:</p><blockquote class="mg mh mi"><p id="c9eb" class="km kn lq jr b js kz ij ko ju la im kp mj lb kr ks mk lc ku kv ml ld kx ky kc hb bi translated"><em class="hi">您希望使用主成分分析(PCA)来降低数据的维数。如果您不知道将如何影响您的数据，您将如何决定保留多少主成分？显然，你需要知道算法的机制来做这个决定。</em></p></blockquote><p id="aedb" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">通过对线性代数的理解，你将能够对机器学习和深度学习算法产生更好的直觉，而不是将它们视为黑盒。这将允许您选择适当的超参数并开发更好的模型。</p><p id="0e40" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">你也可以从头开始编写算法，并对它们进行自己的修改。这不正是我们当初热爱数据科学的原因吗？试验和摆弄我们的模型的能力？将线性代数视为开启全新世界的钥匙。</p><h1 id="34a9" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">机器学习中的线性代数</h1><p id="727f" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">最大的问题是——线性代数适合机器学习吗？让我们来看四个大家都很熟悉的应用程序。</p><h1 id="5ae4" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">1.损失函数</h1><p id="1165" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">您必须非常熟悉模型(比如线性回归模型)如何拟合给定的数据:</p><ul class=""><li id="fb6a" class="jp jq hi jr b js kz ju la jw mm jy mn ka mo kc kd ke kf kg bi translated">你从一些任意的预测函数(线性回归模型的线性函数)开始</li><li id="7fdc" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">对数据的独立特征使用它来预测输出</li><li id="2d15" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">计算预测输出与实际输出的差距</li><li id="672e" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">使用这些计算值，通过梯度下降等策略来优化您的预测函数</li></ul><p id="f1c9" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated"><strong class="jr hj">但是等等——你怎么能计算出你的预测和预期的产出有多大的差异呢？当然是损失函数。</strong></p><p id="1cd5" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">损失函数是线性代数中<strong class="jr hj">向量范数</strong>的应用。向量的范数可以简单地是它的大小。向量范数有很多种。我将很快解释其中的两个:</p><ul class=""><li id="214e" class="jp jq hi jr b js kz ju la jw mm jy mn ka mo kc kd ke kf kg bi translated"><strong class="jr hj"> L1标准</strong>:也被称为曼哈顿距离或出租车标准。如果唯一允许的方向是平行于空间的轴，L1范数就是你从原点到向量的距离。</li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mp"><img src="../Images/ecdb2f48f9a0fafa996d213d4db92462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*BL8jsAXIjLoD91OL.jpg"/></div></div></figure><p id="9ee7" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">在这个2D空间中，您可以通过沿x轴移动3个单位，然后平行于y轴移动4个单位来到达向量(3，4)(如图所示)。或者，您可以先沿y轴移动4个单位，然后平行于x轴移动3个单位。无论哪种情况，您都将总共旅行7个单位。</p><ul class=""><li id="cf67" class="jp jq hi jr b js kz ju la jw mm jy mn ka mo kc kd ke kf kg bi translated"><strong class="jr hj"> L2范数</strong>:又称欧氏距离。L2范数是矢量到原点的最短距离，如下图中红色路径所示:</li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mp"><img src="../Images/705adc16d60f99717b039cda8a7ae492.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zbJQyroGOLrp2frP.jpg"/></div></div></figure><p id="699a" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">这个距离是用毕达哥拉斯定理计算出来的(我可以看到旧的数学概念在你的脑海中闪现！).它是(3 + 4)的平方根，等于5。</p><p id="3e99" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">但是如何使用范数来找出预测值和期望值之间的差异呢？假设预测值存储在一个向量中，期望值存储在一个向量中<strong class="jr hj">T3】ET5】。那么<strong class="jr hj"> <em class="lq"> P-E </em> </strong>就是差向量。<strong class="jr hj"> <em class="lq"> P-E </em> </strong>的范数为预测的总损失。</strong></p><h1 id="a8e5" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">2.正规化</h1><p id="6540" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">正则化是数据科学中一个非常重要的概念。这是我们用来防止模型过度拟合的技术。<em class="lq">正则化其实是范数的另一种应用。</em></p><p id="6b1f" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">当一个模型与训练数据拟合得太好时，就称之为过度拟合。这种模型对新数据表现不佳，因为它甚至已经学习了训练数据中的噪声。它将无法对以前没有见过的数据进行归纳。下图很好地总结了这个想法:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es mq"><img src="../Images/6e9d7ef552a3a3ad13f7ca1a59e21b98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/0*VZ5Jx0akxgzkwMSE.png"/></div></figure><p id="c2dd" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">正则化<strong class="jr hj">通过将权重向量的范数添加到成本函数来惩罚过于复杂的模型。由于我们想要最小化成本函数，我们需要最小化这个范数。这使得权重向量的不需要的分量减少到零，并防止预测函数过于复杂。</strong></p><p id="230b" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">您可以阅读下面的文章，了解正则化背后的完整数学原理:</p><ul class=""><li id="e48e" class="jp jq hi jr b js kz ju la jw mm jy mn ka mo kc kd ke kf kg bi translated"><a class="ae lr" href="https://www.analyticsvidhya.com/blog/2015/02/avoid-over-fitting-regularization/?utm_source=blog&amp;utm_medium=10-applications-linear-algebra-data-science" rel="noopener ugc nofollow" target="_blank">如何使用正则化避免过拟合？</a></li></ul><p id="1530" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">我们上面讨论的L1和L2规范用于两种类型的正则化:</p><ul class=""><li id="f31d" class="jp jq hi jr b js kz ju la jw mm jy mn ka mo kc kd ke kf kg bi translated">L1正则化与<strong class="jr hj">套索</strong>配合使用<strong class="jr hj">回归</strong></li><li id="040e" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">L2正则化与<strong class="jr hj">岭回归</strong>一起使用</li></ul><p id="c646" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">参考我们的<a class="ae lr" href="https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/?utm_source=blog&amp;utm_medium=10-applications-linear-algebra-data-science" rel="noopener ugc nofollow" target="_blank">Python中脊和套索回归的完整教程</a>来了解更多这些概念。</p><h1 id="a33c" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">3.协方差矩阵</h1><p id="03b3" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">双变量分析是数据探索的重要步骤。我们想研究变量对之间的关系。协方差或相关性是用于研究两个连续变量之间关系的度量。</p><p id="3a5d" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated"><strong class="jr hj">协方差表示变量之间线性关系的方向。正协方差表示一个变量的增加或减少伴随着另一个变量的增加或减少。负协方差表示一个协方差的增加或减少伴随着另一个协方差的增加或减少。</strong></p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es mr"><img src="../Images/d44d5bf9c347f526ecc31f58d2977421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*p3srqvhRS62cQ7wv.png"/></div></div></figure><p id="7fb4" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">另一方面，<strong class="jr hj">相关性是协方差</strong>的标准化值。相关值告诉我们线性关系的强度和方向，范围从-1到1。</p><p id="4e10" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">现在，你可能认为这是统计学的概念，而不是线性代数。还记得我告诉过你线性代数是无孔不入的吗？使用线性代数中的<strong class="jr hj">转置和矩阵乘法</strong>的概念，我们有一个非常简洁的协方差矩阵表达式:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es ms"><img src="../Images/7f62a2fbefe93e8279ad8aaec7d87492.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/0*w6h8gz-7O79vJkX4.jpg"/></div></figure><p id="c803" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">这里，X是包含所有数字特征的标准化数据矩阵。</p><p id="139d" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">我鼓励你阅读我们的<a class="ae lr" href="https://www.analyticsvidhya.com/blog/2016/01/guide-data-exploration/?utm_source=blog&amp;utm_medium=10-applications-linear-algebra-data-science" rel="noopener ugc nofollow" target="_blank">数据探索完全教程</a>，以了解更多关于协方差矩阵、双变量分析和探索性数据分析中涉及的其他步骤。</p><h1 id="80ea" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">4.支持向量机分类</h1><p id="2798" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">是的，支持向量机。最常见的分类算法之一，经常产生令人印象深刻的结果。是线性代数中<strong class="jr hj">向量空间</strong>概念的一个应用。</p><p id="3340" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">支持向量机，或SVM，是一种通过寻找决策面来工作的判别分类器。这是一种受监督的机器学习算法。</p><p id="e0e9" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">在该算法中，我们将每个数据项绘制为一个n维空间<strong class="jr hj">中的一个点</strong>(其中n是您拥有的特征的数量)，每个特征的值是特定坐标的值。然后，我们通过找到能够很好地区分两类的<strong class="jr hj">超平面</strong>来执行分类，即具有<strong class="jr hj">最大间隔</strong>，在这种情况下是C。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es mt"><img src="../Images/3658459a996958bbc536be0c7bfad4b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/0*SSnyrcPRDBaieqgq.png"/></div></figure><p id="b5c0" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">超平面是一个<strong class="jr hj">子空间</strong>，它的维数比它对应的向量空间小一，所以它对于2D向量空间是一条直线，对于3D向量空间是一个2D平面，等等。再次使用向量范数来计算差值。</p><p id="12eb" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">但是如果数据不是像下面这种线性可分的呢？</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es mu"><img src="../Images/5df48722d49a89e2d5019cb043893226.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/0*KzWb0I8wkFeElHdb.png"/></div></figure><p id="54f0" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">我们的直觉告诉我们，决策面必须是圆形或椭圆形，对吗？但是怎么找呢？这里，<strong class="jr hj">内核转换</strong>的概念开始发挥作用。从一个空间转换到另一个空间的思想在线性代数中很常见。</p><p id="164d" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">下面我们介绍一个变量<strong class="jr hj"> <em class="lq"> z = x + y </em> </strong>。如果我们沿着z轴和x轴绘图，数据看起来是这样的:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es mv"><img src="../Images/c18e54d32f4f3298eef927e4391bea0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/0*4R9m3SdQM7gkVSRA.png"/></div></figure><p id="2c42" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">现在，这显然可以通过一条线<strong class="jr hj"> <em class="lq"> z = a </em> </strong>进行线性分离，其中<em class="lq"> a </em>是某个正常数。在转换回原来的空间，我们得到的决策表面，这是一个圆！</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es mw"><img src="../Images/ff7f62b36a9fecb72707c571e2033af0.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/0*ifV_FjbvpqVrLinF.png"/></div></figure><p id="f794" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">最棒的是。我们不需要自己添加额外的功能。SVM有一种技术叫做<strong class="jr hj">内核技巧</strong>。阅读这篇关于支持向量机的<a class="ae lr" href="https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/?utm_source=blog&amp;utm_medium=10-applications-linear-algebra-data-science" rel="noopener ugc nofollow" target="_blank">文章，了解SVM、内核技巧以及如何用Python实现它。</a></p><h1 id="dd88" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">降维</h1><p id="271f" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">您将经常使用包含数百甚至数千个变量的数据集。这就是这个行业的运作方式。查看每个变量并决定哪个更重要是否实际？</p><p id="543d" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">那真的没有意义。我们需要减少变量的数量来进行任何一种连贯的分析。这就是降维。现在，我们在这里看两种常用的降维方法。</p><h1 id="d53e" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">5.主成分分析</h1><p id="a2c8" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">主成分分析(PCA)是一种无监督的降维技术。PCA找到最大方差的<strong class="jr hj">方向</strong>，并沿着它们投影数据以降低维数。</p><p id="d591" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">不用进入数学，这些方向就是数据的协方差矩阵的<a class="ae lr" href="https://www.youtube.com/watch?v=PFDu9oVAE-g" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hj">特征向量</strong> </a> <strong class="jr hj">。</strong></p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es mx"><img src="../Images/e6bd3c49580f593cc2984a7386f731b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/0*8Mz0sZBrEhpdAI9Z.jpg"/></div></figure><p id="c20b" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">方阵的特征向量是特殊的非零向量，其方向即使在对矩阵应用线性变换(这意味着相乘)后也不会改变。它们在下图中显示为红色向量:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es my"><img src="../Images/772921f5a53159594a2314b0b86bfbf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*y8rls0eGrub-3qlM.png"/></div></figure><p id="5405" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">您可以使用scikit-learn包中的PCA类轻松地在Python中实现PCA:</p><figure class="lf lg lh li fd lj"><div class="bz dy l di"><div class="mz na l"/></div></figure><p id="7ee0" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">我对来自sklearn的<a class="ae lr" href="https://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html" rel="noopener ugc nofollow" target="_blank">数字数据集</a>应用了PCA一组8×8的手写数字图像。我得到的情节令人印象深刻。数字看起来很好地聚集在一起:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es my"><img src="../Images/7fd2ca1a4ecda60b61130a9c8bae820b.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*N-WZjimWMM2MaAIL.png"/></div></figure><p id="4efa" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">前往我们的<a class="ae lr" href="https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/?utm_source=blog&amp;utm_medium=10-applications-linear-algebra-data-science" rel="noopener ugc nofollow" target="_blank">综合指南，使用Python代码实现12种降维技术</a>，深入了解PCA和11种其他降维技术。老实说，这是你在任何地方都能找到的关于这个话题的最好的文章之一。</p><h1 id="9f0e" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">6.奇异值分解</h1><p id="6431" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">在我看来，奇异值分解(SVD)被低估了，没有得到足够的讨论。这是一个惊人的技术<strong class="jr hj">矩阵分解</strong>具有不同的应用。我将在以后的文章中尝试介绍其中的一些。</p><p id="1fc0" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">现在，我们来谈谈降维中的奇异值分解。具体来说，这就是所谓的<strong class="jr hj">截断SVD </strong>。</p><ul class=""><li id="2306" class="jp jq hi jr b js kz ju la jw mm jy mn ka mo kc kd ke kf kg bi translated">我们从大的m×n数字数据矩阵A开始，其中m是行数，n是特征数</li><li id="6b3d" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">将它分解成3个矩阵，如下所示:</li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nb"><img src="../Images/f66217c529ddc6fbc0a50949394990d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*wATkNo-sNRt3g9aA5X_3xg.png"/></div><figcaption class="nc nd et er es ne nf bd b be z dx translated"><em class="ng">来源:hadrienj.github.io </em></figcaption></figure><ul class=""><li id="aed4" class="jp jq hi jr b js kz ju la jw mm jy mn ka mo kc kd ke kf kg bi translated">基于对角矩阵选择k个奇异值，并相应地截断(修剪)3个矩阵:</li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es nh"><img src="../Images/17ff169d3008253ef3b342afcaace1fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VZayMAFvPAItLOtZoh806w.png"/></div></div><figcaption class="nc nd et er es ne nf bd b be z dx translated">【researchgate.net】来源:T4</figcaption></figure><ul class=""><li id="8157" class="jp jq hi jr b js kz ju la jw mm jy mn ka mo kc kd ke kf kg bi translated">最后，将截断的矩阵相乘，得到变换后的矩阵<em class="lq"> A_k </em>。它的尺寸为m×k。因此，它有k个特征，k为n</li></ul><p id="c116" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">下面是用Python实现截断SVD的代码(与PCA非常相似):</p><figure class="lf lg lh li fd lj"><div class="bz dy l di"><div class="mz na l"/></div></figure><p id="5ec4" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">对数字数据应用截断奇异值分解，我得到了下图。您会注意到，它没有我们在PCA后获得的聚类好:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es my"><img src="../Images/4867dad0f99f489d5b46da7cdf7cfbfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*OrELdiQMAXz_oVwi.png"/></div></figure><h1 id="2043" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">自然语言处理</h1><p id="97cb" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated"><a class="ae lr" href="https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&amp;utm_medium=10-applications-linear-algebra-data-science" rel="noopener ugc nofollow" target="_blank">自然语言处理(NLP) </a>是目前数据科学中最热门的领域。这主要归功于过去18个月的重大突破。如果你还没有决定选择哪个分支，你应该考虑NLP。</p><p id="f2f3" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">让我们来看看线性代数在NLP中的一些有趣的应用。这应该有助于改变你的决定！</p><h1 id="567e" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">7.单词嵌入</h1><p id="7aff" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">机器学习算法无法处理原始文本数据。我们需要将文本转换成一些数字和统计特征来创建模型输入。从文本数据中获取工程特征的方法有很多，例如:</p><ol class=""><li id="fb70" class="jp jq hi jr b js kz ju la jw mm jy mn ka mo kc ni ke kf kg bi translated">文本的元属性，如字数，特殊字符数等。</li><li id="253a" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc ni ke kf kg bi translated">使用词性标签和语法关系(如专有名词的数量)的文本的NLP属性</li><li id="ff26" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc ni ke kf kg bi translated">词向量符号或词嵌入</li></ol><p id="4519" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">单词嵌入是一种将单词表示为数字的低维向量的方式，同时保留它们在文档中的上下文。这些表示是通过在大量文本上训练不同的神经网络获得的，这些文本被称为<strong class="jr hj">语料库</strong>。它们还有助于分析单词之间的句法相似性:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es nj"><img src="../Images/b0355715f44a7a8cafa82f8471b55140.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YgCW96SbuWxSml8x.png"/></div></div></figure><p id="899d" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated"><strong class="jr hj"> Word2Vec </strong>和<strong class="jr hj"> GloVe </strong>是创建单词嵌入的两种流行模型。</p><p id="b965" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">在使用Word2Vec进行了一些简单的预处理之后，我在<a class="ae lr" href="https://norvig.com/ngrams/shakespeare.txt" rel="noopener ugc nofollow" target="_blank">莎士比亚语料库</a>上训练了我的模型，并获得了单词“world”的单词嵌入:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nk"><img src="../Images/5ab57ebd3cfff7e3216dd92107fbedd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/0*fp999PYqnziwQgnT.jpg"/></div></figure><p id="c756" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">相当酷！但更棒的是我获得的词汇表的下图。注意到句法上相似的单词靠得更近。我已经突出了几个这样的单词组。结果并不完美，但仍然相当惊人:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es nl"><img src="../Images/3287fb989d7a9b3cab779a40525a5e11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wkGrjPBR8VWLCo18.png"/></div></div></figure><p id="c4af" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">有几种其他方法可以获得单词嵌入。阅读我们的文章，对单词嵌入有一个直观的理解:从计数向量到Word2Vec。</p><h1 id="424b" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">8.潜在语义分析(LSA)</h1><p id="4436" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">当你听到这组词——“王子、皇室、国王、贵族”，你的第一个想法是什么？这些非常不同的词几乎是同义词<strong class="jr hj"/>。</p><p id="c398" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">现在，考虑下面的句子:</p><ul class=""><li id="daa8" class="jp jq hi jr b js kz ju la jw mm jy mn ka mo kc kd ke kf kg bi translated">主队的<em class="lq">投手</em>似乎状态不佳</li><li id="d847" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">桌子上有一大罐果汁供你享用</li></ul><p id="0f84" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">单词pitcher根据两个句子中的其他单词有不同的意思。它在第一句话里意味着一个棒球运动员，在第二句话里意味着一罐果汁。</p><p id="d4a4" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">对于我们这些有着多年语言经验的人来说，这两组单词都很容易理解。但是机器呢？这里，主题建模的NLP概念开始发挥作用:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nm"><img src="../Images/f7f864ca709a044782fbfcf709deb1ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*yToAxym8EsEnGNlM.png"/></div></figure><p id="08a5" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated"><a class="ae lr" href="https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/?utm_source=blog&amp;utm_medium=10-applications-linear-algebra-data-science" rel="noopener ugc nofollow" target="_blank"> <strong class="jr hj">主题建模</strong> </a> <strong class="jr hj">是一种跨各种文本文档寻找主题的无监督技术。这些话题不过是一串相关的词。每个文档可以有多个主题。主题模型输出各种主题、它们在每个文档中的分布以及它包含的不同单词的频率。</strong></p><p id="2974" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">潜在语义分析(LSA)，或潜在语义索引，是主题建模技术之一。是<strong class="jr hj">奇异值分解</strong>的另一个应用。</p><p id="4755" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">潜伏的意思是“隐藏的”。顾名思义，LSA试图通过利用单词周围的上下文来捕捉文档中隐藏的主题或话题。</p><p id="a7d5" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">我将简要描述LSA中的步骤，所以请确保您查看了这个<a class="ae lr" href="https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/?utm_source=blog&amp;utm_medium=10-applications-linear-algebra-data-science" rel="noopener ugc nofollow" target="_blank">使用Python代码的潜在语义分析进行主题建模的简单介绍</a>以获得正确和深入的理解。</p><ul class=""><li id="7c96" class="jp jq hi jr b js kz ju la jw mm jy mn ka mo kc kd ke kf kg bi translated">首先，为您的数据生成<strong class="jr hj">文档术语</strong>矩阵</li><li id="7ec6" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc kd ke kf kg bi translated">使用SVD将矩阵分解为3个矩阵:</li></ul><ol class=""><li id="d03a" class="jp jq hi jr b js kz ju la jw mm jy mn ka mo kc ni ke kf kg bi translated"><strong class="jr hj">文档-主题</strong>矩阵</li><li id="fed4" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc ni ke kf kg bi translated"><strong class="jr hj">主题重要性</strong>对角线矩阵</li><li id="bef2" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc ni ke kf kg bi translated"><strong class="jr hj">主题-术语</strong>矩阵</li></ol><ul class=""><li id="8c5c" class="jp jq hi jr b js kz ju la jw mm jy mn ka mo kc kd ke kf kg bi translated">根据主题的重要性截断矩阵</li></ul><figure class="lf lg lh li fd lj"><div class="bz dy l di"><div class="mz na l"/></div></figure><p id="f3a6" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">为了获得自然语言处理的实践经验，您可以使用Python 查看我们关于<a class="ae lr" href="https://courses.analyticsvidhya.com/courses/natural-language-processing-nlp?utm_source=blog&amp;utm_medium=10-applications-linear-algebra-data-science" rel="noopener ugc nofollow" target="_blank"> NLP的课程。该课程是初学者友好的，你可以建立5个现实生活中的项目！</a></p><h1 id="f7d1" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">计算机视觉</h1><p id="b420" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">深度学习的另一个领域正在掀起波澜——<a class="ae lr" href="https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning-version2?utm_source=blog&amp;utm_medium=10-applications-linear-algebra-data-science" rel="noopener ugc nofollow" target="_blank">计算机视觉</a>。如果你想把你的技能扩展到表格数据之外(你应该这样做)，那就学习如何处理图像。</p><p id="296e" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">这将拓宽你目前对机器学习的理解，也有助于你快速破解面试。</p><h1 id="2ea8" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">9.作为张量的图像表示</h1><p id="8b8f" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">你如何解释计算机视觉中的“视觉”？显然，计算机不像人类那样处理图像。就像我之前提到的，机器学习算法需要数字特征来处理。</p><p id="c0f0" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">数字图像由称为像素的不可分割的小单元组成。考虑下图:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es my"><img src="../Images/219a58cf29fa3ae5d2fded57469ef30e.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*UZdjlDBhI6Y0lm2_.png"/></div></figure><p id="2c7c" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">这个数字0的<strong class="jr hj">灰度图像</strong>由8×8 = 64个像素组成。每个像素的值在0到255的范围内。值0表示黑色像素，255表示白色像素。</p><p id="026e" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">方便的是，<em class="lq">m×n</em>灰度图像可以表示为具有<em class="lq"> m </em>行和<em class="lq"> n </em>列的<strong class="jr hj"> 2D矩阵</strong>，其单元包含各自的像素值:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es my"><img src="../Images/998b507125e7b425f74f932094fc0141.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*oYG7rPneZFiBTEDm.png"/></div></figure><p id="7754" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">但是一个<strong class="jr hj">彩色图像</strong>呢？彩色图像通常存储在RGB系统中。每个图像可以被认为是由三个2D矩阵表示的，每个矩阵对应一个R、G和B通道。R通道中的像素值0表示红色的零强度，像素值255表示红色的全强度。</p><p id="d88f" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">每个像素值是三个通道中相应值的组合:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es my"><img src="../Images/b2e3a4814aa07f4eb901f90dd94156f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*pZEj9v9pLxANtyVW.png"/></div></figure><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nn"><img src="../Images/3aa81b572952d080d839024c2ac76306.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/0*ibLyY6J8gSNE9PT8.png"/></div></figure><p id="5be3" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">实际上，不是使用3个矩阵来表示图像，而是使用张量<strong class="jr hj">来表示图像。张量是一个<strong class="jr hj">广义的n维矩阵</strong>。对于RGB图像，使用三阶张量。想象一下，这是三个2D矩阵，一个在另一个后面:</strong></p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es no"><img src="../Images/091fcffd5f1a2e4bdce9d291cf246ed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*r05c4OUyw1AaFSdMCUWCIg.jpeg"/></div><figcaption class="nc nd et er es ne nf bd b be z dx translated">来源:slidesharecdn </figcaption></figure><h1 id="2b27" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">10.卷积和图像处理</h1><p id="ebb6" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated"><strong class="jr hj"> 2D卷积</strong>是图像处理中非常重要的运算。它由以下步骤组成:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es my"><img src="../Images/287adccaa70c6b010dda8d7380486441.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/0*Fy0zaPzaHcOkGx1h.gif"/></div></figure><ol class=""><li id="585f" class="jp jq hi jr b js kz ju la jw mm jy mn ka mo kc ni ke kf kg bi translated">从一个小的权重矩阵开始，称为内核或过滤器</li><li id="dc37" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc ni ke kf kg bi translated">在2D输入数据上滑动这个内核，执行逐元素乘法</li><li id="54ae" class="jp jq hi jr b js kh ju ki jw kj jy kk ka kl kc ni ke kf kg bi translated">将获得的值相加，并将总和放入单个输出像素中</li></ol><p id="ff2e" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">该函数看起来有点复杂，但它被广泛用于执行各种图像处理操作，如<strong class="jr hj">锐化和模糊图像以及边缘检测</strong>。我们只需要知道我们要完成的任务的正确内核。这里有一些您可以使用的内核:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es np"><img src="../Images/0f71cf74cd50c3743883fc8f307d5fa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/0*JC4I2syY1TAGuLe4.jpg"/></div></figure><figure class="lf lg lh li fd lj"><div class="bz dy l di"><div class="mz na l"/></div></figure><p id="ada5" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">你可以下载<a class="ae lr" href="https://drive.google.com/file/d/1aM4otWKSsDz1Rof3LZkY055YkYXeO-vf/view" rel="noopener ugc nofollow" target="_blank">我用的图像</a>，用上面的代码和内核亲自尝试这些图像处理操作。还有，试试这个<a class="ae lr" href="https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/?utm_source=blog&amp;utm_medium=10-applications-linear-algebra-data-science" rel="noopener ugc nofollow" target="_blank">关于图像分割技术的计算机视觉教程</a>！</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es nq"><img src="../Images/4e5a8fb80988fce9664fa6e8eca5fb06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*35H1dewfnvIoZoVi-wm0Mg.png"/></div></div></figure><p id="20fe" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">很神奇，对吧？这是迄今为止我最喜欢的线性代数在数据科学中的应用。</p><p id="a2bf" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">现在你已经熟悉了计算机视觉的基础知识，是时候用16个令人敬畏的OpenCV函数开始你的计算机视觉之旅了。我们还有一门关于使用深度学习的<a class="ae lr" href="https://courses.analyticsvidhya.com/courses/computer-vision-using-deep-learning-version2?utm_source=blog&amp;utm_medium=10-applications-linear-algebra-data-science" rel="noopener ugc nofollow" target="_blank">计算机视觉的综合课程</a>，在这门课程中，您可以进行现实生活中的计算机视觉案例研究！</p><h1 id="e8ec" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">结束注释</h1><p id="0d0e" class="pw-post-body-paragraph km kn hi jr b js jt ij ko ju jv im kp jw kq kr ks jy kt ku kv ka kw kx ky kc hb bi translated">我在这里的目的是让线性代数变得比你以前想象的更有趣。对我个人来说，学习一门学科的应用激励我去学习更多。</p><p id="ef5b" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated">我相信你和我一样对这些应用程序印象深刻。或者你知道一些我可以添加到列表中的其他应用程序？请在下面的评论区告诉我。</p></div><div class="ab cl nr ns gp nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="hb hc hd he hf"><p id="e273" class="pw-post-body-paragraph km kn hi jr b js kz ij ko ju la im kp jw lb kr ks jy lc ku kv ka ld kx ky kc hb bi translated"><em class="lq">原载于2019年7月23日</em><a class="ae lr" href="https://www.analyticsvidhya.com/blog/2019/07/10-applications-linear-algebra-data-science/" rel="noopener ugc nofollow" target="_blank"><em class="lq">https://www.analyticsvidhya.com</em></a><em class="lq">。</em></p></div></div>    
</body>
</html>