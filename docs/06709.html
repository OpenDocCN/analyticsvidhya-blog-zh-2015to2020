<html>
<head>
<title>Semantic Segmentation on Indian Driving Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">印度驾驶数据集上的语义分割</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/semantic-segmentation-on-indian-driving-dataset-aaf556a0f353?source=collection_archive---------9-----------------------#2020-05-31">https://medium.com/analytics-vidhya/semantic-segmentation-on-indian-driving-dataset-aaf556a0f353?source=collection_archive---------9-----------------------#2020-05-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="bb0d" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">UNet和PSPNet的实施</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/52fbbd0efc046f79b7825cbde85ae6d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n4B18g9icx9_nNsB_7NtIw.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">印度驾驶环境(图片来源- <a class="ae jn" href="https://www.edriving.com/three60/driver-training-programme-helps-make-indian-roads-safer/" rel="noopener ugc nofollow" target="_blank">页面</a></figcaption></figure><h2 id="68f0" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">目录</h2><ol class=""><li id="6342" class="km kn hi ko b kp kq kr ks jz kt kd ku kh kv kw kx ky kz la bi translated">什么是图像语义分割？</li><li id="1877" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw kx ky kz la bi translated">印度驾驶数据集介绍</li><li id="c342" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw kx ky kz la bi translated">数据集概述</li><li id="785d" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw kx ky kz la bi translated">绩效指标</li><li id="6fe4" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw kx ky kz la bi translated">数据可视化</li><li id="2074" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw kx ky kz la bi translated">数据准备</li><li id="1746" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw kx ky kz la bi translated">模型结构</li><li id="8c5a" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw kx ky kz la bi translated">结论</li><li id="e196" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw kx ky kz la bi translated">进一步的改进</li><li id="3598" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw kx ky kz la bi translated">参考</li></ol><h1 id="dada" class="lg jp hi bd jq lh li lj ju lk ll lm jy io ln ip kc ir lo is kg iu lp iv kk lq bi translated">什么是语义切分？</h1><p id="694a" class="pw-post-body-paragraph lr ls hi ko b kp kq ij lt kr ks im lu jz lv lw lx kd ly lz ma kh mb mc md kw hb bi translated"><strong class="ko hj">语义分割</strong>是一项计算机视觉任务，将数字图像分割成多个部分。在一个相机和其他设备越来越需要看到和解释周围世界的时代，图像分割已经成为教授设备如何理解周围世界的一项不可或缺的技术。</p><p id="2661" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">它不同于图像识别，图像识别给整个图像分配一个或多个标签；以及对象检测，它通过在对象周围绘制边界框来定位图像中的对象。<strong class="ko hj">语义分割</strong>提供关于图像内容的更精细的信息。</p><p id="33c9" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">我们可以把<strong class="ko hj">图像语义分割</strong>看作是像素级的图像分类。例如，在有许多汽车的图像中，分割会将所有对象标记为汽车对象。每张照片都由许多单独的像素组成，图像分割的目标是将这些像素中的每一个像素分配给它所属的对象。分割图像使我们能够将前景与背景分开，识别公共汽车或建筑物的精确位置，并清楚地标记出树木与天空的分界线。如需更清晰和详细的解释，请访问<a class="ae jn" href="https://www.fritz.ai/image-segmentation/" rel="noopener ugc nofollow" target="_blank">源页面</a>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mj"><img src="../Images/2ae0d5997efb3f70f1a2d1df4ca32777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3-HNafFzoe4qSfsWDFxzA.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">语义分割示例(图片来源— <a class="ae jn" href="https://theaisummer.com/Semantic_Segmentation/" rel="noopener ugc nofollow" target="_blank">页面</a></figcaption></figure><h1 id="2c50" class="lg jp hi bd jq lh li lj ju lk ll lm jy io ln ip kc ir lo is kg iu lp iv kk lq bi translated">印度驾驶数据集介绍</h1><p id="3e9c" class="pw-post-body-paragraph lr ls hi ko b kp kq ij lt kr ks im lu jz lv lw lx kd ly lz ma kh mb mc md kw hb bi translated">大多数自主导航数据集倾向于关注结构化驾驶环境。这通常对应于轮廓分明的基础设施，如车道、少量明确定义的交通参与者类别、物体或背景外观的低变化以及对交通规则的严格遵守。</p><p id="0118" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">来自包含<strong class="ko hj"> <em class="mk">结构化驾驶环境</em> </strong>的Cityscapes数据集的图像:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ml"><img src="../Images/384fd4090b66e0ee9d9caef205f9105f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SUudRpiXkNRI3vVz6m7rbA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来自Cityscapes数据集的示例图像</figcaption></figure><p id="804e" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">这就是典型的<strong class="ko hj"> <em class="mk">印度驾驶环境</em> </strong>的样子:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mm"><img src="../Images/1120e9856080d9700c1afe31a1022d75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NiZSUdaSpQnhf0CM2qQRAw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">一条印度“路”</figcaption></figure><p id="cdf6" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated"><strong class="ko hj">印度驾驶数据集(IDD) </strong>是在印度道路上捕获的自动驾驶注释街道级图像的集合，并且被格式化以方便用于训练AI系统和神经网络的目的。它由来自非结构化环境的图像组成，在非结构化环境中，上述假设基本上不满足。它反映了与现有数据集显著不同的道路场景的标注分布，大多数类显示出更大的类内多样性。与实际驾驶行为一致，它还识别新的类别，例如道路以外的可驾驶区域。</p><p id="7cc1" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">很难完全避免某些标签之间的歧义。例如，由于所收集数据中场景和车辆的多样性，无法精确定义停车场、大篷车或拖车等标签。为了解决这个问题，数据集被设计为4级标签层次，具有<strong class="ko hj"> <em class="mk">【第1级】</em> </strong>、16(第2级)、26(第3级)和30(第4级)标签。idd20k_lite数据集有<strong class="ko hj"> 7个类</strong>，包括<strong class="ko hj"> <em class="mk">可驾驶、不可驾驶、生物、车辆、路边物体、远处物体和天空</em> </strong>。分割挑战是标签层次结构的第一级<strong class="ko hj"><em class="mk"/></strong>的所有<strong class="ko hj"> <em class="mk"> 7类</em> </strong>的像素级预测。</p><p id="ffcd" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">这些图像是从安装在汽车上的前置摄像头获得的。这辆车在海德拉巴、班加罗尔城市及其郊区行驶。</p><p id="4126" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">(挑战来源— <a class="ae jn" href="http://idd.insaan.iiit.ac.in/" rel="noopener ugc nofollow" target="_blank">页面</a>)</p><h1 id="2425" class="lg jp hi bd jq lh li lj ju lk ll lm jy io ln ip kc ir lo is kg iu lp iv kk lq bi translated">数据集概述</h1><ul class=""><li id="1f15" class="km kn hi ko b kp kq kr ks jz kt kd ku kh kv kw mn ky kz la bi translated">有<strong class="ko hj">1403</strong>T24】列车图像、<strong class="ko hj">204</strong>T28】验证图像、<strong class="ko hj">404</strong>T32】测试图像</li><li id="e99f" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw mn ky kz la bi translated">输入图像和分割掩模的形状是<strong class="ko hj">【227，320，3】。</strong></li><li id="2294" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw mn ky kz la bi translated">期望的输出分段掩码的形状是<strong class="ko hj">【256，128】。</strong></li><li id="e5e4" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw mn ky kz la bi translated">对于每个训练和验证图像，我们有其对应的包含每个像素标签的注释图像。</li></ul><p id="5cd2" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">数据集可以从这个<a class="ae jn" href="http://idd.insaan.iiit.ac.in/dataset/download/" rel="noopener ugc nofollow" target="_blank">页面</a>下载。</p><h1 id="2685" class="lg jp hi bd jq lh li lj ju lk ll lm jy io ln ip kc ir lo is kg iu lp iv kk lq bi translated">绩效指标</h1><p id="c52b" class="pw-post-body-paragraph lr ls hi ko b kp kq ij lt kr ks im lu jz lv lw lx kd ly lz ma kh mb mc md kw hb bi translated">性能指标是<strong class="ko hj">联合平均交集(mIoU) </strong>。</p><p id="eb32" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">mIoU是语义图像分割的常用评估度量，它首先计算每个语义类别的IoU，然后计算类别的平均值。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mo"><img src="../Images/a1b4b9a65b04b63f10f0a30047763e83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RgdW0a1lA_MMfFmn-NG1ww.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">交集超过联合公式可视化(来源- <a class="ae jn" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" rel="noopener ugc nofollow" target="_blank">页面</a></figcaption></figure><p id="5b4e" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">检查这个等式，你可以看到交集除以并集仅仅是一个比率。在分子中，我们计算预测边界框和地面真实边界框之间的重叠区域。分母是并集的面积，或者更简单地说，是由预测边界框和实际边界框包围的面积。</p><p id="c1dd" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">将重叠面积除以并集面积，得到我们的最终分数— <strong class="ko hj">并集上的交集。</strong>(来源- <a class="ae jn" href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" rel="noopener ugc nofollow" target="_blank">页面</a>)</p><h1 id="26b8" class="lg jp hi bd jq lh li lj ju lk ll lm jy io ln ip kc ir lo is kg iu lp iv kk lq bi translated">数据可视化</h1><p id="db7d" class="pw-post-body-paragraph lr ls hi ko b kp kq ij lt kr ks im lu jz lv lw lx kd ly lz ma kh mb mc md kw hb bi translated">让我们取一个样本图像，并从数据集中分析它和它的带注释的图像。</p><p id="873a" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated"><em class="mk">这是图片:</em></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mp"><img src="../Images/b446496876aae0cf74f956646cdef65d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*308qYnuL48bIlwBY6bMK_g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来自idd20k _ lite数据集的示例图像</figcaption></figure><p id="6789" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">现在，让我们绘制一个直方图来找出像素强度值的频率。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mq"><img src="../Images/aad3862ed7b223d9683302d431e5155d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lnCNTu2HLceLZGQZ3w7Lhg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">直方图找到上面的样本图像的像素强度值的频率</figcaption></figure><p id="adc2" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">现在让我们绘制带注释的图像。</p><p id="ef2c" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">首先，<strong class="ko hj">什么是图像标注？</strong></p><p id="e089" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">它是一个标记数据(可用图像格式)的过程，通过计算机视觉技术使图像中的对象可被机器识别。基本上，它用于检测、分类和分组机器学习训练中的对象。在带注释的图像中，每个像素都被分配了类别标签。</p><p id="1945" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated"><em class="mk">下面是带注释的图片:</em></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mr"><img src="../Images/f41c0d021a0a2b02126e7bb0593b5008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p28D98tDXw_jjYGqqvYEbA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">样本图像的注释图像</figcaption></figure><p id="b991" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated"><strong class="ko hj"> <em class="mk">没有任何意义吧？</em>T13】</strong></p><p id="628f" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">这是因为对于数据集中要预测的7个类，每个像素都有从0到6的标签。所以基本上，所有像素的亮度值都在范围[0，6]内。通常，0强度值为黑色，255为白色。因此，该图像看起来大部分是黑色的。中间的小白点像素值为255。</p><p id="5f10" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">现在，让我们绘制带注释的图像的直方图，以获得像素亮度值的分布。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ms"><img src="../Images/d9a0b99f98b5b0bc3220f76508b8b531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EYjCvAd1Zt0SyDjKjxvV5A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">注释图像的直方图</figcaption></figure><p id="3be7" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">我们可以看到所有的像素亮度值都在0-6的范围内。</p><p id="78b8" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">为了更清楚地了解属于每个类标签的值，让我们统计属于每个类的值。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mt"><img src="../Images/8b5380f7e920aae43e3bb2b72d9fe655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iY-WtDx9lD11eaYYhRY5XQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">从0到6的值的放大直方图</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ms"><img src="../Images/11854dd7a5785c75b783a5eaf42dd32c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bMur7rlf8jf6FPJZLMXq7A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">每个类别的像素亮度值的计数</figcaption></figure><p id="85ae" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">我们可以观察到，类标签0具有最高的值计数，而类2具有最低的值计数。我们可以将值255视为类标签7，因为它更容易预测。</p><p id="c855" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">为了可视化带注释的图像并获得更清晰的图片，我们可以使用以下方法。</p><p id="54d0" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">因此，在给定的带注释的图像中，所有像素(不包括像素值255)都来自范围0–6(对于7个类)。我们可以增强这些像素以获得更清晰的注释，因为像素范围更大，颜色在更大范围内变化时更容易区分像素。我们将乘以每个像素，不包括0和255与40，所以我们会得到不同的颜色，差异较大的值。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mu"><img src="../Images/6f796154f420262f46dba2ad8cb385ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BdLxB01oDRWy0fT5tUWqRA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">已处理的注释图像</figcaption></figure><p id="6616" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">我们可以看到，对像素进行强化后，很容易区分不同的类。</p><h1 id="e12a" class="lg jp hi bd jq lh li lj ju lk ll lm jy io ln ip kc ir lo is kg iu lp iv kk lq bi translated">数据准备</h1><p id="5955" class="pw-post-body-paragraph lr ls hi ko b kp kq ij lt kr ks im lu jz lv lw lx kd ly lz ma kh mb mc md kw hb bi translated">首先，让我们定义图像大小、通道数量和要预测的类别数量。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="ea03" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">现在，我们将定义一个函数，该函数给定一个图像，将加载它，并且还加载其相应的注释图像并返回一个字典。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="2be8" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">我们需要得到训练和验证图像的所有文件名。为此，我们可以使用<em class="mk">tensor flow . data . dataset . list _ files。</em>这个方法返回所有匹配一个或多个glob模式的文件的数据集。基本上，它返回一个字符串数据集，这些字符串对应于调用方法时定义的模式所对应的文件名。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="9771" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">我们现在需要加载训练和验证图像。但是，在加载火车图像之前，我们将应用一个简单的转换。它可以通过在数据集中引入变化来帮助增加相关数据的数量。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="870c" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">有关数据增强技术的详细解释，请访问<a class="ae jn" href="https://nanonets.com/blog/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2/" rel="noopener ugc nofollow" target="_blank">页面</a></p><p id="8d10" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">在应用最终转换之前，让我们定义批处理大小、缓冲区大小和将要转换的最终数据集。最终的数据集将是一个字典，以“train”为键，以<em class="mk"> train_dataset </em>为值，以“val”为键，以<em class="mk"> val_dataset </em>为值。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="b3b3" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">我们将对数据集应用以下转换。</p><ol class=""><li id="bc6e" class="km kn hi ko b kp me kr mf jz mx kd my kh mz kw kx ky kz la bi translated"><strong class="ko hj">地图功能</strong></li></ol><p id="9071" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">跨数据集的所有元素映射<em class="mk"> map_func </em>。</p><p id="7034" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">该转换将<em class="mk"> map_func </em>应用于数据集的每个元素，并返回包含已转换元素的新数据集，其顺序与元素在输入中出现的顺序相同。<em class="mk"> map_func </em>可用于改变数据集元素的值和结构。</p><p id="60fb" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">在我们的例子中，我们将在训练数据集的每个元素上映射<em class="mk"> load_image_train </em>函数，在验证数据集的每个元素上映射<em class="mk"> load_image_test </em>函数。</p><p id="52b9" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">2.<strong class="ko hj">洗牌</strong></p><p id="93ab" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">随机打乱该数据集的元素。</p><p id="03b0" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">这个数据集用<em class="mk"> buffer_size </em>元素填充一个缓冲区，然后从这个缓冲区随机采样元素，用新元素替换所选元素。为了实现完美的混洗，缓冲区大小需要大于或等于数据集的完整大小。</p><p id="b4cd" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">我们将只在训练数据集上使用这种方法。</p><p id="c18d" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">3.<strong class="ko hj">重复</strong></p><p id="423f" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">重复该数据集，以便每个原始值被查看<em class="mk">计数</em>次。默认行为(如果计数为None或-1)是数据集无限重复。</p><p id="b8bd" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">4.<strong class="ko hj">批次</strong></p><p id="2954" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">将此数据集的连续元素组合成批。</p><p id="446f" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">5.<strong class="ko hj">预取</strong></p><p id="1649" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">创建一个数据集，从该数据集中预取元素。</p><p id="e799" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">大多数数据集输入管道应该以调用预取结束。这允许在处理当前元素的同时准备后面的元素。这通常会改善延迟和吞吐量，但代价是使用额外的内存来存储预取的元素。</p><p id="513a" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">我们可以使用下面的代码在数据集上应用上述所有转换。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="8fa6" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">现在，我们已经完成了数据准备工作。</p><p id="cfc7" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">现在，让我们从数据集中提取一个样本图像并将其可视化。</p><p id="cae0" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">可视化数据集中的样本图像</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es na"><img src="../Images/7bb92ea908857de068e2e7b5b0a26b94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J9tqhkArb3WMqZctNCFGQg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">显示示例函数的输出</figcaption></figure><p id="6684" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">现在我们已经完成了数据准备，让我们继续建模。</p><h1 id="7023" class="lg jp hi bd jq lh li lj ju lk ll lm jy io ln ip kc ir lo is kg iu lp iv kk lq bi translated">模型结构</h1><p id="3950" class="pw-post-body-paragraph lr ls hi ko b kp kq ij lt kr ks im lu jz lv lw lx kd ly lz ma kh mb mc md kw hb bi translated">大多数分割模型基本上由两部分组成。首先是编码器，其中我们对输入的空间分辨率进行下采样，开发分辨率较低的特征映射，据了解，这些映射在区分类别时非常高效。</p><p id="3e0e" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">然后，在解码器部分，我们将特征表示上采样为全分辨率分割图。</p><p id="e471" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">我在这个数据集上实现并训练了两个模型，UNet和PSPNet</p><h2 id="1528" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">u网模型</h2><p id="cdc6" class="pw-post-body-paragraph lr ls hi ko b kp kq ij lt kr ks im lu jz lv lw lx kd ly lz ma kh mb mc md kw hb bi translated">U-Net是一种卷积网络架构，用于快速精确地分割图像。到目前为止，在电子显微镜堆栈中神经元结构分割的ISBI挑战中，它已经超过了先前的最佳方法(滑动窗口卷积网络)。</p><p id="bc81" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">这种架构背后的主要思想是通过连续的层来补充通常的收缩网络，这些层提高了输出的分辨率。为了定位，来自收缩路径的高分辨率特征与上采样输出相结合。随后，后续卷积层可以学习基于该信息来组装更精确的输出。</p><p id="d45c" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">在上采样部分，有大量的特征通道，这允许网络将上下文信息传播到更高分辨率的层。因此，扩展路径或多或少与收缩路径对称，并产生u形结构。该网络不具有任何完全连接的层，并且仅使用每个卷积的有效部分，即分割图仅包含像素，对于这些像素，在输入图像中可以获得完整的上下文。</p><h2 id="5913" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">U-Net架构</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nb"><img src="../Images/ed76d135f7ccaf0044c7a8eee6ddee8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S8ZjT0aSU5D3o6vHVpvcwg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">U-Net架构</figcaption></figure><p id="1d33" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated"><strong class="ko hj"> U-net架构</strong>是对称的，由两个主要部分组成——<br/>第一部分称为收缩路径(编码器)，由通用卷积过程构成，第二部分是扩展路径(解码器)，由上采样技术构成。</p><h2 id="445d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">编码器</h2><p id="ad55" class="pw-post-body-paragraph lr ls hi ko b kp kq ij lt kr ks im lu jz lv lw lx kd ly lz ma kh mb mc md kw hb bi translated">收缩路径遵循卷积网络的典型架构。它由两个3×3卷积(无填充卷积)的重复应用组成，每个卷积后跟一个整流线性单元(ReLU)和一个跨距为2的2×2最大池操作，用于下采样。在每个下采样步骤中，我们将特征通道的数量增加一倍。</p><p id="f4a8" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated"><strong class="ko hj">编码器代码</strong></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><h2 id="7a1b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">解码器</h2><p id="c5e5" class="pw-post-body-paragraph lr ls hi ko b kp kq ij lt kr ks im lu jz lv lw lx kd ly lz ma kh mb mc md kw hb bi translated">扩展路径中的每一步都包括特征图的上采样，随后是将特征通道数量减半的2×2卷积(“上卷积”)，与收缩路径中相应裁剪的特征图的连接，以及两个3×3卷积，每个卷积之后是ReLU。由于每次卷积都会丢失边界像素，因此裁剪是必要的。</p><p id="9fb6" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated"><strong class="ko hj">解码器代码</strong></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="034b" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">现在，我们将把不同滤波器尺寸的编码器和解码器模块堆叠在一个模型中。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><h2 id="45ea" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">训练模型</h2><p id="5d64" class="pw-post-body-paragraph lr ls hi ko b kp kq ij lt kr ks im lu jz lv lw lx kd ly lz ma kh mb mc md kw hb bi translated">为了训练模型，首先，我们需要定义一个损失对象、一个优化器和一些性能指标来监控模型的性能</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="98af" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">现在，我们将使用TensorFlow的GradientTape函数创建自定义训练循环来训练模型。我们将讨论如何在急切执行中用TensorFlow计算梯度。</p><p id="8c9b" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">启用急切执行后，Tensorflow将计算代码中出现的张量值。这意味着它不会预先计算通过占位符输入的静态图形。这意味着反向传播错误，我们必须跟踪计算的梯度，然后将这些梯度应用于优化器。</p><p id="7d89" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">张量流提供了tf。用于自动微分的梯度带API(自动微分对于实现机器学习算法是有用的，例如用于训练神经网络的反向传播)也就是说，计算计算相对于其输入变量的梯度。TensorFlow“记录”了在tf上下文中执行的所有操作。将磁带渐变到“磁带”上。然后，它使用该磁带和与每个记录的操作相关联的梯度来计算使用反向模式微分的“记录的”计算的梯度。</p><p id="ea1e" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">梯度带使用内存来存储中间结果，包括输入和输出，供反向传递时使用。为了提高效率，一些运算(如ReLU)不需要保存它们的中间结果，它们在向前传递的过程中被删除。但是，如果您在磁带上使用persistent=True，则不会丢弃任何内容，并且您的峰值内存使用率会更高。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="5554" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">要了解更多关于Tensorflow 2.0中的GradeintTape，您可以访问此<a class="ae jn" href="https://www.pyimagesearch.com/2020/03/23/using-tensorflow-and-gradienttape-to-train-a-keras-model/" rel="noopener ugc nofollow" target="_blank">页面</a></p><p id="4f15" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated"><strong class="ko hj">训练代码</strong></p><p id="4004" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">下面的代码将为给定数量的纪元定型模型。首先，我们调用train_and_checkpoint函数，它将首先检查一个检查点。如果先前的检查点存在，它将从该检查点继续向前训练。然后，它将遍历整个数据集并保存指标。我们使用tf.summary.scalar来保存损失和准确性指标，这些指标可用于Tensorboard可视化。然后，我们为验证数据检查模型的性能。如果精度提高，我们将模型权重保存到磁盘。最后，我们打印特定时期的指标，并重置丢失和指标对象。</p><p id="744d" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated"><strong class="ko hj">代号</strong></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><h2 id="0ccd" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">印度驾驶数据集上的U-Net性能</h2><p id="2720" class="pw-post-body-paragraph lr ls hi ko b kp kq ij lt kr ks im lu jz lv lw lx kd ly lz ma kh mb mc md kw hb bi translated">我们可以使用Tensorboard跟踪和可视化损失和准确性等指标，以监控模型的性能。</p><p id="ef06" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">在对模型进行100个时期的训练后，图表如下所示:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nc"><img src="../Images/59fbba83151e20c61f0f198e4a1243cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*YL-oc0YY02sU0G4CeL3bMA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来自IDD的训练和验证数据的准确性和损失图</figcaption></figure><p id="17a3" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">现在，让我们在加载最佳权重后预测单个图像，并分析预测。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nd"><img src="../Images/4022bfe5f60fbdaae5116e2ad330b686.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GGiuOnC2EPpMVr3gF5Tagg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">单个图像的预测</figcaption></figure><p id="d980" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">我们还可以获得预测图像中每个类别的真阳性、假阳性、假阴性和IoU值。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ne"><img src="../Images/15e316e57c6f2ee492d4451250060228.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fVcKeq51TeJM7huskSDsXA.png"/></div></div></figure><p id="7dc3" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">现在，我们可以计算所有验证图像的mIoU，然后取其平均值。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="f56f" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated"><strong class="ko hj">U-Net模型的输出:</strong></p><pre class="iy iz ja jb fd nf ng nh ni aw nj bi"><span id="92e4" class="jo jp hi ng b fi nk nl l nm nn">Validation mIoU =  0.44561768240488786</span></pre><h2 id="617f" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">PSPNet模型</h2><p id="3fe1" class="pw-post-body-paragraph lr ls hi ko b kp kq ij lt kr ks im lu jz lv lw lx kd ly lz ma kh mb mc md kw hb bi translated">PSPNet架构考虑了图像的全局上下文来预测局部级别的预测，因此在PASCAL VOC 2012和cityscapes等基准数据集上提供了更好的性能。该模型是必要的，因为基于FCN的像素分类器不能捕捉整个图像的上下文。</p><p id="11a1" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">在这个模型中，他们引入了金字塔池模块，经验证明这是一个有效的全局上下文先验。全局平均池作为全局上下文先验是一个很好的基线模型，常用于图像分类任务。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es no"><img src="../Images/30bc1a26a1b70f16a40fb1b5a7a75f25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pG5ufjlUc6ddO6YuPn-YpA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">PSPNet架构</figcaption></figure><p id="35ea" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">金字塔池模块融合了四种不同金字塔等级下的要素。以红色突出显示的最粗略级别是全局池，用于生成单个容器输出。下面的金字塔等级将特征地图分成不同的子区域，并形成不同位置的汇集表示。金字塔池模块中不同级别的输出包含不同大小的要素地图。为了保持全局特征的权重，如果金字塔的级别大小为N，则在每个金字塔级别后使用1×1卷积层将上下文表示的维度降低到原始维度的1/N。然后通过双线性插值直接对低维特征图进行上采样，以获得与原始特征图相同大小的特征。最后，不同级别的要素被连接为最终的金字塔池全局要素。</p><p id="af31" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">可以修改金字塔等级的数量和每个等级的大小。它们与输入金字塔池图层的要素地图的大小有关。该结构通过在几个步骤中采用不同大小的池内核来抽象不同的子区域。因此，多级内核应该在表示中保持合理的间隙。</p><p id="122b" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">现在，我们可以使用我们在U-Net中使用的相同的编码器-解码器方法来实现PSPNet。</p><p id="52ab" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">编码器包含卷积块，这些卷积块将在PSPNet架构中生成特征映射，直到步骤(b)。</p><p id="b329" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated"><strong class="ko hj">编码器代码</strong></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="be76" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">对于解码器块，我们需要两个辅助类，PyramidFeatureMap和PyramidPoolingModule，这是一个有效的全局上下文先验。<br/>对PyramidPoolingModule中的每个特征地图进行子区域平均池化。</p><p id="9b7a" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated"><strong class="ko hj">解码代码</strong></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="5675" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">现在，我们可以将所有块堆叠在一起，建立分段模型。</p><p id="8368" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated"><strong class="ko hj">分割模型代码</strong></p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mv mw l"/></div></figure><p id="c426" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">培训阶段将与U-Net模型相同。</p><p id="4991" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated"><strong class="ko hj"> PSPNet在印度驾驶数据集上的性能</strong></p><p id="27ce" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">经过50个时期的训练后，图表如下所示:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es np"><img src="../Images/4fcc9a422d4013c721e413cbef7e3f0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*_y0R68_04Yjh1dqBTki2TQ.png"/></div></figure><p id="ce90" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">我们可以使用PSPNet模型预测相同的样本图像，并分析其性能。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nq"><img src="../Images/9b3af67bf73d8c98d81d2bd2fcfc9450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*emaofhPtB35f9lAFg3LoiA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">样本图像预测</figcaption></figure><p id="6c63" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">我们还可以获得预测图像中每个类别的真阳性、假阳性、假阴性和IoU值。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nr"><img src="../Images/4b457c13b0835efefe6b7d23c2c74b90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4NwEvr6Mp0aWITSF8m5BwA.png"/></div></div></figure><p id="4af6" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">PSPNet预测的所有验证图像的平均mIoU为:</p><pre class="iy iz ja jb fd nf ng nh ni aw nj bi"><span id="f943" class="jo jp hi ng b fi nk nl l nm nn">Validation mIoU =  0.4333882146774802</span></pre><p id="8f9c" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">因此，我们可以观察到PSPNet在这个数据集上的性能不如U-Net模型。</p><h1 id="037a" class="lg jp hi bd jq lh li lj ju lk ll lm jy io ln ip kc ir lo is kg iu lp iv kk lq bi translated">结论</h1><ul class=""><li id="91e9" class="km kn hi ko b kp kq kr ks jz kt kd ku kh kv kw mn ky kz la bi translated">实施了两种不同的模型(UNet和PSPNet)。</li><li id="1f3f" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw mn ky kz la bi translated">这两个模型在过去对于语义分割挑战都表现得很好。</li><li id="c151" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw mn ky kz la bi translated">对于该数据集，UNet给出的验证mIoU为0.44561，而PSPNet给出的验证mIoU为0.43338。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ns"><img src="../Images/933f52863a4835c42f068078b20c9109.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*eP5KY8_cHGRZOA_WdvrudA.png"/></div></figure><h1 id="a975" class="lg jp hi bd jq lh li lj ju lk ll lm jy io ln ip kc ir lo is kg iu lp iv kk lq bi translated">进一步的改进</h1><ul class=""><li id="f1aa" class="km kn hi ko b kp kq kr ks jz kt kd ku kh kv kw mn ky kz la bi translated">可以在这个数据集上尝试U-Net的变体。</li><li id="b46e" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw mn ky kz la bi translated">我们可以在两个模型中增加或减少卷积块的数量，以查看性能是提高还是降低。</li><li id="2421" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw mn ky kz la bi translated">也可以尝试专门为分割模型定义的不同损失函数。</li></ul><p id="757b" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">你可以在我的<a class="ae jn" href="https://github.com/yashmarathe21/Semantic-Segmentation-on-Indian-Driving-Dataset" rel="noopener ugc nofollow" target="_blank"> Github知识库</a>中找到我的完整解决方案，如果你有任何建议，请评论或通过<a class="ae jn" href="https://www.linkedin.com/in/yash-marathe-a3658917b/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我</p><h1 id="acb4" class="lg jp hi bd jq lh li lj ju lk ll lm jy io ln ip kc ir lo is kg iu lp iv kk lq bi translated">参考</h1><ol class=""><li id="5af7" class="km kn hi ko b kp kq kr ks jz kt kd ku kh kv kw kx ky kz la bi translated">【https://arxiv.org/pdf/1505.04597.pdf T4】</li><li id="d205" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw kx ky kz la bi translated"><a class="ae jn" href="https://arxiv.org/pdf/1612.01105.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1612.01105.pdf</a></li><li id="9687" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw kx ky kz la bi translated"><a class="ae jn" href="https://yann-leguilly.gitlab.io/post/2019-12-14-tensorflow-tfdata-segmentation/" rel="noopener ugc nofollow" target="_blank">https://yann-leguilly . git lab . io/post/2019-12-14-tensor flow-TF data-segmentation/</a></li><li id="044d" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw kx ky kz la bi translated"><a class="ae jn" rel="noopener" href="/analytics-vidhya/semantic-segmentation-in-pspnet-with-implementation-in-keras-4843d05fc025">https://medium . com/analytics-vid hya/semantic-segmentation-in-PSP net-with-implementation-in-keras-4843d 05 fc 025</a></li><li id="c5cc" class="km kn hi ko b kp lb kr lc jz ld kd le kh lf kw kx ky kz la bi translated"><a class="ae jn" href="https://github.com/junhoning/machine_learning_tutorial/" rel="noopener ugc nofollow" target="_blank">https://github.com/junhoning/machine_learning_tutorial/</a></li></ol><p id="f3b3" class="pw-post-body-paragraph lr ls hi ko b kp me ij lt kr mf im lu jz mg lw lx kd mh lz ma kh mi mc md kw hb bi translated">感谢您的阅读！！</p></div></div>    
</body>
</html>