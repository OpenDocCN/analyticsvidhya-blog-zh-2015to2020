<html>
<head>
<title>Going beyond traditional Sentiment Analysis Techniques</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超越传统的情感分析技术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/going-beyond-traditional-sentiment-analysis-technique-b9c91b313c07?source=collection_archive---------0-----------------------#2019-07-08">https://medium.com/analytics-vidhya/going-beyond-traditional-sentiment-analysis-technique-b9c91b313c07?source=collection_archive---------0-----------------------#2019-07-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="a6d5" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">传统的方法如TF-IDF，CountVectorizer等。失败？</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/27f4ce984fcd694264ca92923bc70e0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sNLB8-lr7V4V5qQ2jr-2vg.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">丹尼尔·里卡洛斯在<a class="ae jn" href="https://unsplash.com/search/photos/coding?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="d4c3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">情感分析仍然是一个非常具有挑战性的任务，因为像讽刺性的反馈，主观反馈，客观反馈和更多的挑战。</p><p id="34b7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">首先，让我们看看TF-IDF、计数矢量器或BOW等方法的失败之处。</p><p id="fc59" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了训练任何TF-IDF模型，我们首先移除所有冗余单词。然后使用TF-IDF或计数矢量器将单词编码成数字。这些方法不太关心单词的顺序。那么，他们失败在哪里呢？</p><ol class=""><li id="cf20" class="kk kl hi jq b jr js ju jv jx km kb kn kf ko kj kp kq kr ks bi translated"><strong class="jq hj">操控对比</strong></li></ol><p id="43bb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">当我们谈论比较时，这些方法就失效了。</p><p id="849a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">示例— <em class="kt">产品XYZ优于ABC </em>。</p><p id="0c8d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">2.<strong class="jq hj">处理否定</strong></p><p id="faa5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">像<em class="kt">不</em>、<em class="kt">不</em>和<em class="kt">从不</em>这样的词很难处理</p><p id="05d7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">产品还不错。</p><p id="d171" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了克服这些问题，我们将使用Word2Vec在Keras中嵌入LSTM。我将展示的代码与Keras兼容得最好，因为它有一个预定义的函数，我保证这将是您可能见过的最简单的方法。</p><p id="be79" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">这篇文章的独特之处在于，我们将通过一个简单的函数</strong>使用Word2Vec在Keras中的嵌入。</p><p id="230b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">简而言之，这些是我们将要讨论的内容:</p><ol class=""><li id="93b2" class="kk kl hi jq b jr js ju jv jx km kb kn kf ko kj kp kq kr ks bi translated">嵌入以及为什么它们更好</li><li id="34bd" class="kk kl hi jq b jr ku ju kv jx kw kb kx kf ky kj kp kq kr ks bi translated">了解LSTM是如何运作的</li><li id="7cd5" class="kk kl hi jq b jr ku ju kv jx kw kb kx kf ky kj kp kq kr ks bi translated">了解Keras中的嵌入层</li><li id="7229" class="kk kl hi jq b jr ku ju kv jx kw kb kx kf ky kj kp kq kr ks bi translated">编写情感分析程序。</li></ol></div><div class="ab cl kz la gp lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="hb hc hd he hf"><h2 id="d53c" class="lg lh hi bd li lj lk ll lm ln lo lp lq jx lr ls lt kb lu lv lw kf lx ly lz ma bi translated">了解Word2Vec模型</h2><p id="64e2" class="pw-post-body-paragraph jo jp hi jq b jr mb ij jt ju mc im jw jx md jz ka kb me kd ke kf mf kh ki kj hb bi translated">让我们从理解Word2Vec模型开始。</p><p id="b94f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">将任何文本转换为数字形式的需求是因为计算机以及我们的机器学习模型只能理解数字。</p><p id="7f32" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">像术语频率-逆文档频率(TF-IDF)、计数矢量器或一键编码这样的方法创建了一个不是很有效的稀疏矩阵。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mg"><img src="../Images/68177f37c3b3269c0b96d90509969489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*_ZUyCQjSEQR0XlnB"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源- <a class="ae jn" href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiukPvM45jjAhXIV30KHaG6CMQQjB16BAgBEAQ&amp;url=https%3A%2F%2Fdziganto.github.io%2FSparse-Matrices-For-Efficient-Machine-Learning%2F&amp;psig=AOvVaw39qY29_Lc7VXw1z8mMX2xa&amp;ust=1562244277181520" rel="noopener ugc nofollow" target="_blank"> ganto.github.io </a></figcaption></figure><p id="dd12" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了克服这个问题并创建单词之间的关系，使用了单词嵌入。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="ab fe cl mh"><img src="../Images/48a8e134b3b7243dfae0d4cc843b575f.png" data-original-src="https://miro.medium.com/v2/0*mRGKYujQkI7PcMDE."/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源-https://www . medium . com/jayeshbahire</figcaption></figure><p id="1d6a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">上面你可以看到我们有一个4维向量来表示单词。嵌入的最大优点是它在单词之间建立了一种联系</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mi"><img src="../Images/6aa42bbd527d60d375e8912eb0838fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*6WY1rIc1Hg93fT5mCX-TNw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://www.google.co.in/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwj43NPfzJ3jAhXL6Y8KHYzKBqQQjB16BAgBEAQ&amp;url=https%3A%2F%2Fmc.ai%2Fhow-to-get-same-word2vec-doc2vec-paragraph-vectors-in-every-time-of-training%2F&amp;psig=AOvVaw1JRjAtpCEwtuKExSymFuK-&amp;ust=1562409939423397" rel="noopener ugc nofollow" target="_blank"> https://mc.ai </a></figcaption></figure><p id="449f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">当你在训练你的网络时，这种嵌入非常有效。我现在不想谈太多细节。当我们学习在Keras中使用嵌入时，我们会讨论更多。</p></div><div class="ab cl kz la gp lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="hb hc hd he hf"><h2 id="3abb" class="lg lh hi bd li lj lk ll lm ln lo lp lq jx lr ls lt kb lu lv lw kf lx ly lz ma bi translated">了解LSTM是如何运作的</h2><p id="9da7" class="pw-post-body-paragraph jo jp hi jq b jr mb ij jt ju mc im jw jx md jz ka kb me kd ke kf mf kh ki kj hb bi translated">LSTM代表长期短期记忆。它是RNN(递归神经网络)的升级版本，没有消失梯度的问题(当有许多隐藏层时会发生)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mj"><img src="../Images/98f37944fcc77c89509e977909df6d06.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*OxV6DHaIPt1tJqaUbSO-Pg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">LSTM细胞</figcaption></figure><p id="fd30" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">LSTM有4个门:</p><ol class=""><li id="dfe9" class="kk kl hi jq b jr js ju jv jx km kb kn kf ko kj kp kq kr ks bi translated"><strong class="jq hj">遗忘之门</strong>:使用先前的LTM来遗忘不必要的东西。</li><li id="d006" class="kk kl hi jq b jr ku ju kv jx kw kb kx kf ky kj kp kq kr ks bi translated"><strong class="jq hj">学习门</strong>:使用当前i/p(x)使用以前的STM进行学习。</li><li id="d24c" class="kk kl hi jq b jr ku ju kv jx kw kb kx kf ky kj kp kq kr ks bi translated"><strong class="jq hj">记住门</strong>:使用忘记门和学习门来计算新LTM需要记住什么。</li><li id="ddcf" class="kk kl hi jq b jr ku ju kv jx kw kb kx kf ky kj kp kq kr ks bi translated"><strong class="jq hj">使用门</strong>:使用LTM、STM和i/p(x)创建将被进一步使用的新STM。</li></ol><p id="ed61" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在的问题是，为什么有必要搬到LSTM进行情绪分析？</p><p id="5ce7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">长短期网络在模型中有记忆。在网络中有一个存储器是很有用的，因为当处理像文本这样的有序数据时，一个单词的意思取决于先前文本的上下文。这些依赖性对文档的含义和整体极性有很大的影响。你可以在这里阅读更多关于它的<a class="ae jn" href="https://beta.vu.nl/nl/Images/werkstuk-miedema_tcm235-895557.pdf" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl kz la gp lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="hb hc hd he hf"><p id="0b05" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">了解Keras中的嵌入层</strong></p><p id="828f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">嵌入是可以被训练的固定长度的向量，或者也可以使用预训练的嵌入。</p><p id="d444" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们写一些代码来更好地理解</p><p id="1cc8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我已经创建了一个<a class="ae jn" href="https://colab.research.google.com/drive/19Ghc_Q21-k3n4ma8UuoRGXzGFeRN4qKB" rel="noopener ugc nofollow" target="_blank"> Google Colab文件</a>。您可以在本地计算机上执行所有步骤，也可以创建Colab文件的副本。我们要用的预训练嵌入比较大，所以我会推荐用Colab。</p><ol class=""><li id="3734" class="kk kl hi jq b jr js ju jv jx km kb kn kf ko kj kp kq kr ks bi translated">从nlp.stanform.edu下载手套模型</li></ol><pre class="iy iz ja jb fd mk ml mm mn aw mo bi"><span id="1ad8" class="lg lh hi ml b fi mp mq l mr ms">!wget <a class="ae jn" href="http://nlp.stanford.edu/data/glove.42B.300d.zip" rel="noopener ugc nofollow" target="_blank">http://nlp.stanford.edu/data/glove.42B.300d.zip</a><br/>!apt install unzip<br/>!unzip "glove.420B.300d.zip"</span></pre><p id="d9ad" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这个预训练的嵌入具有300d向量表示。</p><p id="79a3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">2.接下来的任务是将手套嵌入转换为Word2vec嵌入。转换并加载文件需要一段时间。</p><p id="4d8e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">将手套嵌入转换为Word2Vec嵌入</p><p id="90d8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">3.现在，让我们看看它能做什么</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mt"><img src="../Images/30edf5d5e3588eb2d24563498a38b7ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*Cway-7lDowZ6_TlSEbzxDA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">和狗最相似的词</figcaption></figure><p id="d16a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">Word2Vec基于向量位置定位与dog最相似的单词。</p><p id="8604" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，让我们了解它在情感分析中是如何有用的。</p><p id="4c9a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果单词“dog”和“dogs”在训练时出现，模型将知道它们有85%的相似性。</p><p id="e62e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">那么，为什么我们不使用一个自动将<em class="kt">狗</em>转换成<em class="kt">狗</em>的分类器呢？嗯，<em class="kt">送</em>和<em class="kt">送</em>这两个差不多的就不行了。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mu"><img src="../Images/622ae9e3c9d5728edac6067a7d170416.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*FozGrTTRmgIdFa7Y8HSzSA.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mv"><img src="../Images/3395773627716328dcd208cdf8d55988.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*JJnUmsTSlOkc_m61fEEU3w.png"/></div></figure><p id="4667" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，您已经看到了嵌入可以做什么。我们可以在情感分析中利用它的力量。</p><p id="d26d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在Keras中使用嵌入有两种方式。</p><ol class=""><li id="b982" class="kk kl hi jq b jr js ju jv jx km kb kn kf ko kj kp kq kr ks bi translated">创建一个新的数组，并从glove_model中复制嵌入值。这可能不是一个好主意，因为它消耗了太多的内存。</li><li id="5cff" class="kk kl hi jq b jr ku ju kv jx kw kb kx kf ky kj kp kq kr ks bi translated">使用word2vec的get_keras_embedding函数。这个函数非常容易使用，可以直接在嵌入层内部使用。</li></ol></div><div class="ab cl kz la gp lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="hb hc hd he hf"><h1 id="9ff7" class="mw lh hi bd li mx my mz lm na nb nc lq io nd ip lt ir ne is lw iu nf iv lz ng bi translated">情感分析代码</h1><p id="98ca" class="pw-post-body-paragraph jo jp hi jq b jr mb ij jt ju mc im jw jx md jz ka kb me kd ke kf mf kh ki kj hb bi translated"><strong class="jq hj">让我们开始编码</strong> —</p><ol class=""><li id="3f7d" class="kk kl hi jq b jr js ju jv jx km kb kn kf ko kj kp kq kr ks bi translated">打开这个<a class="ae jn" href="https://colab.research.google.com/drive/19Ghc_Q21-k3n4ma8UuoRGXzGFeRN4qKB" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>笔记本或者在你的本地系统上使用Jupyter笔记本。</li><li id="4df8" class="kk kl hi jq b jr ku ju kv jx kw kb kx kf ky kj kp kq kr ks bi translated">下载预先训练好的手套模型。</li></ol><pre class="iy iz ja jb fd mk ml mm mn aw mo bi"><span id="6c2c" class="lg lh hi ml b fi mp mq l mr ms">!wget <a class="ae jn" href="http://nlp.stanford.edu/data/glove.42B.300d.zip" rel="noopener ugc nofollow" target="_blank">http://nlp.stanford.edu/data/glove.42B.300d.zip</a><br/>!apt install unzip<br/>!unzip "glove.42B.300d.zip"</span></pre><p id="da63" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">3.将手套嵌入转换为Word2Vec嵌入</p><p id="69d2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">4.现在，让我们准备好数据集。</p><pre class="iy iz ja jb fd mk ml mm mn aw mo bi"><span id="fe6d" class="lg lh hi ml b fi mp mq l mr ms">!pip install nlppreprocess</span></pre><blockquote class="nh ni nj"><p id="1c3c" class="jo jp kt jq b jr js ij jt ju jv im jw nk jy jz ka nl kc kd ke nm kg kh ki kj hb bi translated">以上套餐可选。我们将使用它来清理数据。</p></blockquote><p id="7673" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">4.a)导入所有包</p><p id="f558" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">4.b)下载数据集</p><pre class="iy iz ja jb fd mk ml mm mn aw mo bi"><span id="5402" class="lg lh hi ml b fi mp mq l mr ms">!wget !wget <a class="ae jn" href="https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/00462/drugsCom_raw.zip</a></span><span id="2e23" class="lg lh hi ml b fi nn mq l mr ms">!unzip 'drugsCom_raw.zip'</span></pre><p id="d763" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">4.c)导入训练集</p><p id="b378" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">4.d)我这里用的是二元分类法。所有高于5的评分都转换为1，低于5的则转换为0。</p><blockquote class="nh ni nj"><p id="7069" class="jo jp kt jq b jr js ij jt ju jv im jw nk jy jz ka nl kc kd ke nm kg kh ki kj hb bi translated">以上步骤是完全可选的</p></blockquote><p id="20ac" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">4.e)将我们的标签转换成稀疏矩阵形式</p><p id="9379" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">5.查找出现频率最高的单词，在训练集和填充序列中替换它。</p><p id="5a99" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">5.a) max_features-可以出现的唯一单词的数量。越多越好。</p><p id="9c22" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">5.b) max_len =句子的最大长度。它将在填充过程中用来填充较长的短句。</p><blockquote class="nh ni nj"><p id="6304" class="jo jp kt jq b jr js ij jt ju jv im jw nk jy jz ka nl kc kd ke nm kg kh ki kj hb bi translated">如果你仔细观察，我们正在做一些预处理。这种预处理删除所有的停用词，数字，HTML标签和标点符号。这一点很重要，因为您正在为数据集选择重要的要素，并且您不希望它是一些无用的随机单词。</p></blockquote><p id="ee2f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们为什么要做句子的填充？</p><blockquote class="nh ni nj"><p id="b6fa" class="jo jp kt jq b jr js ij jt ju jv im jw nk jy jz ka nl kc kd ke nm kg kh ki kj hb bi translated">不是所有的句子都是等长的，这可能是个问题。为了避免这种情况，所有句子的开头或结尾都用零填充(取决于实现)。</p></blockquote><p id="4857" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">6.建筑模型</p><blockquote class="nh ni nj"><p id="8924" class="jo jp kt jq b jr js ij jt ju jv im jw nk jy jz ka nl kc kd ke nm kg kh ki kj hb bi translated">我们使用glove _ model . get _ keras _ embedding()在keras中创建嵌入层。</p></blockquote><p id="ddaf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">7.让我们编译模型。</p><p id="a398" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，等待模型训练</p><p id="6597" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">训练后，您可以使用其预测功能来生成预测</p><p id="6632" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果你已经做到了这一步，干得好！！</p></div><div class="ab cl kz la gp lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="hb hc hd he hf"><blockquote class="no"><p id="b121" class="np nq hi bd nr ns nt nu nv nw nx kj dx translated">一切都有瑕疵</p></blockquote><p id="b477" class="pw-post-body-paragraph jo jp hi jq b jr ny ij jt ju nz im jw jx oa jz ka kb ob kd ke kf oc kh ki kj hb bi translated">到目前为止，我们只讨论了这个模型的优点，以及为什么它优于传统的情感分析技术。让我们看一个失败的例子</p><ol class=""><li id="00fd" class="kk kl hi jq b jr js ju jv jx km kb kn kf ko kj kp kq kr ks bi translated">word2vec的嵌入是静态的。我们将通过一个例子来理解为什么动态嵌入很重要。</li></ol><p id="9c4e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">例1 — <em class="kt">劫匪抢劫了</em> <strong class="jq hj"> <em class="kt">银行</em> </strong> <em class="kt">。</em></p><p id="ef93" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">例2 — <em class="kt">看到渔民在河流的</em> <strong class="jq hj"> <em class="kt">岸边</em> </strong> <em class="kt">抓鱼。</em></p><p id="b19a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">银行这个词在两个句子中有不同的意思。静态嵌入有固定的嵌入，不会因周围的词而改变词义。这就是为什么我们需要动态嵌入，它可以根据周围的文字而变化。你可以在这里阅读更多关于动态嵌入<a class="ae jn" href="https://arxiv.org/abs/1702.08359" rel="noopener ugc nofollow" target="_blank">的内容。</a></p><p id="5c15" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">结论</strong></p><p id="bcd0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对于情感分析，如果你有足够的数据，单词嵌入比TF-IDF和CountVectorizer要好得多。在单词嵌入的帮助下训练神经网络可以产生最先进的准确度。如果你在现有的系统或模型中寻找更高的精确度，那么转向神经网络可能是个好主意。</p><p id="61c5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">谢谢大家！</p><p id="50ad" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">参考文献</strong></p><ol class=""><li id="1260" class="kk kl hi jq b jr js ju jv jx km kb kn kf ko kj kp kq kr ks bi translated"><a class="ae jn" href="http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.XSL483Uzbix" rel="noopener ugc nofollow" target="_blank">http://ka vita-ganesan . com/gensim-word 2 vec-tutorial-starter-code/# . xsl 483 uz bix</a></li><li id="f714" class="kk kl hi jq b jr ku ju kv jx kw kb kx kf ky kj kp kq kr ks bi translated"><a class="ae jn" href="https://machinelearningmastery.com/what-are-word-embeddings/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/what-are-word-embedding/</a></li><li id="c661" class="kk kl hi jq b jr ku ju kv jx kw kb kx kf ky kj kp kq kr ks bi translated"><a class="ae jn" href="https://adventuresinmachinelearning.com/keras-lstm-tutorial/" rel="noopener ugc nofollow" target="_blank">https://adventuresinmachine learning . com/keras-lstm-tutorial/</a></li><li id="e0d8" class="kk kl hi jq b jr ku ju kv jx kw kb kx kf ky kj kp kq kr ks bi translated"><a class="ae jn" href="https://skymind.ai/wiki/word2vec" rel="noopener ugc nofollow" target="_blank">https://skymind.ai/wiki/word2vec</a></li></ol></div></div>    
</body>
</html>