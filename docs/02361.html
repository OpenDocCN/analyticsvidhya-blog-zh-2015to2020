<html>
<head>
<title>How to use a Machine Learning Model to Make Predictions on Streaming Data using PySpark?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用机器学习模型对使用PySpark的流数据进行预测？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-use-a-machine-learning-model-to-make-predictions-on-streaming-data-using-pyspark-f3d27722a48c?source=collection_archive---------16-----------------------#2019-12-12">https://medium.com/analytics-vidhya/how-to-use-a-machine-learning-model-to-make-predictions-on-streaming-data-using-pyspark-f3d27722a48c?source=collection_archive---------16-----------------------#2019-12-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c87e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">想象一下，每秒钟有超过8500条推文被发送，超过900张照片被上传到Instagram，超过4200次Skype通话，超过78000次谷歌搜索，超过200万封电子邮件被发送(根据<a class="ae jd" href="https://www.internetlivestats.com/one-second/" rel="noopener ugc nofollow" target="_blank">互联网直播统计</a>)。</p><p id="375e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在正在以前所未有的速度和规模生成数据。在数据科学领域工作是多么美好的时光啊！但是海量数据也带来了同样复杂的挑战。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jj jk l"/></div></figure><p id="28f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">主要是——我们如何收集这种规模的数据？我们如何确保我们的机器学习管道在数据生成和收集后立即继续产生结果？这些是行业面临的重大挑战，也是为什么流数据的概念在组织中获得更多关注的原因。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jl"><img src="../Images/b435775056dde465ad396607bdd3240a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ymZ8YHQ0P1raLlu-.png"/></div></div></figure><p id="6bb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">添加处理流数据的能力将大幅提升您当前的<a class="ae jd" href="https://courses.analyticsvidhya.com/courses/ace-data-science-interviews?utm_source=blog&amp;utm_medium=streaming-data-pyspark-machine-learning-model" rel="noopener ugc nofollow" target="_blank">数据科学组合</a>。这是业内非常需要的技能，如果你能掌握它，它将帮助你获得下一个数据科学职位。</p><p id="0903" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，在本文中，我们将学习什么是流数据，了解Spark流的基础知识，然后处理一个行业相关的数据集，使用Spark实现流数据。</p><h1 id="8b24" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">什么是流数据？</h1><p id="486a" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">我们看到了上面的社交媒体数据——我们正在处理的数据令人难以置信。你能想象存储所有这些数据需要什么吗？这是个复杂的过程！因此，在我们深入本文的Spark方面之前，让我们花一点时间来理解什么是流数据。</p><blockquote class="kv kw kx"><p id="4697" class="if ig ky ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated"><em class="hi">流数据没有离散的开始或结束。每秒钟都会从数千个数据源中生成这些数据，并且需要尽快对其进行处理和分析。相当多的流数据需要实时处理，比如谷歌搜索结果。</em></p></blockquote><p id="619d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们知道，一些见解在事件发生后更有价值，但随着时间的推移，它们往往会失去价值。以任何体育赛事为例，我们希望看到即时分析、即时统计洞察，以便在那一刻真正享受比赛，对吗？</p><p id="1fe0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，假设你正在观看一场激动人心的网球比赛，对手是罗杰·费德勒和诺瓦克·德约科维奇。</p><p id="0086" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这场比赛在两局中打成平手，你想知道费德勒反手回球的百分比与他职业生涯的平均水平相比是多少。几天后或者在决胜局开始前的那一刻看到这一点有意义吗？</p><h1 id="55a5" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">火花流的基本原理</h1><blockquote class="kv kw kx"><p id="6cce" class="if ig ky ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated"><em class="hi"> Spark Streaming是核心Spark API的扩展，支持实时数据流的可扩展和容错流处理。</em></p></blockquote><p id="ee8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在跳到实现部分之前，让我们先了解一下Spark流的不同组件。</p><h1 id="4f4b" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">离散流</h1><p id="9e37" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated"><strong class="ih hj">离散化流，或数据流，代表连续的数据流。</strong>这里，数据流要么直接从任何来源接收，要么在我们对原始数据进行一些处理后接收。</p><p id="5de4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">构建流应用程序的第一步是为我们收集数据的数据源定义批处理持续时间。如果批处理持续时间为2秒，则数据将每2秒收集一次，并存储在RDD中。并且这些rdd的连续系列的链是不可变的并且可以被Spark用作分布式数据集的数据流。</p><p id="e53e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我强烈推荐您阅读这篇文章，以便更好地了解RDDs—<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/?utm_source=blog&amp;utm_medium=streaming-data-pyspark-machine-learning-model" rel="noopener ugc nofollow" target="_blank">Spark:RDDs</a>的全面介绍。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es lc"><img src="../Images/e9f992720f7482db0e81d18ab5a7b4af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*f7F_eEfwRIgZfsDv.png"/></div></div></figure><p id="c691" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">想想一个典型的数据科学项目。在数据预处理阶段，我们需要转换变量，包括将分类变量转换为数字变量、创建箱、移除异常值和许多其他事情。Spark维护我们对任何数据定义的所有转换的历史。因此，无论何时出现任何错误，它都可以追溯转换的路径，并再次生成计算结果。</p><p id="6ac1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们希望我们的Spark应用程序能够24 x 7全天候运行，无论何时出现任何故障，我们都希望它能够尽快恢复。但在处理大规模数据时，Spark需要重新计算所有转换，以防出现任何错误。可以想象，这可能相当昂贵。</p><h1 id="0511" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">贮藏</h1><p id="f63f" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">这里有一个应对这一挑战的方法。我们可以临时存储计算(缓存)的结果，以维护数据上定义的转换结果。这样，当任何错误发生时，我们不必一次又一次地重新计算这些转换。</p><p id="75c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据流允许我们将流数据保存在内存中。当我们想要对相同的数据进行多重运算时，这很有帮助。</p><h1 id="84bc" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">检查点</h1><p id="4aa7" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">当我们正确使用缓存时，它非常有用，但是它需要大量的内存。并不是每个人都有数百台128 GB内存的机器来缓存所有东西。</p><p id="2a7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是检查点的概念将帮助我们的地方。</p><blockquote class="kv kw kx"><p id="9fe5" class="if ig ky ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated"><em class="hi">检查点是另一种保存转换数据帧结果的技术。它会不时地将运行中的应用程序的状态保存在任何可靠的存储设备上，比如HDFS。但是，它比缓存慢，灵活性差。</em></p></blockquote><p id="161f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们有流数据时，我们可以使用检查点。变换结果依赖于先前的变换结果，并且需要被保留以便使用它。我们还检查元数据信息，如用于创建流数据的配置和一组数据流操作的结果等。</p><h1 id="2352" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">流数据中的共享变量</h1><p id="d919" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">有时，我们需要为Spark应用程序定义map、reduce或filter等函数，这些函数必须在多个集群上执行。该功能中使用的变量被复制到每台机器(集群)中。</p><p id="ca1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，每个集群都有一个不同的执行器，我们需要一些东西来给出这些变量之间的关系。</p><p id="9471" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，让我们假设我们的Spark应用程序运行在100个不同的集群上，这些集群捕获来自不同国家的人发布的Instagram图像。我们需要一个帖子中提到的特定标签的计数。</p><p id="a525" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，每个集群的执行程序将计算该特定集群上的数据结果。但是我们需要一些东西来帮助这些集群进行通信，这样我们就可以得到聚合的结果。在Spark中，我们共享了允许我们克服这个问题的变量。</p><h1 id="636b" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">累加器变量</h1><p id="8794" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">像错误发生的次数、空白日志的数量、我们从特定国家收到请求的次数等用例，所有这些都可以使用累加器来解决。</p><p id="76a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个集群上的执行器将数据发送回驱动程序进程，以更新累加器变量的值。累加器只适用于结合律和交换律。例如，总和与最大值可以工作，而平均值则不行。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ld"><img src="../Images/699d910456a399adb14fe9f08318cfdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HVIqToGzPWxHA9x7.png"/></div></div></figure><h1 id="2337" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">广播变量</h1><p id="ad73" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">当我们处理位置数据时，比如城市名称和邮政编码的映射——这些是固定变量，对吗？现在，如果每次任何集群上的特定转换都需要这种类型的数据，我们就不需要向驱动程序发送请求，因为这太昂贵了。</p><p id="095f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相反，我们可以在每个集群上存储这些数据的副本。这些类型的变量被称为广播变量。</p><blockquote class="kv kw kx"><p id="cc57" class="if ig ky ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated"><em class="hi">广播变量允许程序员在每台机器上缓存一个只读变量。通常，Spark使用有效的广播算法自动分配广播变量，但是如果我们的任务需要多个阶段的相同数据，我们也可以定义它们。</em></p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es le"><img src="../Images/e148fcaa75f1ba822400243ed4057fc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QPnSIUYrUxEm3NPM.png"/></div></div></figure><h1 id="2b46" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">使用PySpark对流数据执行情感分析</h1><p id="b352" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">是时候启动你最喜欢的IDE了！让我们在这一节开始编码，并以实用的方式理解流数据。</p><h1 id="422e" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">理解问题陈述</h1><p id="e9aa" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">在本节中，我们将使用真实世界的数据集。我们的目标是检测推文中的仇恨言论。为了简单起见，如果一条推文带有种族主义或性别歧视的情绪，我们就说这条推文包含仇恨言论。</p><p id="392b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，任务是将种族主义或性别歧视的推文从其他推文中分类。我们将使用推文和标签的训练样本，其中标签“1”表示推文是种族主义/性别歧视的，标签“0”表示不是。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lf"><img src="../Images/bc1d4ec10897cbf99097221f7ba64e6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/0*jLNK7Uaza2FeCEhw.png"/></div></figure><p id="7656" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ky">来源:TechCrunch </em></p><p id="1f2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为什么这是一个相关的项目？因为社交媒体平台以评论和状态更新的形式接收庞大的流数据。这个项目将帮助我们缓和什么是公开张贴。</p><p id="16fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在这里查看更详细的问题陈述— <a class="ae jd" href="https://datahack.analyticsvidhya.com/contest/practice-problem-twitter-sentiment-analysis/?utm_source=blog&amp;utm_medium=streaming-data-pyspark-machine-learning-model" rel="noopener ugc nofollow" target="_blank">练习题:推特情绪分析</a>。我们开始吧！</p><h1 id="5c52" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">设置项目工作流</h1><ol class=""><li id="57e8" class="lg lh hi ih b ii kq im kr iq li iu lj iy lk jc ll lm ln lo bi translated"><strong class="ih hj">模型构建</strong>:我们将构建一个逻辑回归模型管道，对推文是否包含仇恨言论进行分类。这里，我们的重点不是建立一个非常精确的分类模型，而是了解如何使用任何模型并返回流数据的结果</li><li id="571a" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated"><strong class="ih hj">初始化Spark流上下文</strong>:一旦模型构建完成，我们需要定义主机名和端口号，从那里获取流数据</li><li id="fa35" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated"><strong class="ih hj">流数据:</strong>接下来，我们将从定义的端口添加来自netcat服务器的tweets，Spark流API将在指定的持续时间后接收数据</li><li id="6978" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated"><strong class="ih hj">预测并返回结果:</strong>一旦我们接收到tweet文本，我们将数据传递到我们创建的机器学习管道中，并从模型中返回预测的情绪</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es lu"><img src="../Images/9b1966450e563cdcd8d99823ea8b0260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8BDOjcnC9LjnXFea.png"/></div></div></figure><p id="cbf7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是我们工作流程的简洁说明:</p><h1 id="b14a" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">用于构建逻辑回归模型的定型数据</h1><p id="e7ac" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">我们在一个映射到标签的CSV文件中有关于Tweets的数据。我们将使用一个<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/?utm_source=blog&amp;utm_medium=streaming-data-pyspark-machine-learning-model" rel="noopener ugc nofollow" target="_blank">逻辑回归模型</a>来预测推文是否包含仇恨言论。如果是，那么我们的模型将预测标签为1(否则为0)。可以参考本文<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2019/10/pyspark-for-beginners-first-steps-big-data-analysis/?utm_source=blog&amp;utm_medium=streaming-data-pyspark-machine-learning-model" rel="noopener ugc nofollow" target="_blank"> PySpark初学者</a>来设置Spark环境。</p><p id="4baf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在这里下载<a class="ae jd" href="https://github.com/lakshay-arora/PySpark/tree/master/spark_streaming" rel="noopener ugc nofollow" target="_blank">数据集和代码</a>。</p><p id="a300" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们需要定义CSV文件的模式。否则，Spark会将每一列的数据类型视为string。读取数据并检查模式是否符合定义:</p><pre class="je jf jg jh fd lv lw lx ly aw lz bi"><span id="e88e" class="ma jt hi lw b fi mb mc l md me"># importing required libraries<br/>from pyspark import SparkContext<br/>from pyspark.sql.session import SparkSession<br/>from pyspark.streaming import StreamingContext<br/>import pyspark.sql.types as tp<br/>from pyspark.ml import Pipeline<br/>from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler<br/>from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer<br/>from pyspark.ml.classification import LogisticRegression<br/>from pyspark.sql import Row</span><span id="7554" class="ma jt hi lw b fi mf mc l md me"># initializing spark session<br/>sc = SparkContext(appName="PySparkShell")<br/>spark = SparkSession(sc)<br/>    <br/># define the schema<br/>my_schema = tp.StructType([<br/>  tp.StructField(name= 'id',          dataType= tp.IntegerType(),  nullable= True),<br/>  tp.StructField(name= 'label',       dataType= tp.IntegerType(),  nullable= True),<br/>  tp.StructField(name= 'tweet',       dataType= tp.StringType(),   nullable= True)<br/>])<br/>    <br/>  <br/># read the dataset  <br/>my_data = spark.read.csv('twitter_sentiments.csv',<br/>                         schema=my_schema,<br/>                         header=True)</span><span id="f6be" class="ma jt hi lw b fi mf mc l md me"># view the data<br/>my_data.show(5)</span><span id="2e76" class="ma jt hi lw b fi mf mc l md me"># print the schema of the file<br/>my_data.printSchema()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mg"><img src="../Images/94fd529c30d210764325f58299deb41d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kTRjaFkg_8bn5ZU3.png"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mh"><img src="../Images/71039640c1a096cc66f15013552bfd24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AYYnGesEhmmaWwNA.png"/></div></div></figure><h1 id="dbbd" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">定义我们机器学习管道的阶段</h1><p id="0445" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">现在我们已经有了Spark数据框架中的数据，我们需要定义我们想要转换数据的不同阶段，然后使用它从我们的模型中获得预测标签。</p><p id="ba7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第一阶段，我们将使用<em class="ky"> RegexTokenizer </em>将Tweet文本转换成单词列表。然后，我们将从单词列表中移除停用单词，并创建<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2019/07/how-to-build-recommendation-system-word2vec-python/?utm_source=blog&amp;utm_medium=streaming-data-pyspark-machine-learning-model" rel="noopener ugc nofollow" target="_blank">单词向量</a>。在最后阶段，我们将使用这些词向量来建立逻辑回归模型，并获得预测的情感。</p><p id="7dc2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ky">你可以参考这篇文章——《</em><a class="ae jd" href="https://www.analyticsvidhya.com/blog/2018/07/hands-on-sentiment-analysis-dataset-python/?utm_source=blog&amp;utm_medium=streaming-data-pyspark-machine-learning-model" rel="noopener ugc nofollow" target="_blank"><em class="ky">Twitter情感分析综合动手指南</em></a><em class="ky">》——构建一个更准确、更健壮的文本分类模型。并且你也可以在这里阅读更多关于构建Spark机器学习管道的内容:</em> <a class="ae jd" href="https://www.analyticsvidhya.com/blog/2019/11/build-machine-learning-pipelines-pyspark/?utm_source=blog&amp;utm_medium=streaming-data-pyspark-machine-learning-model" rel="noopener ugc nofollow" target="_blank"> <em class="ky">想要构建机器学习管道？使用PySpark </em> </a> <em class="ky">的快速介绍。</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mi"><img src="../Images/baeb849a8f6786e453617892b77dee35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GAb9W9BlG4F2d-v0.png"/></div></div></figure><p id="9e31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请记住，我们的重点不是建立一个非常准确的分类模型，而是看看如何使用预测模型来获得流数据的结果。</p><pre class="je jf jg jh fd lv lw lx ly aw lz bi"><span id="a80d" class="ma jt hi lw b fi mb mc l md me"># define stage 1: tokenize the tweet text    <br/>stage_1 = RegexTokenizer(inputCol= 'tweet' , outputCol= 'tokens', pattern= '\\W')<br/># define stage 2: remove the stop words<br/>stage_2 = StopWordsRemover(inputCol= 'tokens', outputCol= 'filtered_words')<br/># define stage 3: create a word vector of the size 100<br/>stage_3 = Word2Vec(inputCol= 'filtered_words', outputCol= 'vector', vectorSize= 100)<br/># define stage 4: Logistic Regression Model<br/>model = LogisticRegression(featuresCol= 'vector', labelCol= 'label')</span></pre><h1 id="c6c1" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">设置我们的机器学习管道</h1><p id="b16a" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">让我们在管道对象中添加阶段，然后我们将按顺序执行这些转换。用训练数据集拟合管道，现在，每当我们有新的Tweet时，我们只需要通过管道对象传递它，并转换数据以获得预测:</p><pre class="je jf jg jh fd lv lw lx ly aw lz bi"><span id="abc6" class="ma jt hi lw b fi mb mc l md me"># setup the pipeline<br/>pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, model])</span><span id="30be" class="ma jt hi lw b fi mf mc l md me"># fit the pipeline model with the training data<br/>pipelineFit = pipeline.fit(my_data)</span></pre><h1 id="1a46" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">流数据并返回结果</h1><p id="6a87" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">假设我们每秒收到数百条评论，我们希望通过阻止用户发布包含仇恨言论的评论来保持平台的清洁。因此，每当我们收到新的文本，我们将把它传递到管道，并获得预测的情绪。</p><p id="6c8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将定义一个函数<strong class="ih hj"> get_prediction </strong>，它将删除空白句子并创建一个数据帧，其中每行包含一条Tweet。</p><p id="82fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，初始化Spark流上下文并定义3秒的批处理持续时间。这意味着我们将对每3秒钟收到的数据进行预测:</p><pre class="je jf jg jh fd lv lw lx ly aw lz bi"><span id="a41c" class="ma jt hi lw b fi mb mc l md me"># define a function to compute sentiments of the received tweets<br/>def get_prediction(tweet_text):<br/> try:<br/>    # filter the tweets whose length is greater than 0<br/>  tweet_text = tweet_text.filter(lambda x: len(x) &gt; 0)<br/>    # create a dataframe with column name 'tweet' and each row will contain the tweet<br/>  rowRdd = tweet_text.map(lambda w: Row(tweet=w))<br/>    # create a spark dataframe<br/>  wordsDataFrame = spark.createDataFrame(rowRdd)<br/>    # transform the data using the pipeline and get the predicted sentiment<br/>  pipelineFit.transform(wordsDataFrame).select('tweet','prediction').show()<br/> except : <br/>  print('No data')<br/>    <br/># initialize the streaming context <br/>ssc = StreamingContext(sc, batchDuration= 3)</span><span id="bd87" class="ma jt hi lw b fi mf mc l md me"># Create a DStream that will connect to hostname:port, like localhost:9991<br/>lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))</span><span id="ad14" class="ma jt hi lw b fi mf mc l md me"># split the tweet text by a keyword 'TWEET_APP' so that we can identify which set of words is from a single tweet<br/>words = lines.flatMap(lambda line : line.split('TWEET_APP'))</span><span id="1f18" class="ma jt hi lw b fi mf mc l md me"># get the predicted sentiments for the tweets received<br/>words.foreachRDD(get_prediction)</span><span id="cadd" class="ma jt hi lw b fi mf mc l md me"># Start the computation<br/>ssc.start()</span><span id="0eb7" class="ma jt hi lw b fi mf mc l md me"># Wait for t<br/>ssc.awaitTermination()</span></pre><p id="5ace" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在一个终端中运行程序，并使用Netcat(一个实用工具，可用于向定义的主机名和端口号发送数据)。您可以使用以下命令启动TCP连接:</p><pre class="je jf jg jh fd lv lw lx ly aw lz bi"><span id="ee1e" class="ma jt hi lw b fi mb mc l md me">nc -lk port_number</span></pre><p id="bcc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，在第二个终端中键入文本，您将在另一个终端中实时获得预测:</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mj jk l"/></div></figure><p id="80c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">完美！</p><p id="c6ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">流数据在未来几年只会增加，所以你应该开始熟悉这个话题。请记住，数据科学不仅仅是建立模型，还有一个完整的管道需要处理。</p><p id="f68b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文介绍了Spark流的基础知识以及如何在真实数据集上实现它。我鼓励您采用另一个数据集或收集实时数据，并实现我们刚刚介绍的内容(您也可以尝试不同的模型)。</p><p id="67ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我期待在下面的评论区听到你对这篇文章的反馈和你的想法。</p></div><div class="ab cl mk ml gp mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="hb hc hd he hf"><p id="b7e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ky">原载于2019年12月12日</em><a class="ae jd" href="https://www.analyticsvidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/" rel="noopener ugc nofollow" target="_blank"><em class="ky">【https://www.analyticsvidhya.com】</em></a><em class="ky">。</em></p></div></div>    
</body>
</html>