<html>
<head>
<title>Convolutional Neural Networks in plain English</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简明英语中的卷积神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/convolutional-neural-networks-in-plain-english-b5464f704a50?source=collection_archive---------23-----------------------#2020-05-27">https://medium.com/analytics-vidhya/convolutional-neural-networks-in-plain-english-b5464f704a50?source=collection_archive---------23-----------------------#2020-05-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c6f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在深度学习中，卷积神经网络(也称为CNN或ConvNet)是一类深度神经网络，最常用于分析图像和视觉图像。为此，ConvNets有几个专门的和分层的隐藏层。这意味着第一层可以检测直线或曲线，而更深的层可以识别更复杂的形状，如人脸或动物轮廓。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/41c885ac6ff440fbdde6ee8af2f2fbdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4OUonEDfZwCfR4Y-G-h1fw.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">卷积神经网络的典型架构(<a class="ae jt" href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53" rel="noopener" target="_blank">来源</a></figcaption></figure><h1 id="3cfe" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">康文内斯的食谱</strong></h1><ol class=""><li id="7d04" class="ks kt hi ih b ii ku im kv iq kw iu kx iy ky jc kz la lb lc bi translated"><strong class="ih hj">大量带标签的数字图像</strong>用于训练<strong class="ih hj">。</strong>对于计算机来说，数字图像<strong class="ih hj"> </strong>就是<strong class="ih hj"> </strong>只不过是表示像素网格上特定位置的红色、绿色和蓝色变化的数字。这些矩阵应该进行预处理，这意味着扁平化和规范化。</li><li id="52cb" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">现在开始了ConvNets的独特过程:<strong class="ih hj">卷积</strong>。这包括从输入图像中提取一组相近的像素，并对一个称为内核的小矩阵进行卷积(实际上它是一个标量积)，从而生成输出矩阵。实际上，我们不仅仅应用一个内核，而是应用几个内核(内核的集合被称为过滤器)来产生几个输出图像。这些新图像用于检测与原始图像不同的特征。</li><li id="5086" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">激活功能</strong>。ConvNets中最常见的隐藏层激活函数是整流线性单元或ReLu，定义为<em class="li"> f(x) = max(0，x)。</em></li><li id="2b21" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">除了卷积，ConvNets几乎总是使用另一种叫做<strong class="ih hj">池的操作。</strong>其功能是逐步减少层的尺寸，从而<em class="li">减少网络中的参数数量</em> <em class="li">和计算量</em>。联营最常用的方法是<strong class="ih hj">最大联营</strong>。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lj"><img src="../Images/7a8df20b646f4eadba51d340b20bcfc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*Yks8-D0ZyrgUsbs4DIOE3w.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">2x2最大池(图片由作者制作)</figcaption></figure><p id="b662" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">卷积层、激活功能和池层的组被称为<strong class="ih hj">卷积层</strong>。那么，在一个卷积层之后，我们是否已经完成了模型？不要！！！现在更多的回旋！</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lk"><img src="../Images/f48187bfba46c17e46fb52fbebd8b0f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VPQSHBwI_szE31W8xZsQJg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">第一个卷积层(图片由作者制作)</figcaption></figure><p id="29a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第一次卷积中，我们的模型只能检测简单的特征，如直线或曲线。因此，为了能够识别更复杂的形式，我们需要添加更多的层。下表总结了多层网络的发展</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ll"><img src="../Images/ea2d9fc998815207ff76f0d95b281ccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VVnEAR6ZBQa_imtC37lfzA.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">(表格由作者制作)</figcaption></figure><p id="a337" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第三个卷积层之后，我们得到一个大小为3x3的图像。所以不可能继续了。有了更大的图像，我们可以继续进行卷积。</p><p id="bc04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们已经接近终点了！</p><h2 id="9eb8" class="lm jv hi bd jw ln lo lp ka lq lr ls ke iq lt lu ki iu lv lw km iy lx ly kq lz bi translated"><strong class="ak">连接到完全连接的层。</strong></h2><p id="8fa8" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq ma is it iu mb iw ix iy mc ja jb jc hb bi translated">为了最终构建我们的模型，我们需要将最后一个输出层连接到一个完全连接的层。然而，由于最后一层是一个3D数组(3x3x384)，我们需要将其转换为一个矢量，因此我们将其展平(并获得一个3x3x384=3456的矢量)。</p><p id="fc1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，在一个或多个完全连接的层之后，我们应用最终的激活函数。通常，Softmax激活功能。</p><p id="a41a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">全连接层与ConvNet的比较。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es md"><img src="../Images/45e204e12a7f1a54b57b52f8f5e29d28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*AqqEWzCVQigdxy2KRm3UzQ.jpeg"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">(表格由作者制作)</figcaption></figure><p id="4fa0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">康文网络的优势是什么？</strong></p><p id="89c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在完全连接的层中，每个输出单元都与每个输入单元交互，这使得训练和适应存储器的训练成本极高。然而，卷积网络具有稀疏的相互作用，因此我们需要存储更少的参数，这既降低了模型的内存需求，也减少了操作的数量。此外，ConvNets不容易过度拟合数据。</p></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><p id="0860" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在接下来的博文中，我们将在TensorFlow 2.0中编写一些经典的ConvNets</p></div></div>    
</body>
</html>