<html>
<head>
<title>Automate Architecture Modelling with Neural Architecture Search (NAS)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用神经架构搜索(NAS)进行自动化架构建模</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/automate-architecture-modelling-with-neural-architecture-search-nas-1388b55fe565?source=collection_archive---------14-----------------------#2020-10-12">https://medium.com/analytics-vidhya/automate-architecture-modelling-with-neural-architecture-search-nas-1388b55fe565?source=collection_archive---------14-----------------------#2020-10-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/713ab33da89749ccd3ec1a6bd92bfb5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/1*4FqIyMp2sRZ4IQI_lrhMvw.gif"/></div></figure><h1 id="919b" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">神经架构搜索是怎么回事？</h1><blockquote class="jk jl jm"><p id="940e" class="jn jo jp jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">"<strong class="jq hj">创造 AI 的 AI </strong> —神经架构搜索<strong class="jq hj"> </strong> (NAS)是<strong class="jq hj">自动化设计</strong> <a class="ae km" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj">人工神经网络</strong> </a> (ANN)的技术，是<a class="ae km" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>领域广泛使用的模型。"</p></blockquote><p id="f5a5" class="pw-post-body-paragraph jn jo hi jq b jr js jt ju jv jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk kl hb bi kq translated"><span class="l kr ks kt bm ku kv kw kx ky di"> O </span>多年来，深度学习范式在图像识别、语音识别和机器翻译等各种任务上取得了惊人的进展。许多像谷歌这样的领先公司和个人学术研究人员已经能够提供少量最先进的深度学习模型，以在上述这些应用程序上提供一些令人惊叹的结果。如果我们将这个主题缩小到图像分类，非常复杂的架构(如 VGG16、GoogleNet 和 ResNet)在标准数据集(如 ImageNet 和 CIFAR-10)上表现非常好，但在一些特定于组织的数据集上表现不太好。然而，这些当前采用的机器学习模型大多是由人类专家(如技术熟练的工程师和科学家团队)手动设计的，这是一个困难、耗时且容易出错的过程。</p><p id="057a" class="pw-post-body-paragraph jn jo hi jq b jr js jt ju jv jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk kl hb bi translated">因此，AutoML 的一个称为神经架构搜索的子学科专注于自动设计机器学习模型，同时使这一过程更容易实现。<strong class="jq hj">神经结构搜索旨在自动执行在给定数据集的情况下寻找最佳模型结构的过程。</strong></p><p id="8199" class="pw-post-body-paragraph jn jo hi jq b jr js jt ju jv jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk kl hb bi translated">当我开始为我的最后一年研究项目研究 NAS 时，我发现了这个关于 NAS 的有趣调查，它非常有帮助，由<a class="ae km" href="https://arxiv.org/abs/1808.05377" rel="noopener ugc nofollow" target="_blank"> Elsken 等人 2019 </a>。在这篇博客中，我将尝试以一种非常直观和简单的方式解释 NAS 背后的这些理论，因为我个人在第一次学习它时经历了很多困难。</p></div><div class="ab cl kz la gp lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="hb hc hd he hf"><h1 id="9dcf" class="im in hi bd io ip lg ir is it lh iv iw ix li iz ja jb lj jd je jf lk jh ji jj bi translated">NAS 概述</h1><p id="51a7" class="pw-post-body-paragraph jn jo hi jq b jr ll jt ju jv lm jx jy kn ln kb kc ko lo kf kg kp lp kj kk kl hb bi translated">通常，NAS 系统中有三个主要组件。</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/b5f00ff24e8d53e65ac43ae1161a49c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*C4MiuKuRu68eML6nm6Qtwg.jpeg"/></div></figure><h1 id="23ff" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">1.搜索空间</h1><p id="78d4" class="pw-post-body-paragraph jn jo hi jq b jr ll jt ju jv lm jx jy kn ln kb kc ko lo kf kg kp lp kj kk kl hb bi translated">该组件描述了原则上 NAS 方法可能发现哪些神经架构。因此，<strong class="jq hj">定义了一组操作(例如卷积、全连接、池化)以及如何连接操作以形成有效的网络架构</strong>。尽管我们将 NAS 称为自动任务，但 NAS 方法的这一阶段需要人工干预来设计特定于给定应用程序的搜索空间。例如，对于计算机视觉任务，搜索空间可以是最先进的卷积网络的空间，而对于语言建模任务，搜索空间可以是递归网络的空间。然而，这在整个过程中引入了人为的偏见，这可能会阻止找到超出当前人类知识的新颖的架构构建块。但是，它仍然有助于减少搜索空间的大小并简化搜索。</p><h2 id="8b34" class="lv in hi bd io lw lx ly is lz ma mb iw kn mc md ja ko me mf je kp mg mh ji mi bi translated">顺序分层操作</h2><p id="dcfe" class="pw-post-body-paragraph jn jo hi jq b jr ll jt ju jv lm jx jy kn ln kb kc ko lo kf kg kp lp kj kk kl hb bi translated">这可能是为神经网络架构设计搜索空间的最天真和相对简单的方法。它由一系列 n 层组成，其中第<em class="jp"> i </em>层 L_i 从第<em class="jp">(I-1)</em>层接收其输入，其输出作为第<em class="jp"> (i +1) </em>层的输入。</p><p id="e071" class="pw-post-body-paragraph jn jo hi jq b jr js jt ju jv jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk kl hb bi translated">搜索空间被参数化，</p><ol class=""><li id="6a00" class="mj mk hi jq b jr js jv jw kn ml ko mm kp mn kl mo mp mq mr bi translated"><strong class="jq hj">层数</strong></li><li id="931e" class="mj mk hi jq b jr ms jv mt kn mu ko mv kp mw kl mo mp mq mr bi translated"><strong class="jq hj">每层执行的操作类型</strong>(例如:卷积、池化、深度可分卷积等)</li><li id="87e0" class="mj mk hi jq b jr ms jv mt kn mu ko mv kp mw kl mo mp mq mr bi translated"><strong class="jq hj">与操作</strong>相关的超参数(例如:滤波器数量、内核大小、卷积层的步距、全连接层的单元数量)</li></ol><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/dcf8492871098456c8dc733ee278aef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*raLPtUUHeEyanC3ICXGV9w.jpeg"/></div><figcaption class="my mz et er es na nb bd b be z dx translated">图中的每个节点对应于神经网络中的一层，例如卷积层或汇集层。不同的图层类型以不同的颜色显示。</figcaption></figure><h2 id="a5a4" class="lv in hi bd io lw lx ly is lz ma mb iw kn mc md ja ko me mf je kp mg mh ji mi bi translated">链式结构空间</h2><p id="519e" class="pw-post-body-paragraph jn jo hi jq b jr ll jt ju jv lm jx jy kn ln kb kc ko lo kf kg kp lp kj kk kl hb bi translated">这结合了更复杂的搜索空间，具有一些附加的层类型和多个分支以及跳过连接。</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es nc"><img src="../Images/d11ce25aaec6ef05982dfd51f95949f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*4HWJ8SQQ2PYVDeXGsfMnew.jpeg"/></div></figure><h2 id="b567" class="lv in hi bd io lw lx ly is lz ma mb iw kn mc md ja ko me mf je kp mg mh ji mi bi translated">基于单元的表示</h2><p id="e6f6" class="pw-post-body-paragraph jn jo hi jq b jr ll jt ju jv lm jx jy kn ln kb kc ko lo kf kg kp lp kj kk kl hb bi translated">如果我们要观察一些先进的视觉模型架构，如 Inception、ResNet 和 MobileNet，这些模型是使用重复的模块或单元设计的。因此，我们可以将搜索空间定义为重复多次的同一个单元，每个单元包含 NAS 算法预测的几个操作，而不是手动设计搜索空间的整个架构。这也使得通过调整单元重复的数量来缩小或放大模型尺寸变得容易。实现基于单元的搜索空间的主要优点之一是由于减少了搜索空间而获得了加速。(即，单元通常由比整个架构少得多的层组成)。NASNet 搜索空间学习用于网络构建的两种类型的单元:</p><ol class=""><li id="a701" class="mj mk hi jq b jr js jv jw kn ml ko mm kp mn kl mo mp mq mr bi translated"><strong class="jq hj"> <em class="jp">正常单元</em> </strong>:输入输出特征图的维数相同。</li><li id="ef25" class="mj mk hi jq b jr ms jv mt kn mu ko mv kp mw kl mo mp mq mr bi translated"><strong class="jq hj"> <em class="jp">缩小单元</em> </strong>:输出的特征地图宽度和高度缩小一半。</li></ol><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/3e0de1ee90cdc58f81c58ec137db6c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*ZKbaUpdfApNlyRx3e1_usQ.jpeg"/></div><figcaption class="my mz et er es na nb bd b be z dx translated">通过顺序堆叠单元而构建的架构。请注意，单元也可以以更复杂的方式组合，例如在多分支空间中，只需用单元替换层即可。</figcaption></figure><h1 id="e847" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">2.搜索算法</h1><p id="dbba" class="pw-post-body-paragraph jn jo hi jq b jr ll jt ju jv lm jx jy kn ln kb kc ko lo kf kg kp lp kj kk kl hb bi translated">这个组件<strong class="jq hj">决定了如何探索搜索空间以找到好的架构</strong>。探索搜索空间的一种简单方法是试错法。随机搜索是一种反复试验的技术，它从搜索空间<em class="jp">中随机抽取一个有效的架构候选者</em>，不涉及学习模型。然而，这为设计良好的搜索空间提供了最佳结果。</p><p id="cf62" class="pw-post-body-paragraph jn jo hi jq b jr js jt ju jv jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk kl hb bi translated">现有的 NAS 搜索算法或搜索策略主要可以分为两类:黑盒优化策略和差分架构搜索策略。</p><h2 id="0b2b" class="lv in hi bd io lw lx ly is lz ma mb iw kn mc md ja ko me mf je kp mg mh ji mi bi translated">黑盒优化策略</h2><p id="636a" class="pw-post-body-paragraph jn jo hi jq b jr ll jt ju jv lm jx jy kn ln kb kc ko lo kf kg kp lp kj kk kl hb bi translated">黑盒优化策略具有离散的搜索空间，因此梯度下降优化不能直接应用。各种自适应方法，如强化学习、进化编程和贝叶斯优化都属于黑盒范畴。</p><h2 id="f99a" class="lv in hi bd io lw lx ly is lz ma mb iw kn mc md ja ko me mf je kp mg mh ji mi bi translated">差异架构搜索策略</h2><p id="f644" class="pw-post-body-paragraph jn jo hi jq b jr ll jt ju jv lm jx jy kn ln kb kc ko lo kf kg kp lp kj kk kl hb bi translated">与黑盒优化策略不同，差分架构搜索策略将搜索空间放宽为连续的，因此使用梯度下降进行优化。<a class="ae km" href="https://arxiv.org/pdf/1806.09055.pdf" rel="noopener ugc nofollow" target="_blank"> DARTS </a>和<a class="ae km" href="https://arxiv.org/abs/1907.05737" rel="noopener ugc nofollow" target="_blank"> PC-DARTS </a>就是两种这样的差分架构搜索策略，它们比早期的黑盒优化策略更高效、更快速。</p><h1 id="d707" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">3.评估策略</h1><p id="ccef" class="pw-post-body-paragraph jn jo hi jq b jr ll jt ju jv lm jx jy kn ln kb kc ko lo kf kg kp lp kj kk kl hb bi translated">如前所述，搜索算法的目标是找到一个神经架构，最大限度地提高一些性能指标，如对看不见的数据的准确性。因此，我们需要一种策略来估计或预测给定架构的性能，以指导搜索算法。</p><h2 id="d01a" class="lv in hi bd io lw lx ly is lz ma mb iw kn mc md ja ko me mf je kp mg mh ji mi bi translated">从头开始训练</h2><p id="c781" class="pw-post-body-paragraph jn jo hi jq b jr ll jt ju jv lm jx jy kn ln kb kc ko lo kf kg kp lp kj kk kl hb bi translated">一个显而易见的方法是根据训练数据独立地从头开始训练候选模型，然后评估它在验证数据上的性能。然而，这是一项高度计算的任务，需要数千个 GPU 日来从头开始训练每个要评估的架构。因此，已经提出了几种评估策略来加速性能估计。</p><h2 id="56e7" class="lv in hi bd io lw lx ly is lz ma mb iw kn mc md ja ko me mf je kp mg mh ji mi bi translated">代理任务性能</h2><p id="77f7" class="pw-post-body-paragraph jn jo hi jq b jr ll jt ju jv lm jx jy kn ln kb kc ko lo kf kg kp lp kj kk kl hb bi translated">这种方法通常以更便宜和更快速的方式估计子网络的性能。它包括在一个更小的数据集上进行训练，这个数据集通常被称为代理数据集。从而在较大程度上减少了训练子网络的时间。此外，通过训练更少的时期，减少了训练时间。虽然这些近似降低了计算成本，但是它们也在估计中引入了偏差，因为性能通常会被低估。然而，只要搜索策略仅依赖于对不同架构的排名，并且相对排名保持稳定，这可能仍然不是问题。</p><h1 id="e770" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">摘要</h1><p id="1ba8" class="pw-post-body-paragraph jn jo hi jq b jr ll jt ju jv lm jx jy kn ln kb kc ko lo kf kg kp lp kj kk kl hb bi translated">在这篇文章中，我们回顾了神经结构搜索的问题，分为三个主要部分:搜索空间、搜索算法和评估策略。在深入研究更深层次的实现之前，这为您提供了对神经架构搜索的基本理解。</p><p id="37f9" class="pw-post-body-paragraph jn jo hi jq b jr js jt ju jv jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk kl hb bi translated">我希望这篇文章对你有用。</p><p id="f497" class="pw-post-body-paragraph jn jo hi jq b jr js jt ju jv jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk kl hb bi translated">在后面的文章中，我们将开始更详细地研究一些有效的架构搜索策略，比如 DARTS。</p><h1 id="d9b1" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">参考</h1><p id="96b3" class="pw-post-body-paragraph jn jo hi jq b jr ll jt ju jv lm jx jy kn ln kb kc ko lo kf kg kp lp kj kk kl hb bi translated">[1]<a class="ae km" href="https://arxiv.org/pdf/1806.09055.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1806.09055.pdf</a></p><p id="67d2" class="pw-post-body-paragraph jn jo hi jq b jr js jt ju jv jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk kl hb bi translated">[2]https://arxiv.org/abs/1806.09055<a class="ae km" href="https://arxiv.org/abs/1806.09055" rel="noopener ugc nofollow" target="_blank"/></p><p id="bc7c" class="pw-post-body-paragraph jn jo hi jq b jr js jt ju jv jw jx jy kn ka kb kc ko ke kf kg kp ki kj kk kl hb bi translated">[3]https://arxiv.org/abs/1806.09055<a class="ae km" href="https://arxiv.org/abs/1806.09055" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>