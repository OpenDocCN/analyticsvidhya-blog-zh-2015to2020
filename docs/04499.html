<html>
<head>
<title>Using cross-validation to evaluate different models — Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用交叉验证评估不同的模型—回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/using-cross-validation-to-evaluate-different-models-regression-5f61ec89531?source=collection_archive---------4-----------------------#2020-03-22">https://medium.com/analytics-vidhya/using-cross-validation-to-evaluate-different-models-regression-5f61ec89531?source=collection_archive---------4-----------------------#2020-03-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1dd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑到大量可用的机器学习模型，选择最适合给定数据的模型非常重要。有许多方法可用于评估模型在特定数据集(R平方、RMSE等)上的性能。).</p><p id="82f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但通常，对训练数据的误差计算可能无法很好地估计模型在某些未知数据上的表现。这个问题的一个可能的解决方案是将数据分成训练集和测试集。将使用训练集来训练该模型，并且使用测试集来执行误差计算。</p><p id="8f17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种方法在概念上简单且易于实现。但是它有一些潜在的缺点:</p><ol class=""><li id="91aa" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">测试错误率可能变化很大，具体取决于哪些观察值包含在训练集中，哪些观察值包含在测试集中。</li><li id="09a7" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">当在较少的观察值上训练时，统计方法往往表现更差。</li></ol><p id="76d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有一些方法可以克服这些问题。本文将介绍的一种方法是K-fold交叉验证。</p><h1 id="0273" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated"><strong class="ak"> K倍交叉验证</strong></h1><p id="6fcf" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">这种方法包括将一组观察值随机分成k个大小大致相等的组或折叠。第一个折叠被视为一个测试集，模型适合其余的k1折叠。然后根据保留折叠中的观察值计算误差。这个过程重复k次；每次，不同的观察组被视为一个测试集。这个过程产生了k个测试误差估计值。通过平均这些误差值来计算k倍CV估计值。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ku"><img src="../Images/bcf3ff6ff6c100b13605bd21083affd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L90fdYb2oT0BOP2CCMSIbg.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated"><a class="ae lk" href="https://scikit-learn.org/stable/modules/cross_validation.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/cross _ validation . html</a></figcaption></figure><h1 id="867d" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated"><strong class="ak">波士顿房产数据集上的K倍CV</strong></h1><p id="f786" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">波士顿住房数据集可在<a class="ae lk" href="https://www.kaggle.com/schirmerchad/bostonhoustingmlnd" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>获得。在包含必要的库和处理数据之后，就可以应用模型了。完整的代码可在<a class="ae lk" href="https://www.kaggle.com/pranavkaushik/boston-housing/edit" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="8e64" class="lq js hi lm b fi lr ls l lt lu">from sklearn.model_selection import cross_val_score</span></pre><p id="95bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">cross_val_score计算应用模型的R平方度量。接近1的r平方误差意味着更好的拟合和更小的误差。</p><ol class=""><li id="051e" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih hj">线性回归</strong></li></ol><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="7daf" class="lq js hi lm b fi lr ls l lt lu">from sklearn.linear_model import LinearRegression<br/>lr = LinearRegression()<br/>np.mean(cross_val_score(lr, X, Y, cv=5))</span></pre><p id="fc8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简历得分:</p><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="049f" class="lq js hi lm b fi lr ls l lt lu">0.4237735326662294</span></pre><p id="2198" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj">岭回归</strong></p><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="eaa1" class="lq js hi lm b fi lr ls l lt lu">from sklearn.linear_model import RidgeCV<br/>ridge = RidgeCV(cv=5).fit(X, Y)<br/>ridge.score(X, Y)</span></pre><p id="24e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简历得分:</p><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="fb09" class="lq js hi lm b fi lr ls l lt lu">0.7172506647287367</span></pre><p id="f1dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。拉索回归</strong></p><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="c5bd" class="lq js hi lm b fi lr ls l lt lu">from sklearn.linear_model import LassoCV<br/>lasso = LassoCV(cv=5).fit(X, Y)<br/>lasso.score(X, Y)</span></pre><p id="25ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简历得分:</p><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="ca97" class="lq js hi lm b fi lr ls l lt lu">0.7175254518891412</span></pre><p id="be87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。多项式回归</strong></p><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="c989" class="lq js hi lm b fi lr ls l lt lu">from sklearn.preprocessing import PolynomialFeatures</span><span id="cade" class="lq js hi lm b fi lv ls l lt lu">def create_polynomial_regression_model(degree):<br/> poly_features = PolynomialFeatures(degree=degree)<br/> X_poly = poly_features.fit_transform(X)<br/> poly = LinearRegression()<br/> return np.mean(cross_val_score(poly, X_poly, Y, cv=5))</span><span id="f572" class="lq js hi lm b fi lv ls l lt lu">poly_cv = []<br/>for i in range(1,4):<br/> poly_cv.append(create_polynomial_regression_model(i))</span><span id="a683" class="lq js hi lm b fi lv ls l lt lu">plt.scatter(range(1,4),poly_cv)</span></pre><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lw"><img src="../Images/faa41046242e9a86ad52e712344b662c.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*40vFKaLtZpNNXouHSGt1CA.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">通过拟合二次多项式获得最高CV值</figcaption></figure><p id="1bc2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">二次多项式的CV分数:</p><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="b5a3" class="lq js hi lm b fi lr ls l lt lu">0.6989409158148152</span></pre><p id="6467" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。KNN回归</strong></p><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="4630" class="lq js hi lm b fi lr ls l lt lu">from sklearn.neighbors import KNeighborsRegressor</span><span id="d089" class="lq js hi lm b fi lv ls l lt lu">cv_score=[]<br/>for i in range(1,10):<br/> knn = KNeighborsRegressor(n_neighbors= i)<br/> cv_score.append(np.mean(cross_val_score(knn,X,Y,cv=5)))</span><span id="7f62" class="lq js hi lm b fi lv ls l lt lu">x = range(1,10)<br/>plt.scatter(x,cv_score)</span></pre><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lx"><img src="../Images/f454041ef8b8a9fae6b0fe3b3d6db23f.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*TJrZbSuwvrAhwoeRlG89QA.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">K = 8时获得的最高CV值</figcaption></figure><p id="0cc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">K = 8时的CV分数:</p><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="7329" class="lq js hi lm b fi lr ls l lt lu">0.5788133442607475</span></pre><p id="71ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 6。决策树</strong></p><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="7c8e" class="lq js hi lm b fi lr ls l lt lu">from sklearn.tree import DecisionTreeRegressor<br/>dt = DecisionTreeRegressor()<br/>np.mean(cross_val_score(dt, X, Y, cv=5))</span></pre><p id="e606" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简历得分:</p><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="181f" class="lq js hi lm b fi lr ls l lt lu">0.4254202824604191</span></pre><p id="d2c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 7。随机森林</strong></p><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="9a92" class="lq js hi lm b fi lr ls l lt lu">from sklearn.ensemble import RandomForestRegressor<br/>rf = RandomForestRegressor()<br/>np.mean(cross_val_score(rf, X, Y, cv=5))</span></pre><p id="1c09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简历得分:</p><pre class="kv kw kx ky fd ll lm ln lo aw lp bi"><span id="90de" class="lq js hi lm b fi lr ls l lt lu">0.6654863188252762</span></pre><h1 id="d2ef" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated"><strong class="ak">结论</strong></h1><p id="25e6" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">基于CV分数评估了不同的回归模型，观察到与所有其他方法相比，脊/套索回归最适合数据。</p><p id="50aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还可以观察到，对于给定的数据集，完美的线性模型不是很好的近似。</p><p id="e7b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过观察观察变量和目标变量之间的关系，可以进一步理解各个模型对于给定数据的性能背后的原因。</p></div></div>    
</body>
</html>