<html>
<head>
<title>3D Visualization of K-means Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">K均值聚类的三维可视化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/3d-visualization-of-k-means-clustering-47d3d3e82117?source=collection_archive---------0-----------------------#2018-11-07">https://medium.com/analytics-vidhya/3d-visualization-of-k-means-clustering-47d3d3e82117?source=collection_archive---------0-----------------------#2018-11-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8e77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上一篇文章中，我解释了如何为K-Means聚类选择最佳K值。由于帖子的主要目的不是介绍K-means的实现，所以我使用了sklearn库的内置函数，并认为读者已经知道这个算法是什么。</p><p id="c594" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是今天我想重点说一下实现部分，做一个关于K-means的教程。我们将编写自定义函数来计算数据点和聚类之间的距离，并在每次迭代后更新聚类中心。最后，我们将通过3D图形来可视化我们的集群，并得到一个非常酷的图片。所以，如果你准备好了，那就开始吧！</p><p id="4290" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本教程中，我们将像上一课一样使用Iris数据集。我们已经知道，虹膜数据集包含3种不同类型的花和每种花的4个特征。但是在本教程中我们将只使用3个特征，因为我们不能想象一个四维空间。因此，选择3个随机的聚类中心作为起点是一个明智的想法。</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="19a1" class="jm jn hi ji b fi jo jp l jq jr">import numpy as np<br/>from sklearn import datasets<br/>import random<br/>iris=datasets.load_iris()<br/>x=iris.data<br/>target=iris.target<br/>min_of_features=np.zeros((1,3))<br/>max_of_features=np.zeros((1,3))<br/>for i in range(3):<br/>    min_of_features[0,i]=min(x[:,i])<br/>    max_of_features[0,i]=max(x[:,i])</span><span id="9515" class="jm jn hi ji b fi js jp l jq jr">cluster_centers=np.zeros((3,3))<br/>for i in range(3):<br/>    for j in range(3):<br/>        cluster_centers[i,j]=round(random.uniform(min_of_features[0,j],max_of_features[0,j]),3)</span></pre><p id="f2f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“Cluster_centers”是一个3×3矩阵，包括3个中心和每个中心的3个维度。我找到了每个特征的最小值和最大值，并在这些值之间分配了一个随机值(为了接近数据点)。</p><p id="4fac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我们已经分配了聚类中心，所以我们可以开始计算数据点和每个聚类中心之间的距离。我们有3个聚类中心，因此，每个数据点有3个距离值。对于聚类，我们必须选择最近的中心，并将相关数据点分配给该中心。让我们看看这部分的代码:</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="db55" class="jm jn hi ji b fi jo jp l jq jr">#a  is the row vector including 4 features(only 3 will be used)<br/>#b is a 3x3 matrix containing cluster centers</span><span id="1ec2" class="jm jn hi ji b fi js jp l jq jr">def <strong class="ji hj">distance_find</strong>(a,b):<br/>    total_1=np.square(a[0]-b[0,0])+np.square(a[1]-b[0,1])+np.square(a[2]-b[0,2])<br/>    <br/>total_2=np.square(a[0]-b[1,0])+np.square(a[1]-b[1,1])+np.square(a[2]-b[1,2])<br/>    <br/>total_3=np.square(a[0]-b[2,0])+np.square(a[1]-b[2,1])+np.square(a[2]-b[2,2])<br/>    <br/>vec=np.array([total_1,total_2,total_3])<br/>    if min(vec)==total_1:<br/>        return 0<br/>    elif min(vec)==total_2:<br/>        return 1<br/>    elif min(vec)==total_3:<br/>        return 2</span></pre><p id="9d43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的代码为每个数据点找到3个距离值，并在最后对它们进行比较。例如，如果“total_1”是3个距离值中的最小值，我们推断第一个分类是最接近的，并返回零。</p><p id="8235" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们完成了距离测量部分，但还没有完成。因为我们从随机的聚类中心开始，我们不期望我们的聚类是正确的。我们需要继续更新集群中心，直到系统稳定，集群中没有发生变化。</p><p id="2e44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更新聚类中心意味着取每个聚类的平均值，并将该值指定为新的中心。平均有助于我们缩小聚类的范围，并消除异常值或噪声。因此，我们将迫使我们的集群尽可能相互靠近，从而拥有更好的群组。</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="2cf3" class="jm jn hi ji b fi jo jp l jq jr">def mean_finder():<br/>    cluster_new=np.zeros((3,3))<br/>    for i in range(3):<br/>        number_of_elements=sum(cluster_labels==i)<br/>        for j in range(3):<br/>            total=0<br/>            for z in range(len(cluster_labels)):<br/>                if cluster_labels[z]==i:<br/>                    total=total+x[z,j]<br/>                else:<br/>                    total=total<br/>            cluster_new[i,j]=round(total/(number_of_elements[0]+0.001),4)<br/>    return cluster_new</span></pre><p id="b4cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面，我们编写了更新聚类中心的函数。它基于4个特征对每个聚类中的数据点进行平均，并返回一个新的3x4矩阵。这些中心应该比以前的更好，因为我们降低了随机性水平。我想指出的是，在下面的代码中，我在右边表达式的分母上加了0.001，以避免元素个数为零时出现NaN值。("<strong class="ih hj"><em class="jt">)cluster _ new[I，j]= round(total/(number _ of _ elements[0]+0.001)，4) </em> </strong>))</p><p id="5326" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然我们的函数已经准备好了，我们可以开始运行代码并观察结果:</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="9614" class="jm jn hi ji b fi jo jp l jq jr">cluster_labels=np.zeros((len(x),1))</span><span id="40bc" class="jm jn hi ji b fi js jp l jq jr">for iteration in range(15):<br/>    for i in range(len(x)):<br/>        row=x[i,:]<br/>        cluster_labels[i]=distance_find(row,cluster_centers)<br/>    cluster_centers=mean_finder()</span></pre><p id="5b7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">仅此而已！我将迭代次数设置为15，我希望看到一个好的结果，因为我们有一个小的数据集(150行和4列)。现在，我们准备好查看我们的3D图形。让我们来看看该图在聚类之前的样子，以便比较我们的模型的性能:</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="d2a3" class="jm jn hi ji b fi jo jp l jq jr">fig=plt.figure()<br/>ax=Axes3D(fig)<br/>ax.scatter(x[:50,0],x[:50,1],x[:50,2],color='red')<br/>ax.scatter(x[50:100,0],x[50:100,1],x[50:100,2],color='green')<br/>ax.scatter(x[100:150,0],x[100:150,1],x[100:150,2],color='blue')<br/>plt.show()</span></pre><figure class="jd je jf jg fd jv er es paragraph-image"><div class="er es ju"><img src="../Images/4c5cad7c0fb4dba577951d2e6ee9b556.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*w93DQFxxij3OtTnx5IJLzg.png"/></div></figure><p id="94bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看群集后的情况:</p><pre class="jd je jf jg fd jh ji jj jk aw jl bi"><span id="93e0" class="jm jn hi ji b fi jo jp l jq jr">cluster_labels2=np.array(cluster_labels)<br/>cluster_labels2=np.zeros(len(x))<br/>cluster_labels2[:]=cluster_labels[:,0]<br/>fig=plt.figure()<br/>ax=Axes3D(fig)</span><span id="ad95" class="jm jn hi ji b fi js jp l jq jr">ax.scatter(x[cluster_labels2==0,0],x[cluster_labels2==0,1],x[cluster_labels2==0,2],color='red')</span><span id="55f8" class="jm jn hi ji b fi js jp l jq jr">ax.scatter(cluster_centers[0,0],cluster_centers[0,1],cluster_centers[0,2],color='red',marker='o',s=120)</span><span id="70d0" class="jm jn hi ji b fi js jp l jq jr">ax.scatter(x[cluster_labels2==2,0],x[cluster_labels2==2,1],x[cluster_labels2==2,2],color='green')</span><span id="f4a2" class="jm jn hi ji b fi js jp l jq jr">ax.scatter(cluster_centers[2,0],cluster_centers[2,1],cluster_centers[2,2],color='green',marker='o',s=120)</span><span id="8c22" class="jm jn hi ji b fi js jp l jq jr">ax.scatter(x[cluster_labels2==1,0],x[cluster_labels2==1,1],x[cluster_labels2==1,2],color='blue')</span><span id="bcc1" class="jm jn hi ji b fi js jp l jq jr">ax.scatter(cluster_centers[1,0],cluster_centers[1,1],cluster_centers[1,2],color='blue',marker='o',s=120)</span><span id="a582" class="jm jn hi ji b fi js jp l jq jr">plt.show()</span></pre><figure class="jd je jf jg fd jv er es paragraph-image"><div class="er es jy"><img src="../Images/e62b7e3acbc4b8662556274f37e6815f.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*AvHZX7cPRdMYUmeJydnBCg.png"/></div></figure><p id="93ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你所看到的，两个图表非常相似。但是有一些不同之处需要强调。例如，箭头所指的绿点最初是蓝色的。然而，它和绿点聚集在一起。原因很简单。由于该点距离绿色聚类中心比距离蓝色聚类中心更近，因此被归类为绿色点。顺便说一下，每个星团中间较大的球代表星团中心。因此，我们可以看到，聚类中心是这样一个点，它能够携带整个集群的平均特征。</p><p id="6ae8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望您喜欢本教程，并提高您对K均值聚类的理解。下一篇文章再见！</p></div></div>    
</body>
</html>