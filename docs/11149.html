<html>
<head>
<title>Feature Selection: Filter Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择:过滤方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-selection-73bc12a9b39e?source=collection_archive---------5-----------------------#2020-11-20">https://medium.com/analytics-vidhya/feature-selection-73bc12a9b39e?source=collection_archive---------5-----------------------#2020-11-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="68a8" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">权威指南</h2><div class=""/><div class=""><h2 id="a3b4" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">4 种基于过滤器的方法来选择相关特征</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/4cc410c9184bae03e96a6c0af7889548.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hel9Dh46AibbOH1fxKNaoQ.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">照片由<a class="ae jw" href="https://unsplash.com/@fahrulazmi" rel="noopener ugc nofollow" target="_blank">法鲁尔·阿兹米</a>拍摄</figcaption></figure><h2 id="3588" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">目录</h2><ul class=""><li id="2051" class="ku kv hi kw b kx ky kz la ki lb km lc kq ld le lf lg lh li bi translated"><a class="ae jw" href="#bf65" rel="noopener ugc nofollow">什么，什么时候&amp;为什么</a></li><li id="9219" class="ku kv hi kw b kx lj kz lk ki ll km lm kq ln le lf lg lh li bi translated"><a class="ae jw" href="#1a92" rel="noopener ugc nofollow">滤波方法</a> <br/> - <a class="ae jw" href="#f29a" rel="noopener ugc nofollow">皮尔逊相关</a><br/>-<a class="ae jw" href="#6ba8" rel="noopener ugc nofollow">LDA</a>-<br/>-<a class="ae jw" href="#1ce6" rel="noopener ugc nofollow">ANOVA</a><br/>-<a class="ae jw" href="#dd6f" rel="noopener ugc nofollow">卡方</a></li><li id="1298" class="ku kv hi kw b kx lj kz lk ki ll km lm kq ln le lf lg lh li bi translated">非线性关系呢？</li><li id="b658" class="ku kv hi kw b kx lj kz lk ki ll km lm kq ln le lf lg lh li bi translated"><a class="ae jw" href="#3260" rel="noopener ugc nofollow"> PCA 不是特征选择</a></li><li id="d719" class="ku kv hi kw b kx lj kz lk ki ll km lm kq ln le lf lg lh li bi translated"><a class="ae jw" href="#16d4" rel="noopener ugc nofollow">功能选择#不</a></li></ul><p id="054f" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">特征选择是面试时非常受欢迎的问题；不考虑 ML 域。这篇文章是关于特性选择的博客系列的一部分。看看 Wrapper(<a class="ae jw" rel="noopener" href="/analytics-vidhya/feature-selection-85539d6a2a88"><em class="md">part 2</em></a><em class="md">)和 Embedded(</em><a class="ae jw" href="https://tzinie.medium.com/feature-selection-embedded-methods-a7940036973f" rel="noopener"><em class="md">part 3</em></a><em class="md">)方法。</em></p></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><h1 id="bf65" class="ml jy hi bd jz mm mn mo kd mp mq mr kh ix ms iy kl ja mt jb kp jd mu je kt mv bi translated">什么，什么时候，为什么</h1><p id="fdc8" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mw lv lw km mx ly lz kq my mb mc le hb bi translated">你熟悉<a class="ae jw" href="https://en.wikipedia.org/wiki/Iris_flower_data_set" rel="noopener ugc nofollow" target="_blank">鸢尾花数据集</a>吗？即使是现存的最简单的算法也能产生如此美妙的结果，这难道不令人惊讶吗？</p><p id="9ac8" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">嗯…很抱歉让你失望了，但这是不现实的。大多数情况下，特征的数量(p)比样本的数量(N)多得多(p &gt; &gt; N)——这也被称为<a class="ae jw" href="https://towardsdatascience.com/the-curse-of-dimensionality-minus-the-curse-of-jargon-520da109fc87" rel="noopener" target="_blank"> <em class="md">维数灾难</em> </a> <em class="md">。但是，为什么这是一个问题呢？</em></p><p id="d274" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">高维数据可能导致以下情况:</p><ul class=""><li id="30d0" class="ku kv hi kw b kx lq kz ls ki mz km na kq nb le lf lg lh li bi translated">训练时间长</li><li id="5dbf" class="ku kv hi kw b kx lj kz lk ki ll km lm kq ln le lf lg lh li bi translated">过度拟合</li></ul><p id="e647" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">即使 p&gt;&gt;N 不是这样，也有一长串的机器学习算法做出了自变量的假设。应用特征选择方法将移除相关特征。此外，将特征空间的维度减少到相关特征的子集将降低训练的计算成本，并且可以提高模型的泛化性能。</p><p id="4929" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated"><strong class="kw hs">特征选择</strong>是从数据集中移除不相关和冗余特征的过程。反过来，模型的复杂性将会降低，从而更容易解释。</p><blockquote class="nc"><p id="7e2e" class="nd ne hi bd nf ng nh ni nj nk nl le dx translated">“有时候，越少越好！”</p><p id="50e1" class="nd ne hi bd nf ng nh ni nj nk nl le dx translated">—罗汉·拉奥</p></blockquote></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><h1 id="f020" class="ml jy hi bd jz mm mn mo kd mp mq mr kh ix ms iy kl ja mt jb kp jd mu je kt mv bi translated"><strong class="ak">过滤方法</strong></h1><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nm"><img src="../Images/98146f9b9e9df9a66e1a65aadda41372.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*vmr6LePa6XsaPLY-yxEEgg.jpeg"/></div></figure><p id="a75a" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">基于特征与目标变量的关系来选择特征的子集。该选择不依赖于任何机器学习算法。相反，过滤方法通过统计测试测量特征与输出的<em class="md">【相关性】</em>。可以参考下表:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nn"><img src="../Images/b7ee09013d63360cfcd4b71be02cdab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePPP9vWG0kPW6Cj2B1dHmw.png"/></div></div></figure><h2 id="f29a" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">皮尔逊相关</h2><p id="a2f0" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mw lv lw km mx ly lz kq my mb mc le hb bi translated">衡量两个连续变量之间线性相关性的统计量。它从-1 到+1 不等，其中+1 对应正线性相关，0 对应无线性相关，1 对应负线性相关。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es no"><img src="../Images/6c21bc32db91d53cc1ec8d86e7d690c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/0*4zXn6kx-nuYRnfyv.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">皮松河</figcaption></figure><p id="534c" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated"><strong class="kw hs">数据集</strong>:波士顿房价<a class="ae jw" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html" rel="noopener ugc nofollow" target="_blank">数据集</a>。它包括 13 个连续特征和以千美元为单位的自有住房中值(目标变量)。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es np"><img src="../Images/ab4c4f7897dfef89827d6bfa443aa6f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/0*RhgIswVy79VvgHZ7.png"/></div></figure><pre class="jh ji jj jk fd nq nr ns nt aw nu bi"><span id="60d0" class="jx jy hi nr b fi nv nw l nx ny">import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from sklearn.datasets import load_boston</span><span id="6b0e" class="jx jy hi nr b fi nz nw l nx ny">X, y = load_boston(return_X_y=<strong class="nr hs">True</strong>)<br/>feature_names = load_boston().feature_names</span><span id="9296" class="jx jy hi nr b fi nz nw l nx ny">data = pd.DataFrame(X, columns=feature_names)<br/>data['MEDV'] = y</span><span id="004d" class="jx jy hi nr b fi nz nw l nx ny"># compute pearson's r<br/>target_correlation = data.corr()[['MEDV']]</span><span id="c915" class="jx jy hi nr b fi nz nw l nx ny"># we only care about the target variable<br/>plt.figure(figsize=(7,5))<br/>sns.heatmap(target_correlation, annot=True, cmap=plt.cm.Reds)<br/>plt.show()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es oa"><img src="../Images/6c193f7d98e91b9ff6cdb552322c734f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*zXE3ox91_tuGSGHSsuLX2g.png"/></div></figure><pre class="jh ji jj jk fd nq nr ns nt aw nu bi"><span id="cb7f" class="jx jy hi nr b fi nv nw l nx ny"># extract the most correlated features with the output variable<br/>target_correlation[abs(target_correlation)&gt;0.5].dropna()</span></pre><p id="9e1b" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated"><strong class="kw hs">相关性</strong>系数的幅度在<strong class="kw hs"> 0.5 </strong>和 0.7 之间，表示可以认为是中度相关的变量，因此我们将阈值设置为 0.5。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ob"><img src="../Images/337d21870428de04aa3aab04f7783334.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b-zSxD8lW9QEXp4An4sI5w.png"/></div></div></figure><p id="24d5" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">在 13 个特征中，只有 3 个与目标高度相关(<em class="md">相关特征</em>)；RM、PTRATIO 和 LSTAT。然而，我们只检查了每个特性与输出变量的相关性。由于许多算法，如线性回归，假设输入特征是不相关的，我们必须计算前 3 个特征之间的皮尔逊 r。</p><pre class="jh ji jj jk fd nq nr ns nt aw nu bi"><span id="169e" class="jx jy hi nr b fi nv nw l nx ny">sns.heatmap(data.corr().loc[['RM', 'PTRATIO', 'LSTAT'], ['RM', 'PTRATIO', 'LSTAT']], annot=True, cmap=plt.cm.Reds)<br/>plt.show()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es oc"><img src="../Images/55afe7ed8715949b34e905f7c1cb8fc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*ADWOpTsFKojHdYlkRI9H0g.png"/></div></figure><p id="0e88" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">RM 和 LSTAT 相互关联，因此我们选择其中之一(删除 ie RM 相当于删除<em class="md">冗余特征</em>)。因为 LSTAT 与目标变量 MEDV 之间的相关性高于与 RM 之间的相关性，所以我们选择 LSTAT。</p><h2 id="6ba8" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">皱胃向左移</h2><p id="0712" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mw lv lw km mx ly lz kq my mb mc le hb bi translated">线性判别分析是一种受监督的线性算法，它将数据投影到一个更小的子空间 k (k &lt; N-1) while maximising the separation between the classes. More specifically, the model finds linear combinations of the features that achieve maximum separability between the classes and minimum variance within each class.</p><p id="1f5c" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated"><strong class="kw hs">数据集</strong> : <a class="ae jw" href="https://www.kaggle.com/uciml/breast-cancer-wisconsin-data" rel="noopener ugc nofollow" target="_blank">乳腺癌威斯康星(诊断)数据集</a>，其中包括 569 条记录，每条记录由 30 个特征描述。任务是将肿瘤分类为恶性或良性。</p><pre class="jh ji jj jk fd nq nr ns nt aw nu bi"><span id="df26" class="jx jy hi nr b fi nv nw l nx ny">import pandas as pd<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.preprocessing import LabelEncoder<br/>from sklearn.discriminant_analysis import LinearDiscriminantAnalysis<br/>from sklearn.model_selection import StratifiedKFold, cross_val_score<br/>from sklearn.pipeline import Pipeline</span><span id="a44d" class="jx jy hi nr b fi nz nw l nx ny">df = pd.read_csv('breast_cancer.csv').iloc[:,1:-1]<br/>X = df.drop(['diagnosis'], axis=1)</span><span id="1a9d" class="jx jy hi nr b fi nz nw l nx ny">le = LabelEncoder()<br/>y = le.fit_transform(df.diagnosis)<br/>labels = le.classes_</span><span id="8e62" class="jx jy hi nr b fi nz nw l nx ny">steps = [('lda', LinearDiscriminantAnalysis()), ('m', LogisticRegression(C=10))]<br/>model = Pipeline(steps=steps)</span><span id="85b1" class="jx jy hi nr b fi nz nw l nx ny"># evaluate model<br/>cv = StratifiedKFold(n_splits=5)<br/>n_scores_lda = cross_val_score(model, X, y, scoring='f1_macro', cv=cv, n_jobs=-1)</span><span id="39ef" class="jx jy hi nr b fi nz nw l nx ny">model = LogisticRegression(C=10)<br/>n_scores = cross_val_score(model, X, y, scoring='f1_macro', cv=cv, n_jobs=-1)</span><span id="da6f" class="jx jy hi nr b fi nz nw l nx ny"># report performance<br/>print('f1-score (macro)\n')<br/>print('With LDA: %.2f' % np.mean(n_scores_lda))<br/>print('Without LDA: %.2f' % np.mean(n_scores))</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es od"><img src="../Images/8b0e50de6cc8b07848e3628b31e24169.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-uZ_z3krHi9v0IfRNgGn8A.png"/></div></div></figure><p id="f511" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">使用 LDA 作为预处理步骤，性能提高了 4%。</p><h2 id="1ce6" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">方差分析</h2><p id="7519" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mw lv lw km mx ly lz kq my mb mc le hb bi translated">方差分析是一种统计方法，它检验不同的输入类别对于输出变量是否有显著不同的值。<code class="du oe of og nr b">sklearn</code>中的<code class="du oe of og nr b">f_classif</code>方法允许对多组数据进行分析，以确定样本之间和样本内部的可变性，从而获得关于因变量和自变量之间关系的信息(<a class="ae jw" href="http://www.mit.edu/~6.s085/notes/lecture6.pdf" rel="noopener ugc nofollow" target="_blank">了解更多</a>)。例如，我们可能想测试两个过程，看哪一个在收益方面比另一个执行得更好。</p><pre class="jh ji jj jk fd nq nr ns nt aw nu bi"><span id="0795" class="jx jy hi nr b fi nv nw l nx ny">from sklearn.feature_selection import f_classif, SelectKBest</span><span id="9cbf" class="jx jy hi nr b fi nz nw l nx ny">fs = SelectKBest(score_func=f_classif, k=5)<br/><br/>X_new = fs.fit(X, y)</span></pre><blockquote class="oh oi oj"><p id="76e9" class="lo lp md kw b kx lq is lr kz ls iv lt ok lu lv lw ol lx ly lz om ma mb mc le hb bi translated">注意:之前，我们只是设置 k=5。如果不是 5，是 4 呢？我们可以通过使用 k-fold 交叉验证执行网格搜索来微调所选特征的数量</p></blockquote><pre class="jh ji jj jk fd nq nr ns nt aw nu bi"><span id="3a77" class="jx jy hi nr b fi nv nw l nx ny">from sklearn.model_selection import StratifiedKFold, GridSearch<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.linear_model import LinearRegression</span><span id="8e03" class="jx jy hi nr b fi nz nw l nx ny">cv = StratifiedKFold(n_splits=5)</span><span id="e7c8" class="jx jy hi nr b fi nz nw l nx ny">pipeline = Pipeline(steps=[('anova',fs), ('lr', LinearRegression(solver='liblinear'))])<br/>params = {['anova__k']: [i+1 for i in range(X.shape[1])]}</span><span id="3f61" class="jx jy hi nr b fi nz nw l nx ny">search = GridSearchCV(pipeline, params, scoring='accuracy', n_jobs=-1, cv=cv)</span><span id="4e98" class="jx jy hi nr b fi nz nw l nx ny">results = search.fit(X, y)<br/>print('Best k: %s' % results.best_params_)</span></pre><h2 id="dd6f" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi">χ²</h2><p id="96a2" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mw lv lw km mx ly lz kq my mb mc le hb bi translated">卡方检验使用频率分布来检验特定特征和特定类的出现是否独立。无效假设是这两个变量是独立的。然而，较大的χ值表明应该拒绝零假设。在选择特征时，我们希望提取那些高度依赖于输出的特征。</p><p id="c3f8" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated"><strong class="kw hs">数据集</strong> : <a class="ae jw" href="https://www.kaggle.com/burak3ergun/loan-data-set" rel="noopener ugc nofollow" target="_blank">梦想房屋融资</a>一家公司经营所有房屋贷款，希望自动化贷款资格流程。该数据集包含 11 个描述客户特征的分类和数字特征。目标变量是二元的——客户是否有资格获得贷款。</p><pre class="jh ji jj jk fd nq nr ns nt aw nu bi"><span id="58e8" class="jx jy hi nr b fi nv nw l nx ny">from sklearn.feature_selection import chi2, SelectKBest</span><span id="4c07" class="jx jy hi nr b fi nz nw l nx ny">loan = pd.read_csv('loan_data_set.csv')<br/>loan = loan.drop('Loan_ID', axis=1) # irrelevant feature</span><span id="7b5c" class="jx jy hi nr b fi nz nw l nx ny">#Transform the numerical feature into categorical feature<br/>loan['Loan_Amount_Term'] = loan['Loan_Amount_Term'].astype('object')<br/>loan['Credit_History'] = loan['Credit_History'].astype('object')</span><span id="ea48" class="jx jy hi nr b fi nz nw l nx ny">#Dropping all the null value<br/>loan.dropna(inplace = True)</span><span id="ff4f" class="jx jy hi nr b fi nz nw l nx ny">#Retrieve all the categorical columns except the target<br/>categorical_columns = loan.select_dtypes(exclude='number').drop('Loan_Status', axis=1).columns</span><span id="499f" class="jx jy hi nr b fi nz nw l nx ny">X = loan[categorical_columns].apply(LabelEncoder().fit_transform)<br/>y = LabelEncoder().fit_transform(loan['Loan_Status'])</span><span id="807f" class="jx jy hi nr b fi nz nw l nx ny">fs = SelectKBest(score_func=chi2, k=5)<br/>X_kbest = fs.fit_transform(X, y)</span></pre></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><h2 id="20cf" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">非线性关系呢？</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es on"><img src="../Images/1a6b567aff60e44967b6f050cdff2da1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*BJLL_dPhcYvdC37E.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><a class="ae jw" href="https://www.freecodecamp.org/news/how-machines-make-predictions-finding-correlations-in-complex-data-dfd9f0d87889/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="c628" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">到目前为止，我们一直在讨论假设两个元素 X，y 之间存在线性关系的方法。这些方法无法捕捉除此之外的任何关系。为了解决这个问题，我们可以看一下特征和目标变量之间的<strong class="kw hs">互信息<em class="md"/></strong>(MI)<strong class="kw hs"><em class="md"/></strong>。MI 的范围从 0(无互信息)到 1(完全相关)。Sklearn 提供了回归和分类任务的实现。</p><pre class="jh ji jj jk fd nq nr ns nt aw nu bi"><span id="f24e" class="jx jy hi nr b fi nv nw l nx ny">from sklearn.feature_selection import mutual_info_regression, mutual_info_classif, SelectKBest</span><span id="288d" class="jx jy hi nr b fi nz nw l nx ny">fs = SelectKBest(score_func=mutual_info_classif, k=5) # top 5 features<br/>X_subset = fs.fit_transform(X, y)</span></pre><p id="d76b" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">你可以在这里阅读更多关于捕捉两个变量之间非线性关系的其他方法<a class="ae jw" href="https://www.freecodecamp.org/news/how-machines-make-predictions-finding-correlations-in-complex-data-dfd9f0d87889/" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="3260" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">可以用主成分分析吗？</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es oo"><img src="../Images/6c1b852c3ff4f204ad687839b25ffa46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*y-ebAkRNutVYRj4V.gif"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><a class="ae jw" href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="4dad" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">你当然可以。但是请不要混淆特征提取和特征选择。PCA 是一种<strong class="kw hs">无监督</strong> <strong class="kw hs">线性变换技术</strong>。这是另一种降低维度的方法，但要小心，在这种方法中，我们不选择特征，而是通过将数据投影到一个低维空间来转换特征空间，同时保持最大方差。该技术产生不相关的变量(主成分),它们是旧变量的线性组合。不幸的是，您并不真正知道这些新特性代表了什么，所以尽管您在降维方面有所收获，但在可解释性方面肯定会有所损失。</p><blockquote class="oh oi oj"><p id="f1b3" class="lo lp md kw b kx lq is lr kz ls iv lt ok lu lv lw ol lx ly lz om ma mb mc le hb bi translated">注意:不要犯年轻 ML 从业者常犯的一个最常见的错误:对非连续特征应用 PCA。我知道当你对离散变量运行 PCA 时，代码不会中断，但这并不意味着你应该中断(<a class="ae jw" href="https://stackoverflow.com/questions/40795141/pca-for-categorical-features" rel="noopener ugc nofollow" target="_blank">简短解释</a>)。</p></blockquote><h2 id="16d4" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">功能选择#否</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es op"><img src="../Images/0f92830514810b46f48800a711889825.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*Y3tdvY64YUAAGAmE.gif"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><a class="ae jw" href="http://giphy.com" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="4972" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">虽然我们已经看到了很多进行特征选择的方法(还有更多；查 blog2，blog3)，总有答案<em class="md"/><strong class="kw hs"><em class="md">我不会做</em> </strong> <em class="md">。我知道这听起来可能很奇怪，尤其是当它来自这篇文章的作者时，但我需要给出所有可能的答案，这是其中之一。</em></p><p id="6201" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">特性选择需要时间，你可以考虑不投入时间和精力。你必须永远记住两件事:1。您将丢失信息，因为您正在删除特征和 2。即使您尝试了所有的技术，也可能看不到模型性能的重大改进。</p></div></div>    
</body>
</html>