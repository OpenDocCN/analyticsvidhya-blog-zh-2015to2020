# åˆ›å»ºæ‚¨è‡ªå·±çš„æ„å›¾åˆ†ç±»å™¨

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/creating-your-own-intent-classifier-b86e000a4926?source=collection_archive---------0----------------------->

![](img/613957715cd5e20a15fee88d0061e37e.png)

ä½œä¸º NLP çš„ç²‰ä¸ï¼Œæˆ‘æ€»æ˜¯æƒ³çŸ¥é“å½“æˆ‘è¦æ±‚è°·æ­ŒåŠ©æ‰‹æˆ– Alexa åšä¸€äº›äº‹æƒ…æ—¶ï¼Œå®ƒæ˜¯å¦‚ä½•ç†è§£çš„ã€‚é—®é¢˜æ¥ç€æ˜¯ï¼Œæˆ‘æ˜¯å¦èƒ½è®©æˆ‘çš„æœºå™¨ä¹Ÿç†è§£æˆ‘ï¼Ÿè§£å†³æ–¹æ³•æ˜¯-æ„å›¾åˆ†ç±»ã€‚

> æ„å›¾åˆ†ç±»æ˜¯è‡ªç„¶è¯­è¨€ç†è§£çš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä¸­æœºå™¨å­¦ä¹ /æ·±åº¦å­¦ä¹ ç®—æ³•å­¦ä¹ åœ¨å®ƒæ¥å—è®­ç»ƒçš„çŸ­è¯­çš„åŸºç¡€ä¸Šå¯¹ç»™å®šçŸ­è¯­è¿›è¡Œåˆ†ç±»ã€‚

è®©æˆ‘ä»¬ä¸¾ä¸€ä¸ªæœ‰è¶£çš„ä¾‹å­ï¼›æˆ‘åœ¨åšä¸€ä¸ªåƒ Alexa ä¸€æ ·çš„åŠ©æ‰‹ã€‚

![](img/85e950876c98c0c32457935b682ab9da.png)

ä¸ºç®€å•èµ·è§ï¼Œæˆ‘ä»¬å°†æ‰§è¡Œ 3 é¡¹ä»»åŠ¡ï¼Œå³å¼€ç¯ã€å…³ç¯å’Œå‘Šè¯‰æˆ‘ä»¬å¤©æ°”å¦‚ä½•ã€‚è®©æˆ‘ä»¬ç»™è¿™ä¸‰ä¸ªä»»åŠ¡èµ·ä¸ªåå­—:å¼€ç¯ã€å…³ç¯å’Œå¤©æ°”ã€‚è¿™äº›æ‰€æœ‰çš„ä»»åŠ¡åœ¨ NLU è¢«ç§°ä¸º*â€˜æ„å›¾â€™*ã€‚æ¢å¥è¯è¯´ï¼Œ*æ„å›¾æ˜¯ä¸€ç»„å±äºå…±åŒåç§°çš„ç›¸ä¼¼çŸ­è¯­ï¼Œè¿™æ ·æ·±åº¦å­¦ä¹ ç®—æ³•å°±å¾ˆå®¹æ˜“ç†è§£ç”¨æˆ·è¦è¯´çš„è¯*ã€‚æ¯ä¸ªæ„å›¾è¢«ç»™äºˆä¸€å®šæ•°é‡çš„è®­ç»ƒçŸ­è¯­ï¼Œä»¥ä¾¿å®ƒèƒ½å¤Ÿå­¦ä¹ å¯¹å®æ—¶çŸ­è¯­è¿›è¡Œåˆ†ç±»ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬çŸ¥é“ä»€ä¹ˆæ˜¯æ„å›¾åˆ†ç±»ï¼Œè®©æˆ‘ä»¬å¼€å§‹é…·çš„ä¸œè¥¿ï¼æˆ‘å†™äº†ä¸€ä¸ª[ç¬”è®°æœ¬](https://github.com/horizons-ml/intent-classifier/blob/main/intent_classification.ipynb)å¦‚æœä½ æƒ³è·Ÿç€æˆ‘ï¼Œä½ å¯ä»¥åœ¨æˆ‘çš„ [Github repo](https://github.com/horizons-ml/intent-classifier) è¿™é‡Œæ‰¾åˆ°ã€‚

ä¸ºæ–¹ä¾¿èµ·è§ï¼Œè®©æˆ‘ä»¬éµå¾ªä»¥ä¸‹ç›®å½•ç»“æ„:

```
Your directory
â”œâ”€â”€â”€models 
â”œâ”€â”€â”€utils
â””â”€â”€â”€intent_classification.ipynb
```

## å®‰è£…ä¾èµ–é¡¹

ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…æ‰€éœ€çš„ä¾èµ–é¡¹:

```
pip install wget tensorflow==1.5 pandas numpy keras
```

## èµ„æ–™ç»„

æˆ‘ä»¬å°†ä½¿ç”¨å…¬å¼€å¯ç”¨çš„ [CLINC150 æ•°æ®é›†](https://github.com/clinc/oos-eval)ã€‚å®ƒæ”¶é›†äº† 10 ä¸ªé¢†åŸŸä¸­ 150 ç§ä¸åŒæ„å›¾çš„çŸ­è¯­ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ›´å¤šå…³äºæ•°æ®é›†çš„ä¿¡æ¯ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹æ–¹å¼ä¸‹è½½æ•°æ®é›†:

```
**import** **wget**
url = 'https://raw.githubusercontent.com/clinc/oos-eval/master/data/data_full.json'
wget.download(url)
```

## å‡†å¤‡æ•°æ®é›†

æ•°æ®é›†å·²ç»åˆ†ä¸ºâ€œè®­ç»ƒâ€ã€â€œæµ‹è¯•â€å’Œâ€œéªŒè¯â€é›†ï¼Œä½†æˆ‘ä»¬å°†åˆ›å»ºè‡ªå·±çš„è®­ç»ƒå’ŒéªŒè¯é›†ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦æµ‹è¯•é›†ã€‚æˆ‘ä»¬å°†é€šè¿‡åˆå¹¶æ‰€æœ‰é›†åˆï¼Œç„¶åä½¿ç”¨ scikit-learn å°†å®ƒä»¬åˆ†æˆâ€œè®­ç»ƒâ€å’Œâ€œéªŒè¯â€é›†åˆæ¥å®ç°è¿™ä¸€ç‚¹ã€‚è¿™ä¹Ÿå°†åˆ›å»ºæ›´å¤šçš„è®­ç»ƒæ•°æ®ã€‚

```
**import** **numpy** **as** **np**
**import** **json** *# Loading json data*
**with** open('data_full.json') **as** file:
  data = json.loads(file.read())

*# Loading out-of-scope intent data*
val_oos = np.array(data['oos_val'])
train_oos = np.array(data['oos_train'])
test_oos = np.array(data['oos_test'])

*# Loading other intents data*
val_others = np.array(data['val'])
train_others = np.array(data['train'])
test_others = np.array(data['test'])

*# Merging out-of-scope and other intent data*
val = np.concatenate([val_oos,val_others])
train = np.concatenate([train_oos,train_others])
test = np.concatenate([test_oos,test_others])data = np.concatenate([train,test,val])
data = data.T

text = data[0]
labels = data[1]
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å†…å®¹åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯æ‹†åˆ†:

```
**from** **sklearn.model_selection** **import** train_test_splittrain_txt,test_txt,train_label,test_labels = train_test_split(text,labels,test_size = 0.3)
```

## æ•°æ®é›†é¢„å¤„ç†

ç”±äºæ·±åº¦å­¦ä¹ æ˜¯ä¸€ä¸ªæ•°å­—æ¸¸æˆï¼Œå®ƒå¸Œæœ›æˆ‘ä»¬çš„æ•°æ®æ˜¯æ•°å­—å½¢å¼çš„ã€‚æˆ‘ä»¬å°†æ ‡è®°æˆ‘ä»¬çš„æ•°æ®é›†ï¼›æ„æ€æ˜¯å°†å¥å­åˆ†è§£æˆä¸ªä½“ï¼Œå¹¶å°†è¿™äº›ä¸ªä½“è½¬æ¢æˆæ•°å­—è¡¨ç¤ºã€‚æˆ‘ä»¬å°†ä½¿ç”¨ K [æ—¶ä»£æ ‡è®°å™¨](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)æ¥æ ‡è®°æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ä»£ç çš„çŸ­è¯­:

```
**from** **tensorflow.python.keras.preprocessing.text** **import** Tokenizer
**from** **tensorflow.python.keras.preprocessing.sequence** **import** pad_sequencesmax_num_words = 40000
classes = np.unique(labels)

tokenizer = Tokenizer(num_words=max_num_words)
tokenizer.fit_on_texts(train_txt)
word_index = tokenizer.word_index
```

ä¸ºäº†å°†æˆ‘ä»¬çš„æ•°æ®è¾“å…¥æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬æ‰€æœ‰çš„çŸ­è¯­å¿…é¡»é•¿åº¦ç›¸åŒã€‚æˆ‘ä»¬å°†ç”¨ *0* å¡«å……æ‰€æœ‰çš„è®­ç»ƒçŸ­è¯­ï¼Œè¿™æ ·å®ƒä»¬çš„é•¿åº¦å°±ç›¸åŒäº†ã€‚

```
ls=[]
**for** c **in** train_txt:
    ls.append(len(c.split()))
maxLen=int(np.percentile(ls, 98))train_sequences = tokenizer.texts_to_sequences(train_txt)
train_sequences = pad_sequences(train_sequences, maxlen=maxLen,              padding='post')test_sequences = tokenizer.texts_to_sequences(test_txt)
test_sequences = pad_sequences(test_sequences, maxlen=maxLen, padding='post')
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦å°†æ ‡ç­¾è½¬æ¢æˆç‹¬çƒ­ç¼–ç å½¢å¼ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ›´å¤šå…³äºä¸€é”®ç¼–ç [çš„ä¿¡æ¯ã€‚](https://victorzhou.com/blog/one-hot/)

```
**from** **sklearn.preprocessing** **import** OneHotEncoder,LabelEncoder

label_encoder = LabelEncoder()
integer_encoded = label_encoder.fit_transform(classes)

onehot_encoder = OneHotEncoder(sparse=**False**)
integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
onehot_encoder.fit(integer_encoded)train_label_encoded = label_encoder.transform(train_label)
train_label_encoded = train_label_encoded.reshape(len(train_label_encoded), 1)
train_label = onehot_encoder.transform(train_label_encoded)test_labels_encoded = label_encoder.transform(test_labels)
test_labels_encoded = test_labels_encoded.reshape(len(test_labels_encoded), 1)
test_labels = onehot_encoder.transform(test_labels_encoded)
```

## åœ¨æˆ‘ä»¬åˆ›å»ºæ¨¡å‹ä¹‹å‰..

åœ¨æˆ‘ä»¬å¼€å§‹è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ä¹‹å‰ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[å…¨å±€å‘é‡](https://nlp.stanford.edu/projects/glove/)ã€‚æ‰‹å¥—æ˜¯ç”±æ–¯å¦ç¦å¤§å­¦åœ¨å¤§å‹è¯­æ–™åº“ä¸Šè®­ç»ƒçš„å•è¯çš„ N ç»´å‘é‡è¡¨ç¤ºã€‚ç”±äºå®ƒæ˜¯åœ¨å¤§å‹è¯­æ–™åº“ä¸Šè®­ç»ƒçš„ï¼Œå®ƒå°†å¸®åŠ©æ¨¡å‹æ›´å¥½åœ°å­¦ä¹ çŸ­è¯­ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹æ–¹å¼ä¸‹è½½ GloVe:

```
**import** **wget**
url ='https://www.dropbox.com/s/a247ju2qsczh0be/glove.6B.100d.txt?dl=1'
wget.download(url)
```

ä¸‹è½½å®Œæˆåï¼Œæˆ‘ä»¬ä¼šå°†å…¶å­˜å‚¨åœ¨ Python å­—å…¸ä¸­:

```
embeddings_index={}
**with** open('glove.6B.100d.txt', encoding='utf8') **as** f:
    **for** line **in** f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
```

å› ä¸º GloVe åŒ…å«æ¥è‡ªå¤§å‹è¯­æ–™åº“çš„æ‰€æœ‰å•è¯çš„å‘é‡è¡¨ç¤ºï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦é‚£äº›å‡ºç°åœ¨è¯­æ–™åº“ä¸­çš„å•è¯å‘é‡ã€‚æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªåµŒå…¥çŸ©é˜µï¼Œå®ƒåªåŒ…å«æ•°æ®é›†ä¸­å‡ºç°çš„å•è¯çš„å‘é‡è¡¨ç¤ºã€‚å› ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†å·²ç»è¢«æ ‡è®°åŒ–äº†ï¼Œæ‰€ä»¥ Keras æ ‡è®°åŒ–å™¨ä¸ºæ•°æ®é›†ä¸­çš„æ¯ä¸ªæ ‡è®°åˆ†é…äº†ä¸€ä¸ªå”¯ä¸€çš„ç¼–å·ã€‚è¿™ä¸ªå”¯ä¸€çš„æ•°å­—å¯ä»¥è¢«è®¤ä¸ºæ˜¯åµŒå…¥çŸ©é˜µä¸­æ¯ä¸ªå•è¯çš„å‘é‡çš„ç´¢å¼•ï¼›è¿™æ„å‘³ç€æ¥è‡ªè®°å·èµ‹äºˆå™¨çš„æ¯ä¸ªç¬¬*n*ä¸ªå­—ç”±åµŒå…¥çŸ©é˜µä¸­ç¬¬*n*ä¸ªä½ç½®å¤„çš„å‘é‡è¡¨ç¤ºã€‚

```
all_embs = np.stack(embeddings_index.values())
emb_mean,emb_std = all_embs.mean(), all_embs.std()num_words = min(max_num_words, len(word_index))+1embedding_dim=len(embeddings_index['the'])embedding_matrix = np.random.normal(emb_mean, emb_std, (num_words, embedding_dim))
**for** word, i **in** word_index.items():
    **if** i >= max_num_words:
        **break**
    embedding_vector = embeddings_index.get(word)
    **if** embedding_vector **is** **not** **None**:
        embedding_matrix[i] = embedding_vector
```

## æ¨¡å‹å‡†å¤‡

è®©æˆ‘ä»¬æŠŠæˆ‘ä»¬çš„æ¨¡å‹çš„æ¶æ„æ”¾åœ¨ä¸€èµ·ï¼Œçœ‹çœ‹æ¨¡å‹çš„è¿è¡Œæƒ…å†µã€‚

```
**from** **tensorflow.python.keras.models** **import** Sequential
**from** **tensorflow.python.keras.layers** **import** Dense, Input, Dropout, LSTM, Activation, Bidirectional,Embeddingmodel = Sequential()

model.add(Embedding(num_words, 100, trainable=**False**,input_length=train_sequences.shape[1], weights=[embedding_matrix]))
model.add(Bidirectional(LSTM(256, return_sequences=**True**, recurrent_dropout=0.1, dropout=0.1), 'concat'))
model.add(Dropout(0.3))
model.add(LSTM(256, return_sequences=**False**, recurrent_dropout=0.1, dropout=0.1))
model.add(Dropout(0.3))
model.add(Dense(50, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(classes.shape[0], activation='softmax'))model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])
```

æˆ‘ä»¬å°†åœ¨åµŒå…¥å±‚ä¸­ä¼ é€’åµŒå…¥çŸ©é˜µä½œä¸º*æƒé‡ã€‚*

## æ¨¡ç‰¹åŸ¹è®­

æœ€åæ˜¯è®­ç»ƒæ¨¡å‹çš„æ—¶é—´ã€‚

```
history = model.fit(train_sequences, train_label, epochs = 20,
          batch_size = 64, shuffle=**True**,
          validation_data=[test_sequences, test_labels])
```

è¿™å¤§çº¦éœ€è¦ä¸€ä¸ªå°æ—¶å·¦å³ï¼Œå–å†³äºæ‚¨çš„æœºå™¨ã€‚åŸ¹è®­å®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥å°†æŒ‡æ ‡æƒ³è±¡ä¸º:

```
**import** **matplotlib.pyplot** **as** **plt**
%matplotlib inlineplt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
```

![](img/1a44b0bdebdb86af58725b5f9539754d.png)

æ¨¡å‹ç²¾åº¦æ›²çº¿

Wohooï¼ï¼æˆ‘ä»¬å¾—åˆ°äº† 92.45%çš„è®­ç»ƒå‡†ç¡®ç‡å’Œ 88.86%çš„éªŒè¯å‡†ç¡®ç‡ï¼Œè¿™æ˜¯ç›¸å½“ä¸é”™çš„ã€‚

è¿™æ˜¯æŸå¤±æ›²çº¿:

```
**import** **matplotlib.pyplot** **as** **plt**
%matplotlib inlineplt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
```

![](img/95166bff41235dcdbfbc92d8babf1335.png)

åŸ¹è®­æŸå¤±çº¦ä¸º 0.2ï¼ŒéªŒè¯æŸå¤±çº¦ä¸º 0.5ã€‚æ‚¨å¯ä»¥å°è¯•æ¨¡å‹æ¶æ„ï¼Œçœ‹çœ‹æŸå¤±æ˜¯å¦ä¼šä¸‹é™[ğŸ˜‰](https://emojipedia.org/winking-face/)

## ä¿å­˜æ¨¡å‹ã€æ ‡è®°å™¨ã€æ ‡ç­¾ç¼–ç å™¨å’Œæ ‡ç­¾

è®©æˆ‘ä»¬ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ã€æ ‡è®°å™¨ã€æ ‡ç­¾ç¼–ç å™¨å’Œæ ‡ç­¾ï¼Œä»¥ä¾¿åœ¨å°†æ¥çš„æ¡ˆä¾‹ä¸­ä½¿ç”¨å®ƒä»¬ã€‚

```
**import** **pickle**
**import** **json**model.save('models/intents.h5')

**with** open('utils/classes.pkl','wb') **as** file:
   pickle.dump(classes,file)

**with** open('utils/tokenizer.pkl','wb') **as** file:
   pickle.dump(tokenizer,file)

**with** open('utils/label_encoder.pkl','wb') **as** file:
   pickle.dump(label_encoder,file)
```

## æ˜¯æ—¶å€™çœ‹çœ‹æ‰€æœ‰çš„æ´»åŠ¨äº†

æˆ‘ä»¬ç»å†äº†æ¼«é•¿çš„æ—…ç¨‹..è®©æˆ‘ä»¬çœ‹çœ‹æœ€ç»ˆçš„ç›®çš„åœ°æ˜¯ä»€ä¹ˆæ ·çš„ã€‚

æˆ‘åˆ›å»ºäº†ä¸‹é¢çš„ç±»æ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹:

```
import numpy as np
from tensorflow.python.keras.preprocessing.sequence import pad_sequences**class** **IntentClassifier**:
    **def** __init__(self,classes,model,tokenizer,label_encoder):
        self.classes = classes
        self.classifier = model
        self.tokenizer = tokenizer
        self.label_encoder = label_encoder

    **def** get_intent(self,text):
        self.text = [text]
        self.test_keras = self.tokenizer.texts_to_sequences(self.text)
        self.test_keras_sequence = pad_sequences(self.test_keras, maxlen=16, padding='post')
        self.pred = self.classifier.predict(self.test_keras_sequence)
        **return** self.label_encoder.inverse_transform(np.argmax(self.pred,1))[0]
```

è¦ä½¿ç”¨è¯¥ç±»ï¼Œæˆ‘ä»¬åº”è¯¥é¦–å…ˆåŠ è½½æˆ‘ä»¬ä¿å­˜çš„æ–‡ä»¶:

```
**import** **pickle**

**from** **tensorflow.python.keras.models** **import** load_modelmodel = load_model('models/intents.h5')

**with** open('utils/classes.pkl','rb') **as** file:
  classes = pickle.load(file)

**with** open('utils/tokenizer.pkl','rb') **as** file:
  tokenizer = pickle.load(file)

**with** open('utils/label_encoder.pkl','rb') **as** file:
  label_encoder = pickle.load(file)
```

è€ƒè¯•æ—¶é—´åˆ°äº†ï¼ğŸ˜‹

```
nlu = IntentClassifier(classes,model,tokenizer,label_encoder)
print(nlu.get_intent("is it cold in India right now"))
*#* Prints 'weather'
```

![](img/8fbd245584752b4a1fd3fcbd62d4953e.png)

å°±æ˜¯è¿™æ ·ï¼Œä¼™è®¡ä»¬ï¼æ„Ÿè°¢æ‚¨çš„é˜…è¯»ğŸ˜ƒã€‚å¿«ä¹å­¦ä¹ ï¼