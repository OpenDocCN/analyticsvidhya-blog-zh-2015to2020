<html>
<head>
<title>Movie Tags Prediction Using Machine Learning Models.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用机器学习模型的电影标签预测。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/movie-tag-s-prediction-using-machine-learning-models-d5fde119db6d?source=collection_archive---------10-----------------------#2020-05-25">https://medium.com/analytics-vidhya/movie-tag-s-prediction-using-machine-learning-models-d5fde119db6d?source=collection_archive---------10-----------------------#2020-05-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/943622626ac192930efe325426e4e7be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U4_Y5S1rfjI3aAHlllu34g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:<a class="ae iu" href="https://www.researchgate.net/figure/Color-online-Word-Clouds-for-a-Childrens-and-b-Romantic-Movies_fig3_329106615" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/figure/Color-online-Word-Clouds-for-a-children-and-b-Romantic-Movies _ fig 3 _ 329106615</a></figcaption></figure><p id="9237" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">致敬Les gars！！！</em>T5】</strong></p><p id="019a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇博文中，我们将讨论如何解决多标签分类问题。</p><p id="886c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">预测电影的标签有助于我们找出电影的类型、情节结构、元数据和情感体验等信息。这些信息可以用于构建自动系统来预测电影的标签。</p><p id="7a45" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本案例研究中，我们将重点构建一个自动引擎，它可以从电影情节概要数据中提取标签。剧情梗概不过是一部电影的详细或部分概要。请注意，特定的电影可能只有一个标签，也可能有多个标签。这就是多标签分类发挥作用的地方。</p><p id="000d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">既然我们在讨论多标签分类问题，那就让我们来讨论多类和多标签问题之间的区别。</p><p id="0ca9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">多类分类</strong>指两个以上类的分类任务；例如对一组电影类型进行分类，这些电影类型可以是动作片、喜剧片或冒险片。多类别分类假设每个样本被分配给一个且只有一个标签:一个类型可以是动作片或喜剧片，但不能同时是两者。</p><p id="0106" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一方面，<strong class="ix hj">多标签分类</strong>给每个电影分配一组目标标签。例如对一组电影进行分类，这些电影可以是动作片、喜剧片、冒险片或动作片、喜剧片、恐怖片、惊悚片。一个标签可以同时涉及宗教、政治、金融或教育中的任何一个，也可以什么都不涉及。</p><p id="7b67" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可以阅读更多关于多标签和多类别分类的详细示例<a class="ae iu" href="https://scikit-learn.org/stable/modules/multiclass.html" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ju"><img src="../Images/9e40d7c041c3515691f79be671e4a5f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KwoCU9zbIAu530NAZQQQaw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在左侧，我们有一个二进制分类设置，可以将电子邮件分类为垃圾邮件或非垃圾邮件。中间的图片显示了一个多类分类设置，一个动物可以属于且只能属于一个类。在最右边，我们有一个多标签分类设置，其中图像中的对象可以属于一个或多个类别——“猫”和“鸟”。来源:<a class="ae iu" href="https://www.microsoft.com/en-us/research/uploads/prod/2017/12/40250.jpg" rel="noopener ugc nofollow" target="_blank">https://www . Microsoft . com/en-us/research/uploads/prod/2017/12/40250 . jpg</a></figcaption></figure><h1 id="8ef5" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">数据:</h1><p id="559a" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">我们将在这个博客中使用的数据来源于<a class="ae iu" href="https://www.kaggle.com/cryptexcode/mpst-movie-plot-synopses-with-tags" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>。该数据包含大约14K部电影和71个独特的标签。</p><h1 id="08ff" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">每个数据点都有以下属性。</h1><ol class=""><li id="7d26" class="lc ld hi ix b iy kx jc ky jg le jk lf jo lg js lh li lj lk bi translated"><strong class="ix hj"> IMDB_ID </strong>:包含IMDB分配的电影ID的唯一标识符。</li><li id="5a06" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated"><strong class="ix hj">片名</strong>:该片名属性包含电影名称。</li><li id="8331" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated"><strong class="ix hj">剧情概要</strong>:该字段包含电影剧情概要。</li><li id="80ae" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated"><strong class="ix hj">标签</strong>:该属性包含电影被分配到的所有标签的信息。</li><li id="f7ab" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated"><strong class="ix hj">分割</strong>:电影在标准数据分割中的位置，表示一个数据点是否属于训练、测试或验证数据</li><li id="de57" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated"><strong class="ix hj"> Synopsis_Source </strong>:每部电影的剧情概要的来源IMDB或维基百科。</li></ol><h1 id="b0a4" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">标签特征的探索性数据分析。</h1><p id="7bd9" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated"><strong class="ix hj"> 1。a .加载数据，显示前2行。</strong></p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="085f" class="lv ka hi lr b fi lw lx l ly lz">data = pd.read_csv(“mpst_full_data.csv”)<br/>print(“Number of data points: “, data.shape[0])<br/>data.head(2)</span></pre><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/0829c6715c5a604c0ae00584e05eb807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zqoZvDXLJB38_xOjcxMhNA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图1:数据集的前2行</figcaption></figure><p id="e3fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> b .让我们找出数据集中存在的数据点和属性的数量:</strong></p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="ae7a" class="lv ka hi lr b fi lw lx l ly lz">data.shape<br/>(14828, 6)</span></pre><p id="ce29" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，我们可以看到总共有14828个数据点/行。如上所示，每个数据点有6个属性/列。</p><h2 id="6eca" class="lv ka hi bd kb mb mc md kf me mf mg kj jg mh mi kn jk mj mk kr jo ml mm kv mn bi translated"><strong class="ak"> 2。从给定的CSV文件创建SQL db文件，并删除数据集中存在的重复条目。</strong></h2><p id="6522" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated"><strong class="ix hj">a .</strong>T22】创建db文件:</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="bece" class="lv ka hi lr b fi lw lx l ly lz">#Learn SQL: <a class="ae iu" href="https://www.w3schools.com/sql/default.asp" rel="noopener ugc nofollow" target="_blank">https://www.w3schools.com/sql/default.asp</a><br/>start = datetime.now()<br/>if not os.path.isfile('mpst.db'):<br/>    disk_engine = create_engine('sqlite:///mpst.db')<br/>    start = dt.datetime.now()<br/>    chunksize = 15000<br/>    j = 0<br/>    index_start = 1<br/>    for df in pd.read_csv('mpst_full_data.csv', chunksize=chunksize, iterator=True, encoding='utf-8'):<br/>        df.index += index_start<br/>        j+=1<br/>        df.to_sql('mpst_full_data', disk_engine, if_exists='append')<br/>        index_start = df.index[-1] + 1<br/>else:<br/>    print("Database Already Exist.")</span><span id="c5b1" class="lv ka hi lr b fi mo lx l ly lz">print("Time taken to run this cell :", datetime.now() - start)</span></pre><p id="9f4d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> b .删除数据集中存在的重复条目:</strong></p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="9056" class="lv ka hi lr b fi lw lx l ly lz">con = sqlite3.connect('mpst.db')</span><span id="d53d" class="lv ka hi lr b fi mo lx l ly lz">data_no_dup = pd.read_sql_query('SELECT title,plot_synopsis,tags,split,synopsis_source,COUNT(*) as cnt_dup FROM mpst_full_data GROUP BY title', con)</span><span id="bfb5" class="lv ka hi lr b fi mo lx l ly lz">con.close()</span></pre><p id="488d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们创建另一个名为“tag_count”的列，它将计算每部电影的标签数量。</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="f14a" class="lv ka hi lr b fi lw lx l ly lz">data_no_dup["tag_count"] = data_no_dup["tags"].apply(lambda text: len(str(text).split(", ")))<br/>data_no_dup.head()</span></pre><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/996d07762672bc00b483fdd1dfd03cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TyWlkl74i45JaTCZsyf5ig.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图2:带有新列“tag_count”的修改后的数据集</figcaption></figure><p id="4eb3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> d .让我们找出数据集中存在的数据点和属性的数量:</strong></p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="d6e0" class="lv ka hi lr b fi lw lx l ly lz">data_no_dup.shape<br/>(13757, 7)</span></pre><p id="252d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，我们可以看到总共有13757个数据点。如上所示，每个数据点有7个属性。(之前创建了6 + 1新自定义)</p><p id="5fd1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 3。列车、验证和测试数据点的分布</strong></p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="1caa" class="lv ka hi lr b fi lw lx l ly lz">sns.countplot(data_no_dup['split'])<br/>plt.show()</span></pre><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/1a44965765d2980062bcf9b80b788476.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*Yoi5nyIC704gxUeYybJLNg.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图3:数据点的分布</figcaption></figure><p id="761e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 4。检查数据分布的来源</strong></p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="3a3b" class="lv ka hi lr b fi lw lx l ly lz">sns.countplot(data_no_dup['synopsis_source'])<br/>plt.show()</span></pre><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/b143d682494a207c82356a7d172ef31c.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*E8A60LYFSFqv9bKah2JMvQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图4:不同来源的电影分布</figcaption></figure><p id="cd0d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 5。检查每部电影的标签分布:</strong></p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="95e5" class="lv ka hi lr b fi lw lx l ly lz">plt.figure(figsize=(20,5))<br/>plt.plot(data_no_dup["tag_count"])<br/>plt.xlabel('movies')<br/>plt.ylabel("no.of tags per movie")<br/>plt.show()</span></pre><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/ca4dbebfbefb596fea08b3a93c178d65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5lveOFnE4C7DX4ujuBVa1Q.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图5:每部电影的标签</figcaption></figure><p id="54cd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 6。找出确切的数字:</strong></p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="e0a6" class="lv ka hi lr b fi lw lx l ly lz">data_no_dup["tag_count"].value_counts()</span></pre><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/19627591474d04b86114d2cf41b35565.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*zj7JhT4XO0Im0XvgZZVOXQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图6:多少部电影包含多少个标签<strong class="bd kb">【左栏</strong>:标签数量，<strong class="bd kb">右栏</strong>:电影数量】</figcaption></figure><p id="64d5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 7。统计数据集中唯一标签的数量:</strong></p><p id="1564" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为此，我们通过应用<a class="ae iu" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">计数矢量器</a>方法来遵循BoW(单词包)技术。</p><p id="9432" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">什么是弓？</strong></p><p id="749e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它是文本文档的最简单表示。换句话说，它将找出特定单词在特定文档中出现的次数。</p><p id="9c29" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">更简单地说，我们可以说它会选择单词及其出现频率，然后将它们放入一个包中。因此它的名字叫做单词袋(Bow)</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="8c04" class="lv ka hi lr b fi lw lx l ly lz">vectorizer = CountVectorizer(preprocessor=lambda x: x,tokenizer = lambda x: str(x).split(", ") )<br/>tag_vect = vectorizer.fit_transform(data_no_dup["tags"])</span><span id="9b69" class="lv ka hi lr b fi mo lx l ly lz">print("Number of data points :", tag_vect.shape[0])<br/>print("Number of unique tags :", tag_vect.shape[1])</span><span id="0fc7" class="lv ka hi lr b fi mo lx l ly lz">Number of data points : 13757<br/>Number of unique tags : 71</span><span id="b5c5" class="lv ka hi lr b fi mo lx l ly lz">tags = vectorizer.get_feature_names()</span><span id="1be9" class="lv ka hi lr b fi mo lx l ly lz">#zipping tags and tags_count into one list<br/>freqs = tag_vect.sum(axis=0).A1<br/>result = list(zip(tags, freqs))</span></pre><p id="312c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 8。最常见标签的词云:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mu"><img src="../Images/72c62d0ccbe24757209790e3ed439b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ceCmuc8urYLzmucmSQ3sA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图7:出现最多的标签的Wordcloud</figcaption></figure><p id="dadc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上图我们可以得出结论:“谋杀”、“闪回”、“暴力”、“浪漫”、“迷幻”、“邪教”等标签出现最多。</p><p id="6ae6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">类似地，像“善恶”、“娱乐”、“悬疑”这样的标签属于较少出现的。</p><h1 id="b0df" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">数据清理</h1><p id="46f5" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">在本节中，我们将看到我们用来进行数据预处理的技术。</p><ol class=""><li id="e574" class="lc ld hi ix b iy iz jc jd jg mv jk mw jo mx js lh li lj lk bi translated">移除数据集中存在的任何HTML标记。</li><li id="ec6d" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">单词的去缩略(像不会=不会)。</li><li id="1f83" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">将每个单词转换成小写。</li><li id="c967" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">移除<a class="ae iu" href="https://en.wikipedia.org/wiki/Stop_words" rel="noopener ugc nofollow" target="_blank">停用词</a>。</li><li id="1da8" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">单词被<a class="ae iu" href="https://en.wikipedia.org/wiki/Lemmatisation" rel="noopener ugc nofollow" target="_blank">词条化</a>。(第三人称的单词被改为第一人称，过去将来时的动词被改为现在时)。</li><li id="8d97" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">最后滚雪球<a class="ae iu" href="https://en.wikipedia.org/wiki/Stemming" rel="noopener ugc nofollow" target="_blank">词干</a>。</li></ol><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="d02a" class="lv ka hi lr b fi lw lx l ly lz">def decontracted(phrase):<br/>    # specific<br/>    phrase = re.sub(r"won't", "will not", phrase)<br/>    phrase = re.sub(r"can\'t", "can not", phrase)</span><span id="aa20" class="lv ka hi lr b fi mo lx l ly lz"># general<br/>    phrase = re.sub(r"n\'t", " not", phrase)<br/>    phrase = re.sub(r"\'re", " are", phrase)<br/>    phrase = re.sub(r"\'s", " is", phrase)<br/>    phrase = re.sub(r"\'d", " would", phrase)<br/>    phrase = re.sub(r"\'ll", " will", phrase)<br/>    phrase = re.sub(r"\'t", " not", phrase)<br/>    phrase = re.sub(r"\'ve", " have", phrase)<br/>    phrase = re.sub(r"\'m", " am", phrase)<br/>    return phrase</span><span id="418c" class="lv ka hi lr b fi mo lx l ly lz"><br/>stopwords = set(stopwords.words('english'))<br/>sno = nltk.stem.SnowballStemmer('english')<br/>lemmatizer = WordNetLemmatizer()</span><span id="7505" class="lv ka hi lr b fi mo lx l ly lz"><br/>preprocessed_synop = []<br/>for sentance in tqdm(data_no_dup['plot_synopsis'].values):<br/>    sentance = re.sub(r"http\S+", "", sentance)<br/>    sentance = BeautifulSoup(sentance, 'lxml').get_text()<br/>    sentance = decontracted(sentance)<br/>    sentance = re.sub("\S*\d\S*", "", sentance).strip()<br/>    sentance = re.sub('[^A-Za-z]+', ' ', sentance)<br/>    stemmed_sentence = []<br/>    for e in sentance.split():<br/>        if e.lower() not in stopwords:<br/>            s=(sno.stem(lemmatizer.lemmatize(e.lower()))).encode('utf8')#lemitizing and stemming each word<br/>            stemmed_sentence.append(s)<br/>    sentance = b' '.join(stemmed_sentence)<br/>    preprocessed_synop.append(sentance)<br/>    <br/>data_no_dup['CleanedSynopsis'] = preprocessed_synop #adding a column of CleanedText which displays the data after pre-processing of the review <br/>data_no_dup['CleanedSynopsis'] = data_no_dup['CleanedSynopsis'].str.decode("utf-8")</span></pre><p id="135b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">原文:</em> </strong></p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="3795" class="lv ka hi lr b fi lw lx l ly lz">A 6th grader named Griffin Bing decides to gather their entire grade in a sleepover protest in an old house about to be demolished after their plan for using a new space in their town was thrown out because of their youth. However, only Griffin and his best friend Ben Slovak show up. Griffin discovers a Babe Ruth baseball card that, unbeknownst to him, is worth huge amounts of money. Excited that the card could help his family, which is struggling financially, Griffin takes it to the local collectibles dealer, S. Wendell Palomino. S. Wendell tells the boys that the card is an old counterfeit of a valuable one, worth only one hundred dollars. A dejected Griffin later chances upon Palomino on television, stating that the card he stole was worth at least a million dollars. Enraged, Griffin and Ben try to steal it back from Swindle\'s shop, only to find that it has gone, and they have to break into Swindle\'s house. Now, in order to get the card back, Griffin must gather a team of local students with unique skills to break into Palomino\'s heavily guarded home to retrieve the card before the big auction where Swindle plans to sell the card. The team consists of seven people (including Ben and Griffin): Savannah the Dog whisperer, to get past Swindle\'s massive, violent Guard Dog Luthor; Logan the actor, to distract Swindle\'s eagle-eyed neighbor who spends his days watching the entire street\'s goings-on; Antonia "Pitch" Benson the "born to climb" girl, to scale the skylight in Swindle\'s house; Darren Vader who the others had no choice but to add to the team, for he threatened to rat them out (But Darren proved to be useful pulling people up the skylight); Melissa the unsociable computer genius, who was used to break into Swindle\'s UltraTech alarm system. The tension is piled with an unexpected visit from the auctioneer, yet another even more menacing guard dog, and a betrayal from the person who begged to be in the group. The book was followed by multiple sequels, titled Zoobreak, Framed!, Showoff, Hideout , Jackpot and Unleashed.</span></pre><p id="685b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">已清理文本:</em> </strong></p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="376e" class="lv ka hi lr b fi lw lx l ly lz">grader name griffin bing decid gather entir grade sleepov protest old hous demolish plan use new space town thrown youth howev griffin best friend ben slovak show griffin discov babe ruth basebal card unbeknownst worth huge amount money excit card could help famili struggl financi griffin take local collect dealer wendel palomino wendel tell boy card old counterfeit valuabl one worth one hundr dollar deject griffin later chanc upon palomino televis state card stole worth least million dollar enrag griffin ben tri steal back swindl shop find gone break swindl hous order get card back griffin must gather team local student uniqu skill break palomino heavili guard home retriev card big auction swindl plan sell card team consist seven peopl includ ben griffin savannah dog whisper get past swindl massiv violent guard dog luthor logan actor distract swindl eagl eye neighbor spend day watch entir street go antonia pitch benson born climb girl scale skylight swindl hous darren vader other choic add team threaten rat darren prove use pull peopl skylight melissa unsoci comput genius use break swindl ultratech alarm system tension pile unexpect visit auction yet anoth even menac guard dog betray person beg group book follow multipl sequel titl zoobreak frame showoff hideout jackpot unleash</span></pre><h1 id="9808" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">预测电影标签的机器学习方法</h1><p id="7841" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">首先，我们必须从csv文件中读取数据，并使用数据集<strong class="ix hj">中给出的拆分列将数据拆分为train和test。</strong></p><p id="ce04" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">读取数据</strong></p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="63c2" class="lv ka hi lr b fi lw lx l ly lz">data_with_all_tags = pd.read_csv("/content/drive/My Drive/ML/data_with_all_tags.csv")<br/>data_with_all_tags.head()</span></pre><p id="cf0f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">拆分成列车并测试</strong></p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="7e25" class="lv ka hi lr b fi lw lx l ly lz">conn = sqlite3.connect('data.db')<br/>data_with_all_tags.to_sql('data', conn, if_exists='replace', index=False)<br/>train = pd.read_sql("Select * From data where split = 'train' OR split='val'",conn)<br/>test =  pd.read_sql("Select * From data where split = 'test'",conn)<br/>conn.close()</span></pre><p id="5f7e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们讨论一下我们将在模型中使用的一些术语。</p><p id="9960" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj">OneVSRestClassifier</strong></a><strong class="ix hj">:</strong>该策略包括为每个类安装一个分类器。对于每个分类器，该类与所有其他类相匹配。</p><p id="8b0e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">计数向量化</strong> </a> <strong class="ix hj"> : </strong>将文本文档的集合转换为令牌计数的矩阵</p><p id="19b7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> TfidfVectorizer </strong> </a>:给不太常用的词分配更多权重。简单来说，TFIDF是词频(TF)和逆文档频(IDF)的乘积。</p><p id="10d1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">TF =(术语t在文档中出现的次数)/(文档中的术语总数)</p><p id="26a8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">IDF = log(文档总数/其中包含术语t的文档数)</p><p id="46db" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">指标:</strong></p><blockquote class="my mz na"><p id="14f0" class="iv iw jt ix b iy iz ja jb jc jd je jf nb jh ji jj nc jl jm jn nd jp jq jr js hb bi translated">真阳性(TP) =所有正确预测的阳性点中，实际上是正确的。</p><p id="aecb" class="iv iw jt ix b iy iz ja jb jc jd je jf nb jh ji jj nc jl jm jn nd jp jq jr js hb bi translated">假阳性(FP) =在所有正确预测的阳性点中，实际上是不正确的。</p><p id="029f" class="iv iw jt ix b iy iz ja jb jc jd je jf nb jh ji jj nc jl jm jn nd jp jq jr js hb bi translated">真阴性(TN) =在所有正确预测的阴性点中，实际上是正确的。</p><p id="5acd" class="iv iw jt ix b iy iz ja jb jc jd je jf nb jh ji jj nc jl jm jn nd jp jq jr js hb bi translated">假阴性(FN) =所有正确预测的阴性点中，实际上是不正确的</p></blockquote><p id="1f86" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">精度是真阳性与假阳性之和中真阳性的分数。</p><blockquote class="my mz na"><p id="293a" class="iv iw jt ix b iy iz ja jb jc jd je jf nb jh ji jj nc jl jm jn nd jp jq jr js hb bi translated">精度=TP/(TP+FP)</p></blockquote><p id="5f7a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">回忆是真阳性和假阴性总和中真阳性的一部分。</p><blockquote class="my mz na"><p id="f626" class="iv iw jt ix b iy iz ja jb jc jd je jf nb jh ji jj nc jl jm jn nd jp jq jr js hb bi translated">召回=TP/(TP+FN)</p></blockquote><p id="a338" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">F1分数是精确度和召回率的调和平均值。</p><blockquote class="my mz na"><p id="b851" class="iv iw jt ix b iy iz ja jb jc jd je jf nb jh ji jj nc jl jm jn nd jp jq jr js hb bi translated">f1= 2(精度*召回)/(精度+召回)</p></blockquote><p id="ed11" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们要用的度量是微f1，它是微精度和微召回的调和平均值。</p><p id="43e6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">微精度是所有真阳性的总和与所有真阳性和假阳性的总和。</p><blockquote class="my mz na"><p id="53ff" class="iv iw jt ix b iy iz ja jb jc jd je jf nb jh ji jj nc jl jm jn nd jp jq jr js hb bi translated">微精度=TP1+TP2+…/((TP1+TP2+……)+(FP1+FP2+……))</p></blockquote><p id="f318" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于交叉验证，我使用了<a class="ae iu" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"> GridSearchCV </a>和5重交叉验证。</p><h1 id="b103" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">机器学习模型:</strong></h1><h2 id="9bef" class="lv ka hi bd kb mb mc md kf me mf mg kj jg mh mi kn jk mj mk kr jo ml mm kv mn bi translated">使用所有标签:</h2><ol class=""><li id="7c5f" class="lc ld hi ix b iy kx jc ky jg le jk lf jo lg js lh li lj lk bi translated"><strong class="ix hj">使用TFIDF矢量器:</strong></li></ol><p id="e76e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在基线模型构建部分，我们尝试了多项模型，包括多项式、逻辑回归、具有对数损失的SGD分类器、具有铰链损失的SGD分类器。在所有情况下，我们都希望最大化微观平均F1分数。给我们微观平均F1分数的最大值的基线模型是逻辑回归(0.2601)</p><p id="0609" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们来看一个简单的代码片段，它是我们在OneVsRest和逻辑回归中使用的。在整个实验中，我使用了相同的代码结构。所以，如果你想改变任何模型，只要把LogisticRegression换成任何模型就行了。</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="98e1" class="lv ka hi lr b fi lw lx l ly lz">lr = LogisticRegression(class_weight='balanced')</span><span id="44d3" class="lv ka hi lr b fi mo lx l ly lz">clf = OneVsRestClassifier(lr)<br/>clf.fit(X_train_multilabel, y_train_multilabel)</span><span id="f692" class="lv ka hi lr b fi mo lx l ly lz"><br/>prediction16 = clf.predict(X_test_multilabel)</span><span id="1134" class="lv ka hi lr b fi mo lx l ly lz">precision16 = precision_score(y_test_multilabel, prediction16, average='micro')</span><span id="faf3" class="lv ka hi lr b fi mo lx l ly lz">recall16 = recall_score(y_test_multilabel, prediction16, average='micro')</span><span id="7335" class="lv ka hi lr b fi mo lx l ly lz">f1_score16 = 2*((precision16 * recall16)/(precision16 + recall16))</span><span id="fb7d" class="lv ka hi lr b fi mo lx l ly lz">print("precision16: {:.4f}, recall16: {:.4f}, F1-measure: {:.4f}".format(precision16, recall16, f1_score16))</span></pre><p id="26c7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上述代码块的输出:</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="edf9" class="lv ka hi lr b fi lw lx l ly lz">precision16: 0.1673, recall16: 0.5839, F1-measure: 0.2601</span></pre><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ne"><img src="../Images/afc980cfbe097ff534f291593bde4916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*r5l2umBFijgXCA1jjXEQCw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图8:基线模型的不同模型的得分比较</figcaption></figure><p id="cbb8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">让我们比较实际标签和模型预测标签</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nf"><img src="../Images/778e82b03e1ff42d9aa445d561ab5182.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j3_BlAW0xtVSu9ml8CHqxg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图9:实际标签和模型预测标签之间的比较</figcaption></figure><p id="46da" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 2。使用AVGW2V <em class="jt"> : </em> </strong></p><p id="4c62" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">与上述讨论相同，物流回归给出了最高的F1分(0.214)</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ng"><img src="../Images/2d712c921b435e6ba87d5316a88fd436.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*wNLA0rFDBSByEetsVO0pUA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图10:AVG W2V车型评分对比</figcaption></figure><p id="18e0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">让我们看看模型预测的标签:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nh"><img src="../Images/37e463185391872a34392f1e3add2266.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*otYTaCftrnnQot7eEj3uYA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图11:实际标签和预测标签之间的标签比较</figcaption></figure><p id="db05" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 3。使用LSTM-CNN模型:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/27aea0d1f24ef2a682db325bbc2a17cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*2coFs1AA82d1jI0x_GbsHQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图12: LSTM-CNN模型</figcaption></figure><p id="afa0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">我们来看看准确度:</strong></p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="ec63" class="lv ka hi lr b fi lw lx l ly lz">test_loss, test_acc = model.evaluate(X_test, y_test_multilabel, verbose=2)</span><span id="6492" class="lv ka hi lr b fi mo lx l ly lz">print('\nTest accuracy:', test_acc)<br/>Test accuracy: 0.13583815097808838</span></pre><p id="d11e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以我们在这里也没有得到太多的准确性。</p><p id="c542" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">标签比较:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/6e11cc31228113929ef0fc0bf5d80d9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*vi0W8uFFGwF71a55x1ofYA.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图13:标签比较</figcaption></figure><h2 id="92cb" class="lv ka hi bd kb mb mc md kf me mf mg kj jg mh mi kn jk mj mk kr jo ml mm kv mn bi translated"><strong class="ak">用前3个标签建模</strong></h2><p id="88b0" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">在EDA部分，我们已经看到一部普通的电影由三个标签组成。因此，让我们建立一个模型，可以预测前三个标签。我们使用了相同的特征集，但是这一次，标签的数量等于3</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="7f9a" class="lv ka hi lr b fi lw lx l ly lz">cnt_vectorizer = CountVectorizer(tokenizer = tokenize, max_features=3, binary='true').fit(y_train</span></pre><ol class=""><li id="5ef8" class="lc ld hi ix b iy iz jc jd jg mv jk mw jo mx js lh li lj lk bi translated"><strong class="ix hj">使用TFIDF矢量器:</strong></li></ol><p id="ac8b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，具有对数损失的SGD分类器给了我们最高的F1分数，即0.586</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es nk"><img src="../Images/f4819c12a419a5ecb0a323972845b228.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*8RmPAKbUUhj8vwI3wKARuQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图14:前3个标签的得分比较</figcaption></figure><p id="a47e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，我们可以看到准确性分数的显著提高。</p><p id="7dcb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">标签比较:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nl"><img src="../Images/6c166e44f9f6d30d9de4440bd94ad27e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x0vzAc6J7QeoK15zLkqwLQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图15:实际标签和模型预测标签之间的比较</figcaption></figure><p id="998f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 2。使用AVGW2V : </strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/59caede296c8c0e8ec61545ad7536166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*3mc-FnoadVIHTgQuotZopQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图16:分数比较</figcaption></figure><p id="6f69" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里我们可以看到LogisticRegression给出了最高的F1分(0.562)</p><p id="6029" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">标签比较:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es nm"><img src="../Images/14f6e08a4276285838bad626611e1433.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*b-zBjDeYvxjYwBFDw4rAcA.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图17:标签比较</figcaption></figure><p id="0818" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 3。LSTM-CNN模式:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/9f66a3c2af01d2bd2fe02721437aef95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*Jp_OhQOfZKlfoF9C5efjNQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图18:排名前三的标签的LSTM-CNN模型</figcaption></figure><p id="66db" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里我们得到了0.283的精度</p><p id="a723" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">标签对比:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nn"><img src="../Images/198fe3b988d6ca03aaa1ad0325345502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yqR-di91HDeUDkK6I35Bog.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图19:实际标签与预测标签</figcaption></figure><h2 id="dc58" class="lv ka hi bd kb mb mc md kf me mf mg kj jg mh mi kn jk mj mk kr jo ml mm kv mn bi translated">用前5个标签建模</h2><p id="9ca7" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">在这里，代码是变化是非常简单的。我们只需将“max_features”的值从3更改为5，代码片段的其余部分保持不变。</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="0a2a" class="lv ka hi lr b fi lw lx l ly lz">cnt_vectorizer = CountVectorizer(tokenizer = tokenize, max_features=5, binary='true').fit(y_train)</span></pre><ol class=""><li id="1bfe" class="lc ld hi ix b iy iz jc jd jg mv jk mw jo mx js lh li lj lk bi translated"><strong class="ix hj">使用TFIDF矢量器:</strong></li></ol><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es no"><img src="../Images/d84485e44dfef7dd035601be74942229.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*uYkEgrfT3S_EILt9Yu25FA.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图20:准确度分数</figcaption></figure><p id="be0d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里物流回归也给出了最高的F1分(0.535)</p><p id="61f5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">标签比较:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es np"><img src="../Images/efc654e7c5cf4b39f3eb0185d5298d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*KksR6UkKHgcLj-tnEBPUkQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图21:实际标签与预测标签</figcaption></figure><p id="83ab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 2。使用AVGW2V : </strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es nq"><img src="../Images/fe67ab43f8cce54ae411ab595414ee30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*KxcAV1IhR1v7b_xCLVfBoQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图22:使用AVG W2V的准确度分数</figcaption></figure><p id="be15" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，具有铰链损失的SGD分类器给出了最高的F1分数。</p><p id="680b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">标签比较:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es nr"><img src="../Images/1b815a0f4cce2beee70be126173b57fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*5FMuR6BUI4O7LZ1pYnbVow.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图23:实际标签与预测标签</figcaption></figure><p id="b8b3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 3。LSTM-CNN模式:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ns"><img src="../Images/50c5f5f2c3d1bea3d1acd79cfeeff0bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*vPJV_P2HYG326CggWOV0Ig.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图24 : LSTM-CNN模型</figcaption></figure><p id="8192" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里我们得到了0.213的精度</p><p id="277e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">标签比较:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es nt"><img src="../Images/62b698f65e1142b7d257b906274c1ebb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*G2Nf_ii2J40lrSMz2vLyfw.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图25:实际标签与预测标签</figcaption></figure><h2 id="abd5" class="lv ka hi bd kb mb mc md kf me mf mg kj jg mh mi kn jk mj mk kr jo ml mm kv mn bi translated">用前30个标签建模</h2><p id="5a73" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">在这一部分，我们必须从所有71个标签中手动选择前30个出现的标签。要做到这一点，首先我们必须读取数据集，然后使用BoW技术对标签进行矢量化，以找出哪些标签出现了多少次。</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="4631" class="lv ka hi lr b fi lw lx l ly lz">data = pd.read_csv("data_with_all_tags.csv")</span><span id="63c5" class="lv ka hi lr b fi mo lx l ly lz">vectorizer = CountVectorizer(preprocessor=lambda x: x, tokenizer = lambda x: str(x).split(", ") )<br/>tag_vect = vectorizer.fit_transform(data["tags"])</span><span id="013e" class="lv ka hi lr b fi mo lx l ly lz">tags = vectorizer.get_feature_names()<br/>freqs = tag_vect.sum(axis=0).A1<br/>result = list(zip(tags, freqs))</span><span id="f58a" class="lv ka hi lr b fi mo lx l ly lz">print((result))</span></pre><p id="3084" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是上述代码单元的输出:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nu"><img src="../Images/4047a75ae16a986cce2c0da519e47e81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YoLGsViqrrqO-0th-YOH7Q.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图26:所有出现的标签</figcaption></figure><p id="b7e8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们必须由此创建一个dataframe，然后根据标签出现的次数对其进行降序排序。之后，我们必须根据它们的频率选择前30个标签。</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="e280" class="lv ka hi lr b fi lw lx l ly lz">tag_counts = pd.DataFrame(result,columns=['tag','tag_counts'])</span><span id="d5e3" class="lv ka hi lr b fi mo lx l ly lz">tag_counts_sorted = tag_counts.sort_values(['tag_counts'], ascending=False)</span><span id="9dfb" class="lv ka hi lr b fi mo lx l ly lz">tag_counts = tag_counts_sorted['tag'][:30]<br/>print(tag_counts)</span></pre><p id="60a0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看前30个标签:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es nv"><img src="../Images/da1003d2b42a28b3aa24182aca85ef17.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*7Hv6_WyQvsWbwtpQpNn9eQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图27:前30个标签</figcaption></figure><p id="e254" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们必须将这30个标签与每部电影中出现的所有标签进行比较，然后删除这30个标签旁边的所有标签。</p><p id="b47f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后删除由于我们所做的预处理而没有标签的行。</p><p id="69ba" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">之前比较:</em> </strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es nw"><img src="../Images/714868cbc5d377ee430af3e2c722fa13.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*WwnSH-unxjtuxZkLzvEC2Q.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图28:每部电影的原始标签</figcaption></figure><p id="3f18" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里我们可以看到长度:13757，这意味着我们有13757个数字行(或)电影。</p><p id="ab5a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">后<em class="jt">比较:</em>后</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es nx"><img src="../Images/94e994102ceeef25ffb32c86f57855ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*fyQ1bYAWQ6ZXoYfC3B9XEw.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图29:比较后每个电影的标签</figcaption></figure><p id="e809" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，我们可以注意到长度已经减少到13010，这意味着所有没有标签的电影都被删除。</p><p id="30d7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，将按照与之前车型相同的流程来获取F1分数</p><ol class=""><li id="842a" class="lc ld hi ix b iy iz jc jd jg mv jk mw jo mx js lh li lj lk bi translated"><strong class="ix hj"> AVGW2V : </strong></li></ol><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ny"><img src="../Images/775808131aa7f4b4e2e2ac42f606b490.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*Kx-Tn8xIpCa5zxAAk8oVyg.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图30:准确度分数</figcaption></figure><p id="a015" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">物流回归给出了最高的F1分(0.32)</p><p id="2947" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">标签比较:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es nz"><img src="../Images/8cc8d730480241c73f0f532af324ac72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*B5tsKP-gOXQvZfwow18HYQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图31:实际标签与预测标签</figcaption></figure><p id="e245" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 2。LSTM-美国有线电视新闻网:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/961309519e050d90231fe57fa89e0d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*FyWh0lETyqVD5GY38CrMfw.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图32 : LSTM-CNN模型</figcaption></figure><p id="5c4c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里我们得到了0.043的精度</p><p id="7c48" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">标签比较:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es oa"><img src="../Images/ae45c2375b1b9aec4816817a69181e76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mRaOPZIIwqwVXmRU6bUk2w.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图33:实际标签与预测标签</figcaption></figure><h2 id="de31" class="lv ka hi bd kb mb mc md kf me mf mg kj jg mh mi kn jk mj mk kr jo ml mm kv mn bi translated">用前5个标签建模</h2><p id="83fe" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">在这里，我们将遵循与上一个相同的步骤(前30个)，而不是30个标签，将为前5个标签。</p><p id="c1db" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本节中，我们不会使用sklearn的CountVectorizer方法，而是手动使用它来获得更高的精度。</p><p id="70ad" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">手动实现标签的<a class="ae iu" href="https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/" rel="noopener ugc nofollow" target="_blank"> Onehotencoding </a>后，应该是这样的:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ob"><img src="../Images/b52a37350d00ee23a2eba35b431bb254.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*HIg0yA83PkGbTAAyrvF8lg.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图34:标签的一个酒店编码</figcaption></figure><p id="b089" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在此之后，其余的过程都是相同的。</p><ol class=""><li id="fe52" class="lc ld hi ix b iy iz jc jd jg mv jk mw jo mx js lh li lj lk bi translated"><strong class="ix hj"> AVGW2V: </strong></li></ol><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es oc"><img src="../Images/f5df8749808c67012af23cc655760a31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*UdL3z18WYFtg7OumPf6aiQ.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图35:准确度分数</figcaption></figure><p id="8d5a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，我们可以看到LogisticRegression给出了最高的f1分数(0.59)，不仅是这个模型，而且是我们尝试过的所有其他模型。</p><p id="8797" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">标签对比:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es od"><img src="../Images/8d1fae477222c8625c161e065a9276c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*GTJeMhMgqeV-IQB3T8fq6w.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图36:实际标签与预测标签</figcaption></figure><p id="fe05" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在标签预测中，我们还可以看到该模型与其他模型之间的差异。</p><p id="f7c6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 2。LSTM型号:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es oe"><img src="../Images/ef507ad2f4b2af83b87edeb72d218817.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*dZKHfPRyeYttp0jT2C7T3g.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图37 : LSTM模型</figcaption></figure><p id="ad1b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里我们得到了0.66的精度，与其他LSTM模型或ML模型相比，这是非常显著的</p><p id="82cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">标签预测:</strong></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es of"><img src="../Images/1f7781ff1ba0dba824e17072336ead8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*GUrgZU252PeFeh0M7kvTjA.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图38:实际标签与预测标签</figcaption></figure><h1 id="b7e6" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">主题建模</h1><p id="6a5c" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">在<a class="ae iu" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>和<a class="ae iu" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">自然语言处理</a>中，<strong class="ix hj">主题模型</strong>是<a class="ae iu" href="https://en.wikipedia.org/wiki/Statistical_model" rel="noopener ugc nofollow" target="_blank">统计模型</a>的一种，用于发现文档集合中出现的抽象“主题”。</p><p id="3b36" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">主题建模是一种常用的文本挖掘工具，用于发现文本中隐藏的语义结构。</p><p id="28ca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> <em class="jt">【潜在狄利克雷分配(LDA) </em> </strong> </a> <em class="jt">是一个流行的主题建模算法，在Python的Gensim包中有很好的实现。</em></p><h2 id="afb8" class="lv ka hi bd kb mb mc md kf me mf mg kj jg mh mi kn jk mj mk kr jo ml mm kv mn bi translated"><strong class="ak"> <em class="og">让我们开始吧</em> </strong></h2><p id="3c04" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">让我们读取预处理的数据集</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="25f6" class="lv ka hi lr b fi lw lx l ly lz">dataframe = pd.read_csv("data_with_all_tags.csv")</span></pre><p id="691c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们只提取那些我们需要的特征</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="07b9" class="lv ka hi lr b fi lw lx l ly lz">data = dataframe[['title', 'plot_synopsis', 'tags', 'split', 'CleanedSynopsis']]</span><span id="4137" class="lv ka hi lr b fi mo lx l ly lz">data.shape<br/>(13757, 5)</span></pre><p id="e9c8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们添加一个概要的所有单词来创建数据语料库</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="3f70" class="lv ka hi lr b fi lw lx l ly lz">data["synopsis_words"] = data["CleanedSynopsis"].apply(lambda x: x.split())</span><span id="2d1a" class="lv ka hi lr b fi mo lx l ly lz">data_words=[]<br/>for sent in data["synopsis_words"].values:<br/>    data_words.append(sent)</span></pre><p id="d5eb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们为LDA创建一个字典和语料库</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="6a37" class="lv ka hi lr b fi lw lx l ly lz">id2word = corpora.Dictionary(data_words)</span><span id="41e8" class="lv ka hi lr b fi mo lx l ly lz">corpus = [id2word.doc2bow(text) for text in data_words]</span></pre><h2 id="f452" class="lv ka hi bd kb mb mc md kf me mf mg kj jg mh mi kn jk mj mk kr jo ml mm kv mn bi translated">构建主题模型:</h2><p id="f53d" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">我们拥有训练LDA模型所需的一切。除了语料库和词典，您还需要提供主题的数量。</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="7f90" class="lv ka hi lr b fi lw lx l ly lz"># Build LDA model<br/>lda_model = gensim.models.LdaMulticore(corpus=corpus,<br/>                                           id2word=id2word,<br/>                                           num_topics=10, <br/>                                           random_state=100,<br/>                                           chunksize=10,<br/>                                           passes=10,<br/>                                           alpha='symmetric',<br/>                                           iterations=100,<br/>                                           per_word_topics=True,<br/>                                           workers=7)</span></pre><h2 id="4252" class="lv ka hi bd kb mb mc md kf me mf mg kj jg mh mi kn jk mj mk kr jo ml mm kv mn bi translated">查看LDA模型中的主题:</h2><p id="e748" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">上面的LDA模型是用10个不同的主题构建的，其中每个主题是关键字的组合，并且每个关键字对主题有一定的权重。</p><p id="4641" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您可以使用<code class="du oh oi oj lr b">lda_model.print_topics()</code>查看每个主题的关键词以及每个关键词的权重(重要性),如下所示。</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="abfa" class="lv ka hi lr b fi lw lx l ly lz">pprint(lda_model.print_topics())</span><span id="de03" class="lv ka hi lr b fi mo lx l ly lz">[(0,<br/>  '0.009*"kill" + 0.006*"father" + 0.006*"get" + 0.005*"love" + 0.004*"famili" '<br/>  '+ 0.004*"brother" + 0.004*"son" + 0.004*"meet" + 0.004*"take" + '<br/>  '0.004*"fight"'),<br/> (1,<br/>  '0.014*"tell" + 0.012*"go" + 0.011*"see" + 0.011*"say" + 0.011*"back" + '<br/>  '0.010*"get" + 0.007*"ask" + 0.007*"find" + 0.007*"look" + 0.007*"room"'),<br/> (2,<br/>  '0.015*"kill" + 0.011*"polic" + 0.007*"car" + 0.006*"shoot" + 0.006*"offic" '<br/>  '+ 0.006*"frank" + 0.006*"man" + 0.006*"john" + 0.006*"find" + 0.005*"gun"'),<br/> (3,<br/>  '0.008*"georg" + 0.008*"get" + 0.008*"billi" + 0.007*"jim" + 0.007*"ray" + '<br/>  '0.006*"scott" + 0.006*"go" + 0.005*"rachel" + 0.005*"find" + 0.005*"bella"'),<br/> (4,<br/>  '0.009*"hous" + 0.008*"find" + 0.007*"mother" + 0.007*"mari" + '<br/>  '0.007*"father" + 0.006*"child" + 0.006*"kill" + 0.006*"home" + '<br/>  '0.005*"woman" + 0.005*"famili"'),<br/> (5,<br/>  '0.016*"go" + 0.015*"tell" + 0.014*"get" + 0.012*"say" + 0.012*"sam" + '<br/>  '0.011*"david" + 0.009*"mike" + 0.009*"charli" + 0.008*"ask" + '<br/>  '0.008*"sarah"'),<br/> (6,<br/>  '0.008*"human" + 0.007*"power" + 0.006*"world" + 0.005*"use" + 0.005*"earth" '<br/>  '+ 0.005*"destroy" + 0.004*"find" + 0.004*"one" + 0.004*"alien" + '<br/>  '0.004*"reveal"'),<br/> (7,<br/>  '0.006*"king" + 0.006*"kill" + 0.005*"return" + 0.005*"vampir" + '<br/>  '0.004*"villag" + 0.004*"take" + 0.004*"arriv" + 0.004*"one" + '<br/>  '0.003*"father" + 0.003*"son"'),<br/> (8,<br/>  '0.008*"kill" + 0.006*"war" + 0.006*"attack" + 0.006*"soldier" + '<br/>  '0.005*"ship" + 0.005*"order" + 0.005*"forc" + 0.005*"men" + 0.005*"group" + '<br/>  '0.005*"escap"'),<br/> (9,<br/>  '0.006*"new" + 0.005*"love" + 0.005*"life" + 0.005*"time" + 0.005*"one" + '<br/>  '0.005*"friend" + 0.005*"make" + 0.005*"day" + 0.004*"go" + 0.004*"film"')]</span></pre><p id="6782" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">这个怎么解读？</em></p><p id="a897" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">题目0是a表示为<em class="jt"> '0.009* "杀"+ 0.006* "父"+ 0.006* "得"+ 0.005* "爱"+ 0.004* "家" ' '+ 0.004* "兄"+ 0.004* "子"+ 0.004* "遇"+ 0.004* "取"+ ' '0.004* "斗" '</em></p><p id="6b9c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这意味着促成这个话题的前10个关键词是:“杀死”、“父亲”、“得到”..依此类推，主题0上“杀死”的权重是0.009</p><p id="c2c1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">权重反映了关键词对主题的重要性。</p><h2 id="5764" class="lv ka hi bd kb mb mc md kf me mf mg kj jg mh mi kn jk mj mk kr jo ml mm kv mn bi translated">找出每个句子中的主导主题:</h2><p id="456d" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">主题建模的一个实际应用是确定给定文档是关于什么主题的。</p><p id="891f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为此，我们找到了在该文档中贡献百分比最高的主题号。</p><p id="d569" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面的<code class="du oh oi oj lr b">format_topics_sentences()</code>函数很好地将这些信息聚集在一个可显示的表中。</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="f41e" class="lv ka hi lr b fi lw lx l ly lz">data_list = dataframe.CleanedSynopsis.values.tolist()</span><span id="acb8" class="lv ka hi lr b fi mo lx l ly lz">def format_topics_sentences(ldamodel=None, corpus=corpus, texts = data_list):<br/>    # Init output<br/>    sent_topics_df = pd.DataFrame()</span><span id="935d" class="lv ka hi lr b fi mo lx l ly lz"># Get main topic in each document<br/>    for i, row_list in enumerate(ldamodel[corpus]):<br/>        row = row_list[0] if ldamodel.per_word_topics else row_list            <br/>        # print(row)<br/>        row = sorted(row, key=lambda x: (x[1]), reverse=True)<br/>        # Get the Dominant topic, Perc Contribution and Keywords for each document<br/>        for j, (topic_num, prop_topic) in enumerate(row):<br/>            if j == 0:  # =&gt; dominant topic<br/>                wp = ldamodel.show_topic(topic_num)<br/>                topic_keywords = ", ".join([word for word, prop in wp])<br/>                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)<br/>            else:<br/>                break<br/>    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']</span><span id="4c3d" class="lv ka hi lr b fi mo lx l ly lz"># Add original text to the end of the output<br/>    contents = pd.Series(texts)<br/>    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)<br/>    return(sent_topics_df)</span><span id="553f" class="lv ka hi lr b fi mo lx l ly lz">df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_words)</span><span id="9811" class="lv ka hi lr b fi mo lx l ly lz"># Format<br/>df_dominant_topic = df_topic_sents_keywords.reset_index()<br/>df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']</span></pre><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ok"><img src="../Images/ba69f5fc222eceb9bf8df54f678a44a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Zx-9u3LI4YmN1XFHTRgVQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图29:主导主题</figcaption></figure><p id="880e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们将这些主要主题保存到一个csv文件中，然后将它与我们的原始数据连接起来。</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="7d63" class="lv ka hi lr b fi lw lx l ly lz">df_dominant_topic.to_csv("dominant_topics.csv")</span><span id="26b6" class="lv ka hi lr b fi mo lx l ly lz">df_topic = pd.read_csv("dominant_topics.csv")<br/>combined_df = pd.concat([data, df_topic], axis=1)</span></pre><p id="54f2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们将数据集分为训练和测试，像之前一样应用矢量器来获得f1分数</p><pre class="jv jw jx jy fd lq lr ls lt aw lu bi"><span id="4b9e" class="lv ka hi lr b fi lw lx l ly lz">data_test=combined_df.loc[(combined_df['split'] == 'test')]<br/>data_train=combined_df.loc[(combined_df['split'] == 'val') | (combined_df['split'] == 'train')]</span></pre><p id="a825" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">应用TFIDF矢量器后，我们得到了以下准确度分数。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ol"><img src="../Images/e1ac99482c1d5c10ba70af1ed3b6618e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*4uHtC8tY7xhEeTXyIoLC8A.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图30:准确度分数</figcaption></figure><h1 id="2a05" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">让我们使用Flask来实现我们的模型</h1><p id="4bf3" class="pw-post-body-paragraph iv iw hi ix b iy kx ja jb jc ky je jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">首先在Flask中编写所需的代码来创建一个API。<a class="ae iu" href="https://www.tutorialspoint.com/flask" rel="noopener ugc nofollow" target="_blank">这里</a>是它的一个很好的教程。</p><p id="0c12" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们运行它。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es om"><img src="../Images/292d4d9df499dd5ad255486f744627e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*fD92nscdnPySQ1VFnPmwsw.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图31:索引页面</figcaption></figure><p id="0187" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">打开索引页面后，输入剧情梗概来预测标签</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es on"><img src="../Images/21c28e8ed2a4f7297e7d8cbdc6b6da18.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*nPfOlz2MoNg_xRNJn9eqfg.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图32:预测的标签</figcaption></figure><p id="6ad7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，我们的模型预测了我们给出的大纲的标签，即“闪回”。</p><h1 id="d0c9" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak">结论:</strong></h1><ol class=""><li id="2a73" class="lc ld hi ix b iy kx jc ky jg le jk lf jo lg js lh li lj lk bi translated">我们从整个项目中获得的最高微观平均F1分数是0.59。</li><li id="ef6f" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">一般来说，我们习惯于看到90%以上的准确率，但我们没有一个大的数据集，但我们仍然得到了体面的F1分数。</li><li id="0cf5" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">对于主题建模，我们得到的最高f1分数是LogisticRegression的0.374</li></ol><h1 id="b705" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">参考资料:</h1><ol class=""><li id="2963" class="lc ld hi ix b iy kx jc ky jg le jk lf jo lg js lh li lj lk bi translated">研究论文:<a class="ae iu" href="https://arxiv.org/pdf/1802.07858.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1802.07858.pdf</a></li><li id="bcbe" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">概念帮助:<a class="oo op ge" href="https://medium.com/u/bc8571e39021?source=post_page-----d5fde119db6d--------------------------------" rel="noopener" target="_blank">应用人工智能课程</a></li><li id="5428" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">数据集:<a class="ae iu" href="https://www.kaggle.com/cryptexcode/mpst-movie-plot-synopses-with-tags" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/cryptexcode/mpst-movie-plot-synopses-with-tags</a></li><li id="4465" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">Github链接:<a class="ae iu" href="https://github.com/sandeeppanda22/Movie-Tag-Prediction" rel="noopener ugc nofollow" target="_blank">https://github.com/sandeeppanda22/Movie-Tag-Prediction</a></li><li id="7169" class="lc ld hi ix b iy ll jc lm jg ln jk lo jo lp js lh li lj lk bi translated">领英简介:<a class="ae iu" href="https://www.linkedin.com/in/sandeepkumarpanda/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/sandeepkumarpanda/</a></li></ol></div></div>    
</body>
</html>