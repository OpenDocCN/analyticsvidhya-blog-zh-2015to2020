<html>
<head>
<title>Bangla Character Recognition System — The Deep Learning Way (2/n)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">孟加拉文字识别系统—深度学习方式(2/n)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bangla-character-recognition-system-the-deep-learning-way-2-n-d5b16333d77b?source=collection_archive---------24-----------------------#2020-03-04">https://medium.com/analytics-vidhya/bangla-character-recognition-system-the-deep-learning-way-2-n-d5b16333d77b?source=collection_archive---------24-----------------------#2020-03-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="ba7c" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">用于手写孟加拉文字识别的卷积神经网络。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/59da80ce5dc031e64a163f3323848b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Vs6BGB9TNcH0D-L5pe74Q.jpeg"/></div></div></figure><p id="00ab" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">欢迎回来，这篇文章是我和Christina正在做的系列文章的第二部分，是我们在布朗大学数据科学倡议MS项目的课程项目的一部分。在第一篇文章中，我们讨论了数据集、不同类别以及不同目标的分布。</p><p id="9fd1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在本文中，我们将讨论我们在项目中使用的神经网络架构。但是在我们深入讨论之前，先简单回顾一下我们上节课结束的地方。</p><p id="d033" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们试图将孟加拉语手写字母分为3个目标——字素词根、元音发音符号和辅音发音符号。总共有168个不同的词根子类型、11个元音音调符号子类型和7个辅音词根子类型。值得注意的是，在训练数据中，每种类型的表示都有很大的不平衡，特别是在字形根类中。训练数据由大小为137x236的手写字母的扫描图像组成，但其中大部分是空白。因此，我们对图像进行了裁剪、阈值处理和去噪，使其尺寸为64x64，字母位于中心。现在有了用于训练的数据，让我们来看看架构。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es kg"><img src="../Images/c7c298fc3db7551f15e405322bf48843.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k4iFXZgAsZX4xnXUzbQXhQ.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">处理前的样本图像</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kl"><img src="../Images/ba825f5262f4aa834cfe936fe405180b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*A7T5eeEw6vJA4Z6VtL5KUQ.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">处理后的样本图像</figcaption></figure></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><h1 id="27d5" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">什么是卷积神经网络(CNN)</h1><p id="bc39" class="pw-post-body-paragraph jj jk hi jl b jm ll ij jo jp lm im jr js ln ju jv jw lo jy jz ka lp kc kd ke hb bi translated">我们假设你知道神经网络的基本概念是什么。有无数关于它们的营销描述，但是从数学的角度来看，这不是通过一系列仿射变换将输入向量映射到期望输出向量的一步一步的过程。该算法通过将已知的输入和输出传递给网络，然后要求计算机最小化某个函数(在ML术语中称为损失函数)，以计算出哪些映射在减少实际输出和预测输出之间的误差方面做得很好，从而决定这些算法是什么(没有人类交互，因此有“机器”学习一词)。我们以迭代的方式做了很长时间，我们得到了(希望如此！)我们的概化模型。在我们的例子中，输入向量是4096维向量，因为每个图像是64x64像素，每个像素的值是0或1，这取决于它是白色像素还是黑色像素。在输出端，我们有3个维度168、11和7，分别用于字素根、元音发音符号和辅音发音符号。</p><p id="0051" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">然而，虽然更普通的神经网络(也称为全连接(FC)网络)具有从每个输入到每个输出向量分量的连接，但特定的结构倾向于将每个图像作为一个整体来进行决策。这怎么会是个坏主意呢？好吧，让你自己确认一下，当你看一张猫的图片时，在我们判断它是不是一只猫时，猫相对于图片其他部分的位置并不重要。同样，用整体图像做决策听起来像是一种矫枉过正(并且在计算上无法实现)的做法。</p><blockquote class="lq lr ls"><p id="bee6" class="jj jk lt jl b jm jn ij jo jp jq im jr lu jt ju jv lv jx jy jz lw kb kc kd ke hb bi translated">当你看一张猫的图片时，在我们判断它是不是猫时，猫相对于图片其他部分的位置并不重要。</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lx"><img src="../Images/4c148802863e0948d3493cf51c62ddfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*HO1U1X6Di9gXfJ5D9q1g7w.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">CNN架构的一个例子</figcaption></figure><p id="9662" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">CNN是NN的变体，其中我们试图保持我们的目标(猫/狗/元音/辅音)的前述空间不变性。受人类视觉皮层的启发，我们在图像上滑动一系列过滤器，每个过滤器从图像中提取特定的特征，而不依赖于该特征在整个图像中的绝对位置。更具体地说，想象这些过滤器中一个，选择长的水平笔划，而一些选择垂直笔划。我们也可以将这些过滤器堆叠起来，也就是说，我们只需选取大致的笔画，然后将其传递给后续的过滤器，这些过滤器会对这些笔画进行分类，是水平的还是垂直的。克里斯·奥拉有一篇精彩的文章，他对CNN做了更详细的解释。</p></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><h1 id="56c2" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">联系紧密的CNN或者只是DenseNet⁴ …</h1><p id="4130" class="pw-post-body-paragraph jj jk hi jl b jm ll ij jo jp lm im jr js ln ju jv jw lo jy jz ka lp kc kd ke hb bi translated">不幸的是，由于分类工作的复杂性，一个普通的CNN有时也不符合要求。回到笔画的例子，仅仅将它们分离出来没有多大帮助，因为一个独特的字母是许多不同形状的笔画的组合。缓解这一问题的一种方法是使每个过滤器具有识别笔画的能力，同时使其在进行处理时能够识别在先前层接收到的所有输入。这是一个可能的图像。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/0d995c737c8722f531f8aedb0ed2599b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*0XGkTpXG796n5v2D6NQiLA.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated"><a class="ae kf" rel="noopener" href="/the-advantages-of-densenet/the-advantages-of-densenet-98de3019cdac">真正密集的网络架构</a></figcaption></figure><p id="4a77" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如您所见，红线是前一层的连接，每个后续层都知道其前一层的输入。</p><p id="1746" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">由于内存和计算能力的硬件限制(我们将在后面看到，由于涉及的参数数量太多，这些需要大量内存)，我们无法在合理的时间框架内实现这一点，即使有谷歌或脸书的GPU和计算时间的装备，更不用说研究生的PC了。</p><p id="36de" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在2016年年中，<a class="ae kf" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank"> Gao和他的同事</a>提出了一种架构，在这种架构中，连接不是连接整个网络，而是在层块中完成(统称为密集块)，这些层通过单个卷积层和池化layer⁵相互连接(原因将在下面解释)。最后，我们将卷积层的输出连接到密集输出层，后者现在只需根据卷积层的响应来推断输出。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lz"><img src="../Images/c6d7a81623b4d453e078ff7a5fc4bb40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xN0ofdXd1Y7gyD36tPqYQg.jpeg"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">来自<a class="ae kf" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank">原创论文</a>的密集区块概念</figcaption></figure><h2 id="f183" class="ma ku hi bd kv mb mc md kz me mf mg ld js mh mi lf jw mj mk lh ka ml mm lj mn bi translated">DenseNet螺母和螺栓</h2><p id="32fc" class="pw-post-body-paragraph jj jk hi jl b jm ll ij jo jp lm im jr js ln ju jv jw lo jy jz ka lp kc kd ke hb bi translated">单个致密层可以分成两个块:</p><ul class=""><li id="c0b5" class="mo mp hi jl b jm jn jp jq js mq jw mr ka ms ke mt mu mv mw bi translated">卷积块:这是DenseNet的工作马。<br/>它的主要参数是一个增长率，它决定了每个卷积层中滤波器的数量。它被称为增长率，因为当我们连接输入时，每一层的输入都比增长率乘以前面层的数量要大。</li><li id="6e8b" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke mt mu mv mw bi translated">过渡块:一旦我们处于密集块的输出，在它进入下一层之前，我们将它通过另一个卷积和池层。之所以这样做，是因为如果您注意到每个密集层的输出与原始输入具有相同的大小，但由于我们必须对下一个输入进行下采样，这是CNN架构的标准，因此我们将其通过(1x1)卷积层，然后进行平均合并，以实现所需的维度。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nc"><img src="../Images/1c440b2c7e31d66cf8c0cfefdafe2b46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RhxaNgBi3NgrCAlSIIhVmQ.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">DenseNet结构传播out⁶</figcaption></figure><h2 id="ba30" class="ma ku hi bd kv mb mc md kz me mf mg ld js mh mi lf jw mj mk lh ka ml mm lj mn bi translated">有用吗？</h2><p id="657e" class="pw-post-body-paragraph jj jk hi jl b jm ll ij jo jp lm im jr js ln ju jv jw lo jy jz ka lp kc kd ke hb bi translated">DenseNet因其可与ResNet或InceptionNet等更复杂的架构相媲美的结果而备受瞩目，同时减少了参数数量，从而大幅缩短了训练时间。它还具有与fancier架构相似的优点，例如减轻消失/爆炸梯度问题，以及加强特征传播和重用。下面是与ResNet的对比。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nd"><img src="../Images/6dea3adf16187dad7598cd90738cb84c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DpeNAVF4Aw7-nZ4HSQGDzA.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">ResNets和dense nets-BC(dense nets的变体)的准确性比较</figcaption></figure><p id="ad82" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">观察在相同精度水平下，参数#和触发器#比ResNet低2–3倍。</p><p id="3988" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">它的缺点是在训练时需要大量的内存，因为它必须跟踪密集块中的所有先前输入。</p></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><h1 id="95e7" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">我们的密集网络</h1><h2 id="277d" class="ma ku hi bd kv mb mc md kz me mf mg ld js mh mi lf jw mj mk lh ka ml mm lj mn bi translated">体系结构</h2><p id="97dc" class="pw-post-body-paragraph jj jk hi jl b jm ll ij jo jp lm im jr js ln ju jv jw lo jy jz ka lp kc kd ke hb bi translated">回到手头的项目，这是我们的DenseNet版本(亲切地称为SayanNet v4 :P)</p><blockquote class="lq lr ls"><p id="9bc2" class="jj jk lt jl b jm jn ij jo jp jq im jr lu jt ju jv lv jx jy jz lw kb kc kd ke hb bi translated"><a class="ae kf" href="https://ibb.co/HGtkwWs" rel="noopener ugc nofollow" target="_blank">https://ibb.co/HGtkwWs</a>(也显示在最后。它是巨大的。)</p></blockquote><p id="9ec3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">然而，基本结构如下:</p><ul class=""><li id="33e8" class="mo mp hi jl b jm jn jp jq js mq jw mr ka ms ke mt mu mv mw bi translated">输入图层-以64x64x1的格式输入图像(因为所有图像都是灰度图像)</li><li id="74d3" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke mt mu mv mw bi translated">(可选)增强层。</li><li id="f8ed" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke mt mu mv mw bi translated">3个密集块——每个块有4个卷积层，每个卷积层有12个滤波器。随后是平均池层(2x2，步长为2x2)的过渡块，用于对输出尺寸进行下采样。</li><li id="0266" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke mt mu mv mw bi translated">全局平均池图层-将最终卷积图层输出转换为单个矢量</li><li id="5450" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke mt mu mv mw bi translated">3个输出密集图层-针对每个目标。</li><li id="3e15" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke mt mu mv mw bi translated">卷积层之间的若干Dropout⁷和批量归一化，以减少过拟合并确保信号强度。</li></ul><p id="4431" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们总共有:<br/>总参数:173730<br/>可训练参数:170802<br/>不可训练参数:2928</p><h2 id="39c5" class="ma ku hi bd kv mb mc md kz me mf mg ld js mh mi lf jw mj mk lh ka ml mm lj mn bi translated">其他超参数</h2><p id="d4ea" class="pw-post-body-paragraph jj jk hi jl b jm ll ij jo jp lm im jr js ln ju jv jw lo jy jz ka lp kc kd ke hb bi translated">优化器—我们使用Adam⁸优化器，初始学习率为0.01</p><p id="bc45" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">损失函数-由于每个值都是一个标注(没有数值意义)，我们使用稀疏分类Cross-Entropy⁹作为所有输出的损失函数。</p><p id="9918" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">指标——虽然竞赛网页采用加权召回指标，但我们使用准确度⁰作为我们的训练指标</p><p id="5051" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">学习率—我们使用可变学习率，在所有3个输出损失上使用TensorFlow的本地回调“ReduceLROnPlateau”功能。我们从0.01的学习率开始，每次损失达到0.0001的最小值时，学习率降低0.2倍。</p><h2 id="e211" class="ma ku hi bd kv mb mc md kz me mf mg ld js mh mi lf jw mj mk lh ka ml mm lj mn bi translated">初步结果</h2><p id="00b4" class="pw-post-body-paragraph jj jk hi jl b jm ll ij jo jp lm im jr js ln ju jv jw lo jy jz ka lp kc kd ke hb bi translated">正如之前和上一篇文章中提到的，由于我们的训练数据是不平衡的，我们使用scikit-learning的“train_test_split”函数对整个数据集进行类分层拆分，90%用于训练，10%用于验证。</p><p id="904c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们用32的批量训练了总共100个时期的模型。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ne"><img src="../Images/73fe0a5b0623f0c36be3e98de37e3641.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNNkasa44U8dAo3LOZgB4Q.jpeg"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">从左到右:元音准确性、词根准确性、辅音准确性(红色表示训练，蓝色表示验证)</figcaption></figure><h2 id="8e4a" class="ma ku hi bd kv mb mc md kz me mf mg ld js mh mi lf jw mj mk lh ka ml mm lj mn bi translated">下一步是什么…</h2><p id="03ac" class="pw-post-body-paragraph jj jk hi jl b jm ll ij jo jp lm im jr js ln ju jv jw lo jy jz ka lp kc kd ke hb bi translated">由于我们仅使用具有任意参数的基础模型就取得了成功，下一步是调整模型的超参数，以获得更好的结果。另一件要研究的事情是在输出之前添加几个密集层后精度的变化。</p><p id="67ec" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在训练期间观察图像增强的效果以进一步减少过拟合和提高准确度也是有趣的。</p><p id="778b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在下一篇文章中，我们将详细讨论调整后的网络的结果，并尝试理解每一层/每一块是如何提取图像特征以达到最终结果的。</p></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><p id="3c08" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">项目的所有代码都可以在这个</strong> <a class="ae kf" href="https://github.com/reach2sayan/Bengali-Grapheme_DATA2040" rel="noopener ugc nofollow" target="_blank"> <strong class="jl hj"> github </strong> </a> <strong class="jl hj">资源库中找到。</strong></p></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><h1 id="4058" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">参考</h1><ol class=""><li id="91c8" class="mo mp hi jl b jm ll jp lm js nf jw ng ka nh ke ni mu mv mw bi translated"><a class="ae kf" href="https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414" rel="noopener ugc nofollow" target="_blank">https://news . MIT . edu/2017/explained-neural-networks-deep-learning-0414</a></li><li id="cb26" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke ni mu mv mw bi translated"><a class="ae kf" href="https://towardsdatascience.com/under-the-hood-of-neural-networks-part-1-fully-connected-5223b7f78528" rel="noopener" target="_blank">https://towards data science . com/under-the-hood-of-neural-networks-part-1-fully-connected-5223 b7f 78528</a></li><li id="c257" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke ni mu mv mw bi translated"><a class="ae kf" href="https://towardsdatascience.com/understanding-convolution-neural-networks-the-eli5-way-785330cd1fb7" rel="noopener" target="_blank">https://towards data science . com/understanding-convolution-neural-networks-the-Eli 5-way-785330 cd1fb 7</a></li><li id="433d" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke ni mu mv mw bi translated">【https://towardsdatascience.com/densenet-2810936aeebb T4】</li><li id="52d9" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke ni mu mv mw bi translated"><a class="ae kf" href="https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/pooling-layers-for-convolutionary-neural-networks/</a></li><li id="89c0" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke ni mu mv mw bi translated"><a class="ae kf" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803" rel="noopener" target="_blank">https://towards data science . com/review-dense net-image-class ification-b 6631 A8 ef 803</a></li><li id="4bda" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke ni mu mv mw bi translated"><a class="ae kf" rel="noopener" href="/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5">https://medium . com/@ amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334 da 4 bfc 5</a></li><li id="2db7" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke ni mu mv mw bi translated"><a class="ae kf" href="https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/keras/optimizer/Adam</a></li><li id="53ea" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke ni mu mv mw bi translated"><a class="ae kf" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/keras/loss/spassecategoricalcrossentropy</a></li><li id="19a4" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke ni mu mv mw bi translated"><a class="ae kf" href="https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/keras/metrics/Accuracy</a></li><li id="703e" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke ni mu mv mw bi translated"><a class="ae kf" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ReduceLROnPlateau" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/keras/callbacks/ReduceLROnPlateau</a></li><li id="447b" class="mo mp hi jl b jm mx jp my js mz jw na ka nb ke ni mu mv mw bi translated"><a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . model _ selection . train _ test _ split . html</a></li></ol></div><div class="ab cl km kn gp ko" role="separator"><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr ks"/><span class="kp bw bk kq kr"/></div><div class="hb hc hd he hf"><p id="41a0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这是完整的模型。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nj"><img src="../Images/ff0e10b8f7e7ce3fc54dbb369ab770d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_vp4AOj4Le9G7scxqi7rQg.png"/></div></div></figure></div></div>    
</body>
</html>