<html>
<head>
<title>Sentiment Analysis with Logistic Regression (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于逻辑回归的情感分析(下)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sentiment-analysis-with-logistic-regression-part-2-5306cff5256b?source=collection_archive---------16-----------------------#2020-12-23">https://medium.com/analytics-vidhya/sentiment-analysis-with-logistic-regression-part-2-5306cff5256b?source=collection_archive---------16-----------------------#2020-12-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="25a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<a class="ae jd" rel="noopener" href="/analytics-vidhya/sentiment-analysis-with-logistic-regression-part-1-a2759f155b09">使用逻辑回归的情感分析(第一部分)</a>中，我们讨论了如何使用逻辑回归进行情感分析的整体方法。在这篇文章中，我们将回顾什么是逻辑回归。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/b634b12358a38ac3bf21868671964f78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h5CDHdp1NomV1-N3Rt2FWQ.png"/></div></div></figure><p id="900d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章是“用逻辑回归进行情感分析”的第二部分。在这篇文章中，我将谈论:</p><ul class=""><li id="899d" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">逻辑回归概述</li><li id="3362" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">训练逻辑回归</li><li id="c518" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">检验逻辑回归</li><li id="b5d0" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">逻辑回归成本函数</li></ul><p id="2361" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ke">免责声明:本文基于Coursera上</em> <strong class="ih hj"> <em class="ke">自然语言处理与分类和向量空间</em> </strong> <em class="ke">课程的第一周。学分以下的大部分数字归课程版权所有。</em></p><p id="e234" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">在这里查看我的最终项目</strong> <em class="ke"> : </em> <a class="ae jd" href="https://github.com/KarenJF/deeplearing_nlp/blob/master/c1_nlp_classification_vec_spaces/week1/C1_W1_Assignment.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="ke">点击链接</em> </a></p><h1 id="766e" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">第二部分:逻辑回归回顾</h1><h2 id="dbd2" class="ld kg hi bd kh le lf lg kl lh li lj kp iq lk ll kt iu lm ln kx iy lo lp lb lq bi translated">1.逻辑回归概述</h2><p id="a80e" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi translated">之前(参见<a class="ae jd" rel="noopener" href="/analytics-vidhya/sentiment-analysis-with-logistic-regression-part-1-a2759f155b09">第1部分</a>)，我们已经学习了如何预处理我们的数据并为我们的情感分析提取特征。我们可以用逻辑回归来预测结果。那什么是逻辑回归？在高层次上，<strong class="ih hj">逻辑回归利用了一个sigmoid函数，它输出0到1之间的概率。</strong></p><ul class=""><li id="3aa7" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">在逻辑回归H中用于分类的函数是<strong class="ih hj"> sigmoid函数</strong>，它取决于参数θ和特征向量Xi，其中I用于表示第I个观察值或数据点。</li><li id="8d42" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">视觉上，当θ转置X的点积接近负无穷大时，sigmoid函数接近0，当θ转置X接近无穷大时，sigmoid函数接近1。</li><li id="8da5" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">对于分类，需要一个阈值。通常，它被设置为0.5，并且该值对应于θ转置和等于零的X之间的点积。</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lw"><img src="../Images/57da35d72b04cefb0db6e955de699b0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XQs3_QABI63nlYg_.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">图片来自自然语言处理第一周的分类和向量空间课程</figcaption></figure><p id="8c7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意，作为</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mb"><img src="../Images/7b5e1af6ded88bd240db62208eec8c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:76/format:webp/1*Gd9kZeRsExUI_ZoCEcKwyQ.png"/></div></figure><ul class=""><li id="94b3" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">越来越接近负无穷大，sigmoid函数的分母越来越大</li><li id="ab60" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">结果，乙状结肠变得更接近0。</li></ul><p id="071f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一方面，作为</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mb"><img src="../Images/7b5e1af6ded88bd240db62208eec8c41.png" data-original-src="https://miro.medium.com/v2/resize:fit:76/format:webp/1*Gd9kZeRsExUI_ZoCEcKwyQ.png"/></div></figure><ul class=""><li id="96cf" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">越接近正无穷大，sigmoid函数的分母越接近1。</li><li id="0ba5" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">结果，乙状结肠也变得更接近1。</li></ul><p id="a94d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">示例</strong>:现在给定一条tweet，我们可以将其转换为一个向量，并通过sigmoid函数运行它，以获得如下预测:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mc"><img src="../Images/298ad1376a5abe2b9fc6b7aea112a17f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*64EaCAAgdYSznrj6.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">图片来自自然语言处理第一周的分类和向量空间课程</figcaption></figure><h2 id="aed8" class="ld kg hi bd kh le lf lg kl lh li lj kp iq lk ll kt iu lm ln kx iy lo lp lb lq bi translated">2.逻辑回归:培训</h2><p id="d440" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi translated">为了训练逻辑分类器，我们可以使用梯度下降来迭代，直到我们找到最小化成本函数的参数集θ。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es md"><img src="../Images/4c5fb840d420bc0f78fb110f697ba7e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1_oJ1kpKUsBm4Y7O.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">图片来自自然语言处理第一周的分类和向量空间课程</figcaption></figure><p id="607a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练逻辑分类器，我们使用梯度下降:</p><ul class=""><li id="9419" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">首先，我们必须初始化参数向量θ。</li><li id="e8d4" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">然后，我们使用逻辑函数来获得每个观察值。</li><li id="9b76" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">然后，我们计算成本函数的梯度并更新参数。</li><li id="0feb" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">最后，我们将能够计算成本J，并根据停止参数或最大迭代次数来确定是否需要更多的迭代。</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es me"><img src="../Images/79d067b671ca71f0603748ac42c012b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DVI398AGH7g5wrv4.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">图片来自自然语言处理第一周的分类和向量空间课程</figcaption></figure><h2 id="d2cb" class="ld kg hi bd kh le lf lg kl lh li lj kp iq lk ll kt iu lm ln kx iy lo lp lb lq bi translated">3.逻辑回归:测试</h2><p id="d950" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi translated">为了测试我们的模型，我们将在模型上运行我们的数据子集，称为<strong class="ih hj">验证集</strong>，以获得预测，并将其与真实标签进行比较，以计算模型的准确性。</p><ul class=""><li id="b877" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">首先，我们计算预测，这是sigmoid函数的输出。</li><li id="eaf9" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">然后，我们将输出与阈值进行比较。通常我们设定阈值= 0.5。如果输出&gt; = 0.5，我们会将预测分配到一个正类。否则，我们会把它分配到一个负类。</li><li id="bece" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">最后，我们将有一个填充了0和1的向量，分别表示预测的负样本和正样本。</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mf"><img src="../Images/2779390d74cb06f4eb29d2fa379d31f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*09gVAB_oyjI9fbYc.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">图片来自自然语言处理第一周的分类和向量空间课程</figcaption></figure><ul class=""><li id="64bd" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">最后，我们可以计算模型在验证集上的准确性。准确度是模型预测与验证集中标签数量的真实标签相匹配的次数。此指标给出了您的逻辑回归对未知数据正确工作的估计时间。</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mg"><img src="../Images/a5b93da6997fca8c891d4f398df56949.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/0*zbIfkTCCm9e5WA2r.png"/></div></figure><h2 id="ccdd" class="ld kg hi bd kh le lf lg kl lh li lj kp iq lk ll kt iu lm ln kx iy lo lp lb lq bi translated">4.逻辑回归:成本函数</h2><p id="ce91" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi translated">逻辑回归成本函数定义为:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mh"><img src="../Images/1c6e670247133f8821e8a784e24c75ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dSuvg2TKWAJDZRWy.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">图片来自自然语言处理第一周的分类和向量空间课程</figcaption></figure><ul class=""><li id="cdcd" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">如果y = 1，我们预测的值接近于0，成本接近于无穷大。</li><li id="1762" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">如果y = 0，我们预测它接近1，成本也接近无穷大。</li><li id="35e9" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">另一方面，如果预测等于标签，成本接近于零。</li><li id="d0d9" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">我们正在尝试最小化成本函数，以使预测尽可能接近标签。</li></ul><h2 id="b857" class="ld kg hi bd kh le lf lg kl lh li lj kp iq lk ll kt iu lm ln kx iy lo lp lb lq bi translated">5.高级主题:逻辑回归成本函数的数学推导</h2><p id="ebc2" class="pw-post-body-paragraph if ig hi ih b ii lr ik il im ls io ip iq lt is it iu lu iw ix iy lv ja jb jc hb bi translated">让我们编写一个函数，将两种情况(1和0)压缩成一种情况。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mi"><img src="../Images/fb75d221f3c0a19dda14f4c372b591cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/0*wcLAD49DfNC0TogH.png"/></div></figure><p id="a76d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面可以看出，当y = 1时，我们得到sigmoid函数h(x(i)，theta)，当y = 0时，我们得到(1-sigmoid)。</p><ul class=""><li id="2696" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">这是有意义的，因为两个概率等于1(即，对于一个类分类，预测是1或0)。</li><li id="3f1d" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">在任何一种情况下(1或0)，我们都希望通过使sigmoid函数h(x(i)，theta)尽可能接近1来使其最大化。</li></ul><p id="8b92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们想找到一种方法来模拟整个数据集，而不仅仅是上面的一个例子。为此，我们将可能性定义如下:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mj"><img src="../Images/3f638ffb38b17234f46c0a64198306bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/0*AcRDVP3akb27WmYw.png"/></div></figure><p id="603a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">∏符号告诉我们，我们将各项相乘，而不是相加。<strong class="ih hj">注意</strong>如果我们弄乱了一个例子的分类，我们最终会弄乱总体可能性得分，这正是我们想要的。我们希望让模型适合所有数据点都相关的整个数据集。</p><p id="93e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">一个问题</strong>是当m变大时，L(θ)趋于零，因为两个数h(x(i)，θ)和(1-h(x(i)，θ))都在0和1之间。</p><ul class=""><li id="2045" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">由于我们试图最大化L(θ)中的sigmoid，所以我们可以引入log，并且只最大化函数的log。</li><li id="efb8" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">引入日志可以让我们把一个产品的日志写成每个日志的总和。</li></ul><p id="cd70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以将等式改写如下:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mk"><img src="../Images/9534285b7a70a9fd02a17daf02d5b8fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/0*OMG5_A7wb6MQw1vT.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">图片来自自然语言处理第一周的分类和向量空间课程</figcaption></figure><p id="80d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们现在除以m，因为我们想看到平均成本。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ml"><img src="../Images/193f1a61dff6c605beddfff6d8b98da4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/0*AkKH_ENaP9OPAqda.png"/></div></figure><p id="20d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">记住我们在上面的等式中最大化了sigmoid。<strong class="ih hj">事实证明，最大化一个方程等于最小化它的负值。</strong>因此，我们添加了一个负号，并最终最小化了成本函数。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mm"><img src="../Images/b4521e6aed502dc2b7908026e74e4247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RQlFJRJdGmDqvS2P.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">图片来自自然语言处理第一周的分类和向量空间课程</figcaption></figure></div><div class="ab cl mn mo gp mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="hb hc hd he hf"><p id="a65f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ke">原载于</em><a class="ae jd" href="https://github.com/KarenJF/deeplearing_nlp/blob/master/c1_nlp_classification_vec_spaces/week1/README.md" rel="noopener ugc nofollow" target="_blank"><em class="ke">https://github.com</em></a><em class="ke">。</em></p></div></div>    
</body>
</html>