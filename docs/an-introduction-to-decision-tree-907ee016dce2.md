# 决策树介绍。

> 原文：<https://medium.com/analytics-vidhya/an-introduction-to-decision-tree-907ee016dce2?source=collection_archive---------20----------------------->

![](img/6ff7b01f004ddb8e4477ce49ee0b2cb4.png)

决策树在现实生活中有许多相似之处，事实证明，它影响了广泛的**机器学习领域，**涵盖了**分类**和**回归。**有时决策树也被称为 **CART，**是分类和回归树的简称。在决策分析中，决策树可用于直观、明确地表示决策和决策制定。

在这篇博客中，我们将讨论以下内容:

*   ***什么是决策树？***
*   ***决策树的类型***
*   ***关键术语***
*   ***如何创建决策树***
*   ***基尼杂质***
*   ***卡方***
*   ***信息增益***
*   ***决策树的应用***
*   ***解码超参数***
*   ***编码算法***
*   ***决策树的优缺点。***
*   ***总结和结论。***

# *为什么选择决策树？*

基于树的算法是用于分类和回归的相关**非参数**和**监督**方法的流行家族。如果你想知道监督学习是什么，它是一种机器学习算法，涉及用具有输入和输出标签的数据训练模型。

决策树看起来像一个模糊的颠倒的树，在根上有一个决策规则，随后的决策规则从下面展开。

也可以有没有任何决策规则的节点；这些被称为**叶节点。**

# 决策树的类型

根据目标变量，决策树分为两种类型。

1.  **分类变量决策树:**这是算法有分类目标变量的地方。例如，假设要求您预测一台计算机的相对价格，分为三类:**低、中、**或**高。**
2.  **连续变量决策树:**在这种情况下，输入到决策树的特征(如房屋质量)将用于预测连续输出(如房屋价格)。

# 关键术语

让我们看看决策树是什么样子的，以及当给定一个新的预测输入时，它们是如何工作的。

下图解释了决策树的基本结构。每棵树都有一个**根节点**，输入在这里传递。这个根节点被进一步划分成决策节点集，其中结果和观察是有条件的。将单个节点划分为多个节点的过程称为**分裂。**如果一个节点没有分裂成更多的节点，那么它被称为**叶节点、**或**终端节点。**决策树的一个子部分被称为**分支**或**子树。**

![](img/00357abe4dba05000727435b77f83871.png)

决策树的示例

还有一个概念与分裂完全相反。如果有可以消除的决策规则，我们就把它们从树上砍下来。这个过程被称为**修剪**，有助于最小化算法的复杂性。

现在我们对基本的决策树有了一个清晰的概念。让我们深入了解一下拆分是如何完成的，以及我们如何自己构建一个决策树。

# 如何创建决策树

在这一节中，我们将讨论描述如何创建决策树的核心算法。这些算法完全依赖于目标变量，然而，这些算法不同于用于分类和回归树的算法。

有几种技术可以用来决定如何分割给定的数据。决策树的主要目标是在节点之间进行最佳分割，从而以最佳方式将数据划分到正确的类别中。为此，我们需要使用正确的决策规则。规则直接影响算法的性能。

在我们开始之前，需要考虑一些假设:

*   在开始时，整个数据被认为是根，此后，我们使用算法进行分裂或将根分成子树。
*   特征值被认为是分类的。如果这些值是连续的，则在构建模型之前会将它们分开。
*   记录是基于属性值递归分布的。
*   属性作为树的根或内部节点的排序是使用统计方法来完成的。

让我们从常用的拆分技术开始，从而构建决策树。

# 基尼杂质

如果所有的元素都被正确地划分到不同的类中(一个理想的场景)，那么划分被认为是**纯的。**基尼系数(发音类似“精灵”)用于衡量随机选择的样本被某个节点错误分类的可能性。它被称为“杂质”度量，因为它让我们了解模型与纯除法有什么不同。

基尼杂质分数的程度总是在 0 到 1 之间，其中 0 表示所有元素都属于某一类(或者划分是纯的)，1 表示元素随机分布在各个类中。基尼系数为 0.5 表示元素被平均分配到某些类别中。基尼系数的数学符号由以下公式表示:

![](img/7e5e49f2c06fe5b52988a4e696b607e2.png)

其中 ***Pi*** 是特定元素属于特定类的概率。

现在，让我们看看使用基尼系数作为指导来计算和构建决策树的伪代码。

```
Gini Index:
    for each branch in a split:
        Calculate percent branch represents  # Used for weighting
        for each class in-branch:
            Calculate the probability of that class in the given branch
            Square the class probability
        Sum the squared class probabilities
        Subtract the sum from 1  # This is the Gini Index for that branch
    Weight each branch based on the baseline probability
    Sum the weighted Gini index for each split
```

我们现在来看一个解释上述算法的简单例子。考虑下面的数据表，其中每个元素(行)有两个描述它的变量和一个相关的类标签。

![](img/cdce7c4a72cd5fd45953f0576133eab8.png)

姬内指数示例:

*   ***Var1 的拆分基线:Var1*** 有 4 个实例(4/10)等于 1，6 个实例(6/10)等于 0。
*   for***Var1***= = 1&***Class***= =***A***:1/4 实例有 Class 等于 *A* 。
*   对于***var 1***= = 1&***Class***= =***B***:3/4 实例有 Class 等于 *B* 。
*   这里的基尼指数是 1-((1/4) + (3/4) ) = 0.375
*   对于***Var1***= = 0&***类****= =***A***:4/6 实例有类等于 *A* 。*
*   *对于***Var1***= = 0&***Class*=**=***B***:2/6 实例有 Class 等于 *B* 。*
*   ***基尼指数**这里是 1-((4/6) + (2/6) ) = 0.4444*
*   *然后，我们根据每个拆分所占数据的基线/比例，对每个拆分进行加权和求和。*
*   *4/10 * 0.375 + 6/10 * 0.444 = 0.41667*

# *信息增益*

*信息增益描述了通过属性获得的信息量。它告诉我们属性有多重要。由于决策树的构建完全是为了找到确保高准确性的正确分裂节点，所以信息增益完全是为了找到返回最高信息增益的最佳节点。这是使用称为**熵**的因子计算的。*

*熵定义了系统的无序程度。无序越多，熵就越大。当样本是完全同质的，那么熵是零，如果样本是部分有序的，比如说 50%的样本是有序的，那么熵是一。*

*这作为确定信息增益的基本因素。熵和信息增益一起用来构造决策树，算法称为 **ID3** 。*

*让我们理解用于计算信息增益的一步一步的过程，从而构建决策树，*

*   *使用以下公式计算输出属性(分割前)的熵:*

*![](img/6daf36325794e6d5bafab4346f129286.png)*

*这里 p 是成功的概率，q 是节点失败的概率。比如说，10 个数据值中，5 个属于*真*，5 个属于*假*，那么 *c* 计算为 2， *p_1* 和 *p_2* 计算为。*

*   *使用公式计算所有输入属性的熵，*

*![](img/7cbafa20f79f381884b0615c1412302f.png)*

*t 是输出属性，*

*x 是输入属性，*

*P(c)是 X 处可能出现的数据点的概率，以及*

*E(c)是与可能的数据点相关的熵 w . r . t '' True'。*

*假设输入属性(优先级)有两个可能的值，低*和高*和*高*。关于*低，*有 5 个数据点相关，其中 2 个属于*真*，3 个属于*假。*相对于*高*，其余 5 个数据点相关联，其中 4 个属于*真*，1 个属于*假。那么 E(T，X)就是，**

*![](img/2111aa8f23d521e5d9ded114eff59327.png)*

*在 E(2，3)中，p 是 2，q 是 3。*

*在 E(4，1)中，p 是 4，q 是 1。*

*对给定数据集中的所有输入属性重复进行相同的计算。*

*   *使用上述两个值，通过从分割前的总熵中减去每个属性的熵来计算信息增益或熵的减少，*

*![](img/248ab10d1f8b3e78d772723074c289ab.png)*

*   *选择具有最高信息增益的属性作为分割节点。*
*   *根据分割重复步骤 1-4，分割数据集。该算法一直运行，直到所有数据都被分类。*

***要点记住:***

*   *叶节点是没有熵的节点，或者熵为零的节点。在叶节点上不再进行进一步的分割。*
*   *只有需要进一步分裂的分支，即熵> 0 时(有杂质时)才需要经历这个分裂过程。*

# *卡方检验*

*如果目标变量是分类的，如成功-失败/高-低，卡方方法很有效。该算法的核心思想是找出子节点和父节点之间存在的差异的统计显著性。用于计算卡方的数学方程是，*

*![](img/476b524af0c0b90bcf706b9c85f7a219.png)*

*它代表目标变量的观察频率和预期频率之间的标准化差异的平方和。*

*使用卡方的另一个主要优势是，它可以在单个节点上执行多次分割，从而提高准确度和精度。*

# *决策树的应用*

*决策树是机器学习领域中最基本也是最广泛使用的算法之一。它在分类和回归建模的不同领域得到了应用。由于其描绘可视化输出的能力，人们可以很容易地从建模过程流中获得洞察力。这里有几个可以使用决策树的例子，*

*   *企业管理*
*   *客户关系管理*
*   *欺诈性声明检测*
*   *能耗*
*   *医疗保健管理*
*   *故障诊断*

# *解码超参数*

*Scikit-learn 提供了一些与决策树分类器一起使用的功能或参数，以根据给定的数据提高模型的准确性。*

*   ***判据:**该参数用于衡量分割的质量。该参数的默认值设置为“Gini”。如果你想用熵增益来计算测度，可以把这个参数改成“熵”。*
*   ***分割器**:该参数用于选择每个节点的分割器。如果希望子树具有最佳分割，可以将该参数设置为“最佳”。我们还可以有一个随机分割，其值设置为“随机”。*
*   ***max-depth:** 这是一个整数参数，通过它我们可以限制树的深度。该参数的默认值设置为 None。*
*   ***min_samples_split:** 该参数用于定义分割一个内部节点所需的最小样本数。*
*   ***max _ leaf _ nodes:**max _ leaf _ nodes 的默认值设置为无。该参数用于以最佳优先的方式生长具有 max_leaf_nodes 的树。*

# *算法编码*

## *步骤 1:导入模块*

*构建决策树模型的第一步也是最重要的一步是导入必要的包和模块。我们从 **sklearn** 包中导入 **DecisionTreeClassifier** 类。这是一个内置的类，其中编码了整个决策树算法。在这个程序中，我们将使用可以从 **sklearn.datasets** 导入的 **iris** 数据集。 **pydotplus** 包用于可视化决策树。下面是代码片段，*

```
*import pydotplus
from sklearn.tree import DecisionTreeClassifier 
from sklearn import datasets*
```

## *步骤 2:探索数据*

*接下来，我们通过使用 **load_iris()** 方法从 datasets 包中加载数据来准备好数据。我们将数据分配给**虹膜**变量。这个 iris 变量有两个键，一个是**数据**键，其中显示所有输入，即萼片长度、萼片宽度、花瓣长度和花瓣宽度。在 **target** 键中，我们有花朵类型，其值为鸢尾、杂色鸢尾和海滨鸢尾。我们将这些分别加载到**特征**和**目标**变量中。*

```
*iris = datasets.load_iris()  
features = iris.data  
target = iris.target  
print(features) 
print(target)Output:  
[[5.1 3.5 1.4 0.2]  
[4.9 3\.  1.4 0.2]  
[4.7 3.2 1.3 0.2]  
[4.6 3.1 1.5 0.2]  
[5.8 4\.  1.2 0.2]  
[5.7 4.4 1.5 0.4] 
. . . .  
. . . . ] 
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]*
```

*这是我们的数据集的样子。*

## *步骤 3:创建决策树分类器对象*

*这里，我们将 **DecisionTreeClassifier** 加载到一个名为 **model** 的变量中，这个变量是之前从 **sklearn** 包中导入的。*

```
*decisiontree = DecisionTreeClassifier(random_state=0)*
```

## *步骤 4:拟合模型*

*这是训练过程的核心部分，通过对给定数据进行分割来构建决策树。我们用作为参数发送给 **fit()** 方法的**特征**和**目标**值来训练算法。这种方法是通过在特征和目标上训练模型来拟合数据。*

```
*model = decisiontree.fit(features, target)*
```

## *第五步:做预测*

*在这一步中，我们进行样本观察并做出预测。我们创建了一个新的列表，包括花的萼片和花瓣的尺寸。此外，我们在经过训练的模型上使用 **predict()** 方法来检查它所属的类。我们还可以通过使用 **predict_proba** 方法来检查预测的概率(类概率)。*

```
*observation = [[ 5, 4, 3, 2]] # Predict observation's class
model.predict(observation)
model.predict_proba(observation)Output:
array([1])
array([[0., 1., 0.]])*
```

## ***步骤 6:用于预测的点数据***

*在这一步中，我们以点格式(一种图形描述语言)导出训练好的模型。为了实现这一点，我们使用了可以从 **sklearn** 包导入的**树**类。最重要的是，我们使用 **export_graphviz** 方法，将决策树、特性和目标变量作为参数。*

```
*from sklearn import treedot_data = tree.export_graphviz(decisiontree, out_file=None,
feature_names=iris.feature_names, 
class_names=iris.target_names
)*
```

## ***第七步:绘制图形***

*在最后一步，我们使用一个从 **IPython.display** 包导入的 **Image** 类来可视化决策树。*

```
*from IPython.display import Imagegraph = pydotplus.graph_from_dot_data(dot_data) # Show graphImage(graph.create_png())*
```

*![](img/7601b1f4d003816d31d0a84aba43b01d.png)*

# *优点和缺点*

*决策树有一些优点和缺点。先说优点。与其他算法相比，决策树在处理数据时花费的时间非常少。可以跳过一些预处理步骤，如数据的标准化、转换和缩放。尽管数据集中存在缺失值，但模型的性能不会受到影响。决策树模型直观且易于向技术团队和利益相关者解释，并且可以跨多个组织实施。*

*缺点来了。不确定树，数据的微小变化会导致决策树结构的巨大变化，进而导致不稳定。训练时间急剧增加，与数据集的大小成比例。在某些情况下，与其他传统算法相比，计算可能会变得复杂。*

# *总结和结论*

*在本文中，我们已经深入讨论了决策树算法。这是一种监督学习算法，可用于分类和回归。决策树的主要目标是根据一组规则和条件将数据集分割成一棵树。我们讨论了决策树的关键组成部分，如根节点、叶节点、子树、分裂和修剪。此外，我们已经看到了决策树是如何工作的，以及如何使用流行的算法如 GINI、信息增益和卡方来执行战略分割。此外，我们使用 scikit-learn 在 IRIS 数据集上从头开始编码决策树。最后，我们讨论了使用决策树的优点和缺点。还有很多东西需要学习，本文将为您提供探索其他高级分类算法的快速入门。*