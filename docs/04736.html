<html>
<head>
<title>The battle to run my custom network on a Movidius / Myriad Compute Stick</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Movidius / Myriad计算机棒上运行我的定制网络的战斗</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-battle-to-run-my-custom-network-on-a-movidius-myriad-compute-stick-c7c01fb64126?source=collection_archive---------12-----------------------#2020-03-30">https://medium.com/analytics-vidhya/the-battle-to-run-my-custom-network-on-a-movidius-myriad-compute-stick-c7c01fb64126?source=collection_archive---------12-----------------------#2020-03-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e8acd591f75431c7b43bb180c30ac2bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zet0WfMXcPg4o8Ep"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae iu" href="https://unsplash.com/@_louisreed?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">路易斯·里德</a>拍摄的照片</figcaption></figure><h1 id="7075" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">英特尔移动</h1><p id="3d0d" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><a class="ae iu" href="https://software.intel.com/en-us/neural-compute-stick" rel="noopener ugc nofollow" target="_blank">英特尔Movidius神经计算棒2 </a>吸引了我的注意，因为它是一种廉价且看似简单的可能性，可以在边缘设备上运行神经网络并进行一些严肃的人工智能推理(例如树莓Pi)。它的价格大约是70€，非常实惠。在我的大学里，越来越多的科研人员身边都有一个——不知道该拿它怎么办。由于我的部门并不真正做视觉或NLP，我们不想从模型动物园移植一个标准模型，而是在Movidius上运行我们自己的网络。所以我开始思考如何做到这一点——结果并不那么简单。为了让其他人更容易理解，我决定写这篇博文。希望有帮助！从Scikit-Learn、Pytorch和Keras转换简单NN的示例代码可在我的<a class="ae iu" href="https://github.com/Elli1993/custom_net_on_movidius" rel="noopener ugc nofollow" target="_blank"> github </a>上获得。</p><h1 id="77f7" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">构建和训练您的模型</h1><p id="3bec" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我们希望我们的模型训练方法尽可能通用。在尝试使用类似于<a class="ae iu" rel="noopener" href="/@oviyum/how-to-port-a-custom-tensorflow-model-to-movidius-ncs-6c2d2ba75977"> Oviyum </a>的NCSDK直接从Tensorflow 2.0移植模型时，我感到有些绝望，之后我发现了一种使用<a class="ae iu" href="https://onnx.ai/index.html" rel="noopener ugc nofollow" target="_blank"> ONNX </a>格式的相当简单的传输方式。使用它，我们可以在PyTorch、Tensorflow、Keras甚至SciKit Learn中构建和训练我们的网络——以及任何能够导出到ONNX的内容。在他们的主页上，他们提供了可能的框架和功能的概述。但并非所有功能都得到英特尔的支持，可以在<a class="ae iu" href="https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_Supported_Frameworks_Layers.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到这些功能的列表。我计划在未来的某个时候拥有一个完整的转换器，但是到目前为止，你必须自己检查你的所有层/功能是否都被支持。</p><h1 id="bf07" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">装置</h1><p id="f244" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">首先，我们需要<a class="ae iu" href="https://docs.openvinotoolkit.org/2019_R3.1/_docs_install_guides_installing_openvino_windows.html" rel="noopener ugc nofollow" target="_blank"> Openvino Toolkit </a>将我们的ONNX模型转换成称为中间表示(IR)的Movidius格式。我在我的Windows 10上运行了它，并使用我的Anaconda Shell测试了这些示例——不要忘记在每次打开一个新的Shell时运行bash脚本来首先设置环境变量！此外，你还需要python(第3版——在Windows上运行时，我推荐使用<a class="ae iu" href="https://docs.anaconda.com/anaconda/install/" rel="noopener ugc nofollow" target="_blank"> Anaconda </a>)、一个你自己选择的IDE(我更喜欢<a class="ae iu" href="https://www.jetbrains.com/de-de/pycharm/" rel="noopener ugc nofollow" target="_blank"> PyCharm </a>)和一些额外的包(最好安装在<a class="ae iu" href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html" rel="noopener ugc nofollow" target="_blank">虚拟环境</a>)。你需要一个框架来创建你的模型(Pytorch，Keras或者SciKit Learn或者其他什么)，ONNX包，也许还有你的框架附带的转换器。例如，对于Keras转换，您需要运行:</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="d0e8" class="la iw hi kw b fi lb lc l ld le">conda install scikit-learn, keras, pandas<br/>pip install keras2onnx</span></pre><h1 id="5b24" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">导出您的模型</h1><p id="8b95" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">训练完模型后，您必须将模型和权重导出为ONNX格式。真的很简单。对于您的keras模型，您只需:</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="b062" class="la iw hi kw b fi lb lc l ld le">import keras2onnx<br/>import onnx</span><span id="971c" class="la iw hi kw b fi lf lc l ld le">onnx_model = keras2onnx.convert_keras(model, model.name)<br/>onnx.save_model(onnx_model, 'keras_iris_model.onnx')</span></pre><p id="494a" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">你可以在我的<a class="ae iu" href="https://github.com/Elli1993/custom_net_on_movidius" rel="noopener ugc nofollow" target="_blank"> github </a>上从keras、pytorch和sklearn中找到将你的模型保存为onnx的代码示例。如果可能，尝试保存不带训练参数的模型。</p><p id="09c4" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">现在让我们看看我们的模型，并检查我们保存了哪些参数。</p><figure class="kr ks kt ku fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/827a797debafd78874e3dee4ef485780.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VcdxKqLO3n3C7g5GzgsUyA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用sklearn创建的网络的一部分(使用Netron可视化)</figcaption></figure><p id="1108" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">要打开onnx文件，我推荐Netron。它可以让你看到你所有的层，连接，甚至重量。漂亮！但是正如你所看到的，我们的网络不仅仅是明确定义的层。取决于你的框架，这个模型仍然有一些训练参数或者一些方便的解码参数，就像这个sklearn模型。例如，这里的Zip-Map将输出索引映射到一个具体的类名——这很好，但是对于Openvino模型转换来说是不可能的。</p><h1 id="96e7" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">修剪您的模型</h1><p id="dc1d" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">为了成功地使用Openvino转换你的模型，你(有时)需要修剪它。这意味着，您必须删除所有模型不需要的或者对Openvino转换不可用的内容。使用python onnx包，可以很容易地删除ONNX中调用的层或节点。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="0ca9" class="la iw hi kw b fi lb lc l ld le">import onnx<br/>onnx_model = onnx.load(model_path)<br/>graph = onnx_model.graph<br/>graph.node.remove(graph.node[9])<br/>onnx.save(onnx_model, save_path)</span></pre><p id="c2ac" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">我知道这不是干净的方法，但它仍然有效。这里的长期目标是一个图形修剪程序，删除所有对Openvino不可用的节点，如果模型不可转换，就会抛出一个错误。如果你有这样的事情，不要害羞，分享一下。在此之前，我们需要手动操作。</p><h1 id="b432" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">将您的模型转换为Openvino IR格式</h1><p id="59a5" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">既然我们已经有了“完美的”ONNX模型，我们就可以按照“极其丰富的”<a class="ae iu" href="https://docs.openvinotoolkit.org/latest/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html" rel="noopener ugc nofollow" target="_blank">英特尔教程</a>来进行转换。—这不是很有帮助……所以还是按照这些说明去做吧:</p><p id="53f6" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">1)以管理员身份打开一个Shell(Anaconda)<br/>2)激活setupvars.bat <br/> 3)确保安装了正确的<a class="ae iu" href="https://docs.openvinotoolkit.org/2019_R3.1/_docs_install_guides_installing_openvino_windows.html#set-the-environment-variables" rel="noopener ugc nofollow" target="_blank">先决条件</a> <br/> 4)转到英特尔模型优化器目录(C:\ Program Files(x86)\ Intel swtools \ Open vino \ deployment _ tools \ model _ optimizer \)<br/>5)运行提供输入形状和模型目录的mo.py脚本</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="9483" class="la iw hi kw b fi lb lc l ld le">python mo.py — input_model Path\to\your\model.onnx — input_shape [10,4] — log_level WARNING</span></pre><p id="64c3" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">6)查看成功信息并举行小型聚会<br/> 7)复制。xml和。模型优化器给你的bin文件到你选择的目录</p><p id="4a69" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">我花了一些时间才弄明白这一点，所以这里是我遇到的最常见和最神秘的错误:</p><p id="e393" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">-并非所有输出形状都是为节点“Add”推断或完全定义的。--&gt;意味着您需要设置输入形状<br/> -无法推断节点“ZipMap”的形状或值。<br/> [错误op = " ZipMap "的节点" ZipMap "没有注册的" infer "函数。请在扩展中实现此功能。- &gt;这意味着您需要修剪您的模型，因为ZipMap函数对Openvino不可用<br/> - PermissionError: [Errno 13]权限被拒绝:“Path”-&gt;您需要以管理员身份启动shell</p><h1 id="0151" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">在Movidius上运行您的模型</h1><p id="8214" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在这里，我们可以再次依赖英特尔提供的精彩的<a class="ae iu" href="https://docs.openvinotoolkit.org/latest/_docs_IE_DG_inference_engine_intro.html" rel="noopener ugc nofollow" target="_blank">文档和用户指南</a>——不！</p><p id="9ac8" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">唯一对我有帮助的是看了下C:\ Program Files(x86)\ Intel swtools \ open vino \ _ 2019 . 3 . 379 \ deployment \ _ tools \ inference \ _ engine \ samples \ python \ _ samples下工具包附带的例子——尽管它们不包含太多注释。如果您想在您的IDE中运行代码，请确保在从同一个shell启动您的IDE之前激活setupvars.bat(或任何其他方式进行设置)。为了运行我们的网络，我们需要导入两个类IECore和IENetwork。我们加载我们的模型，定义输入和输出blobs，加载棒上的网络，现在终于准备好推断一些东西了！在我的<a class="ae iu" href="https://github.com/Elli1993/custom_net_on_movidius" rel="noopener ugc nofollow" target="_blank"> github </a>上还有一个更深入的代码。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="aaf1" class="la iw hi kw b fi lb lc l ld le">from openvino.inference_engine import IECore, IENetwork<br/>ie = IECore()<br/>net = IENetwork(model=model_xml, weights=model_bin)<br/>input_blob = next(iter(net.inputs))<br/>out_blob = next(iter(net.outputs))<br/>exec_net = ie.load_network(network=net, device_name=’MYRIAD’)<br/>res = exec_net.infer(inputs={input_blob: X_test[:10,:]})<br/>res = res[out_blob]</span></pre><p id="786e" class="pw-post-body-paragraph jt ju hi jv b jw lg jy jz ka lh kc kd ke li kg kh ki lj kk kl km lk ko kp kq hb bi translated">我希望这有助于您开始使用适用于定制型号的Movidius计算棒，并避免您对我遇到的英特尔文档感到恼怒和绝望。如果您有一些提示或改进，请随时分享！</p></div></div>    
</body>
</html>