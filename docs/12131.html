<html>
<head>
<title>ML17: Tuning Deep Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML17:调整深层网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ml17-a2f9315e5f1a?source=collection_archive---------21-----------------------#2020-12-31">https://medium.com/analytics-vidhya/ml17-a2f9315e5f1a?source=collection_archive---------21-----------------------#2020-12-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><div class=""><h2 id="cd9d" class="pw-subtitle-paragraph ie hg hh bd b if ig ih ii ij ik il im in io ip iq ir is it iu iv dx translated">活化剂、优化剂、时期、小批量、BN、漏失和重量衰减</h2></div><p id="3fe0" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iy hi">关键词</strong>:激活函数、优化器、时期、小批量、批量标准化、漏失、重量衰减</p></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><blockquote class="jz ka kb"><p id="1fb8" class="iw ix kc iy b iz ja ii jb jc jd il je kd jg jh ji ke jk jl jm kf jo jp jq jr ha bi translated"><strong class="iy hi"> <em class="hh">概要</em> </strong> <em class="hh"> <br/> (1) </em> <a class="ae kg" href="#6b6b" rel="noopener ugc nofollow"> <em class="hh">激活器(激活功能)</em></a><em class="hh"><br/>(2)</em><a class="ae kg" href="#9153" rel="noopener ugc nofollow"><em class="hh">优化器</em></a><em class="hh"><br/>(3)</em><a class="ae kg" href="#e332" rel="noopener ugc nofollow"><em class="hh">历元和小批量</em></a><em class="hh"><br/>【4】</em><a class="ae kg" href="#5ce3" rel="noopener ugc nofollow"><em class="hh">批量正常化(BN)【说明】</em></a></p></blockquote></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="6b6b" class="kh ki hh bd kj kk kl km kn ko kp kq kr in ks io kt iq ku ir kv it kw iu kx ky bi translated">(1)激活剂(激活功能)</h1><ul class=""><li id="dd00" class="kz la hh iy b iz lb jc lc jf ld jj le jn lf jr lg lh li lj bi translated"><em class="kc"> sigmoid()、tanh()、ReLU()、PReLU()、LeakyReLU()、eLU()、SELU()、Softmax()【用于分类】</em></li><li id="66ea" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">一般来说，<em class="kc"> ReLU() </em>已经足够好了，尤其是当人们试图减少深度神经网络的训练时间时。</li><li id="6311" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated"><em class="kc"> ELU() </em>已被证明是卷积层之间使用<strong class="iy hi">的一种有前途的非线性</strong> (Clevert 等人，2015)。</li></ul></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="9153" class="kh ki hh bd kj kk kl km kn ko kp kq kr in ks io kt iq ku ir kv it kw iu kx ky bi translated">(2)优化器</h1><ul class=""><li id="a3f3" class="kz la hh iy b iz lb jc lc jf ld jj le jn lf jr lg lh li lj bi translated">SGD，Momentum，AdaGrad，RMSprop，Adam，Nadam，AdaMax </li><li id="5a37" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">一般来说，<em class="kc">亚当</em>就足够好了。</li></ul><figure class="lq lr ls lt fd lu er es paragraph-image"><div class="er es lp"><img src="../Images/67d888b142d1eeb1c80269bf57961497.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*1eLr2joc_Fzw87UDy1W7bA.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">图 1:优化者比较— SGD，Momentum，AdaGrad，Adam。</figcaption></figure></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="e332" class="kh ki hh bd kj kk kl km kn ko kp kq kr in ks io kt iq ku ir kv it kw iu kx ky bi translated">(3)时期和小批量</h1><ul class=""><li id="a28b" class="kz la hh iy b iz lb jc lc jf ld jj le jn lf jr lg lh li lj bi translated">我们的算法学习模型的速度之间的关系通常是<strong class="iy hi"> U 形</strong>(批量大小与训练速度)。这意味着最初<strong class="iy hi">随着批量变大，培训时间将减少</strong>。最终，当我们超过某个过大的批量时，我们会看到训练时间开始增加。</li><li id="cca6" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated"><strong class="iy hi">更大的小批量</strong>意味着<strong class="iy hi">更平滑的梯度</strong>。</li><li id="0de5" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">为了性能(这在 GPU 的情况下是最重要的)，我们<strong class="iy hi">应该使用 32 的倍数作为批量大小</strong>，或者如果不能使用 32 的倍数，则使用 16、8、4 或 2 的倍数(或者在给定其他要求的情况下导致太大的变化)。简而言之，原因很简单:<strong class="iy hi">内存访问和硬件设计得到了更好的优化</strong>，与其他尺寸相比，可以在尺寸为 2 的幂的数组<strong class="iy hi">上运行。例如，我们应该在尺寸 125 上使用尺寸 128 的层，或者在尺寸 250 上使用尺寸 256 的层，等等。</strong></li><li id="e5f6" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">实际操作中，CPU 训练常见 32 到 256，<strong class="iy hi">GPU 训练常见 32 到 1024</strong>。通常，这个范围内的东西对于较小的网络来说已经足够好了，尽管您可能应该对较大的网络进行测试(在较大的网络中，训练时间可能会受到限制)。</li><li id="0510" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">小批量和时期之间的关系:如果我们将小批量增加两倍，我们<strong class="iy hi">需要将时期的数量增加两倍</strong>，以便<strong class="iy hi">保持相同的参数更新数量</strong>。每个时期的参数更新次数正好是我们的训练集中的样本总数除以最小批量大小。</li><li id="38bf" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">使用更大的小批量<strong class="iy hi">可能有助于我们的网络在一些困难的情况下进行学习</strong>，例如对于<em class="kc">有噪声的</em> <strong class="iy hi"> </strong>或<em class="kc">不平衡的数据集</em>。</li></ul></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="5ce3" class="kh ki hh bd kj kk kl km kn ko kp kq kr in ks io kt iq ku ir kv it kw iu kx ky bi translated">(4)批量标准化(BN)</h1><ul class=""><li id="ccec" class="kz la hh iy b iz lb jc lc jf ld jj le jn lf jr lg lh li lj bi translated">Ioffe &amp; Szegedy 在 2015 年推出，允许模型对参数<em class="kc"/><strong class="iy hi"><em class="kc">的初始化不太敏感，简化了学习率</em> </strong>的调整。</li></ul></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="9c81" class="kh ki hh bd kj kk kl km kn ko kp kq kr in ks io kt iq ku ir kv it kw iu kx ky bi translated">(5)辍学</h1><ul class=""><li id="78fc" class="kz la hh iy b iz lb jc lc jf ld jj le jn lf jr lg lh li lj bi translated">由 Geoffrey E. Hinton 等人于 2012 年提出。Nitish Srivastava 等人于 2014 年详细阐述。通常<strong class="iy hi"> 10% ~ 50% </strong>。在<em class="kc"> CNN </em>，往往<strong class="iy hi"> 40% ~ 50% </strong>。在<em class="kc"> RNN </em>，通常为<strong class="iy hi">的 20% ~ 30% </strong>。</li><li id="9a77" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">丢失和权重衰减都是正则化技术。</li><li id="49f0" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">由此产生的神经网络可以被视为所有这些较小的神经网络的<strong class="iy hi">平均集合</strong>。</li><li id="54d5" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">有时我们会在输入端使用<strong class="iy hi">无丢失，特别是对于嘈杂或稀疏的数据集。</strong></li><li id="1713" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">在输出层上使用 dropout<strong class="iy hi">并不常见。</strong></li><li id="5f9e" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">在所有隐藏层中取消比只在一个隐藏层中取消更有效。</li><li id="59b4" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">即使没有其他正则化技术，Dropout 也倾向于使隐藏单元<strong class="iy hi">的激活稀疏</strong>，导致<strong class="iy hi">稀疏表示</strong>。</li></ul></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="6f8d" class="kh ki hh bd kj kk kl km kn ko kp kq kr in ks io kt iq ku ir kv it kw iu kx ky bi translated">(6)重量衰减</h1><ul class=""><li id="893f" class="kz la hh iy b iz lb jc lc jf ld jj le jn lf jr lg lh li lj bi translated">L1(拉索)或 L2(岭)正则化。在<em class="kc"> torch.optim </em>中，参数<em class="kc"> weight_decay (float，可选)</em>采用 L2。</li><li id="2abd" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">丢失和权重衰减都是正则化技术。</li><li id="32e7" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">L1: <strong class="iy hi">稀疏模型</strong>。L1 对大权重的惩罚较少，但导致许多权重被驱动到 0(或非常接近 0)，这意味着得到的权重向量可以是稀疏的。</li><li id="7c1c" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">L2: <strong class="iy hi">密集车型</strong>。L2 对重量大的处罚更重，但不会把重量小的罚到 0。</li><li id="d982" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">在实践中，我们看到<strong class="iy hi"> L2 正则化<em class="kc">在显式特征选择之外给出了优于 L1 的性能</em> </strong>。</li><li id="2565" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">我们可以把和放在一个网里，即弹性网(邹和哈斯蒂，2005)。</li><li id="4125" class="kz la hh iy b iz lk jc ll jf lm jj ln jn lo jr lg lh li lj bi translated">对于我们使用早期停止的情况，我们可能根本不想使用 L2 正则化，因为<strong class="iy hi">早期停止<em class="kc">在执行与 L2 相同的机制时更有效</em></strong>(Bengio，2012)。</li></ul></div><div class="ab cl js jt go ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="ha hb hc hd he"><h1 id="1e8f" class="kh ki hh bd kj kk kl km kn ko kp kq kr in ks io kt iq ku ir kv it kw iu kx ky bi translated">(7)参考文献</h1><p id="87ef" class="pw-post-body-paragraph iw ix hh iy b iz lb ii jb jc lc il je jf mb jh ji jj mc jl jm jn md jp jq jr ha bi translated">[1] Géron，A. (2019)。使用 Scikit-Learn、Keras 和 TensorFlow 进行机器学习(第二版。).加利福尼亚州:奥赖利媒体。</p><p id="1b3f" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[2] Rao，d .和 McMahan，B. (2019 年)。用 PyTroch 进行自然语言处理。加利福尼亚州:奥赖利媒体。</p><p id="494a" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">[3]j .帕特森和 a .吉布森(2017 年)。一个实践者的方法。加利福尼亚州:奥赖利媒体。</p><p id="fbb2" class="pw-post-body-paragraph iw ix hh iy b iz ja ii jb jc jd il je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi">[4] 斎藤康毅 (2016). ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実. Japan, JP: O’Reilly Japan.</p></div></div>    
</body>
</html>