# 关于机器学习和深度学习中的梯度下降！

> 原文：<https://medium.com/analytics-vidhya/all-about-gradient-descent-in-machine-learning-and-deep-learning-3dea4b269bf0?source=collection_archive---------16----------------------->

![](img/ee9111798be5960033417bf7eb940f99.png)

有没有想过机器学习算法如何给我们最佳结果，无论是预测、分类还是任何其他？这些算法是如何工作的？这些算法背后的数学原理是什么，导致了可以被用作评估现实世界问题的工具的结果？**梯度下降**算法就是让这种神奇成为可能的算法。

> `Gradient Descent`是每个机器学习算法的支柱，也是许多深度学习优化器的基础。
> 
> 它基于一个简单的机制工作，通过迭代上面显示的值的误差曲线来找到损失函数的最佳权重，通过该曲线，模型预测和实际值的误差最小。

考虑这样一种情况，假设我们手动使用 15 个值，并根据任何损失函数计算每个值的损失，在这种情况下，我们可以这样做，因为值只有 15，这是一个非常小的数字，但它不会导致最佳结果，在现实世界中，我们必须使用许多值，以便得出一个使损失最小化的值，即模型预测和现实世界值中的误差。这就是梯度下降的作用。

*实际上这种算法所做的是，它只是计算必须最小化的函数的导数(在这个方程中，梯度意味着导数),并不断地朝着最小化损失的方向工作*。现在，有些人可能会问，计算函数的导数没什么大不了的，但我对这些人的问题是，他们能计算一个 100 次方程的导数吗？我知道这对人类来说是不可能的。这就是为什么我们需要借助梯度下降算法。

该算法的思路是最小化导数，使点收敛于损失函数曲线的全局极小值。

![](img/c3efa502304c73c68cfcde18f7c76010.png)

损失函数曲线

现在，假设一个点位于上述曲线边界的左侧，那么如果我们画一条穿过该点的垂直线，如曲线所示，那么这条线的**斜率肯定是负的**，&如果我们继续增加损失函数值，或者换句话说，如果我们继续增加梯度(它是负的)，**它将收敛于全局最小值**。如果该点最初出现在上述曲线边界的右侧，从该点开始，如果我们画一条线，**它的斜率将是正的**，现在我们必须减去梯度，因为它是正的，以便达到全局最小值。

> 因此，根据上面的解释，梯度下降以这样一种方式决定损失函数，它满足正的和负的梯度条件。

到这里为止，根据我的解释梯度下降功可以表示为:

**权重=权重-导数(损失函数)→等式(1)**

在假设导数太大或太小的情况下，上述方程可能会有一些问题，那么到达收敛点，即损失函数图中最优权重所在的最小点将会有问题。

为了处理上述问题，我们使用一个名为`Learning rate`的参数来控制上述问题。学习率乘以导数(损失函数)，这有助于控制导数(损失函数)的增长。

![](img/1ed6576e125d8a94e413a626b544fd8d.png)

与低效学习率相关的问题

在上面显示的图表中，低效学习率的问题被清楚地描绘出来，我们可以清楚地看到，如果学习率很大，那么它将使导数(损失函数)向特定方向射出，如果它很小，那么导数(损失函数)将达到全局最小值，但这将需要很长一段时间。

> 所以，我们必须相应地选择学习率，最佳的学习率值可以通过实验找到。

***现在，最终方程变成:***

**权重=权重— ( learning_rate *导数(损失函数))→** `**Final Equation**`

我希望这篇文章是用简单易懂的语言写的，并且尽可能地解释了这个话题。非常感谢您在本文中投入时间。