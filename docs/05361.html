<html>
<head>
<title>Introduction to Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-linear-regression-5e57137a1293?source=collection_archive---------33-----------------------#2020-04-18">https://medium.com/analytics-vidhya/introduction-to-linear-regression-5e57137a1293?source=collection_archive---------33-----------------------#2020-04-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/eadb264d68057970cd593ec1a66d0b07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*34KwcpvXQXb1kkOyS8xCQg.jpeg"/></div></div></figure><p id="130d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个博客中，我们将讨论一个基本而重要的话题，“线性回归”。为其他机器学习算法奠定了基础。</p><h1 id="fb5c" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">什么是“线性回归”？</h1><p id="9c7c" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">线性回归是一种<strong class="is hj">线性模型</strong>，例如，假设输入变量(x)和单个输出变量(y)之间存在线性关系的模型。更具体地说，y可以从输入变量(x)的线性组合中计算出来。</p><p id="8bc6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当只有一个输入变量(x)时，该方法被称为<strong class="is hj">简单线性回归</strong>。当有<strong class="is hj">多个输入变量</strong>时，统计学文献通常将该方法称为多元线性回归。</p><blockquote class="kr ks kt"><p id="6d80" class="iq ir ku is b it iu iv iw ix iy iz ja kv jc jd je kw jg jh ji kx jk jl jm jn hb bi translated">简而言之<strong class="is hj"> <em class="hi">“利用变量之间的关系，找到可以用来进行预测的最佳拟合线或回归方程”。</em>T9】</strong></p></blockquote><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/419f1030d0c656280cddac0be0c6f90d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*_xQi_TIO2OToCzSCF6Xdzw.png"/></div><figcaption class="ld le et er es lf lg bd b be z dx translated">样品高度与重量线性回归</figcaption></figure><h1 id="677a" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">线性回归假设</h1><p id="51f5" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">线性回归模型可以由下面的等式表示</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/6d0d1c9b64cdf1b589cb38d4f926ba04.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/0*Y40vLDvx0KRgV293.png"/></div></figure><ul class=""><li id="e181" class="li lj hi is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated"><em class="ku"> Y </em>是预测值。</li><li id="edd5" class="li lj hi is b it lr ix ls jb lt jf lu jj lv jn ln lo lp lq bi translated"><em class="ku"> θ </em> ₀是偏差项。</li><li id="8437" class="li lj hi is b it lr ix ls jb lt jf lu jj lv jn ln lo lp lq bi translated"><em class="ku"> θ </em> ₁,…，<em class="ku"> θ </em> ₙ为模型参数</li><li id="598a" class="li lj hi is b it lr ix ls jb lt jf lu jj lv jn ln lo lp lq bi translated"><em class="ku"> x </em> ₁，<em class="ku"> x </em> ₂,…，<em class="ku"> x </em> ₙ为特征值。</li></ul><p id="d1d8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上述假设也可以表示为</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/512b1472f5c3b9d27d9857bbd222e775.png" data-original-src="https://miro.medium.com/v2/resize:fit:162/format:webp/0*tWb7AJUCTMEuN4C1.png"/></div></figure><p id="1894" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里</p><ul class=""><li id="c582" class="li lj hi is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated"><em class="ku"> θ </em>是模型的参数向量，包括偏差项<em class="ku"> θ </em> ₀</li><li id="d68e" class="li lj hi is b it lr ix ls jb lt jf lu jj lv jn ln lo lp lq bi translated"><em class="ku"> x </em>是特征向量wit <strong class="is hj"> h <em class="ku"> x </em> ₀ =1 </strong></li></ul><h2 id="389e" class="lx jp hi bd jq ly lz ma ju mb mc md jy jb me mf kc jf mg mh kg jj mi mj kk mk bi translated">训练模型是什么意思？</h2><p id="ee55" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">在这里，模型的训练意味着找到参数，以使模型最适合数据。</p><h2 id="8462" class="lx jp hi bd jq ly lz ma ju mb mc md jy jb me mf kc jf mg mh kg jj mi mj kk mk bi translated">我们如何确定最佳拟合线？</h2><p id="99b1" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">预测值与观测值之间的<em class="ku">误差</em>最小的线称为最佳拟合线或<strong class="is hj">回归</strong>线。这些误差也被称为<strong class="is hj"> <em class="ku">残差</em> </strong>。残差可以通过从观察数据值到回归线的垂直线来可视化。</p><p id="f546" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了定义和测量我们模型的误差，我们将成本函数定义为残差平方和。成本函数表示为</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/f6f0a2b0951fe9a137f3b178cb90a647.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/0*raKSGARxZNZnLYPR.png"/></div></figure><p id="f986" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中假设函数<em class="ku"> h(x) </em>表示为</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/911cd051a9ccd314ba089edf37c63ad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/0*cIsIXRkONalRmClo.png"/></div></figure><p id="1fb9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">并且<em class="ku"> m </em>是我们的数据集中训练样本的总数。</p><blockquote class="kr ks kt"><p id="4e7e" class="iq ir ku is b it iu iv iw ix iy iz ja kv jc jd je kw jg jh ji kx jk jl jm jn hb bi translated">我们采用残差的平方，而不是残差的绝对值，因为我们的目的是<strong class="is hj"> </strong>惩罚远离回归线的点，而不是靠近回归线的点。</p></blockquote><p id="a36b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将使用梯度下降来寻找模型参数，使得成本函数最小。</p><h1 id="ec1a" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">梯度下降</h1><p id="1e00" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">梯度下降是一种优化算法，用于通过沿梯度负值定义的最陡下降方向迭代移动来最小化某个函数。在机器学习中，我们使用梯度下降来更新模型的参数。梯度下降的步骤概述如下。</p><ol class=""><li id="d537" class="li lj hi is b it iu ix iy jb lk jf ll jj lm jn mn lo lp lq bi translated">我们首先用一些随机值初始化模型参数。这也被称为<strong class="is hj"> <em class="ku">随机初始化</em> </strong>。</li><li id="56c1" class="li lj hi is b it lr ix ls jb lt jf lu jj lv jn mn lo lp lq bi translated">现在我们需要测量成本函数如何随其参数的变化而变化。因此，我们计算成本函数w.r.t对参数<em class="ku"> θ </em> ₀、<em class="ku"> θ </em> ₁、…、<em class="ku"> θ </em> ₙ的偏导数</li></ol><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/165d5be3c49d2bc1516743e6422fa938.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/0*Lzwa7a7-E5DN_z71.png"/></div></figure><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/d6ea876b25d729da846be570fab1133e.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/0*hzv0neSVDBtIn06W.png"/></div></figure><p id="a297" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">类似地，成本函数w.r.t对任何参数的偏导数可以表示为</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/2133669d20eca564e3804a0ebce08f87.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/0*pkyMIZfZeY_RF1_0.png"/></div></figure><p id="da3c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以立刻计算所有参数的偏导数，使用</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/de5ceeaff5204fc24c12c41e185ead80.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/0*NNMkVdUbK6JNo5eK.png"/></div></figure><p id="9cf9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中<em class="ku"> h(x) </em>为</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/ef78b8130dc704fc0772214d0b11b9a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/0*Ga101rkh5XJA5IBp.png"/></div></figure><p id="b766" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.计算导数后，我们更新参数如下</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/2fa9bd51527363e52beb29d7f23b0cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/0*cvyV46nhBd_6_q9I.png"/></div></figure><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ms"><img src="../Images/86502ab5ee2f9ec3a262e9aec88acd20.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/0*xg22YNXZgt7RzlVi.png"/></div></figure><p id="4da3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中<strong class="is hj"> <em class="ku"> α </em> </strong> <em class="ku"> </em>是<strong class="is hj"> <em class="ku">学习参数</em> </strong>。</p><p id="d950" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以一次更新所有参数，</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/28fbbb79b67641601a39262003856723.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/0*rT5r7hv1l0ygAhoC.png"/></div></figure><p id="25b2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们重复步骤2、3，直到成本函数收敛到最小值。<em class="ku"> </em>如果<em class="ku"> α </em>的值太小，代价函数需要更大的时间收敛。如果<em class="ku"> α </em>过大，梯度下降可能会超过最小值，最终可能无法收敛。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mu"><img src="../Images/34efa31448e0662a312cdb573816fce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*22oh44C5tUHbZ0yvIKWDFg.png"/></div></div></figure><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mv"><img src="../Images/57431b8be72f9c2b56c49511f7c29148.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iU1QCnSTKrDjIPjSAENLuQ.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">梯度下降类比:吴恩达课程</figcaption></figure><h1 id="195f" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">模型的性能</h1><p id="5ec0" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">均方根误差(<strong class="is hj"> RMSE </strong>)和决定系数(<strong class="is hj"> R </strong>得分)用于评价模型的性能</p><p id="2987" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> RMSE </strong>是残差平方和的平均值的平方根。</p><p id="bdb5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">RMSE的定义是</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/9af282dabab43c9dc750ab38037253f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/0*UJ58Ji-D4O6kWN1S.png"/></div></figure><p id="dac5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> R </strong>得分或<strong class="is hj">决定系数</strong>解释了通过使用最小二乘回归可以将因变量的总方差减少多少。</p><p id="6176" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ku"> R </em> </strong>由</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/0c49e3b08b1dbbb9312b5ec6eec5d62e.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/format:webp/0*TrXyI7M560pDrvGs.png"/></div></figure><p id="7cc5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="ku"> SSₜ </em> </strong>是我们把观测值的平均值作为预测值的误差总和。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es my"><img src="../Images/08b36f6c2018de4533723881a7cae1c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/0*8d9513n3ityzN08-.png"/></div></figure><p id="66ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"><em class="ku"/></strong><em class="ku"/>是残差的平方和</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mz"><img src="../Images/60de44dd99793e6de6e1d021d1cb676e.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/0*b4m6Cz1ToYSf0J5s.png"/></div></div></figure><h1 id="0a66" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结论</h1><p id="d55a" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">我们已经学习了线性回归、成本函数和梯度下降的概念。</p></div></div>    
</body>
</html>