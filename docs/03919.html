<html>
<head>
<title>A trail to use Transformer to build a time-series prediction model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Transformer建立时间序列预测模型的尝试</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-trail-to-use-transformer-to-build-a-time-series-prediction-model-fa32ce493dc?source=collection_archive---------12-----------------------#2020-02-25">https://medium.com/analytics-vidhya/a-trail-to-use-transformer-to-build-a-time-series-prediction-model-fa32ce493dc?source=collection_archive---------12-----------------------#2020-02-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="16d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Transformer模型已经广泛应用于翻译、摘要等各种自然语言处理任务中。典型NLP任务与时序任务有许多相似之处。时序任务的输入数据总是具有某种顺序(时间顺序)，而典型的文本也具有某种类似的顺序(语言顺序)。根据这样的相似性，Transformer模型似乎有一些天然的优势来完成一些时间序列的工作。本文中的代码可以通过<a class="ae jd" href="https://github.com/adonis1022/time_series_Transformer" rel="noopener ugc nofollow" target="_blank">链接</a>来引用。</p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><p id="0905" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">变压器</strong></p><p id="59a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有很多文章介绍变压器模型。我简单说一下。Transformer模型遵循典型的两级编码器-解码器结构，类似于以前的seq2seq模型。编码器部分将输入文本转换成向量，而解码器部分基于编码器部分的输出和先前的解码器输入产生目标语言。关于该模型的详细知识，请深入研究<a class="ae jd" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">杰伦的博客</a>。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jl"><img src="../Images/95524f1aadcc75ec9ad9b0270aed0896.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vIvwYwW-5WZQ2lJqxRJD1g.png"/></div></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">典型的变压器模型结构</figcaption></figure><p id="0bd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">建模—编码器</strong></p><p id="33cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们开始建立一个基于Transformer的时间序列模型。典型的变换器输入总是跟随着嵌入层，因为输入是离散整数的向量，每个离散整数代表一个单词。但是对于时间序列模型，输入是一个连续数字的向量。将每个连续的数字转换成一个向量是没有意义的，因此不需要添加嵌入层。我想有两种方法可以解决这个问题。第一种方法是只把连续数的向量当作嵌入向量，也就是说我们可以把向量当作一维嵌入向量。第二种方法是使用矩阵将一维向量转换为某一维向量。我在笔记本上用的是前一种方法。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es kb"><img src="../Images/8cd0b09f0296a22238e4f60e4d2f2597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*EZK_fGL_20W4n9-bSyCZEw.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">编码器零件的代码</figcaption></figure><p id="bcef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">建模—解码器</strong></p><p id="bd2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以用不同的方法来构造解码器部分。我们可以简单地使用变压器解码器作为我们的时序模型解码器部分。或者我们可以使用LSTM模型作为我们的解码器部分。为了连接变换器编码器和LSTM解码器，我使用输出矩阵的前两个维度作为LSTM解码器的初始h和c向量。然后我们应该决定解码器输入。有两种方法可以确定解码器输入。第一种是使用前一个位置的输出值。第二种是使用编码器输入的一些位置。哪种方法更好取决于数据的实际模式。如果数据有明显的季节性，最好使用第二种方法。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es kc"><img src="../Images/38e30494b066661c3465d19c46d4c30d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*qjIJGqr3101zX5vWHXLmNg.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">解码器部分的编码器</figcaption></figure><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es kd"><img src="../Images/a2d18b78f2ac19651b85e749c2730d85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gLR2BcxNzy9dyybfaPBpFw.png"/></div></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">整个模型结构</figcaption></figure><p id="99ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结论</strong></p><p id="de58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文只是利用Transformer构建时间序列模型的一个简单尝试。建立时间序列模型还有许多其他方法。没有证据表明NLP模型比传统的时间序列模型表现更好，但本文可以为一些问题提供解决方案。</p></div></div>    
</body>
</html>