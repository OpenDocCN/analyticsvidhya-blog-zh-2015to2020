<html>
<head>
<title>Universal Approximation Theorem, Neural Nets &amp; Lego Blocks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通用逼近定理、神经网络和乐高积木</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/universal-approximation-theorem-neural-nets-lego-blocks-1f5a7d93542a?source=collection_archive---------16-----------------------#2020-12-31">https://medium.com/analytics-vidhya/universal-approximation-theorem-neural-nets-lego-blocks-1f5a7d93542a?source=collection_archive---------16-----------------------#2020-12-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/cafe4331d19c687aaa253f65832ced9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZxmCQ2nh9Dbsd1trXYs3Aw.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">来源:斯坦福大学 Andrej Karpathy 的 CS231N 卷积神经网络</figcaption></figure><p id="c5ff" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">今天，基于多层神经网络的深度学习已经在大多数领域取得了最先进的成果。在这篇文章中，我们将看看通用逼近定理——深度学习的整个概念所基于的基本定理之一。我们将利用乐高积木类比和插图来理解同样的道理。</p><h1 id="a321" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">通用近似定理(Cybenko 1989)指出:</h1><blockquote class="kq"><p id="f722" class="kr ks hh bd kt ku kv kw kx ky kz jq dx translated">“神经网络具有很好的函数表示能力，具有一个隐含层和有限个神经元的前馈神经网络可以表示任何连续函数”</p></blockquote><p id="0774" class="pw-post-body-paragraph it iu hh iv b iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm le jo jp jq ha bi translated">为了让它有意义，让我们把这个定理分成几个部分——</p><p id="70ae" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">👉函数的表示能力</p><p id="92c4" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">👉前馈神经网络</p><p id="d383" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">👉具有有限数量神经元的隐藏层</p><p id="8d05" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">👉代表任何连续函数</p><h1 id="e606" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">为什么机器学习中我们更喜欢连续函数？</h1><ul class=""><li id="4cd9" class="lf lg hh iv b iw lh ja li je lj ji lk jm ll jq lm ln lo lp bi translated">函数定义了一组输入和相应输出之间的关系。函数在本质上可以是连续的，也可以是离散的。</li><li id="d76c" class="lf lg hh iv b iw lq ja lr je ls ji lt jm lu jq lm ln lo lp bi translated">函数广泛应用于机器学习概念中，如损失函数。事实上，整个机器学习模型可以被认为是一个接受输入并提供输出的函数。</li><li id="9866" class="lf lg hh iv b iw lq ja lr je ls ji lt jm lu jq lm ln lo lp bi translated">我们在 ML 中的目标是最小化损失函数，我们通过采用梯度来实现，即损失函数相对于输入参数(如权重和偏差)的一阶导数。</li><li id="c410" class="lf lg hh iv b iw lq ja lr je ls ji lt jm lu jq lm ln lo lp bi translated">因此，可微性在机器学习中是一个重要的考虑因素，我们总是更喜欢连续函数，主要是因为它们在所有点上可微的属性。</li><li id="95bf" class="lf lg hh iv b iw lq ja lr je ls ji lt jm lu jq lm ln lo lp bi translated">因为我们计算梯度，这是一个一阶导数，以尽量减少损失函数，我们总是喜欢连续函数。</li></ul><p id="2fa6" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">现在让我们向前迈进一步，理解为什么我们真的需要复杂的函数，为什么我们需要类似神经网络的东西来近似相同的函数。</p><h1 id="e8e3" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">为什么我们需要复杂的函数？</h1><ul class=""><li id="2101" class="lf lg hh iv b iw lh ja li je lj ji lk jm ll jq lm ln lo lp bi translated">我们在现实生活中有几种类型的关系，即线性函数、二次函数、非线性函数等。</li><li id="4ac3" class="lf lg hh iv b iw lq ja lr je ls ji lt jm lu jq lm ln lo lp bi translated"><strong class="iv hi"> <em class="jr">线性函数:</em> </strong>模拟线性关系的函数，例如，所用的燃料量与行驶的距离成线性关系。</li><li id="002b" class="lf lg hh iv b iw lq ja lr je ls ji lt jm lu jq lm ln lo lp bi translated"><strong class="iv hi"> <em class="jr">二次函数:</em> </strong>模拟二次关系的函数，例如，高尔夫球在击球过程中的轨迹呈现二次关系。</li><li id="9211" class="lf lg hh iv b iw lq ja lr je ls ji lt jm lu jq lm ln lo lp bi translated"><strong class="iv hi"> <em class="jr">非线性函数:</em> </strong>让我们看几个非线性关系的例子。例如，提供给患者的药物可能直到阈值水平才显示出期望的结果；来自 GPS 卫星的信号的三角测量本质上是非线性的。</li><li id="f3ec" class="lf lg hh iv b iw lq ja lr je ls ji lt jm lu jq lm ln lo lp bi translated">因此，为了模拟这种复杂的关系，简单的线性或二次函数可能不是最合适的。</li><li id="b1f0" class="lf lg hh iv b iw lq ja lr je ls ji lt jm lu jq lm ln lo lp bi translated">因此，我们必须要求复杂的函数来模拟复杂的非线性关系。</li></ul><p id="e5eb" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">这就是需要近似任何复杂函数和模型非线性的地方。现在，让我们看看神经网络是如何帮助解决这个问题的。</p><h1 id="c250" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">神经网络是如何学习的？</h1><p id="4464" class="pw-post-body-paragraph it iu hh iv b iw lh iy iz ja li jc jd je lv jg jh ji lw jk jl jm lx jo jp jq ha bi translated">在传统的编程场景中，输入和指导规则被编程，产生输出。</p><p id="722e" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在机器学习中，我们将输入和期望的输出输入到机器学习模型中，然后机器学习模型“学习”函数并提供规则集(也称为函数或模型)。</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ly"><img src="../Images/1c1dede8df30d572e5b457bddcf4b9a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*23b0YIxpaCmdPoeN.png"/></div></div></figure><p id="e0af" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">上述流程图中的计算部分由机器学习模型或深度神经网络实现。顾名思义，神经网络是一个神经元网络，堆叠在多个层中，即接受输入的输入层、进行特征工程的隐藏层和提供目标值的输出层。</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es md"><img src="../Images/f26cb3af051c96ade7da9b8c0c0b0769.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*f1AfnnG3cDR5ULf5.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">来源:ResearchGate</figcaption></figure><p id="c2d1" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">每一个神经元单位都有以下属性—</p><p id="fe42" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">⚡️一阵起哄</p><p id="be5f" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">⚡️有一个与之相关的重量</p><p id="4ac5" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">⚡️有一个与之相关联的偏置单位</p><p id="d03e" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">⚡️和一个决定神经元是否会放电的激活函数</p><p id="cbb3" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">⚡ ️output</p><p id="969f" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在神经网络训练阶段，权重和偏差值将应用于输入，然后基于所使用的激活函数，最终输出将由神经元提供。</p><p id="f972" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">例如，让我们考虑下面的表示——这里<em class="jr"> x1，x2…xn </em>是输入，其中<em class="jr"> w1，w2…wn </em>是它们相应的权重；<em class="jr"> b </em>是偏置单位,‘f’是激活函数。这个神经单元的最终输出将由函数<strong class="iv hi"><em class="jr">f(σXi Wi+b)</em></strong>给出。</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es me"><img src="../Images/c82440f90b45359f45562043edce44b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*diFBNXrG0nrAckWolVWAzg.png"/></div></div></figure><p id="84b7" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">如果‘f’是一个 sigmoid 函数，那么输出的范围将从 0 到 1；类似地，对于一个双曲正切函数，输出范围从-1 到 1。现在，我们已经了解了神经网络的基本结构，让我们继续前进，并了解这种结构如何使用一个称为“函数的<em class="jr">表示能力</em>的概念来帮助建模复杂的关系。</p><h1 id="77a8" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">神经网络如何模拟复杂关系—函数的表示能力:</h1><ul class=""><li id="5be1" class="lf lg hh iv b iw lh ja li je lj ji lk jm ll jq lm ln lo lp bi translated">根据通用逼近定理，神经元网络可以逼近和模拟任何复杂的连续函数。</li><li id="e768" class="lf lg hh iv b iw lq ja lr je ls ji lt jm lu jq lm ln lo lp bi translated">这是通过调整与每个神经元单元及其线性/非线性激活函数相关的权重和偏置参数的值来实现的。</li></ul><p id="cec0" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">让我们用我三岁的孩子用过的积木来打个比方，以便更好地理解它。孩子们是最好的建筑师，他们可以用同一套积木搭建出不同外观的建筑。</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mf"><img src="../Images/de59f71763b38459145d1a240cd4fe10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5ZdaaZTEI_25wG3f.jpg"/></div></div></figure><p id="ecf8" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">使用同一套积木，数字和车辆和动物可以形成(建筑)。这类似于神经网络完成的函数逼近。</p><p id="81c3" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">输入、隐藏和输出层中的权重和偏差以及激活函数充当神经网络的基本构件。通过调整和使用这些积木的不同组合，我们能够逼近任何复杂的函数。</p><p id="cbbe" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">让我们以下面输出层中所示的曲线函数为例——这个函数可以用两个线性神经元(wx+b)和一个线性激活函数来近似。</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mg"><img src="../Images/fa6501b473d691c772dc5f54bcce4e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*m-r-upHfp8ylaNKw.jpg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">来源:Udacity 上的深度学习课程介绍</figcaption></figure><p id="7d3d" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在神经网络层中，</p><blockquote class="kq"><p id="2550" class="kr ks hh bd kt ku kv kw kx ky kz jq dx translated">正权重→以正关系表达输入</p><p id="38ce" class="kr ks hh bd kt ku kv kw kx ky kz jq dx translated">负权重→以负关系表达输入</p><p id="61bd" class="kr ks hh bd kt ku kv kw kx ky kz jq dx translated">零权重→抑制输入</p><p id="1c71" class="kr ks hh bd kt ku kv kw kx ky kz jq dx translated">激活功能→在输出中引入非线性</p></blockquote><p id="2a4e" class="pw-post-body-paragraph it iu hh iv b iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm le jo jp jq ha bi translated">因此，权重在执行“特征工程”中起着重要作用，从原始输入中创建数百万个参数，然后通过最小化总网络损耗对参数进行加权和去加权。</p><blockquote class="kq"><p id="ab36" class="kr ks hh bd kt ku kv kw kx ky kz jq dx translated">神经网络的层次性最适合学习知识的层次来解决现实世界的问题。</p></blockquote><p id="3734" class="pw-post-body-paragraph it iu hh iv b iw la iy iz ja lb jc jd je lc jg jh ji ld jk jl jm le jo jp jq ha bi translated">让我们再看几幅插图，以便更清楚地理解这一点。</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div class="er es mh"><img src="../Images/95d60b9be624562f0c20e5b2074af451.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/0*AcCO6mdfPbJMoID_.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">来源:CS7015 深度学习 NPTEL 课程</figcaption></figure><p id="94e2" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">为了实现看起来像一座塔的最终输出函数，我们可以将两个图(来自 sigmoid 激活函数的输出)组合起来，然后在隐藏层调整权重。让我们取隐藏层中的权重+1 和-1，我们可以看到神经网络正确地预测了输出函数。</p><figure class="lz ma mb mc fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es mi"><img src="../Images/bc4ff365a1a2767b33720aee70854c88.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/0*1DWQei-mz3OgwSFz.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">来源:CS7015 深度学习 NPTEL 课程</figcaption></figure><p id="7350" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">在这个学习过程中最有趣的事情是，我们不需要手动改变权重来获得所需的输出，而是在<a class="ae mj" href="https://en.wikipedia.org/wiki/Backpropagation" rel="noopener ugc nofollow" target="_blank">反向传播</a>期间，根据一种称为“梯度下降”的损失优化算法自动计算和更新权重和偏差值。</p><h1 id="686c" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">关键要点:</h1><p id="250c" class="pw-post-body-paragraph it iu hh iv b iw lh iy iz ja li jc jd je lv jg jh ji lw jk jl jm lx jo jp jq ha bi translated">👉通用逼近定理:神经网络具有很好的函数表示能力，具有一个隐层和有限个神经元的前馈神经网络可以表示任何连续函数。</p><p id="3e85" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">👉神经网络的层次性最适合学习知识的层次来解决现实世界的问题。</p><p id="25f3" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">👉正权重→以正关系表达输入；负权重→以负关系表示输入；零权重→抑制输入；激活功能→在输出中引入非线性</p><p id="bda1" class="pw-post-body-paragraph it iu hh iv b iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq ha bi translated">👉神经网络中的神经元类似于乐高积木——同一组乐高积木每次都可以通过略微调整设计来构建不同的结构。</p><h1 id="fbcb" class="js jt hh bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">参考资料:</h1><ol class=""><li id="704f" class="lf lg hh iv b iw lh ja li je lj ji lk jm ll jq mk ln lo lp bi translated"><a class="ae mj" href="http://neuralnetworksanddeeplearning.com/" rel="noopener ugc nofollow" target="_blank"><em class="jr"/>http://neuralnetworksanddeeplearning.com/</a></li><li id="9f3f" class="lf lg hh iv b iw lq ja lr je ls ji lt jm lu jq mk ln lo lp bi translated"><a class="ae mj" href="https://www.youtube.com/watch?v=aPfkYu_qiF4&amp;list=PLyqSpQzTE6M9gCgajvQbc68Hk_JKGBAYT&amp;ab_channel=NPTEL-NOCIITM" rel="noopener ugc nofollow" target="_blank">T5】https://www.youtube.com/watch?v=aPfkYu_qiF4&amp;list = plyqspqzte 6m 9 gcgajvqbc 68 hk _ JKGBAYT&amp;ab _ channel = NPTEL-NOCIITMT7】</a></li><li id="c87a" class="lf lg hh iv b iw lq ja lr je ls ji lt jm lu jq mk ln lo lp bi translated"><a class="ae mj" href="https://www.youtube.com/watch?v=BR9h47Jtqyw&amp;ab_channel=LuisSerrano" rel="noopener ugc nofollow" target="_blank"><em class="jr">https://www.youtube.com/watch?v=BR9h47Jtqyw&amp;ab _ channel = LuisSerrano</em></a></li><li id="a78e" class="lf lg hh iv b iw lq ja lr je ls ji lt jm lu jq mk ln lo lp bi translated">斯坦福大学 Andrej Karpathy 的 CS231N 卷积神经网络</li><li id="2ed4" class="lf lg hh iv b iw lq ja lr je ls ji lt jm lu jq mk ln lo lp bi translated">Udacity 上的深度学习课程介绍</li></ol></div></div>    
</body>
</html>