# 用回归构建 TicTacToe 机器人

> 原文：<https://medium.com/analytics-vidhya/building-a-tictactoe-bot-with-regression-ba79f2ae74bb?source=collection_archive---------13----------------------->

回归分析是统计学和数据科学中一个令人兴奋且备受推崇的领域。大多数文章都是关于因变量的预测，但是其他问题也可以解决，比如赢得一场比赛。在这篇文章中，我描述了一个简单的 TicTacToe 机器人，当然你可以在 Github 上找到我的代码。

![](img/df388850ff0d112b6285507eb3ecf545.png)

# **理论**

首先，我解释了这项任务的一些理论，如果这让你厌烦，你只需滚动到实现细节:)

对于我们的任务，我们有什么样的培训经验:

*   直接:我们有一套正确移动的棋盘状态
*   间接:我们有一套实际的游戏(所有动作都设定好了)和最终结果

有人监督还是无人监督？

*   一切都给了吗？
*   神谕
*   与自己作对

此外，我们必须问自己，我的培训经验(如数据)是否具有代表性？我们当然假设它。在我们的情况下，我们有一个直接的训练，我们想和自己比赛。

## **目标功能**

现在我们的 bot 应该学习什么样的知识呢？一个好的起点是学习如何在可能的行动中选择最佳的行动。因此，我们想学习一个目标函数 *V* ，用它我们可以选择最佳的移动。

目标函数 *V* 的可能定义如下:

*   如果我们的最终棋盘状态 *b* 获胜: *V* ( *b* )=1
*   如果我们最终的棋盘状态 *b* 是亏损:*V*(*b*)= 1
*   如果我们最后的棋盘状态 *b* 是和棋: *V* ( *b* )=0

但是如果我们在游戏中，所以没有最终状态 *b* 呢？在这种情况下，我们定义*V*(*b*)=*V*(*b*’)，其中*b*’是我们可以从起始板状态 *b* 达到的最佳最终板状态。

这个功能的问题是它不可操作。当 *b* 是最终的棋盘状态时，我们可以有效地计算它，否则我们将不得不寻找一个最优解，直到游戏结束。

现在我们需要一个目标函数的操作描述。在机器学习领域，你通常期望你的学习算法是目标函数的近似，所以我们要做的是函数近似。

## **目标功能表示**

我们现在有一个理想的目标函数 *V* 并且想要这个函数的近似值。因为 *V* 是不可操作的，我们定义了它的一个表示:*V*’。我们的表示将被计算为自定义特征的线性组合:

*   ***x*** ₀:常数 1
*   ***x*** ₁:列/行/对角线的数量，有两个我们自己的棋子和一个空的字段
*   ***x*** ₂ :有两个敌方棋子和一个空棋子的列/行/对角线的数量
*   ***x*** ₃ :我们自己的那块是不是在场地中央？
*   ***x*** ₄ :我们自己在角落里的棋子数
*   ***x*** ₅ :有一个自己的棋子和两个空字段的列数/行数/对角线数
*   ***x*** ₆ :有三个自己棋子的列数/行数/对角线数

现在，我们要添加将由我们的学习算法学习的权重。这个权重将决定一个特征对结果的重要性。我们当前的函数近似现在看起来像这样:

***v*′=*w***₀**∫*x***₀**+*w***₁**∫*x***₁**+*w***₂**∫*x*** ∵*x*₅**+*w***₆**∵*x***₆

你不需要***x***₀**因为它被设置为 1，我只是用它来更容易地迭代权重。***w***₀-weight 给我们的近似值增加了一个附加常数，如果我们所有的***x***-值都为零，我们的***v*′**就会是 ***w*** ₀.因此，在***x*ᵢ**= 0**的情况下，它是***v*’**的期望平均值，也是我们的最佳猜测。****

****现在学习我们的近似值***V*’**我们需要训练示例，其中某个板状态 *b* 给出结果 *Vtrain* ( *b* )。在我们的井字游戏示例中，可能的结果是****

******⟨⟨*x*₀**= 1， *x*** ₁ **=0， *x*** ₂ **=1， *x*** ₃ **=0， *x*** ₄ **=0， *x*** ₅ **=0， *x*********

## ****培训评估****

**根据上层棋盘状态，我们现在将更新我们的权重，但这仅适用于最终的棋盘状态。我们还必须注意中级棋盘的状态，这样我们才能在训练中确定最佳的棋步。一个非常简单且成功的方法是为每个中间板状态分配*V train*(*b*)V′(*succ*(*b*))的值。*V*’是我们当前的近似值， *succ* ( *b* )是我们移动后的棋盘状态的继任者。在井字游戏中，这相当容易，因为你只有有限的可能棋步。所以在你移动之前，你要计算*V*′(*succ*(*b*))的值，如果结果比*V*′(*b*)高，那么这就是一个好的移动。请注意，使用 *succ* ( *b* )时，我们的特性值会发生变化。**

## ****更新权重****

**现在我们想更新我们的权重，以确定哪些特征是重要的。在我们的例子中，我们希望最小化平方误差。平方误差的计算公式如下:**

***E*=(*V train*(*b*)—*V*’(*b*))**

**所以我们想更新我们的权重，这样他们可以最小化这个误差。我们将误差平方，这样我们就有一个正值，大的误差会受到更严厉的惩罚。因此，我们使用 LMS 算法(最小均方)。**

**对于每个训练样本，我们使用当前权重来计算我们的*V*’(*b*)，其中 *b* 是我们当前的棋盘状态。现在，每个重量都更新为:**

***w*ᵢ=*w*ᵢ+*η*(*v train*(*b*)v'(*b*)*x*ᵢ**

***η* 是一个常数(如 0.001)，它调节权重更新的度量。如果我们的项*v train*(*b*)*v*’(*b*)等于零，则权重不更新，如果它是正的*v*’(*b*)太低，则我们希望通过相应特征 *x* ᵢ.的比例来增加我们的权重如果特征为零，我们不更新权重，因为该特征对于结果没有意义。**

**就是这样…我们有一个学习算法。现在我们可以实现它了。**

# ****实施****

**理论到此为止…我们来编码一下。**

## ****我们的玩家****

**![](img/3387d7d99969394118c1a2a938b603d5.png)**

**对于上面提到的每个特征，我们都有一个权重，该权重将被更新以确定哪个特征比另一个特征更重要，通常在回归分析中，我们添加一个截距作为第 7 个权重。我们将每一场胜利定为 1，每一场平局定为 0，每一场失败定为-1。**

```
weights = [1.0 for _ in range(7)]
```

## ****训练循环****

**我们要针对自己训练 10000 个游戏，学习如何玩游戏。因此，我们有一个简单的训练循环:**

```
board = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]while(True):
    pre_features = player1.get_board_features(board)
    pre_approx = player1.evalApproximation(pre_features, weights)
    player1.make_move(board, weights)
    outcome = finished(board, player1)#…
```

**我们的棋盘将为每个训练示例重置，当我们玩游戏时，我们通过`get_board_features(board)`获得当前的棋盘功能。在第一次迭代中，它返回:**

*****x*** ₀ **=0， *x*** ₁ **=0， *x*** ₂ **=， *x*** ₃ **=0， *x*** ₄ **=0， *x*** ₅ **=0，****

***对于稍后的权重更新，我们需要当前与`evalApproximation()`的近似值。在`make_move(board, weights)`中，通过尝试将会计算出最好的移动。首先，我们创建所有可能的新棋盘状态:***

```
*for i in range(3):
    for j in range(3):
        if board[i][j] == 0:
            new_board = copy.deepcopy(board)
            new_board[i][j] = self.color 
            new_boards.append((new_board, (i, j)))*
```

***`deepcopy`非常重要，因为否则我们只有参考，所有的动作都将在同一块板上完成。***

***然后，在创建每个可能的继任者后，我们将获得新的电路板功能，并使用此功能评估每个电路板副本:***

```
*def evalApproximation(self, features, weights): val = 0.0 for i in range(len(weights)):
        val += features[i] * weights[i] return val*
```

***现在我们对每个棋盘副本都有一个评级，我们选择最高值作为最佳移动。每走一步后，我们都要测试游戏是否结束，如果没有，第二个玩家(仍然在训练我们的机器人)就走一步。***

```
*#…if outcome[0] != -1:
    breakplayer2.make_move(board, weights)
outcome = finished(board, player2)
if outcome[0] != -1:
    break#…*
```

***在一次迭代之后，我们必须更新我们的权重，因为我们走了一步，而我们的对手走了一步。***

```
*#…succ_features = player1.get_board_features(board)
succ_approx = player1.evalApproximation(succ_features, weights)weight_update(weights=weights,lr_const=lr,train_val=succ_approx, approx=pre_approx, features=pre_features)*
```

***我们再次评估移动后的特征，并计算我们的函数*V*′(*succ*(*b*))的当前近似值。我们的权重更新将按照 LMS 更新规则执行:***

```
*def weight_update(weights, lr_const, train_val, approx, features): for i in range(len(weights)):
        weights[i] = weights[i] + lr_const* (train_val — approx) *                                              
                                             features[i]*
```

***对于每个权重，我们计算后续近似和当前近似的误差。如果现在我们的值大于零，这是一个很好的举动，因为我们的最终结果应该是 1。如果它比当前的近似值小，这是一个糟糕的举动，因为损失的结果是-1。***

***游戏结束后，执行最后一次权重更新。这里结果被设置为`train_val`，1 表示赢，0 表示平，1 表示输。***

## *****评价*****

***经过 10.000 场比赛的初始训练，1000 场比赛对一个随机设置代币的玩家。如果两个随机的玩家互相对战，结果几乎是平衡的:***

```
*╔═══════════╦════════════════════╦═══════════════╗
║   Won     ║       Draw         ║     Loss      ║
╠═══════════╬════════════════════╬═══════════════╣
║   410     ║       138          ║     452       ║
╚═══════════╩════════════════════╩═══════════════╝*
```

***如果第一个玩家根据我们的回归学习，它赢了 91.9%！***

```
*╔═══════════╦════════════════════╦═══════════════╗
║   Won     ║       Draw         ║     Loss      ║
╠═══════════╬════════════════════╬═══════════════╣
║   919     ║       38           ║     46        ║
╚═══════════╩════════════════════╩═══════════════╝*
```

***我还实现了你可以和机器人对战的地方。如果你迈出第一步，你会很容易赢，因此我认为我们可以添加更多的功能，但对于第一次测试来说，这已经足够了。***

***完整代码在 [Github](https://github.com/Skyy93/TicTacToeRegressionBot) 网站上***

***敬请关注。***

***来源:汤姆·米切尔——机器学习***