<html>
<head>
<title>Emotion Recognition using Text</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用文本的情感识别</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/song-recommendation-based-on-textual-and-facial-emotion-recognition-a95e6259c5d8?source=collection_archive---------1-----------------------#2020-10-29">https://medium.com/analytics-vidhya/song-recommendation-based-on-textual-and-facial-emotion-recognition-a95e6259c5d8?source=collection_archive---------1-----------------------#2020-10-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/df7dfae1624d570585906fe7d5c9243a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZ4OUd5qJeodR-Z2Dh0uAg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@tengyart?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> <strong class="bd iv">滕亚尔</strong> </a>上<a class="ae iu" href="https://unsplash.com/s/photos/emotions?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">下<strong class="bd iv">上</strong>下</a></figcaption></figure><p id="2b77" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">在这篇文章中，我们将探索编写一个使用文本特征检测情绪的程序所需的步骤。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ju"><img src="../Images/687dc7b63b23ede01035b3e8d6750d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*_wbDOXgPIxEsHLu7KEFY_w.png"/></div></figure><p id="22ee" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">程序是用 Python<em class="jz">*假装震惊* </em>写的。</p></div><div class="ab cl ka kb gp kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="hb hc hd he hf"><p id="4b8a" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">现在让我们看看如何着手创建一个文本模型。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kh"><img src="../Images/ef04db588c54eb0abc45b4340db3b935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IANBbXZ6HSQIMfnhLQJHQA.jpeg"/></div></div></figure><p id="8a28" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">对于文本模型的创建，我们将使用<a class="ae iu" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank"><strong class="iy hj">【LSTM(长短期记忆)</strong> </a>，因为与其他学习模型如 SVM、随机福里斯特、朴素贝叶斯等相比，它给出了更高的训练精度。</p><p id="b7cf" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">最重要的是，你需要一个数据集来训练和测试，你可以从<a class="ae iu" href="https://www.kaggle.com/praveengovi/emotions-dataset-for-nlp" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hj">这里</strong> </a>得到。</p><p id="d4ce" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">首先，我们导入所需的包。如果你没有安装这些软件包，你可以<a class="ae iu" href="https://pip.pypa.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hj"> pip </strong> </a>安装它们。</p><pre class="jv jw jx jy fd ki kj kk kl aw km bi"><span id="895c" class="kn ko hi kj b fi kp kq l kr ks">import pandas as pd<br/>import numpy as np<br/>import re<br/>from keras.preprocessing.text import Tokenizer<br/>from keras.preprocessing.sequence import pad_sequences<br/>from keras.utils import to_categorical<br/>from keras.models import load_model<br/>import urllib.request<br/>import zipfile<br/>import os<br/>from keras.models import Sequential<br/>from keras.layers import Embedding,Bidirectional,LSTM,GRU,Dense<br/>import nltk<br/>from nltk.tokenize import word_tokenize<br/>import warnings<br/>import tensorflow as tf<br/>nltk.download('punkt')<br/>warnings.filterwarnings('ignore')</span></pre><p id="7861" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">既然已经导入了包，那么您需要提取句子和它们各自的情感，并将它们分别插入到训练、测试和验证数据框架中。</p><p id="ca32" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">因为文件在里面。txt 格式，我们使用下面的代码将它们放入训练和测试(包括验证)数据帧中。</p><pre class="jv jw jx jy fd ki kj kk kl aw km bi"><span id="2635" class="kn ko hi kj b fi kp kq l kr ks">f=open('train.txt','r')<br/>x_train=[]<br/>y_train=[]<br/>for i in f:<br/>    l=i.split(';')<br/>    y_train.append(l[1].strip())<br/>    x_train.append(l[0])<br/>f=open('test.txt','r')<br/>x_test=[]<br/>y_test=[]<br/>for i in f:<br/>    l=i.split(';')<br/>    y_test.append(l[1].strip())<br/>    x_test.append(l[0])<br/>f=open('val.txt','r')<br/>for i in f:<br/>    l=i.split(';')<br/>    y_test.append(l[1].strip())<br/>    x_test.append(l[0])<br/>data_train=pd.DataFrame({'Text':x_train,'Emotion':y_train})<br/>data_test=pd.DataFrame({'Text':x_test,'Emotion':y_test})<br/>data=data_train.append(data_test,ignore_index=True)</span></pre><p id="f97e" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">现在句子已经插入，我们需要清理它们。基本上，我们去掉所有的介词、冠词、标点符号、停用词，只留下句子中重要的词。</p><p id="a10f" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这里，被去除的单词充当噪声，这就是为什么为了获得期望的结果，即高测试精度，必须消除它们的原因。</p><pre class="jv jw jx jy fd ki kj kk kl aw km bi"><span id="ff58" class="kn ko hi kj b fi kp kq l kr ks">def clean_text(<em class="jz">data</em>):<br/>    data=re.sub(r"(#[\d\w\.]+)", '', data)<br/>    data=re.sub(r"(@[\d\w\.]+)", '', data)<br/>    data=word_tokenize(data)<br/>    return data<br/>texts=[' '.join(clean_text(text)) for text in data.Text]<br/>texts_train=[' '.join(clean_text(text)) for text in x_train]<br/>texts_test=[' '.join(clean_text(text)) for text in x_test]</span></pre><p id="4ec1" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><a class="ae iu" href="https://www.tutorialspoint.com/python_text_processing/python_tokenization.htm" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hj">标记化</strong> </a>是自然语言处理分析中的一个重要过程。它标记每个句子，提取每个独特的词，并创建一个字典，其中每个独特的词被分配一个索引。</p><pre class="jv jw jx jy fd ki kj kk kl aw km bi"><span id="5911" class="kn ko hi kj b fi kp kq l kr ks">tokenizer=Tokenizer()<br/>tokenizer.fit_on_texts(texts)<br/>sequence_train=tokenizer.texts_to_sequences(texts_train)<br/>sequence_test=tokenizer.texts_to_sequences(texts_test)<br/>index_of_words=tokenizer.word_index<br/>vocab_size=len(index_of_words)+1</span></pre><p id="cfa3" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">我们得到的数据集有六种独特的结果或情绪，即:愤怒、悲伤、恐惧、喜悦、惊讶和爱。</p><p id="181f" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">因此，类的数量是六个。此外，这里我们使用 300 个嵌入维度，序列的最大长度被赋值为 500。</p><p id="832a" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">当我们填充训练和测试序列时，必须将它们的“maxlen”参数设置为相同的值，否则将显示错误。</p><p id="e5b8" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">在后面的阶段，每种情绪都被赋予一个<a class="ae iu" href="https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical" rel="noopener ugc nofollow" target="_blank"><strong class="iy hj"/></a>(0-5)。因此，使用了“编码”字典和“to _ categorical”函数。当我们试图获得一个结果时，我们将类别值映射回情感。</p><pre class="jv jw jx jy fd ki kj kk kl aw km bi"><span id="1881" class="kn ko hi kj b fi kp kq l kr ks">num_classes=6<br/>embed_num_dims=300<br/>max_seq_len=500<br/>class_names=['anger','sadness','fear','joy','surprise','love']</span><span id="dbee" class="kn ko hi kj b fi kt kq l kr ks">X_train_pad=pad_sequences(sequence_train,maxlen=max_seq_len)<br/>X_test_pad=pad_sequences(sequence_test,maxlen=max_seq_len)</span><span id="5b49" class="kn ko hi kj b fi kt kq l kr ks">encoding={'anger':0,'sadness':1,'fear':2,'joy':3,'surprise':4,'love':5}<br/>y_train=[encoding[x] for x in data_train.Emotion]<br/>y_test=[encoding[x] for x in data_test.Emotion]<br/>y_train=to_categorical(y_train)<br/>y_test=to_categorical(y_test)</span></pre><p id="a29c" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">为了创建这个模型，我们使用 1M(在维基百科上训练的 100 万个词向量)版本的预训练词向量。</p><p id="e26f" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">有训练过的词向量可以用于这个目的，你可以在这里 查看<a class="ae iu" href="https://fasttext.cc/docs/en/english-vectors.html" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hj">。</strong></a></p><p id="80ad" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">使用这些词向量有助于我们以更有效和更彻底的方式训练我们的模型，从而产生更高的训练准确度。</p><pre class="jv jw jx jy fd ki kj kk kl aw km bi"><span id="4c03" class="kn ko hi kj b fi kp kq l kr ks">def create_embedding_matrix(<em class="jz">filepath</em>,<em class="jz">word_index</em>,<em class="jz">embedding_dim</em>):<br/>    vocab_size=len(word_index)+1<br/>    embedding_matrix=np.zeros((vocab_size,embedding_dim))<br/>    with open(filepath) as f:<br/>        for line in f:<br/>            word,*vector=line.split()<br/>            if word in word_index:<br/>                idx=word_index[word]<br/>                embedding_matrix[idx] = np.array(vector,dtype=np.float32)[:embedding_dim]<br/>    return embedding_matrix</span><span id="7271" class="kn ko hi kj b fi kt kq l kr ks">fname='embeddings/wiki-news-300d-1M.vec'<br/>embedd_matrix=create_embedding_matrix(fname,index_of_words,embed_num_dims)</span></pre><p id="353d" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">现在，我们创建一个用于训练模型的架构。为此，我们首先创建一个<a class="ae iu" href="https://keras.io/api/layers/core_layers/embedding/" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hj">嵌入</strong> </a>层，其权重从单词矢量文件中获得。</p><p id="a73b" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">我们还增加了一个<a class="ae iu" href="https://keras.io/api/layers/recurrent_layers/bidirectional/" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hj">双向</strong> </a>层，其特点是:</p><ul class=""><li id="9dca" class="ku kv hi iy b iz ja jd je jh kw jl kx jp ky jt kz la lb lc bi translated">gru_output_size = 128</li><li id="d4bf" class="ku kv hi iy b iz ld jd le jh lf jl lg jp lh jt kz la lb lc bi translated">辍学= 0.2</li><li id="6e55" class="ku kv hi iy b iz ld jd le jh lf jl lg jp lh jt kz la lb lc bi translated">经常性辍学= 0.2</li></ul><p id="0393" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">最后，添加一个<a class="ae iu" href="https://keras.io/api/layers/core_layers/dense/" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hj">密集</strong> </a>层，该层具有“软最大”激活。</p><p id="57bf" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">Adam 的优化器用作优化器，损失使用“分类 _ 交叉熵”计算。</p><p id="c61c" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">“model.summary()”可用于查看模型中的特征、图层类型、输出形状和参数数量。</p><pre class="jv jw jx jy fd ki kj kk kl aw km bi"><span id="7630" class="kn ko hi kj b fi kp kq l kr ks">embedd_layer=Embedding(vocab_size,embed_num_dims,input_length=max_seq_len,weights=[embedd_matrix],trainable=False)<br/>gru_output_size=128<br/>bidirectional=True<br/>model=Sequential()<br/>model.add(embedd_layer)<br/>model.add(Bidirectional(GRU(units=gru_output_size,dropout=0.2,recurrent_dropout=0.2)))<br/>model.add(Dense(num_classes, activation='softmax'))<br/>model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])</span></pre><p id="b066" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">最后，我们<a class="ae iu" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hj">使用我们的训练集训练模型</strong> </a>，同时测试准确性，因为模型的度量被设置为“准确性”。</p><p id="e7e6" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这里，批次大小取为 128，时期数为 8。</p><p id="44a3" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">可以改变<a class="ae iu" href="https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hj">批次大小</strong> </a>和<a class="ae iu" rel="noopener" href="/@upendravijay2/what-is-epoch-and-how-to-choose-the-correct-number-of-epoch-d170656adaaf"> <strong class="iy hj">历元数</strong> </a>。为了避免过度拟合，时期的数量不应该太高。批处理大小也可以变化，在许多情况下，较大的批处理大小会产生更好的结果，但它们也会占用大量内存，因此在许多系统中是不可能执行的。</p><pre class="jv jw jx jy fd ki kj kk kl aw km bi"><span id="3444" class="kn ko hi kj b fi kp kq l kr ks">batch_size=128<br/>epochs=8<br/>hist=model.fit(X_train_pad,y_train,batch_size=batch_size,epochs=epochs,validation_data=(X_test_pad,y_test))</span></pre><p id="0756" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">8 个时代完成后，我们可以测试模型。</p><p id="5d97" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">请记住，我们已经将情感转换为分类或数值，因此为了获得准确的情感，我们需要将分类值映射回其实际的英语情感。</p><pre class="jv jw jx jy fd ki kj kk kl aw km bi"><span id="ed7a" class="kn ko hi kj b fi kp kq l kr ks">message=['I am sad.']<br/>seq=tokenizer.texts_to_sequences(message)<br/>padded=pad_sequences(seq,maxlen=max_seq_len)<br/>pred=model.predict(padded)<br/>print('Message:'+str(message))<br/>print('Emotion:',class_names[np.argmax(pred)])</span></pre><p id="842e" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">对于上面这句话，我们得到的结果是‘悲伤’，大概是对的。</p><p id="5238" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">通过合并上述代码片段，可以获得整个程序。为了执行这个程序，我建议使用 Google Colab，因为它有一个内置的 GPU，这对训练机器学习模型很有用，除非你的系统已经有一个 GPU，在这种情况下，你是一个幸运的人。</p><p id="a5f8" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">一个有用的花絮，<a class="ae iu" href="https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hj">保存文本模型</strong> </a>以备将来使用(你永远不知道什么时候<a class="ae iu" href="https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model" rel="noopener ugc nofollow" target="_blank"> <strong class="iy hj">你会需要它</strong> </a>)。</p><pre class="jv jw jx jy fd ki kj kk kl aw km bi"><span id="db60" class="kn ko hi kj b fi kp kq l kr ks">tf.keras.models.save_model(model,'textmodel',overwrite=True,include_optimizer=True,save_format=None,signatures=None,options=None)</span></pre><p id="03cc" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">使用这个文本模型，你可以创建一个测验来检测用户的情绪，或者对 tweets 或 Reddit 帖子进行情绪分析，这种可能性是无限的。</p></div><div class="ab cl ka kb gp kc" role="separator"><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf kg"/><span class="kd bw bk ke kf"/></div><div class="hb hc hd he hf"><p id="3a10" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">祝您在数据科学之旅中好运，感谢您的阅读:)</p></div></div>    
</body>
</html>