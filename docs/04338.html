<html>
<head>
<title>Feature Engineering Experiment- Weighted KNN</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征工程实验-加权KNN</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-engineering-experiment-weighted-knn-3f28dfdf30e1?source=collection_archive---------5-----------------------#2020-03-15">https://medium.com/analytics-vidhya/feature-engineering-experiment-weighted-knn-3f28dfdf30e1?source=collection_archive---------5-----------------------#2020-03-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ee56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用KNN学习特征权重</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/38e50d391687a48d2ad9e2d900aa7b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Kxx8JlerOSn3hgZS"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">从如此多的可能性中选择一个</figcaption></figure><p id="0029" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于机器学习问题，我有一个惨痛的教训，那就是大多数机器学习问题都需要花费大量的时间，而不是数据争论，而是EDA和功能探索。</p><p id="f5aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这一关键步骤在大多数课程和演讲中经常被忽视，主要是因为它既是一门科学，也是一门艺术。当我说艺术时，主要是因为如何实现特征工程的可能性，如超参数调整，只受计算资源和时间的限制。</p><p id="948f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些方法从用于子集选择的包装器方法到特征提取和特征构造各不相同。其中每个范围从利用KNNs到随机森林到方差阈值等等。</p><p id="9449" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文讨论使用KNNs来学习可用特征的权重，然后选择具有最大权重的特征。</p><p id="abf5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">直观地看，这种方法是如何工作的</strong></p><ol class=""><li id="c020" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">对于数据集中的所有观察值，它计算每个观察值和所有其他观察值之间的相似性。它与普通香草KNN的不同之处在于相似度是加权的。</li></ol><p id="b71e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在具有欧几里得距离的普通KNN中，我们将有——两个观察点p，q之间的距离由d(p，q)给出</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kc"><img src="../Images/76b95a8ba8d8ec9c63caceb606eaa140.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6n-ZRNOFSDYKpru0wPM7HA.png"/></div></div></figure><p id="bd01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是在加权的形式下，我们会有这样的结果</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kd"><img src="../Images/1f0cd71be8e3f80179cb2f1256c7bf42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E9a5_RYUd0cuXy8kXVtKOQ.png"/></div></div></figure><p id="f6a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中对于每个第I个特征，有一个相应的权重w-i</p><p id="163a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.该相似性/距离度量然后被用于选择特定观察的k个最近邻居</p><p id="9e21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.现在像在KNN一样，我们用邻居来预测观察值的标号。</p><p id="95b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.基于预测，我们可以计算性能指标，如准确性或AUC或对数损失。</p><p id="2e10" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ke">这又将作为我们整体特征学习算法的损失函数。</em></p><p id="f1f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ke"> ie-我们将不断改变权重使用优化算法，如梯度下降，直到我们能够最小化损失函数。</em></p><p id="95e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们用Python做一些实验——代码在这里<a class="ae kf" href="https://github.com/abhijeetdtu/Analytics/blob/master/Stats/feature_engineering/knn_weights.py" rel="noopener ugc nofollow" target="_blank"> #github </a></p><p id="9207" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">导入所需的库</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kg"><img src="../Images/4453cd6d323efb80cd63f93bd14d7966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2KVP6e_ekRi-zc_9DsqZiw.png"/></div></div></figure><p id="7524" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后使用<strong class="ih hj"> sklearn的make_classification </strong>我们创建了一个包含300个观察值的数据集，其中只有5个是信息特征</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kh"><img src="../Images/f4ea28fff80efafa42298681c1b21606.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*b_m8OR0KGLuB7_Qk8CmnSA.png"/></div></figure><p id="4f3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后使用np.random.normal，我们添加了10个基本上只是噪声的特征</p><p id="8af5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们将权重初始化为全零，并将knn的k设置为5</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ki"><img src="../Images/2ef379cb3988794428aab61b86782ea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*Eq0upGrKXEWcMGc0a0muHA.png"/></div></figure><p id="67e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我们将KNN算法封装在一个函数中。让我们回顾一下整个方法。</p><ol class=""><li id="1aa0" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">我们正在迭代所有的观察对</li></ol><p id="c4a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.并计算它们之间的<strong class="ih hj">加权绝对距离</strong>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kj"><img src="../Images/eb41440d5bb0106090bd69dbd5e4ea4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-DpS4lns7kC6SESFHZVSDw.png"/></div></div></figure><p id="5960" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.然后，我们挑选k个最近的邻居，并使用它们来为每个观察预测正确的标签。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kk"><img src="../Images/9e006b7db6f866e02f6080ba5d6769a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pvp24a4NhvWwCwxcH_6fVw.png"/></div></div></figure><p id="781b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.然后，我们使用预测的概率来计算<strong class="ih hj">准确度分数</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kl"><img src="../Images/abfb6328eeb50f40c5bcf2a33a5bd797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vb9XUsJ7LbqlGbktal0brg.png"/></div></div></figure><p id="fc9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是knnWeights函数的完整代码</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es km"><img src="../Images/9f8c04ee85ab26b802b0f7b2bff4154a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6jmZZ13Pn4S22CiEHl2XdA.png"/></div></div></figure><p id="0d53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我们使用<strong class="ih hj"> scipy的最小化</strong>来学习权重</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kn"><img src="../Images/a87197d05a0eb6da4bfdbae6702708f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5jb0xQCu6LriiZYgt24sWA.png"/></div></div></figure><p id="0749" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了了解学习到的权重有多好，我们可以使用所有特征运行KNN，并将其与仅使用具有最高权重的5个特征的KNN运行进行比较(请记住，在创建数据集时，我们将n _ informative设置为5)。我们还随机选择一个特征子集，并比较结果。</p><ul class=""><li id="f849" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc ko jz ka kb bi translated">使用KNN分类器的结果具有所有特征</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kp"><img src="../Images/3cb557a2d18ffcde94dcc1d707ed7c3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NJTiZ68LTKPrRSKd3Uf2zw.png"/></div></div></figure><ul class=""><li id="d3c3" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc ko jz ka kb bi translated">使用具有最高权重特征的KNN分类器的结果</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kq"><img src="../Images/0bc3273f980a3ea34101771db3dbbec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0ZJ_FXPrypsjlZvHLWQkDg.png"/></div></div></figure><ul class=""><li id="d705" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc ko jz ka kb bi translated">使用随机子集的结果</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kr"><img src="../Images/c5ddb4116ff5ac9c0f60e33033c0b1a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Teb1kBWCyimw4z_u9i4tw.png"/></div></div></figure><p id="8cee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以将这些指标——准确性和每次实验的ROC合并到一个数据框架中，并运行10次实验来获得一个总体感觉</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ks"><img src="../Images/455604bfc51d7de23c38d5cec9e2822e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*krF2LL_MTC8-WRO-oPU6Lw.png"/></div></div></figure><p id="c802" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">10次实验的结果是</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kt"><img src="../Images/f15b407f4b63356fd1ff8ee2672166a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uxp3CBBrdbH0U8YoScOrkw.png"/></div></div></figure><p id="ff10" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，rfa =随机特征子集精度</p><p id="4314" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">rfauc =随机特征子集auc</p><p id="ad4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">woa =无特征选择准确性</p><p id="995f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">woauc =无特征选择auc</p><p id="4e22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">wa =特征选择精度</p><p id="9572" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">wauc =特征选择精度</p><p id="a745" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好地理解，我们可以把结果汇总起来</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ku"><img src="../Images/bb95e529515b9ee45cffe8aed1596775.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sadKvH4ZEeDHqgPVYPjRfA.png"/></div></div></figure><p id="3f0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们看到的是，随机特征子集选择的平均性能较差，而使用KNN选择的特征具有最高的平均性能。在这两种方法之间，没有特征选择产生性能。</p></div></div>    
</body>
</html>