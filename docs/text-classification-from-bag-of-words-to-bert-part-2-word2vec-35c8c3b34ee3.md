# æ–‡æœ¬åˆ†ç±»â€”ä»è¯è¢‹åˆ° BERT â€”ç¬¬ 2 éƒ¨åˆ†(Word2Vec)

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-2-word2vec-35c8c3b34ee3?source=collection_archive---------1----------------------->

![](img/ec5da50d6b851aaecaa80a0de2810741.png)

å¡”æ›¼å¨œÂ·èŒ¹ç±³åœ¨ [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) æ‹æ‘„çš„ç…§ç‰‡

è¿™ä¸ªæ•…äº‹æ˜¯ä¸€ç³»åˆ—æ–‡æœ¬åˆ†ç±»çš„ä¸€éƒ¨åˆ†â€”â€”ä»å•è¯è¢‹åˆ°ä¼¯ç‰¹ã€‚å¦‚æœä½ è¿˜æ²¡æœ‰æ£€æŸ¥ä¹‹å‰çš„æ•…äº‹ï¼Œä¸€å®šè¦æ£€æŸ¥ä¸€ä¸‹ï¼Œå› ä¸ºè¿™å°†æœ‰åŠ©äºç†è§£æœªæ¥çš„äº‹æƒ…ã€‚

[ç¬¬ä¸€éƒ¨åˆ†](https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-1e628a2dd4c9)

åœ¨ä¹‹å‰çš„æ•…äº‹ä¸­( [Part 1 (BagOfWords)](https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-1e628a2dd4c9) æˆ‘ä»¬ä½¿ç”¨äº† CountVectorizer(ä¸€ä¸ª sklearn çš„å•è¯åŒ…å®ç°)æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—æ•°æ®é›†ï¼Œæ˜ å°„åˆ°è¾“å‡ºå˜é‡ toxicã€severe_toxicã€æ·«ç§½ã€å¨èƒã€ä¾®è¾±ã€identity_hateï¼Œå¹¶ä½¿ç”¨ sklearn çš„å¤šè¾“å‡ºåˆ†ç±»å™¨åŒ…è£…å™¨ä¸ºæ‰€æœ‰ 6 ä¸ªè¾“å‡ºå˜é‡åˆ›å»ºé€»è¾‘å›å½’æ¨¡å‹ã€‚

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ç”¨ Word2Vec æ¨¡å‹ä»£æ›¿ç¬¬ä¸€éƒ¨åˆ†æ¥åˆ›å»ºåµŒå…¥ï¼Œè€Œä¸æ˜¯ BagOfWords å‘é‡ï¼Œç„¶åå°†å…¶è¾“å…¥åˆ°é€»è¾‘å›å½’æ¨¡å‹ä¸­(ä»»ä½• ML/DL æ¨¡å‹éƒ½å¯ä»¥åœ¨ Word2Vec åµŒå…¥çš„åŸºç¡€ä¸Šæ„å»º)ã€‚

**æ³¨æ„:æˆ‘æ²¡æœ‰åœ¨è¿™ç¯‡åšå®¢ä¸­æ¶‰åŠé€»è¾‘å›å½’å’Œç‰¹å¾é‡è¦æ€§/æ¨¡å‹è§£é‡Šçš„ç»†èŠ‚ï¼Œå› ä¸ºæˆ‘å·²ç»åœ¨ä¸Šä¸€ç¯‡æ–‡ç« (** [**ç¬¬ä¸€éƒ¨åˆ†(BagOfWords)**](https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-1e628a2dd4c9)**)**ä¸­æ¶‰åŠè¿‡

***ä»€ä¹ˆæ˜¯å•è¯åµŒå…¥ï¼Ÿ***

![](img/995155dc1c06ae396e8805c829623ab2.png)

äºŒç»´å•è¯åµŒå…¥

å•è¯åµŒå…¥ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§ä½¿ç”¨é«˜æ•ˆã€å¯†é›†è¡¨ç¤ºçš„æ–¹æ³•ï¼Œå…¶ä¸­ç›¸ä¼¼çš„å•è¯å…·æœ‰ç›¸ä¼¼çš„ç¼–ç ã€‚é‡è¦çš„æ˜¯ï¼Œæ‚¨ä¸å¿…æ‰‹åŠ¨æŒ‡å®šè¿™ç§ç¼–ç ã€‚åµŒå…¥æ˜¯æµ®ç‚¹å€¼çš„å¯†é›†å‘é‡(å‘é‡çš„é•¿åº¦æ˜¯æ‚¨æŒ‡å®šçš„å‚æ•°)ã€‚

ä¸Šé¢æ˜¯ä¸€ä¸ªäºŒç»´å•è¯åµŒå…¥ï¼Œå…¶ä¸­æ˜ŸæœŸæ—¥ä¸å…¶ä»–å·¥ä½œæ—¥çš„ç›¸ä¼¼å€¼å¤šäºå®¶åº­æˆå‘˜

***ä»€ä¹ˆæ˜¯ Word2Vecï¼Ÿ***

Word2Vec æ˜¯åˆ›å»º/å­¦ä¹ è¿™äº›åµŒå…¥çš„æœ€å¤è€çš„æ–¹æ³•ä¹‹ä¸€ã€‚Word2Vec ä¸æ˜¯ä¸€ä¸ªå•ä¸€çš„ç®—æ³•ï¼Œè€Œæ˜¯ä¸€ç³»åˆ—æ¨¡å‹æ¶æ„å’Œä¼˜åŒ–ï¼Œå¯ç”¨äºä»å¤§å‹æ•°æ®é›†å­¦ä¹ å•è¯åµŒå…¥ã€‚é€šè¿‡ Word2Vec å­¦ä¹ çš„åµŒå…¥å·²è¢«è¯æ˜åœ¨å„ç§ä¸‹æ¸¸è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šæ˜¯æˆåŠŸçš„ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€é—®é¢˜å›ç­”ã€‚è¯¥è®ºæ–‡æå‡ºäº†ä¸¤ç§å­¦ä¹ å•è¯è¡¨å¾çš„æ–¹æ³•:

**è¿ç»­è¯è¢‹æ¨¡å‹**åŸºäºå‘¨å›´ä¸Šä¸‹æ–‡è¯é¢„æµ‹ä¸­é—´è¯ã€‚ä¸Šä¸‹æ–‡ç”±å½“å‰(ä¸­é—´)å•è¯å‰åçš„å‡ ä¸ªå•è¯ç»„æˆã€‚è¿™ç§æ¶æ„è¢«ç§°ä¸ºå•è¯è¢‹æ¨¡å‹ï¼Œå› ä¸ºå•è¯åœ¨ä¸Šä¸‹æ–‡ä¸­çš„é¡ºåºå¹¶ä¸é‡è¦ã€‚

**è¿ç»­è·³æ ¼æ¨¡å‹**é¢„æµ‹åŒä¸€å¥å­ä¸­å½“å‰å•è¯å‰åä¸€å®šèŒƒå›´å†…çš„å•è¯ã€‚

![](img/872068ab00a655fb9523ae09ee1a99d1.png)

åœ¨ CBOW ä¸­ï¼Œç»™å®šå•è¯(å¿«é€Ÿçš„æ£•è‰²ç›’å­ï¼Œåœ¨æ‡’æƒ°çš„ç‹—ä¸Šé¢)ï¼Œæˆ‘ä»¬æƒ³è¦é¢„æµ‹è·³è·ƒã€‚åœ¨ Skipgram ä¸­ï¼Œä¸å•è¯ jump æ­£å¥½ç›¸åï¼Œæˆ‘ä»¬æƒ³è¦é¢„æµ‹(å¿«é€Ÿçš„æ£•è‰²æ¡†ï¼Œåœ¨æ‡’ç‹—ä¸Šé¢)

***ä½†æ˜¯æ¨¡ç‰¹ä»¬æ˜¯æ€ä¹ˆå­¦ä¹ çš„å‘¢ï¼Ÿ***

![](img/eecb8f662b0e8fa762be561366eb9c39.png)

ã€CBOW(å·¦)å’Œ Skip-gram(å³)çš„æ¶æ„

è®©æˆ‘ä»¬ä» CBOW å¼€å§‹ï¼Œæˆ‘ä»¬ä»¥å¥å­â€œè‡ªç„¶è¯­è¨€å¤„ç†â€ä¸ºä¾‹ï¼Œå…¶ä¸­â€œè‡ªç„¶â€å’Œâ€œå¤„ç†â€éƒ½æ˜¯ä¸Šä¸‹æ–‡è¯ï¼Œâ€œè¯­è¨€â€æ˜¯ç›®æ ‡è¯ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªæµ…çš„ç½‘ç»œï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œåªæœ‰ä¸€ä¸ªéšè—å±‚ã€‚

å› æ­¤ï¼Œè¾“å…¥æ˜¯ä¸€ä¸ªåªæœ‰ä¸€ä¸ª 1 çš„ V é¡¹(è¯æ±‡çš„å¤§å°/å”¯ä¸€å•è¯çš„æ€»æ•°)çš„ç‹¬ä¸€æ— äºŒçš„ç¼–ç å‘é‡ã€‚å‡è®¾æˆ‘ä»¬åªæœ‰ 5 ä¸ªè¯æ±‡(è‡ªç„¶ã€è¯­è¨€ã€å¤„ç†ã€æ˜¯ã€å¾ˆå¥½)ã€‚è‡ªç„¶çš„å‘é‡å°†æ˜¯[1ï¼Œ0ï¼Œ0ï¼Œ0ï¼Œ0]ã€‚ç±»ä¼¼åœ°ï¼Œå¯¹äºå¤„ç†ï¼Œå®ƒå°†æ˜¯[0ï¼Œ0ï¼Œ1ï¼Œ0ï¼Œ0]ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªå¤§å°ä¸º V * *D* çš„éšæœºåˆå§‹åŒ–çš„åµŒå…¥å‘é‡(E ),å…¶ä¸­ D æ˜¯å¯ä»¥é€‰æ‹©çš„å‘é‡çš„ç»´æ•°ã€‚è¿™æ˜¯è¾“å…¥å›¾å±‚çš„æƒé‡çŸ©é˜µã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†è¾“å…¥çš„ç‹¬çƒ­ç¼–ç å‘é‡ä¹˜ä»¥æƒé‡/åµŒå…¥å‘é‡ã€‚è¿™ç»™å‡ºäº†å°ºå¯¸ä¸º 1 D çš„ä¸Šä¸‹æ–‡å•è¯(è‡ªç„¶çš„å’Œå¤„ç†çš„)çš„åµŒå…¥å‘é‡

ç°åœ¨ï¼Œåœ¨éšè—å±‚ä¸­ï¼Œæˆ‘ä»¬å¯¹ä¸Šä¸‹æ–‡å•è¯çš„åµŒå…¥å‘é‡è¿›è¡Œå¹³å‡ï¼Œè¿™å½¢æˆäº†è¯¥å±‚çš„å¤§å°ä¸º 1 ** D.* çš„è¾“å…¥ã€‚è¿™ä¹˜ä»¥å¦ä¸€ä¸ªç§°ä¸ºä¸Šä¸‹æ–‡å‘é‡(Eâ€™)çš„å¤§å°ä¸º D * V çš„å‘é‡ã€‚è¿™ç»™äº†æˆ‘ä»¬ 1 * V çš„å‘é‡ï¼Œè¯¥å‘é‡ç„¶åé€šè¿‡ sigmoid å‡½æ•°å¾—åˆ°æœ€ç»ˆè¾“å‡ºã€‚

å°†æœ€ç»ˆè¾“å‡ºä¸è¯­è¨€çš„ç‹¬çƒ­ç¼–ç å‘é‡(ä¸­é—´å­—)[0ï¼Œ1ï¼Œ0ï¼Œ0ï¼Œ0]è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è®¡ç®—æŸå¤±å‡½æ•°ã€‚è¯¥æŸå¤±è¢«åå‘ä¼ æ’­ï¼Œå¹¶ä¸”ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥è®­ç»ƒè¯¥æ¨¡å‹

å¯¹äº Skip-gramï¼Œæƒ…å†µæ­£å¥½ç›¸åï¼Œæˆ‘ä»¬æœ‰ä¸­é—´è¯çš„ä¸€ä¸ªçƒ­ç¼–ç å‘é‡ï¼Œå®ƒä¹˜ä»¥æƒé‡/åµŒå…¥å‘é‡ E = V * Dï¼Œæˆ‘ä»¬å¾—åˆ°ä¸­é—´è¯çš„åµŒå…¥ï¼Œä½œä¸ºè¾“å…¥å±‚çš„è¾“å‡ºå’Œéšè—å±‚çš„è¾“å…¥ã€‚å®ƒä¸ä¸Šä¸‹æ–‡å‘é‡ E' = D * V ç›¸ä¹˜ï¼Œæˆ‘ä»¬å¾—åˆ°è¾“å‡ºï¼Œè¯¥è¾“å‡ºé€šè¿‡ sigmoid ä¼ é€’ï¼Œå¹¶ä¸ä¸Šä¸‹æ–‡å•è¯è¿›è¡Œæ¯”è¾ƒï¼Œä»¥å¾—åˆ°æŸå¤±å’Œåå‘ä¼ æ’­ã€‚

åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åªåœ¨æœ€åä¿ç•™åµŒå…¥(E)å‘é‡

***æˆ‘ä»¬å°†å¦‚ä½•è·å¾—åµŒå…¥ï¼Ÿ***

Gensim åº“ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¼€å‘å•è¯åµŒå…¥ã€‚Gensim è®©æ‚¨åœ¨è®­ç»ƒè‡ªå·±çš„åµŒå…¥æ—¶å¯ä»¥é€‰æ‹© CBOW æˆ– Skip-gramã€‚(é»˜è®¤ä¸º CBOW)ã€‚é™¤æ­¤ä¹‹å¤–ï¼ŒGensim è¿˜æœ‰ä¸€ä¸ªé¢„è®­ç»ƒåµŒå…¥çš„ç›®å½•ï¼Œè¿™äº›é¢„è®­ç»ƒåµŒå…¥æ˜¯åœ¨å‡ ä¸ªæ–‡æ¡£ä¸Šè®­ç»ƒçš„ï¼Œå¦‚ wiki é¡µé¢ã€google æ–°é—»ã€Twitter tweets ç­‰ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨åŸºäºè°·æ­Œæ–°é—»è¯­æ–™åº“(30 äº¿ä¸ªè¿è¡Œå•è¯)å•è¯å‘é‡æ¨¡å‹(300 ä¸‡ä¸ª 300 ç»´è‹±è¯­å•è¯å‘é‡)çš„é¢„è®­ç»ƒåµŒå…¥ã€‚å®šä¹‰å¤Ÿäº†ã€‚è®©æˆ‘ä»¬æ·±å…¥ç ”ç©¶ä»£ç ğŸ‘¨â€ğŸ’»

**å®æ–½:**

***1ã€‚è¯»å–æ•°æ®é›†***

![](img/0543de08e345f6b77adbe68a2a0922ba.png)

æé†’ä¸€ä¸‹ï¼Œè¿™æ˜¯è®­ç»ƒæ•°æ®çš„æ ·å­

***2ã€‚åŸºæœ¬é¢„å¤„ç†***

```
def preprocess_corpus(texts):
    *#importing stop words like in, the, of so that these can be removed from texts*
    *#as these words dont help in determining the classes(Whether a sentence is toxic or not)*
    mystopwords = set(stopwords.words("english"))
    def remove_stops_digits(tokens):
        *#Nested function that lowercases, removes stopwords and digits from a list of tokens*
        return [token.lower() for token **in** tokens if token **not** **in** mystopwords **and** **not** token.isdigit()
               **and** token **not** **in** punctuation]
    *#This return statement below uses the above function and tokenizes output further.* 
    return [remove_stops_digits(word_tokenize(text)) for text **in** tqdm(texts)]

*#Preprocess both for training and test data*
train_texts_processed = preprocess_corpus(train_texts)
test_texts_processed = preprocess_corpus(test_texts)
```

![](img/8488434cee3f35efbbd15d47539c7b6d.png)

é¢„å¤„ç†çš„ç»“æœ

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä» NLTK åº“ä¸­åˆ é™¤åœç”¨è¯å’Œå®Œæ•´æ•°å­—ï¼Œå°å†™æ‰€æœ‰æ–‡æœ¬ï¼Œå¹¶ä½¿ç”¨ word_tokenize å¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–(åˆ†è§£æˆå•ç‹¬çš„æ ‡è®°/å•è¯)

***3ã€‚åŠ è½½é¢„è®­ç»ƒåµŒå…¥***

æˆ‘ä»¬ä½¿ç”¨ Gensim åº“ä¸ºåœ¨ Google æ–°é—»æ•°æ®é›†ä¸Šè®­ç»ƒçš„å•è¯åŠ è½½é¢„è®­ç»ƒåµŒå…¥ã€‚è°·æ­Œæ–°é—»æ¨¡å‹/åµŒå…¥å‘é‡æ˜¯ 300 ç»´çš„ã€‚æŠ¤ç›®é•œæ–°é—»æ¨¡å‹/åµŒå…¥å‘é‡å¤§çº¦æœ‰ 3 M å­—ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªåµŒå…¥çš„ä¾‹å­ï¼Œå®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå­—å…¸ï¼Œå…¶ä¸­é”®æ˜¯å•è¯ï¼Œå€¼æ˜¯è¯¥å•è¯çš„åµŒå…¥å‘é‡ã€‚

```
*#Path for the models/ embedding vector*
google_news_model = '../input/gensim-embeddings-dataset/GoogleNews-vectors-negative300.gensim'
*#Loading the models/ embedding vector using KeyedVectors.load function from gensim*
w2v_google_news = KeyedVectors.load(google_news_model)
*#Print Shape of the embedding*
print("Shape of embedding vector", w2v_google_news["Natural"].shape)
*#Let's print first 20 dimensions rather than all 300*
print("First 20 numbers in the embedding of the word Natural**\n\n**", w2v_google_news["Natural"][:20])
```

![](img/975c8bec94d81ac3c0662028b5f5164e.png)

è¿™å°±æ˜¯å•è¯â€œNaturalâ€çš„åµŒå…¥æ–¹å¼ã€‚

***4ã€‚ä½¿ç”¨é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹å°†æ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºåµŒå…¥å†…å®¹***

è¿™é‡Œï¼Œæˆ‘ä»¬ä»å…ˆå‰çš„è¾“å…¥æ ‡è®°åŒ–æ–‡æœ¬ä¸­è·å–è¾“å…¥ï¼Œå¹¶ä»é¢„å…ˆè®­ç»ƒçš„åµŒå…¥å‘é‡ä¸­è·å–æ–‡æœ¬ä¸­æ¯ä¸ªå•è¯çš„åµŒå…¥ã€‚è¿™å°†ä¸ºæˆ‘ä»¬æä¾›æœ€ç»ˆçš„è¾“å…¥æ•°æ®é›†ï¼Œå…¶å½¢å¼ä¸ºæ¯ä¸ªå¥å­çš„åµŒå…¥ï¼Œå¯ç”¨äºä¸è¾“å‡ºå˜é‡ä¸€èµ·è®­ç»ƒã€‚

```
*#Function that takes in the input text dataset in form of list of lists where each sentence is a list of words all the sentences are* 
*#inside a list*
def embedding_feats(list_of_lists, DIMENSION, w2v_model):
    zeros_vector = np.zeros(DIMENSION)
    feats = []
    missing = set()
    missing_sentences = set()
    *#Traverse over each sentence*
    for tokens **in** tqdm(list_of_lists):
        *# Initially assign zeroes as the embedding vector for the sentence*
        feat_for_this = zeros_vector
        *#Count the number of words in the embedding for this sentence*
        count_for_this = 0
        *#Traverse over each word of a sentence*
        for token **in** tokens:
            *#Check if the word is in the embedding vector*
            if token **in** w2v_model:
                *#Add the vector of the word to vector for the sentence*
                feat_for_this += w2v_model[token]
                count_for_this +=1
            *#Else assign the missing word to missing set just to have a look at it*
            else:
                missing.add(token)
        *#If no words are found in the embedding for the sentence*
        if count_for_this == 0:
            *#Assign all zeroes vector for that sentence*
            feats.append(feat_for_this)
            *#Assign the missing sentence to missing_sentences just to have a look at it*
            missing_sentences.add(' '.join(tokens))
        *#Else take average of the values of the embedding for each word to get the embedding of the sentence*
        else:
            feats.append(feat_for_this/count_for_this)
    return feats, missing, missing_sentences*#Embeddings for the train dataset*
train_vectors, missing, missing_sentences = embedding_feats(train_texts_processed, 300, w2v_google_news)
```

![](img/da8fd964ad2159e783128a0d1bd7401d.png)

æ€»ä¹‹ï¼Œæ¯ä¸ªå¥å­å°†æœ‰ä¸€ä¸ª 300 ç»´çš„åµŒå…¥å‘é‡ï¼Œå®ƒå°†æ˜¯è¯¥å¥å­ä¸­å‡ºç°çš„å•è¯åµŒå…¥çš„å¹³å‡å€¼ã€‚å•è¯åµŒå…¥æ¥è‡ªé¢„å…ˆè®­ç»ƒçš„å•è¯åµŒå…¥ï¼Œè¿™äº›å•è¯åµŒå…¥åœ¨ google news ä¸Šè¢«è®­ç»ƒä»¥æ‰¾åˆ°åµŒå…¥ã€‚

***5ã€‚è®­ç»ƒå’ŒéªŒè¯å¤šè¾“å‡ºåˆ†ç±»å™¨***

è¿™ä¸€éƒ¨åˆ†å°†æ¶‰åŠ 5 ä»¶äº‹

1.  è·å–è®­ç»ƒæ•°æ®é›†çš„åµŒå…¥å‘é‡
2.  å°†åµŒå…¥å‘é‡å’Œè¾“å‡ºå˜é‡åˆ†æˆè®­ç»ƒé›†å’ŒéªŒè¯é›†
3.  åœ¨è®­ç»ƒåµŒå…¥å‘é‡å’Œè¾“å‡ºå˜é‡ä¸Šæ‹Ÿåˆå¤šè¾“å‡ºé€»è¾‘å›å½’æ¨¡å‹*(æˆ‘åœ¨ä¹‹å‰çš„æ•…äº‹(* [*ç¬¬ä¸€éƒ¨åˆ†(bagowords)*](https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-1e628a2dd4c9)*)*ä¸­å·²ç»è¯¦ç»†ä»‹ç»äº†é€»è¾‘å›å½’)
4.  å¯¹éªŒè¯åµŒå…¥å‘é‡è¿›è¡Œé¢„æµ‹
5.  æ ¹æ® ROC-AUC è¡¡é‡ç»©æ•ˆ

```
def train_model(DIMENSION, model):
    *#Get the embedding vector for the training data*
    train_vectors, missing, missing_sentences = embedding_feats(train_texts_processed, DIMENSION, model)
    *#Split the embedding vector for the training data along with the output variables into train and validation sets*
    train_data, val_data, train_cats, val_cats = train_test_split(train_vectors, train_labels)
    *#Logistic Regression Model (As we have unbalanced dataset, we use class_weight which will use inverse of counts of that class. It penalizes mistakes in samples of class[i] with class_weight[i] instead of 1)*
    lr = MultiOutputClassifier(LogisticRegression(class_weight='balanced', max_iter=3000)).fit(train_data, train_cats)
    *#Actuals for the validation data*
    y_vals = val_cats
    *#Prediction probability for the validation dataset by the model for class 1*
    y_preds = np.transpose(np.array(lr.predict_proba(val_data))[:,:,1])
    *#Calculate the Mean ROC_AUC* 
    mean_auc = mean(accuracy(y_vals,y_preds))
    return mean_auc, lrmean_auc, lr = train_model(300, w2v_google_news)
```

è¿™ä¸ªæ¨¡å‹è¢«è¯æ˜æ˜¯ç›¸å½“æ¸©å’Œçš„(~0ã€‚60 ROC-AUC)ã€‚ä½†åŒæ ·ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯å­¦ä¹ å¦‚ä½•å®ç°å•è¯åµŒå…¥ã€‚ä½æ€§èƒ½å¯èƒ½æ˜¯å› ä¸ºé¢„è®­ç»ƒçš„åµŒå…¥æ²¡æœ‰æ­£ç¡®åœ°æ•æ‰ç»†èŠ‚ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ Word2Vec æ¥è®­ç»ƒæˆ‘ä»¬è‡ªå·±çš„åµŒå…¥ã€‚

*TODOs:*

1.  ä»å¤´å¼€å§‹è®­ç»ƒ Word2Vec æ¨¡å‹
2.  å°è¯•é›†åˆæ¨¡å‹ï¼Œè€Œä¸æ˜¯æ™®é€šçš„ ML æ¨¡å‹åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ‰“åŒ…å’Œå¢å¼ºæ¨¡å‹æ¯”ç»å…¸çš„ ML æŠ€æœ¯ç»™å‡ºæ›´å¥½çš„ç»“æœ
3.  å¯ä»¥åšæ›´å¥½çš„æ–‡æœ¬é¢„å¤„ç†ã€æ‰“å­—é”™è¯¯çº æ­£ç­‰æ¥è¿›ä¸€æ­¥æ”¹è¿›æ¨¡å‹

è¿™æ˜¯å…³äº Word2Vec çš„ï¼Œä¸‹ä¸€ä¸ªï¼Œå°†æ˜¯å…³äºè„¸ä¹¦çš„ fastTextï¼Œå®ƒå°†å•è¯åµŒå…¥çš„æ€æƒ³å‘å‰æ¨è¿›äº†ä¸€æ­¥ï¼Œå®ç°äº†ä¸€ç§å«åšå­å•è¯åµŒå…¥çš„ä¸œè¥¿ã€‚åœ¨é‚£ä¹‹å‰ä¿æŒå®‰å…¨ã€‚åŒæ ·ï¼Œå®Œæ•´çš„ä»£ç å‡ºç°åœ¨[(è¿™é‡Œ)](https://www.kaggle.com/anirbansen3027/jtcc-word2vec)ã€‚è¯·ä»¥å›ç­”å’Œé¼“æŒçš„å½¢å¼æä¾›æ‚¨çš„åé¦ˆ:)