<html>
<head>
<title>Activation Functions in ML</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML中的激活函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-functions-in-ml-207aaf596b34?source=collection_archive---------15-----------------------#2020-05-20">https://medium.com/analytics-vidhya/activation-functions-in-ml-207aaf596b34?source=collection_archive---------15-----------------------#2020-05-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/7c191032d41c040571b9b939acaf758e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/1*4GhdaPm1vQ13tOgUQWqK1A.gif"/></div></figure><p id="3af6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在机器学习中，神经网络是几个神经元的联合，以试图模仿人类神经元的行为，正如我们所知，我们的神经元相互连接，以执行不同的复杂任务，如对对象进行分类，检测某些顾客等。这就是机器学习中这些神经网络存在的原因。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es jk"><img src="../Images/a0dd3ce19033589f3d4764ca1385aac1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*r0fxAZRpRGapPnC4bniDiQ.png"/></div></figure><p id="b67f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，这些网络能够执行图像分类、预测和决策等操作。组成神经网络的每个节点都接收信息，并将其计算为新值，该值将成为下一层的新输入。每个神经元都必须使用一个激活函数，因为它们与这个执行单元相连。</p><p id="0fdb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Z = W * X + b</p><p id="2815" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">A = act_fun(Z)</p><p id="3d53" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">W =是每个神经元的权重</p><p id="6f25" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">b =每个神经元的偏差</p><p id="8e17" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">A =将作为下一层输入的最终结果</p><p id="19e1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">act_fun =活动函数</p><p id="ed74" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">选择正确的激活函数非常重要，因为它将直接影响网络性能，这就是为什么我将尝试解释主要的激活函数，以便您可以选择与您的算法更好地匹配的函数。</p><h2 id="18a6" class="jp jq hi bd jr js jt ju jv jw jx jy jz ix ka kb kc jb kd ke kf jf kg kh ki kj bi translated">阶跃函数</h2><p id="8a9c" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix km iz ja jb kn jd je jf ko jh ji jj hb bi translated">阶跃函数很容易理解，0是小于或等于阈值的数，1是大于阈值的数</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/81f83fd2125cfc2dd011d382156aa958.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/0*D3WfIdVASNTnDyLp.png"/></div></figure><p id="cd8a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个函数用于分类一个单独的类，所以你可以说一个特定的图片是否属于一个类，所以用这个函数你只能说是或不是，你没有更多的选择或者它属于或不属于，就是这样。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/9bda65517b7869edc4764cabceef9ad4.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*fcN70TaXZJMg4idh2-Nh4A.png"/></div></figure><h2 id="1b4c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ix ka kb kc jb kd ke kf jf kg kh ki kj bi translated"><strong class="ak">线性函数</strong></h2><p id="12ee" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix km iz ja jb kn jd je jf ko jh ji jj hb bi translated">这是一个线性激活函数，我们的函数与神经元或输入的加权和成正比，如果你能看到这是同一条线的话。</p><p id="7c8c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">A = cx</p><p id="41c1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个函数允许多个输出，因为你没有任何限制，正因为如此，你可以分类多个类。</p><p id="f010" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">但是作为二进制分类器，线性激活函数不能很好地工作，主要原因是梯度下降的问题，因为A = cx，对x的导数是c，这意味着梯度与输入无关。这是一个恒定的梯度，下降将是恒定的梯度，这意味着训练不依赖于输入的变化。</p><p id="ac96" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个函数的另一个问题是，在学习过程中，不止一个神经元会被激活。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/4ae196ac981f3c55bdc886fa2e9c25a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*ubzGgdLQfrc44Ctm.png"/></div></figure><h2 id="da62" class="jp jq hi bd jr js jt ju jv jw jx jy jz ix ka kb kc jb kd ke kf jf kg kh ki kj bi translated"><strong class="ak">乙状结肠功能</strong></h2><p id="209b" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix km iz ja jb kn jd je jf ko jh ji jj hb bi translated">Sigmoid是一个非线性函数，输出总是在0和1之间，因此是一个很好的分类器，你可以使用<br/>概率方法来获得更高的值，并确定哪个神经元将被激活。</p><p id="cc86" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Sigmoid函数是当今使用最广泛的激活函数之一。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/81f205885039992f8b5ddf274d270fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/0*uU60OK7wOA_sJfP3.png"/></div></figure><p id="2514" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">你会注意到，当值太大或太小时，该区域的梯度会变小。这就产生了一个“消失梯度”的问题，因此当梯度很小或已经消失时，由于其值极小而不能产生显著的变化。网络拒绝进一步学习或者速度非常慢</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/e75bd6e908f5d96897bf1fb931e2106a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*PJs8YiMoCYPv6wOw.png"/></div></figure><h1 id="654e" class="kt jq hi bd jr ku kv kw jv kx ky kz jz la lb lc kc ld le lf kf lg lh li ki lj bi translated">Tanh函数</h1><p id="76bf" class="pw-post-body-paragraph im in hi io b ip kk ir is it kl iv iw ix km iz ja jb kn jd je jf ko jh ji jj hb bi translated">这个函数与我们上面讨论的sigmoid函数有相似之处。它是非线性的，输出也有一个范围(-1，1)</p><p id="63bf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Tanh也是一个非常流行和广泛使用的激活函数。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/72281cfa43397d1d01f2b875da311b3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:336/format:webp/0*7AmdYFw-mFjx0Ksm.png"/></div></figure><p id="9752" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在sigmoid或tanh之间做出决定将取决于您对梯度强度的要求，因为tanh梯度比sigmoid更强，也更容易对具有强负值、中性值和强正值的输入进行建模。由于这些原因，带有tanh隐层的神经网络更容易训练。</p><p id="0eff" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">问题是，作为sigmoid，tanh也有消失梯度问题。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/c58143f71af2f69757d442aed0ef9164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*pXxG4zRXCsIsns9I.png"/></div></figure><p id="d3a9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> ReLU </strong></p><p id="bfd0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">代表整流线性单元，它混合了线性和非线性功能的优点，提供与Sigmoid相同的优势，但性能更好。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/89e137fa6ae721f9b426357b24906770.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/0*Rs_O985emHkblSfb.png"/></div></figure><p id="44e4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">ReLu在计算上比tanh和sigmoid便宜，因为它涉及更简单的数学运算。此外，这个功能不会同时激活所有的神经元。</p><p id="1284" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">由于另一个函数ReLU也有问题，当输入接近零或为负时，该函数的梯度为零，因此神经网络不会学习，这种现象称为死亡Relu问题。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/c4d39ab4398b64ab8fda1107a51a6a1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*cRgE6BdtJh7Y4A3B.png"/></div></figure><p id="d8c0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> Softmax </strong></p><p id="bd5d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">理想的多类分类，当你需要选择一个类。因此通常用于输出层。</p><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/13a85c6db7fd88da4a7a1f2ce7942247.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/0*IAodRqxlMNNYAxmD.png"/></div></figure><figure class="jl jm jn jo fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/76eaea07507224b575d0a18c039bc00e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*DMKJUMzFWxoWfw_K.png"/></div></figure><p id="3b06" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">该函数计算所有可能的目标类中每个类的概率。所有概率的总和等于1，并且它们的最大值是分类的类别。</p></div></div>    
</body>
</html>