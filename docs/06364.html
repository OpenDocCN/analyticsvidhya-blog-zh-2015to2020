<html>
<head>
<title>Natural Language Processing to evaluate English Newspapers for Vocab, Facts vs Opinions ( Sentimental Analysis ) using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python评估英文报纸词汇、事实与观点(情感分析)的自然语言处理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/natural-language-processing-to-evaluate-english-newspapers-for-vocab-facts-vs-opinions-ebe9cb30942d?source=collection_archive---------17-----------------------#2020-05-19">https://medium.com/analytics-vidhya/natural-language-processing-to-evaluate-english-newspapers-for-vocab-facts-vs-opinions-ebe9cb30942d?source=collection_archive---------17-----------------------#2020-05-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="b07a" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">文本解析(docx2txt)、标记化、词干化、探索性数据分析、基于规则的情感分析、图像提取。</p></blockquote><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jh"><img src="../Images/ceec3764c1af21ae7ecbb1bb0a85a4fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8C52BgD5u5rMtVVeaQIBbQ.jpeg"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">自然语言处理；图片来源<a class="ae jx" href="https://www.mediaupdate.co.za/media/142841/what-is-natural-language-processing" rel="noopener ugc nofollow" target="_blank">MediaUpdate.co.za</a></figcaption></figure><p id="e120" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">考虑到手头的空闲时间，我不知道该看哪份报纸。对我来说，重点是提高我的词汇和事实。这就形成了初级的<strong class="il hj"> <em class="ik">问题陈述</em> </strong>为文章。</p><p id="1383" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">这篇文章是执行自然语言处理的基本方法，用于评估2020年5月10日出版的印度各地的一些知名英文报纸。</p><p id="2ad8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">顾名思义，NLP是一个研究成熟的、进化的人类口语的领域，如英语、印地语或汉语。</p></div><div class="ab cl kb kc gp kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="hb hc hd he hf"><p id="13e2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><em class="ik">我认为评估报纸内容的要点:</em></p><p id="1374" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><em class="ik"> —每段的词汇或独特字数。<br/> —每页的图像数，假设更多的图形&amp;图像增加了放纵。<br/> —事实Vs观点(情感分析—文本点)<br/> —探索性数据分析:可视化中间结果，进一步清理和预处理文本。<br/> —一般情况下提供的统计数据/数字</em></p><p id="92f6" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">NLP包括一步一步地重复文本过滤和格式化，然后开始应用进一步的DS技术进行任何预测。</p></div><div class="ab cl kb kc gp kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="hb hc hd he hf"><h2 id="a181" class="ki kj hi bd kk kl km kn ko kp kq kr ks jy kt ku kv jz kw kx ky ka kz la lb lc bi translated"><strong class="ak">A部分(数据收集)</strong></h2><p id="b719" class="pw-post-body-paragraph ii ij hi il b im ld io ip iq le is it jy lf iw ix jz lg ja jb ka lh je jf jg hb bi translated"><strong class="il hj">第一步</strong> <em class="ik"> : </em>从<a class="ae jx" href="https://www.newspaperpdf.online/" rel="noopener ugc nofollow" target="_blank">这个网站</a>积累报纸的电子版(pdf)并将其转换为word <em class="ik"> docx </em>用于这四家报纸:<em class="ik"> The Hindu，Times of India，Indian Express，Hindustan Times。</em></p><p id="072b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">第二步</strong>:使用<em class="ik"> docx2txt </em> python库加载报纸文本和图片。</p><p id="6c39" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">第三步</strong>:使用正则表达式获取报纸文本中的所有数字&amp;小数。更多的数字。更多的统计数字。</p></div><div class="ab cl kb kc gp kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="hb hc hd he hf"><h2 id="7945" class="ki kj hi bd kk kl km kn ko kp kq kr ks jy kt ku kv jz kw kx ky ka kz la lb lc bi translated"><strong class="ak">B部分(数据清理)</strong></h2><p id="811f" class="pw-post-body-paragraph ii ij hi il b im ld io ip iq le is it jy lf iw ix jz lg ja jb ka lh je jf jg hb bi translated"><strong class="il hj">第一步</strong>:所有单词小写</p><p id="4564" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">第二步</strong>:只取字母，去掉标点、逗号和其他字符，如括号、引号。</p><p id="3c8a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">第三步</strong>:词干提取——将单词分解为词根，去掉前缀和后缀。例如:跑步→跑步；带来→带来。</p><p id="badc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">第四步<em class="ik"> </em> </strong>:删除停用词。停用词是那些不会给文章的上下文增加价值的词。例如' the '，' a '，' in '。</p><p id="0a9c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">在这里，我们进行第一轮数据清理，并将数据保存在<em class="ik"> pickle中。</em></p><p id="f48c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">由于上述步骤有时会占用大量处理器资源，我们将结果保存为一个变量(<em class="ik"> corpus.pkl </em>)，以便以后调用。</p><figure class="ji jj jk jl fd jm"><div class="bz dy l di"><div class="li lj l"/></div></figure></div><div class="ab cl kb kc gp kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="hb hc hd he hf"><h2 id="7908" class="ki kj hi bd kk kl km kn ko kp kq kr ks jy kt ku kv jz kw kx ky ka kz la lb lc bi translated"><strong class="ak">C部分:创建文档术语矩阵</strong></h2><p id="de0c" class="pw-post-body-paragraph ii ij hi il b im ld io ip iq le is it jy lf iw ix jz lg ja jb ka lh je jf jg hb bi translated">从过滤出的文本中，我们创建文档术语矩阵，该矩阵记录每个唯一单词在所有输出文本中的出现次数。</p><pre class="ji jj jk jl fd lk ll lm ln aw lo bi"><span id="20ce" class="ki kj hi ll b fi lp lq l lr ls">cv = CountVectorizer(stop_words = ‘english’,ngram_range = (1,1) )<br/>docTermMatrix = cv.fit_transform(corpus).toarray()<br/>data_dtm = pd.DataFrame(docTermMatrix,columns = cv.get_feature_names())<br/>data_dtm.index = pd.Index(newspapers)<br/>data_dtm = data_dtm.transpose()</span></pre><p id="8239" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">DTM的转置对我们来说是这样的:</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lt"><img src="../Images/2a07c1610e8d7eff4ce5fb1b97d60cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*vFLvWZ2Rfko43fZYpszRWg.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">文档术语矩阵</figcaption></figure></div><div class="ab cl kb kc gp kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="hb hc hd he hf"><h2 id="9150" class="ki kj hi bd kk kl km kn ko kp kq kr ks jy kt ku kv jz kw kx ky ka kz la lb lc bi translated"><strong class="ak">D部分:探索性数据分析</strong></h2><p id="ce25" class="pw-post-body-paragraph ii ij hi il b im ld io ip iq le is it jy lf iw ix jz lg ja jb ka lh je jf jg hb bi translated"><strong class="il hj">第一步</strong>:这里我们看到有一些单字单词，比如:“eeeee”等。所以我们在A部分增加了一个步骤来清除这样的单词。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lu"><img src="../Images/553984bea8627e6dcc1b5f7fe36b0581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ElGdtYF1BRrBRUafG2qs0Q.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">删除单字符单词</figcaption></figure><p id="d441" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">第二步</strong>:有了这样一个矩阵，找出每份报纸的前30个(最常出现的)单词就更容易了。然后检查这些报纸中的热门词汇，并查看所有报纸中常见的热门词汇。</p><p id="0294" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">既然这些词如此普遍，它们并没有给我们的DTM增加价值。我们通过将这些单词作为<em class="ik">停用词</em>添加到现有单词中来过滤掉它们。</p><pre class="ji jj jk jl fd lk ll lm ln aw lo bi"><span id="9252" class="ki kj hi ll b fi lp lq l lr ls"># Checking out top 30 words for all newspapers<br/>top_dict = {}<br/>for c in data_dtm.columns:<br/> top = data_dtm[c].sort_values(ascending =False).head(30)<br/> top_dict[c] = list(zip(top.index,top.values))</span><span id="efee" class="ki kj hi ll b fi lv lq l lr ls"># checking top words collective in these and seeing top occurring words accross <br/>words = []<br/>for newspaper in data_dtm.columns:<br/> top = [word for (word,count) in top_dict[newspaper]]<br/> for t in top:<br/> words.append(t)</span><span id="9b2f" class="ki kj hi ll b fi lv lq l lr ls">from collections import Counter <br/>Counter(words).most_common()</span></pre><p id="5d39" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">第三步</strong>:同样，有了一组新的停用词，我们更新了文档术语矩阵。</p><p id="3c63" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">第四步</strong>:我们将DTM可视化为单词云，以查看最常出现的单词。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lw"><img src="../Images/3d6cfa12f85be857075b164868fdf7b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pBmiBNoRYrkqloUthxyD9g.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">WordCloud</figcaption></figure><p id="dcd8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">第五步</strong>:我们解析保存在“<em class="ik">\ NLP _ extract Images”</em>目录下的图像也有一些小的，比如线条和点状图像。我们发现，一般来说，对于当前的场景，超过5 kb的图像是有意义的。</p></div><div class="ab cl kb kc gp kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="hb hc hd he hf"><h2 id="374b" class="ki kj hi bd kk kl km kn ko kp kq kr ks jy kt ku kv jz kw kx ky ka kz la lb lc bi translated"><strong class="ak">E部分:分析</strong></h2><p id="c23f" class="pw-post-body-paragraph ii ij hi il b im ld io ip iq le is it jy lf iw ix jz lg ja jb ka lh je jf jg hb bi translated">第一步:因为每份报纸的页数不同。我们得到了所有报纸的页数。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lx"><img src="../Images/f0bab975fcbec426fe089b125ea4637e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RctMYk0pNzbNcR74NVRndA.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">所有页面中唯一单词的总数</figcaption></figure><p id="f774" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">此外，我们检查每份报纸每页的独特字数，以创建一个情节:</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lx"><img src="../Images/b0e09d06d8d5abcab927545820171b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4U1IqkKcflfHfRKL-VSzYA.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">每页的唯一字数</figcaption></figure><pre class="ji jj jk jl fd lk ll lm ln aw lo bi"><span id="7142" class="ki kj hi ll b fi lp lq l lr ls"># Getting unique words / Vocabulary<br/>unique_list = []<br/>for newspaper in data_dtm.columns:<br/> uniques = data_dtm[newspaper].to_numpy().nonzero()[0].size<br/> unique_list.append(uniques)</span><span id="d062" class="ki kj hi ll b fi lv lq l lr ls">unique_words = pd.DataFrame(list(zip(newspapers,unique_list)),columns = [‘newspaper’,’unique_word’])<br/>#unique_words= unique_words.sort_values(‘unique_word’,ascending = False)</span><span id="283c" class="ki kj hi ll b fi lv lq l lr ls"># Manually checked <br/>NoOfPages = [ [‘The Hindu’,22], [‘Times Of India’,18], [‘Indian Express’,18],[“Hindustan Times”,16] ]<br/>NoOfPages = pd.DataFrame(NoOfPages, columns = [‘Newspaper’,’PageCount’])<br/>NoOfPages = NoOfPages.transpose()</span><span id="8f0e" class="ki kj hi ll b fi lv lq l lr ls"># Unique words per page<br/>WPP = []<br/>for i,j in enumerate(NoOfPages):<br/> WPP.append( int(unique_words.unique_word[i] / NoOfPages[i].PageCount) )</span></pre><p id="3b30" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">步骤二</strong>:绘制文本中使用的数字。因为我们知道事实和数字让观众参与任何形式的媒体或演示。</p><pre class="ji jj jk jl fd lk ll lm ln aw lo bi"><span id="cc81" class="ki kj hi ll b fi lp lq l lr ls">file = open('stats.pkl', 'rb')<br/>stats = pickle.load(file)<br/>file.close()<br/>statsLen = [len(li) for li in stats ]</span><span id="29a7" class="ki kj hi ll b fi lv lq l lr ls">barlist = plt.barh(X, statsLen , align= 'center', alpha = 0.5)<br/>barlist[0].set_color('0.4')<br/>barlist[1].set_color('r')<br/>barlist[2].set_color('b')<br/>barlist[3].set_color('g')<br/>plt.yticks(X,newspapers)<br/>plt.xlabel('Numeric Figures used')<br/>plt.title('Numeric Figures used')<br/>plt.show()</span></pre><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lx"><img src="../Images/9b07c5895b8b0a4c2b616799e591e2bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KpyHKIJH-G3LyemKzKR-vQ.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">每份报纸使用的数字</figcaption></figure><p id="5ab8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">步骤三</strong>:使用文本块的情感分析</p><p id="190b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">TextBlob是基于规则的方法。所以要记住，它包含的所有英语单词的极性和主观性都是由语言学家“汤姆·德·斯梅特”手动标记的。因此，一个词在不同的上下文中可能有不同的意思。因此，一个词可以以不同的极性和主观性重复自己(如下图所示)。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ly"><img src="../Images/dae2397eaf303c248ec89b029b7718a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*roddopPJFRjl6t1MyEzB_Q.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">courtsey:Alice Zhao(adashofdata)@<a class="ae jx" href="https://www.youtube.com/watch?v=N9CT6Ggh0oE&amp;t=263" rel="noopener ugc nofollow" target="_blank">YouTube</a></figcaption></figure><p id="0d16" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">我们将每份报纸的主观性计算为:</p><pre class="ji jj jk jl fd lk ll lm ln aw lo bi"><span id="da72" class="ki kj hi ll b fi lp lq l lr ls">from textblob import TextBlob<br/>sentiment = []<br/>for i in np.arange(4):<br/> sentiment.append(TextBlob(corpus[i]).subjectivity)</span><span id="0e70" class="ki kj hi ll b fi lv lq l lr ls">plt.scatter(X,sentiment,linewidths=5)<br/>plt.xticks(X,newspapers)<br/>plt.ylabel(“←Facts — — — — — — — — -Opininios →”)<br/>plt.title(“Subjectivity Graph”)<br/>plt.show()</span></pre><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lz"><img src="../Images/e0319011a4e3209f3f93464e2f4759a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oHZEYy1nv0bTFRnthEiXyw.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">主观性情节</figcaption></figure><p id="aa3c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated"><strong class="il hj">第四步</strong>:每份报纸使用的大尺寸图片数量。5 Kb以上，如EDA中所述:</p><pre class="ji jj jk jl fd lk ll lm ln aw lo bi"><span id="a9bf" class="ki kj hi ll b fi lp lq l lr ls">paths = [ BasePath + “\\TH\\”, BasePath + “\\TOI\\” , BasePath + “\\IE\\”, BasePath + “\\HT\\” ]<br/>for path in paths:<br/>  os.scandir(path)<br/>  counter = 0 <br/>  for entry in os.scandir(path):<br/>      size = entry.stat().st_size<br/>      if size &gt; 5000 :<br/>          counter += 1<br/>  imagesCount.append(counter)</span><span id="7a93" class="ki kj hi ll b fi lv lq l lr ls">barlist = plt.bar(X, imagesCount , align= ‘center’, alpha = 0.5)<br/>barlist[0].set_color(‘0.4’)<br/>barlist[1].set_color(‘r’)<br/>barlist[2].set_color(‘b’)<br/>barlist[3].set_color(‘g’)<br/>plt.xticks(X,newspapers)<br/>plt.ylabel(‘No of Significant Images’)<br/>plt.title(‘No of Significant Images’)<br/>plt.show()</span></pre><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ma"><img src="../Images/20e650efa98fccced7c8db8aab9b4faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WqrMoUcq1LzQ3kss_-NEBg.png"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">使用的重要图像数量</figcaption></figure><p id="0cfe" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">前往我的<a class="ae jx" href="https://gist.github.com/getgimphed/e46082cabc1434d793f4889e4a5b7ac9" rel="noopener ugc nofollow" target="_blank"> github </a>集体编译代码。</p></div><div class="ab cl kb kc gp kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="hb hc hd he hf"><h2 id="5a1b" class="ki kj hi bd kk kl km kn ko kp kq kr ks jy kt ku kv jz kw kx ky ka kz la lb lc bi translated"><strong class="ak">结论</strong></h2><p id="439a" class="pw-post-body-paragraph ii ij hi il b im ld io ip iq le is it jy lf iw ix jz lg ja jb ka lh je jf jg hb bi translated">我最初的意图是挑选一本有助于提高我的词汇和知识的读物。上面的锻炼给了我一个很好的准备。#StayHomeStaySafe</p><p id="22de" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jy iv iw ix jz iz ja jb ka jd je jf jg hb bi translated">P.S .文章纯属学术兴趣<strong class="il hj"> <em class="ik">非</em> </strong>比较各自的“印刷媒体品牌”。</p></div></div>    
</body>
</html>