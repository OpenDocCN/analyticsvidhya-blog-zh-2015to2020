<html>
<head>
<title>Active-Learning Based Improvement for Multiple Domains</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于主动学习的多领域改进</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/active-learning-based-improvement-for-multiple-domains-1d228da50f40?source=collection_archive---------13-----------------------#2019-12-06">https://medium.com/analytics-vidhya/active-learning-based-improvement-for-multiple-domains-1d228da50f40?source=collection_archive---------13-----------------------#2019-12-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5100" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习中的模型需要大量的标记数据来获得比其他方法更高的精度和性能。然而，如此大量的标记数据的可用性是各种领域的瓶颈。一种之前被证明可以获得更准确结果的新方法是一种称为Tri-training [6]的方法，其中使用部分训练数据训练3个独立的模型，以在无监督数据上生成代理标签。</p><p id="ed36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在此之后，应用多种轮询策略(见下文)根据生成的预测对数据进行采样，三个模型以循环方式在扩充的数据分割上进行再训练，因此称之为主动学习。我们的目标是使用半监督学习对深度学习模型进行迭代微调，以提高模型性能，并分析不同领域的策略结果，以获得一般方向和规则的例外。</p><p id="fbce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">改进主要集中在减少对大型带注释数据集的需求，同时保持与具有相似数据样本大小的模型接近或更好的性能的最佳策略上。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="9633" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">动机:</strong></h1><p id="b0e7" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">我们可以通过在相对较小的数据集上进行微调，来提高在大型数据集上训练的机器学习模型的准确性。存在大量无监督的数据，可以利用这些数据来增加训练数据的大小。这可以提高测试数据的性能，并可以调整模型以在看不见的数据上表现良好。无监督数据的手动标记是一个繁琐的过程，因此我们希望测试tri-training [6]来从以前没有探索过的领域(如VQA等多模态领域)的无监督数据中生成代理标签。Tri训练被表征为用于无监督域适应的模型不可知代理标记方法。我们还想测试tri-training是否可以用于从无监督的数据集中识别重要的例子。这个过程可以反复重复，以提高模型的准确性(主动学习)。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="c07d" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">相关工作:</strong></h1><p id="6b64" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">在tri-training论文[1]中，在UCI数据集上训练的三个分类器用于产生对未标记数据的预测，然后使用模型之间多数一致的轮询策略来决定是否将数据扩充回原始数据分裂。然而，在这里，采样数据被附加到属于不一致模型的数据分割中。在另一篇tri-training论文[2]中，在Amazon Reviews数据集上使用了两个经过训练的模型来对未标记的数据进行投票，并使用第三个模型来对原始和增强的拆分一起进行训练。</p><p id="22ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们还在图像数据集MNIST、SVHN、SYN Digits和SYN Signs上进行了相同的实验，并注意到与基线相比，准确性有了显著提高。在另一篇论文[9]中，诸如在训练和目标域中最大化信息增益的策略被用于智能地采样数据并扩充回训练分割。我们发现，与基本试运行中的两个模型相比，当所有三个模型都一致时，当预测被反馈时，方法[1]导致相对较差的性能。</p><p id="2480" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们选择采用三种模式协议作为我们的战略。我们对[2]中使用的思想做了进一步的贡献。我们采用了[9]中的循环训练思想，尽管必须注意的是，他们没有采用三种决策模型，而是仅仅依赖于策略。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="3a46" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">方法:</strong></h1><p id="6a13" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">用于所有不同领域的模型都是相对经典的，因为这里的目的不是提出一个新的模型来改进现有的基线，而是提出通过利用无监督数据来提高模型性能的方法。此外，为实现我们的方法而为每个域选择的基础模型是在考虑计算约束的情况下选择的，因此该模型可能不会给出该特定数据集的S.O.T.A .结果。</p><p id="547b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">主动学习是机器学习的一种特殊情况，在这种情况下，学习算法能够交互式地询问用户(或一些其他信息源)以在新的数据点获得期望的输出。有些情况下，未标记的数据非常丰富，可以用来改进现有的模型。在这种情况下，学习算法可以主动向用户/教师查询标签。这种类型的迭代学习被称为主动学习。由于学习者选择例子，学习一个概念的例子的数量通常比正常的监督学习所需的数量少得多。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es kn"><img src="../Images/6b2ed336b5ebf0e74112c428238c072b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/0*2Z2VkNtlGohQh3zp"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图1)主动学习</figcaption></figure><p id="a1c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经使用三元训练来标注未标记的数据。Tri-training是最著名的多视图训练方法之一，它利用三个独立训练的模型的一致性来减少对未标记数据的预测偏差。三训的主要要求是初始模型多样。这可以通过使用三个模型中每个模型的三分之一的输入数据来实现。然后执行一个轮询策略，将来自无监督数据集的示例扩充回我们的输入数据集。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es kz"><img src="../Images/80972ec03fcaf49bca7aa391131856d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mUJxycUqqPJ5T20E"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图2)三元训练</figcaption></figure><p id="6700" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">按照下面的算法，我们使用bootstrap采样来训练原始训练数据的3个模型。然后在这些样本上训练三个模型m1、m2和m3。基于3个模型如何就其标签达成一致，将未标记的数据点添加到模型的训练集中。在我们的实验中，这个过程重复了3次。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es le"><img src="../Images/4278e5a465c9d88e69db73bb19c98ce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*147wI2a5wN6Ii2Sv"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图3)数据分割</figcaption></figure></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="19f5" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">抽样(轮询)策略:</strong></h1><p id="f76e" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated"><strong class="ih hj">抽样策略1 </strong>:任意两个模型同意一个样本。用事实标签代替他们的预测。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lf"><img src="../Images/5d216c1cb377c6b792172b463b9e013d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*I1F8kqS8mTVeH9R6"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图4)采样策略1</figcaption></figure><p id="5b8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">采样策略2: </strong>所有3个模型预测样本上的同一标签。用基本事实标签替换他们的预测(即使预测不同)。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lf"><img src="../Images/f1bbb1a331f189edb2c4b6425543bba1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oMhYMW7q_C1uTHES"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图5)采样策略2</figcaption></figure><p id="7f21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">采样策略3: </strong>所有3个模型预测样本上的同一标签。用他们的预测作为样本的标签。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lf"><img src="../Images/475efb2b6a59be7b6707bc62a400a80b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*V-M1lh4zXWxXIwAj"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图6)采样策略3</figcaption></figure></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="d562" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">域和数据集:</strong></h1><p id="66d2" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">我们使用以下模型在以下领域进行了实验:</p><ol class=""><li id="404f" class="lg lh hi ih b ii ij im in iq li iu lj iy lk jc ll lm ln lo bi translated"><strong class="ih hj">视觉问答</strong>:使用“神经VQA”的VQA 2.0数据集【3】</li><li id="4246" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated"><strong class="ih hj">问题回答</strong>:斯坦福问题回答数据集(SQuAD) 2.0 [12]使用“双向注意力流”[8]</li><li id="c572" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated"><strong class="ih hj">音频分类</strong>:使用“扩张CNN”的城市声音数据集</li><li id="30d6" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated"><strong class="ih hj">图像分类</strong>:使用“VGG16”的CIFAR10</li><li id="b9c1" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated"><strong class="ih hj">图像分类</strong>:使用“VGG16”的CIFAR100</li></ol></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="2770" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">程序:</strong></h1><ol class=""><li id="08d1" class="lg lh hi ih b ii ki im kj iq lu iu lv iy lw jc ll lm ln lo bi translated">为相应的域训练100%的训练数据集。评估验证数据集并记录准确性。我们将这种模型称为“Oracle”</li><li id="d5c2" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">将输入训练数据集分成70% — 30%的比例。30%的分割是我们的无监督数据集</li><li id="5c4e" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">使用所有70%的训练数据训练n个时期的模型。评估验证数据集，并记录准确性和训练数据集大小。在我们的结果中，用70%的数据训练的这个模型被称为“基线”。</li><li id="ee8d" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">使用上述3种策略来训练模型:</li></ol><p id="db85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a.使用所有三个模型一致的预测。</p><p id="bd34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b.使用所有三个模型都一致的预测。用地面实况替换所有预测，将它们添加到训练数据集中。</p><p id="efcc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">c.使用两个模型一致的预测。用地面实况替换所有预测，将它们添加到训练数据集中。</p><p id="3c21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.针对上述3种采样策略情况中的每一种，重复以下步骤进行3次主动学习迭代:</p><p id="20c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a.将基本数据集分成3个子集。</p><p id="5bb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b.基于每个1/3数据分割训练3个模型..</p><p id="5b1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">c.使用所有3个模型从我们的无监督数据集中为剩余的示例生成预测</p><p id="91dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">d.将结果(基于当前聚合方法)附加到所有3个案例的培训数据中</p><p id="6363" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">e.使用原始的70%数据+n个时期的新附加数据来训练新的模型。评估验证数据集的性能，并记录准确性和当前训练数据集的大小。</p><p id="d7eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.最后，用随机采样的数据训练一个新模型，其样本数等于迭代3中的样本数。在这个模型上做预测。这个模型在结果中被称为“随机模型”。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="a964" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">用于加速模型训练的多GPU并行化:</strong></h1><p id="5a0a" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">我们创建了一个新颖的架构，在谷歌云平台(GCP)上托管的多个虚拟机(VM)上并行化和自动化我们的tri-training。我们通过创建三个支持GPU的虚拟机以及一个存储桶(用于存储预测、模型、评估等)和两个数据存储属性来跟踪训练和结果聚合的进度(应用策略后增加的数据)，从而启动了该流程。我们的基本想法是让三个虚拟机分别独立地并行训练一个模型，一旦虚拟机完成了训练，它就将其模型和预测(在未标记的数据集上)上传到存储桶，然后在Google DataStore上将它的状态更新为“已完成”。然后，其中一个虚拟机(被指定为聚合结果)将不断轮询数据存储库(用于训练状态属性),当它发现所有虚拟机的状态都为“已完成”时，它将下载所有虚拟机的预测，对这些虚拟机运行聚合脚本(基于使用的策略),并将聚合结果上传回存储桶。此时，所有三个虚拟机下载聚合结果，将数据扩充到已标记的数据集，并将其从未标记的数据集中删除。然后重复整个过程，直到需要多少次主动学习迭代。以下是我们方法的伪代码:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es lx"><img src="../Images/18628e86de5f26420a5475fc88ea2590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*owxni7IIYkAcCj3Wi5c-Xg.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图8) GCP设置伪代码</figcaption></figure></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="0639" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">不同领域的实验:</strong></h1><h2 id="2f4b" class="ly jl hi bd jm lz ma mb jq mc md me ju iq mf mg jy iu mh mi kc iy mj mk kg ml bi translated"><strong class="ak">领域1:视觉问答:</strong></h2><p id="ffa9" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">VQA v2数据集由来自COCO数据集的82，783幅训练图像组成。每张图片有3到5个相关的问题，总计443，757个问题。对于数据集中的每个问题，有10个基本事实答案，累积到4，437，570个训练注释。验证集有40，504幅图像，214，354个问题和2，143，540个答案。</p><p id="9a51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">基线解释:</em> </strong></p><p id="1aad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用VIS+LSTM [3]模型进行可视化问答。在ImageNet 2014挑战赛上训练的19层牛津VGG Conv网[5]的最后一个隐藏层用于为我们的图像生成特征向量。对于这些问题，我们使用了[3]中的单词嵌入模型。单词嵌入用模型的其余部分来训练。图像被用作句子的第一个单词。使用线性or变换将4096维图像特征向量映射到与单词嵌入的维度相匹配的300或500维向量。这些将作为输入传递给LSTM。LSTM输出在最后一个时间步被馈送到softmax层以生成答案。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es mn"><img src="../Images/d809c06df1f73c8e6beede0fe5be5fc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/0*jE-gMrMfQhlY_2rE"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图9) VIS+LSTM模型</figcaption></figure><p id="bc78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">结果:</em> </strong></p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es mo"><img src="../Images/fbc66a0701af2f1c416c1c5def3e9776.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bxuHrq7zUSl8lQtQ"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图10) VQA v2主动学习结果。</figcaption></figure><p id="fa12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图图例:</p><blockquote class="mp mq mr"><p id="9526" class="if ig mm ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">策略1:任何两个同意</p><p id="912e" class="if ig mm ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">策略2:3个人都同意。使用基本事实</p><p id="94cd" class="if ig mm ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">随机模型:与迭代3具有相同样本数的模型，用于比较性能改进</p></blockquote><p id="3c64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">推论:</em> </strong></p><p id="934a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">策略1和2都能够在三个主动学习迭代的每一个中超过70%的基线。随着更多的数据被增加到我们的原始数据集，该模型在每次主动学习迭代结束时都会迭代地增加准确性。此外，对于两种策略，3次迭代后的模型优于随机模型。这使我们得出结论，两种三训练轮询策略都选择了重要的增强示例。</p><p id="559d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">样本证据:</em> </strong></p><p id="68f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">三元训练模型在“答案类型”=“是/否”的问题上表现良好。其中一些如下所示:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es mv"><img src="../Images/e7d0d5a5d3259e89862ccaaa58626497.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*wVuUhKHHehtoiQOVZV6eYQ.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图11)由tri-training选择的用于增强的图像和问题</figcaption></figure><p id="08ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在tri-training中不一致的大多数图像/问题对的基本事实是“答案类型”=“其他”，这意味着它不属于流行的答案类型“是/否”或“数字”。这可能是因为我们的三训练模型可能没有看到与我们的未标记数据集中的给定图像/问题对相关联的基础事实词汇。因此，不可能正确预测该数据点的标签。这种情况的一些例子如下所示</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es mw"><img src="../Images/73acdef7bdb2abb719a3340680ce25f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*ts42dipXSftyaNIDo5qUog.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图12)3个模型之间不一致的图片/问题</figcaption></figure><h2 id="075e" class="ly jl hi bd jm lz ma mb jq mc md me ju iq mf mg jy iu mh mi kc iy mj mk kg ml bi translated"><strong class="ak">领域二:问答:</strong></h2><p id="4713" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">SQuAD 2.0数据集[12]是由大约130000个问题组成的机器阅读理解数据集。该数据集在最高级别上由442篇文章构成，每篇文章都有一系列段落，每段包含一组问题。每个问题都有一个答案，该答案带有从中提取答案的上下文的索引，或者在三分之一的情况下，没有答案以使模型能够区分这两种情况。</p><p id="fe3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">基线解释:</em> </strong></p><p id="26cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于具有100%数据的基线，我们使用BiDAF模型。我们在嵌入层使用手套向量。为了更快的训练，只有单词嵌入被用于较轻的网络。RNN编码器用于建立嵌入时间步长之间的关系。双向注意力层首先获取问题和上下文之间的相似性矩阵，然后计算与隐藏状态相结合的问题2上下文和上下文2问题注意力。下一个RNN编码器层建立来自前一个注意层的表示之间的关系，最后，输出层吐出一个概率向量，该向量定量地表示答案在上下文中的特定点开始和结束的概率。跨开始和结束上下文位置的负对数似然损失用于优化。</p><p id="63cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们不能主动跟进这个领域的策略3，因为问题的答案注释必须包含从中选取上下文的上下文字符串的索引。这将需要对成千上万的问题进行人工注释，这是一项费力耗时的任务。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es mx"><img src="../Images/3f46e2ce17903a1be53952273871c2b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/0*67DjG2qkkyI0V70j"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图13) BiDAF架构</figcaption></figure><p id="25ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">结果:</em> </strong></p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es my"><img src="../Images/dcb7c7e4d37c89a0ff17439dce3d5a7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/0*7wywtyTZcvhS-SJK"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图14)班上的主动学习结果。请注意，y轴是转换为100分制的F1分数。</figcaption></figure><p id="22c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图图例:</p><blockquote class="mp mq mr"><p id="7b70" class="if ig mm ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">策略1:任何两个同意</p><p id="ade1" class="if ig mm ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">策略2:3个人都同意。使用基本事实</p><p id="f8fb" class="if ig mm ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">随机模型:与迭代3具有相同样本数的模型，用于比较性能改进</p></blockquote><p id="c808" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">推论:</em> </strong></p><p id="ac5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最适合班数据集的策略是策略2。该方法往往工作良好，因为当所有3个模型都一致时，预测往往最接近实际情况(我们发现的时间的85%)，因此它加强了与输入分布的主要部分的关系。小队数据集中有30%的未回答问题。在策略1中，对于一个给定的问题，模型很有可能在没有答案的域中过度拟合。两个模型可能在无答案问题上过度拟合，并且更多的无答案样本可能最终成为被选择的样本，这可能导致比策略2更少的F1。此外，策略2在高维输出空间的情况下效果最好。类别的数量越多，三个自信模型挑选出对训练贡献更大的强样本的可能性就越高。在结果层面上，策略2迭代3以100%的数据击败了基线，并且两个最终的第三次迭代F1分数都击败了随机模型。以下证据证实了这一点。证据样本是根据最常见的模式选择的。</p><p id="5b7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">证据:</em> </strong></p><p id="5d6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="mm">格式:</em></p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es mz"><img src="../Images/c1e246be36027d6a03d71239ef7d8d8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IUzwcurhsPcAEOiixvh8Gw.png"/></div></div></figure><p id="a62b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="mm">策略1的数据样本【2个模型一致】:</em></p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es na"><img src="../Images/18c68455df507d1d141c693f79ed1c0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lTlm25XKKshbGVLUmmXc3g.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">在所有三次迭代中</figcaption></figure><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es nb"><img src="../Images/e32bd8bbadcc79e982ff3bc1f21f4ca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ZTtdQq4BHwWTi2X0TgrDw.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">在第一次迭代后立即开发</figcaption></figure><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es nc"><img src="../Images/d9477074cd4c9b756b5053818fad27ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*04anMlc0U7OmY8tMKVm55g.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">不断出错，直到3次迭代之后</figcaption></figure><p id="3b88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="mm">策略2的QnA对[3个模型一致]: </em></p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es nd"><img src="../Images/d3280015b9e22992bdc55cd7abf5a4af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CtY3bznUlz_4QfeuLBxaMA.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">在所有三次迭代中</figcaption></figure><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ne"><img src="../Images/be1f331cf8580d7b6a7c00855bd2d1d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gQkRvLFSN8S4kVGGiE9JJg.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated"><em class="nf">在第一次迭代后立即开发</em></figcaption></figure><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es ng"><img src="../Images/fc604602561f7682d18df142e84f3f25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xe8SiigzUoicq-Hpv3CYzg.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated"><em class="nf">不断出错，直到3次迭代之后</em></figcaption></figure><p id="4119" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如上面解释和观察到的，随着数据的增加，模型发展了回答更难的问题的能力。对于策略1，即使在三次迭代之后，我们在无答案域上得到一些答案。</p><p id="3c1d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">进一步分析的范围:</em> </strong></p><p id="36c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们没有时间对该数据集进行全面研究的另外两个策略如下。由于其多对一映射，这些只能应用于小队。一个是决定一个段落中正确回答问题的最小数量。如果超过这个阈值，则该段落被附加到列车分割，否则它被丢弃。这确保了较少的数据通过拆分，并且在每次迭代之后增加阈值确保了只有最重要的问题答案对被传递给模型。另一个策略是决定轮询预测的调和等式(子串一致性)。然而，这种策略放宽了从池中挑选样本的下限。</p><h2 id="8e33" class="ly jl hi bd jm lz ma mb jq mc md me ju iq mf mg jy iu mh mi kc iy mj mk kg ml bi translated">领域3:音频分类:</h2><p id="db15" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">城市声音数据集[7]包含8732个标记的声音摘录(&lt;=4s) of urban sounds from 10 classes: air_conditioner, car_horn, children_playing, dog_bark, drilling, enginge_idling, gun_shot, jackhammer, siren, and street_music. The classes are drawn from the urban sound taxonomy. All the audio files of urban sounds are in WAV format. The sampling rate, bit depth, and number of channels are the same as those of the original file uploaded to Freesound (and hence may vary from file to file).</p><p id="5aee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">)基线解释:</em> </strong></p><p id="aed4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里使用的模型是一个扩展卷积DNN模型，它使用Librosa库来分析音频文件。我们能够实现90.15%的基线准确性，所有100%的训练数据都包含在模型的训练数据集中。用70%的数据进行三元训练的模型达到了88.67%的准确率。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es nh"><img src="../Images/e6e76b6b00f066f5af0b23180c6133df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*62imQmMdAaVjY4PH"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图15)音频数据集的架构概述</figcaption></figure><p id="9743" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">结果:</em> </strong></p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es ni"><img src="../Images/af179b25c7927e3362f888ee6ed3c12a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/0*6JQxSZpYx0ERlSbi"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图16)音频域:城市声音数据集主动学习结果。注意，Y轴代表精度，单位为%</figcaption></figure><p id="71c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图图例:</p><blockquote class="mp mq mr"><p id="6234" class="if ig mm ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">策略1:任何两个同意</p><p id="fb43" class="if ig mm ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">策略2:3个人都同意。使用基本事实</p><p id="2324" class="if ig mm ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">策略3:3个人都同意。用他们的预测作为标签</p><p id="2a87" class="if ig mm ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">随机模型:与迭代3具有相同样本数的模型，用于比较性能改进</p></blockquote><p id="7f87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">推论:</em> </strong></p><p id="47ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的结果图中我们可以看到，策略1 (Str1)和策略2 (Str2)能够超越主动学习迭代1本身的70%基线。此外，我们看到，Str1和Str2的精度随着每次主动学习迭代而进一步提高。在Str1和Str2中，经过训练的模型能够击败随机采样的数据，这些数据具有与它们各自的迭代3模型中的数据相同的样本数量，这表明了该方法的有效性。这里需要注意的一件有趣的事情是，Str1的主动学习迭代3模型甚至能够以明显更少的数据量(3.21k与4.34k样本相比)击败在100%数据集上训练的模型。这里关于结果的异常是策略3 (Str3 ),其实际表现比70%基线差，也比随机采样数据差。这种准确度的降低可归因于假阳性，其标签正被迭代地训练，由于假阳性预测的更多加强，这导致准确度的进一步下降。</p><p id="dfb2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">样本证据:</em> </strong></p><p id="3423" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">警笛声在数据集中被最正确地识别出来。然而，这可能是由于过度适应这些声音类别。大多数类似于警笛声的其他声音也被错误地预测为警笛声，比如汽车喇叭声。这是我们的策略3中准确性开始下降的原因之一，因为正确的标签被不正确的“警报”预测所取代。此外，街头音乐是误判最少的类别。可能的原因包括它与其他城市的声音非常不同。从样本中可以推断，在这种方法中，彼此接近的声音比彼此非常不同的声音表现得更差。</p><h2 id="bca5" class="ly jl hi bd jm lz ma mb jq mc md me ju iq mf mg jy iu mh mi kc iy mj mk kg ml bi translated">领域4:图像分类:</h2><p id="3270" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">我们选择CIFAR[10]数据集进行图像分类领域的实验，因为它们为研究人员所熟知，易于使用，并且是我们认为适用于任何图像分类网络的基准数据集。CIFAR-10数据集由10类60000幅32x32彩色图像组成，每类6000幅图像。有50000个训练图像和10000个测试图像。CIFAR-100有100个类，每个类包含600个图像。每个类有500个训练图像和100个测试图像。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es nj"><img src="../Images/33d921791403725793aab7735f22afaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*FVz5_HCzvuNDqSQx6ibPfw.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图17) VGG-16体系结构</figcaption></figure><p id="5856" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">基线解释:</em> </strong></p><p id="17c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们对两个影像分类数据集都使用了经典的卷积神经网络模型— VGG16。VGG16[11]是提交给ILSVRC 2014的更受欢迎的模型之一，与当时流行的AlexNet相比有相当大的改进。我们选择VGG16的主要动机是它易于实现，性能可靠，并且可以针对两个数据集进行优化。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es nk"><img src="../Images/7149089e4dd9cc70aab4721a8462baa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*v3_xugVZXPeiBISyhtmI7Q.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图18)策略证据</figcaption></figure><p id="a219" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">策略证据:</em> </strong></p><p id="9640" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">左上角的船只图像(来自CIFAR-10)就是这样一幅图像，在我们进行的所有实验中，这三个模型都正确地对它进行了分类。这些图像有助于提高(或保持)每次迭代的性能，因为它们不会成为后续迭代的错误示例。另一方面，一只狗的顶部中心图像(来自CIFAR-10)是一个通常仅由两个模型正确预测的示例，因此当使用策略1时，类似这样的示例有助于提高整体模型性能。最后，右上角的青蛙图像通常会被所有模型或其中两个模型错误分类，因此永远无法成功地扩增回标记数据集。</p><p id="cce0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">T5结果 : </strong></p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es nl"><img src="../Images/f809662bc262d12d8266e2240fda6211.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*J6yviCBRtybgVs1e"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图19) CIFAR-10结果</figcaption></figure><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="er es nm"><img src="../Images/256f4c5172d71fb95693f4b2a7aa7cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ALUkmKu71y4C_IoB"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">(图20) CIFAR-100结果</figcaption></figure><p id="b0fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上图表的图例:</p><blockquote class="mp mq mr"><p id="3368" class="if ig mm ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">策略1:任何两个同意</p><p id="542f" class="if ig mm ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">策略2:3个人都同意。使用基本事实</p><p id="cbdf" class="if ig mm ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">策略3:3个人都同意。用他们的预测作为标签</p><p id="6ced" class="if ig mm ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">随机模型:与迭代3具有相同样本数的模型，用于比较性能改进</p></blockquote><p id="6cc6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mm">推论</em> : </strong></p><p id="2cde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于CIFAR-10和CIFAR-100，在策略1中，随着大量(正确标记的数据)的增加，每次迭代后准确度都会增加。这种策略也优于70%基线和随机基线，因为这种策略有助于选择最有帮助的数据，这些数据可以被扩充回来。对于策略2，我们也得到了相似的结果。而对于策略3，精确度在每次迭代中都会增加，但是它不能超过随机基线。原因是，在这种策略中，有可能增加假阳性。所有3个模型对图像进行错误分类的例子会用错误的标签进行增强，从而降低整体性能。CIFAR-10的总赢家是策略2，因为这三个模型一开始就有不错的性能，因此大多数示例都被正确或错误地分类在一起。这可以从策略2中增加的数据量比策略1中多得多看出。对于CIFAR-100，策略1是总体赢家，因为CIFAR-100是更稀疏的数据集(更多的类)，因此从一开始，每个类的模型看到的示例更少，似乎分歧更大。这导致策略1的示例增加了更多，从而极大地提高了性能。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="17cd" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">结论:</strong></h1><p id="43c3" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">由于假阳性(tri训练错误预测的无监督数据),增加tri训练生成的预测偶尔会降低迭代的准确性。用真实数据替换模型生成的预测有助于我们消除这些假阳性，并迭代地提高基线准确性。对于SQuAD数据集和urban sounds数据集，我们的模型在3次迭代后击败了oracle。在几乎所有的领域中，经过3次迭代，我们击败了用相同数量的随机采样数据训练的模型。这使我们得出结论，tri训练从无监督的池中识别重要的例子。因此，半监督的tri训练为主动学习过程提供了良好的基础。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="2fea" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">参考文献:</strong></h1><p id="c53e" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">[1] Tri-Training:使用三个分类器利用未标记数据【http://citeseerx.ist.psu.edu/viewdoc/download? T2】doi = 10 . 1 . 1 . 487 . 2431&amp;rep = re P1&amp;type = pdf</p><p id="7396" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]齐藤，k .，牛池，y .，&amp;原田，T. (2017)。<em class="mm">用于无监督领域适应的非对称三训练</em>。在2017年的ICML。从http://arxiv.org/abs/1702.08400<a class="ae nn" href="http://arxiv.org/abs/1702.08400" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="29d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3]m .任、r .基罗斯和r .泽梅尔(2015年)。<em class="mm">探索图像问答的模型和数据</em>。从https://arxiv.org/pdf/1505.02074.pdf<a class="ae nn" href="https://arxiv.org/pdf/1505.02074.pdf" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="7cd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[4]可视化问答数据集。从https://visualqa.org/download.html<a class="ae nn" href="https://visualqa.org/download.html" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="b35a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[5] K. Simonyan和A. Zisserman，<em class="mm">用于大规模图像识别的极深卷积网络</em>，ICLR，2015。</p><p id="c7c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[6]鲁德，s .，&amp;普兰克，B. (2018)。<em class="mm">域转移下神经半监督学习的强基线</em>。<a class="ae nn" href="https://ruder.io/semi-supervised/index.html#fnref20" rel="noopener ugc nofollow" target="_blank">在ACL 2018的会议录中</a></p><p id="2bf9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[7] J. Salamon、C. Jacoby和J. P. Bello，“城市声音研究的数据集和分类法”，第22届ACM多媒体国际会议，美国奥兰多，2014年11月。</p><p id="e71b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[8]机器学习理解的双向注意力流<a class="ae nn" href="https://arxiv.org/pdf/1611.01603.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1611.01603.pdf</a></p><p id="23ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[9]林，x .，&amp;帕里克，D. (2017)。视觉问答的主动学习:一项实证研究。从https://arxiv.org/pdf/1711.01732.pdf<a class="ae nn" href="https://arxiv.org/pdf/1711.01732.pdf" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="c759" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[10] Alex Krizhevsky (2009)，从微小图像中学习多层特征</p><p id="faca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">检索自<a class="ae nn" href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cs . Toronto . edu/~ kriz/learning-features-2009-tr . pdf</a></p><p id="87d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[11]卡伦·西蒙扬和安德鲁·齐泽曼(2014)，用于大规模图像识别的非常深的卷积网络。从https://arxiv.org/abs/1409.1556<a class="ae nn" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="3b5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[12]小队数据集2.0，检索自:<a class="ae nn" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank">https://rajpurkar.github.io/SQuAD-explorer/</a></p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="44ab" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">团队:</strong></h1><p id="a6e9" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">IVA团队成员:</p><ol class=""><li id="0642" class="lg lh hi ih b ii ij im in iq li iu lj iy lk jc ll lm ln lo bi translated">阿尤什·沙阿</li><li id="ee0f" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">阿南德·戈库尔·马哈林加姆</li><li id="3ee6" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">阿克谢·古拉提</li><li id="9a62" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">罗伊斯顿·玛丽安·马斯卡雷尼亚斯</li><li id="49a3" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">拉克什塔·宾童龙</li></ol><p id="c6b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这项工作是作为2019年秋季南加州大学CSCI-566级“深度学习及其应用”的课程项目完成的，由Joseph Lim教授负责。</p></div></div>    
</body>
</html>