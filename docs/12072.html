<html>
<head>
<title>Data Classification Using K-Nearest Neighbors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用K近邻的数据分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/data-classification-using-k-nearest-neighbors-58ee1c0bea17?source=collection_archive---------7-----------------------#2020-12-30">https://medium.com/analytics-vidhya/data-classification-using-k-nearest-neighbors-58ee1c0bea17?source=collection_archive---------7-----------------------#2020-12-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8c9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分类是数据科学中最基本的概念之一。这是一种机器学习方法，通过该方法，使用预测建模来预测数据输入的类别标签。分类算法是预测性计算，用于通过分析训练数据集将数据分配到预设类别。</p><p id="1f7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">适当的数据分类允许人们基于预定的类别数据来应用适当的控制。数据分类可以节省时间和资源，因为人们可以专注于重要的事情，而不是浪费时间进行不必要的控制。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/8dd2ef2de2b971e8230d73292657a506.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/0*MSh-eze7y2N0R6w9"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">来源:<a class="ae jp" href="https://www.javatpoint.com/classification-algorithm-in-machine-learning" rel="noopener ugc nofollow" target="_blank">https://www . Java point . com/classification-algorithm-in-machine-learning</a></figcaption></figure><p id="b4dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">机器学习中有<a class="ae jp" href="https://machinelearningmastery.com/types-of-classification-in-machine-learning/" rel="noopener ugc nofollow" target="_blank"> 4种主要的分类类型</a>:</p><ol class=""><li id="7704" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">二元分类:两个类别标签；提供是或否的答案，例如:识别垃圾邮件</li><li id="5808" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">多类别分类:两个以上的类别标签——例如:面部或植物种类的分类</li><li id="d08a" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">多标签分类:预测一个或多个标签的两个或多个类别标签，例如:照片分类</li><li id="7026" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">不平衡的分类:不平等的分布—例如:欺诈检测或医疗诊断</li></ol><p id="b912" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据分类中常用的几种算法包括:</p><ul class=""><li id="0e93" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc ke jw jx jy bi translated">k-最近邻</li><li id="57d2" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc ke jw jx jy bi translated">逻辑回归</li><li id="9ca7" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc ke jw jx jy bi translated">人工神经网络/深度学习</li><li id="8292" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc ke jw jx jy bi translated">支持向量机</li></ul><p id="bdfb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将讨论分类中使用的最基本和最著名的机器学习算法之一:<strong class="ih hj">K-最近邻算法(KNN) </strong>。</p><h2 id="c3f0" class="kf kg hi bd kh ki kj kk kl km kn ko kp iq kq kr ks iu kt ku kv iy kw kx ky kz bi translated"><strong class="ak">K-最近邻分类器</strong></h2><p id="694c" class="pw-post-body-paragraph if ig hi ih b ii la ik il im lb io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">KNN根据先前存储的数据点的相似性度量对新数据点进行分类。该算法查找查询和数据中所有示例之间的距离，选择最接近查询的指定数量的示例(K)，然后投票选择最频繁的标签(用于分类)或平均标签(用于回归)。</p><p id="bc2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该算法因其计算时间短和非参数性质(意味着它不对数据做任何假设)而被业内许多人使用。在<em class="lf">概念搜索</em>中可以看到KNN的一个常见应用。例如，这些软件包用来帮助公司在电子邮件收件箱中定位类似的文件。另一个例子是<em class="lf">推荐系统</em>，其中一种算法可以根据个人之前的购买或参与情况为其推荐产品、媒体或广告。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lg"><img src="../Images/dd2a7e429218b5709ddb66fe17bcb919.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*2S4-xzWxjpQbNNPvNrWd1Q.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><a class="ae jp" href="https://www.edureka.co/blog/k-nearest-neighbors-algorithm/" rel="noopener ugc nofollow" target="_blank">https://www.edureka.co/blog/k-nearest-neighbors-algorithm/</a></figcaption></figure><p id="52c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然KNN算法可能相对容易使用和训练，但KNN分类器的准确性将取决于数据的质量和所选的特定K值。</p><p id="5fcc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实施KNN分类器时，数据科学家还必须决定要考虑多少个邻域。换句话说，我们需要考虑邻居的最佳数量以及它如何影响我们的分类器。最佳K值(考虑的邻域数)将影响预测模型。不同的数据集有不同的要求。根据<a class="ae jp" href="https://www.datacamp.com/community/tutorials/k-nearest-neighbor-classification-scikit-learn" rel="noopener ugc nofollow" target="_blank">datacamp.com</a>的说法，在邻居数量较少的情况下，噪声会对结果产生较高的影响，大量的邻居使其计算量很大。研究还表明，少量的邻居是最灵活的拟合，其将具有低偏差但高方差，而大量的邻居将具有更平滑的决策边界，这意味着较低的方差但较高的偏差。</p><h2 id="3222" class="kf kg hi bd kh ki kj kk kl km kn ko kp iq kq kr ks iu kt ku kv iy kw kx ky kz bi translated">何时以及为什么使用KNN？</h2><p id="a30d" class="pw-post-body-paragraph if ig hi ih b ii la ik il im lb io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">当数据集被标注、无噪声且相对较小时，KNN最适用于数据集。给定训练集中数据点的分类，该算法可以基于该信息对未来的未知数据进行分类。此外，具有过多特征的数据集对数据点的分类没有帮助，可能会导致算法错过数据中的模式。数据集中的这些噪声可能包括与数据集的其余部分不相关的无关数据点以及无助于识别分类的要素。因为KNN算法是基于实例的，这意味着不需要明确的训练步骤，所以与其他方法相比，训练阶段相对较快。因此，对于具有同质要素以及少量异常值和缺失值的数据集，KNN分类器可以证明是一个准确的分类器。</p><h2 id="2da2" class="kf kg hi bd kh ki kj kk kl km kn ko kp iq kq kr ks iu kt ku kv iy kw kx ky kz bi translated"><strong class="ak">KNN背后的数学</strong></h2><p id="c503" class="pw-post-body-paragraph if ig hi ih b ii la ik il im lb io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">将数据集中的数据点转换为数学成分后，KNN算法会计算不同点之间的距离来建立关联。找到这个距离的一个常用方法是使用两点之间的<strong class="ih hj">欧几里德距离</strong>。</p><p id="0f9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在二维场中，点和距离可以计算如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lh"><img src="../Images/078789cee5b9ec4091dcccd539c67395.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/0*DcDp7HIx1IfTA9gV"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><a class="ae jp" href="https://mccormickml.com/2013/08/15/the-gaussian-kernel/" rel="noopener ugc nofollow" target="_blank">https://mccormickml.com/2013/08/15/the-gaussian-kernel/</a></figcaption></figure><p id="3f5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个原理可以应用于“多维”空间，其中每个维度代表数据点的不同特征。下面，三维空间显示了计算的点之间的欧几里德距离。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es li"><img src="../Images/fba293c5d5e347477a705e6de7024a5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/0*dKmIfdnprzaMPozq"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><a class="ae jp" href="https://hlab.stanford.edu/brian/euclidean_distance_in.html" rel="noopener ugc nofollow" target="_blank">https://hlab.stanford.edu/brian/euclidean_distance_in.html</a></figcaption></figure><p id="8f47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当空间由n维组成时，可以使用以下公式来计算欧几里德距离:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lj"><img src="../Images/32dd4f9dca0131efe33ce5f6b511b9a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/0*tEFTDPzHkGEY5tpj"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">https://en.wikipedia.org/wiki/Euclidean_distance<a class="ae jp" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><p id="abdd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些欧几里德距离计算产生最佳分类结果的维数是有限的。随着维度数量的增加，结果可能会被稀释且无用，因此必须小心测试数据集以检查其有效性。</p><h2 id="c07d" class="kf kg hi bd kh ki kj kk kl km kn ko kp iq kq kr ks iu kt ku kv iy kw kx ky kz bi translated"><strong class="ak">用Python计算欧几里德距离</strong></h2><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="7f8a" class="kf kg hi ll b fi lp lq l lr ls">#import the library<br/>from scipy.spacial import distance</span><span id="2a86" class="kf kg hi ll b fi lt lq l lr ls">#initialize points<br/>p1 = (10, 15, 20)<br/>p2 = (25, 30, 35)</span><span id="e2a6" class="kf kg hi ll b fi lt lq l lr ls">#calculate euclidean distance<br/>euclidean_distance = distance.euclidean(p1, p2)<br/>print('The Euclidean distance b/w', p1, 'and', p2, 'is: ', euclidean_distance)</span></pre><h2 id="1b9f" class="kf kg hi bd kh ki kj kk kl km kn ko kp iq kq kr ks iu kt ku kv iy kw kx ky kz bi translated"><strong class="ak">使用K近邻(KNN)的简单示例—虹膜数据</strong></h2><p id="da71" class="pw-post-body-paragraph if ig hi ih b ii la ik il im lb io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">下面的例子将利用鸢尾花数据集的数据，通常被称为Fisher的鸢尾数据集，我从<a class="ae jp" href="https://archive.ics.uci.edu/ml/datasets/iris" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库</a>访问它。这个多变量数据集包含了对三种不同鸢尾的50个样本的测量:刚毛鸢尾、海滨鸢尾和杂色鸢尾。每个样本包括四个以厘米为单位的特征:萼片和花瓣的长度和宽度。该集合总共包含150个记录，具有5个特征(4个测量值和分类/物种)。</p><p id="f02a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我从导入整个程序中使用的库开始。<a class="ae jp" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank">sk learn . model _ selection</a>库是一个流行的机器学习库，我将用它来进行分类。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="bdb0" class="kf kg hi ll b fi lp lq l lr ls">import numpy as np <br/>import pandas as pd <br/>import matplotlib.pyplot as plt <br/>from sklearn.model_selection import train_test_split</span></pre><p id="adf1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在加载和读取数据后，我们可以预览数据集。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="1eb7" class="kf kg hi ll b fi lp lq l lr ls">#read csv file into a data frame<br/>iris = pd.read_csv('iris.csv')</span><span id="a341" class="kf kg hi ll b fi lt lq l lr ls">#display initial rows of data frame <br/>iris.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/15919d2131201e1d0656004e2a490216.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*NArex_cjaAVw3ZK21y2-Ag.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">输出:的。head()函数允许我们查看数据帧的前5行</figcaption></figure><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="cf43" class="kf kg hi ll b fi lp lq l lr ls">iris_species = {'setosa': 0, 'versicolor': 1, 'virginica': 2}</span><span id="2877" class="kf kg hi ll b fi lt lq l lr ls">iris['species_num'] = iris.apply(lambda x: iris_species[x.species], axis=1) # axis=1 - applies to each row</span></pre><p id="aa2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我在“物种”列中列举了物种的名称，以便根据上面的数组将名称转换成数值。这将用于创建从iris标签值到物种名称的映射，以使结果更容易解释。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="ea37" class="kf kg hi ll b fi lp lq l lr ls">iris_number = dict(zip(iris.species_num.unique(), iris.species.unique()))</span></pre><p id="3fd5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是映射:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/c2f3f1e81a30e18c211379d82c8d8cb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*gU3b4pn2vxeHzJ-m_kDfZg.png"/></div></figure><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="167e" class="kf kg hi ll b fi lp lq l lr ls">X = iris[['sepal_length', 'sepal_width', 'petal_length']]<br/>y = iris['species_num']</span><span id="4f06" class="kf kg hi ll b fi lt lq l lr ls"># default is 75% / 25% train-test split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span></pre><p id="4873" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的代码段中，用于训练的每朵花的萼片长度、萼片宽度和花瓣长度特征。然后，数据集被分成测试子集和训练子集。</p><p id="ea74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面，使用之前分离的训练集，我绘制了一个3D散点图来可视化特征之间的关系。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="f612" class="kf kg hi ll b fi lp lq l lr ls"># plotting a 3D scatter plot<br/>from mpl_toolkits.mplot3d import Axes3D</span><span id="e132" class="kf kg hi ll b fi lt lq l lr ls">from matplotlib import cm</span><span id="581c" class="kf kg hi ll b fi lt lq l lr ls">fig = plt.figure()<br/>ax = fig.add_subplot(111, projection = '3d')<br/>ax.scatter(X_train['sepal_length'], X_train['sepal_width'], X_train['petal_length'], c = y_train, marker = 'o', s=100)<br/>ax.set_xlabel('sepal_length')<br/>ax.set_ylabel('sepal_width')<br/>ax.set_zlabel('petal_length')<br/>plt.show()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/24b331af5573c36641b81f1c41605b4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*Lhl_2EOl6JpPmTd72mA1Wg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">输出:黄色、蓝色和紫色的簇代表不同种类的鸢尾。</figcaption></figure><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="7faa" class="kf kg hi ll b fi lp lq l lr ls">from sklearn.neighbors import KNeighborsClassifier</span><span id="a1e6" class="kf kg hi ll b fi lt lq l lr ls">knn = KNeighborsClassifier(n_neighbors = 5)</span></pre><p id="d931" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">创建分类器对象后，我定义了K值，即要考虑的邻居数量。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="f963" class="kf kg hi ll b fi lp lq l lr ls">knn.fit(X_train, y_train)</span></pre><p id="3929" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用训练数据，训练分类器以适合估计器。</p><p id="ae5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们可以使用测试数据来评估所开发的分类器的准确性。</p><pre class="je jf jg jh fd lk ll lm ln aw lo bi"><span id="e1a3" class="kf kg hi ll b fi lp lq l lr ls">knn.score(X_test, y_test)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/a6497c5df0cdc0c46fb7012f2e21025d.png" data-original-src="https://miro.medium.com/v2/resize:fit:890/format:webp/1*oMJD6IWN01EbODZsuh_M1w.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">输出:KNN分类器的精确度通常在0-1的范围内测量。</figcaption></figure><p id="da06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上所述，KNN算法可以相对容易地准确分类数据集的数据点。对于具有少量异常值的合理大小的数据集，该方法是一种可靠的算法，与其他模型相比，它可以在相对较短的时间内产生结果。</p></div></div>    
</body>
</html>