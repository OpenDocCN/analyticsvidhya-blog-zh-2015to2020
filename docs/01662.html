<html>
<head>
<title>A Dead Simple Example of Fine-tuning BERT with AllenNLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用AllenNLP微调BERT的简单例子</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fine-tuning-bert-with-allennlp-7459119b736c?source=collection_archive---------2-----------------------#2019-11-07">https://medium.com/analytics-vidhya/fine-tuning-bert-with-allennlp-7459119b736c?source=collection_archive---------2-----------------------#2019-11-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="51f6" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">以SNLI数据集为例，简单演示如何在AllenNLP库中微调BERT。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/a790aa53d8d7e332a1b73317238e336e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HP8QshOfUo_oDCCPFl87GQ.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">芝麻街的伯特的强制性形象</figcaption></figure><p id="0a1a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi kj translated">听说了这么多关于<a class="ae ks" href="https://allennlp.org/" rel="noopener ugc nofollow" target="_blank">allenlp</a>图书馆的事情后，我终于腾出一些时间来熟悉它。因为我的大部分工作(最近)涉及微调预先训练的语言模型，所以我决定学习这个库，尝试用它来微调一个BERT模型。</p><p id="bde0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">有些随意地，我选择使用<a class="ae ks" href="https://nlp.stanford.edu/projects/snli/" rel="noopener ugc nofollow" target="_blank">斯坦福自然语言推理(SNLI)数据集</a>来微调<a class="ae ks" href="https://paperswithcode.com/task/natural-language-inference" rel="noopener ugc nofollow" target="_blank"> <strong class="jp hj">自然语言推理</strong> </a>的任务。然而，我注意到用于分类的内置BERT模型(<a class="ae ks" href="https://allenai.github.io/allennlp-docs/api/allennlp.models.bert_for_classification.html" rel="noopener ugc nofollow" target="_blank">allennlp . models . BERT _ for _ classification</a>)不能直接与用于SNLI的内置数据集读取器(<a class="ae ks" href="https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset_readers.snli.html" rel="noopener ugc nofollow" target="_blank">allennlp . data . dataset _ readers . SNLI</a>)一起工作。这提供了一个完美的学习机会，我决定在这篇短文中解释一下。</p><blockquote class="kt ku kv"><p id="e230" class="jn jo kw jp b jq jr ij js jt ju im jv kx jx jy jz ky kb kc kd kz kf kg kh ki hb bi translated">请注意，本教程并不是对AllenNLP的介绍。为此，我推荐AllenNLP官方演示<a class="ae ks" href="https://allennlp.org/tutorials" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></blockquote></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h1 id="b6ea" class="lh li hi bd lj lk ll lm ln lo lp lq lr io ls ip lt ir lu is lv iu lw iv lx ly bi translated">✅入门</h1><p id="1fca" class="pw-post-body-paragraph jn jo hi jp b jq lz ij js jt ma im jv jw mb jy jz ka mc kc kd ke md kg kh ki hb bi translated">本教程唯一的依赖项是AllenNLP，它可以和pip一起安装。首先确保你有一个干净的Python 3.6或者3.7虚拟环境，然后用pip安装。举个例子，</p><pre class="iy iz ja jb fd me mf mg mh aw mi bi"><span id="2b5d" class="mj li hi mf b fi mk ml l mm mn"># Assuming conda is installed<br/>conda create -n bert_snli python=3.7 -y<br/>source activate bert_snli</span><span id="2edb" class="mj li hi mf b fi mo ml l mm mn">pip install allennlp</span></pre><p id="7479" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如果你计划在本地训练模型<strong class="jp hj">，你会想要一个<a class="ae ks" href="https://developer.nvidia.com/cuda-zone" rel="noopener ugc nofollow" target="_blank">支持CUDA </a>的GPU。</strong></p><p id="4c88" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">最后，你需要从<a class="ae ks" href="https://nlp.stanford.edu/projects/snli/" rel="noopener ugc nofollow" target="_blank">这里</a>下载SNLI数据集。请记住您保存此的位置，因为我们稍后将需要此路径。</p><h2 id="582b" class="mj li hi bd lj mp mq mr ln ms mt mu lr jw mv mw lt ka mx my lv ke mz na lx nb bi translated">使用Colab</h2><p id="5c1f" class="pw-post-body-paragraph jn jo hi jp b jq lz ij js jt ma im jv jw mb jy jz ka mc kc kd ke md kg kh ki hb bi translated">如果你愿意的话，我也可以在<strong class="jp hj"> Google Colab </strong>中找到这个教程。你可以跟着它走，不需要在本地安装任何东西。</p><h1 id="15bc" class="lh li hi bd lj lk nc lm ln lo nd lq lr io ne ip lt ir nf is lv iu ng iv lx ly bi translated">🔧进行必要的更改</h1><p id="ac93" class="pw-post-body-paragraph jn jo hi jp b jq lz ij js jt ma im jv jw mb jy jz ka mc kc kd ke md kg kh ki hb bi translated">为了在SNLI数据集上训练BERT，我们唯一需要修改的是<a class="ae ks" href="https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset_readers.snli.html" rel="noopener ugc nofollow" target="_blank"> AllenNLP SNLI数据集读取器</a>。</p><p id="6ef9" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这是因为，对于像NLI这样的任务，输入示例是成对的序列，BERT期望下面的输入结构(借用来自遥远星系的<a class="ae ks" href="https://www.techly.com.au/2016/03/31/darth-vader-never-said-luke-i-am-your-father-in-star-wars/" rel="noopener ugc nofollow" target="_blank">著名台词</a>作为示例):</p><pre class="iy iz ja jb fd me mf mg mh aw mi bi"><span id="e280" class="mj li hi mf b fi mk ml l mm mn">[CLS] I am your father . [SEP] No . No ! That ’ s not true ! [SEP]</span></pre><p id="7e1d" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">具体来说，我们需要用特殊的[SEP]标记连接我们的两个句子(在本例中是我们的<em class="kw">前提</em>和我们的<em class="kw">假设</em>)。为此，我们将以下代码添加到AllenNLP SnliReader中(就在这里的<a class="ae ks" href="https://github.com/allenai/allennlp/blob/master/allennlp/data/dataset_readers/snli.py#L76-L78" rel="noopener ugc nofollow" target="_blank">周围</a>):</p><pre class="iy iz ja jb fd me mf mg mh aw mi bi"><span id="4d26" class="mj li hi mf b fi mk ml l mm mn"><em class="kw"># Join the premise and the hypothesis.<br/># Because we will use </em>bert-pretrained as token indexer, <br/># both the premise and hypothesis already contain <br/># [CLS] and [SEP] tokens. We simply remove the [CLS]<br/># token from sequence 2 (hypothesis) and concatenate it with<br/># sequence 1 (premise)<em class="kw"><br/></em>tokens = premise_tokens + hypothesis_tokens[1:]<br/>fields["tokens"] = TextField(tokens, self._token_indexers)</span></pre><p id="8b63" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在这个<a class="ae ks" href="https://gist.github.com/JohnGiorgi/6930320f36f21cce501514a689fbb907" rel="noopener ugc nofollow" target="_blank">要点</a>中为你做了什么:</p><blockquote class="kt ku kv"><p id="6239" class="jn jo kw jp b jq jr ij js jt ju im jv kx jx jy jz ky kb kc kd kz kf kg kh ki hb bi translated">Medium渲染Gist中的所有文件，这里我们指的是第一个渲染的文件(bert_snli.py)</p></blockquote><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nh ni l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">用于BERT的改进的SnliReader</figcaption></figure><h1 id="30c3" class="lh li hi bd lj lk nc lm ln lo nd lq lr io ne ip lt ir nf is lv iu ng iv lx ly bi translated">⚙️正在写配置</h1><p id="706e" class="pw-post-body-paragraph jn jo hi jp b jq lz ij js jt ma im jv jw mb jy jz ka mc kc kd ke md kg kh ki hb bi translated">最后但同样重要的是，我们需要编写一个<a class="ae ks" href="https://jsonnet.org/" rel="noopener ugc nofollow" target="_blank"> jsonnet </a> config来使用我们的定制数据集加载器，定义我们的模型并设置训练。事实证明这非常简单:</p><blockquote class="kt ku kv"><p id="db8a" class="jn jo kw jp b jq jr ij js jt ju im jv kx jx jy jz ky kb kc kd kz kf kg kh ki hb bi translated">Medium渲染Gist中的所有文件，这里，我们指的是第二个渲染文件(train.jsonnet)</p></blockquote><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nh ni l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">用于定义模型、数据集读取器和训练制度的配置文件</figcaption></figure><p id="52b2" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">注意，我们将自己的子类BertSnliReader作为“type”提供给“dataset_reader”。</p><h1 id="33d1" class="lh li hi bd lj lk nc lm ln lo nd lq lr io ne ip lt ir nf is lv iu ng iv lx ly bi translated">🚀训练模型</h1><p id="6d59" class="pw-post-body-paragraph jn jo hi jp b jq lz ij js jt ma im jv jw mb jy jz ka mc kc kd ke md kg kh ki hb bi translated">最后，我们准备训练模型。首先下载train.jsonnet和bert_snli.py文件(见<a class="ae ks" href="https://gist.github.com/JohnGiorgi/6930320f36f21cce501514a689fbb907" rel="noopener ugc nofollow" target="_blank">要诀</a>)。然后，将“train_data_path”和“validation_data_path”更改为SNLI数据集的本地副本的路径。</p><p id="c3d0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">激活虚拟环境后，我们可以启动如下培训课程:</p><pre class="iy iz ja jb fd me mf mg mh aw mi bi"><span id="04f5" class="mj li hi mf b fi mk ml l mm mn">allennlp train train.jsonnet --include-package bert_snli -s ./tmp -f</span></pre><blockquote class="kt ku kv"><p id="2822" class="jn jo kw jp b jq jr ij js jt ju im jv kx jx jy jz ky kb kc kd kz kf kg kh ki hb bi translated">请注意，这将把序列化模型保存在。/tmp，如有必要，覆盖此目录。</p></blockquote><p id="54f4" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">加载数据集后，您应该会看到训练进度输出，例如:</p><pre class="iy iz ja jb fd me mf mg mh aw mi bi"><span id="9f9d" class="mj li hi mf b fi mk ml l mm mn">accuracy: 0.7972, loss: 0.5115 ||:  10%|#         | 1761/17168 [18:18&lt;2:31:52,  1.69it/s]</span></pre></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><h1 id="d2fe" class="lh li hi bd lj lk ll lm ln lo lp lq lr io ls ip lt ir lu is lv iu lw iv lx ly bi translated">♻️总结</h1><p id="2b2c" class="pw-post-body-paragraph jn jo hi jp b jq lz ij js jt ma im jv jw mb jy jz ka mc kc kd ke md kg kh ki hb bi translated">就是这样！用AllenNLP微调BERT轻而易举。对于以句子对作为输入的任务，我们只需要修改现有的数据集读取器，用BERTs特殊的[SEP]标记连接句子。</p><p id="30c0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">有很多事情可以改善这个设置，比如</p><ul class=""><li id="0db2" class="nj nk hi jp b jq jr jt ju jw nl ka nm ke nn ki no np nq nr bi translated">从一个URL下载SNLI数据集，并将其缓存在磁盘上(我相信这可以用AllenNLP实现，但我无法弄清楚)。</li><li id="1c4c" class="nj nk hi jp b jq ns jt nt jw nu ka nv ke nw ki no np nq nr bi translated">添加一个学习率调度器，类似于在<a class="ae ks" href="https://www.aclweb.org/anthology/N19-1423/" rel="noopener ugc nofollow" target="_blank">原始BERT实现</a>中使用的那个。</li><li id="a89e" class="nj nk hi jp b jq ns jt nt jw nu ka nv ke nw ki no np nq nr bi translated">编写一个<a class="ae ks" href="https://allenai.github.io/allennlp-docs/api/allennlp.predictors.html" rel="noopener ugc nofollow" target="_blank">预测器</a>，并提供一个经过训练的模型作为一个简单的网络服务。</li></ul><p id="74cc" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如果你对本教程有任何疑问，请在这里留下你的评论🙌</p></div></div>    
</body>
</html>