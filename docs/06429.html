<html>
<head>
<title>Knowledge Distillation for Object Detection 1: Start from simple classification model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于对象检测的知识蒸馏1:从简单分类模型开始</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/knowledge-distillation-for-object-detection-1-start-from-simple-classification-model-921e1b2bfed2?source=collection_archive---------6-----------------------#2020-05-22">https://medium.com/analytics-vidhya/knowledge-distillation-for-object-detection-1-start-from-simple-classification-model-921e1b2bfed2?source=collection_archive---------6-----------------------#2020-05-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/8f7ab5bd093dae0916bddacd4ae72c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/0*2JoRrcnNFnkp3r1z"/></div><figcaption class="im in et er es io ip bd b be z dx translated">知识蒸馏的概念</figcaption></figure><h1 id="7f7d" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">动机</h1><p id="8fdf" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">知识蒸馏(KD)是一种通过转移由大型网络(教师)产生的提炼知识来提高小型网络(学生)的准确性的技术。我们也可以说KD是为了压缩模型(教师→学生)而损失最小的精度。</p><p id="b814" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">本系列的最终目标是将这项技术应用于我的轻量级对象检测模型。在这个故事中，作为第一步，我将实现一个简单的分类模型，并测试KD的能力。</p><p id="5ad9" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">关于这个话题的参考论文是这样的:<a class="ae kr" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank"> Hinton等人。艾尔。“在神经网络中提取知识”。NIPS2014 </a>。([1])他们介绍了知识提取的基本概念，并表明应用它可以提高分类精度。我将重新实现本文中的实验，并检查它实际上是否工作良好。</p><h1 id="61af" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">为分类器提取知识:软目标</h1><p id="0faa" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在这篇论文中，他们展示了关于“如何概括”的知识可以通过<strong class="jq hj">软目标</strong>从教师转移到学生。与传统的<strong class="jq hj">硬目标</strong>相比，软目标对所有阶层都有评分。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es ks"><img src="../Images/3b9b4c12149966e53697b7d498982816.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BTPLzy3hy1ihQRo0"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated"><strong class="bd is">*参考:</strong><a class="ae kr" href="https://www.ttic.edu/dl/dark14.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="bd is">https://www.ttic.edu/dl/dark14.pdf</strong></a><strong class="bd is">(【3】)</strong></figcaption></figure><h1 id="a863" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">软目标损失</h1><p id="1642" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">为了训练具有软目标的模型，他们用新的softmax公式修改了传统的损失函数。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/7bc7807150a7ed122c06deaa5c40ab82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*c78n2MfQi_xPg-YBkI9aJA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">Softmax随温度变化</figcaption></figure><p id="fa23" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在更高的温度(T)设置中，课堂分数变化更平稳，并且将转移更大量的知识。在本文中，他们选择20作为温度值。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es lc"><img src="../Images/99865a76bb41c41fed7118b782171e74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KEq2Sncr4q5veZ4r"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">用于二进制分类的Softmax，(红色:T=1，蓝色:T=20)</figcaption></figure><p id="9634" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">为了训练有软标签和硬标签学生，他们使用了两个损失的加权和。第一个损失是预测和给定硬标签之间的常规交叉熵损失。第二个损失是预测和给定软标签之间的交叉熵损失，softmax的温度较高。第一次损失的权重是1，第二次损失的权重是T(文中解释了原因)。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es ld"><img src="../Images/ba92d9f552a7eb42db0b44d71402267d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qkoyQsFpIDNygnoM.png"/></div></div></figure></div><div class="ab cl le lf gp lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="hb hc hd he hf"><h1 id="2e3a" class="iq ir hi bd is it ll iv iw ix lm iz ja jb ln jd je jf lo jh ji jj lp jl jm jn bi translated">实验:真的提高了准确率吗？</h1><p id="239c" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">这是这个故事的主要部分。我写了代码来复制论文[1]的结果。你可以找到这个实验的完整代码:<a class="ae kr" href="https://github.com/poperson1205/knowledge_distillation" rel="noopener ugc nofollow" target="_blank">https://github.com/poperson1205/knowledge_distillation</a></p><h2 id="cb45" class="lq ir hi bd is lr ls lt iw lu lv lw ja jz lx ly je kd lz ma ji kh mb mc jm md bi translated">1.实现一个大(老师)和一个小(学生)网络进行分类</h2><ul class=""><li id="e798" class="me mf hi jq b jr js jv jw jz mg kd mh kh mi kl mj mk ml mm bi translated">教师:<br/>784→ReLU→1200→ReLU→1200→10<br/>(丢失20%的输入，两个隐层输出的80%)</li><li id="a2f6" class="me mf hi jq b jr mn jv mo jz mp kd mq kh mr kl mj mk ml mm bi translated">学生:<br/> 784 →热卢→ 800 →热卢→ 800 → 10</li></ul><h2 id="b4a5" class="lq ir hi bd is lr ls lt iw lu lv lw ja jz lx ly je kd lz ma ji kh mb mc jm md bi translated">2.使用MNIST数据集训练网络(教师和学生独立进行)</h2><p id="ac26" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">基本上，我遵循了本文中描述的训练设置[2](见附录A)。</p><p id="c243" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">培训老师和学生的主要区别如下。</p><ul class=""><li id="6228" class="me mf hi jq b jr km jv kn jz ms kd mt kh mu kl mj mk ml mm bi translated">教师:抖动输入图像，并约束权重范数为15.0</li><li id="10f9" class="me mf hi jq b jr mn jv mo jz mp kd mq kh mr kl mj mk ml mm bi translated">学生:香草反向传播</li></ul><h2 id="9ab0" class="lq ir hi bd is lr ls lt iw lu lv lw ja jz lx ly je kd lz ma ji kh mb mc jm md bi translated">3.从老师那里提炼知识给学生</h2><p id="b6be" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">正如在这个故事的前面部分所写的，我改变了培训学生网络的损失项，并给出了从教师网络生成的软标签。</p><h2 id="a88a" class="lq ir hi bd is lr ls lt iw lu lv lw ja jz lx ly je kd lz ma ji kh mb mc jm md bi translated">4.评估网络(教师、学生、学生+蒸馏)</h2><p id="16c5" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">每个网络的错误率如下。</p><ul class=""><li id="42a5" class="me mf hi jq b jr km jv kn jz ms kd mt kh mu kl mj mk ml mm bi translated">老师:100 / 10000 (1.00%)</li><li id="3ed8" class="me mf hi jq b jr mn jv mo jz mp kd mq kh mr kl mj mk ml mm bi translated">学生:171 / 10000 (1.71%)</li><li id="6c18" class="me mf hi jq b jr mn jv mo jz mp kd mq kh mr kl mj mk ml mm bi translated">患有KD的学生:111 / 10000 (1.11%)</li></ul><p id="5422" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们可以看到，通过应用知识发现，学生的错误率显著降低(1.71% → 1.11%)。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/c84da32c5a4447e53bad4e2013b22574.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*pFAlL287bOkD2toMU6o1Kg.png"/></div></figure><h1 id="ffa6" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">结论</h1><p id="154a" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">通过我自己的代码来观察知识升华的结果给我留下了非常深刻的印象。现在我相信真的管用了！…我的下一步将是实现简单的对象检测网络。如果您对此实施有任何疑问，请随时提问:)</p><p id="a103" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">你可以在这里得到这个实验的完整代码:<a class="ae kr" href="https://github.com/poperson1205/knowledge_distillation" rel="noopener ugc nofollow" target="_blank">https://github.com/poperson1205/knowledge_distillation</a></p><h1 id="0085" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">参考</h1><p id="3ca9" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">[1]辛顿等人。艾尔。<a class="ae kr" href="https://arxiv.org/abs/1503.02531" rel="noopener ugc nofollow" target="_blank">“在神经网络中提取知识”。NIPS2014。</a></p><p id="a574" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">[2]辛顿等人。艾尔。“通过防止特征检测器的共同适应来改进神经网络”。<a class="ae kr" href="https://arxiv.org/abs/1207.0580" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1207.0580</a></p><p id="7d02" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">[3]论文[1]的演示材料:<a class="ae kr" href="https://www.ttic.edu/dl/dark14.pdf" rel="noopener ugc nofollow" target="_blank">https://www.ttic.edu/dl/dark14.pdf</a></p></div></div>    
</body>
</html>