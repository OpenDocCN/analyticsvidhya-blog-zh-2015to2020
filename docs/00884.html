<html>
<head>
<title>Leveraging Spark for Large Scale Deep Learning Data Preparation and Inference</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用Spark进行大规模深度学习数据准备和推理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/leveraging-spark-for-large-scale-deep-learning-data-preparation-and-inferencing-c42e92183f53?source=collection_archive---------3-----------------------#2019-09-16">https://medium.com/analytics-vidhya/leveraging-spark-for-large-scale-deep-learning-data-preparation-and-inferencing-c42e92183f53?source=collection_archive---------3-----------------------#2019-09-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="0fa1" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">虽然众所周知，训练深度学习模型需要大量数据才能产生良好的结果，但快速增长的业务数据通常需要部署深度学习模型才能处理越来越大的数据集。如今，深度学习从业者发现自己在大数据世界中运营并不罕见。</h2></div><p id="832e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了解决训练中的大数据集问题，引入了分布式深度学习框架。在推理方面，机器学习模型，特别是深度学习模型通常被部署为Rest API端点，可扩展性是通过在Kubernetes等框架中的多个节点上复制部署来实现的。</p><p id="b8fb" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这些机制通常需要大量的工程工作来正确设置，并且并不总是有效的，尤其是在非常大的数据量中。</p><p id="cea9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇文章中，我想介绍两种技术方法来解决大数据中深度学习的两个挑战:</p><p id="fd70" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">1.并行处理结构化和非结构化数据的大量数据预处理</p><p id="0119" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.使用Spark在大数据管道中部署高性能批量评分的深度学习模型。</p><p id="1d36" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这些方法利用了Spark Framework和Tensorflow 2.0中的最新特性和增强功能。</p><h1 id="7d46" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">1.介绍</h1><p id="e544" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">深度学习通常用大量的非结构化数据进行训练，如图像、音频、文本…或具有数千个特征的结构化数据。提供数百GB的数据集作为训练的输入并不少见。如果使用单节点架构，可能需要非常长的时间来完成消化和训练。现在有一些分布式深度学习框架，如Horovod和分布式Tensorflow，它们有助于全面扩展深度学习培训。但是这些框架专注于分配核心训练任务，即从模型的副本中计算数据碎片上的梯度，而没有很好地并行化其他一般计算步骤。Apache Spark是一个<a class="ae kq" href="https://analyticsindiamag.com/10-open-source-data-science-big-data-applications-well-supported-linux/" rel="noopener ugc nofollow" target="_blank">开源</a>集群计算框架，它提供了一个接口，用于通过隐式数据并行和容错对整个集群进行编程，是一个以简单方式卸载一般计算步骤的优秀工具。</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es kr"><img src="../Images/8fe9f1a404574905b2d9a1f7d11e0fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5yWy4j11Y6wz3ZjAvxI0OA.png"/></div></div></figure><p id="ee9e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> <em class="ld">图1</em></strong><em class="ld">:ML开发的不同阶段，橙色的阶段可以卸载到Spark </em></p><p id="9b0c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过使用Apache Spark，我们可以卸载预处理步骤，包括数据清理、功能工程，并生成一个易于使用的数据集，然后可以使用深度学习框架。这将节省时间和成本，因为GPU等昂贵的资源可以用来专注于它的亮点:从大型张量矩阵计算模型的梯度。</p><p id="9be7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一方面，传统上ML模型是作为Rest APIs部署的。虽然这种方法适用于实时评分场景，但当数据以非常大的批量进入时，它可能无法提供最佳吞吐量。</p><p id="33e7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">同样，Spark在这里大放异彩，是深度学习模型大批量、批量评分的绝佳选择。</p><p id="774c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然Spark已经存在了很长时间，并且已经有多个计划试图利用Spark来扩展深度学习，但是这些方法与其他方法有什么不同？</p><p id="9320" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本文中的方法利用了Spark中的两个非常重要的特性，这两个特性解决了当前其他框架中的弱点:</p><ul class=""><li id="07d6" class="le lf hi iz b ja jb jd je jg lg jk lh jo li js lj lk ll lm bi translated">支持二进制文件格式，以处理深度学习中几乎任何类型的非结构化数据</li><li id="71c4" class="le lf hi iz b ja ln jd lo jg lp jk lq jo lr js lj lk ll lm bi translated">使用最新的熊猫UDF，包括标量迭代器，允许缓存和重用内存中的多个批次的模型，而不必从存储重新加载</li></ul><h1 id="9ddb" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">2.对任何类型的数据进行缩放数据预处理</h1><p id="99aa" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">数据预处理包括数据清洗和特征工程，产生可用于深度学习训练和推理的数据。</p><p id="e38c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将这些逻辑卸载到Spark分布式框架可以显著提高深度学习训练的性能。</p><p id="0817" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Spark中有几个特性允许高效的分布式数据预处理:</p><ol class=""><li id="9055" class="le lf hi iz b ja jb jd je jg lg jk lh jo li js ls lk ll lm bi translated">Spark对二进制数据输入和tfrecords输出的支持</li><li id="c5aa" class="le lf hi iz b ja ln jd lo jg lp jk lq jo lr js ls lk ll lm bi translated">在worker节点支持深度学习和Python库，并使用UDF来执行复杂的功能工程</li></ol><p id="1949" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，Spark不仅能够处理结构化数据和文本数据，而且能够处理二进制数据，这一点很重要，因为深度学习通常处理图像、音频和其他非结构化数据。这简化了数据加载和预处理步骤，因为我们不需要为任务编写定制的读取器。</p><p id="ac1f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最近Spark对二进制数据的支持提供了这种能力。Spark的二进制文件数据集提供了将二进制数据读入以下数据帧表示的能力</p><ul class=""><li id="c0a7" class="le lf hi iz b ja jb jd je jg lg jk lh jo li js lj lk ll lm bi translated">文件路径</li><li id="39db" class="le lf hi iz b ja ln jd lo jg lp jk lq jo lr js lj lk ll lm bi translated">修改时间</li><li id="9aa5" class="le lf hi iz b ja ln jd lo jg lp jk lq jo lr js lj lk ll lm bi translated">长度</li><li id="f30a" class="le lf hi iz b ja ln jd lo jg lp jk lq jo lr js lj lk ll lm bi translated">二进制内容</li></ul><p id="2cf9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其次，Spark支持输出到深度学习的原生格式。对于Tensorflow，有一个名为Spark Tensorflow connector的库，它允许将TFRecords格式的数据读取到Spark Dataframe。</p><p id="d8a5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据和特征工程的预处理可能涉及使用某些自定义库，如Pillow for image、Scipy Librosa for audio，当然还有tensorflow本身。幸运的是，商业Spark framework使得跨集群中的节点安装定制库变得很容易。</p><p id="679a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在的策略是使用Spark框架分割数据，并有一个预处理数据的机制。在这里，我们将使用UDF，特别是UDF大熊猫来有效地做到这一点。</p><p id="4ecc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在下面的代码演示中，这是语音识别练习的一部分，我设计了一个UDF函数和所需的库，从一个音频文件中获取声谱图特征。</p><p id="96c6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一种数据预处理算法</p><ul class=""><li id="d5e8" class="le lf hi iz b ja jb jd je jg lg jk lh jo li js lj lk ll lm bi translated">使用Spark Dataframe类型的二进制文件读取二进制数据</li><li id="20db" class="le lf hi iz b ja ln jd lo jg lp jk lq jo lr js lj lk ll lm bi translated">选择二进制内容列到UDF函数中以提取特征</li><li id="3a46" class="le lf hi iz b ja ln jd lo jg lp jk lq jo lr js lj lk ll lm bi translated">根据需要选择其他列，如进入另一个UDF的文件路径(例如从文件名创建标签列)</li><li id="b254" class="le lf hi iz b ja ln jd lo jg lp jk lq jo lr js lj lk ll lm bi translated">在特征提取UDF函数中，导入所需的库来从二进制数据中提取特征</li></ul><p id="a659" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于Spark本身支持的结构化数据，如csv和parquet文件，这个过程要简单得多。您可以对列值执行直接转换，最终将数据表单转换为TFRecords。</p><p id="332f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是为深度学习训练预处理音频数据的示例代码</p><pre class="ks kt ku kv fd lt lu lv lw aw lx bi"><span id="247f" class="ly ju hi lu b fi lz ma l mb mc">#Load wav data files using binaryFile format reader</span><span id="ea2a" class="ly ju hi lu b fi md ma l mb mc">df = spark.read.format("binaryFile") \<br/>          .option("pathGlobFilter", "*.wav") \<br/>          .option("recursiveFileLookup", "true") \<br/>          .load("mnt/audioproject/data/voicecmd/data_speech_commands_v0.02")</span><span id="8674" class="ly ju hi lu b fi md ma l mb mc">import pyspark.sql.functions as f<br/>import pyspark.sql.types as t<br/>from pyspark.sql.functions import col, pandas_udf</span><span id="2d1a" class="ly ju hi lu b fi md ma l mb mc">#UDF 1: process core binary content using librosa and extract spectrogram as features<br/>def audio_prep_tf(binary):<br/>  import pandas as pd<br/>  import numpy as np<br/>  from io import BytesIO<br/>  import os<br/>  import librosa<br/>  import random<br/>  import string</span><span id="d15a" class="ly ju hi lu b fi md ma l mb mc">audio_list = []<br/>  for binary in binary:<br/>    x, sr = librosa.load(BytesIO(binary), sr=16000)<br/>    X = librosa.stft(x,n_fft=2048)<br/>    Xdb = librosa.amplitude_to_db(abs(X))<br/>    zeros_to_pad = 38-Xdb.shape[1]<br/>    Xdb = np.pad(Xdb, ((0, 0),(0,zeros_to_pad)),'constant')<br/>    audio_list.append(Xdb.tobytes())</span><span id="dcd4" class="ly ju hi lu b fi md ma l mb mc">return pd.Series(audio_list)</span><span id="bbc2" class="ly ju hi lu b fi md ma l mb mc">#UDF 2: Get input file path and extract file name and look up index position in a label list</span><span id="27fb" class="ly ju hi lu b fi md ma l mb mc">def process_label_tf(org_file_path):<br/>  import pandas as pd<br/>  label_list = ['backward','bed','bird','cat','dog',<br/>                'down','eight','five','follow','forward','four','go',<br/>                'happy','house','learn','left','marvin','nine','no',<br/>                'off','on','one','right','seven','sheila','six','stop',<br/>                'three','tree','two','up','visual','wow','yes','zero']<br/>  label_index_list=[]<br/>  for org_file_path in org_file_path:<br/>    label = org_file_path.split("/")[-2]<br/>    label_index=label_list.index(label)<br/>    label_index_list.append(label_index)<br/>  return pd.Series(label_index_list)</span><span id="8c95" class="ly ju hi lu b fi md ma l mb mc">#Register as Pandas UDF<br/>audio_prep_tf_udf = pandas_udf(audio_prep_tf,returnType=BinaryType())<br/>process_label_udf = pandas_udf(process_label_tf,returnType=FloatType())</span><span id="a07c" class="ly ju hi lu b fi md ma l mb mc">fields = [t.StructField("train/label", t.FloatType ()), <br/>          t.StructField("train/audio", t.BinaryType())]<br/>schema = t.StructType(fields)</span><span id="0762" class="ly ju hi lu b fi md ma l mb mc">#Apply the UDFs to transform data. Consolidate into 100 output TF files</span><span id="669b" class="ly ju hi lu b fi md ma l mb mc">results_sdf = (<br/>    df.coalesce(100)<br/>    .select(<br/>        audio_prep_tf_udf( f.col('content')).alias("train/audio"),<br/>      process_label_udf( f.col('path')).alias("train/label")<br/>    )</span><span id="7cda" class="ly ju hi lu b fi md ma l mb mc">)<br/>#Write out the result to distributed file system</span><span id="eb87" class="ly ju hi lu b fi md ma l mb mc">results_sdf.write.format("tfrecords") \<br/>           .option("recordType", "Example") \<br/>           .option("schema",schema).mode("overwrite") \<br/>           .save("/mnt/audioproject/data/voicecmd/converted_tf_data5")</span></pre><h1 id="8b36" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">3.高效大规模推理</h1><p id="fe51" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">Spark作为深度学习部署的环境已经变得越来越好，它允许跨工作节点安装Python和深度学习库并与之互操作。最近的进步允许Spark在内存中加载和缓存大规模深度学习模型，以便在多批次评分中重复使用(Spark Pandas UDF)。Spark数据表示和深度学习框架(Python object…)的数据转换有Pyarrow更好的支持。这些特性是在Apache Spark 2.3版本的Pandas UDFs(也称为矢量化UDF)特性中实现的，该特性大大提高了Python中用户定义函数(UDF)的性能和可用性。熊猫UDF解决了常规UDF一次操作一行的问题，因此受到高序列化和调用开销的困扰</p><figure class="ks kt ku kv fd kw er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es me"><img src="../Images/3c5de2131733fddafd6b861962d7859b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-TPeFVcq1Mqq_fYi3YrV2w.png"/></div></div></figure><p id="7846" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">图2: </strong> <em class="ld">熊猫UDF的表现全面优于一次一行的UDF，从</em> <strong class="iz hj"> <em class="ld"> 3x到超过100x </em> </strong> <em class="ld">(来源databricks.com)</em></p><p id="d5ac" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了进一步支持深度学习大规模推理，有一个新版本的熊猫标量迭代器熊猫UDF，它与上面的标量熊猫UDF相同，只是底层Python函数将一个批处理迭代器作为输入，而不是单个批处理，并且它不是返回单个输出批处理，而是产生输出批处理或返回输出批处理的迭代器。当UDF执行需要初始化某个状态时，标量迭代器熊猫UDF是有用的，例如，加载机器学习模型文件以对每个输入批次应用推理。</p><p id="f291" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是一个使用标量迭代器UDF的持久化Keras模型批量评分的示例实现。再次使用二进制文件Spark数据帧加载原始数据，然后预处理和模型评分的逻辑可以有效地结合在一起。数据被拆分成熊猫系列的批次。请注意，对于多个批处理，模型加载和初始化只执行一次，因为这是一个开销很大的操作。</p><pre class="ks kt ku kv fd lt lu lv lw aw lx bi"><span id="f4a2" class="ly ju hi lu b fi lz ma l mb mc">#Batch Scoring from raw data using Pandas UDF Batch Iterator<br/>df = spark.read.format("binaryFile") \<br/>          .option("pathGlobFilter", "*.wav") \<br/>          .option("recursiveFileLookup", "true") \<br/>          .load("mnt/audioproject/data/voicecmd/data_speech_commands_v0.02")</span><span id="46ff" class="ly ju hi lu b fi md ma l mb mc">import pyspark.sql.functions as f<br/>import pyspark.sql.types as t<br/>from pyspark.sql.functions import col, pandas_udf, PandasUDFType</span><span id="a6b6" class="ly ju hi lu b fi md ma l mb mc">#UDF : preprocess binary data, load DL model and score  <br/><a class="ae kq" href="http://twitter.com/pandas_udf" rel="noopener ugc nofollow" target="_blank">@pandas_udf</a>("string", PandasUDFType.SCALAR_ITER)<br/>def predict(binary_batch):<br/>  import pandas as pd<br/>  import numpy as np<br/>  from io import BytesIO<br/>  import os<br/>  import librosa<br/>  import random<br/>  import string<br/>  import tensorflow as tf<br/>  from tensorflow.keras.models import load_model</span><span id="03c7" class="ly ju hi lu b fi md ma l mb mc">path ='/dbfs/mnt/ml/tmp/audioml'<br/>  loaded_model = load_model(path)<br/>  <br/>  for binary_series in binary_batch:<br/>    input_array=[]<br/>    i=0<br/>    for binary in binary_series:<br/>      x, sr = librosa.load(BytesIO(binary), sr=16000)<br/>      X = librosa.stft(x,n_fft=2048)<br/>      Xdb = librosa.amplitude_to_db(abs(X))<br/>      zeros_to_pad = 38-Xdb.shape[1]<br/>      Xdb = np.pad(Xdb, ((0, 0),(0,zeros_to_pad)),'constant')<br/>      audio = tf.reshape(Xdb, [1025,38,1])<br/>      audio= tf.image.grayscale_to_rgb(audio)<br/>      i=i+1<br/>      input_array.append(audio)<br/>    predicted_index = loaded_model.predict(np.array(input_array))<br/>    predicted_index= [np.argmax(i) for i in predicted_index]<br/>    yield pd.Series(label_list[predicted_index], index= range(i))</span><span id="d61a" class="ly ju hi lu b fi md ma l mb mc">predict_result_df = df.select(predict(f.col('content')).alias("predict"))</span></pre><h1 id="7d8f" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">4.结论</h1><p id="28ee" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">使用Spark可以大幅提高深度学习开发的生产力，并利用大数据对性能进行评分。二进制文件和熊猫UDF的新高效实现使Spark成为几乎任何类型的深度学习开发和部署场景的可行解决方案，无论数据类型和模型复杂性如何。在培训中，Spark可以用于卸载生产管道中的数据预处理任务，在Spark已经流行的地方，深度学习模型可以直接部署，以利用Spark强大的可扩展性，并且复杂性低得多。</p><h1 id="7fd0" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">5.数据</h1><p id="bbad" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">本文中的示例是在语音命令上执行的</p><p id="4fc4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">谷歌数据集(<a class="ae kq" href="https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2017/08/launching-speech-commands-dataset . html</a>)</p><h1 id="1ae2" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">参考</h1><ol class=""><li id="333e" class="le lf hi iz b ja kl jd km jg mf jk mg jo mh js ls lk ll lm bi translated">李阳，，Bobbie Chern和Andy Feng (@afeng76)，雅虎大ML团队，大数据集群的分布式深度学习，2017年</li></ol><p id="0202" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.Databricks，Spark深度学习管道，2017年。</p><p id="d579" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.Tensorflow团队，Tensorflow 2.0项目</p><p id="f04b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">4.阿帕奇星火组织，熊猫UDF，2017</p><p id="43ed" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">5.Databricks &amp; Apache Spark Org，熊猫UDF标量迭代器，2019。</p><p id="91f5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">6.Databricks &amp; Apache Spark Org，Spark二进制文件Dataframe，2019。</p><p id="bbe2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">7.Tensorflow团队，Spark Tensorflow连接器，2016</p></div></div>    
</body>
</html>