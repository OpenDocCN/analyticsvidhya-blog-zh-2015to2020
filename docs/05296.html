<html>
<head>
<title>How to Implement a Neural Network with only +1 and -1 ?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何实现一个只有+1和-1的神经网络？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-implement-a-neural-network-with-only-1-and-1-5170e80fd65?source=collection_archive---------20-----------------------#2020-04-16">https://medium.com/analytics-vidhya/how-to-implement-a-neural-network-with-only-1-and-1-5170e80fd65?source=collection_archive---------20-----------------------#2020-04-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="cc4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度神经网络在过去十年中已经成为许多领域的一个伟大工具，从图像识别到语音识别，从机器翻译到AlphaGo这样的游戏。由于数百万个参数(神经元)和巨大的学习能力，它们非常强大。根据“通用逼近定理”，具有单个隐层和有限个神经元的神经网络可以逼近R^n.意义的闭子集上的连续函数，它可以用充分和适当的参数表示各种各样的函数。因此，它们被称为“通用近似器”。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jk"><img src="../Images/a8150e44ef20d38e38f8a083eb7d0735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ysUVjIIZiHVIkdtZqsGFXQ.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">神经网络结构</figcaption></figure><p id="f4e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管它们非常强大，并且通常被用来解决具有挑战性的问题，但是它们的成本非常高。在过去的十年中，为ImageNet分类挑战建立了许多模型。每个模型的目标都是在准确性得分方面超过之前的模型。如果我们将AlexNet ( 2012)与ResNet ( 2015)进行比较，我们将看到误差的巨大减少。16%的误差在ResNet中降至3.5%。然而，当我们比较模型结构时，成本就开始起作用了。虽然AlexNet只有8层，但ResNet有152层！好的性能不是免费的，相反，它是相当昂贵的。</p><p id="dcba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上所述，由于深度神经网络的结构由多层和数百万个参数组成，因此出现了许多新的挑战:</p><h2 id="0f43" class="ka kb hi bd kc kd ke kf kg kh ki kj kk iq kl km kn iu ko kp kq iy kr ks kt ku bi translated">1速</h2><p id="f904" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">随着参数数量的增加，DRAM访问和加法/乘法操作花费的时间越多。例如，虽然ResNet18花了2.5天训练，但ResNet152花了1.5周。</p><h2 id="0ef2" class="ka kb hi bd kc kd ke kf kg kh ki kj kk iq kl km kn iu ko kp kq iy kr ks kt ku bi translated">2-存储空间</h2><p id="fffa" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">正如所料，使用数百万个参数会导致内存空间的极度使用。基于ResNet152中使用的60M参数，我们可以发现，假设每个参数都是32位浮点，整个模型的大小超过200兆。200兆对台式电脑来说可能不成问题。但是对于像手机这样的小设备来说，这是一个大问题。通过使用这样的深度神经网络(DNN)模型实现的应用需要数百兆字节。这就是为什么通过互联网更新分发如此大的模型也具有挑战性。</p><h2 id="8ee4" class="ka kb hi bd kc kd ke kf kg kh ki kj kk iq kl km kn iu ko kp kq iy kr ks kt ku bi translated">3-能源使用</h2><p id="2ad9" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">由于内存问题，很难在低功率设备上运行此类应用程序，因为它们会耗尽电池。代表参数的尺寸越大，消耗的能量就越多。下表显示了8位整数和32位浮点在能量方面的差异。</p><pre class="jl jm jn jo fd la lb lc ld aw le bi"><span id="6c58" class="ka kb hi lb b fi lf lg l lh li">+-----------------------+----------------+----------+<br/>| <strong class="lb hj">Operation             | Multiplication | Addition </strong>|<br/>+-----------------------+----------------+----------+<br/>| 8-bit integer         | 0.2pJ          | 0.03pJ   |<br/>+-----------------------+----------------+----------+<br/>| 32-bit Floating Point | 3.7pJ          | 0.9pJ    |<br/>+-----------------------+----------------+----------+</span></pre><p id="7331" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们可以观察到的，使用32位浮点而不是8位整数将所需的能量增加了几乎30倍！所以，如果我们能找到一种方法，用更小的词来表示我们的重量，我们就能大大减少这些运算的能量。</p><blockquote class="lj lk ll"><p id="cfd2" class="if ig lm ih b ii ij ik il im in io ip ln ir is it lo iv iw ix lp iz ja jb jc hb bi translated">著名AlphaGo模型由1920个CPU和280个GPU训练，每局3000美元！</p></blockquote><h2 id="790a" class="ka kb hi bd kc kd ke kf kg kh ki kj kk iq kl km kn iu ko kp kq iy kr ks kt ku bi translated">解决办法</h2><p id="0149" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">为了解决这些与硬件相关的问题，人们开发了许多不同的方法。在这篇文章中，我们将讨论二值化神经网络(BNN)。它建议用约束到+1和-1的权重和激活函数来训练神经网络。不像传统的神经网络有数千甚至数百万个不同的权重，用32位浮点表示，BNN的权重可以用一位表示。在传统的二进制化中，该值为0或1。然而，在这种情况下，为了不丢失任何乘以0的信息，我们使用-1和+1。有了这样的实现，内存消耗减少了32倍。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es lq"><img src="../Images/5098b9ba0acc6b55c47511e2cad0da0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*a_E8g0lf6XjBoCrsTfvu6w.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">二值化神经网络</figcaption></figure><p id="be84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了具有二进制权重之外，激活函数也被二进制化。实现二元激活函数有两种选择，确定的或随机的。我们可以使用符号(x)作为确定性激活函数:</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es lr"><img src="../Images/bee15402e8d5762949249e4ea31c88ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*okBnENtBIMUrOzqDj_BXkA.png"/></div></div></figure><pre class="jl jm jn jo fd la lb lc ld aw le bi"><span id="6276" class="ka kb hi lb b fi lf lg l lh li">#Implementation of a sgn(x) function on a vector<br/>def sign(x): <br/>     x[x &gt;= 0] = 1 <br/>     x[x &lt; 0] = -1 <br/>     return x</span></pre><p id="df03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">确定性函数更容易实现，因为它只将数字与零进行比较。随机激活函数将需要生成一些随机位，这将增加硬件的必要性，这是我们已经在努力避免的。</p><p id="13da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，参数的二进制化允许我们执行逐位操作。在BNNs中，复杂的32位浮点乘法运算被简化为简单的移位运算。因此，BNNs通过仅具有两个不需要数百万次DRAM访问的权重，显著降低了存储器和能量消耗，并提高了运行速度。</p><p id="42f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">准确性呢？事实上，我们预计预测性能会显著下降。然而，根据建议在神经网络中仅使用+1和-1作为权重的论文[2]，在众所周知的数据集如MNIST、CIFAR-10和SVHN上获得了几乎最先进的结果。</p><p id="8513" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简而言之，BNNs为降低内存/能耗铺平了道路，并通过深度学习的新方法提高了运行速度。</p><blockquote class="lj lk ll"><p id="93e5" class="if ig lm ih b ii ij ik il im in io ip ln ir is it lo iv iw ix lp iz ja jb jc hb bi translated">本文由<a class="ls lt ge" href="https://medium.com/u/beec6ddf56d8?source=post_page-----5170e80fd65--------------------------------" rel="noopener" target="_blank"> MERVE TURHAN </a>共同撰写。</p></blockquote><h1 id="efe4" class="lu kb hi bd kc lv lw lx kg ly lz ma kk mb mc md kn me mf mg kq mh mi mj kt mk bi translated">参考</h1><p id="00bb" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">[1]- Balázs Csanád Csáji (2001)用人工神经网络逼近；科学学院；匈牙利etvs loránd大学</p><p id="fe4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]- <a class="ae ml" href="https://arxiv.org/abs/1602.02830" rel="noopener ugc nofollow" target="_blank">马蒂厄·库尔巴里奥、伊泰·胡巴拉、丹尼尔·苏德里、兰·埃尔-亚尼夫、约舒阿·本吉奥，“二值化神经网络:训练深度神经网络，其权重和激活被限制为+1或-1”</a></p></div></div>    
</body>
</html>