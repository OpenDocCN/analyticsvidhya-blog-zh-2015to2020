<html>
<head>
<title>Better results with MIXUP at Final Layer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最终图层混合效果更好</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/better-result-with-mixup-at-final-layer-e9ba3a4a0c41?source=collection_archive---------7-----------------------#2019-12-07">https://medium.com/analytics-vidhya/better-result-with-mixup-at-final-layer-e9ba3a4a0c41?source=collection_archive---------7-----------------------#2019-12-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="0b69" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">关于这篇文章</h1><p id="3f99" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">本文内容如下。</p><ul class=""><li id="7074" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">介绍了维卡斯·维尔马、亚历克斯·兰姆等人提出的在中间层混合数据的流形混合[2]，并表明在最终层混合时可能会获得更好的结果。</li><li id="5383" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">我其实是用tensorflow实现的，确认了确实得到了这样的结果。</li></ul></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="5ca5" class="if ig hi bd ih ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc bi translated">什么是MIXUP？</h1><p id="5adf" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">MIXUP[1]是一种数据增强方法，由张弘毅等人于2017年10月25日提出。基于从Beta分布中采样的混合比率，这是一种通过混合输入和输出来扩展数据的方法。通过使用这一点，可以说泛化性能提高了，因为判定边界变得平滑了。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ld"><img src="../Images/5c94f88f81f5a99c5773347fe0102442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oe-rU_2RtFonJxdYGoZwlQ.png"/></div></div></figure><p id="e7d7" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">从上面的公式可以看出，扩充数据是通过混合原始数据创建的。让我们试着用玩具数据来混合。原始数据为蓝色(标签=0)和红色(标签=1)点，扩充数据为粉红色。扩充数据具有介于0之间的标签。和1。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ls"><img src="../Images/63f2a0f36f36604e8bb6996a32e66ff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L66PQBziaDqgDaVR8QpFJw.png"/></div></div></figure><p id="07b3" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">下图显示了各种alpha的混合。您可以看到放大到接近实际数据的位置(α &lt;1) or middle (α&gt; 1)，以α = 1为边界。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es lt"><img src="../Images/99f99760ac6544c9d42bf1faf82808f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yJshivyxRIEKCtgafBsWoA.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">(左)α=0.2时的混合(右)β分布(α=0.2，β=0.2)</figcaption></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ly"><img src="../Images/0a67e3eba2f9f3fa405883471248b9b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pPSshUowjR57b965bOV7Ww.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">(左)α=1时的混合。(右)贝塔分布(α=1。, β=1.)</figcaption></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es lz"><img src="../Images/1656957591898c2956b5f2a1b246b7fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dQTIouvK0CE2kIWiAF5Xrg.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">(左)α= 5°时的混合。(右)贝塔分布(α=5。, β=5.)</figcaption></figure><p id="adf0" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">如果将MIXUP应用于整个数据，将如下所示。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es ma"><img src="../Images/db6c98c57793227b6479eb24378b1ea3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*OM9mvV-eXwmt1k9DXnvf7g.png"/></div></figure></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="6428" class="if ig hi bd ih ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc bi translated">流形混合</h1><h2 id="0ce7" class="mb ig hi bd ih mc md me il mf mg mh ip jo mi mj it js mk ml ix jw mm mn jb mo bi translated">混乱的问题</h2><p id="2b5c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">但是，并不是所有由MIXUP生成的数据都是好数据。</p><p id="5e4b" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">在这些数据中，下图左侧可能是良好的数据扩充(标签=0。~1.)这平滑了决策边界，但是正确的不是好数据。这是因为扩充数据的标签都是“红色”(标签=1)，但是它们在蓝色数据的特征空间中(标签=0)。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mp"><img src="../Images/127f6da56be016fdc5a218fe4aa3793c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9mK7fIKh_oZUuq_vpQnzHw.png"/></div></div></figure><p id="1a59" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">流形混合[2]解决了这个问题。流形混合是一种在中间层执行混合的数据扩充方法，在中间层中，特征空间比输入空间更对齐。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mq"><img src="../Images/bbfbe72ba6f3b1ce5db16f5294d3a1f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*pz7Xt006v8b6CNCCUzuSMA.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">(左)通常的混淆(右)流形混淆</figcaption></figure><p id="10fb" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">上图是在输入层和中间层进行MIXUP时的概念图。在输入层混合(左)中，由于蓝色和红色的复杂特征空间，无法获得良好的增强数据。另一方面，在中间层，与输入层相比，特征空间更加一致，因此可以获得良好的扩充数据。</p><h2 id="15d1" class="mb ig hi bd ih mc md me il mf mg mh ip jo mi mj it js mk ml ix jw mm mn jb mo bi translated">我们在哪一层执行混音？</h2><p id="5c71" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">那么哪一层是最好的混合层呢？有研究[3]似乎是一个提示。本研究在ResNet的中间层引入了带有温度项的软最近邻损失，并研究了它的行为。较大的软最近邻损失值表示按类划分的要素相互交织，较小的值表示按类划分的要素。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mr"><img src="../Images/239cfb31a3981804b33c6acc48cd06eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zLVS4DrXi8rYkEo3dxBe_A.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">软最近邻损失</figcaption></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ms"><img src="../Images/acc4936887c2b09f5042734810128c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5BJZYiaifh8XBb82KzMNDg.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">软最近邻损失的特征图及其值</figcaption></figure><p id="6a63" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">下图显示了ResNet的每个块中的软最近邻损失的值。ResNet被认为是一种高性能的图像分类网络，但它表明，除了最后一层之外，每个类的特征都没有分离。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mt"><img src="../Images/181be7a7d7b01f77ffe13d3d6d02604a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*-MTeRKRvbwDLvIFSpDXc_w.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">每个Resblock中的软最近邻损失值</figcaption></figure><p id="5534" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">鉴于此，假设出来了，如果你使用ResNet，你应该在最后一层混淆。在流形混合论文[2]中，作者说当你混合最后一层时，效果最好。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mu"><img src="../Images/f69bd9e2812370291feeb18a65fe04f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f-AeZOFrw9PNPG2_CuP-kQ.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">每层的歧管混合。最后一层的混合是最好的(红色)</figcaption></figure><p id="a411" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">然而，这个数字本身存储在附录中。看起来《Manifold Mixup》的作者并没有重视结果本身。</p><h1 id="3ab8" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">实验</h1><p id="7127" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我比较了最终图层混合和输入图层混合。使用的数据是CIFAR10。这个实验用的代码(Jupyter笔记本)已经上传到<a class="ae mv" href="https://github.com/AkiraTOSEI/Final_layer_mixup" rel="noopener ugc nofollow" target="_blank">我的github </a>。</p><p id="d9a2" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">因为需要在中间层中进行混合操作，所以混合层被创建如下。通过将两个输入和一个比率作为输入，图层根据比率混合数据。</p><figure class="le lf lg lh fd li"><div class="bz dy l di"><div class="mw mx l"/></div></figure><p id="11af" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">通过使用这一层，输入混合(正常混合)和最终层混合(在最终层中执行的流形混合)可以实现如下。我在<a class="ae mv" href="https://github.com/AkiraTOSEI/tf__resnet" rel="noopener ugc nofollow" target="_blank">这里</a>用的是ResNet18。</p><figure class="le lf lg lh fd li"><div class="bz dy l di"><div class="mw mx l"/></div></figure><h1 id="1e74" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">培训代码实施</h1><p id="79df" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">代码摘录如下。随机数是根据每个时期的beta分布生成的，并且以该比率执行混合。(第95行)</p><figure class="le lf lg lh fd li"><div class="bz dy l di"><div class="mw mx l"/></div></figure></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="b3ab" class="if ig hi bd ih ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc bi translated">结果</h1><p id="775f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">下表显示了mixup α = [0.04，0.2，1.0，5.0]的实验中的最终有效损耗。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es my"><img src="../Images/ded233110e4058f2c442bb4dfe07c320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*hYWMgXDKyA6BPRC2lzO10w.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">各种混淆及其有效损失</figcaption></figure><p id="1baf" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">最好的事情是最终的层混合，alpha为0.2，但是即使alpha = 1.0，结果也几乎没有变化。</p><p id="f939" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">输入混音应具有较小的α，当超过1.0时，精度会急剧下降。另一方面，当α较大时，最终层混合也更差，但不如输入混合差。这种差异被认为反映了要混合的特征空间是否对齐。</p><p id="0d67" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">即使在流形混合中，精度也会随着较大的α而下降。如果特征空间是对齐的，则认为最终层中的特征空间如下。表示插值数据不一定是好的。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mz"><img src="../Images/6573ca153fcb186a3aeaf3ecb93a3350.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*suHnsGMHMAru74cUyzP5IQ.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">使用Softmax和交叉熵的特征映射[4]</figcaption></figure></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="98ce" class="if ig hi bd ih ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc bi translated">结论</h1><p id="d8ef" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在这篇文章中，我介绍了在中间层混合的流形混合，并展示了在最后一层混合可以获得良好的结果。我想这可能是实践中的一个重要结果。</p><p id="560f" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated"><strong class="jf hj"> <em class="na">碎碎念</em> </strong></p><div class="nb nc ez fb nd ne"><a href="https://twitter.com/AkiraTOSEI" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab dw"><div class="ng ab nh cl cj ni"><h2 class="bd hj fi z dy nj ea eb nk ed ef hh bi translated">阿基拉</h2><div class="nl l"><h3 class="bd b fi z dy nj ea eb nk ed ef dx translated">akira的最新推文(@AkiraTOSEI)。机器学习工程师/数据科学家/物理学硕士/…</h3></div><div class="nm l"><p class="bd b fp z dy nj ea eb nk ed ef dx translated">twitter.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ln ne"/></div></div></a></div></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="7b7b" class="if ig hi bd ih ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc bi translated"><strong class="ak">参考</strong></h1><ol class=""><li id="e43f" class="kb kc hi jf b jg jh jk jl jo nt js nu jw nv ka nw kj kk kl bi translated">张弘毅，混合:超越经验风险最小化，arXiv:1710.09412 (2017)</li><li id="8953" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka nw kj kk kl bi translated">维卡斯·维尔马，亚历克斯·兰姆等。流形混合:通过插值隐藏状态的更好表示arXiv:1806.05236(2018)</li><li id="5062" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka nw kj kk kl bi translated">Nicholas Frosst等人。使用软最近邻损失分析和改进表示，arXiv:1902.01889 (2019)</li><li id="4a3f" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka nw kj kk kl bi translated">Munawar Hayat，具有高斯相似性的最大间隔类不平衡学习，arXiv:1901.07711 (2019)</li></ol></div></div>    
</body>
</html>