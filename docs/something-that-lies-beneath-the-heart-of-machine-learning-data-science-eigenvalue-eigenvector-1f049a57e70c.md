# 机器学习和数据科学的核心“特征值和特征向量”之下的东西。

> 原文：<https://medium.com/analytics-vidhya/something-that-lies-beneath-the-heart-of-machine-learning-data-science-eigenvalue-eigenvector-1f049a57e70c?source=collection_archive---------18----------------------->

![](img/e04de7378a813f1a20cf944849fc5662.png)

丹·鲁索在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

> **特征值和特征向量介绍**

我们非常清楚，要做出好的预测，我们的模型需要大量的数据，而收集这些数据本身就是一项复杂的任务。

以任何形式或格式收集的数据，将其处理为机器可读形式，存储并优化它。这通常将我们的数据转换成 n 维数据，这一直困扰着许多工程师，这就是特征值和特征向量的问题。在我们开始讨论特征值和特征向量的实际作用之前，让我们为一个问题创建一个代数表达式，以便更精确地理解它。

假设我们有一个数 X 和一个数 Z，我们想找出一个数 Y，当相乘时(为什么相乘？想想吧。)通过简单的数学我们可以解决这个问题，现在让我们假设 X & Z 是一个大数据的矩阵，Y 是一个向量，这样当我们将 X 缩放到 Y 时，我们得到 Z。

我们从上面的介绍中了解到，大型数据集可以表示为一个矩阵，我们需要以某种方式压缩稀疏矩阵的列，以加快我们的计算速度。另外，如果我们把一个矩阵乘以一个向量，那么我们得到一个新的向量。矩阵乘以向量被称为**变换矩阵。**

新的向量可以被认为有两种形式:

1.  有时，新的变换向量只是原始向量的缩放形式。这意味着新向量可以通过简单地将一个标量(数字)乘以原始向量来重新计算，就像上面向量 X 和 Z 的例子一样。
2.  其他时候，变换后的向量与我们用来乘以矩阵的原始向量没有直接的标量关系。

如果新的变换向量只是原始向量的缩放形式，则已知原始向量是原始矩阵的**特征向量**。特征向量可以用来表示一个大维矩阵。

因此，如果我们的输入是一个大的稀疏矩阵 M，那么我们可以找到一个可以代替矩阵 M 的向量 o。标准是矩阵 M 和向量 o 的乘积应该是向量 o 和标量 n 的乘积:

> *M * o = n* o*

这意味着矩阵 M 和向量 o 可以被标量 n 和向量 o 代替。

**在这种情况下，o 是特征向量，n 是特征值，我们的目标是找到 o 和 n。**

因此，特征向量是一种在应用变换时不会改变的向量，只是它变成了原始向量的缩放版本。

特征向量可以帮助我们计算一个大矩阵作为一个较小的向量的近似值。还有许多其他的用途，我将在文章的后面解释。

特征向量用于使线性变换变得可以理解。把特征向量想象成拉伸/压缩一个 X-Y 线图，而不改变它们的方向。

**特征值** —用于变换(拉伸)特征向量的标量。

> **特征值和特征向量有多种用途:**

1.  特征值和特征向量在线性微分方程中有它们的重要性，当你想找到一个变化率或者当你想保持两个变量之间的关系时。

> *认为特征值和特征向量提供了一个大矩阵的概要*

2.我们可以用一个矩阵来表示大量的信息。在大型矩阵上执行计算是一个非常缓慢的过程。具体来说，提高计算密集型任务效率的关键方法之一是在确保保留大部分关键信息后降低维度。因此，一个特征值和特征向量用于捕获存储在大矩阵中的关键信息。这种技术也可以用来提高数据搅动组件的性能。

3.成分分析是在不损失有价值信息的情况下减少维度空间的关键策略之一。**成分分析(PCA)的核心建立在特征值和特征向量的概念上。**该概念围绕计算特征的协方差矩阵的特征向量和特征值。

4.此外，特征向量和特征值用于面部识别技术，如特征脸。

5.它们用于减少维度空间。使用特征向量和特征值技术来压缩数据。如上所述，许多算法如 PCA 依靠特征值和特征向量来降低维数。

6.特征值也用于正则化，它们可以用来防止过拟合。