<html>
<head>
<title>Introduction to Web-Scrapping with Beautiful-Soup and Requests</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">介绍网络报废与美丽的汤和要求</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-web-scrapping-with-beautiful-soup-and-requests-9bb1ff389f55?source=collection_archive---------10-----------------------#2020-09-18">https://medium.com/analytics-vidhya/introduction-to-web-scrapping-with-beautiful-soup-and-requests-9bb1ff389f55?source=collection_archive---------10-----------------------#2020-09-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/23d88e107035bcd69dc9a8cc77667bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*c4M51-ez5JkUehemMPVvug.jpeg"/></div></figure><p id="5ac9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi jk translated"><span class="l jl jm jn bm jo jp jq jr js di"> W </span> <strong class="io hj"> eb 抓取</strong>是从互联网上收集信息的过程，我们知道数据已经成为<strong class="io hj">新的石油</strong>，web 抓取在各种应用中已经变得更加重要和实用。Web 抓取处理从网站中提取或抓取信息。</p><p id="c0d7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Web 抓取处理从网站提取中提取或抓取信息。从网站复制文本并粘贴到您的本地系统也是网络抓取！</p><p id="96f3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">说一下<strong class="io hj"> Beautiful-Soup </strong>，Beautiful Soup 是一个纯 Python 库，用于从网站中提取结构化数据。它允许您解析来自<strong class="io hj"> HTML </strong>和<strong class="io hj"> XML </strong>文件的数据。它充当一个助手模块，与 HTML 交互的方式与使用其他可用开发工具与网页交互的方式类似，但更好。</p><p id="9e48" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">而且，它通常会节省程序员几个小时或几天的工作，因为它与您最喜欢的解析器如<code class="du jt ju jv jw b">lxml</code>和<code class="du jt ju jv jw b">html5lib</code>一起工作，提供了导航、搜索和修改解析树的有机 Python 方式。而且，请求模块允许您使用 Python 发送 HTTP 请求。</p><p id="6361" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以通过以下命令安装 Beautiful-Soup 和 Request:</p><pre class="jx jy jz ka fd kb jw kc kd aw ke bi"><span id="14ed" class="kf kg hi jw b fi kh ki l kj kk">pip install beautifulsoup4<br/>pip install requests</span></pre><p id="7a24" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里需要澄清最重要的一点，<strong class="io hj">“这个博客只是为了教育目的，也不鼓励在没有任何事先书面许可的情况下，或者无视他们的服务条款(ToS)在网上搜集数据”。</strong></p><p id="5084" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，让我们开始吧！</p><p id="b824" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将取消以下网站的链接:<a class="ae kl" href="https://www.whitehouse.gov/briefings-statements/" rel="noopener ugc nofollow" target="_blank"><strong class="io hj">https://www.whitehouse.gov/briefings-statements/</strong></a></p><p id="9725" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个网站的内容之一是总统简报和声明的记录。而我们的“<strong class="io hj">目标</strong>将提取页面上分别指向简报和声明的所有链接！</p><p id="7b97" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将在 Anaconda 环境中实现我们的代码，并开始导入重要的库，</p><pre class="jx jy jz ka fd kb jw kc kd aw ke bi"><span id="eb23" class="kf kg hi jw b fi kh ki l kj kk">import requests<br/>from bs4 import BeautifulSoup</span></pre><p id="34f9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用 requests 模块，我们使用提供的“<strong class="io hj"> get </strong>”函数来访问作为该函数的参数提供的网页。</p><pre class="jx jy jz ka fd kb jw kc kd aw ke bi"><span id="5e02" class="kf kg hi jw b fi kh ki l kj kk">result = requests.get("https://www.whitehouse.gov/briefings-statements/")</span></pre><p id="d8f3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了确保网站是可访问的，我们可以确保获得一个“200”响应来表明该页面确实存在，我们可以通过传递一个代码来实现这一点:</p><pre class="jx jy jz ka fd kb jw kc kd aw ke bi"><span id="a289" class="kf kg hi jw b fi kh ki l kj kk">print(result.status_code)</span></pre><p id="ac1e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们还可以检查网站的<strong class="io hj"> HTTP 头</strong>，以验证我们确实访问了正确的页面:</p><pre class="jx jy jz ka fd kb jw kc kd aw ke bi"><span id="632c" class="kf kg hi jw b fi kh ki l kj kk">print(result.headers)</span></pre><p id="c3b5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，让我们将从请求中访问的网站的页面内容分别存储到一个<strong class="io hj">变量</strong>“var”中:</p><pre class="jx jy jz ka fd kb jw kc kd aw ke bi"><span id="1762" class="kf kg hi jw b fi kh ki l kj kk">var = result.content</span></pre><p id="796f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">到目前为止，我们只是从特定的网站或页面获取内容，并将其存储在一个变量“var”中。您可以查看我刚才传递的命令内容:<strong class="io hj"> print(var) </strong></p><p id="9eae" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在我们已经存储了页面源代码，我们将使用 Beautiful-Soup 模块来解析和处理源代码。为此，我们基于上面创建的源变量创建一个“Beautiful-Soup”对象:</p><pre class="jx jy jz ka fd kb jw kc kd aw ke bi"><span id="9f1f" class="kf kg hi jw b fi kh ki l kj kk">soup = BeautifulSoup(src, 'lxml')</span></pre><p id="af46" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">“<strong class="io hj"> lxml </strong>”是用 Python 语言处理 xml 和 HTML 的功能最丰富、最易于使用的库。</p><p id="fddf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，页面源已经通过 Beautifulsoup 进行了处理，我们可以直接从中访问特定的信息。例如，假设我们希望看到页面上所有链接的列表！</p><p id="c8db" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">嗯，我们的工作是提取页面上指向简报和声明的所有链接，<strong class="io hj">那么我们怎么才能做到呢？</strong></p><p id="a0da" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以通过访问我们想要的废信息页面，并按照简单的步骤，作为参考刮任何允许的网站。</p><h1 id="eec8" class="km kg hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">第一步:访问我们想要废弃的网站</strong></h1><p id="29fa" class="pw-post-body-paragraph im in hi io b ip lj ir is it lk iv iw ix ll iz ja jb lm jd je jf ln jh ji jj hb bi translated">让我们访问我们想要废弃的网站，它是<a class="ae kl" href="https://www.whitehouse.gov/briefings-statements/" rel="noopener ugc nofollow" target="_blank">https://www.whitehouse.gov/briefings-statements/</a></p><figure class="jx jy jz ka fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lo"><img src="../Images/e7be31ba6bc46c074fa9b8ba8a3c9c11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hGAQSp8H18Gbjiqcoqgjhg.jpeg"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图:1</figcaption></figure><h1 id="6527" class="km kg hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">第二步:点击“检查”按钮</h1><p id="9341" class="pw-post-body-paragraph im in hi io b ip lj ir is it lk iv iw ix ll iz ja jb lm jd je jf ln jh ji jj hb bi translated">点击鼠标右键，在底部会碰到一个“<strong class="io hj"> inspect </strong>按钮，会弹出一个指定页面的 Html 脚本，我们要报废！</p><p id="357f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">看起来像这样，</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lx"><img src="../Images/d97e96608096a52706e014bb320d1af6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OABsAr5U-a_HXzg3gU_Xsg.jpeg"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图:2</figcaption></figure><p id="b368" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们选择我们想要删除的文本/链接作为参考，对于这些链接，HTML 标签(<strong class="io hj"> h1，h2，h3，h4 </strong>)、类、超链接元素“<strong class="io hj"> &lt; a &gt; </strong>”对于我们想要删除的所有链接都是相同的！</p><p id="063f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">放大看看！让我们看看在什么标签里，我们的链接被附上了！</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/5246b801dc615d46b662ad04e0920b02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*B7YXTlIb8QB46ywsBqK0Vg.jpeg"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图:3</figcaption></figure><p id="9bb5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这里我们可以看到，我们想要删除的链接包含在“H2”HTML 标签中，超链接元素<a>。</a></p><h1 id="66ef" class="km kg hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">步骤 3:从 html 中提取<a>元素(可选)</a></h1><p id="700f" class="pw-post-body-paragraph im in hi io b ip lj ir is it lk iv iw ix ll iz ja jb lm jd je jf ln jh ji jj hb bi translated">Beautiful soup 为我们提供了提取我们想要废弃的网站的特定内容的强大功能，其中一个命令是，</p><pre class="jx jy jz ka fd kb jw kc kd aw ke bi"><span id="6e61" class="kf kg hi jw b fi kh ki l kj kk">data = soup.find_all("a")</span></pre><p id="2c03" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这里，我们找到并存储<strong class="io hj">超链接元素&lt;a&gt;T19】下的每一个内容，并将数据存储在变量“data”中。同样，我们可以看到数据，py 传递命令，<strong class="io hj">【打印数据】</strong>。</strong></p><h1 id="cb3a" class="km kg hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">步骤 4:将特定内容存储在列表中</h1><p id="44d7" class="pw-post-body-paragraph im in hi io b ip lj ir is it lk iv iw ix ll iz ja jb lm jd je jf ln jh ji jj hb bi translated">正如我们所看到的，在步骤 3 中，我们使用 Beautiful-Soup 提供的 find_all 命令单独提取了超链接元素。</p><p id="609c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在步骤 4 中，我们将在一个“<strong class="io hj"> for 循环</strong>中得到这个单独的步骤，该循环将为网站中的所有链接迭代每个&lt;一个&gt;元素，该元素包含在“<strong class="io hj"> h2 标签</strong>”中(请看图 3)。</p><p id="ec51" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将通过传递以下代码来实现这一点，</p><pre class="jx jy jz ka fd kb jw kc kd aw ke bi"><span id="180d" class="kf kg hi jw b fi kh ki l kj kk">links = [ ] #Creating a list, in which we will store all our links</span><span id="39e3" class="kf kg hi jw b fi lz ki l kj kk">for h2_tag in soup.find_all('h2'):<br/>    a_tag = h2_tag.find('a')<br/>    links.append(a_tag.attrs['href'])</span><span id="e393" class="kf kg hi jw b fi lz ki l kj kk">print(links)</span></pre><p id="3edd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这些是我们从各自的网站上删除的链接，在分别运行上述命令后，您可以得到输出。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es ma"><img src="../Images/80751702ecb6eab8f622d01a27837c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cd8LspQkaKt6u3cW1Nm4Aw.jpeg"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">图 4(输出数据)</figcaption></figure><p id="e4b5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">好了，这都是从我这边来的，如果你真的喜欢这个解释和内容，请在 Medium 上做“<strong class="io hj">拍手</strong>”。</p><p id="a897" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">请参考本教程的代码，来自我下面附上的<strong class="io hj"> GitHub 链接</strong>，</p><h1 id="6d1e" class="km kg hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">GitHub 链接:</h1><div class="mb mc ez fb md me"><a href="https://github.com/GauravSahani1417/Tutorial-for-Beautiful-Soup-and-Requests" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hj fi z dy mj ea eb mk ed ef hh bi translated">gauravsahani 1417/美丽的汤和要求教程</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">这个存储库是作为我的博客“用漂亮的汤清理网络简介”的参考内容保存的</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">github.com</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms ik me"/></div></div></a></div><p id="0a64" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">还有，在<strong class="io hj"> Linkedin </strong>上联系我，分别给<strong class="io hj">教程的正版评论</strong>，</p><h1 id="37ad" class="km kg hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">Linkedin 链接:</h1><div class="mb mc ez fb md me"><a href="https://www.linkedin.com/in/gaurav-sahani-6177a7179/" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hj fi z dy mj ea eb mk ed ef hh bi translated">Gaurav Sahani -数据分析师实习生-neu brain Solutions Pvt Ltd | LinkedIn</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">查看高拉夫·萨哈尼在全球最大的职业社区 LinkedIn 上的个人资料。高拉夫有 3 个工作列在他们的…</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">www.linkedin.com</p></div></div><div class="mn l"><div class="mt l mp mq mr mn ms ik me"/></div></div></a></div></div></div>    
</body>
</html>