<html>
<head>
<title>What we need to know about Ensemble Learning Methods— Simply Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于集成学习方法我们需要知道什么——简单解释</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-we-need-to-know-about-ensemble-learning-methods-simply-explained-a285397a9ddd?source=collection_archive---------20-----------------------#2020-05-16">https://medium.com/analytics-vidhya/what-we-need-to-know-about-ensemble-learning-methods-simply-explained-a285397a9ddd?source=collection_archive---------20-----------------------#2020-05-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/55d8ea234c89ed8c132d73bf9453bde3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*d3mloDltdnVX_gnz"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Wylly Suhendra 在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。<strong class="bd iv"><em class="iw"/></strong></figcaption></figure><p id="27c0" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">这篇文章的目的是介绍集成学习模型的各种概念。我将解释一些必要的关键，以便读者很好地理解相关方法的使用，并能够在需要时设计合适的解决方案。</p><p id="9faf" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">学习模型中误差的主要原因是由于:</p><p id="8808" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">噪音，</p><p id="babe" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj"> <em class="jv">偏差</em> </strong>(预测值与实际值的平均差值，高偏差模型:表现不佳的模型，持续遗漏重要趋势)</p><p id="befc" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">和<strong class="iz hj"> <em class="jv">方差</em> </strong>(量化如果使用不同的训练数据，预测估计会有多大的不同，高方差模型:在训练数据上过度拟合，并且不能在看不见的数据上推广模型)。</p><p id="1108" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff" rel="noopener ugc nofollow" target="_blank">参见偏差-方差权衡</a>。</p><p id="9846" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">因此，首先让我们简要解释集成学习，以了解它如何针对这些类型的错误:</p><p id="b8da" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">集成学习是将不同的弱学习者组合成一个预测模型的策略。它站在观念的投票上:一个所谓的“智慧的<strong class="iz hj"><em class="jv"><br/></em></strong>”的方法或“团结就是力量的<strong class="iz hj"><em class="jv"/></strong>”。</p><p id="ef21" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">主要思想是基于“更多的预测器可以比任何一个单独的预测器产生更好的模型”。</p><p id="df63" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">这可以通过简单的技术来实现，如最大投票(采用所有预测的模式，主要用于分类问题)、平均或加权平均，或者通过更复杂的计算来实现。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jw"><img src="../Images/10c79abdaa191506e2eda154f90148df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KjtSTVtGliW2_1Qu"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae iu" href="https://unsplash.com/@gwundrig?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Manuel n Geli</a>拍摄的照片。合奏模型就像一个管弦乐队！:)</figcaption></figure><p id="8aa7" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">我们可以使用单个基础学习算法来产生<strong class="iz hj">同质</strong>基础学习器(相同类型的学习器)从而导致同质集成，或者使用<strong class="iz hj">异质</strong>学习器(不同类型的学习器)从而导致异质集成。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es kb"><img src="../Images/6658f6b586b8a4e2b6d68ed4279bc581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*BuF3Rg8CHBy6xuEVjZ2rLQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">集成模型结合了来自多个学习者的决策，以提高整体性能。</figcaption></figure><p id="1199" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">有四种高级类型的集合方法:</p><p id="2704" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">装袋、增压、堆垛、混合。</p><p id="6709" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">让我们了解这些先进技术背后的理念:</p><p id="179f" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi kc translated"><span class="l kd ke kf bm kg kh ki kj kk di">B</span><strong class="iz hj"><em class="jv">agging</em></strong>:<strong class="iz hj">B</strong>ootstrap<strong class="iz hj">AGG</strong>regat<strong class="iz hj">ING</strong>ensemble方法，该方法通过获取训练数据的自举子样本并构建B模型，在不同的子样本上进行训练。基础(弱)模型独立地建立在这些子集中的每一个上。它们在<em class="jv">平行</em>中独立运行。最终的预测将通过结合所有模型的结果来确定。</p><p id="de5e" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">(<a class="ae iu" href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" rel="noopener ugc nofollow" target="_blank"> Bootstrap </a>:带替换的随机抽样)。</p><p id="217a" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">Bootstrapping通过为模型提供不同的数据子集来提供帮助。因此，减少这些模型给出相同结果的机会是有用的。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kl"><img src="../Images/311a9da22c58b5cab0dc7c1c2b2d31fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T_yYovqkfhDCcNckEYp3WA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">随机森林:打包的一个小调整</figcaption></figure><p id="66a3" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><strong class="iz hj"> <em class="jv">随机森林</em> </strong>遵循bagging技术，通过一个小的调整去树的相关性，产生一个非常强大的模型。我们做的一切都和装袋一样，但当我们构建树时，每次我们考虑分割时，都会选择一个随机的特征样本作为分割候选，而不是完整的特征集。当使用全部功能时，我们只是进行打包。</p><p id="8ef5" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated"><em class="jv">它适用于</em> <strong class="iz hj"> <em class="jv">高方差低偏差</em> </strong> <em class="jv">车型。</em>如果问题是单个模型得到的性能非常低(高偏差)，那么bagging很少会得到更好的偏差。</p><p id="a2e8" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi kc translated"><span class="l kd ke kf bm kg kh ki kj kk di">B</span>oosting<strong class="iz hj"><em class="jv">:</em></strong>主要思想是通过使用来自先前构建的模型的信息来改进最终模型。Boosting旨在减少偏差，同时保持方差较小。它通过缓慢增长来追求方差，通过将许多弱模型组合成一个“超级模型”来追求偏差。</p><p id="3e71" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">增强可以应用于非树模型，但最常用于树。</p><p id="3166" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">在Boosting中，第一个算法在整个数据集上训练。然后依次建立后续算法，拟合前一算法的残差。每个学习器拟合前一个学习器的误差，每个阶段选择最优的学习器和权重。它对先前模型预测不佳的观测值给予较高的权重。嗯，我们都应该从错误中吸取教训(也许不是所有人，但大多数人)😏)。</p><p id="7729" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">提升是团队合作的结果。</p><p id="4bd5" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">损失函数和计算(伪)残差的方式取决于精确的boosting算法和如何设置学习参数λ。</p><p id="64cc" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">这些是使用不同底层引擎的一些类型的增压:</p><ol class=""><li id="80b1" class="km kn hi iz b ja jb je jf ji ko jm kp jq kq ju kr ks kt ku bi translated">AdaBoost ( <strong class="iz hj"> Ada </strong>感受性<strong class="iz hj"> Boost </strong> ing):给错误预测的观察值分配权重</li><li id="9116" class="km kn hi iz b ja kv je kw ji kx jm ky jq kz ju kr ks kt ku bi translated">梯度推进机器(GBM):使用梯度下降来用树更新<br/>模型，这有助于最小化我们的损失</li><li id="a0d8" class="km kn hi iz b ja kv je kw ji kx jm ky jq kz ju kr ks kt ku bi translated">极限梯度提升机(XGBM):还是梯度提升，但是更巧妙！</li><li id="b54a" class="km kn hi iz b ja kv je kw ji kx jm ky jq kz ju kr ks kt ku bi translated">LightGBM:在大数据集方面优于XGBM，但与XGBM相似，不同之处在于它拆分树的方式(LightGBM是按叶方式，而其他Boostings是按层方式)</li><li id="28a0" class="km kn hi iz b ja kv je kw ji kx jm ky jq kz ju kr ks kt ku bi translated">CatBoost(<strong class="iz hj">Cat</strong>egory<strong class="iz hj">Boost</strong>ing):使用类别和目标之间的<br/>统计关系处理类别变量</li></ol><p id="e6cd" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">Boosting不同于Bagging，Bagging以并行格式拟合多个独立的模型，每棵树都将在不同的数据子集上创建。两者都降低了单个估计的方差，因为它们组合了来自不同模型的多个估计，所以结果可能是具有更高稳定性的模型。请记住，如果单一模型的难度过大，那么装袋是更好的选择。</p><p id="67ad" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi kc translated"><span class="l kd ke kf bm kg kh ki kj kk di"> S </span>也叫<em class="jv">叠加概括。</em>堆叠结合多个模型，基于完整的训练集训练基础层模型，然后在基础层模型的输出上训练元模型作为特征。基础层通常由不同的学习算法组成，因此堆叠集成通常是异构的。</p><p id="a417" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">所以，首先我们把训练数据分成K倍，就像K倍交叉验证一样。然后，我们对不同的基本型号重复以下步骤:</p><p id="a6cb" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">在K-1个部分上拟合基础模型，并对第K个部分进行预测。我们对训练数据的每一部分都这样做，并且基础模型在整个训练数据集上拟合，以计算它在测试集上的性能。</p><p id="d416" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">然后，我们使用来自训练集的预测作为第二级模型的特征。最后，使用第二层模型对测试集进行预测。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/d259e1b1a19b471282c35522eade4511.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*i57BUitipkZXuijZ"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">罗伯特·阿纳施在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。堆叠级别:)</figcaption></figure><p id="47f8" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi kc translated"><span class="l kd ke kf bm kg kh ki kj kk di"> B </span> lending:类似于堆叠的方法，但是仅使用来自训练集的维持(验证)集来进行预测，并且仅在维持集上进行预测。与堆叠相比，它更简单，信息泄露的风险也更低。</p><p id="7e57" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">因此，首先我们将训练集分成训练集和验证集。然后在训练集上训练基本模型，并只在验证集和测试集上进行预测。验证预测是进行最终模型预测的特征。</p><h1 id="cb89" class="lb lc hi bd iv ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx bi translated">外卖食品</h1><p id="56a7" class="pw-post-body-paragraph ix iy hi iz b ja ly jc jd je lz jg jh ji ma jk jl jm mb jo jp jq mc js jt ju hb bi translated">在这篇文章中，我写了集成学习的基本概述和一些主要概念:引导，打包，随机森林，提升，堆叠和混合。在这些模型的背后，还有很多代码和数学。但这将是一个良好的开端！</p><p id="05cc" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">这篇文章的主要观点如下:</p><ul class=""><li id="d35e" class="km kn hi iz b ja jb je jf ji ko jm kp jq kq ju md ks kt ku bi translated">集成学习使用多个模型(弱学习器/基础模型)来解决同一个问题，然后将它们组合起来以获得更好的性能</li><li id="e008" class="km kn hi iz b ja kv je kw ji kx jm ky jq kz ju md ks kt ku bi translated">在Bagging(引导聚集)中，在训练数据的不同子样本(引导)上训练平行独立的相同基础模型，然后在某种“平均”过程中聚集。它适用于高方差低偏差模型。</li><li id="b52e" class="km kn hi iz b ja kv je kw ji kx jm ky jq kz ju md ks kt ku bi translated">在Boosting中，第一个模型在整个数据集上进行训练。然后，它依次建立后续模型，并拟合前一个模型的残差。它通过缓慢增长来追求方差，通过将许多弱模型组合成一个“超级模型”来追求偏差。</li><li id="dd58" class="km kn hi iz b ja kv je kw ji kx jm ky jq kz ju md ks kt ku bi translated">在堆叠中，基于完整的训练集训练基础层模型，然后基于基础层模型的输出作为特征训练元模型。</li><li id="24df" class="km kn hi iz b ja kv je kw ji kx jm ky jq kz ju md ks kt ku bi translated">在混合中，它类似于堆叠，但仅使用训练集中的维持集来进行预测，并且仅在维持集上进行预测。与堆叠相比，它更简单，信息泄露的风险也更低。</li></ul><p id="4c64" class="pw-post-body-paragraph ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju hb bi translated">最后，集成学习可以使用不同的模型和方法来获得针对特定问题的更好的性能。我们只需要很好地理解问题，更有创造力。</p></div></div>    
</body>
</html>