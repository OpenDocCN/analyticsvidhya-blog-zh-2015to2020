<html>
<head>
<title>Hands-On Tutorial to Analyze Data using Spark SQL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Spark SQL分析数据的实践教程</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/hands-on-tutorial-to-analyze-data-using-spark-sql-b5b7e7a9e605?source=collection_archive---------23-----------------------#2020-02-06">https://medium.com/analytics-vidhya/hands-on-tutorial-to-analyze-data-using-spark-sql-b5b7e7a9e605?source=collection_archive---------23-----------------------#2020-02-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8ccf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">几乎所有组织都使用关系数据库来完成各种任务，从管理和跟踪大量信息到组织和处理事务。这是我们在编码学校学到的第一个概念。</p><p id="79bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们对此心存感激，因为这是数据科学家技能组合中至关重要的一环！如果不知道数据库是如何工作的，你就无法生存。这是任何机器学习项目的一个关键方面。</p><p id="e6e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://courses.analyticsvidhya.com/courses/structured-query-language-sql-for-data-science" rel="noopener ugc nofollow" target="_blank">结构化查询语言(SQL) </a>很容易成为数据库中最流行的语言。不像其他编程语言，它很容易学习，并帮助我们开始我们的数据提取过程。对于大多数数据科学工作来说，对SQL的熟练程度高于大多数其他编程语言。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/e1e78fbbcab9a072decdd6df2ebf0e36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ebpCe13QBkLl24zB.jpg"/></div></div></figure><p id="9f4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是SQL有一个重大的挑战——当您处理海量数据集时，您将很难让它工作。这就是Spark SQL占据首要位置并弥合差距的地方。我将在下一节中详细介绍这一点。</p><p id="452c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本实践教程将向您介绍Spark SQL的世界，它是如何工作的，它提供了哪些不同的特性，以及如何使用Python实现它。我们还将讨论一个你在面试中经常会遇到的重要概念——catalyst optimizer。</p><p id="75e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们开始吧！</p><h1 id="6742" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">扩展关系数据库的挑战</h1><p id="3fb3" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">问题是你为什么要学习Spark SQL？我之前简单地提到了这一点，但是现在让我们更详细地看一下。</p><p id="1618" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个大型(机器学习)项目的关系数据库包含数百或数千个表，并且一个表中的大多数特征被映射到一些其他表中的一些其他特征。这些数据库被设计成只在一台机器上运行，以便维护表映射的规则并避免分布式计算的问题。</p><p id="2119" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当组织希望使用这种设计进行扩展时，这通常会成为一个问题。它需要更复杂、更昂贵的硬件，以及更高的处理能力和存储能力。你可以想象，<strong class="ih hj">从简单的硬件升级到复杂的硬件是极具挑战性的。</strong></p><blockquote class="kt ku kv"><p id="d21b" class="if ig kw ih b ii ij ik il im in io ip kx ir is it ky iv iw ix kz iz ja jb jc hb bi translated">一个组织可能需要将其网站离线一段时间，以进行任何必要的更改。在此期间，他们可能会失去潜在的新客户。</p></blockquote><p id="f999" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，随着数据量的增加，组织很难使用传统的关系数据库来处理如此大量的数据。这就是Spark SQL的用武之地。</p><h1 id="9a58" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">Spark SQL概述</h1><blockquote class="kt ku kv"><p id="7b94" class="if ig kw ih b ii ij ik il im in io ip kx ir is it ky iv iw ix kz iz ja jb jc hb bi translated"><em class="hi">“没有大数据分析，公司就像瞎子和聋子一样，在网络上游荡，就像高速公路上的小鹿。”</em></p><p id="b148" class="if ig kw ih b ii ij ik il im in io ip kx ir is it ky iv iw ix kz iz ja jb jc hb bi translated"><em class="hi"> ~杰弗里·摩尔</em></p></blockquote><p id="aa6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Hadoop和Map Reduce框架在大数据分析领域已经存在了很长时间。但是这些框架需要在硬盘上进行大量的读写操作，这使得它在时间和速度上非常昂贵。</p><p id="41a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://www.analyticsvidhya.com/blog/2016/09/comprehensive-introduction-to-apache-spark-rdds-dataframes-using-pyspark/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>是当今企业中最有效的数据处理框架。诚然，Spark的成本很高，因为它需要大量RAM来进行内存计算，但它仍然是数据科学家和大数据工程师的最爱。</p><p id="92b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Spark生态系统中，我们有以下组件:</p><ol class=""><li id="589f" class="la lb hi ih b ii ij im in iq lc iu ld iy le jc lf lg lh li bi translated"><strong class="ih hj"> MLlib: </strong>这是Spark的可扩展机器学习库，为回归、聚类、分类等提供高质量的算法。您可以使用本文开始使用Spark的MLlib构建机器学习管道:<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2019/11/build-machine-learning-pipelines-pyspark/" rel="noopener ugc nofollow" target="_blank">如何使用PySpark构建机器学习管道？</a></li><li id="44fd" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><strong class="ih hj"> Spark Streaming: </strong>我们现在正在以前所未有的速度和规模生成数据。我们如何确保我们的机器学习管道在数据生成和收集后立即继续产生结果？<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2019/12/streaming-data-pyspark-machine-learning-model/" rel="noopener ugc nofollow" target="_blank">了解如何使用机器学习模型对使用PySpark的流数据进行预测？</a></li><li id="c21a" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">GraphX: 它是一个用于图形的Spark API，一个支持并行图形计算的网络图形引擎</li><li id="097a" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><strong class="ih hj"> Spark SQL: </strong>这是Spark提供的用于结构化数据处理的分布式框架</li></ol><p id="1737" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们知道，在关系数据库中，不同变量之间的关系以及不同的表也被存储和设计成能够处理复杂查询的方式。</p><p id="721d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark SQL是关系处理和Spark函数式编程的完美结合。它提供了对各种数据源的支持，并使SQL查询成为可能，从而为大规模分析结构化数据提供了一个非常强大的工具。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lo"><img src="../Images/74d997f7bf18fc09ff65d552111c9ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/0*ccy_fMT1kaF4cVXQ.png"/></div></figure><h1 id="98c5" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">Spark SQL的特性</h1><p id="50bd" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">Spark SQL拥有大量出色的特性，但我想重点介绍几个您在工作中会经常用到的关键特性:</p><ol class=""><li id="a317" class="la lb hi ih b ii ij im in iq lc iu ld iy le jc lf lg lh li bi translated"><strong class="ih hj">在Spark程序中查询结构化数据:</strong>大多数人可能已经熟悉SQL了。因此，使用Spark不需要学习如何在Python或Scala中定义复杂函数。您可以使用完全相同的查询来获得更大数据集的结果！</li><li id="66cb" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><strong class="ih hj">兼容Hive: </strong>不仅是SQL，还可以使用Spark SQL引擎运行相同的Hive查询。它允许与当前的配置单元查询完全兼容</li><li id="15ec" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><strong class="ih hj">访问数据的一种方式:</strong>在典型的企业级项目中，您没有公共的数据源。相反，您需要处理多种类型的文件和数据库。Spark SQL支持几乎所有类型的文件，并为您提供了一种访问各种数据源的通用方法，如Hive、Avro、Parquet、JSON和JDBC</li><li id="ad19" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><strong class="ih hj">性能和可伸缩性:</strong>在处理大型数据集时，在查询运行期间可能会出现错误。Spark SQL支持完整的中间查询容错，因此我们甚至可以同时处理一千个节点</li><li id="f3fc" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><strong class="ih hj">用户定义的函数:</strong> UDF是Spark SQL的一个特性，它定义了新的基于列的函数，扩展了Spark SQL的词汇表，用于转换数据集</li></ol><h1 id="3f40" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">Spark SQL如何执行查询？</h1><p id="b50e" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">Spark SQL本质上是如何工作的？我们先来了解这一节的流程。</p><ul class=""><li id="03ce" class="la lb hi ih b ii ij im in iq lc iu ld iy le jc lp lg lh li bi translated"><strong class="ih hj">分析:</strong>首先，当你查询某个东西时，Spark SQL会找到需要计算的关系。它是使用抽象语法树(AST)计算的，它检查用于定义查询的元素的正确用法，然后创建一个逻辑计划来执行查询</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lq"><img src="../Images/0d972629dd5f158266fd4c72af8f2fd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tTBKN52DEeRjVxnD.png"/></div></div></figure><p id="2a4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://www.waitingforcode.com/apache-spark-sql/writing-custom-optimization-apache-spark-sql-parser/read" rel="noopener ugc nofollow" target="_blank">图像来源</a></p><ul class=""><li id="7a6c" class="la lb hi ih b ii ij im in iq lc iu ld iy le jc lp lg lh li bi translated"><strong class="ih hj">逻辑优化:</strong>在下一步中，基于规则的优化将应用于逻辑计划。它使用的技术包括:如果查询包含一个<strong class="ih hj"> where </strong>子句，则尽早过滤数据，利用表中可用的索引，因为这可以提高性能，甚至确保不同的数据源以最有效的顺序连接</li><li id="53ce" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lp lg lh li bi translated"><strong class="ih hj">物理规划:</strong>在该步骤中，使用逻辑规划形成一个或多个物理规划。Spark SQL然后选择能够以最有效的方式执行查询的计划，即使用较少的计算资源</li><li id="5a93" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lp lg lh li bi translated"><strong class="ih hj">代码生成:</strong>在最后一步，Spark SQL生成代码。它包括生成在每台机器上运行的Java字节代码。Catalyst使用一种叫做“准引号”的Scala语言特性来简化代码生成</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lr"><img src="../Images/7432ac6b16089ddf6f100b2127ab3470.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DJ32LW3UL9XwkweL.png"/></div></div></figure><h1 id="9d34" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">什么是催化剂优化器？</h1><p id="d707" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">优化意味着升级现有系统或工作流程，使其以更高效的方式工作，同时使用更少的资源。Spark SQL中实现了一个称为<strong class="ih hj"> Catalyst Optimizer </strong>的优化器，它支持<strong class="ih hj">基于规则和基于成本的优化技术</strong>。</p><p id="ea00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在基于规则的优化中，我们定义了一组规则，这些规则将决定如何执行查询。它将以更好的方式重写现有的查询以提高性能。</p><p id="6236" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，假设表上有一个索引。然后，索引将用于根据规则执行查询，如果可能，过滤器将首先应用于初始数据(而不是在最后应用)。</p><p id="2814" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，在某些情况下，索引的使用会降低查询速度。我们知道，一套明确的规则并不总是能够做出伟大的决策，对吗？</p><blockquote class="kt ku kv"><p id="9207" class="if ig kw ih b ii ij ik il im in io ip kx ir is it ky iv iw ix kz iz ja jb jc hb bi translated"><em class="hi">问题来了——基于规则的优化没有考虑数据分布。这就是我们求助于基于成本的优化器的地方。它使用关于表、其索引和数据分布的统计信息来做出更好的决策。</em></p></blockquote><h1 id="d66c" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">使用Spark执行SQL命令</h1><p id="d3ef" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">该编码了！</p><p id="6b2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我创建了一个2500万行的随机数据集。你可以点击下载<a class="ae jd" href="https://drive.google.com/open?id=1sy5G4Y1RnlndLK86jzcV3CZJcTeo8Foj" rel="noopener ugc nofollow" target="_blank">整个数据集。我们有一个用逗号分隔值的文本文件。因此，首先，我们将导入所需的库，读取数据集，并查看Spark如何将数据划分为分区:</a></p><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="a2cb" class="lx jr hi lt b fi ly lz l ma mb"># importing required libraries<br/>from pyspark.sql import SQLContext<br/>from pyspark.sql import Row</span><span id="3d8d" class="lx jr hi lt b fi mc lz l ma mb"># read the text data<br/>raw_data = sc.textFile('sample_data_final_wh.txt').cache()</span><span id="8458" class="lx jr hi lt b fi mc lz l ma mb"># get number of partitions<br/>raw_data.getNumPartitions()<br/>## &gt;&gt; 19</span><span id="4134" class="lx jr hi lt b fi mc lz l ma mb"># view top 2 rows<br/>raw_data.take(2)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es md"><img src="../Images/48d2fb0ebc3c29f1390c55aa2a3a7be1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/0*w5-HLRfqsfcZhI46.png"/></div></figure><p id="b493" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，</p><ul class=""><li id="5c19" class="la lb hi ih b ii ij im in iq lc iu ld iy le jc lp lg lh li bi translated">每行的第一个值是人的年龄(需要是整数)</li><li id="9752" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lp lg lh li bi translated">第二个值是人的血型(需要是字符串)</li><li id="3570" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lp lg lh li bi translated">第三和第四个值是城市和性别(都是字符串)，以及</li><li id="6a32" class="la lb hi ih b ii lj im lk iq ll iu lm iy ln jc lp lg lh li bi translated">最后一个值是一个id(整数类型)</li></ul><p id="21b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用Spark行将每一行的数据映射到特定的数据类型和名称:</p><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="a31b" class="lx jr hi lt b fi ly lz l ma mb"># create spark sql context<br/>sql_context = SQLContext(sc)</span><span id="f48b" class="lx jr hi lt b fi mc lz l ma mb"># split the data<br/>csv_rdd = raw_data.map(lambda row: row.split(','))</span><span id="8822" class="lx jr hi lt b fi mc lz l ma mb"># top 2 rows<br/>csv_rdd.take(2)</span><span id="3eb3" class="lx jr hi lt b fi mc lz l ma mb"># map the datatypes of each column<br/>parsed = csv_rdd.map(lambda r : Row( age = int(r[0]),<br/>                                     blood_group = r[1],<br/>                                     city = r[2],<br/>                                     gender = r[3],<br/>                                     id_ = int(r[4])))<br/># top 5 rows<br/>parsed.take(5)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es me"><img src="../Images/fa8387f48a27ac5a9119b8730eed0b53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*35lxPs-blMGSHbbC.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mf"><img src="../Images/ef33f34be9e3c9eecb662801fdbb25f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4fcEcpvMGVSjPpCw.png"/></div></div></figure><p id="1db9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们将使用解析后的行创建一个dataframe。我们的目标是通过在数据帧上使用简单的<em class="kw"> groupby </em>函数找到性别变量的值计数:</p><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="e25e" class="lx jr hi lt b fi ly lz l ma mb"># create dataframe <br/>data_frame = sql_context.createDataFrame(parsed)</span><span id="5a03" class="lx jr hi lt b fi mc lz l ma mb"># view the dataframe<br/>data_frame.show(5)</span><span id="9fab" class="lx jr hi lt b fi mc lz l ma mb"># value counts of gender<br/>data_frame.groupby('gender').count().show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mg"><img src="../Images/d43cf7eb880c2bbb69213a8cde5ff6c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/0*TkLCt79CYmZ1kzqu.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mh"><img src="../Images/73224ef5c2dfee1679ac91bae54e6d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/0*Q-uDvrM0M3HWG9nN.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mi"><img src="../Images/9d5ed64fbde6717cb721d8bcb39e3c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kZgWo2G5CdHr9NQw.png"/></div></div></figure><p id="bfa3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在dataframe上使用groupby函数计算2500万行的值计数大约需要26毫秒。您可以在Jupyter笔记本的特定单元格中使用<em class="kw"> %%time </em>来计算时间。</p><p id="7a03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将使用Spark SQL执行相同的查询，看看它是否能提高性能。</p><p id="c12b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，您需要使用函数<strong class="ih hj"> registerTempTable将dataframe注册为一个临时表。</strong>这将创建一个内存中的表，该表的范围仅限于创建它的集群。该临时表的生存期仅限于一个会话。它是使用<a class="ae jd" href="https://forums.databricks.com/questions/400/what-is-the-difference-between-registertemptable-a.html" rel="noopener ugc nofollow" target="_blank"> Hive的内存列格式</a>存储的，这种格式针对关系数据进行了高度优化。</p><p id="a165" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，如果您熟悉SQL，您甚至不需要编写复杂的函数来获得结果！在这里，您只需要传递相同的SQL查询，就可以在更大的数据上获得想要的结果:</p><pre class="jf jg jh ji fd ls lt lu lv aw lw bi"><span id="354f" class="lx jr hi lt b fi ly lz l ma mb"># register temporary table<br/>data_frame.registerTempTable('sample')</span><span id="c30c" class="lx jr hi lt b fi mc lz l ma mb"># get the value count using the sql query<br/>gender = sql_context.sql(" SELECT gender, count(*) as freq from sample GROUP BY gender ")</span><span id="da9e" class="lx jr hi lt b fi mc lz l ma mb"># view the results<br/>gender.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mh"><img src="../Images/07ba8e724ba125b04a0b98bfc247ecce.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/0*C8KDhGGEfqbiPDE_.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mj"><img src="../Images/bd5ed4dac57a9db0fd3e96dc7c138c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qd6ozoYsUKwK-Act.png"/></div></div></figure><p id="2d7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算值计数仅用了大约18 ms。这甚至比火花数据帧快得多。</p><p id="2ae6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们将执行另一个SQL查询来计算一个城市的平均年龄:</p><p id="451b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看<a class="ae jd" href="https://gist.github.com/lakshay-arora/c2daf526af67398023234c51934b5693" rel="noopener ugc nofollow" target="_blank">要点</a>上的代码。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mk"><img src="../Images/05f09759f8991f3dc5423dcc379a321c.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/0*bhjK11jsqErYXUEE.png"/></div></figure><h1 id="64d4" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">Apache Spark大规模使用案例</h1><p id="434a" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">我们知道脸书每月有超过20亿活跃用户，随着数据的增多，他们面临着同样复杂的挑战。对于单个查询，他们需要在单个查询中分析数十TB的数据。脸书认为，Spark已经成熟到我们可以在许多批处理用例中将其与Hive进行比较的程度。</p><p id="366d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我用脸书的一个案例来说明这一点。他们的任务之一是为脸书在各种在线服务中使用的实体排名准备功能。早期，他们使用基于Hive的基础设施，这是资源密集型的，维护起来很困难，因为流水线被分割成数百个Hive作业。然后，他们用Spark构建了一个更快、更易管理的管道。你可以在这里阅读他们的完整旅程<a class="ae jd" href="https://engineering.fb.com/core-data/apache-spark-scale-a-60-tb-production-use-case/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="3041" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们比较了Spark和Hive Pipeline的结果。这是一个关于延迟(作业的端到端运行时间)的比较图，它清楚地表明Spark比Hive快得多。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ml"><img src="../Images/07edabe13514dcc3ef71c44cf5483a3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/0*-kEi9l9pUjByGA5F.png"/></div></figure><h1 id="680e" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">结束注释</h1><p id="9957" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">在本文中，我们已经介绍了Spark SQL背后的核心思想，并学习了如何利用它。我们还使用了一个大型数据集，并在Python中应用了我们的知识。</p><p id="b2d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark SQL对于许多数据科学爱好者来说是一个相对陌生的领域，但它会在你的行业角色甚至面试中派上用场。在招聘经理看来，这是一个相当重要的附加条件。</p></div><div class="ab cl mm mn gp mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hb hc hd he hf"><p id="4f11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kw">原载于2020年2月6日</em><a class="ae jd" href="https://www.analyticsvidhya.com/blog/2020/02/hands-on-tutorial-spark-sql-analyze-data/" rel="noopener ugc nofollow" target="_blank"><em class="kw">【https://www.analyticsvidhya.com】</em></a><em class="kw">。</em></p></div></div>    
</body>
</html>