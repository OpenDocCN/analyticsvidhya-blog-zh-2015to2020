# äº†è§£å¤šå…ƒçº¿æ€§å›å½’ã€‚

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/understanding-multiple-linear-regression-e5cc68bef652?source=collection_archive---------10----------------------->

å¤šå…ƒçº¿æ€§å›å½’ä¸­çš„æœ¯è¯­â€œ**å¤šé‡**â€è¡¨ç¤ºä¸¤ä¸ªæˆ–å¤šä¸ªç‹¬ç«‹è¾“å…¥å˜é‡ä¸ä¸€ä¸ªå“åº”å˜é‡ä¹‹é—´çš„å…³ç³»ã€‚

å½“ä¸€ä¸ªå˜é‡ä¸è¶³ä»¥åˆ›å»ºå¥½çš„æ¨¡å‹å¹¶åšå‡ºå‡†ç¡®çš„é¢„æµ‹æ—¶ï¼Œå°±éœ€è¦å¤šå…ƒçº¿æ€§å›å½’ã€‚

è®©æˆ‘ä»¬ä»ä¸€ä¸ªä½æˆ¿æ•°æ®é›†å¼€å§‹ç†è§£å®ƒâ€¦

# é—®é¢˜é™ˆè¿°ã€‚

å‡è®¾ä¸€å®¶æˆ¿åœ°äº§å…¬å¸æœ‰ä¸€ä¸ªåŒ…å«å¾·é‡Œåœ°åŒºæˆ¿äº§ä»·æ ¼çš„æ•°æ®é›†ã€‚å®ƒå¸Œæœ›åˆ©ç”¨è¿™äº›æ•°æ®ï¼Œæ ¹æ®é¢ç§¯ã€å§å®¤ã€åœè½¦åœºç­‰é‡è¦å› ç´ ï¼Œä¼˜åŒ–æˆ¿äº§çš„é”€å”®ä»·æ ¼ã€‚

***æœ¬è´¨ä¸Šï¼Œå…¬å¸æƒ³è¦â€”***

*   ç¡®å®šå½±å“æˆ¿ä»·çš„å˜é‡ï¼Œå¦‚é¢ç§¯ã€æˆ¿é—´æ•°é‡ã€æµ´å®¤ç­‰ã€‚
*   åˆ›å»ºä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼Œå°†æˆ¿ä»·ä¸æˆ¿é—´æ•°é‡ã€é¢ç§¯ã€æµ´å®¤æ•°é‡ç­‰å˜é‡å®šé‡è”ç³»èµ·æ¥ã€‚
*   äº†è§£æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œå³è¿™äº›å˜é‡å¯¹æˆ¿ä»·çš„é¢„æµ‹èƒ½åŠ›ã€‚

***æˆ‘ä»¬å¼€å§‹ç¼–ç å§â€¦â€¦..***

# æ­¥éª¤ 1:é˜…è¯»å’Œç†è§£æ•°æ®

è®©æˆ‘ä»¬é¦–å…ˆå¯¼å…¥ NumPy å’Œ Pandas å¹¶è¯»å–ä½æˆ¿æ•°æ®é›†ã€‚

***å¯¼å…¥æ‰€éœ€çš„åº“***

```
# importing required librariesimport numpy as np
import pandas as pd
```

***è¯»å–æ•°æ®é›†***

```
housing = pd.read_csv("Housing.csv")
```

***æ˜¾ç¤ºæ•°æ®é›†***

```
# Check the head of the dataset
housing.head()
```

![](img/729fe20f8430f5ed8332c04c90b6431b.png)

# æ­¥éª¤ 2:å¯è§†åŒ–æ•°æ®

ç°åœ¨è®©æˆ‘ä»¬èŠ±ä¸€äº›æ—¶é—´æ¥åšå¯ä»¥è¯´æ˜¯æœ€é‡è¦çš„ä¸€æ­¥â€”â€”ç†è§£æ•°æ®ã€‚

*   å¦‚æœå­˜åœ¨æ˜æ˜¾çš„å¤šé‡å…±çº¿æ€§ï¼Œè¿™æ˜¯å‘ç°å®ƒçš„ç¬¬ä¸€ä¸ªåœ°æ–¹
*   åœ¨è¿™é‡Œï¼Œæ‚¨è¿˜å¯ä»¥ç¡®å®šä¸€äº›é¢„æµ‹å› ç´ æ˜¯å¦ä¸ç»“æœå˜é‡ç›´æ¥ç›¸å…³

æˆ‘ä»¬å°†ä½¿ç”¨`matplotlib`å’Œ`seaborn`æ¥å¯è§†åŒ–æˆ‘ä»¬çš„æ•°æ®ã€‚

```
import matplotlib.pyplot as plt
import seaborn as sns
```

## å¯è§†åŒ–æ•°å­—å˜é‡

è®©æˆ‘ä»¬åšä¸€ä¸ªæ‰€æœ‰æ•°å­—å˜é‡çš„é…å¯¹å›¾ã€‚

```
sns.pairplot(housing)
plt.show()
```

![](img/7642f2c61dd519c6f27662d783ceddb0.png)

æ•°å€¼å˜é‡çš„é…å¯¹å›¾ã€‚

## å¯è§†åŒ–åˆ†ç±»å˜é‡

æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œè¿˜æœ‰ä¸€äº›åˆ†ç±»å˜é‡ã€‚è®©æˆ‘ä»¬ä¸ºè¿™äº›å˜é‡åšä¸€ä¸ªç®±çº¿å›¾ã€‚

```
plt.figure(figsize=(20, 12))
plt.subplot(2,3,1)
sns.boxplot(x = 'mainroad', y = 'price', data = housing)
plt.subplot(2,3,2)
sns.boxplot(x = 'guestroom', y = 'price', data = housing)
plt.subplot(2,3,3)
sns.boxplot(x = 'basement', y = 'price', data = housing)
plt.subplot(2,3,4)
sns.boxplot(x = 'hotwaterheating', y = 'price', data = housing)
plt.subplot(2,3,5)
sns.boxplot(x = 'airconditioning', y = 'price', data = housing)
plt.subplot(2,3,6)
sns.boxplot(x = 'furnishingstatus', y = 'price', data = housing)
plt.show()
```

![](img/3ad4f7959ba6e6d0602b5dbab42ff1ba.png)

å„ç§åˆ†ç±»å˜é‡çš„ç®±çº¿å›¾

æˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ä½¿ç”¨`hue`è®ºè¯æ¥å¹³è¡Œåœ°æƒ³è±¡è¿™äº›åˆ†ç±»ç‰¹å¾ã€‚ä¸‹é¢æ˜¯ä»¥`airconditioning`ä¸ºè‰²è°ƒçš„`furnishingstatus`çš„å‰§æƒ…ã€‚

```
plt.figure(figsize = (10, 5))
sns.boxplot(x = 'furnishingstatus', y = 'price', hue=airconditioning', data = housing)
plt.show()
```

![](img/a1687bf339011f804a80292abdd55ba3.png)

# ç¬¬ä¸‰æ­¥:æ•°æ®å‡†å¤‡

*   æ‚¨å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ•°æ®é›†æœ‰è®¸å¤šå€¼ä¸ºâ€œæ˜¯â€æˆ–â€œå¦â€çš„åˆ—ã€‚
*   ä½†æ˜¯ä¸ºäº†æ‹Ÿåˆå›å½’çº¿ï¼Œæˆ‘ä»¬éœ€è¦æ•°å€¼è€Œä¸æ˜¯å­—ç¬¦ä¸²ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°†å®ƒä»¬è½¬æ¢æˆ 1 å’Œ 0ï¼Œå…¶ä¸­ 1 è¡¨ç¤ºâ€œæ˜¯â€ï¼Œ0 è¡¨ç¤ºâ€œå¦â€ã€‚

```
# List of variables to mapvarlist =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']# Defining the map function
def binary_map(x):
    return x.map({'yes': 1, "no": 0})# Applying the function to the housing list
housing[varlist] = housing[varlist].apply(binary_map)# Check the housing dataframe nowhousing.head()
```

![](img/ec39e9b3464140a0af85549560d50cca.png)

# è™šæ‹Ÿå˜é‡

å˜é‡`furnishingstatus`æœ‰ä¸‰ä¸ªçº§åˆ«ã€‚æˆ‘ä»¬è¿˜éœ€è¦å°†è¿™äº›çº§åˆ«è½¬æ¢æˆæ•´æ•°ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå«åš`dummy variables`çš„ä¸œè¥¿ã€‚

```
# Get the dummy variables for the feature 'furnishingstatus' and store it in a new variable - 'status'status = pd.get_dummies(housing['furnishingstatus'])
```

è¿™é‡Œæˆ‘ä»¬ä¸ºå˜é‡â€œfurnishingstatusâ€åˆ›å»ºè™šæ‹Ÿå˜é‡

```
# Check what the dataset 'status' looks like
status.head()
```

![](img/f5c4f97e7c8bf51066e2da3ddcc13da9.png)

ç°åœ¨ï¼Œä½ ä¸éœ€è¦ä¸‰åˆ—ã€‚æ‚¨å¯ä»¥åˆ é™¤`furnished`åˆ—ï¼Œå› ä¸ºåªéœ€æœ€åä¸¤åˆ—å³å¯è¯†åˆ«å®¶å…·ç±»å‹ï¼Œå…¶ä¸­â€”

*   `00`å°†å¯¹åº”`furnished`
*   `01`å°†å¯¹åº”`unfurnished`
*   `10`å°†å¯¹åº”äº`semi-furnished`

è¿™æ˜¯ä¸ºäº†é¿å…å†—ä½™å’Œå¤šé‡å…±çº¿æ€§æ•ˆåº”ã€‚

```
# Let's drop the first column from status df using 'drop_first = True'status = pd.get_dummies(housing['furnishingstatus'], drop_first = True)
```

åœ¨è¿™é‡Œæˆ‘ä»¬åˆ é™¤äº† ***å¸¦å®¶å…·çš„*** å˜é‡

```
# Add the results to the original housing dataframehousing = pd.concat([housing, status], axis = 1)
```

å°†æ–°çš„è™šæ‹Ÿå˜é‡è¿æ¥åˆ°æ•°æ®é›†

```
# Drop 'furnishingstatus' as we have created the dummies for ithousing.drop(['furnishingstatus'], axis = 1, inplace = True)
```

åˆ é™¤***furnishingstatus***ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»ä¸ºå®ƒåˆ›å»ºäº†è™šæ‹Ÿå˜é‡ã€‚

```
housing.head()
```

![](img/b3e41658d67f1b476707db7bc3f21168.png)

# æ­¥éª¤ 4:å°†æ•°æ®åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†

å¦‚ä½ æ‰€çŸ¥ï¼Œå›å½’çš„ç¬¬ä¸€ä¸ªåŸºæœ¬æ­¥éª¤æ˜¯æ‰§è¡Œè®­ç»ƒæµ‹è¯•åˆ†å‰²ã€‚

```
from sklearn.model_selection import train_test_split# We specify this so that the train and test data set always have the same rows, respectivelynp.random.seed(0)df_train, df_test = train_test_split(housing, train_size = 0.7, test_size = 0.3, random_state = 100)
```

# é‡æ–°ç¼©æ”¾è¦ç´ 

è¿™é‡Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œé™¤äº†`area`ï¼Œæ‰€æœ‰çš„åˆ—éƒ½æ˜¯å°æ•´æ•°å€¼ã€‚å› æ­¤ï¼Œé‡æ–°è°ƒæ•´å˜é‡ä»¥ä½¿å®ƒä»¬å…·æœ‰å¯æ¯”è¾ƒçš„è§„æ¨¡æ˜¯æå…¶é‡è¦çš„ã€‚

å¦‚æœæˆ‘ä»¬æ²¡æœ‰å¯æ¯”è¾ƒçš„å°ºåº¦ï¼Œé‚£ä¹ˆé€šè¿‡æ‹Ÿåˆå›å½’æ¨¡å‹è·å¾—çš„ä¸€äº›ç³»æ•°ä¸å…¶ä»–ç³»æ•°ç›¸æ¯”å¯èƒ½éå¸¸å¤§æˆ–éå¸¸å°ã€‚

åœ¨æ¨¡å‹è¯„ä¼°æ—¶ï¼Œè¿™å¯èƒ½ä¼šå˜å¾—éå¸¸çƒ¦äººã€‚å› æ­¤ï¼Œå»ºè®®ä½¿ç”¨æ ‡å‡†åŒ–æˆ–è§„èŒƒåŒ–ï¼Œä»¥ä¾¿è·å¾—çš„ç³»æ•°å•ä½éƒ½åœ¨åŒä¸€æ ‡åº¦ä¸Šã€‚

æ­£å¦‚æˆ‘ä»¬æ‰€çŸ¥ï¼Œæœ‰ä¸¤ç§å¸¸è§çš„é‡æ–°è°ƒæ•´æ–¹æ³•:

1.  æœ€å°-æœ€å¤§ç¼©æ”¾
2.  æ ‡å‡†åŒ–(å¹³å‡å€¼-0ï¼Œè¥¿æ ¼ç›-1)

è¿™ä¸€æ¬¡ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æœ€å°æœ€å¤§ç¼©æ”¾ã€‚

```
from sklearn.preprocessing import MinMaxScaler
```

å°† scaler()åº”ç”¨äºé™¤â€œæ˜¯-å¦â€å’Œâ€œè™šæ‹Ÿâ€å˜é‡ä¹‹å¤–çš„æ‰€æœ‰åˆ—

```
scaler = MinMaxScaler()# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variablesnum_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price']df_train[num_vars] = scaler.fit_transform(df_train[num_vars])
```

è®©æˆ‘ä»¬æ£€æŸ¥ç›¸å…³ç³»æ•°ï¼Œçœ‹çœ‹å“ªäº›å˜é‡æ˜¯é«˜åº¦ç›¸å…³çš„ã€‚

```
# Let's check the correlation coefficients to see which variables are highly correlatedplt.figure(figsize = (16, 10))
sns.heatmap(df_train.corr(), annot = True, cmap="YlGnBu")
plt.show()
```

![](img/0be1c5a25ebe70514de221865c8edd29.png)

åˆ—è½¦æ•°æ®çƒ­å›¾ã€‚

ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°äº†ï¼Œ`area`ä¼¼ä¹ä¸`price`æœ€ç›¸å…³ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹`area`å’Œ`price`çš„é…å¯¹å›¾ã€‚

```
plt.figure(figsize=[6,6])
plt.scatter(df_train.area, df_train.price)
plt.show()
```

![](img/f108e61fba5975bd946d7609f7cd90b2.png)

x è½´ä»£è¡¨é¢ç§¯ï¼Œy è½´ä»£è¡¨ä»·æ ¼

å› æ­¤ï¼Œæˆ‘ä»¬é€‰æ‹©`area`ä½œä¸ºç¬¬ä¸€ä¸ªå˜é‡ï¼Œå¹¶å°è¯•æ‹Ÿåˆä¸€æ¡å›å½’çº¿ã€‚

# åˆ†ä¸º X å’Œ Y ä¸¤ä¸ªé›†åˆè¿›è¡Œå»ºæ¨¡

```
y_train = df_train.pop('price')
X_train = df_train
```

# æ­¥éª¤ 5:å»ºç«‹çº¿æ€§æ¨¡å‹

ä½¿ç”¨`statsmodels`é€šè¿‡è®­ç»ƒæ•°æ®æ‹Ÿåˆä¸€æ¡å›å½’çº¿ã€‚

è¯·è®°ä½ï¼Œåœ¨`statsmodels`ä¸­ï¼Œæ‚¨éœ€è¦ä½¿ç”¨`sm.add_constant(X)`æ˜ç¡®æ‹Ÿåˆä¸€ä¸ªå¸¸æ•°ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬ä¸æ‰§è¡Œè¿™ä¸ªæ­¥éª¤ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œ`statsmodels`ä¼šæ‹Ÿåˆä¸€æ¡ç©¿è¿‡åŸç‚¹çš„å›å½’çº¿ã€‚

```
import statsmodels.api as sm# Add a constant
X_train_lm = sm.add_constant(X_train[['area']])# Create a first fitted model
lr = sm.OLS(y_train, X_train_lm).fit()
```

lr.params è¿”å›å˜é‡çš„ç³»æ•°ã€‚

```
# Check the parameters obtainedlr.params
```

![](img/f3433737d82246a64a3f87fb16492f1f.png)

```
# Let's visualise the data with a scatter plot and the fitted regression lineplt.scatter(X_train_lm.iloc[:, 1], y_train)
plt.plot(X_train_lm.iloc[:, 1], 0.127 + 0.462*X_train_lm.iloc[:, 1], 'r')
plt.show()
```

![](img/500a19865ae70a78d7d7a235ef5f7802.png)

è®©æˆ‘ä»¬æ‰“å°çº¿æ€§å›å½’æ¨¡å‹çš„æ‘˜è¦ã€‚

```
# Print a summary of the linear regression model obtained
print(lr.summary())
```

![](img/024dfbd2816533c4937709f7b9583735.png)

å¾—åˆ°çš„ R å¹³æ–¹å€¼ä¸º`0.283`ã€‚

# æ·»åŠ å¦ä¸€ä¸ªå˜é‡

æ—¢ç„¶æˆ‘ä»¬æœ‰å¦‚æ­¤å¤šçš„å˜é‡ï¼Œæˆ‘ä»¬æ˜¾ç„¶å¯ä»¥åšå¾—æ¯”è¿™æ›´å¥½ã€‚å› æ­¤ï¼Œè®©æˆ‘ä»¬ç»§ç»­æ·»åŠ ç¬¬äºŒé«˜åº¦ç›¸å…³çš„å˜é‡ï¼Œå³`bathrooms`ã€‚

```
# Assign all the feature variables to XX_train_lm = X_train[['area', 'bathrooms']]
```

å¯¼å…¥ç»Ÿè®¡åº“å¹¶æ‹Ÿåˆ OLS

```
# Build a linear modelimport statsmodels.api as sm
X_train_lm = sm.add_constant(X_train_lm)lr = sm.OLS(y_train, X_train_lm).fit()lr.params
```

![](img/c89537cb9ee4e5381c599dc23f7815f2.png)

```
# Check the summary
print(lr.summary())
```

![](img/7e20ef495b7038f091b5a34e4185a0ff.png)

æˆ‘ä»¬æ˜æ˜¾æ”¹è¿›äº†æ¨¡å‹ï¼Œè°ƒæ•´åçš„ R å¹³æ–¹å€¼ä»`0.281`ä¸Šå‡åˆ°`0.477`ã€‚è®©æˆ‘ä»¬ç»§ç»­æ·»åŠ å¦ä¸€ä¸ªå˜é‡`bedrooms`ã€‚

```
# Assign all the feature variables to X
X_train_lm = X_train[['area', 'bathrooms','bedrooms']]# Build a linear modelimport statsmodels.api as sm
X_train_lm = sm.add_constant(X_train_lm)lr = sm.OLS(y_train, X_train_lm).fit()lr.params
```

![](img/b688f79e4ddbbf062b3c09c6cdea9ef0.png)

```
# Print the summary of the modelprint(lr.summary())
```

![](img/7e8d2de288558b864ee7387c9f2a77c0.png)

æˆ‘ä»¬å†æ¬¡æ”¹è¿›äº†è°ƒæ•´åçš„ R å¹³æ–¹ã€‚ç°åœ¨è®©æˆ‘ä»¬ç»§ç»­æ·»åŠ æ‰€æœ‰çš„ç‰¹å¾å˜é‡ã€‚

# å°†æ‰€æœ‰å˜é‡æ·»åŠ åˆ°æ¨¡å‹ä¸­

```
# Check all the columns of the dataframehousing.columns
```

![](img/a8d3b06f49b3791b6e66580da95ceae4.png)

**è®©æˆ‘ä»¬å»ºç«‹ä¸€ä¸ªçº¿æ€§æ¨¡å‹**

```
#Build a linear modelimport statsmodels.api as sm
X_train_lm = sm.add_constant(X_train)lr_1 = sm.OLS(y_train, X_train_lm).fit()lr_1.params
```

![](img/4fe8cf2acc5fe1fd9659d2c8af022b48.png)

```
print(lr_1.summary())
```

![](img/7c9136fba2170557175e23967def4384.png)

æŸ¥çœ‹ p å€¼ï¼Œçœ‹èµ·æ¥æœ‰äº›å˜é‡å¹¶ä¸çœŸæ­£é‡è¦(åœ¨å­˜åœ¨å…¶ä»–å˜é‡çš„æƒ…å†µä¸‹)ã€‚

ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥æ”¾å¼ƒä¸€äº›ï¼Ÿ

æˆ‘ä»¬å¯ä»¥ç®€å•åœ°å»æ‰ p å€¼æœ€é«˜ã€ä¸é‡è¦çš„å˜é‡ã€‚ä¸€ä¸ªæ›´å¥½çš„æ–¹æ³•æ˜¯ç”¨ VIF çš„ä¿¡æ¯æ¥è¡¥å……è¿™ä¸€ç‚¹ã€‚

# æ£€æŸ¥ VIF

æ–¹å·®è†¨èƒ€å› å­æˆ– VIF ç»™å‡ºäº†ä¸€ä¸ªåŸºæœ¬çš„é‡åŒ–æ¦‚å¿µï¼Œå³ç‰¹å¾å˜é‡ä¹‹é—´çš„ç›¸å…³ç¨‹åº¦ã€‚è¿™æ˜¯æ£€éªŒæˆ‘ä»¬çš„çº¿æ€§æ¨¡å‹çš„ä¸€ä¸ªæå…¶é‡è¦çš„å‚æ•°ã€‚è®¡ç®—`VIF`çš„å…¬å¼ä¸º:

***VIF = 1/1-R .***

```
# Check for the VIF values of the feature variables. from statsmodels.stats.outliers_influence import variance_inflation_factor# Create a dataframe that will contain the names of all the feature variables and their respective VIFsvif = pd.DataFrame()vif['Features'] = X_train.columnsvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]vif['VIF'] = round(vif['VIF'], 2)vif = vif.sort_values(by = "VIF", ascending = False)
vif 
```

![](img/bd4e19182a4d96f12232bd26efcc64b6.png)

æˆ‘ä»¬é€šå¸¸å¸Œæœ› VIF å°äº 5ã€‚æ‰€ä»¥å¾ˆæ˜æ˜¾æˆ‘ä»¬éœ€è¦å»æ‰ä¸€äº›å˜é‡ã€‚

ä¸ºä»€ä¹ˆæˆ‘ä»¬è¿˜è¦è€ƒè™‘æ•°å­— 5ï¼Ÿï¼Ÿï¼Ÿæˆ‘æ¥è§£é‡Šâ€¦â€¦

å‡è®¾ VIF=5ã€‚å³

1/1-R = 5

1-R = 1/5

1-R = 0.2

R = 0.8

è¿™æ„å‘³ç€ä»»ä½• VIF åˆ†æ•°å¤§äºç­‰äº 5 çš„å˜é‡éƒ½å¯ä»¥è§£é‡Šæ•°æ®ä¸­ 80%ä»¥ä¸Šçš„å˜å¼‚ã€‚

å¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å¯èƒ½ä¼šé¢ä¸´å¤šé‡å…±çº¿æ€§é—®é¢˜ã€‚

å› æ­¤æœ‰äº†æ•°å­— 5ã€‚

# åˆ é™¤å˜é‡å¹¶æ›´æ–°æ¨¡å‹

ä»æ‘˜è¦å’Œ VIF æ•°æ®æ¡†æ¶ä¸­å¯ä»¥çœ‹å‡ºï¼Œä¸€äº›å˜é‡ä»ç„¶æ— å…³ç´§è¦ã€‚å…¶ä¸­ä¸€ä¸ªå˜é‡æ˜¯`semi-furnished`ï¼Œå› ä¸ºå®ƒå…·æœ‰éå¸¸é«˜çš„ p å€¼`0.938`ã€‚è®©æˆ‘ä»¬ç»§ç»­ä¸‹å»ï¼Œæ”¾å¼ƒè¿™ä¸ªå˜é‡ã€‚

```
# Dropping highly correlated variables and insignificant variablesX = X_train.drop('semi-furnished', 1,)# Build a third fitted model
X_train_lm = sm.add_constant(X)lr_2 = sm.OLS(y_train, X_train_lm).fit()# Print the summary of the model
print(lr_2.summary())
```

![](img/b62c204edf764ae2934b625c42a97bd0.png)

è®©æˆ‘ä»¬å†æ¬¡è®¡ç®— vif åˆ†æ•°ã€‚

```
# Calculate the VIFs again for the new modelvif = pd.DataFrame()vif['Features'] = X.columnsvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]vif['VIF'] = round(vif['VIF'], 2)vif = vif.sort_values(by = "VIF", ascending = False)
vif 
```

![](img/fc53cf9c7d84339533d29b5c8ce14285.png)

# åˆ é™¤å˜é‡å¹¶æ›´æ–°æ¨¡å‹

æ­£å¦‚æ‚¨æ‰€æ³¨æ„åˆ°çš„ï¼Œä¸€äº›å˜é‡å…·æœ‰é«˜ VIF å€¼å’Œé«˜ p å€¼ã€‚è¿™æ ·çš„å˜é‡æ˜¯æ— å…³ç´§è¦çš„ï¼Œåº”è¯¥æ”¾å¼ƒã€‚

æ‚¨å¯èƒ½å·²ç»æ³¨æ„åˆ°ï¼Œå˜é‡`bedroom`å…·æœ‰éå¸¸é«˜çš„ VIF ( `6.6`)å’Œé«˜ p å€¼(`0.206`)ã€‚å› æ­¤ï¼Œè¿™ä¸ªå˜é‡æ²¡æœ‰å¤šå¤§ç”¨å¤„ï¼Œåº”è¯¥åˆ é™¤ã€‚

```
# Dropping highly correlated variables and insignificant variables
X = X.drop('bedrooms', 1)# Build a second fitted model
X_train_lm = sm.add_constant(X)lr_3 = sm.OLS(y_train, X_train_lm).fit()# Print the summary of the modelprint(lr_3.summary())
```

![](img/df7c4640f7f691102397131916a97555.png)

```
# Calculate the VIFs again for the new model
vif = pd.DataFrame()vif['Features'] = X.columnsvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]vif['VIF'] = round(vif['VIF'], 2)vif = vif.sort_values(by = "VIF", ascending = False)vif 
```

![](img/7d4b7353d13ecb9491906e3da76699f0.png)

# åˆ é™¤å˜é‡å¹¶æ›´æ–°æ¨¡å‹

æ­£å¦‚ä½ å¯èƒ½å·²ç»æ³¨æ„åˆ°çš„ï¼Œæ‰è½`semi-furnised`ä¹Ÿå‡å°‘äº†`mainroad`çš„ VIFï¼Œæ‰€ä»¥å®ƒç°åœ¨ä½äº 5ã€‚

ä½†æ˜¯ä»æ€»ç»“ä¸­ï¼Œæˆ‘ä»¬ä»ç„¶å¯ä»¥çœ‹åˆ°ä»–ä»¬ä¸­çš„ä¸€äº›äººæœ‰å¾ˆé«˜çš„ p å€¼ã€‚ä¾‹å¦‚ï¼Œ`basement`çš„ p å€¼ä¸º 0.03ã€‚æˆ‘ä»¬ä¹Ÿåº”è¯¥å»æ‰è¿™ä¸ªå˜é‡ã€‚

```
X = X.drop('basement', 1)# Build a fourth fitted modelX_train_lm = sm.add_constant(X)lr_4 = sm.OLS(y_train, X_train_lm).fit()lr_4.params
```

![](img/50b3c8da6fce8b2fb1dd349c915b1c2d.png)

***ç°åœ¨ä½ å¯ä»¥çœ‹åˆ°ï¼ŒVIFs å’Œ p å€¼éƒ½åœ¨å¯æ¥å—çš„èŒƒå›´å†…ã€‚æ‰€ä»¥æˆ‘ä»¬åªä½¿ç”¨è¿™ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹ã€‚***

***æˆ‘ä»¬è®­ç»ƒæ•°æ®é›†çš„æœ€ç»ˆ R åˆ†æ•°æ˜¯ 0.676***

# æ­¥éª¤ 7:è®­ç»ƒæ•°æ®çš„æ®‹å·®åˆ†æ

å› æ­¤ï¼Œç°åœ¨æ£€æŸ¥è¯¯å·®é¡¹æ˜¯å¦ä¹Ÿæ˜¯æ­£æ€åˆ†å¸ƒçš„(äº‹å®ä¸Šï¼Œè¿™æ˜¯çº¿æ€§å›å½’çš„ä¸»è¦å‡è®¾ä¹‹ä¸€)ï¼Œ

è®©æˆ‘ä»¬ç”»å‡ºè¯¯å·®é¡¹çš„ç›´æ–¹å›¾ï¼Œçœ‹çœ‹å®ƒæ˜¯ä»€ä¹ˆæ ·å­ã€‚

```
y_train_price = lr_4.predict(X_train_lm)# Plot the histogram of the error terms
fig = plt.figure()sns.distplot((y_train - y_train_price), bins = 20)fig.suptitle('Error Terms', fontsize = 20)   

plt.xlabel('Errors', fontsize = 18) 
```

![](img/d3cfeff0dd263552af8998d7cc010626.png)

# æ­¥éª¤ 8:ä½¿ç”¨æœ€ç»ˆæ¨¡å‹è¿›è¡Œé¢„æµ‹

æ—¢ç„¶æˆ‘ä»¬å·²ç»æ‹Ÿåˆäº†æ¨¡å‹å¹¶æ£€æŸ¥äº†è¯¯å·®é¡¹çš„æ­£æ€æ€§ï¼Œé‚£ä¹ˆæ˜¯æ—¶å€™ç»§ç»­ä½¿ç”¨æœ€ç»ˆæ¨¡å‹ï¼Œå³ç¬¬å››ä¸ªæ¨¡å‹è¿›è¡Œé¢„æµ‹äº†ã€‚

## å¯¹æµ‹è¯•é›†åº”ç”¨ç¼©æ”¾

```
num_vars = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking','price']df_test[num_vars] = scaler.transform(df_test[num_vars])
```

## åˆ†ä¸º X æ£€éªŒå’Œ y æ£€éªŒ

```
y_test = df_test.pop('price')X_test = df_test# Adding constant variable to test dataframeX_test_m4 = sm.add_constant(X_test)# Creating X_test_m4 dataframe by dropping variables from X_test_m4X_test_m4 = X_test_m4.drop(["bedrooms", "semi-furnished", "basement"], axis = 1)# Making predictions using the fourth modely_pred_m4 = lr_4.predict(X_test_m4)
```

è®¡ç®—æµ‹è¯•æ•°æ®é›†çš„ R åˆ†æ•°

```
from sklearn.metrics import r2_scorer2_score(y_true=y_test,y_pred=y_pred_m4)
```

![](img/ed2f46ef53eec86be1c33aadc2581198.png)

***æˆ‘ä»¬è®­ç»ƒæ•°æ®é›†çš„æœ€ç»ˆ R åˆ†æ•°æ˜¯ 0.676***

***å¯¹äºæˆ‘ä»¬çš„æµ‹è¯•æ•°æ®é›†ï¼Œæˆ‘ä»¬å¾—åˆ°äº† 0.66 çš„ R åˆ†æ•°ã€‚***

è¿™æ„å‘³ç€æˆ‘ä»¬çš„æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šä¹Ÿè¡¨ç°è‰¯å¥½ã€‚

# æ­¥éª¤ 9:æ¨¡å‹è¯„ä¼°

ç°åœ¨è®©æˆ‘ä»¬ç»˜åˆ¶å®é™…å€¼ä¸é¢„æµ‹å€¼çš„å›¾è¡¨ã€‚

```
# Plotting y_test and y_pred to understand the spreadfig = plt.figure()plt.scatter(y_test, y_pred_m4)fig.suptitle('y_test vs y_pred', fontsize = 20)  

plt.xlabel('y_test', fontsize = 18)  

plt.ylabel('y_pred', fontsize = 16)
```

![](img/98248001f48790d323708714ef89e4a0.png)

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæœ€ä½³æ‹Ÿåˆçº¿çš„æ–¹ç¨‹ä¸º:

ğ‘ğ‘Ÿğ‘–ğ‘ğ‘’=**0.236Ã—ğ‘ğ‘Ÿğ‘’ğ‘+0.202Ã—ğ‘ğ‘ğ‘¡â„ğ‘Ÿğ‘œğ‘œğ‘šğ‘ +0.11Ã—ğ‘ ğ‘¡ğ‘œğ‘Ÿğ‘–ğ‘’ğ‘ +0.05Ã—ğ‘šğ‘ğ‘–ğ‘›ğ‘Ÿğ‘œğ‘ğ‘‘+0.04Ã—ğ‘”ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘Ÿğ‘œğ‘œğ‘š+0.0876Ã—â„ğ‘œğ‘¡ğ‘¤ğ‘ğ‘¡ğ‘’ğ‘Ÿâ„ğ‘’ğ‘ğ‘¡ğ‘–ğ‘›ğ‘”+0.0682Ã—ğ‘ğ‘–ğ‘Ÿğ‘ğ‘œğ‘›ğ‘‘ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›ğ‘–ğ‘›ğ‘”+0.0629Ã—ğ‘ğ‘ğ‘Ÿğ‘˜ğ‘–ğ‘›ğ‘”+0.0637Ã—ğ‘ğ‘Ÿğ‘’ğ‘“ğ‘ğ‘Ÿğ‘’ğ‘âˆ’0.0337Ã—ğ‘¢ğ‘›ğ‘“ğ‘¢ğ‘Ÿğ‘›ğ‘–ğ‘ â„ğ‘’ğ‘‘.**

**æ³¨æ„:ä¸ºäº†åˆ é™¤ä¸ç›®æ ‡å˜é‡ç›¸å…³æ€§ä½çš„å˜é‡ï¼Œæˆ‘ä»¬å¯ä»¥*ä½¿ç”¨ sklearn æä¾›çš„é€’å½’ç‰¹å¾æ¶ˆé™¤ã€‚***

**ç»“è®º:**

æˆ‘å¸Œæœ›è¯»è€…å¯¹å¤šå…ƒçº¿æ€§å›å½’æœ‰ä¸€ä¸ªç›´è§‚çš„è®¤è¯†ã€‚