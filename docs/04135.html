<html>
<head>
<title>Enhance learning by Transfering ResNet Architecture into Big Transform Architecture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过将ResNet体系结构转换为大转换体系结构来增强学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/enhance-learning-by-transfering-resnet-architecture-into-big-transform-architecture-44603f537fcf?source=collection_archive---------7-----------------------#2020-03-06">https://medium.com/analytics-vidhya/enhance-learning-by-transfering-resnet-architecture-into-big-transform-architecture-44603f537fcf?source=collection_archive---------7-----------------------#2020-03-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/7c5260c28243ea0dda8ff21c8c669249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*fjwN_lM1uoc01iM-11QYXA.png"/></div></figure><p id="a7b9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我写这篇博客的主要目的是为了简化对这篇<a class="ae jk" href="https://arxiv.org/pdf/1912.11370.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj">论文</strong> </a> <strong class="io hj"> </strong>的理解，这篇论文提到通过改变ResNet-101架构的一些调谐旋钮(超参数)来增强其功能，使该模型对于新的图像数据集更加健壮和可伸缩。</p><p id="3a11" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">ResNet-101是在卷积神经网络和残差块的帮助下建立的迁移学习模型。ResNet模型已经在一个巨大的图像数据集上进行了训练，因此我们可以使用它们来训练我们的数据集，只需更改其输入和输出参数。这里101表示用于其架构的101个深层神经层。ResNet-101的示意图如下:</p><figure class="jm jn jo jp fd ij er es paragraph-image"><div class="er es jl"><img src="../Images/529316c53849d005d1d09276f2f951ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*bk3CUajUNyLenV34sAnl-w.png"/></div></figure><p id="7078" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们的新模型<strong class="io hj">位</strong>(大转换)模型将看起来像→</p><figure class="jm jn jo jp fd ij er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jq"><img src="../Images/1edcf3a933b12343e8b6306744a4c99f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hQfDNHZp1DSKHfmEXMcSLw.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">从ResNet-101到BiT的架构变化</figcaption></figure><p id="4a57" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">BiT旨在简化和创建一个预先训练好的模型或传输管道，它使用最少数量的必要技巧，在广泛的流行分类任务中获得非常强的性能。</p><h2 id="ca7c" class="jz ka hi bd kb kc kd ke kf kg kh ki kj ix kk kl km jb kn ko kp jf kq kr ks kt bi translated">钻头型号类型:→</h2><ol class=""><li id="6381" class="ku kv hi io b ip kw it kx ix ky jb kz jf la jj lb lc ld le bi translated">BiT-L : →表示将在数据集JFT-300M(包含10到10亿个图像数据集)上训练的<strong class="io hj">大变换大</strong></li><li id="c50d" class="ku kv hi io b ip lf it lg ix lh jb li jf lj jj lb lc ld le bi translated">BiT-M : →表示将在数据集Imagenet-21K(包含1400万个图像数据集)上训练的<strong class="io hj">大变换介质</strong></li><li id="7431" class="ku kv hi io b ip lf it lg ix lh jb li jf lj jj lb lc ld le bi translated">位-S : →表示将在数据集ils vrc-2012(130万图像数据集)上训练的<strong class="io hj">大变换小</strong></li></ol><p id="a4f8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于初学者，我想推荐使用像CIFAR-10、CIFAR-100或Fashion-MNIST这样的小图像数据集来制作钻头模型</p><h2 id="2b08" class="jz ka hi bd kb kc kd ke kf kg kh ki kj ix kk kl km jb kn ko kp jf kq kr ks kt bi translated">如何生成位模型？: →</h2><p id="3044" class="pw-post-body-paragraph im in hi io b ip kw ir is it kx iv iw ix lk iz ja jb ll jd je jf lm jh ji jj hb bi translated">因此，首先我们需要理解ResNet-101模型的工作代码</p><p id="27e5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这里，这是一个我们可以理解ResNet-101模型的链接。<a class="ae jk" href="https://www.youtube.com/watch?v=lK5rm2_OPGo&amp;t=1218s" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj">点击</strong> </a></p><p id="f9c6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">实现模型，然后开始对其进行以下更改→</p><p id="3b0f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"># resnet 101模型的每一批归一化层都将被替换为<a class="ae jk" href="https://pytorch.org/docs/stable/nn.html#groupnorm" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj">分组归一化</strong> </a> <strong class="io hj"> </strong>增加了<strong class="io hj"> </strong> <a class="ae jk" href="https://github.com/KushajveerSingh/Deep-Learning-Notebooks/blob/master/Blog%20Posts%20Notebooks/Weight%20Standardization:%20A%20New%20Normalization%20in%20town/Weight%20Standardization%20on%20CIFAR-10.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj">权重归一化</strong> </a>，这将提高其性能。</p><p id="9ab3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">#在数据扩充中，我们将使用以下内容→</p><ol class=""><li id="fd99" class="ku kv hi io b ip iq it iu ix ln jb lo jf lp jj lb lc ld le bi translated">如果图像尺寸小于96 X 96，那么我们将连续调整图像尺寸并将其裁剪为160 X 160和128 X 128，否则，如果图像尺寸大于96 X 96，那么我们将调整图像尺寸并将其裁剪为448 X 448和384 X 384</li><li id="9cf9" class="ku kv hi io b ip lf it lg ix lh jb li jf lj jj lb lc ld le bi translated">如果数据集的大小超过20K，则使用<a class="ae jk" href="https://forums.fast.ai/t/mixup-data-augmentation/22764" rel="noopener ugc nofollow" target="_blank"> <strong class="io hj"> MixUp </strong> </a>数据扩充概念。</li><li id="2f92" class="ku kv hi io b ip lf it lg ix lh jb li jf lj jj lb lc ld le bi translated">使用批量512。</li></ol><p id="ca68" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"># For <strong class="io hj"> BiT-S和BiT-M</strong>；使用<strong class="io hj">随机梯度下降作为优化器</strong>，学习率将为<strong class="io hj"> 0.003 </strong>，通过在第30、60和80个时期以<a class="ae jk" href="https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR" rel="noopener ugc nofollow" target="_blank">衰减<strong class="io hj"> 10 </strong>的学习率来训练<strong class="io hj"> 90个时期</strong>。</a></p><p id="17cf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"># For<strong class="io hj">BiT-L</strong>；使用<strong class="io hj">随机梯度下降作为优化器</strong>，学习率将为<strong class="io hj"> 0.003 </strong>，在第23、30和37个时期用<a class="ae jk" href="https://pytorch.org/docs/stable/optim.html#torch.optim.lr_scheduler.StepLR" rel="noopener ugc nofollow" target="_blank">衰减10 的学习率对其进行<strong class="io hj"> 40个时期</strong>的训练。</a></p><p id="5416" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">#在ResNet-101中，剩余块被加宽了4倍，我们需要将其改变为3倍。</p><p id="1763" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了使BiT发挥作用，这些是我们需要进行的强制性变革。让我知道，如果我的读者有任何问题，评论或关注，直到那时享受学习。</p></div></div>    
</body>
</html>