<html>
<head>
<title>Introduction to N-grams, Language Modeling, Text-classification &amp; Naive Bayes classification in Natural Language Processing.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的N-grams、语言建模、文本分类和朴素贝叶斯分类介绍。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-n-grams-language-modeling-text-classification-naive-bayes-classification-in-5623ce2a36f1?source=collection_archive---------22-----------------------#2020-05-13">https://medium.com/analytics-vidhya/introduction-to-n-grams-language-modeling-text-classification-naive-bayes-classification-in-5623ce2a36f1?source=collection_archive---------22-----------------------#2020-05-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex if ig ih ii"><div class="bz dy l di"><div class="ij ik l"/></div></figure><blockquote class="il im in"><p id="29a1" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated"><strong class="ir hj">语言建模</strong></p></blockquote><p id="50dc" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">语言建模是自然语言处理中最重要的课题之一。语言建模的目标是给一个句子分配一个概率。(为什么？)</p><p id="9551" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">比方说机器翻译一个好句子和一个坏句子</p><p id="e1b3" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated"><em class="iq"> P(今夜大风)&gt; P(今夜大风)</em></p><p id="b196" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">或者在拼写纠正中</p><p id="8316" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated"><em class="iq"> P(约十五</em> <strong class="ir hj"> <em class="iq">分钟</em> </strong> <em class="iq">从)&gt; P(约十五</em> <strong class="ir hj"> <em class="iq">分钟</em> </strong> <em class="iq">从)</em></p><p id="3751" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">或者语音识别</p><p id="e16a" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">P(我看到了一凡)&gt; &gt; P(眼神敬畏的安)</p><p id="4e3c" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">所以计算一个句子或单词序列的概率是P(W)=P(W1，W2，W3…Wn)或者即将出现单词的概率将是P(W5|W1，W2，W3，W4)，计算这两者之一的模型被称为<strong class="ir hj">语言模型。因此，我们如何计算P(W)或语言模型如何实际工作，是依靠概率的链式法则的直觉，即P(A|B)=P(A，B)/P(B)或P(A，B)=P(A|B) P(B)，链式法则一般是P(X1，X2，X3…Xn)=P(X1)P(X2|X1)P(X3|X1，X2)…。P(Xn|X1…Xn-1)，但是计算大数据集的概率将非常耗时，所以为了减少这种情况，我们使用<a class="ae jq" href="https://en.wikipedia.org/wiki/Markov_property" rel="noopener ugc nofollow" target="_blank"> <strong class="ir hj">马尔可夫假设</strong> </a> <strong class="ir hj"> </strong>，即给定最后几个单词的前缀，即P(W1，W2，W3…，单词序列的概率是该单词的条件概率的乘积。Wn)=P(Wi|Wi-k…..Wi-1)例如，假设我们有一个阶段“他跑得很快”</strong></p><p id="8d3d" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">他跑得太快了</p><p id="282a" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">马尔可夫模型最简单的例子是<strong class="ir hj">单字模型</strong>，其中我们有一组单个单词(fifth，of，an，a…等等)，对于我们计算概率的每个单词，稍微聪明一点的是一个<strong class="ir hj">双字母模型</strong>(外面，新的，汽车，停车场…类似地，还有<strong class="ir hj">三元模型</strong>，或者我们可以扩展我们的N元模型，使其更加智能。</p><blockquote class="il im in"><p id="6669" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated"><strong class="ir hj">估计N元概率</strong></p></blockquote><p id="ea70" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">因此，为了找到概率，我们有P(Wi|PWi-1)=count (Wi-1，Wi)/count(Wi)，其中Wi是期望的单词，Wi-1是Wi的前一个单词，count (Wi-1，Wi)是Wi和Wi-1一起出现的次数。让我们举一个例子来简化它，假设我们有一个文档</p><p id="dc2e" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated"><S>我是b福布莱克</S></p><p id="2313" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated"><S> bforBlack我是</S></p><p id="68c7" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated"><S> bforBlack是编码的ted</S></p><p id="4a60" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">P( <s> |I)=1/3，P( <s> |bforBlack)=2/3，P(bforBlack|I)=1/3等等，可视化的最佳方式将是一个度量，它告诉哪个单词在哪个单词之后具有更多的计数。</s></s></p><figure class="js jt ju jv fd ii er es paragraph-image"><div class="er es jr"><img src="../Images/ba78c99b4d16317039e55c1b4a83a600.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/1*hWTGMZ9Y63wSNGXlQkS4Lg.png"/></div></figure><p id="a229" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">有许多公开可用的语言建模工具包，例如<a class="ae jq" href="http://www.speech.sri.com/projects/srilm/" rel="noopener ugc nofollow" target="_blank"> SRILM </a>或<a class="ae jq" href="http://ai.googleblog.com/2006/08/all-our-n-gram-are-belong-to-you.html" rel="noopener ugc nofollow" target="_blank"> Google -N gram Release </a>。</p><p id="c2db" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">我们的语言模型的目标必须是给真实的或经常观察到的句子分配更高的概率。我们需要训练健壮的模型来做更好的概括工作。<strong class="ir hj">一种概括是处理零，</strong> by <strong class="ir hj"> </strong>零我指的是在我们的训练集中从未出现过但在我们的测试集中确实出现过的东西，为了处理它我们使用<a class="ae jq" href="https://en.wikipedia.org/wiki/Additive_smoothing" rel="noopener ugc nofollow" target="_blank"> <strong class="ir hj"> add-one平滑或者拉普拉斯平滑</strong> </a> <strong class="ir hj">。</strong></p><blockquote class="il im in"><p id="32af" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated"><strong class="ir hj">拉普拉斯平滑或加一平滑</strong></p></blockquote><p id="2bbf" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">假设我们的语言模型遇到了一些我们的语言模型从未识别出的单词或双词，yes是一个有效的陈述，它绕过了我们之前的方法，不幸的是，我们对它的预测将是零，并抵消了一切，因此为了克服这一点，我们使用拉普拉斯或加一平滑，即从其他人那里分享少量的概率，并在每种情况下抵消零，从而将我们的公式修改为</p><p id="5458" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">P(Wi|Wi-1)=(count(Wi-1，Wi)+1)/count(Wi-1)+V</p><p id="c0ea" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">其中V是词汇量。</p><blockquote class="il im in"><p id="7cf6" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated"><strong class="ir hj">文本分类</strong></p></blockquote><p id="0546" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">文本分类的任务之一是性别识别，即确定给定作者是男性还是女性。一项关于性别鉴定的研究表明，如果我们研究代词、限定词、名词短语和其他特征的数量，有助于确定作者的性别。女性作家倾向于使用更多的代词，而男性作家倾向于使用更多的事实和限定词。</p><p id="fab1" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">另一个例子可以是电影或产品评论。看过一部电影或一部产品评论后，我可以将其分为正面或负面，例如，如果一篇评论“令人难以置信地失望”，我们称之为负面评论，或者“我见过的最伟大的喜剧电影”，我们称之为正面评论。</p><p id="c165" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">文本分类也有助于主题类别层次结构，你可以说，通过浏览标题或特征词，根据类别自动索引。文本分类的其他一些例子是<strong class="ir hj">垃圾邮件检测、作者身份识别、语言识别、情感分析。</strong></p><blockquote class="il im in"><p id="120b" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated"><strong class="ir hj">我们如何进行文本分类</strong></p></blockquote><p id="dc27" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">假设我们得到一个文档为d，我们有一组固定的类c={c1，c2，c3…cn}，我们的工作是获取一个文档并为该文档分配一个类，但是我们如何做呢？</p><p id="a3e6" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">一个简单的方法是手写规则，例如，如果我们正在进行垃圾邮件检测，那么我们可能会有一个黑名单电子邮件列表，或者我们可能会寻找类似于<strong class="ir hj">特殊字符的短语或单词，如“数百万美元或卢比”或“您已被选中”，</strong>如果这些规则由专家定义，您可以获得很高的准确性，但构建和维护这些规则的成本很高。<strong class="ir hj">垃圾邮件分类归入</strong> <a class="ae jq" href="https://en.wikipedia.org/wiki/Supervised_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="ir hj">监督机器学习</strong> </a> <strong class="ir hj">。</strong></p><blockquote class="il im in"><p id="764f" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated"><strong class="ir hj">朴素贝叶斯分类算法</strong></p></blockquote><p id="ed21" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">朴素贝叶斯是文本分类中最重要的算法之一。朴素贝叶斯的直觉基于贝叶斯规则，依赖于一种非常简单的文档表示方式，称为<strong class="ir hj">单词包。</strong>让我们来看看单词包表征的直觉。</p><p id="b267" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">想象一下，我有一份文件(<a class="ae jq" href="https://www.commonsensemedia.org/movie-reviews/the-irishman/user-reviews/adult" rel="noopener ugc nofollow" target="_blank"><strong class="ir hj"/></a>)上面写着</p><p id="d1cf" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">“我喜欢这部电影。它很长，但我认为它需要让你进入故事结束的漫长时期，并捕捉不同时代的情绪。有点慢，但我再次认为节奏符合故事情节，节奏均匀，这总是一件好事。剪辑也让故事以有趣的方式展开。”</p><p id="c9a4" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">我们的工作是建立一个函数，它接受这个文档并返回一个类(正的或负的)。为了解决这个任务，我们可以做的一件事是查看文档中的单个单词，如“喜爱、心情、好、有趣”或查看所有单词或查看一些子集。单词包释放了关于单词顺序的所有信息，并专注于出现的单词集及其计数，或者我们可以说是单词集及其计数的向量，因此单词包的想法是用单词列表及其计数来表示文档。</p><blockquote class="il im in"><p id="e708" class="io ip iq ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated"><strong class="ir hj">形式化朴素贝叶斯分类算法</strong></p></blockquote><p id="d624" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">对于文档d和类c，我们的目标是计算每个类的概率以及给定文档的条件概率，即P(c|d ),我们将使用这个概率来选择最佳类。</p><p id="439d" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">P(c|d)=(P(d|c)P(c))/P(d)或P(d|c)P(c)或P(x1，x2，x3…xn|c)P(c)</p><p id="07f4" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">因此，如果P(c1|d1)&gt;P(c2|d1)，我们为文档d1选择类c1。</p><p id="71c4" class="pw-post-body-paragraph io ip hi ir b is it iu iv iw ix iy iz jn jb jc jd jo jf jg jh jp jj jk jl jm hb bi translated">我希望到目前为止，你可能已经对N-grams、语言建模、文本分类和朴素贝叶斯分类算法有所了解。</p></div></div>    
</body>
</html>