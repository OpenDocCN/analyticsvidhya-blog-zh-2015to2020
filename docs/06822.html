<html>
<head>
<title>Machine Learning: Decision Tree Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:决策树回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-decision-tree-regression-ff8563ffaf52?source=collection_archive---------16-----------------------#2020-06-03">https://medium.com/analytics-vidhya/machine-learning-decision-tree-regression-ff8563ffaf52?source=collection_archive---------16-----------------------#2020-06-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/156bc897a424e0ca74d23ee1cd94a167.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bg0_UGT6xo89Ij-GX61yPg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">基本决策树结构。来源:谷歌图片。</strong></figcaption></figure><p id="acd2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated"><span class="l ju jv jw bm jx jy jz ka kb di"> M </span>我们大多数人都觉得决策树很难，但它是机器学习中最强大的技术之一。这很容易实现，属于监督学习技术。顾名思义，它是一棵树，但没有真正的根和绿叶。它是通过经历各种条件来构建树，形成称为头节点的根和称为末端叶的叶，从而将数据分解成小段。</p><p id="26de" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="kc">数据是如何分割成段的？我们很容易说数据被分割成段，但是，我们有没有想过这是如何发生的，为什么会发生？这个问题的答案在于两个重要的概念信息熵(E)和信息增益(IG)。</em></strong></p><p id="fb65" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="kc">信息熵(E): </em> </strong>熵是指无序或不确定性。在这里，信息熵有助于找出决策树决定如何分割数据。这有助于维持决策的界限。</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es kd"><img src="../Images/6e9ddef4fb5b738532d6c2dd4ebe470e.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*AKUvTcw2wjo0lTHYl2SYiw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">熵的方程式。</strong></figcaption></figure><p id="3be6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="kc">信息增益(IG): </em> </strong>它衡量一个特定的特征给了我们多少关于类的信息。</p><blockquote class="ki kj kk"><p id="9fe0" class="iv iw kc ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated">信息增益是建立决策树的主要关键，信息增益最高的属性将首先被拆分。</p></blockquote><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es ko"><img src="../Images/1804dcdfe22f5cd7a7248bff0bf60e7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*vb100g5d_zmKBDg-XYyjeQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">信息增益方程。</strong></figcaption></figure><blockquote class="ki kj kk"><p id="2070" class="iv iw kc ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated">完成上述所有步骤后，数据点的分段如下所示:</p></blockquote><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/0da6cb117b51a3cc0d1d8f5fcac9ba13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*vGHM6H1QR_Eg9hh9pARoYw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">分割线段。</strong></figcaption></figure><p id="5543" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面例子中的决策树看起来像是有5个终端叶子的流程图。</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/3b7b4b6338f54f878a6e5d61bd3f6528.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*gBar9Uegdy62_8Wsh9PfaA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">决策树。</strong></figcaption></figure><blockquote class="ki kj kk"><p id="9649" class="iv iw kc ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated">末端叶中的值用于预测位于该段中的任何新观察值。上面描述了递归分割数据以形成决策树。</p></blockquote><p id="82a1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="kc">实现:</em> </strong></p><p id="ca7c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们以数据集中的一个自变量和一个因变量为例。因此，我们可以很容易地绘制决策树图。</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/bbb530c38001215d7811eded9a9feef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*3z9qA9e9KDfhabloUnGkmw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">导入库。</strong></figcaption></figure><blockquote class="ki kj kk"><p id="294b" class="iv iw kc ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated">导入的基本库，如Numpy、Matplotlib和pandas。</p></blockquote><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/65b113ca403c1a82c720d8f24ccc7549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*8cjUvKzoXkEP-MdhmXJJLg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">导入数据集。</strong></figcaption></figure><blockquote class="ki kj kk"><p id="4348" class="iv iw kc ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated">导入我们必须处理的数据集，并将自变量列值存储在X中，将因变量列值存储在y中。</p></blockquote><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es kt"><img src="../Images/52cb74ffed9ef134e318010596ae2e36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*oZ-RbgUFwThrG9HSAixY7Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">X和y中的值。</strong></figcaption></figure><p id="671d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为我们只有10个不同的值，所以我们不会将数据分成训练集和测试集。我们希望我们的模型能够学习数据中所有的值。</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es ku"><img src="../Images/ad8b3a64129f39c81eecac7c4410957e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1368/format:webp/1*-SPcFk9kErBXC9AKfzxxHw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">训练回归模型。</strong></figcaption></figure><blockquote class="ki kj kk"><p id="0180" class="iv iw kc ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated">决策树回归模型在两个特征X和y上被训练</p></blockquote><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es kv"><img src="../Images/18c0a4abe6a1db956d0f3d401bb863a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*-0Ie6bAF7JBMswjn1n23Vg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">回归模型结果的可视化。</strong></figcaption></figure><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/250331d2fcb14c3147f00fe4be5b2c1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*9Qq8Zx82PMlzYUfLj8HBRw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><strong class="bd iu">薪资预测。</strong></figcaption></figure><blockquote class="ki kj kk"><p id="f850" class="iv iw kc ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated">以上回归正确地预测了位于5和6级位置之间的区段中的值。我们可以把结果与绘制的图表进行比较。这就完成了决策树回归的简单实现。</p></blockquote><p id="83e0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated"><span class="l ju jv jw bm jx jy jz ka kb di"> C </span> <strong class="ix hj"> <em class="kc">结束语:</em> </strong>最后我们得出结论，决策树回归在预测或预报新观测值的输出方面起着重要的作用。它适用于更多数量的独立变量。但是，我们不能绘制图表，因为它会生成一个超过3维的图表。因此，决策树的形成遵循数据的递归分割。</p><blockquote class="kx"><p id="8b94" class="ky kz hi bd la lb lc ld le lf lg js dx translated">"非常感谢您抽出宝贵的时间阅读这篇关于决策树回归的博客. "</p></blockquote></div></div>    
</body>
</html>