<html>
<head>
<title>Understanding of SVD and PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解奇异值分解和主成分分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-of-svd-and-pca-2ebeae8c6ad0?source=collection_archive---------4-----------------------#2020-06-02">https://medium.com/analytics-vidhya/understanding-of-svd-and-pca-2ebeae8c6ad0?source=collection_archive---------4-----------------------#2020-06-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/9163f4082d0e17f3e8241dfe27e80d31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FWUnCW8-xS0Lq8G0wdxy7g.jpeg"/></div></div></figure><div class=""/><p id="b575" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们不喜欢复杂的东西，我们喜欢简洁的形式，或者在不丢失重要信息的情况下代表复杂事物的模式，以使我们的生活更容易。</p><p id="60c6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我们处理一个高维的矩阵(作为收集由行和列组成的数据的工具)时，有没有一种方法可以让我们更容易理解数据信息，并找到它的低维代表？</p><h1 id="5a82" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">内容:</strong></h1><p id="c8fd" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">1、特征分解的几何解释</p><p id="72d7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2、奇异值分解</p><p id="6759" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3、如何将SVD连接到数据？</p><p id="f6f2" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4、方差-协方差矩阵的性质</p><p id="a35c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5、主成分分析</p><p id="ba95" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">6、主成分分析的局限性</p><h1 id="832b" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">特征分解的几何解释:</strong></h1><p id="a214" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">特征分解的概念在计算机视觉和机器学习等许多领域都非常重要。矩阵特征分解的几何解释有助于使冗长的理论更容易理解。让我们来看一个等式:</p><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es kr"><img src="../Images/b431666b31b8af86998a020724db8f30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*4cLYEzek8yLjkSOkqmmXjQ.png"/></div></figure><ul class=""><li id="b025" class="kw kx ht is b it iu ix iy jb ky jf kz jj la jn lb lc ld le bi translated">其中A—方阵；X —特征向量；λ —特征值。</li><li id="2510" class="kw kx ht is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">右边的图是左边方程的一个简单例子。</li><li id="09d3" class="kw kx ht is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">如果两边都乘以任何一个正标量，两边仍然相等</li></ul><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es lk"><img src="../Images/f14162b83e2951c4610e62f34002495f.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*FaRTmNcGsHpmVTL0Re326w.png"/></div></figure><p id="126b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">X和X’都对应于相同的特征向量λ。由于s可以是任何非零标量，我们看到这个唯一的λ可以有无穷多个特征向量。那么特征向量和特征值是什么意思呢？让我们看看2×2矩阵的几何。在飞机上:</p><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es ll"><img src="../Images/876474f8399e04ef8fad600d61482728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*SG12cywfkL_Zruivwz1NcQ.png"/></div></figure><p id="d11e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这两个向量(红色和蓝色线从原点开始到点(2，1)和(4，5))对应矩阵a的两个列向量，根据例子，λ = 6，X = (1，1)′，我们在上面的RHS子图上加上向量(1，1)′</p><ul class=""><li id="7e53" class="kw kx ht is b it iu ix iy jb ky jf kz jj la jn lb lc ld le bi translated">子情节的解释(rhs):</li></ul><h2 id="32b4" class="lm jp ht bd jq ln lo lp ju lq lr ls jy jb lt lu kc jf lv lw kg jj lx ly kk lz bi translated"><strong class="ak">在向量X上应用矩阵A将等于向量X的λ倍拉伸(</strong> λ可视为扩展的幅度)<strong class="ak">。</strong>最长的红线(λX)是将特征向量X= (1，1)’拉伸特征值(λ = 6)倍。</h2><p id="3b10" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">再次，在方程中:AsX = λsX，如果我们设s = 2，那么特征向量更新，AX′=λX′，新的特征向量X′= 2X =(2，2)′但是对应的λ不变。下面是A的特征分解的另一个几何图形。最长的红色向量意味着当在特征向量X′=(2，2)′上应用矩阵A时，它将等于将新的特征向量X′=(2，2)′λ= 6倍拉伸的最长的红色向量。</p><figure class="ks kt ku kv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ma"><img src="../Images/9e4afcf5b3f8a044bea193014d499836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YFApQsxO1fk9gTYAdNthtg.png"/></div></div></figure><h2 id="9d3f" class="lm jp ht bd jq ln lo lp ju lq lr ls jy jb lt lu kc jf lv lw kg jj lx ly kk lz bi translated"><strong class="ak">有意思吧？如果找到矩阵的特征向量和特征值，就可以用更简单的方式来表示了！</strong></h2><h1 id="8c3a" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">奇异值分解(SVD): </strong></h1><p id="d774" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">奇异值分解基于特征值计算，它将方阵A的特征分解推广到任意m×n维矩阵M。</p><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es mb"><img src="../Images/567c5ab595cade675cc644a4e4d28c8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*t1_4WqmF-se8e2odeKtOrg.png"/></div></figure><ul class=""><li id="32db" class="kw kx ht is b it iu ix iy jb ky jf kz jj la jn lb lc ld le bi translated">m被分解成三个矩阵U，σ和V，它可以展开为系数为σ的正交基方向(U和V)的线性组合。</li><li id="a0b9" class="kw kx ht is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">u和V都是正交矩阵，这意味着UU′= VV′= I，I是单位矩阵。所以在上面的等式中:</li></ul><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es mc"><img src="../Images/3afad74572cc3c6b707d953076f32391.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*K9eDWjVruxAyYna3sJwxzw.png"/></div></figure><ul class=""><li id="d61e" class="kw kx ht is b it iu ix iy jb ky jf kz jj la jn lb lc ld le bi translated">σ是对角线矩阵，奇异值位于对角线上。σ1 ≥ σ2 ≥ … ≥ σp ≥ 0，降序排列，非常类似于本征分解中的伸缩参数λ。</li></ul><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es lk"><img src="../Images/8f9df8c529bc2ee265526b946278cda9.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*uvJTRP7aqKv5vJF19OjJyA.png"/></div><figcaption class="md me et er es mf mg bd b be z dx translated"><strong class="bd jq"> σ1 ≥ σ2 ≥ … ≥ σp ≥ 0均为奇异值，p=min(m，n) </strong></figcaption></figure><ul class=""><li id="a15c" class="kw kx ht is b it iu ix iy jb ky jf kz jj la jn lb lc ld le bi translated">方程M = uσV′的几何解释:</li></ul><figure class="ks kt ku kv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/d918806140ae53c593ee15db9e753024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MgPV7fcXsbzCAJQnNJQTfg.png"/></div></div></figure><p id="a655" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">将矩阵M = uσV’应用于X，</p><ul class=""><li id="958c" class="kw kx ht is b it iu ix iy jb ky jf kz jj la jn lb lc ld le bi translated">步骤1–2:V′X是X上的旋转。</li><li id="fcec" class="kw kx ht is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">步骤2–3:σ(V’X)进行拉伸。</li><li id="65cf" class="kw kx ht is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">步骤3–4:U(σV′X)= MX再次旋转。</li></ul><h1 id="0937" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">记得我们提到过“拉伸”，对吗？那么奇异值分解和特征分解有什么关系呢？</strong></h1><p id="67b8" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">回想在特征分解中，AX = λX，a是方阵，我们也可以把方程写成:A = XλX^(-1).</p><p id="384c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(1)在特征分解中，我们对行和列空间使用相同的基X(特征向量),但是在SVD中，我们使用两个不同的基U和V，其中列跨越m的列和行空间。</p><p id="fa44" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(2)特征分解中U和V的列是正交基，而X的列不是正交基。</p><p id="47a8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(3) SVD用于所有有限维矩阵，而特征分解仅用于方阵。</p><p id="7637" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(4) <strong class="is hu">对于对称正定矩阵S如协方差矩阵，SVD和特征分解相等，我们可以写成:</strong></p><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/ccf88cb525379d97239755198291300b.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*MFaSmF2ArPJQ9_d2UKjeHw.png"/></div></figure><h1 id="94c9" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">如何将SVD连接到数据？</strong></h1><p id="9226" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">让我们来看看x和y的散点图:</p><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es mj"><img src="../Images/91190ad9c7b4d86669aa84353bf8a1b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*BmdYR1VS06PsJkklRS1IhQ.png"/></div></figure><p id="8edc" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设我们收集两个维度的数据，乍一看，你认为可以表征数据的重要特征是什么？</p><p id="9c70" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(1)所有那些数据的位置，对吗？例如，这组数据的中心位置——平均值</p><p id="9d00" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(2)数据如何在不同的方向上传播(幅度)。—方差</p><p id="7b3a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你有没有一种感觉，这个图与我们已经讨论过的一些图非常相似？下一个呢？</p><figure class="ks kt ku kv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mk"><img src="../Images/a9e3c61070f886f9c9d8be4b77a7e306.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U2th9ycYFW5OU-zjw1ar4Q.png"/></div></div></figure><p id="800f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">是不是很像我们现在在SVD的几何解释里？旋转方向和拉伸之类的东西？这两者之间有什么联系吗？</p><p id="72ae" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">好了，我们来看上面这个图，两个有方向的轴X(黄色箭头)和Y(绿色箭头)是互相正交的。散点图的主要形状，由椭圆线(红色)显示，清晰可见。椭圆内的新箭头(黄色和绿色)仍然是正交的。想象一下我们如何把原来的X和Y轴旋转到新的轴上，也许还可以把它们拉伸一点。现在，</p><p id="c0ca" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">太棒了。</p><p id="f3ef" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们能在数据分布上应用奇异值分解的概念吗？如果能找到正交基和拉伸量级，就能表征数据了吗？如果当数据有很多维度时，我们还能使用SVD吗？它将如何帮助我们处理高维空间？</p><p id="e0f8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">先来看看方差-协方差矩阵的好性质。</p><h1 id="e57e" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">方差-协方差矩阵性质:</strong></h1><p id="02a6" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">很幸运我们知道方差-协方差矩阵是:</p><p id="5393" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(1)对称</p><p id="639e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(2)正定(至少半定，这里我们忽略半定)</p><p id="8e03" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，对方差-协方差矩阵进行特征分解和奇异值分解是相同的。并且在方差-协方差矩阵s上计算特征分解或SVD是如此容易</p><h1 id="be2b" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">主成分分析(PCA): </strong></h1><p id="6395" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">让我们一起研究方程式，我们有</p><ul class=""><li id="73b0" class="kw kx ht is b it iu ix iy jb ky jf kz jj la jn lb lc ld le bi translated">x = uσV′</li><li id="95e1" class="kw kx ht is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">s = X′X = VDV′，D =σ×σ和</li></ul><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es ml"><img src="../Images/5ba7b46174b38d8da89d0c0fbad82702.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*Whc9ga4Q0eBjAl0HZ65XGw.png"/></div></figure><ul class=""><li id="f29a" class="kw kx ht is b it iu ix iy jb ky jf kz jj la jn lb lc ld le bi translated">给定VV′= I，我们可以得到XV = UΣ，并让:</li></ul><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es mm"><img src="../Images/a21bacacef3f3c6c62a248093a9e2ef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*3p-1XHWA2YMzvj-KgtGGmQ.png"/></div></figure><ul class=""><li id="e403" class="kw kx ht is b it iu ix iy jb ky jf kz jj la jn lb lc ld le bi translated">因为σ1 ≥ σ2 ≥ … ≥ σp ≥ 0，所以Z1被称为X对应于最大σ1的第一分量。Var(Z1) = Var(u1σ1) = σ1× σ1。</li><li id="612a" class="kw kx ht is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">u1是所谓的归一化第一主分量。</li><li id="9d22" class="kw kx ht is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">我们看到Z1是X = (X1，X2，X3，… Xm)在m维空间的线性组合。</li></ul><figure class="ks kt ku kv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mn"><img src="../Images/e9f3bf0c507c94eb9ef33f495a9459b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*325kOgAagwOgyPBPCbdZLg.png"/></div></div></figure><ul class=""><li id="bbf3" class="kw kx ht is b it iu ix iy jb ky jf kz jj la jn lb lc ld le bi translated">如果σp明显小于之前的σi，那么我们可以忽略它，因为它对总方差-协方差的贡献较小。对于那些明显小于前面σ的，我们可以全部忽略。</li></ul><h1 id="501c" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">这就是我们如何通过两步将方差-协方差的主成分分析作为降维方法:</strong></h1><p id="cb39" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">(1)对原始数据进行线性变换，形成作为新轴方向的正交基主分量。</p><p id="ad52" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(2)第一个分量具有最大的可能方差。第二个在与前一个正交的基础上具有第二大方差，依此类推。然后，我们只保留前j个有效的最大主分量，它们描述了方差的大部分(对应于前j个最大拉伸幅度),因此降低了维数。</p><h1 id="c0e9" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">PCA的局限性:</h1><ul class=""><li id="5db5" class="kw kx ht is b it km ix kn jb mo jf mp jj mq jn lb lc ld le bi translated">很难解释当我们进行真实世界数据回归分析时，我们不能说哪些变量是最重要的，因为每一个分量都是原始特征空间的线性组合。</li><li id="94f4" class="kw kx ht is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">它依赖于线性假设。</li><li id="cad7" class="kw kx ht is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">计算速度慢。</li><li id="3a2e" class="kw kx ht is b it lf ix lg jb lh jf li jj lj jn lb lc ld le bi translated">取决于原始数据结构的质量。</li></ul></div></div>    
</body>
</html>