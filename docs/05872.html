<html>
<head>
<title>Review: AdaConv — Video Frame Interpolation via Adaptive Convolution (Video Frame Interpolation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综述:AdaConv —通过自适应卷积实现视频帧插值(视频帧插值)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/review-adaconv-video-frame-interpolation-via-adaptive-convolution-video-frame-interpolation-fbce6acaa2a5?source=collection_archive---------19-----------------------#2020-05-04">https://medium.com/analytics-vidhya/review-adaconv-video-frame-interpolation-via-adaptive-convolution-video-frame-interpolation-fbce6acaa2a5?source=collection_archive---------19-----------------------#2020-05-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="1a66" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用CNN来内插帧，而不是使用光流+像素合成</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/7a388d7c4133e5c12ca59c6683c3d9d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*jgh8N1vLXLtF9s5d3hldQw.gif"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">视频帧插值作者(来自他们的</strong> <a class="ae jk" href="http://web.cecs.pdx.edu/~fliu/project/adaconv/" rel="noopener ugc nofollow" target="_blank"> <strong class="bd jj">网站</strong> </a> <strong class="bd jj"> ) </strong></figcaption></figure><p id="2eba" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi kh translated"><span class="l ki kj kk bm kl km kn ko kp di">在</span>这篇报道中，我们回顾了波特兰州立大学的<strong class="jn hj">视频帧自适应卷积插值(AdaConv) </strong>。(它被称为AdaConv，因为它是在他们的网站上而不是在报纸上命名的。在他们后来的论文中，也引用了AdaConv的名字。)</p><p id="7154" class="pw-post-body-paragraph jl jm hi jn b jo jp ij jq jr js im jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">通常使用两阶段方法。首先，使用光流来估计运动。然后，基于估计的运动合成像素。本文采用一步法，即卷积神经网络(CNN)。这是一篇发表在<strong class="jn hj"> 2017 CVPR </strong>的论文，引用超过<strong class="jn hj"> 130次</strong>。(<a class="kq kr ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----fbce6acaa2a5--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p></div><div class="ab cl ks kt gp ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="hb hc hd he hf"><h1 id="0f06" class="kz la hi bd jj lb lc ld le lf lg lh li io lj ip lk ir ll is lm iu ln iv lo lp bi translated">概述</h1><ol class=""><li id="cc84" class="lq lr hi jn b jo ls jr lt ju lu jy lv kc lw kg lx ly lz ma bi translated"><strong class="jn hj">视频帧插值</strong></li><li id="14fb" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg lx ly lz ma bi translated"><strong class="jn hj">卷积像素插值</strong></li><li id="70da" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg lx ly lz ma bi translated"><strong class="jn hj">网络架构&amp;损失函数</strong></li><li id="e99a" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg lx ly lz ma bi translated"><strong class="jn hj">移位和针脚实现</strong></li><li id="d10d" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg lx ly lz ma bi translated"><strong class="jn hj">训练&amp;超参数选择</strong></li><li id="467f" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg lx ly lz ma bi translated"><strong class="jn hj">实验结果</strong></li></ol></div><div class="ab cl ks kt gp ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="hb hc hd he hf"><h1 id="ac94" class="kz la hi bd jj lb lc ld le lf lg lh li io lj ip lk ir ll is lm iu ln iv lo lp bi translated">1.视频帧插值</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mg"><img src="../Images/51e0d03d5fcb975ea71a36b756fef1df.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*tDuOQ4j9mDsYu1k8BY6pWg.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj"> (a)传统方法，(b) CNN方法</strong></figcaption></figure><ul class=""><li id="9b73" class="lq lr hi jn b jo jp jr js ju mh jy mi kc mj kg mk ly lz ma bi translated">给定两个视频帧<em class="ml"> I </em> 1和<em class="ml"> I </em> 2，插值方法旨在在两个输入帧的中间临时插值一个帧ˇ<em class="ml">I</em>。</li><li id="a3ca" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">(a)传统上，如图的上部所示，在估计中间帧中每个像素的运动之后，运动指向的输入帧处的像素被用于内插中间帧处的像素ˇ<em class="ml">I</em>(<em class="ml">x</em>，<em class="ml"> y </em> ) <em class="ml"> </em>。</li><li id="0690" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">(b)本文采用CNN，在提供感受野的情况下，采用数据驱动的方法，通过训练好的CNN得到像素ˇ<em class="ml">I</em>(<em class="ml">x</em>，<em class="ml"> y </em>)。</li></ul></div><div class="ab cl ks kt gp ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="hb hc hd he hf"><h1 id="e668" class="kz la hi bd jj lb lc ld le lf lg lh li io lj ip lk ir ll is lm iu ln iv lo lp bi translated">2.<strong class="ak">通过卷积进行像素插值</strong></h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es mm"><img src="../Images/f7dbd5a71942ca57a713328bb0ad613d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c1oEVisaHKDEt3X4J1q6Fg.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">通过卷积进行像素插值</strong></figcaption></figure><ul class=""><li id="16dd" class="lq lr hi jn b jo jp jr js ju mh jy mi kc mj kg mk ly lz ma bi translated">具体来说，为了估计输出像素(<em class="ml"> x </em>，<em class="ml"> y </em>)的卷积核<em class="ml"> K </em>，CNN将感受野面片<em class="ml"> R </em> 1( <em class="ml"> x </em>，<em class="ml"> y </em>)和<em class="ml"> R </em> 2( <em class="ml"> x </em>，<em class="ml"> y </em>)作为输入，其中<em class="ml"> R </em> 1( <em class="ml"> x </em>，)</li><li id="e611" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">输出内核为了产生输出像素的颜色而将卷积的面片<em class="ml"> P </em> 1和<em class="ml">P</em>2(<em class="ml">x</em>，<em class="ml"> y </em>)与这些感受野位于同一中心位置，但尺寸较小，如上图所示。</li><li id="54d1" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">比小块大的大感受野用于更好地处理运动估计中的孔径问题。</li><li id="5ebe" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">默认的感受野大小为79×79像素。卷积面片大小为41×41，核大小为41 × 82，因为它用于与两个面片进行卷积。</li></ul></div><div class="ab cl ks kt gp ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="hb hc hd he hf"><h1 id="42a6" class="kz la hi bd jj lb lc ld le lf lg lh li io lj ip lk ir ll is lm iu ln iv lo lp bi translated">3.<strong class="ak">网络架构&amp;损失函数</strong></h1><h2 id="0b32" class="mr la hi bd jj ms mt mu le mv mw mx li ju my mz lk jy na nb lm kc nc nd lo ne bi translated">3.1.网络体系结构</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es nf"><img src="../Images/b5b139f0353e3ee5ae71eb8456eb92c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2yHZXP8N_uQ5FGoWwdp0bw.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">网络架构</strong></figcaption></figure><ul class=""><li id="1b11" class="lq lr hi jn b jo jp jr js ju mh jy mi kc mj kg mk ly lz ma bi translated">CNN由几个卷积层和下卷积层组成，作为最大池层的替代。使用ReLU和以及批处理规范化(BN)。</li><li id="7fe4" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">该网络是完全卷积的。因此，它不限于固定大小的输入。</li><li id="8768" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated"><strong class="jn hj">一个关键约束是输出卷积核的系数应该是非负的，并且总和为1。因此，最终的卷积层连接到一个空间softmax层来输出卷积核</strong>，它隐式地满足了这个重要的约束。</li></ul><h2 id="0952" class="mr la hi bd jj ms mt mu le mv mw mx li ju my mz lk jy na nb lm kc nc nd lo ne bi translated">3.2.损失函数</h2><ul class=""><li id="0824" class="lq lr hi jn b jo ls jr lt ju lu jy lv kc lw kg mk ly lz ma bi translated">一个可能的损失函数可以是插值像素颜色和地面真实颜色之间的差异，如下所示:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ng"><img src="../Images/4b419bb63f08cd0d8b1fd8798f6ddd0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*I1uYazzgXTU4aoFo6Szvbw.png"/></div></figure><ul class=""><li id="05dc" class="lq lr hi jn b jo jp jr js ju mh jy mi kc mj kg mk ly lz ma bi translated">其中,<em class="ml"> Ci </em>为插值帧中(<em class="ml"/>，<em class="ml"> yi </em>)处的底色。</li><li id="a4d5" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">然而，这个<strong class="jn hj">颜色损失</strong>单独会导致模糊的结果。</li><li id="564a" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">为了解决这个问题，考虑<strong class="jn hj">梯度损失</strong>:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nh"><img src="../Images/e63f4ae952486a5e5ad48bdafebdf935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*kW7TKpKccec1UViIeg1t3g.png"/></div></figure><ul class=""><li id="aeb1" class="lq lr hi jn b jo jp jr js ju mh jy mi kc mj kg mk ly lz ma bi translated">其中k表示我们计算梯度的八种方法之一。</li><li id="4a45" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated"><em class="ml"> Gki </em>，1和<em class="ml"> Gki </em>，2是输入面片<em class="ml"> Pi </em>，1和<em class="ml"> Pi </em>，2的梯度。</li><li id="4003" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">和\\<em class="ml">Gki</em>是插值帧中(<em class="ml">、</em>、<em class="ml">易</em>)处的地面实况梯度。</li><li id="37ee" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">计算输入面片的梯度，然后执行与估计的核的卷积，这将导致感兴趣像素处的插值图像的梯度。</li><li id="04af" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">因此，最终的损失是颜色损失和梯度损失加在一起。</li></ul></div><div class="ab cl ks kt gp ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="hb hc hd he hf"><h1 id="d547" class="kz la hi bd jj lb lc ld le lf lg lh li io lj ip lk ir ll is lm iu ln iv lo lp bi translated">4.移位和缝合实现</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es ni"><img src="../Images/58f61b69810b1992f0d4fd849134d7ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pE_1N5XuEdc07TGJiyMIJw.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">移位并缝合</strong></figcaption></figure><ul class=""><li id="dfd5" class="lq lr hi jn b jo jp jr js ju mh jy mi kc mj kg mk ly lz ma bi translated">移位和拼接[17，32，39]方法，其中使用相同输入的稍微移位的版本。这种方法返回稀疏的结果，这些结果可以组合起来形成插值帧的密集表示，如上所示。</li><li id="7529" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">考虑尺寸为1280×720的帧，<strong class="jn hj">神经网络的像素方式实现</strong>将需要<strong class="jn hj"> 921，600次通过神经网络的前向传递</strong>。</li><li id="47e3" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">对于输入的64种不同的移位版本，<strong class="jn hj">移位和缝合实现</strong> <strong class="jn hj">只需要64次向前传递</strong>。</li><li id="317d" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">与Nvidia Titan X上每帧花费104秒的<strong class="jn hj">像素方式实现相比，<strong class="jn hj">移位缝合实现只需要9秒。</strong></strong></li><li id="0ee2" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">为了处理边界问题，只需简单地使用零填充。</li></ul></div><div class="ab cl ks kt gp ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="hb hc hd he hf"><h1 id="eea3" class="kz la hi bd jj lb lc ld le lf lg lh li io lj ip lk ir ll is lm iu ln iv lo lp bi translated">5.训练和超参数选择</h1><h2 id="87b9" class="mr la hi bd jj ms mt mu le mv mw mx li ju my mz lk jy na nb lm kc nc nd lo ne bi translated">5.1.训练数据集</h2><ul class=""><li id="5021" class="lq lr hi jn b jo ls jr lt ju lu jy lv kc lw kg mk ly lz ma bi translated">使用来自Flickr的带有知识共享许可的公开可用视频。使用关键词下载的3，000个视频，如“驾驶”、“跳舞”、“冲浪”、“骑行”和“滑雪”，产生了多样化的选择。下载的视频被缩放到1280×720像素的固定大小。</li></ul><h2 id="90ce" class="mr la hi bd jj ms mt mu le mv mw mx li ju my mz lk jy na nb lm kc nc nd lo ne bi translated">5.2.超参数选择</h2><ul class=""><li id="e689" class="lq lr hi jn b jo ls jr lt ju lu jy lv kc lw kg mk ly lz ma bi translated">理论上，卷积核必须大于两帧之间的<strong class="jn hj">像素运动</strong>，以便捕捉运动(隐含地)来产生良好的插值结果。</li><li id="e4b4" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated"><strong class="jn hj">应选择大仁。</strong>另一方面，大核涉及大量待估计值，增加了网络的复杂度。</li><li id="2efc" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">选择一个足够大的卷积核来捕获训练数据集中的最大运动，即38个像素。</li><li id="1803" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">因此，我们系统中的<strong class="jn hj">卷积核</strong>大小为<strong class="jn hj"> 41×82 </strong>，其将被应用于两个41×41的面片。</li><li id="bc04" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">这个内核有<strong class="jn hj">几个比38像素</strong>大的像素，为重采样提供像素支持<strong class="jn hj">。</strong></li><li id="60d6" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">并且使用验证数据集发现较大的<strong class="jn hj">感受野</strong>为<strong class="jn hj"> 79×79 </strong>，在其中实现了良好的平衡。</li></ul></div><div class="ab cl ks kt gp ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="hb hc hd he hf"><h1 id="5ce5" class="kz la hi bd jj lb lc ld le lf lg lh li io lj ip lk ir ll is lm iu ln iv lo lp bi translated">6.实验结果</h1><h2 id="47af" class="mr la hi bd jj ms mt mu le mv mw mx li ju my mz lk jy na nb lm kc nc nd lo ne bi translated">6.1.SOTA比较</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es nj"><img src="../Images/0f3fbcee00084a76426a3e6bcd05b135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-j6qW-ZQliQ2_cUdawRcqw.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">平均插补误差</strong></figcaption></figure><ul class=""><li id="80d2" class="lq lr hi jn b jo jp jr js ju mh jy mi kc mj kg mk ly lz ma bi translated">米德尔伯里光流基准用于评估。</li><li id="fefe" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">在Middlebury基准测试中报告的100多种方法中，所提出的方法在Evergreen和Basketball上取得了最好的结果，在Dumptruck上取得了第二好的结果，在后院(当时)上取得了第三好的结果。</li></ul><h2 id="d7ba" class="mr la hi bd jj ms mt mu le mv mw mx li ju my mz lk jy na nb lm kc nc nd lo ne bi translated">6.2.定性评价</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es nk"><img src="../Images/3dce4cf4341e113235be62f80f76092b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0LBhZm9IIHNYo1XXCeCdjg.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">模糊</strong></figcaption></figure><ul class=""><li id="40f2" class="lq lr hi jn b jo jp jr js ju mh jy mi kc mj kg mk ly lz ma bi translated"><strong class="jn hj">模糊</strong>:所提出的方法产生更清晰的图像，尤其是在运动较大的区域。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es nk"><img src="../Images/92e8ed34ee8f3ad161b6b90b4dcc746a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KLV9JSYM_K_-lY6BGB_tyw.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">亮度突变</strong></figcaption></figure><ul class=""><li id="f9e2" class="lq lr hi jn b jo jp jr js ju mh jy mi kc mj kg mk ly lz ma bi translated"><strong class="jn hj">亮度突变</strong>:所提出的方法比基于流动的方法产生更具视觉吸引力的插值结果。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es nk"><img src="../Images/3219432bfb738a0f44f9781a3b161a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i6CRW_sb4gSXJcri2bT2hQ.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">咬合</strong></figcaption></figure><ul class=""><li id="32a5" class="lq lr hi jn b jo jp jr js ju mh jy mi kc mj kg mk ly lz ma bi translated"><strong class="jn hj">遮挡</strong>:所提出的方法采用了一种学习方法来获得适当的卷积核，从而为遮挡区域带来视觉上吸引人的像素合成结果，而通常光流在遮挡区域是不可靠或不可用的。</li></ul><h2 id="dc51" class="mr la hi bd jj ms mt mu le mv mw mx li ju my mz lk jy na nb lm kc nc nd lo ne bi translated">6.3.遮挡处理</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es nl"><img src="../Images/0e87a9795dcf4cbfa919aabbef450184.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yls0SxHvb3yMY22PGtKnAw.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">遮挡处理</strong></figcaption></figure><ul class=""><li id="1df6" class="lq lr hi jn b jo jp jr js ju mh jy mi kc mj kg mk ly lz ma bi translated"><strong class="jn hj">绿色x </strong>在两个帧中都可见，内核显示该像素的颜色是从两个帧内插的。</li><li id="ed1f" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">相比之下，由<strong class="jn hj">红色x </strong>指示的像素仅在第2帧中可见。我们发现<strong class="jn hj">帧1的子核中所有系数的总和几乎为零，这表明帧1对该像素</strong>没有贡献。</li><li id="22da" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">类似地，由青色x指示的像素仅在帧1中可见。</li></ul><h2 id="4b86" class="mr la hi bd jj ms mt mu le mv mw mx li ju my mz lk jy na nb lm kc nc nd lo ne bi translated">6.4.边缘感知像素插值</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es nm"><img src="../Images/07439dea4ab7752ab69da104d0174aec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MY_7X9td2D_Pp_0LLdH1tg.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">卷积核</strong></figcaption></figure><ul class=""><li id="3b4d" class="lq lr hi jn b jo jp jr js ju mh jy mi kc mj kg mk ly lz ma bi translated">上图显示了内核如何适应图像特征。</li><li id="1dac" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">首先，对于所有这些内核，只有极少数内核元素具有非零值。此外，所有这些非零元素在空间上分组在一起。这非常符合典型的基于流量的插值方法。</li><li id="3f99" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">第二，对于<strong class="jn hj">平坦区域</strong>中的像素，例如由<strong class="jn hj">绿色x </strong>指示的像素，其内核<strong class="jn hj">仅具有两个具有有效值</strong>的元素。这也与基于流量的插值方法一致。</li><li id="6e35" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">第三，更有趣的是，<strong class="jn hj">对于沿着图像边缘</strong>的像素，例如由<strong class="jn hj">红色和青色x </strong>、<strong class="jn hj">指示的像素，内核是各向异性的，并且它们的方向与边缘方向</strong>很好地对齐。</li></ul><h2 id="e936" class="mr la hi bd jj ms mt mu le mv mw mx li ju my mz lk jy na nb lm kc nc nd lo ne bi translated">6.5.运行时间和内存</h2><ul class=""><li id="b5c6" class="lq lr hi jn b jo ls jr lt ju lu jy lv kc lw kg mk ly lz ma bi translated">在单个Nvidia Titan X上，对于640×480的图像，这种实现需要大约2.8秒，内存为3.5，</li><li id="1bdf" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">9.1秒，4.7千兆字节，1280×720，以及</li><li id="f5d9" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">1920×1080的21.6秒。</li></ul><h2 id="d764" class="mr la hi bd jj ms mt mu le mv mw mx li ju my mz lk jy na nb lm kc nc nd lo ne bi translated">6.6.内核大小</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es nn"><img src="../Images/acbeed0c32f27b2f2d9c02cedce627c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9XweYgXWSL-9iCoGSyWDWw.png"/></div></div></figure><ul class=""><li id="97bb" class="lq lr hi jn b jo jp jr js ju mh jy mi kc mj kg mk ly lz ma bi translated">该方法所能处理的必然受到卷积核大小的限制。系统目前无法处理任何超过41像素的大运动。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="er es no"><img src="../Images/2d3386136f32db728b92b98a2cd3df33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z7pUzAJNWwJ204PtYm-tug.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated"><strong class="bd jj">立体图像的插值</strong></figcaption></figure><ul class=""><li id="6a03" class="lq lr hi jn b jo jp jr js ju mh jy mi kc mj kg mk ly lz ma bi translated">上图显示了来自KITTI基准的一对立体图像。</li><li id="e960" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">当使用所提出的方法在左视图和右视图之间内插中间帧时，由于大的视差(超过41个像素)，汽车是模糊的，如(c)所示。</li><li id="2e1e" class="lq lr hi jn b jo mb jr mc ju md jy me kc mf kg mk ly lz ma bi translated">在将输入图像缩小到其原始尺寸的一半之后，所提出的方法插值得很好，如(d)所示。</li></ul><h2 id="7017" class="mr la hi bd jj ms mt mu le mv mw mx li ju my mz lk jy na nb lm kc nc nd lo ne bi translated">6.7.其他人</h2><ul class=""><li id="ace1" class="lq lr hi jn b jo ls jr lt ju lu jy lv kc lw kg mk ly lz ma bi translated">所提出的方法不能在任意时间内插一帧。现在只能运行在<em class="ml"> t </em> =0.5。</li></ul></div><div class="ab cl ks kt gp ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="hb hc hd he hf"><blockquote class="np nq nr"><p id="d049" class="jl jm ml jn b jo jp ij jq jr js im jt ns jv jw jx nt jz ka kb nu kd ke kf kg hb bi translated">在冠状病毒肆虐的日子里，给我一个挑战，这个月再写30个故事..？好吃吗？这是这个月的第五个故事了。感谢访问我的故事..</p></blockquote></div><div class="ab cl ks kt gp ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="hb hc hd he hf"><h2 id="ae03" class="mr la hi bd jj ms mt mu le mv mw mx li ju my mz lk jy na nb lm kc nc nd lo ne bi translated">参考</h2><p id="5578" class="pw-post-body-paragraph jl jm hi jn b jo ls ij jq jr lt im jt ju nv jw jx jy nw ka kb kc nx ke kf kg hb bi translated">【2017 CVPR】【阿达康】<br/> <a class="ae jk" href="https://arxiv.org/abs/1703.07514" rel="noopener ugc nofollow" target="_blank">通过自适应卷积的视频帧内插</a></p><h2 id="1c31" class="mr la hi bd jj ms mt mu le mv mw mx li ju my mz lk jy na nb lm kc nc nd lo ne bi translated">视频帧插值</h2><p id="a7ab" class="pw-post-body-paragraph jl jm hi jn b jo ls ij jq jr lt im jt ju nv jw jx jy nw ka kb kc nx ke kf kg hb bi translated">[ <a class="ae jk" rel="noopener" href="/analytics-vidhya/review-adaconv-video-frame-interpolation-via-adaptive-convolution-video-frame-interpolation-fbce6acaa2a5"> AdaConv </a> ]</p><h2 id="e98f" class="mr la hi bd jj ms mt mu le mv mw mx li ju my mz lk jy na nb lm kc nc nd lo ne bi translated"><a class="ae jk" rel="noopener" href="/@sh.tsang/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e">我之前的其他评论</a></h2></div></div>    
</body>
</html>