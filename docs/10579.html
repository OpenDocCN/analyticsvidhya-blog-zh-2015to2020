<html>
<head>
<title>Dimensionality Reduction Techniques in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的降维技术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/dimensionality-reduction-techniques-in-machine-learning-9098037baddc?source=collection_archive---------5-----------------------#2020-10-25">https://medium.com/analytics-vidhya/dimensionality-reduction-techniques-in-machine-learning-9098037baddc?source=collection_archive---------5-----------------------#2020-10-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/467e5da6d6fcf33cf4858900cc52926e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9aMzUlzHYAZyUDHM__gnXw.jpeg"/></div></div></figure><h1 id="8a09" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">什么是降维？</strong></h1><p id="9b88" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在机器学习问题中，往往有太多的因素是在这些因素的基础上做最后的分类的。这些因素基本上是称为特征的变量。特征的数量越多，可视化训练集并对其进行处理就越困难。<strong class="jq hj">选择用于模型构建的特征子集的过程被称为降维。</strong></p><p id="3974" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在学习降维技术之前，让我们理解为什么在我们的数据集中降维是重要的。</p><p id="0660" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">原因:<br/> 1) </strong>冗余和无关特征的丰富度<br/> <strong class="jq hj"> 2) </strong>在训练样本数量固定的情况下，预测能力随着维度的增加而降低。【<em class="kr">休斯现象</em>】<br/><strong class="jq hj">3)</strong>其他条件相同的情况下，简单的解释一般比复杂的好。<br/> <strong class="jq hj"> 4) </strong>如果选择了正确的子集，它会提高模型的准确性。<br/> <strong class="jq hj"> 5) </strong>减少过拟合。<br/>T22)6)它减少了计算时间。它有助于数据压缩，从而减少存储空间。</p><h1 id="faeb" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">维度缩减技术</h1><ol class=""><li id="1b82" class="ks kt hi jq b jr js jv jw jz ku kd kv kh kw kl kx ky kz la bi translated">缺失值的百分比</li><li id="f05f" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl kx ky kz la bi translated">变化量</li><li id="0308" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl kx ky kz la bi translated">多重共线性</li><li id="5a46" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl kx ky kz la bi translated">主成分分析</li><li id="7736" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl kx ky kz la bi translated">(与目标的)相关性</li><li id="c730" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl kx ky kz la bi translated">预选</li><li id="f3cd" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl kx ky kz la bi translated">反向消除</li><li id="3ad2" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl kx ky kz la bi translated">套索</li></ol><h1 id="54a1" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">1.缺失值的百分比</h1><ul class=""><li id="c5ec" class="ks kt hi jq b jr js jv jw jz ku kd kv kh kw kl lg ky kz la bi translated">删除缺失值百分比非常高的变量/要素。</li><li id="3a28" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl lg ky kz la bi translated">查看或可视化缺失值百分比较高的变量</li></ul><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/8eb7cbeecd34596747fbe4e5096f4bd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MCq_61MYErGEAsEYlVYLbw.jpeg"/></div></div></figure><h1 id="2803" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">2.变化量</h1><ul class=""><li id="af16" class="ks kt hi jq b jr js jv jw jz ku kd kv kh kw kl lg ky kz la bi translated">丢弃变化很小的变量。</li><li id="4fe6" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl lg ky kz la bi translated">要么对所有变量进行标准化，要么使用标准差𝜎来解释不同尺度的变量。</li><li id="66ab" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl lg ky kz la bi translated">删除零变异变量。</li></ul><h1 id="35d1" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 3。多重共线性</strong></h1><ul class=""><li id="245c" class="ks kt hi jq b jr js jv jw jz ku kd kv kh kw kl lg ky kz la bi translated">许多变量经常是相互关联的，因此是多余的。</li><li id="44da" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl lg ky kz la bi translated">如果两个或更多的变量高度相关，只保留一个将有助于减少维度，而不会丢失太多信息。</li><li id="023a" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl lg ky kz la bi translated">要保留哪个变量？与目标具有较高相关系数的那个。</li></ul><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/954f88d811d1509fb3e5c97fcd44589f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rHrsklkNkk1w39Yr_fcdYg.png"/></div></div></figure><h1 id="f3bf" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 4。主成分分析</strong></h1><ul class=""><li id="7339" class="ks kt hi jq b jr js jv jw jz ku kd kv kh kw kl lg ky kz la bi translated"><a class="ae ln" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank"> PCA </a>被定义为一种正交线性变换，它将数据变换到一个新的坐标系，使得数据的某个标量投影的最大方差位于第一坐标上(称为第一主分量)，第二大方差位于第二坐标上，依此类推。</li><li id="32bc" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl lg ky kz la bi translated">强调变化的降维技术。</li></ul><p id="60e7" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">何时使用:</strong></p><ul class=""><li id="bdee" class="ks kt hi jq b jr km jv kn jz lo kd lp kh lq kl lg ky kz la bi translated">过度多重共线性</li><li id="8fed" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl lg ky kz la bi translated">对预测因子的解释并不重要。</li></ul><h1 id="b19c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">5.(与目标的)相关性</h1><ul class=""><li id="b385" class="ks kt hi jq b jr js jv jw jz ku kd kv kh kw kl lg ky kz la bi translated">丢弃与目标相关性非常低的变量。</li><li id="b7d6" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl lg ky kz la bi translated">如果一个变量对目标的修正非常低，那么它对模型(预测)就没有用。</li></ul><h1 id="c5ea" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">6.预选</h1><p id="49f1" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">前向选择是一种迭代方法，我们从模型中没有特征开始。在每一次迭代中，我们不断地添加最能改进我们模型的特性，直到添加一个新变量不能改进模型的性能。</p><ol class=""><li id="3dc6" class="ks kt hi jq b jr km jv kn jz lo kd lp kh lq kl kx ky kz la bi translated">确定最佳变量。(例如，基于模型精度)</li><li id="2b9c" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl kx ky kz la bi translated">将次佳变量添加到模型中。</li><li id="2761" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl kx ky kz la bi translated">等等，直到满足一些预定的标准。</li></ol><h1 id="bf8f" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">7.反向消除</h1><p id="6caa" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在向后消除中，我们从所有特征开始，并在每次迭代中移除最不重要的特征，这提高了模型的性能。我们重复这一过程，直到在删除特征时没有观察到改进。</p><ol class=""><li id="6d40" class="ks kt hi jq b jr km jv kn jz lo kd lp kh lq kl kx ky kz la bi translated">从模型中包含的所有变量开始。</li><li id="863b" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl kx ky kz la bi translated">丢弃最没用的变量(例如，基于模型精度的最小下降)</li><li id="b706" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl kx ky kz la bi translated">等等，直到满足一些预定的标准。</li></ol><h1 id="c927" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">8.套索</h1><ul class=""><li id="5b8f" class="ks kt hi jq b jr js jv jw jz ku kd kv kh kw kl lg ky kz la bi translated">使用带有L1正则化的线性回归称为Lasso正则化。</li><li id="7859" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl lg ky kz la bi translated"><a class="ae ln" href="https://en.wikipedia.org/wiki/Lasso_(statistics)" rel="noopener ugc nofollow" target="_blank"><strong class="jq hj"/></a>套索方法对模型参数的绝对值之和进行约束，该和必须小于一个固定值(上限)。为了做到这一点，该方法应用收缩(正则化)过程，其中它惩罚回归变量的系数，将它们中的一些收缩到零。</li></ul></div></div>    
</body>
</html>