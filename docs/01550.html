<html>
<head>
<title>Artificial Neural Networks -2, explained differently to my son</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工神经网络-2，对我儿子的解释不同</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/artificial-neural-networks-2-explained-differently-to-my-son-e9fcfa19a5b7?source=collection_archive---------6-----------------------#2019-10-30">https://medium.com/analytics-vidhya/artificial-neural-networks-2-explained-differently-to-my-son-e9fcfa19a5b7?source=collection_archive---------6-----------------------#2019-10-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6d2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“唯一真正有价值的是直觉。”艾伯特·爱因斯坦。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/8db9513011d8d2e1e28c932e38021a2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nKy9VENaj4a1JML1-zZOKw.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">不管自然做什么，不管它是什么，我们都认为它是自然的。这是绝对的信仰。相信我们的直觉会更快乐。</figcaption></figure><blockquote class="jt"><p id="18cf" class="ju jv hi bd jw jx jy jz ka kb kc jc dx translated">这篇文章之前是另一篇文章:<a class="ae kd" rel="noopener" href="/@fleetpro/artificial-neural-networks-1-explained-to-beginners-and-my-son-36722943fca2">人工神经网络-1，对我儿子的解释不同。</a></p></blockquote><figure class="kf kg kh ki kj ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ke"><img src="../Images/f1a8ebc5a222e9ea20f8d3d2b1874719.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Jr6ngiYgpJEqLzMHG9lvw.png"/></div></div></figure><p id="7708" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">目标</strong></p><p id="4b76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章的目的是用最简单自然直观的方式向人工智能初学者解释人工神经网络训练技术的基本原理。</p><p id="3578" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">在之前的帖子中，我们做了… </strong></p><p id="6bc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们发现，如果我们通过从范围中移除所有领域知识来在训练过程中进行纯统计，预测就会变得模糊不清。我们必须引入预测的直观定义。如果一个预测符合我们的直觉，那么它就是正确的。AI预测很直观。</p><p id="a406" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将一个简单的统计任务定义如下:</p><ul class=""><li id="1823" class="kk kl hi ih b ii ij im in iq km iu kn iy ko jc kp kq kr ks bi translated">已知的事实有:(1，5)和(3，25)</li><li id="581a" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">什么是x如果(2，x)是事实，</li></ul><p id="a833" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中(features，label)代表一个领域知识:“母鸡在‘features’——第个月下了‘label’蛋”，虽然现在领域知识的重要性被忽略了。</p><p id="f991" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在模型空间中随机选取了初始的SM(主观模型),并打算将SM推向OM(客观模型),这是我们无法精确定位的。我们只知道事实(1，OM(1))和(3，OM(3))。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ky"><img src="../Images/8eb33e17b11567119edd8d37abfa5c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bgpsCsJXr4QarM-U.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">将SM驱动到OM意味着同时将SM(1)和SM(3)分别驱动到OM(1)和OM(3)。</figcaption></figure><p id="f0ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">耐心点，加入乐趣。</p><p id="b640" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">如何接近OM(目标模型)</strong></p><p id="7152" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将SM驱动到OM意味着同时将SM(1)和SM(3)分别驱动到OM(1)和OM(3)。</p><ul class=""><li id="1928" class="kk kl hi ih b ii ij im in iq km iu kn iy ko jc kp kq kr ks bi translated">SM(1) -&gt; OM(1)</li><li id="8bca" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">SM(3) -&gt; OM(3)</li></ul><p id="e667" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">，这要通过调整SM的参数来完成，即[y = w * x + b]的(w，b)。这意味着SM在模型空间中朝着OM航行。注OM本身可能没有嵌入到(主观)模型空间中。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kz"><img src="../Images/87aa82be07fb91ed52f445af50b84f46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*sDO3YdKBi8cMff6P.png"/></div></div></figure><p id="b1b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">啰嗦。只管打！</strong></p><p id="76dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“离om近或远”意味着距离的概念。当前SM和幻影OM之间的距离也是部分已知的。</p><p id="4778" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将两个向量之间的距离定义如下:</p><ul class=""><li id="808d" class="kk kl hi ih b ii ij im in iq km iu kn iy ko jc kp kq kr ks bi translated">distVec，作为两个实向量的距离。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es la"><img src="../Images/2b40e1bd129bdd6defe1d969461597ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wl37YapnnJvYCZgKZgWBVw.png"/></div></div></figure><p id="9524" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然距离有直观的定义，但我们选择如上所示的失真距离的原因将在后面讨论。(距离的合格性由<a class="ae kd" href="https://en.wikipedia.org/wiki/Metric_(mathematics)" rel="noopener ugc nofollow" target="_blank">度量公理</a>定义，如果你是一个数学极客。)</p><p id="12a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">定义距离的代码</strong></p><p id="90f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面定义的距离可以编码如下:</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="7157" class="lg lh hi lc b fi li lj l lk ll"># Comments were removed to save space.</span><span id="a30c" class="lg lh hi lc b fi lm lj l lk ll">import numpy as np</span><span id="badb" class="lg lh hi lc b fi lm lj l lk ll">def distVec(A, B):<br/>   """Returns the distance of couples of vectors, which reside on   <br/>   the last axises of A and B, respectively."""<br/>   C = np.power(A - B, 2) / 2<br/>   return np.sum(C, axis = len(C.shape) - 1)</span></pre><p id="1401" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">函数distVec返回向量对的距离，可以接受任何形状的多维矩阵，该矩阵在其最后一个轴上包含向量。</p><p id="0f53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意！我们在潜水。</strong></p><p id="f27a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们定义了从给定事实中分别提取特征集和标签的函数factElems。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="8f32" class="lg lh hi lc b fi li lj l lk ll">def factsElems(facts):<br/>   """Returns the features set and label of facts, which reside on    <br/>   the last axis."""<br/>   return facts[..., :-1], facts[..., -1]</span></pre><p id="cef2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还定义了从给定的事实中分别提取权重和偏差的函数。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="d95b" class="lg lh hi lc b fi li lj l lk ll">def smElems(sm):<br/>   """Returns the weight and bias components of linear subjective <br/>   models sm."""<br/>   return sm[..., :-1], sm[..., -1:]</span></pre><p id="1299" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于上面定义的函数层，我们引入了函数smLinPredict，它返回由线性SM模型为给定特征集预测的标签。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="89a0" class="lg lh hi lc b fi li lj l lk ll">def smLinPredict(sm, features):<br/>   """Returns the labels predicted by linear SM models for given <br/>   features set."""<br/>   smW, smB = smElems(sm)<br/>   return np.matmul( smW, np.transpose(features) ) + smB</span></pre><p id="3c84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SM和OM之间的距离是SM的缺点导致的损失。我们不知道它们之间的完整距离，只是部分距离。它是部分的，因为它是关于部分已知/已知的事实。我们希望减少每一个部分的距离将有助于减少未知的，完整的距离。注意，距离是一个简单的线性和。</p><p id="c046" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，对于fact1 = (1，5)，SM和OM之间的距离或SM的损耗为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ln"><img src="../Images/79bc9da0a9698971d0004d3caea0308e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OYEn4GTCFxjECj7V8TCbXA.png"/></div></div></figure><p id="2c40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，fact2 = (3，25)的距离是450。因此，事实fact1和fact2的距离是50 + 450 = 500。</p><p id="1d4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们必须找到一个新的(w，b ),而不是上面的(-10，25 ),给出一个小于500的新损失。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lo"><img src="../Images/416220f6159b6dbcf657bcb6a4f295ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JGgNEX5ycK7w05VoEPohqQ.png"/></div></div></figure><p id="611b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，是时候定义函数损失了:</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="1de3" class="lg lh hi lc b fi li lj l lk ll">def loss(sm, facts):<br/>   """Returns the loss by an array of SMs for given facts."""<br/>   features, omLabels = factsElems(facts)<br/>   smLabels = smLinPredict(sm, features)<br/>   #smLabels = sigmoid( smLinPredict(sm, features), 0.1 )<br/>   return distVec(smLabels, omLabels)</span></pre><p id="99f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是一些测试，也有助于我们理解:</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="f75d" class="lg lh hi lc b fi li lj l lk ll">fact1 = np.array([1,5]); facts = np.array([ fact1 ])<br/>sm1 = np.array([-10, 25]); sm = np.array([sm1])<br/>print("loss = {}".format(loss(sm, facts)))<br/>Outputs: loss = [50.]</span><span id="6cbb" class="lg lh hi lc b fi lm lj l lk ll">fact2 = np.array([3,25]); facts = np.array([ fact2 ])<br/>sm1 = np.array([-10, 25]); sm = np.array([sm1])<br/>print("loss = {}".format(loss(sm, facts)))<br/>Outputs: loss = [450.]</span><span id="cee4" class="lg lh hi lc b fi lm lj l lk ll">facts = np.array([ fact1, fact2 ])<br/>sm1 = np.array([-10, 25]); sm = np.array([sm1])<br/>print("loss = {}".format(loss(sm, facts)))<br/>Outputs: loss = [500.] # 50 + 450</span></pre><p id="d5d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们定义的基本函数层通过一组SMs为一组facts批量计算损失，测试如下:</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="96a6" class="lg lh hi lc b fi li lj l lk ll">facts = np.array([ fact1, fact2 ])<br/>sm11 = np.array([-10, 25]); sm12 = np.array([-9, 20]); sm21 = np.array([-12, 20]); sm22 = np.array([-8, 28])<br/>sm = np.array([ [sm11, sm12], [sm21, sm22] ])<br/>print("loss = {}".format(loss(sm, facts)))<br/>Outputs: loss = [ [500. 530.] [845. 333.] ]</span></pre><p id="f23c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">“唯一真正有价值的东西是直觉。”</strong></p><p id="8ed3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">漫长的编码之旅，我们已经准备好批量处理一个模型空间，而不是一次一个模型，来可视化损失分布。</p><p id="b8da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算损失分布的表面:</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="4d96" class="lg lh hi lc b fi li lj l lk ll">w = np.arange(-10, 20, 1)<br/>b = np.arange(-10, 10, 1)<br/>W, B = np.meshgrid(w, b)<br/>smW = W.reshape(tuple(np.append(W.shape,1)))<br/>smB = B.reshape(tuple(np.append(B.shape,1)))<br/>sm = np.append(smW, smB, axis = len(smW.shape)-1)<br/>fact1 = np.array([1, 5]); fact2 = np.array([3, 25]);<br/>facts = np.array([fact1, fact2])<br/>Z = loss(sm, facts)</span></pre><p id="d138" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为我们配备绘图工具:</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="5674" class="lg lh hi lc b fi li lj l lk ll">import matplotlib.pyplot as plt<br/>from matplotlib import animation<br/>from matplotlib import cm<br/>from matplotlib.ticker import LinearLocator, FormatStrFormatter<br/>from mpl_toolkits.mplot3d import Axes3D</span></pre><p id="900f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">创建一个图形来绘制损失面，并格式化轴:</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="65ee" class="lg lh hi lc b fi li lj l lk ll">fig = plt.figure(dpi = 120)<br/>ax = fig.gca(projection = '3d')</span><span id="993d" class="lg lh hi lc b fi lm lj l lk ll">title = 'Loss by (w, b) for (fact1, fact2)'; zmax = ax.set_title(title, fontsize=12, fontweight='normal', color='b')<br/>ax.set_xlabel('w', fontsize=14, fontweight='normal', color='b')<br/>ax.set_ylabel('b', fontsize=14, fontweight='normal', color='b')</span></pre><p id="add4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决定代表损失面高度的颜色:</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="7b20" class="lg lh hi lc b fi li lj l lk ll">norm = plt.Normalize(Z.min(), Z.max())<br/>colors = cm.viridis(norm(Z))<br/>rcount, ccount, _ = colors.shape</span></pre><p id="97e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将绘制损失面的任务交给图轴:</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="8884" class="lg lh hi lc b fi li lj l lk ll">surf = ax.plot_surface(W, B, Z, rcount=rcount, ccount=ccount, facecolors=colors, shade=False)<br/>surf.set_facecolor((0,0,0,0))</span></pre><p id="aec1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让情节执行所有给定的任务:</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="2523" class="lg lh hi lc b fi li lj l lk ll">plt.show()</span></pre><p id="bad0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该图显示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lp"><img src="../Images/091c07bebc75f2dadb7a8b9ce33ebcec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_kuKEpRazHSXYt14bHo4zQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">损失在(w，b) = (10，-5)处具有最小值= 0，这表示主观模型</figcaption></figure><p id="436f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下用代数计算，与图相符。</p><ul class=""><li id="2578" class="kk kl hi ih b ii ij im in iq km iu kn iy ko jc kp kq kr ks bi translated">SM(10，-5)最符合(因素1，因素2)，损失为0</li><li id="9624" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">SM(5，15)最符合(fact3，fact4)，损耗为0</li><li id="78bd" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">SM(6，5)最符合(因素1，因素2，因素3，因素4)，损失15</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lq"><img src="../Images/c983c62973ed74e1df9012208c637017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_jn7tejg-bsOEg-e5ItW3w.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">a.min = 0 at (10，-5)，b. min = 0 at (5，15)，c. min = 15 at (8，-2)</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lr"><img src="../Images/3474c31ae2d54194dbf703066c0c91a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Qlqz10e16dhBX2xm-XiCYg.gif"/></div></div></figure><p id="0fe9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上述计算的直觉如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ls"><img src="../Images/8d58048f74f19e5d4fdfe5281bf995ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*axq7W3OtipQ0ftKPoQzHvg.png"/></div></div></figure><p id="374f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">挑战:</strong></p><ul class=""><li id="a2f9" class="kk kl hi ih b ii ij im in iq km iu kn iy ko jc kp kq kr ks bi translated">如何在统计上得到(fact1，fact2)的(10，5)</li><li id="36a1" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">如何统计地得到(fact3，fact4)的(5，15)</li><li id="2242" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">如何在统计上得到(fact1，…，fact4)的(6，5)</li><li id="5c9e" class="kk kl hi ih b ii kt im ku iq kv iu kw iy kx jc kp kq kr ks bi translated">了解全部事实(事实1，…，事实4)，还是挖墙脚？</li></ul><p id="4f08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望我们能在下一篇文章中完成这段旅程。</p><blockquote class="jt"><p id="0448" class="ju jv hi bd jw jx lt lu lv lw lx jc dx translated">这个帖子之后会有另一个帖子:AI有人工神经元，而你有真神经元。掌握它，我的儿子！</p></blockquote></div></div>    
</body>
</html>