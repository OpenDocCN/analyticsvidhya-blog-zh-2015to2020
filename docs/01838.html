<html>
<head>
<title>Getting hands dirty in Spark Delta Lake</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在火花三角洲湖弄脏手</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/getting-hands-dirty-in-spark-delta-lake-1963921e4de6?source=collection_archive---------0-----------------------#2019-11-17">https://medium.com/analytics-vidhya/getting-hands-dirty-in-spark-delta-lake-1963921e4de6?source=collection_archive---------0-----------------------#2019-11-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="2168" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">从亲身体验中了解三角洲湖的世界</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/a1fc5004243058a4100d64ceebdcd363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*vxgVZ4LubyxAW43YYH8Law.jpeg"/></div></figure><p id="0e0b" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">嘿伙计们，</p><p id="4f6e" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这是我文章的第二部分，我在文章中概述了Delta Lake及其使用案例，如果您碰巧直接来到这里，请阅读我的第一篇文章以了解一些背景。这是链接</p><div class="kb kc ez fb kd ke"><a rel="noopener follow" target="_blank" href="/@merchantabid4/spark-delta-lake-d05dd480287a"><div class="kf ab dw"><div class="kg ab kh cl cj ki"><h2 class="bd hj fi z dy kj ea eb kk ed ef hh bi translated">火花三角洲湖</h2><div class="kl l"><h3 class="bd b fi z dy kj ea eb kk ed ef dx translated">嘿伙计们，</h3></div><div class="km l"><p class="bd b fp z dy kj ea eb kk ed ef dx translated">medium.com</p></div></div><div class="kn l"><div class="ko l kp kq kr kn ks jd ke"/></div></div></a></div><p id="0e1b" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因此，在本文中，我们将介绍Delta Lake所做的令人惊叹的工作，代码在Pyspark中。首先，让我们看看如何让三角洲湖出火花笔记本。</p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="7fbf" class="ky kz hi ku b fi la lb l lc ld">pip install --upgrade pyspark</span><span id="9691" class="ky kz hi ku b fi le lb l lc ld">pyspark --packages io.delta:delta-core_2.11:0.4.0</span></pre><p id="4d06" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如果你已经有一个升级的spark，第一个命令是不必要的，第二个命令将启动我们的pyspark shell和Delta包，瞧！安装部分没有更多的事情要做。请注意，Delta lake所需的最低Spark版本是Spark 2.4.2。</p><p id="4566" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因此，根据我以前的文章，三角洲湖给我们带来了以下重要的业务:</p><ul class=""><li id="dcb0" class="lf lg hi jh b ji jj jl jm jo lh js li jw lj ka lk ll lm ln bi translated">架构实施</li><li id="2edc" class="lf lg hi jh b ji lo jl lp jo lq js lr jw ls ka lk ll lm ln bi translated">数据湖上的ACID事务</li><li id="751e" class="lf lg hi jh b ji lo jl lp jo lq js lr jw ls ka lk ll lm ln bi translated">时间旅行能力</li></ul><p id="4587" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们将逐一介绍这些功能，首先让我们了解模式实施。</p><h1 id="2d61" class="lt kz hi bd lu lv lw lx ly lz ma mb mc io md ip me ir mf is mg iu mh iv mi mj bi translated">架构实施</h1><p id="f150" class="pw-post-body-paragraph jf jg hi jh b ji mk ij jk jl ml im jn jo mm jq jr js mn ju jv jw mo jy jz ka hb bi translated">正如我所说的，Delta Lake遵循“写时模式”,所以写时模式的任何变化都将被跟踪，任何差异都将在那时引发异常。下面的代码将制作一个由1-5个数字组成的数据帧，我们将把它写成一个增量表。</p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="c0a1" class="ky kz hi ku b fi la lb l lc ld">data = spark.range(1,5)<br/>data.write.format("delta").mode("overwrite").save("/mnt/data/delta_sample")</span></pre><p id="3296" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们签出输出文件</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mp"><img src="../Images/4ba3f7d1f22eea43050a9f37e99c7548.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*KqNg5qFiZgmNAPwQ4FBXkg.png"/></div></figure><p id="fec3" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">你可以看到写格式是拼花地板。维护一个包含所有交易日志的日志文件。日志文件是一个JSON文件，其中包含关于在数据集上执行的操作的元数据，该日志文件将首先由spark读取，而不是直接读取零件文件。</p><p id="2cb4" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在，我将创建一个数字为5-10的数据帧，并将它的数据类型指定为字符串，并尝试将数据集追加到我们现有的数据集上。</p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="48f6" class="ky kz hi ku b fi la lb l lc ld">import pyspark.sql.functions as fn<br/>new_data = spark.range(5,10)<br/>new_data = new_data.withColumn("id",fn.col("id").cast("String"))<br/>new_data.write.format("delta").mode("append").save("/mnt/data/delta_sample")</span></pre><p id="f354" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">执行下面的代码片段导致我犯了一个错误，其中提到:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="er es mq"><img src="../Images/7adb269bfa45d38e1be5f56e15595b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VscMxQ2RmjyySgJQuxoI3g.png"/></div></div></figure><p id="18b7" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因此，Delta lake阻止了不正确的数据进入我们的delta Lake，而且在将我的列名从“id”改为“id1”时，我得到了另一个异常。因此，作为一个整体，在写入任何现有数据集时，不允许任何更改。最后，我将用正确的模式追加数据集。</p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="682a" class="ky kz hi ku b fi la lb l lc ld">new_data = spark.range(5,10)<br/>new_data.write.format("delta").mode("append").save("/mnt/data/delta_sample")</span></pre><p id="65b7" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在，我的数据集包含以下文件:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mv"><img src="../Images/42d70a94e718eb70134211f853bed64a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*OZrKosgyMwNchqVhEUzExQ.png"/></div></figure><p id="f5ac" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们看一下日志文件:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mw"><img src="../Images/1a8c8005b306a344ab18d8220ab44745.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*XtYAHlA2oEL1heF0ZTnp2g.png"/></div></figure><p id="1ee5" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因此，当我写入数据集时，会生成新的日志文件，该日志文件包含以下内容:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="er es mx"><img src="../Images/58798c99967fb949840ab9ebbebd926a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5dnzqEHumU4ZuVfSX03rQ.png"/></div></div></figure><p id="6578" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">它指示spark添加新写入数据集的零件文件以及指定信息，如写入模式和修改时间。</p><p id="b1fb" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因此，这就是写入模式的实施方式和Delta Lake表的工作方式。现在，我们将深入了解Spark中的合并、更新和删除操作。</p><h1 id="02ea" class="lt kz hi bd lu lv lw lx ly lz ma mb mc io md ip me ir mf is mg iu mh iv mi mj bi translated">删除</h1><p id="4af6" class="pw-post-body-paragraph jf jg hi jh b ji mk ij jk jl ml im jn jo mm jq jr js mn ju jv jw mo jy jz ka hb bi translated">现在，在执行删除操作之前，让我们读取Delta格式的表，我们将读取刚刚写入的数据集。遵循下面的代码行。</p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="5a9c" class="ky kz hi ku b fi la lb l lc ld">from delta.tables import *</span><span id="528e" class="ky kz hi ku b fi le lb l lc ld">delta_df = DeltaTable.forPath(spark, "/mnt/data/delta")</span></pre><p id="0c48" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">Delta_df属于Delta table类型，我们不能对其执行spark数据帧操作，但可以通过delta_df.toDF()操作将其转换为Spark数据帧。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="er es my"><img src="../Images/b48677368c284ebc0dfe364aa8ce1b21.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*rr5jJR_-pdKd0S6ILs_dJA.png"/></div></div></figure><p id="04a1" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在，我们将删除id≤2的数据。</p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="e4af" class="ky kz hi ku b fi la lb l lc ld">delta_df.delete("id&lt;=2")</span></pre><p id="2f31" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">就是这样，增量表是<strong class="jh hj"> <em class="mz">自动刷新</em> </strong>，我们在这里对数据做的任何更改都会直接反映到我们的数据集中。所以，就在执行上述删除操作之后，另一个会话上的某人读取我们的数据集将不会找到id≤2。删除后，又创建了一个提交日志，让我们检查一下提交日志是如何为删除操作编写的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="er es na"><img src="../Images/eea216a3f2bbf758ff12ac12456b1a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3X0HcEd2Y13eeSYNrPH4QA.png"/></div></div></figure><p id="732c" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">它指导spark通过remove删除原始零件文件，然后添加谓词为id≤2的新零件文件，即使指定了执行的操作(删除)。因此，提交日志是详尽的，并且是我们数据集的所有元数据的一站式服务。现在，我们将对数据集进行更新。</p><h1 id="b335" class="lt kz hi bd lu lv lw lx ly lz ma mb mc io md ip me ir mf is mg iu mh iv mi mj bi translated">更新</h1><p id="6938" class="pw-post-body-paragraph jf jg hi jh b ji mk ij jk jl ml im jn jo mm jq jr js mn ju jv jw mo jy jz ka hb bi translated">现在，在这一部分中，我们将尝试对数据进行更新操作。我们将再次读取数据集，并将值从5更新到500。签出更新操作的代码段</p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="66f1" class="ky kz hi ku b fi la lb l lc ld">delta_df = DeltaTable.forPath(spark, "/mnt/data/delta")<br/>delta_df.update(condition = "id = 5", set = { "id": "500" })\</span></pre><p id="b128" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">上面的操作将id设置为500，其中id为5，更不用说当增量表自动刷新时，数据被更新。正如你所看到的，语法非常简单，即使是外行也能一目了然。所以，我们进行最后的合并操作。</p><h1 id="5dab" class="lt kz hi bd lu lv lw lx ly lz ma mb mc io md ip me ir mf is mg iu mh iv mi mj bi translated">合并</h1><p id="2a5d" class="pw-post-body-paragraph jf jg hi jh b ji mk ij jk jl ml im jn jo mm jq jr js mn ju jv jw mo jy jz ka hb bi translated">现在，我们将在增量表上执行合并操作。在执行合并之前，我将读取一个包含国家、年份和温度列的新数据集，并将它写成一个增量表。</p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="08fc" class="ky kz hi ku b fi la lb l lc ld">df = spark.read.csv("/mnt/data/dataset", inferSchema=True, sep=',', header=True)</span><span id="bde7" class="ky kz hi ku b fi le lb l lc ld">df.write.format("delta").save("/mnt/data/delta_merge")</span></pre><p id="e8ed" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们的增量表包含以下数据…</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="er es nb"><img src="../Images/ef5b91a6b99200f2a97b1f0979459602.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*SKtWr5uN52lUQJC5-Csg_Q.png"/></div></div></figure><p id="683e" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在，我们有增量馈送，它必须合并到Spark数据帧中的增量表中，数据是…</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nc"><img src="../Images/1ecd870dfb85d4366a30b57e654ee4c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*rVHIvIl_CuJomGR1FN0Mew.png"/></div></figure><p id="6cb1" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们收到了2个新记录，其中<em class="mz">澳大利亚</em>将被添加，而<em class="mz">印度</em>将被合并到我们的增量表中。</p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="fdb2" class="ky kz hi ku b fi la lb l lc ld">delta_merge.alias("delta_merge").merge(<br/>    updatesDf.alias("updates"),<br/>    "delta_merge.country = updates.country") \<br/>  .whenMatchedUpdate(set = { <br/>        "temperature" : "updates.temperature",<br/>        "year" : "updates.year"<br/>  } ) \<br/>  .whenNotMatchedInsert(values =<br/>    {<br/>      "country": "updates.country",<br/>      "year": "updates.year",<br/>      "temperature": "updates.temperature"<br/>    }<br/>  ) \<br/>  .execute()</span></pre><p id="464b" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">merge语法非常类似于Hive中的merge语法。首先，我们给我们的表table一个别名“delta_merge ”,给我们的Spark数据帧一个别名“updates”。现在，当国家列在两个数据集中匹配时，我们将把温度和年份列从Spark Dataframe更新到我们的Delta Lake，当不匹配时，我们将把所有列插入到我们的Delta Lake中，这很简单，不是吗！</p><p id="b53f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">最终合并的记录将看起来像…</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="er es nd"><img src="../Images/d60617079d149d31338b52c60dde9995.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*YWonFa7pYhL-5FRmg5EqUQ.png"/></div></div></figure><p id="8f36" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因此，印度的温度更新为100.0，年份更新为2019年，这就是合并操作的全部内容。</p><h1 id="c0af" class="lt kz hi bd lu lv lw lx ly lz ma mb mc io md ip me ir mf is mg iu mh iv mi mj bi translated">时间旅行</h1><p id="6232" class="pw-post-body-paragraph jf jg hi jh b ji mk ij jk jl ml im jn jo mm jq jr js mn ju jv jw mo jy jz ka hb bi translated">这是三角洲湖最凉爽的部分。正如我在第一篇文章中解释的那样，使用Delta Lake，我们将能够维护数据集的不同版本，并且可以在需要时重用。在这一节中，我们将进一步了解如何获得数据集的不同版本。</p><p id="f97c" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在解释如何获取任何版本的数据之前，首先我将向您展示如何知道我必须获取哪个版本的数据，但是我如何知道我上次更新数据的时间、我更新了什么以及实际创建了多少个版本？要知道，Delta表维护了一个历史记录，可以通过调用history()方法获得，该方法将返回一个Spark数据帧，这样我们就可以查询它并知道我们想要获得的版本。历史数据将看起来像:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mr ms di mt bf mu"><div class="er es ne"><img src="../Images/5636554a58a7598012e942dcb8e595ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZYckr43SxWQlZoNCJdY1ag.png"/></div></div></figure><p id="a05e" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因此，如您所见，它包含了我们理解数据操作所需的每个细节。现在，我将读取我们最初创建的数据集，它只包含一个由数字组成的列“id”。我们摆弄了那个数据集，但在这么做的同时，我们所有的活动都被跟踪了，没有任何东西被德尔塔湖发现。事不宜迟，我将读取数据集。</p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="a04e" class="ky kz hi ku b fi la lb l lc ld">delta_df = DeltaTable.forPath(spark, "/mnt/data/delta")</span></pre><p id="009e" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们数据集的最终版本包含:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nf"><img src="../Images/be02b8716ecb2c712263b1052191704d.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*8rNV0k_LtOflAwOZeQJUsw.png"/></div></figure><p id="0571" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在，我将获取“版本1”的数据集，如历史表所示，我们已将数据追加到现有的数据集，如果您能回想一下，我们已将5–10个数字追加到我们的数据集。现在，让我们将这些数据放入Spark数据框中:</p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="6273" class="ky kz hi ku b fi la lb l lc ld">version_1 = spark.read.format("delta").option("versionAsOf",1).load("/mnt/data/delta")</span></pre><p id="2fc2" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这将为我们提供Spark Dataframe中从0到9的数据，您可以指定您想要的版本，并让您丢失的宝宝回到婴儿车中玩耍。</p><p id="a5e5" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">除了这些，Delta Lake的一个更重要的特性是在一段时间后删除旧版本数据。因为，我们不希望我们的存储系统在一段时间后爆炸。为此，我们有一个名为vacuum()的函数，它会递归地删除表中不需要的文件和目录，以便将旧版本保持到给定的保留阈值。该方法将在成功完成时返回一个空数据帧。</p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="1e6b" class="ky kz hi ku b fi la lb l lc ld">deltaTable<strong class="ku hj">.</strong>vacuum<strong class="ku hj">()</strong>     <em class="mz"># vacuum files not required by versions more than 7 days old</em><br/><br/>deltaTable<strong class="ku hj">.</strong>vacuum<strong class="ku hj">(100)</strong>  <em class="mz"># vacuum files not required by versions more than 100 hours old</em></span></pre><p id="13f8" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">所以，这就是所有的乡亲，希望你发现我的文章有帮助。</p><p id="743d" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如果你想核实我写的是真的:请找到下面的链接供你参考。</p><div class="kb kc ez fb kd ke"><a href="https://docs.delta.io/0.4.0/api/python/index.html" rel="noopener  ugc nofollow" target="_blank"><div class="kf ab dw"><div class="kg ab kh cl cj ki"><h2 class="bd hj fi z dy kj ea eb kk ed ef hh bi translated">欢迎来到Delta Lake的Python文档页面- delta-core 0.4.0文档</h2><div class="kl l"><h3 class="bd b fi z dy kj ea eb kk ed ef dx translated">用于以编程方式与增量表交互的主类。您可以使用以下路径创建DeltaTable实例…</h3></div><div class="km l"><p class="bd b fp z dy kj ea eb kk ed ef dx translated">文档增量io</p></div></div></div></a></div><div class="kb kc ez fb kd ke"><a href="https://docs.databricks.com/delta/delta-update.html" rel="noopener  ugc nofollow" target="_blank"><div class="kf ab dw"><div class="kg ab kh cl cj ki"><h2 class="bd hj fi z dy kj ea eb kk ed ef hh bi translated">表删除、更新和合并</h2><div class="kl l"><h3 class="bd b fi z dy kj ea eb kk ed ef dx translated">Delta Lake支持几个语句，以便于从Delta表中删除和更新数据。你可以…</h3></div><div class="km l"><p class="bd b fp z dy kj ea eb kk ed ef dx translated">docs.databricks.com</p></div></div></div></a></div></div></div>    
</body>
</html>