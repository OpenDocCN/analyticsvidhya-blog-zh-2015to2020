<html>
<head>
<title>Transforming Data with Embedded Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用嵌入式Spark转换数据</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/transforming-data-with-embedded-spark-65fecaf682ba?source=collection_archive---------7-----------------------#2019-11-27">https://medium.com/analytics-vidhya/transforming-data-with-embedded-spark-65fecaf682ba?source=collection_archive---------7-----------------------#2019-11-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ec00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们继续探索用Scala编写的Spark jobs，构建在本系列前一篇文章“Scala中Spark jobs的谨慎开发”中建立的基础之上。本文的目标是增强我们的transformer作业，以处理实际用例，并深入了解Spark作业执行的操作细节。文章组织如下:</p><p id="011b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.<a class="ae jd" href="#5d43" rel="noopener ugc nofollow">生成测试TPC-H数据</a> <br/> 2。<a class="ae jd" href="#da50" rel="noopener ugc nofollow">转换行项目数据</a> <br/> 2.1 <a class="ae jd" href="#8bea" rel="noopener ugc nofollow">选择表模式表示</a> <br/> 2.2 <a class="ae jd" href="#400e" rel="noopener ugc nofollow">定义行项目表模式</a> <br/> 2.3 <a class="ae jd" href="#198c" rel="noopener ugc nofollow">在配置单元中创建行项目表</a> <br/> 2.4 <a class="ae jd" href="#fea8" rel="noopener ugc nofollow">读取原始行项目数据</a> <br/> 2.5 <a class="ae jd" href="#f2eb" rel="noopener ugc nofollow">将行项目数据从CSV转换为ORC </a> <br/> 3 .<a class="ae jd" href="#6d7e" rel="noopener ugc nofollow">拾遗补缺洞察星火工作绩效</a> <br/> 4。<a class="ae jd" href="#226a" rel="noopener ugc nofollow">敲定功能实现</a> <br/> 5。<a class="ae jd" href="#0afb" rel="noopener ugc nofollow">总结</a></p><h1 id="5d43" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">生成测试TPC-H数据</h1><p id="b415" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在本文中，我们将实现<a class="ae jd" href="http://www.tpc.org/tpch/" rel="noopener ugc nofollow" target="_blank"> TPC-H基准</a>数据从原始文本格式到列格式的转换。来自TPC-H网站:</p><blockquote class="kh ki kj"><p id="773e" class="if ig kk ih b ii ij ik il im in io ip kl ir is it km iv iw ix kn iz ja jb jc hb bi translated">TPC Benchmark H(TPC-H)是一个决策支持基准。它由一套面向业务的特别查询和并发数据修改组成。</p></blockquote><p id="e860" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的目标不是对Spark转换作业进行基准测试，因为<a class="ae jd" href="http://www.tpc.org/tpcdi/default.asp" rel="noopener ugc nofollow" target="_blank"> TPC-DI基准测试</a>可能更合适，而是生成一些数据，我们可以使用这些数据根据转换的数据对Spark和其他查询引擎的查询性能进行基准测试。</p><p id="cb8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们用<a class="ae jd" href="https://github.com/gregrahn/tpch-kit" rel="noopener ugc nofollow" target="_blank"> tpch-kit </a>工具生成一些TPC-H样本数据。下面将创建一个大约100KB的基准数据集，它足够小，可以签入到项目repo中，并在由我们的<a class="ae jd" href="https://docs.gitlab.com/ee/ci/" rel="noopener ugc nofollow" target="_blank"> GitLab CI/CD管道</a>运行的集成测试中使用。或者，我们可以将样本数据集上传到一个共享位置，比如S3的一个存储桶，并配置构建过程在运行测试之前下载文件。(如果不是在Mac OS上运行，您可能需要调整build命令。)</p><figure class="ko kp kq kr fd ks"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="adab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">生成的TPC-H数据集由以下文件组成:customer.tbl、lineitem.tbl、nation.tbl、orders.tbl、part.tbl、partsupp.tbl、region.tbl、supplier.tbl。这些文件包含模拟自虚拟零件订购系统的数据，在该系统中，客户订购零件，供应商完成这些订单。</p><p id="7f31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们也生成可以针对TPC-H样本数据运行的TPC-H查询。下面将生成tpch-q${i}。qgen可执行文件所在文件夹中的sql文件。</p><figure class="ko kp kq kr fd ks"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="79d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">生成的查询允许我们回答一些关于模拟数据的业务问题。例如，下面是来自<a class="ae jd" href="http://www.tpc.org/tpc_documents_current_versions/current_specifications.asp" rel="noopener ugc nofollow" target="_blank"> TPC-H规范</a>的查询#3的描述:</p><blockquote class="kh ki kj"><p id="a0c5" class="if ig kk ih b ii ij ik il im in io ip kl ir is it km iv iw ix kn iz ja jb jc hb bi translated"><strong class="ih hj">发货优先级查询(Q3) </strong> <br/>该查询检索价值最高的10个未发货订单。</p></blockquote><p id="3c1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这应该让您对TPC-H基准数据和查询有所了解。</p><h1 id="da50" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">转换行项目数据</h1><p id="ac68" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">TPC-H基准数据集由几个表组成，在本节中，我们将为LineItem表实现一个转换器。所有表格的模式在完整的TPC-H规范中有所描述，可以从<a class="ae jd" href="http://www.tpc.org/tpc_documents_current_versions/current_specifications.asp" rel="noopener ugc nofollow" target="_blank"> TPC下载页面</a>下载PDF格式的表格。</p><p id="a514" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在这里的目标是从上一节中生成的lineitem.tbl文件中读取LineItem数据，将数据转换为所需的格式，如ORC或Parquet，并存储转换后的数据以供进一步分析。这是包含6行项目的lineitem.tbl文件的内容，我们将在下面的测试中使用它。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kv"><img src="../Images/a7d877e826573aecad184f87d6dd37f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6YL9veY5-AT1n9XHYPT5wA.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">由TPC-H生成的比例尺为0.001的lineitem.tbl</figcaption></figure><p id="77c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首要任务是创建一个新的Git分支来托管我们的更改。</p><pre class="ko kp kq kr fd lg lh li lj aw lk bi"><span id="03ce" class="ll jf hi lh b fi lm ln l lo lp">&gt; git branch feature/lineitem-transform<br/>&gt; git checkout feature/lineitem-transform</span></pre><h2 id="8bea" class="ll jf hi bd jg lq lr ls jk lt lu lv jo iq lw lx js iu ly lz jw iy ma mb ka mc bi translated">选择表模式表示</h2><p id="8199" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在深入研究代码之前，让我们考虑一下我们希望转换器产生的数据的模式。</p><p id="07d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，让我们决定如何表示LineItem表的模式。一种选择是使用SQL DDL并编写一个“CREATE TABLE AS”语句，该语句将为我们创建一个具有正确模式的表。SQL已经存在了很长一段时间，最初是设计用于关系数据库(RDBMs)的。但是，ANSI SQL标准不支持表达表模式的某些属性，例如与底层物理存储的格式、分区和数据布局有关的属性。相反，标准的ANSI SQL遵从实现标准的RDBMs来做出这些决定。</p><p id="df7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一个选择是使用<a class="ae jd" href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual" rel="noopener ugc nofollow" target="_blank"> Apache Hive SQL </a>语言来表达我们的表模式。Hive是一个数据仓库软件，最初是作为Apache Hadoop框架的一部分开发的。Hive的核心组件之一是metastore。Hive metastore是一个注册表，它维护有关数据库、表模式和其他元数据的信息。它的作用类似于关系数据库中的<a class="ae jd" href="https://en.wikipedia.org/wiki/Information_schema" rel="noopener ugc nofollow" target="_blank">信息模式</a>。流行的数据技术，如Apache Spark和Presto，与Hive metastore集成。</p><p id="11a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Hive DDL似乎是表示我们的表模式的一个不错的选择。然而，如果我们能够将Hive表模式表示为Scala对象并避免解析Hive DDL命令，那就太好了。为此我们将借用另一项技术，叫做<a class="ae jd" href="https://aws.amazon.com/glue/" rel="noopener ugc nofollow" target="_blank"> AWS胶水</a>。Hive metastore如此受欢迎，以至于AWS实现了自己的完全托管的元数据存储库，称为<a class="ae jd" href="https://docs.aws.amazon.com/glue/latest/dg/populate-data-catalog.html" rel="noopener ugc nofollow" target="_blank"> AWS粘合数据目录</a>，以与Hive metastore兼容。AWS Glue实现了一个<a class="ae jd" href="https://docs.aws.amazon.com/glue/latest/webapi/API_CreateTable.html" rel="noopener ugc nofollow" target="_blank"> Create Table Web API </a>，我们可以用它作为在代码中表达表格模式的模板。将AWS Glue Create Table API请求有效负载转换为Scala会产生以下简单的配置单元表模式表示:</p><figure class="ko kp kq kr fd ks"><div class="bz dy l di"><div class="kt ku l"/></div></figure><h2 id="400e" class="ll jf hi bd jg lq lr ls jk lt lu lv jo iq lw lx js iu ly lz jw iy ma mb ka mc bi translated">定义行项目表架构</h2><p id="005c" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">Hive中表模式的定义由传统的列名和类型以及物理数据格式和布局的描述组成。我们可以为保存由LineItemDataFrameTransformer生成的LineItem ORC数据的表定义模式，如下所示:</p><figure class="ko kp kq kr fd ks"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="dbcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，除了列类型之外，该模式还定义了SerDe(序列化程序/反序列化程序)库<em class="kk">org . Apache . Hadoop . hive . QL . io . orc . orc SerDe</em>，它将使用<em class="kk">org . Apache . Hadoop . hive . QL . io . orc . orc input format</em>从底层存储(如HDFS)读取数据，并使用<em class="kk">org . Apache . Hadoop . hive . QL . io . orc . orc output向底层存储写入数据</em></p><h2 id="198c" class="ll jf hi bd jg lq lr ls jk lt lu lv jo iq lw lx js iu ly lz jw iy ma mb ka mc bi translated">在配置单元中创建行项目表</h2><p id="1bb6" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">Spark与Hive metastore集成，后者可以以嵌入式、本地和远程模式部署。在嵌入式模式下，Hive使用Derby数据库来持久存储元数据，这对于我们的集成测试来说很好，但是对于生产级的工作负载来说可能不可伸缩。下图显示了集成测试中的数据和元数据流。Hive metastore维护有关输入CSV文件存储位置和转换后的数据应写入位置的信息，Spark使用这些信息来执行数据的实际读取和写入。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es md"><img src="../Images/d8ce5a1e0482c2469594916240f924bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*EIHlu9Fo85NmfmvRSSQCew.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx translated">嵌入式Spark和Hive metastore</figcaption></figure><p id="7c1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了在配置单元表中存储转换后的数据，我们首先需要用适当的模式创建表。我们可以在SparkTestHelper类中引入下面的helper方法，它将创建一个配置单元表，给出一个模式表示参数<em class="kk"> tableSchema </em>:</p><figure class="ko kp kq kr fd ks"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="b2c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们在LineItemDataFrameTransformerSpec中添加一个测试，以验证我们可以使用LineItemDataFrameTransformer中定义的LineItem模式创建一个配置单元表。</p><figure class="ko kp kq kr fd ks"><div class="bz dy l di"><div class="kt ku l"/></div></figure><h2 id="fea8" class="ll jf hi bd jg lq lr ls jk lt lu lv jo iq lw lx js iu ly lz jw iy ma mb ka mc bi translated">读取原始行项目数据</h2><p id="9af2" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated"><a class="ae jd" rel="noopener" href="/@alecswan_76492/playing-with-spark-in-scala-warm-up-game-8bfbb7cfbcc4#37c5">在上一篇文章</a>中，我展示了如何在内存中创建一个数据帧，并用SparkSession将其注册为一个表。这里，我们可以通过将数据从文件读入DataFrame来扩展这种方法。</p><p id="fd64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下代码片段显示了如何使用<a class="ae jd" href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html#csv-java.lang.String-" rel="noopener ugc nofollow" target="_blank">data frame reader # csv</a>Spark API读取一个没有标题的CSV文件，并将|作为数据帧的自定义分隔符。</p><figure class="ko kp kq kr fd ks"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="6730" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">遵循<a class="ae jd" rel="noopener" href="/@alecswan_76492/playing-with-spark-in-scala-warm-up-game-8bfbb7cfbcc4#3628">上一篇文章</a>中建立的模式，我们创建一个DataFrameCsvFileReader类来读取CSV文件。</p><figure class="ko kp kq kr fd ks"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="b193" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们实现一个集成测试，检查从CSV文件中读取的数据是否正确。请注意，尽管可能，我们并没有为读取的数据帧指定模式，因为我们无论如何都要将这些数据转换成不同的模式。因此，在这一点上，将所有列作为字符串读取就足以满足我们的需求了。</p><figure class="ko kp kq kr fd ks"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="1c70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管这种方法可行，但它一次只能对一个文件进行操作。当大规模运行时，我们希望我们的作业一次读取多个文件。实现这一点的一种方法是使用对多个文件进行操作的<a class="ae jd" href="https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html#csv-scala.collection.Seq-" rel="noopener ugc nofollow" target="_blank"> DataFrameReader#csv </a> API。这是可行的，但是需要我们找到所有需要在运行时读取的CSV文件。如果需要，这可以通过列出目录中的文件或监视目录中的文件修改来完成。然而，存储中的数据通常是按照写入时间来划分的，例如，存储结构是按照日期/小时层次结构来组织的。请注意，这种数据组织的优势在于它保证了数据一旦被写入，就不可改变。这种数据组织的缺点是，如果不在存储中创建大量的文件夹/分区组合，就很难按照其他属性(如发货日期)对相关数据进行分组。数据不变性是一个非常重要的属性，因为它简化了数据的增量处理和状态推理。这就是为什么通常按写入时间组织原始数据，然后将数据转换为更合适的格式，如ORC或Parquet，以及分区结构，如按发货日期。</p><p id="e226" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">读取文件夹中所有文件的一个选项是创建一个位置指向该文件夹的Hive表，然后使用我们的DataFrameTableReader从Hive表中读取数据。下面展示了一个测试。</p><figure class="ko kp kq kr fd ks"><div class="bz dy l di"><div class="kt ku l"/></div></figure><h2 id="f2eb" class="ll jf hi bd jg lq lr ls jk lt lu lv jo iq lw lx js iu ly lz jw iy ma mb ka mc bi translated">将行项目数据从CSV转换为ORC</h2><p id="0538" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">此时，我们有代码将行项目数据从CSV文件读入数据帧，并将数据帧写入配置单元表。请注意，数据是从CSV读入包含所有字符串类型列的数据帧中的。因此，我们想要做的一个转换是将每一列转换为适当的类型。下面显示了LineItemDataFrameTransformer类中此类转换的实现。转换将输入字符串列转换为输出表模式定义中声明的类型，该定义来自<em class="kk">lineitemdataframeformer . get schema()</em>:</p><figure class="ko kp kq kr fd ks"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="f0b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下测试将CSV文件存放在文件夹中，在该文件夹上创建一个配置单元表，将配置单元表读入数据帧，将数据帧传递给转换器，并验证转换器是否生成了具有正确类型方案的正确行数的数据帧。</p><figure class="ko kp kq kr fd ks"><div class="bz dy l di"><div class="kt ku l"/></div></figure><h1 id="6d7e" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">收集对Spark工作绩效的见解</h1><p id="ea73" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">围绕我们的Spark工作进行测试是很棒的，但是当这些测试在嵌入式模式下执行时，我们能知道Spark实际上在做什么吗？我们如何深入了解幕后发生的事情？</p><p id="85d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">事实证明，就像独立的本地和远程模式一样，Spark也可以在嵌入式模式下运行UI服务器。Spark UI是一个web界面，它提供了对Spark作业的性能特征的可见性，包括作业如何分解成阶段以及每个作业如何利用Spark执行器。</p><p id="780c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在嵌入式模式下，Spark UI与测试在同一个JVM中运行，并在测试结束前关闭。因此，为了能够在测试运行时访问UI，我们在Spark作业完成后立即放置了一个断点。在上面的测试中，Spark作业是由<em class="kk"> transformedDF.count() </em>调用启动的，因为在Spark DAG术语中，<em class="kk"> count() </em>是一个<strong class="ih hj">动作</strong>，与在我们的测试中触发DAG执行的<strong class="ih hj">转换</strong>相反。请参考<a class="ae jd" href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations" rel="noopener ugc nofollow" target="_blank"> Apache Spark文档</a>了解Spark支持的转换和动作的详细信息。</p><p id="5ebe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里需要注意的是，我们希望断点暂停执行测试的线程，但允许服务于Spark UI的线程继续执行。在IntelliJ IDEA中，这可以通过右键单击断点并选择Suspend部分中的Thread来完成，如下所示。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es me"><img src="../Images/56e0e7e4e208d5d03931b81ae8d697ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6jTPIaiVGrjzqdWwQjbjkg.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">测试执行暂停以允许访问Spark UI</figcaption></figure><p id="cc93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们可以在调试模式下运行测试，并在遇到断点时访问Spark UI。请注意，Spark UI服务器默认启动，但可以通过将<em class="kk">Spark . UI . enabled</em>config设置设置为<em class="kk"> false </em>来禁用。如果启用了Spark UI，那么您将会在测试期间生成的日志中看到下面一行:</p><pre class="ko kp kq kr fd lg lh li lj aw lk bi"><span id="dfdb" class="ll jf hi lh b fi lm ln l lo lp">19/11/24 11:12:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.</span></pre><p id="0899" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这意味着当测试在断点处暂停时，可以通过在浏览器中打开<a class="ae jd" href="http://localhost:4040/" rel="noopener ugc nofollow" target="_blank"> http://localhost:4040/ </a>来访问Spark UI。可通过将spark.ui.port spark配置设置为所需端口来更改UI端口，例如在<code class="du mf mg mh lh b">SparkSession.builder().config(“spark.ui.port”, “4041”).getOrCreate()</code>中创建Spark会话时。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es mi"><img src="../Images/0992a0f1c8c8488217c6dda24d04c7af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nSbXbe1aedeeK7C8AZLFkw.png"/></div></div></figure><p id="5628" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个屏幕截图显示了Spark UI，其中的作业及其阶段在测试到达断点之前已经完成。有趣的是，一个简单的count()操作导致了两个作业阶段的数据混乱。原因是<a class="ae jd" href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2788" rel="noopener ugc nofollow" target="_blank">数据集#count() </a>是使用groupBy()实现的。使用空键计数()。这导致在每个分区中计算部分计数，并将所有这些计数发送到映射到空键的单个分区。然后，该分区将生成最终的聚合计数。因此，在这种情况下，shuffle会移动少量数据，这不是一个需要关注的问题。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es mj"><img src="../Images/cc37b23541e4db28ec7b6f733e6feae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*cVGs4YT2Xyh_jd2VZYFhlQ.png"/></div></div></figure><p id="62aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有趣的是，从<a class="ae jd" href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2788" rel="noopener ugc nofollow" target="_blank">数据集#count() </a>切换到<a class="ae jd" href="https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L1227" rel="noopener ugc nofollow" target="_blank"> RDD#count() </a> API消除了这种额外的洗牌，因为后者不使用groupBy，而是直接累加从每个分区接收的部分计数。</p><p id="5bdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在使用Spark UI时，我们可以发现许多类似这样的细微差别。Spark作业优化的主题超出了本文的范围，在网上有大量的支持文献。</p><p id="a988" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我想指出的最后一点是UI中的SQL选项卡，它显示了作业运行的Spark SQL查询及其执行计划。注意，通过在查询前加上<em class="kk">解释</em>或<em class="kk">解释扩展</em>，可以以编程方式获得相同的执行计划。</p><h1 id="226a" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">最终完成功能实现</h1><p id="6b74" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">此时，行项目转换已经实现，我们需要确保所有的测试仍然通过，并且我们有一个良好的代码覆盖率。一种方法是在IDE中运行覆盖测试，就像我们在<a class="ae jd" rel="noopener" href="/@alecswan_76492/playing-with-spark-in-scala-warm-up-game-8bfbb7cfbcc4#1659">上一篇文章</a>中所做的那样。另一种方法是运行以下命令来生成覆盖率报告:</p><pre class="ko kp kq kr fd lg lh li lj aw lk bi"><span id="e6b5" class="ll jf hi lh b fi lm ln l lo lp">&gt; sbt clean coverage test<br/>&gt; sbt coverageReport</span></pre><p id="6c28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后一个命令将在项目文件夹中产生一个<em class="kk">target/Scala-2.11/s coverage-report/index . html</em>。在web浏览器中打开该文件，并查看覆盖率报告。</p><p id="e2ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们对代码覆盖率感到满意，我们就可以将特性分支推送到Git服务器，这将触发CI/CD管道。</p><pre class="ko kp kq kr fd lg lh li lj aw lk bi"><span id="e886" class="ll jf hi lh b fi lm ln l lo lp">&gt; git push --set-upstream origin feature/lineitem-transform</span></pre><p id="012d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦对通过测试和CI/CD产生的代码覆盖率感到满意，让我们用我们在<code class="du mf mg mh lh b">feature/lineitem-transform</code>分支中所做的更改创建一个pull请求(在GitLab中称为“合并请求”),供其他开发人员审查。当创建这样的请求时，使用<code class="du mf mg mh lh b">feature/lineitem-transform</code>作为源分支，<code class="du mf mg mh lh b">master</code>作为目标分支，提供变更的清晰描述，并将其分配给应该审阅请求的用户。以下是GitLab中此类拉取请求的一个示例:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es mk"><img src="../Images/7560285a03478a2748f9f5586c9d489d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wT03JgGUeG7p8fmwx1lyXw.png"/></div></div><figcaption class="lc ld et er es le lf bd b be z dx translated">GitLab中的拉/合并请求</figcaption></figure><p id="f153" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦请求被批准，通过点击UI中的合并按钮或使用<code class="du mf mg mh lh b"><a class="ae jd" href="https://git-scm.com/docs/git-merge" rel="noopener ugc nofollow" target="_blank">git merge</a></code>命令，请求中的变更可以被合并到主分支中。请注意，如果在我们从主分支中分支出特性分支之后，在主分支中进行了更改，那么我们会希望在将拉请求合并到主分支之前将这些更改应用到特性分支。这可以通过使用<code class="du mf mg mh lh b"><a class="ae jd" href="https://git-scm.com/docs/git-rebase" rel="noopener ugc nofollow" target="_blank">git rebase</a></code>命令来完成，该命令将暂时回滚在特征分支中所做的更改，重放在主分支中所做的更改，并重放在重放的更改之上恢复的更改。有时，这可能会导致合并冲突，您必须在将特征分支合并到主分支之前，在特征分支中解决这些冲突。</p><h1 id="0afb" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">摘要</h1><p id="327f" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在本文中，我们介绍了TPC-H基准，并简要讨论了它所包含的数据集和查询。我们评估了使用Scala对象模型在Spark中表示表模式的不同选项，并确定了在<a class="ae jd" href="https://docs.aws.amazon.com/glue/latest/webapi/API_CreateTable.html" rel="noopener ugc nofollow" target="_blank"> Create Table AWS Glue Web API </a>中使用的格式。我们回顾了用于读写文件的不同Spark API，并使用它们实现了将TPC-H LineItem数据从原始格式转换为ORC格式的Spark作业。接下来，我们讨论了在底层物理存储中组织数据的不同方法的利弊。最后，我们窥视了Spark API的幕后，在嵌入式模式下运行Spark作业时，查看了Spark UI中的作业执行细节。一直以来，我们都遵循最佳的软件工程实践，确保我们产生高质量的干净代码。</p></div></div>    
</body>
</html>