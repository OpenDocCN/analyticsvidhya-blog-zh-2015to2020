<html>
<head>
<title>Ordinary Least Square (OLS) Method for Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归的普通最小二乘法(OLS)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ordinary-least-square-ols-method-for-linear-regression-ef8ca10aadfc?source=collection_archive---------0-----------------------#2020-07-09">https://medium.com/analytics-vidhya/ordinary-least-square-ols-method-for-linear-regression-ef8ca10aadfc?source=collection_archive---------0-----------------------#2020-07-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7182" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章是关于简单线性回归的普通最小二乘法(OLS)。如果您是线性回归的新手，请阅读这篇文章，了解简单线性回归的实现。这篇文章将帮助你逐步理解简单线性回归是如何工作的。</p><p id="4098" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单线性回归是一种模型，它有一个与响应(因变量或目标变量)y有关系的单个回归变量(自变量)x</p><p id="c287" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">y =β0+β1 x+ε—————( 1)</p><p id="c3b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中β0:截距</p><p id="8f3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">β1:斜率(未知常数)</p><p id="4395" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ε:随机误差分量</p><p id="d8ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一条线，其中y是我们要预测的因变量，x是自变量，β0和β1是我们需要估计的系数。</p><p id="aa77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">β0和β1的估计:</strong></p><p id="716a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">OLS方法用于估计<strong class="ih hj"> </strong> β0和β1。OLS方法寻求最小化残差平方和。这意味着从给定的数据中，我们计算从每个数据点到回归线的距离，平方它，以及所有平方误差的总和。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/93177afa2dd11dc8239fda09e9652c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*gglavDlTUWKn4Loe"/></div></figure><p id="973a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从等式(1)我们可以写出</p><p id="c218" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">yi = β0 + β1 x + εi，<em class="jl"> i </em> = 1，2，…..n————( 2)</p><p id="cf7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">等式(2)是一个样本回归模型，根据n对数据(yi，)写成(i = 1，2，…)..，n)。因此，最小二乘准则为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jm"><img src="../Images/d635b7f2e2f6b70bea899ca27a0ed1e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d4y8jE1R6yPlnh7BsuOBcA.jpeg"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jm"><img src="../Images/9c1513b3447ad6804906ec9e54a2b033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jmh8qropV2EluJcnbS501w.jpeg"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jm"><img src="../Images/d75e6dd4773c7fb289ea039ee71f8ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fkZUx5bMN6RyJSP_msvDZA.jpeg"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jm"><img src="../Images/ffa2545de93bf0ce28c996dd710b03fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*esj6-SmvIKPVEENEYDCimA.jpeg"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">普通最小二乘法</figcaption></figure></div><div class="ab cl jv jw gp jx" role="separator"><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka kb"/><span class="jy bw bk jz ka"/></div><div class="hb hc hd he hf"><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kc"><img src="../Images/3fdae4bac363fd06280359003831f58a.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*RFfg-kIoXyw8beUndWIhDw.png"/></div></figure><p id="9a00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们举个简单的例子。该表显示了制造公司的一些数据。表格中的每一行都显示了一年的销售额和该年的广告支出。这里我们的目标变量是销售——我们想要预测的。</p><p id="a241" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归估计销售额= β0 + β1 *(广告)</p><p id="a46e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">估计斜率(β1): </strong></p><ol class=""><li id="f6e5" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc ki kj kk kl bi translated">计算x和y的平均值</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es km"><img src="../Images/27033744950296d788b68b561ac199e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*_eIxoB-P736SGJTTy2Yxww.png"/></div></figure><p id="7d1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.根据平均值计算每个变量的误差</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kn"><img src="../Images/ce4fe89f8ef89e69979d2f28ce3897bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*YT_NvQhLDhkUt1-LkEIHIA.png"/></div></figure><p id="b098" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.将每个x的误差乘以每个y的误差，并计算这些乘积的总和</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ko"><img src="../Images/734c06e9ab3990b2dc7bc0ab6ca539d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1348/format:webp/1*rI7k_qWUI4FeOCOHOKrXZg.png"/></div></figure><p id="cd53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.从这些平方值的平均值和总和中求每个x值的残差的平方</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es kp"><img src="../Images/063e63472dce11c73b901e011c9bee7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u46I8hxohOrEviOa6xofaA.png"/></div></div></figure><p id="3e6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有了所有的值来计算斜率(β1) = 221014.5833/8698.694 = 25.41</p><p id="e9f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">估计截距(β0): </strong></p><p id="4264" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">β0 =均值(y)-(β1 <strong class="ih hj"> * </strong>均值(x))</p><p id="86e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经知道了计算β0的所有值。</p><p id="765d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">β0 = 968.5-(37.83333333*25.41)=7.239</p><p id="7636" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">做预测:</strong></p><p id="b47d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们有了简单线性回归的系数。</p><p id="107c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">y = 7.239 + 25.41* x</p><p id="c357" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们对我们的数据进行预测。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es kq"><img src="../Images/c53cd3adaf2785f885d80e44d4be5426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ADgCdalF6qctPjG19vaExg.png"/></div></div></figure><p id="7434" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">估计误差:</strong>计算我们预测的误差(均方根误差或RMSE)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kr"><img src="../Images/725adfcca0ce1200920b59c0cbfdb475.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*3iS6PJV54Kn_VR0YGVX82g.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">均方根误差</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ks"><img src="../Images/dc01052639fa53cc3e2f042e9a4a7e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wUJhzUt_QTb8OEkoEtf1Yg.png"/></div></div></figure><p id="388d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据计算的RMSE，我们可以说每个预测平均误差约为39.2059个单位。</p><p id="ec56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以使用优化技术，通过最小化模型对数据的误差来计算系数。你可以在给定的链接上阅读我关于优化的文章。</p><p id="8cdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae kt" rel="noopener" href="/@aishwaryagulve97/implementation-of-stochastic-gradient-descent-1d36b6a0c013">https://medium . com/@ aishwaryagulve 97/implementation-of-random-gradient-descent-1d 36 b 6 A0 c 013</a></p></div></div>    
</body>
</html>