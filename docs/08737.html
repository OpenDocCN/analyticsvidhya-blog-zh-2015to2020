<html>
<head>
<title>Solving a Rubik’s Cube with Reinforcement Learning (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用强化学习解魔方(第一部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/solving-a-rubiks-cube-with-reinforcement-learning-part-1-4f0405dd07f2?source=collection_archive---------4-----------------------#2020-08-11">https://medium.com/analytics-vidhya/solving-a-rubiks-cube-with-reinforcement-learning-part-1-4f0405dd07f2?source=collection_archive---------4-----------------------#2020-08-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="c1ef" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">解决常见难题的深度Q学习方法</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/6b0a0e7ad01c596d8c2c61112f3e9de6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uDMmJVLmYjFHguwIolYFkA.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://openai.com/blog/solving-rubiks-cube/" rel="noopener ugc nofollow" target="_blank">https://openai.com/blog/solving-rubiks-cube/</a>。OpenAI在2019年秋季成为头条新闻，因为它解决了相关但更困难的RL任务，即教会一只手操纵立方体。实际上解决立方体的任务是使用<a class="ae jn" href="https://en.wikipedia.org/wiki/Optimal_solutions_for_Rubik%27s_Cube#Kociemba's_algorithm" rel="noopener ugc nofollow" target="_blank">科辛巴的算法</a>完成的。</figcaption></figure><p id="d261" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">去年，我通过康奈尔理工学院的硕士项目开始了我的机器学习之旅。一个特别引起我注意的话题是强化学习<em class="kk"> </em> (RL)，我们从马尔可夫决策过程(MDP)的传统方向和深度学习(DL)的方向来研究它。虽然课程内容丰富，但我想更进一步。在这里，我记录了我(正在进行的)尝试，通过训练一个智能体来解决一个魔方。</p><p id="81d8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">一些介绍性注释:</p><ol class=""><li id="72f2" class="kl km hi jq b jr js ju jv jx kn kb ko kf kp kj kq kr ks kt bi translated">用强化学习解决魔方并不是一个新问题，我的大部分工作将基于Stephen McAleer等人的这篇<a class="ae jn" href="https://arxiv.org/abs/1805.07470" rel="noopener ugc nofollow" target="_blank">论文</a>，并做一些修改。</li><li id="1103" class="kl km hi jq b jr ku ju kv jx kw kb kx kf ky kj kq kr ks kt bi translated">对于某些概念，我将尝试尽可能详细地介绍我的特定任务实现，因此建议您熟悉概率、机器学习、RL和DL。也就是说，我绝不是专家，任何和所有的反馈都非常感谢！</li></ol><h1 id="4d6e" class="kz la hi bd lb lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated"><strong class="ak">介绍马尔可夫决策过程</strong></h1><p id="9114" class="pw-post-body-paragraph jo jp hi jq b jr lr ij jt ju ls im jw jx lt jz ka kb lu kd ke kf lv kh ki kj hb bi translated">马尔可夫决策过程捕获<em class="kk">代理</em>如何在<em class="kk">环境中采取<em class="kk">动作</em>。</em>每个动作将代理置于不同的环境<em class="kk">状态中，</em>通常根据某种概率分布<em class="kk">，</em>代理有可能获得某种<em class="kk">奖励。</em>代理人的目标是学习一个<em class="kk">策略</em>(即在给定状态下采取的适当行动)，以使代理人获得的长期回报最大化。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lw"><img src="../Images/de0c9ce06cab468dfaf6b00f83d35109.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*6ROSXTrzaeyos4me.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<em class="lx">萨顿，R. S .，&amp;巴尔托，A. G. (2018)。强化学习:导论。麻省理工出版社。</em></figcaption></figure><p id="a9e6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">更具体地说，MDP是由一个元组(<em class="kk"> S，a，Pₐ，Rₐ </em>)定义的。其中:</p><p id="8c40" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">S是环境中所有可能状态的集合</p><p id="4107" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="kk"> A </em>是代理可以采取的所有可能动作的集合</p><p id="bef3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="kk"> Pₐ </em>定义了采取行动<em class="kk"> a </em>时，从状态<em class="kk"> s⁰ </em>到状态<em class="kk"> s </em>转换的概率分布</p><p id="0845" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="kk"> Rₐ </em>指定在状态<em class="kk"> s </em>采取行动<em class="kk"> a </em>所收到的奖励</p><h1 id="965a" class="kz la hi bd lb lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated">将魔方任务公式化为MDP</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/5b5b8d442041d3c97c336336da73c478.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/0*40znCRrjmQyi36-2.gif"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://mathematica.stackexchange.com/questions/195064/pascal-records-and-mathematica-programming" rel="noopener ugc nofollow" target="_blank">https://Mathematica . stack exchange . com/questions/195064/Pascal-records-and-Mathematica-programming</a></figcaption></figure><p id="1a73" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了训练我们的代理，我们必须指定一个MDP来表示解决魔方的任务，这是通过定义元组的每个组件来完成的。</p><p id="7838" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们的一组可能状态，<em class="kk"> S </em>，是魔方的所有可能排列。我们应该注意到大约有4.3×10个⁹态！这将对我们的代理如何学习完成任务起到主要作用。</p><p id="8a30" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们的一组可能的动作，<em class="kk"> A </em>，由立方体的12个可能的90度旋转组成。按照惯例，这12个移动是根据立方体的六个面中的哪一个被移动来命名的，例如正面或正面，以及该面是顺时针还是逆时针旋转。这些移动用以下符号缩写:(<em class="kk"> F，R，L，U，D，F’，R’，L’，U’，D’</em>)。</p><p id="14bd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">幸运的是，指定状态转移概率分布<em class="kk"> Pₐ </em>非常容易，因为每个动作都会决定性地改变状态。例如，执行<em class="kk"> F </em>动作只会将立方体从其当前状态转换到其正面顺时针旋转90度的状态。如果我们把这些状态分别叫做<em class="kk"> s₁ </em>和<em class="kk"> s₂ </em>，那么p(<em class="kk">s₁</em>-&gt;t24】s₂；<em class="kk"> F </em> ) = 1，p(<em class="kk">S1-&gt;s)；F </em> ) = 0表示所有的<em class="kk">！= s₂ </em>。然后，我们可以将同样的逻辑应用于所有其他状态/动作组合，以获得概率分布。</p><p id="20c9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最后，我们必须指定奖励函数(即，代理人在每个状态下采取每个行动会得到什么奖励，如果有的话)。为了确保我们的代理只关心解决魔方，而不关心其他次要目标，我们将说，当代理采取行动导致魔方被解决时，它将获得1的奖励，如果行动导致任何其他状态，则获得0的奖励。</p><h1 id="b25f" class="kz la hi bd lb lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated">一种解决立方体的策略</h1><p id="4769" class="pw-post-body-paragraph jo jp hi jq b jr lr ij jt ju ls im jw jx lt jz ka kb lu kd ke kf lv kh ki kj hb bi translated">代理在MDP的每个状态中应该采取哪个动作的选择被称为代理的<em class="kk">策略</em>。特别是，我们想学习一个<em class="kk">最优策略</em>，它将使代理人获得最大的长期回报。给定我们上面指定的奖励函数，魔方代理的最优策略是最终导致解决状态的策略，因为这是给出奖励的唯一状态。</p><p id="5421" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">策略是将状态映射到操作的函数，用π()表示。对于简单的MDP，最优策略可以通过求解一组称为贝尔曼方程的递归方程来确定。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lz"><img src="../Images/d11286233c3c02f6213d164ee47e908d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*13O83_yJ3VQALCUr5OSZ9A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">贝尔曼方程</figcaption></figure><p id="142a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">直观地说，上述等式的左侧可以被认为是代表当遵循策略π( <em class="kk"> s </em>)时处于状态<em class="kk"> s </em>的代理的“值”。贝尔曼方程表示，该值等于采取策略指定的下一个行动的预期回报(该行动可达到的每个状态的平均回报，以该状态结束的可能性加权)加上这些状态中每个状态的“值”，再减去某个因子γ。从这些等式中，最优策略可以定义为在每个状态中采取使<em class="kk"> V(s) </em>最大化的行动。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ma"><img src="../Images/9bf2490b82c1e4d6141dc7bc233eadb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WrEn8dPnWiV5Ji5UJuEZ2A.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">最佳策略定义</figcaption></figure><p id="63a6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">虽然可以使用动态规划为简单的MDP计算Bellman方程，但是这对于较大的动作和状态空间很快变得不可行。对于这些更大的问题，我们不得不采取强化学习的方法来解决MDP。一种这样的方法叫做<em class="kk"> Q-learning，</em>它利用了一个叫做Q-function的相关函数。一旦Q函数被指定(或至少被估计)，代理就可以遵循一个策略，采取使每个状态下的Q函数最大化的行动。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mb"><img src="../Images/85f5db49cd0b2f1c77e8f8f23f49665b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ViRNsgG3fQkdgyUaG1owTw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">q函数</figcaption></figure><p id="a1be" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最简单形式的Q函数可以被认为是一个巨大的查找表，具有维度| <em class="kk"> S </em> | x | <em class="kk"> A </em> |。然而，在魔方的例子中，这将产生一个尺寸为4.3×10⁹×12的表格，我们很快发现确定表格的每个条目是不可行的。相反，我们需要一种方法来估计所有状态-动作对的Q函数。</p><h2 id="0166" class="mc la hi bd lb md me mf lf mg mh mi lj jx mj mk ll kb ml mm ln kf mn mo lp mp bi translated">进入深度学习</h2><p id="e9b4" class="pw-post-body-paragraph jo jp hi jq b jr lr ij jt ju ls im jw jx lt jz ka kb lu kd ke kf lv kh ki kj hb bi translated">我们用来估计Q函数的机器是一个神经网络。具体来说，我们将利用神经网络的<a class="ae jn" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank">通用逼近定理</a>，该定理指出，在某些条件下，神经网络可以用来逼近任何函数。</p><p id="ced0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在我的下一篇文章中，我将介绍用于训练神经网络来估计Q函数的开创性算法(如果你想同时继续阅读，可以在这里查看原始论文<a class="ae jn" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" rel="noopener ugc nofollow" target="_blank"/>)、我用于魔方任务的特定模型架构，以及一些初步结果。</p></div><div class="ab cl mq mr gp ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="hb hc hd he hf"><p id="4022" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">感谢您阅读我在Medium上的第一篇博文！我希望你觉得这很有趣，并愿意在下面的评论中讨论任何反馈或进一步的对话。敬请期待<a class="ae jn" rel="noopener" href="/@mgd67/solving-a-rubiks-cube-with-reinforcement-learning-part-2-b4ff0f3522b3">第二部</a>！</p></div></div>    
</body>
</html>