<html>
<head>
<title>Neural Networks &amp; Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络和深度学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-networks-deep-learning-1184fb3635f6?source=collection_archive---------35-----------------------#2020-08-03">https://medium.com/analytics-vidhya/neural-networks-deep-learning-1184fb3635f6?source=collection_archive---------35-----------------------#2020-08-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="62d8" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">深度学习入门笔记</h2></div><p id="a11c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated">持续学习又是一个人们经常混淆和互换使用的术语。</p><h1 id="de97" class="kg kh hi bd ki kj kk kl km kn ko kp kq io kr ip ks ir kt is ku iu kv iv kw kx bi translated">什么是神经网络？</h1><p id="821d" class="pw-post-body-paragraph ix iy hi iz b ja ky ij jc jd kz im jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">像人脑一样，试图识别输入数据的潜在关系的<code class="du kc kd ke kf b">Neural Network is a series of algorithms</code>。这样，神经网络指的是<code class="du kc kd ke kf b">systems of neurons</code>，无论是自然的还是人工的。神经网络可以适应变化的输入；因此，网络无需重新设计输出标准即可生成最佳结果。</p><p id="934d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">神经网络<code class="du kc kd ke kf b">a series of algorithms</code>努力识别输入数据之间的<code class="du kc kd ke kf b">underlying relationship</code>，就像人脑一样。</p><p id="6254" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最小的神经网络可能有一个<strong class="iz hj">神经元</strong>，一个<strong class="iz hj">输入(x) </strong>和一个<strong class="iz hj">输出(y)。<br/> </strong>这里的神经元是<code class="du kc kd ke kf b"><strong class="iz hj">computing unit</strong></code> <strong class="iz hj">，</strong>它接受一个输入，并反馈一个输出。</p><p id="0536" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在<code class="du kc kd ke kf b"><strong class="iz hj">input layer(x)</strong></code>和<code class="du kc kd ke kf b"><strong class="iz hj">output layer(y)</strong></code>之间的神经元层被称为<code class="du kc kd ke kf b"><strong class="iz hj">hidden layers,</strong></code> <strong class="iz hj"> </strong>，这些隐藏层中的每一个神经元被称为<code class="du kc kd ke kf b"><strong class="iz hj">hidden unit.</strong></code> <strong class="iz hj"> </strong>这些隐藏单元中的每一个都从所有<code class="du kc kd ke kf b"><strong class="iz hj">input features(x).</strong></code>接收输入</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ld"><img src="../Images/f0a635672288f408c602e44a83d0dc69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Y4xTJQNCU3uvk0up.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">标准神经网络</figcaption></figure><p id="5dca" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">大多数令人着迷的神经网络应用来自于<a class="ae lt" href="https://en.wikipedia.org/wiki/Supervised_learning#:~:text=Supervised%20learning%20is%20the%20machine,a%20set%20of%20training%20examples." rel="noopener ugc nofollow" target="_blank">监督学习</a>，基于<code class="du kc kd ke kf b">inputs</code>的类型可以有不同的神经网络，例如<code class="du kc kd ke kf b">Ad or user info</code> <code class="du kc kd ke kf b"><strong class="iz hj">Standard NN</strong></code> <strong class="iz hj">，</strong>表示<code class="du kc kd ke kf b">images</code>我们使用<code class="du kc kd ke kf b"><strong class="iz hj">CNN</strong></code> <strong class="iz hj">，</strong>表示<code class="du kc kd ke kf b">sequential data</code> <strong class="iz hj"> </strong>例如音频和自然语言我们有<code class="du kc kd ke kf b"><strong class="iz hj">RNN</strong></code> <strong class="iz hj">，</strong>用于输入图像和雷达信息，这些有时会更复杂我们使用<code class="du kc kd ke kf b"><strong class="iz hj">custom made/hybrid NN architecture.</strong></code></p><div class="le lf lg lh fd ab cb"><figure class="lu li lv lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><img src="../Images/37388cd8c026e1f75486a733ae04aff2.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*F9SWPmBliUBZUn6bVkKYGw.png"/></div></figure><figure class="lu li ma lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><img src="../Images/03c49aee70aff1a425866995ec7392fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*OI_OLnNTbhVpkxuwhfQaqQ.png"/></div></figure><figure class="lu li mb lw lx ly lz paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><img src="../Images/99e06edcf41b07b52d175ed94c80b143.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*2jcJfkFJhgRaQmcfCSDUsw.png"/></div><figcaption class="lp lq et er es lr ls bd b be z dx mc di md me translated">不同的神经网络架构</figcaption></figure></div><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mf"><img src="../Images/701164e7ffaa2cdb109c044dca26dcd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NhuJ3k5XYINRLCAO.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">只是想让你知道还有比你现在知道的更多的东西: )来源—<a class="ae lt" href="https://www.asimovinstitute.org/neural-network-zoo/" rel="noopener ugc nofollow" target="_blank">https://www.asimovinstitute.org</a></figcaption></figure><p id="450d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">参考上面的<a class="ae lt" href="https://www.asimovinstitute.org/neural-network-zoo/" rel="noopener ugc nofollow" target="_blank">图片</a>，他们对所有的神经网络架构都有简要的解释..！</p><h2 id="cb41" class="mg kh hi bd ki mh mi mj km mk ml mm kq jg mn mo ks jk mp mq ku jo mr ms kw mt bi translated">结构化与非结构化数据</h2><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mu"><img src="../Images/3725778b84f5a6c7c16b4eda04ff339c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*tIawr2n9HSTIoaq7.jpg"/></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">结构化与非结构化数据(来源—<a class="ae lt" href="http://www.datamation.com" rel="noopener ugc nofollow" target="_blank">www.datamation.com</a>)</figcaption></figure><p id="5ba6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于传统的学习算法，如<code class="du kc kd ke kf b">SVM</code>和<code class="du kc kd ke kf b">Logistic regression</code>，当数据增加时，算法的性能会增加，但随着数据的进一步增加，性能会趋于平稳。随着我们变得更加数字化，几乎在每个地方都介绍 IOT，在过去 20 年里，数据有了很大程度增长。因此我们需要一个<code class="du kc kd ke kf b">learning algorithm that could give more performance with respect to rising data</code>，为此我们有了<code class="du kc kd ke kf b"><strong class="iz hj">Deep learning</strong></code>。</p><p id="95d8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">即<code class="du kc kd ke kf b"><strong class="iz hj">larger the NN </strong></code>与<code class="du kc kd ke kf b"><strong class="iz hj">amount of data</strong></code>同在，<code class="du kc kd ke kf b"><strong class="iz hj"> better </strong></code>就是我们的<code class="du kc kd ke kf b"><strong class="iz hj">performance</strong></code>。</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mv"><img src="../Images/083d42c5a7f5e0afdb799976cd534e36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d2ALAKf96BVivOXkOMRgWQ.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">吴恩达机器学习公司的<strong class="bd ki">数据量(x) </strong>到<strong class="bd ki">性能(y) </strong>图</figcaption></figure><p id="87ec" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然对于<code class="du kc kd ke kf b">smaller training set<strong class="iz hj">(the amount of data)</strong></code>来说，事情是<code class="du kc kd ke kf b">not certain</code>并且更依赖于什么特征被赋予输入数据，对于一些甚至<code class="du kc kd ke kf b">SVM</code>可能比<code class="du kc kd ke kf b">NNs.</code>给出的更好</p><p id="f6c0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在过去的几年里，我们不仅获得了<code class="du kc kd ke kf b">added up the data</code>，还获得了<code class="du kc kd ke kf b">much more computation ability</code>，更重要的是，在过去的几年里，我们获得了巨大的<code class="du kc kd ke kf b">algorithmic innovation</code>。</p><p id="21ad" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">突破之一是从<code class="du kc kd ke kf b"><a class="ae lt" href="https://en.wikipedia.org/wiki/Sigmoid_function#:~:text=4%20Applications-,Definition,refer%20to%20the%20same%20object." rel="noopener ugc nofollow" target="_blank">Sigmoid</a></code> <a class="ae lt" href="https://en.wikipedia.org/wiki/Sigmoid_function#:~:text=4%20Applications-,Definition,refer%20to%20the%20same%20object." rel="noopener ugc nofollow" target="_blank"> </a>切换到<code class="du kc kd ke kf b"><a class="ae lt" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#:~:text=The%20rectifier%20is%2C%20as%20of,neural%20nets%20and%20computational%20neuroscience." rel="noopener ugc nofollow" target="_blank">ReLu</a></code>功能。ReLu 函数使得像<code class="du kc kd ke kf b">gradient descent</code>这样的算法更快，因为 sigmoid 接近于 0，对这样的值应用梯度会使它更慢，需要更多的时间。</p><p id="0d9e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这种算法创新和计算可行性的影响比预期的要大得多，因为这种学习方法通常是<code class="du kc kd ke kf b">iterative</code>、you <code class="du kc kd ke kf b">train a model</code>、you get<code class="du kc kd ke kf b">better idea</code>、you code 和<code class="du kc kd ke kf b">again train a model</code>，如果训练一个迭代过程的模型需要很多时间，那就需要更多的时间<code class="du kc kd ke kf b">much more time and computation expensive</code>。对于较少的培训时间，可以做<code class="du kc kd ke kf b">more experiments</code>并可能找到更适合应用的模型。这对创新新学习算法的人来说是一个福音。</p><h2 id="5088" class="mg kh hi bd ki mh mi mj km mk ml mm kq jg mn mo ks jk mp mq ku jo mr ms kw mt bi translated">逻辑回归</h2><p id="720b" class="pw-post-body-paragraph ix iy hi iz b ja ky ij jc jd kz im jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">这是一个<code class="du kc kd ke kf b">algorithm for binary classification</code>，让我们在进入之前先了解一些符号，</p><p id="712e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是一个<code class="du kc kd ke kf b">training pair</code>示例的样子(x，y ),其中 x <strong class="iz hj"> ∈ </strong></p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mw"><img src="../Images/f8116a032a7819f66ed7e466cf83d3b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:112/format:webp/1*WI8HrKpwT5XIhXQltzg1Ig.png"/></div></figure><p id="f335" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">y <strong class="iz hj"> ∈ {0，1} </strong></p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mx"><img src="../Images/1c13e2190e433b3c6772a98803b1c73d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eoVbE_MmewBgw007dJlARQ.png"/></div></div></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es my"><img src="../Images/5244ef6254f8e7213648941917f34fd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9t9tI2P8C8EXDaEC8fXaPw.png"/></div></div></figure><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es mz"><img src="../Images/211aeb3bed8d06d0c5e21064863640ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*g-O0InFNmjiM25GAgJd3Ng.png"/></div></figure><p id="2f49" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">就好像我们谈论的是<code class="du kc kd ke kf b">64 x 64</code>像素的彩色图像，我们首先需要将图像转换成一个<code class="du kc kd ke kf b">feature vector <strong class="iz hj">X</strong></code>，对其进行逻辑回归，所以在这里输入特征 nₓ = <code class="du kc kd ke kf b">64 x 64 x 3 =12288</code> 3 这里是 RGB 通道，也就是我们图像的深度。</p><p id="1b84" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">逻辑回归对于二元分类是有用的，因为它给出了给定特征向量 X. <br/> <code class="du kc kd ke kf b">yˆ= P(y=1|X)</code>时输出为 y 的概率，这里<code class="du kc kd ke kf b">yˆ</code>是输出为<code class="du kc kd ke kf b">y</code>的概率。</p><p id="02e7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">逻辑回归参数为</strong>:</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es na"><img src="../Images/71c3b2e6bfb06311edc2a5a38d7e4e20.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*c6LRoeaip-6sdG-HJtK1mg.png"/></div></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nb"><img src="../Images/a5830e96c45e43bd5adf1eaeccddb919.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5iG8sA4DYRc2PCDJGUb2wQ.png"/></div></div></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nc"><img src="../Images/77477a0b593505ffe0923b50e7345da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dvGXahjonMm0hv-jFymhMA.png"/></div></div></figure><p id="904c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Sigmoid 基本上是将值重新调整为 0 和 1</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es nd"><img src="../Images/21bb9ede8462e329210ba552ea76b94a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*m9nHtiPDdWZwWe3y6CdqJg.png"/></div></figure><p id="e281" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果输出变得更负，值变得更接近 0，而对于更正的值，它变得更接近 1。</p><p id="d0ad" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> Sigmoid </strong>和<strong class="iz hj"> Softmax </strong>功能听起来可能相似，但它们是不同的，</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es ne"><img src="../Images/541c0f9bdb850b0125edf0016f0cebac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*piEf_ifskaOp46xJJLU98Q.png"/></div></div></figure><h2 id="115b" class="mg kh hi bd ki mh mi mj km mk ml mm kq jg mn mo ks jk mp mq ku jo mr ms kw mt bi translated">总结:</h2><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es nf"><img src="../Images/15da040e7c763f53cc26f4049d4b26da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*P0-XBmaf-GUyYDtn9KvYHA.png"/></div></figure><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es ng"><img src="../Images/561f43c43512d252336e6e60dad6b3a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*JQfWkELsrAE_cuMSCd53lg.png"/></div></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nh"><img src="../Images/ae429b2b6673239ad14af203e23a6eb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QMI4Sz9PEnNcgq9IWMrFWw.png"/></div></div></figure><h2 id="b403" class="mg kh hi bd ki mh mi mj km mk ml mm kq jg mn mo ks jk mp mq ku jo mr ms kw mt bi translated">损失函数</h2><p id="d567" class="pw-post-body-paragraph ix iy hi iz b ja ky ij jc jd kz im jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">我们可以像线性回归一样计算 MSE(均方误差),但如果我们在这里这样做，我们将得到一个非凸函数，这将导致一个以上的最小值，因此不是最优解。</p><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es ni"><img src="../Images/808c245e6ccc5da07485a62b10b60b68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*LMXMP4YZqUnth4-kbg3sQg.png"/></div></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nj"><img src="../Images/31743dee06836a2c0c25596b82472726.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*lER-OJYBPpPGNoXF.png"/></div></div></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nk"><img src="../Images/a93abe0a599bc0aafd3c713590104be6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SvLZZWlIZIYg1M-bV0CONg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">凸函数与非凸函数</figcaption></figure><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nl"><img src="../Images/24bab39ff7435fca10c535e51678b402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*srcakHg1ha-fEtFyGnf4cQ.png"/></div></div></figure><h2 id="3349" class="mg kh hi bd ki mh mi mj km mk ml mm kq jg mn mo ks jk mp mq ku jo mr ms kw mt bi translated">成本函数(J)</h2><p id="acd8" class="pw-post-body-paragraph ix iy hi iz b ja ky ij jc jd kz im jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">由于<strong class="iz hj">损失函数</strong>是针对单个训练示例定义的，它显示了您在单个训练示例中的表现。<br/>为了衡量你在整个训练集上的表现，你需要成本函数，</p><figure class="le lf lg lh fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es nm"><img src="../Images/1a72b722f638a4d544518e2957f9ec72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c-nFTOBvDrTOBDjV3rBkkw.png"/></div></div></figure><figure class="le lf lg lh fd li er es paragraph-image"><div class="er es nn"><img src="../Images/768d0834958b2e7cb471bae8b4f27336.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/0*0FRhRoYOHa_hVDcF"/></div></figure></div><div class="ab cl no np gp nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="hb hc hd he hf"><p id="8c2b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我会就此做更多的介绍，如果有任何疑问，你可以联系<a class="nv nw ge" href="https://medium.com/u/ef356a883f8e?source=post_page-----1184fb3635f6--------------------------------" rel="noopener" target="_blank">哈什特·贾因</a>——LinkedIn:<a class="ae lt" href="https://www.linkedin.com/in/harshit-jain-1202/" rel="noopener ugc nofollow" target="_blank">哈什特·贾因</a></p><p id="f5d8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">别忘了加上掌声👏👏如果你觉得有用的话。</p></div></div>    
</body>
</html>