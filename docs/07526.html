<html>
<head>
<title>The Mathematics Behind Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降背后的数学</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-mathematical-essence-of-ml-4182f3d172c3?source=collection_archive---------28-----------------------#2020-06-28">https://medium.com/analytics-vidhya/the-mathematical-essence-of-ml-4182f3d172c3?source=collection_archive---------28-----------------------#2020-06-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ec0ed96c33db918807d6afe67418b1bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sEDONUEsoiZnYsT6vXtiaw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://laconicml.com/stochastic-gradient-descent-in-python/" rel="noopener ugc nofollow" target="_blank">https://laconicml . com/random-gradient-descent-in-python/</a></figcaption></figure><p id="d123" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这个时代，由于Pytorch、TensorFlow和scikit-learn等开源机器学习库，部署机器学习算法变得相对简单，而无需真正掌握驱动ML的核心数学概念。</p><p id="d222" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我无法创造的东西，我不明白——理查德·费曼</p><p id="01ba" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作为一名工科学生，数学的力量一直让我着迷，学习算法背后的核心概念帮助我调整了许多需要微调的超参数。我写这篇文章的原因是为了帮助你看到最优化的数学之美。为此，我将尝试解释ML背后的主要算法，即梯度下降。这种算法如此强大，以至于它的变体被用在我们每天听到的强大的神经网络中。</p><p id="de3e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我假设您已经熟悉线性回归的基本原理。但是提醒一下，这里有一个公式:<em class="jt">y = wφ(x)+b</em></p><p id="a049" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于一个输入<em class="jt">φ(x)</em>，其特征向量为:</p><p id="094c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">φ(x)=[φ1(x)、..，φd(x)]—</em>你可以简单地把<em class="jt">φ(x)</em>看成高维空间中的一个点<em class="jt">。</em></p><p id="3432" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt"> b : </em>只是我们引入的偏差，让我们的模型更强大。你可以把它想成线方程中的截距。</p><p id="8481" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">直觉:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ju"><img src="../Images/11c6024f5b585330cc78b4d50e68c398.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fbQhJ0d24sT8NFeN6NrbYA.png"/></div></div></figure><p id="6a53" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">总体思路是找到一组<em class="jt"> W的</em>(权重)和一个最能描述上述数据的偏差。</p><p id="3ced" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们考虑第一个数据点，如您所见，它的特征向量是二维的，这意味着有两个特征会影响结果。</p><p id="ed5f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第一个数据点的矩阵表示:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es jz"><img src="../Images/114507642e9a4ce994b133757d3d9741.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*Xp_U-fbdDu8pu7wOMfqM7A.png"/></div></figure><p id="d4c2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的目标是找到一个权重向量和一个偏差，使模型的预测和𝕪i的真实值之间的误差最小化</p><p id="57ca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们必须引入一种方法，让模型知道训练过程是否在正确的方向上进行。为此，我们将引入一个称为列车损失的函数，该函数取决于两个变量<em class="jt"> w和b: </em></p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ka"><img src="../Images/fa56def78bfa0a524986a18e7695bb7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*nzNPhlFeXtapWXfBqgdAaw.png"/></div></figure></div><div class="ab cl kb kc gp kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="hb hc hd he hf"><h1 id="9ab8" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">这一切背后的学问在哪里？</h1><p id="c9de" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">有趣的部分来了，学习。我们如何教导数百万个硅晶体管针对上述功能进行优化？这就是梯度下降的作用。首先，我们来看看梯度下降的图形直觉。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/49b1e8c78cfa38341c96ec892be7838f.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*_y1KAoQ-jzi0xkDLUOsOTg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://ml-cheatsheet.readthedocs.io" rel="noopener ugc nofollow" target="_blank">https://ml-cheat sheet . readthedocs . io</a></figcaption></figure><p id="a0a1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然上图只考虑了1维特征向量，但是这个概念可以扩展到数千维。这个图表基本上是在描绘变量和它的成本函数之间的关系。在这种情况下，对成本函数的唯一变量进行简单求导就足够了。在我们的例子中，我们有成千上万的变量，需要偏导数。不要让名字吓到你；偏导数就是对影响成本函数的不同变量取不同的导数，下面我将计算我们的问题所需的偏导数:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/d3397505a8a8777b7b03160783219175.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fzQYx1n92j7SbMdGhpHApg.png"/></div></div></figure></div><div class="ab cl kb kc gp kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="hb hc hd he hf"><h2 id="a275" class="ln kj hi bd kk lo lp lq ko lr ls lt ks jg lu lv kw jk lw lx la jo ly lz le ma bi translated"><strong class="ak">算法:</strong></h2><p id="a86d" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">最后，我们几乎做到了，下面算法的简单性会让你震惊，但同时，让你体会到矩阵演算的强大</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/1a017b742ca781d20c73b35d5a58133f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Hg6jK5KtUmI931Fcvju9Q.png"/></div></div></figure></div><div class="ab cl kb kc gp kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="hb hc hd he hf"><p id="2899" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们到了压轴戏，现在让我们用python来实现这一切。</p><p id="1001" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这个python程序中，我将使用随机数生成器来创建虚构的d维特征向量。下一步将是分配真实偏差和真实权重向量。该程序将执行点积运算，以找到数据点。最终目标是找到权重向量和我初始化的偏差。<br/>实际权重和实际偏差如下:</p><pre class="jv jw jx jy fd mc md me mf aw mg bi"><span id="aba9" class="ln kj hi md b fi mh mi l mj mk">trueWeight = np.array([2, 4, 5, 9, 11])<br/>trueBias = 2</span></pre><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/a1121cfac28193d37df523086a2d6dda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m0bMPrmCdKT_91xRCK27zA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在前12次迭代中预测的权重和偏差。</figcaption></figure><p id="20ff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在250次迭代之后，模型接近实际的权重向量和偏差，损失几乎接近零。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/ba53e57f7ab265a2f6c0b64b0b288782.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qYR9WvFkt9kuWWrfZQokSQ.png"/></div></div></figure><h1 id="037e" class="ki kj hi bd kk kl mn kn ko kp mo kr ks kt mp kv kw kx mq kz la lb mr ld le lf bi translated">密码</h1><pre class="jv jw jx jy fd mc md me mf aw mg bi"><span id="eadb" class="ln kj hi md b fi mh mi l mj mk">import numpy as np<br/><br/>trueWeight = np.array([2, 4, 5, 9, 11])<br/>trueBias = 2<br/>d = len(trueWeight)<br/>data = []<br/>for i in range(5000):<br/>    x = np.random.randn(d)<br/>    y = trueWeight.dot(x) + trueBias<br/>    data.append((x, y))<br/><br/># For gradient descent<br/>def l(w,b):      #here we are calculating the cost function Train loss.<br/>    return sum((w.dot(x) + b - y)**2 for x, y in data) / len(data)<br/><br/>def df(w,b):   #calculating the partial derivatives.<br/>    return sum(2*(w.dot(x) + b - y) * x for x, y in data) / len(data), sum(2*(w.dot(x) + b - y) * 1 for x, y in data) / len(data)<br/><br/>#the gradient descent algorithm<br/>def gradientdescent(F, dF, d):<br/>    w = np.zeros(d)<br/>    b = 0<br/>    eta = 0.01<br/>    for i in range(1000):<br/>        loss = l(w,b)<br/>        gradientw , gradientb = df(w,b)<br/>        w = w - eta * gradientw<br/>        b = b - eta * gradientb<br/><br/><br/>        print(f'iteration {i}: w = {w},b = {b}, loss = {loss}')<br/><br/><br/>gradientdescent(l, df, d)</span></pre><h1 id="bd04" class="ki kj hi bd kk kl mn kn ko kp mo kr ks kt mp kv kw kx mq kz la lb mr ld le lf bi translated">带什么</h1><p id="2a3a" class="pw-post-body-paragraph iv iw hi ix b iy lg ja jb jc lh je jf jg li ji jj jk lj jm jn jo lk jq jr js hb bi translated">本文旨在展示ML中使用的最重要技术背后的简单数学概念。我希望我能激发你对最优化和数学的兴趣。快乐学习:)</p><h2 id="6c74" class="ln kj hi bd kk lo lp lq ko lr ls lt ks jg lu lv kw jk lw lx la jo ly lz le ma bi translated">提高ML所需数学知识的来源:</h2><figure class="jv jw jx jy fd ij"><div class="bz dy l di"><div class="ms mt l"/></div></figure></div></div>    
</body>
</html>