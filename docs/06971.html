<html>
<head>
<title>Text Clustering- Identifying Relationships in Clinical documents</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本聚类-识别临床文档中的关系</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/identifying-relationships-in-clinical-text-nlp-clustering-929eb04b5942?source=collection_archive---------3-----------------------#2020-06-09">https://medium.com/analytics-vidhya/identifying-relationships-in-clinical-text-nlp-clustering-929eb04b5942?source=collection_archive---------3-----------------------#2020-06-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0047" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是4部分系列的最后一部分！到目前为止，我们一直在谈论:</p><ol class=""><li id="719e" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><a class="ae jm" rel="noopener" href="/@tyagigaurika27/nlp-preprocessing-clinical-data-to-find-sections-461fdadbec77">预处理和清洗</a></li><li id="c917" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><a class="ae jm" rel="noopener" href="/@tyagigaurika27/text-summarization-for-clustering-documents-2e074da6437a">文本摘要</a></li><li id="2c4e" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><a class="ae jm" rel="noopener" href="/@tyagigaurika27/nlp-topic-modeling-to-identify-clusters-ca207244d04f">使用潜在狄利克雷分配(LDA </a>)的主题建模</li><li id="4dcb" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><strong class="ih hj">集群——我们来了！！</strong></li></ol><blockquote class="js jt ju"><p id="29a3" class="if ig jv ih b ii ij ik il im in io ip jw ir is it jx iv iw ix jy iz ja jb jc hb bi translated"><em class="hi">如果您想自己尝试整个代码或跟随，请访问我在GitHub上发布的jupyter笔记本:</em><a class="ae jm" href="https://github.com/gaurikatyagi/Natural-Language-Processing/blob/master/Introdution%20to%20NLP-Clustering%20Text.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="hi">https://GitHub . com/gaurikatyagi/Natural-Language-Processing/blob/master/introduction % 20 to % 20 NLP-Clustering % 20 text . ipynb</em></a></p></blockquote><h1 id="f100" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">聚类的输入数据(第3节的概述)</h1><p id="a9a0" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">使用词性标注和识别短语中单词的层次来清理和总结文本。这然后被输入到一个<strong class="ih hj">潜在狄利克雷分配(LDA) </strong>算法中以得到主题:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lc"><img src="../Images/160665b9b6f1eab145229f4d87eb0e29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8MZ4MB9nBK1UiwoH.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">LDA输出</figcaption></figure><p id="fbd5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本节中，我们将把上述内容转换成每个文本(输入行)的数据框架，得到每个主题(特征)的%倾向。</p><pre class="ld le lf lg fd ls lt lu lv aw lw bi"><span id="3083" class="lx ka hi lt b fi ly lz l ma mb">topics_all = pd.DataFrame.from_dict(document_topic, orient='index')<br/>topic_column_names = ['topic_' + str(i) for i in range(0, 30)]<br/>topics_all.columns = topic_column_names<br/>display(topics_all.head())</span></pre><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mc"><img src="../Images/dd7685d3dd43971656dd97184fbe9864.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9GYmHtTF2gRwyO6yH-FYrg.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">聚类输入</figcaption></figure><p id="4d69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在可以在图表注释中可视化这些主题的层次结构！这将有助于确定我们需要的集群数量。</p><h1 id="8448" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">可视化以识别集群数量</h1><pre class="ld le lf lg fd ls lt lu lv aw lw bi"><span id="b173" class="lx ka hi lt b fi ly lz l ma mb">from scipy.cluster import hierarchy<br/>plt.figure(figsize=(10, 7))  <br/>plt.title("Dendrograms")  <br/>dend = hierarchy.dendrogram(hierarchy.linkage(topics_all, method='ward'))<br/>plt.axhline(y=9, color='r', linestyle='--')</span></pre><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es md"><img src="../Images/be91406b7a55708c11567c5bc126174b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8qzqtzykhpsvVmE8_yA7pQ.png"/></div></div></figure><p id="d850" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">x轴包含样本，y轴表示这些样本之间的距离。距离最大的垂直线是蓝线，因此我们可以决定阈值为9，并在该点切割树状图(水平虚线)。</p><p id="829a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有4个集群，因为这条线在4个点上切割树状图。现在让我们对这些集群应用层次聚类。</p><h1 id="a070" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">使聚集</h1><p id="9a7b" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">在我们聚集之前。我们应该知道你在选择什么参数以及为什么:联系决定了在一组观察值之间使用哪个距离。该算法合并最小化所选标准的聚类对。</p><ol class=""><li id="a8a0" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated"><strong class="ih hj"> ward </strong>最小化被合并的聚类的方差。</li><li id="edb8" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><strong class="ih hj">平均值</strong>使用两个集合的每次观察距离的平均值。</li><li id="7a4d" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><strong class="ih hj">完全或最大关联</strong>使用两组所有观察值之间的最大距离。</li><li id="0230" class="jd je hi ih b ii jn im jo iq jp iu jq iy jr jc ji jj jk jl bi translated"><strong class="ih hj"> single </strong>使用两个集合的所有观测值之间的最小距离。</li></ol><p id="c756" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我想最小化合并后的聚类之间的差异。现在，我只能用<strong class="ih hj">欧几里德距离。</strong></p><pre class="ld le lf lg fd ls lt lu lv aw lw bi"><span id="5904" class="lx ka hi lt b fi ly lz l ma mb">from sklearn.cluster import AgglomerativeClustering<br/>cluster_model = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')  <br/>cluster = cluster_model.fit_predict(topics_all).tolist()<br/># print(type(cluster))<br/>topics_all["cluster"] = cluster</span><span id="fa9e" class="lx ka hi lt b fi me lz l ma mb">##VISUALIZZATION</span><span id="7289" class="lx ka hi lt b fi me lz l ma mb">df_for_h_visual = df ## derived from topics_all to get 1 topic per text- refer to part 3 of this series.<br/>df_for_h_visual["cluster"] = topics_all["cluster"]<br/>df_for_h_visual.drop(['propensity'], axis = 1, inplace=True)<br/>df_for_h_visual.topic.fillna(value="Unknown", inplace=True)<br/>df_for_h_visual.head()</span></pre><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mf"><img src="../Images/028099f4968f6be48abf06d2e4095939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PQS3YnwK4YoC-xLF8pn6fA.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">每个文本及其集群1个主题</figcaption></figure><h1 id="4b19" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">可视化集群</h1><pre class="ld le lf lg fd ls lt lu lv aw lw bi"><span id="df71" class="lx ka hi lt b fi ly lz l ma mb">df_histo = df_for_h_visual.groupby(['topic','cluster']).count().reset_index()<br/>df_histo = df_histo.pivot(index='topic', columns='cluster', values='SUMMARY')<br/>df_histo.columns = ["c0", "c1", "c2", "c3"]</span><span id="6e96" class="lx ka hi lt b fi me lz l ma mb">ax = df_histo.plot.bar(stacked=True,   colormap='inferno', edgecolor='black', linewidth=1)</span><span id="603e" class="lx ka hi lt b fi me lz l ma mb">ax.legend(loc='center left', bbox_to_anchor=(1.0, .5))<br/>ax.spines['top'].set_visible(False)<br/>ax.spines['right'].set_visible(False)<br/>ax.spines['bottom'].set_visible(False)<br/>ax.spines['left'].set_visible(False)</span><span id="1fa8" class="lx ka hi lt b fi me lz l ma mb">plt.show()</span></pre><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mg"><img src="../Images/59e2b9bcb0def1e9dfee17e2c57d5e92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ArIQQfBT_b-4xhnGD1VT-w.png"/></div></div></figure><h1 id="76e8" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">结论</h1><p id="aa36" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">来自建模主题的“未知”类别的大约50个文档实际上与主题_23和主题_5(集群1)在相同的域空间中。</p><p id="2d56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个4部分的教程中，你已经看到了如何从原始文本到摘要文本。然后对主题进行建模，最后，将这些主题聚集在一起，以识别相近但不是相同主题的文本之间的关系。这也有助于确定以前未确定的临床笔记的“潜在主题”。我希望你喜欢这个！</p></div></div>    
</body>
</html>