<html>
<head>
<title>Bank Data: Classification Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">银行数据:分类第2部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bank-data-classification-part-2-af9cfb38c18a?source=collection_archive---------19-----------------------#2020-09-20">https://medium.com/analytics-vidhya/bank-data-classification-part-2-af9cfb38c18a?source=collection_archive---------19-----------------------#2020-09-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/af4db06851968cd741fa4b5c3884b9eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OJBPR8fieYJe9YF6lYJqNA.png"/></div></div></figure><p id="f8a2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是分类部分的第2部分，共4部分。</p><h2 id="f934" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">k个最近邻居</h2><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kj"><img src="../Images/df73973b1a5f29d54a35b1de445da9d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y1NorN-FsO9Lirw5M0e1Fw.png"/></div></div></figure><p id="40c7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我决定对银行数据使用的第二个算法是K近邻算法。k最近邻是一个分组分类器，可用于回归或分类，即利用数据点的距离来分类。如果一定数量的点非常接近，那么它们将属于同一类。</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="66f6" class="jo jp hi kp b fi kt ku l kv kw"><em class="kx"># Instantiating KNN</em><br/>knn_clf = KNeighborsClassifier()</span><span id="7a67" class="jo jp hi kp b fi ky ku l kv kw"><em class="kx"># Grid Search</em><br/>param_knn = {"n_neighbors": [5, 7, 9, 11]}<br/>grid_knn = GridSearchCV(knn_clf, param_grid=param_knn)</span><span id="1bb6" class="jo jp hi kp b fi ky ku l kv kw"><em class="kx"># Fitting model</em><br/>grid_knn.fit(X_train_rescaled, y_train_new)</span><span id="f310" class="jo jp hi kp b fi ky ku l kv kw">grid_knn.cv_results_</span></pre><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kz"><img src="../Images/1651ce470c5091d715dd128b8dbc687b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OsNsk0asmESB1BkbX6dXeg.png"/></div></div></figure><p id="df0b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我相信您已经注意到，与用于适应模型的训练集略有不同。对于KNN，我们使用了重新调整的训练数据。我们使用要素缩放是因为KNN的一个假设是假设您的数据是缩放的。如果不进行缩放，在使用该算法时会出现问题，因为KNN的主要功能是距离。数字越大，它的重量就越大。</p><p id="9bb1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的代码和图像显示了使用超参数调整的结果。如果我们查看交叉验证结果，我们可以看到我们肯定会得到大约71%的测试分数</p><h2 id="02a4" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">混淆矩阵</h2><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="92ea" class="jo jp hi kp b fi kt ku l kv kw"><em class="kx"># Confusion Matrix</em><br/>print('Confusion Matrix - Testing Dataset')<br/>print(pd.crosstab(y_test, grid_knn.predict(X_test), rownames=['True'], colnames=['Predicted'], margins=<strong class="kp hj">True</strong>))</span></pre><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/9ed3591cca60502c6f01cca1e9138e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VxATV5zmET83ELQ0fGC9pg.png"/></div></div></figure><p id="15be" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的图像和代码显示了混淆矩阵的结果。KNN收到了不好的结果，所以我不在乎继续下去。</p><h2 id="98f5" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">为什么用KNN？</h2><p id="7c3a" class="pw-post-body-paragraph iq ir hi is b it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj lf jl jm jn hb bi translated">优点:</p><ol class=""><li id="e368" class="lg lh hi is b it iu ix iy jb li jf lj jj lk jn ll lm ln lo bi translated">K-NN非常直观简单 : K-NN算法非常容易理解，也同样容易实现。为了对新数据点进行分类，K-NN算法遍历整个数据集以找出K个最近邻。</li><li id="ea37" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj"> K-NN没有假设</strong> : K-NN是一种非参数算法，这意味着实现K-NN需要满足一些假设。像线性回归这样的参数模型在实现之前需要满足大量的数据假设，而K-NN则不是这种情况。</li><li id="e684" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj">否</strong> <strong class="is hj">训练步骤:</strong> K-NN不显式地构建任何模型，它只是基于从历史数据的学习来标记新的数据条目。新的数据条目将被标记为最近邻中的多数类。</li><li id="7bdc" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">它不断地进化:这是一种基于实例的学习；k-NN是一种基于记忆的方法。当我们收集新的训练数据时，分类器会立即适应。它允许算法在实时使用期间快速响应输入的变化。</li><li id="9cc2" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj">对于多类问题非常容易实现:</strong>大多数分类器算法对于二元问题很容易实现，对于多类问题需要努力实现，而K-NN无需任何额外的努力就可以调整到多类。</li><li id="ef08" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj">既可用于分类也可用于回归:</strong>K-NN最大的一个优点就是K-NN既可用于分类也可用于回归问题。</li><li id="347d" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj">一个超参数:</strong> K-NN在选择第一个超参数时可能会花费一些时间，但在此之后，其余的参数会与之对齐。</li><li id="f029" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj">多种距离标准可供选择:</strong> K-NN算法使用户在建立K-NN模型时可以灵活选择距离。</li></ol><p id="2812" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">缺点:</p><ol class=""><li id="a2ab" class="lg lh hi is b it iu ix iy jb li jf lj jj lk jn ll lm ln lo bi translated"><strong class="is hj"> K-NN慢算法</strong> : K-NN可能非常容易实现，但随着数据集的增长，算法的效率或速度下降得非常快。</li><li id="e0ad" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj">维数灾难:</strong> KNN在输入变量数量较少的情况下工作良好，但随着变量数量的增加，K-NN算法很难预测新数据点的输出。</li><li id="b776" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj"> K-NN需要同质特征</strong>:如果您决定使用普通距离构建K-NN，如欧几里德距离或曼哈顿距离，则特征具有相同的尺度是完全必要的，因为特征的绝对差异权重相同，即特征1中的给定距离对特征2来说必须是相同的。</li><li id="38fc" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj">最佳邻居数量</strong>:K-NN的最大问题之一是在对新数据条目进行分类时，选择要考虑的最佳邻居数量。</li><li id="0a09" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj">不平衡数据导致问题</strong> : k-NN对不平衡数据表现不佳。如果我们考虑两个类，A和B，并且大多数训练数据被标记为A，那么模型最终将给予A很多偏好。这可能导致不太常见的B类被错误分类。</li><li id="27e3" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj">异常值敏感性:</strong> K-NN算法对异常值非常敏感，因为它只是基于距离标准选择邻居。</li><li id="c5c2" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj">缺失值处理:</strong> K-NN天生没有处理缺失值问题的能力。</li></ol></div></div>    
</body>
</html>