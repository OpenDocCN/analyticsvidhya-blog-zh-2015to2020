<html>
<head>
<title>Build your semantic document search engine with TF-IDF and Google-USE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ç”¨TF-IDFå’ŒGoogle-USEæ„å»ºä½ çš„è¯­ä¹‰æ–‡æ¡£æœç´¢å¼•æ“</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/analytics-vidhya/build-your-semantic-document-search-engine-with-tf-idf-and-google-use-c836bf5f27fb?source=collection_archive---------3-----------------------#2020-01-26">https://medium.com/analytics-vidhya/build-your-semantic-document-search-engine-with-tf-idf-and-google-use-c836bf5f27fb?source=collection_archive---------3-----------------------#2020-01-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es im"><img src="../Images/52dd1a38422cd37cd6a15649c623dbf9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tVAOE4YZHlckxwPdB0wkUQ.jpeg"/></div></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">æ¥æº:<a class="ae jc" href="http://inmyownterms.com/six-document-search-engines-use/" rel="noopener ugc nofollow" target="_blank"> inmyownterms </a></figcaption></figure><p id="5f82" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">è®©æˆ‘ä»¬æ¥äº†è§£ä¸€ä¸‹å¦‚ä½•ä½¿ç”¨pythonè¯­è¨€æ¥æ„å»ºä¸€ä¸ªæ–‡æ¡£æœç´¢å¼•æ“ã€‚</p><p id="af7a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">åœ¨æœ¬å¸–ä¸­ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨<a class="ae jc" href="http://qwone.com/~jason/20Newsgroups/" rel="noopener ugc nofollow" target="_blank"> 20newsgroupå¼€æºæ•°æ®é›†</a>æ„å»ºä¸€ä¸ª<strong class="jf hj">è¯­ä¹‰æ–‡æ¡£æœç´¢å¼•æ“</strong>ã€‚</p><h1 id="1421" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">å…ˆå†³æ¡ä»¶</h1><ul class=""><li id="9cbb" class="kz la hi jf b jg lb jk lc jo ld js le jw lf ka lg lh li lj bi translated"><a class="ae jc" href="https://www.python.org/" rel="noopener ugc nofollow" target="_blank"> Python 3.5 </a> +</li><li id="3709" class="kz la hi jf b jg lk jk ll jo lm js ln jw lo ka lg lh li lj bi translated"><a class="ae jc" href="https://pypi.org/project/pip/" rel="noopener ugc nofollow" target="_blank"> pip 19 </a> +æˆ–pip3</li><li id="e7fe" class="kz la hi jf b jg lk jk ll jo lm js ln jw lo ka lg lh li lj bi translated"><a class="ae jc" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK </a></li><li id="ed6e" class="kz la hi jf b jg lk jk ll jo lm js ln jw lo ka lg lh li lj bi translated"><a class="ae jc" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a></li><li id="f69c" class="kz la hi jf b jg lk jk ll jo lm js ln jw lo ka lg lh li lj bi translated"><a class="ae jc" href="https://www.tensorflow.org" rel="noopener ugc nofollow" target="_blank"> TensorFlow-GPU </a></li></ul><h1 id="f7f4" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">1.åšå¥½å‡†å¤‡</h1><p id="1e42" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">å¯¹äºè¿™ä¸ªèŒä½ï¼Œæˆ‘ä»¬å°†éœ€è¦ä¸Šè¿°å…ˆå†³æ¡ä»¶<strong class="jf hj">ï¼Œ</strong>å¦‚æœä½ è¿˜æ²¡æœ‰ï¼Œè¯·åšå¥½å‡†å¤‡ã€‚</p><h1 id="f3c9" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">2.æ•°æ®æ”¶é›†</h1><p id="ddb1" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨20ä¸ªæ–°é—»ç»„æ•°æ®é›†æ¥åˆ†æç»™å®šè¾“å…¥å…³é”®è¯/å¥å­è¾“å…¥çš„æ–‡æœ¬æœç´¢å¼•æ“ã€‚</p><p id="b7bb" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">20ä¸ªæ–°é—»ç»„æ•°æ®é›†æ˜¯å¤§çº¦11Kä¸ªæ–°é—»ç»„æ–‡æ¡£çš„é›†åˆï¼Œå¹³å‡åˆ†å¸ƒåœ¨20ä¸ªä¸åŒçš„æ–°é—»ç»„ä¸­ã€‚</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="55d0" class="lx kc hi lt b fi ly lz l ma mb">news = pd.read_json('<a class="ae jc" href="https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/newsgroups.json" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/newsgroups.json</a>')</span></pre><h2 id="d8fe" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">2.1æ•°æ®æ¸…ç†:</h2><p id="9f43" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">åœ¨è¿›å…¥æ¸…ç†é˜¶æ®µä¹‹å‰ï¼Œæˆ‘ä»¬ä»æ–‡æœ¬ä¸­æ£€ç´¢æ–‡æ¡£çš„ä¸»é¢˜ã€‚</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="e1d8" class="lx kc hi lt b fi ly lz l ma mb">for i,txt in enumerate(news['content']):<br/>    subject = re.findall('Subject:(.*\n)',txt)<br/>    if (len(subject) !=0):<br/>        news.loc[i,'Subject'] =str(i)+' '+subject[0]<br/>    else:<br/>        news.loc[i,'Subject'] ='NA'<br/>df_news =news[['Subject','content']]</span></pre><p id="abe0" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">ç°åœ¨ï¼Œæˆ‘ä»¬ä»æ–‡æœ¬å†…å®¹å’Œæ•°æ®é›†çš„ä¸»é¢˜ä¸­åˆ é™¤ä¸éœ€è¦çš„æ•°æ®ã€‚</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="8376" class="lx kc hi lt b fi ly lz l ma mb">df_news.content =df_news.content.replace(to_replace='from:(.*\n)',value='',regex=True) ##remove from to email <br/>df_news.content =df_news.content.replace(to_replace='lines:(.*\n)',value='',regex=True)<br/>df_news.content =df_news.content.replace(to_replace='[!"#$%&amp;\'()*+,/:;&lt;=&gt;?@[\\]^_`{|}~]',value=' ',regex=True) #remove punctuation except<br/>df_news.content =df_news.content.replace(to_replace='-',value=' ',regex=True)<br/>df_news.content =df_news.content.replace(to_replace='\s+',value=' ',regex=True)    #remove new line<br/>df_news.content =df_news.content.replace(to_replace='  ',value='',regex=True)                #remove double white space<br/>df_news.content =df_news.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace</span></pre><h2 id="e5f6" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">2.2æ•°æ®é¢„å¤„ç†</h2><p id="8fab" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">é¢„å¤„ç†æ˜¯æˆ‘ä»¬å¤„ç†ä»»ä½•æ–‡æœ¬æ¨¡å‹çš„ä¸»è¦æ­¥éª¤ä¹‹ä¸€ã€‚åœ¨æ­¤é˜¶æ®µï¼Œæˆ‘ä»¬å¿…é¡»æŸ¥çœ‹æ•°æ®çš„åˆ†å¸ƒæƒ…å†µï¼Œéœ€è¦ä»€ä¹ˆæŠ€æœ¯ä»¥åŠåº”è¯¥æ¸…ç†å¤šæ·±ã€‚</p><h2 id="4e6b" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">å°å†™å­—æ¯</h2><p id="266d" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">å°†æ–‡æœ¬è½¬æ¢æˆå°å†™å½¢å¼ã€‚å³'<strong class="jf hj">ç‹—'</strong>å˜æˆ'<strong class="jf hj">ç‹—</strong></p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="0a78" class="lx kc hi lt b fi ly lz l ma mb">df_news['content']=[entry.lower() for entry in df_news['content']]</span></pre><h2 id="d20e" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">å•è¯æ ‡è®°åŒ–</h2><p id="61f6" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">å•è¯æ ‡è®°åŒ–æ˜¯å°†å¥å­åˆ†æˆå•è¯å½¢å¼çš„è¿‡ç¨‹ã€‚</p><p id="9ffb" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">â€œ<strong class="jf hj">çº¦ç¿°åœ¨è½¨é“</strong>ä¸Šè·‘â€â†’â€œ<strong class="jf hj">çº¦ç¿°</strong>â€ã€â€œ<strong class="jf hj">æ˜¯</strong>â€ã€â€œ<strong class="jf hj">è·‘</strong>â€ã€â€œä¸­çš„<strong class="jf hj">â€ã€â€œ<strong class="jf hj">ä¸­çš„</strong>â€ã€â€œ<strong class="jf hj">è½¨é“</strong>â€</strong></p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="f2bd" class="lx kc hi lt b fi ly lz l ma mb">df_news['Word tokenize']= [word_tokenize(entry) for entry in df_news.content]</span></pre><h2 id="476e" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">åœæ­¢è¨€è¯­</h2><p id="054a" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">åœç”¨è¯æ˜¯æœ€å¸¸è§çš„è¯ï¼Œå®ƒä¸ä¼šç»™æ–‡æ¡£å‘é‡å¸¦æ¥ä»»ä½•é™„åŠ å€¼ã€‚äº‹å®ä¸Šï¼Œåˆ é™¤è¿™äº›å°†å¢åŠ è®¡ç®—å’Œç©ºé—´æ•ˆç‡ã€‚NLTKåº“æœ‰ä¸€ä¸ªä¸‹è½½åœç”¨è¯çš„æ–¹æ³•ã€‚</p><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es mp"><img src="../Images/c07e2a457bd4cedd5adaa29910de1fc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*PdgWsOM1ep9Z2rfkQ6UJZA.png"/></div></figure><h2 id="d30d" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">å•è¯è¯æ±‡åŒ–</h2><p id="e608" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">å¼•ç†åŒ–æ˜¯ä¸€ç§å°†å•è¯ç®€åŒ–ä¸ºå•è¯çš„è¯æ ¹åŒä¹‰è¯çš„æ–¹æ³•ã€‚ä¸è¯å¹²æå–ä¸åŒï¼Œè¯æ±‡åŒ¹é…ç¡®ä¿ç¼©å‡åçš„å•è¯å†æ¬¡æˆä¸ºè¯å…¸ä¸­çš„å•è¯(åŒä¸€ç§è¯­è¨€ä¸­çš„å•è¯)ã€‚WordNetLemmatizerå¯ç”¨äºå¯¹ä»»ä½•å•è¯è¿›è¡Œè¯æ±‡åŒ–ã€‚</p><p id="6d13" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">å³<strong class="jf hj">å²©çŸ³â†’å²©çŸ³ï¼Œæ›´å¥½â†’å¥½ï¼Œè¯­æ–™åº“â†’è¯­æ–™åº“</strong></p><p id="f5c8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">è¿™é‡Œåˆ›å»ºäº†wordLemmatizerå‡½æ•°æ¥åˆ é™¤ä¸€ä¸ª<strong class="jf hj">å•å­—ç¬¦</strong>ã€<strong class="jf hj">åœç”¨è¯</strong>å’Œ<strong class="jf hj">å•è¯ã€‚</strong></p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="6375" class="lx kc hi lt b fi ly lz l ma mb"># WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun<br/>def wordLemmatizer(data):<br/>    tag_map = defaultdict(lambda : wn.NOUN)<br/>    tag_map['J'] = wn.ADJ<br/>    tag_map['V'] = wn.VERB<br/>    tag_map['R'] = wn.ADV<br/>    file_clean_k =pd.DataFrame()<br/>    for index,entry in enumerate(data):<br/>        <br/>        # Declaring Empty List to store the words that follow the rules for this step<br/>        Final_words = []<br/>        # Initializing WordNetLemmatizer()<br/>        word_Lemmatized = WordNetLemmatizer()<br/>        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.<br/>        for word, tag in pos_tag(entry):<br/>            # Below condition is to check for Stop words and consider only alphabets<br/>            if len(word)&gt;1 and word not in stopwords.words('english') and word.isalpha():<br/>                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])<br/>                Final_words.append(word_Final)<br/>            # The final processed set of words for each iteration will be stored in 'text_final'<br/>                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)<br/>                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)<br/>                file_clean_k=file_clean_k.replace(to_replace ="\[.", value = '', regex = True)<br/>                file_clean_k=file_clean_k.replace(to_replace ="'", value = '', regex = True)<br/>                file_clean_k=file_clean_k.replace(to_replace =" ", value = '', regex = True)<br/>                file_clean_k=file_clean_k.replace(to_replace ='\]', value = '', regex = True)<br/>    return file_clean_k</span></pre><p id="ff52" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">é€šè¿‡ä½¿ç”¨è¿™ä¸ªå‡½æ•°ï¼ŒèŠ±è´¹äº†å¤§çº¦<strong class="jf hj"> 13å°æ—¶</strong>çš„æ—¶é—´æ¥æ£€æŸ¥å’Œè¯æ¡åŒ–20ä¸ªæ–°é—»ç»„æ•°æ®é›†çš„11Kä¸ªæ–‡æ¡£çš„å•è¯ã€‚åœ¨ä¸‹é¢æ‰¾åˆ°è¿™ä¸ªå•è¯çš„JSONæ–‡ä»¶ã€‚</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="533f" class="lx kc hi lt b fi ly lz l ma mb"><a class="ae jc" href="https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/WordLemmatize20NewsGroup.json" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/WordLemmatize20NewsGroup.json</a></span></pre><h2 id="1f4c" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">2.3æ•°æ®å·²å‡†å¤‡å°±ç»ª</h2><p id="cefc" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">æŸ¥çœ‹å¹²å‡€æ•°æ®çš„ç¤ºä¾‹-</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="c7ff" class="lx kc hi lt b fi ly lz l ma mb">df_news.Clean_Keyword[0]</span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es mq"><img src="../Images/73e7a4167f0017cc75df9b2343e379a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Br5cASjTPcoN0J1QhXIYuA.png"/></div></div></figure><h1 id="59b8" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">3.æ–‡æ¡£æœç´¢å¼•æ“</h1><p id="13a8" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸‰ç§æ–¹æ³•æ¥ç†è§£æ–‡æœ¬åˆ†æã€‚</p><p id="3cf3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">1.ä½¿ç”¨<strong class="jf hj"> TF-IDF </strong>çš„æ–‡æ¡£æœç´¢å¼•æ“</p><p id="0498" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">2.å¸¦æœ‰<strong class="jf hj">è°·æ­Œé€šç”¨è¯­å¥ç¼–ç å™¨</strong>çš„æ–‡æ¡£æœç´¢å¼•æ“</p><h2 id="0bba" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">3.1ä½¿ç”¨<a class="ae jc" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">ä½™å¼¦ç›¸ä¼¼åº¦</a>è®¡ç®—æ’å</h2><p id="8d5d" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">è¿™æ˜¯æ ¹æ®è¾“å…¥å…³é”®å­—/å¥å­è®¡ç®—æ–‡æ¡£æ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§çš„æœ€å¸¸è§çš„åº¦é‡ã€‚æ•°å­¦ä¸Šï¼Œå®ƒæµ‹é‡çš„æ˜¯åœ¨å¤šç»´ç©ºé—´ä¸­æŠ•å½±çš„ä¸¤ä¸ªå‘é‡çš„è§’åº¦b/wçš„ä½™å¼¦å€¼ã€‚</p><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es mr"><img src="../Images/4a415cd5889ab06dceaf78570f4384fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*nJT7q9nlDWgXllSHcI4ZJA.jpeg"/></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">è¦æŸ¥è¯¢çš„é»‘ç™½æ–‡æ¡£çš„ä½™å¼¦ç›¸ä¼¼åº¦</figcaption></figure><p id="178a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">åœ¨ä¸Šå›¾ä¸­ï¼Œç©ºé—´ä¸­æœ‰3ä¸ªæ–‡æ¡£å‘é‡å€¼å’Œä¸€ä¸ªæŸ¥è¯¢å‘é‡ã€‚å½“æˆ‘ä»¬è®¡ç®—3ä¸ªæ–‡æ¡£çš„ä½™å¼¦ç›¸ä¼¼åº¦b/wæ—¶ã€‚æœ€ç›¸ä¼¼å€¼å°†æ˜¯ä¸‰ä¸ªæ–‡æ¡£ä¸­çš„D3æ–‡æ¡£ã€‚</p><h1 id="e6f5" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">1.ä½¿ç”¨TF-IDFçš„æ–‡æ¡£æœç´¢å¼•æ“:</h1><p id="031e" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated"><a class="ae jc" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hj"> TF-IDF </strong> </a>ä»£è¡¨<strong class="jf hj">â€œè¯é¢‘â€”é€†æ–‡æ¡£é¢‘â€</strong>ã€‚è¿™æ˜¯ä¸€ç§è®¡ç®—æ¯ä¸ªå•è¯çš„æƒé‡çš„æŠ€æœ¯ï¼Œè¡¨ç¤ºè¯¥å•è¯åœ¨æ–‡æ¡£å’Œè¯­æ–™åº“ä¸­çš„é‡è¦æ€§ã€‚è¯¥ç®—æ³•ä¸»è¦ç”¨äºä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬æŒ–æ˜é¢†åŸŸã€‚</p><h2 id="60af" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">æœ¯è¯­é¢‘ç‡(TF)</h2><p id="156d" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">å•è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°é™¤ä»¥æ–‡æ¡£ä¸­çš„æ€»å•è¯æ•°ã€‚æ¯ä¸ªæ–‡æ¡£éƒ½æœ‰å…¶è¯é¢‘ã€‚</p><figure class="in io ip iq fd ir er es paragraph-image"><div role="button" tabindex="0" class="is it di iu bf iv"><div class="er es ms"><img src="../Images/87ac28bd18de9ce28e20848c7143bca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:686/format:webp/0*0Uzik-cTMA-i6BUt.png"/></div></div></figure><h2 id="1a29" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">åå‘æ•°æ®é¢‘ç‡(IDF)</h2><p id="d827" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">æ—¥å¿—ä¸­çš„æ–‡æ¡£æ•°é™¤ä»¥åŒ…å«å•è¯<strong class="jf hj"> <em class="mt"> w </em> </strong>çš„æ–‡æ¡£æ•°ã€‚é€†æ•°æ®é¢‘ç‡å†³å®šäº†è¯­æ–™åº“ä¸­æ‰€æœ‰æ–‡æ¡£ä¸­ç¨€æœ‰è¯çš„æƒé‡ã€‚</p><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es mu"><img src="../Images/2c4cee08c4949ce478536fe6f799f7fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/0*t2Uxb_43L3vjwDPm.png"/></div></figure><p id="4e3d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">æœ€åï¼Œ<strong class="jf hj"> TF-IDF </strong>å°±æ˜¯TFä¹˜ä»¥IDFã€‚</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="9dc2" class="lx kc hi lt b fi ly lz l ma mb"><strong class="lt hj">TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)</strong></span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es mv"><img src="../Images/44cf9d26b4295a628fe18837da398987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/0*yJm1bH6Ds0vFFyhP.png"/></div></figure><p id="d4a1" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">æˆ‘ä»¬å¯ä»¥ä½¿ç”¨<a class="ae jc" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> Sklearn </a>æä¾›çš„ç±»ï¼Œè€Œä¸æ˜¯è‡ªå·±æ‰‹åŠ¨å®ç°<a class="ae jc" href="http://www.tfidf.com/" rel="noopener ugc nofollow" target="_blank"> TF-IDF </a>ã€‚</p><h2 id="fd7e" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">ä½¿ç”¨Sklearnçš„TfidfVectorizerç”ŸæˆTF-IDF</h2><p id="8e06" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">å¯¼å…¥åŒ…:</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="651a" class="lx kc hi lt b fi ly lz l ma mb">import pandas as pd<br/>import numpy as np<br/>import os <br/>import re<br/>import operator<br/>import nltk <br/>from nltk.tokenize import word_tokenize<br/>from nltk import pos_tag<br/>from nltk.corpus import stopwords<br/>from nltk.stem import WordNetLemmatizer<br/>from collections import defaultdict<br/>from nltk.corpus import wordnet as wn<br/>from sklearn.feature_extraction.text import TfidfVectorizer</span></pre><p id="cf72" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">TF-IDF</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="91b5" class="lx kc hi lt b fi ly lz l ma mb">from sklearn.feature_extraction.text import TfidfVectorizer<br/>import operator</span><span id="6839" class="lx kc hi lt b fi mw lz l ma mb">## Create Vocabulary<br/>vocabulary = set()</span><span id="8ab7" class="lx kc hi lt b fi mw lz l ma mb">for doc in df_news.Clean_Keyword:<br/>    vocabulary.update(doc.split(','))</span><span id="4b25" class="lx kc hi lt b fi mw lz l ma mb">vocabulary = list(vocabulary)</span><span id="c167" class="lx kc hi lt b fi mw lz l ma mb"># Intializating the tfIdf model<br/>tfidf = TfidfVectorizer(vocabulary=vocabulary)</span><span id="4e83" class="lx kc hi lt b fi mw lz l ma mb"># Fit the TfIdf model<br/>tfidf.fit(df_news.Clean_Keyword)</span><span id="0c87" class="lx kc hi lt b fi mw lz l ma mb"># Transform the TfIdf model<br/>tfidf_tran=tfidf.transform(df_news.Clean_Keyword)</span></pre><p id="06d3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">ä¸Šé¢çš„ä»£ç å·²ç»åˆ›å»ºäº†æ•´ä¸ªæ•°æ®é›†çš„TF-IDFæƒé‡ï¼Œç°åœ¨å¿…é¡»åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥ä¸ºè¾“å…¥æŸ¥è¯¢ç”Ÿæˆä¸€ä¸ªå‘é‡ã€‚</p><h2 id="c045" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">ä¸ºæŸ¥è¯¢/æœç´¢å…³é”®å­—åˆ›å»ºå‘é‡</h2><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="2394" class="lx kc hi lt b fi ly lz l ma mb">def gen_vector_T(tokens):</span><span id="57f3" class="lx kc hi lt b fi mw lz l ma mb">Q = np.zeros((len(vocabulary)))    <br/>    x= tfidf.transform(tokens)<br/>    #print(tokens[0].split(','))<br/>    for token in tokens[0].split(','):<br/>        #print(token)<br/>        try:<br/>            ind = vocabulary.index(token)<br/>            Q[ind]  = x[0, tfidf.vocabulary_[token]]<br/>        except:<br/>            pass<br/>    return Q</span></pre><h2 id="aae2" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">ç”¨äºè®¡ç®—çš„ä½™å¼¦ç›¸ä¼¼å‡½æ•°</h2><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="7ee1" class="lx kc hi lt b fi ly lz l ma mb">def cosine_sim(a, b):<br/>    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))<br/>    return cos_sim</span></pre><h2 id="118f" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">ä½™å¼¦ç›¸ä¼¼åº¦é»‘ç™½æ–‡æ¡£åˆ°æŸ¥è¯¢å‡½æ•°</h2><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="c4e8" class="lx kc hi lt b fi ly lz l ma mb">def cosine_similarity_T(k, query):<br/>    preprocessed_query = preprocessed_query = re.sub("\W+", " ", query).strip()<br/>    tokens = word_tokenize(str(preprocessed_query))<br/>    q_df = pd.DataFrame(columns=['q_clean'])<br/>    q_df.loc[0,'q_clean'] =tokens<br/>    q_df['q_clean'] =wordLemmatizer(q_df.q_clean)<br/>    d_cosines = []<br/>    <br/>    query_vector = gen_vector_T(q_df['q_clean'])<br/>    for d in tfidf_tran.A:<br/>        d_cosines.append(cosine_sim(query_vector, d))<br/>                    <br/>    out = np.array(d_cosines).argsort()[-k:][::-1]<br/>    #print("")<br/>    d_cosines.sort()<br/>    a = pd.DataFrame()<br/>    for i,index in enumerate(out):<br/>        a.loc[i,'index'] = str(index)<br/>        a.loc[i,'Subject'] = df_news['Subject'][index]<br/>    for j,simScore in enumerate(d_cosines[-k:][::-1]):<br/>        a.loc[j,'Score'] = simScore<br/>    return a</span></pre><h2 id="72cc" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">æµ‹è¯•åŠŸèƒ½</h2><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="1af0" class="lx kc hi lt b fi ly lz l ma mb">cosine_similarity_T(10,â€™computer scienceâ€™)</span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es mx"><img src="../Images/710a08fd464236eb2387baadbad27831.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*iSxlgzrxMz9Epnp4WkhxBQ.png"/></div><figcaption class="iy iz et er es ja jb bd b be z dx translated"><strong class="bd kd">å…³äºâ€œè®¡ç®—æœºç§‘å­¦â€å•è¯çš„å‰5ä¸ªç›¸ä¼¼æ€§æ–‡æ¡£çš„ç»“æœ</strong></figcaption></figure><h1 id="f133" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">2.å¸¦æœ‰è°·æ­Œé€šç”¨å¥å­ç¼–ç å™¨çš„æ–‡æ¡£æœç´¢å¼•æ“</h1><h2 id="65fc" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">è°·æ­Œä½¿ç”¨ç®€ä»‹</h2><p id="5be7" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">é¢„å…ˆè®­ç»ƒçš„<a class="ae jc" href="https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html" rel="noopener ugc nofollow" target="_blank">é€šç”¨è¯­å¥ç¼–ç å™¨</a>åœ¨<a class="ae jc" href="https://www.tensorflow.org/hub/" rel="noopener ugc nofollow" target="_blank"> Tensorflow-hub </a>ä¸­å…¬å¼€ã€‚å®ƒæœ‰ä¸¤ç§å˜åŒ–ï¼Œå³ä¸€ç§ç”¨<a class="ae jc" href="https://tfhub.dev/google/universal-sentence-encoder-large/5" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hj">å˜å‹å™¨ç¼–ç å™¨</strong> </a>è®­ç»ƒï¼Œå¦ä¸€ç§ç”¨<a class="ae jc" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hj">æ·±åº¦å¹³å‡ç½‘ç»œ(DAN) </strong> </a>è®­ç»ƒã€‚å®ƒä»¬æ˜¯åœ¨å¤§å‹è¯­æ–™åº“ä¸Šé¢„å…ˆè®­ç»ƒçš„ï¼Œå¯ä»¥ç”¨äºå„ç§ä»»åŠ¡(æƒ…æ„Ÿåˆ†æã€åˆ†ç±»ç­‰)ã€‚è¿™ä¸¤è€…åœ¨å‡†ç¡®æ€§å’Œè®¡ç®—èµ„æºéœ€æ±‚ä¹‹é—´æœ‰ä¸€ä¸ªæŠ˜è¡·ã€‚è™½ç„¶å…·æœ‰å˜æ¢å™¨ç¼–ç å™¨çš„æ–¹æ³•å…·æœ‰æ›´é«˜çš„ç²¾åº¦ï¼Œä½†æ˜¯å®ƒåœ¨è®¡ç®—ä¸Šæ›´æ˜‚è´µã€‚ä½¿ç”¨DNAç¼–ç çš„æ–¹æ³•åœ¨è®¡ç®—ä¸ŠèŠ±è´¹è¾ƒå°‘ï¼Œå¹¶ä¸”å‡†ç¡®æ€§ä¹Ÿç¨ä½ã€‚</p><p id="f625" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ç¬¬äºŒä¸ªä¸¹é€šç”¨å¥å­ç¼–ç å™¨ï¼Œå¯åœ¨è¿™ä¸ªç½‘å€:- <a class="ae jc" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank">è°·æ­Œä½¿ç”¨ä¸¹æ¨¡å‹</a></p><p id="c930" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">ä¸¤ä¸ªæ¨¡å‹éƒ½ä»¥ä¸€ä¸ªå•è¯ã€å¥å­æˆ–æ®µè½ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ª<strong class="jf hj"> 512 </strong>ç»´åº¦å‘é‡ã€‚</p><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es my"><img src="../Images/05ec3cbaab87c1fbf74e57a4ce29143b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/0*pGl0O-Z_Way5sT_U"/></div><figcaption class="iy iz et er es ja jb bd b be z dx translated">ä¸€ä¸ªåŸå‹è¯­ä¹‰æ£€ç´¢ç®¡é“ï¼Œç”¨äºæ–‡æœ¬ç›¸ä¼¼æ€§ã€‚</figcaption></figure><p id="7e0b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">åœ¨ä½¿ç”¨å¼ é‡æµæ¢çº½æ¨¡å‹ä¹‹å‰ã€‚</p><p id="8afb" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">å…ˆå†³æ¡ä»¶:</strong></p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="aa83" class="lx kc hi lt b fi ly lz l ma mb">!pip install --upgrade tensorflow-gpu<br/> #Install TF-Hub.<br/>!pip install tensorflow-hub<br/>!pip install seaborn</span></pre><p id="c582" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">ç°åœ¨å¯¼å…¥åŒ…:</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="5d1c" class="lx kc hi lt b fi ly lz l ma mb">import pandas as pd<br/>import numpy as np<br/>import re, string<br/>import os <br/>import tensorflow as tf<br/>import tensorflow_hub as hub<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from sklearn.metrics.pairwise import linear_kernel</span></pre><p id="b3dd" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">ä»è°ƒç”¨ç›´æ¥URLçš„<a class="ae jc" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank"> TensorFlow-hub </a>ä¸‹è½½æ¨¡å‹:</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="2851" class="lx kc hi lt b fi ly lz l ma mb">! curl -L -o 4.tar.gz "<a class="ae jc" href="https://tfhub.dev/google/universal-sentence-encoder/4?tf-hub-format=compressed" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/universal-sentence-encoder/4?tf-hub-format=compressed</a>"</span><span id="421a" class="lx kc hi lt b fi mw lz l ma mb">or<br/>module_url = "<a class="ae jc" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/universal-sentence-encoder/4</a>"</span></pre><p id="92f3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">åŠ è½½è°·æ­Œä¸¹é€šç”¨å¥å­ç¼–ç å™¨</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="20a0" class="lx kc hi lt b fi ly lz l ma mb">#Model load through local path:</span><span id="5fb6" class="lx kc hi lt b fi mw lz l ma mb">module_path ="/home/zettadevs/GoogleUSEModel/USE_4"<br/>%time model = hub.load(module_path)</span><span id="7ce3" class="lx kc hi lt b fi mw lz l ma mb">#Create function for using model training<br/>def embed(input):<br/>    return model(input)</span></pre><h2 id="35c6" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">ç”¨ä¾‹1:-å•è¯è¯­ä¹‰</h2><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="eed7" class="lx kc hi lt b fi ly lz l ma mb">WordMessage =[â€˜big dataâ€™, â€˜millions of dataâ€™, â€˜millions of recordsâ€™,â€™cloud computingâ€™,â€™awsâ€™,â€™azureâ€™,â€™saasâ€™,â€™bankâ€™,â€™accountâ€™]</span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es mz"><img src="../Images/7acadeae27507f5c1ac810032013f41c.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*DfKpd8bPS1PA-UugtEDCFA.png"/></div></figure><h2 id="61e6" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">ç”¨ä¾‹2:å¥å­è¯­ä¹‰</h2><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="fd99" class="lx kc hi lt b fi ly lz l ma mb">SentMessage =['How old are you?','what is your age?','how are you?','how you doing?']</span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es na"><img src="../Images/8f7eff0ac3646c46794abaec7fb68b8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*-eUVfSMSsv8rmNoqPTmQ2w.png"/></div></figure><h2 id="dc53" class="lx kc hi bd kd mc md me kh mf mg mh kl jo mi mj kp js mk ml kt jw mm mn kx mo bi translated">ç”¨ä¾‹3:å•è¯ã€å¥å­å’Œæ®µè½è¯­ä¹‰</h2><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="9f32" class="lx kc hi lt b fi ly lz l ma mb">word ='Cloud computing'</span><span id="02f2" class="lx kc hi lt b fi mw lz l ma mb">Sentence = 'what is cloud computing'</span><span id="43cb" class="lx kc hi lt b fi mw lz l ma mb">Para =("Cloud computing is the latest generation technology with a high IT infrastructure that provides us a means by which we can use and utilize the applications as utilities via the internet."<br/>        "Cloud computing makes IT infrastructure along with their services available 'on-need' basis." <br/>        "The cloud technology includes - a development platform, hard disk, computing power, software application, and database.")</span><span id="bab0" class="lx kc hi lt b fi mw lz l ma mb">Para5 =(<br/>    "Universal Sentence Encoder embeddings also support short paragraphs. "<br/>    "There is no hard limit on how long the paragraph is. Roughly, the longer "<br/>    "the more 'diluted' the embedding will be.")</span><span id="e369" class="lx kc hi lt b fi mw lz l ma mb">Para6 =("Azure is a cloud computing platform which was launched by Microsoft in February 2010."<br/>       "It is an open and flexible cloud platform which helps in development, data storage, service hosting, and service management."<br/>       "The Azure tool hosts web applications over the internet with the help of Microsoft data centers.")<br/>case4Message=[word,Sentence,Para,Para5,Para6]</span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es nb"><img src="../Images/bec7f52e225b3494e8e4d2e82c630410.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*ejcbdMkwG1nUHBMtYUyjTg.png"/></div></figure><h1 id="07fa" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">è®­ç»ƒæ¨¡å‹</h1><p id="c1e2" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥æ‰¹å¤„ç†æ–¹å¼è®­ç»ƒæ•°æ®é›†ï¼Œå› ä¸ºç”Ÿæˆæ•°æ®é›†çš„å›¾å½¢éœ€è¦å¾ˆé•¿çš„æ‰§è¡Œæ—¶é—´ã€‚å› æ­¤ï¼Œæ›´å¥½åœ°è®­ç»ƒæ‰¹é‡æ•°æ®ã€‚</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="4d0a" class="lx kc hi lt b fi ly lz l ma mb">Model_USE= embed(df_news.content[0:2500])</span></pre><p id="ee8a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">ä¿å­˜æ¨¡å‹</strong>ï¼Œä»¥ä¾¿é‡ç”¨æ¨¡å‹ã€‚</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="677f" class="lx kc hi lt b fi ly lz l ma mb">exported = tf.train.Checkpoint(v=tf.Variable(Model_USE))<br/>exported.f = tf.function(<br/>    lambda  x: exported.v * x,<br/>    input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])</span><span id="0bf9" class="lx kc hi lt b fi mw lz l ma mb">tf.saved_model.save(exported,'/home/zettadevs/GoogleUSEModel/TrainModel')</span></pre><p id="9006" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">ä»è·¯å¾„:</strong>åŠ è½½æ¨¡å‹</p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="5883" class="lx kc hi lt b fi ly lz l ma mb">imported = tf.saved_model.load(â€˜/home/zettadevs/GoogleUSEModel/TrainModel/â€™)<br/>loadedmodel =imported.v.numpy()</span></pre><p id="b930" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">æ–‡ä»¶æœç´¢åŠŸèƒ½:</strong></p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="a49c" class="lx kc hi lt b fi ly lz l ma mb">def SearchDocument(query):<br/>    q =[query]<br/>    # embed the query for calcluating the similarity<br/>    Q_Train =embed(q)<br/>    <br/>    #imported_m = tf.saved_model.load('/home/zettadevs/GoogleUSEModel/TrainModel')<br/>    #loadedmodel =imported_m.v.numpy()<br/>    # Calculate the Similarity<br/>    linear_similarities = linear_kernel(Q_Train, con_a).flatten() <br/>    #Sort top 10 index with similarity score<br/>    Top_index_doc = linear_similarities.argsort()[:-11:-1]<br/>    # sort by similarity score<br/>    linear_similarities.sort()<br/>    a = pd.DataFrame()<br/>    for i,index in enumerate(Top_index_doc):<br/>        a.loc[i,'index'] = str(index)<br/>        a.loc[i,'File_Name'] = df_news['Subject'][index] ## Read File name with index from File_data DF<br/>    for j,simScore in enumerate(linear_similarities[:-11:-1]):<br/>        a.loc[j,'Score'] = simScore<br/>    return a</span></pre><p id="905c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">æµ‹è¯•æœç´¢:</strong></p><pre class="in io ip iq fd ls lt lu lv aw lw bi"><span id="ef59" class="lx kc hi lt b fi ly lz l ma mb">SearchDocument('computer science')</span></pre><figure class="in io ip iq fd ir er es paragraph-image"><div class="er es nc"><img src="../Images/27504e01cef8cb49cbf404063f2564cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*8Btzc5dq-HlTzCaFLw_jdQ.png"/></div></figure><p id="ad9a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">æ‰¾åˆ°è¯¥é¡¹ç›®çš„<a class="ae jc" href="https://github.com/zayedrais/DocumentSearchEngine" rel="noopener ugc nofollow" target="_blank"><strong class="jf hj"/></a><strong class="jf hj"/></p><h1 id="d9e5" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">ç»“è®º:</h1><p id="2213" class="pw-post-body-paragraph jd je hi jf b jg lb ji jj jk lc jm jn jo lp jq jr js lq ju jv jw lr jy jz ka hb bi translated">åœ¨æœ¬æ•™ç¨‹çš„æœ€åï¼Œæˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œâ€œè°·æ­Œé€šç”¨å¥å­ç¼–ç å™¨â€æ¨¡å‹æä¾›äº†è¯­ä¹‰æœç´¢ç»“æœï¼Œè€ŒTF-IDFæ¨¡å‹ä¸çŸ¥é“å•è¯çš„æ„æ€ã€‚åªæ˜¯æ ¹æ®æ–‡æ¡£ä¸­å¯ç”¨çš„å•è¯ç»™å‡ºç»“æœã€‚</p><p id="ae5a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">ä¸€äº›å‚è€ƒæ–‡çŒ®:</strong></p><ul class=""><li id="d5ec" class="kz la hi jf b jg jh jk jl jo nd js ne jw nf ka lg lh li lj bi translated"><a class="ae jc" href="https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089" rel="noopener" target="_blank"> TF-IDF </a></li><li id="d614" class="kz la hi jf b jg lk jk ll jo lm js ln jw lo ka lg lh li lj bi translated"><a class="ae jc" href="https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html" rel="noopener ugc nofollow" target="_blank">è°·æ­Œä½¿ç”¨</a></li></ul><h1 id="bcb0" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">åˆ«å¿˜äº†ç»™æˆ‘ä»¬ä½ çš„ğŸ‘ï¼</h1></div></div>    
</body>
</html>