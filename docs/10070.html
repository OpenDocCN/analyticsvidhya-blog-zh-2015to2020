<html>
<head>
<title>Reinforcement Learning — Beginner’s Approach Chapter -II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习——初学者方法第二章</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-beginners-approach-chapter-ii-a72e415c57ca?source=collection_archive---------15-----------------------#2020-10-03">https://medium.com/analytics-vidhya/reinforcement-learning-beginners-approach-chapter-ii-a72e415c57ca?source=collection_archive---------15-----------------------#2020-10-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a70fd707c051fea155e8d92c864fe939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mgnPWGA9VIQI6uyZ"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">朱丽安·利贝曼在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="8915" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">你好，</em></p><p id="d8d7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在最后一章中，我们讨论了强化学习及其应用。快速回顾- </p><p id="187a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">强化学习(Reinforcement learning)是机器学习方法的一个子集，其中代理在下一个时间步接收延迟奖励，以评估其之前的行为。常用于Atari和Mario之类的游戏。在最近的研究中，强化学习嵌入了神经网络来解决复杂的任务。</em></p><p id="59f1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">简而言之，强化学习就是将情境映射到行动。任何强化学习算法的主要目标都是最大化数字奖励信号。学习者寻找哪种行为会产生最大的回报，而不是采取哪种行为。</em></p><blockquote class="ju"><p id="6931" class="jv jw hi bd jx jy jz ka kb kc kd js dx translated">如果你不熟悉强化学习的基础知识，我强烈建议读者跳过<a class="ae iu" rel="noopener" href="/analytics-vidhya/reinforcement-learning-beginners-approach-chapter-i-689f999cf572?source=your_stories_page---------------------------"> <strong class="ak">强化学习—初学者方法第一章</strong> </a>。</p></blockquote><figure class="ke kf kg kh ki ij"><div class="bz dy l di"><div class="kj kk l"/></div></figure><p id="cf9d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">在这一章中，我们的主要焦点将是用无模型和基于模型的学习来打破强化学习算法。</em></p><blockquote class="kl km kn"><p id="f6d0" class="iv iw jt ix b iy iz ja jb jc jd je jf ko jh ji jj kp jl jm jn kq jp jq jr js hb bi translated"><strong class="ix hj">路线图- </strong></p></blockquote><ul class=""><li id="579d" class="kr ks hi ix b iy iz jc jd jg kt jk ku jo kv js kw kx ky kz bi translated"><strong class="ix hj"> <em class="jt">与其他ML算法的比较</em> </strong></li><li id="2ff2" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><strong class="ix hj"> <em class="jt">多臂土匪-框架简介</em> </strong></li><li id="63fa" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><strong class="ix hj"> <em class="jt">马氏决策过程</em> </strong></li><li id="2281" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><strong class="ix hj"> <em class="jt">策略优化或策略迭代方法</em> </strong></li><li id="7fce" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><strong class="ix hj"> <em class="jt">异步优势优评(A3C) </em> </strong></li><li id="c34f" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><strong class="ix hj"> <em class="jt">参考文献</em> </strong></li></ul><blockquote class="kl km kn"><p id="eb4f" class="iv iw jt ix b iy iz ja jb jc jd je jf ko jh ji jj kp jl jm jn kq jp jq jr js hb bi translated"><strong class="ix hj">与其他机器学习算法的比较</strong></p></blockquote><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/09ed9c3de49152861c603adcedc713be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*5WShezH1tkw49TQXTipI8A.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Ftowardsdatascience.com%2Freinforcement-learning-101-e24b50e1d292&amp;psig=AOvVaw27IH7CMuGvgww7ifA4lVQb&amp;ust=1592214345626000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCLD2u-uCgeoCFQAAAAAdAAAAABAJ" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="00fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">强化学习是机器学习技术的一个子集，它迫使代理在一个环境中使用来自行动者的动作和经验的反馈来学习。</p><p id="d53b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">有监督学习- </strong></p><p id="828e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">强化学习和监督学习使用映射进行输入和输出，在监督学习的情况下需要正确的<em class="jt">动作来执行任务</em> <strong class="ix hj"> <em class="jt">，但是在强化学习中使用奖励和惩罚进行正反馈和负反馈。</em> </strong></p><p id="ed0e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，在监督学习中，我们有一个完全了解环境的监督者，他将与代理共享环境知识以完成任务。但是，当我们有多个子任务组合，代理可以执行这些子任务来实现目标时，问题就变得复杂了。</p><p id="7996" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">无监督学习- </strong></p><p id="ee11" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然监督学习和强化学习都使用输入和输出之间的映射，但与监督学习不同，监督学习中提供给代理的反馈是执行任务的正确动作集，强化学习使用奖励和惩罚作为积极和消极行为的信号。<strong class="ix hj"> <em class="jt">无监督学习</em> <em class="jt">分析是根据未标记的数据进行的，我们发现数据点之间的相互联系，并通过相似性或差异来构建它们。然而，强化学习引入了获得长期回报的最佳行动模式。</em>T13】</strong></p><p id="d2fd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">用深度学习- </strong></p><p id="dc93" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">RL与深度学习算法紧密结合，因为大多数RL使用深度学习模型。智能体训练的核心方法是神经网络。虽然神经网络最适合识别图像、声音和文本中的复杂模式。</p><figure class="lg lh li lj fd ij"><div class="bz dy l di"><div class="lk kk l"/></div></figure><blockquote class="kl km kn"><p id="4411" class="iv iw jt ix b iy iz ja jb jc jd je jf ko jh ji jj kp jl jm jn kq jp jq jr js hb bi translated"><strong class="ix hj">多臂土匪-框架介绍</strong></p></blockquote><p id="5526" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据维基百科-</p><blockquote class="ju"><p id="a38d" class="jv jw hi bd jx jy jz ka kb kc kd js dx translated">“在<a class="ae iu" href="https://en.wikipedia.org/wiki/Probability_theory" rel="noopener ugc nofollow" target="_blank">概率论</a>中，<strong class="ak">多武装土匪问题</strong>(有时称为<strong class="ak"> <em class="ll"> K </em>或<em class="ll">N</em>-武装土匪问题</strong>)是一个问题，其中必须在竞争(备选)选择之间以最大化其期望收益的方式分配一组固定的有限资源，当每个选择的属性在分配时只是部分已知，并且可以通过向选择分配资源而变得更好理解久而久之。”</p></blockquote><p id="d9c6" class="pw-post-body-paragraph iv iw hi ix b iy lm ja jb jc ln je jf jg lo ji jj jk lp jm jn jo lq jq jr js hb bi translated">多臂强盗是一种强化学习问题陈述，其中我们有由n个臂组成的吃角子老虎机，或者具有臂的强盗有其自己的成功概率分布。如果我们拉任何人的手臂，那么结果将是失败的随机奖励R =0，成功的随机奖励R =+1。这里的主要目的是以这样的顺序拉臂，使我们的总回报最大化。 </p><figure class="lg lh li lj fd ij"><div class="bz dy l di"><div class="lr kk l"/></div></figure><p id="3a8e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">解决多臂强盗问题的某些方法-</p><ul class=""><li id="de9b" class="kr ks hi ix b iy iz jc jd jg kt jk ku jo kv js kw kx ky kz bi translated"><em class="jt">没有探索:最幼稚的方法，也是最糟糕的方法。</em></li><li id="322d" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><em class="jt">随意探索</em></li><li id="2829" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><em class="jt">聪明地探索，偏好不确定性</em></li></ul><p id="e426" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">置信上限是著名的多臂土匪问题解决方案之一。该算法将乐观原则与不确定性联系在一起。概括地说，如果我们对武器的选择没有把握，那就让我们把精力更多地放在它的探索上。</em> </strong></p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/5167eb33e88427000472053cc8350566.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UvwrTGfG242eLeR1mAwRMw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.geeksforgeeks.org/upper-confidence-bound-algorithm-in-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="74f6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">例如，我们有这四种行为，它们在某种程度上与不确定性有关。代理似乎不知道它的行动，所以用UCB算法，它会选择有机会更高上限的行动，这样他就可以获得更高的行动奖励，而且他可以了解这些行动。</p><p id="88cf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">置信上限具有减少不确定性的能力，但是随着时间的推移会有探索的滞后。UCB平均比其他算法如ε-贪婪算法、乐观初始值算法等获得更大的回报。</p><blockquote class="kl km kn"><p id="a618" class="iv iw jt ix b iy iz ja jb jc jd je jf ko jh ji jj kp jl jm jn kq jp jq jr js hb bi translated"><strong class="ix hj">马尔可夫决策过程</strong></p></blockquote><p id="b34d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据维基百科-</p><blockquote class="ju"><p id="5572" class="jv jw hi bd jx jy jz ka kb kc kd js dx translated">“在数学中，马尔可夫决策过程是离散时间随机控制过程。它提供了一个数学框架，用于在结果部分随机、部分受决策者控制的情况下模拟决策”</p></blockquote><figure class="ke kf kg kh ki ij"><div class="bz dy l di"><div class="ls kk l"/></div></figure><p id="33a6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">简单来说，<strong class="ix hj"> <em class="jt">马尔科夫的决策过程当前状态已经被观察，以便由一个智能体</em> </strong>来决定最佳行动。在了解MDP之前，让我们先了解几个术语</p><ul class=""><li id="0df0" class="kr ks hi ix b iy iz jc jd jg kt jk ku jo kv js kw kx ky kz bi translated"><strong class="ix hj"> <em class="jt">马氏性</em> </strong> <em class="jt">由下面的数学等式所陈述——</em></li></ul><p id="3b75" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">状态</em><strong class="ix hj"><em class="jt">S</em></strong><em class="jt">t具有</em> <strong class="ix hj"> <em class="jt"> </em> </strong> <em class="jt">马氏性，当且仅当；</em></p><blockquote class="ju"><p id="886f" class="jv jw hi bd jx jy jz ka kb kc kd js dx translated"><strong class="ak"><em class="ll">P</em></strong><strong class="ak">S</strong><em class="ll">t+1</em><em class="ll">|</em><strong class="ak">S</strong><em class="ll"/><em class="ll">=</em><strong class="ak"><em class="ll">P</em></strong><strong class="ak">S</strong><em class="ll">t+1</em><strong class="ak"><em class="ll">|</em></strong>..，<strong class="ak"> <em class="ll"> S </em> </strong> <em class="ll"> t </em>，</p></blockquote><p id="6753" class="pw-post-body-paragraph iv iw hi ix b iy lm ja jb jc ln je jf jg lo ji jj jk lp jm jn jo lq jq jr js hb bi translated">马尔可夫性质的结论是，如果当前状态是已知的，并且我们没有关于该状态的任何历史信息，那么该状态足以提供未来的相同特征，就好像我们有所有的历史一样。</p><ul class=""><li id="468f" class="kr ks hi ix b iy iz jc jd jg kt jk ku jo kv js kw kx ky kz bi translated"><strong class="ix hj">马尔可夫过程</strong></li></ul><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/8d562eed403568ca43f6b4eb96644439.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*-SwnWvR-VhZRhX-a9ruF6Q.png"/></div></figure><ul class=""><li id="bfb4" class="kr ks hi ix b iy iz jc jd jg kt jk ku jo kv js kw kx ky kz bi translated">马尔可夫链或过程基本上是一个元组绑定状态S和转移函数P (S，P)。整个系统可以由这两个分量S和P来定义，然而我们可以称之为具有马尔可夫性质的随机状态序列<strong class="ix hj"> <em class="jt">。</em>T3】</strong></li><li id="70d4" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated">举个<strong class="ix hj">的例子</strong>，如果你为一个婴儿的行为建立一个<strong class="ix hj">马尔可夫链模型</strong>，你可能会包括“玩”、“吃”、“睡觉”和“哭”等状态，这些状态和其他行为一起可以形成一个“状态空间”:一个所有可能状态的列表。此外，在状态空间的顶部，马尔可夫链告诉你从一个状态跳跃或“过渡”到任何其他状态的概率，例如，一个正在玩耍的婴儿在接下来的五分钟内不哭着入睡的可能性。</li><li id="2393" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated">使用转移矩阵来匹配转移概率的马尔可夫链模型。<strong class="ix hj"> <em class="jt">矩阵由具有状态空间行和列组成，且单元具有它们各自从一种状态转移到另一种状态的概率。</em>T15】</strong></li></ul><p id="ca4d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，MDP是一个预测结果的模型。只有当信息由当前状态提供时，它才试图预测结果。决策者必须选择当前状态下可用的动作，导致模型进入下一步并向决策者提供奖励。</p><p id="7965" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在每个时间戳处，代理的工作是执行可以改变环境状态的动作，并且代理从环境接收奖励或惩罚。<strong class="ix hj"> <em class="jt">智能体的目标是发现最优策略，即在每种状态下需要采取的一组可能的行动，以最大化从环境中获得的回报的总价值。</em> </strong>此外，MDP还用来把代理和环境配置成一种正式的方式。</p><p id="d8ce" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当受控系统的完美模型可用时，<strong class="ix hj"><em class="jt">MDP问题可以用动态规划(DP)技术来解决。当受控系统模型未知时，可应用时域差分(TD)技术求解MDP </em> </strong>。考虑了用于解决MDP问题的三种算法:分别用于有限和无限状态空间的Q学习(QL)和模糊Q学习(FQL)算法。</p><figure class="lg lh li lj fd ij"><div class="bz dy l di"><div class="lu kk l"/></div></figure><blockquote class="kl km kn"><p id="cd50" class="iv iw jt ix b iy iz ja jb jc jd je jf ko jh ji jj kp jl jm jn kq jp jq jr js hb bi translated"><strong class="ix hj">策略优化或策略迭代方法</strong></p></blockquote><p id="c62e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在跳到策略迭代之前，让我们先了解一下价值迭代的基础</p><p id="dd92" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">值迭代</strong>无非是学习所有状态的值，然后根据梯度行动。贝尔曼更新用于描述从价值迭代中获得的所有价值。在一些非限制性条件下，贝尔曼更新保证收敛到最优值。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/a21e8398853835bdff31eaf9e69f171d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/0*CDUmntmuf0dgkGy3.png"/></div></figure><p id="838c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">一直在想跳过这一章后面的数学！！！</em></p><p id="723c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">据观察，学习一个值可能需要无限长的时间。所以学习政策应该是最好的方法，而不是学习价值。</p><p id="f3a6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">策略迭代</strong>基本上是增量查看当前值并提取策略。由于作用空间有限，它比值迭代收敛得快。从概念上讲，对动作的任何改变都将在小的滚动平均更新结束之前发生。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/d9a34b0348d6054632641a5309bdaf92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-d8VCCAaOmTay3Ym18HpAg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://towardsdatascience.com/policy-iteration-in-rl-an-illustration-6d58bdcb87a7" rel="noopener" target="_blank">来源</a></figcaption></figure><p id="040f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，有两种类型的策略迭代技术</p><ul class=""><li id="48c8" class="kr ks hi ix b iy iz jc jd jg kt jk ku jo kv js kw kx ky kz bi translated"><strong class="ix hj"> <em class="jt">策略提取</em> </strong> <em class="jt">，这就是你如何从一个值到一个策略——策略使</em> <strong class="ix hj"> <em class="jt">最大化</em> </strong> <em class="jt">高于期望值</em></li><li id="8b5a" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><strong class="ix hj"> <em class="jt">政策评估</em> </strong> <em class="jt">。评估基本上是通过采用一个策略并根据策略运行值迭代来完成的。样本永远与策略联系在一起，但是我们知道我们必须运行迭代算法来减少提取相关</em> <strong class="ix hj"> <em class="jt">动作</em> </strong> <em class="jt">信息的步骤。</em></li></ul><p id="5ff6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt"> Q值迭代</em> </strong></p><p id="9038" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最佳值可能很难从中提炼出政策。Q值迭代以某种方式在Q因子上执行值迭代。q因子只不过是简单的状态-动作值函数。让我们举一个直观的例子，S和A是状态和动作空间。因此，应用状态-动作值函数，我们获得所有的状态-动作<em class="jt"> (s，a) </em>对，对于<em class="jt"> S </em>中的所有<em class="jt"> s </em>和<em class="jt">A<em class="jt"> a </em>，</em>中的所有<em class="jt">A</em>，并且在这个扩展空间中建立一个具有对之间的转换的新的MDP。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/cea60f3cca56a05c9ddc04ed142de107.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YHhX613HN6ZTUF7lWgpOlA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.quora.com/What-are-the-advantages-of-using-Q-value-iteration-versus-value-iteration-in-reinforcement-learning" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="93f0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">大多数指令以值迭代开始的原因是，它更自然地嵌入到贝尔曼更新中。<strong class="ix hj"> Q值迭代需要将两个关键MDP值关系替换在一起</strong>。这样做之后，就离Q-learning又近了一步，我们会了解的</p><p id="faa0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">质量学习或Q学习</em> </strong></p><p id="fd25" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们直截了当地介绍Q学习/质量学习——它是一种脱离策略的RL算法，帮助我们找到给定当前状态的最佳行动。这就是所谓的非策略算法——因为它从当前策略之外的动作中学习，比如采取随机动作，因此不需要策略。此外，Q学习还帮助我们找到最大化总回报的策略。</p><p id="7cce" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Q-learning通常需要准备一个q-table或矩阵，其中包含状态和行动值的维度。矩阵表中的初始值将为零。在矩阵表的某些更新之后，它成为我们的代理基于q值选择最佳动作的参考表。</p><p id="cdec" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"><em class="jt">——学习Q的阶段</em> </strong></p><ul class=""><li id="fa77" class="kr ks hi ix b iy iz jc jd jg kt jk ku jo kv js kw kx ky kz bi translated"><em class="jt">代理从一个状态(s)开始，采取一个动作(a)并收到一个报酬(r)。</em></li><li id="752b" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><em class="jt">代理通过指向具有最大值</em> <strong class="ix hj"> <em class="jt">或</em> </strong> <em class="jt">随机值(ε，ε)的Q表来选择动作。</em></li><li id="92a5" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><em class="jt">更新q值。</em></li></ul><blockquote class="ju"><p id="b7ad" class="jv jw hi bd jx jy ly lz ma mb mc js dx translated">Q[状态，动作]= Q[状态，动作] + lr *(奖励+gamma * NP . max(Q[新状态，:)—Q[状态，动作])</p></blockquote><p id="4b49" class="pw-post-body-paragraph iv iw hi ix b iy lm ja jb jc ln je jf jg lo ji jj jk lp jm jn jo lq jq jr js hb bi translated">关于q-learning还有很多，但我认为这足以让你有一个良好的开端。请参考参考部分了解更多信息。</p><figure class="lg lh li lj fd ij"><div class="bz dy l di"><div class="lk kk l"/></div></figure><blockquote class="kl km kn"><p id="314a" class="iv iw jt ix b iy iz ja jb jc jd je jf ko jh ji jj kp jl jm jn kq jp jq jr js hb bi translated"><strong class="ix hj">异步优势演员-评论家(A3C) </strong></p></blockquote><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/7d935c6c1add1e8a465ddccee8d47b49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*usFiGuLNxzdVQNURuRkXUg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://arxiv.org/pdf/1803.02912v1.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="ll">强化学习走向强人工智能的布兰登观点</em> </a></figcaption></figure><p id="83d1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Google Deep-mind <strong class="ix hj"> <em class="jt">发布的A3C是一个更简单、更健壮的算法，能够获得比其他任何强化学习算法都好得多的分数。此外，A3C的一大亮点是它既有连续的动作空间，也有离散的动作空间。</em> </strong></p><p id="87b7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">A3C是一个非常庞大的增强算法，因此我将会处理它以提供更好的理解。此外，请参考下一节阅读更多资料。</p><p id="a3a9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">先用它的算法名来破一破——<strong class="ix hj">异步优势演员-评论家(A3C) </strong></p><ul class=""><li id="88c7" class="kr ks hi ix b iy iz jc jd jg kt jk ku jo kv js kw kx ky kz bi translated"><strong class="ix hj">异步</strong>这种算法<strong class="ix hj"> <em class="jt">使用多个代理，每个代理都有自己的环境副本和网络参数。代理之间的通信或交互是异步发生的。这类似于<strong class="ix hj"> </strong>中的真实场景<strong class="ix hj"> </strong>，每个人都从其他人的经历中获得知识，从而让整个全球网络变得更好。</em></strong></li><li id="5696" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated">基本上，A3C是价值迭代方法和政策梯度方法的结合。它利用这两种方法的力量来预测价值函数和最优政策函数。然而，学习代理用于更新价值函数，即政策函数(即行动者)的批评家。这里的政策功能无非是行动空间的<strong class="ix hj">概率分布。</strong></li></ul><p id="4c50" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">A3C的优势:</strong></p><ul class=""><li id="b8f9" class="kr ks hi ix b iy iz jc jd jg kt jk ku jo kv js kw kx ky kz bi translated">该算法比强化学习算法更快、更鲁棒。</li><li id="fe64" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><em class="jt">可用于离散和连续动作空间。</em></li><li id="6b24" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><em class="jt">由于其获取知识的架构，A3C比其他强化学习技术足够高效。</em></li></ul><figure class="lg lh li lj fd ij"><div class="bz dy l di"><div class="kj kk l"/></div></figure><blockquote class="ju"><p id="fd61" class="jv jw hi bd jx jy ly lz ma mb mc js dx translated"><strong class="ak"> <em class="ll">”将用深度强化学习算法的直观演练继续这个博客系列！！！！敬请期待"</em> </strong></p></blockquote><p id="50a7" class="pw-post-body-paragraph iv iw hi ix b iy lm ja jb jc ln je jf jg lo ji jj jk lp jm jn jo lq jq jr js hb bi translated"><strong class="ix hj"><em class="jt"/></strong><a class="ae iu" href="https://shashwatwork.github.io/blog/" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj"><em class="jt">在这里做结帐我的其他与ML/DL相关的博客</em> </strong> </a> <strong class="ix hj"> <em class="jt">。”</em>T29】</strong></p><blockquote class="kl km kn"><p id="1b31" class="iv iw jt ix b iy iz ja jb jc jd je jf ko jh ji jj kp jl jm jn kq jp jq jr js hb bi translated"><strong class="ix hj">参考文献</strong></p></blockquote><ul class=""><li id="38c4" class="kr ks hi ix b iy iz jc jd jg kt jk ku jo kv js kw kx ky kz bi translated"><a class="ae iu" href="http://incompleteideas.net/book/bookdraft2017nov5.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">强化学习入门</strong> </a></li><li id="e420" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><a class="ae iu" href="https://pathmind.com/wiki/deep-reinforcement-learning" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> Pathmind </strong> </a></li><li id="0f7c" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><a class="ae iu" href="https://github.com/aikorea/awesome-rl" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">牛逼-rl </strong> </a></li><li id="3342" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><a class="ae iu" href="http://www.argmin.net/2018/06/25/outsider-rl/" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">局外人的RL之旅</strong> </a></li><li id="36bb" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><a class="ae iu" href="https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">多臂强盗问题及其解决方案</strong> </a></li><li id="d763" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><a class="ae iu" href="https://towardsdatascience.com/reinforcement-learning-demystified-markov-decision-processes-part-1-bf00dda41690" rel="noopener" target="_blank"> <strong class="ix hj">马氏决策过程</strong> </a></li><li id="5e95" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><a class="ae iu" rel="noopener" href="/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa"> <strong class="ix hj">策略迭代方法</strong> </a></li><li id="99aa" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><a class="ae iu" href="https://arxiv.org/pdf/1602.01783.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">深度强化学习的异步方法</strong> </a></li><li id="646f" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><a class="ae iu" rel="noopener" href="/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2"> <strong class="ix hj">直观解释为A3C </strong> </a></li><li id="8624" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><a class="ae iu" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html" rel="noopener ugc nofollow" target="_blank">T5】OpenAIT7】</a></li><li id="ece0" class="kr ks hi ix b iy la jc lb jg lc jk ld jo le js kw kx ky kz bi translated"><a class="ae iu" href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ" rel="noopener ugc nofollow" target="_blank"><strong class="ix hj">RL-YouTube教程视频</strong> </a></li></ul><p id="e769" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你喜欢这个帖子，请关注我。如果你注意到思维方式、公式、动画或代码有任何错误，请告诉我。</p><p id="d0ae" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">干杯！</em></p></div></div>    
</body>
</html>