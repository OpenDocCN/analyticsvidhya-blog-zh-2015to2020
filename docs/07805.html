<html>
<head>
<title>Build a Deep Neural Network using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python构建深度神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/create-a-deep-neural-network-from-scratch-15c9b07c1388?source=collection_archive---------13-----------------------#2020-07-08">https://medium.com/analytics-vidhya/create-a-deep-neural-network-from-scratch-15c9b07c1388?source=collection_archive---------13-----------------------#2020-07-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/623c0bf22256e09754a0831c7e6a6036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g-MWJr9Z3mQw0t15fEmgWQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">神经网络的例子</figcaption></figure><p id="c603" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">深度学习和神经网络是数据科学中非常流行的概念，它利用大量数据来训练可以进行非常准确分类的模型。神经网络广泛用于图像和语音识别以及自然语言处理。</p><p id="c045" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我做了一些关于神经网络的基础研究(现在网上有很多资源可以学习任何东西)，结果证明，这个概念相当容易理解。了解一点线性代数和微积分知识就足以理解神经网络的基础。</p><p id="5c16" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">有许多框架可用于建模和训练神经网络，如Tensorflow和Keras。使用这些框架将使实现变得如此容易，但为什么不尝试困难的方式，从头实现一个神经网络。</p><p id="8a4f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">所以让我们用python建立一个模型，来训练和测试一个神经网络，这个神经网络有任意数量的层，由任意数量的隐藏单元(神经元)组成。</p><p id="0d8b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在开始实施之前，首先，让我们看看需要做些什么。我们可以把这个过程分成几个步骤。</p><ol class=""><li id="acb2" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">初始化参数</li><li id="72c6" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">正向传播</li><li id="9d18" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">计算成本</li><li id="fa96" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">反向传播</li><li id="7ed3" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">更新参数</li></ol><p id="a25c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们看看如何实现这些步骤。</p><p id="8826" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">重要的事情先来。我们需要一个数据集。我用来训练和测试模型的数据集是来自sklearn数据集的make_moons。你可以在这里看到数据集<a class="ae kg" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html#sklearn.datasets.make_moons" rel="noopener ugc nofollow" target="_blank">的文档。</a></p><p id="14c8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">您还可以从sklearn 探索许多其他<a class="ae kg" href="https://scikit-learn.org/stable/datasets/" rel="noopener ugc nofollow" target="_blank">数据集。(make_moons数据集位于“生成的数据集”部分)</a></p><p id="489c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">首先，安装sklearn数据集并导入数据集。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="0a1b" class="kq kr hi km b fi ks kt l ku kv">from sklearn.datasets import make_moons</span></pre><p id="0798" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，让我们加载数据集，并为训练和测试进行设置。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="90d6" class="kq kr hi km b fi ks kt l ku kv">X,Y = make_moons(n_samples=2000, shuffle=True, noise=None, random_state=None) </span><span id="0637" class="kq kr hi km b fi kw kt l ku kv">X = X.T<br/>m = X.shape[1]<br/>Y = Y.reshape(1,m)</span></pre><p id="fc2e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这个实现中，</p><p id="9d36" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">x是n*m矩阵，其中n是特征的数量，m是示例的数量。</p><p id="4d96" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">y是一个1 *m向量</p><p id="0404" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">您可以切换X和Y的维度，但是您应该相应地切换权重的维度。</p><p id="f621" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将使用80%的数据集来训练模型，20%用于测试。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="8cf7" class="kq kr hi km b fi ks kt l ku kv">margin = m//10*8 <br/>X_train, X_test = X[:, :margin], X[:, margin:]<br/>Y_train, Y_test = Y[:, :margin], Y[:, margin:]</span></pre><p id="9b39" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在我们已经准备好了数据集，是时候实现模型了。我们将一步一步地实现这个模型。</p><p id="efcf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">初始化参数</strong></p><p id="3bdb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这是我们初始化每一层的权重和偏差的地方。为了初始化参数，我们将一个包含每层中单元(神经元)数量的数组作为输入。例如,[n，4，1]将意味着具有n个输入特征的3个层，以及分别在隐藏层中的4个和1个神经元。</p><p id="6317" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">然后，我们遍历这些层，初始化权重和偏差，并将它们存储在一个字典中({W1: _，b1: _，W2: _，b2: _，……})</p><figure class="kh ki kj kk fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><p id="8014" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这个实现中，</p><p id="8473" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">特定层l (W[l])的权重矩阵的维数是(层l中的神经元数目，层l-1中的神经元数目)</p><p id="3003" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">特定层l (b[l])的偏置向量的维数是(层l中神经元的数量，1)</p><p id="1dec" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这些尺寸取决于X和Y的尺寸。如果您交换X和Y的尺寸，W和b的尺寸也应该交换。</p><p id="146f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">总是用随机值初始化权重来打破对称性。如果所有的权重和偏置被初始化为零，则单层的所有神经元中的激活将是相同的。</p><p id="c2f2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">正向传播</strong></p><p id="8baf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在该步骤中，通过将前一层的激活作为输入来计算每一层的激活。</p><p id="d0a0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">正向传播可以分为两个子步骤:</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/38493490423f40ca1c6b15b1332bd3c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/0*E7zEaJG3Y8aVc0M1.png"/></div></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es la"><img src="../Images/08de0d927c64625b9f9bab4cce62d8e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/0*r-5d0t0Gr4NYpg02.png"/></div></figure><p id="07a0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">虽然我使用sigmoid激活函数来计算所有层的激活，但这不是最好的选择。对于隐藏层，还有其他比sigmoid更好的激活函数，如tanh和ReLU。</p><p id="8bff" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了实现前向传播，我们需要实现sigmoid函数。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/01d9e803d69f3d2d57a73a11883ab236.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/0*yZNTLnxM9YjQrJpH.png"/></div></figure><figure class="kh ki kj kk fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><p id="c4b0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们来计算单层的激活。</p><figure class="kh ki kj kk fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><p id="78c4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">您可能已经注意到，除了激活该层之外，我们还返回了一个缓存，用于存储该层的一些值。当我们到达反向传播部分时，你就会明白为什么了。</p><p id="804d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在我们有了计算单层激活的函数，我们可以用它来进行正向传播。我们需要做的是使用前一层直到最后一层的激活来计算每一层的激活。</p><figure class="kh ki kj kk fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><p id="8aec" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从这个函数，我们返回AL，最后一层的激活(或者y hat)。我们还返回缓存，一个包含我们在每一层收集的所有缓存的数组。稍后您将看到缓存的用途。</p><p id="6cd2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">计算成本</strong></p><p id="8562" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，我们计算成本，看看我们通过正向prop计算的AL与y有多大差异。为此，我们将使用交叉熵成本。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/60929a030c3cd9d8b54649aeea1f8baf.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/0*lrCIMwGFHfrjjR2t.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">交叉熵成本</figcaption></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/1b466efc65fb197e24ab30e27f303c0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/0*3h27HFGn_QQxzqKU.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">交叉熵成本矢量化</figcaption></figure><figure class="kh ki kj kk fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><p id="712f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">您也可以使用均方差来计算成本。</p><p id="05bc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">反向传播</strong></p><p id="3330" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在到了棘手的部分，backprop！我们的最终目标是通过调整参数使成本最小化。为此，我们使用梯度下降。为了使用梯度下降来调整参数，我们需要找到成本相对于每个参数的偏导数。(dW1、db1、dW2、DB2……)</p><p id="e283" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果你懂微积分的基础，backprop也没那么难。通过使用链式法则，我们可以找到成本相对于每个参数的偏导数。</p><p id="c867" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了帮助反向推进，下面是我们用于正向推进的方程式:</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es le"><img src="../Images/30db3db06399675c9c14e6c4c2106c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/0*qCca3dOBVeK-yFjN.png"/></div></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es la"><img src="../Images/0282dbb046965edb66babf36f1f03bc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/0*90-8g9EYDzeMTmJw.png"/></div></figure><p id="7ea0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们用链式法则推导出dWL(成本相对于WL的导数)，其中L是最后一层。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/10b9c52530116d8f0c8feee755b238fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/0*Rgmc5xFM_4H81HtT.png"/></div></figure><p id="6031" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们看看是否能推导出这些项中的每一项。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es le"><img src="../Images/cee5758c9ab16e94506c95396fd125be.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/0*2Ibnkyi-HhF7GvkS.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">成本的导数w.r.t AL(如果您使用成本的均方误差，这一项会有所不同)</figcaption></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/5d6061bcfb6564a5277d30aa626261f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/0*5DITwCnXN9h-J94W.png"/></div></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/7c54b09c0fd53ba77ee474a53e04c44e.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/0*3FPbjtYc5WRMjp8X.png"/></div></figure><p id="cf76" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们把前两个导数的乘积取为dZ[L](成本相对于Z[L]的导数)。我们以后会需要它。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es li"><img src="../Images/3e14ee5ef107de15fbc2da38a897cc46.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/0*padzfz1LRESkcFpN.png"/></div></figure><p id="6a9b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，如果我们有Z[L]和前一层的激活(A[L-1])，我们将能够计算dW[L]。还记得我们在正向推进期间在每层的缓存中存储了一些值吗？如果您不记得我们存储了什么，请向上滚动查看。我们存储了A_prev和Z！因此，如果我们访问最后一层的缓存，我们将能够轻松地计算dW[L]。</p><p id="2153" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们再返回一层，看看我们是否能为每个dW[l]导出一个模式。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lj"><img src="../Images/81c798fade728ca6b7ec857309daa5ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/0*cMeS8h8p2w8icd6d.png"/></div></div></figure><p id="0c48" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">回想一下，</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es li"><img src="../Images/57dbeeab14d951b3f282efb1c5e739d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/0*Yq1EUGe0hxqkIJBN.png"/></div></figure><p id="d937" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">然后我们可以把链条简化到，</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/ec2acde74415fe0ce49350b49b59b7de.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/0*hJN81Y8Kd0RsKxR1.png"/></div></figure><p id="fdf7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们看看能否推导出其余的项。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/047c601fe24d9faed9652e53f141b10e.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/0*N90i0hKSSZH28ODy.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">(从Z[l] = A[l-1] * W[l] + b[l])</figcaption></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/33634c9dfedd7c19b0a57acdfd42e6ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/0*YAsZDo-NEEZlSpc4.png"/></div></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/7c7231d32e2a61f29946c806fa81ca25.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/0*3ZNxFhQEeRK0Yiyh.png"/></div></figure><p id="ad55" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，如果我们有W[L]，Z[L-1]，和A[L-2]，我们可以计算dW[L-1]。如果你回头看看，我们已经在缓存中存储了我们需要的值。</p><p id="999f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果我们对A[L-1]的成本求导，</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/9283f5b6d3e94c611d9697a22094766a.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/0*ZeBEpCuNwnXZRt2Y.png"/></div></figure><p id="3f58" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">然后我们可以推广一个等式来得到成本相对于单层中的权重的导数，</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/6a2363ec91d69eb7324bd12b5dc4771b.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/0*h7DQpNP8FrqApSMF.png"/></div></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/8b6976253c95f01c4ecc57ccd520c30a.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/0*8Vzn6iTJ_wVkHAhu.png"/></div></figure><p id="fdb3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">成本相对于层的偏差的导数可以使用相同的链式法则来表示，</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/83b05dfdba605603c7f90416bf35ac60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/0*PEEmZn3Soiuz6hbT.png"/></div></figure><p id="ace0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这种情况下，最后一项将是1。所以，</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/a2ec3324779f3ae22d853d324634c16c.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/0*0H2E8EYlxLF2rfxK.png"/></div></figure><p id="b3ca" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">唷！既然推导已经完成，让我们来实现backprop。</p><p id="752c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">首先，我们需要一个函数来计算sigmoid导数。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/31ea46e2248fce94a09d0d551601e0f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/0*5hvfVe25LIQa4oDt.png"/></div></figure><figure class="kh ki kj kk fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><p id="a838" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，让我们将推导出的方程应用于一个单层。</p><figure class="kh ki kj kk fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><p id="2b42" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在让我们使用该函数，并将其应用于从最后一层到第一层的所有层。</p><figure class="kh ki kj kk fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><p id="254f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们使用在正向推进期间存储的缓存来计算每一层中的导数，并将它们存储在字典中。({dW1: _，db1: _，dW2: _，db2: _，……})</p><p id="8951" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">更新参数</strong></p><p id="2d59" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">最后，我们可以使用我们计算的导数并更新参数。</p><figure class="kh ki kj kk fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><p id="a02f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">所以我们已经完成了所有的步骤。现在是时候把它们放在一起了。我们初始化参数一次，然后循环执行剩余的步骤，直到成本最小化。(我们可以决定需要多少次迭代)</p><figure class="kh ki kj kk fd ij"><div class="bz dy l di"><div class="kx ky l"/></div></figure><p id="b690" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这就是我们的模型。您还可以将每次迭代的成本存储在一个数组中，并根据迭代次数绘制它，以查看成本的降低。</p><p id="da51" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，我们要做的就是，对我们之前加载的数据集运行这个函数。我们将训练一个具有3个层的模型，每个层有5、3和1个单元，学习率为0.005，迭代10，000次。</p><pre class="kh ki kj kk fd kl km kn ko aw kp bi"><span id="30d8" class="kq kr hi km b fi ks kt l ku kv">layer_units = [X_train.shape[0], 5, 3, 1]<br/>parameters = neural_network(X_train, Y_train, layer_units, 0.005, 10000)</span></pre><p id="6288" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">就是这样！</p><p id="f200" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，您可以实现预测器函数来预测训练集和测试集的值，并计算准确性。您可以使用不同的数据集和激活函数，还可以调整学习率、隐藏层和隐藏单元，看看模型的表现如何。</p><p id="382a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">希望你理解代码，最重要的是，概念。完整的代码可以在<a class="ae kg" href="https://github.com/JayaniH/Neural-Networks/blob/master/NeuralNetwork.py" rel="noopener ugc nofollow" target="_blank">我的GitHub库</a>中查看。</p></div></div>    
</body>
</html>