<html>
<head>
<title>Your First Reinforcement Learning Environment from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">您的第一个强化学习环境从零开始</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-simple-reinforcement-learning-environment-from-scratch-72c37bb44843?source=collection_archive---------15-----------------------#2020-08-03">https://medium.com/analytics-vidhya/a-simple-reinforcement-learning-environment-from-scratch-72c37bb44843?source=collection_archive---------15-----------------------#2020-08-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="6639" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi jk translated">e enforcement Learning(RL)是机器学习的一个强大类别，与监督学习(另一个强大类别)不同，它不需要带标签的数据来训练机器/代理做出智能决策。RL只围绕两个元素:</p><ul class=""><li id="0361" class="jt ju hi io b ip iq it iu ix jv jb jw jf jx jj jy jz ka kb bi translated"><strong class="io hj">环境:</strong>演员/机器(即代理)交互的(模拟)世界。</li><li id="a5b1" class="jt ju hi io b ip kc it kd ix ke jb kf jf kg jj jy jz ka kb bi translated"><strong class="io hj">代理:</strong>演员(如机器人、计算机器等。)由RL算法训练以独立和智能地运行。</li></ul><p id="5349" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">大多数关于强化学习的资源过多地(并且在大多数情况下是单独地)谈论了强化学习代理的训练技术(代理-环境交互)。这些资源很少或根本不关注环境的发展。这也是有意义的，因为开发RL算法是一天结束时的主要目标。诸如动态规划、Q学习、SARSA、蒙特卡罗、深度Q学习等方法。只专注于培训代理(通常称为RL-agent)。</p><p id="5330" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一个<strong class="io hj"> RL-AGENT </strong>在与<strong class="io hj">环境</strong>、<strong class="io hj"> </strong>交互时学会智能行为，并因其<strong class="io hj">动作</strong>而接受<strong class="io hj">奖励</strong>或<strong class="io hj">惩罚</strong>。但是，如果我们没有这样的环境，我们怎么能训练一个特工呢？在大多数实际应用中，程序员可能不得不开发他们自己的培训环境。</p><p id="8c3f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这个博客中，我们将学习如何开发一个简单的RL环境，RL的Hello-World，<em class="kh">网格世界环境</em>！</p><p id="bb92" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">但是首先，让我们学习一些关于环境的基础知识。</p><h1 id="98b9" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated"><strong class="ak">RL环境基础</strong></h1><p id="fb7c" class="pw-post-body-paragraph im in hi io b ip lg ir is it lh iv iw ix li iz ja jb lj jd je jf lk jh ji jj hb bi translated">以下是RL环境的必需品:</p><ul class=""><li id="436d" class="jt ju hi io b ip iq it iu ix jv jb jw jf jx jj jy jz ka kb bi translated"><strong class="io hj">环境的状态/观察集</strong>。</li><li id="5dad" class="jt ju hi io b ip kc it kd ix ke jb kf jf kg jj jy jz ka kb bi translated"><strong class="io hj">环境对主体的奖惩</strong>。</li><li id="1250" class="jt ju hi io b ip kc it kd ix ke jb kf jf kg jj jy jz ka kb bi translated"><strong class="io hj">为代理设置动作，在环境边界内</strong>。</li></ul><p id="ec94" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">简而言之，RL环境包含关于代理可以采取的所有可能的动作的信息，代理在这些动作之后可以实现的状态，以及作为交互的回报，环境给予代理的奖励或惩罚(负奖励)。</p><h1 id="7b65" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated"><strong class="ak">网格世界环境</strong></h1><p id="56d2" class="pw-post-body-paragraph im in hi io b ip lg ir is it lh iv iw ix li iz ja jb lj jd je jf lk jh ji jj hb bi translated">下面是一个网格世界环境的示例，其中一个代理(即一个机器人)从初始位置(0，0)开始，试图到达目标位置(3，3)，一次一步。</p><figure class="lm ln lo lp fd lq er es paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="er es ll"><img src="../Images/d8556042aaa799636d8598fe8a8d3983.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8-ze5WWyZirVRQlHTiEV2Q.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">代理人与环境的相互作用</figcaption></figure><p id="07ae" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在上面的4x4网格世界中，机器人可以在每一步从集合{左、上、右、下}中随机选择任何动作。当代理在采取行动后从一个状态移动到另一个状态时，它得到-1的奖励。这种负奖励(即惩罚)迫使代理以最少的步数完成旅程并到达终点状态。</p><p id="590e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果代理想要跑出网格，它会得到-2的惩罚。例如，如果代理从任何状态(0，0)，(0，1)，(0，2)，<br/> (0，3)采取左动作，它会收到-2的惩罚。</p><p id="6830" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当代理从状态(2，3)，(3，2)中的任何一个移动到<strong class="io hj">终端状态</strong> (3，3)时，它得到奖励0。</p><p id="334b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">实施</strong></p><p id="3e3d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将把网格世界环境作为Python类来实现。下面给出了环境的框架。GridWorld_Env类中有一个构造函数和四个方法。稍后我们将展示如何填充这些方法，并为额外的原因添加一个额外的方法。大部分代码都是不言自明的，因此我们不会添加太多的细节。但是，如果需要额外的解释或者环境可以进一步改善，请在评论区告诉我。</p><pre class="lm ln lo lp fd mb mc md me aw mf bi"><span id="5f09" class="mg kj hi mc b fi mh mi l mj mk">class GridWorld_Env:<br/>    def __init__(self):<br/>        pass<br/>    # reset the agent when an episode begins    <br/>    def reset(self):<br/>        pass</span><span id="5dfc" class="mg kj hi mc b fi ml mi l mj mk">    # Agent takes the step, i.e. take action to interact with <br/>      the environment<br/>    def step(self, action):<br/>        pass<br/>    <br/>    # Action reward given to the agent<br/>    def get_reward(self):<br/>        pass<br/>    <br/>    # Actual action that agent takes.<br/>    def take_action(self):<br/>        pass</span></pre><p id="9e45" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">就是这样！</p><p id="8206" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这就是创造环境所需要的一切！</p><p id="4e4f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，让我们填充环境类。记住动作集{左，上，右，下}，奖励0，-1，-2，状态(x，y) :(0，0) → (3，3)</p><pre class="lm ln lo lp fd mb mc md me aw mf bi"><span id="3417" class="mg kj hi mc b fi mh mi l mj mk">class GridWorld_Env:</span><span id="b0bb" class="mg kj hi mc b fi ml mi l mj mk">''' Important paramenters<br/>--&gt; self.x, self.y : x,y coordinate for the grid agent. <br/>--&gt; self.done : Flag variable which turns True when (i) either <br/>    episode ends, or (ii) maximum number of steps (200) are acheived<br/>    in the episode.<br/>--&gt; self.MAX_HOR_VAL, self.MAX_VER_VAL : Maximuma Horizontal and <br/>    Vertical grid size of the environment. <br/>'''</span><span id="de4a" class="mg kj hi mc b fi ml mi l mj mk">    # Constructor for GridWorld_Env Object, i.e. our agent<br/>    def __init__(self, hor, ver):<br/>        self.actions = ["left", "up", "right", "down"] <br/>        self.x = 0<br/>        self.y = 0<br/>        self.MAX_HOR_VAL = hor-1<br/>        self.MAX_VER_VAL = ver-1<br/>        self.done = False<br/>        self.episode_length = 0<br/>        self.state_observation = [self.x, self.y]</span><span id="65a2" class="mg kj hi mc b fi ml mi l mj mk">    # Reset the agent at the start of each episode<br/>    def reset(self):<br/>        self.done = False<br/>        self.episode_length = 0<br/>        self.x, self.y = 0, 0<br/>        self.state_observation = [self.x, self.y]<br/>        return [self.x, self.y]<br/>    <br/>    # Returns the number of actions in the action set<br/>    def action_space(self):<br/>        return self.actions</span><span id="ca3a" class="mg kj hi mc b fi ml mi l mj mk">    # Agent takes the step, i.e. take action to interact with <br/>      the environment<br/>    def step(self, action):</span><span id="a40b" class="mg kj hi mc b fi ml mi l mj mk">    # If agent is at terminal state, end the episode, set <br/>      self.done to be True<br/>        if self.state_observation == [self.MAX_HOR_VAL, <br/>           self.MAX_VER_VAL]:<br/>            self.done = True<br/>            return np.array(self.state_observation), self.reward,      <br/>                   self.done, self.episode_length<br/>        <br/>        elif self.episode_length &gt; 200:<br/>            self.done = True<br/>            return np.array(self.state_observation), self.reward, <br/>                   self.done,self.episode_length<br/>        <br/>        self.action = action<br/>        self.reward = self.get_reward()<br/>        self.state_observation = self.take_action()<br/>        self.episode_length += 1<br/>        <br/>        if(self.episode_length &gt;= 200):<br/>            self.done = True<br/>        <br/>        return np.array(self.state_observation), self.reward, <br/>               self.done, self.episode_length<br/>    <br/>    def get_reward(self):<br/>    # If agent tries to run out of the grid, penalize -2<br/>        if (self.x == 0 and self.action == "left") or <br/>           (self.x == self.MAX_HOR_VAL and self.action == "right" ):<br/>            return -2<br/>        elif (self.y == 0 and self.action == "down") or <br/>             (self.y == self.MAX_VER_VAL and self.action == "up" ):<br/>            return -2<br/>    # If agent reached Terminal state, reward = 0 <br/>        elif (self.x, self.y) == (self.MAX_HOR_VAL-1, <br/>             self.MAX_VER_VAL) and self.action == "right":<br/>            return 0<br/>        elif (self.x, self.y) == (self.MAX_HOR_VAL, <br/>             self.MAX_VER_VAL-1) and self.action == "up":<br/>            return 0<br/>    # For all other states, penalize agent with -1<br/>        else:<br/>            return -1<br/>    <br/>    # Method to take action, remain in the same box if agent tries<br/>       to run outside the grid, otherwise move one box in the <br/>       direction of the action<br/>     def take_action(self):  <br/>        if self.x &gt; -1 and self.x &lt;= self.MAX_HOR_VAL:<br/>            if (self.action == "left" and self.x == 0) or <br/>               (self.action == "right" and <br/>                self.x == self.MAX_HOR_VAL):<br/>                self.x = self.x<br/>            elif(self.action == "left"):<br/>                self.x -= 1<br/>            elif(self.action == "right"):<br/>                self.x += 1<br/>            else:<br/>                self.x = self.x<br/>                <br/>        if self.y &gt; -1 and self.y &lt;= self.MAX_VER_VAL:<br/>            if (self.action == "down" and self.y == 0) or <br/>               (self.action == "up" and self.y == self.MAX_HOR_VAL):<br/>                self.y = self.y<br/>            elif(self.action == "down"):<br/>                self.y -= 1<br/>            elif(self.action == "up"):<br/>                self.y += 1<br/>            else:<br/>                self.y = self.y<br/>                        <br/>        return [self.x, self.y]</span></pre><p id="8ba4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这种环境可以用任何RL算法来测试。我们用Q-Learning进行了测试。完整的代码(带Q学习测试)可以在<a class="ae mm" href="https://github.com/SRJaffry/GridWorld_Environment" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="ca5f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">然而，在下文中，我们仅仅展示了如何将代理作为GridWorld_Env类的对象来调用</p><pre class="lm ln lo lp fd mb mc md me aw mf bi"><span id="6796" class="mg kj hi mc b fi mh mi l mj mk"># Since we have 3x3 grid, hence, we define following parameters<br/>EPISODES = 100<br/>MAX_HOR_LENGTH = 3<br/>MAX_HOR_LENGTH = 3</span><span id="75e4" class="mg kj hi mc b fi ml mi l mj mk">agent = GridWorld_Env(MAX_HOR_LENGTH, MAX_VER_LENGTH)<br/>...<br/>...</span><span id="962e" class="mg kj hi mc b fi ml mi l mj mk">for ep in EPISODES:<br/>    # Reset the state of agent in the beginning of each episode<br/>    current_state = agent.reset() <br/>    done = False<br/>    ...<br/>    ...<br/>    while not done:<br/>      ...<br/>      ...<br/>      # Select action using, for example Q-Learning and take step.<br/>      next_state, reward, done, _ = agent.step(action_space[action])<br/>      ...</span></pre><p id="201c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">祝你用RL算法训练代理人好运！如果可以做进一步的改进，请在评论中告诉我。</p><p id="a8b9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">干杯</p></div></div>    
</body>
</html>