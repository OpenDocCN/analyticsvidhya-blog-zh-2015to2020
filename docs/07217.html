<html>
<head>
<title>Model-Based Offline Reinforcement Learning (MOReL)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于模型的离线强化学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/model-based-offline-reinforcement-learning-morel-f5cd991d9fd5?source=collection_archive---------11-----------------------#2020-06-17">https://medium.com/analytics-vidhya/model-based-offline-reinforcement-learning-morel-f5cd991d9fd5?source=collection_archive---------11-----------------------#2020-06-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="b69e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将简要介绍由<strong class="ih hj">拉胡尔·基丹比</strong> &amp; <strong class="ih hj">阿拉温德·拉杰斯瓦兰</strong>等人于2020年发表的论文<a class="ae jd" href="https://arxiv.org/abs/2005.05951" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">莫雷尔:基于模型的离线强化学习</strong> </a></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/3cd5eb86769adf24c7b8642be8aed4e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wr8iqkrOf-f2fLMWGOwh1Q.png"/></div></div></figure><p id="20c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">强化学习通常被认为是一个在线学习过程，在这个过程中，一个智能体反复使用与环境的交互来改进策略，因此数据收集和策略学习同时发生。这通常导致缓慢且简单的低效学习，而这通常只有在模拟器存在的情况下才是可行的。</p><p id="4714" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在离线RL(也称为批处理RL)中，目标是仅使用与环境的历史交互的数据集来学习成功的策略，而没有任何进一步的交互。离线强化学习的前期工作几乎仅限于无模型强化学习。</p><p id="c2eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于新政策的状态-动作访问分布可能与行为政策的显著不同，所以非政策评估可能很困难。这种因政策更新而导致的分布变化通常被称为<strong class="ih hj">分布转变</strong>，构成了离线RL的主要挑战。</p><p id="fe71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原则上，可以对离线数据集使用任何非策略RL算法(如Q-learning或actor-critic)。然而，<a class="ae jd" href="https://arxiv.org/abs/1812.02900" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">BCQ</strong></a>&amp;<a class="ae jd" href="https://arxiv.org/abs/1906.00949" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">熊</strong> </a>的论文提出，由于前面提到的分布偏移，直接使用这种算法产生的结果很差。为了克服这一挑战，他们提出了修改方案，如使用Q-网络系综，将学习策略调整为行为策略。</p><p id="feda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相比之下，<strong class="ih hj">基于模型的RL (MBRL) </strong>提出了一组使用离线数据集的替代方法。MBRL支持在构建模型时使用一般先验知识，以及在学习模型后使用广泛的规划算法的能力，包括模型预测控制(<strong class="ih hj"> MPC </strong>)、蒙特卡洛树搜索(<strong class="ih hj"> MCTS </strong>)、动态规划和政策优化。这些优势使MBRL能够在在线RL环境中高效采样。</p><p id="6aa6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，本文研究的问题是如何在离线学习环境中有效地使用基于模型的学习技术。</p><p id="e6bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MOReL是一个基于模型的离线学习算法框架，包括两个步骤:</p><ol class=""><li id="b628" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated">使用离线数据集构建悲观MDP模型。</li><li id="20b1" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">对上述悲观MDP的规划或政策优化。</li></ol><p id="c4b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作者声称，MOReL易于进行详细的理论分析，能够轻松透明地设计实用算法，并在广泛使用的离线基准任务上产生最先进的结果。</p><h1 id="516b" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">问题定式化</h1><h2 id="ad36" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">马尔可夫决策过程</h2><p id="07e7" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">我们使用元组<strong class="ih hj"> M={S，A，r，P，ρ0，γ} </strong>来定义MDP，其中，<strong class="ih hj"> S </strong>是状态空间，<strong class="ih hj"> A </strong>是动作空间，<strong class="ih hj">r:S×A→【Rmax，Rmax】</strong>是回报函数，<strong class="ih hj"> P :S×A→S </strong>是转移核，<strong class="ih hj"> ρ0 </strong>是初始状态分布，而<strong class="ih hj">策略定义了从状态到动作概率分布的映射，<strong class="ih hj"> π : S×A→R+ </strong>。任意状态<strong class="ih hj"> s </strong>下策略<strong class="ih hj"> π </strong>的值定义为:</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lv"><img src="../Images/e58f7cb3dc0d0fddb4684f1c2de699c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k9vlLDbC3nhepIhlvEt3fQ.png"/></div></div></figure><p id="f4bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">策略和一些开始状态分布的性能<strong class="ih hj"> β </strong>由下式给出:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lw"><img src="../Images/5bd3c72d54edd69d192235cc7bcc468a.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*PMNg2IaNC72ORtOyV48W-A.png"/></div></figure><p id="eb49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常，我们对根据MDP起始状态分布优化性能感兴趣，从而导致优化:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lx"><img src="../Images/91cf7789352c5491359499dc106d2c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v8UK3AXmOGC0O9ctxDxVpw.png"/></div></div></figure><p id="a260" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了避免符号混乱，我们还抑制了从上下文理解时对<strong class="ih hj"> ρ0 </strong>的依赖，即</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ly"><img src="../Images/b136bec6adb194e7d569218a6edb575d.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*OQJ1YUstmKBZaV0oI0GDhw.png"/></div></div></figure><h2 id="6b53" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">离线RL</h2><p id="223d" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">离线RL的目标是设计一种算法，该算法将离线数据集<strong class="ih hj"> D </strong>作为输入，输出具有最小次优的策略<strong class="ih hj"> πout </strong>，即</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lz"><img src="../Images/40ec0f643b38ad1a4b29f58f970301f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*mSKm3vIx6v5l_tlWlKoYRQ.png"/></div></figure><p id="32e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">即使提供了无限大小的数据集，通常也不可能在离线RL中找到最优策略。因此，离线RL的目标是设计出尽可能低次优的算法。</p><h2 id="f6da" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">基于模型的RL</h2><p id="4b76" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">在MBRL，我们构造另一个MDP <strong class="ih hj"> M*={S，A，r，P*，ρ0*，γ} </strong>，它的状态和动作空间与原MDP环境<strong class="ih hj"> (M) </strong>相同。为了简单起见，我们假设报酬函数是已知的，并且在真实MDP <strong class="ih hj"> M </strong>和模型<strong class="ih hj"> M* </strong>中是相同的。如果<strong class="ih hj"> J(π，M*) </strong>紧密跟踪<strong class="ih hj"> J(π，M) </strong>，我们可以使用模型作为策略搜索的代理。然而，使用静态离线数据集学习这样一个精确的模型可能是困难的。有些策略可能会访问状态空间中我们在<strong class="ih hj"> D </strong>中没有足够数据的部分。在下一节中，我们将概述为离线RL设置构建有效模型的方法。</p><h1 id="ce13" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">算法框架</h1><p id="a91a" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">在这一节中，我将首先描述一个理想化的MOReL框架。然后我将描述我们在实验结果中使用的这个算法框架的实际实例。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ma"><img src="../Images/95c1e974f5ad04127e5576c284d759b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yP2oFE_Kyk7tufpFysFlVA.png"/></div></div></figure><h2 id="3299" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">学习动力学模型</h2><p id="00fd" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">第一步涉及使用离线数据集<strong class="ih hj"> D </strong>通过使用最大似然估计或来自生成建模的其他技术来学习近似动力学模型。</p><p id="6213" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于离线数据集可能在状态-动作空间的所有部分都没有足够的支持，我们不能期望模型是全局精确的。一种简单的MBRL方法是使用学习到的近似模型直接进行规划。然而，这种没有任何保障的方法可能导致代理人在具有最少数据支持的状态空间的部分中错误地过度预测回报，从而导致非常次优的策略。他们通过如下所述修改MDP克服了模型偏差的挑战。</p><h2 id="9b21" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">未知状态动作检测器(USAD)</h2><p id="6fa5" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">为了确保策略不会访问模型不准确的状态，它们根据学习模型的准确性将状态空间划分为已知和未知区域。随后，他们旨在将策略搜索限制在已知状态内的策略。为了进行这种划分，他们使用了如下定义的used。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mb"><img src="../Images/15725d2e42ed1946a17d3d56c1817ce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kFFUPVwjIEy3gPbcogCpLA.png"/></div></div></figure><h2 id="e9d5" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">悲观的MDP建筑</h2><p id="9fa0" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">接下来，我们使用学习动力学模型<br/>和前面提到的USAD构建一个悲观的MDP。这基于以下定义:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mc"><img src="../Images/3a2b6b39466a39d63008b65ad763dc60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0fnNzvk3ltr4xu4lU9o-4w.png"/></div></div></figure><p id="90a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们用<strong class="ih hj">δ(s’= HALT)</strong>来表示狄拉克δ函数，它迫使MDP从每个未知的状态-作用对跃迁到特殊的状态HALT。对于这种特殊的状态，我们使用<strong class="ih hj">K</strong>的奖励，而其他的状态动作接收与环境中相同的奖励。</p><h2 id="e344" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">悲观的MDP的规划</h2><p id="ce47" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">莫雷尔的最后一步是计划在构建悲观的MDP。多种技术可用于此，包括MPC、基于搜索的规划、动态规划或策略优化，视上下文而定。对于理想化的情况，他们假设了一个规划先知，可以在悲观的MDP中返回一个<strong class="ih hj"> ∈π次优政策</strong>。</p><h1 id="d8af" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">MOReL的实现</h1><p id="2b90" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">他们通过基于模型的自然政策梯度提出了一个实际的MOReL实现。我们现在将看到详细的步骤:</p><h2 id="fa9e" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">1.动力学模型学习</h2><p id="c662" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">我们通过最大化对数似然来学习参数，或者等效地，神经网络参数为</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es md"><img src="../Images/5ae9daa57b8010b804718856b2cc13d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6bM4K1_Cg31QGPPvhMJoTA.png"/></div></div></figure><p id="8e7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<strong class="ih hj"> μs，σs，μa，σa </strong>是从<strong class="ih hj"> D </strong>计算的状态和动作的平均值和标准偏差。<strong class="ih hj">∈</strong>是数据集中差异的标准偏差，即<strong class="ih hj">∈= s’s</strong>。</p><h2 id="7dd5" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">2.未知状态动作检测器</h2><p id="954c" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">我们如上面的等式中所概述的那样训练模型的集合，并且针对查询<strong class="ih hj"> (s，a) </strong>对跨集合计算差异，如下所示</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es me"><img src="../Images/3426819f8b2ab375edd368d7a3fa1723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R_LmXSW2oxO6vLoD9Eqrfw.png"/></div></div></figure><h2 id="7d02" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">3.政策优化</h2><p id="b5ba" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">为了在悲观的MDP优化政策，作者使用了与<a class="ae jd" href="https://arxiv.org/abs/1502.05477" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">【TRPO】</strong></a><strong class="ih hj">PPO</strong><strong class="ih hj">密切相关的<a class="ae jd" href="https://github.com/aravindr93/mjrl" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> MJRL代码库</strong> </a>。</strong></p><h1 id="304d" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">莫雷尔在不同探索策略下的表现</h1><p id="5c10" class="pw-post-body-paragraph if ig hi ih b ii lq ik il im lr io ip iq ls is it iu lt iw ix iy lu ja jb jc hb bi translated">我们将会看到MOReL用最新的离线RL设置算法得到的一些结果，比如BCQ、BEAR和BRAC的一些变体。他们展示了四种环境和五种探索设置的结果，总共20种不同的变量。他们复制了先前算法的结果(来自BRAC ),并将其结果作为最后一行包含在每个表中。他们观察到他们的算法获得了20种可能的环境探测配置中的12种的最先进的结果。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mf"><img src="../Images/b977f46286de5c9556ac497ec1d426f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O2hfjMpHVK4SpBIsHUTE-w.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mg"><img src="../Images/e43d81fb3eece5474d22d7e9a52d9ce0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4hRf1G_NSvT-N2XxLri2qQ.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/5d7c664c6cc0fbc9b8002d03976f530b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zou5Y5vdL6CZ6i1XCkbGzw.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/b6c10947bbc270c6c06c92ccd25d2ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CNiQ5p6hxmQFbBeYl38NEg.png"/></div></div></figure><p id="75ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总之，MOReL结合了一般化和悲观主义，帮助它在静态离线数据集中可能不会直接出现但可以使用数据集预测的已知状态中执行策略搜索，同时不会漂移到使用静态离线数据无法预测的未知状态。</p><p id="4750" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总的来说，他们的工作证明了对于离线RL问题，基于模型的方法优于无模型的方法。特别地，基于模型的方法提供了在已知的状态-动作空间上约束要支持的策略的能力，而不限制它接近行为策略，这可以导致最终策略质量的显著改进。</p><p id="468a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从实践的角度来看，一个重要的问题是政策评估。所有当前的工作(包括本文)使用来自真实MDP的新轨迹执行政策评估，并使用这些评估来决定何时停止训练。真正的离线RL算法也必须仅使用静态离线数据集来执行该步骤。</p><p id="f80c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢看完:)</p><p id="2c14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">建议是最受欢迎的！</p></div></div>    
</body>
</html>