<html>
<head>
<title>Pros and Cons of popular Supervised Learning Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">流行的监督学习算法的优缺点</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pros-and-cons-of-popular-supervised-learning-algorithms-d5b3b75d9218?source=collection_archive---------1-----------------------#2020-11-04">https://medium.com/analytics-vidhya/pros-and-cons-of-popular-supervised-learning-algorithms-d5b3b75d9218?source=collection_archive---------1-----------------------#2020-11-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/02647940d0074c46657a57fba0bdc0fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*kSBA_gWgytxQFmlR9Xqjaw.png"/></div></figure><blockquote class="im"><p id="d566" class="in io hi bd ip iq ir is it iu iv iw dx translated">我们都曾使用以下监督学习算法之一进行预测分析:</p></blockquote><ol class=""><li id="6f17" class="ix iy hi iz b ja jb jc jd je jf jg jh ji jj iw jk jl jm jn bi translated">逻辑回归</li><li id="a557" class="ix iy hi iz b ja jo jc jp je jq jg jr ji js iw jk jl jm jn bi translated">里脊回归</li><li id="486f" class="ix iy hi iz b ja jo jc jp je jq jg jr ji js iw jk jl jm jn bi translated">套索回归</li><li id="0c25" class="ix iy hi iz b ja jo jc jp je jq jg jr ji js iw jk jl jm jn bi translated">线性判别分析(LDA)</li><li id="df0b" class="ix iy hi iz b ja jo jc jp je jq jg jr ji js iw jk jl jm jn bi translated">k 最近邻(KNN)</li><li id="405e" class="ix iy hi iz b ja jo jc jp je jq jg jr ji js iw jk jl jm jn bi translated">朴素贝叶斯</li><li id="33ac" class="ix iy hi iz b ja jo jc jp je jq jg jr ji js iw jk jl jm jn bi translated">支持向量机(SVM)</li><li id="8748" class="ix iy hi iz b ja jo jc jp je jq jg jr ji js iw jk jl jm jn bi translated">决策图表</li><li id="1fe6" class="ix iy hi iz b ja jo jc jp je jq jg jr ji js iw jk jl jm jn bi translated">随机森林</li><li id="9d28" class="ix iy hi iz b ja jo jc jp je jq jg jr ji js iw jk jl jm jn bi translated">梯度推进</li></ol></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><blockquote class="im"><p id="2177" class="in io hi bd ip iq ka kb kc kd ke iw dx translated">但是你想过它们的利弊吗？这里我列举了几个:</p></blockquote><h2 id="55bf" class="kf kg hi bd kh ki kj kk kl km kn ko kp je kq kr ks jg kt ku kv ji kw kx ky kz bi translated"><strong class="ak"> 1。逻辑回归:</strong></h2><p id="f785" class="pw-post-body-paragraph la lb hi iz b ja lc ld le jc lf lg lh je li lj lk jg ll lm ln ji lo lp lq iw hb bi translated">优点:</p><p id="a231" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)当数据可线性分离时使用。</p><p id="b3e9" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)更易于实施、解释，培训效率也很高。</p><p id="a8a8" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">c)它给出了预测值在正向或负向的重要性的度量。</p><p id="fabc" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">缺点:</p><p id="a3c8" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)它可能在高维数据集中过度拟合。</p><p id="61cf" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)不支持预测值和结果之间的非线性关系。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h2 id="285d" class="kf kg hi bd kh ki lw kk kl km lx ko kp je ly kr ks jg lz ku kv ji ma kx ky kz bi translated">2.岭回归:</h2><p id="ea30" class="pw-post-body-paragraph la lb hi iz b ja lc ld le jc lf lg lh je li lj lk jg ll lm ln ji lo lp lq iw hb bi translated">优点:</p><p id="d613" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)防止在更高的尺寸上过度配合。</p><p id="8b2f" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)平衡偏差-方差权衡。有时，比零偏差更高的偏差比高方差和零偏差更适合。</p><p id="26f5" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">缺点:</p><p id="7d96" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)它增加了偏差。</p><p id="9419" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)我们需要选择最佳α(超参数)</p><p id="4ed8" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">c)模型可解释性低。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h2 id="06b3" class="kf kg hi bd kh ki lw kk kl km lx ko kp je ly kr ks jg lz ku kv ji ma kx ky kz bi translated">3.套索回归:</h2><p id="f2c1" class="pw-post-body-paragraph la lb hi iz b ja lc ld le jc lf lg lh je li lj lk jg ll lm ln ji lo lp lq iw hb bi translated">优点:</p><p id="17bb" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)通过向零收缩系数来执行特征选择。</p><p id="a668" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)避免过度配合。</p><p id="ed67" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">缺点:</p><p id="641b" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)选定的特征可能会有很大的偏差。</p><p id="99c0" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)对于 n &lt; &lt; p(n-数据点数，p-要素数)，LASSO 最多选择 n 个要素。</p><p id="0b0a" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">c)对于不同的引导数据，选择的特征可以非常不同。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h2 id="be5e" class="kf kg hi bd kh ki lw kk kl km lx ko kp je ly kr ks jg lz ku kv ji ma kx ky kz bi translated">4.线性判别分析(LDA):</h2><p id="252d" class="pw-post-body-paragraph la lb hi iz b ja lc ld le jc lf lg lh je li lj lk jg ll lm ln ji lo lp lq iw hb bi translated">优点:</p><p id="685c" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)它是简单、快速和可移植的算法。当它的假设被满足时，它仍然胜过一些算法(逻辑回归)。</p><p id="0571" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">缺点:</p><p id="05e2" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)需要对特征/预测值进行正态分布假设。</p><p id="c072" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)有时对少数类别变量不好。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h2 id="a44f" class="kf kg hi bd kh ki lw kk kl km lx ko kp je ly kr ks jg lz ku kv ji ma kx ky kz bi translated">5.k 最近邻(KNN)</h2><p id="e36a" class="pw-post-body-paragraph la lb hi iz b ja lc ld le jc lf lg lh je li lj lk jg ll lm ln ji lo lp lq iw hb bi translated">优点:</p><p id="cf01" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)这是仅用一个参数实现的最简单的算法。</p><p id="8288" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)用户可以插入任何距离度量，甚至是用户定义的距离度量。这允许处理复杂的对象，比如时间序列、图表、地理坐标，以及基本上任何可以定义距离度量的东西。</p><p id="8e56" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">c)该算法可用于分类、排序、回归(使用邻居平均值或加权平均值)、推荐、缺失值插补等。</p><p id="5bfd" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">缺点:</p><p id="90fa" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a) KNN 是一个懒惰的学习者，因为它不从训练数据中学习模型权重或函数，而是“记忆”训练数据集。因此，推理比训练需要更长的时间。</p><p id="84a1" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)这是一种基于距离的方法，因此模型可能会受到异常值的严重影响，换句话说，它容易过度拟合。</p><p id="df61" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">c)模型大小随着新数据的加入而增长。</p><p id="24ce" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">d)该模型遭受维数灾难。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h2 id="53e4" class="kf kg hi bd kh ki lw kk kl km lx ko kp je ly kr ks jg lz ku kv ji ma kx ky kz bi translated">6.朴素贝叶斯</h2><p id="0a20" class="pw-post-body-paragraph la lb hi iz b ja lc ld le jc lf lg lh je li lj lk jg ll lm ln ji lo lp lq iw hb bi translated">优点:</p><p id="41e8" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)预测测试数据集的类别是容易且快速的。它在多类预测中也表现良好。</p><p id="7c3e" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)当独立性假设成立时，NB 分类器与其他模型(如逻辑回归)相比表现更好，并且您需要更少的训练数据。</p><p id="fc30" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">缺点:</p><p id="681c" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)如果分类变量具有在训练数据集中未观察到的类别(在测试数据集中)，则模型将分配 0(零)概率，并且将无法进行预测。这就是通常所说的“零频率”。为了解决这个问题，我们可以使用平滑技术。最简单的平滑技术之一叫做拉普拉斯估计。</p><p id="fffa" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)对特征的分布有一组强有力的假设，如正态、多项式等。</p><p id="1d73" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">c)预测者也有独立性的假设。在现实生活中，我们几乎不可能得到完全独立的预测器。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h2 id="4c4a" class="kf kg hi bd kh ki lw kk kl km lx ko kp je ly kr ks jg lz ku kv ji ma kx ky kz bi translated">7.支持向量机(SVM):</h2><p id="4f46" class="pw-post-body-paragraph la lb hi iz b ja lc ld le jc lf lg lh je li lj lk jg ll lm ln ji lo lp lq iw hb bi translated">优点:</p><p id="2cc5" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)它工作得非常好，具有清晰的分离界限</p><p id="0ee1" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)在高维空间有效。</p><p id="f3ce" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">c)它在决策函数中使用训练点的子集(称为支持向量)，因此它也是内存高效的。</p><p id="8baa" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">d)它在非线性模型拟合中也是有效的。</p><p id="0209" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">缺点:</p><p id="f2f0" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)当我们拥有大型数据集时，它的表现不佳，因为训练变得非常耗时。</p><p id="547a" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)当数据集具有更多噪声时，即目标类重叠时，它也不能很好地执行。</p><p id="79b2" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">c) SVM 不直接输出概率。需要使用其他方法将 SVM 的输出转换为概率。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h2 id="b360" class="kf kg hi bd kh ki lw kk kl km lx ko kp je ly kr ks jg lz ku kv ji ma kx ky kz bi translated">8.决策树:</h2><p id="f4bd" class="pw-post-body-paragraph la lb hi iz b ja lc ld le jc lf lg lh je li lj lk jg ll lm ln ji lo lp lq iw hb bi translated">优点:</p><p id="c180" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)易于理解和解释，非常适合视觉表现。</p><p id="34ce" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)它需要很少的数据预处理，即不需要一次性编码、标准化等。</p><p id="b24b" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">c)它是非参数模型。因此，不需要关于数据分布的假设。</p><p id="c9fe" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">d)特征选择自动发生。因此，不重要的特征不会影响结果。</p><p id="b080" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">缺点:</p><p id="f932" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)它倾向于过度拟合。</p><p id="0ab3" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)非常敏感。数据的微小变化会极大地影响预测(高方差)。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h2 id="9418" class="kf kg hi bd kh ki lw kk kl km lx ko kp je ly kr ks jg lz ku kv ji ma kx ky kz bi translated">9.随机森林(RF):</h2><p id="2bf0" class="pw-post-body-paragraph la lb hi iz b ja lc ld le jc lf lg lh je li lj lk jg ll lm ln ji lo lp lq iw hb bi translated">优点:</p><p id="9ee1" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)它对异常值是稳健的。它降低了过度拟合的风险。</p><p id="fa99" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)它也适用于非线性数据。</p><p id="32e5" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">c)它在大型数据集上高效运行。</p><p id="e5e8" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">d)通常它比其他分类算法给出更好的准确性。</p><p id="8959" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">缺点:</p><p id="6e06" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)发现随机森林在处理分类变量时有偏差。</p><p id="aa39" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)慢训练。</p><p id="3e5c" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">c)它不适合具有大量稀疏特征的线性方法。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h2 id="0e79" class="kf kg hi bd kh ki lw kk kl km lx ko kp je ly kr ks jg lz ku kv ji ma kx ky kz bi translated">10.梯度提升:</h2><p id="2a4e" class="pw-post-body-paragraph la lb hi iz b ja lc ld le jc lf lg lh je li lj lk jg ll lm ln ji lo lp lq iw hb bi translated">优点:</p><p id="2f5e" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a) Boosting 附带一个易于阅读和解释的算法，使其预测解释易于处理。</p><p id="d383" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)增压是一种弹性方法，可轻松抑制过度拟合。</p><p id="5044" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">缺点:</p><p id="7461" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">a)它对异常值敏感，因为每个分类器都必须修正先前分类器中的错误。因此，该方法过于依赖异常值。</p><p id="5c62" class="pw-post-body-paragraph la lb hi iz b ja lr ld le jc ls lg lh je lt lj lk jg lu lm ln ji lv lp lq iw hb bi translated">b)提升几乎不可能扩大规模。这是因为每一个估计量的正确性都是建立在先前预测量的基础上的，因此使得这个过程很难简化。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><blockquote class="im"><p id="f194" class="in io hi bd ip iq ka kb kc kd ke iw dx translated">我希望你喜欢这篇文章。请鼓掌，写下你的评论，并跟随我获取更多与数据科学相关的文章。</p><p id="3ea9" class="in io hi bd ip iq ka kb kc kd ke iw dx translated">T <!-- -->渴望阅读，再见！</p></blockquote></div></div>    
</body>
</html>