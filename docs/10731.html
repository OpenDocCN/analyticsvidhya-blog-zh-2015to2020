<html>
<head>
<title>Akira’s ML News #Week44, 2020</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2020 年第 44 周</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/akiras-ml-news-week44-2020-eda5ee9fff33?source=collection_archive---------26-----------------------#2020-10-31">https://medium.com/analytics-vidhya/akiras-ml-news-week44-2020-eda5ee9fff33?source=collection_archive---------26-----------------------#2020-10-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="372a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是我在 2020 年第 44 周(10 月 25 日~)读到的一些我觉得特别有意思的论文和文章。我已经尽量介绍最近的了，但是论文提交的日期可能和星期不一样。</p><h2 id="fde9" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">主题</h2><ol class=""><li id="8914" class="jy jz hi ih b ii ka im kb iq kc iu kd iy ke jc kf kg kh ki bi translated">机器学习论文</li><li id="4153" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">技术文章</li><li id="48ef" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">机器学习用例的例子</li><li id="bfe3" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">其他主题</li></ol><h1 id="e315" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">—每周编辑精选</h1><ul class=""><li id="561b" class="jy jz hi ih b ii ka im kb iq kc iu kd iy ke jc lf kg kh ki bi translated"><a class="ae lg" href="https://arxiv.org/abs/2007.00151" rel="noopener ugc nofollow" target="_blank">使用错误和正确标签之间的学习速度差异的正则化</a> (1。机器学习论文)</li><li id="4a92" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc lf kg kh ki bi translated"><a class="ae lg" href="https://arxiv.org/abs/2010.07922" rel="noopener ugc nofollow" target="_blank">使用因果图的自我监督学习</a> (1。机器学习论文)</li><li id="8da5" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc lf kg kh ki bi translated"><a class="ae lg" href="https://arxiv.org/abs/2010.02178" rel="noopener ugc nofollow" target="_blank">填充精度的位置依赖性</a> (1。机器学习论文)</li></ul><h1 id="a352" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">—过去的文章</h1><p id="46d3" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated"><a class="ae lg" rel="noopener" href="/analytics-vidhya/akiras-ml-news-week43-2020-c4ad1cfa5de7">第 43 周</a> ⇦第 44 周(本帖)⇨ <a class="ae lg" rel="noopener" href="/analytics-vidhya/akiras-ml-news-week45-2020-c58112bd184f">第 45 周</a></p><p id="f80f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lg" rel="noopener" href="/analytics-vidhya/akiras-ml-news-september-2020-80ed65bd7ea4">2020 年 9 月汇总</a></p></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><p id="670a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><h1 id="b72a" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">1.机器学习论文</h1><p id="ce61" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi">— —</p><h1 id="175d" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">多语言文本到文本海量模型</h1><p id="d3a0" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated"><em class="lr"> mT5:一个大规模多语言预训练文本到文本转换器</em><br/><a class="ae lg" href="https://arxiv.org/abs/2010.11934" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2010.11934</a></p><p id="be1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们提出了 mT5，T5 的多语言版本，将所有任务统一为 text2text 格式，并遵循预学习到微调的策略，以及 mC4，包含 101 种语言的大型多语言数据集。它有多达 130 亿个参数，具有各种任务的最高性能。</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es ls"><img src="../Images/0d027c352d51c837cd413830565c35c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aUYsD8ogA9Zmk8qg.png"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx translated">图和标题摘自上述论文</figcaption></figure></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="75e8" class="ko je hi bd jf kp mi kr jj ks mj ku jn kv mk kx jq ky ml la jt lb mm ld jw le bi translated">利用错误标签和正确标签之间学习速度差异的正则化</h1><p id="8a61" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated"><em class="lr">早期学习正规化防止记忆嘈杂的标签</em><br/><a class="ae lg" href="https://arxiv.org/abs/2007.00151" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2007.00151</a></p><p id="07bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们发现，在标签噪声的情况下，具有正确标签的数据可以被正确地学习，而具有错误标签的数据起初可以预测正确的标签，但后来被错误的标签所拉动，并记住了数据。利用这一现象，他们提出了 ESR，一种使用模型输出移动平均值的正则化方法。这些结果在存在标签噪声的情况下非常有效。</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div class="er es mn"><img src="../Images/c935396c35afef9c0ae2bcf3c793d8ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/0*5oYw2FZdpEGXTYif.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">图和标题摘自上述论文</figcaption></figure></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="9fa2" class="ko je hi bd jf kp mi kr jj ks mj ku jn kv mk kx jq ky ml la jt lb mm ld jw le bi translated">使用因果图的自我监督学习</h1><p id="923e" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated"><em class="lr">经由不变因果机制的表征学习</em><br/><a class="ae lg" href="https://arxiv.org/abs/2010.07922" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2010.07922</a></p><p id="bd85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑到图像是由内容(动物种类)和风格(背景等)的因果图构成的。)，他们提出了一个自我监督的学习遗迹，学习图像对风格不变。具体来说，该系统被设计成对各个图像进行分类并匹配它们的分布，使得它们对于通过数据扩充进行的风格变换是不变的。这不仅与以前的研究相当，而且在强化学习中也是有效的。</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mo"><img src="../Images/560c0bf4a3f164b833f9b8b6dc8cfffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CJW0SX3OTgWL3Vxc.png"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx translated">图和标题摘自上述论文</figcaption></figure></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="e35f" class="ko je hi bd jf kp mi kr jj ks mj ku jn kv mk kx jq ky ml la jt lb mm ld jw le bi translated">数据不仅关乎数量，也关乎质量。</h1><p id="bde2" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated"><em class="lr">数据集制图:用训练动力学</em><br/><a class="ae lg" href="https://arxiv.org/abs/2009.10795" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2009.10795</a>对数据集进行制图和诊断</p><p id="4cfa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一项研究调查了 NLP 数据的质量。研究表明，数据可以分为有助于学习收敛的区域、由于错误标注和其他因素而难以学习的区域以及有助于泛化性能的具有可变置信度的模糊区域。最近 NLP 倾向于强调数量而不是质量，但建议复习质量也不错。</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mp"><img src="../Images/30877b82ef449130b5e353f22a150e47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uCjZNvbwHYnpNtj5.png"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx translated">图和标题摘自上述论文</figcaption></figure></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="3f84" class="ko je hi bd jf kp mi kr jj ks mj ku jn kv mk kx jq ky ml la jt lb mm ld jw le bi translated">通过填充的精度的位置依赖性</h1><p id="8ad9" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated"><em class="lr">注意护垫——CNN 会产生盲点</em><br/><a class="ae lg" href="https://arxiv.org/abs/2010.02178" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2010.02178</a></p><p id="4192" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">研究表明，填充导致准确性的位置依赖性；对于像 ResNet 这样使用 stride=2 进行缩减采样的网络，填充像素的应用并不相同，具体取决于图像的大小。(使用最左边的填充，但不使用最右边的填充。)只要改变图像大小使其填充处理相等，精度就提高了。</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mq"><img src="../Images/03b79d83ea4b0b23d52d330c2ae00fc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*w9YE-rLb9IYhDIjT.png"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx translated">图和标题摘自上述论文</figcaption></figure></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="6967" class="ko je hi bd jf kp mi kr jj ks mj ku jn kv mk kx jq ky ml la jt lb mm ld jw le bi translated">高维空间中的嵌入表示</h1><p id="ce1e" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">增加输入维度能否提高深度强化学习？<br/><a class="ae lg" href="https://arxiv.org/abs/2003.01629" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2003.01629</a></p><p id="65d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常情况下，表征学习旨在使其低维化，但由于强化学习中的状态表征被视为中间层，因此这项研究受到大规模网络取得结果的趋势的启发，并将其纳入高维空间。它在许多环境中是有效的。</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mr"><img src="../Images/a77d04ee39e454d95aee599564dd2e67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MqNtq1OzFLcg-7ck.png"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx translated">图和标题摘自上述论文</figcaption></figure></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="63b9" class="ko je hi bd jf kp mi kr jj ks mj ku jn kv mk kx jq ky ml la jt lb mm ld jw le bi translated">通过主动学习的分段实现可视化</h1><p id="f88b" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated"><em class="lr">深度主动学习进行联合分类&amp;分割用弱标注器</em><br/><a class="ae lg" href="https://arxiv.org/abs/2010.04889" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2010.04889</a></p><p id="a7a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于只有少量掩码的标记数据，他们提出了一种方法，通过主动学习逐渐增加掩码数据的数量，同时进行分类和分割，这种方法比 CAM 提供了更好的可视化性能。</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div class="er es ms"><img src="../Images/d82fae7452e6b448b4eb935f3c85d08b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*W3cZW-77tH8FkTsG.png"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">图和标题摘自上述论文</figcaption></figure></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="dc40" class="ko je hi bd jf kp mi kr jj ks mj ku jn kv mk kx jq ky ml la jt lb mm ld jw le bi translated">各种数据域的比例法则</h1><p id="ae6a" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated"><em class="lr">自回归生成建模的标度法则</em><br/><a class="ae lg" href="https://arxiv.org/abs/2010.14701" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2010.14701</a></p><p id="5597" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该研究调查了各种数据域中计算资源、数据量和模型大小的尺度规律。他们在所有研究的领域中发现幂律，并且一个领域的最佳模型大小显示了一个普遍的趋势，而与该领域无关。</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mt"><img src="../Images/eadae83f2976b699c0cd8af3364f4a9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2kNEjCAIPR8fKGHg.png"/></div></div><figcaption class="me mf et er es mg mh bd b be z dx translated">图和标题摘自上述论文</figcaption></figure></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><p id="803f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><h1 id="2ef3" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">2.技术文章</h1><p id="1694" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi">— — — —</p><h1 id="5829" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">合成数据的有效性</h1><p id="537e" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">讨论如何在现场使用合成数据的对话文章。它讨论了合成数据可以增加数据的多样性，以及他们如何在收集实际数据的同时运行一个循环来提高合成数据的质量。</p><div class="mu mv ez fb mw mx"><a href="https://www.wandb.com/podcast/daeil-kim" rel="noopener  ugc nofollow" target="_blank"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hj fi z dy nc ea eb nd ed ef hh bi translated">用金正日合成数据的不合理的有效性</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">涵盖的主题:0:00 内容多样化 0:23 简介+生物 1:00 从文科到综合数据 8:48 什么是…</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">www.wandb.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl mc mx"/></div></div></a></div></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="902e" class="ko je hi bd jf kp mi kr jj ks mj ku jn kv mk kx jq ky ml la jt lb mm ld jw le bi translated">ViT 解释</h1><p id="5ea9" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">这个视频解释了变压器模型的论文，击败了 CNN 模型的 ViT。像 CNN 一样，变压器如何随着层的加深从局部特征获得全局特征，以及变压器如何比 CNN 和 LSTMs 具有更小的感应偏差，以便可以用于大规模。他解释说，如果你有数据集，你可以超越这些。等。</p><figure class="lt lu lv lw fd lx"><div class="bz dy l di"><div class="nm nn l"/></div></figure></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><p id="cb32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><h1 id="3dba" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">3.机器学习用例的例子</h1><p id="08b3" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi">— — — —</p><h1 id="6226" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">使用机器学习帮助清理河流垃圾</h1><p id="5f78" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">在 2018 年和 2019 年的两次微软全球黑客马拉松比赛中，黑客马拉松团队与海洋清理组织(Ocean Cleanup)合作，建立了一个机器学习模型，以帮助量化流向海洋的河流中的塑料污染量。</p><div class="mu mv ez fb mw mx"><a href="https://news.microsoft.com/features/microsoft-hackathon-leads-to-ai-and-sustainability-collaboration-to-rid-plastic-from-rivers-and-the-ocean/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hj fi z dy nc ea eb nd ed ef hh bi translated">微软黑客马拉松导致人工智能和可持续发展的合作，以摆脱河流和…</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">苏珊娜·乔尼塑料袋。塑料瓶。塑料玩具。将近 900 万吨的塑料碎片被丢弃在…</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">news.microsoft.com</p></div></div><div class="ng l"><div class="no l ni nj nk ng nl mc mx"/></div></div></a></div></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><h1 id="00cb" class="ko je hi bd jf kp mi kr jj ks mj ku jn kv mk kx jq ky ml la jt lb mm ld jw le bi translated">用人工智能分析信息战</h1><p id="68d8" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">本文讨论了如何使用自然语言处理来分析大量的新闻和信息，以应对信息战。除其他外，报告分析说，在亚美尼亚和阿塞拜疆之间最近发生冲突的几个月前，就有人散布信息，故意将该国的一方描绘成侵略者。</p><div class="mu mv ez fb mw mx"><a href="https://www.wired.com/story/ai-helping-pentagon-assess-disinfo-campaigns/" rel="noopener  ugc nofollow" target="_blank"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hj fi z dy nc ea eb nd ed ef hh bi translated">帮助五角大楼评估反信息作战的人工智能公司</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">九月，阿塞拜疆和亚美尼亚因纳戈尔诺-卡拉巴赫——高加索地区的一块有争议的领土——重新开战</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">www.wired.com</p></div></div><div class="ng l"><div class="np l ni nj nk ng nl mc mx"/></div></div></a></div></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><p id="eab3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><h1 id="ebcf" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">4.其他主题</h1><p id="03c8" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi">— — — —</p><h1 id="c732" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">采访竞赛大师</h1><p id="edda" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated">这是对一位大师的视频采访。它长约 5 分钟，包括他如何开始玩 Kaggle，对他来说成为一名大师意味着什么，以及对 Kaggle 初学者的建议。</p><figure class="lt lu lv lw fd lx"><div class="bz dy l di"><div class="nm nn l"/></div></figure></div><div class="ab cl lk ll gp lm" role="separator"><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp lq"/><span class="ln bw bk lo lp"/></div><div class="hb hc hd he hf"><p id="1603" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><h1 id="9b4f" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">—过去的文章</h1><p id="ca16" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated"><a class="ae lg" rel="noopener" href="/analytics-vidhya/akiras-ml-news-week43-2020-c4ad1cfa5de7">第 43 周</a> ⇦第 44 周(本帖)⇨ <a class="ae lg" rel="noopener" href="/analytics-vidhya/akiras-ml-news-week45-2020-c58112bd184f">第 45 周</a></p><p id="36f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【2020 年 9 月摘要</p><p id="9fe6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><h1 id="5dc2" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">推特，我贴一句纸评论。</h1><p id="f3a1" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lh is it iu li iw ix iy lj ja jb jc hb bi translated"><a class="ae lg" href="https://twitter.com/AkiraTOSEI" rel="noopener ugc nofollow" target="_blank">https://twitter.com/AkiraTOSEI</a></p></div></div>    
</body>
</html>