<html>
<head>
<title>Which one is better: Reinforcement Learning or Model Predictive Control? Inverted Pendulum — Case*</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习和模型预测控制哪个更好？倒立摆—案例*</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/which-one-is-better-reinforcement-learning-or-model-predictive-control-inverted-pendulum-case-7fc29e52bbfb?source=collection_archive---------2-----------------------#2020-05-23">https://medium.com/analytics-vidhya/which-one-is-better-reinforcement-learning-or-model-predictive-control-inverted-pendulum-case-7fc29e52bbfb?source=collection_archive---------2-----------------------#2020-05-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4aab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">“你想过吗？如果有，那哪个更好？”</em>T3】</strong></p><p id="96b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">*免责声明:这篇文章是根据我的经验和知识撰写的。所以，如果你认为这里有些地方写得不对，请随时联系我。</p><p id="ed1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi je translated"><span class="l jf jg jh bm ji jj jk jl jm di">如果你来自控制工程背景，那肯定是个棘手的问题。作为一名过去学习控制工程的学生，在为系统设计控制器之前，我总是通过实现最能代表系统的数学模型来进行系统建模。有了收集到的系统模型，我们可以设计一个控制系统，在这个系统中，我们可以100%地保证系统将根据我们的设计来跟随它的参考点。</span></p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es jn"><img src="../Images/31672922d86887bdfe25a72ac3f2fef6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gWNygeXuy5a1vOis1YnLAg.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">数学模型代表:电气、流体、机械系统</figcaption></figure><p id="4a70" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当设计一个控制系统时，我们可以使用很多框架和方法，从经典控制到现代控制理论。在处理单输入单输出(SISO)系统和传递函数的经典控制中，我们可以使用开关控制、PID控制、极点配置等。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es kd"><img src="../Images/fda30a1fb286e924d169917c2fbb429e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yZxtFkdJytSyAf6W76UDMQ.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">经典控制</figcaption></figure><p id="ea50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在现代控制中，当我们处理多输入多输出(MIMO)系统时，我们需要将系统转换到状态空间，从而将多阶系统分解成一组一阶系统。有很多方法可以控制这种情况，如模型预测控制(MPC)，线性二次调节器(LQR)，鲁棒控制，庞特里亚金最大最小原理(PMP)，卡尔曼滤波器等。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es ke"><img src="../Images/1c935ad8ab309035455d34e8a5e78715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ztalahwj5OVKOb0jcFhMdQ.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">现代控制</figcaption></figure><p id="64d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最近几天，许多处理人工智能(AI)和机器学习(ML)的人试图实现这一点来控制理论。实际上这并不是一件新鲜事，因为一些研究人员已经尝试用神经网络来控制系统，但它仍然有局限性，因为神经网络只能在特定条件下处理控制任务，只有在它被训练的地方。如果我们有其他情况，我们需要重新训练它。当然，这与经典/现代控制非常不同，在经典/现代控制中，我们可以设置控制系统鲁棒性来处理这种情况。</p><p id="adda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">后来，科学家们尝试使用PID控制框架，其中PID参数通过神经网络进行调整。此后，人工智能在控制领域的应用迅速发展，许多新的方法被用于控制系统，如蚁群优化、遗传算法、模糊控制等。现在，随着人工智能和人工智能世界的快速发展，有一种方法叫做强化学习。</p><blockquote class="kf kg kh"><p id="58c9" class="if ig jd ih b ii ij ik il im in io ip ki ir is it kj iv iw ix kk iz ja jb jc hb bi translated">总的来说，我们可以把控制世界分为一个<strong class="ih hj">保守控制系统</strong> <strong class="ih hj">(古典和现代控制)</strong>和<strong class="ih hj">智能控制系统(AI和ML) </strong>。</p></blockquote><p id="c189" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将尝试回答上面提到的关于控制倒立摆、模型预测控制和强化学习哪个更好的问题？</p><blockquote class="kf kg kh"><p id="2912" class="if ig jd ih b ii ij ik il im in io ip ki ir is it kj iv iw ix kk iz ja jb jc hb bi translated"><strong class="ih hj">强化学习</strong></p></blockquote><p id="36cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">强化学习</strong> (RL)是<strong class="ih hj">机器学习</strong>的一个领域，它有两个组成部分:代理和环境。它被认为是与监督学习和非监督学习并列的三种基本机器学习范式之一。</p><blockquote class="kl"><p id="2313" class="km kn hi bd ko kp kq kr ks kt ku jc dx translated">它是如何工作的？</p></blockquote><p id="8dea" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">这个代理将把行动放入环境中，以最大化累积回报的概念。当代理对环境采取行动时，环境将更新其状态，作为代理计算奖励和更新行动的输入。在控制术语中，我们可以把代理想象成控制器，把环境想象成我们想要控制的系统。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es la"><img src="../Images/367f26b6c3e8842257bbab98992b8a53.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/format:webp/1*dF2iro7UEMwNmYhANDZrFA.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">强化学习工作流程</figcaption></figure><p id="f313" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了给倒立摆任务的环境建模，我们将使用由OpenAI开发的工具包，名为<a class="ae lb" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI Gym </a>。OpenAI Gym是一个用于开发和比较强化学习算法的工具包，它为训练和测试强化学习代理提供了几个预定义的环境，包括用于经典物理控制任务的环境。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es lc"><img src="../Images/c84a3433d1fbf775b5b4b2c58a360a27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BZDPIU8ARez28GdXPy1mWQ.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">奥鹏健身馆</figcaption></figure><p id="c26f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以下面是用Python实现OpenAI Gym的RL的步骤[1]:</p><p id="8fd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一步:安装OpenAI Gym并给图书馆打电话</p><pre class="jo jp jq jr fd ld le lf lg aw lh bi"><span id="ecd0" class="li lj hi le b fi lk ll l lm ln">!pip install gym</span><span id="d64d" class="li lj hi le b fi lo ll l lm ln">import tensorflow as tf<br/>import numpy as np<br/>import base64, io, time, gym<br/>import IPython, functools<br/>import matplotlib.pyplot as plt<br/>from tqdm import tqdm</span></pre><p id="ea73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二步:初始化环境，我们将使用车杆环境，因为它最能代表倒立摆的情况</p><pre class="jo jp jq jr fd ld le lf lg aw lh bi"><span id="b7e9" class="li lj hi le b fi lk ll l lm ln">env = gym.make(“CartPole-v0”)<br/>env.seed(1)</span></pre><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es lp"><img src="../Images/2a62123fde51f56388e692e4d63c613e.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*-bDl63U-9yCNlOj0uuGn3w.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">开放式体育馆的推车杆系统</figcaption></figure><p id="42c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第3步:定义推车杆代理</p><pre class="jo jp jq jr fd ld le lf lg aw lh bi"><span id="feaf" class="li lj hi le b fi lk ll l lm ln">### Define the Cartpole agent ###<br/># Defines a feed-forward neural network</span><span id="c0f9" class="li lj hi le b fi lo ll l lm ln">def create_cartpole_model():<br/>model = tf.keras.models.Sequential([</span><span id="1251" class="li lj hi le b fi lo ll l lm ln"># First Dense layer<br/>tf.keras.layers.Dense(units=32, activation=’relu’),</span><span id="eed1" class="li lj hi le b fi lo ll l lm ln"># Define the last Dense layer, which will provide the network’s output.<br/>tf.keras.layers.Dense(units=n_actions, activation=None) # TODO])</span><span id="236c" class="li lj hi le b fi lo ll l lm ln">return model<br/>cartpole_model = create_cartpole_model()</span></pre><p id="ae00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个cart极点代理被描述为具有32个隐藏密集单元和2个输出的前馈神经网络。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es lq"><img src="../Images/4c8bee52d18459f982491b95c36f9b6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*3FfBW0GmxtLFGMZ2gGCZ9A.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">手推车杆代理描述</figcaption></figure><p id="47b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤4:定义代理的动作</p><pre class="jo jp jq jr fd ld le lf lg aw lh bi"><span id="56ef" class="li lj hi le b fi lk ll l lm ln">### Define the agent’s action function #### <br/># Function that takes observations as input, executes a forward pass through model, and outputs a sampled action.</span><span id="da4d" class="li lj hi le b fi lo ll l lm ln"># Arguments:<br/># model: the network that defines our agent<br/># observation: observation which is fed as input to the model</span><span id="009f" class="li lj hi le b fi lo ll l lm ln"># Returns:<br/># action: choice of agent action</span><span id="0bdc" class="li lj hi le b fi lo ll l lm ln">def choose_action(model, observation):<br/>   # add batch dimension to the observation<br/>   observation = np.expand_dims(observation, axis=0)</span><span id="c9a1" class="li lj hi le b fi lo ll l lm ln">   # Feed the observations through the model to predict the log      probabilities of each possible action<br/>   logits = model.predict(observation)</span><span id="5e81" class="li lj hi le b fi lo ll l lm ln">   # pass the log probabilities through a softmax to compute true probabilities<br/>   prob_weights = tf.nn.softmax(logits).numpy()</span><span id="7b85" class="li lj hi le b fi lo ll l lm ln">   # Randomly sample from the prob_weights to pick an action.<br/>   action = np.random.choice(n_actions, size=1,   p=prob_weights.flatten())[0] # TODO</span><span id="dc03" class="li lj hi le b fi lo ll l lm ln">return action</span></pre><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es lr"><img src="../Images/2aacb20decd2e1d7cb08dfd0062cd8b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*7c-3vtybcQHmBqS0mOZtRA.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">代理人的行动</figcaption></figure><p id="c495" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第五步:定义代理的记忆</p><pre class="jo jp jq jr fd ld le lf lg aw lh bi"><span id="85e6" class="li lj hi le b fi lk ll l lm ln">### Agent Memory ###</span><span id="d265" class="li lj hi le b fi lo ll l lm ln">class Memory:<br/>   def __init__(self):<br/>      self.clear()</span><span id="e825" class="li lj hi le b fi lo ll l lm ln"># Resets/restarts the memory buffer<br/>   def clear(self):<br/>      self.observations = []<br/>      self.actions = []<br/>      self.rewards = []</span><span id="87d0" class="li lj hi le b fi lo ll l lm ln"># Add observations, actions, rewards to memory</span><span id="a077" class="li lj hi le b fi lo ll l lm ln">   def add_to_memory(self, new_observation, new_action, new_reward):<br/>      self.observations.append(new_observation)<br/>      self.actions.append(new_action) <br/>      # Update the list of rewards with new reward<br/>      self.rewards.append(new_reward)</span><span id="b8ea" class="li lj hi le b fi lo ll l lm ln">memory = Memory()</span></pre><p id="e5c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第六步:定义奖励函数</p><pre class="jo jp jq jr fd ld le lf lg aw lh bi"><span id="d087" class="li lj hi le b fi lk ll l lm ln">### Reward function ###<br/># Helper function that normalizes an np.array x</span><span id="285a" class="li lj hi le b fi lo ll l lm ln">def normalize(x):<br/>   x -= np.mean(x)<br/>   x /= np.std(x)<br/>   return x.astype(np.float32)</span><span id="6165" class="li lj hi le b fi lo ll l lm ln"># Compute normalized, discounted, cumulative rewards (i.e., return)<br/># Arguments:<br/># rewards: reward at timesteps in episode<br/># gamma: discounting factor</span><span id="334e" class="li lj hi le b fi lo ll l lm ln"># Returns:<br/># normalized discounted reward<br/>def discount_rewards(rewards, gamma=0.95):<br/>   discounted_rewards = np.zeros_like(rewards)<br/>   R = 0<br/>   for t in reversed(range(0, len(rewards))):<br/>      # update the total discounted reward<br/>      R = R * gamma + rewards[t]<br/>      discounted_rewards[t] = R</span><span id="3606" class="li lj hi le b fi lo ll l lm ln">   return normalize(discounted_rewards)</span></pre><p id="ee89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤7:定义损失函数</p><pre class="jo jp jq jr fd ld le lf lg aw lh bi"><span id="014d" class="li lj hi le b fi lk ll l lm ln">### Loss function ###<br/># Arguments:<br/># logits: network’s predictions for actions to take<br/># actions: the actions the agent took in an episode<br/># rewards: the rewards the agent received in an episode</span><span id="f86c" class="li lj hi le b fi lo ll l lm ln"># Returns:<br/># loss</span><span id="8ddb" class="li lj hi le b fi lo ll l lm ln">def compute_loss(logits, actions, rewards):<br/>   # Compute the negative log probabilities<br/>   neg_logprob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=actions</span><span id="1a7e" class="li lj hi le b fi lo ll l lm ln">   # Scale the negative log probability by the rewards<br/>   loss = tf.reduce_mean( neg_logprob * rewards ) # TODO<br/>   return loss</span></pre><p id="b370" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤8:使用损失函数来定义学习算法的训练步骤</p><pre class="jo jp jq jr fd ld le lf lg aw lh bi"><span id="fe24" class="li lj hi le b fi lk ll l lm ln">### Training step (forward and backpropagation) ###</span><span id="42ae" class="li lj hi le b fi lo ll l lm ln">def train_step(model, optimizer, observations, actions, discounted_rewards):</span><span id="9794" class="li lj hi le b fi lo ll l lm ln">   with tf.GradientTape() as tape:<br/>      # Forward propagate through the agent network<br/>      logits = model(observations)<br/>      # Call the compute_loss function to compute the loss’’’<br/>      loss = compute_loss(logits, actions, discounted_rewards)<br/>      # Run backpropagation to minimize the loss using the    tape.gradient method</span><span id="4abe" class="li lj hi le b fi lo ll l lm ln">   grads = tape.gradient(loss, model.trainable_variables)<br/>   optimizer.apply_gradients(zip(grads, model.trainable_variables))</span></pre><p id="4984" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第九步:运行车杆</p><pre class="jo jp jq jr fd ld le lf lg aw lh bi"><span id="d64f" class="li lj hi le b fi lk ll l lm ln">### Cartpole training! ###</span><span id="12cb" class="li lj hi le b fi lo ll l lm ln"># Learning rate and optimizer<br/>learning_rate = 1e-3<br/>optimizer = tf.keras.optimizers.Adam(learning_rate)</span><span id="b2ed" class="li lj hi le b fi lo ll l lm ln"># instantiate cartpole agent<br/>cartpole_model = create_cartpole_model()</span><span id="9cc5" class="li lj hi le b fi lo ll l lm ln"># to track our progress<br/>smoothed_reward = mdl.util.LossHistory(smoothing_factor=0.9)<br/>plotter = mdl.util.PeriodicPlotter(sec=2, xlabel=’Iterations’, ylabel=’Rewards’)<br/>if hasattr(tqdm, ‘_instances’): tqdm._instances.clear() # clear if it exists</span><span id="e0dd" class="li lj hi le b fi lo ll l lm ln">for i_episode in range(500):<br/>   plotter.plot(smoothed_reward.get())<br/>   # Restart the environment<br/>   observation = env.reset()<br/>   memory.clear()<br/>   while True:<br/>   # using our observation, choose an action and take it in the environment<br/>   action = choose_action(cartpole_model, observation)<br/>   next_observation, reward, done, info = env.step(action)<br/>   # add to memory<br/>   memory.add_to_memory(observation, action, reward)<br/>   # is the episode over? did you crash or do so well that you’re done?<br/>   if done:<br/>      # determine total reward and keep a record of this<br/>      total_reward = sum(memory.rewards)<br/>      smoothed_reward.append(total_reward)<br/>      # initiate training — remember we don’t know anything about how the<br/>      # agent is doing until it has crashed!<br/>      train_step(cartpole_model, optimizer, observations=np.vstack(memory.observations),actions=np.array(memory.actions),discounted_rewards = discount_rewards(memory.rewards))</span><span id="0781" class="li lj hi le b fi lo ll l lm ln">      # reset the memory<br/>      memory.clear()<br/>      break</span><span id="211b" class="li lj hi le b fi lo ll l lm ln">   # update our observatons<br/>   observation = next_observation</span></pre><p id="7f2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一般来说，下图总结了所有的工作流程:</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es ls"><img src="../Images/f5dba99edffeb3f0c0268040a2804e48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*VRs3hBh6szrFBTSA6NWZPQ.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">强化学习工作流程</figcaption></figure><blockquote class="kf kg kh"><p id="ef55" class="if ig jd ih b ii ij ik il im in io ip ki ir is it kj iv iw ix kk iz ja jb jc hb bi translated"><strong class="ih hj">模型预测控制</strong></p></blockquote><p id="d212" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">模型预测控制</strong> (MPC)被广泛认为是过程控制的高级方法，用于在满足一组约束的同时控制过程。但近年来，它也被用于控制电气和机械系统。</p><blockquote class="kl"><p id="6236" class="km kn hi bd ko kp kq kr ks kt ku jc dx translated">它是如何工作的？</p></blockquote><p id="1ce4" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq kx is it iu ky iw ix iy kz ja jb jc hb bi translated">MPC使用系统模型来预测系统的未来行为。MPC求解一个在线优化算法，以找到将预测输出驱动到参考值的最优控制动作。MPC可以处理输入和输出之间可能存在相互作用的多输入多输出系统。它还可以处理输入和输出约束[3]。</p><p id="3139" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简而言之，MPC是一种优化方法，您可以在每个有限时间范围内迭代优化输入。它被称为在线，因为优化是迭代完成的，直到系统达到其设定点。这不同于其它优化方法，在其它优化方法中，控制增益的计算仅在过程开始之前进行一次。</p><p id="6169" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以下面是用Python实现OpenAI Gym的MPC的步骤[2]:</p><p id="abed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一步:确定大车行走杆的数学模型</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es lt"><img src="../Images/375ec7d69ed4a3445fea8255e50d8d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*ThUBNdqUCOZU4ct3wC4Vdw.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">推车杆数学模型</figcaption></figure><p id="e651" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">事实上，我们可以使用很多cart杆数学模型，这取决于假设，因为有一个模型忽略了杆的惯性矩。所以我们需要明智地选择模型。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es lu"><img src="../Images/14094b070cb70e59f6d811123d79bd17.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*EiijhAabjmmXUfOnY74VBw.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">状态空间中的其他车极点数学模型<a class="ae lb" href="https://apmonitor.com/do/index.php/Main/InvertedPendulum" rel="noopener ugc nofollow" target="_blank">https://apmonitor.com/do/index.php/Main/InvertedPendulum</a></figcaption></figure><p id="1b22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤2:确定成本函数</p><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es lv"><img src="../Images/e1d22beda97b82a4a7132cf33d72a8ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*HzfpQ24ok6AJrfytE1ynYA.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">二次成本函数</figcaption></figure><p id="11cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第三步:打电话给图书馆</p><pre class="jo jp jq jr fd ld le lf lg aw lh bi"><span id="b053" class="li lj hi le b fi lk ll l lm ln">import matplotlib.animation as animation<br/>import numpy as np<br/>from mpc import MPC<br/>import numpy as np<br/>import gym<br/>import mitdeeplearning as mdl</span></pre><p id="d6c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤4:运行MPC模型</p><pre class="jo jp jq jr fd ld le lf lg aw lh bi"><span id="e748" class="li lj hi le b fi lk ll l lm ln">env = gym.make(‘CartPole-v0’)<br/>env.seed(1)<br/> <br/>start_theta = 0 <br/>mpc = MPC(0.5,0,start_theta,0) <br/>action = 0<br/>for i_episode in range(1):<br/>   observation = env.reset()<br/>    for t in range(500):<br/>    env.render()<br/>    observation, reward, done, info = env.step(action)<br/>    a = mpc.update(observation[0] + 0.5, observation[1],     observation[2]+np.pi, observation[3])<br/>    env.env.force_mag = abs(a)<br/>    #print(a)<br/>    if a &lt; 0:<br/>       action = 0<br/>    else:<br/>       action = 1<br/>    if done:<br/>       pass</span></pre><blockquote class="kf kg kh"><p id="162d" class="if ig jd ih b ii ij ik il im in io ip ki ir is it kj iv iw ix kk iz ja jb jc hb bi translated">结果</p></blockquote><figure class="jo jp jq jr fd js er es paragraph-image"><div class="er es lw"><img src="../Images/c08e6b08843e74a8301ae4af04270aca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*MRzlsk2BMKicHN2SK3dKcw.gif"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">模拟结果</figcaption></figure><p id="2641" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的结果中，假设目标是保持极点平直，最大偏差为15度，我们可以看到RL比MPC给出了更令人满意的定性结果。RL成功地从一开始就保持极点笔直，而MPC在开始时失败，即使它在那之后开始保持极点笔直。</p><p id="d338" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这一结果肯定会让我们感到惊讶，因为我们需要有一个系统的数学模型，而这往往是控制工程师的一个痛点，所以我们花了很多精力来做MPC计算。当然，我们可以通过找到另一个数学模型或MPC参数来克服这个问题。但这里最重要的是RL给了我们更好的结果，即使我们不知道推车杆的数学模型，通过使用基于策略的策略，该算法成功地控制推车以保持杆笔直。</p><blockquote class="kf kg kh"><p id="43b3" class="if ig jd ih b ii ij ik il im in io ip ki ir is it kj iv iw ix kk iz ja jb jc hb bi translated">讨论</p></blockquote><p id="6163" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当然，从控制背景来看，我不能说RL完全克服了MPC，所以我们将来不需要任何控制理论。系统的模型仍然很重要，因为有了它，我们可以用李亚普诺夫定理[4]检查系统的稳定性。如果你能保证系统的稳定性，那么你就能保证你的控制器在将来发生意外时不会出故障。</p><p id="5c71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">尽管RL给了我们一个满意的结果，但是仍然有一些限制，因为我们不能证明RL算法的成功，直到我们模拟它。好的一面是，现在对可解释机器学习的研究正在发展。希望这将是对持怀疑态度的保守控制工程师在实际情况下实施RL的一个回答，特别是在对安全要求非常严格的工业情况下[5]。</p><blockquote class="kf kg kh"><p id="786e" class="if ig jd ih b ii ij ik il im in io ip ki ir is it kj iv iw ix kk iz ja jb jc hb bi translated">参考</p></blockquote><p id="1cad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[1]代码改编自MIT深度学习训练营:<a class="ae lb" href="https://github.com/aamini/introtodeeplearning" rel="noopener ugc nofollow" target="_blank">https://github.com/aamini/introtodeeplearning</a></p><p id="794b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]代码改编自Philip Zucker的博客:<a class="ae lb" href="http://www.philipzucker.com/model-predictive-control-of-cartpole-in-openai-gym-using-osqp/" rel="noopener ugc nofollow" target="_blank">http://www . Philip Zucker . com/model-predictive-control-of-cart pole-in-open ai-gym-using-osqp/</a></p><p id="4198" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3]https://en.wikipedia.org/wiki/Model_predictive_control<a class="ae lb" href="https://en.wikipedia.org/wiki/Model_predictive_control" rel="noopener ugc nofollow" target="_blank"/></p><p id="0b2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Lyapunov_stability" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Lyapunov_stability</a></p><p id="e5de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae lb" href="https://arxiv.org/pdf/1901.04592.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1901.04592.pdf</a></p></div></div>    
</body>
</html>