<html>
<head>
<title>Machine Learning in PySpark — Part 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark中的机器学习—第4部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-in-pyspark-part-4-5813e831922f?source=collection_archive---------11-----------------------#2020-07-12">https://medium.com/analytics-vidhya/machine-learning-in-pyspark-part-4-5813e831922f?source=collection_archive---------11-----------------------#2020-07-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/b1cbd2bd81403d5b8cec65e866e36fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*ml6bP0O8M8u2Frk0_YmYCQ.jpeg"/></div></figure><p id="f3aa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当我们步入大数据时代，每天都要处理大量的数据时，世界正在同时走向机器学习和人工智能。近年来，随着每天产生的数据量的增加，机器学习在几乎每个领域的重要性和流行程度都有了巨大的提升。特别是在当今，大多数商业企业都需要处理大量的数据，以便通过分析将数据转化为业务，从而更好地理解业务，从而找到有意义的见解来应对风险和机遇，从而使业务具有竞争优势。机器学习在这个过程中起着举足轻重的作用。</p><p id="111a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">行业通过创建统一的机器学习管道，使用部署在大型集群上的Apache Spark进行大规模数据分析。Spark的MLlib库有助于Spark上的机器学习，这是可能的。PySpark MLlib是Spark的机器学习库，充当PySpark核心的包装器，为机器学习提供一组统一的API，使用各种机器学习算法(如分类、回归、聚类等)的分布式实现来执行数据分析。</p><p id="de16" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这篇博客中，我们不会涉及理论概念。取而代之的是，我们会有一个实践环节，你可以通过编码来学习。我假设你有足够的机器学习知识作为先决条件，以获得最好的结果。让我们从Spark中的机器学习开始，使用它的MLlib库，通过创建一个简单的线性回归模型来使用房价数据集预测房价。虽然使用的数据集足够小，可以很好地使用Pandas来代替Spark，但为了简单演示机器学习如何在PySpark中完成，我们使用了这个数据集。数据集可以从<a class="ae jk" href="https://github.com/Anveshrithaa/housing-price-prediction-pyspark/blob/master/housing.csv" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><p id="5081" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们准备好了。我们开始吧！</p><h2 id="d5a1" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated"><strong class="ak">创建SparkContext </strong></h2><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="3afb" class="jl jm hi kl b fi kp kq l kr ks">from pyspark.sql import SparkSession</span><span id="fc5a" class="jl jm hi kl b fi kt kq l kr ks">spark = SparkSession.builder.master(“local”)\<br/>.appName(“Housing Price Prediction”)\<br/>.config(“spark.executor.memory”, “1gb”)\<br/>.getOrCreate()</span><span id="7843" class="jl jm hi kl b fi kt kq l kr ks">sc = spark.sparkContext</span></pre><h2 id="3fdf" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">数据探索和预处理</h2><p id="94c7" class="pw-post-body-paragraph im in hi io b ip ku ir is it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj hb bi translated">第一个也是最重要的一个步骤是数据预处理，将数据转换为干净可用的数据，以便对模型进行分析和训练。所使用的数据极大地决定了机器学习模型的性能。因此，数据探索、清理、预处理和特征工程有助于提高模型性能。</p><p id="78eb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">数据集包含9个属性的20，640个实例</p><ul class=""><li id="8c8d" class="kz la hi io b ip iq it iu ix lb jb lc jf ld jj le lf lg lh bi translated"><strong class="io hj"> <em class="li">经度</em> </strong></li><li id="156d" class="kz la hi io b ip lj it lk ix ll jb lm jf ln jj le lf lg lh bi translated"><strong class="io hj"> <em class="li">纬度</em> </strong></li><li id="1172" class="kz la hi io b ip lj it lk ix ll jb lm jf ln jj le lf lg lh bi translated"><strong class="io hj"> <em class="li">房屋中位年龄</em> </strong> —一个街区内房屋的中位年龄。较低的数字意味着较新的建筑</li><li id="7a86" class="kz la hi io b ip lj it lk ix ll jb lm jf ln jj le lf lg lh bi translated"><strong class="io hj"><em class="li"/></strong>—一个街区内的总房间数</li><li id="a57c" class="kz la hi io b ip lj it lk ix ll jb lm jf ln jj le lf lg lh bi translated"><strong class="io hj"> <em class="li">卧室总数</em> </strong> —一个街区内的卧室总数</li><li id="26b7" class="kz la hi io b ip lj it lk ix ll jb lm jf ln jj le lf lg lh bi translated"><strong class="io hj"> <em class="li">人口</em> </strong> —居住在一个街区内的总人数</li><li id="2802" class="kz la hi io b ip lj it lk ix ll jb lm jf ln jj le lf lg lh bi translated"><strong class="io hj"> <em class="li">户</em> </strong> —一个街区的总户数</li><li id="9d2e" class="kz la hi io b ip lj it lk ix ll jb lm jf ln jj le lf lg lh bi translated"><strong class="io hj"> <em class="li">收入中位数</em> </strong> —一个街区内家庭的收入中位数</li><li id="90e9" class="kz la hi io b ip lj it lk ix ll jb lm jf ln jj le lf lg lh bi translated"><strong class="io hj"> <em class="li">房屋价值中位数</em> </strong>(因变量)——街区内住户的房屋价值中位数</li><li id="9e93" class="kz la hi io b ip lj it lk ix ll jb lm jf ln jj le lf lg lh bi translated"><strong class="io hj"> <em class="li">海洋邻近度</em> </strong> —房子的位置w.r.t ocean</li></ul><p id="bfde" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">加载数据</strong></p><p id="dcfc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">首先，让我们将数据加载到Spark环境中。我们可以通过使用<em class="li"> spark.read.csv() </em>方法从CSV读取数据来直接创建PySpark数据帧(就像Pandas数据帧一样)。设置<em class="li"> header = True </em>将CSV的第一行作为标题。</p><p id="afba" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="li">注:</em> </strong> <em class="li"> DataFrame支持多种可以应用机器学习的数据类型。</em></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="7788" class="jl jm hi kl b fi kp kq l kr ks">myDF = spark.read.csv(‘C:/Users/a/pyspark/housing.csv’, header=True)</span></pre><p id="0478" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">作为第一步，让我们研究一下已经加载的数据。我们将花一些时间来检查它，以便理解数据集和数据的特征，这样它将有助于确定要执行的预处理步骤。</p><p id="d217" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">既然已经加载了数据，我们可以使用<em class="li"> printSchema() </em>方法来理解dataframe的模式。您也可以使用返回列名的<em class="li"> df.columns </em>来获取标题。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="f5e8" class="jl jm hi kl b fi kp kq l kr ks">myDF.printSchema()</span><span id="2087" class="jl jm hi kl b fi kt kq l kr ks">myDF.columns</span></pre><p id="0523" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="li">输出:</em> </strong></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="6dfb" class="jl jm hi kl b fi kp kq l kr ks">root<br/> |-- longitude: string (nullable = true)<br/> |-- latitude: string (nullable = true)<br/> |-- housing_median_age: string (nullable = true)<br/> |-- total_rooms: string (nullable = true)<br/> |-- total_bedrooms: string (nullable = true)<br/> |-- population: string (nullable = true)<br/> |-- households: string (nullable = true)<br/> |-- median_income: string (nullable = true)<br/> |-- median_house_value: string (nullable = true)</span><span id="5e04" class="jl jm hi kl b fi kt kq l kr ks">[‘longitude’, ‘latitude’, ‘housing_median_age’, ‘total_rooms’, ‘total_bedrooms’, ‘population’, ‘households’, ‘median_income’, ‘median_house_value’]</span></pre><p id="9a54" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这里，您可以看到所有的列都是字符串类型。但是为了更有效的分析，我们需要将列转换成浮点数据类型。为此，我们将创建一个函数，将dataframe中每一列的数据类型转换为float。<em class="li"> withColumn() </em>函数用于操作(重命名、更改值、转换数据类型)数据帧中的现有列或创建新列。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="b202" class="jl jm hi kl b fi kp kq l kr ks">from pyspark.sql.types import *</span><span id="0c41" class="jl jm hi kl b fi kt kq l kr ks">def convertColDatatype(df, colNames, newDatatype):<br/>      for colName in colNames:<br/>           df= df.withColumn(colName, df[colName].cast(newDatatype))<br/>      return df</span><span id="1e4c" class="jl jm hi kl b fi kt kq l kr ks">columns= myDF.columns<br/>myDF= convertColDatatype(myDF, columns, FloatType())<br/>myDF.printSchema()</span></pre><p id="7dfe" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="li">输出:</em> </strong></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="f51b" class="jl jm hi kl b fi kp kq l kr ks">root<br/> | — longitude: float (nullable = true)<br/> | — latitude: float (nullable = true)<br/> | — housing_median_age: float (nullable = true)<br/> | — total_rooms: float (nullable = true)<br/> | — total_bedrooms: float (nullable = true)<br/> | — population: float (nullable = true)<br/> | — households: float (nullable = true)<br/> | — median_income: float (nullable = true)<br/> | — median_house_value: float (nullable = true)</span></pre><p id="b9bc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，dataframe中的所有列都被转换为float数据类型。</p><p id="b99f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">此外，使用<em class="li"> show() </em>方法以表格格式显示数据帧(默认显示前20行),使用<em class="li"> select() </em>方法选择特定的列。您还可以尝试使用其他方法，如<em class="li"> take()、top()、head() </em>和<em class="li"> first() </em>来获取dataframe的行。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="4890" class="jl jm hi kl b fi kp kq l kr ks">myDF.show()</span></pre><p id="5724" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="li">输出:</em> </strong></p><figure class="kg kh ki kj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lo"><img src="../Images/d423df32e953682fb233dea84a96f166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y5DWk5GPSrCiuMZb0fqA6g.png"/></div></div></figure><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="43ea" class="jl jm hi kl b fi kp kq l kr ks">#Select and display first 5 rows of data from the selected columns<br/>myDF.select(‘total_bedrooms’, ‘median_income’).show(5)</span></pre><p id="0572" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="li">输出:</em> </strong></p><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/4d0157e6118da3f899664c667707f870.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*Xxn0X7FsXi_0gPcXASq_MA.png"/></div></figure><p id="629a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在dataframe上使用<em class="li"> count() </em>方法获得dataframe中的总行数。您可以随时使用<em class="li"> groupBy()、sort()、count()、distinct() </em>等。去探索更多。比方说，你想知道这些街区的10个最高的不同的中值房价。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="9305" class="jl jm hi kl b fi kp kq l kr ks">print(myDF.count())</span><span id="7f69" class="jl jm hi kl b fi kt kq l kr ks">myDF.sort(“median_house_value”,ascending=False)\<br/>.select(“median_house_value”).distinct().show(10)</span></pre><p id="41f3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="li">输出:</em> </strong></p><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/068d40a6f4cae5e8e95287bfbae5b02a.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*XwMk9kekaPXt1laso4wq_A.png"/></div></figure><p id="bd7b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在让我们使用<em class="li"> describe() </em>方法获得数据的摘要。这给出了每列的计数、平均值、标准偏差、最小值和最大值。这有助于我们深入了解数据的范围和分布等细节，这将有助于决定如何预处理数据。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="8f5c" class="jl jm hi kl b fi kp kq l kr ks">myDF.describe().show()</span></pre><figure class="kg kh ki kj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lv"><img src="../Images/32580c817f15cf922de7b5d3c64aa828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4exJsy9rizsUz93fzvrkWQ.png"/></div></div></figure><p id="59a4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从最小值和最大值中，我们可以看到几列中的数据分布在很大的值范围内。要解决这个问题，我们必须对数据进行规范化，这将在后面进行。</p><p id="8750" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">处理缺失值和空值</strong></p><p id="e8f0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了检查数据中是否有空值，让我们使用<em class="li"> isnull() </em>统计每一列中空值的数量并显示出来。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="4697" class="jl jm hi kl b fi kp kq l kr ks">from pyspark.sql.functions import *</span><span id="5f03" class="jl jm hi kl b fi kt kq l kr ks">myDF.select([count(when(isnull(c), c))\<br/>.alias(c) for c in myDF.columns]).show()</span></pre><figure class="kg kh ki kj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lw"><img src="../Images/578e048c62fb1f0cb2415d91f4370885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*njTzJFRgXLn_tnpT_PANVg.png"/></div></div></figure><p id="a0f2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这里，我们可以看到在“total _ thumbers”列中有200多个空值。为了解决这个问题，我们将用该列中可用值的平均值替换该列中的所有空值。从describe()函数的输出结果来看，列“total _ hydrokets”的平均值约为537。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="5577" class="jl jm hi kl b fi kp kq l kr ks">#replace null values in the column with the mean value</span><span id="3033" class="jl jm hi kl b fi kt kq l kr ks">myDF= myDF.fillna({‘total_bedrooms’: 537})</span><span id="7307" class="jl jm hi kl b fi kt kq l kr ks">myDF.select([count(when(isnull(c), c))\<br/>.alias(c) for c in myDF.columns]).show()</span></pre><figure class="kg kh ki kj fd ij er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lx"><img src="../Images/9b685da59d82bab4b76f7de59c968ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xk3XbSXQMi5WFhIxeZJ-IQ.png"/></div></div></figure><p id="6285" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，您可以看到我们的数据帧中没有任何空值。</p><p id="bb27" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">接下来，您会注意到目标变量'<em class="li"> median_house_value </em>'包含了相当大的值。为了修正这一点，我们用10万为单位来表示。</p><p id="c5aa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="li">注意:</em> </strong> <em class="li">虽然这是绝对没有必要做的，但这只是为了展示我们可以预处理数据的不同方式。即使你跳过这一步，它仍然是好的。</em></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="d163" class="jl jm hi kl b fi kp kq l kr ks">from pyspark.sql.functions import *</span><span id="cb57" class="jl jm hi kl b fi kt kq l kr ks">#Divide all the values in the ‘median_house_value’ column by 100,000</span><span id="4060" class="jl jm hi kl b fi kt kq l kr ks">myDF = myDF.withColumn(“median_house_value”, col(“median_house_value”)/100000)</span><span id="d4e5" class="jl jm hi kl b fi kt kq l kr ks">#myDF.take(2)</span></pre><p id="8661" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">此外，您还可以尝试通过从可用数据创建新要素并将它们添加到现有数据来进行要素工程。这里，我们有像<em class="li"> total_rooms </em>、<em class="li"> total_bedroom </em> s和<em class="li"> population </em>这样的变量，它们表示整个街区的累积值。从这些特征中，我们可以通过将可用特征的值(分别为人口、总卧室数和总房间数)除以该街区的总家庭数，得出每个街区的附加属性，如<em class="li">人口_每户</em>、<em class="li">卧室_每户</em>或<em class="li">房间_每户</em>。这将给出该街区每个家庭的平均卧室/房间/人数。为此，使用<em class="li"> select()、col() </em>和<em class="li"> withColumn() </em>方法来处理列。</p><figure class="kg kh ki kj fd ij"><div class="bz dy l di"><div class="ly lz l"/></div></figure><p id="da38" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">T39】输出:T41】</strong></p><blockquote class="ma mb mc"><p id="8151" class="im in li io b ip iq ir is it iu iv iw md iy iz ja me jc jd je mf jg jh ji jj hb bi translated">['经度'，'纬度'，'住房_中值_年龄'，'总房间'，'总卧室'，'人口'，'家庭'，'中值_收入'，'中值_住房_价值'，'人口_每个家庭'，'每个家庭房间'，'每个家庭卧室']</p></blockquote><p id="e5a1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">您可以看到新列被添加到数据帧中。虽然我们添加了一些新特性，但是我们也可以删除一些对于分析来说可能不必要或不有用的特性。我们将删除类似于<em class="li">纬度、经度、总房间数和总卧室数</em>的属性。您可以使用<em class="li"> df.drop() </em>方法来丢弃您不需要的列。这里，我们将使用<em class="li"> select() </em>方法来选择并重新排列dataframe中所需的列。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="87fb" class="jl jm hi kl b fi kp kq l kr ks">myDF= myDF.select(‘median_house_value’, ‘housing_median_age’, ‘median_income’, ‘population’, ‘households’, ‘population_per_household’, ‘rooms_per_household’)</span></pre><p id="cc93" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">既然我们已经对数据帧中的列进行了重新排序，现在是时候将数据帧中第一列的目标变量(<em class="li"> median_house_value </em>)与独立变量(目标变量以外的列)分开了。为此，我们将使用<em class="li"> map() </em>和<em class="li"> DenseVector() </em>将数据帧转换为包含目标变量和所有自变量的DenseVector的RDD。完成后，使用<em class="li"> createDataFrame() </em>将它转换回dataframe，它将数据作为第一个参数，将列名作为第二个参数。DenseVector存储值的数组。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="2b27" class="jl jm hi kl b fi kp kq l kr ks">from pyspark.ml.linalg import DenseVector</span><span id="49d2" class="jl jm hi kl b fi kt kq l kr ks">#x[0] is the target variable (label) and x[1:] are the features</span><span id="9eb9" class="jl jm hi kl b fi kt kq l kr ks">data = myDF.rdd.map(lambda x: (x[0], DenseVector(x[1:])))</span><span id="1471" class="jl jm hi kl b fi kt kq l kr ks">#creating a dataframe with columns ‘label’ (target variable) and #‘features’(dense vector of independent variables)</span><span id="9055" class="jl jm hi kl b fi kt kq l kr ks">myDF = spark.createDataFrame(data, [“label”, “features”])<br/>myDF.printSchema()</span></pre><p id="7656" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="li">输出:</em> </strong></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="0c0b" class="jl jm hi kl b fi kp kq l kr ks">root<br/> |-- label: double (nullable = true)<br/> |-- features: vector (nullable = true)</span></pre><p id="19d5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里，<em class="li">特征</em>列包含所有独立变量的密集向量(独立变量的值的数组)，而<em class="li">标签</em>列包含目标变量。</p><p id="1a9f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一旦完成，在继续构建模型之前，我们还有一个预处理步骤要执行。正如我们已经看到的，少数几列中的值的范围很大，我们必须使用<em class="li"> StandardScalar </em>转换数据，使平均值为0，标准偏差为1，从而使数据标准化。这里，输入列是要缩放的要素，输出列是缩放后的要素，将作为第三列包含在数据框中。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="550a" class="jl jm hi kl b fi kp kq l kr ks">from pyspark.ml.feature import StandardScaler</span><span id="279a" class="jl jm hi kl b fi kt kq l kr ks">ss= StandardScaler(inputCol=”features”, outputCol=”scaled_features”)</span><span id="d03e" class="jl jm hi kl b fi kt kq l kr ks">scaler = ss.fit(myDF)</span><span id="6bde" class="jl jm hi kl b fi kt kq l kr ks">myDF = scaler.transform(myDF)</span><span id="ea72" class="jl jm hi kl b fi kt kq l kr ks">myDF.printSchema()</span><span id="5721" class="jl jm hi kl b fi kt kq l kr ks">myDF.take(1)</span></pre><p id="94b8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="li">输出:</em> </strong></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="4ad4" class="jl jm hi kl b fi kp kq l kr ks">root<br/> | — label: double (nullable = true)<br/> | — features: vector (nullable = true)<br/> | — scaled_features: vector (nullable = true)</span><span id="0125" class="jl jm hi kl b fi kt kq l kr ks">[Row(label=4.526, features=DenseVector([41.0, 8.3252, 322.0, 126.0, 2.5556, 6.9841]), scaled_features=DenseVector([3.2577, 4.3821, 0.2843, 0.3296, 0.2461, 2.8228]))]</span></pre><p id="6fd6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，数据预处理已经完成，我们终于可以开始构建机器学习模型了。到目前为止，您应该已经理解了探索性数据分析如何帮助我们收集有关数据的信息，以便进行进一步的预处理。对数据的简单探索性分析让我们对数据的结构有了一个简单的了解，这有助于我们决定预处理步骤，使数据更清晰，更适合模型。</p><h2 id="2b3e" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">构建机器学习模型</h2><p id="4471" class="pw-post-body-paragraph im in hi io b ip ku ir is it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj hb bi translated">现在是我们开始构建机器学习模型的时候了。由于这是一个回归问题，我们将使用一个简单的线性回归模型。在我们开始创建模型之前，让我们使用<em class="li"> randomSplit() </em>将预处理的数据分成训练集和测试集。您可以遵循80:20或75:25的训练/测试分流比率。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="b165" class="jl jm hi kl b fi kp kq l kr ks">train, test = myDF.randomSplit([.8,.2],seed=1)</span></pre><p id="b02c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了初始化线性回归模型，让我们使用默认参数。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="66b1" class="jl jm hi kl b fi kp kq l kr ks">from pyspark.ml.regression import LinearRegression</span><span id="aa9b" class="jl jm hi kl b fi kt kq l kr ks">linearReg= LinearRegression(featuresCol= “scaled_features”, labelCol=”label”)</span><span id="c978" class="jl jm hi kl b fi kt kq l kr ks">#fit the model to the the training data<br/>model=linearReg.fit(train)</span><span id="d494" class="jl jm hi kl b fi kt kq l kr ks">#make predictions on the test set<br/>predictions= model.transform(test)</span><span id="348a" class="jl jm hi kl b fi kt kq l kr ks"># show the predicted values and the actual values<br/>predictions.select(“prediction” ,“label”).show(10)</span></pre><p id="b251" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="li">输出:</em> </strong></p><figure class="kg kh ki kj fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/366a41e88ca2ef60316eb8ba9ddf33a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*3YLDvVfQ_IXXVdW_VQO-4w.png"/></div></figure><h2 id="e181" class="jl jm hi bd jn jo jp jq jr js jt ju jv ix jw jx jy jb jz ka kb jf kc kd ke kf bi translated">评估模型</h2><p id="854b" class="pw-post-body-paragraph im in hi io b ip ku ir is it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj hb bi translated">我们可以通过查看一些性能指标来了解该模型的效率。为了评估线性回归模型的性能，我们将使用<em class="li"> RMSE </em>(均方根误差)和<em class="li"> R2 </em>分数(R平方)，前者给出实际值和预测值之间误差的绝对度量，后者给出模型拟合优度的信息。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="06a0" class="jl jm hi kl b fi kp kq l kr ks">#R2 score on test set<br/>r2_test= model.evaluate(test).r2<br/>print(“R2 score on test set: “, r2_test)</span><span id="b60b" class="jl jm hi kl b fi kt kq l kr ks">from pyspark.ml.evaluation import RegressionEvaluator</span><span id="e71e" class="jl jm hi kl b fi kt kq l kr ks">#RMSE on test set<br/>evaluator = RegressionEvaluator(predictionCol=”prediction”, labelCol=”label”,metricName=”rmse”)</span><span id="ba55" class="jl jm hi kl b fi kt kq l kr ks">rmse_test = evaluator.evaluate(predictions)</span><span id="21be" class="jl jm hi kl b fi kt kq l kr ks">print(“RMSE on test set: “, rmse_test)</span></pre><p id="b3d0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="li">输出:</em> </strong></p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="cae1" class="jl jm hi kl b fi kp kq l kr ks">R2 score on test set:  0.5687396999670697<br/>RMSE on test set:  0.7590445544382896</span></pre><p id="74fa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个模型的性能不是很好。但是，我们可以通过进一步预处理数据来提高模型性能，如去除异常值、转换倾斜特征、通过主成分分析提取特征等。我们还可以调整模型的超参数，以获得更好的性能。</p></div><div class="ab cl mh mi gp mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hb hc hd he hf"><p id="64f6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">希望这能让你在PySpark的机器学习中有一个好的开始。虽然这只是一个简单的回归问题，但你也可以使用Spark的机器学习库提供的各种不同的监督和非监督机器学习算法来执行聚类和分类。现在您已经知道了这是如何工作的，您可以使用pandas和sklearn尝试一些机器学习任务的PySpark实现，这样您会有更好的理解。为您选择的数据集构建简单的机器学习模型，并尝试提高它们的性能。</p><p id="2d19" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">另外，PySpark中有一个简单的电影推荐系统，它使用交替最小二乘法进行协同过滤，这是一种非常流行的推荐算法。为此，我们将使用Spark的机器学习库和movielens 100k数据集(我们已经在之前的PySpark RDD博客中使用过)来训练模型，为给定的用户ID推荐10部电影。PySpark实现和数据集可以在<a class="ae jk" href="https://github.com/Anveshrithaa/Apache-Spark-Projects/tree/master/movies-recommendation" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="c93d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在本系列的下一篇博客中，我们将建立一个端到端的机器学习管道，不是在我们的本地机器上，而是在基于云的统一分析平台——data bricks上，以获得使用Apache Spark进行大规模数据处理、分析和机器学习的真实感受。您还将了解AWS和更多内容，了解数据科学家和工程师如何在现实世界中处理大量数据。准备好学习新的令人兴奋的东西。不要错过！点击查看下一篇博客<a class="ae jk" rel="noopener" href="/@anveshrithaas/end-to-end-machine-learning-pipeline-on-databricks-part-5-c10273e2cd88">。</a></p></div><div class="ab cl mh mi gp mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hb hc hd he hf"><p id="968b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">查看本系列中的其他博客</p><p id="cb9e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" rel="noopener" href="/@anveshrithaas/getting-started-with-apache-spark-part-1-91b379204ae0"> <strong class="io hj"> <em class="li">第1部分Apache Spark入门</em> </strong> </a></p><p id="c258" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" rel="noopener" href="/@anveshrithaas/introduction-to-pyspark-part-2-6d6113e31592"> <strong class="io hj"> <em class="li">第二部分PySpark简介</em> </strong> </a></p><p id="b2b1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" rel="noopener" href="/@anveshrithaas/understanding-spark-rdds-part-3-3b1b9331652a"> <strong class="io hj"> <em class="li">第三部分——了解星火RDDs </em> </strong> </a></p><p id="9e9f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" rel="noopener" href="/@anveshrithaas/end-to-end-machine-learning-pipeline-on-databricks-part-5-c10273e2cd88"> <strong class="io hj"> <em class="li">第五部分——数据块上的端到端机器学习流水线</em> </strong> </a></p></div></div>    
</body>
</html>