<html>
<head>
<title>Word2Vector using Gensim</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Gensim的Word2Vector</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/word2vector-using-gensim-e055d35f1cb4?source=collection_archive---------1-----------------------#2019-10-27">https://medium.com/analytics-vidhya/word2vector-using-gensim-e055d35f1cb4?source=collection_archive---------1-----------------------#2019-10-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="4f0c" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">简介:</h1><p id="8eba" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj">什么是Word2Vec？</strong></p><ul class=""><li id="1ced" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">通俗地说，就是以语料库为输入，以向量的形式输出的算法。</li><li id="0278" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">一点技术定义:Word2Vec是形成/创建单词嵌入的模型。这是一种表示单词的现代方法，其中每个单词由一个向量(基于嵌入大小的数字数组)表示。向量只不过是神经元的权重，所以如果我们将神经元的大小设置为100，那么我们将有100个权重，这些权重就是我们的单词嵌入或简单的密集向量。</li><li id="c927" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">输入字必须是一个热编码。例如:</li></ul><figure class="ks kt ku kv fd kw er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es kr"><img src="../Images/e0bd116eedcc4dc17d32e51965917889.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w618R-zIRcvxLXeQ"/></div></div></figure><p id="9dcc" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated"><strong class="jf hj">但为什么是Word2Vec呢？</strong></p><ul class=""><li id="3d51" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">Word2Vec发现单词之间的关系(语义或句法),这是我们传统的TF-IDF或基于频率的方法所不能实现的。当我们训练模型时，每一个热编码单词都在一个维度空间中获得一个点，在那里它学习并分组具有相似含义的单词。</li><li id="1ed1" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">这里结合的神经网络是一个浅层的。</li><li id="01b3" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">这里需要注意的一点是，我们需要将大量文本数据传递到Word2Vec模型中，以便找出单词之间的关系或生成有意义的结果。</li><li id="7741" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">一般来说，Word2Vec是基于窗口方法的，我们必须指定一个窗口大小。</li></ul><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es lg"><img src="../Images/2bad543ac2b69f8531e80fef7af9ba4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/0*nTWCeWnK_tkp0H6_"/></div></figure><ul class=""><li id="ba61" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">在上面的可视化表示中，窗口大小设置为1。因此，来自目标双方的1个字都被考虑。类似地，在每次迭代中，窗口将滑动一步，我们的邻居将不断变化。</li><li id="685b" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">有两种类型的算法:CBOW和Skip-gram。(随着我们的进展，我们将看到它的细节)</li></ul><h1 id="c83a" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">数据收集:</h1><h2 id="2439" class="lh ig hi bd ih li lj lk il ll lm ln ip jo lo lp it js lq lr ix jw ls lt jb lu bi translated">来源:</h2><ul class=""><li id="87fa" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">我们使用了Kaggle网站上的“一袋文字和一袋爆米花”中的一组未标记的数据:【https://www.kaggle.com/c/word2vec-nlp-tutorial/data T4】</li></ul><h2 id="f7ba" class="lh ig hi bd ih li lj lk il ll lm ln ip jo lo lp it js lq lr ix jw ls lt jb lu bi translated">详细信息:</h2><ul class=""><li id="6042" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">unlabeledTrainData没有标签的额外训练集。制表符分隔的文件有一个标题行，后跟50，000行，包含每个评论的id和文本。</li></ul><h2 id="e441" class="lh ig hi bd ih li lj lk il ll lm ln ip jo lo lp it js lq lr ix jw ls lt jb lu bi translated"><strong class="ak">数据字段:</strong></h2><ul class=""><li id="2635" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">id:每个审核的唯一ID</li><li id="4cbd" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">审查:审查的文本</li></ul><h2 id="b51b" class="lh ig hi bd ih li lj lk il ll lm ln ip jo lo lp it js lq lr ix jw ls lt jb lu bi translated">目标:</h2><ul class=""><li id="6193" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">目标是在Gensim的帮助下建立Word2Vec单词嵌入模型。</li></ul><h1 id="a0a5" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">Gensim:</h1><ul class=""><li id="c105" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">Gensim是相当容易使用的模块，它继承了CBOW和Skip-gram。</li><li id="6564" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">我们可以用<strong class="jf hj">安装！pip在Jupyter笔记本上安装gensim </strong>。</li><li id="a825" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">实现Word2Vec的另一种方法是从头开始构建，这相当复杂。</li><li id="25e5" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">阅读更多关于Gensim的信息:<a class="ae ly" href="https://radimrehurek.com/gensim/index.html" rel="noopener ugc nofollow" target="_blank">https://radimrehurek.com/gensim/index.html</a></li><li id="f17e" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated"><strong class="jf hj">仅供参考，Gensim是由NLP研究员拉迪姆·řehůřek和他的公司RaRe Technologies开发和维护的。</strong></li></ul><h1 id="f313" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">加载包和数据:</h1><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="9a1a" class="lh ig hi ma b fi me mf l mg mh"><em class="mi">#Import packages<br/></em><strong class="ma hj">import</strong> <strong class="ma hj">pandas</strong> <strong class="ma hj">as</strong> <strong class="ma hj">pd<br/>import</strong> <strong class="ma hj">gensim</strong></span><span id="6794" class="lh ig hi ma b fi mj mf l mg mh"><em class="mi">#import beautiful soup , regex<br/></em><strong class="ma hj">from</strong> <strong class="ma hj">bs4</strong> <strong class="ma hj">import</strong> BeautifulSoup<br/><strong class="ma hj">import</strong> <strong class="ma hj">re</strong>, <strong class="ma hj">string</strong></span><span id="04b1" class="lh ig hi ma b fi mj mf l mg mh"><em class="mi">#import warnings<br/>#warnings.filterwarnings(action='ignore')</em></span></pre><ul class=""><li id="49ef" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">上面我们使用漂亮的汤库来消除HTML标签。另一种方法是我们可以使用正则表达式来消除标签。</li></ul><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="02f9" class="lh ig hi ma b fi me mf l mg mh"><em class="mi">#reading dataset</em><br/><br/>df = pd.read_csv('unlabeledTrainData.tsv', header=0, delimiter='<strong class="ma hj">\t</strong>', quoting=3)<br/>df.head()</span></pre><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es mk"><img src="../Images/769aee28e214024a389589b418a4069e.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*u4_C0IfSvy290EzDpn7hrA.png"/></div></figure><ul class=""><li id="234e" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">数据集的形状是50000个例子，具有2个属性，即id和review。</li></ul><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="f3a5" class="lh ig hi ma b fi me mf l mg mh">'"Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \\"Hey, let\'s pool our money together and make a really bad movie!\\" Or something like that. What ever they said, they still ended up making a really bad movie--dull story, bad script, lame acting, poor cinematography, bottom of the barrel stock music, etc. All corners were cut, except the one that would have prevented this film\'s release. Life\'s like that."'</span></pre><ul class=""><li id="d7fe" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">我们可以观察到我们的语料库可能包含HTML标签、特殊字符等。</li><li id="de95" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">我们需要通过消除这些标签和字符来清理语料库。</li></ul><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="3c3d" class="lh ig hi ma b fi me mf l mg mh"><em class="mi">#empty list</em><br/>clean = [ ]<br/><br/><em class="mi">#for loop to clean dataset while removing html tags, special characters</em><br/><strong class="ma hj">for</strong> doc <strong class="ma hj">in</strong> df['review']:<br/>    x = doc.lower()                     <em class="mi">#lowerthe case</em><br/>    x = BeautifulSoup(x, 'lxml').text   <em class="mi">#html tag removal</em><br/>    x = re.sub('[^A-Za-z0-9]+', ' ', x) <em class="mi">#separate words by space</em><br/>    clean.append(x)<br/><br/><em class="mi">#assigning clean list to new attribute</em><br/>df['clean'] = clean</span></pre><ul class=""><li id="33a6" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">上面我们使用漂亮的汤库来消除HTML标签。另一种方法是我们可以使用正则表达式来消除标签。</li></ul><figure class="ks kt ku kv fd kw er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es ml"><img src="../Images/12f0cbb415d3ae87239d1fde5b112787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mV18ZNUtw_Sv00VXfiQzZg.png"/></div></div></figure><ul class=""><li id="cfb2" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">最后，我们将clean集合添加到list中，并形成一个新的属性clean。</li></ul><h1 id="e29a" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">(a)连续单词包(CBOW):</h1><ul class=""><li id="0f92" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">在这里，邻近单词被提供作为预测目标的输入。换句话说，提供上下文作为预测目标的输入。例如:</li><li id="2d6d" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">让我们用视觉表现来看看这个:</li></ul><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es mm"><img src="../Images/00d41701014050fe3776f8cf50e13b4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/0*MSh8GQGo4k1-uZsO"/></div></figure><ul class=""><li id="af0a" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">我们可以观察到，<strong class="jf hj"> W(t) </strong>在给定上下文/相邻单词作为输入的情况下被预测。基于馈送到网络的输入来估计目标单词的p值。</li><li id="a46f" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">我们这里有一个浅层神经网络。嵌入的每个单词或单个密集向量的大小取决于我们拥有的神经元数量。</li><li id="5d43" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">这些向量是每个神经元学习的权重。</li><li id="566c" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">无论单词有多大或多小，它都将由我们设置的嵌入大小来表示。</li></ul><p id="6e1a" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated"><strong class="jf hj">优势:</strong></p><ul class=""><li id="154b" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">CBOW速度更快，并且更好地表示频繁出现的单词。</li><li id="6770" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">当谈到内存利用率时，CBOW倾向于消耗低内存。</li></ul><p id="5528" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated"><strong class="jf hj">缺点:</strong></p><ul class=""><li id="97af" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">无法表示不常用的单词。</li><li id="642c" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">需要大量的文本数据。(<strong class="jf hj"> <em class="mi">注:不能说是缺点，因为它消耗内存少，但值得一提</em> </strong>)</li></ul><h1 id="7a62" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">(b)跳格图:</h1><ul class=""><li id="ebe1" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">Skip-gram与CBOW相反/相反，其中提供目标单词作为输出，以便预测上下文/相邻单词。</li><li id="6fdc" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">让我们用视觉表现来看看这个:</li></ul><figure class="ks kt ku kv fd kw er es paragraph-image"><div class="er es mn"><img src="../Images/34d651f82e7047407fefe9984d3c35d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/0*Y4O2tC4piSUN_QaK"/></div></figure><ul class=""><li id="e1b3" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">这里，一个目标词被输入到我们的浅层神经网络，权重从隐含层学习。从相邻单词中随机挑选一个单词(基于我们的窗口大小)。此外，对于与我们输入给网络的输入单词接近的上下文单词，正在计算P值。</li></ul><h2 id="8ea3" class="lh ig hi bd ih li lj lk il ll lm ln ip jo lo lp it js lq lr ix jw ls lt jb lu bi translated"><strong class="ak">优点:</strong></h2><ul class=""><li id="e84e" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">Skip-gram适用于较小的数据集。</li><li id="570e" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">它能够很好地表示生僻字。</li></ul><h2 id="051d" class="lh ig hi bd ih li lj lk il ll lm ln ip jo lo lp it js lq lr ix jw ls lt jb lu bi translated"><strong class="ak">缺点:</strong></h2><ul class=""><li id="1e6c" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">如果数据集很大，训练速度会很慢。</li><li id="43ae" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">并且不是存储器有效的。</li></ul><h1 id="e6c0" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">我们可能会面临什么问题？</h1><ul class=""><li id="1013" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">这两种技术都基于概率估计。</li><li id="93cf" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">让我们考虑一个例子，我们有一个1000字的语料库。我们需要预测目标单词(CBOW)或上下文(Skip-gram)。现在，我们指定一个固定的窗口大小来在语料库中滑动。假设我们的窗口大小是1，即在CBOW的情况下，输入将是2个单词(目标两侧的单词)，而在Skip-gram的情况下，输入将是单个单词，以便预测上下文(2个单词)。</li><li id="51eb" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">在这种情况下，将对特定窗口内的单词进行概率估计，并且这些概率将高于远离邻域的单词的概率。</li><li id="480d" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">计算巨大语料库(例如:数百万个单词)的概率在计算上将是昂贵的，因为算法将在每次迭代中估计所有单词的概率。在这种情况下，模型也可能给出不相关的结果。因此，为了克服这个问题<strong class="jf hj">，提出了负采样</strong>。</li><li id="7bcb" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">在<strong class="jf hj">负抽样</strong>中，概率是在我们的固定窗口大小内为单词估计的，就像以前一样，但只有少数随机单词被选择出固定窗口。通过这种方式，p值估计的负荷与以前相比降低了。训练速度更快，效果更满意。</li></ul><h2 id="2728" class="lh ig hi bd ih li lj lk il ll lm ln ip jo lo lp it js lq lr ix jw ls lt jb lu bi translated">CBOW或Skip-gram:何时选择哪一个？</h2><ul class=""><li id="508a" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">这取决于问题的性质。</li><li id="f786" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">两者都有自己的一套好处。</li></ul><h1 id="d80a" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">Word2Vec建模</h1><ul class=""><li id="502b" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">此外，我们将看看如何实现Word2Vec并获得密集向量。</li></ul><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="b757" class="lh ig hi ma b fi me mf l mg mh"><em class="mi">#Word2vec implementation</em></span><span id="0942" class="lh ig hi ma b fi mj mf l mg mh">model = gensim.models.Word2Vec(docs,<br/>                              min_count=10, <br/>                              workers=4,<br/>                              size=50,<br/>                              window=5,<br/>                              iter = 10)</span></pre><p id="63ac" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated">这里有几个可以使用的参数:</p><ul class=""><li id="466d" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated"><strong class="jf hj">句子:</strong>句子接受一个令牌列表列表。(如果是CBOW，最好有大的文件列表)</li><li id="7db8" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated"><strong class="jf hj">大小:</strong>要并入隐藏层的神经元数量或单词嵌入的大小。默认情况下，其设置为100。</li><li id="79f9" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated"><strong class="jf hj">窗口:</strong>窗口大小或围绕目标考虑的字数。如果size = 1，则将考虑两边的1个字。默认情况下，5是固定的窗口大小。</li><li id="4bfc" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated"><strong class="jf hj">最小计数:</strong>默认值为5。基于提到的最小计数的不频繁的单词将被忽略。(<em class="mi">我们选择了10作为阈值</em>)</li><li id="1a3d" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated"><strong class="jf hj"> workers : </strong>一次使用的CPU线程数，用于更快的训练。</li><li id="b3e1" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated"><strong class="jf hj"> sg : </strong>不是0就是1。默认值为0或CBOW。必须通过传递1来显式定义Skip-gram。</li><li id="3146" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated"><strong class="jf hj"> iter : </strong>迭代次数/历元数。</li></ul><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="a324" class="lh ig hi ma b fi me mf l mg mh"><em class="mi">#vocab size</em><br/>len(model.wv.vocab.keys())</span></pre><ul class=""><li id="ae97" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">除去少于10个的单词后，我们的词汇表中还有28656个单词可供使用。</li><li id="122d" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">并且每个单词会用50个数字/权重来表示(这是我们的嵌入大小)。</li></ul><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="cd9c" class="lh ig hi ma b fi me mf l mg mh"><em class="mi">#uncomment to view vocabulary</em><br/><em class="mi">#model.wv.vocab</em></span></pre><ul class=""><li id="251c" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">上面这段代码将揭示完整的词汇。</li></ul><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="25a0" class="lh ig hi ma b fi me mf l mg mh"><em class="mi">#word2vector representation</em><br/>model.wv['great']</span></pre><h2 id="02c3" class="lh ig hi bd ih li lj lk il ll lm ln ip jo lo lp it js lq lr ix jw ls lt jb lu bi translated">输出:</h2><p id="c3b9" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">数组([-2.7726305，-1.0800452，-1.788979，2.340867，-0.32861072，0.0651653，1.6166486，-3.2617207，-3.6233435，-3.32576，-1.7012835，1.0813012，-0.2412</p><ul class=""><li id="6876" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">上面我们可以观察到“伟大”这个词是用50个权重来表示的。</li></ul><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="ae5d" class="lh ig hi ma b fi me mf l mg mh"><em class="mi">#find similar words to the given word</em><br/>model.wv.most_similar('dumb')</span></pre><h2 id="0f9d" class="lh ig hi bd ih li lj lk il ll lm ln ip jo lo lp it js lq lr ix jw ls lt jb lu bi translated">输出:</h2><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="ca7e" class="lh ig hi ma b fi me mf l mg mh">[('stupid', 0.899315595626831),<br/> ('lame', 0.8604525923728943),<br/> ('silly', 0.8508281111717224),<br/> ('generic', 0.7499771118164062),<br/> ('pathetic', 0.7478170394897461),<br/> ('retarded', 0.7461548447608948),<br/> ('corny', 0.7456734776496887),<br/> ('cheesy', 0.7454365491867065),<br/> ('ridiculous', 0.7351659536361694),<br/> ('bad', 0.7300432920455933)]</span></pre><ul class=""><li id="03f4" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">关于Gensim的一个美丽的事情是，我们有这样的方法，可以给我们以最高的概率排列相似的单词。</li><li id="c25e" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">计算单词之间的余弦相似度，以找到最相似的单词。</li><li id="a614" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">当我们经过“dumb”时，我们可以观察到大多数相似的单词是如何通过它们各自的P值对齐的。</li><li id="0238" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">鉴于“哑”，大多数类似的词是愚蠢，跛脚，傻等。</li></ul><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="c637" class="lh ig hi ma b fi me mf l mg mh"><em class="mi">#words which doesn't match</em><br/>model.wv.doesnt_match('house rent trust apartment'.split())</span></pre><h2 id="2062" class="lh ig hi bd ih li lj lk il ll lm ln ip jo lo lp it js lq lr ix jw ls lt jb lu bi translated">输出:</h2><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="cdd3" class="lh ig hi ma b fi me mf l mg mh">'trust'</span></pre><ul class=""><li id="8fdb" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">我们还可以识别哪些单词关系不密切。</li><li id="aa49" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">上面我们通过了“房子，租金，信托，公寓”，输出是“信托”——这显然是令人满意的。</li></ul><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="9e12" class="lh ig hi ma b fi me mf l mg mh"><em class="mi">#arithmetic operations</em><br/>model.most_similar(positive=['woman','man'], negative=['king'])[0]</span></pre><h2 id="740b" class="lh ig hi bd ih li lj lk il ll lm ln ip jo lo lp it js lq lr ix jw ls lt jb lu bi translated">输出:</h2><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="cef1" class="lh ig hi ma b fi me mf l mg mh">('girl', 0.7452774047851562)</span></pre><ul class=""><li id="9844" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">上面我们已经完成了基本的算术问题。</li><li id="43bd" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">如果女人+男人=国王-？</li><li id="70c0" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">输出是p值为0.74的“女孩”，这是可以接受的，如果我们有更大的语料库，那么我们可能会得到作为“女王”的输出。</li></ul><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="2b6c" class="lh ig hi ma b fi me mf l mg mh"><em class="mi">#word embeddings</em><br/>model.wv.vectors</span></pre><h2 id="d921" class="lh ig hi bd ih li lj lk il ll lm ln ip jo lo lp it js lq lr ix jw ls lt jb lu bi translated">输出:</h2><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="e74b" class="lh ig hi ma b fi me mf l mg mh">array([[-1.0976436 , -1.0300254 ,  1.0157741 , ...,  2.0884573 ,<br/>         1.2929492 , -0.50073916],<br/>       [-0.10799529,  1.1408263 ,  0.23857029, ..., -0.6748595 ,<br/>         1.4341863 , -0.03276591],<br/>       [-0.67246455,  1.097886  ,  0.97168344, ..., -0.7108262 ,<br/>        -0.20907082, -0.37925354],<br/>       ...,<br/>       [ 0.44054744,  0.13000782,  0.04757666, ...,  0.09963967,<br/>         0.2213286 , -0.13963003],<br/>       [ 0.42221656,  0.37493396,  0.28681585, ..., -0.1619114 ,<br/>         0.2634282 ,  0.0619109 ],<br/>       [ 0.43446574,  0.11213642,  0.05965824, ..., -0.02202358,<br/>         0.1439297 , -0.01608269]], dtype=float32)</span></pre><ul class=""><li id="e67f" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">上面是我们的50维数组。</li><li id="dd6c" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">我们可以在嵌入层的帮助下进一步将其用于情感分析。</li></ul><h1 id="1ead" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">保存和加载模型:</h1><ul class=""><li id="0d48" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">我们可以将我们的模型保存为“<name>”。bin”文件并查看内容。</name></li></ul><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="3f92" class="lh ig hi ma b fi me mf l mg mh"><em class="mi">#saving model</em><br/>model.wv.save('word2vector-50.bin')</span></pre><ul class=""><li id="359e" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">此外，我们还可以使用load()方法加载模型。</li></ul><pre class="ks kt ku kv fd lz ma mb mc aw md bi"><span id="db3f" class="lh ig hi ma b fi me mf l mg mh">#loading model<br/>model = gensim.models.Word2Vec.load('word2vec-50.bin')</span></pre><ul class=""><li id="da72" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">我们也可以从这里下载文件来加载Google Word2Vec模型:<a class="ae ly" href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit" rel="noopener ugc nofollow" target="_blank">https://drive . Google . com/file/d/0b 7 xkcwpi 5 kdynlnuttlss 21 pqmm/edit</a></li><li id="dceb" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">这是谷歌预先训练的模型，它是在谷歌新闻数据上训练的，其中每个词由300个嵌入大小表示。</li><li id="c444" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">有许多这样的预训练模型可用，仅举几个例子，如Stanford的GloVe(单词表示的全局向量),脸书人工智能研究实验室的FastText。人们可以加载它们并检查哪个工作得更好。</li></ul><h1 id="957f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结论</h1><ul class=""><li id="776d" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">在这个案例研究中，我们详细学习了CBOW和Skip-gram。</li><li id="c11a" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">我们也看到了我们所面临的障碍，以及研究人员如何通过添加负采样来克服它。</li><li id="99a6" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">我们通过传递一个干净的标记化的语料库来训练模型。</li><li id="ab30" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">最后，我们看了如何保存/加载模型，并在将来使用它。</li></ul><h1 id="19ea" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">下一步是什么？</h1><ul class=""><li id="8727" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">在嵌入层的帮助下，我们可以使用单词嵌入作为情感分析的特征。</li></ul><h1 id="b113" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">参考资料:</h1><ul class=""><li id="44e1" class="kb kc hi jf b jg jh jk jl jo lv js lw jw lx ka ki kj kk kl bi translated">在这里阅读快速文本:<a class="ae ly" href="https://fasttext.cc/" rel="noopener ugc nofollow" target="_blank">https://fasttext.cc/</a></li><li id="7164" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">在这里阅读手套:<a class="ae ly" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a></li><li id="fe84" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">在这里阅读关于谷歌新闻的word 2 vec:<a class="ae ly" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank">https://code.google.com/archive/p/word2vec/</a></li><li id="6b43" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">维基百科:<a class="ae ly" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Word2vec</a></li><li id="33a3" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated"><strong class="jf hj">提及:</strong>此处使用的图片来自各种来源，如ResearchGate、MiroMedium、RaRe Technologies等。</li></ul><p id="2133" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated"><strong class="jf hj">注:<em class="mi"> </em> </strong> <em class="mi">如有错误，请指正。如果你觉得有帮助，请分享。</em></p><p id="c2e3" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo ld jq jr js le ju jv jw lf jy jz ka hb bi translated"><strong class="jf hj"> <em class="mi">多谢。</em>T9】</strong></p></div></div>    
</body>
</html>