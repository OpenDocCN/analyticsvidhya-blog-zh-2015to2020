<html>
<head>
<title>Implementing DeepPose Baseline HPE model with Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Keras实现深度基线HPE模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementing-deeppose-baseline-model-with-keras-67f8c8ab63c1?source=collection_archive---------9-----------------------#2020-09-07">https://medium.com/analytics-vidhya/implementing-deeppose-baseline-model-with-keras-67f8c8ab63c1?source=collection_archive---------9-----------------------#2020-09-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/20cfa25d2c4848e14775d76e0eab25e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U-OqLnAeACy5gW2JXao50g.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">威尔逊·蒙托亚在<a class="ae iu" href="https://unsplash.com/photos/XqQwigfqje4" rel="noopener ugc nofollow" target="_blank">的照片</a></figcaption></figure><h1 id="fc4c" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">动机</h1><p id="edca" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">DeepPose论文是基于深度学习的HPE(人体姿态估计)的基础论文之一。我，作为一个在HPE复习以前关于HPE的论文的学生，不得不阅读这篇论文。</p><h1 id="526d" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">什么是HPE</h1><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/3a3aac5725cc925c1eed949097b60580.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1jtfxtet2-Ono-yc4qjMMw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">2D人体姿态估计的例子</figcaption></figure><p id="b53b" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">姿态估计是通过基于2D/3D镜头定位关节来预测人体姿态的任务。我们可以用不同的方法来解决这个问题，比如回归与检测，自顶向下与自底向上，以及生成与鉴别。HPE的各种变体和方法将在本系列关于姿态估计的后续故事中描述。</p><h1 id="61a6" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">准备数据(LSP扩展)</h1><p id="01dc" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><a class="ae iu" href="https://sam.johnson.io/research/lspet.html" rel="noopener ugc nofollow" target="_blank"> LSP数据集</a>将用于训练我们的模型。在url中下载zip存档文件并解压缩文件。该数据集包含一个图像文件夹，其中有10，000张不同尺寸的运动员图像。图像的关节在joints.mat文件中被标记为每个像素的像素坐标。我们将所有的图片重新整形为统一的大小(220，220)，并将所有的图片合并成一个numpy文件。</p><pre class="ks kt ku kv fd lb lc ld le aw lf bi"><span id="7254" class="lg iw hi lc b fi lh li l lj lk">import cv2<br/>import os<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import scipy.io<br/>import progressbar</span><span id="f901" class="lg iw hi lc b fi ll li l lj lk">base_directory='YOUR DIRECTORY TO DATA\\lspet_dataset\\images'<br/>images_directory=os.listdir(base_directory)<br/>target_size=(220,220)<br/>resized_images=[]<br/>resized_poses=[]<br/>original_scale=[]<br/>poses=scipy.io.loadmat('YOUR DIRECTORY TO DATA \\lspet_dataset\\joints.mat')['joints']</span><span id="193c" class="lg iw hi lc b fi ll li l lj lk">for idx,x in progressbar.progressbar(enumerate(images_directory)):<br/>    path=os.path.join(base_directory,x)<br/>    <br/>    image = plt.imread(path)<br/>    height,width=image.shape[0],image.shape[1]<br/>    scale_w,scale_h=220/width,220/height<br/>    original_scale.append([width,height])<br/>    <br/>    resized_poses.append([poses[:,0,idx]*scale_w*2-1,poses[:,1,idx]*scale_h*2-1])<br/>    <br/>    image=cv2.resize(image,target_size,interpolation=cv2.INTER_AREA)<br/>    resized_images.append(image)</span><span id="c0f0" class="lg iw hi lc b fi ll li l lj lk">resized_images=np.array(resized_images)<br/>resized_poses=np.array(resized_poses)<br/>original_scale=np.array(original_scale)</span><span id="215a" class="lg iw hi lc b fi ll li l lj lk">np.save('Leeds Sports Dataset NPY.npy',resized_images[:9000])<br/>np.save('Resized Poses.npy',resized_poses[:9000])<br/>np.save('Image scale.npy',original_scale[:9000])</span><span id="7fd1" class="lg iw hi lc b fi ll li l lj lk">np.save('Leeds Sports Dataset NPY Test.npy',resized_images[9000:])<br/>np.save('Resized Poses Test.npy',resized_poses[9000:])<br/>np.save('Image scale Test.npy',original_scale[9000:])</span></pre><p id="4c59" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">姿势像素坐标按照图像大小缩放成[-1，1]。在所有预处理阶段之后，数据被保存到单个。npy文件。我必须将数据转换成合并文件，因为对于大量的文件来说，上传到google drive(供colab使用)需要不确定的时间。</p><h1 id="dfd5" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">定义模型</h1><figure class="ks kt ku kv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/900e5583bac1fd31762f3c3abac87f8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ko4FtjVHlaKBNfIHRz2cRw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">DeepPose的模型架构</figcaption></figure><p id="bd63" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">我们将使用Tensorflow Keras定义DeepPose论文中提出的基线模型架构。该模型基本上是具有28个输出的AlexNet模型，每个输出回归图像中关节的坐标。文中定义了具体的模型超参数(如滤波器尺寸)。为了方便起见，本地响应规范化层被批量规范化所取代。</p><pre class="ks kt ku kv fd lb lc ld le aw lf bi"><span id="8c1d" class="lg iw hi lc b fi lh li l lj lk">def define_model(self):<br/>  #Changes made on model architecture<br/>  #LRN-&gt;BN, Pooling-&gt;Strided Convolution</span><span id="1e55" class="lg iw hi lc b fi ll li l lj lk">  model=tf.keras.models.Sequential()<br/>  model.add(tf.keras.layers.Conv2D(48,11,(4,4),padding='same',input_shape=(220,220,3)))<br/>  model.add(tf.keras.layers.BatchNormalization())</span><span id="1678" class="lg iw hi lc b fi ll li l lj lk">  model.add(tf.keras.layers.ReLU())<br/>  model.add(tf.keras.layers.Conv2D(128,5,(2,2),padding='same'))</span><span id="f31c" class="lg iw hi lc b fi ll li l lj lk">  model.add(tf.keras.layers.BatchNormalization())<br/>  model.add(tf.keras.layers.ReLU())</span><span id="d2d3" class="lg iw hi lc b fi ll li l lj lk">  model.add(tf.keras.layers.Conv2D(192,3,(2,2),padding='same',activation='relu'))</span><span id="38dd" class="lg iw hi lc b fi ll li l lj lk">  model.add(tf.keras.layers.Conv2D(192,3,(1,1),padding='same',activation='relu'))<br/>  model.add(tf.keras.layers.Conv2D(192,3,(1,1),padding='same',activation='relu'))<br/>  model.add(tf.keras.layers.MaxPooling2D())<br/>  model.add(tf.keras.layers.Flatten())</span><span id="98d9" class="lg iw hi lc b fi ll li l lj lk">  model.add(tf.keras.layers.Dense(4096,activation='relu'))<br/>  model.add(tf.keras.layers.Dropout(0.4))<br/>  model.add(tf.keras.layers.Dense(4096,activation='relu'))<br/>  model.add(tf.keras.layers.Dropout(0.4))<br/>  model.add(tf.keras.layers.Dense(28,activation='linear'))<br/>  return model</span></pre><p id="9543" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">因为DeepPose论文中的姿态估计问题是作为回归问题来处理的，所以我们应用原始的MSE误差来进行训练。更多细节和完整代码在<a class="ae iu" href="https://colab.research.google.com/drive/1J2oHgwX5RtsCI7DU3ToNgz00rb_p9S2T?usp=sharing" rel="noopener ugc nofollow" target="_blank"> colab链接</a>中提供。</p><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/59b003eaa2783247f174428ae2a51b38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*BnIiylB5Q1abj-Pvfe0iaw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">训练模型的结果</figcaption></figure><h1 id="79a9" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">模型评估</h1><figure class="ks kt ku kv fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/e3b1c1c71c3e0a131222bfc15c8ca5d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*JCOlBkRhy5pHna4clL4Ehg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">损失图</figcaption></figure><p id="15f7" class="pw-post-body-paragraph jt ju hi jv b jw kw jy jz ka kx kc kd ke ky kg kh ki kz kk kl km la ko kp kq hb bi translated">基线模型未能对数据进行正则化，导致过度拟合。丢弃参数已经设置得很高，但是似乎需要更高的正则化惩罚。论文中提出的姿态回归器级联没有成功实现，这也可能是一个原因。验证损失的图表非常怪异，需要更多的分析。</p></div></div>    
</body>
</html>