<html>
<head>
<title>Understanding Data Science Job Requirements</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解数据科学工作要求</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-data-science-job-requirements-part-1-web-scrapping-9205083eb296?source=collection_archive---------13-----------------------#2019-10-19">https://medium.com/analytics-vidhya/understanding-data-science-job-requirements-part-1-web-scrapping-9205083eb296?source=collection_archive---------13-----------------------#2019-10-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="7775" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">第1部分—网络搜集、探索性数据分析</h2></div><p id="757f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是一个成为数据科学家进入就业市场的好时机。这是根据求职网站Indeed和Dice的最新数据得出的。</p><p id="e834" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">“数据科学家的工作变得越来越性感，”位于德克萨斯州奥斯汀的Indeed公司的经济学家、Indeed报告的作者安德鲁·弗劳尔斯说。“越来越多的雇主希望雇佣数据科学家。”</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/54291a6ca1a411f729c1a3dc6d8a322c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*omrnVNP7i9RrHxrJM5HUwA.jpeg"/></div></div></figure><p id="6624" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这些话不是我说的；相反，它们是从无数赞美数据科学家未来的文章中抄袭来的。我在这个领域工作了超过27个月，我个人认为机器学习将会改变很多行业的游戏规则。</p><p id="efb9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在专业层面上，我应用机器学习的原理来开发跨零售、商业和保险等多个领域的销售预测、客户细分和定价策略。这些项目进一步强化了我的假设，即在不久的将来，数据科学家将会非常吃香。</p><p id="7616" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，正如我们在数据科学中经常做的那样，假设公式是一部分，测试假设是另一部分(也许是最重要的！).因此，我决定通过搜集Indeed.com的数据科学工作清单来测试我的假设，以便更好地了解美国各大城市数据科学家的最低资格和工作角色。</p><p id="aece" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是我分析的第一部分，我向你解释我是如何从Indeed中提取这些工作列表的。我花了将近24个下班后的时间在Python上编写完整的报废代码，并多次测试，以确保该过程可以快速自动化，天哪，这真有趣！我写的最终代码<strong class="iz hj"> <em class="kf">提取了全美800个薪酬超过6.5万美元的数据科学职位</em> </strong>而这一切都是在仅仅<strong class="iz hj"> <em class="kf"> 20分钟</em> </strong>的时间内<strong class="iz hj"> <em class="kf"> </em> </strong>完成的。这是我用原始数据做的转换的视图。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kg"><img src="../Images/9aaaf8272561f0b27a19da80149683ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ThJ2zy47T5jpXojt7fZh4w.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">查看原始职务列表</figcaption></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kl"><img src="../Images/c57d7dd2e008a08c427bcbe79fbea5cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QeY6d0g8uqRzgr9xlNPVCQ.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">转换后的职务列表视图—表格形式</figcaption></figure><p id="aba2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我将描述我是如何废弃了100多个Indeed的网页，并清理废弃的数据以获得上面的格式。我还将执行一些基本的探索性数据分析(EDA)来检查废弃数据的有效性。在本系列的第二篇文章(第2部分)中，我将解释我们是如何从作业摘要中生成有意义的特征的，因为所有的金子都藏在那里！我们开始吧。</p><pre class="ju jv jw jx fd km kn ko kp aw kq bi"><span id="dbde" class="kr ks hi kn b fi kt ku l kv kw">#Importing all the necessary libraries<br/>import matplotlib<br/>import numpy as np<br/>import random <br/>import os<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>from datetime import datetime<br/>import re<br/>import time<br/>import matplotlib.pyplot as plt<br/>import time<br/>from bs4 import BeautifulSoup<br/>import urllib.request<br/>from pandas import ExcelWriter<br/>from itertools import combinations</span></pre><p id="8885" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">提及将提取数据科学工作的城市名称:-</p><pre class="ju jv jw jx fd km kn ko kp aw kq bi"><span id="d5a0" class="kr ks hi kn b fi kt ku l kv kw">city_list = ['Philadelphia','San+Francisco', 'New+York', 'California', 'Houston', 'Boston', 'Chicago', 'Seattle', 'Austin', 'Maryland']</span></pre><p id="97de" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为涉及到多个城市和多个网页，所以编写了一个for循环中的for循环来实现自动化。为了更好地解释，我用一个静态的例子来解释整个报废过程。在本文的末尾，您可以找到GitHub链接，其中提供了完整的报废代码。</p><pre class="ju jv jw jx fd km kn ko kp aw kq bi"><span id="1800" class="kr ks hi kn b fi kt ku l kv kw">#Mentioning the city_name and html link from where the data will be scrapped</span><span id="e0b7" class="kr ks hi kn b fi kx ku l kv kw">city_name = city_list[0]</span><span id="c43a" class="kr ks hi kn b fi kx ku l kv kw">original_html = '<a class="ae ky" href="https://www.indeed.com/jobs?q=data+scientist+$65,000&amp;l='+city_name+'&amp;start=10'" rel="noopener ugc nofollow" target="_blank">https://www.indeed.com/jobs?q=data+scientist+$65,000&amp;l='+city_name+'&amp;start=10'</a></span><span id="4611" class="kr ks hi kn b fi kx ku l kv kw">#Parsing the html using beautiful soup and store in variable 'soup'<br/>urlpage =  str(original_html)<br/>page = urllib.request.urlopen(urlpage)<br/>soup = BeautifulSoup(page, 'html.parser')</span><span id="fc29" class="kr ks hi kn b fi kx ku l kv kw">print("Extracting Data Science Job Postings for -", city_name, "from the link - ", urlpage)</span><span id="e5f5" class="kr ks hi kn b fi kx ku l kv kw">##OUTPUT - <br/><strong class="kn hj"><em class="kf">Extracting Data Science Job Postings for - Philadelphia from the link -  </em></strong><a class="ae ky" href="https://www.indeed.com/jobs?q=data+scientist+$65,000&amp;l=Philadelphia&amp;start=10" rel="noopener ugc nofollow" target="_blank"><strong class="kn hj"><em class="kf">https://www.indeed.com/jobs?q=data+scientist+$65,000&amp;l=Philadelphia&amp;start=10</em></strong></a></span></pre><p id="582a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">浏览了几个网页后，我意识到作业列表存在于类名为“title”的<div>容器单元中。我进一步清理了字符串“href=”和“id=”之间的文本，它们具有角色的确切名称。在我看来，web报废就是通过一遍又一遍地查看页面资源来找到正确的容器。这就是为什么我花了这么多时间来写完整的代码:P</div></p><p id="636d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">编写一个循环来提取网页上的工作角色和工作描述的链接:</p><pre class="ju jv jw jx fd km kn ko kp aw kq bi"><span id="da43" class="kr ks hi kn b fi kt ku l kv kw">all_links = []<br/>all_roles = []</span><span id="006a" class="kr ks hi kn b fi kx ku l kv kw">role_list = soup.findAll('div', attrs={'class':'title'})</span><span id="3ba3" class="kr ks hi kn b fi kx ku l kv kw">for i in range(0,len(role_list)):<br/>    #i = 0<br/>    s = str(role_list[i])<br/>    start = 'href='<br/>    end = ' id='<br/>    link = s[s.find(start)+len(start):s.rfind(end)]<br/>    link = link.replace('"', "")<br/>    all_links.append(link)<br/>    title_name = str(role_list[i].text)<br/>    title_name = title_name.strip()<br/>    all_roles.append(title_name)<br/>    <br/>print("Job Roles present on this page are - ", all_roles)</span><span id="aad6" class="kr ks hi kn b fi kx ku l kv kw">##OUTPUT - <br/><strong class="kn hj"><em class="kf">Job Roles present on this page are -  ['Data Scientist', 'SR DATA SCIENTIST', 'Data Scientist', 'Data Scientist - Forecasting', 'Data Scientist', 'Data Scientist', 'Chief Data Scientist', 'Genomic Data Scientist', 'Data Scientist', 'Analyst, Data Scientist –Medical Voice of Customer/Text Anal...']</em></strong></span></pre><p id="56fe" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我分析了我刚刚提取的工作描述的链接，我意识到它们没有统一的模式。</p><pre class="ju jv jw jx fd km kn ko kp aw kq bi"><span id="9c0c" class="kr ks hi kn b fi kt ku l kv kw">##Printing all the links obtained</span><span id="2c2e" class="kr ks hi kn b fi kx ku l kv kw">print(all_links)</span><span id="6526" class="kr ks hi kn b fi kx ku l kv kw">##OUTPUT - <br/>['/rc/clk?jk=450accf3cc188a49&amp;amp;fccid=a054959bd2e612a2&amp;amp;vjs=3',<br/> '/rc/clk?jk=b8946db7aad572ee&amp;amp;fccid=0bed8e17bc113980&amp;amp;vjs=3',<br/> '/rc/clk?jk=6b4ec424a41a8ee9&amp;amp;fccid=4c3bc2bd3fda6e72&amp;amp;vjs=3',<br/> '/rc/clk?jk=67a1dad2fca55e88&amp;amp;fccid=8a2222c2ef6a251d&amp;amp;vjs=3',<br/> '/rc/clk?jk=f6c3ef61212721db&amp;amp;fccid=b5f5a475a9aae6da&amp;amp;vjs=3',<br/> '/rc/clk?jk=81521b7a6303cb7b&amp;amp;fccid=6b7a1dfe07e7f037&amp;amp;vjs=3',<br/> '/rc/clk?jk=51f0b38ce1225e4a&amp;amp;fccid=19313a51d729b8e6&amp;amp;vjs=3',<br/> '/rc/clk?jk=72bd3bbbec1f33e4&amp;amp;fccid=4e42ec53f4b93e02&amp;amp;vjs=3',<br/> '/company/Gap-International/jobs/Data-Scientist-45b06c797b37e391?fccid=c9cfc7300b9963c1&amp;amp;vjs=3',<br/> '/rc/clk?jk=14fbb6832d453b6a&amp;amp;fccid=4e42ec53f4b93e02&amp;amp;vjs=3']</span></pre><p id="8be0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如您在上面看到的，10个链接中有9个以字符串“/rc/clk”开头，但是有一个链接以“/company/”开头。这一点必须注意，否则，这一特定工作说明的内容就不能取消。我是这样做的:</p><pre class="ju jv jw jx fd km kn ko kp aw kq bi"><span id="8748" class="kr ks hi kn b fi kt ku l kv kw">all_podcast_links = all_links<br/>all_podcast_links = [i.split('-')[-1] for i in all_podcast_links]</span><span id="903f" class="kr ks hi kn b fi kx ku l kv kw">modified_links = []<br/>for i in range(0,len(all_podcast_links)):<br/>    main_link_text = all_podcast_links[i]<br/>    string_find = '/rc/clk'<br/>    if(main_link_text.find(string_find) == -1):<br/>        main_link_text = '/rc/clk?jk=' + main_link_text<br/>        text_replace = "?fccid="<br/>        main_link_text = main_link_text.replace(text_replace, '&amp;amp;fccid=')<br/>        print('Caution !! Link modified for link number', i)<br/>    else:<br/>        main_link_text = main_link_text<br/>    modified_links.append(main_link_text)</span><span id="063d" class="kr ks hi kn b fi kx ku l kv kw">text_delete = '/rc/clk'<br/>all_podcast_links = [w.replace(text_delete,"") for w in modified_links]<br/>text_replace = "/jobs/"<br/>all_podcast_links = [w.replace(text_replace,"&amp;t=") for w in all_podcast_links]<br/>text_to_add = '<a class="ae ky" href="https://www.indeed.com/viewjob?'" rel="noopener ugc nofollow" target="_blank">https://www.indeed.com/viewjob?'</a><br/>final_links = [text_to_add+s for s in all_podcast_links]<br/>text_replace = "/viewjob?/company/"<br/>final_links = [w.replace(text_replace,"/company/") for w in final_links]<br/>print("/n All links after modification - ", final_links)</span><span id="84a9" class="kr ks hi kn b fi kx ku l kv kw">##OUTPUT - <br/><strong class="kn hj"><em class="kf">Caution !! Link modified for link number 8</em></strong></span><span id="1ba5" class="kr ks hi kn b fi kx ku l kv kw"><strong class="kn hj"><em class="kf">All links after modification -<br/>['https://www.indeed.com/viewjob??jk=450accf3cc188a49&amp;amp;fccid=a054959bd2e612a2&amp;amp;vjs=3', 'https://www.indeed.com/viewjob??jk=b8946db7aad572ee&amp;amp;fccid=0bed8e17bc113980&amp;amp;vjs=3', 'https://www.indeed.com/viewjob??jk=6b4ec424a41a8ee9&amp;amp;fccid=4c3bc2bd3fda6e72&amp;amp;vjs=3', 'https://www.indeed.com/viewjob??jk=67a1dad2fca55e88&amp;amp;fccid=8a2222c2ef6a251d&amp;amp;vjs=3', 'https://www.indeed.com/viewjob??jk=f6c3ef61212721db&amp;amp;fccid=b5f5a475a9aae6da&amp;amp;vjs=3', 'https://www.indeed.com/viewjob??jk=81521b7a6303cb7b&amp;amp;fccid=6b7a1dfe07e7f037&amp;amp;vjs=3', 'https://www.indeed.com/viewjob??jk=51f0b38ce1225e4a&amp;amp;fccid=19313a51d729b8e6&amp;amp;vjs=3', 'https://www.indeed.com/viewjob??jk=72bd3bbbec1f33e4&amp;amp;fccid=4e42ec53f4b93e02&amp;amp;vjs=3', 'https://www.indeed.com/viewjob??jk=45b06c797b37e391&amp;amp;fccid=c9cfc7300b9963c1&amp;amp;vjs=3', 'https://www.indeed.com/viewjob??jk=14fbb6832d453b6a&amp;amp;fccid=4e42ec53f4b93e02&amp;amp;vjs=3']</em></strong></span></pre><p id="db4a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">编写一个循环来提取网页上所有公司的名称:</p><pre class="ju jv jw jx fd km kn ko kp aw kq bi"><span id="87e0" class="kr ks hi kn b fi kt ku l kv kw">all_companies = []<br/>company_list = soup.findAll('span', attrs={'class':'company'})<br/>for i in range(0,len(company_list)):<br/>    #i = 2<br/>    comp_name = str(company_list[i].text)<br/>    comp_name = comp_name.strip()<br/>    all_companies.append(comp_name)<br/>    <br/>print("Companies present on the page -", all_companies)</span><span id="00c8" class="kr ks hi kn b fi kx ku l kv kw">##OUTPUT - <br/><strong class="kn hj"><em class="kf">Companies present on the page - ["Philadelphia District Attorney's Office", 'Johnson &amp; Johnson Family of Companies', 'Social Science Research Solutions (SSRS)', 'goPuff', 'RS Energy Group', 'IQVIA', 'Panna Knows LLC', 'GSK', 'Gap International', 'GSK']</em></strong></span></pre><p id="85e4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">编写一个循环来提取网页上所有公司的位置:</p><pre class="ju jv jw jx fd km kn ko kp aw kq bi"><span id="a51b" class="kr ks hi kn b fi kt ku l kv kw">all_locations = []<br/>location_list = soup.findAll('span', attrs={'class':'location accessible-contrast-color-location'})<br/>for i in range(0,len(location_list)):<br/>    #i = 2<br/>    loc_name = str(location_list[i].text)<br/>    loc_name = loc_name.strip()<br/>    all_locations.append(loc_name)<br/>    <br/>print("List of Company Locations -", all_locations)</span><span id="ec51" class="kr ks hi kn b fi kx ku l kv kw">##OUTPUT - <br/><strong class="kn hj"><em class="kf">List of Company Locations - ['Philadelphia, PA 19107 (City Center East area)', 'Spring House, PA', 'Glen Mills, PA', 'Philadelphia, PA', 'Conshohocken, PA', 'Plymouth Meeting, PA', 'Wayne, PA', 'Collegeville, PA 19426', 'Springfield, PA 19064', 'Philadelphia, PA 19112 (Marconi Plaza-Packer Park area)']</em></strong></span></pre><p id="4455" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">编写一个循环来提取网页上所有公司的工作摘要:</p><pre class="ju jv jw jx fd km kn ko kp aw kq bi"><span id="ab5d" class="kr ks hi kn b fi kt ku l kv kw">all_job_summary = []<br/>job_summary_list = soup.findAll('div', attrs={'class':'summary'})<br/>for i in range(0,len(location_list)):<br/>    #i = 2<br/>    job_summary = str(job_summary_list[i].text)<br/>    job_summary = job_summary.strip()<br/>    all_job_summary.append(job_summary)<br/>    <br/>print("One of the job summary - ", all_job_summary[0])</span><span id="db16" class="kr ks hi kn b fi kx ku l kv kw">##OUTPUT - <br/><strong class="kn hj"><em class="kf">One of the job summary -  The Philadelphia District Attorney’s Office (DAO), led by District Attorney Larry Krasner, is committed to the mission of providing fair and just prosecution to…</em></strong></span></pre><p id="797f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将所有结果整理到一个数据框中，以便正确查看:</p><pre class="ju jv jw jx fd km kn ko kp aw kq bi"><span id="b805" class="kr ks hi kn b fi kt ku l kv kw">full_company_df = pd.DataFrame(<br/>            {'Company_Name': all_companies,'Role': all_roles,'Job_Summary': all_job_summary,<br/>             'Location' :all_locations, 'JD_Link' :final_links<br/>            })</span></pre><p id="165f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是整理后的数据框最终的样子:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kz"><img src="../Images/41ba2afab18932c658e6e1161a170c10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ju92UNCpWz0xMpx7sD_qhw.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">校对数据帧</figcaption></figure><p id="d0ef" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我编写了一个for循环，从上面获得的表的列名“JD_Link”中提取整个职务描述:</p><pre class="ju jv jw jx fd km kn ko kp aw kq bi"><span id="11ba" class="kr ks hi kn b fi kt ku l kv kw">all_company_df = pd.DataFrame()<br/>for i in range(0, len(full_company_df)):<br/>    #i = 0<br/>    new_link = full_company_df['JD_Link'][i]<br/>    urlpage =  str(new_link)<br/>    page = urllib.request.urlopen(urlpage)<br/>    # parse the html using beautiful soup and store in variable 'soup'<br/>    soup = BeautifulSoup(page, 'html.parser')</span><span id="0806" class="kr ks hi kn b fi kx ku l kv kw">script = soup.find('script')<br/>    <br/>    #Extracting full JD from the respective div field<br/>    divs = soup.find_all('div',attrs={"id" : "jobDescriptionText"})<br/>    for d in divs:<br/>        job_description = str(d.text)<br/>        <br/>    #Creating master_Dataframe</span><span id="f74a" class="kr ks hi kn b fi kx ku l kv kw">master_df = pd.DataFrame({'JD_Link' : urlpage, 'job_description' : job_description}, index = [0])<br/>    all_company_df = all_company_df.append(master_df)<br/>    print('JD extracted for ', (i+1), "companies out of -", len(full_company_df))<br/>    <br/>all_company_df = all_company_df.reset_index(drop = True)</span><span id="3892" class="kr ks hi kn b fi kx ku l kv kw">##OUTPUT - <br/><strong class="kn hj"><em class="kf">JD extracted for  1 companies out of - 10<br/>JD extracted for  2 companies out of - 10<br/>JD extracted for  3 companies out of - 10<br/>JD extracted for  4 companies out of - 10<br/>JD extracted for  5 companies out of - 10<br/>JD extracted for  6 companies out of - 10<br/>JD extracted for  7 companies out of - 10<br/>JD extracted for  8 companies out of - 10<br/>JD extracted for  9 companies out of - 10<br/>JD extracted for  10 companies out of - 10</em></strong></span></pre><p id="eed5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我现在检查其中一份工作描述，以确保一切就绪。</p><pre class="ju jv jw jx fd km kn ko kp aw kq bi"><span id="e086" class="kr ks hi kn b fi kt ku l kv kw">print(all_company_df['job_description'][0])</span><span id="8b8d" class="kr ks hi kn b fi kx ku l kv kw">##OUTPUT - <br/>'Overview\nThe Philadelphia District Attorney’s Office (DAO), led by District Attorney Larry Krasner, is committed to the mission of providing fair and just prosecution to all Philadelphians. DA Krasner believes that justice is best achieved through policies grounded in research, data, and science. Holding true to these commitments, the DAO has improved its capabilities to process data, craft policy, and conduct research. As we continue to modernize a progressive and fast-paced office, we aim to be responsive, forward thinking, and collaborative.\nThe Data Scientist will join an innovative team of researchers, lawyers, analysts, and other software engineers in the development and deployment of applications to support prosecution, analytics, data-driven policy, and public transparency. They will support DA Krasner’s targeted areas for informed reform, including charging, diversion, sentencing, focusing on the most serious crimes, preventing violence, enhancing harm reduction, and ameliorating disparities in the criminal justice process. The Data Scientist will be focused on the development of metrics, visualizations, and unique analyses to support policy makers, evaluate policy, and communicate with the public. The Data Scientist will be expected to:\nbe involved with development, processing, and reporting on a variety of data both internal and external to the District Attorney’s Office;\nbe involved with the development of an internal and external data dashboard that surfaces key reports and information to policymakers, executives, and the public\ndevelop visualizations to show trends and summarize data as well as perform deeper dives to support policy proposals or policy testing;\nhelp develop thoughtful metrics to track the efficacy of policies, to help develop new policies, to support research, and to help communicate to the public the work of the DAO;\nbe responsible for working with both the analytical and development teams to ensure appropriate data capture, storage, and accessibility;\nassist in development, implementation, and maintenance of policies and procedures for ensuring the security and integrity of sensitive data;\nassist in creating, modifying or maintaining ETL processes to support application development, data analysis, and research;\nleverage their significant experience to contribute to data analysis and policy research and implementation;\nperform other duties as required.\n\nQualifications\nA Bachelor’s or Master’s degree in Computer Science, Software Engineering, Mathematics, Statistics, Business Analytics or related technical discipline is preferred, but not required. Additional years of experience exceeding the minimum requirement as well as a proven track record of work may stand in for a degree. The preferred candidate will also have or demonstrate:\n3 years of hands on experience with data I/O, cleaning, coding, manipulation, transformation, modeling, summarizing, and visualizing;\nexperience working with relational databases and non-relational databases and exploring data to analyze business cases to identify areas for improvement;\nexperience with data visualization, especially interactive visualizations\nextensive knowledge of R, experience working with the tidyverse family of packages; experience with statistical analysis; comfort with packages utilized for modeling data;\nexperience with interactive web frameworks such as React, Angular, Dash, or R Shiny a plus\nthe Data Scientist will also have a solid understanding of PostgreSQL, AWS, data analysis and visualization, and statistical principles and methods;\nan enthusiasm to collaborate, innovate, and solve problems;\na proven ability to prioritize work in a team and independently, and handle multiple complex tasks simultaneously;\nhave strong communication skills and be able to interact effectively with team members, executives, funders, City government, academics, and stakeholders inside and outside of the DAO;\nexperience working in the legal field/with attorneys and a knowledge of the criminal justice system are both a plus.'</span></pre><p id="ea88" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，我将工作描述数据帧与最初获得的数据帧进行映射，以获得以下格式的整理数据帧:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es la"><img src="../Images/6f866506032e1f3144f2c18e5dac9fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YWut1FRkcD5jn0YNMuAlgw.png"/></div></div></figure><p id="e508" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就是我如何提取费城第一页上的数据科学工作清单。以下链接包含前面提到的所有城市的完整自动代码:</p><div class="lb lc ez fb ld le"><a href="https://github.com/tapobrata22/Indeed_Data_Science_Scrapping/blob/master/indeed_scrapping_v2.py" rel="noopener  ugc nofollow" target="_blank"><div class="lf ab dw"><div class="lg ab lh cl cj li"><h2 class="bd hj fi z dy lj ea eb lk ed ef hh bi translated">tapobrata 22/Indeed _ Data _ Science _ screwing</h2><div class="ll l"><h3 class="bd b fi z dy lj ea eb lk ed ef dx translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="lm l"><p class="bd b fp z dy lj ea eb lk ed ef dx translated">github.com</p></div></div><div class="ln l"><div class="lo l lp lq lr ln ls kd le"/></div></div></a></div><p id="01c5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是包含这些城市800个职位的最终数据集视图:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lt"><img src="../Images/306ac5a5dcae90c2c6fb190e8410325a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2kvReRQ8asDs1SKBanTNvA.png"/></div></div></figure><p id="eeda" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">希望能在第二部分看到你，在那里我将上面获得的“工作描述”栏的内容分解成如下实体——I)最小_工作_工作_前_年<br/> ii)最小_学位_资格<br/> iii)统计_软件_经验</p><p id="e00e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">谢谢你坚持到最后:)</p></div></div>    
</body>
</html>