<html>
<head>
<title>Text Generation Using Long Short Term Memory Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ä½¿ç”¨é•¿çŸ­æœŸè®°å¿†ç½‘ç»œçš„æ–‡æœ¬ç”Ÿæˆ</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/analytics-vidhya/text-generation-using-long-short-term-memory-network-2c288966f648?source=collection_archive---------24-----------------------#2020-07-20">https://medium.com/analytics-vidhya/text-generation-using-long-short-term-memory-network-2c288966f648?source=collection_archive---------24-----------------------#2020-07-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a3072ca57310660f0b7a68cfa3d02e0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mH_2C4g3Cre5CPGw6311_Q.jpeg"/></div></div></figure><p id="1842" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">æˆ‘ä»¬å°†åœ¨æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒä¸€ä¸ªLSTMç½‘ç»œï¼Œå®ƒè‡ªå·±å­¦ä¹ ç”Ÿæˆä¸è®­ç»ƒææ–™å½¢å¼ç›¸åŒçš„æ–°æ–‡æœ¬ã€‚å¦‚æœä½ åœ¨æ–‡æœ¬æ•°æ®ä¸Šè®­ç»ƒä½ çš„LSTMï¼Œå®ƒä¼šå­¦ä¹ äº§ç”Ÿæ–°çš„å•è¯ï¼Œç±»ä¼¼äºæˆ‘ä»¬è®­ç»ƒçš„å•è¯ã€‚LSTMé€šå¸¸ä¼šä»æºæ•°æ®ä¸­å­¦ä¹ äººç±»è¯­æ³•ã€‚å½“ç”¨æˆ·åƒèŠå¤©æœºå™¨äººä¸€æ ·è¾“å…¥æ–‡æœ¬æ—¶ï¼Œä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ç±»ä¼¼çš„æŠ€æœ¯æ¥å®Œæˆå¥å­ã€‚</p><p id="b573" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ä½¿ç”¨ <em class="jo"> tensorflow 2.x </em>å¯¼å…¥æˆ‘ä»¬çš„ä¾èµ–é¡¹â€” <em class="jo"/></p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="2afb" class="jy jz hi ju b fi ka kb l kc kd">import string<br/>import numpy as np<br/>from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.utils import to_categorical<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense,LSTM,Embedding<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences</span></pre><p id="2cdd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">è¯»å–æ•°æ®</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="465a" class="jy jz hi ju b fi ka kb l kc kd">file=open('t8.shakespeare.txt','r+')<br/>data=file.read()</span></pre><h2 id="2f7c" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">æ–‡æœ¬æ¸…ç†</h2><p id="136c" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">è·å¾—æ–‡æœ¬æ•°æ®åï¼Œæ¸…ç†æ–‡æœ¬æ•°æ®çš„ç¬¬ä¸€æ­¥æ˜¯å¯¹ä½ è¦è¾¾åˆ°çš„ç›®æ ‡æœ‰ä¸€ä¸ªæ¸…æ™°çš„è®¤è¯†ï¼Œå¹¶åœ¨è¿™ç§èƒŒæ™¯ä¸‹å›é¡¾ä½ çš„æ–‡æœ¬ï¼Œçœ‹çœ‹åˆ°åº•ä»€ä¹ˆä¼šæœ‰å¸®åŠ©ã€‚</p><p id="662e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">åœ¨æ•°æ®ä¸­æœ‰è®¸å¤šæ ‡ç‚¹ç¬¦å·å’Œæ•°å­—å­—ç¬¦ï¼Œä»¥æ¶ˆé™¤å®ƒ</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="d79a" class="jy jz hi ju b fi ka kb l kc kd">data=data.split('\n') <br/>data=data[253:]<br/>data=' '.join(data)</span></pre><p id="ea24" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">cleanerå‡½æ•°æœ‰åŠ©äºåˆ é™¤æ•°æ®ä¸­çš„æ ‡ç‚¹å’Œæ•°å­—ï¼Œå¹¶å°†ä¸­çš„æ‰€æœ‰å­—ç¬¦è½¬æ¢ä¸ºå°å†™</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="79fc" class="jy jz hi ju b fi ka kb l kc kd">def cleaner(data):<br/>    token=data.split()<br/>    table=str.maketrans('','',string.punctuation)<br/>    token=[w.translate(table) for w in token]<br/>    token=[word for word in token if word.isalpha()]<br/>    token=[word.lower() for word in token]<br/>    return token</span><span id="8c7e" class="jy jz hi ju b fi lc kb l kc kd">words=cleaner(data=data)</span></pre><h2 id="c4ab" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">åˆ›é€ ä¸€ä¸ªå•è¯åºåˆ—</h2><p id="62c0" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">seed_lengthæ˜¯50ï¼Œè¿™æ„å‘³ç€å‰50ä¸ªå•è¯å°†æ˜¯æˆ‘çš„è¾“å…¥ï¼Œä¸‹ä¸€ä¸ªå•è¯å°†æ˜¯æˆ‘çš„è¾“å‡ºã€‚å®ƒéœ€è¦å¤§é‡çš„è®¡ç®—èƒ½åŠ›å’Œå†…å­˜æ¥å¤„ç†æ‰€æœ‰æ•°æ®ã€‚æ‰€ä»¥æˆ‘åªç”¨å‰10ä¸‡ä¸ªå•è¯æ¥è®­ç»ƒæˆ‘çš„ç¥ç»ç½‘ç»œã€‚</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="b681" class="jy jz hi ju b fi ka kb l kc kd">seed_length=50+1<br/>sentence=list()<br/>for i in range(seed_length,len(words)):<br/>    sequence=words[i-seed_length:i]<br/>    line=' '.join(sequence)<br/>    sentence.append(line)<br/>    if i &gt;100000:<br/>        break</span></pre><p id="372d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ç¥ç»ç½‘ç»œè¦æ±‚å¯¹è¾“å…¥æ•°æ®è¿›è¡Œæ•´æ•°ç¼–ç ï¼Œè¿™æ ·æ¯ä¸ªå•è¯éƒ½ç”±ä¸€ä¸ªå”¯ä¸€çš„æ•´æ•°è¡¨ç¤ºã€‚ç¼–ç åå°†æ•´æ•°è½¬æ¢æˆæ•´æ•°åºåˆ—</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="5f0b" class="jy jz hi ju b fi ka kb l kc kd">tokenizer=Tokenizer()<br/>tokenizer.fit_on_texts(sentence)<br/>sequence=tokenizer.texts_to_sequences(sentence)<br/>sequence=np.array(sequence)</span></pre><p id="6344" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">åˆ†ç¦»è‡ªå˜é‡å’Œç›®æ ‡å˜é‡</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="60c0" class="jy jz hi ju b fi ka kb l kc kd">X,y=sequence[:,:-1],sequence[:,-1]<br/>vocab_size=len(tokenizer.word_index)+1<br/>y=to_categorical(y,num_classes=vocab_size)</span></pre><h2 id="cec7" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">åˆ›å»ºLSTMç½‘ç»œ</h2><p id="b898" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">åµŒå…¥å±‚è¢«å®šä¹‰ä¸ºç½‘ç»œçš„ç¬¬ä¸€ä¸ªéšè—å±‚ã€‚å®ƒå¿…é¡»éœ€è¦3ä¸ªå‚æ•°</p><ol class=""><li id="68af" class="ld le hi is b it iu ix iy jb lf jf lg jj lh jn li lj lk ll bi translated">vocab_size â€”æ–‡æœ¬æ•°æ®ä¸­è¯æ±‡çš„å¤§å°ã€‚</li><li id="d7b9" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn li lj lk ll bi translated">output_dim â€”å•è¯å°†åµŒå…¥å…¶ä¸­çš„å‘é‡çš„å¤§å°ã€‚</li><li id="ef14" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn li lj lk ll bi translated">è¾“å…¥é•¿åº¦â€”è¾“å…¥åºåˆ—çš„é•¿åº¦ã€‚</li></ol><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="3330" class="jy jz hi ju b fi ka kb l kc kd">model=Sequential()<br/>model.add(Embedding(vocab_size,50,input_length=50))<br/>model.add(LSTM(100,return_sequences=True))<br/>model.add(LSTM(100))<br/>model.add(Dense(100,activation='relu'))<br/>model.add(Dense(vocab_size,activation='softmax'))<br/>model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])<br/>model.summary()</span></pre><h2 id="3ccd" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹</h2><p id="fd5f" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">è®­ç»ƒä½ çš„æ¨¡å‹æ›´å¤šçš„æ—¶ä»£ï¼Œæˆ‘ä»¬çš„ç½‘ç»œå°†èƒ½å¤Ÿå­¦ä¹ å¦‚ä½•ç”Ÿæˆå•è¯ã€‚</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="cf41" class="jy jz hi ju b fi ka kb l kc kd">model.fit(X,y,batch_size=256,epochs=1000)</span></pre><p id="5ce6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">generateå‡½æ•°å¸®åŠ©æˆ‘ä»¬ç”Ÿæˆ50ä¸ªå•è¯ä¹‹åçš„å•è¯ï¼Œä½œä¸ºæ¨¡å‹çš„è¾“å…¥</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="7c99" class="jy jz hi ju b fi ka kb l kc kd">def generate(text,n_words):<br/>    text_q=[]<br/>    for _ in range(n_words):<br/>        encoded=tokenizer.texts_to_sequences(text)[0]<br/>        encoded=pad_sequences([encoded],maxlen=sequence_length,truncating='pre')<br/>        prediction=model.predict_classes(encoded)<br/>        for word , index in tokenizer.word_index.items():<br/>            if index==prediction:<br/>                predicted_word=word<br/>                break<br/>        text=text+' '+predicted_word<br/>        text_q.append(predicted_word)<br/>    return ' '.join(text_q)</span></pre><p id="75e4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ä½¿ç”¨å‡½æ•°å¹¶ç”Ÿæˆæ¥ä¸‹æ¥çš„100ä¸ªå•è¯</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="c104" class="jy jz hi ju b fi ka kb l kc kd">input = sentence[0]<br/>generate(input,100)</span></pre><p id="9b1c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">æ„Ÿè°¢é˜…è¯»ï¼æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« æ˜¯æœ‰å¸®åŠ©çš„ã€‚</p><p id="2401" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ä½ ä»¬çš„è¯„è®ºå’ŒæŒå£°è®©æˆ‘æœ‰åŠ¨åŠ›åˆ›ä½œæ›´å¤šçš„ææ–™ã€‚æˆ‘å¾ˆæ¬£èµä½ ï¼ğŸ˜Š</p></div></div>    
</body>
</html>