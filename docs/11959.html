<html>
<head>
<title>Feature Importance Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释了功能重要性</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-importance-explained-bfc8d874bcf?source=collection_archive---------0-----------------------#2020-12-26">https://medium.com/analytics-vidhya/feature-importance-explained-bfc8d874bcf?source=collection_archive---------0-----------------------#2020-12-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/18eba6b9086d59f1c6707037c83c299d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/0*G1O8YZitm6KcGJvX.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><a class="ae iq" href="https://www.google.com/search?q=selection&amp;sxsrf=ALeKk01iWPWYWnza3HDTmkksNF1MEIfyrg:1607154041483&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwjhnfetq7btAhWQbSsKHWmYAm0Q_AUoAnoECBYQBA&amp;biw=1366&amp;bih=625#imgrc=vWpaCP5g1DXMNM" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><blockquote class="ir is it"><p id="5c42" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">什么是特征重要性？</strong></p></blockquote><p id="0442" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">它根据输入要素的重要性来分配输入要素的分数，以预测输出。负责预测输出的特征越多，它们的得分就越大。我们可以在分类和回归问题中使用它。假设你有一桶 10 个水果，你想从中挑选芒果，荔枝，橘子，所以这些水果对你来说很重要，就像机器学习中的特征重要性一样。在这个博客中，我们将了解各种特性重要性方法。</p><h2 id="6c83" class="jw jx hi bd jy jz ka kb kc kd ke kf kg jt kh ki kj ju kk kl km jv kn ko kp kq bi translated">1.排列特征重要性:</h2><p id="a50a" class="pw-post-body-paragraph iu iv hi ix b iy kr ja jb jc ks je jf jt kt ji jj ju ku jm jn jv kv jq jr js hb bi translated">对于那些本身不支持特征重要性的算法是最好的。它独立于使用模型计算相对重要性分数<strong class="ix hj">。</strong>这是进行特征选择的最佳技术之一，让我们来理解它；</p><p id="7454" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">步骤 1 : - </strong>它随机选取一个特征，改变该特征中的变量，并进行预测。</p><p id="7f1f" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">步骤 2 :- </strong>在该步骤中，使用损失函数找出损失，并检查预测产量和实际产量之间的可变性。</p><p id="7b2f" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">第三步:- </strong>将特征变量恢复到原来的顺序或取消洗牌。</p><p id="e095" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">步骤 4 :- </strong>对数据集中存在的所有特征执行上述三个步骤。</p><p id="cbe6" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj">步骤 5 :- </strong>将通过比较个人得分和平均重要性得分来计算最终重要特征。</p><p id="d73d" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">该特征选择模型克服了基于树的特征选择技术中最常见的过度拟合。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="fb78" class="jw jx hi lb b fi lf lg l lh li">from sklearn.datasets import make_classification<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.inspection import permutation_importance<br/>from matplotlib import pyplot<br/># define dataset<br/>X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=1)<br/># define the model<br/>model = KNeighborsClassifier()<br/># fit the model<br/>model.fit(X, y)<br/># perform permutation importance<br/>results = permutation_importance(model, X, y, scoring='accuracy')<br/># get importance<br/>importance = results.importances_mean<br/>importance=np.sort(importance)<br/># summarize feature importance<br/>for i,v in enumerate(importance):<br/>    print('Feature: {}  Score: {}' .format(i,v))<br/># plot feature importance<br/>pyplot.bar([x for x in range(len(importance))] ,importance)<br/>pyplot.show()</span></pre><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/f719d14890ea54f86591adfb34eb62dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*UuAvQr7vLsAbS53_jIPrVg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">排列重要性</figcaption></figure><h2 id="df30" class="jw jx hi bd jy jz ka kb kc kd ke kf kg jt kh ki kj ju kk kl km jv kn ko kp kq bi translated"><strong class="ak"> 2。作为特征重要性的系数:</strong></h2><p id="3b23" class="pw-post-body-paragraph iu iv hi ix b iy kr ja jb jc ks je jf jt kt ji jj ju ku jm jn jv kv jq jr js hb bi translated">在线性模型(逻辑回归、线性回归、正则化)的情况下，我们通常会找到系数来预测输出。让我们通过代码来理解它。</p><p id="3c63" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated"><strong class="ix hj"> a .线性模型中分类问题的特征重要性</strong></p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="bc3b" class="jw jx hi lb b fi lf lg l lh li">import pandas as pd<br/>import numpy as np<br/>from sklearn.datasets import make_classification<br/>from sklearn.linear_model import LogisticRegression<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)<br/>model=LogisticRegression()<br/>model.fit(X,y)<br/>importance=model.coef_[0]<br/>importance=np.sort(importance)<br/>importance</span><span id="2c36" class="jw jx hi lb b fi lk lg l lh li">[out]&gt;&gt; aarray([-0.64301454, -0.51785423, -0.46189527, -0.4060204 , -0.11978098,0.03771881,  0.16319742,  0.18431777,  0.26539871,  0.4849665 ])</span></pre><p id="0110" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">按升序打印所有重要特征</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="6229" class="jw jx hi lb b fi lf lg l lh li">for index,val in enumerate(importance):<br/>    print("Feature : {} has score  : {} ".format(index,val))</span></pre><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/742a1ffcf17035f49bc5641341b1feb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*Wv0pvaEMWCiI-bB0dX9Vnw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">图 1</figcaption></figure><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="633f" class="jw jx hi lb b fi lf lg l lh li">#plotting the features and their score in ascending order<br/>sns.set_style("darkgrid")<br/>plt.bar([i for i in range (len(importance))],importance)<br/>plt.show()</span></pre><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/39893a40a4ed03f98d23735a01b76279.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*lh3RWkigG36veVcrYndjZg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">图 2</figcaption></figure><p id="750e" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">b.线性模型中回归问题的特征重要性</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="17d4" class="jw jx hi lb b fi lf lg l lh li">import pandas as pd<br/>import numpy as np<br/>from sklearn.datasets import make_regression<br/>from sklearn.linear_model import LinearRegression<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)<br/>model=LinearRegression()<br/>model.fit(X,y)<br/>importance=model.coef_<br/>importance=np.sort(importance)<br/>#plotting the features and their score in ascending order<br/>sns.set_style("darkgrid")<br/>plt.bar([i for i in range (len(importance))],importance)<br/>plt.show()</span></pre><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/f370ebcac3b50fa0a14520fd8afbbc84.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*34-t2_bysql8mIjujEKtUw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">图 3</figcaption></figure><h2 id="d507" class="jw jx hi bd jy jz ka kb kc kd ke kf kg jt kh ki kj ju kk kl km jv kn ko kp kq bi translated">3 .决策树作为特征的重要性:</h2><p id="4a30" class="pw-post-body-paragraph iu iv hi ix b iy kr ja jb jc ks je jf jt kt ji jj ju ku jm jn jv kv jq jr js hb bi translated">决策树使用 CART 技术来找出其中存在的重要特征。所有基于决策树的算法都使用相似的技术来发现重要的特征。</p><pre class="kw kx ky kz fd la lb lc ld aw le bi"><span id="f363" class="jw jx hi lb b fi lf lg l lh li">#decision tree for feature importance on a regression problem<br/>from sklearn.datasets import make_regression<br/>from sklearn.tree import DecisionTreeRegressor<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>import numpy as np<br/>X, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)<br/># define the model<br/>model = DecisionTreeRegressor()<br/># fit the model<br/>model.fit(X, y)<br/># get importance<br/>importance = model.feature_importances_<br/>importance=np.sort(importance)<br/># summarize feature importance<br/>for i,v in enumerate(importance):<br/>    print('Feature: {}, Score: {}'.format(i,v))<br/># plot feature importance<br/>plt.bar([x for x in range(len(importance))], importance)<br/>plt.show()</span></pre><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/130e8172f93634ed591be1897058bd0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*hxi3Owuc0QYEAzCFC_9iKw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">图 4</figcaption></figure><p id="3282" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi lp translated">我们在决策树中发现重要特征的方法与在随机森林和 Xgboost 中发现特征重要性的方法相同。</p><blockquote class="ir is it"><p id="7289" class="iu iv iw ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">为什么特性重要性如此重要？</strong></p></blockquote><p id="90f6" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">特征重要性给了我们更好的数据可解释性。</p><p id="0585" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">让我们来理解解释能力；</p><p id="62d1" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">假设你有一个医院的数据集，现在所有者想知道哪种症状的人会再次来医院。每种疾病(特征)如何使他们获利。人们对这家医院的治疗有什么看法？这些都被称为可解释性。这也有助于我们找到最重要的预测特征。</p><p id="8114" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">结论:-</p><p id="34d5" class="pw-post-body-paragraph iu iv hi ix b iy iz ja jb jc jd je jf jt jh ji jj ju jl jm jn jv jp jq jr js hb bi translated">这些都是我的观点，如果你有任何建议，请在下面评论。</p></div></div>    
</body>
</html>