<html>
<head>
<title>Fixing Words with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python修正单词</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fixing-words-with-python-1db017e1b160?source=collection_archive---------18-----------------------#2020-08-18">https://medium.com/analytics-vidhya/fixing-words-with-python-1db017e1b160?source=collection_archive---------18-----------------------#2020-08-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="7ffa" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">我的文本分析之旅:文本预处理</h2></div><p id="9a56" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我最近刚在大学里开设了一个新的模块，内容是文本分析。我被教导扮演文字技术员的角色，这很有趣。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/d0ea93599f11dfb7e42066c342307af2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*tZafVnYagYbGEnQa"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">由<a class="ae kj" href="https://unsplash.com/@codestorm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">萨法尔·萨法罗夫</a>在<a class="ae kj" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="5412" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi kk translated">文本预处理是文本分析的重要组成部分。尽管人类阅读和解释大块文本很容易，但对计算机来说却很难，因为它们没有语言的概念。语境也很关键，因为它决定了意义，比如“我喜欢苹果”是指水果还是公司。</p><p id="4a5c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但首先，在我们试图解读文本之前，我们需要对它们进行预处理！以下是阶段和定义，以及示例。</p><h1 id="f707" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">标记化</h1><p id="95d1" class="pw-post-body-paragraph ix iy hi iz b ja ll ij jc jd lm im jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">当我们收到一堆要分析的文本时，我们要做的第一件事就是把它们分成单词和标点符号。你也可以使用python的<code class="du lq lr ls lt b">split()</code>函数来实现。</p><pre class="ju jv jw jx fd lu lt lv lw aw lx bi"><span id="44d3" class="ly ku hi lt b fi lz ma l mb mc">from nltk.tokenize import word_tokenize</span><span id="9330" class="ly ku hi lt b fi md ma l mb mc">tweets = ["This year General Elections is really intense!","Wah GE queues sibeh long... #hot #sweaty", "I have been queueing for too long!", "It will be troubling if youths today do not vote wisely."]</span><span id="2f81" class="ly ku hi lt b fi md ma l mb mc">tokenized_tweet = [word_tokenize(tweet) for tweet in tweets]</span></pre><h1 id="30de" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">文本规范化</h1><p id="4cf7" class="pw-post-body-paragraph ix iy hi iz b ja ll ij jc jd lm im jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">我们将每个单词标准化，以准备对它们进行统一处理。这是通过把所有的词放在一个公平的竞技场上来完成的。有许多方法可以规范化文本，但这是两种流行的方法。</p><h2 id="4284" class="ly ku hi bd kv me mf mg kz mh mi mj ld jg mk ml lf jk mm mn lh jo mo mp lj mq bi translated">堵塞物</h2><p id="2a73" class="pw-post-body-paragraph ix iy hi iz b ja ll ij jc jd lm im jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">词干化就是去掉单词的后缀或前缀。尽管这可能会导致无效或不相关的单词，如源于“麻烦”的“troubl ”,但词干化通常比变元化在更短的运行时间内完成。</p><pre class="ju jv jw jx fd lu lt lv lw aw lx bi"><span id="44e4" class="ly ku hi lt b fi lz ma l mb mc">import nltk<br/>from nltk.tokenize import word_tokenize</span><span id="4e45" class="ly ku hi lt b fi md ma l mb mc">porter = nltk.PorterStemmer()</span><span id="69e9" class="ly ku hi lt b fi md ma l mb mc">tweets = ["This year General Elections is really intense!","Wah GE queues sibeh long... #hot #sweaty", "I have been queueing for too long!", "It will be troubling if youths today do not vote wisely."]<br/>tokenized_tweets = [word_tokenize(tweet) for tweet in tweets]<br/>stemmed_tweets = []</span><span id="832d" class="ly ku hi lt b fi md ma l mb mc">for tweet in tokenized_tweets:<br/>    stemmed_tweets.append([porter.stem(w) for w in tweet])</span></pre><h2 id="12e4" class="ly ku hi bd kv me mf mg kz mh mi mj ld jg mk ml lf jk mm mn lh jo mo mp lj mq bi translated">词汇化</h2><p id="4fc2" class="pw-post-body-paragraph ix iy hi iz b ja ll ij jc jd lm im jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">词汇化就是从一篇文章的实际词根处推导出来。例如，“困扰”的<em class="mr">引理</em>(词根)会是“困扰”。这通常更准确，因为引用了语料库(词典)，但这通常会导致比词干提取更长的运行时间。</p><pre class="ju jv jw jx fd lu lt lv lw aw lx bi"><span id="8a6a" class="ly ku hi lt b fi lz ma l mb mc">from nltk.tokenize import word_tokenize<br/>from nltk.stem import WordNetLemmatizer</span><span id="f399" class="ly ku hi lt b fi md ma l mb mc">WNL = WordNetLemmatizer()</span><span id="fbdb" class="ly ku hi lt b fi md ma l mb mc">tweets = ["This year General Elections is really intense!","Wah GE queues sibeh long... #hot #sweaty", "I have been queueing for too long!", "It will be troubling if youths today do not vote wisely."]<br/>tokenized_tweets = [word_tokenize(tweet) for tweet in tweets]<br/>lemma_tweets = []</span><span id="fc8e" class="ly ku hi lt b fi md ma l mb mc">for tweet in tokenized_tweets:<br/>    lemma_tweets.append([WNL.lemmatize(t,'v') for t in tweet])<br/>print(lemma_tweets)</span></pre><h1 id="556c" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">停用词移除</h1><p id="987e" class="pw-post-body-paragraph ix iy hi iz b ja ll ij jc jd lm im jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">停用词是对给定的句子没有多少意义的词。这些词包括“the”、“a”、“I”。它们通常对句子的整体情绪没有贡献，因此被删除。</p><pre class="ju jv jw jx fd lu lt lv lw aw lx bi"><span id="d60b" class="ly ku hi lt b fi lz ma l mb mc">from nltk.corpus import stopwords</span><span id="c746" class="ly ku hi lt b fi md ma l mb mc">clean_tweets = []<br/>stopwords_list = stopwords.words('english')<br/>tweets = ["This year General Elections is really intense!","Wah GE queues sibeh long... #hot #sweaty", "I have been queueing for too long!", "It will be troubling if youths today do not vote wisely."]<br/>tokenized_tweets = [word_tokenize(tweet) for tweet in tweets]<br/>lemma_tweets = []<br/>for tweet in tokenized_tweets:<br/>    lemma_tweets.append([WNL.lemmatize(t,'v') for t in tweet])</span><span id="3d14" class="ly ku hi lt b fi md ma l mb mc">for t in lemma_tweets:<br/>    clean_tweet = [w for w in t if w.lower() not in stopwords_list]<br/>    clean_tweets.append(" ".join(clean_tweet))<br/>print(clean_tweets)</span></pre><h1 id="8e15" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">噪声消除</h1><p id="c10f" class="pw-post-body-paragraph ix iy hi iz b ja ll ij jc jd lm im jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">噪音是指标签、网址、表情符号等等。它们要么没有给句子增加价值，要么没有意义。去除这一点将有助于突出数据源中句子的关键词。下面的例子将探索移除标签。</p><pre class="ju jv jw jx fd lu lt lv lw aw lx bi"><span id="b9b1" class="ly ku hi lt b fi lz ma l mb mc">import re</span><span id="bad7" class="ly ku hi lt b fi md ma l mb mc">hash_tweets = ["Wah GE queues sibeh long... #hot #sweaty"]</span><span id="7fc6" class="ly ku hi lt b fi md ma l mb mc">for t in hash_tweets:<br/>    t = re.sub('\s#[\S]+',"",t)<br/>    print(t)</span></pre><p id="b3d3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">后来，我意识到' \s#[\w]+'也有效，尽管它的意思不同。\s#[\S]+'实际上是指删除#符号后的任何非空白字符，但是' \s#[\w]+'仅指删除任何字母或数字。我想使用' S '更全面，所以我们可以坚持这样做！</p><h1 id="a8ed" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">这还不是全部，但我们会在这里休息一下</h1><p id="8494" class="pw-post-body-paragraph ix iy hi iz b ja ll ij jc jd lm im jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">文本处理帮助我理解计算机是如何理解单词的。从这个简短的教程中，我能够理解如何在句子中修复单词，删除或替换，直到我们剩下最有价值的单词。</p><p id="e985" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下次见！</p></div></div>    
</body>
</html>