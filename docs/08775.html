<html>
<head>
<title>R-commands for fitting Basic Regression Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于拟合基本回归模型的 r 命令</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/r-commands-for-fitting-basic-regressions-4a1bc1962856?source=collection_archive---------18-----------------------#2020-08-12">https://medium.com/analytics-vidhya/r-commands-for-fitting-basic-regressions-4a1bc1962856?source=collection_archive---------18-----------------------#2020-08-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ih ii ij ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es ig"><img src="../Images/96a72300678be9755789722bbef6bf61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5gngQnww0aEAH_sMuPRiA.png"/></div></div><figcaption class="ir is et er es it iu bd b be z dx translated"><strong class="bd iv">图片来自</strong><a class="ae iw" href="https://twitter.com/rcodes_official" rel="noopener ugc nofollow" target="_blank"><strong class="bd iv">r . codes(@ r codes _ Official)| Twitter</strong></a></figcaption></figure><h2 id="3bed" class="ix iy hi bd iv iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">介绍</h2><p id="0197" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke jh kf kg kh jl ki kj kk jp kl km kn ko hb bi translated">在本文中，我将为您提供所有的<strong class="jw hj"> R 命令</strong>，帮助我们<strong class="jw hj">拟合基本回归模型</strong>。基本上，本文由以下部分组成-</p><ol class=""><li id="1f6b" class="kp kq hi jw b jx kr kb ks jh kt jl ku jp kv ko kw kx ky kz bi translated">拟合简单线性回归<strong class="jw hj">的 r 命令</strong></li><li id="cf6d" class="kp kq hi jw b jx la kb lb jh lc jl ld jp le ko kw kx ky kz bi translated">拟合<strong class="jw hj">多元线性回归</strong>的 r 命令</li><li id="66b1" class="kp kq hi jw b jx la kb lb jh lc jl ld jp le ko kw kx ky kz bi translated">拟合<strong class="jw hj">多项式回归</strong>的 r 命令</li><li id="1599" class="kp kq hi jw b jx la kb lb jh lc jl ld jp le ko kw kx ky kz bi translated">拟合<strong class="jw hj">正交多项式回归的 r 命令</strong></li><li id="b987" class="kp kq hi jw b jx la kb lb jh lc jl ld jp le ko kw kx ky kz bi translated">用λ的最佳值拟合<strong class="jw hj">岭回归的 r 命令</strong></li><li id="397d" class="kp kq hi jw b jx la kb lb jh lc jl ld jp le ko kw kx ky kz bi translated">用λ的最佳值拟合<strong class="jw hj">套索回归的 r 命令</strong></li><li id="2177" class="kp kq hi jw b jx la kb lb jh lc jl ld jp le ko kw kx ky kz bi translated">用超参数的最佳值拟合<strong class="jw hj">弹性网回归的 r 命令</strong></li></ol><p id="bd47" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated"><strong class="jw hj"> 1。拟合简单线性回归的 r 命令</strong></p><p id="c168" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">希望符合以下模型-</p><figure class="lj lk ll lm fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es li"><img src="../Images/39842a36d7c3d3798af21918acf10544.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CSo0DFHOVgwD06rTcWVhLg.png"/></div></div></figure><p id="4b23" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">使用已经包含在 R 的<strong class="jw hj">基础包中的<strong class="jw hj"> lm() </strong>函数如下-</strong></p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="a47d" class="ix iy hi lo b fi ls lt l lu lv">Simple_Linear_Model &lt;- lm(formula = y ~ X  , data =              train_data_with_target_and_predictor)</span></pre><p id="85f0" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">如果想去掉截距项，只需在<strong class="jw hj">公式的</strong>参数中添加<strong class="jw hj"> -1 </strong>，如下-</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="10f5" class="ix iy hi lo b fi ls lt l lu lv">Simple_Linear_Model &lt;- lm(formula = y ~ X <strong class="lo hj">-1</strong>  ,  data = train_data_with_target_and_predictor)</span></pre><p id="490d" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated"><strong class="jw hj"> 2。拟合多元线性回归的 r 命令</strong></p><p id="dc67" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">希望符合以下模型-</p><figure class="lj lk ll lm fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es lw"><img src="../Images/67ce57fdf79290a35600422f310f6ee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dA2yJXxucxes1y42IVe_ng.png"/></div></div></figure><p id="783b" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">再次使用已经包含在 R 的<strong class="jw hj">基础包中的<strong class="jw hj"> lm() </strong>函数，如下-</strong></p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="2e3b" class="ix iy hi lo b fi ls lt l lu lv">Multiple_Linear_Model &lt;- lm(formula = y ~ X_1 + X_2 + ... + X_k  ,  data = train_data_with_target_and_predictor)</span></pre><p id="f490" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">另一种方法是，用一个<strong class="jw hj">单圆点</strong> <strong class="jw hj">(。)</strong>代替如下分别写出所有预测值-</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="2822" class="ix iy hi lo b fi ls lt l lu lv">Multiple_Linear_Model &lt;- lm(formula = Y ~ <strong class="lo hj">.</strong>   ,  data = train_data_with_target_and_predictor)</span></pre><p id="272f" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">如果你想删除一个特定的预测值，比如 X_2，那么只需在<strong class="jw hj">公式</strong>参数中添加<strong class="jw hj"> -X_2 </strong>，如下所示</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="60b0" class="ix iy hi lo b fi ls lt l lu lv">Multiple_Linear_Model &lt;- lm(formula = Y ~ . - X_2 , data = train_data_with_target_and_predictor)</span></pre><p id="fdbb" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">想去掉截距项，只需在<strong class="jw hj">公式中添加<strong class="jw hj">-1</strong></strong>自变量如下-</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="94fc" class="ix iy hi lo b fi ls lt l lu lv">Multiple_Linear_Model &lt;- lm(formula = Y ~ . - 1 , data = train_data_with_target_and_predictor)</span></pre><p id="0cbe" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated"><strong class="jw hj"> 3。拟合多项式回归的 r 命令</strong></p><p id="46e8" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">希望符合以下模型-</p><figure class="lj lk ll lm fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es lx"><img src="../Images/0eeb15ed46b0c28a73a7c75b625e89ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pKTqLvhEYxE4pTi9aqmU_g.png"/></div></div></figure><p id="7aea" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">再次使用已经包含在 R 的<strong class="jw hj">基础包中的<strong class="jw hj"> lm() </strong>函数。另外，在<strong class="jw hj">公式</strong>参数中使用<strong class="jw hj"> I() </strong>来包含高阶预测项，如下所示-</strong></p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="2b54" class="ix iy hi lo b fi ls lt l lu lv">Polynomial_Model &lt;- lm(formula = y ~ X + I(X^2) , data = train_data_with_target_and_predictor)</span></pre><p id="c28c" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">希望符合以下模型-</p><figure class="lj lk ll lm fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es ly"><img src="../Images/715ca66849ad5958e87c8adcbb7aa5fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IMFfeDz3-Ms_gwBO9MPMVA.png"/></div></div></figure><p id="1128" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">用于此目的的 r 代码如下-</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="1037" class="ix iy hi lo b fi ls lt l lu lv">Polynomial_Model &lt;- lm(formula = y ~ X_1 + X_2 + I((X_1)^2) + I((X_2)^2) + X_1:X_2 , data = train_data_with_target_and_predictor)</span></pre><p id="720a" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated"><strong class="jw hj"> 4。拟合正交多项式回归的 r 命令</strong></p><p id="e1a5" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">希望符合以下模型-</p><figure class="lj lk ll lm fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es lz"><img src="../Images/2ddfa3d2ff313fb7a36e5aae3df7afd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lsBxOlrktzXRMmFAIhh8rg.png"/></div></div></figure><p id="4895" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">同样，我们将使用<strong class="jw hj"> lm() </strong>函数。同样，在<strong class="jw hj">公式</strong>参数中使用<strong class="jw hj"> poly() </strong>函数如下-</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="f40a" class="ix iy hi lo b fi ls lt l lu lv">Ortho_Polynomial_Model &lt;- lm(formula = Y ~ poly(X_1 , upto_which_order) + poly(X_2 , upto_which_order) +.....+ poly(X_k , upto_which_order) , data = train_data_with_target_and_predictor)</span></pre><p id="5bb3" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">在上面的代码中，我为每个预测器拟合了正交多项式。但是，如果需要以线性形式包含任何预测值，那么就相应地包含该预测值，就像我在多元回归拟合或简单线性回归中所做的那样。请看下面的代码</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="9c37" class="ix iy hi lo b fi ls lt l lu lv">Ortho_Polynomial_Model &lt;- lm(formula = Y ~ X_1 + poly(X_2 , upto_which_order) +.....+ poly(X_k , upto_which_order) , <br/>data =train_data_with_target_and_predictor)</span></pre><p id="d191" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">在上面的代码中，<strong class="jw hj"> <em class="ma">我已经以线性形式包含了预测因子 X_1，而其他所有预测因子都以正交多项式</em> </strong> <em class="ma">的形式包含了。</em></p><p id="303c" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated"><strong class="jw hj"> 5。用λ的最佳值拟合岭回归的 r 命令</strong></p><p id="b456" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">众所周知，要拟合岭回归，我们必须找出超参数λ的最佳值。为此，使用以下 R 代码-</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="450d" class="ix iy hi lo b fi ls lt l lu lv">cv_for_best_lambda &lt;-  cv.glmnet(x_train, y_train, family = "gaussian", alpha = 0, type.measure = "mse")</span></pre><p id="04c3" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">在上面的代码中，请注意<strong class="jw hj">训练数据集以稍微不同的方式</strong>提供。我们必须分离预测值和目标值，并以<strong class="jw hj">矩阵</strong>的形式提供它们，正如我提供 x_train 和 y_train 一样</p><p id="61bb" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">现在，为了拟合岭回归，使用下面的 R 代码-</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="c6a3" class="ix iy hi lo b fi ls lt l lu lv">ridge = glmnet(x_train, y_train, family = "gaussian",alpha = 0, lambda = cv_for_best_lambda$lambda.min)</span></pre><p id="670d" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">在上面的代码中，您必须注意参数 lambda 的值。<strong class="jw hj"> cv_for_best_lambda </strong>是在拟合脊线恢复的第一步中创建的 R 对象，您必须在<strong class="jw hj"> cv_for_best_lambda </strong>后写入<strong class="jw hj"> $lambda.min </strong></p><p id="b4e4" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">6。用λ的最佳值拟合套索回归的 r 命令</p><p id="5305" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">为了拟合 Lasso 回归，我们必须找出超参数λ的最佳值。为此，使用以下 R 代码-</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="17b1" class="ix iy hi lo b fi ls lt l lu lv">cv_for_best_lambda &lt;-  cv.glmnet(x_train, y_train, family = "gaussian", alpha = 1, type.measure = "mse")</span></pre><p id="bd7b" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">这里再次注意到<strong class="jw hj">训练数据集以稍微不同的方式</strong>提供。我们必须将预测值和目标值分开，并以<strong class="jw hj">矩阵</strong>的形式提供，就像我提供 x_train 和 y_train 一样</p><p id="238c" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">现在，为了拟合 Lasso 回归，使用下面的 R 代码-</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="e43c" class="ix iy hi lo b fi ls lt l lu lv">lasso = glmnet(x_train, y_train, family = "gaussian", alpha = 1, lambda = cv_for_best_lambda$lambda.min)</span></pre><p id="3db8" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">同样，在上面的代码中，您必须注意参数 lambda 的值。<strong class="jw hj"> cv_for_best_lambda </strong>是在拟合套索回归的第一步中创建的 R 对象，您必须在<strong class="jw hj"> cv_for_best_lambda </strong>后写入<strong class="jw hj"> $lambda.min </strong></p><p id="0076" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated"><strong class="jw hj"> 7。用超参数的最佳值拟合弹性网回归的 r 命令</strong></p><p id="9ec6" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">为了拟合弹性网回归，我们必须找出超参数α和λ的最佳值。为此，使用以下 R 代码-</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="1c9f" class="ix iy hi lo b fi ls lt l lu lv"><strong class="lo hj"># Define Training Control</strong><br/>set.seed(123)<br/>train.control &lt;- trainControl(method = "repeatedcv", number = 10, repeats = 3 , search = "random")<br/>                              <br/><strong class="lo hj"># Train the model</strong><br/>cv_for_best_value &lt;- train(target_name ~ ., data = train_data, method="glmnet", trControl = train.control)</span></pre><p id="9f44" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">在上面的代码中，请注意<strong class="jw hj"> train_data </strong>由<strong class="jw hj">目标</strong>和<strong class="jw hj">预测器</strong>组成，位于<strong class="jw hj">数据帧</strong>中。</p><p id="4b49" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">现在，为了适应弹性网回归，使用下面的 R 代码-</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="9227" class="ix iy hi lo b fi ls lt l lu lv">enet &lt;- glmnet(x_train, y_train, alpha = cv_for_best_value$bestTune[1,1] , lambda = cv_for_best_value$bestTune[1,2] , family = "gaussian"</span></pre><p id="ec04" class="pw-post-body-paragraph ju jv hi jw b jx kr jz ka kb ks kd ke jh lf kg kh jl lg kj kk jp lh km kn ko hb bi translated">在上面的代码中，注意参数 alpha 和 lambda 的值。关于脊、套索和弹性网回归的更多信息，请阅读我的文章<a class="ae iw" rel="noopener" href="/analytics-vidhya/multicollinearity-ridge-lasso-elastic-net-regression-using-r-6582cbabf7f3?source=friends_link&amp;sk=9aefa18ff84a10e36d0ab2695172f638"> <strong class="jw hj">多重共线性/脊/套索/弹性网回归</strong> </a></p><blockquote class="mb mc md"><p id="44ee" class="ju jv ma jw b jx kr jz ka kb ks kd ke me lf kg kh mf lg kj kk mg lh km kn ko hb bi translated">直到现在，这些都在这个笔记本里。但是这个笔记本会定期更新。我想在这个笔记本中包含不同回归技术的 R 代码。请与此笔记本保持联系，获取更多 R 代码。</p></blockquote><h2 id="9e4d" class="ix iy hi bd iv iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt bi translated">感谢阅读我的文章！</h2><p id="66e1" class="pw-post-body-paragraph ju jv hi jw b jx jy jz ka kb kc kd ke jh kf kg kh jl ki kj kk jp kl km kn ko hb bi translated">我的 kaggle 笔记本:<a class="ae iw" href="https://www.kaggle.com/pranjalpandey12/r-commands-for-fitting-basic-regressions?scriptVersionId=40603664" rel="noopener ugc nofollow" target="_blank"> <strong class="jw hj">点击</strong> </a></p></div></div>    
</body>
</html>