<html>
<head>
<title>Python Implementation of Andrew Ng’s Machine Learning Course (Part 2.1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">吴恩达机器学习课程的Python实现(第2.1部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-2-1-1a666f049ad6?source=collection_archive---------1-----------------------#2018-09-04">https://medium.com/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-2-1-1a666f049ad6?source=collection_archive---------1-----------------------#2018-09-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/94ebbf40d69a4802596d583eed4026ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FGGge_GilZ_KJYaoryaxkA.png"/></div></div></figure><p id="da54" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我的<a class="ae jo" rel="noopener" href="/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-1-6b8dd1c73d80">上一篇文章</a>中，我们已经讨论了作为第一周和第二周编程作业的一部分的单个和多个独立变量的线性回归的Pythonic实现。现在我们将转到第3周的内容，即逻辑回归。</p><p id="0ca2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">既然这将是一篇相当长的文章，我将把这篇文章分成两部分。请注意第2.2部分，该部分探讨了如何解决过度拟合问题。</p><p id="e390" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你是新来的，我鼓励你阅读我以前的帖子</p><p id="e2ce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" rel="noopener" href="/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-1-6b8dd1c73d80">吴恩达机器学习教程的Python实现(上)</a></p><blockquote class="jp jq jr"><p id="b8fa" class="iq ir js is b it iu iv iw ix iy iz ja jt jc jd je ju jg jh ji jv jk jl jm jn hb bi translated">先决条件</p><p id="32ff" class="iq ir js is b it iu iv iw ix iy iz ja jt jc jd je ju jg jh ji jv jk jl jm jn hb bi translated"><em class="hi">强烈建议您首先观看第三周</em><a class="ae jo" href="https://www.coursera.org/learn/machine-learning/home/week/3" rel="noopener ugc nofollow" target="_blank"><em class="hi"/></a><em class="hi">视频讲座。</em></p><p id="a41d" class="iq ir js is b it iu iv iw ix iy iz ja jt jc jd je ju jg jh ji jv jk jl jm jn hb bi translated"><em class="hi">应该对Python生态系统有基本的了解。</em></p></blockquote><p id="d5fa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里，我们将研究业界使用最广泛的ML算法之一。</p><p id="4c21" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">逻辑回归</strong></p><p id="4689" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这部分练习中，您将构建一个逻辑回归模型来预测学生是否被大学录取。</p><blockquote class="jp jq jr"><p id="2490" class="iq ir js is b it iu iv iw ix iy iz ja jt jc jd je ju jg jh ji jv jk jl jm jn hb bi translated">问题背景</p><p id="b063" class="iq ir js is b it iu iv iw ix iy iz ja jt jc jd je ju jg jh ji jv jk jl jm jn hb bi translated">假设你是一个大学部门的管理人员，你想根据每个申请人两次考试的成绩来决定他们被录取的机会。您有以前申请者的历史数据，可以用作逻辑回归的训练集。对于每个培训示例，您都有申请人的两次考试成绩和录取决定。</p><p id="8e90" class="iq ir js is b it iu iv iw ix iy iz ja jt jc jd je ju jg jh ji jv jk jl jm jn hb bi translated">你的任务是建立一个分类模型，根据这两次考试的分数来估计申请人的录取概率。</p></blockquote><p id="e906" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先让我们加载必要的库。</p><pre class="jw jx jy jz fd ka kb kc kd aw ke bi"><span id="14fc" class="kf kg hi kb b fi kh ki l kj kk">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import scipy.optimize as opt    # more on this later</span></pre><p id="8889" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，我们读取数据(必要的数据可在<a class="ae jo" href="https://www.coursera.org/learn/machine-learning/home/week/3" rel="noopener ugc nofollow" target="_blank"> week-3 </a> content下获得)</p><pre class="jw jx jy jz fd ka kb kc kd aw ke bi"><span id="6e73" class="kf kg hi kb b fi kh ki l kj kk">data = pd.read_csv('ex2data1.txt', header = None)<br/>X = data.iloc[:,:-1]<br/>y = data.iloc[:,2]<br/>data.head()</span></pre><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es kl"><img src="../Images/29c830a974dff5b994e61b1ae75cdb53.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*53FXZu8l1LvBOXLQbnxIoQ.png"/></div></figure><p id="44f0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以我们有两个独立特征和一个因变量。这里的<code class="du km kn ko kb b">0</code>表示考生未能被录取，而<code class="du km kn ko kb b">1</code>则相反。</p><p id="7b54" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">可视化数据</strong></p><p id="4f05" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在开始实现任何学习算法之前，如果可能的话，可视化数据总是好的。</p><pre class="jw jx jy jz fd ka kb kc kd aw ke bi"><span id="4829" class="kf kg hi kb b fi kh ki l kj kk">mask = y == 1<br/>adm = plt.scatter(X[mask][0].values, X[mask][1].values)<br/>not_adm = plt.scatter(X[~mask][0].values, X[~mask][1].values)<br/>plt.xlabel('Exam 1 score')<br/>plt.ylabel('Exam 2 score')<br/>plt.legend((adm, not_adm), ('Admitted', 'Not admitted'))<br/>plt.show()</span></pre><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/58670a4bc6a6c06de7b98c6048a3dd4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*7UhyaaObo3LmCzIziWSBQA.png"/></div></figure><p id="b589" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">实施</strong></p><p id="4230" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在开始实际成本函数之前，回想一下逻辑回归假设使用了sigmoid函数。让我们定义我们的sigmoid函数。</p><p id="d4ad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">乙状结肠功能</strong></p><pre class="jw jx jy jz fd ka kb kc kd aw ke bi"><span id="d0f5" class="kf kg hi kb b fi kh ki l kj kk">def sigmoid(x):<br/>  return 1/(1+np.exp(-x))</span></pre><p id="4347" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意，这里我们写的是矢量化代码。所以不管<code class="du km kn ko kb b">x</code>是标量、矢量、矩阵还是张量都没关系；-).当然，编写和理解矢量化代码需要一些思考(经过一些练习后，任何人都会变得很擅长)。然而，它消除了循环的<em class="js">，也有助于高效和通用的代码。</em></p><p id="9914" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">成本函数</strong></p><p id="f4f6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们实现逻辑回归的成本函数。</p><pre class="jw jx jy jz fd ka kb kc kd aw ke bi"><span id="4186" class="kf kg hi kb b fi kh ki l kj kk">def costFunction(theta, X, y):<br/>    J = (-1/m) * np.sum(np.multiply(y, np.log(sigmoid(X @ theta))) <br/>        + np.multiply((1-y), np.log(1 - sigmoid(X @ theta))))<br/>    return J</span></pre><p id="efba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意，我们在上面的<code class="du km kn ko kb b">costFunction</code>中使用了<code class="du km kn ko kb b">sigmoid</code>函数。</p><blockquote class="jp jq jr"><p id="d71a" class="iq ir js is b it iu iv iw ix iy iz ja jt jc jd je ju jg jh ji jv jk jl jm jn hb bi translated">编码成本函数有多种方法。更重要的是潜在的数学思想和我们将它们转化为代码的能力。</p></blockquote><p id="7471" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">渐变功能</strong></p><pre class="jw jx jy jz fd ka kb kc kd aw ke bi"><span id="7eda" class="kf kg hi kb b fi kh ki l kj kk">def gradient(theta, X, y):<br/>    return ((1/m) * X.T @ (sigmoid(X @ theta) - y))</span></pre><p id="9ae5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">请注意，虽然此梯度看起来与线性回归梯度相同，但公式实际上是不同的，因为线性和逻辑回归对假设函数有不同的定义。</p><p id="a6af" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们用初始参数调用这些函数。</p><pre class="jw jx jy jz fd ka kb kc kd aw ke bi"><span id="bfad" class="kf kg hi kb b fi kh ki l kj kk">(m, n) = X.shape<br/>X = np.hstack((np.ones((m,1)), X))<br/>y = y[:, np.newaxis]<br/>theta = np.zeros((n+1,1)) # intializing theta with all zeros</span><span id="efa6" class="kf kg hi kb b fi kq ki l kj kk">J = costFunction(theta, X, y)<br/>print(J)</span></pre><p id="6fcf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这应该给我们一个j的值<code class="du km kn ko kb b">0.693</code>。</p><p id="6d27" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">使用fmin_tnc学习参数</strong></p><p id="3b7f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在<a class="ae jo" rel="noopener" href="/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-1-6b8dd1c73d80">之前的任务</a>中，我们通过实施梯度下降算法找到了线性回归模型的最佳参数。我们写了一个成本函数，并计算了它的梯度，然后采取梯度下降步骤。这一次，我们将使用<code class="du km kn ko kb b">scipy </code>库中的内置函数<code class="du km kn ko kb b">fmin_tnc</code>，而不是采用梯度下降的步骤。</p><p id="6c04" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du km kn ko kb b">fmin_tnc</code>是一个优化求解器，可找到无约束函数的最小值。对于逻辑回归，您希望使用参数<code class="du km kn ko kb b">theta</code>优化成本函数。</p><blockquote class="jp jq jr"><p id="9b35" class="iq ir js is b it iu iv iw ix iy iz ja jt jc jd je ju jg jh ji jv jk jl jm jn hb bi translated">优化中的约束通常是指对参数的约束。例如，限制可能值<code class="du km kn ko kb b">theta</code>的约束可以取值(例如<code class="du km kn ko kb b">theta</code> ≤ 1)。逻辑回归没有这样的约束，因为<code class="du km kn ko kb b">theta</code>允许取任何实数值。</p></blockquote><p id="2436" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">具体来说，在给定固定数据集(X和y值)的情况下，您将使用<code class="du km kn ko kb b">fmin_tnc</code>找到逻辑回归成本函数的最佳或最优参数<code class="du km kn ko kb b">theta</code>。您将向<code class="du km kn ko kb b">fmin_tnc </code>传递以下输入:</p><ul class=""><li id="73c6" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">我们试图优化的参数的初始值。</li><li id="7b36" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">一个函数，当给定训练集和特定的<code class="du km kn ko kb b">theta</code>时，计算数据集(X，y)相对于<code class="du km kn ko kb b">theta </code>的逻辑回归成本和梯度。</li></ul><pre class="jw jx jy jz fd ka kb kc kd aw ke bi"><span id="426a" class="kf kg hi kb b fi kh ki l kj kk">temp = opt.fmin_tnc(func = costFunction, <br/>                    x0 = theta.flatten(),fprime = gradient, <br/>                    args = (X, y.flatten()))<br/>#the output of above function is a tuple whose first element #contains the optimized values of theta<br/>theta_optimized = temp[0]<br/>print(theta_optimized)</span></pre><blockquote class="jp jq jr"><p id="b0af" class="iq ir js is b it iu iv iw ix iy iz ja jt jc jd je ju jg jh ji jv jk jl jm jn hb bi translated"><em class="hi">关于</em> <code class="du km kn ko kb b"><a class="ae jo" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html" rel="noopener ugc nofollow" target="_blank"><em class="hi">flatten()</em></a></code> <em class="hi">功能的说明:不幸的是</em> <code class="du km kn ko kb b"><em class="hi">scipy’s fmin_tnc </em></code> <em class="hi">不能很好地与列或行向量一起工作。它要求参数采用数组格式。</em> <code class="du km kn ko kb b"><em class="hi">flatten()</em></code> <em class="hi">函数将一个列或行向量缩小成数组格式。</em></p></blockquote><p id="c7a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的代码应该给<code class="du km kn ko kb b">[-25.16131862, 0.20623159, 0.20147149]</code>。</p><p id="d585" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果您已经正确地完成了<code class="du km kn ko kb b">costFunction </code>，那么<code class="du km kn ko kb b">fmin_tnc</code>将收敛到正确的优化参数，并返回<code class="du km kn ko kb b">theta</code>的最终值。请注意，通过使用<code class="du km kn ko kb b">fmin_tnc</code>，您不必自己编写任何循环，也不必像梯度下降那样设置学习速率。这都是由<code class="du km kn ko kb b">fmin_tnc</code> :-)完成的，你只需要提供一个计算成本和梯度的函数。</p><p id="d94f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们使用这些优化的<code class="du km kn ko kb b">theta</code>值来计算成本。</p><pre class="jw jx jy jz fd ka kb kc kd aw ke bi"><span id="55f8" class="kf kg hi kb b fi kh ki l kj kk">J = costFunction(theta_optimized[:,np.newaxis], X, y)<br/>print(J)</span></pre><p id="c8ea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您应该会看到一个值<code class="du km kn ko kb b">0.203</code>。将此与使用初始<code class="du km kn ko kb b">theta</code>获得的成本<code class="du km kn ko kb b">0.693 </code>进行比较。</p><p id="212d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">绘制决定边界(可选)</strong></p><p id="dc7c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个最终的<code class="du km kn ko kb b">theta</code>值将用于在训练数据上绘制决策边界，产生一个类似于下图的图。</p><pre class="jw jx jy jz fd ka kb kc kd aw ke bi"><span id="6521" class="kf kg hi kb b fi kh ki l kj kk">plot_x = [np.min(X[:,1]-2), np.max(X[:,2]+2)]<br/>plot_y = -1/theta_optimized[2]*(theta_optimized[0] <br/>          + np.dot(theta_optimized[1],plot_x))  </span><span id="a00f" class="kf kg hi kb b fi kq ki l kj kk">mask = y.flatten() == 1<br/>adm = plt.scatter(X[mask][:,1], X[mask][:,2])<br/>not_adm = plt.scatter(X[~mask][:,1], X[~mask][:,2])<br/>decision_boun = plt.plot(plot_x, plot_y)<br/>plt.xlabel('Exam 1 score')<br/>plt.ylabel('Exam 2 score')<br/>plt.legend((adm, not_adm), ('Admitted', 'Not admitted'))<br/>plt.show()</span></pre><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/6e4da986083407c1c8e57b277f6e8e93.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*ozTMFz2neE890gXJG-YrWw.png"/></div></figure><p id="3d64" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">看起来我们的模型在区分被录取的学生和没被录取的学生方面做得很好。现在让我们量化我们的模型精度，为此我们将写一个函数，名为<code class="du km kn ko kb b">accuracy</code></p><pre class="jw jx jy jz fd ka kb kc kd aw ke bi"><span id="a924" class="kf kg hi kb b fi kh ki l kj kk">def accuracy(X, y, theta, cutoff):<br/>    pred = [sigmoid(np.dot(X, theta)) &gt;= cutoff]<br/>    acc = np.mean(pred == y)<br/>    print(acc * 100)</span><span id="3625" class="kf kg hi kb b fi kq ki l kj kk">accuracy(X, y.flatten(), theta_optimized, 0.5)</span></pre><p id="1c55" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这应该给我们一个<code class="du km kn ko kb b">89%</code>的准确度分数。嗯…还不错。</p><p id="bca6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你现在已经学会了如何进行逻辑回归。干得好！</p><p id="274c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个帖子到此为止。如果你喜欢我的作品，请给我一个(或几个)掌声。</p><p id="4853" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以在这里找到这个系列的下一篇文章<a class="ae jo" rel="noopener" href="/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-2-2-dceff1a12a12"/>。</p></div></div>    
</body>
</html>