<html>
<head>
<title>Running TPU (Tensor Processing Unit) in Kaggle</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Kaggle 中运行 TPU(张量处理单元)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/running-tpu-tensor-processing-unit-in-kaggle-96fbce4684d2?source=collection_archive---------9-----------------------#2020-10-27">https://medium.com/analytics-vidhya/running-tpu-tensor-processing-unit-in-kaggle-96fbce4684d2?source=collection_archive---------9-----------------------#2020-10-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="36ba" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">你的 GPU 配额快用完了，但你还有其他神经网络要训练。你能做什么？</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/95339947b89a715109d19aa8f4db8cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*x0Q6xeBMGb1LfqQI.jpg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图一。张量处理单元(TPU)。来源:<a class="ae jn" href="https://storage.googleapis.com/kaggle-media/tpu/tpuv3angle.jpg" rel="noopener ugc nofollow" target="_blank">https://storage . Google APIs . com/ka ggle-media/TPU/TPU v3 angle . jpg</a></figcaption></figure><h1 id="42af" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">介绍</h1><p id="8fa0" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">用 GPU 训练神经网络无疑比 CPU 快得多。然而，并不是所有的深度学习爱好者都拥有一台配备了像样 GPU 的计算机。我自己用的是 Nvidia GeForce MX150 笔记本电脑，但我很少用它来训练神经网络，因为我觉得即使只训练了几分钟，我的笔记本电脑也变得非常热。由于我经常做深度学习的事情，我不认为长时间加热我的笔记本电脑是一个好主意。</p><p id="7355" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">这就是我现在使用 Kaggle 的基本原因。但我在这里遇到了一个问题:它每周只给我 42 小时来运行与 GPU 的交互会话。但是，相信我，如果你不是像我一样的深度学习狂人，那么一周 42 小时的编码应该足够了。然而，如果你愿意，你可以使用 TPU(张量处理单元)来训练你的神经网络，它也是由 Kaggle 提供的，每周 30 小时。现在每周 42+30 = 72 小时真的很多了吧？</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lh"><img src="../Images/b42dc20b5acf2939bd896c2145e3c9aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PTJ25UO-HkOw8oQTw6r-Pw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图二。GPU (42 小时)和 TPU (30 小时)配额由 Kaggle 提供。</figcaption></figure><p id="9e81" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">有一点要记住:神经网络不会仅仅通过打开笔记本上的 TPU 加速器来训练。为了实际运行 TPU，我们需要添加一些代码。因此，在这篇文章中，我想解释一下如何使用这个神经网络加速器，以及它如何影响训练持续时间。</p></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><h1 id="d281" class="jo jp hi bd jq jr lp jt ju jv lq jx jy io lr ip ka ir ls is kc iu lt iv ke kf bi translated">模块，CIFAR-10 数据集，CNN 模型</h1><p id="5fda" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">在我们开始之前，我想先通知你，我们在这里要做的深度学习项目基本上已经在我之前的文章中解释过了(CIFAR-10 图像分类任务)。如果你想知道它的细节，请点击下面的链接查看，因为在这里我将更关注 TPU 是如何被用于训练的。</p><div class="lu lv ez fb lw lx"><a href="https://becominghuman.ai/cifar-10-image-classification-fd2ace47c5e8" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hj fi z dy mc ea eb md ed ef hh bi translated">CIFAR-10 图像分类</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">如何用 CNN 教机器区分图像？</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">becominghuman.ai</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml jh lx"/></div></div></a></div><p id="51de" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">好了，下面是我们需要导入的所有必需模块。请注意，在下面代码的最后一行，我导入了<em class="mm">时间</em>模块，这将有助于精确计算训练持续时间。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mn mo l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图 3。正在导入模块。</figcaption></figure><p id="ea78" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">接下来，我们将创建一个函数来完成整个预处理阶段。关于下面代码的所有解释已经在上面显示链接的文章中讨论过了。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mn mo l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图 4。整个预处理阶段。</figcaption></figure><p id="1df3" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">现在，对于模型本身，我们也将把它包装在一个<em class="mm"> create_model() </em>函数中。我这样做是因为基本上我会创建另外两个函数，分别是<em class="mm">run _ with _ TPU()</em>和<em class="mm"> run_with_tpu() </em>，其中<em class="mm"> create_model() </em>会在这两个函数内部被调用。这样做比手动定义神经网络的每一层两次更简单。下面是我们的神经网络结构的样子。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mn mo l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图 5。创建模型的函数。</figcaption></figure></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><h1 id="605e" class="jo jp hi bd jq jr lp jt ju jv lq jx jy io lr ip ka ir ls is kc iu lt iv ke kf bi translated">创建不带和带 TPU 运行的函数</h1><p id="1fd6" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">我觉得<em class="mm"> run_without_tpu() </em>的函数名是不言自明的。下面的代码简单地运行，首先初始化 CNN 模型，然后是 training 命令。这里的<em class="mm"> start </em>变量用于存储当前时间，其值将用于计算整个训练持续时间。</p><p id="a66e" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">下面的代码没有什么特别的，因为它基本上就像我们在训练深度学习模型时通常做的一样。我们可以使用这段代码在 CPU 或 GPU 上训练模型，这取决于您是否激活了 GPU 加速器。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mn mo l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图 6。无需 TPU 即可训练神经网络的函数。</figcaption></figure><p id="ef21" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">另一方面，如果我们想在 tpu 上训练模型，我们应该使用<em class="mm"> run_with_tpu() </em>函数。下面图 7 中的代码实际上是基于 Kaggle 的文档[3]给出的模板，其中的步骤基本上可以分为几个部分:</p><ol class=""><li id="f240" class="mp mq hi ki b kj lc km ld kp mr kt ms kx mt lb mu mv mw mx bi translated">初始化 TPU</li><li id="cb67" class="mp mq hi ki b kj my km mz kp na kt nb kx nc lb mu mv mw mx bi translated">初始化策略</li><li id="8657" class="mp mq hi ki b kj my km mz kp na kt nb kx nc lb mu mv mw mx bi translated">在初始化的策略中构建模型</li><li id="fc3c" class="mp mq hi ki b kj my km mz kp na kt nb kx nc lb mu mv mw mx bi translated">像往常一样训练模型</li></ol><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mn mo l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图 7。用 TPU 训练神经网络的函数。</figcaption></figure></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><h1 id="7f36" class="jo jp hi bd jq jr lp jt ju jv lq jx jy io lr ip ka ir ls is kc iu lt iv ke kf bi translated">如何使用以上所有功能</h1><p id="331e" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">现在，既然我们已经创建了所有需要的函数，那么我们可以开始实际调用它们来运行培训。首先，调用<em class="mm"> preprocess_data() </em>函数加载所需的数据集及其预处理步骤。返回值分别存储在<em class="mm"> X_train </em>、<em class="mm"> y_train </em>、<em class="mm"> X_test </em>、<em class="mm"> y_test </em>和<em class="mm"> input_shape </em>中。如果您想了解我是如何定义这些回报的，只需查看图 4 中显示的代码。</p><p id="ddd8" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">接下来，我们可以调用<em class="mm"> run_with_tpu() </em>函数来开始训练。在我们运行完图 8 中的所有代码后，进度条应该会立即出现。如果你想使用 CPU 或 GPU，我们可以将第 3 行改为<em class="mm"> run_without_tpu() </em>。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mn mo l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图 8。调用我们之前定义的函数。</figcaption></figure><p id="034a" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">为了用 TPU 加速器训练一个神经网络模型，这几乎是你需要做的所有事情。CPU、GPU 和 TPU 的性能比较将在下一节讨论。</p></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><h1 id="f476" class="jo jp hi bd jq jr lp jt ju jv lq jx jy io lr ip ka ir ls is kc iu lt iv ke kf bi translated">性能比较</h1><p id="ed4f" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">根据许多消息来源(如[1]和[2])，TPU 比其他人跑得更快。但在这里我想亲自测试一下。注意，为了公平比较，在下面的图 9 中，我将所有实验的批处理大小设置为 32，epoch 设置为 20。请记住，我们并不真正关心最终的准确性分数，因为这个实验的目的只是为了找出 TPU 是否真的比 GPU 运行得更快。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nd"><img src="../Images/31dc6b04d1920585b71148953f8dff4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WanOv7prYAYknLL8VK5d8Q.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图 9。不同加速器之间的训练持续时间比较。</figcaption></figure><p id="559b" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">根据上面的实验结果，看起来 TPU 的运行速度比 CPU 快大约 1.89 倍。嗯，这是相当快，但我们仍然看到，GPU 完成整个训练过程更快，速度比 CPU 快 3.96 倍。但是为什么会这样呢？</p><p id="ae96" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">Kaggle 的文档[3]提到，我们需要使用 128 的批处理大小，以便使 TPU 以其最高性能运行。因此，我尝试在不同的批量大小之间进行比较，同时坚持使用值 20 作为纪元。实验总结如图 10 所示。请注意，这里我没有包括 CPU 的训练持续时间，因为它将远远高于条形图中的其他时间。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ne"><img src="../Images/e5b8d90a00d097bb67d911e028be812e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r7U2z0iunslpp-jSp6h0cA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图 10。不同批量之间的训练持续时间比较。</figcaption></figure><p id="f632" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">我想我在这里找到了一些有趣的东西。我们可以从上面的图 10 中看到，当我们使用 TPU 时，增加批量极大地减少了训练持续时间。事实上，GPU 显示了类似的行为，但训练时间的减少没有我们在 TPU 看到的那么显著。此外，我认为使用 128 批次大小的 TPU 没有 Kaggle 在他们的文档中声称的那么快，因为它仍然需要比 GPU 所实现的更长的时间。</p><p id="b852" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">当我们使用 512 或更大的批量时，TPU 的训练速度实际上高于 GPU。但是重要的是要记住，使用如此大规模的批处理并不常见——嗯，至少出于某些原因，我通常使用相对较小的值。</p><p id="ad5f" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">按照[4]的说法，TPU 确实是针对大的训练批量进行优化的，而 GPU 在处理较小的批量时相对更快。然而，仅仅因为这个原因而增加批量的大小并不总是最好的解决方案，因为大批量可能会导致神经网络分类器收敛得非常慢。事实上，图 11 所示的结果实验证明了这一点。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nf"><img src="../Images/55e85184a95de1dfd4066ccb25f280fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mHRV0G1LSctWilfMhOgfGQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图 11。批量大小如何影响培训过程。</figcaption></figure><p id="5ec0" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">同样重要的是要知道，1024 批次大小可能最终达到高精度以及较小的大小，但它将需要几个更多的纪元，这也导致总训练持续时间的增加。因此，如果我说，选择使用 TPU 或 GPU 是一种超参数调整。</p></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><p id="310f" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">事实上，我得到了另一个可能不太重要的东西，但对我来说它有点有趣，所以我还是把它写了下来。也就是说，当我们使用 GPU 开始训练时，我们会看到每个历元所用的时间是相对稳定的。你可以看到下面这个块，在批量为 256 的情况下，GPU 每次迭代训练模型需要 3 秒。</p><pre class="iy iz ja jb fd ng nh ni nj aw nk bi"><span id="54cd" class="nl jp hi nh b fi nm nn l no np">Epoch 1/20<br/>49/49 [==============================] - <strong class="nh hj">3s 64ms/step</strong> - loss: 2.3475 - acc: 0.1031 - val_loss: 2.2463 - val_acc: 0.1433<br/>Epoch 2/20<br/>49/49 [==============================] - <strong class="nh hj">3s 62ms/step</strong> - loss: 2.1589 - acc: 0.1840 - val_loss: 2.0372 - val_acc: 0.2417<br/>Epoch 3/20<br/>49/49 [==============================] - <strong class="nh hj">3s 61ms/step</strong> - loss: 1.9511 - acc: 0.2610 - val_loss: 1.8399 - val_acc: 0.3026<br/>Epoch 4/20<br/>49/49 [==============================] - <strong class="nh hj">3s 60ms/step</strong> - loss: 1.8450 - acc: 0.2965 - val_loss: 1.8113 - val_acc: 0.3169<br/>Epoch 5/20<br/>49/49 [==============================] - <strong class="nh hj">3s 60ms/step</strong> - loss: 1.7943 - acc: 0.3229 - val_loss: 1.7674 - val_acc: 0.3455</span></pre><p id="e760" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">另一方面，当我使用 TPU 加速器时，它显示第一个历元需要 5 秒来完成，但是在随后的迭代中这个速度降低到只有 1 秒。培训详情显示在下面的方框中。</p><pre class="iy iz ja jb fd ng nh ni nj aw nk bi"><span id="e6c1" class="nl jp hi nh b fi nm nn l no np">Epoch 1/20<br/>49/49 [==============================] - <strong class="nh hj">5s 110ms/step</strong> - acc: 0.1101 - loss: 2.3297 - val_acc: 0.1746 - val_loss: 2.2359<br/>Epoch 2/20<br/>49/49 [==============================] - <strong class="nh hj">1s 26ms/step</strong> - acc: 0.2218 - loss: 2.1254 - val_acc: 0.2474 - val_loss: 2.0586<br/>Epoch 3/20<br/>49/49 [==============================] - <strong class="nh hj">1s 26ms/step</strong> - acc: 0.2797 - loss: 1.9639 - val_acc: 0.3286 - val_loss: 1.8029<br/>Epoch 4/20<br/>49/49 [==============================] - <strong class="nh hj">1s 26ms/step</strong> - acc: 0.3454 - loss: 1.7732 - val_acc: 0.3861 - val_loss: 1.6769<br/>Epoch 5/20<br/>49/49 [==============================] - <strong class="nh hj">1s 26ms/step</strong> - acc: 0.3902 - loss: 1.6665 - val_acc: 0.4260 - val_loss: 1.5748</span></pre><p id="56e6" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">老实说，我实际上一直在网上寻找它为什么会那样的答案，但是到目前为止我还是一无所获。我会更新这篇文章，如果我已经有一个。</p></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><h1 id="f823" class="jo jp hi bd jq jr lp jt ju jv lq jx jy io lr ip ka ir ls is kc iu lt iv ke kf bi translated">结论</h1><p id="e70d" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">总之，我们已经学习了如何使用 TPU 加速器通过添加几行代码来训练一个模型，模板可以从图 7 中获得。</p><p id="548a" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">谈到性能，我们在即将对批量相对较小的模型进行训练时，可以使用 GPU。否则，如果我们想要大批量训练我们的模型，应该选择 TPU。但是重要是要注意，较大的批量也可能导致较慢的收敛，这迫使您添加更多的历元来获得高精度分数，因此也消耗更多的时间。</p><p id="6625" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">回到我的案例，我自己更喜欢先使用 GPU，直到我用完 Kaggle 提供的配额，因为我通常使用相对较小的批处理大小。同时，如果我不再有 GPU 配额，TPU 将是我的唯一选择。CPU 呢？我想我永远不会在我的深度学习项目中使用它，因为它占用了我很多时间。</p><p id="a426" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">今天的文章到此为止。感谢阅读！</p></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><p id="fbfa" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">顺便说一下，这是这个项目中使用的全部代码。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mn mo l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图 12。这个项目中使用的全部代码。</figcaption></figure></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><h1 id="ba97" class="jo jp hi bd jq jr lp jt ju jv lq jx jy io lr ip ka ir ls is kc iu lt iv ke kf bi translated">参考</h1><p id="1b6d" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">[1]在 Paul Mooney 的 Kaggle 竞赛中何时使用 CPUs vs GPUs vs TPUs。<a class="ae jn" href="https://towardsdatascience.com/when-to-use-cpus-vs-gpus-vs-tpus-in-a-kaggle-competition-9af708a8c3eb" rel="noopener" target="_blank">https://towards data science . com/when-to-use-CPU-vs-GPU-vs-tpus-in-a-ka ggle-competition-9af 708 a 8 C3 EB</a></p><p id="eb7d" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">[2]你应该选择 GPU 还是 TPU 来训练你的机器学习模型？作者高拉夫·贝兰尼。<a class="ae jn" href="https://www.predictiveanalyticsworld.com/machinelearningtimes/should-you-choose-a-gpu-or-a-tpu-to-train-your-machine-learning-models/10460/" rel="noopener ugc nofollow" target="_blank">https://www . predictiveanalyticsworld . com/machine learning times/should-you-choose-a-GPU-or-a-TPU-to-train-your-machine-learning-models/10460/</a></p><p id="ad4d" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">【3】如何使用 Kaggle？<a class="ae jn" href="https://www.kaggle.com/docs/tpu" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/docs/tpu</a></p><p id="4dfe" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">[4] TPU Vs GPU Vs CPU:深度学习应该选择哪个硬件 Ambika Choudhury。<a class="ae jn" href="https://analyticsindiamag.com/tpu-vs-gpu-vs-cpu-which-hardware-should-you-choose-for-deep-learning/" rel="noopener ugc nofollow" target="_blank">https://analyticsindiamag . com/TPU-vs-GPU-vs-CPU-你应该选择哪个硬件用于深度学习/ </a></p></div><div class="ab cl li lj gp lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="hb hc hd he hf"><p id="0b2b" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><strong class="ki hj">给我的印尼朋友们的提示。</strong>很高兴通知你，我刚刚写完一本电子书(印度尼西亚语)。这本书的题目是《用 Matplotlib 和 Seaborn 实现数据可视化》。顾名思义，它讲述了如何使用两个 Python 模块来可视化数据。它只需要 50，000 印尼盾。如果你有兴趣，请随时通过 books.by.ardi@gmail.com 打电话给我！</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nq"><img src="../Images/602b7c4fbd580ef383fc9635a7985c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bM-xmCUCbtzJYkpcj_-0qg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">书的封面是什么样的。</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nr"><img src="../Images/27d957d6135e7cf7a29a5b30eb82a197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FJO2vmlZxjrUaO9ktLperQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图书内容截图。</figcaption></figure></div></div>    
</body>
</html>