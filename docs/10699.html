<html>
<head>
<title>Get to know Markov, and how he decide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解马尔科夫，以及他是如何决定</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/get-to-know-markov-and-how-he-decide-dd6deb3254da?source=collection_archive---------16-----------------------#2020-10-30">https://medium.com/analytics-vidhya/get-to-know-markov-and-how-he-decide-dd6deb3254da?source=collection_archive---------16-----------------------#2020-10-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="60e5" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">马尔可夫决策过程作为一个强化学习框架JML x AWS深赛车Bootcamp Pt。2</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/3dd86f278882cf34a1281b545c86ec78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ut0A5qpTlcQVyDbL"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">弗拉季斯拉夫·巴比延科在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="f7bc" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在本系列的<a class="ae jn" href="https://nickodemus-r.medium.com/how-a-self-driving-car-works-9956274ca6f6" rel="noopener"> <strong class="jq hj">第一部分</strong> </a>中，我们采用了什么是<strong class="jq hj"> <em class="kk">强化学习</em> </strong>的高级方法，并在AWS Deepracer控制台上进行了直升机视图。现在，我们将更仔细地看看如何用数学方法处理强化学习概念。</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h2 id="69cd" class="ks kt hi bd ku kv kw kx ky kz la lb lc jx ld le lf kb lg lh li kf lj lk ll lm bi translated">马尔可夫链</h2><p id="68c3" class="pw-post-body-paragraph jo jp hi jq b jr ln ij jt ju lo im jw jx lp jz ka kb lq kd ke kf lr kh ki kj hb bi translated">20世纪初，俄罗斯数学家安德烈·马尔科夫(Andrey Markov)学习了没有记忆的随机过程，后来被称为马尔科夫链(Markov chain)。它是过程序列的统计模型，其中事件下一状态的预测概率完全取决于当前状态。例如，我们想预测天气，现在(<strong class="jq hj"> <em class="kk">当前状态</em> </strong>)有风，一小时后(<strong class="jq hj"> <em class="kk">下一状态</em> </strong>)会下雨。下一个小时下雨的概率将只取决于当前的风况，我们甚至不关心上一个小时是晴天还是雷雨。从当前状态到下一个状态的概率称为<strong class="jq hj"> <em class="kk">转移概率</em>。</strong>马尔可夫链可以分为两种类型，离散型和连续型。</p><h2 id="42b2" class="ks kt hi bd ku kv kw kx ky kz la lb lc jx ld le lf kb lg lh li kf lj lk ll lm bi translated">马尔可夫奖励过程</h2><p id="e0b7" class="pw-post-body-paragraph jo jp hi jq b jr ln ij jt ju lo im jw jx lp jz ka kb lq kd ke kf lr kh ki kj hb bi translated">马尔可夫链的一个扩展是<em class="kk">马尔可夫奖励过程。</em>在这个概念中，我们引入了<em class="kk">奖励函数的概念。因此，模型将在其选择的状态上获得奖励，奖励可以是正的，也可以是负的。</em></p><h2 id="6d0a" class="ks kt hi bd ku kv kw kx ky kz la lb lc jx ld le lf kb lg lh li kf lj lk ll lm bi translated">马尔可夫决策过程(MDP)</h2><p id="1dfc" class="pw-post-body-paragraph jo jp hi jq b jr ln ij jt ju lo im jw jx lp jz ka kb lq kd ke kf lr kh ki kj hb bi translated">基于马尔可夫链和马尔可夫奖励过程，我们接着定义了一个马尔可夫决策过程(MDP)，一个对<em class="kk">强化学习</em>问题进行数学建模的框架。在强化学习中，有一个学习者，<strong class="jq hj"> <em class="kk">智能体，</em> </strong>从它之外的一切事物中学习，<strong class="jq hj"> <em class="kk">环境</em>。</strong>环境给代理一个<strong class="jq hj"> <em class="kk">状态</em> </strong>，代理用一个<strong class="jq hj"> <em class="kk">动作</em>来响应。</strong>基于其行动，代理人将获得<strong class="jq hj"> <em class="kk">奖励</em> </strong>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ls"><img src="../Images/6d2edc9b446d588dd66a0e4bac54c443.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rIF3NX0BJtnGoWLb83ElDw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">《强化学习导论》，作者:R .萨顿&amp; A .巴尔托(麻省理工学院出版社)</a></figcaption></figure><p id="ac6f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">所以在MDP有一些关键词:</p><ul class=""><li id="8e01" class="lt lu hi jq b jr js ju jv jx lv kb lw kf lx kj ly lz ma mb bi translated"><strong class="jq hj">州</strong></li><li id="bf3a" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj ly lz ma mb bi translated"><strong class="jq hj">动作</strong></li><li id="e8b8" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj ly lz ma mb bi translated"><strong class="jq hj">转移矩阵</strong></li><li id="780e" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj ly lz ma mb bi translated"><strong class="jq hj">奖励</strong></li><li id="f015" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj ly lz ma mb bi translated"><strong class="jq hj">政策</strong></li></ul><p id="9d70" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如前所述，马尔可夫链是过程序列。在给定的时间步长内，主体-环境交互将遵循这一过程顺序<em class="kk"> t = 0，1，2，3，… </em>在给定的时间步长内，将有一个状态、一个动作和一个奖励。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mh"><img src="../Images/5048475351acfb05193baec6b2859155.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*l9h1pvlMZ-AB40o9dszvrw.png"/></div></figure><p id="bf8b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对于那些喜欢看数学方程的人来说，给定<em class="kk">之前的</em>状态和动作的特定值，那么在时间<em class="kk"> t，</em>出现值的概率可以写成</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mi"><img src="../Images/9308b321d41af580464a111a8494d4f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*MnLBYKxR9d9oGkTAyPNSsg.png"/></div></figure><p id="61aa" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这个概率也被称为<strong class="jq hj"><em class="kk"/></strong><em class="kk">。</em>当我们把它写成矩阵形式时，我们可以把它定义为<strong class="jq hj"> <em class="kk">转移矩阵</em> </strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mj"><img src="../Images/681fa000bf65d7f250b640ee8c01ce9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PLrJJ7pO4lOvHwj7G8a6aQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="noopener ugc nofollow" target="_blank">MDP的例子，来源:用Scikit-Learn、Keras和TensorFlow进行机器实践学习，第二版，作者Aurélien Géron </a></figcaption></figure><p id="47f9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们在图上看到s <em class="kk"> tate </em>是圆形节点，<em class="kk">动作</em>是菱形节点，正奖励是心，负奖励是火。因此，在状态0时，代理可以选择给定概率为0.7的动作a0来获得奖励+10或概率为0.3并移动到状态1，或采取概率为0.2的动作a2来移动到状态1或概率为0.8的动作来返回状态0，依此类推。</p><h2 id="0477" class="ks kt hi bd ku kv kw kx ky kz la lb lc jx ld le lf kb lg lh li kf lj lk ll lm bi translated"><em class="mk">奖励</em></h2><p id="c6dd" class="pw-post-body-paragraph jo jp hi jq b jr ln ij jt ju lo im jw jx lp jz ka kb lq kd ke kf lr kh ki kj hb bi translated">我们读了很多关于奖励的书，但是什么是奖励呢？奖励可以说是环境基于主体的行动而给予主体的信号。奖励可以是正数或负数，但不能是零(0)。如果我们选择零作为报酬，那么代理人不能从它的行为中学到任何东西。它不会知道这个行为是好是坏。在强化学习中我们有<em class="kk">奖励假设</em>的想法</p><blockquote class="ml"><p id="e5b3" class="mm mn hi bd mo mp mq mr ms mt mu kj dx translated">我们所说的目标和目的都可以被认为是接收到的标量信号的累积和的期望值的最大化(称为回报)。</p></blockquote><p id="6566" class="pw-post-body-paragraph jo jp hi jq b jr mv ij jt ju mw im jw jx mx jz ka kb my kd ke kf mz kh ki kj hb bi translated">因此，代理在强化学习中的目标是最大化期望的累积回报。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es na"><img src="../Images/80338c277ef808d3f37e754f409668d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z0oJ1BXcvZLvOxjz"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">由<a class="ae jn" href="https://unsplash.com/@peter_mc_greats?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">彼得罗·德·格兰迪</a>在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="6297" class="ks kt hi bd ku kv kw kx ky kz la lb lc jx ld le lf kb lg lh li kf lj lk ll lm bi translated">插曲</h2><p id="9762" class="pw-post-body-paragraph jo jp hi jq b jr ln ij jt ju lo im jw jx lp jz ka kb lq kd ke kf lr kh ki kj hb bi translated">当我们谈论<em class="kk">有限</em> MDP时，代理人不会无限地做序列。所以有一个终态。从代理开始与环境交互直到终端状态的时间称为事件。在模拟中训练自动驾驶汽车的例子，当汽车停止时，无论是因为它已经越过终点线，撞车，还是作为一个情节脱离轨道。然后汽车又从起点出发，直到又一次停下来。基于报酬假设，代理人希望获得最大的预期累积报酬。一集获得的累计<em class="kk">奖励</em>称为<em class="kk">回报。</em>如果我们把return表示为G，它可以写成这样，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nb"><img src="../Images/3d2f0871ab54880ff53f7ba5a5c84e13.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*g0WjtBMcDDluJTccuaGLEQ.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nc"><img src="../Images/06f88efba9b3a05c63bd4c2b22ef83eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yEnyw33ToAev5Qhq"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">兰迪·法特在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="388b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们可以通过使用<strong class="jq hj"> <em class="kk">贴现因子γ来控制是考虑更多的眼前报酬还是未来报酬。</em> </strong>这是一个介于0和1之间的数字。如果我们更多地考虑<em class="kk">即时奖励</em>我们可以将数字设置为接近0，如果我们想更多地考虑<em class="kk">未来奖励</em>那么将因子设置为接近1。那么如何贯彻这个理念呢？想象下象棋，如果代理的目标是赢得象棋比赛。代理人将获得积极的回报时，它可以杀死对手的棋子，但从长远来看，赢得国际象棋比赛是杀死国王。因此，当我们更加重视<em class="kk">即时</em>奖励时，代理人将最终学会杀死尽可能多的兵，并可能最终输掉游戏，因为它采取了错误的行动，导致其国王被杀。相反，如果我们更重视未来的回报，也许代理人不会学会杀死许多棋子，但它可以学会采取更好的策略来获得对手的王。</p><h2 id="767e" class="ks kt hi bd ku kv kw kx ky kz la lb lc jx ld le lf kb lg lh li kf lj lk ll lm bi translated">政策</h2><p id="31b2" class="pw-post-body-paragraph jo jp hi jq b jr ln ij jt ju lo im jw jx lp jz ka kb lq kd ke kf lr kh ki kj hb bi translated">我们已经了解了代理、状态、转移矩阵和奖励。现在，什么是政策？实际上，策略是我们给定的规则，用来定义代理在环境中的行为。策略告诉代理在每个状态下要执行的操作。有两种类型的策略。</p><ol class=""><li id="adc4" class="lt lu hi jq b jr js ju jv jx lv kb lw kf lx kj nd lz ma mb bi translated">确定性策略 →告诉代理在给定的状态下该做什么。如果状态为X，则Y</li><li id="f789" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj nd lz ma mb bi translated"><strong class="jq hj">随机策略→ </strong>告诉代理在给定状态下行动的概率。如果状态是X，那么用概率z做Y。</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ne"><img src="../Images/6d1692797bc28ca998619286a76c6937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wq92elHqzsDCEGyQCOxqTg.png"/></div></div></figure></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h2 id="a07c" class="ks kt hi bd ku kv kw kx ky kz la lb lc jx ld le lf kb lg lh li kf lj lk ll lm bi translated">MDP和强化学习</h2><p id="4c8f" class="pw-post-body-paragraph jo jp hi jq b jr ln ij jt ju lo im jw jx lp jz ka kb lq kd ke kf lr kh ki kj hb bi translated">我们已经对MDP的这一特定关键术语有所了解。</p><ol class=""><li id="0969" class="lt lu hi jq b jr js ju jv jx lv kb lw kf lx kj nd lz ma mb bi translated"><strong class="jq hj">状态(<em class="kk"> S </em> ) → </strong>一组<em class="kk">状态</em></li><li id="718d" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj nd lz ma mb bi translated"><strong class="jq hj">动作(<em class="kk">一套</em> ) → </strong>一套<em class="kk">动作</em></li><li id="b923" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj nd lz ma mb bi translated"><strong class="jq hj">转换矩阵→ </strong>代理从一个状态到下一个状态的转换</li><li id="13ae" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj nd lz ma mb bi translated"><strong class="jq hj">奖励(<em class="kk"> R </em> ) → </strong>一套<em class="kk">奖励</em></li><li id="c5ca" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj nd lz ma mb bi translated"><strong class="jq hj">策略→ </strong>一个给定的函数将每个状态映射到另一个状态</li></ol><p id="9f5f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，作为总结，我们将看看它如何与<em class="kk">强化学习相关联。</em>简而言之，在强化学习方面，我们不像在MDP那样给出代理的转移矩阵和策略，但是代理会通过代理与环境的交互来自己学习。</p><ul class=""><li id="7be5" class="lt lu hi jq b jr js ju jv jx lv kb lw kf lx kj ly lz ma mb bi translated">状态</li><li id="d928" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj ly lz ma mb bi translated">行动</li><li id="c281" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj ly lz ma mb bi translated">t̷r̷a̷n̷s̷i̷t̷i̷o̷n̷̷m̷a̷t̷r̷i̷x̷</li><li id="d61c" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj ly lz ma mb bi translated">报酬</li><li id="ee9e" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj ly lz ma mb bi translated">̷P̷o̷l̷i̷c̷y̷</li></ul><p id="c2d6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这就是学习这个术语的由来，也是我们称之为人工智能的原因。这不仅仅是如果…否则…的条件，但是代理人会自己计算出什么样的策略是好的，可以最大化累积报酬。当然有一种方法可以帮助代理学习更好的策略，比如<strong class="jq hj"> <em class="kk">近似策略。我们将在下一部分了解它。</em></strong></p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><p id="da6f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">恭喜你们和好如初。希望你能更好地理解MDP，以及我们如何将它用作强化学习的框架。随着我们继续在JML x AWS DeepRacer Bootcamp上的旅程，我们将在下一篇文章中更深入地讨论强化学习的政策，以及如何在AWS DeepLearning控制台上构建我们的强化学习模型。</p></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h2 id="581d" class="ks kt hi bd ku kv kw kx ky kz la lb lc jx ld le lf kb lg lh li kf lj lk ll lm bi translated">资源:</h2><ol class=""><li id="63d6" class="lt lu hi jq b jr ln ju lo jx nf kb ng kf nh kj nd lz ma mb bi translated">JML x AWS深赛车训练营MDP介绍唐尼和苏曼</li><li id="a215" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj nd lz ma mb bi translated">强化学习，导论，第2版。理查德·萨顿和安德鲁·伯顿</li><li id="5606" class="lt lu hi jq b jr mc ju md jx me kb mf kf mg kj nd lz ma mb bi translated">使用Scikit-Learn、Keras和TensorFlow进行机器学习，第2版。作者:奥雷连·盖伦</li></ol></div></div>    
</body>
</html>