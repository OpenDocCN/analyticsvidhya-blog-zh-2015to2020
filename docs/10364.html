<html>
<head>
<title>Understanding Backpropagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解反向传播</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-backpropagation-1f0bd9d44d76?source=collection_archive---------13-----------------------#2020-10-15">https://medium.com/analytics-vidhya/understanding-backpropagation-1f0bd9d44d76?source=collection_archive---------13-----------------------#2020-10-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="3700" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们都知道什么是神经网络吧？这是一个神经元网络，其中每个连接都与一个权重相关联，这标志着该连接的重要性。</p><p id="765e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在神经网络中，输入数据是正向输入的。每一层接受一个输入数据，根据激活函数对其进行处理，并传递到下一层，最后得到输出。这被称为<strong class="ih hj">正向传播</strong>。</p><p id="41b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们来计算发生的错误。</p><h1 id="d071" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">损失函数</h1><p id="6d27" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">这里我们比较来自网络的输出(即预测值)和实际输出(这是数据集中的<em class="kg">标签</em>或<em class="kg">目标</em>值)。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/8b6f9841ff9be34d063f39350b9608b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*hZ0-1mMLecYVGFCEuclegQ.jpeg"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">损失函数—逻辑回归</figcaption></figure><p id="e2bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于对数函数，它要么总是增加，要么总是减少。看看下面的图表</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kt"><img src="../Images/bf3560c9696ea7ea086a09512a3e0c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*-fc2aWoahhPntdTbvKTbXw.jpeg"/></div></figure><p id="c91f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">需要注意的关键是，成本函数对自信和错误预测的惩罚大于对自信和正确预测的奖励！随着预测精度的提高(更接近0或1)，成本会更低。</p><h1 id="56b7" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">价值函数</h1><p id="4ebb" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">为数据集中的每个输入计算损失。如果训练数据中有n行，我们计算每行的损失。<strong class="ih hj">成本函数</strong>是所有损失的平均值。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ku"><img src="../Images/cf0529c737118fa951f28dd682fe8584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3QxGNT5QsEWc2AZtlY5niw.jpeg"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">成本函数—逻辑回归</figcaption></figure><p id="5615" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的“m”是训练数据中的行数。成本是指我们的模型中出现了多少误差。由于我们需要一个具有良好精度的模型，我们的工作是最小化成本函数。这就是我们需要<strong class="ih hj">反向传播</strong>的地方。</p><h1 id="de0c" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">最小化权重和偏差</h1><p id="95d0" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">我们用的是线性函数z = wx + b，后面是激活函数。</p><p id="1967" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">反向传播(又名梯度下降)的概念是调整权重和偏差，以获得尽可能小的误差。梯度下降需要一个我们想要最小化的成本函数。最小化简单地说就是找到函数中最深的谷(局部最小值)。</p><p id="6b4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">损失函数导数的证明超出了本文的范围(这也不重要)。经过复杂的计算，我们得到了调整权重的公式。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kz"><img src="../Images/8325a5c54f6015b80304e767ca24d3ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*D3EQ9Ovg20kY92nRzK2zIw.jpeg"/></div></figure><p id="beff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，α是学习率。这个变量被称为超参数(不可训练的参数)。如果步长(α)太小，我们将需要太多的迭代才能达到最小值。如果它太大，我们将超过最小值(即，我们将无法在最小值着陆)。所以阿尔法需要恰到好处。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es la"><img src="../Images/3261cd0d993540a48da243bc3b8162e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ykRTMpIdFqmyvTY6aEVoVw.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">梯度下降</figcaption></figure><p id="1a85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们看看每个重量的更新表达式。这里，I的范围是从0到权重的数量，Wi是权重向量中的第I个权重。类似地，用Bj替换Wi以更新偏置(j的范围从0到偏置量，Bj是偏置向量中的第j个偏置)。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lb"><img src="../Images/184e22e6a79b5068dddcbf8c3a2a39a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*ICxwtGhmYr5XwvUAwafBzQ.jpeg"/></div></figure><p id="1f44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">找到最佳的学习速度可能相当具有挑战性。首先，尝试使用一个小的alpha值，并稍微增加它。如果这不起作用，你可以尝试使用梯度下降的其他变化，如<strong class="ih hj"> RMSprop </strong>和<strong class="ih hj">自适应梯度</strong>。</p><p id="015c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">…</p><p id="cceb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢您阅读^_^这篇文章</p></div></div>    
</body>
</html>