# æ–°å† è‚ºç‚æ¨æ–‡â€”åœ°ç†å’Œæƒ…æ„Ÿåˆ†æ

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/covid-19-tweets-geographical-and-sentiment-analysis-c33b4aa5b777?source=collection_archive---------9----------------------->

## åœ¨ Python ä¸­ä½¿ç”¨ leav çš„äº¤äº’å¼çƒ­å›¾å’Œä½¿ç”¨ NLTK VADER çš„æƒ…æ„Ÿåˆ†æ

![](img/53ea6359442a1746a9f6cb153a7e1499.png)

å°±åƒä»–ä»¬è¯´çš„ï¼Œ**ä½ çš„** **æ¨ç‰¹ï¼Œä½ çš„å£°éŸ³ã€‚**å¤šå¹´æ¥ï¼ŒTwitter ä¸€ç›´è¢«è®¤ä¸ºæ˜¯æ¯ä¸ªäººéƒ½å–œæ¬¢è°ˆè®ºæ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…çš„åœ°æ–¹ã€‚æ ¹æ® [Omnicore](https://www.omnicoreagency.com/twitter-statistics/) å‘å¸ƒçš„ç»Ÿè®¡æ•°æ®ï¼Œæˆªè‡³å»å¹´ï¼Œæœˆæ´»è·ƒç”¨æˆ·è¶…è¿‡ 3 äº¿ï¼Œæ¯å¤©æœ‰è¶…è¿‡ 5 äº¿æ¡æ¨æ–‡ã€‚åœ¨ã€Šæ–°å† è‚ºç‚æ—¶æŠ¥ã€‹æœŸé—´ï¼Œtwitter å·²ç»äº§ç”Ÿäº†æµ·é‡çš„æ•°æ®ï¼Œæ¥è‡ªä¸–ç•Œå„åœ°çš„äººä»¬éƒ½åœ¨æ¨ç‰¹ä¸Šè°ˆè®ºå®ƒã€‚æœ‰è¶£çš„æ˜¯ï¼Œä»Šå¤©çš„æŠ€æœ¯å¦‚ä½•ä½¿æˆ‘ä»¬èƒ½å¤Ÿå¤„ç†æ‰€æœ‰è¿™äº›æ•°æ®ï¼Œå¹¶é€šè¿‡åº”ç”¨æ— æ•°çš„ç®—æ³•å’Œæ¨¡å‹æ¥äº§ç”Ÿæ´å¯ŸåŠ›ã€‚

åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘å°†å¸¦æ‚¨æµè§ˆä¸€ä¸ªè¿™æ ·çš„åˆ†æï¼Œé‡ç‚¹æ˜¯ä½¿ç”¨ Twitter API æ”¶é›†çš„å¸¦æœ‰é«˜é¢‘æ ‡ç­¾(#covid19)çš„æ¨æ–‡ã€‚æˆ‘ä»¬ä½¿ç”¨äº† 2020 å¹´ 7 æœˆè‡³ 2020 å¹´ 8 æœˆæœŸé—´çº¦ 18 ä¸‡æ¡æ¨æ–‡çš„æ ·æœ¬é›†ã€‚å…³äºæ•°æ®å’Œ python è„šæœ¬çš„æ›´å¤šç»†èŠ‚å¯ä»¥åœ¨ [Kaggle](http://www.kaggle.com/gpreda/covid19-tweets) ä¸Šè®¿é—®ï¼›æ„Ÿè°¢ [Gabriel Preda](https://medium.com/u/a963dff30149?source=post_page-----c33b4aa5b777--------------------------------) å‘å¸ƒè¿™ä¸ªæ•°æ®é›†ã€‚

ä½¿ç”¨è¿™äº›æ•°æ®ï¼Œæˆ‘ä»¬è¯•å›¾å›ç­”ä¸€äº›å•†ä¸šé—®é¢˜ã€‚å‘æ¨ç‰¹æœ€å¤šçš„äººæ¥è‡ªå“ªä¸ªå›½å®¶å’ŒåŸå¸‚ï¼Ÿ
2ã€‚æ–°å† è‚ºç‚ç—…ä¾‹æ•°é‡å¯¹**æ¯æ—¥æ¨æ–‡**æœ‰ä½•å½±å“ï¼Ÿ
3ã€‚è¿™äº›æ¨æ–‡ä¸­çš„**æƒ…ç»ª**æ˜¯ä»€ä¹ˆï¼Ÿæœ€å—æ¬¢è¿çš„æ¨æ–‡æ˜¯æ­£é¢çš„è¿˜æ˜¯è´Ÿé¢çš„ï¼Ÿ
4ã€‚è¿™äº›æ¨æ–‡ä¸­æœ€å¸¸è§çš„è¯è¯­å’Œå®ä½“æ˜¯ä»€ä¹ˆï¼Ÿ
5ã€‚å“ªç§ç±»å‹çš„äººå‘å¾®åšæ›´å¤šâ€”â€”ä»–ä»¬ä½¿ç”¨ç½‘ç»œæˆ–æ‰‹æœºå‘å¾®åšå—ï¼Œä»–ä»¬æœ‰å¾ˆå¤šç²‰ä¸å—ï¼ŒæŸäº›è´¦æˆ·å€¾å‘äºç‰¹å®šçš„è¯­æ°”å—ï¼Ÿ

åœ¨æˆ‘ä»¬å¼€å§‹åˆ†ææ¨æ–‡ä¹‹å‰ï¼Œæœ‰å‡ ä¸ªåŸºæœ¬æ­¥éª¤æ¶‰åŠåˆ°æ•°æ®æ¢ç´¢(EDA)ã€ä¿®å¤æ•°æ®ç±»å‹ã€å¡«è¡¥æ•°æ®ç¼ºå£ç­‰ã€‚å·²ç»å®æ–½ã€‚å°±åƒæ¥è‡ªç¤¾äº¤åª’ä½“çš„ä»»ä½•å…¶ä»–éç»“æ„åŒ–æ•°æ®ä¸€æ ·ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†ä¸­çš„æ–‡æœ¬å­—æ®µä¹Ÿéœ€è¦æŸç§ç¨‹åº¦çš„æ¸…ç†å’Œè§„èŒƒåŒ–ï¼Œä»¥ä¾¿ä¸ºè¿›ä¸€æ­¥çš„åˆ†æåšå‡†å¤‡ã€‚ä¸ä»…å¦‚æ­¤ï¼Œåœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è¿˜å¿…é¡»é€šè¿‡åº”ç”¨ä¸€äº›ä¸šåŠ¡è§„åˆ™æ¥åŒºåˆ†å›½å®¶å’ŒåŸå¸‚ï¼Œè¿™äº›ä¸šåŠ¡è§„åˆ™åœ¨åŸå§‹æ•°æ®é›†ä¸­å‡ºç°åœ¨ä¸€åˆ—ä¸­ï¼Œæ²¡æœ‰éµå¾ªæ ‡å‡†æ ¼å¼ã€‚æœ‰æ—¶ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ä»å¤–éƒ¨æ¥æºå¼•å…¥é¢å¤–çš„æ•°æ®ç‚¹æ¥ä¸°å¯Œæ•°æ®é›†ã€‚ä¸ºäº†è¿›è¡Œåœ°ç†åˆ†æï¼Œåœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†[åç§°](https://nominatim.openstreetmap.org/ui/search.html)æ¥è·å–è¿™äº›å›½å®¶å’ŒåŸå¸‚çš„çº¬åº¦å’Œç»åº¦ï¼Œç„¶åç»˜åˆ¶åœ¨çƒ­å›¾ä¸Šã€‚å…³äºè¿™äº›æ­¥éª¤çš„ç»†èŠ‚ï¼Œè¯·è®¿é—®ç¬”è®°æœ¬[è¿™é‡Œ](https://www.kaggle.com/arushi2/covid19-tweets-geo-and-sentiment-analysis)ã€‚

ä¸€æ—¦æ•°æ®éƒ½å‡†å¤‡å¥½å¹¶æ¸…ç†å®Œæ¯•ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¼€å§‹æ·±å…¥åˆ†æï¼Œå†³å®šä½¿ç”¨ä»€ä¹ˆ APIs æ¨¡å‹ï¼Œå¹¶å›ç­”å·²ç¡®å®šçš„ä¸šåŠ¡é—®é¢˜ã€‚

# 1.åœ°ç†åˆ†æï¼Œä»¥ç¡®å®šå¤§å¤šæ•°äººå‘æ¨æ–‡æ¥è‡ªå“ªä¸ª**å›½å®¶å’ŒåŸå¸‚**ï¼Ÿ

é¦–å…ˆï¼Œæˆ‘ä»¬æ ¹æ®å¤§é‡çš„æ¨æ–‡æå–äº†æ’åé å‰çš„å›½å®¶å’ŒåŸå¸‚ï¼Œå¹¶ç»˜åˆ¶å‡ºå®ƒä»¬çš„è¶‹åŠ¿ã€‚

![](img/dc60e424f8479b6977f73ba14dc19a2e.png)![](img/74962706714ef31bc0bd18c6236dd1f8.png)

å¾ˆæ˜æ˜¾ï¼Œ**ç¾å›½çš„æ¨ç‰¹æ•°é‡æœ€å¤š**ï¼Œç´§éšå…¶åçš„æ˜¯å°åº¦ï¼Œå‡ ä¹æ˜¯ç¾å›½çš„ä¸€åŠã€‚æ‰€æœ‰å…¶ä»–å›½å®¶çš„æ¨ç‰¹æ•°é‡éƒ½è¿œè¿œå°‘äºç¾å›½æˆ–å°åº¦ã€‚å°±åŸå¸‚è€Œè¨€ï¼Œä¼¦æ•¦ã€æ–°å¾·é‡Œã€çº½çº¦ã€å­Ÿä¹°å’Œåç››é¡¿æ˜¯æ¨ç‰¹æ•°é‡æœ€å¤šçš„å‰äº”åã€‚

æ¥ä¸‹æ¥ï¼Œä¸ºäº†åœ¨äº¤äº’å¼è§†è§‰ä¸­è½»æ¾å®šä½æ¨æ–‡é«˜åº¦é›†ä¸­çš„å›½å®¶/åŸå¸‚ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªçƒ­å›¾ï¼Œå¹¶æä¾›äº†æ”¾å¤§å’Œç¼©å°é€‰é¡¹ã€‚ä¸ºäº†èƒ½å¤Ÿç»˜åˆ¶çƒ­å›¾ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦è·å¾—è¿™äº›å›½å®¶å’ŒåŸå¸‚çš„åæ ‡ï¼Œè¿™æ˜¯ä½¿ç”¨ get_coordinates()å‡½æ•°å®Œæˆçš„ã€‚

```
def get_coordinates(input_type, name, output_as='center'):
    """
    Function to get coordinates of country/ cityAttributes
    ----------
    input_type : str
        Pass 'country' or 'city' to generate the respective URL
     name : str
        Name of the country or city we need the coordinates for
    output_as : str
        Pass 'center' or 'boundingbox' depending upon what      coordinates type to fetch Methods
    -------
        Returns the coordinates of the country or city
    """ # create url
    url = '{0}{1}{2}'.format('[http://nominatim.openstreetmap.org/search?'+input_type+'='](http://nominatim.openstreetmap.org/search?'+input_type+'='\),name,'&format=json&polygon=0')
    response = requests.get(url)
    try:
        response = response.json()[0]
        # parse response to list
        if output_as == 'center':
            lst = [response.get(key) for key in ['lat','lon']]
            output = [float(i) for i in lst]
        if output_as == 'boundingbox':
            lst = response[output_as]
            output = [float(i) for i in lst]
        return output

    except (IndexError, ValueError):
        # this will log the whole traceback
        return [0,0]
```

ä¸€æ—¦è·å¾—åæ ‡ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨å…¶ä¸­ä¸€ä¸ªæä¾›åˆ›å»ºçƒ­å›¾åŠŸèƒ½çš„åº“ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªçŸ¥åçš„ Python åº“ï¼Œå«åš[](https://python-visualization.github.io/folium/)**ã€‚å‡½æ•° generateBaseMap()è§£é‡Šäº†å®ƒç”¨æ¥ç”Ÿæˆçƒ­å›¾çš„å‚æ•°ä»¥åŠå®ƒä¸Šé¢çš„æ ‡è®°ã€‚æ ‡è®°ä¸Šæ·»åŠ äº†å¼¹å‡ºæ ‡ç­¾å’Œå›¾æ ‡ï¼Œå½“æˆ‘ä»¬æ‚¬åœåœ¨ä¸Šé¢æ—¶ï¼Œå®ƒä»¬ä¼šæ˜¾ç¤ºå›½å®¶/åŸå¸‚åç§°å’Œæ¨æ–‡æ•°é‡ç­‰è¯¦ç»†ä¿¡æ¯ã€‚æœ‰è®¸å¤šå…¶ä»–å‚æ•°å’Œé€‰é¡¹å¯ç”¨äºæ ¹æ®éœ€è¦å®šåˆ¶æ‚¨çš„å¯è§†åŒ–ã€‚**

```
import folium
from folium import plugins
from folium.plugins import HeatMap
import branca.colormap# Create a heatmap using folium
def color(magnitude):
    if magnitude>=2000:
        col='red'
    elif (magnitude>=500 and magnitude<2000):
        col='beige'
    elif magnitude<500:
        col='green'
    return coldef generateBaseMap(input_type,df,default_location=[40.693943, -73.985880], default_zoom_start=2):
    """
    Function to generate the heatmapAttributes
    ----------
    input_type : str
        Pass 'country' or 'city' to generate the respective heatmap
    df : str
        Name of the dataframe having the country/city coordinates  and other details
    default_location : int
        Pass the default location for the displayed heatmap
    default_zoom_start: int
        Pass the default zoom for the displayed heatmap

    Methods
    -------
        Returns the base_map
    """

    base_map = folium.Map(location=default_location, control_scale=True, zoom_start=default_zoom_start)
    marker_cluster = plugins.MarkerCluster().add_to(base_map)

    HeatMap(data=df[['lat','long']].values.tolist(),radius=20,max_zoom=13).add_to(base_map)
    for lat,lan,tweet,name in zip(df['lat'],df['long'],df['# of tweets'],df.iloc[:,0]): 
        # Marker() takes location coordinates as a list as an argument 
        folium.Marker(location=[lat,lan],popup = [name,tweet], 
                      icon= folium.Icon(color=color(tweet), 
                      icon_color='white', icon='twitter', prefix='fa')).add_to(marker_cluster)

    #specify the min and max values of your data
    min, max = df['# of tweets'].min(), df['# of tweets'].max()
    colormap = cm.LinearColormap(colors=['green','beige','red'], vmin=min,vmax=max)

    colormap.caption = input_type.title() +' distribution of COVID-19 tweets'
    colormap.add_to(base_map)
    return base_map
```

**ä¸‹é¢æ˜¯æˆ‘ä»¬ç”Ÿæˆçš„çƒ­å›¾ï¼Œæ˜¾ç¤ºäº†æ¨æ–‡åœ¨å„ä¸ªåŸå¸‚çš„åˆ†å¸ƒæƒ…å†µã€‚å½“æˆ‘ä»¬æ”¾å¤§å’Œç¼©å°æ—¶ï¼Œåœ°å›¾å…è®¸æˆ‘ä»¬æŸ¥çœ‹æ‰€é€‰åŸå¸‚çš„ç»†èŠ‚ã€‚å›¾æ ‡çš„çº¢ã€é»„ã€ç»¿è‰²æ˜¾ç¤ºäº†æ¨æ–‡éŸ³é‡çš„å¼ºåº¦ã€‚æ¯ä¸€ä¸ªå•ç‹¬çš„å›¾æ ‡ï¼Œå½“ç‚¹å‡»æ—¶ï¼Œç»™æˆ‘ä»¬åŸå¸‚çš„åç§°å’Œæ¨æ–‡çš„æ•°é‡ã€‚**

**![](img/0680ad1ffa60d91581b79aa3b6c59b08.png)****![](img/0ad934506e1fb5ea1818a0cd87608a50.png)**

# **2.æ¯æ—¥æ¨æ–‡è¶‹åŠ¿**

**å½“æˆ‘ä»¬ç»˜åˆ¶æ¯å¤©çš„æ¨æ–‡æ•°é‡æ—¶ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°æ¨æ–‡æ•°é‡åœ¨ 7 æœˆçš„æœ€åä¸€å‘¨å‡ºç°äº†ä¸€ä¸ªé«˜å³°ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºæ®ä¸–ç•Œæ°”è±¡ç»„ç»‡æŠ¥é“ï¼Œåœ¨é‚£æ®µæ—¶é—´é‡Œå‘ç”Ÿäº†æœ€å¤šçš„æ—¥å†•äº‹ä»¶ã€‚**7 æœˆ 24 æ—¥ï¼Œæˆªè‡³ 8 æœˆåº•ï¼Œæ¯æ—¥ç”µæ™•** **ç—…ä¾‹**æœ€é«˜ï¼Œå…¨çƒçº¦ 29 ä¸‡ä¾‹ï¼Œç¾å›½çº¦ 8 ä¸‡ä¾‹ã€‚è¿™å¯èƒ½è§£é‡Šäº†é‚£ä¸€å‘¨ï¼Œå°¤å…¶æ˜¯ 7 æœˆ 25 æ—¥çš„å¤§é‡æ¨æ–‡ã€‚**

**![](img/6df43e3e536002265f0e5d3fa801dc92.png)****![](img/8a272f724f369e057ac7a16e613e2479.png)**

**æ¥æº:[ä¸–ç•Œè®¡é‡è¡¨](http://www.worldometers.info/coronavirus/country/us/)**

**éšç€ç—…ä¾‹æ•°é‡çš„ä¸Šå‡ï¼Œäººä»¬ä¼¼ä¹å€¾å‘äºåœ¨æ¨ç‰¹ä¸Šå‘å¸ƒæ›´å¤šå…³äºå† çŠ¶ç—…æ¯’çš„ä¿¡æ¯ã€‚çŸ¥é“è¿™äº›äººåœ¨æ¨ç‰¹ä¸Šè°ˆè®ºä»€ä¹ˆéš¾é“ä¸æ˜¯å¾ˆæœ‰è¶£å—â€”â€”ä»–ä»¬æ˜¯å¯¹è¶Šæ¥è¶Šå¤šçš„ç”µæ™•ç—…ä¾‹æ„Ÿåˆ°æ‹…å¿§å’Œç„¦è™‘ï¼Œè¿˜æ˜¯å¯¹è¿™ç§æƒ…å†µå……æ»¡å¸Œæœ›ã€‚å¥½å§â€¦è®©æˆ‘ä»¬ç»§ç»­è¯»ä¸‹å»ï¼Œå¯»æ‰¾ç­”æ¡ˆã€‚**

# **3.Tweet **æƒ…ç»ª**åˆ†æè¯†åˆ«ç§¯ææˆ–æ¶ˆæçš„è¯­æ°”**

**ä¸ºäº†å¯¹æ¨æ–‡è¿›è¡Œæ–‡æœ¬åˆ†æå¹¶æ£€æµ‹æƒ…ç»ªï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡å°†é™¤ a-zã€A-Zã€0â€“9 ä¹‹å¤–çš„æ‰€æœ‰å†…å®¹è½¬æ¢ä¸ºç©ºæ ¼æ¥æ¸…ç†æ¨æ–‡ï¼Œä»æ¨æ–‡ä¸­åˆ é™¤ twitter é“¾æ¥å’Œå…¶ä»–å™ªéŸ³ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è®¸å¤šå¯ç”¨çš„æƒ…æ„Ÿåˆ†æåº“ä¹‹ä¸€ã€‚å¦‚æœä½ åœ¨é€‰æ‹©åˆé€‚çš„åº“æ–¹é¢æœ‰å›°éš¾ï¼Œè¿™é‡Œçš„[æ˜¯ä¸€æœ¬å¸®åŠ©ä½ è§£å†³è¿™ä¸ªé—®é¢˜çš„å¥½ä¹¦ã€‚](https://elitedatascience.com/python-nlp-libraries)**

**è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ NLTK VADER SentimentIntensityAnalyzer è¿™æ˜¯ä¸€ä¸ªåŸºäºè¯æ±‡å’Œè§„åˆ™çš„æƒ…æ„Ÿåˆ†æå·¥å…·ï¼Œåœ¨å¤„ç†ç¤¾äº¤åª’ä½“æ–‡æœ¬æ—¶éå¸¸æˆåŠŸã€‚å®ƒä¸ä»…ä¸ºæˆ‘ä»¬æä¾›æ­£é¢ã€è´Ÿé¢ã€ä¸­æ€§åˆ†æ•°ï¼Œè¿˜æä¾›å¤åˆåˆ†æ•°ï¼Œè¯¥å¤åˆåˆ†æ•°æ˜¯ä¸€ç§è®¡ç®—æ‰€æœ‰è¯å…¸è¯„çº§æ€»å’Œçš„åº¦é‡ï¼Œè¿™äº›è¯„çº§å·²åœ¨-1(æœ€è´Ÿé¢)å’Œ+1(æœ€æ­£é¢)ä¹‹é—´æ ‡å‡†åŒ–ã€‚ä¸‹é¢çš„ä»£ç è§£é‡Šäº†ä¸šåŠ¡è§„åˆ™â€”â€”å¦‚æœå¤åˆå¾—åˆ†â‰¥ 0.5ï¼Œåˆ™æ€»ä½“æƒ…ç»ªä¸ºâ€œç§¯æâ€,å¦‚æœå¤åˆå¾—åˆ†â‰¤ -0.5ï¼Œåˆ™ä¸ºâ€œæ¶ˆæâ€,å¦‚æœä»‹äºä¸¤è€…ä¹‹é—´ï¼Œåˆ™ä¸ºâ€œä¸­æ€§â€ã€‚è¦äº†è§£æ›´å¤šå…³äº [VADER](/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f) æƒ…ç»ªåˆ†æçš„çŸ¥è¯†ï¼Œè¯·é˜…è¯»è¿™ç¯‡ç”±[å¸•é²å°”Â·æ½˜è¿ª](https://medium.com/u/7053de462a28?source=post_page-----c33b4aa5b777--------------------------------)æ’°å†™çš„æƒŠäººåšæ–‡ã€‚**

```
# Cleaning the tweets for characters other than a-z, A-Z, 0-9
tweets['clean_tweet'] = tweets['text'].apply(lambda x: re.sub("([^0-9A-Za-z \t])|(\w+:\/\/\S+)"," ", x))# Run sentiment analysis
from nltk.sentiment.vader import SentimentIntensityAnalyzer
sid = SentimentIntensityAnalyzer()
for index, row in tqdm(tweets.iterrows()): #tqdm 
    ss = sid.polarity_scores(row['clean_tweet'])
    if ss['compound'] >= 0.05 : 
        tweets.at[index,'sentiment'] = "Positive"
    elif ss['compound'] <= - 0.05 : 
        tweets.at[index,'sentiment'] = "Negative"
    else : 
        tweets.at[index,'sentiment'] = "Neutral"
```

**![](img/bfb90f97824939212333406cd56f108e.png)**

**åœ¨æ€»å…±çº¦ 180ï¼Œ000 æ¡æ¨æ–‡ä¸­ï¼Œ**~ 70ï¼Œ000 æ¡æœ‰æ­£é¢æƒ…ç»ª**ï¼Œå å›¾è¡¨ä¸­çº¦ 40%çš„æ¨æ–‡ï¼Œæ˜¯æœ€å¤§çš„ä¸€å—é¥¼ã€‚**

**åœ¨**æœ€å—æ¬¢è¿çš„ 10 æ¡æ¨æ–‡ä¸­ï¼Œ30%çš„æ¨æ–‡è¯­æ°”ç§¯æ**ï¼Œè°ˆè®ºä¸€æ—¦ç–«è‹—åˆ¶æˆï¼Œæƒ…å†µå¯èƒ½ä¼šå¦‚ä½•æ”¹å–„ï¼Œ40%çš„æ¨æ–‡è¯­æ°”æ¶ˆæï¼Œæ‹…å¿ƒæ–°å† è‚ºç‚ç—…ä¾‹å¢åŠ ä»¥åŠå›½å®¶æ— åŠ›æ§åˆ¶ç–«æƒ…ã€‚å‰©ä¸‹çš„ 30%å¸¦æœ‰ä¸­æ€§è¯­æ°”ï¼Œä¸»è¦æ˜¯å¯¹æ–°å† è‚ºç‚ç—…ä¾‹æ•°é‡çš„æ›´æ–°ã€‚**

# **4.è¯†åˆ«æ¨æ–‡ä¸­æœ€å¸¸è§çš„è¯å’Œå®ä½“(äººã€åœ°ç‚¹ã€ç»„ç»‡)**

**ä¸ºäº†èƒ½å¤Ÿè¯†åˆ«å®ä½“ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹ä¸Šä¸€æ­¥ä¸­å·²ç»æ¸…ç†è¿‡çš„ tweets è¿›è¡Œæ ‡è®°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ NLTK æä¾›çš„åŸºæœ¬æ ‡è®°å™¨ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ä¸“é—¨ä¸º twitter æ–‡æœ¬ç¼–å†™çš„ TweetTokenizeã€‚ç„¶åï¼Œè¿™äº›æ ‡è®°è¢«ä¼ é€’ç»™ [pos_tag](http://www.nltk.org/book/ch05.html) ï¼Œå®ƒå°†å•è¯åˆ†ç±»æˆè¯æ€§ã€‚è¿™åˆè¢«ä¼ é€’ç»™ [ne_chunk](https://www.nltk.org/book/ch07.html) ï¼Œå®ƒæ˜¯ä¸€ä¸ªåŸºäºåˆ†ç±»å™¨çš„å‘½åå®ä½“è¯†åˆ«å™¨ã€‚ä¸‹é¢ä»£ç ä¸­çš„â€œbinaryâ€å‚æ•°æä¾›äº†ä¸€ä¸ªé€‰é¡¹ï¼Œå½“è®¾ç½®ä¸º True æ—¶ï¼Œè·å–æ‰€æœ‰å‘½åå®ä½“çš„ NE æ ‡ç­¾ï¼Œæˆ–è€…å½“è®¾ç½®ä¸º False æ—¶ï¼Œè·å–æ›´å…·ä½“çš„ç±»åˆ«æ ‡ç­¾ï¼Œå¦‚ PERSONã€ORGANIZATION å’Œ GPEã€‚**

```
from nltk.tokenize import TweetTokenizer
from nltk import pos_tag, ne_chunk# Create dictionary of entities and their frequency in the tweets then create a wordcloud
tt = TweetTokenizer()
entities={}for sent in tqdm(tweets.clean_tweet):
    for chunk in ne_chunk(pos_tag(tt.tokenize(sent)), binary=True):
        if hasattr(chunk, 'label'):
            if chunk[0][0] in entities.keys():
                entities[chunk[0][0]] = entities[chunk[0][0]]+1
            else:
                entities[chunk[0][0]]=1

#sorted by value, return a list of tuples   
top50_entities = sorted(entities.items(), key=lambda x: x[1], reverse=True)[:50]
entities_text = " ".join([(k + " ")*v for k,v in dict(top50_entities).items()])
```

**æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªå­—å…¸â€œå®ä½“â€,å¦‚ä¸Šé¢çš„ä»£ç æ‰€ç¤ºï¼Œå®ƒæ•è·äº†æ¯ä¸ªå®ä½“çš„å‡ºç°é¢‘ç‡ï¼Œå¦‚ä¸‹é¢çš„æŠ˜çº¿å›¾æ‰€ç¤ºã€‚æ­£å¦‚é¢„æœŸçš„é‚£æ ·ï¼ŒCOVID å‡ºç°çš„é¢‘ç‡æœ€é«˜ï¼Œå…¶æ¬¡æ˜¯å°åº¦ï¼Œç¬¬äºŒæ˜¯ realDonaldTrumpï¼Œç¬¬ä¸‰æ˜¯å…¶ä»–å›½å®¶ã€‚**

**![](img/be06b4ef4e8b753322c6b65cf00793b0.png)**

**æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ª wordcloudï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°ä»å…¶ä»–å®ä½“ä¸­è¯†åˆ«å‡ºå‡ºç°é¢‘ç‡é«˜çš„å®ä½“ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å·²ç»å¯¼å…¥äº† WordCloud åº“ï¼Œå¹¶è®¾ç½®äº†æ‰€æœ‰éœ€è¦çš„å‚æ•°ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è½»æ¾å®šåˆ¶ã€‚**

```
from nltk.corpus import stopwords
from wordcloud import WordClouddef createWordCloud(input_type, text):
    """
    Function to generate the wordcloudAttributes
    ----------
    input_type : str
        Pass 'words' or 'entities' to update the chart title based on the text passed
    text : str
        Name of the string text to make the wordcloud

    Methods
    -------
        Returns the wordcloud
    """
    wordcloud = WordCloud(width = 1000, height = 600, 
                      #colormap = 'Paired',
                      background_color ='white',
                      collocations = False,
                      stopwords=stop_words
                     ).generate(text)plt.figure(figsize = (12, 12), facecolor = None)
    plt.title("Most common "+ input_type +" in the tweets \n", fontsize=20, color='Black')
    plt.imshow(wordcloud, interpolation='bilinear') 
    plt.axis("off") 
    plt.tight_layout(pad = 0) 
    plt.show()
```

**![](img/767c01575b60372ee615ab499ce646d0.png)**

**æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œæ‰€æœ‰é«˜é¢‘è¯çš„å¤§å°éƒ½å¾ˆå¤§ï¼Œä½¿å®ƒä»¬éå¸¸ç‹¬ç‰¹ã€‚é™¤äº†æœ€æ˜æ˜¾çš„å•è¯â€œCOVID â€,å…¶ä»–å¸¸è§çš„å®ä½“æœ‰ Donald Trumpã€Joe Bidenã€Boris Johnsonã€å›½ä¼šã€ä¸–å«ç»„ç»‡ã€CDCã€‚ä¸€äº›å›½å®¶ï¼Œå¦‚å°åº¦ã€ç¾å›½ã€ä¸­å›½ã€ä¿„ç½—æ–¯ï¼Œä»¥åŠä¸€äº›åŸå¸‚/å·ï¼Œå¦‚å¥¥è¿ªæ²™(å°åº¦çš„ä¸€ä¸ªåŸå¸‚)ã€ä½›ç½—é‡Œè¾¾ã€å¾·å…‹è¨æ–¯ç­‰ã€‚åœ¨æ¨ç‰¹ä¸Šä¹Ÿç»å¸¸è¢«æåŠã€‚**

# **5.æ›´å¤šåœ°äº†è§£å‘å¾®åšå¤šçš„**ç±»å‹çš„äºº****

**è™½ç„¶åˆ†ææ¨æ–‡å‘Šè¯‰æˆ‘ä»¬å®ƒçš„å†…å®¹å’Œæƒ…ç»ªï¼Œå¦ä¸€ä¸ªé‡è¦çš„æ–¹é¢æ˜¯äº†è§£å‘æ¨æ–‡æ›´å¤šçš„äººçš„ç±»å‹ï¼Œä»¥åŠæˆ‘ä»¬å¦‚ä½•å¯¹ä»–ä»¬è¿›è¡Œåˆ†ç±»ã€‚**

**é¦–å…ˆï¼Œæˆ‘ä»¬çœ‹äº†è¿™äº›äººå‘æ¨æ–‡çš„æ¥æºã€‚ç»“æœæ˜¾ç¤ºï¼Œ32%çš„äººä½¿ç”¨ Web åº”ç”¨ç¨‹åºå‘æ¨æ–‡ï¼Œå…¶æ¬¡æ˜¯ Android ç”¨æˆ·ï¼Œä¸º 22%ï¼ŒiPhone ç”¨æˆ·æ¥è¿‘ 20%ã€‚ç»“åˆ Android å’Œ iPhone ç”¨æˆ·ï¼Œç§»åŠ¨ç”¨æˆ·å‡ ä¹å åˆ° 45%ï¼Œæ˜¯ç½‘ç»œç”¨æˆ·çš„ 1.5 å€ã€‚**

**![](img/3641eb52cc553535adaacb76232244de.png)****![](img/3a91ba48b7ac57f70eb383c2d110ad99.png)**

**æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æŸ¥çœ‹äº†æ‹¥æœ‰å¤§é‡ tweets çš„æœ€é«˜ç²‰ä¸çš„è´¦æˆ·ã€‚ä¸å‡ºæ‰€æ–™ï¼Œå¤§éƒ¨åˆ†æ–°é—»é¢‘é“éƒ½é«˜å±…æ¦œé¦–ã€‚CNNã€å›½å®¶åœ°ç†ã€CGTNã€NDTV å’Œã€Šå°åº¦æ—¶æŠ¥ã€‹æ˜¯æ’åå‰äº”çš„è´¦å·ï¼ŒCNN æ‹¥æœ‰è¶…è¿‡ 5000 ä¸‡ç²‰ä¸ï¼Œå›½å®¶åœ°ç†æ‹¥æœ‰å¤§çº¦ 2500 ä¸‡ç²‰ä¸ã€‚**

**æœ€åï¼Œæˆ‘ä»¬æŸ¥çœ‹äº†æ­£é¢ã€è´Ÿé¢å’Œä¸­æ€§æ¨æ–‡æ•°é‡æœ€å¤šçš„è´¦æˆ·ã€‚å…¨çƒå¤§æµè¡Œã€‚â€œç½‘ç»œâ€æœ‰æœ€å¤šçš„æ­£é¢æ¨æ–‡ï¼Œè€Œâ€œå…¬å¼€ä¿¡â€åœ¨è´Ÿé¢æ¨æ–‡åˆ—è¡¨ä¸­é¢†å…ˆï¼Œâ€œå† çŠ¶ç—…æ¯’æ›´æ–°â€æ˜¯ä¸­æ€§æ¨æ–‡ã€‚**

**![](img/41f2180d7be88cbb58f7b16290986600.png)**

# ****ç»“æŸè¯­:****

**å½“æˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„ NLP æ¨¡å‹æ—¶ï¼Œæœ‰è®¸å¤šé¢å¤–çš„ä¸œè¥¿å¯ä»¥æ¢ç´¢å’Œå®ç°ã€‚ä¸ä»…å¦‚æ­¤ï¼Œè¿˜å¯ä»¥åº”ç”¨é¢å¤–çš„ä¸šåŠ¡è§„åˆ™æ¥è¿›ä¸€æ­¥æ¸…ç†è¿™äº›æ–‡æœ¬æ•°æ®å­—æ®µã€‚å¯ä»¥åˆå¹¶æ›´å¤šçš„å¤–éƒ¨æºæ¥æ‰©å……æ•°æ®é›†å¹¶å®ç°å¤šç»´åˆ†æã€‚æˆ‘å¸Œæœ›ä½ å–œæ¬¢æˆ‘çš„ä½œå“ã€‚ğŸ˜ƒ**

***è¦è®¿é—®å®Œæ•´çš„ä»£ç ï¼Œè¯·ç‚¹å‡»* *æŸ¥çœ‹æˆ‘çš„ GitHub çš„é“¾æ¥* [*ã€‚*](https://github.com/ArushiC/COVID-19-Tweets-Analysis)**

> ****å‚è€ƒæ–‡çŒ®:****
> 
> **[https://www.omnicoreagency.com/twitter-statistics/](https://www.omnicoreagency.com/twitter-statistics/)
> [http://www.kaggle.com/gpreda/covid19-tweets](http://www.kaggle.com/gpreda/covid19-tweets)
> [http://www.worldometers.info/coronavirus/country/us/](http://www.worldometers.info/coronavirus/country/us/)
> [https://www . geeks forgeeks . org/python-æƒ…æ“-åˆ†æ-ä½¿ç”¨-ç»´å¾·/](https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/)**