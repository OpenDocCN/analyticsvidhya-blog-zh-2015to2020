<html>
<head>
<title>Web Scraping with Scrapy and Django.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Scrapy和Django的网页抓取。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/web-scraping-with-scrapy-and-django-94a77386ac1b?source=collection_archive---------4-----------------------#2020-06-01">https://medium.com/analytics-vidhya/web-scraping-with-scrapy-and-django-94a77386ac1b?source=collection_archive---------4-----------------------#2020-06-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/92c936a0035c6ad6eb25893f0d983160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t3QCxjAFxBuZHh5TvgMpbQ.png"/></div></div></figure><p id="4a02" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">网络抓取是从网站提取数据的过程。它模拟人与网页的交互，在抓取器的帮助下检索想要的信息。</p><p id="5955" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当抓取一个网站，你应该确保他们没有违反其条款和条件。大多数网站都有一个<strong class="is hj"> robots.txt </strong>文件，指导抓取者可以或不可以从网站请求哪些页面或文件。这主要是用来避免网站请求过载，这很可能会影响用户。</p><p id="0fd1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" href="https://scrapy.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> Scrapy </strong> </a>是一个免费开源的网页抓取框架。它能够以快速、简单且可扩展的方式从特定网站提取所需的数据。也是有据可查的。</p><p id="a27c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Scrapy允许我们定义数据结构，编写数据提取器，并带有内置的CSS和XPath选择器，我们可以用它们来提取数据。抓取器向网站发出GET请求，解析HTML响应，并使用外部定义的数据提取器提取目标数据。有JSON、CSV、XML等多种输出格式。输出也可以保存到数据库中。</p><p id="fc4c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Scrapy对许多网站限制采取了对策，例如默认情况下随机选择请求之间的时间，这有助于避免被禁止访问。</p><h1 id="95d0" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">我们正在建造的东西</h1><p id="456b" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">我们将构建一个应用程序，抓取可供出租或出售的房屋属性。这些属性将从<a class="ae jo" href="https://realestatedatabase.net/FindAHouse/houses-for-rent-in-kampala-uganda.aspx?Title=Houses+for+rent+in+kampala" rel="noopener ugc nofollow" target="_blank"><strong class="is hj">realestatedatabase</strong></a>网站中抓取。</p><h2 id="9162" class="ks jq hi bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf bi translated">假设</h2><p id="9e28" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">本文假设读者对网络抓取、Scrapy和Django有所了解。</p><h2 id="51e6" class="ks jq hi bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf bi translated">先决条件</h2><ul class=""><li id="1111" class="lg lh hi is b it kn ix ko jb li jf lj jj lk jn ll lm ln lo bi translated">Python 3(这个应用程序是用python 3.7构建和测试的)。</li><li id="7ddc" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">PostgreSQL。</li><li id="a3de" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated">代码编辑器。</li></ul><p id="228b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个应用程序是建立在Scrapy 2.0和Django 3.0之上的</p><p id="7b26" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">安装依赖关系</strong></p><p id="e8cb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建一个PostgreSQL数据库，稍后将使用它的凭据将其连接到应用程序。</p><p id="f301" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建一个虚拟环境并激活它。</p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="84b0" class="ks jq hi lz b fi md me l mf mg">$ python3.7 -m venv venv<br/>$ source venv/bin/activate</span></pre><p id="8424" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">安装Django和Scrapy。</p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="5469" class="ks jq hi lz b fi md me l mf mg">pip install django scrapy</span></pre><p id="b5d1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">安装我们将使用的其他依赖项。</p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="c473" class="ks jq hi lz b fi md me l mf mg">pip install scrapy-djangoitem django-phone-field django-environ word2number</span></pre><p id="40bc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建Django项目并创建我们的属性应用程序。</p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="6064" class="ks jq hi lz b fi md me l mf mg">django-admin startproject main .</span><span id="309f" class="ks jq hi lz b fi mh me l mf mg">python manage.py startapp properties</span></pre><p id="5afb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建一个Scrapy项目，然后将其添加到<code class="du mi mj mk lz b">settings.py</code>中的<code class="du mi mj mk lz b">INSTALLED_APPS</code></p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="6087" class="ks jq hi lz b fi md me l mf mg">scrapy startproject scraper</span></pre><p id="15a7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建模型以保存抓取的数据。用这个更新properties文件夹中的models.py文件。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="e0a3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">用数据库凭证更新<code class="du mi mj mk lz b">settings.py</code>。将<code class="du mi mj mk lz b">properties </code> app和<code class="du mi mj mk lz b">phone_field</code>添加到<code class="du mi mj mk lz b">INSTALLED_APPS</code>。这些凭证应该添加到<code class="du mi mj mk lz b"><strong class="is hj">.</strong>env</code> <strong class="is hj"> </strong>文件中，以便被保密。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="0f84" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后运行迁移。</p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="9aa1" class="ks jq hi lz b fi md me l mf mg">python manage.py makemigrations<br/>python manage.py migrate</span></pre><h2 id="db15" class="ks jq hi bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf bi translated">项目</h2><p id="17c4" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">该项用于构建蜘蛛解析的数据。前面创建的Django模型将用于创建该项目。<a class="ae jo" href="https://github.com/scrapy-plugins/scrapy-djangoitem" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj"> DjangoItem </strong> </a>是一个从Django模型中获取其字段定义的项目类，这里使用的是之前创建的主应用程序中定义的<strong class="is hj">属性</strong>模型。更新<code class="du mi mj mk lz b">scraper/scraper/items.py</code>。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><h2 id="6d58" class="ks jq hi bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf bi translated">管道。</h2><p id="337e" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">项目管道是从蜘蛛中提取项目后处理数据的地方。管道执行诸如验证和在数据库中存储项之类的任务。更新<code class="du mi mj mk lz b">scraper/scraper/pipelines.py</code>。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><h2 id="0d7f" class="ks jq hi bd jr kt ku kv jv kw kx ky jz jb kz la kd jf lb lc kh jj ld le kl lf bi translated">蜘蛛</h2><p id="0785" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">定义如何抓取网站，包括如何执行抓取(即跟随链接)以及如何从网页中提取结构化数据。创建一个属性蜘蛛<code class="du mi mj mk lz b">scraper/spiders/properties_spider.py</code>。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="5527" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">蜘蛛具有如下定义的特征。</p><ul class=""><li id="e257" class="lg lh hi is b it iu ix iy jb mn jf mo jj mp jn ll lm ln lo bi translated"><strong class="is hj">名称</strong>:定义它的字符串。</li><li id="12a4" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj"> allowed_domains </strong>:一个可选的字符串列表，包含允许这个蜘蛛爬行的域。</li><li id="3033" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj"> start_urls </strong>:蜘蛛开始爬行的URL列表。</li><li id="70a7" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj">规则</strong>:一个(或多个)<code class="du mi mj mk lz b"><a class="ae jo" href="https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Rule" rel="noopener ugc nofollow" target="_blank"><strong class="is hj">Rule</strong></a></code>对象的列表。每一个<code class="du mi mj mk lz b"><a class="ae jo" href="https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Rule" rel="noopener ugc nofollow" target="_blank"><strong class="is hj">Rule</strong></a></code>都定义了爬行站点的特定行为。<strong class="is hj"> link_extractor </strong>是一个<a class="ae jo" href="https://docs.scrapy.org/en/latest/topics/link-extractors.html#topics-link-extractors" rel="noopener ugc nofollow" target="_blank"> Link Extractor </a>对象，它定义了如何从每一个被抓取的页面中提取链接，而<strong class="is hj"> call_back </strong>是一个来自蜘蛛对象的<strong class="is hj"> </strong>方法，它将用于每一个用指定的链接提取器提取的链接。</li><li id="bf5e" class="lg lh hi is b it lp ix lq jb lr jf ls jj lt jn ll lm ln lo bi translated"><strong class="is hj"> Item_loaders </strong>:提供一个方便的机制来填充报废的项目。<strong class="is hj"> add_css </strong>方法接收一个存储提取数据的项目字段和一个用于从网页提取数据的css选择器。<strong class="is hj"> load_item </strong>方法<strong class="is hj"> </strong>用收集到的数据填充该项并返回，通过管道保存到数据库。</li></ul><p id="0352" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">用下面的值更新<code class="du mi mj mk lz b">scraper/scraper/items.py</code>，激活<code class="du mi mj mk lz b">pipelines</code>，添加<code class="du mi mj mk lz b">user_agent</code>，更新<code class="du mi mj mk lz b">spider_modules</code>和<code class="du mi mj mk lz b">newspider_modules</code>。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="1866" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建一个Django命令，用于启动蜘蛛爬行。这在scraper中初始化了Django，并需要在spider中访问Django。在<code class="du mi mj mk lz b">scraper</code>文件夹中创建一个<code class="du mi mj mk lz b">management</code>文件夹。在<code class="du mi mj mk lz b">management</code>文件夹下创建一个<code class="du mi mj mk lz b">commands</code>文件夹。确保所有新创建的文件都有一个<code class="du mi mj mk lz b">__init__.py</code>。</p><p id="346d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在<code class="du mi mj mk lz b">commands</code>文件夹中创建一个<code class="du mi mj mk lz b">crawl.py</code>文件，如下图。</p><figure class="lu lv lw lx fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="645f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这将使用<a class="ae jo" href="https://docs.scrapy.org/en/latest/topics/api.html#scrapy.crawler.CrawlerProcess" rel="noopener ugc nofollow" target="_blank"> CrawlerProcess </a>来运行django项目的内部。</p><p id="172b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要运行蜘蛛并将所有属性保存到数据库，请运行以下命令。</p><pre class="lu lv lw lx fd ly lz ma mb aw mc bi"><span id="6b73" class="ks jq hi lz b fi md me l mf mg">python manage.py crawl</span></pre><p id="f372" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在后续文章中，我们将构建一个页面来显示所有属性。</p><p id="d32f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是一篇很长的文章，感谢阅读。<em class="mq">干杯</em>！！。</p><p id="bcce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">代码可以在这里找到<a class="ae jo" href="https://github.com/peterwade153/house-bob" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="9e98" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">喜欢这篇文章，请联系twitter @peterwade153。在<a class="ae jo" href="mailto:peterwade153@gmail.com" rel="noopener ugc nofollow" target="_blank">peterwade153@gmail.com</a>发邮件。</p><p id="1ea9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">参考文献。我使用了Henriette Brand的文章作为参考。这里可以找到<a class="ae jo" href="https://blog.theodo.com/2019/01/data-scraping-scrapy-django-integration/" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>