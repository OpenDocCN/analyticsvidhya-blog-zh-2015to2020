<html>
<head>
<title>Spark your Analytics, with Apache Spark 🌟</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">借助Apache Spark激发您的分析能力🌟</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/spark-your-analytics-with-apache-spark-f4a47936b2f0?source=collection_archive---------8-----------------------#2019-10-06">https://medium.com/analytics-vidhya/spark-your-analytics-with-apache-spark-f4a47936b2f0?source=collection_archive---------8-----------------------#2019-10-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f1d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在深入到一个简单的用例之前，让我们先了解一下Apache Spark的定义。<br/> <strong class="ih hj"> Apache Spark: </strong> Spark是一个内存优化的数据处理引擎，可以对Hadoop数据(在HDFS)执行操作，性能优于MapReduce。</p><p id="ffcb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这意味着，Apache Spark提供了以分布式方式运行单个作业的能力，其中，范围从两个节点到1000个节点的节点群集(为简单起见，是一组计算机)被组装起来在内存中执行一项作业，并且具有足够的容错能力来跟踪任何丢失的节点，并将其任务(职责)分配给群集中任何其他可用的节点，最终所有节点都向驱动程序节点(领导节点)报告作业的结果。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/c37dbd8685e99527ff8eb3817a1b2885.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jh1R3TIfvxDhXmKcwXK09Q.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图片来源:<a class="ae jt" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">https://unsplash.com</a></figcaption></figure><p id="769c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个简单的用例是假设用户想要找到他的商店中销量最大的杂货项目，该商店在2010年至2011年期间拥有大约10，000名客户，用户只需运行一个提供该项目的详细信息的SQL查询。<br/>现在，同一用户希望确定2000年至2019年期间每位顾客购买最多的杂货，假设销售额很高，并且在某一天至少有3000笔销售额。这为我们提供了每个客户的销售记录计数(2000–2019)总计_天数*总计_销售数量= 9、***、***、***..或者可能更多。在这个用例上运行传统的SQL查询不是一个好主意。<br/> Apache Spark开始行动，拯救我们的创造力。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="f2db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">示例用例:</strong> <br/>让我们考虑一个简单的CSV(逗号分隔值)文件，它包含4列，大约有9997851条记录。<br/> userId，movie，rating，timestamp <br/> 18295，909090，2.5，1515203646000 <br/>在上述数据的帮助下，可以导出n个列，这有助于开发者和/或数据分析师基于用户行为做出建设性的决策。<br/>对于这个场景，我在装有<strong class="ih hj"> IntelliJ IDE </strong>并安装了<strong class="ih hj">Spark的本地机器(笔记本电脑)上运行Spark(我将在下一篇文章中讨论如何设置IntelliJ和Spark)</strong>。如果您愿意的话，也可以在一个100倍节点的集群上使用spark session-cluster_level设置运行。</p><p id="ede2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">计划对数据执行任何类型的分析时要问的问题？<br/> -数据质量如何？<br/> -数据中有多少空值或空字符串？<br/> -这些数据是否包含足够的用户标识或产品标识，以便做出决策？<br/> -是否需要格式标准化？比如日期格式、User_id格式或替换空值等……<br/>-我可以基于任何列值(比如日期****-**-01、日期****-**-02)对数据进行分区以提高性能吗？</strong></p><p id="a788" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个故事中，让我们基于单个User_Id导出下面的列。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="1abb" class="kg kh hi kc b fi ki kj l kk kl">user_id — The User ID.<br/>mov_watch_count — No. of Movies watched by that User. <br/>latest_movie_rated — The Movie_id which User recently rated.<br/>first_movie_rated — The Movie_id which User First rated.<br/>more_than_six_rtngs — Does the user rated more than 6 movies? Y/N<br/>less_than_two_rtngs — Does the user rated less than 2 movies? Y/N<br/>highest_rating — The Highest rating given by the User ever.<br/>lowest_rating — The Lowest rating given by the User ever.</span></pre><p id="d911" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们快速浏览一下如何实现这一点的代码片段，下面将包含重要的。</p><p id="045b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">创建一个spark会话，它是Spark库世界的入口。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="6401" class="kg kh hi kc b fi ki kj l kk kl"><strong class="kc hj">//Create your sparkSession to Light it up.</strong><br/>val spark = SparkSession<br/>  .<em class="km">builder</em>()<br/>  .appName("userIdAndRatings")<br/>  .master("local[*]")<br/>  .config("spark.sql.warehouse.dir", "file:///C:/temp")<br/>  .getOrCreate()<br/><strong class="kc hj">//Read the CSV File(There are multiple ways to do this.)</strong><br/>val rawRecords = spark.read.format("csv")<br/>  .option("header", "true")<br/>  .option("delimiter", ",")<br/>  .option("inferSchema", "true")<br/>  .load("C:/sparktempfiles/training.csv")<br/>rawRecords.printSchema()<br/>rawRecords.createOrReplaceTempView("raw_records_view")<br/><strong class="kc hj">//Fix the Time Column from Epoch time to Human readable Date</strong><br/>val rawRecordsTimeFix =<br/>  """<br/>    |select userId, movieId, rating, date_format(from_unixtime(timestamp/1000), 'yyyy-MM-dd') as dateOfRating<br/>    |from raw_records_view<br/>  """.stripMargin</span></pre><p id="9378" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">想出最终表格或数据帧或CSV头文件的布局。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="3389" class="kg kh hi kc b fi ki kj l kk kl"><strong class="kc hj">//Create LayOut for your Output Table<br/></strong>val <em class="km">userRatingsLayout </em>= <em class="km">StructType</em>(<br/>  StructField("user_id", IntegerType, true) ::<br/>    StructField("mov_watch_count", IntegerType, true) ::<br/>    StructField("latest_movie_rated", IntegerType, true) ::<br/>    StructField("first_movie_rated", IntegerType, true) ::<br/>    StructField("more_than_six_rtngs", StringType, true) ::<br/>    StructField("less_than_two_rtngs", StringType, true) ::<br/>    StructField("highest_rating", DoubleType, true) ::<br/>    StructField("lowest_rating", DoubleType, true) ::<br/>    <em class="km">Nil</em>)</span></pre><p id="0220" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">创建一个原始记录对象，然后对user_id进行分组，并将其转换为一个列表，为每个用户派生新列。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="2f87" class="kg kh hi kc b fi ki kj l kk kl"><strong class="kc hj">//Construct DataFrame which can shown on Console, can be saved as CSV file or as a Table.</strong><br/>val rawRecordsConstruct = spark.sql(rawRecordsTimeFix).map(RawRatings.<em class="km">parse</em>(_)).groupByKey(_.user_id).mapGroups {<br/>  case (user_id, ratings_iter) =&gt; {<br/>    val ratings_List = ratings_iter.toList<br/>val ratings_attr_vals = <em class="km">getRatingsAttributes</em>(ratings_List)</span><span id="8dfd" class="kg kh hi kc b fi kn kj l kk kl">    <em class="km">Row</em>(user_id, ratings_attr_vals.mov_watch_count, ratings_attr_vals.latest_movie_rated, ratings_attr_vals.first_movie_rated, ratings_attr_vals.more_than_six_rtngs, ratings_attr_vals.less_than_two_rtngs, ratings_attr_vals.highest_rating, ratings_attr_vals.lowest_rating)<br/>  }<br/>}(<em class="km">userRatingsColumnsEncoder</em>)</span></pre><p id="0d05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">创建一个为我们的表布局计算列值的方法。</p><pre class="je jf jg jh fd kb kc kd ke aw kf bi"><span id="7800" class="kg kh hi kc b fi ki kj l kk kl">def getRatingsAttributes(rawList: List[RawRecords]): ratingsRec = {<br/><strong class="kc hj">  //Create variables which has to be returned.</strong></span><span id="a3d9" class="kg kh hi kc b fi kn kj l kk kl">  var user_id = 999999<br/>  var mov_watch_count = 999999<br/>  var latest_movie_rated = 999999<br/>  var first_movie_rated = 999999<br/>  var more_than_six_rtngs = "NA"<br/>  var less_than_two_rtngs = "NA"<br/>  var highest_rating: Double = 999999.0<br/>  var lowest_rating: Double = 999999.0<br/></span><span id="56fd" class="kg kh hi kc b fi kn kj l kk kl">user_id = rawList.map(x =&gt; x.user_id).head.toInt<br/>  mov_watch_count = rawList.map(x =&gt; x.movie_id).size<br/>  latest_movie_rated = rawList.sortWith(_.date_of_rating &gt; _.date_of_rating).map(x =&gt; x.movie_id).head<br/>  first_movie_rated = rawList.sortWith(_.date_of_rating &lt; _.date_of_rating).map(x =&gt; x.movie_id).head<br/>  more_than_six_rtngs = if (rawList.map(x =&gt; x.rating).size &gt; 6) "Y" else "NA"<br/>  less_than_two_rtngs = if (rawList.map(x =&gt; x.rating).size &lt; 2) "Y" else "NA"<br/>  highest_rating = rawList.map(x =&gt; x.rating).sortWith(_ &gt; _).head<br/>  lowest_rating = rawList.map(x =&gt; x.rating).sortWith(_ &lt; _).head</span><span id="26ad" class="kg kh hi kc b fi kn kj l kk kl">  <em class="km">ratingsRec</em>(user_id, mov_watch_count, latest_movie_rated, first_movie_rated, more_than_six_rtngs, less_than_two_rtngs, highest_rating, lowest_rating)<br/>}</span><span id="357f" class="kg kh hi kc b fi kn kj l kk kl"><strong class="kc hj"><em class="km">//Show the Results on IntelliJ Run Window.<br/></em></strong><em class="km">println</em>("Showing the Results now")<br/>rawRecordsConstruct.show(false)</span></pre><p id="5408" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是时候点击IntelliJ上绿色按钮(或)运行程序来查看输出结果了。Spark开始工作，不到一分钟就返回结果。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="ko kp l"/></div></figure><p id="e594" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意</strong>:上面的User_id和movie_id都是重构的。</p><p id="8518" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好了，你认为我们还可以在这里添加哪些列，比如给定用户的<strong class="ih hj">一年内没有电影被观看，一个月内没有电影被观看，</strong>等等..<br/>如果我们可以用<strong class="ih hj">‘movie_id’s and Movie _ Name’</strong>加入Movie _ Id的back，并根据用户评级找到最受欢迎的电影或向用户推荐电影，我们将很快看到这些结果。</p><p id="87e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">欢迎任何建议。<br/>找到我的GitHub存储库页面，其中包含了本文使用的全部代码:<a class="ae jt" href="https://github.com/pavanobj/spark_series" rel="noopener ugc nofollow" target="_blank">https://github.com/pavanobj/spark_series</a><br/>网上有很多免费的数据集，可以用来进行各种分析。</p><p id="f1b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢您花时间阅读本文，下次再见😃</p></div></div>    
</body>
</html>