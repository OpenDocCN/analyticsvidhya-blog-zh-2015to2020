<html>
<head>
<title>Minecraft Mapper</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">《我的世界》制图仪</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/minecraft-mapper-ab4478e98297?source=collection_archive---------14-----------------------#2020-06-23">https://medium.com/analytics-vidhya/minecraft-mapper-ab4478e98297?source=collection_archive---------14-----------------------#2020-06-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="ef7a" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">应用计算机视觉</h2><div class=""/><div class=""><h2 id="788c" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">比较Tesseract和Google Cloud Vision OCR引擎</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/1e66178c83bb0a8ead0bc5761ad8216f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HhiUq5E6rfi6NPMB0F-hcg.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">照片由N. on Unsplash拍摄</figcaption></figure><p id="526a" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi ks translated">俗话说“一幅画胜过千言万语”，那么如果一幅画里有千言万语呢？要使该图像有用，首先需要从图像中提取文本。考虑到乍看起来有多简单，这实际上是一项相当棘手的任务。幸运的是，已经有一些非常聪明的人在研究这个问题。在本帖中，我们将讨论使用对象字符识别(OCR)系统来执行一个相对简单的任务:从《我的世界》调试屏幕中提取一些文本。</p><blockquote class="lb lc ld"><p id="85a1" class="jw jx le jy b jz ka is kb kc kd iv ke lf kg kh ki lg kk kl km lh ko kp kq kr hb bi translated">免责声明:这可能不是这项任务的最佳解决方案，而是一个有趣的项目来测试和学习，而不是创建一个优化的解决方案。实际上，我们可能只是使用单个字符识别，而不是成熟的LSTM，只是在用一些图像处理提取字母组后训练一个简单的字符分类器。由于文本相当僵硬，我们可以制定一个具体的解决方案，也许我会在未来。</p></blockquote><h2 id="c299" class="li lj hi bd lk ll lm ln lo lp lq lr ls kf lt lu lv kj lw lx ly kn lz ma mb ho bi translated">一些背景</h2><p id="dda2" class="pw-post-body-paragraph jw jx hi jy b jz mc is kb kc md iv ke kf me kh ki kj mf kl km kn mg kp kq kr hb bi ks translated">首先，如果你不熟悉《我的世界》调试界面，它经常被用来收集玩家当前所在位置的信息。菲尔·沃森，也被称为菲尔扎，你们中的一些人可能知道他5年的不朽的硬核跑步，经常使用这种方法来保存最喜欢的位置。XYZ旁边给出了玩家的坐标:……(见下图红色方框所示)在基于Java的游戏中，按F3键会弹出屏幕。随后点击F2将保存一个截图，通常保存在AppData\Roaming\位置。minecraft \截图。我学会了这个巧妙的小技巧，但发现自己希望当我试图记住某个特定位置(如某个特定的生物群落)时，能够以更快可用的方式更容易地提取和汇集数据。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mh"><img src="../Images/ab0d62acf247b8093ab23dba9de52461.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Na89UDk0i774qSgeEGE95Q.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">minecraft中的调试屏幕截图示例，其中突出显示了感兴趣的部分</figcaption></figure><p id="a032" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">现在，你是否对《我的世界》感兴趣并不重要。我们将会看到，这是一种应用和测试现有的两个OCR引擎的好方法:<a class="ae mi" href="https://tesseract-ocr.github.io/" rel="noopener ugc nofollow" target="_blank"> Tesseract </a>(通过包装器<a class="ae mi" href="https://pypi.org/project/pytesseract/" rel="noopener ugc nofollow" target="_blank"> pytesseract </a>使用)和<a class="ae mi" href="https://cloud.google.com/vision/docs/ocr" rel="noopener ugc nofollow" target="_blank"> Google Cloud Vision </a> API。</p><p id="c46a" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi ks translated">Tesseract最初于1985年至1994年间由布里斯托尔的惠普实验室和科罗拉多州格里利的惠普公司开发，2005年由惠普开源。自2006年以来，它是由谷歌开发的[1]。宇宙魔方4，这是我们将使用的版本，增加了一个新的神经网络，长短期记忆。<strong class="jy hs">长短期记忆(LSTM) </strong>是一种人工递归神经网络，具有处理数据序列而不是单个数据点的反馈连接。本质上，这允许单个字符串成单词/序列。Google Vision OCR引擎的一般工作流程如下所示。这将允许字母和字符序列被分组和分离，然后被提取。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mj"><img src="../Images/74e6c23dac6e778003a49ba13c9c3d26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*GVVEoCY89T26pvsOVuiFGg.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">宇宙魔方如何使用LSTMs [2]</figcaption></figure><p id="0594" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">再说一次，并不是所有这些都太重要，因为我们从事的是使用的<strong class="jy hs">业务。Google Vision API没有训练的方法，它只是简单地使用开箱即用，而Tesseract是用C++构建的，因此使用python包装器减少了模型访问。有些参数可以由pytesseract通过config参数指定；例如方向、语言和分类引擎。然而，我建议查看一下关于<a class="ae mi" href="https://tesseract-ocr.github.io/tessdoc/ImproveQuality" rel="noopener ugc nofollow" target="_blank"> pytesseract </a>的文档或这篇精彩的概述<a class="ae mi" href="https://nanonets.com/blog/ocr-with-tesseract/#technologyhowitworks?&amp;utm_source=nanonets.com/blog/&amp;utm_medium=blog&amp;utm_content=%5BTutorial%5D%20OCR%20in%20Python%20with%20Tesseract,%20OpenCV%20and%20Pytesseract" rel="noopener ugc nofollow" target="_blank">文章</a> [2】，因为我们不会在这里具体讨论太多细节。</strong></p><h2 id="3f62" class="li lj hi bd lk ll lm ln lo lp lq lr ls kf lt lu lv kj lw lx ly kn lz ma mb ho bi translated">事实真相</h2><p id="d9f1" class="pw-post-body-paragraph jw jx hi jy b jz mc is kb kc md iv ke kf me kh ki kj mf kl km kn mg kp kq kr hb bi ks translated">现在，这个项目的目标是从图像中提取文本，并以某种方式使用这些文本。要做到这一点，我们需要完成几项任务，这也是这个项目的真正目标:在更真实的环境中组织、练习和解决问题；而不是教科书上令人生厌的重复例子。该项目有几项主要任务，我们将以某种形式进行介绍:</p><ol class=""><li id="0102" class="mk ml hi jy b jz ka kc kd kf mm kj mn kn mo kr mp mq mr ms bi translated">从图像中提取文本——这将需要一些预处理+ OCR引擎</li><li id="d531" class="mk ml hi jy b jz mt kc mu kf mv kj mw kn mx kr mp mq mr ms bi translated">将文本解析成可用的结构——我们将使用正则表达式</li><li id="f5d9" class="mk ml hi jy b jz mt kc mu kf mv kj mw kn mx kr mp mq mr ms bi translated">抓取一组图像并序列化以供重用——文件处理和序列化，主要使用pandas</li><li id="65a5" class="mk ml hi jy b jz mt kc mu kf mv kj mw kn mx kr mp mq mr ms bi translated"><strong class="jy hs">从截图中提取文本</strong></li></ol><p id="f419" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这是我们将使用OCR引擎的地方。我不会详细介绍我所做的每一个细节和测试，因为我认为它不会提供任何有用的信息，github页面上还有一个更详细的Jupyter笔记本，但我想提出几点。首先，我从最容易获得的——pytesserac开始。安装需要几个步骤，但是如果您跟随并在本地运行，安装是相当简单的。我用的是宇宙魔方4.0版。您可以使用以下命令获得确切的版本:</p><pre class="jh ji jj jk fd my mz na nb aw nc bi"><span id="b630" class="li lj hi mz b fi nd ne l nf ng">pytesseract.get_tesseract_version()<br/>[1]: LooseVersion ('4.0.0.20181030')</span></pre><p id="7e4f" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">为了获得文本，我使用了以下代码:</p><pre class="jh ji jj jk fd my mz na nb aw nc bi"><span id="aebb" class="li lj hi mz b fi nd ne l nf ng">def get_string(img, config = r'--psm 3'):<br/>    string = pytesseract.image_to_string(img, lang='eng', config = config)<br/>    return string</span><span id="90a6" class="li lj hi mz b fi nh ne l nf ng">def clean_string(string):<br/>    tmp = ''<br/>    for line in string:<br/>        tmp += line.replace('\n', ' ')<br/>    tmp = tmp.replace("'",'"')<br/>    return tmp</span><span id="c8e9" class="li lj hi mz b fi nh ne l nf ng">img = cv2.imread(os.path.join(screenshot_dir, img_name))<br/>string = get_string(img)<br/>string = clean_string(string)</span></pre><p id="2aa1" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这是本文顶部图像的输出:</p><pre class="jh ji jj jk fd my mz na nb aw nc bi"><span id="7e19" class="li lj hi mz b fi nd ne l nf ng">Mimecraft 1,144 1 1499-0ptiFine_HO_U_F32 wanillas &amp;H/59 fps o chunk updates: T: inf wsanc fancu—clouds vbo Integrated server @ 2 m= ticks, 11 tx, 3685 rx IS T el el O o I S = W 1 1 I o = e E: 6795 E: A, OptiFine_1.14.4_HO F: 136 T: 95, A: 33 Client Chunk Cache: 1829 723 ServerChunkBache: Z6E1 minectattoverworld FCE  ATZ 157.545 / 628606868 ¢ 26860 685 Block: 157 &amp;2 2608 . : Chunk: 12 1528 in9 32 12 e Facing: north (Towards negative 22 (1433 £ 7.2 Client Light: 12 13 =ky, 8 blocks  Serwver Light: 13 sky, 8 blocks  CH S 79 H: 74  SHS 7074 M 79 HL: 62  Biome: minecraft:forest  Lozal Difficulty; 225 £ 812 (Dag &amp;  Looking at Lliguid: 158 &amp;2 188  Sounds: 52497 + 158  Oebug: Fie [shiftl: hidden FFS + TFS [altl hidden  : Digp_ga,%  Jf 3  For help: press F3 + 1 PS A e Rk  - A - A                                           Jdaua: 1208 51 cdhbit Mem: 474 971 2892ME Allozated: 235K 17eBME Hative: 257 =26848+52M6  CEL: 16x AMD Fuzen 7 17688 Eight-Core =Tl =t 19281886 (HYIDIA Corporation?  efgroe 0T 1868 60B/FCle 33E2 C 9 P R R T  Targeted Fluid e s R e falling: fal== #minecraftwater</span></pre><p id="0c7b" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">正如你可以告诉只是定性，它并不伟大。提醒我们，我真正关心的是XYZ后面的数字:..和生物群落的类型。它在这些章节中摘录了以下内容:</p><pre class="jh ji jj jk fd my mz na nb aw nc bi"><span id="c740" class="li lj hi mz b fi nd ne l nf ng">ATZ 157.545 / 628606868 ¢ 26860<br/>Biome: minecraft:forest</span></pre><p id="cf24" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">生物群落部分实际上是正确的。现在，一种可能提高准确性的方法是对我们的图像进行一些处理，因为通过一些测试，我注意到Tesseract根据字体做得更好。为了理解接下来的处理步骤，如果我们实际上做得更好或更差，我将使用Jaccard相似性度量[4]。我们可以这样写:</p><pre class="jh ji jj jk fd my mz na nb aw nc bi"><span id="08be" class="li lj hi mz b fi nd ne l nf ng">def get_jaccard_sim(str1, str2): <br/> a = set(str1.split()) <br/> b = set(str2.split())<br/> c = a.intersection(b)<br/> return float(len(c)) / (len(a) + len(b) — len(c))</span></pre><p id="ef5d" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">在写出我认为是基本事实的东西后，最初的尝试有一个大约0.2的Jaccard相似度。然后，我开始尝试一些不同的预处理步骤，如转换为灰度，阈值，并应用一些过滤器，试图使文本更容易处理。事实证明，字体的类型起着很大的作用，尽管这种字体看起来非常简单，但事实并非如此。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ni"><img src="../Images/a7662a6665a8ea1832e91cb94e314da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y-fD36AUi7q4v2vB68tuTw.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">应用处理步骤后的图像</figcaption></figure><p id="aa2b" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">以下是不同尝试后的结果:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nj"><img src="../Images/c77f9817101d564897b53f1530b29585.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*cTqMMurrjC5oIWbkC1tHVg.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">应用不同处理步骤后的Jaccard相似性结果</figcaption></figure><p id="d00d" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">正如你所看到的，最后一组使用阈值处理和一些过滤的处理给出了一个更好的结果。我感兴趣的部分的确切文本，即坐标和生物群落是:</p><pre class="jh ji jj jk fd my mz na nb aw nc bi"><span id="5d65" class="li lj hi mz b fi nd ne l nf ng">8YZ: 157.545 / 63.00000 / 200.666</span><span id="d330" class="li lj hi mz b fi nh ne l nf ng">Biome: minecraft.forest</span></pre><p id="7cb8" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这是他们最亲密的一次。下面是我使用的处理代码:</p><pre class="jh ji jj jk fd my mz na nb aw nc bi"><span id="c0fb" class="li lj hi mz b fi nd ne l nf ng">def process_image(img):<br/> thresh_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)<br/> _,thresh_img = cv2.threshold(thresh_img,220,255,cv2.THRESH_BINARY)<br/> thresh_img = cv2.GaussianBlur(thresh_img,(5,5), 0)<br/> thresh_img = cv2.bilateralFilter(thresh_img,5,50,10)<br/> _,thresh_img = cv2.threshold(thresh_img,50,255,cv2.THRESH_BINARY)<br/> kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))<br/> thresh_img = cv2.morphologyEx(thresh_img, cv2.MORPH_OPEN, kernel)<br/> return thresh_img</span></pre><p id="ccc9" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">玩了一段时间后，我决定研究另一种方法，这使我想到了Google Cloud Vision API。同样，这很容易设置。安装指南可以在<a class="ae mi" href="https://googleapis.dev/python/vision/latest/index.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到，一些文档可以在<a class="ae mi" href="https://cloud.google.com/vision/docs/ocr" rel="noopener ugc nofollow" target="_blank">这里</a>找到。为此，你需要一个API密匙，这是一项你最终可以付费的服务。即使有“永远免费”的使用限制，你也必须有一个有效的付费账户。无论如何，一定要检查所有的文档。我建议您像我一样，在处理每张图像后序列化并保存文本输出，这样就不会过多地调用Vision API。说了这么多，但是精确度确实是我一直期待的。我们有一个智能卡sim卡。0.8的！是我用宇宙魔方能做到的两倍。更重要的是，我们还获得了我们感兴趣的文本的良好准确性:</p><pre class="jh ji jj jk fd my mz na nb aw nc bi"><span id="ef3f" class="li lj hi mz b fi nd ne l nf ng">XYZ: 157.545 / 63.00000 / 200.666<br/>Biome: minecraft:forest</span></pre><p id="bccf" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">因此，我们现在将使用这个解决方案，但是我仍然非常赞成使用本地运行的解决方案，所以我计划将来研究pytesseract或其他解决方案。此外，我将图像的最终处理作为图像输入到Vision API中。</p><blockquote class="lb lc ld"><p id="5fcd" class="jw jx le jy b jz ka is kb kc kd iv ke lf kg kh ki lg kk kl km lh ko kp kq kr hb bi translated">这里的关键是，谷歌视觉解决方案绝对优于宇宙魔方。但是，通过一些处理，从Tesseract获得的结果可以得到改善。使用训练有素的网络和关于这些图像的一些知识的更简单但更具体的定制解决方案将是最好的方法。</p></blockquote><p id="bce4" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated"><strong class="jy hs"> 2。将文本解析成可用格式</strong></p><p id="5f55" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">既然我们已经以足够准确的方式从图像中获得了文本，我们就可以解析它，只收集我们关心的部分。为此，我使用了标准的<a class="ae mi" href="https://docs.python.org/3/library/re.html" rel="noopener ugc nofollow" target="_blank">正则表达式库</a>。这是我的两个功能:</p><pre class="jh ji jj jk fd my mz na nb aw nc bi"><span id="15a0" class="li lj hi mz b fi nd ne l nf ng">def get_coords(data):<br/>    p = re.compile(r'-?\d+\.\d+\s?\/\s?-?\d+\.\d+\s?\/\s?-?\d+\.\d+')<br/>    coords = p.findall(data)<br/>    if coords:<br/>        coords = re.findall(r'-?\d+\.\d+', coords[0])<br/>        coords = [float(x) for x in coords]<br/>    else:<br/>        pass<br/>    return coords</span><span id="428e" class="li lj hi mz b fi nh ne l nf ng">def get_biome(data):<br/>    p = re.compile(r'Biome\: minecraft\:?(\w+)')<br/>    biome = p.findall(data)<br/>    return biome</span></pre><blockquote class="lb lc ld"><p id="558a" class="jw jx le jy b jz ka is kb kc kd iv ke lf kg kh ki lg kk kl km lh ko kp kq kr hb bi translated">注意:由于我从Tesseract获得的文本非常接近，我可能会在正则表达式中使用一些变化来解释任何不准确之处，而不是从图像中获得完美的文本。这也是需要进一步探索的。</p></blockquote><p id="42ca" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">上面给出了坐标列表的输出:[157.545，63.0，200.666]和生物群落“森林”。</p><p id="02a0" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated"><strong class="jy hs"> 3。抓取图像集，序列化并绘制</strong></p><p id="904c" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这部分是比较直截了当的部分。我们只需浏览截图目录中的所有文件，并将保存的位置、生物群落和文本添加到JSON文件中。在我看来，创建一个熊猫数据框架是处理这个问题最简单的方法。我还实现了一个检查，看看我们是否已经处理了截图，以免浪费OCR计算时间。首先，迭代的主要步骤:</p><pre class="jh ji jj jk fd my mz na nb aw nc bi"><span id="c584" class="li lj hi mz b fi nd ne l nf ng">def do_steps(entry):<br/>    coords = None<br/>    biomes = None<br/>    img = cv2.imread(entry.path)<br/>    processed_img = process_image(img)<br/>    string_vision = get_string_gvision(processed_img)<br/>    string_vision = clean_string(string_vision)<br/>    coord = get_coords(string_vision)<br/>    biome = get_biome(string_vision)<br/>    if coord:<br/>        coords = coord<br/>        if biome:<br/>            biomes = biome[0]<br/>        else:<br/>            biomes = 'unknown'<br/>    return entry.name, string_vision, coords, biomes</span></pre><p id="8c5f" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">主迭代循环处理图像的读取并保存到JSON，同时检查图像是否已经处理过:</p><pre class="jh ji jj jk fd my mz na nb aw nc bi"><span id="bc3e" class="li lj hi mz b fi nd ne l nf ng">df = None<br/>try:<br/>    df = pd.read_json(os.path.join('output','dataframe.json'), orient='records')<br/>except ValueError:<br/>    print('No dataframe found, creating new.')</span><span id="f0d0" class="li lj hi mz b fi nh ne l nf ng">if df is None:<br/>    print('[INFO] Creating dataframe...')<br/>    df = pd.DataFrame(columns=['Screenshot', 'Text', 'Coords_x', 'Coords_y', 'Coords_z', 'Biomes'])<br/>    for entry in os.scandir(screenshot_dir):<br/>        if entry.path.endswith(".png") and entry.is_file():<br/>            entry_name, text, coords, biomes = do_steps(entry)<br/>            if coords:<br/>                print('[INFO] Adding new entry: {}'.format(entry.name))<br/>                df = df.append({'Screenshot': [entry_name], <br/>                                'Text': text, <br/>                                'Coords_x': coords[0],<br/>                                'Coords_y': coords[1],<br/>                                'Coords_z': coords[2],<br/>                                'Biomes': biomes}, ignore_index=True)<br/>            else:<br/>                print('[INFO] no coordinates found: {}'.format(entry.name))</span><span id="30ac" class="li lj hi mz b fi nh ne l nf ng">else: #df already exists<br/>    for entry in os.scandir(screenshot_dir):<br/>        if entry.path.endswith(".png") and entry.is_file():<br/>            if entry.name in df.Screenshot.str[0].values:<br/>                print('[INFO] Entry already exists: {}'.format(entry.name))<br/>            else:<br/>                print('[INFO] Adding new entry: {}'.format(entry.name))<br/>                entry_name, text, coords, biomes = do_steps(entry)<br/>                if coords:<br/>                    df = df.append({'Screenshot': [entry_name], <br/>                                    'Text': text, <br/>                                    'Coords_x': coords[0],<br/>                                    'Coords_y': coords[1],<br/>                                    'Coords_z': coords[2],<br/>                                    'Biomes': biomes}, ignore_index=True)<br/>                else:<br/>                    print('[INFO] no coordinates found: {}'.format(entry.name))<br/>                <br/>df.to_json(os.path.join('output','dataframe.json'), orient='records')</span></pre><p id="47cc" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">终于有情节了！我使用散点图，将显示悬停截图的缩略图。这是matplotlib和seaborn的组合。我不得不称赞这个<a class="ae mi" href="https://stackoverflow.com/questions/42867400/python-show-image-upon-hovering-over-a-point" rel="noopener ugc nofollow" target="_blank">帖子</a>的形象展示。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nk"><img src="../Images/15d0388de65264d734a229a3d8514aef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*oi651gtmc9FZM5g06LzocA.gif"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">显示截图、位置和各自生物群落的标绘数据。</figcaption></figure><h2 id="3172" class="li lj hi bd lk ll lm ln lo lp lq lr ls kf lt lu lv kj lw lx ly kn lz ma mb ho bi translated">未来更新</h2><p id="16e8" class="pw-post-body-paragraph jw jx hi jy b jz mc is kb kc md iv ke kf me kh ki kj mf kl km kn mg kp kq kr hb bi translated">我总是喜欢添加一个未来工作部分，也许是出于科学写作的习惯，但也是为了让我未来的自己可以探索这个项目或下一个项目的改进。</p><ul class=""><li id="57e6" class="mk ml hi jy b jz ka kc kd kf mm kj mn kn mo kr nl mq mr ms bi translated">当然，正如我提到的，为这个任务创建一个定制的解决方案将允许它在本地运行，并且使用标准的图像处理和简单的字符识别模型架构应该很容易做到。</li><li id="3945" class="mk ml hi jy b jz mt kc mu kf mv kj mw kn mx kr nl mq mr ms bi translated">更有效的图像序列化，像hdf5而不是json。</li><li id="6192" class="mk ml hi jy b jz mt kc mu kf mv kj mw kn mx kr nl mq mr ms bi translated">一个不同的情节，也许使用plotly或一些基于网络的可视化，使其更容易访问。</li></ul><p id="7316" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">如果你在这篇文章的任何部分提供了一些有用的信息或一点灵感，请考虑给予认可，并关注我以获取更多信息。</p><p id="0300" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">你可以在我的<a class="ae mi" href="https://github.com/robintwhite" rel="noopener ugc nofollow" target="_blank"> github </a>上找到源代码。</p><h1 id="2e08" class="nm lj hi bd lk nn no np lo nq nr ns ls ix nt iy lv ja nu jb ly jd nv je mb nw bi translated">参考</h1><ol class=""><li id="4f73" class="mk ml hi jy b jz mc kc md kf nx kj ny kn nz kr mp mq mr ms bi translated">宇宙魔方光学字符识别。【https://github.com/tesseract-ocr/tesseract T4】</li><li id="7570" class="mk ml hi jy b jz mt kc mu kf mv kj mw kn mx kr mp mq mr ms bi translated">泽卢克和萨贝尔。使用Tesseract、OpenCV和Python进行OCR的综合指南。2020年1月。<a class="ae mi" href="https://nanonets.com/blog/ocr-with-tesseract/" rel="noopener ugc nofollow" target="_blank">https://nanonets.com/blog/ocr-with-tesseract/</a></li><li id="b5b8" class="mk ml hi jy b jz mt kc mu kf mv kj mw kn mx kr mp mq mr ms bi translated">沃克j，藤井y，Popat A. C..2018.<a class="ae mi" href="https://das2018.cvl.tuwien.ac.at/media/filer_public/85/fd/85fd4698-040f-45f4-8fcc-56d66533b82d/das2018_short_papers.pdf" rel="noopener ugc nofollow" target="_blank">基于网络的文档OCR服务</a>。</li><li id="9b60" class="mk ml hi jy b jz mt kc mu kf mv kj mw kn mx kr mp mq mr ms bi translated">古普塔。Python中文本相似性度量的概述。<a class="ae mi" href="https://towardsdatascience.com/overview-of-text-similarity-metrics-3397c4601f50" rel="noopener" target="_blank">https://towards data science . com/overview-of-text-similarity-metrics-3397 c 4601 f 50</a>。2018年5月</li></ol></div></div>    
</body>
</html>