<html>
<head>
<title>Feed forward and back propagation back-to-back — Part 3 (Neural Network’s forward pass)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">背对背前馈和反向传播—第3部分(神经网络的正向传递)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feed-forward-and-back-propagation-back-to-back-part-3-neural-networks-forward-pass-559f57a437f2?source=collection_archive---------9-----------------------#2020-08-01">https://medium.com/analytics-vidhya/feed-forward-and-back-propagation-back-to-back-part-3-neural-networks-forward-pass-559f57a437f2?source=collection_archive---------9-----------------------#2020-08-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="93b8" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">前言</h1><p id="4791" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在本系列的第1部分(<a class="ae kb" rel="noopener" href="/@brunoosiek/feed-forward-and-back-propagation-back-to-back-part-1-linear-equation-4f98abbd0d14">作为神经网络构建模块的线性方程</a>)中，我们看到了什么是线性方程，也对它们在构建神经网络中的重要性有所了解。</p><p id="74e3" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">在第2部分(<a class="ae kb" rel="noopener" href="/analytics-vidhya/feed-forward-and-back-propagation-back-to-back-part-2-linear-equation-in-multidimensional-space-585f7d137be1">多维空间中的线性方程</a>)中，我们看到了如何处理向量空间中的线性方程，这方便了我们处理许多变量。</p><p id="d32c" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">现在，我将向您展示如何将一个线性方程嵌入另一个线性方程(数学上称为函数合成)来构建神经网络。然后，我将继续讲述权重矩阵和特征向量的线性组合如何帮助我们解决前馈过程中涉及的所有数学问题，并以一个工作示例结束本文。</p><p id="371d" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">本系列有一个咒语(安慰的话)，我将在下面重复一遍，以防最终读者没有阅读第1部分。</p><h1 id="797e" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">咒语:给最终读者的安慰</h1><p id="a49b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我不会让梯度和梯度下降、微积分和多元微积分、导数、链式法则、线性代数、线性组合和线性方程等概念成为阻碍你理解掌握神经网络所需数学的巨石。希望在本系列结束时，这些概念将被读者视为强大的工具，以及它们如何简单地应用于构建神经网络。</p><h1 id="32eb" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">功能组成</h1><p id="3e3c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如果线性方程是神经网络的构建模块，那么函数组合就是将它们绑定在一起的东西。太好了！但是什么是函数构成呢？</p><p id="d6ce" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">让我们考虑下面的两个线性方程:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/f3c0f455b22b8859cbde1de497f853e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*sdnn0DDhpuPIHXvD9tlpMg.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">等式1:函数组合</figcaption></figure><p id="4643" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">这里有什么不同？不同的是，为了计算<strong class="jf hj"> <em class="kt"> f(x) </em> </strong>的值，对于任意给定的<strong class="jf hj"> <em class="kt"> x </em> </strong>，我们首先需要计算<strong class="jf hj"> <em class="kt"> g(x) </em> </strong>的值。这个简单的概念被称为函数组合。</p><p id="c189" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">上面的定义和符号虽然正确，但不是常用的。函数合成通常被定义为将函数<strong class="jf hj"><em class="kt">【g(x)</em></strong>的结果应用于函数<strong class="jf hj"><em class="kt">【f(x)</em></strong>的运算，产生函数<strong class="jf hj"><em class="kt">【h(x)</em></strong>。于是:<strong class="jf hj"> <em class="kt"> h(x) = f(g(x))。</em> </strong>另一种批注<strong class="jf hj"> <em class="kt"> </em> </strong>是:<strong class="jf hj"><em class="kt">(f ∘g)(x)=f(g(x))</em></strong>。并且上述等式被写成如下:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ku"><img src="../Images/e343b21901f04167faadc2a65e06f117.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*M_vfqmW6d0FL2WAepLWZnQ.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">等式2: <strong class="bd ih"> h(x) </strong>写成<strong class="bd ih"> f(x) </strong>和<strong class="bd ih"> g(x) </strong>的合成</figcaption></figure><p id="844d" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">示意性地，上述函数组合可以如图1所示绘制。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es kv"><img src="../Images/215ce3d1ebc9d35f8e2abfde89649423.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S0bJpIxXHWHZCC1j-YBqoA.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">图1:描述为网络的功能组合</figcaption></figure><p id="cc44" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">上图中的<strong class="jf hj"> <em class="kt"> x </em> </strong>是输入，或者自变量。该输入乘以角度系数<strong class="jf hj"> <em class="kt"> a </em> ₂ </strong>，再加上<strong class="jf hj"> <em class="kt"> b </em> ₂ </strong>得到<strong class="jf hj"> <em class="kt"> g(x) </em> </strong>。依次，<strong class="jf hj"> <em class="kt"> g(x) </em> </strong>乘以<strong class="jf hj"><em class="kt"/></strong>再加到<strong class="jf hj"><em class="kt"/></strong>得出<strong class="jf hj"> <em class="kt"> f(x) </em> </strong>。</p><p id="ef3a" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">我觉得这真的很酷！我们不是离神经网络越来越近了吗？</p><p id="01f2" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">相信我。如果你理解了这两个概念(线性方程和函数组合)是什么，那么你在数学上就理解了80%的前馈神经网络是什么。剩下的就是理解如何添加额外的独立变量(<strong class="jf hj"><em class="kt">x₁</em></strong><strong class="jf hj"><em class="kt">x₂</em></strong>，…，<strong class="jf hj"> <em class="kt"> xₙ </em> </strong>)来使我们的神经网络能够处理许多(可能是大多数)真实世界的问题。</p><p id="4311" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">在本系列的第2部分中，我们看到了在处理n维线性方程时，在向量空间中工作是多么容易。为此，我们需要两个额外的简单向量操作:向量和矩阵转置；还有向量和矩阵的点积。</p><h1 id="671c" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">线性代数:向量和矩阵转置</h1><p id="d1a1" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">向量转置是列向量到行向量的变换，反之亦然，即行向量到列向量的变换。形式上:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es la"><img src="../Images/46e3bb24c11d5a319788209770f96bcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*XQUZ_arOJt8tc_Vvr79ZKw.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">等式3:向量转置</figcaption></figure><p id="45fd" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">类似地，矩阵转置包括将行变为列，反之亦然，如下所示。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lb"><img src="../Images/269b1e2f08ce714cd1cf434f014b6cc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*jyOc0fDaCeP8jvB_vvxrzw.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">等式4:矩阵的转置</figcaption></figure><h1 id="a363" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">线性代数:点积</h1><p id="a607" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jf hj"><em class="kt">【vector(u)=(u₁、u₂,…、uₙ) </em> </strong>与<strong class="jf hj"><em class="kt">【vector(v)=(v₁、v₂,…、vₙ) </em> </strong>的点积由下式给出:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lc"><img src="../Images/20eddac9cfde006a410512342a630db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/1*yVRyIVSsoXS0iOheRhat7Q.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">等式5:两个向量的点积</figcaption></figure><p id="b93e" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">而矩阵<strong class="jf hj"><em class="kt"/></strong><strong class="jf hj"><em class="kt">【矢量(u) </em> </strong>之间的点积是:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ld"><img src="../Images/e4ef59f155b33eb3522342d2dfc6e2d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*b268IRUMjSoSeRQYyFKMfA.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">等式6:矩阵和向量之间的点积</figcaption></figure><p id="9420" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">从上面的公式我们可以看出，两个向量需要有相同的维数<strong class="jf hj"> <em class="kt"> n </em> </strong>。两个向量的点积是一个实数(一个标量)。在矩阵和向量的情况下，矩阵的行数必须等于向量的列数。后者的点积产生一个列向量。</p><h1 id="560d" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">二维函数合成</h1><p id="26fa" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">图2是图1中包含的模式，增加了<strong class="jf hj"> <em class="kt">、x₂ </em> </strong>。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es le"><img src="../Images/dafb4105d1a4c23fa0c984e94eb8e717.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hYaZLZm9aJJ8-dKrAMK2ew.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">图2:一个二维函数组合</figcaption></figure><p id="90e0" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">数学上:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lf"><img src="../Images/ab2ea7839a6db19b6b4aff75ee1b8634.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*hiM3xLwsaEOdSSO-yJ_FAg.png"/></div></figure><p id="ce0c" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">现在让我们考虑函数<strong class="jf hj"><em class="kt">【g(x₁】</em></strong>的角度系数的向量为<strong class="jf hj"><em class="kt">【vector(aᵍ)=(a₂，a₃) </em> </strong>并且输入的向量为<strong class="jf hj"> <em class="kt">向量(x) = (x₁，x₂) </em> </strong>我们可以将函数<strong class="jf hj"> <em class="kt"> g(x₁，x \8307; 18】</em></strong>重写为:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lg"><img src="../Images/8cebbb8370c7150dd596b6e9b8e48d70.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*kI3Y0Cnatei0pCTbBexyWQ.png"/></div></figure><p id="f122" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">考虑到<strong class="jf hj"><em class="kt">【vector(aᶠ)=(a₁】0)</em></strong>我们可以把<strong class="jf hj"><em class="kt">【f(x₁】【x₂】</em></strong>改写为:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lh"><img src="../Images/c599bfe1caf7cc0ab7401fd4b5e42c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*zklWy7iYDXgc4hV3iXcUOg.png"/></div></figure><p id="5ba9" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">在上述两个方程中，矢量的乘法实际上是这些矢量的点积，产生如上所述的标量。</p><h1 id="ea53" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">神经网络前馈通路</h1><p id="7121" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">要达到神经网络前馈传递几乎没有什么需要添加的。它只是将输入向量提交给<strong class="jf hj"><em class="kt">(∘g)(x₁、x₂)=f(g(x₁、x₂)，</em> </strong>计算结果，如上面图5所示。</p><p id="dd16" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">在真实的神经网络中，仍然存在附加的函数组合，即将函数<strong class="jf hj"><em class="kt">【g(x)】</em></strong>和<strong class="jf hj"> <em class="kt"> f(x) </em> </strong>的输出提交给挤压函数。最常见的一种是下面描述的Sigmoid函数(σ(x))，它将所有输出转换到(0，1)范围内:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es li"><img src="../Images/46327006248606ff8780e4ba0646712c.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/format:webp/1*9eNQFCKDV5ZQxYYYVIgpxA.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">等式7: Sigmoid函数</figcaption></figure><p id="330e" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">对这种挤压功能的需要，在神经网络术语中称为激活功能，将在本系列的另一篇文章中阐明。现在可以说，这种组合使网络能够学习非线性模式。</p><h1 id="1155" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">一个例子</h1><p id="2bb0" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在本例中，我们将使用2层网络。第一层是隐藏层，第二层是输出层。在网络的层数中，我们不考虑输入层。网络如下图所示。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lj"><img src="../Images/a4282385cd26633161a45a990fa4911a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OCiSGk0ygKsi2V2PfbhGsA.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">图3:一个两层神经网络</figcaption></figure><p id="f701" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">在这里，我们改变了我们正在使用的术语，使之更接近我们通常在神经网络文档中找到的术语。我们使用字母<strong class="jf hj"><em class="kt"/></strong>作为角度系数。现在我们就用字母<strong class="jf hj"> <em class="kt"> w </em> </strong>(重量)。</p><p id="b1d7" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">上述网络接收两个输入:<strong class="jf hj"> <em class="kt"> x₁ </em> </strong>和<strong class="jf hj"> <em class="kt"> x₂.</em> </strong>隐藏层有两个节点(<strong class="jf hj"> <em class="kt"> H1 </em> </strong>和<strong class="jf hj"> <em class="kt"> H2 </em> </strong>)，而输出层有一个节点(<strong class="jf hj"> <em class="kt"> O1 </em> </strong>)。所有节点都有一个名为bias的固定输入，如图6用<strong class="jf hj"><em class="kt"/></strong><strong class="jf hj"><em class="kt"/></strong>和<strong class="jf hj"> <em class="kt"> b₃.来表示</em> </strong></p><p id="d216" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">在输入层和隐藏层之间有4个权重。w中的上标1是对这些权重的引用，因为w是对隐藏层和输出层之间的权重的引用。下标<strong class="jf hj"> <em class="kt"> w ₁₁ </em> </strong>是对<strong class="jf hj"><em class="kt">【x₁】</em></strong>和<strong class="jf hj"> <em class="kt"> H1之间的权重(或角度系数)的引用。</em> </strong>重量<strong class="jf hj"> <em class="kt"> w ₂₁ </em> </strong>是指<strong class="jf hj"> <em class="kt"> x₂ </em> </strong>和<strong class="jf hj"> <em class="kt"> H1之间的角度系数。</em> </strong>同样地我们有<strong class="jf hj"> <em class="kt"> w ₁₂ </em> </strong>作为重量介于<strong class="jf hj"><em class="kt"/></strong><strong class="jf hj"><em class="kt">H2</em></strong>和<strong class="jf hj"> <em class="kt"> w ₂₂ </em> </strong>之间的重量<strong class="jf hj"><em class="kt"/></strong>和<strong class="jf hj"> <em class="kt"> H2。</em>T79】</strong></p><p id="5e2a" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">当我们在向量空间中工作时，我们可以用矩阵<strong class="jf hj"> <em class="kt"> W </em> </strong>来表示所有提到的权重，如下所示:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lk"><img src="../Images/fbd8f9085e3f91e507b9e1f2de7276b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*Waq6ZNBgKE750K7WmyxWGw.png"/></div></figure><p id="9c48" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">有了这个<strong class="jf hj"><em class="kt">【g(x)</em></strong>就变成了下面的等式其中<strong class="jf hj"/>w和<strong class="jf hj"> <em class="kt">矢量(x) </em> </strong>实际上是它们和<strong class="jf hj"> <em class="kt">矢量(b)</em></strong>=(<strong class="jf hj"><em class="kt">b₂</em></strong>，<strong class="jf hj"><em class="kt">b₃<em class="kt"/></em></strong></p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ll"><img src="../Images/4a7fbb6cd94b41884d5e1a488cbfbe16.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*sBLj0bbHZ4s_qdARIoyGTA.png"/></div></figure><p id="91e0" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">节点<strong class="jf hj"> <em class="kt"> H1 </em> </strong>的结果是这样的:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="er es lm"><img src="../Images/a3aa46e4d139bce4737705d483d27fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:180/format:webp/1*AT4fBgQ_4juHS9SI-HrXYA.png"/></div></div></figure><p id="2f14" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">考虑到<strong class="jf hj"> <em class="kt">向量(b ) = (b₁，0)，</em> </strong>同样，我们得到的是<strong class="jf hj"><em class="kt"/></strong>隐藏层和输出层之间的权重矩阵，为:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ln"><img src="../Images/abcff19db422aa401c7190ff17ac09bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*XFSP8hrEIZKOn4yYGtP9zg.png"/></div></figure><p id="0acf" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated"><strong class="jf hj"> <em class="kt"> f(x) </em> </strong>中的屈服等于:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lo"><img src="../Images/8c4db9a7931e822dddd3350e31e977ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*WhKrv-mstqPjJLdV8W8jcw.png"/></div></figure><p id="55c4" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">网络的输出为:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lp"><img src="../Images/108dae29f92354e25b79c63d6ccba36a.png" data-original-src="https://miro.medium.com/v2/resize:fit:176/format:webp/1*OQWe4IuyiE0HWjTvQXT_Bw.png"/></div></figure><h1 id="934c" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结论</h1><p id="b8a7" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们看到，神经网络的前馈过程基本上由向量空间中的线性方程的函数组合组成。示例的图3中所示的网络也被称为多层感知器(MLP ),在图像和文本分类中具有有趣的结果。</p><p id="e538" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">在本系列的第一部分中，我们学习了如何在二维空间中找到线性方程的角系数。在具有许多层和多个维度的神经网络中，计算所有权重(角度系数)和偏差的过程称为反向传播，这是本系列的主题。</p><p id="2943" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">请关注此空间的更新！</p></div></div>    
</body>
</html>