<html>
<head>
<title>Tips for Single Shot Object Detectors</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单次拍摄物体探测器提示</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/tips-for-single-shot-object-detectors-aa8ad4b3ea06?source=collection_archive---------19-----------------------#2020-12-30">https://medium.com/analytics-vidhya/tips-for-single-shot-object-detectors-aa8ad4b3ea06?source=collection_archive---------19-----------------------#2020-12-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6703" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目标检测是计算机视觉中最核心和最关键的任务之一。这也是一项有许多实际好处的任务。从自动驾驶到监控，训练有素的物体检测器可以带来很多性能优势。</p><p id="217d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">主要由卷积神经网络(CNN)架构以及最近由 Transformer 架构驱动的深度学习辅助计算机视觉的最新进展，已经产生了许多优秀的对象检测器，供计算机视觉从业者使用。针对中枢神经网络，已经开发了一系列两阶段方法的模型。这些包括快速的 R-CNN 和更快的 R-CNN，两个是为从业者设计的。正如描述所暗示的，这些设计需要两次通过图像:在快速通过中，网络学习制定良好的感兴趣区域(RoI ),在第二次通过中，RoI 被链接到要检测的对象。</p><p id="23f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以想象，两遍设计使得这些设计训练起来更慢，因此开发了单次检测器(SSD ),要求单次通过图像。该网络执行产生感兴趣区域的任务，在该设计中称为锚盒，以及在这些设计中同时进行物体分类。这种架构的例子包括 SSD、YOLO、RetinaNet 和 EfficientDet。虽然最初的单次检测器不够精确，但最近的修改极大地提高了这些设计的精度，并且它们更快的训练时间使它们非常适合实际应用。</p><p id="bb0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习架构的性能往往取决于精心选择的超参数，毫不奇怪，单次检测器也不例外——特别是，<strong class="ih hj"> <em class="jd">锚定比例</em> </strong>和<strong class="ih hj"> <em class="jd">锚定比例</em> </strong>就是这种参数的主要例子。这些参数以及所使用的图像大小和形状(例如 512x512 或 1024x1024 等)决定了被训练模型的整体准确性。让我们更深入地了解一下，我们如何为一项任务确定这些因素的最佳值。对于我们的例子，我们将在不同角度拍摄的图像中检测 NFL 球员的头盔。该数据集是最近 NFL 第一届和未来 Kaggle 挑战赛的一部分。我们将使用 EfficientDet 作为正在研究的模型。数据以复合系数 0 (512x512 图像)和批量大小 4(由于 GPU 限制)呈现用于训练。</p><p id="a318" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图像为 720x1280 RGB，并在头盔周围标注了边框:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/1b9945624cf5bddb3c95bf7dc59f9797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*K_oIZhSXETd1_UbN"/></div></div></figure><p id="a80a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，上面的基本图像是矩形的，与整个图像相比，对象(头盔)较小。默认情况下，EfficientDet 带有 COCO 参数。</p><p id="2910" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd"> anchors_scales: '[2 ** 0，2 ** (1.0 / 3.0)，2 ** (2.0 / 3.0)]' </em> </strong></p><p id="0cc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd"> anchors_ratios: '[(1.0，1.0)，(1.4，0.7)，(0.7，1.4)]' </em> </strong></p><p id="3b63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在没有调整的情况下，当模型在 NFL 数据上训练时，我们看到许多 0 损失步骤:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jq"><img src="../Images/4e14c7b4776d0e31f3a91ae2486b22e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*848t9Qct3CltaC5l"/></div></div></figure><p id="8066" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不幸的是，这并不意味着我们已经完美地拟合了数据。要了解发生了什么，我们需要深入了解这个模型是如何工作的。该模型使用不同要素图层的有效网络主干要素(BiFPN)来(1)生成回归量，(2)计算覆盖图像的锚点，然后(3)计算与回归量生成最佳 IoU(联合交集)的锚点。换句话说，该模型在不同的部分检查图像，但不使用原始像素值，而是由主干模型在不同层构建的抽象。欠条交集才是问题所在。</p><p id="94b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面(图 1)，我们形象地看到 10 个随机锚点:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jr"><img src="../Images/d4e35d5d897f0183d41c695d13e349b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/0*jqU2WjOfLIXK9974"/></div></div></figure><p id="bb8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图 1</p><p id="4046" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以看出，由于小头盔盒的尺寸，锚并没有被设置成用小头盔盒产生好的白条。大多数模型认为 0.5 或以上的 IoU 为正匹配。</p><p id="d448" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一件要记住的事情是，如果模型使用正方形图像，而源图像是矩形的，那么大量的“锚定房地产”可能会被浪费掉。灰色区域中建议的所有锚定框都不会导致重叠，因此对训练没有任何帮助(图 2)。假设图像中的大部分信息被保留，将图像裁剪成正方形可以使训练更有效。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es js"><img src="../Images/6018e81fc2e9d1a96acfb81227406182.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/0*fVD4n5GYWVINTN1-"/></div></figure><p id="bc31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图 2</p><p id="42d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，上面的<strong class="ih hj"> anchor_scale、scales </strong>和<strong class="ih hj">ratio</strong>参数可用于调整每个盒子的分辨率/覆盖范围。下面，我们展示了对(1)锚定比例(4.0 → 3.0)，(2)比例(生产的盒子)和(3)盒子的(纵横比)的调整。这些图像清楚地显示了每次修改所产生的不同形状和大小的盒子。图 1 使用默认的参数集生成了盒子。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jt"><img src="../Images/028a26f7a08bd2f6bbd1c46b9d632612.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/0*N--F6gJooQ0JFm7A"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ju"><img src="../Images/fe093aea6dd2f57c040614e9f173229e.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/0*W2dY8k2M5Dnsyxpy"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jv"><img src="../Images/f53cb7a3458a798a39218aed5b10b280.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/0*DQC7GDjkA33HTSEm"/></div></figure><p id="86db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用一些样本图像试验这些参数的不同值，以挑选产生良好 IoU 分数的选项，可以帮助训练更准确的 SSD 对象检测器。</p><p id="02ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我上面的分析有任何错误，或者如果你想提供任何建议，我将很高兴收到反馈。</p><p id="84b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">资源:</p><ol class=""><li id="d9d1" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated">PyTorch EfficientDet 实施<a class="ae kf" href="https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch" rel="noopener ugc nofollow" target="_blank">https://github.com/zylo117/Yet-Another-EfficientDet-Pytorch</a></li><li id="7ead" class="jw jx hi ih b ii kg im kh iq ki iu kj iy kk jc kb kc kd ke bi translated">卡格尔 NFL 比赛<a class="ae kf" href="https://www.kaggle.com/c/nfl-impact-detection" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/nfl-impact-detection</a></li><li id="9d7c" class="jw jx hi ih b ii kg im kh iq ki iu kj iy kk jc kb kc kd ke bi translated">EfficientDet:可扩展且高效的对象检测<a class="ae kf" href="https://arxiv.org/abs/1911.09070" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1911.09070</a></li></ol></div></div>    
</body>
</html>