<html>
<head>
<title>Dimensionality Reduction — PCA vs LDA vs t-SNE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维——主成分分析与线性判别分析和t-SNE</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/dimensionality-reduction-pca-vs-lda-vs-t-sne-681636bc686?source=collection_archive---------10-----------------------#2020-12-30">https://medium.com/analytics-vidhya/dimensionality-reduction-pca-vs-lda-vs-t-sne-681636bc686?source=collection_archive---------10-----------------------#2020-12-30</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><p id="bc6c" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">在本文中，我们将了解三种最流行的降维技术背后的直觉。</p><h2 id="6623" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">议程</h2><p id="32a3" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">在本文中，我们旨在提供降维技术背后的直觉。我们将主要关注三种最流行的技术——PCA、t -SNE和LDA。我们将讨论它们的优点、何时使用以及它们的实现。我们还将在数据集上实现所有三种算法。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/33a4a4f61894ac04acdc799d76da7579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LHRQpmz6DY_Tap1y.jpg"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">资料来源:epfl.ch</figcaption></figure></div><div class="ab cl ks kt go ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ha hb hc hd he"><h2 id="a427" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">介绍</h2><p id="6c94" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">在机器学习中，我们将特征或输入变量的数量称为维度。</p><p id="87d7" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">降维是指将数据从高维空间转换到低维空间，同时保持原始数据中大部分有意义的洞察力的过程。</p><p id="8c17" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">例如，一个数据集包含100列(即特征)，或者它可以是组成三维空间中一个大球体的点的阵列。降维是将列的数量减少到一个较低的数量，比如说两个维度。</p><p id="8e15" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">那么，为什么我们不能将所有的数据输入到一个机器学习模型中，而不进行任何降维呢？</p><h2 id="e8bd" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">维度的诅咒</h2><p id="6542" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">维度的诅咒指的是处理高维数据时遇到的问题。具有大量特征的数据集(通常有数百个或更多)被称为高维数据。高维数据带来的一些困难出现在分析或可视化数据以识别模式的过程中，一些困难出现在训练机器学习模型的过程中。</p><p id="12b4" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">由于高维数据导致的与训练机器学习模型相关的困难被称为维数灾难。</p><p id="148a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">随着维数的增加，分类器的性能会提高，直到达到最佳特征数。在不增加训练样本数量的情况下进一步增加维度会导致分类器性能下降。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kz"><img src="../Images/14ff4f037e742b612ed1d1d625752ee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/0*RdkljqFVTsIXRSQ9.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated"><strong class="bd je">休斯现象</strong></figcaption></figure><h2 id="2538" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">降维</h2><p id="eed8" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">降维可以通过特征选择和特征工程来实现。</p><p id="29c1" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要素选择是从数据中的所有可用要素中选择所需要素的过程。要素选择的主要目的是选择能够完美代表数据集的要素。</p><p id="b244" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">要素工程是将原始数据转换为要素的过程，这些要素可以很好地表示数据集。</p></div><div class="ab cl ks kt go ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ha hb hc hd he"><h2 id="0647" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated"><strong class="ak"> PCA —主成分分析</strong></h2><p id="1f75" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">主成分分析是一种非监督学习类的统计技术，用于使用数量较少的变量(称为主成分)来解释高维数据。</p><p id="c02a" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">主成分是数据集中原始变量的线性组合。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es la"><img src="../Images/4fa719321ca4a7847054b757e4d65325.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/0*wGJ0C7twJteSuqH6.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">资料来源:weigend.com</figcaption></figure><h1 id="a299" class="lb jd hh bd je lc ld le ji lf lg lh jm li lj lk jp ll lm ln js lo lp lq jv lr bi translated">步骤:</h1><ol class=""><li id="28f9" class="ls lt hh ig b ih jx il jy ip lu it lv ix lw jb lx ly lz ma bi translated">以d维数据集为例</li><li id="a4bb" class="ls lt hh ig b ih mb il mc ip md it me ix mf jb lx ly lz ma bi translated">计算每个维度的平均向量。</li><li id="3c05" class="ls lt hh ig b ih mb il mc ip md it me ix mf jb lx ly lz ma bi translated">计算协方差矩阵。</li><li id="8239" class="ls lt hh ig b ih mb il mc ip md it me ix mf jb lx ly lz ma bi translated">计算每个维度的特征向量和相应的特征值。</li><li id="713a" class="ls lt hh ig b ih mb il mc ip md it me ix mf jb lx ly lz ma bi translated">接下来，通过减少特征值对特征向量进行排序，并选择具有最大特征值的k个特征向量，以形成d×k维矩阵</li><li id="15e2" class="ls lt hh ig b ih mb il mc ip md it me ix mf jb lx ly lz ma bi translated">使用这个d×k特征向量矩阵将样本变换到新的子空间上。</li></ol><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mg"><img src="../Images/71ca1c7af42f27edc3177548edda45a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*g8YS0OTIdkC-wtcLuElabQ.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">酒店预订数据集的主成分分析。</figcaption></figure><p id="4206" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">使用sklearn实现PCA</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es mh"><img src="../Images/7e5220b913bdda49e7d528b483c606bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0XBjVMsi7SHv43oS1La3ZA.png"/></div></div></figure><h2 id="86da" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">PCA的弱点</h2><p id="d5f9" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">它很容易受到数据中异常值的影响。</p><p id="ccde" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">为了克服这个问题，已经开发了许多健壮版本的PCA，包括随机化PCA、稀疏PCA等。</p><p id="3925" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">PCA仅在处理连续数据时效果最佳。</p><h2 id="6888" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">线性鉴别分析</h2><p id="f9b7" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">线性判别分析寻求通过样本的类值来最好地分离(或区分)训练数据集中的样本。具体来说，该模型寻求找到输入变量的线性组合，该组合实现类之间样本的最大分离(类质心或均值)和每个类内样本的最小分离。</p><h2 id="349b" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">LDA的弱点</h2><p id="7995" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">LDA有一些严重的局限性:</p><p id="6611" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">I)如果设计不平衡(即，不同类别中的对象数量(高度)不同)，LDA不能很好地工作。</p><p id="e71f" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">ii)LDA对过度拟合敏感，并且LDA模型的验证至少是有问题的。</p><p id="6678" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">iii) LDA对于非线性问题(橙-香蕉形状点云的分离，类中类情况)不适用(较差)</p><h2 id="98a8" class="jc jd hh bd je jf jg jh ji jj jk jl jm ip jn jo jp it jq jr js ix jt ju jv jw bi translated">t-SNE——t-分布随机邻居嵌入</h2><p id="f101" class="pw-post-body-paragraph ie if hh ig b ih jx ij ik il jy in io ip jz ir is it ka iv iw ix kb iz ja jb ha bi translated">t-SNE是一种非线性降维技术，最常用于可视化高维数据集。</p><p id="5c11" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">有兴趣了解算法详细工作原理的可以参考这篇<a class="ae mi" href="http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf" rel="noopener ugc nofollow" target="_blank">研究论文</a>。</p><p id="4bf2" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这是一个使用sklearn库实现的t-SNE算法</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es mj"><img src="../Images/77660b3ecbd3dd7862a752467f736e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lEVOL0_4r2PFV74sZB43hQ.png"/></div></div></figure><p id="076e" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">参数—</p><ul class=""><li id="7127" class="ls lt hh ig b ih ii il im ip mk it ml ix mm jb mn ly lz ma bi translated"><strong class="ig hi"> n_components </strong>(默认:2):嵌入空间的尺寸。</li><li id="d37e" class="ls lt hh ig b ih mb il mc ip md it me ix mf jb mn ly lz ma bi translated"><strong class="ig hi">困惑度</strong>(默认:30):困惑度与其他流形学习算法中使用的最近邻个数有关。考虑选择一个介于5和50之间的值。</li><li id="7a5f" class="ls lt hh ig b ih mb il mc ip md it me ix mf jb mn ly lz ma bi translated"><strong class="ig hi">early _夸张</strong>(默认:12.0):控制原始空间中的自然簇在嵌入空间中的紧密程度，以及它们之间将有多大的空间。</li><li id="de3b" class="ls lt hh ig b ih mb il mc ip md it me ix mf jb mn ly lz ma bi translated"><strong class="ig hi"> learning_rate </strong>(默认值:200.0):t-SNE的学习率通常在(10.0，1000.0)范围内。</li><li id="47e3" class="ls lt hh ig b ih mb il mc ip md it me ix mf jb mn ly lz ma bi translated"><strong class="ig hi"> n_iter </strong>(默认值:1000):优化的最大迭代次数。应该至少250。</li></ul><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mo"><img src="../Images/5fd58f0a3b363dfdc78456c033e2d12d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*5OoooTN__FZ6DVwTnrslzQ.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">酒店预订数据集上的t-SNE(困惑度= 30，itr = 1000)。</figcaption></figure><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mp"><img src="../Images/6ac949323cddfba028d6294fb49d6f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*xgi4tdHjD7IyAGVymIthPg.png"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">酒店预订数据集上的t-SNE(困惑度= 30，itr = 5000)。</figcaption></figure><p id="d158" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">SNE霸王龙的弱点—</p><ol class=""><li id="c0cb" class="ls lt hh ig b ih ii il im ip mk it ml ix mm jb lx ly lz ma bi translated">t-SNE在数据点数量上具有二次时空复杂度。这使得它在应用于包含超过10，000个观察值的数据集时特别慢、计算量相当大并且消耗资源。</li><li id="f7a7" class="ls lt hh ig b ih mb il mc ip md it me ix mf jb lx ly lz ma bi translated">也可以在随机噪声中发现模式，因此在决定数据中是否存在模式之前，必须检查具有不同超参数集的算法的多次运行。</li></ol></div><div class="ab cl ks kt go ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="ha hb hc hd he"><p id="2edd" class="pw-post-body-paragraph ie if hh ig b ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb ha bi translated">这个项目的所有代码都可以在我的GitHub配置文件中找到。</p><div class="mq mr ez fb ms mt"><a href="https://github.com/vipinkatara/Hotel-booking-demand-ml" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hi fi z dy my ea eb mz ed ef hg bi translated">vipinkatara/酒店-预订-需求-ml</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">通过在GitHub上创建一个帐户，为vipinkatara/酒店预订需求ml开发做出贡献。</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">github.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh km mt"/></div></div></a></div><div class="mq mr ez fb ms mt"><a href="https://www.linkedin.com/in/vipin-katara-951a5750/" rel="noopener  ugc nofollow" target="_blank"><div class="mu ab dw"><div class="mv ab mw cl cj mx"><h2 class="bd hi fi z dy my ea eb mz ed ef hg bi translated">Vipin katara - Amity工程学院&amp;技术学院-新德里，德里，印度| LinkedIn</h2><div class="na l"><h3 class="bd b fi z dy my ea eb mz ed ef dx translated">查看Vipin katara在全球最大的职业社区LinkedIn上的个人资料。唯品教育上市于…</h3></div><div class="nb l"><p class="bd b fp z dy my ea eb mz ed ef dx translated">www.linkedin.com</p></div></div><div class="nc l"><div class="ni l ne nf ng nc nh km mt"/></div></div></a></div></div></div>    
</body>
</html>