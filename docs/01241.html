<html>
<head>
<title>Introduction to Web Scraping in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的Web抓取简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-web-scraping-in-python-41408f9c9762?source=collection_archive---------7-----------------------#2019-10-10">https://medium.com/analytics-vidhya/introduction-to-web-scraping-in-python-41408f9c9762?source=collection_archive---------7-----------------------#2019-10-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="9d0f" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><em class="hi">“我们拥有的数据太少，无法建立机器学习模型。我们需要更多的数据！”</em></p></blockquote><p id="caf1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">如果这听起来很熟悉，你并不孤单！想要更多的数据来训练我们的<a class="ae jk" href="https://courses.analyticsvidhya.com/courses/applied-machine-learning-beginner-to-professional/?utm_source=blog&amp;utm_medium=web-scraping-hands-on-introduction-python" rel="noopener ugc nofollow" target="_blank">机器学习</a>模型，这是一个永恒的问题。我们没有干净的现成的Excel或。数据科学项目中的csv文件，对吗？</p><p id="d131" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">那么，我们如何应对数据匮乏的障碍呢？</p><p id="023b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">最有效和简单的方法之一就是通过网络抓取。我个人发现网络抓取是一种从多个网站收集数据的非常有用的技术。如今，一些网站还为你可能想要使用的许多不同类型的数据提供API，比如Tweets或LinkedIn帖子。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jl"><img src="../Images/a022ecaf300edd8d81b8d740e819bbbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HV7DBqJQK3ipubsJ"/></div></div></figure><p id="9346" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">但是，有时您可能需要从不提供特定API的网站收集数据。这就是执行web抓取功能派上用场的地方。作为一名数据科学家，您可以编写一个简单的Python脚本并提取您正在寻找的数据。</p><p id="17dd" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">因此，在本文中，我们将学习web抓取的不同组件，然后直接进入<a class="ae jk" href="https://courses.analyticsvidhya.com/courses/introduction-to-data-science-2/?utm_source=blog&amp;utm_medium=web-scraping-hands-on-introduction-python" rel="noopener ugc nofollow" target="_blank"> Python </a>，看看如何使用流行且高效的<em class="ik"> BeautifulSoup </em>库来执行web抓取。</p><p id="0893" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><em class="ik">这里需要注意的是——网络抓取要遵守许多指导原则和规则。并非每个网站都允许用户抓取内容，因此存在一定的法律限制。在尝试抓取之前，请务必阅读网站的条款和条件。</em></p><h1 id="377e" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">网页抓取的组件</h1><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es kv"><img src="../Images/0ca41b8ec06396973b5ad523a6b526af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hG27q0d6jbOBM0eW"/></div></div></figure><p id="9ce9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">让我们详细了解一下这些组件。我们将从<em class="ik"> goibibo </em>网站收集酒店的详细信息，如酒店名称和每间客房的价格:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es kw"><img src="../Images/2ea93db6bf10f65e049b040589fdb1f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9_pjHKETbL7pbF-U"/></div></div></figure><p id="5591" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj"> <em class="ik">注意:</em> </strong> <em class="ik">始终遵循目标网站的</em><a class="ae jk" href="https://www.goibibo.com/robots.txt" rel="noopener ugc nofollow" target="_blank"><strong class="il hj"><em class="ik">robots . txt</em></strong></a><em class="ik">文件，也就是所谓的机器人排除协议。这告诉网络机器人不要抓取哪些页面。</em></p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es kx"><img src="../Images/1849d95d7de2c2a498b8e87d3c9df13e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/0*zygSLtiBWfDQ1wpC"/></div></div></figure><p id="c58f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">所以，看起来我们被允许从我们的目标URL中抓取数据。我们可以开始写我们网络机器人的脚本了。我们开始吧！</p><h1 id="2a75" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">第一步:爬行</h1><p id="0699" class="pw-post-body-paragraph ii ij hi il b im ky io ip iq kz is it jh la iw ix ji lb ja jb jj lc je jf jg hb bi translated">第一步是导航到目标网站并下载网页的源代码。我们将使用<a class="ae jk" href="https://pypi.org/project/requests/" rel="noopener ugc nofollow" target="_blank"> <strong class="il hj">请求</strong> </a>库来做到这一点。<em class="ik">其他几个可以发出请求并下载源代码的库是</em><a class="ae jk" href="https://docs.python.org/3/library/http.client.html#module-http.client" rel="noopener ugc nofollow" target="_blank"><em class="ik">http . client</em></a><em class="ik">和</em><a class="ae jk" href="https://docs.python.org/2/library/urllib2.html" rel="noopener ugc nofollow" target="_blank"><em class="ik">urlib 2</em></a><em class="ik">。</em></p><p id="992f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">一旦我们下载了网页的源代码，我们需要过滤我们需要的内容:</p><pre class="jm jn jo jp fd ld le lf lg aw lh bi"><span id="1190" class="li jy hi le b fi lj lk l ll lm"># importing required libraries<br/>import requests<br/>from bs4 import BeautifulSoup<br/>import pandas as pd</span><span id="52d0" class="li jy hi le b fi ln lk l ll lm"># target URL to scrap<br/>url = "<a class="ae jk" href="https://www.goibibo.com/hotels/hotels-in-shimla-ct/" rel="noopener ugc nofollow" target="_blank">https://www.goibibo.com/hotels/hotels-in-shimla-ct/</a>"</span><span id="7968" class="li jy hi le b fi ln lk l ll lm"># headers<br/>headers = {<br/>    'User-Agent': "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.90 Safari/537.36"<br/>    }</span><span id="59a7" class="li jy hi le b fi ln lk l ll lm"># send request to download the data<br/>response = requests.request("GET", url, headers=headers)</span><span id="0c5c" class="li jy hi le b fi ln lk l ll lm"># parse the downloaded data<br/>data = BeautifulSoup(response.text, 'html.parser')<br/>print(data)</span></pre><figure class="jm jn jo jp fd jq"><div class="bz dy l di"><div class="lo lp l"/></div></figure><h1 id="1446" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">步骤2:解析和转换</h1><p id="4838" class="pw-post-body-paragraph ii ij hi il b im ky io ip iq kz is it jh la iw ix ji lb ja jb jj lc je jf jg hb bi translated">下一步是将这些数据解析到HTML解析器中，为此，我们将使用<em class="ik"> BeautifulSoup </em>库。现在，如果你已经注意到了我们的目标网页，某个特定酒店的详细信息就像大多数网页一样在不同的卡片上。</p><p id="5230" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">所以下一步将是从完整的源代码中过滤出这些卡数据。接下来，我们将选择该卡，并单击“Inspect Element”选项以获取该特定卡的源代码。你会得到这样的东西:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es lq"><img src="../Images/1b4075e6f87eb40d0d432534a27f75c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_d-_Y9hNDu3MF9w-"/></div></div></figure><p id="0523" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">所有卡片的类名都是相同的，我们可以通过传递标签名和属性来获得这些卡片的列表，比如下面的<em class="ik"> &lt; class &gt; </em>标签，它的名称如下所示:</p><pre class="jm jn jo jp fd ld le lf lg aw lh bi"><span id="6c7f" class="li jy hi le b fi lj lk l ll lm"># find all the sections with specified class nam<br/>e<br/>cards_data = data.find_all('div', attrs={'class', 'width100 fl htlListSeo hotel-tile-srp-container hotel-tile-srp-container-template new-htl-design-tile-main-block'})</span><span id="0bc6" class="li jy hi le b fi ln lk l ll lm"># total number of cards<br/>print('Total Number of Cards Found : ', len(cards_data))</span><span id="9243" class="li jy hi le b fi ln lk l ll lm"># source code of hotel cards<br/>for card in cards_data:<br/>    print(card)</span></pre><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es lr"><img src="../Images/815af6b52a39cee5e400f5f0cee74f09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*31-bCHnmynU6yhsc"/></div></div></figure><p id="bbdd" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">我们已经从网页的完整源代码中过滤了卡片数据，这里的每张卡片都包含了关于一个单独酒店的信息。仅选择酒店名称，执行检查元素步骤，并对房间价格执行同样的操作:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es ls"><img src="../Images/249381b6e5e95f07d7db7676bff91582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KAtEmxUF5kYcfaqd"/></div></div></figure><p id="5554" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">现在，对于每张卡片，我们必须找到上面的酒店名称，该名称只能从<em class="ik"> &lt; p &gt; </em>标签中提取。<strong class="il hj">这是因为每张卡只有一个<em class="ik"> &lt; p &gt; </em>标签，房价由&lt;李&gt;标签连同&lt;类&gt;标签和类名称:</strong></p><pre class="jm jn jo jp fd ld le lf lg aw lh bi"><span id="1d27" class="li jy hi le b fi lj lk l ll lm"># extract the hotel name and price per room<br/>for card in cards_data:</span><span id="ccea" class="li jy hi le b fi ln lk l ll lm"># get the hotel name<br/>    hotel_name = card.find('p')</span><span id="2a55" class="li jy hi le b fi ln lk l ll lm"># get the room price<br/>    room_price = card.find('li', attrs={'class': 'htl-tile-discount-prc'})<br/>    print(hotel_name.text, room_price.text)</span></pre><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es lt"><img src="../Images/a7ef6bd779ba76ff128b92ff8ef7cdb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gR8QQRs_t245j7Zo"/></div></div></figure><h1 id="3e00" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">步骤3:存储数据</h1><p id="f561" class="pw-post-body-paragraph ii ij hi il b im ky io ip iq kz is it jh la iw ix ji lb ja jb jj lc je jf jg hb bi translated">最后一步是将提取的数据存储在CSV文件中。在这里，对于每张卡，我们将提取酒店名称和价格，并将其存储在一个Python字典中。然后我们将最终把它添加到一个列表中。</p><p id="cf3d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">接下来，让我们将这个列表转换为Pandas数据帧，因为它允许我们将数据帧转换为CSV或JSON文件:</p><pre class="jm jn jo jp fd ld le lf lg aw lh bi"><span id="09d1" class="li jy hi le b fi lj lk l ll lm"># create a list to store the data<br/>scraped_data = []</span><span id="6795" class="li jy hi le b fi ln lk l ll lm">for card in cards_data:</span><span id="0668" class="li jy hi le b fi ln lk l ll lm"># initialize the dictionary<br/>    card_details = {}</span><span id="d2ac" class="li jy hi le b fi ln lk l ll lm"># get the hotel name<br/>    hotel_name = card.find('p')</span><span id="7c89" class="li jy hi le b fi ln lk l ll lm"># get the room price<br/>    room_price = card.find('li', attrs={'class': 'htl-tile-discount-prc'})</span><span id="6979" class="li jy hi le b fi ln lk l ll lm"># add data to the dictionary<br/>    card_details['hotel_name'] = hotel_name.text<br/>    card_details['room_price'] = room_price.text</span><span id="3d34" class="li jy hi le b fi ln lk l ll lm"># append the scraped data to the list<br/>    scraped_data.append(card_details)</span><span id="dad2" class="li jy hi le b fi ln lk l ll lm"># create a data frame from the list of dictionaries<br/>dataFrame = pd.DataFrame.from_dict(scraped_data)</span><span id="500e" class="li jy hi le b fi ln lk l ll lm"># save the scraped data as CSV file<br/>dataFrame.to_csv('hotels_data.csv', index=False)</span></pre><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es lu"><img src="../Images/cbc2730d53cf398bae3d8af751a0a177.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/0*TPNeY79e6UXy_8pR"/></div></figure><p id="8c6a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">恭喜你。我们已经成功地创建了一个基本的网络刮刀。我希望你尝试这些步骤，并尝试获得更多数据，如酒店的评级和地址。现在让我们看看如何执行一些常见的任务，如抓取URL、电子邮件id、图像和抓取页面加载数据。</p><h1 id="6395" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">从网页上抓取URL和电子邮件id</h1><p id="c969" class="pw-post-body-paragraph ii ij hi il b im ky io ip iq kz is it jh la iw ix ji lb ja jb jj lc je jf jg hb bi translated">我们试图抓取的两个最常见的特征是网站URL和电子邮件id。我敢肯定，您曾经参与过需要批量提取电子邮件id的项目或挑战(参见营销团队！).所以我们来看看如何在Python中刮这些方面。</p><h1 id="f1e9" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">使用Chrome扩展电子邮件提取器</h1><ul class=""><li id="2577" class="lv lw hi il b im ky iq kz jh lx ji ly jj lz jg ma mb mc md bi translated">电子邮件提取器是一个Chrome插件，可以捕获我们当前正在浏览的页面上的电子邮件id</li><li id="3a27" class="lv lw hi il b im me iq mf jh mg ji mh jj mi jg ma mb mc md bi translated">它甚至允许我们下载CSV或文本文件形式的电子邮件id列表:</li></ul><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es mj"><img src="../Images/c3f14d9a3ec16a4651335c2f1ae36a8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/0*jVc43tcHtLv-x1yF"/></div></figure><h1 id="43a8" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">使用BeautifulSoup和Regex</h1><p id="394f" class="pw-post-body-paragraph ii ij hi il b im ky io ip iq kz is it jh la iw ix ji lb ja jb jj lc je jf jg hb bi translated">只有当我们想从一个页面中抓取数据时，上述解决方案才是有效的。但是如果我们想在多个网页上做同样的步骤呢？</p><p id="1c4a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">有许多网站可以以一定的价格为我们做到这一点。但是这里有一个好消息——我们也可以使用Python编写我们自己的web scraper！在我的<a class="ae jk" href="https://www.analyticsvidhya.com/blog/2019/10/web-scraping-hands-on-introduction-python/" rel="noopener ugc nofollow" target="_blank">原帖中查看现场编码窗口。</a></p><blockquote class="if ig ih"><p id="184e" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">这是使用强大的<em class="hi"> BeautifulSoup </em>库对Python中的web抓取进行的简单且初学者友好的介绍。老实说，当我在寻找一个新项目或需要现有项目的信息时，我发现网络抓取非常有用。</p></blockquote></div><div class="ab cl mk ml gp mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="hb hc hd he hf"><p id="4ed2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><em class="ik">原载于2019年10月10日</em><a class="ae jk" href="https://www.analyticsvidhya.com/blog/2019/10/web-scraping-hands-on-introduction-python/" rel="noopener ugc nofollow" target="_blank"><em class="ik">https://www.analyticsvidhya.com</em></a><em class="ik">。</em></p></div></div>    
</body>
</html>