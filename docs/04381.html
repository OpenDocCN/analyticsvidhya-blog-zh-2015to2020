<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/abstractive-text-summarization-430901a602c0?source=collection_archive---------8-----------------------#2020-03-17">https://medium.com/analytics-vidhya/abstractive-text-summarization-430901a602c0?source=collection_archive---------8-----------------------#2020-03-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/08b1b3635ba05c30168f0044f6dc11ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5-LPEVWi63BBxPeFFan-Gg.png"/></div></div></figure></div><div class="ab cl hr hs gp ht" role="separator"><span class="hu bw bk hv hw hx"/><span class="hu bw bk hv hw hx"/><span class="hu bw bk hv hw"/></div><div class="hb hc hd he hf"><h1 id="c656" class="hy hz ia bd ib ic id ie if ig ih ii ij ik il im in io ip iq ir is it iu iv iw bi translated">抽象文本摘要</h1><h2 id="f549" class="ix hz ia bd ib iy iz ja if jb jc jd ij je jf jg in jh ji jj ir jk jl jm iv jn bi translated">有时，我们需要给定文档中简洁的信息，而不是太多的细节…</h2><h1 id="454b" class="hy hz ia bd ib ic jo ie if ig jp ii ij ik jq im in io jr iq ir is js iu iv iw bi translated"><strong class="ak">简介:</strong></h1><p id="bf3e" class="pw-post-body-paragraph jt ju ia jv b jw jx jy jz ka kb kc kd je ke kf kg jh kh ki kj jk kk kl km kn hb bi translated">自然语言处理和理解中的一个挑战是文本生成，它可以应用于例如文本摘要。事实上，它包括理解给定文本所传达的信息，并将其缩减为包含主要重要和相关信息的简明摘要。在这种情况下，进行了几项研究，以建立基于频率主义方法的摘要。这些算法的主要思想是建立一个评分系统，为假设重要的句子分配高值，反之亦然。然而，复杂性在于概念化一个贯穿整个文本的模型，将重要的信息保存在它的记忆中，并跳过像冗余、细节这样的噪音…简而言之，模型必须从一个长文本中创建一个上下文向量。</p><h2 id="79f0" class="ix hz ia bd ib iy iz ja if jb jc jd ij je jf jg in jh ji jj ir jk jl jm iv jn bi translated">第一种方法:TFIDF摘要生成器</h2><p id="385b" class="pw-post-body-paragraph jt ju ia jv b jw jx jy jz ka kb kc kd je ke kf kg jh kh ki kj jk kk kl km kn hb bi translated">很明显，一个模型在集中的文本上表现得很好，它只包含简洁的信息和较少的干扰。在这种情况下，我们可以使用一些提取摘要器从给定的文本中提取重要的部分，这些摘要器将作为摘要模型的输入。我选择使用TFIDF vectoriser来减少文本的大小，并根据以下评分系统获得最重要的句子。为了测试该模型，我们从维基百科中选择了一篇关于“人工智能”的文本，以观察该模型的表现，因为它将成为摘要模型的输入。</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ko"><img src="../Images/62b2dfc85393a459d161747fecef101c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cQWDFZ_p9jkhxOr8bcGoDg.png"/></div></div></figure><figure class="kp kq kr ks fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kt"><img src="../Images/532af2576576c8183660b4f401cc1552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dIiNUYJmJjNtB_CV5YRAIg.png"/></div></div></figure><p id="22ba" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">在阅读生成的摘要时，我们可以注意到句子之间没有连贯性，知道它们将被输入到随后构建的抽象摘要模型中..因此，如果我们放任自流，我们就会在模型中引入有偏差的数据。这意味着我们的模型必须从一个不可理解的非同质文本中获取上下文向量。因此，我们会得到有偏见的总结。</p><h1 id="aac5" class="hy hz ia bd ib ic jo ie if ig jp ii ij ik jq im in io jr iq ir is js iu iv iw bi translated">方法的改变:</h1><h1 id="96dd" class="hy hz ia bd ib ic jo ie if ig jp ii ij ik jq im in io jr iq ir is js iu iv iw bi translated">一个人如何生成摘要？</h1><ul class=""><li id="84cd" class="kz la ia jv b jw jx ka kb je lb jh lc jk ld kn le lf lg lh bi translated">例如，如果你被要求写一篇科学文章的摘要，你要做的第一步是阅读全文，然后你将分别总结每一部分。因此，我们可以克服文本大小的问题，并假设我们将以10行为一步来总结文本。</li></ul><h1 id="e00c" class="hy hz ia bd ib ic jo ie if ig jp ii ij ik jq im in io jr iq ir is js iu iv iw bi translated">我的算法如何应用于商业？</h1><ul class=""><li id="86a6" class="kz la ia jv b jw jx ka kb je lb jh lc jk ld kn le lf lg lh bi translated">客户的评论通常很长，而且是描述性的，你无法想象从这些评论中分析和获得要点所耗费的时间和精力。在这种背景下，我们面临着一个自然语言理解的问题。我们如何为那些冗长的评论生成简短的摘要呢？</li></ul><h1 id="6bc1" class="hy hz ia bd ib ic jo ie if ig jp ii ij ik jq im in io jr iq ir is js iu iv iw bi translated">研究案例:亚马逊顾客评论</h1><ul class=""><li id="1f89" class="kz la ia jv b jw jx ka kb je lb jh lc jk ld kn le lf lg lh bi translated">作为我项目的一个案例研究，我选择了亚马逊提供的美食评论。目标是使用主要基于Keras库的抽象摘要方法为亚马逊美食评论生成一个摘要。</li><li id="3a30" class="kz la ia jv b jw li ka lj je lk jh ll jk lm kn le lf lg lh bi translated">项目管道下方:</li></ul><figure class="kp kq kr ks fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ln"><img src="../Images/cb44cb25ea9d20c432b1e88cbe68633c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y2d1nRCJi08k8SzSSR67Ig.png"/></div></div></figure><h1 id="de82" class="hy hz ia bd ib ic jo ie if ig jp ii ij ik jq im in io jr iq ir is js iu iv iw bi translated">使用Keras在Python中实现文本摘要</h1><h2 id="ab1c" class="ix hz ia bd ib iy iz ja if jb jc jd ij je jf jg in jh ji jj ir jk jl jm iv jn bi translated">A-数据准备:</h2><p id="e2b7" class="pw-post-body-paragraph jt ju ia jv b jw jx jy jz ka kb kc kd je ke kf kg jh kh ki kj jk kk kl km kn hb bi translated">如前所述，数据集由亚马逊客户评论组成。它包含大约500000篇评论及其摘要，这需要巨大的训练容量。受此约束的限制，我们将行数固定为100000，假设它将被快速训练。当我们加载数据时，我们把它输入到由几个步骤组成的处理部分。首先，我们删除重复的行和缺失的值，以确保所有的观察都是不同的。然后，我们定义一个包含以下子功能的处理功能:</p><pre class="kp kq kr ks fd lo lp lq lr aw ls bi"><span id="516f" class="ix hz ia lp b fi lt lu l lv lw">&gt; Convert the reviews and summaries to lowercase<br/>&gt; Remove HTML tags<br/>&gt; Contraction mapping: that consists of importing a contraction dictionary for example: didn't becomes did not etc...<br/>&gt; Remove (‘s)<br/>&gt; Remove any text inside the parenthesis ( )<br/>&gt; Eliminate punctuation and special characters<br/>&gt; Eliminate stop words<br/>&gt; Eliminate short words</span></pre><p id="ec02" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">下面是一些客户评论的样本和预处理后的总结。正如您所注意到的，摘要分别以-Start和-END标记开始和结束，作为它们开始和结束的限制。</p><pre class="kp kq kr ks fd lo lp lq lr aw ls bi"><span id="4905" class="ix hz ia lp b fi lt lu l lv lw">Review: bought several vitality canned dog food products found good quality product looks like stew processed meat smells better labrador finicky appreciates product better<br/>Summary: _START_ good quality dog food  _END_<br/><br/><br/>Review: product arrived labeled jumbo salted peanuts peanuts actually small sized unsalted sure error vendor intended represent product jumbo<br/>Summary: _START_ not as advertised  _END_<br/><br/><br/>Review: confection around centuries light pillowy citrus gelatin nuts case filberts cut tiny squares liberally coated powdered sugar tiny mouthful heaven chewy flavorful highly recommend yummy treat familiar story lewis lion witch wardrobe treat seduces edmund selling brother sisters witch<br/>Summary: _START_ delight says it all  _END_<br/><br/><br/>Review: looking secret ingredient robitussin believe found got addition root beer extract ordered made cherry soda flavor medicinal<br/>Summary: _START_ cough medicine  _END_<br/><br/><br/>Review: great taffy great price wide assortment yummy taffy delivery quick taffy lover deal<br/>Summary: _START_ great taffy  _END_</span></pre><h2 id="b5bd" class="ix hz ia bd ib iy iz ja if jb jc jd ij je jf jg in jh ji jj ir jk jl jm iv jn bi translated">b型建模:</h2><h2 id="efdd" class="ix hz ia bd ib iy iz ja if jb jc jd ij je jf jg in jh ji jj ir jk jl jm iv jn bi translated">模型架构:</h2><p id="f70c" class="pw-post-body-paragraph jt ju ia jv b jw jx jy jz ka kb kc kd je ke kf kg jh kh ki kj jk kk kl km kn hb bi translated">清理完数据后，我们继续进行建模部分，包括构建一个采用标记化数据的分析模型。</p><h1 id="9df2" class="hy hz ia bd ib ic jo ie if ig jp ii ij ik jq im in io jr iq ir is js iu iv iw bi translated">一、回顾与总结标记化:</h1><p id="0400" class="pw-post-body-paragraph jt ju ia jv b jw jx jy jz ka kb kc kd je ke kf kg jh kh ki kj jk kk kl km kn hb bi translated">对于文本标记化，我们通过3个函数使用Keras tokenizer:</p><ol class=""><li id="264b" class="kz la ia jv b jw ku ka kv je lx jh ly jk lz kn ma lf lg lh bi translated"><strong class="jv mb"> fit_on_texts: </strong></li></ol><p id="d89f" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">基于文本列表更新内部词汇。该方法基于词频创建词汇索引。所以，如果你给它一个类似“猫坐在垫子上”的东西。它将创建一个字典:word _ index[" the "]= 1；word_index["cat"] = 2。</p><p id="5a23" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">它是单词索引字典，所以每个单词都有一个唯一的整数值。0保留用于填充。整数越小意味着单词越频繁(通常前几个是停用词，因为它们出现得很多)。</p><p id="3e34" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated"><strong class="jv mb"> 2。texts _ to _ sequence:</strong></p><p id="6d70" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">将文本中的每个文本转换为整数序列。所以它基本上是将文本中的每个单词替换为word_index字典中相应的整数值。</p><p id="50b3" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">我们举一个例子:[“地球是一个令人敬畏的地方”，“保护地球是我们共同的目标”]</p><pre class="kp kq kr ks fd lo lp lq lr aw ls bi"><span id="c215" class="ix hz ia lp b fi lt lu l lv lw">sequences :  [[3, 1, 2, 4, 6, 7], [3, 8, 1, 2]] <br/>word_index :  {'earth': 1, 'is': 2, 'the': 3, 'an': 4, 'awesome': 5, 'place': 6, 'live': 7, 'protection': 8, 'our': 9, 'common': 10, 'goal': 11}</span></pre><ul class=""><li id="15e8" class="kz la ia jv b jw ku ka kv je lx jh ly jk lz kn le lf lg lh bi translated">这只是一个显示上面列出的函数输出的例子。正如我们可以注意到的，序列有不同的大小，所以它不能被输入到我们将要概念化的模型中。为了做到这一点，我们使用填充技术，包括添加零，以具有相同的序列长度。</li></ul><p id="5e53" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated"><strong class="jv mb"> 3。pad_sequences函数:将序列填充到相同的长度。</strong></p><p id="cb13" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">这个函数将num_samples序列的列表(整数列表)转换为shape (num_samples，num_timesteps)的2D Numpy数组。num_timesteps或者是最大长度参数(如果提供),或者是最长序列的长度。</p><ul class=""><li id="8a83" class="kz la ia jv b jw ku ka kv je lx jh ly jk lz kn le lf lg lh bi translated">短于num_timesteps的序列在末尾用值填充。</li><li id="d87b" class="kz la ia jv b jw li ka lj je lk jh ll jk lm kn le lf lg lh bi translated">长度超过num_timesteps的序列会被截断，以便符合所需的长度。</li></ul><p id="de83" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">填充或截断发生的位置分别由参数padding和truncating确定。</p><h1 id="d78c" class="hy hz ia bd ib ic jo ie if ig jp ii ij ik jq im in io jr iq ir is js iu iv iw bi translated">二。Seq2Seq:序列到序列模型</h1><ol class=""><li id="cff9" class="kz la ia jv b jw jx ka kb je lb jh lc jk ld kn ma lf lg lh bi translated"><strong class="jv mb">理论方法</strong></li></ol><p id="b4ce" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">我们将在之前预处理的评论和摘要上实现seq2seq模型。Seq2seq是一个深度学习模型，将输入序列转换成输出序列。我们的输入和输出分别用X和Y表示；x指的是客户评论，Y指的是总结。</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mc"><img src="../Images/9b3431646304361fe8143ceba779c35e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zDNHflkGLcealHoJnOavCA.png"/></div></div></figure><p id="437d" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">seq2seq的目的是在给定输入序列X的情况下，理解并模拟生成输出Y的概率。由于该模型一个字一个字地生成，所以条件概率建模可以被视为当给定属于摘要的一组字和输入时生成第j个字的概率；</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es md"><img src="../Images/a80af9266402577b175b56062b0f59a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*-EBgtBkUN38RK8140tVqow.png"/></div></figure><p id="e533" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">seq2seq包括2个处理步骤:</p><p id="ef5c" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">a-从给定的输入序列X中生成固定大小的向量Z:Z = f(X)</p><ul class=""><li id="0781" class="kz la ia jv b jw ku ka kv je lx jh ly jk lz kn le lf lg lh bi translated">f可以是任何递归神经网络，如RNN，LSTM…</li></ul><p id="5ff9" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">b-从固定大小的向量z生成摘要输出。</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es me"><img src="../Images/6efd10f017711f4a8d0f92453e0696dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*j31EQmRjbPJ5zLFbU_oT2w.png"/></div></figure><p id="3fdc" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">k:生成隐藏向量hj的函数</p><p id="f8cf" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">g:计算证明上述递归方面的独热向量yj的生成概率的函数。</p><p id="eaa4" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated"><strong class="jv mb"> 2。模型架构:</strong></p><p id="bfd8" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">seq2seq的总体架构由两个主要部分组成:编码器和解码器。它们中的每一个都是由给定顺序的层的组合构成的。</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mf"><img src="../Images/88e9bb03d113db4628c3589ac5ffbe5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZbrUnhV6SQpq8nRlRoLbjQ.png"/></div></div></figure><p id="a8ae" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">编码器由两层组成:嵌入层和重现层，解码器由三层组成:嵌入层、重现层和输出层。</p><h2 id="13bd" class="ix hz ia bd ib iy iz ja if jb jc jd ij je jf jg in jh ji jj ir jk jl jm iv jn bi translated">编码器嵌入层:</h2><ul class=""><li id="d1ab" class="kz la ia jv b jw jx ka kb je lb jh lc jk ld kn le lf lg lh bi translated">嵌入层将输入序列的每个单词转换成嵌入向量。</li></ul><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es mg"><img src="../Images/e2ba7fd1b0f86052bf699286046c1d32.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*XQdt86QTV4Uk04PAz2SmCA.png"/></div></figure><h2 id="46cf" class="ix hz ia bd ib iy iz ja if jb jc jd ij je jf jg in jh ji jj ir jk jl jm iv jn bi translated">编码器重现层:</h2><p id="cefd" class="pw-post-body-paragraph jt ju ia jv b jw jx jy jz ka kb kc kd je ke kf kg jh kh ki kj jk kk kl km kn hb bi translated">编码器递归层从嵌入向量中生成隐藏状态<em class="mh">hi</em>: F是指通常采用非线性的激活函数(sigmoid，tanh…)</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/6911bb62e4bb477cced3a0ebff062ef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*ieKimsKQR_L6qtq812Bnrw.png"/></div></figure><h2 id="3d59" class="ix hz ia bd ib iy iz ja if jb jc jd ij je jf jg in jh ji jj ir jk jl jm iv jn bi translated">解码器嵌入层:</h2><p id="c932" class="pw-post-body-paragraph jt ju ia jv b jw jx jy jz ka kb kc kd je ke kf kg jh kh ki kj jk kk kl km kn hb bi translated">解码器递归层将输出序列中的每个单词转换成嵌入向量</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es mj"><img src="../Images/2478fe7756b9845514ed4da003cd66f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*N5sRN08lrOPPh1SdOC01FA.png"/></div></figure><h2 id="c880" class="ix hz ia bd ib iy iz ja if jb jc jd ij je jf jg in jh ji jj ir jk jl jm iv jn bi translated">解码器重现层:</h2><p id="05bd" class="pw-post-body-paragraph jt ju ia jv b jw jx jy jz ka kb kc kd je ke kf kg jh kh ki kj jk kk kl km kn hb bi translated">解码器递归层从嵌入向量中生成隐藏向量。</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/af250c71aac1221f255d67eafb12e8d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*RoZPWFuUz6hPVANp0AM8XA.png"/></div></figure><h2 id="04ed" class="ix hz ia bd ib iy iz ja if jb jc jd ij je jf jg in jh ji jj ir jk jl jm iv jn bi translated">解码器输出层:</h2><p id="335c" class="pw-post-body-paragraph jt ju ia jv b jw jx jy jz ka kb kc kd je ke kf kg jh kh ki kj jk kk kl km kn hb bi translated">解码器输出层从隐藏向量生成输出句子的第j个单词的概率。</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mk"><img src="../Images/f3960b3b4e83fd54356da56a34b411fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eopH6cn-MSzU2sOWsZI4ag.png"/></div></div></figure><h2 id="ec19" class="ix hz ia bd ib iy iz ja if jb jc jd ij je jf jg in jh ji jj ir jk jl jm iv jn bi translated">注意机制:Bahdanau注意</h2><p id="ee19" class="pw-post-body-paragraph jt ju ia jv b jw jx jy jz ka kb kc kd je ke kf kg jh kh ki kj jk kk kl km kn hb bi translated">在将输入序列输入编码器部分后，模型必须识别文本的所有重要部分，并跳过所有有噪声的信息，包括冗余信息。</p><p id="ee65" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">因此，“注意”是编码器和解码器之间的接口，向解码器提供来自每个编码器隐藏状态的信息。通过这种设置，模型能够选择性地关注输入序列的有用部分，并因此学习它们之间的对齐。这有助于模型有效地处理长输入句子。</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es ml"><img src="../Images/cce5d9529f780167dc4a82ee41b8e08a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*bdVeq1K9sguGr51iKN5TEQ.png"/></div></figure><p id="787b" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">编码器照常工作，区别仅在于解码器部分。解码器的隐藏状态是用上下文向量、先前的输出和先前的隐藏状态来计算的。但是现在我们不是使用单个上下文向量c，而是为每个目标单词使用单独的上下文向量c_i。</p><p id="f0df" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated"><strong class="jv mb"> RMSPROP优化器:</strong></p><p id="0ecd" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">出于这些原因，为了优化seq2seq模型的成本函数，我们选择了RMSPROP优化器</p><ul class=""><li id="d299" class="kz la ia jv b jw ku ka kv je lx jh ly jk lz kn le lf lg lh bi translated">它试图调整学习率，并自动完成。</li><li id="673f" class="kz la ia jv b jw li ka lj je lk jh ll jk lm kn le lf lg lh bi translated">它为每个参数选择不同的学习速率；在我们的例子中是层权重。</li></ul><p id="fc48" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">重量的这种更新单独如下进行:</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es mm"><img src="../Images/2e16eccffbc0483ee9fb955ac8788eaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*3wQHf9JTqCSGWMESfdS4KA.png"/></div></figure><p id="16fc" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">其中:</p><p id="7232" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">𝜂:初始学习率</p><p id="6a69" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">梯度平方的𝑣𝑡:指数平均</p><p id="c7a1" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">沿𝑤𝑗在时间t的𝑔𝑡:梯度</p><p id="bde9" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">我们在100000篇评论及其摘要上训练seq2seq模型。在编码器部分，我们使用了3个堆叠的LSTM，然后是注意力层。我们在使用早期停止技术预处理的数据上拟合模型，以避免过度拟合。在每个时期之后，如果模型没有改进，我们停止转动模型。</p><p id="44a1" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">训练阶段包括设置层的权重，这允许我们通过推理对看不见的评论做出准确的预测。下面是我们对新客户评论的预测总结:</p><pre class="kp kq kr ks fd lo lp lq lr aw ls bi"><span id="e235" class="ix hz ia lp b fi lt lu l lv lw">Review: ordered salmon thursday january received january salmon delicious wooden box nice design used store items future <br/>Original summary: alaska smokehouse smoked salmon <br/>Predicted summary:  great deal<br/><br/><br/>Review: drank cold could pleased coffee high quality arabica always notice coffee arabica robusta sweet without sweet mean cannot compare major bottled coffee brand reason alone keep fridge work offer people without looking like pushing calories along caffeine long time would stock highly sweetened coffee beverages fridge work noticed people stopped consuming happened around time lost interest level sweetness recommend drink prefer lower level sugar old school arabica coffee types <br/>Original summary: it was perfect little sweet without being too sweet <br/>Predicted summary:  great coffee<br/><br/><br/>Review: variety granola one ranks top favorites crunch granola coconut main taste sweet overly eat snack craving something fills fast satisfies sugar cravings <br/>Original summary: great taste <br/>Predicted summary:  yummy<br/><br/><br/>Review: bought dollar tree yummy taste like french fries crunchy going buy bulk pass kid test <br/>Original summary: yum <br/>Predicted summary:  great snack<br/><br/><br/>Review: used eating flaxseed brownie hodgson mill brownies super easy make taste great since like dark chocolate usually add little cocoa <br/>Original summary: delicious brownie <br/>Predicted summary:  great gluten free bread<br/><br/><br/>Review: picked local grocery store organic brown rice pleasantly surprised cooks really well sticky tastes great typically brown rice lover kind really good kids husband ate asked seconds well delighted see amazon great price subscribe save <br/>Original summary: love this rice <br/>Predicted summary:  great rice<br/><br/><br/>Review: stuff oily probably could run car takes cup creamer make change black tan ick <br/>Original summary: yuk <br/>Predicted summary:  not so good<br/><br/><br/>Review: good concentrate product packaging recyclable good taste rather unripe taste liking <br/>Original summary: good not so good <br/>Predicted summary:  not bad<br/><br/><br/>Review: cream instead compare office coffee cafe cube farm definitely matches taste quality easy quick mellow great idea soul wants cup coffee cannot bear brew one two lonely cups big disadvantage instant coffee far concerned serving product abut caffeine compared caffeine brewed cup coffee want caffeine flavored versions product even less caffeine per serving although could tell disadvantage suppose whole house fill sound fragrance brewing coffee choose instant alas still given convenience decency beverage buying little single sticks pretty regularly recommended <br/>Original summary: pleasant beverage convenient format <br/>Predicted summary:  good coffee<br/><br/><br/>Review: price right comparison local arts store steal color dark picture pastel red set pink anything reccomended measurement usage delivered bubble wrap package <br/>Original summary: red or pink <br/>Predicted summary:  great candy<br/><br/><br/>Review: crumby grapes sweet enough wine good excuse press bad news things rediscovered many busy helping image culinary california australia everybody getting act thin flabby faux hitting shelves cute backbone courage heavy lifting kitchen might well pour lemonade grilled salmon acidic glory taste alone compare anything might run across vineyards make veritable next try gently fresh really ripe fruit loves swirled beurre blanc whip best cream roasting pan old time gravy nutmeg nice five pepper blend need steamed vegetables tell truth <br/>Original summary: the truth in <br/>Predicted summary:  fun to use in usa<br/><br/><br/>Review: huge tea fan one favorites think taste subtle honey compliments well next time add lemon slices maybe orange slice yummy <br/>Original summary: very nice tea <br/>Predicted summary:  very good<br/><br/><br/>Review: price excellent heard many good things kind bars mango macadamia bar taste great would willing try flavors would buy flavor <br/>Original summary: they did not taste that great <br/>Predicted summary:  good but not great<br/><br/><br/>Review: used mcp pectin probably years making freezer jam jam always sets looks tastes perfect hard time finding mcp recently forced try another product making jam last summer jam set unhappy result thrilled find mcp amazon supply last couple years convinced mcp best wish could buy locally grocery store <br/>Original summary: mcp is the best <br/>Predicted summary:  the best<br/><br/><br/>Review: little guy ate could even write review <br/>Original summary: great deal <br/>Predicted summary:  not my favorite<br/><br/><br/>Review: like cashews way biased really enjoyed earn name salty though product need eat low sodium rest tasty treat <br/>Original summary: tasty treat <br/>Predicted summary:  good but salty<br/><br/><br/>Review: younger prone trying whatever new product came along various benefits seeking seldom ever payoff see advertisement product promising bigger better stronger even consider imagine surprise tried new shampoo actually noticed volume hair blow drying even two people asked would hair cut day used seriously miraculous change shampoo weave definitely makes noticeable difference negative comment fairly strong perfume scent tend avoid soaps shampoos absolutely continue use <br/>Original summary: unbelievable literally <br/>Predicted summary:  good but not the best<br/><br/><br/>Review: item allows convenience safe place emergency funds hides well pantry know whenever need <br/>Original summary: safe safe <br/>Predicted summary:  good for the price<br/><br/><br/>Review: mostly enjoyed paul newman donut shop ones one day saw count twenty bucks thought hmmm good deal must awful heck try best coffee ever tasted never see searched online hot simply poured ice hands favorite cannot drink constantly recommending people agree hooked also great environmentally packaged bet lack plastic makes taste better cannot lose drink two three cups day sip good first fact going make cup right san francisco bay coffee organic one cup keurig cup brewers rainforest blend count <br/>Original summary: best coffee ever <br/>Predicted summary:  best tasting coffee have ever had<br/><br/><br/>Review: love product fact become addictive time however noticed hard pieces chewing nuts examining closely think somehow shells got nuts shelled made careful bitterness otherwise wonderful flavor also want crack chip tooth hope isolated instance batch <br/>Original summary: emerald dry roasted walnuts review <br/>Predicted summary:  not so good<br/><br/><br/>Review: reading reviews decided purchase husband husband drinks tea every day prefers stronger blends like one used drink tips tried barry tea prefers barry looking nice strong basic tea look <br/>Original summary: my husband favorite everyday tea <br/>Predicted summary:  great tasting green tea<br/><br/><br/>Review: rich man peanut butter cap crunch poor man honey nut cheerios tastes like former looks like latter wife dug one evening hours later demolished peanut except artificial peanut flavor looking sugar brand names satisfy craving nom nom <br/>Original summary: sugary nut crunch <br/>Predicted summary:  not so good<br/><br/><br/>Review: eat gluten free tried almost every type bar available pleasantly surprised got try bar moist delicious tasted like blueberry lemon cookie big bursts blueberries cannot wait try flavors thanks pamela making another great tasting product <br/>Original summary: thank goodness <br/>Predicted summary:  delicious</span></pre><p id="2295" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">即使最初的总结和预测的总结并不一致，但两者都传达了相同的意思，它们让我们对客户对产品的判断有了一个大致的了解。然而，我们需要量化原始概要和预测概要之间的接近程度。</p><h2 id="5a73" class="ix hz ia bd ib iy iz ja if jb jc jd ij je jf jg in jh ji jj ir jk jl jm iv jn bi translated">模型评估:。语义相似性评估:</h2><p id="0fcd" class="pw-post-body-paragraph jt ju ia jv b jw jx jy jz ka kb kc kd je ke kf kg jh kh ki kj jk kk kl km kn hb bi translated">在这一部分，我们将使用语义相似性方法来评估我们预测的质量，这意味着我们将使用预训练的word2vec模型来量化预测和原始摘要之间的意义相似性。</p><ul class=""><li id="1604" class="kz la ia jv b jw ku ka kv je lx jh ly jk lz kn le lf lg lh bi translated">在这种情况下，我们使用在Google News上训练的预训练单词嵌入模型“Word2vec”来获得每个摘要的单词在原始和预测中的矢量表示。因此，我们面临一个相似度计算的问题，呈现如下:</li></ul><p id="a7a2" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">假设我们想计算这两个句子之间的相似度:</p><p id="5405" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">S1 = '奥巴马对媒体讲话伊利诺伊州'和S2 = '总统问候媒体芝加哥'</p><p id="9b6f" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">的确，这些句子用词不同，但也许它们传达了相同的信息。在这个阶段，每个句子都被表示为一个矩阵，但是出现的问题是，如何在属于每个句子的相似单词之间进行匹配。例如，你怎么能把奥巴马和总统联系起来呢？(下图展示了维斯)</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es mn"><img src="../Images/1abac793d0758a3091e016a002ea4a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*YLPy76H9IrmIXuD9ygi68g.png"/></div></figure><p id="e5e2" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">因此，我们使用字移动距离优化算法的灵感来自通常的优化问题推土机的距离。它允许将每个单词从句子1转移到句子2，因为算法不知道“奥巴马”应该转移到“总统”。最后，它会选择最小的运输成本将每个单词从句子1运输到句子2。</p><h2 id="85ea" class="ix hz ia bd ib iy iz ja if jb jc jd ij je jf jg in jh ji jj ir jk jl jm iv jn bi translated">- WMD算法:</h2><p id="9003" class="pw-post-body-paragraph jt ju ia jv b jw jx jy jz ka kb kc kd je ke kf kg jh kh ki kj jk kk kl km kn hb bi translated">我们假设我们有n个单词的词汇表{1，.。。，n}和文档集合。我们假设d和d’的不同单词集分别是{𝑤1,…,𝑤|𝐷|}和{𝑤1,…,𝑤|𝐷′|}.此外，我们使用𝐷𝑖来表示𝑤𝑖的归一化频率，即d中𝑤𝑖在d中的总字数中的出现次数。我们使用𝐷′𝑗来类似地表示𝑤′𝑗的归一化频率。请注意:</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es mo"><img src="../Images/b10affa062c25b4a2f0cd9fdd6a32c64.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*5XjwmK-XHlMVidelG13oAg.png"/></div></figure><p id="42cb" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">𝑥𝑖表示单词I在d维向量空间中的嵌入，c(i，j)表示单词I和j的嵌入之间的欧几里德距离，也就是说，</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es mp"><img src="../Images/fbee25abb2f0767cc66ef40d8b494e04.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*3dYRc7d2gfJftxf4Vz1jqQ.png"/></div></figure><ul class=""><li id="39fa" class="kz la ia jv b jw ku ka kv je lx jh ly jk lz kn le lf lg lh bi translated">约翰喜欢算法。玛丽也喜欢算法</li><li id="1a01" class="kz la ia jv b jw li ka lj je lk jh ll jk lm kn le lf lg lh bi translated">d):约翰也喜欢数据结构。</li></ul><p id="46c9" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">我们的词汇:V = {1:'约翰'，2:'喜欢'，3:'算法'，4:'玛丽'，5:'太'，6:'也'，7:'数据，8:'结构' }</p><p id="647c" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">D和D’的不同单词组是:</p><pre class="kp kq kr ks fd lo lp lq lr aw ls bi"><span id="2019" class="ix hz ia lp b fi lt lu l lv lw">D = {1:'John', 2:'likes', 3:'algorithms', 4:'Mary', 5:'too'}<br/>D'= {1:'John', 6:'also', 2:'likes', 7:'data, 8:'structures'</span></pre><p id="32c4" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">因此，规范化文档表示为:</p><pre class="kp kq kr ks fd lo lp lq lr aw ls bi"><span id="a5a4" class="ix hz ia lp b fi lt lu l lv lw">D = {1/7,2/7,2/7,1/7,1/7}<br/>D'= {1/5,1/5,1/5,1/5,1/5}</span></pre><h2 id="c87c" class="ix hz ia bd ib iy iz ja if jb jc jd ij je jf jg in jh ji jj ir jk jl jm iv jn bi translated">-问题表述:</h2><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es mq"><img src="../Images/5827735a599fc34557905bf3bf5f76e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*MIAwaCPE_A1nBKo3pwmWPg.png"/></div></figure><p id="d4a4" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">其中:c(i，j)将字I运送到字j的成本。</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es mr"><img src="../Images/b7061d185ac536d85f4658d085729ea1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ojKxtFC1pNEaIelBSdwRhQ.png"/></div></figure><p id="f8b1" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">在加载预训练的向量并解决优化问题之后，最终给出每个预测的和原始的摘要及其评论之间的语义相似度。</p><p id="a192" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">评论与其原始和预测摘要之间的语义相似度的变化分别以蓝色和橙色显示。因此，我们可以注意到这两个量变化相同。</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es ms"><img src="../Images/f7091be3e0b28526df8712e71ce1acb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*UNvHEhDveiFFIukG8AMLoA.png"/></div></figure><p id="cc52" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">如果我们对评论及其摘要之间的语义相似性感兴趣；原创&amp;预测。我们可以在下面的方框图中看到，这两个量的分布几乎相同。事实上，50%的预测摘要与它们的评论有52%的语义相似度。另一方面，假设50%的原始摘要在含义上与他们自己的评论有48%相似。</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es mt"><img src="../Images/f9048345b48d057172fa7212da9f7e8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*G1agbDNFtRp52YY4MSNMPQ.png"/></div></figure><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es mu"><img src="../Images/e2e54359d4349bd6638bdbada3da4f39.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*TScl7hdhxsjCECCzqkMNUg.png"/></div></figure><figure class="kp kq kr ks fd hk er es paragraph-image"><div class="er es mv"><img src="../Images/e3d5cc12e8e7cecfacac8310fc9c0dc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*isjT0AlRkwmJEDjqFNeWXg.png"/></div></figure><h1 id="af4b" class="hy hz ia bd ib ic jo ie if ig jp ii ij ik jq im in io jr iq ir is js iu iv iw bi translated">结论:</h1><p id="1076" class="pw-post-body-paragraph jt ju ia jv b jw jx jy jz ka kb kc kd je ke kf kg jh kh ki kj jk kk kl km kn hb bi translated">seq2seq模型生成的摘要和上面列出的所有技术包含了客户正在查看的产品和他的判断，用一个简洁的短句表达出来。事实上，该模型能够遍历整个文本，保留重要信息，特别是产品和判断，并使用递归神经网络(如LSTMs)跳过所有有噪声的细节。然而，有时模型会产生有偏差的总结，特别是当客户开始将产品与其他具有相同功能的产品或替代品进行比较时。在这种情况下，模型在检测客户正在查看的主要产品和判断时会出错。因此，我们可以简单地通过在更多数据上训练模型来克服这个问题，并质疑已经做出的假设，尤其是在数据预处理步骤中。</p><p id="5ca2" class="pw-post-body-paragraph jt ju ia jv b jw ku jy jz ka kv kc kd je kw kf kg jh kx ki kj jk ky kl km kn hb bi translated">该项目的python实现可在以下Github存储库中获得:<a class="ae mw" href="https://github.com/Fatima-EzzahraFettah/AbstractivesSummarizationSeq2seq" rel="noopener ugc nofollow" target="_blank">https://Github . com/Fatima-EzzahraFettah/abstractivessummarizationseq 2 seq</a></p></div></div>    
</body>
</html>