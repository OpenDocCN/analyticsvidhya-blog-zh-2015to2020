# å¦‚ä½•ä½¿ç”¨ Pythonã€BeautyfulSoup å’Œ Selenium æ„å»ºå•è¯äº‘æ¥æ„å»ºæ–°é—»ç½‘ç«™

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/how-to-scrape-a-news-website-using-python-beautyfulsoup-and-selenium-to-build-the-word-cloud-3355066b72dc?source=collection_archive---------13----------------------->

## é€šè¿‡ä½¿ç”¨è¯äº‘å®ç°æ›´å¥½çš„å¯è§†åŒ–

![](img/cbf0f04ba3d902e324244702922cf234.png)

ä» 2020 å¹´åˆåˆ°ç°åœ¨ï¼Œé¦™æ¸¯æŠ¥çº¸ä¸Šæœ€å¸¸å‡ºç°çš„æ–°é—»æ˜¯ä»€ä¹ˆï¼Ÿé¦™æ¸¯ç¤¾ä¼šè¿åŠ¨ï¼Œ2019 å† çŠ¶ç—…æ¯’è¿˜æ˜¯ä¸­ç¾è´¸æ˜“æˆ˜ï¼Ÿæˆ‘åœ¨é¦™æ¸¯åˆ®äº†ä¸€ä»½çŸ¥åæŠ¥çº¸ï¼Œå¸Œæœ›ç”¨ä¸€ä¸ªå¥½çš„å¯è§†åŒ–æ–¹æ³•~ **å­—äº‘**å¾—åˆ°æˆ‘æƒ³è¦çš„ç­”æ¡ˆã€‚

**ä»€ä¹ˆæ˜¯ç½‘é¡µæŠ“å–ï¼Ÿ**

ç½‘ç»œæŠ“å–å·¥å…·ä¸“é—¨ç”¨äºä»ç½‘ç«™ä¸Šæå–ä¿¡æ¯ã€‚å®ƒä»¬ä¹Ÿè¢«ç§°ä¸ºç½‘ç»œæ”¶é›†å·¥å…·æˆ–ç½‘ç»œæ•°æ®æå–å·¥å…·ã€‚

**ä¸ºä»€ä¹ˆè¦åšç½‘é¡µæŠ“å–ï¼Ÿ**

ç½‘ç»œæŠ“å–å·¥å…·å¯ä»¥åœ¨å„ç§æƒ…å†µä¸‹ç”¨äºå„ç§ç›®çš„ã€‚ä¾‹å¦‚:

1.  æ”¶é›†å¸‚åœºç ”ç©¶æ•°æ®
2.  æ”¶é›†è‚¡ç¥¨å¸‚åœºä¿¡æ¯
3.  æ”¶é›†è”ç³»ä¿¡æ¯
4.  æ”¶é›†æ•°æ®ä»¥ä¸‹è½½ä¾›ç¦»çº¿é˜…è¯»æˆ–å­˜å‚¨
5.  è·Ÿè¸ªå¤šä¸ªå¸‚åœºçš„ä»·æ ¼ç­‰ã€‚

**å¦‚ä½•åœ¨ Python ä¸­åˆ®ï¼Ÿ**

ç”¨ Python æŠ“å–ä¸€ä¸ªç½‘ç«™éå¸¸å®¹æ˜“ï¼Œå°¤å…¶æ˜¯åœ¨ BeautifulSoup å’Œ Selenium åº“çš„å¸®åŠ©ä¸‹ã€‚Beautiful Soup æ˜¯ä¸€ä¸ª Python åº“æ¨¡å—ï¼Œå…è®¸å¼€å‘äººå‘˜é€šè¿‡ç¼–å†™å°‘é‡ä»£ç ï¼Œå¿«é€Ÿè§£æç½‘é¡µ HTML ä»£ç å¹¶ä»ä¸­æå–æœ‰ç”¨çš„æ•°æ®ï¼Œå‡å°‘å¼€å‘æ—¶é—´ï¼ŒåŠ å¿«ç½‘é¡µæŠ“å–çš„ç¼–ç¨‹é€Ÿåº¦ã€‚Selenium æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æµ‹è¯•ç½‘é¡µçš„å·¥å…·ï¼Œå¯ä»¥é€šè¿‡å®ƒæä¾›çš„ä¸€äº›æ–¹æ³•è‡ªåŠ¨æ“ä½œæµè§ˆå™¨ï¼Œå¯ä»¥å®Œå…¨æ¨¡æ‹ŸçœŸäººçš„æ“ä½œã€‚

åœ¨æŠ“å–ä¸€ä¸ªç½‘ç«™ä¹‹å‰ï¼Œå¿…é¡»åšä¸€äº›å‡†å¤‡å·¥ä½œï¼Œåœ¨ Jupiter ç¬”è®°æœ¬ä¸­è¾“å…¥å‘½ä»¤ï¼Œå®ƒå°†å®‰è£…ä»¥ä¸‹åº“ã€‚

```
!pip install BeautifulSoup4
!pip install selenium
```

**ä»€ä¹ˆæ˜¯è¯äº‘ï¼Œæˆ‘ä»¬ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ**

Word Cloud æ˜¯ Python ä¸­éå¸¸å¥½çš„ç¬¬ä¸‰æ–¹ word cloud å¯è§†åŒ–åº“ã€‚å•è¯äº‘æ˜¯åœ¨æ–‡æœ¬ä¸­æ›´é¢‘ç¹å‡ºç°çš„å…³é”®è¯çš„å¯è§†åŒ–æ˜¾ç¤ºã€‚WordCloud ä¼šè¿‡æ»¤æ‰å¤§é‡ä½é¢‘ä½è´¨é‡çš„æ–‡å­—ä¿¡æ¯ï¼Œè®©å—ä¼—ä¸€çœ¼å°±èƒ½æ˜ç™½æ–‡å­—çš„ä¸»æ—¨ã€‚

**ä»€ä¹ˆæ˜¯åˆ†è¯ï¼Ÿ**

åˆ†è¯æ˜¯å°†è¿ç»­çš„è¯åºåˆ—æŒ‰ç…§ä¸€å®šçš„è§„èŒƒé‡æ–°ç»„åˆæˆè¯åºåˆ—çš„è¿‡ç¨‹ã€‚Jieba æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¥½çš„ä¸­æ–‡åˆ†è¯åº“ï¼Œå› ä¸ºä¸­æ–‡é€šå¸¸åŒ…å«æ•´ä¸ªå¥å­ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦ä½¿ç”¨ Jieba æ¥è¾…åŠ©åˆ†è¯çš„å·¥ä½œã€‚

![](img/2218b1e03ecc10a326d8ae50b5cb7f1e.png)

åœ¨ç”Ÿæˆ word äº‘æ˜ åƒä¹‹å‰ï¼Œå¿…é¡»åšä¸€äº›å‡†å¤‡å·¥ä½œï¼Œåœ¨ Jupiter ç¬”è®°æœ¬ä¸­è¾“å…¥å‘½ä»¤ï¼Œå®ƒå°†å®‰è£…ä»¥ä¸‹åº“ã€‚

```
!pip install wordcloud
!pip install jieba
```

**#åŠ è½½åº“**

```
#! /usr/bin/env python
# -*- encoding UTF-8 -*-import os
import sys
from importlib import reload
reload(sys)if sys.version[0] == '2':
    sys.setdefaultencoding("utf-8")

    import parse
    import urllib2
else:
    import urllib.parse
    from urllib.request import urlopenimport re
import jiebaimport pandas as pd
import numpy as np#from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver import ChromeOptionsfrom wordcloud import WordCloud, STOPWORDS, ImageColorGeneratorfrom PIL import Imageimport matplotlib.pyplot as plt
%matplotlib inlinejieba.enable_paddle()
```

**#æ¸…ç† CSSã€JavaScript å’Œ HTML æ ‡ç­¾**

```
def cleanHTML(html):
    for script in html(["script", "style"]): # remove all javascript and stylesheet code
        script.extract()
    # get text
    text = html.get_text()
    # break into lines and remove leading and trailing space on each
    lines = (line.strip() for line in text.splitlines())
    # break multi-headlines into a line each
    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
    # drop blank lines
    text = '\n'.join(chunk for chunk in chunks if chunk)

    return text
```

**#æŸ¥æ‰¾å½“æ—¥ä¸»è¦ç„¦ç‚¹æ–°é—»é“¾æ¥**

```
def getNewsLink(news_date):
    try:
        options = ChromeOptions()
        options.add_argument('headless')
        driver = webdriver.Chrome(options=options) url = '[https://orientaldaily.on.cc/cnt/news/'](https://orientaldaily.on.cc/cnt/news/') + str(news_date) + '/mobile/index.html' driver.get(url)
        driver.implicitly_wait(30) html_source = (driver.page_source.encode('utf-8')) driver.quit() soup = BeautifulSoup(html_source, 'html.parser')
        news = soup.find('div', attrs={'id':'swipe'})
        main_focus = soup.find('div', attrs={'class':'main-focus-container'})
        main_focus_link = '[https://orientaldaily.on.cc'](https://orientaldaily.on.cc') + main_focus.find('a', href = re.compile(r'[/]([a-z]|[A-Z])\w+')).attrs['href'] return (main_focus_link)
    except:
        return 0
```

**#æ•°æ®æ”¶é›†**

```
def getNews(news_url):
    options = ChromeOptions()
    options.add_argument('headless')
    driver = webdriver.Chrome(options=options)

    driver.get(news_url)
    driver.implicitly_wait(30)

    html_source = (driver.page_source.encode('utf-8'))

    driver.quit()

    soup = BeautifulSoup(html_source, 'html.parser')
    paragraph = soup.find_all('div', attrs={'class':'paragraph'})
    paragraph_list = []

    for sub_paragraph in paragraph:
        clean_sub_paragraph = cleanHTML(sub_paragraph)
        paragraph_list.append(clean_sub_paragraph)

    full_paragraph_list = [e for e in paragraph_list if e]

    if (len(full_paragraph_list) > 0):
        f=open("news.txt", "a+")

        for i in range(len(full_paragraph_list)):

            for line in full_paragraph_list[i].splitlines():
                clean_paragraph = cleanText(line)

                f.write(clean_paragraph)

        f.write('\n\n')
        f.close()
```

**#ç”»å­—äº‘**

```
def draw_word_cloud(text, images_name, plt_title):
    images_path = images_name
    images = Image.open(images_path)

    #create a write mask 
    images_mask = Image.new("RGB", images.size, (255,255,255))
    images_mask.paste(images, images)
    images_mask = np.array(images_mask)

    color = ImageColorGenerator(images_mask)

    #Chinese need to use another font, download it from [https://www.freechinesefont.com/](https://www.freechinesefont.com/)
    font_path = 'HanyiSentyCandy.ttf'

    #create wordcloud ~ 
    wc = WordCloud(font_path=font_path, max_font_size=250, max_words=1000, mask=images_mask, \
                   margin=5, background_color="black").generate_from_text(text)
    wc.recolor(color_func = color, random_state = 7)

    #Save the image
    wc.to_file("news.png")

    plt.rcParams["figure.figsize"] = (16, 12)

    plt.title(plt_title)
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.show()
```

**#ä¸»é€»è¾‘:**

```
if __name__ == "__main__":
    start_date = input("Enter the start date (yyyymmdd): ")
    end_date = input("Enter the end date (yyyymmdd): ")

    if ((start_date != "") and (end_date != "")):
        daterange = pd.date_range(start_date, end_date)

        #Download the main focus news
        for news_date in daterange:
            print ('Downloading ' + str(news_date) + ' news...')
            single_news_date = dateConvert(news_date)
            getNews(getNewsLink(single_news_date))

        source_text = open('news.txt', 'r',encoding= 'UTF-8').read()
        tokens = ' '.join(jieba.cut_for_search(source_text))

        title = str(start_date) + ' - ' + str(end_date) + ' Main Focus News on on.cc'

        draw_word_cloud(tokens, 'hongkongpng.png', title)
```

**#æœªæ¥æ”¹å–„**

1.  æ·»åŠ åœç”¨è¯çš„å¤„ç†
2.  ä½¿ç”¨ä¸åŒçš„åˆ†è¯åº“ï¼Œå¦‚ thulacã€FoolNLTKã€HanLPã€nlpir å’Œ ltpã€‚

æ„Ÿè°¢é˜…è¯»ï¼å¦‚æœä½ å–œæ¬¢è¿™ç¯‡æ–‡ç« ï¼Œè¯·é€šè¿‡é¼“æŒæ¥æ„Ÿè°¢ä½ çš„æ”¯æŒ(ğŸ‘ğŸ¼)æŒ‰é’®ï¼Œæˆ–è€…é€šè¿‡å…±äº«è¿™ç¯‡æ–‡ç« è®©å…¶ä»–äººå¯ä»¥æ‰¾åˆ°å®ƒã€‚

æœ€åå¸Œæœ›ä½ èƒ½å­¦ä¼šåˆ®ç—§çš„æŠ€å·§ã€‚ä½ ä¹Ÿå¯ä»¥åœ¨ [GitHub](https://github.com/kindersham/100DaysDS/tree/master/NewsWordCloud) åº“ä¸Šæ‰¾åˆ°å®Œæ•´çš„é¡¹ç›®ã€‚

**å‚è€ƒæ–‡çŒ®**

[](/the-artificial-impostor/nlp-four-ways-to-tokenize-chinese-documents-f349eb6ba3c3) [## [NLP]ä¸­æ–‡æ–‡æ¡£çš„å››ç§åˆ†è¯æ–¹æ³•

### æœ‰äº†ä¸€äº›æ”¯æŒå­è¯åˆ†å‰²æŠ€æœ¯çš„ç»éªŒè¯æ®

medium.com](/the-artificial-impostor/nlp-four-ways-to-tokenize-chinese-documents-f349eb6ba3c3) [](https://amueller.github.io/word_cloud/index.html) [## WordCloud for Python æ–‡æ¡£-word cloud 1 . 6 . 0 . post 54+GB 870 feb æ–‡æ¡£

### åœ¨è¿™é‡Œä½ å¯ä»¥æ‰¾åˆ°å¦‚ä½•ç”¨æˆ‘çš„ Python wordcloud é¡¹ç›®åˆ›å»º wordcloud çš„è¯´æ˜ã€‚ä¸å…¶ä»–è¯äº‘ç›¸æ¯”â€¦

amueller.github.io](https://amueller.github.io/word_cloud/index.html)