<html>
<head>
<title>Artificial Neural Networks(Part-3)-Loss and Cost functions and Gradient Descent.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工神经网络(第三部分)-损失和成本函数和梯度下降。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/artificial-neural-networks-part-3-loss-and-cost-functions-and-gradient-descent-76e650bc5162?source=collection_archive---------19-----------------------#2020-02-24">https://medium.com/analytics-vidhya/artificial-neural-networks-part-3-loss-and-cost-functions-and-gradient-descent-76e650bc5162?source=collection_archive---------19-----------------------#2020-02-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="e6a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在人工神经网络的这一部分，我们将尝试了解什么是损失函数，以及如何使用它来计算成本函数，最后是梯度下降及其在优化中的作用。</p><p id="add8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">损失函数</em> </strong>是一种通过计算实际值与预测值的差值来评估模型性能的方法。一般来说，我们使用均方误差函数或对数损失误差函数。它衡量我们函数输出的好坏。</p><p id="bdb5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Log Loss : <strong class="ih hj"> <em class="jd"> L(p，y)=-(ylogp+(1-y)log(1-p))</em></strong>；其中y是实际值，p是预测值。<strong class="ih hj">若y = 1，则L(p，y) =-logp </strong>。<strong class="ih hj">如果y = 0，则L(p，y) = -log(1-p) </strong></p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><p id="3ef3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了训练参数<strong class="ih hj"> <em class="jd"> w </em> </strong>和<strong class="ih hj"> <em class="jd"> b </em> </strong>，我们需要一个<strong class="ih hj"> <em class="jd">代价函数</em> </strong>。成本函数可以看作是模型中所有损失函数的总和。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jl"><img src="../Images/47f098463e2a96bc55375d4c781dc48e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fXaRw_HKQk7QpiTQ6LAxtg.png"/></div></div></figure><h2 id="575d" class="jx jy hi bd jz ka kb kc kd ke kf kg kh iq ki kj kk iu kl km kn iy ko kp kq kr bi translated">梯度下降</h2><p id="c9f5" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi kx translated"><span class="l ky kz la bm lb lc ld le lf di"> H </span>超参数用于调整模型训练。(y-p)相对于权重和偏差的导数，告诉我们对于给定的样本，<strong class="ih hj"> <em class="jd">损失</em> </strong>如何变化。我们在使损失最小化的方向上反复采取小步骤，称为梯度步骤。这种策略被称为梯度下降。梯度只不过是函数的斜率。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es lg"><img src="../Images/7af32d6f8ee3e33947a54bbc7d662a26.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*js5asAr_DNT5NobLivd13A.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><a class="ae ll" href="http://rasbt.github.io/mlxtend/user_guide/general_concepts/gradient-optimization/" rel="noopener ugc nofollow" target="_blank">http://rasbt . github . io/mlx tend/user _ guide/general _ concepts/gradient-optimization/</a></figcaption></figure><p id="7fc3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度向量既有方向又有大小:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es lm"><img src="../Images/d56f6f577b6cb14d0abf8c887b5cc2a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*Ha8PDpCtsGlyxikgUPveTA.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">函数的梯度<strong class="bd jz"> f </strong></figcaption></figure><p id="c5f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度总是指向沿着损失函数增加最陡的方向。因此，沿着梯度的反向，损失函数的下降速度最快。</p><p id="7ea6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">即权重更新如下</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es ln"><img src="../Images/2b3cae4b74d81bf3432c40f5dd93e2ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*TXSwQh3DzS1s9zDMq5qprw.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">使用梯度函数更新权重</figcaption></figure><p id="b93f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降算法将梯度乘以一个称为学习率的标量，以确定下一个点，即如果梯度幅度为1.5，学习率为0.01，则梯度下降算法将选择与前一个点相距0.015的下一个点。</p><p id="d9cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了达到最小损失点，我们正在减少权重，如果我们以较小幅度或较大幅度减少权重会怎样？</p><p id="ad4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们以较低的幅度减少权重，则模型需要更多的训练时间，如果我们以较高的幅度减少权重，则模型穿过最小点。因此，我们需要谨慎选择学习速度。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es lo"><img src="../Images/de16976098a701cb2c706a97f4227f4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*heQR61fa32f8OzjYSyE5tg.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><a class="ae ll" href="https://srdas.github.io/DLBook/GradientDescentTechniques.html" rel="noopener ugc nofollow" target="_blank">https://srdas . github . io/dl book/gradientdescenttechniques . html</a></figcaption></figure><blockquote class="lp lq lr"><p id="9bbf" class="if ig jd ih b ii ij ik il im in io ip ls ir is it lt iv iw ix lu iz ja jb jc hb bi translated">一维数据的理想学习率是1/f (x ),而二维数据的理想学习率是1/Hessian矩阵</p></blockquote><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es lv"><img src="../Images/3d4d3e486c5c0c0d9488057c006b1e6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*lt0DBMBfak3zzwrpuEJVPw.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">海森矩阵</figcaption></figure><p id="42a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，梯度下降可以实现如下:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es lm"><img src="../Images/2f6b17197ec3a054f64ab646f7cd84d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*fxhodcE6mGbwqV0OmDE-VA.png"/></div></figure></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><p id="70ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好的理解，请看这个<a class="ae ll" href="https://youtu.be/z_xiwjEdAC4" rel="noopener ugc nofollow" target="_blank">视频</a></p></div></div>    
</body>
</html>