<html>
<head>
<title>An Introductory Guide to Deep Learning and Neural Networks (Notes from deeplearning.ai Course #1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习和神经网络的入门指南(来自deeplearning.ai课程#1的笔记)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/an-introductory-guide-to-deep-learning-and-neural-networks-notes-from-deeplearning-ai-course-1-5057d5556956?source=collection_archive---------1-----------------------#2018-10-21">https://medium.com/analytics-vidhya/an-introductory-guide-to-deep-learning-and-neural-networks-notes-from-deeplearning-ai-course-1-5057d5556956?source=collection_archive---------1-----------------------#2018-10-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2b88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如今，牢牢掌握深度学习技术就像获得了一种超能力。从分类图像和翻译语言到制造自动驾驶汽车，所有这些任务都由计算机驱动，而不是人工操作。深度学习已经渗透到多个不同的行业，并且几乎每周都在继续突破新的领域。</p><p id="b9a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以理解的是，很多人突然对这个领域感兴趣。但是应该从哪里开始呢？应该学什么？构成这个复杂而有趣的领域的核心概念是什么？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/9622275a36da9ee30e2ee48de853a07b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*e5Cl6h2K5bdddqRp.jpg"/></div></figure><p id="22b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我很高兴写下一系列文章，在这些文章中，我将分解每个深度学习爱好者都应该彻底了解的基本组件。我的灵感来自deeplearning.ai，他发布了一个非常棒的深度学习专业化课程，我发现这对我的学习之旅非常有帮助。</p><p id="36e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将写专业化的课程1，其中伟大的<a class="jl jm ge" href="https://medium.com/u/592ce2a67248?source=post_page-----5057d5556956--------------------------------" rel="noopener" target="_blank">吴恩达</a>解释了神经网络的基础知识以及如何实现它们。我们开始吧！</p><p id="6881" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jn">注意:在整个系列中，我们将遵循自下而上的方法——我们将首先从头开始理解这个概念，然后才是它的实现。这种方法被证明对我很有帮助。</em></p><h1 id="4b9b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">目录</h1><ol class=""><li id="530c" class="km kn hi ih b ii ko im kp iq kq iu kr iy ks jc kt ku kv kw bi translated">了解课程结构</li><li id="75d6" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">课程1:神经网络和深度学习<br/> 1。模块1:深度学习简介<br/> 2。模块2:神经网络基础知识<br/> a .作为神经网络的逻辑回归<br/> b. Python和矢量化<br/> 3 .模块3:浅层神经网络<br/> 4。模块4:深度神经网络</li></ol><h1 id="eff7" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">1.了解课程结构</h1><p id="88d1" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">这个深度学习专业总共由5门课程组成。课程#1是本文的重点，它进一步分为4个子模块:</p><ol class=""><li id="8197" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">第一个模块简要概述了深度学习和神经网络</li><li id="6b7c" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">在模块2中，我们深入学习神经网络的基础知识。吴恩达解释了如何使用神经网络解决逻辑回归问题</li><li id="1053" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">在模块3中，讨论转向浅层神经网络，简要介绍激活函数、梯度下降以及前向和后向传播</li><li id="2609" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">在最后一个模块中，吴恩达教授了最令人期待的话题——深度神经网络</li></ol><p id="9b9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">准备好开始了吗？接着读下去！</p><h1 id="e142" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">2.课程1:神经网络和深度学习</h1><p id="9bbb" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">好了，现在我们对这篇文章的结构有了一个概念，是时候从头开始了。戴上你的学习帽，因为这将是一次有趣的经历。</p><h1 id="f101" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">2.1模块1:深度学习简介</h1><p id="27cc" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">第一个模块的目标如下:</p><ul class=""><li id="143e" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">了解推动深度学习兴起的主要趋势</li><li id="85b2" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">能够解释深度学习如何应用于监督学习</li><li id="b4cb" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">了解模型的主要类别(如CNN和rnn)，以及何时应用它们</li><li id="55ac" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">能够认识到深度学习何时会(或不会)很好地工作的基础</li></ul><h2 id="cc02" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">什么是神经网络？</h2><p id="243f" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">先说事情的症结，一个很关键的问题。什么是神经网络？</p><p id="d09b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑一个例子，我们必须预测房子的价格。给我们的变量是房子的平方英尺(或平方米)大小和房子的价格。现在假设我们有6栋房子。首先让我们画一张图来形象化我们正在看的东西:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/4df31603a1f0cd2c9ab80bac4149c533.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/0*Fyq_d3F9pQwUhfuz.png"/></div></figure><p id="3e99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">x轴代表房子的大小，y轴代表相应的价格。线性回归模型会尝试绘制一条直线来拟合数据:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/fba031864d5eb05e55a9c23377112abe.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/0*m4gZc7bhSQXGOupS.png"/></div></figure><p id="965e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，这里的输入(x)是房子的大小，输出(y)是价格。现在让我们看看如何使用一个简单的神经网络来解决这个问题:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lz"><img src="../Images/7f772969c9dd5fc08ab6049912b1b64f.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/0*bTN-OTb3PN4uU5NW.png"/></div></figure><p id="42b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，一个神经元将接受一个输入，对其应用一些激活函数，并生成一个输出。最常用的激活功能之一是ReLU(整流线性单元):</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ma"><img src="../Images/9c8431cd6bc39c584f858e31220a8b94.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/0*YLDlf2pjyI8pj2Gp.png"/></div></figure><p id="d993" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ReLU接受一个实数作为输入，并返回0或该数的最大值。所以，如果我们传递10，输出就是10，如果输入是-10，输出就是0。我们将在本文后面详细讨论激活函数。</p><p id="c280" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们坚持我们的例子。如果我们使用ReLU activation函数根据房子的大小预测房子的价格，预测结果可能是这样的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mb"><img src="../Images/3577f1fa13b0e4daadba5060860687a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/0*00HpU1FND4QzNxL_.png"/></div></figure><p id="5b91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，我们已经看到了一个具有单个神经元的神经网络，即，我们只有一个特征(房子的大小)来预测房价。但在现实中，我们必须考虑多个特征，如卧室数量、邮政编码等。？房价还取决于家庭规模、社区位置或学校质量。在这种情况下，我们如何定义神经网络？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mc"><img src="../Images/9ba03c760099ffa2ad28fff2e590b80d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/0*jqfv6z0i3uxA3Vb4.png"/></div></figure><p id="aa5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里变得有点复杂。阅读时请参考上面的图像-我们将4个特征作为输入传递给神经网络作为x，它会自动从输入中识别一些隐藏的特征，并最终生成输出y。这是具有4个输入和单个隐藏层输出的神经网络的样子:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es md"><img src="../Images/246189aeb853197b07b5147de0a33cbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/0*vMPvcfoTE3sJ__hZ.png"/></div></figure><p id="41de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然我们对什么是神经网络有了一个直觉，让我们看看如何将它们用于监督学习问题。</p><h2 id="edde" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">神经网络监督学习</h2><p id="d60e" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">监督学习指的是一项任务，我们需要找到一个可以将输入映射到相应输出的函数(给定一组输入输出对)。对于每个给定的输入，我们都有一个定义的输出，我们根据这些例子训练模型。下面是一个非常方便的表格，它显示了监督学习的不同应用以及可用于解决这些问题的不同类型的神经网络:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es me"><img src="../Images/157ec9870951276dc4eebd230ca4b268.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*fKNTQacKIvfZTqK7w6rS-g.png"/></div></figure><p id="7310" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是最常见的神经网络类型的直观表示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mf"><img src="../Images/5a1884da72478b81c10c19b407b4c98c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*BQZApmZS3zE7GkW7.png"/></div></div></figure><p id="84b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将关注标准神经网络。不要担心，我们将在接下来的文章中讨论其他类型。</p><p id="1291" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可能知道，监督学习可以用于结构化和非结构化数据。</p><p id="aded" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的房价预测示例中，给定的数据告诉我们卧室的大小和数量。这是结构化数据，意味着每个特征，如房子的大小，卧室的数量等。有着非常明确的含义。</p><p id="86f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相比之下，非结构化数据指的是音频、原始音频或图像等您可能想要识别图像或文本中的内容的东西(如对象检测)。这里，特征可能是图像中的像素值，或者是一段文本中的单个单词。不太清楚图像的每个像素代表什么，因此这属于非结构化数据范畴。</p><p id="5bd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单的机器学习算法适用于结构化数据。但当涉及到非结构化数据时，它们的性能往往会大幅下降。这就是神经网络被证明如此有效和有用的地方。它们在非结构化数据上表现出色。如今，大多数突破性的研究都以神经网络为核心。</p><h2 id="e595" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">深度学习为什么会腾飞？</h2><p id="72da" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">要理解这一点，请看下图:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mk"><img src="../Images/c6fbf65ec32b08e9fc18018339eab5ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3Jy9n_XImP5u2VLO.png"/></div></div></figure><p id="6ff6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随着数据量的增加，传统学习算法的性能，如SVM和逻辑回归，并没有提高很多。事实上，在某个点之后，它趋于稳定。就类神经网路而言，模型的效能会随著您输入模型的资料增加而提升。</p><p id="bdc3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基本上有三个尺度驱动典型的深度学习过程:</p><ol class=""><li id="9009" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">数据</li><li id="cfac" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">计算时间</li><li id="799c" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">算法</li></ol><p id="de1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了提高模型的计算时间，激活函数起着重要的作用。如果我们使用sigmoid激活函数，这就是我们最终得到的结果:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/b1af81c5150f5bf794be7ff872f41427.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/0*3SaE6Uh66mck59mn.png"/></div></figure><p id="ad2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">斜率，或者这个函数的梯度，在极端情况下接近于零。所以参数更新非常慢，导致学习非常慢。因此，从sigmoid激活函数切换到ReLU(校正线性单元)是我们在神经网络中看到的最大突破之一。当x&gt;0时，当斜率为1时，ReLU更新参数要快得多。这是模型计算更快的主要原因。</p><h1 id="6dd6" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">2.2模块2:深度学习简介</h1><p id="0c0e" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">模块2的目标是:</p><ul class=""><li id="7cf1" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">建立一个逻辑回归模型，结构为一个浅层神经网络</li><li id="34a6" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">实现最大似然算法的主要步骤，包括预测、导数计算和梯度下降</li><li id="3f51" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">实现计算高效且高度矢量化的模型版本</li><li id="1fff" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">理解如何使用反向传播思维计算逻辑回归的导数</li><li id="3918" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">熟悉Python和Numpy</li></ul><p id="d07d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模块进一步分为两个部分:</p><ul class=""><li id="8ae6" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">第一部分:作为神经网络的逻辑回归</li><li id="d6fb" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">第二部分:Python和矢量化</li></ul><p id="66c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们详细了解每一部分。</p><h1 id="c2b0" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">第一部分:作为神经网络的逻辑回归</h1><h2 id="dbcb" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">二元分类</h2><p id="5dd5" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">在一个二进制分类问题中，我们有一个输入x，比如说一张图片，我们要把它分类为有没有猫。如果它是一只猫，我们将为它赋值1，否则为0。所以在这里，我们只有两个输出——要么图像包含一只猫，要么不包含。这是一个二元分类问题的例子。</p><p id="dc79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种情况下，我们当然可以使用最流行的分类技术，逻辑回归。</p><h2 id="19ac" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">逻辑回归</h2><p id="6b2f" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">我们有一个输入X(图像)，我们想知道图像属于类别1(即一只猫)的概率。对于给定的X向量，输出将是:</p><p id="6e8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> y = w(转置)X + b </strong></p><p id="6d91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里w和b是参数。因为我们的输出y是概率，它应该在0和1之间。但是在上面的等式中，它可以取任何实值，这对得到概率没有意义。因此，逻辑回归也使用sigmoid函数来输出概率:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mm"><img src="../Images/764c6ede9337f759dffeede986ab536a.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/0*kZJJAZhZqHgkyuQQ.png"/></div></figure><p id="50e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于作为输入的任何值，它将只返回0到1范围内的值。sigmoid函数的公式为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mn"><img src="../Images/9f8dc10b431de5a4fd675d57cc869e4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/0*LW47glQ_LhtZ0Yrp.png"/></div></figure><p id="8751" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，如果z非常大，exp(-z)将接近0，因此sigmoid的输出将为1。类似地，如果z非常小，exp(-z)将是无穷大，因此sigmoid的输出将是0。</p><p id="d30a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意，参数w是nx维向量，b是实数。现在让我们看看逻辑回归的成本函数。</p><h2 id="ed6d" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">逻辑回归成本函数</h2><p id="14c2" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">为了训练逻辑回归的参数w和b，我们需要一个成本函数。我们希望找到参数w和b，使得至少在训练集上，您的输出(y-hat)接近实际值(y)。</p><p id="100a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以使用下面定义的损失函数:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mo"><img src="../Images/8017d7518b2fc42da648a4437415fa09.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/0*fmPdqpkPHrrDW4n1.png"/></div></figure><p id="bc56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个函数的问题是优化问题变得非凸，导致多个局部最优。因此，梯度下降不能很好地处理这个损失函数。因此，对于逻辑回归，我们定义了一个不同的损失函数，其作用与上述损失函数相似，并且还通过给出一个凸函数来解决优化问题:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mp"><img src="../Images/b0e0aa2f3423c66d62113ef5bd2f6c9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/0*pNmYyhRTw-h31tWu.png"/></div></figure><p id="e860" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">损失函数是为单个训练示例定义的，它告诉我们在该特定示例上做得有多好。另一方面，成本函数是针对整个训练集的。逻辑回归的成本函数是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mq"><img src="../Images/286fbbd24102e4fb4997205ba0033c30.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/0*-0l1J5XaJKopGtJG.png"/></div></figure><p id="adb3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我们希望我们的成本函数尽可能小</strong>。为此，我们希望我们的参数w和b得到优化。</p><h2 id="d796" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">梯度下降</h2><p id="602b" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">这是一种有助于以最小化成本函数的方式学习参数w和b的技术。逻辑回归的成本函数本质上是凸的(即只有一个全局最小值)，这就是选择该函数而不是平方误差(可以有多个局部最小值)的原因。</p><p id="c9fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看梯度下降的步骤:</p><ol class=""><li id="b4b4" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">初始化w和b(对于逻辑回归通常初始化为0)</li><li id="eaa0" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">向最陡的下坡方向迈一步</li><li id="e3ef" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">重复步骤2，直到达到全局最优</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mr"><img src="../Images/5b91712b9d445377383e6cfaffcb3364.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/0*LqlCvolxlFuNd7AO.png"/></div></figure><p id="d0da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降的更新方程变为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ms"><img src="../Images/979cc99fa5a1fc6d8553cf705a432a91.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/0*vVTgox5mMmZ0hMwy.png"/></div></figure><p id="6bb3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，⍺是学习率，它控制着我们在每次迭代后应该迈出多大的一步。</p><p id="3e2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们在上图的右边，斜率将是正的。使用更新的等式，我们将向左移动(即向下)，直到达到全局最小值。然而，如果我们在左侧，斜率将是负的，因此我们将向右(向下)移动一步，直到达到全局最小值。很直观，对吧？</p><p id="e705" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归参数的更新方程为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mt"><img src="../Images/2b287af87170562ceb1136cd9de09adb.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/format:webp/0*mEBMYgFDT3GRUMjk.png"/></div></figure><h2 id="a9a3" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">派生物</h2><p id="11fa" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">考虑一个函数，f(a) = 3a，如下所示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mu"><img src="../Images/55cf56d91f86023d3b3cfe4f66216c5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/0*oegLXP6FVf0Thbkj.png"/></div></figure><p id="2a13" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个函数在任一点的导数将给出该点的斜率。所以，</p><p id="65ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">f(a=2) = 3*2 = 6</p><p id="4ab3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">f(a=2.001) = 3*2.001 = 6.003</p><p id="5e08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a = 2时函数的斜率/导数为:</p><p id="4fcf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">斜率=高度/宽度</p><p id="d1db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">斜率= 0.003 / 0.001 = 3</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mv"><img src="../Images/1915962f87552f57b0ce486e36c79158.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/0*UsBwN1sm851MmNXx.png"/></div></figure><p id="5313" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我们计算函数的导数/斜率的方法。我们再来看几个导数的例子。</p><h2 id="7cf5" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">更多衍生例子</h2><p id="ea8b" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">考虑以下3个函数及其相应的导数:</p><p id="0020" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">f(a) = a2，d(f(a))/d(a) = 2a</p><p id="42a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">f(a) = a3，d(f(a))/d(a) = 3a2</p><p id="7f60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，f(a) = log(a)，d(f(a))/d(a) = 1/a</p><p id="458b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面所有的例子中，导数是a的函数，这意味着函数在不同点的斜率是不同的。</p><h2 id="6b8a" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">计算图</h2><p id="fce1" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">这些图形组织了特定函数的计算。考虑下面的例子:</p><p id="d95f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">J(a，b，c) = 3(a+bc)</p><p id="8a6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给定a、b和c，我们必须计算J，我们可以将其分为三步:</p><p id="147f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们想象一下a = 5、b = 3和c = 2的这些步骤:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/62c417d9b7f1755df398745bf27e1d54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/0*PG6_yhOMdawMyZus.png"/></div></figure><p id="f98c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是前向传播步骤，其中我们已经计算了输出，即j。我们也可以使用计算图进行后向传播，其中我们更新上述示例中的参数a、b和c。</p><h2 id="88f5" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">带计算图的导数</h2><p id="311e" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">现在让我们看看如何在计算图的帮助下计算导数。假设我们要计算dJ/da。这些步骤将是:</p><ol class=""><li id="13a2" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">由于J是v的函数，计算dJ/dv: <br/> dJ/dv = d(3v)/dv = 3</li><li id="7240" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">由于v是a和u的函数，计算dv/da: <br/> dv/da = d(a+u)/da = 1</li><li id="f970" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">计算dJ/da:<br/>dJ/da =(dJ/dv)*(dv/da)= 3 * 1 = 3</li></ol><p id="06f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，我们可以计算dJ/db和dJ/dc:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mx"><img src="../Images/56788506f07c0aa3d4caab9460be5a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zMlmS7nwB6oiMLMS.png"/></div></div></figure><p id="30fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将计算图和梯度下降的概念结合起来，看看如何更新逻辑回归的参数。</p><h2 id="0a55" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">逻辑回归梯度下降</h2><p id="54e5" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">简单回顾一下，逻辑回归方程是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es my"><img src="../Images/649abc9097f88fff65c34ce56667c3de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/0*35Y1r3MJdiXFb24C.png"/></div></figure><p id="4982" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中L是损失函数。现在，对于两个特征(x1和x2)，用于计算损失的计算图将是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mz"><img src="../Images/e814ae93adcdd7ebb9a347b79d3eb2a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*krjlI6R7dZtv91tl.png"/></div></div></figure><p id="6018" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，w1、w2和b是需要更新的参数。以下是执行此操作的步骤(对于w1):</p><ol class=""><li id="7876" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">计算da:<br/>da = dL/da =(-y/a)+(1-y)/(1-a)</li><li id="f989" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">计算dz:<br/>dz =(dL/da)*(da/dz)=[(-y/a)+(1-y)/(1-a)]*[a(1-a)]= a-y</li><li id="02d2" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">计算dw1:<br/>dw1 =[(dL/da)*(da/dz)]* dz/dw1 =(a-y)* dz/dw1</li></ol><p id="0bd0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，我们可以计算dw2和db。最后，将使用以下等式更新权重:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es na"><img src="../Images/fbcf02e78b4b1c698c8ef959ceb92767.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/0*zrS1QxEtrWn1fINm.png"/></div></figure><p id="7ad7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请记住，这是针对单个培训示例的。我们将在真实场景中有多个例子。因此，让我们看看如何为“m”训练示例计算梯度下降。</p><h2 id="30e2" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">“m”示例上的梯度下降</h2><p id="f5df" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">我们可以将“m”训练示例的预测和成本函数定义为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nb"><img src="../Images/8cc5e20d90bed069cb7c007d00df9869.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/0*CEF76tXAROS94MUO.png"/></div></figure><p id="db9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">损失函数对参数的导数可以写成:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nc"><img src="../Images/dd1c3acad7134257b3d9ade98d552c75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/0*0tN5LS3II2vrdCU1.png"/></div></figure><p id="d599" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们看看如何对“m”示例应用逻辑回归:</p><pre class="je jf jg jh fd nd ne nf ng aw nh bi"><span id="9e48" class="lj jp hi ne b fi ni nj l nk nl">J = 0; dw1 = 0; dw2 =0; db = 0;                <br/>w1 = 0; w2 = 0; b=0;                            <br/>for i = 1 to m<br/>    # Forward pass<br/>    z(i) = W1*x1(i) + W2*x2(i) + b<br/>    a(i) = Sigmoid(z(i))<br/>    J += (Y(i)*log(a(i)) + (1-Y(i))*log(1-a(i)))<br/>    <br/>    # Backward pass<br/>    dz(i) = a(i) - Y(i)<br/>    dw1 += dz(i) * x1(i)<br/>    dw2 += dz(i) * x2(i)<br/>    db  += dz(i)<br/>J /= m<br/>dw1/= m<br/>dw2/= m<br/>db/= m<br/><br/># Gradient descent<br/>w1 = w1 - alpa * dw1<br/>w2 = w2 - alpa * dw2<br/>b = b - alpa * db</span></pre><p id="f159" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些for循环最终使得计算非常缓慢。有一种方法可以替换这些循环，从而提高代码效率。我们将在接下来的章节中研究这些技巧。</p><h1 id="174c" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">第二部分— Python和矢量化</h1><p id="95a6" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">至此，我们已经看到了如何使用梯度下降来更新逻辑回归的参数。在上面的示例中，我们看到，如果我们有“m”个训练示例，我们必须运行循环“m”次才能得到输出，这使得计算非常缓慢。</p><p id="463f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代替这些for循环，我们可以使用矢量化，这是一种有效且省时的方法。</p><h2 id="954f" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">…向量化…</h2><p id="429a" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">矢量化基本上是一种消除代码中for循环的方法。它对“m”个训练样本一起执行所有操作，而不是单独计算它们。让我们看看逻辑回归的非矢量化和矢量化表示:</p><p id="7eeb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">非矢量化形式:</p><pre class="je jf jg jh fd nd ne nf ng aw nh bi"><span id="1731" class="lj jp hi ne b fi ni nj l nk nl">z = 0<br/>for i in range(nx):<br/>   z += w[i] * x[i]<br/>z +=b</span></pre><p id="e8a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们看看矢量化的形式。我们可以用矢量形式表示w和x:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nm"><img src="../Images/0c5bb36ad46cc22ef20e8ba67182887a.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/0*yiYGaBdWZnKNR3_S.png"/></div></figure><p id="397f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们可以使用以下公式计算所有训练示例的Z:</p><p id="6584" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jn"> Z = np.dot(W，X)+b ( </em> numpy <em class="jn">导入为np) </em></p><p id="1090" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">NumPy库的点函数默认使用矢量化。这就是我们如何对乘法进行矢量化的方法。现在让我们看看如何对整个逻辑回归算法进行矢量化。</p><h2 id="e57e" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">向量化逻辑回归</h2><p id="4c25" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">与“m”训练示例保持一致，第一步是计算所有这些示例的Z:</p><p id="1e6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jn"> Z = np.dot(W.T，X) + b </em></p><p id="3028" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，X包含所有训练示例的特征，而W是这些示例的系数矩阵。下一步是计算输出(A ),即Z的sigmoid:</p><p id="12c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jn"> A = 1 / 1 + np.exp(-Z) </em></p><p id="c4dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，计算损耗，然后使用反向传播将损耗降至最低:</p><p id="bc6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dz <em class="jn"> = </em> A — Y</p><p id="062f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们将计算参数的导数并更新它们:</p><p id="9176" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">dw <em class="jn"> = np.dot(X，dz。T) / m </em></p><p id="d6ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">db <em class="jn"> = </em> dz <em class="jn">。sum() / m </em></p><p id="1a09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jn"> W = W — ⍺dw </em></p><p id="29b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jn"> b = b — ⍺db </em></p><h2 id="ce3d" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">用Python广播</h2><p id="1e9a" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">广播使得代码的某些部分更加有效。但是不要只相信我的话！让我们看一些例子:</p><ul class=""><li id="0868" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">obj.sum(axis = 0)对列求和，而obj.sum(axis = 1)对行求和</li><li id="10b5" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">obj . shape(1，4)通过传播值来改变矩阵的形状</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es nn"><img src="../Images/9b4bebaed4d79e047797732fdeb78355.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*h-97eqkPwxqpZHkl.png"/></div></div></figure><p id="f9b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们把100加到一个(4×1)矩阵，它会把100复制到一个(4×1)矩阵。类似地，在下面的示例中，将复制(1×3)矩阵以形成(2×3)矩阵:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es no"><img src="../Images/cce456350c264403a1bc597434fe20d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*X_t4NW9A1PgKkn7n.png"/></div></div></figure><p id="a539" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总的原则是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es np"><img src="../Images/518be9c40b7694edef13d5e728e74181.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/0*TL-jlawdfFugkBrw.png"/></div></figure><p id="86ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们用一个(1，n)矩阵加、减、乘或除一个(m，n)矩阵，这将把它复制m次成为一个(m，n)矩阵。这被称为广播，它使计算速度更快。自己试试吧！</p><h2 id="c231" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">关于Python/Numpy向量的一个注记</h2><p id="b9e6" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">如果使用以下方式组成数组:</p><p id="9cc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a = np.random.randn(5)</p><p id="fe6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它将创建一个shape (5)数组，这是一个秩为1的数组。使用此数组会导致在转置数组时出现问题。相反，我们可以使用以下代码来形成一个向量，而不是秩为1的数组:</p><p id="bca3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a = np.random.randn(5，1) # shape (5，1)列向量</p><p id="6280" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a = np.random.randn(1，5) # shape (1，5)行向量</p><p id="1121" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要将(1，5)行向量转换为(5，1)列向量，可以使用:</p><p id="3c57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a = a . shape((5，1))</p><p id="bf30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模块2到此结束。在下一节中，我们将深入浅出的神经网络的细节。</p><h1 id="ca22" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">2.3模块3:浅层神经网络</h1><p id="00a6" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">模块3的目标是:</p><ul class=""><li id="c045" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">了解隐藏单元和隐藏层</li><li id="f8c9" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">能够在神经网络中应用各种激活函数。</li><li id="e23f" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">使用隐藏层构建您的第一个向前和向后传播</li><li id="2c58" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">对你的神经网络应用随机初始化</li><li id="5172" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">熟练使用深度学习符号和神经网络表示法</li><li id="076f" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">建立并训练具有一个隐藏层的神经网络</li></ul><h2 id="088b" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">神经网络概述</h2><p id="d33e" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">在逻辑回归中，为了计算输出(y = a)，我们使用了下面的计算图:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nq"><img src="../Images/ac1ad04ed774aacea2aebc6a04c03ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/0*omhI_XYngF0xA9CP.png"/></div></figure><p id="3079" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在只有一个隐藏层的神经网络的情况下，该结构看起来像是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mr"><img src="../Images/9f10c261b76ef8410a5b7dc8fd40c88b.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/0*Yb_97QVNHgpaIP0T.png"/></div></div></figure><p id="7a40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算输出的计算图将是:</p><pre class="je jf jg jh fd nd ne nf ng aw nh bi"><span id="86a2" class="lj jp hi ne b fi ni nj l nk nl">X1  \ <br/>X2   =&gt; z1 = XW1 + B1 =&gt; a1 = Sigmoid(z1) =&gt; z2 = a1W2 + B2 =&gt; a2    = Sigmoid(z2) =&gt; l(a2,Y)<br/>X3  /</span></pre><h2 id="2c3f" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">神经网络表示</h2><p id="a4ad" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">考虑神经网络的以下表示:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nr"><img src="../Images/148f8e2c0e5dce27a30749780a09d426.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/0*z6yg8Mx7uFbuGObt.png"/></div></figure><p id="2984" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你能识别出上述神经网络的层数吗？请记住，在计算神经网络的层数时，我们不计算输入层。因此，在上面显示的NN中有2层，即一个隐藏层和一个输出层。</p><p id="8cd0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一层称为[0]，第二层称为[1]，最后一层称为[2]。这里“a”代表激活，这是神经网络不同层传递给下一层的值。对应的参数是w[1]，b[1]和w[1]，b[2]:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ns"><img src="../Images/fcbbf2dd89872313a133944b45c5c9c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*XbjasRuq6eTUVgun.png"/></div></figure><p id="9e39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是神经网络的表现方式。接下来，我们将看看如何计算神经网络的输出。</p><h2 id="e3c0" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">计算神经网络的输出</h2><p id="1edc" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">让我们详细看看神经网络的每个神经元是如何工作的。每个神经元接受一个输入，对其执行一些操作(计算z = w[T]+ b)，然后应用sigmoid函数:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nt"><img src="../Images/8f4971163593b9e7a46cecf8ce9e9d08.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/0*XXlldFORcmhVFTpd.png"/></div></figure><p id="14a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个步骤由每个神经元执行。具有四个神经元的第一隐藏层的方程将是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nu"><img src="../Images/9114028f5c62b686a7b457172fb03fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/0*sNmoYPtamzT5qFgy.png"/></div></figure><p id="e5b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，对于给定的输入X，每个神经元的输出将为:</p><p id="011f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">z[1]= W[1]x + b[1]</p><p id="e690" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一[1]= 𝛔(z[1])</p><p id="da24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">z[2]= W[2]x + b[2]</p><p id="7981" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一[2]= 𝛔(z[2])</p><p id="c019" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了计算这些输出，我们需要运行一个for循环，为每个神经元单独计算这些值。但是回想一下，使用for循环会使计算非常慢，因此我们应该优化代码，去掉这个for循环，使它运行得更快。</p><h2 id="5b5b" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">跨多个示例进行矢量化</h2><p id="fa47" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">计算神经网络输出的非矢量化形式为:</p><pre class="je jf jg jh fd nd ne nf ng aw nh bi"><span id="a8a2" class="lj jp hi ne b fi ni nj l nk nl">for i=1 to m:<br/>    z[1](i) = W[1](i)x + b[1]<br/>    a[1](i) = 𝛔(z[1](i))<br/>    z[2](i) = W[2](i)x + b[2]<br/>    a[2](i) = 𝛔(z[2](i))</span></pre><p id="f653" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用这个for循环，我们分别为每个训练示例计算z和a值。现在我们来看看如何将其矢量化。所有训练示例将被合并到单个矩阵X中:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nv"><img src="../Images/d4e099ea16266b3356f792817e21d871.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/0*hxqQfRLFvqsFuVMi.png"/></div></figure><p id="e930" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，nx是特征的数量，m是训练样本的数量。用于计算输出的矢量化形式为:</p><p id="9e67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Z[1]= W[1]X + b[1]</p><p id="df1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一[1]= 𝛔(Z[1])</p><p id="0def" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Z[2]= W[2]X + b[2]</p><p id="d508" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一[2]= 𝛔(Z[2])</p><p id="9f47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这将减少计算时间(在大多数情况下显著减少)。</p><h2 id="5b3f" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">激活功能</h2><p id="d1df" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">计算输出时，应用激活函数。激活函数的选择极大地影响了模型的性能。到目前为止，我们已经使用了sigmoid激活函数:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nw"><img src="../Images/904a1f8fa230147fb994bb131ef43335.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/0*4p7WwzW5HYayi4Cx.png"/></div></figure><p id="71f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，在某些情况下，这可能不是最佳选择。为什么？因为在图的极端，导数将接近于零，因此梯度下降将非常缓慢地更新参数。</p><p id="df1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还有其他功能可以替代该激活功能:</p><ul class=""><li id="f3ab" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">tanh:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ns"><img src="../Images/55fe54a194b06030d3feaa15dd7c4419.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*WP6dn_BBF1vrNrj4.png"/></div></figure><ul class=""><li id="8624" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">ReLU(之前已经介绍过):</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ns"><img src="../Images/9012d71c2098d6ac273430e5d7b036d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*7aqlMYo3GlxV-FOU.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es nx"><img src="../Images/724cc03560344afba22488c735ce0b5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RDn_4GkW26puFnqhljCLLg.png"/></div></div></figure><p id="4f08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以根据要解决的问题选择不同的激活函数。</p><h2 id="1bd3" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">为什么我们需要非线性激活函数？</h2><p id="f7e7" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">如果我们对图层的输出使用线性激活函数，它会将输出计算为输入要素的线性函数。我们首先将Z值计算为:</p><p id="af0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Z = WX + b</p><p id="02de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在线性激活函数的情况下，输出将等于Z(而不是计算任何非线性激活):</p><p id="f632" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">A = Z</p><p id="2fe5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用线性激活本质上是没有意义的。两个线性函数的组合本身就是一个线性函数，除非我们使用一些非线性激活，否则我们不会计算更有趣的函数。这就是为什么大多数专家坚持使用非线性激活函数。</p><p id="e621" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">只有一种情况下我们倾向于使用线性激活函数。假设我们想预测一栋房子的价格(可以是任何正实数)。如果我们使用sigmoid或tanh函数，输出范围将分别为(0，1)和(-1，1)。但价格也将超过1英镑。在这种情况下，我们将在输出层使用线性激活函数。</p><p id="2bae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们有了输出，下一步是什么？我们想要执行反向传播，以便使用梯度下降来更新参数。</p><h2 id="830e" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">神经网络的梯度下降</h2><p id="4399" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">在两层神经网络中，我们必须更新的参数是:w[1]，b[1]，w[2]和b[2]，我们要最小化的成本函数是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ny"><img src="../Images/bf3f412377456d0bbe79c524d6d393bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/0*hEqheOoKZqL6-jz7.png"/></div></figure><p id="4857" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降步骤可总结为:</p><pre class="je jf jg jh fd nd ne nf ng aw nh bi"><span id="bf9f" class="lj jp hi ne b fi ni nj l nk nl">Repeat:<br/>    Compute predictions (y'(i), i = 1,...m)<br/>    Get derivatives: dW[1], db[1], dW[2], db[2]<br/>    Update: W[1] = W[1] - ⍺ * dW[1]<br/>            b[1] = b[1] - ⍺ * db[1]<br/>            W[2] = W[2] - ⍺ * dW[2]<br/>            b[2] = b[2] - ⍺ * db[2]</span></pre><p id="760e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们快速看一下两层神经网络的正向和反向传播步骤。</p><p id="c634" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正向传播:</p><pre class="je jf jg jh fd nd ne nf ng aw nh bi"><span id="acde" class="lj jp hi ne b fi ni nj l nk nl">Z[1] = W[1]*A[0] + b[1]    # A[0] is X<br/>A[1] = g[1](Z[1])<br/>Z[2] = W[2]*A[1] + b[2]<br/>A[2] = g[2](Z[2])</span></pre><p id="3070" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">反向传播:</p><pre class="je jf jg jh fd nd ne nf ng aw nh bi"><span id="5056" class="lj jp hi ne b fi ni nj l nk nl">dZ[2] = A[2] - Y   <br/>dW[2] = (dZ[2] * A[1].T) / m<br/>db[2] = Sum(dZ[2]) / m<br/>dZ[1] = (W[2].T * dZ[2]) * g'[1](Z[1])  # element wise product (*)<br/>dW[1] = (dZ[1] * A[0].T) / m   # A[0] = X<br/>db[1] = Sum(dZ[1]) / m</span></pre><p id="0ea6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些是神经网络生成输出的完整步骤。注意，我们必须在开始时初始化权重(W ),然后在反向传播步骤中更新权重。所以让我们看看这些权重应该如何初始化。</p><h2 id="afaa" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">随机初始化</h2><p id="3015" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">我们之前已经看到，在逻辑回归算法的情况下，权重被初始化为0。但是我们应该将神经网络的权重初始化为0吗？这是一个中肯的问题。让我们考虑下面的例子:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nz"><img src="../Images/3d5bcbf651a91ea14df986ad39f88ca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/0*eY96axtBooHMTsE9.png"/></div></figure><p id="7e54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果权重被初始化为0，则W矩阵将为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es oa"><img src="../Images/f634ed988f829360109fced8d1bcece9.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/0*5VFLO_Ko2pTife-1.png"/></div></figure><p id="ac92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用这些重量:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ob"><img src="../Images/7ac536f7f8b54bf55c3ef0378c557aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/0*UW1rA-LX1psUY1gF.png"/></div></figure><p id="d2b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后在反向传播步骤:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es oc"><img src="../Images/f261d5da60fc64d33c7db6d87c1f432e.png" data-original-src="https://miro.medium.com/v2/resize:fit:322/format:webp/0*4XEQBf6BUO6_KmiD.png"/></div></figure><p id="2250" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">无论我们在一层中使用多少个单元，我们总是得到相同的输出，这与使用单个单元的输出相似。因此，我们没有将权重初始化为0，而是使用以下代码随机初始化它们:</p><pre class="je jf jg jh fd nd ne nf ng aw nh bi"><span id="14aa" class="lj jp hi ne b fi ni nj l nk nl">w[1] = np.random.randn((2,2)) * 0.01<br/>b[1] = np.zero((2,1))</span></pre><p id="44d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将权重乘以0.01来初始化小权重。如果我们初始化大的权重，激活将会很大，导致零斜率(在sigmoid和tanh激活函数的情况下)。因此，学习会很慢。所以我们一般随机初始化小权重。</p><h1 id="ee57" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">2.4模块4:深度神经网络</h1><p id="3269" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">终于到了学习深度神经网络的时候了！这些已经成为当今业界和研究领域的流行语。这些天来，无论我拿起哪篇研究论文，都不可避免地提到深度神经网络是如何被用来驱动这项研究背后的思维过程的。</p><p id="4735" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们最后一个模块的目标是:</p><ul class=""><li id="69d2" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">将深度神经网络视为一个接一个的连续块</li><li id="386f" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">构建并训练深度L层神经网络</li><li id="27b3" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">分析矩阵和向量维数以检查神经网络实现</li><li id="5d75" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">了解如何使用缓存将信息从正向传播传递到反向传播</li><li id="28d6" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">理解超参数在深度学习中的作用</li></ul><h2 id="24f0" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">深层L层神经网络</h2><p id="8be5" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">在本节中，我们将研究如何将前向和后向传播的概念应用于深度神经网络。但是此时你可能会想知道深度神经网络到底是什么？</p><p id="ef63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">浅与深是一个程度问题。逻辑回归是一个非常浅的模型，因为它只有一层(请记住，我们不将输入算作一层):</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es od"><img src="../Images/df7729035cddd0aa5450c182e606ddc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/0*CWeVSrKf55m6oxh4.png"/></div></figure><p id="1b9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更深的神经网络具有更多数量的隐藏层:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es oe"><img src="../Images/87049f1b56e51d3b88cc2b6ba80146d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/0*MPQOYr1683oYhAuy.png"/></div></figure><p id="1239" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看与深度神经网络相关的一些符号:</p><ul class=""><li id="a396" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">l是神经网络的层数</li><li id="d98f" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">n[l]是层l中单元的数量</li><li id="31d2" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">a[l]是层l中的激活</li><li id="3c0c" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">w[l]是z[l]的权重</li></ul><p id="9e8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些是我们将在接下来的章节中使用的一些符号。在我们进行的过程中，请记住它们，或者只是快速跳回到这里，以防您错过了什么。</p><h2 id="8645" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">深度神经网络中的前向传播</h2><p id="b39a" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">对于单个训练示例，正向传播步骤可以写成:</p><pre class="je jf jg jh fd nd ne nf ng aw nh bi"><span id="bbc1" class="lj jp hi ne b fi ni nj l nk nl">z[l] = W[l]a[l-1] + b[l]<br/>a[l] = g[l](a[l])</span></pre><p id="d9b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以对“m”训练示例的这些步骤进行矢量化，如下所示:</p><pre class="je jf jg jh fd nd ne nf ng aw nh bi"><span id="5962" class="lj jp hi ne b fi ni nj l nk nl">Z[l] = W[l]A[l-1] + B[l]<br/>A[l] = g[l](A[l])</span></pre><p id="540e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一层的输出作为下一层的输入。如果没有for循环，我们无法计算神经网络所有层的前向传播，所以这里有for循环就可以了。在继续之前，让我们看看各种矩阵的维数，这将有助于我们更好地理解这些步骤。</p><h2 id="f652" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">获得正确的矩阵维度</h2><p id="9576" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">分析矩阵的维数是检查代码正确性的最佳调试工具之一。我们将在本节讨论每个矩阵的正确维度。考虑下面的例子:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es of"><img src="../Images/349c202910a72d8bd3ec845d711b13e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/0*xJd7G4-yWI02_QAu.png"/></div></figure><p id="4a4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你能算出这个神经网络的层数(L)吗？如果你猜对了5，你就答对了。有4个隐藏层和1个输出层。每层中的单位是:</p><p id="0c0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">n[0]= 2，n[1]= 3，n[2]= 5，n[3]= 4，n[4]= 2，n[5]= 1</p><p id="8a81" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">W、b及其导数的量纲的一般形式是:</p><ul class=""><li id="c50f" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">W[l]= (n[l]，n[l-1])</li><li id="122e" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">b[l]= (n[l]，1)</li><li id="5a14" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">dW[l]= (n[l]，n[l-1])</li><li id="9341" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">db[l]= (n[l]，1)</li><li id="d77c" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">Z[l]，A[l]，dZ[l]，dA[l]= (n[l]，m)的维数</li></ul><p id="4b07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中“m”是训练样本的数量。这些是一些广义的矩阵维数，它们将帮助你平稳地运行你的代码。</p><p id="437e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，我们已经看到了深度神经网络的一些基础知识。但是为什么我们首先需要深层表现呢？既然有更简单的解决方案，为什么还要把事情复杂化呢？让我们来了解一下！</p><h2 id="02fe" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">为什么要深度交涉？</h2><p id="547b" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">在深度神经网络中，我们有大量的隐藏层。这些隐藏层实际上在做什么？为了理解这一点，请看下图:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es og"><img src="../Images/0604aecd4f24a26ff272c2046943ac7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/0*sQjHDVTT2Nf0vxq4.png"/></div></figure><p id="78ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度神经网络寻找与数据的关系(从简单到复杂的关系)。第一个隐藏层可能做的是，试图找到简单的功能，如识别上面图像中的边缘。随着我们深入网络，这些简单的功能组合在一起形成更复杂的功能，如识别人脸。利用深度神经网络的一些常见示例有:</p><ul class=""><li id="6ff1" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">人脸识别<br/>图像== &gt;边缘== &gt;人脸部分== &gt;人脸== &gt;想要的人脸</li><li id="ab57" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">音频识别<br/>音频== &gt;低级声音特征，如(sss，bb) == &gt;音素== &gt;单词== &gt;句子</li></ul><h2 id="0c5a" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">深度神经网络的构建模块</h2><p id="e476" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">考虑深度神经网络中的任何一层。该层的输入将是来自前一层(l-1)的激活，并且该层的输出将是它自己的激活。</p><ul class=""><li id="53e9" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">输入:a[l-1]</li><li id="34af" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">输出:a[l]</li></ul><p id="e69f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该层首先计算应用激活的z[l]。这个z[l]被保存为缓存。对于反向传播步骤，它将首先计算da[l]，即层l处的激活的导数、权重dw[l]、db[l]、dz[l]的导数，最后是da[l-1]。让我们将这些步骤形象化，以降低复杂性:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es oh"><img src="../Images/d0a1e75ca890a3ed5b92b646601a1320.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/0*gZRgeV6MpnTGJexk.png"/></div></div></figure><p id="aea1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度神经网络的每个块(层)就是这样工作的。接下来，我们将看到如何实现所有这些模块。</p><h2 id="2b06" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">向前和向后传播</h2><p id="861b" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">前向传播步骤中的输入是a[l-1]，输出是a[l]和高速缓存z[l]，它是w[l]和b[l]的函数。因此，计算Z[l]和A[l]的矢量化形式为:</p><p id="41ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Z[l]= W[l]* A[l-1]+ b[l]</p><p id="8e4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">A[l]= g[l](Z[l])</p><p id="aeab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将计算网络每一层的Z和A。计算激活后，下一步是反向传播，我们使用导数更新权重。反向传播的输入为da[l]，输出为da[l-1]、dW[l]和db[l]。让我们看看反向传播的矢量化方程:</p><pre class="je jf jg jh fd nd ne nf ng aw nh bi"><span id="1e61" class="lj jp hi ne b fi ni nj l nk nl">dZ[l] = dA[l] * g'[l](Z[l])<br/>dW[l] = 1/m * (dZ[l] * A[l-1].T)<br/>db[l] = 1/m * np.sum(dZ[l], axis = 1, keepdims = True)<br/><br/>dA[l-1] = w[l].T * dZ[l]</span></pre><p id="4ae0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是我们实现深度神经网络的方式。</p><p id="63eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度神经网络的表现令人惊讶地好(如果你以前使用过它们，可能就不那么令人惊讶了！).只运行几行代码就能得到令人满意的结果。这是因为我们正在向网络提供大量数据，而网络正在使用隐藏层从这些数据中学习。</p><p id="3023" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择正确的超参数有助于我们提高模型的效率。我们将在本系列的下一篇文章中讨论超参数调优的细节。</p><h2 id="28ac" class="lj jp hi bd jq lk ll lm ju ln lo lp jy iq lq lr kc iu ls lt kg iy lu lv kk lw bi translated">参数与超参数</h2><p id="6d68" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">这是深度学习新人经常问的问题。参数和超参数之间的主要区别在于，模型在训练期间学习参数，而超参数可以在训练模型之前改变。</p><p id="5bd8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度神经网络的参数是W和b，模型在反向传播步骤中更新它们。另一方面，深度神经网络有许多超参数，包括:</p><ul class=""><li id="8479" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc li ku kv kw bi translated">学习率——⍺</li><li id="013e" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">迭代次数</li><li id="c00e" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">隐藏层数</li><li id="3268" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">每个隐藏层中的单位</li><li id="7705" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc li ku kv kw bi translated">激活函数的选择</li></ul><p id="b278" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是对这两个方面的区别的简要概述。我很乐意在下面的评论区回答你对此可能有的任何问题。</p><h1 id="e327" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结束注释</h1><p id="77bd" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">祝贺您完成本专业的第一门课程！我们现在知道如何实现深度神经网络的前向和后向传播以及梯度下降。我们还看到了矢量化如何帮助我们摆脱显式for循环，使我们的代码在这个过程中变得高效。</p><p id="e914" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下一篇文章中(将涵盖课程#2)，我们将看到如何通过超参数调整、正则化和优化来改善深度神经网络。这是深度学习中更加棘手和迷人的方面之一。</p><p id="3a97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你有任何反馈或任何疑问/问题，请在下面的评论区分享。期待听到大家的想法！</p></div><div class="ab cl oi oj gp ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="hb hc hd he hf"><p id="4a81" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jn">原载于2018年10月21日</em><a class="ae op" href="https://www.analyticsvidhya.com/blog/2018/10/introduction-neural-networks-deep-learning/" rel="noopener ugc nofollow" target="_blank"><em class="jn">【www.analyticsvidhya.com】</em></a><em class="jn">。</em></p></div></div>    
</body>
</html>