# SHAP (SHapley é™„åŠ è§£é‡Š)å’Œ LIME(å±€éƒ¨å¯è§£é‡Šæ¨¡å‹ä¸å¯çŸ¥è§£é‡Š)ç”¨äºæ¨¡å‹å¯è§£é‡Šæ€§ã€‚

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/shap-shapley-additive-explanations-and-lime-local-interpretable-model-agnostic-explanations-8c0aa33e91f?source=collection_archive---------4----------------------->

![](img/5e932ec926e86e2901db176af714300c.png)

ä¸ºä»€ä¹ˆæ¨¡å‹çš„å¯è§£é‡Šæ€§å¦‚æ­¤é‡è¦ï¼Ÿ

æ¨¡æ‹Ÿæ™ºèƒ½åœ¨æœŸæœ›ç²¾åº¦ã€æµ‹é‡ç”Ÿå­˜èƒ½åŠ›å’Œè¯„ä¼°ç†Ÿç»ƒç¨‹åº¦æ–¹é¢ä»¤äººå…´å¥‹ã€‚æ— è®ºå¦‚ä½•ï¼Œè®¡ç®—æœºé€šå¸¸ä¸ä¼šè§£é‡Šå®ƒä»¬çš„æ¬²æœ›ã€‚è¿™è½¬åŒ–ä¸ºå¯¹äººå·¥æ™ºèƒ½æ¨¡å‹èšé›†çš„é™åˆ¶ã€‚å¦‚æœå®¢æˆ·ä¸ä¿¡ä»»æŸä¸ªå‹å·æˆ–éœ€æ±‚ï¼Œä»–ä»¬å°±ä¸ä¼šä½¿ç”¨æˆ–å‘é€å®ƒã€‚å› æ­¤ï¼Œé—®é¢˜æ˜¯å¸®åŠ©å®¢æˆ·ä¿¡ä»»æ¨¡å‹çš„æ–¹æ³•ã€‚

è™½ç„¶è¾ƒç®€å•ç±»åˆ«çš„æ¨¡å‹(å¦‚çº¿æ€§æ¨¡å‹å’Œå†³ç­–æ ‘)é€šå¸¸å®¹æ˜“è¢«äººç±»ç†è§£ï¼Œä½†å¯¹äºå¤æ‚æ¨¡å‹(å¦‚é›†æˆæ–¹æ³•ã€æ·±åº¦ç¥ç»ç½‘ç»œ)æ¥è¯´ï¼Œæƒ…å†µå¹¶éå¦‚æ­¤ã€‚è¿™ç§å¤æ‚çš„æ¨¡å‹å®é™…ä¸Šæ˜¯é»‘ç›’ã€‚ç†è§£è¿™ç§åˆ†ç±»å™¨è¡Œä¸ºçš„ä¸€ç§æ–¹æ³•æ˜¯å»ºç«‹æ›´ç®€å•çš„è§£é‡Šæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æ˜¯è¿™äº›é»‘ç›’çš„å¯è§£é‡Šè¿‘ä¼¼ã€‚

ä¸ºæ­¤ï¼Œåœ¨ç°æœ‰æ–‡çŒ®ä¸­å·²ç»æå‡ºäº†å‡ ç§æŠ€æœ¯ã€‚è±å§†å’Œ SHAP æ˜¯ä¸¤ç§æµè¡Œçš„æ¨¡å‹ä¸å¯çŸ¥çš„å±€éƒ¨è§£é‡Šæ–¹æ³•ï¼Œæ—¨åœ¨è§£é‡Šä»»ä½•ç»™å®šçš„é»‘ç›’åˆ†ç±»å™¨ã€‚è¿™äº›æ–¹æ³•é€šè¿‡åœ¨æ¯ä¸ªé¢„æµ‹å‘¨å›´å±€éƒ¨åœ°å­¦ä¹ å¯è§£é‡Šçš„æ¨¡å‹(ä¾‹å¦‚ï¼Œçº¿æ€§æ¨¡å‹),ä»¥å¯è§£é‡Šçš„å’Œå¿ å®çš„æ–¹å¼è§£é‡Šä»»ä½•åˆ†ç±»å™¨çš„å„ä¸ªé¢„æµ‹ã€‚å…·ä½“æ¥è¯´ï¼Œè±å§†å’Œ SHAP ä¼°è®¡ä¸ªä½“å®ä¾‹çš„ç‰¹å¾å±æ€§ï¼Œè¿™æ•è·äº†æ¯ä¸ªç‰¹å¾å¯¹é»‘ç›’é¢„æµ‹çš„è´¡çŒ®ã€‚ä¸‹é¢ï¼Œæˆ‘ä»¬æä¾›äº†è¿™äº›æ–¹æ³•çš„ä¸€äº›ç»†èŠ‚ï¼ŒåŒæ—¶ä¹Ÿå¼ºè°ƒäº†å®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚

## ä»€ä¹ˆæ˜¯çŸ³ç°ï¼Ÿ

LIME(å±€éƒ¨å¯è§£é‡Šæ¨¡å‹ä¸å¯çŸ¥è§£é‡Š)æ˜¯ä¸€ç§æ–°é¢–çš„è§£é‡ŠæŠ€æœ¯ï¼Œå®ƒé€šè¿‡å­¦ä¹ é¢„æµ‹å‘¨å›´çš„å±€éƒ¨å¯è§£é‡Šæ¨¡å‹ï¼Œä»¥å¯è§£é‡Šå’Œå¿ å®çš„æ–¹å¼è§£é‡Šä»»ä½•åˆ†ç±»å™¨çš„é¢„æµ‹ã€‚

## ä»€ä¹ˆæ˜¯ SHAPï¼Ÿ

SHapley ä»£è¡¨æ²™æ™®åˆ©é™„åŠ è§£é‡Šâ€”â€”å¾ˆå¯èƒ½æ˜¯æœºå™¨å­¦ä¹ åˆç†æ€§çš„å‰æ²¿ã€‚Lundberg å’Œ Lee åœ¨ 2017 å¹´é¦–æ¬¡å‘å¸ƒäº†è¿™ç§è®¡ç®—æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§è®¡ç®—ä»»ä½•æœ‰å…ˆè§ä¹‹æ˜çš„è®¡ç®—ç»“æœçš„æå¥½æ–¹æ³•ã€‚

æ— è®ºä½ æœ‰ä¸€ä¸ªä»¤äººéš¾ä»¥ç½®ä¿¡çš„æ¨¡å‹(å¯èƒ½æ˜¯æ¢¯åº¦æ¨è¿›ã€ç¥ç»ç½‘ç»œæˆ–ä»»ä½•å°†ä¸€äº›ç‰¹å¾ä½œä¸ºè¾“å…¥å¹¶äº§ç”Ÿä¸€äº›é¢„æµ‹ä½œä¸ºè¾“å‡ºçš„ä¸œè¥¿)ï¼Œä½ éƒ½éœ€è¦ç†è§£æ¨¡å‹æ­£åœ¨åšå‡ºä»€ä¹ˆé€‰æ‹©ï¼ŒSHAP æ¨å´‡å¤‡è‡³ã€‚

åˆ©ç”¨é…’åº—è¯„è®ºåˆ†ç±»æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†å»ºç«‹ä¸€ä¸ªå¤šç±»æ–‡æœ¬åˆ†ç±»æ¨¡å‹ï¼Œç„¶ååˆ†åˆ«åº”ç”¨*çŸ³ç°* & *SHAP* æ¥è§£é‡Šè¯¥æ¨¡å‹ã€‚å› ä¸ºæˆ‘ä»¬ä»¥å‰å·²ç»åšè¿‡å¤šæ¬¡æ–‡æœ¬åˆ†ç±»ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å¿«é€Ÿæ„å»º NLP æ¨¡å‹ï¼Œå¹¶å…³æ³¨æ¨¡å‹çš„å¯è§£é‡Šæ€§ã€‚

[*LIME*](https://github.com/marcotcr/lime)&[*SHAP*](https://github.com/slundberg/shap)å¸®åŠ©æˆ‘ä»¬ä¸ä»…å‘æœ€ç»ˆç”¨æˆ·ï¼Œä¹Ÿå‘æˆ‘ä»¬è‡ªå·±è§£é‡Š NLP æ¨¡å‹æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚

# æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹å’Œé€»è¾‘å›å½’:

ä»»ä½•æœºå™¨å­¦ä¹ æ¨¡å‹æœ€é‡è¦çš„éƒ¨åˆ†æ˜¯ä»æ•°æ®é›†ä¸­è·å¾—æœ€ä½³æ•ˆæœ

```
import pandas as pd
import numpy as np
import sklearn
import sklearn.ensemble
import sklearn.metrics
from sklearn.utils import shuffleimport refrom nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
import lime
from lime import lime_text
from lime.lime_text import LimeTextExplainer
from sklearn.pipeline import make_pipelinedf = pd.read_csv('hotel-reviews.csv')
df.head()df = df[pd.notnull(df['Browser_Used'])]
df = df.sample(frac=0.5, random_state=99).reset_index(drop=True)
df = shuffle(df, random_state=22)
df = df.reset_index(drop=True)
df['class_label'] = df['Browser_Used'].factorize()[0]
class_label_df = df[['Browser_Used', 'class_label']].drop_duplicates().sort_values('class_label')
label_to_id = dict(class_label_df.values)
id_to_label = dict(class_label_df[['class_label', 'Browser_Used']].values)
```

æ–‡æœ¬æ¸…æ´—å’Œé¢„å¤„ç†ã€‚ä¸ºæ›´å¥½çš„æ¨¡å‹è§£é‡Šç”Ÿæˆå¹²å‡€çš„ CSV

```
REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))def clean_text(text):
    """
        text: a string

        return: modified initial string
    """
   # text = BeautifulSoup(text, "lxml").text # HTML decoding. BeautifulSoup's text attribute will return a string stripped of any HTML tags and metadata.
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.
    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. 
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text
    return text

df['Description'] = df['Description'].apply(clean_text)
df['class_label'].value_counts()
```

# ç”¨ LIME è§£é‡Šæ–‡æœ¬é¢„æµ‹

è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è§[çŸ³ç°æ•™ç¨‹](https://marcotcr.github.io/lime/tutorials/Lime%20-%20multiclass.html)ã€‚

```
c = make_pipeline(vectorizer, logreg)
class_names=list(df.Browser_Used.unique())
explainer = LimeTextExplainer(class_names=class_names)
```

ç›®æ ‡ä¸æ˜¯äº§ç”Ÿæ›´é«˜çš„ç»“æœï¼Œè€Œæ˜¯æ›´å¥½çš„åˆ†æã€‚

```
print ('Explanation for class %s' % class_names[0])
print ('\n'.join(map(str, exp.as_list(label=1))))
```

![](img/3d0d755d3e4fbc867aeac44fd1ed5685.png)

å®ƒç»™äº†æˆ‘ä»¬***ie æµè§ˆå™¨*** å’Œ ***Firefoxã€‚***

```
exp.show_in_notebook(text=False)
```

![](img/ca15ea0e59ed8f7b035230bae0533523.png)

è®©æˆ‘è¯•ç€è§£é‡Šä¸€ä¸‹è¿™ç§å½¢è±¡åŒ–:

*   å¯¹äºè¯¥æ–‡æ¡£ï¼Œå•è¯â€œç»„â€å¯¹äºç±»åˆ«***Internet Explorer***å…·æœ‰æœ€é«˜çš„æ­£é¢å¾—åˆ†ã€‚
*   æˆ‘ä»¬çš„æ¨¡å‹é¢„æµ‹è¯¥æ–‡æ¡£åº”è¯¥ä»¥ 0.25%çš„æ¦‚ç‡è¢«æ ‡è®°ä¸º ***ç»„*** ã€‚
*   å¦ä¸€æ–¹é¢ï¼Œeasy å¯¹äºç±» Firefox æ¥è¯´æ˜¯è´Ÿæ•°ï¼Œæˆ‘ä»¬çš„æ¨¡å‹å·²ç»äº†è§£åˆ°å•è¯â€œprivateâ€å¯¹äºç±» ***firefox æ¥è¯´æœ‰ä¸€ä¸ªå°çš„æ­£æ•°ã€‚***

# ç”¨ SHAP è§£é‡Šæ–‡æœ¬é¢„æµ‹

ä»¥ä¸‹è¿‡ç¨‹æ˜¯ä»[æœ¬æ•™ç¨‹](https://stackoverflow.blog/2019/05/06/predicting-stack-overflow-tags-with-googles-cloud-ai/)ä¸­å¾—çŸ¥çš„ã€‚

```
from sklearn.preprocessing import MultiLabelBinarizer
import tensorflow as tf
from tensorflow.keras.preprocessing import text
import keras.backend.tensorflow_backend as K
K.set_session
import shaptags_split = [tags.split(',') for tags in df['Browser_Used'].values]
print(tags_split[:10])tag_encoder = MultiLabelBinarizer()
tags_encoded = tag_encoder.fit_transform(tags_split)
num_tags = len(tags_encoded[0])
print(df['Description'].values[0])
print(tag_encoder.classes_)
print(tags_encoded[0])train_size = int(len(df) * .8)
print('train size: %d' % train_size)
print('test size: %d' % (len(df) - train_size))y_train = tags_encoded[: train_size]
y_test = tags_encoded[train_size:]class TextPreprocessor(object):
    def __init__(self, vocab_size):
        self._vocab_size = vocab_size
        self._tokenizer = None
    def create_tokenizer(self, text_list):
        tokenizer = text.Tokenizer(num_words = self._vocab_size)
        tokenizer.fit_on_texts(text_list)
        self._tokenizer = tokenizer
    def transform_text(self, text_list):
        text_matrix = self._tokenizer.texts_to_matrix(text_list)
        return text_matrixmodel.fit(X_train, y_train, epochs = 2, batch_size=128, validation_split=0.1)
print('Eval loss/accuracy:{}'.format(model.evaluate(X_test, y_test, batch_size = 128)))
```

*   åœ¨æ¨¡å‹å‡†å¤‡å¥½ä¹‹åï¼Œæˆ‘ä»¬åˆ©ç”¨æœ€åˆçš„ 200 æ¡å‡†å¤‡è®°å½•ä½œä¸ºæˆ‘ä»¬çš„ç»éªŒä¿¡æ¯é›†åˆæ¥æ•´åˆå¹¶åˆ¶ä½œä¸€ä¸ª SHAP è§£é‡Šå™¨å¯¹è±¡ã€‚
*   æˆ‘ä»¬å¾—åˆ°äº†å¯¹æµ‹è¯•é›†å­é›†çš„å¥‡å¼‚æœŸæœ›çš„å½’å› å°Šé‡ã€‚
*   å°†ç´¢å¼•è½¬æ¢ä¸ºå•è¯ã€‚
*   åˆ©ç”¨ SHAP çš„ **summary_plot** æŠ€å·§æ¥å±•ç¤ºå½±å“æ¨¡å‹é¢„æœŸçš„ä¸»è¦äº®ç‚¹ã€‚

```
attrib_data = X_train[:200]
explainer = shap.DeepExplainer(model, attrib_data)
num_explanations = 40
shap_vals = explainer.shap_values(X_test[:num_explanations])words = processor._tokenizer.word_index
word_lookup = list()
for i in words.keys():
  word_lookup.append(i)word_lookup = [''] + word_lookup
shap.summary_plot(shap_vals, feature_names=word_lookup, class_names=tag_encoder.classes_)
```

![](img/bf36b938229d10c13c436df9e972a736.png)

*   å•è¯â€œhotelâ€æ˜¯æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨çš„æœ€å¤§ä¿¡å·å•è¯ï¼Œå¯¹ç±»åˆ« ***Edgw*** é¢„æµ‹è´¡çŒ®æœ€å¤§ã€‚
*   å•è¯â€œroomsâ€æ˜¯æˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨çš„ç¬¬å››å¤§ä¿¡å·è¯ï¼Œå½“ç„¶å¯¹ç±» ***firefox*** è´¡çŒ®æœ€å¤§ã€‚

å°±æœºå™¨å­¦ä¹ çš„å¯è§£é‡Šæ€§è€Œè¨€ï¼Œè±å§†& SHAP æœ‰å¾ˆå¤šä¸œè¥¿éœ€è¦å­¦ä¹ ã€‚

å¸Œæœ›è¿™æœ‰æ‰€å¸®åŠ©:)

å¦‚æœä½ å–œæ¬¢æˆ‘çš„å¸–å­ï¼Œè¯·å…³æ³¨ã€‚*å¦‚éœ€æ›´å¤šå¸®åŠ©ï¼Œè¯·æŸ¥çœ‹æˆ‘çš„* [*Github*](https://github.com/Afaf-Athar/NLP_With_Python)

é€šè¿‡*LinkedIn*[https://www.linkedin.com/in/afaf-athar-183621105/è¿æ¥](https://www.linkedin.com/in/afaf-athar-183621105/)

å¿«ä¹å­¦ä¹ ğŸ˜ƒ