<html>
<head>
<title>Decision Tree-End to End Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树-端到端实施</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decision-tree-end-to-end-implementation-adf1bc246254?source=collection_archive---------7-----------------------#2020-10-27">https://medium.com/analytics-vidhya/decision-tree-end-to-end-implementation-adf1bc246254?source=collection_archive---------7-----------------------#2020-10-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="79ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗯，在这个博客中，我非常兴奋能够从决策树这个概念开始。我将讨论对决策树的端到端理解。所以让我们先理解它，并使用 python 实现它</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/bd3f9bb77107e2288189e8bcd8e764f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cFtZ9zHBjczckMUE"/></div></div></figure><h1 id="d75e" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">什么是决策树？</h1><ul class=""><li id="178c" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc ku kv kw kx bi translated">决策树是一种<strong class="ih hj">监督学习技术</strong>，可用于分类和回归问题，但大多数情况下，它更适用于解决分类问题。它是一个树形结构的分类器，其中<strong class="ih hj">内部节点代表数据集的特征，分支代表决策规则</strong>和<strong class="ih hj">每个叶子节点代表结果。</strong></li><li id="0964" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">决策树中有两个节点，分别是<strong class="ih hj">决策节点</strong>和<strong class="ih hj">叶节点。</strong>决策节点用于做出任何决策并具有多个分支，而叶节点是这些决策的输出，不包含任何进一步的分支。</li><li id="41f3" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">基于给定数据集的特征来执行决策或测试。</li></ul><blockquote class="ld le lf"><p id="d0fe" class="if ig lg ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="hi">它是一种图形表示，用于根据给定的条件获得问题/决策的所有可能的解决方案。</em>T13】</strong></p></blockquote><ul class=""><li id="158f" class="kn ko hi ih b ii ij im in iq lk iu ll iy lm jc ku kv kw kx bi translated">它之所以被称为决策树，是因为与树类似，它从根节点开始，然后扩展到更多的分支并构建一个树状结构。</li><li id="5aac" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">为了构建树，我们使用<strong class="ih hj"> CART 算法，</strong>代表<strong class="ih hj">分类和回归树算法。</strong></li><li id="5055" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">决策树简单地问一个问题，并根据答案(是/否)，进一步将树分成子树。</li><li id="1e21" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">下图解释了决策树的一般结构:</li></ul><h2 id="49d0" class="ln jq hi bd jr lo lp lq jv lr ls lt jz iq lu lv kd iu lw lx kh iy ly lz kl ma bi translated">注意:决策树可以包含分类数据(是/否)以及数字数据。</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mb"><img src="../Images/fbc1d7707116ae83988e6b04c74cc5d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*IWrLQKKfTzZYpdMA.png"/></div></figure><h1 id="4a14" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">为什么要使用决策树？</h1><p id="9c6c" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq mc is it iu md iw ix iy me ja jb jc hb bi translated">机器学习中有各种算法，因此为给定的数据集和问题选择最佳算法是创建机器学习模型时要记住的要点。下面是使用决策树的两个原因:</p><ul class=""><li id="de0e" class="kn ko hi ih b ii ij im in iq lk iu ll iy lm jc ku kv kw kx bi translated">决策树通常在做出决策时模仿人类的思维能力，因此很容易理解。</li><li id="bbba" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">决策树背后的逻辑很容易理解，因为它显示了一个树状结构。</li></ul><h1 id="049b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">决策树术语</h1><ul class=""><li id="d789" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc ku kv kw kx bi translated"><strong class="ih hj">根节点:</strong>根节点是决策树开始的地方。它代表整个数据集，而数据集又被进一步分成两个或更多同类的集合。</li><li id="a82b" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">叶节点:</strong>叶节点是最终的输出节点，得到一个叶节点后树就不能再分了。</li><li id="095d" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">分裂:</strong>分裂是将决策节点/根节点按照给定的条件划分为子节点的过程。</li><li id="9d16" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">分支/子树:</strong>将树劈开形成的树。</li><li id="1fd2" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">修枝:</strong>修枝是把树上不需要的树枝去掉的过程。</li><li id="d27e" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">父/子节点:</strong>树的根节点称为父节点，其他节点称为子节点。</li></ul><p id="0595" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">决策树算法是如何工作的？</strong></p><p id="bbc0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在决策树中，为了预测给定数据集的类别，算法从树的根节点开始。该算法将根属性的值与记录(真实数据集)属性的值进行比较，并根据比较结果，沿着分支跳转到下一个节点。</p><p id="18ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于下一个节点，该算法再次将属性值与其他子节点进行比较，并进一步移动。它继续这个过程，直到到达树的叶节点。使用以下算法可以更好地理解整个过程:</p><ul class=""><li id="87af" class="kn ko hi ih b ii ij im in iq lk iu ll iy lm jc ku kv kw kx bi translated"><strong class="ih hj">步骤 1: </strong>从根节点开始树，S 表示，它包含完整的数据集。</li><li id="057b" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">步骤 2: </strong>使用<strong class="ih hj">属性选择度量(ASM)在数据集中找到最佳属性。</strong></li><li id="384d" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">步骤 3: </strong>将 S 分成包含最佳属性的可能值的子集。</li><li id="e5ec" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">第四步:</strong>生成决策树节点，包含最佳属性。</li><li id="720a" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">步骤 5: </strong>使用步骤 3 中创建的数据集的子集递归地生成新的决策树。继续这个过程，直到到达不能再对节点进行分类的阶段，并将最后一个节点称为叶节点。</li></ul><p id="45d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">例子:</strong>假设有一个应聘者，他有一份工作邀约，想要决定自己是否应该接受这个邀约。所以，为了解决这个问题，决策树从根节点(ASM 的 Salary 属性)开始。根节点根据相应的标签进一步分为下一个决策节点(离办公室的距离)和一个叶节点。下一个决策节点进一步分成一个决策节点(Cab 工具)和一个叶节点。最后，决策节点分成两个叶节点(接受的提议和拒绝的提议)。考虑下图:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mf"><img src="../Images/a2b624c1501ed38006287aa4ab04a90b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*T-9E6AStgonuYxOs.png"/></div></figure><h1 id="d1d5" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">属性选择度量</h1><p id="a8d1" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq mc is it iu md iw ix iy me ja jb jc hb bi translated">在实现决策树时，主要问题是如何为根节点和子节点选择最佳属性。因此，有一种被称为<strong class="ih hj">属性选择度量或 ASM 的技术来解决这样的问题。</strong>通过这种度量，我们可以很容易地为树的节点选择最佳属性。ASM 有两种流行的技术，它们是:</p><ul class=""><li id="e58d" class="kn ko hi ih b ii ij im in iq lk iu ll iy lm jc ku kv kw kx bi translated"><strong class="ih hj">信息增益</strong></li><li id="f471" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">基尼指数</strong></li></ul><h1 id="45ee" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">1.信息增益:</h1><ul class=""><li id="0de0" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc ku kv kw kx bi translated">信息增益是基于属性对数据集进行分段后熵变化的度量。</li><li id="b541" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">它计算一个特性为我们提供了多少关于一个类的信息。</li><li id="d645" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">根据信息增益的大小，对节点进行分割，构建决策树。</li><li id="d98d" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">决策树算法总是试图最大化信息增益的值，并且首先分裂具有最高信息增益的节点/属性。可以使用以下公式进行计算:</li></ul><p id="e8e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">熵:</strong>熵是衡量给定属性中杂质的度量。它规定了数据的随机性。熵可以计算为:</p><pre class="je jf jg jh fd mg mh mi mj aw mk bi"><span id="cd09" class="ln jq hi mh b fi ml mm l mn mo">Entropy(s)= -P(yes)log2 P(yes)- P(no) log2 P(no)</span></pre><p id="7e60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">哪里，</strong></p><ul class=""><li id="6ebc" class="kn ko hi ih b ii ij im in iq lk iu ll iy lm jc ku kv kw kx bi translated"><strong class="ih hj"> S=样本总数</strong></li><li id="7bae" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj"> P(是)=是的概率</strong></li><li id="02c9" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj"> P(否)=否的概率</strong></li></ul><h1 id="af59" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">2.基尼指数:</h1><ul class=""><li id="78d4" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc ku kv kw kx bi translated">基尼指数是在 CART(分类和回归树)算法中创建决策树时使用的杂质或纯度的度量。</li><li id="b25f" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">与高基尼指数相比，低基尼指数的属性应该是优选的。</li><li id="b1d3" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">它只创建二进制拆分，而 CART 算法使用基尼指数来创建二进制拆分。</li><li id="aa26" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">基尼指数可以用下面的公式计算:</li></ul><pre class="je jf jg jh fd mg mh mi mj aw mk bi"><span id="dce4" class="ln jq hi mh b fi ml mm l mn mo">Gini Index= 1- ∑jPj2</span></pre><h1 id="491e" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">修剪:获得最佳决策树</h1><p id="22c9" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq mc is it iu md iw ix iy me ja jb jc hb bi translated"><em class="lg">剪枝是从树上删除不必要的节点，以得到最优决策树的过程。</em></p><p id="182e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">太大的树会增加过度拟合的风险，而小的树可能无法捕获数据集的所有重要特征。因此，一种在不降低准确性的情况下减小学习树大小的技术被称为修剪。主要使用两种类型的树<strong class="ih hj">修剪</strong>技术:</p><ul class=""><li id="80dc" class="kn ko hi ih b ii ij im in iq lk iu ll iy lm jc ku kv kw kx bi translated"><strong class="ih hj">成本复杂性修剪</strong></li><li id="9e30" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">减少了错误修剪。</strong></li></ul><h1 id="d1c3" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">决策树的优势</h1><ul class=""><li id="e42b" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc ku kv kw kx bi translated">清晰的可视化:该算法易于理解、解释和可视化，因为这个想法在我们的日常生活中经常使用。决策树的输出很容易被人理解。</li><li id="c0e6" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">简单易懂:决策树看起来像简单的 if-else 语句，非常容易理解。</li><li id="dd83" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">决策树可用于分类和回归问题。</li><li id="8ea6" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">决策树可以处理连续变量和分类变量。</li><li id="c5e4" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">不需要特征缩放:在决策树的情况下不需要特征缩放(标准化和规范化),因为它使用基于规则的方法而不是距离计算。</li><li id="c25b" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">有效处理非线性参数:与基于曲线的算法不同，非线性参数不会影响决策树的性能。因此，如果独立变量之间存在高度非线性，决策树可能会优于其他基于曲线的算法。</li><li id="1cbf" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">决策树可以自动处理缺失值。</li><li id="9e19" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">决策树通常对异常值是健壮的，并且可以自动处理它们。</li><li id="a58d" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">更少的训练周期:与随机森林相比，训练周期更少，因为它只生成一棵树，而不像随机森林中的树的森林。</li></ul><h1 id="ab3c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">决策树的缺点</h1><ul class=""><li id="cb10" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc ku kv kw kx bi translated">过度拟合:这是决策树的主要问题。这通常会导致数据的过度拟合，最终导致错误的预测。为了适应数据(即使是有噪声的数据)，它不断生成新的节点，最终树变得太复杂而无法解释。这样，它就失去了泛化能力。它在训练数据上表现很好，但在看不见的数据上开始犯很多错误。</li><li id="3fa8" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">高方差:如第 1 点所述，决策树通常会导致数据过拟合。由于过拟合，输出中高方差的机会非常高，这导致最终估计中的许多误差，并显示结果的高度不准确性。为了达到零偏差(过拟合)，导致方差高。</li><li id="b890" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">不稳定:添加新的数据点会导致整个树的重新生成，所有节点都需要重新计算和重新创建。</li><li id="9ba7" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">受噪音影响:一点点噪音都会使它不稳定，从而导致错误的预测。</li><li id="ede6" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">不适合大型数据集:如果数据量很大，那么一个单独的树可能会变得复杂并导致过度拟合。所以在这种情况下，我们应该使用随机森林，而不是单一的决策树。</li></ul><h1 id="f9ae" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">决策树的 Python 实现</h1><h2 id="57ab" class="ln jq hi bd jr lo lp lq jv lr ls lt jz iq lu lv kd iu lw lx kh iy ly lz kl ma bi translated">使用 SKLearn</h2><ol class=""><li id="1e5f" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc mp kv kw kx bi translated">#导入决策树分类器</li><li id="0e92" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc mp kv kw kx bi translated"><strong class="ih hj">从</strong> sklearn.tree <strong class="ih hj">导入</strong>决策树分类器</li><li id="7ce9" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc mp kv kw kx bi translated">#导入数据集</li><li id="0f65" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc mp kv kw kx bi translated">dataset = pd.read_csv('zoo.csv '，names=['animal_name '，' hair '，' feathers '，' eggs '，' milk '，' airbone '，' aquatic '，' predator '，'齿状'，'脊椎'，'呼吸'，'有毒'，'鳍'，'腿'，'尾巴'，'家养'，'猫大小'，'，])</li><li id="2465" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc mp kv kw kx bi translated">#我们放弃了动物名称，因为这不是分割数据的好特征</li><li id="4801" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc mp kv kw kx bi translated">dataset = dataset . drop(' animal _ name '，轴=1)</li></ol><p id="0bdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7.train _ features = dataset . iloc[:80，:-1]</p><p id="bd20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">8 . test _ features = dataset . iloc[80:，-1]</p><p id="5125" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">9 . train _ targets = dataset . iloc[:80，-1]</p><p id="2328" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">10 . test _ targets = dataset . iloc[80:，-1]</p><p id="6b49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">11 . tree = decision tree classifier(criteria = ' entropy ')。拟合(训练特征，训练目标)</p><p id="6e31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">预测= tree.predict(test_features)</p><p id="82f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 13.print </strong>(“预测精度为: "，tree.score(test_features，test_targets)*100，" % ")</p><p id="6dee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我得到的输出是:</p><p id="12a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">预测准确率为:80.95238095238095 %。</p><h1 id="28da" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">结论:</h1><p id="838a" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq mc is it iu md iw ix iy me ja jb jc hb bi translated">感谢阅读！将来我也会写更多的机器学习文章。<strong class="ih hj">关注<strong class="ih hj">媒体</strong>上的</strong> me，了解他们。我也是一名自由职业者，如果有一些数据相关项目的自由职业工作，请随时联系 Linkedin。没有什么比做真正的项目更好的了！</p><h1 id="c367" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">如果你喜欢这篇文章，请鼓掌！</h1></div></div>    
</body>
</html>