<html>
<head>
<title>News Data Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">新闻数据分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/news-data-classification-61c04721d30c?source=collection_archive---------19-----------------------#2020-03-28">https://medium.com/analytics-vidhya/news-data-classification-61c04721d30c?source=collection_archive---------19-----------------------#2020-03-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d4e4048ea0d350a984268f1f3e5cbec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kBk-pXJrVU3AMLIqhzemeA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">用以上标签对新闻标题进行分类</figcaption></figure><h1 id="8bca" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">目标</h1><p id="a0a2" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">这个项目可以在标题的基础上直接给新闻数据添加一个类别。这是使用word2vec、逻辑回归、nltk等技术实现的。</p><h1 id="2fc2" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">资料组</h1><p id="f4bf" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">该数据集包含2014年3月10日至2014年8月10日期间由网络聚合器收集的422，937条新闻故事的标题、URL和类别。该数据集中的新闻类别包括商业、科技、娱乐和健康。涉及同一新闻条目的不同新闻文章(例如，关于最近发布的就业统计的几篇文章)也被分类在一起。新闻出版商包括经济时报、路透社、华尔街日报等。原始数据集可以从下面的链接下载。<br/><a class="ae kq" href="https://www.kaggle.com/arjunchandrasekhara/news-classification/data" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/arjunchandrasekhara/news-classification/data</a></p><h1 id="457a" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">预训练word2vec模型</h1><p id="8ac6" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">该项目使用谷歌训练的word2vec模型。这个模型是专门在谷歌的新闻数据集上训练的。word2vec模型用于获得每个单词的300维向量。基于该模型去除停用词和词汇表外的词。标题中剩余单词的300维向量是通过获得单词的元素方式的平均值而获得的。特征中单个记录的形状为(300，1)。</p><h1 id="5100" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">项目详情</h1><p id="e426" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">5万条干净记录作为特征。干净的记录意味着记录中没有停用词，没有词汇和标点符号。这些记录有干净的标签，属于b类(商业)，t类(技术)，m类(医学)，e类(娱乐)。对于每个标题，记录具有300维向量，并且其各自的标签具有单个字符。特征的形状为50000*300，标签的形状为50000*1。训练集与测试集的比例为4:1。即用于训练模型的40，000条记录和用于验证模型的10，000条记录。</p><h1 id="0f90" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">密码</h1><p id="95b5" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">下载原始和原始csv数据的回购。进入目录并签出到开发文件夹。然后，下载训练好的word2vec模型并解压。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="66fe" class="la iv hi kw b fi lb lc l ld le">!git clone <a class="ae kq" href="https://github.com/omkarsk98/NewsDataClassification.git" rel="noopener ugc nofollow" target="_blank">https://github.com/omkarsk98/NewsDataClassification.git</a><br/>%cd NewsDataClassification<br/>!git checkout development<br/>!wget -c "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"<br/>!gunzip GoogleNews-vectors-negative300.bin.gz</span></pre><p id="9a41" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">下载nltk库和依赖项，并导入所有需要的库。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="8d23" class="la iv hi kw b fi lb lc l ld le">import nltk<br/>nltk.download('punkt')<br/>nltk.download('stopwords')</span><span id="2d01" class="la iv hi kw b fi lk lc l ld le">import pandas as pd<br/>from gensim import models<br/>from sklearn.linear_model import LogisticRegression<br/>import numpy as np<br/>from nltk.tokenize import word_tokenize<br/>from nltk.corpus import stopwords<br/>from sklearn.metrics import accuracy_score ,confusion_matrix<br/>import time<br/>from sklearn.manifold import TSNE<br/>from google.colab import files</span></pre><p id="53ef" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">读取原始数据并设置要使用的最大记录数。此外，对所有单词进行标记，并获得英文的停用词，并定义所有可用于后续阶段的函数。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="aeec" class="la iv hi kw b fi lb lc l ld le"># read csv<br/>main_data = pd.read_csv('News_Final.csv')</span><span id="0ac1" class="la iv hi kw b fi lk lc l ld le"># read titles from it<br/>article_titles = main_data['TITLE']<br/>labels = main_data["CATEGORY"]</span><span id="1e48" class="la iv hi kw b fi lk lc l ld le"># Create a list of strings, one for each title<br/>titles_list = [title for title in article_titles]</span><span id="8d56" class="la iv hi kw b fi lk lc l ld le"># form a single string fro the list of strings<br/>big_title_string = ' '.join(titles_list)</span><span id="59a6" class="la iv hi kw b fi lk lc l ld le"># define total records to be considered for analysis<br/>total = 50000</span><span id="776c" class="la iv hi kw b fi lk lc l ld le"># Tokenize the string into words<br/>tokens = word_tokenize(big_title_string)</span><span id="aa43" class="la iv hi kw b fi lk lc l ld le"># Remove non-alphabetic tokens, such as punctuation<br/>words = [word.lower() for word in tokens if word.isalpha()]<br/>stop_words = set(stopwords.words('english'))</span></pre><p id="678b" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">定义所有的辅助函数来获得每个文档的向量，标记和删除停用词，在预处理后获得至少有一个词的文档。</p><p id="dea5" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">删除停用词、非词汇词、空文档，为每个标题准备向量。</p><figure class="kr ks kt ku fd ij"><div class="bz dy l di"><div class="ll lm l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">使用辅助函数预处理整个数据集</figcaption></figure><p id="4592" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">删除停用词和空文档后，</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="61bb" class="la iv hi kw b fi lb lc l ld le">241 docs removed<br/>1st filter: Length of corpus:422178, Length of titles_list:422178, Length of labels:422178<br/>0 docs removed<br/>2nd filter: Length of corpus:422178, Length of titles_list:422178, Length of labels:422178</span></pre><p id="3df2" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">每个标题的向量</strong></p><p id="cad4" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">所有标题和标签的300维向量列表包含各自的标签。这些向量的形状是(422178，300)。<br/>它各自标签的形状(422178，1)。</p><p id="1acb" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">过滤掉标签不正确的数据。标签只能是以下类型。</p><ol class=""><li id="827f" class="ln lo hi ju b jv lf jz lg kd lp kh lq kl lr kp ls lt lu lv bi translated">商务</li><li id="d7da" class="ln lo hi ju b jv lw jz lx kd ly kh lz kl ma kp ls lt lu lv bi translated">t:技术</li><li id="959d" class="ln lo hi ju b jv lw jz lx kd ly kh lz kl ma kp ls lt lu lv bi translated">e:娱乐</li><li id="1fd5" class="ln lo hi ju b jv lw jz lx kd ly kh lz kl ma kp ls lt lu lv bi translated">男:健康</li></ol><figure class="kr ks kt ku fd ij"><div class="bz dy l di"><div class="ll lm l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">过滤数据以仅包含有效标签。</figcaption></figure><p id="aa9c" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">创建一个数据帧，对其进行洗牌和分割。</p><pre class="kr ks kt ku fd kv kw kx ky aw kz bi"><span id="727e" class="la iv hi kw b fi lb lc l ld le">finalData = pd.DataFrame.from_records(features)<br/>finalData.columns = range(1,301)<br/>finalData["labels"] = labels</span><span id="1880" class="la iv hi kw b fi lk lc l ld le"># finalData.to_csv('FinalData.csv')<br/>data = finalData.sample(frac=1) #shuffles the data<br/>labels = data["labels"]<br/>labels = np.array(labels)<br/>labels = labels.reshape(labels.shape[0],1)<br/>del data["labels"]<br/>features = np.array(data)<br/>features.shape, labels.shape<br/># ((50000, 300), (50000, 1))</span></pre><p id="8ec4" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">拆分数据。</strong></p><p id="29e2" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">训练数据:</strong>将80%的数据用于训练目的。<br/> <strong class="ju hj">测试数据</strong>:使用20%的数据进行测试。<br/> <strong class="ju hj">特征</strong>:使用300维向量作为特征。它可以在vectorsForEachDocument中找到。<br/> <strong class="ju hj">标签</strong>:使用类别作为标签。我可以在标签中找到。</p><p id="a610" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">数据的形状</strong> <br/>训练特征:(160000，300) <br/>训练标签:(160000，1) <br/>测试特征:(40000，300) <br/>测试标签:(40000，1)</p><p id="f9b8" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated"><strong class="ju hj">训练逻辑回归模型</strong></p><figure class="kr ks kt ku fd ij"><div class="bz dy l di"><div class="ll lm l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">训练逻辑回归模型。</figcaption></figure><h1 id="e5af" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated">培训成果</h1><p id="e71b" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">逻辑回归模型可以在12秒内达到86%的准确率。下面列出了各种实验模型的细节。</p><figure class="kr ks kt ku fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/a359836bdfbe548b9867b781ac3aee85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5xxg-CHqw6ldoHvyugLHdA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">不同回归模型的结果。</figcaption></figure><p id="465d" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">对于按['b '，' t '，' e '，' m']顺序排列的标签，精度和召回率如下<br/><br/>精度:[82.947297775 83.25883789.44478787.46594 005】<br/>召回率:[84.79757 80.9585966</p><p id="f6d9" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">ROC曲线如下</p><div class="kr ks kt ku fd ab cb"><figure class="mc ij md me mf mg mh paragraph-image"><img src="../Images/f1c3a7910046394981e167dc9382b3e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*J3_FmdEZHBwaja0ned-tLA.png"/></figure><figure class="mc ij md me mf mg mh paragraph-image"><img src="../Images/a4201a8ec05e1df3f256b77985b5e43b.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*uKLPwtKajfIqlfNjbQvBcw.png"/></figure></div><div class="ab cb"><figure class="mc ij md me mf mg mh paragraph-image"><img src="../Images/74ec5ae6288e2e64f993573d63f05a92.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*VV29F4LiWGWEMME3V8xxIQ.png"/></figure><figure class="mc ij md me mf mg mh paragraph-image"><img src="../Images/2cf7f2b6556b8b77192c87c8c5a47fc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*wdCoTw5M00JNWQ-5ZyhqOA.png"/></figure></div><p id="bc96" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">这方面的一个主要参考来自于<a class="ae kq" href="https://towardsdatascience.com/using-word2vec-to-analyze-news-headlines-and-predict-article-success-cdeda5f14751" rel="noopener" target="_blank">https://towards data science . com/using-word 2 vec-to-analyze-news-headlines-and-predict-article-success-CDE da 5 f 14751</a>。</p><p id="6182" class="pw-post-body-paragraph js jt hi ju b jv lf jx jy jz lg kb kc kd lh kf kg kh li kj kk kl lj kn ko kp hb bi translated">同样的github repo可以在这里找到<a class="ae kq" href="https://github.com/omkarsk98/NewsDataClassification/tree/v1.5" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>