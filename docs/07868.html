<html>
<head>
<title>How does PCA (Principal Component Analysis) really work?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PCA(主成分分析)到底是怎么工作的？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-does-pca-really-work-e53f640e5323?source=collection_archive---------5-----------------------#2020-07-10">https://medium.com/analytics-vidhya/how-does-pca-really-work-e53f640e5323?source=collection_archive---------5-----------------------#2020-07-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="37c6" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">揭开主成分分析工作背后的数学，为什么它是重要的，以及如何解释其结果。</h2></div><p id="9b7b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">PCA或主成分分析是一种无监督算法，用于降低数据的维数，而不尽可能补偿信息的损失。通过只提取重要变量，然后创建新的不相关变量来最大化方差，PCA有助于解决维数灾难和过拟合等问题。实现PCA的代码(用Python或R)相当简单且容易获得。因此，我不会在这上面浪费时间，而是将重点放在算法成功运行背后的数学上。<br/>在这个阶段，我认为在深入研究PCA之前，关注一下我们所说的维数灾难是很重要的，因为这将帮助我们认识到使用PCA的重要性以及何时有必要这样做。除非我们确信某样东西确实有用和重要，否则它几乎不值得学习。</p><h1 id="e337" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">这个“诅咒”是什么？</h1><p id="e078" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">两个假设形成了半监督学习的基础:<strong class="iz hj">嵌入</strong>(高维数据的底层结构通常要简单得多)和<strong class="iz hj">连续性</strong>(彼此靠近的数据点比相距较远的数据点彼此更相似)。这两个假设，以及一些统计结果，在处理多维数据时会产生一些问题。</p><ol class=""><li id="030c" class="kq kr hi iz b ja jb jd je jg ks jk kt jo ku js kv kw kx ky bi translated">随着维度数量的增加，数据呈指数增长，可能会影响性能速度和质量中的一个或两个。对于n个各有m维的观测值，我们基本上有nᵐ条信息。</li><li id="205d" class="kq kr hi iz b ja kz jd la jg lb jk lc jo ld js kv kw kx ky bi translated">并非所有的维度都是真正相互独立的，而是相互关联的，并且违反了重要的统计假设。</li><li id="fbc5" class="kq kr hi iz b ja kz jd la jg lb jk lc jo ld js kv kw kx ky bi translated">在极高的维度中，随着空闲空间的增加，数据变得更加分散。几乎所有的观察都变得彼此等距，从而使距离度量的有效性无效。</li></ol><h1 id="5c43" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">那么，PCA是如何工作的呢？</h1><p id="f570" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">现在我们知道了为什么PCA是重要的，让我们借助一些可视化的例子来了解它的数学知识以及它是如何工作的。毕竟，我们为什么要盲目地相信一个算法，并想当然地认为无论它在做什么都是正确的呢？！我将首先带您看一个简单的二维例子，然后说明它在三维甚至更高维数据集的情况下是如何工作的。为什么是2D？因为对我来说这是最容易说明的，对你来说也是最容易想象的。毕竟，在三维空间之外，事物对人类来说更有趣而不清晰。如下所示，我们给出了以下数据，并将其绘制在相应的二维平面上。</p><div class="le lf lg lh fd ab cb"><figure class="li lj lk ll lm ln lo paragraph-image"><img src="../Images/9ef66f3e0e659445693526d3f2e8d13c.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*RlZIKouRn7oEvH6E-HJ9gw.png"/></figure><figure class="li lj lr ll lm ln lo paragraph-image"><img src="../Images/0fa62cf144fc1756812e79131bd9b45b.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*oJ-W-4BfjgahEwjaj26S6Q.png"/></figure></div><p id="4d7e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们要做的是沿着2个变量轴取所有观察值的平均值，并用它们来计算数据的中心。之后，我们将沿着平面移动观测值，使数据的中心与2D平面的原点重合。</p><div class="le lf lg lh fd ab cb"><figure class="li lj ls ll lm ln lo paragraph-image"><img src="../Images/9a09b24fd164494c97ba65be89e6f329.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*aTe2zE0aRnFi0XBzjI4Y6Q.png"/></figure><figure class="li lj lt ll lm ln lo paragraph-image"><img src="../Images/b84c7f3388d2d7ac2aa65c318aa4f659.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*6in1s3B5AYhkTjPokEjVFQ.png"/><figcaption class="lu lv et er es lw lx bd b be z dx ly di lz ma translated"><strong class="bd jv"> a. </strong>沿各轴的观察中心。<strong class="bd jv"> b. </strong>将中心转移到原点。</figcaption></figure></div><p id="c12f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里我们不能忽略的一件重要事情是，这种变换不会改变点之间的相对位置。最右边的点保持最右边，最下面的点保持最右边，以此类推。我提到这一点只是为了说明点的移动不会影响数据的相对分布，因此是一个可以接受的操作。这样做的原因很快就会清楚了。</p><h1 id="471e" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">寻找第一个主成分:</h1><p id="0d8e" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">现在，我们将尝试找到点的最佳拟合线，通过中心。如何选择最佳拟合线与其他需要使用最佳拟合线的技术非常相似，使用OLS(普通最小二乘法)的方法。然而，这条线穿过中心的必要性允许我们稍微调整一下，让事情变得简单一点。如下图所示，首先选择任意一条线，然后将点投影到其上。</p><div class="le lf lg lh fd ab cb"><figure class="li lj mb ll lm ln lo paragraph-image"><img src="../Images/78924aeab52e95a12c87eb275fdf0fb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*rCAg_yM7k-Y-Wwwrc5jqZA.png"/></figure><figure class="li lj mc ll lm ln lo paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><img src="../Images/7c50b1d43422bfc9c03b7bf6a18ec3d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*Akj5UsGU8LGAQLS5MI_mpA.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx mh di mi ma translated"><strong class="bd jv"> a. </strong>一条随机线。<strong class="bd jv"> b. </strong>旋转它，使用距离平方和找到最佳拟合线</figcaption></figure></div><p id="aa7f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们接下来要做的是测量从这些投影点到原点(在上图中标记为dᵢ's)的距离，将它们平方，然后最大化其总和。<strong class="iz hj"> <em class="mj">对，最大化！</em> </strong>直观上，这就跟最小化点到线的距离平方和一样。(希望你能用毕达哥拉斯定理算出来)。这两种方法会产生相同的结果，从而产生有效的优化，但我更喜欢阐述PCA算法使用的确切方法。<br/> * <strong class="iz hj">最佳拟合线是d</strong>₁<strong class="iz hj">+d</strong>₂<strong class="iz hj">+d</strong>₃<strong class="iz hj">+d</strong>₄<strong class="iz hj">+d</strong>₅<strong class="iz hj">+d</strong>₆<strong class="iz hj">=最大值* </strong></p><p id="63ba" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，我们得到了第一个主成分，即PC1。我们现在需要做的就是找出这条线的斜率。为了便于讨论，假设PC1对于我们这样获得的数据的斜率是0.25。这在几何上意味着，对于沿Var1轴的每4个单位步长，我们在Var2轴上移动1个单位步长。在我们的数据环境中，PC1只是两个变量的组合。一种看待它的方式类似于化学溶液的成分:我们需要混合4份Var1和1份Var2以获得精确的化学溶液。</p><p id="619e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在是谈论某些重要术语的合适时机。最佳拟合直线的一个单位称为特征向量。相应组件的长度称为加载分数。对于我们的例子(直线斜率= 0.25)，我们可以计算如下:</p><div class="le lf lg lh fd ab cb"><figure class="li lj mk ll lm ln lo paragraph-image"><img src="../Images/658df4a0fb5b6eb6b4b15a0f8f5c04ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*uU_Q1R61y_G4Z5kVTwMHrA.png"/></figure><figure class="li lj ml ll lm ln lo paragraph-image"><img src="../Images/971dee6a49c35afdd43485090ff9032b.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*Jx29hiQ6fRtCEXgJ97LfTQ.png"/><figcaption class="lu lv et er es lw lx bd b be z dx mm di mn ma translated">将最佳拟合线缩小到1个单位</figcaption></figure></div><p id="2e31" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这条1单位长度的最佳拟合线被称为PC1的<strong class="iz hj">特征向量或奇异向量</strong>。<br/>对应的值0.97和0.242称为PC1的Var1和Var2的<strong class="iz hj">加载分数</strong>。(正如所料，比例保持不变，因此我们可以考虑将0.97份Var1与0.242份Var2混合)。<br/>已经获得的原始平方和值就是<strong class="iz hj">特征值</strong>。</p><h1 id="e3c8" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">寻找第二个主成分:</h1><p id="e1dc" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">现在我们已经完成了第一个主要步骤，但是我们还没有达到最终的目标。为此，我们也需要找到第二个主成分。(因为只有两个变量，我们最多可以有两个分量。)</p><p id="867e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在是讨论PCA的一个非常重要的性质的时候了。<br/> <strong class="iz hj">任何数据集的所有主成分总会组成一个相互正交的系统。</strong>简单来说，这是因为主成分本质上是数据集的多个(或所有)变量的线性组合。<em class="mj">协方差矩阵是对称的，对称矩阵总是有实特征值和正交特征向量</em>。<br/>详细的数学解释请参考以下链接:<a class="ae mo" href="https://stats.stackexchange.com/questions/130882/why-are-principal-components-in-pca-eigenvectors-of-the-covariance-matrix-mutu" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/130882/why-are-principal-components-in-PCA-featured vectors-of-the-协方差矩阵-mutu </a></p><p id="8fb8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，在我们的例子中，PC2将只是一条穿过原点并垂直于PC1的线，我们已经知道了。以类似于上面解释的方式进行计算，我们得到PC2的以下结果:Var1的1部分必须与Var2的4部分混合。之后，我们只需要旋转所有的东西，把投影点从直线上反过来。我们已经完成了我们的电脑绘图。</p><div class="le lf lg lh fd ab cb"><figure class="li lj mp ll lm ln lo paragraph-image"><img src="../Images/53a5affcd105139f50d5af9ea5aca6eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*5jhBqbZTVUniT9E94qPPZQ.png"/></figure><figure class="li lj mq ll lm ln lo paragraph-image"><img src="../Images/bafee70d111c71bfdc4884a59100f61a.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*YyY7Lq1LcfHl_FePOlwcZQ.png"/></figure><figure class="li lj mr ll lm ln lo paragraph-image"><img src="../Images/65faed8af2980c6874776c894b45442e.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*d48LTIsgRkPOwOUjyfwggQ.png"/></figure></div><p id="2239" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是等等，这对我们有什么帮助？我们开始时只有两个变量，在这里，我们再次以两个主成分结束！那么，这对我们有什么用呢？为此，我们需要检查数据中的总方差有多少是由每个主成分单独解释的。</p><p id="2513" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设SS₁和SS₂分别是PC1和PC2的平方和。这些平方和除以(样本量减1)将给出每个样本在原点附近的变化。从这些数据中，我们现在可以计算出由每个成分解释的<strong class="iz hj">变化百分比。<br/> </strong>对于这个例子，让我们考虑SS₁= 20，SS₂ = 5。<br/>那么由PC1单独解释的变异百分比:<strong class="iz hj"> 20/(20+5) = 0.8 = 80% <br/> </strong>和由PC1单独解释的变异百分比:<strong class="iz hj"> 5/(20+5) = 0.2 = 20% <br/> </strong>(需要注意的一点:由于样本量自始至终是相同的，所以简单的SS评分就足以计算出变异百分比。我们可以跳过除法步骤)</p><p id="203b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，我们可以得出结论，PC1本身就足以解释整个数据中的大部分差异。<em class="mj">这正是我们最初的目标——用较少的变量解释尽可能多的变化。</em></p><p id="9655" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们已经看到了数学是如何工作的，让我们试着做一个三维的例子！</p><h1 id="bce5" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">三维数据的主成分分析:</h1><div class="le lf lg lh fd ab cb"><figure class="li lj ms ll lm ln lo paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><img src="../Images/526449b47e1a81d9a5b9b054039d235d.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*Ay1R5vBQ6ba7mM_XlbZ66w.png"/></div></figure><figure class="li lj mt ll lm ln lo paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><img src="../Images/4be12a47f6f540528d299659f2133170.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*VGBbevLj4h-tF7B1nMBGRQ.png"/></div></figure><figure class="li lj mu ll lm ln lo paragraph-image"><img src="../Images/92d98b0f393aa7c85e83dbe640e8e243.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*8Kq1735VL1naro8oGWKuqw.png"/></figure></div><p id="e556" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">按照类似于2D情况的步骤，我们得到如上所示的第一主成分。然而，由于我们现在处理3个变量，我们可以有多达3个主分量——当然相互垂直，并且每个都经过原点。</p><figure class="le lf lg lh fd lj er es paragraph-image"><div class="er es mv"><img src="../Images/a149967c0ea75bedb3f8c5573e48ef6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*knZNDInx4ydz4hUW67bjXw.png"/></div></figure><p id="ac12" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">想想看，SS₁、SS₂and SS₃分别是79岁、15岁和6岁。那么，这三个因素解释的变异百分比分别为79%，15%和6%。因此，PC1和PC2合起来可以解释94%的数据差异。阈值的选择可以因用户而异，也取决于手头的问题。但是，最重要的结论是<strong class="iz hj">我们已经成功地减少了变量的数量！</strong></p><h1 id="98fe" class="jt ju hi bd jv jw jx jy jz ka kb kc kd io ke ip kf ir kg is kh iu ki iv kj kk bi translated">概括和结论:</h1><p id="3955" class="pw-post-body-paragraph ix iy hi iz b ja kl ij jc jd km im jf jg kn ji jj jk ko jm jn jo kp jq jr js hb bi translated">我希望我已经在某种程度上能够抓住这个精确而美丽的算法背后的主要本质。但是，我用的数据集是完全荒谬的，不需要还原。PCA的真正威力可以在处理真实世界的数据集时实现，这些数据集很容易具有数十甚至数百个变量。很明显，并不是所有的变量对我们的任务都有用，主成分分析通常是减少变量但不丢失太多信息的常用技术。</p><figure class="le lf lg lh fd lj er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es mw"><img src="../Images/721d95c83c4a1e20a16f87af18f25772.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W7Jpu8osu4Gr2S1bZm7JSQ.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">显示累积方差的Scree图</figcaption></figure><p id="2698" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这方面，有一个工具非常方便:<strong class="iz hj"> Scree Plots </strong>。它们描述了由前“n”个主成分解释的数据的累积变化。然后，用户可以选择哪些要使用，哪些要丢弃。<br/>在随附的图中，7个成分足以解释总方差的80%以上，并且与最初存在的11个变量相比有相当大的下降</p><p id="6677" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我想回到我在上面没有详细谈到的一些事情上来，以免事情的流程被打乱。我定义的术语，即特征向量和特征值，有一定的价值。特征向量代表方向，特征值代表大小(或重要性)。自然，特征值越大，对应的方向(特征向量)越重要。这与我们早期的结果一致，我们发现具有最大特征向量的<em class="mj"> PC可以最有效地解释数据的总体方差。</em></p><p id="d5e3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，除非你知道如何正确实施，否则数据科学没有任何用处。没有必要浪费时间解释代码行，但我会留下一些易于访问和理解的资源:</p><ol class=""><li id="6095" class="kq kr hi iz b ja jb jd je jg ks jk kt jo ku js kv kw kx ky bi translated"><a class="ae mo" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . decomposition . PCA . html</a></li><li id="5a20" class="kq kr hi iz b ja kz jd la jg lb jk lc jo ld js kv kw kx ky bi translated"><a class="ae mo" href="http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html" rel="noopener ugc nofollow" target="_blank">http://sebastianraschka . com/Articles/2015 _ PCA _ in _ 3 _ steps . html</a></li></ol><p id="dcdf" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">附:这是我第一篇关于媒介的文章。非常感谢你，如果你有足够的耐心通读整篇文章。请随时留下您的真实反馈，指出这篇文章中存在的技术和印刷错误，或者在评论部分澄清您的疑问和疑问。干杯:)</p></div></div>    
</body>
</html>