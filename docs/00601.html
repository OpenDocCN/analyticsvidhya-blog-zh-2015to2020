<html>
<head>
<title>Partitional Clustering using CLARANS Method with Python Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用CLARANS方法和Python示例进行划分聚类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/partitional-clustering-using-clarans-method-with-python-example-545dd84e58b4?source=collection_archive---------1-----------------------#2019-08-13">https://medium.com/analytics-vidhya/partitional-clustering-using-clarans-method-with-python-example-545dd84e58b4?source=collection_archive---------1-----------------------#2019-08-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="2249" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">基于随机搜索(CLARANS)聚类技术的大型应用程序聚类的详细说明，并以Python为例</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/01bb917e473dfa5eb8a6c6f2b9f05667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*bgU5dvXcwwg-bO1GVq46fw.jpeg"/></div></figure><h2 id="8c51" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><strong class="ak">目录:</strong></h2><ol class=""><li id="2991" class="kd ke hi kf b kg kh ki kj jq kk ju kl jy km kn ko kp kq kr bi translated">集群概述。</li><li id="dcd3" class="kd ke hi kf b kg ks ki kt jq ku ju kv jy kw kn ko kp kq kr bi translated">分区方法的简要描述。</li><li id="718c" class="kd ke hi kf b kg ks ki kt jq ku ju kv jy kw kn ko kp kq kr bi translated">分区方法的比较。</li><li id="7b5c" class="kd ke hi kf b kg ks ki kt jq ku ju kv jy kw kn ko kp kq kr bi translated">CLARANS概述</li><li id="2364" class="kd ke hi kf b kg ks ki kt jq ku ju kv jy kw kn ko kp kq kr bi translated">CLARANS算法</li><li id="1854" class="kd ke hi kf b kg ks ki kt jq ku ju kv jy kw kn ko kp kq kr bi translated">Python示例</li><li id="68f9" class="kd ke hi kf b kg ks ki kt jq ku ju kv jy kw kn ko kp kq kr bi translated">参考</li></ol><h2 id="c454" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">1.集群概述。</h2><p id="9d1e" class="pw-post-body-paragraph kx ky hi kf b kg kh ij kz ki kj im la jq lb lc ld ju le lf lg jy lh li lj kn hb bi translated">聚类是无监督学习的一种形式，因为在这种算法中不存在类别标签。</p><p id="7be2" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">一般来说，<strong class="kf hj">聚类</strong>是将一组数据对象划分成子集的过程。其中每个子集是一个集群，使得一个集群中的对象彼此相似，但与其他集群中的对象不相似。</p><p id="ddb6" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">基于算法的特征，有4种主要类型的聚类技术:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lp"><img src="../Images/143bf24c294878e6430aad98c6b25d16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*d9khpIO6X0i3ocs3fHzFHQ.png"/></div></figure><p id="19f1" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">CLARANS是一种分区方法。</p><h2 id="e5b4" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><strong class="ak"> 2。</strong>分区方法简述。</h2><p id="2e92" class="pw-post-body-paragraph kx ky hi kf b kg kh ij kz ki kj im la jq lb lc ld ju le lf lg jy lh li lj kn hb bi translated">划分方法是聚类分析的最基本类型，它们将一个集合中的对象组织成几个排他的聚类组(<em class="lq">即每个对象只能出现在一个组中</em>)。</p><p id="92e1" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">分区算法需要数量为<strong class="kf hj">的簇(k ) </strong>作为它的起点。</p><p id="5633" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">因此，给定由n个点组成的数据集D和k <strong class="kf hj"> (k &lt; &lt; n) </strong>，分区算法<strong class="kf hj">将对象组织成k个分区</strong>(集群)。</p><p id="3195" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">通过<strong class="kf hj">优化目标划分标准</strong>，例如基于距离的相异度函数，来形成聚类，使得在数据集属性方面，聚类内的<strong class="kf hj">对象彼此“相似”</strong>并且与其他聚类中的对象“相异”<strong class="kf hj">。</strong></p><h2 id="721d" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><strong class="ak"> 3。分区方法的比较。</strong></h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es lr"><img src="../Images/cf054ad7196a7dfa071f43e0c66986a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PDH7AdGKBhrq4rnO9w4tBw.png"/></div></div></figure><p id="f6b7" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated"><strong class="kf hj">K-means:</strong>K-means算法将聚类的质心定义为该聚类内的点的平均值。</p><p id="2f68" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">这就是为什么<strong class="kf hj"> K-means对噪声和异常值</strong>敏感的原因，因为少量的此类数据会显著影响平均值。</p><p id="3d29" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated"><strong class="kf hj"> 3.1 — K-medoids: </strong>为了克服对离群点敏感的问题，我们可以不用均值作为质心，而是取实际的数据点来表示聚类，这就是K-medoids所做的。</p><p id="f119" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">但是当数据集和k值很大时,<strong class="kf hj"> k-medoids方法非常昂贵。</strong></p><p id="67b8" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated"><strong class="kf hj"> 3.2 — CLARA: </strong>为了扩大K-medoids方法，引入了CLARA。CLARA不考虑整个数据集，而是使用数据集的随机样本，从中选取最佳的medoids。</p><p id="137d" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">但是CLARA的<strong class="kf hj">有效性取决于样本量</strong>。如果任何最佳采样的中值点远离最佳k-中值点，CLARA就不能找到好的聚类。</p><p id="ea65" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated"><strong class="kf hj"> 3.3 — CLARANS ( <em class="lq">基于随机搜索对大型应用进行聚类</em> ) : </strong>它提出了使用样本获得聚类的成本和有效性之间的权衡。</p><h2 id="b393" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><strong class="ak"> 4。CLARANS概述:</strong></h2><p id="98dc" class="pw-post-body-paragraph kx ky hi kf b kg kh ij kz ki kj im la jq lb lc ld ju le lf lg jy lh li lj kn hb bi translated">它提出了使用样本获得聚类的成本和有效性之间的权衡。</p><p id="8688" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">首先，它随机选择数据集中的<strong class="kf hj"> k </strong>个对象作为当前的medoids。然后，它随机选择当前的medoid x和不是当前medoid之一的对象y。</p><p id="b4ec" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">然后，它检查以下情况:</p><blockquote class="lw lx ly"><p id="fe7b" class="kx ky lq kf b kg lk ij kz ki ll im la lz lm lc ld ma ln lf lg mb lo li lj kn hb bi translated">用<strong class="kf hj"> y </strong>代替<strong class="kf hj"> x </strong>能提高绝对误差标准吗？</p></blockquote><p id="0ca3" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">如果是，则进行替换。CLARANS进行了l次这样的随机搜索。在<strong class="kf hj"> l </strong>步之后的当前medoids组被认为是局部最优。</p><p id="70f9" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">CLARANS重复这个随机过程m次，并返回最佳的局部最优值作为最终结果。</p><h2 id="39ec" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><strong class="ak"> 5。克拉伦斯算法</strong></h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mc"><img src="../Images/de0541024c14e5ca78335a0c830edcf6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NaDyBvy9o3wRoWmnNBb65w.png"/></div></div></figure><p id="f6af" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated"><strong class="kf hj">解释:</strong></p><p id="6ea6" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">该算法需要<strong class="kf hj"> numlocal </strong> ( <em class="lq">解决问题的迭代次数</em>)、<strong class="kf hj"> maxneighbor </strong> ( <em class="lq">检查的最大邻居数</em>)和要形成的聚类数(<strong class="kf hj"> k </strong>)作为输入。</p><p id="2504" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">然后迭代开始，<strong class="kf hj"> i </strong>设置为1，在此之前<strong class="kf hj"> mincost </strong> ( <em class="lq">为最优成本</em>)设置为无穷大，<strong class="kf hj"> bestnode </strong> ( <em class="lq">最优medoids </em>)设置为空元组。</p><p id="6049" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">现在<strong class="kf hj"> k个随机数据点</strong>被选择作为当前的medoid，并且使用这些数据点<em class="lq">形成聚类(欧几里德距离可用于找到最近的med oid以形成聚类</em>)。</p><p id="6a17" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">此后，新的循环开始，其中<strong class="kf hj"> j </strong>被设置为<strong class="kf hj"> 1 </strong>。选择一个随机的当前medoid，并选择一个<strong class="kf hj">随机候选</strong> ( <em class="lq">随机邻居</em>)数据点来替换当前medoid。如果候选数据点的替换产生比当前medoid更低的<strong class="kf hj">总成本</strong> ( <em class="lq">是聚类中所有点与其各自med oid</em>之间的距离总和)，则进行替换。如果更换完成，则<strong class="kf hj"> j </strong>不会增加，否则<strong class="kf hj"> j = j +1。</strong></p><p id="483b" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">一旦<strong class="kf hj"> j &gt; maxneighbor </strong>，则获取当前的medoids，并将它们的<strong class="kf hj">总成本</strong>与<strong class="kf hj">最小成本</strong>进行比较。如果<strong class="kf hj">总成本</strong>小于<strong class="kf hj">最小成本</strong>，则<strong class="kf hj">最佳节点</strong>被更新为当前medoids。</p><p id="68fe" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated"><strong class="kf hj"> i </strong>随后递增，如果大于<strong class="kf hj"> numlocal </strong>，则输出<strong class="kf hj"> Bestnode </strong>，否则重复整个过程。</p><h2 id="5ea5" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><strong class="ak"> 6。Python例子。</strong></h2><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="md me l"/></div></figure><p id="d037" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">产出:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mf"><img src="../Images/5e940fed5c74e2faaba0af4622b86b87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SaHSLDssY_Mhj0VDzvA5AA.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">输出:print("窥视数据集: "，data[:4])</figcaption></figure><p id="875a" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">上面的输出显示iris数据集包含具有4个特征的数据点。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mk"><img src="../Images/692712ae4efa2ffe5a5f960dd56eec8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aint-zkwG-JOa7hzs2YzNw.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">输出:print("簇中的点的索引: "，簇)</figcaption></figure><p id="19f4" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">我们可以看到有3个集群，一个以<strong class="kf hj">第53个数据点</strong>作为其第一个元素开始，另一个以<strong class="kf hj"> 0 </strong>作为其第一个元素，最后一个以<strong class="kf hj">第50个数据点</strong>开始。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es ml"><img src="../Images/0c3a407207e3aeb7d6a6a4acf6713438.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HGfV0tcHLJRNjgwJ4yUP3Q.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">输出:print("每个数据点的目标类: "，iris.target)</figcaption></figure><p id="ae5b" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">输出显示第1到第50个数据点属于第1类，第51到第100个属于第2类，其余的属于第3类。这些是每个数据点所属的实际类别，即setosa、versicolor和virginica</p><p id="e0b3" class="pw-post-body-paragraph kx ky hi kf b kg lk ij kz ki ll im la jq lm lc ld ju ln lf lg jy lo li lj kn hb bi translated">因此，可以注意到，CLARANS算法能够在iris数据集上的适当聚类中以显著的精度划分数据点。</p><h2 id="acda" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><strong class="ak"> 7。参考文献</strong></h2><ol class=""><li id="6b28" class="kd ke hi kf b kg kh ki kj jq kk ju kl jy km kn ko kp kq kr bi translated">Swarndeep Saket J，Sharnil Pandya博士，“聚类技术中划分算法的概述”(ijar cet)2016年6月第五卷第六期。</li><li id="6a82" class="kd ke hi kf b kg ks ki kt jq ku ju kv jy kw kn ko kp kq kr bi translated">诺维科夫，a，2019。数据挖掘库。开放源码软件杂志，4(36)，第1230页</li><li id="dbb0" class="kd ke hi kf b kg ks ki kt jq ku ju kv jy kw kn ko kp kq kr bi translated">吴家伟，韩家伟，“空间数据挖掘中的聚类方法”，2002年9月/10月。</li><li id="1f05" class="kd ke hi kf b kg ks ki kt jq ku ju kv jy kw kn ko kp kq kr bi translated">韩佳玮，米凯琳.坎伯，简佩。"数据挖掘的概念和技术."</li></ol></div></div>    
</body>
</html>