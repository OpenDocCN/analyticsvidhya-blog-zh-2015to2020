<html>
<head>
<title>“Transfer Learning in Neural Network” in nutshell.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“神经网络中的迁移学习”一言以蔽之。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/transfer-learning-in-nutshell-f49cf3ab5f7f?source=collection_archive---------19-----------------------#2020-07-27">https://medium.com/analytics-vidhya/transfer-learning-in-nutshell-f49cf3ab5f7f?source=collection_archive---------19-----------------------#2020-07-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8910" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">构建完美的神经网络是<strong class="ih hj">经验过程</strong>，需要一点时间和经验来实现它。但是，现在一天的<strong class="ih hj">转移学习</strong>使它的方式更快，更少忙乱。那么，什么是迁移学习呢？简单来说。迁移学习基本上是一个借用部分或全部现有的<strong class="ih hj">预训练模型</strong>的过程。有哪些预先训练好的模型？该模型已经在大型数据集上进行了训练，可供所有人使用。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/b8e3e69092507abd42e851cd6b008fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jhQAzAHau_TkpG0RbByA8Q.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图片中迁移学习的定义。</figcaption></figure><p id="7d5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以你有了迁移学习的想法。现在我们来讨论一下如何动手做。有两种方法可以完成这一过程，第一种是<strong class="ih hj">只是转移学习</strong>，它只是将预训练的模型原样用于您的模型，而不对其进行任何更改。第二步<strong class="ih hj">用迁移学习进行微调</strong>，这里的过程和你平时做的一样，只是做了一些剪裁。目前有许多预训练模型可用，如Xception、VGG16、VGG19、ResNet50、InceptionV3、MobileNet、MobileNetV2等。在这里，我在这篇文章中使用的是16层CNN模型，在属于1000个类别的1400多万张图像上进行训练。</p><p id="dd5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，首先让我们把注意力集中在标准的迁移学习上，然后你将开始微调。这里的VGG16模型在keras库中可用，所以基本上我们可以从keras导入它并开始工作</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="cb1e" class="jz ka hi jv b fi kb kc l kd ke">from tensorflow.keras.applications.vgg16 import VGG16</span></pre><p id="c352" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就是这样！你做到了，简单吧？现在让我们为模型创建一个对象，你已经为训练做好了准备。</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="78b8" class="jz ka hi jv b fi kb kc l kd ke">vgg_model = VGG16(input_shape=&lt;image input size&gt;,<br/>                  weights="imagenet",<br/>                  include_top=False)</span></pre><p id="88bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，对象vgg_model是VGG16模型，它采用类似“input_shape”的参数，这将是您想要给出的图像数据集的形状。“权重”是模型训练的基础，如果你想为自己初始化模型，那么你可以简单地将“图像网”替换为“无”。“include_top”基本上是网络的最后一层或输出层，我们不需要它，所以我将其设置为“False ”,因为它有1000个输出，而我们的模型输出与此不同。如果你想要它，那么只要把它变成“真”就可以了。</p><p id="b3ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过这两个简单的步骤，你得到了vgg_model对象中的VGG16。你会得到这个。</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="a68e" class="jz ka hi jv b fi kb kc l kd ke">&gt;&gt;vgg_model.summary()<br/></span><span id="cc3e" class="jz ka hi jv b fi kf kc l kd ke">Model: "vgg16" _________________________________________________________________ Layer (type)                 Output Shape              Param #    ================================================================= input_1 (InputLayer)         [(None, 224, 224, 3)]     0          _________________________________________________________________ block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792       _________________________________________________________________ block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928      _________________________________________________________________ block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0          _________________________________________________________________ block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856      _________________________________________________________________ block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584     _________________________________________________________________ block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0          _________________________________________________________________ block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168     _________________________________________________________________ block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080     _________________________________________________________________ block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080     _________________________________________________________________ block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0          _________________________________________________________________ block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160    _________________________________________________________________ block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808    _________________________________________________________________ block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808    _________________________________________________________________ block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0          _________________________________________________________________ block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808    _________________________________________________________________ block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808    _________________________________________________________________ block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808    _________________________________________________________________ block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0          ================================================================= Total params: 14,714,688 Trainable params: 14,714,688 Non-trainable params: 0 _________________________________________________________________</span></pre><p id="a3bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里你可以看到VGG16模型的所有16层，在底部有一些描述。“总参数”是一个模型可以拥有的参数总数。“可训练参数”是你可以训练的参数数量，基本上这个模型是空的，带有权重，这意味着你只有vgg16的架构。最后“不可训练参数”顾名思义，这些参数在训练过程中是冻结的，不可更新。请注意，这里您看不到最后一层，因为我们将其设置为“假”。</p><p id="34e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在你有了迁移学习的想法，让我们继续微调它。这里的过程是一样的，但是我们只取那些我们需要的，比如模型的层数。现在让我们举一个例子，假设我们需要从VGG16模型构建一个只有10层的模型，并向它添加另一个额外的层。这可以通过创建两个对象来实现，一个用于我们的模型，另一个用于完整的VGG16模型。我们首先声明一个空的顺序对象，然后我们继续从vgg16对象添加层到这个对象。为此，我们进口了一些keras组件。</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="5598" class="jz ka hi jv b fi kb kc l kd ke">from tensorflow.python.keras.models import Sequential<br/>from tensorflow.keras.applications.vgg16 import VGG16<br/>from tensorflow.python.keras.layers import Dense, Conv2D, Flatten</span></pre><p id="7c45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们构建顺序模型时，我们将导入顺序和进一步的密集、Conv2D和扁平化，以便向网络添加额外的层。这可以这样做，</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="de61" class="jz ka hi jv b fi kb kc l kd ke">#model to be built<br/>model = Sequential()</span><span id="aef5" class="jz ka hi jv b fi kf kc l kd ke">#vgg16 model<br/>vgg_model = VGG16(input_shape=&lt;image input size&gt;,<br/>                  weights="imagenet",<br/>                  include_top=False)</span></pre><p id="33a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以这里model和vgg_model是对象，其中model是空序列模型，vgg_model是16层模型。因为我们应该从16层中只取10层VGG16。这可以通过，</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="6513" class="jz ka hi jv b fi kb kc l kd ke">for layer in vgg_model.layers[:11]:<br/>      model.add(layer)</span></pre><p id="3b5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里“层”将迭代vgg_model的所有16层，这是通过使用一种称为“vgg_model.layers”或<model_name> .layers的方法来完成的。由于我们只需要10层，所以我们通过说“vgg_model.layers[:11]”将迭代限制到第10层。如果我们总结一下这个模型，</model_name></p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="7def" class="jz ka hi jv b fi kb kc l kd ke">&gt;&gt;model.summary()<br/></span><span id="7b36" class="jz ka hi jv b fi kf kc l kd ke">Model: "sequential_1" _________________________________________________________________ Layer (type)                 Output Shape              Param #    ================================================================= block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792       _________________________________________________________________ block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928      _________________________________________________________________ block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0          _________________________________________________________________ block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856      _________________________________________________________________ block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584     _________________________________________________________________ block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0          _________________________________________________________________ block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168     _________________________________________________________________ block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080     _________________________________________________________________ block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080     _________________________________________________________________ block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0          ================================================================= Total params: 1,735,488 Trainable params: 1,735,488 Non-trainable params: 0 _________________________________________________________________</span></pre><p id="0f93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你所看到的，我们只从VGG16模型中提取了10层，我知道这很简单，对吗？现在我们得到了“模型”对象，让我们完成这个CNN模型。我们可以添加另一层，就像我们在构建CNN时所做的一样。</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="5a7f" class="jz ka hi jv b fi kb kc l kd ke">model.add(Conv2D(32, (3, 3), activation = "relu"))<br/>model.add(Flatten())<br/>model.add(Dense(2, activation="softmax"))</span></pre><p id="5892" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，这增加了另一个Conv2D层，进一步平坦的完全连接，这里我已经添加了2个节点作为输出。现在，如果我们总结一下这个模型，它看起来会像这样。</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="c843" class="jz ka hi jv b fi kb kc l kd ke">&gt;&gt;model.summary()<br/></span><span id="5385" class="jz ka hi jv b fi kf kc l kd ke">Model: "sequential_1" _________________________________________________________________ Layer (type)                 Output Shape              Param #    ================================================================= block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792       _________________________________________________________________ block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928      _________________________________________________________________ block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0          _________________________________________________________________ block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856      _________________________________________________________________ block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584     _________________________________________________________________ block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0          _________________________________________________________________ block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168     _________________________________________________________________ block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080     _________________________________________________________________ block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080     _________________________________________________________________ block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0          _________________________________________________________________ conv2d_1 (Conv2D)            (None, 26, 26, 32)        73760      _________________________________________________________________ flatten_1 (Flatten)          (None, 21632)             0          _________________________________________________________________ dense_1 (Dense)              (None, 2)                 43266      ================================================================= Total params: 1,852,514 Trainable params: 1,852,514 Non-trainable params: 0 _________________________________________________________________</span></pre><p id="a441" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们也完成了微调，就这样！简单吧？现在尝试在你的项目中使用其他预先训练的模型，这将使你的项目在性能上更加有效。</p><p id="2553" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是…… <em class="kg">等一下，</em></p><p id="d5ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有一个技巧可以产生实际的差异，采用没有任何权重的完全空模型或使所有可训练参数为真会使你的网络失去一些重要的价值。这些网络是在数百万个数据集上训练的，因此模型肯定会有一些复杂的参数，这些参数是用常规方法无法获得的。所以我的建议是保持开始的一些层打开学习，其余的冻结。这样，您的模型将在初始层熟悉您的数据集，并在网络末端变得更加智能。为此，您只需在为模型创建对象后添加几行代码。</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="128c" class="jz ka hi jv b fi kb kc l kd ke">for layer in model.layers[7:]:<br/>    layer.trainable = False</span></pre><p id="3650" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这两行将使你的模型半开放供学习，其余的不开放。在这里，我允许模型学习到第7层，之后的任何东西都不允许有任何重量的增加。如果我们进行总结，我们会得到:</p><pre class="je jf jg jh fd ju jv jw jx aw jy bi"><span id="ccbe" class="jz ka hi jv b fi kb kc l kd ke">&gt;&gt; model.summary()<br/></span><span id="168c" class="jz ka hi jv b fi kf kc l kd ke">Model: "sequential_1" _________________________________________________________________ Layer (type)                 Output Shape              Param #    ================================================================= block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792       _________________________________________________________________ block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928      _________________________________________________________________ block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0          _________________________________________________________________ block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856      _________________________________________________________________ block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584     _________________________________________________________________ block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0          _________________________________________________________________ block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168     _________________________________________________________________ block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080     _________________________________________________________________ block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080     _________________________________________________________________ block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0          _________________________________________________________________ conv2d_1 (Conv2D)            (None, 26, 26, 32)        73760      _________________________________________________________________ flatten_1 (Flatten)          (None, 21632)             0          _________________________________________________________________ dense_1 (Dense)              (None, 2)                 43266      ================================================================= Total params: 1,852,514 Trainable params: 555,328 Non-trainable params: 1,297,186 _________________________________________________________________</span></pre><p id="9bff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，您可以看到，我们有555，328个参数可以训练或用于反向传播中的权重更新，1，297，186个参数将保持与VGG16模型相同。原来如此！现在你知道迁移学习了🎉。检查以下链接的完整代码脚本，不要忘记尝试自己和尝试它ALOT。</p><div class="kh ki ez fb kj kk"><a href="https://github.com/Jairus313/TransferLearning" rel="noopener  ugc nofollow" target="_blank"><div class="kl ab dw"><div class="km ab kn cl cj ko"><h2 class="bd hj fi z dy kp ea eb kq ed ef hh bi translated">jai RUS 313/转移学习</h2><div class="kr l"><h3 class="bd b fi z dy kp ea eb kq ed ef dx translated">在GitHub上创建一个帐户，为Jairus313/TransferLearning的发展做出贡献。</h3></div><div class="ks l"><p class="bd b fp z dy kp ea eb kq ed ef dx translated">github.com</p></div></div><div class="kt l"><div class="ku l kv kw kx kt ky jn kk"/></div></div></a></div><p id="f90c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不断学习🤓💻….！</p></div></div>    
</body>
</html>