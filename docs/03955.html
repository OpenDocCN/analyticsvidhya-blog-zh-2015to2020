<html>
<head>
<title>CaseStudy-Mercedes Benz-Greener Manufacturing Challenge</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">案例研究-梅赛德斯·奔驰-绿色制造挑战</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/casestudy-mercedes-benz-greener-manufacturing-challenge-1336ba321d27?source=collection_archive---------9-----------------------#2020-02-27">https://medium.com/analytics-vidhya/casestudy-mercedes-benz-greener-manufacturing-challenge-1336ba321d27?source=collection_archive---------9-----------------------#2020-02-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/44238a99ad8a0a64bf81b6dbaab4db8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YUtRgwlpEOV6MYSehf2Iew.jpeg"/></div></figure><p id="704c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">内容:</strong></p><ol class=""><li id="3f0b" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jp jq jr js bi translated">商业问题</li><li id="b4eb" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">现有方法</li><li id="edb7" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">探索性数据分析</li><li id="9850" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">数据准备</li><li id="d335" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">系统模型化</li><li id="d0b3" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">模型比较</li><li id="cf57" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">未来的工作</li><li id="c0ca" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">参考</li></ol><h2 id="5ac5" class="jy jz hi bd ka kb kc kd ke kf kg kh ki ix kj kk kl jb km kn ko jf kp kq kr ks bi translated"><strong class="ak"> 1。业务问题:</strong></h2><p id="5469" class="pw-post-body-paragraph im in hi io b ip kt ir is it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj hb bi translated">梅赛德斯-奔驰绿色制造竞赛的目标是开发一个机器学习模型，可以根据车辆配置准确预测汽车在测试台上花费的时间。ML模型将有助于加快测试速度，从而降低二氧化碳排放量。</p><div class="ky kz ez fb la lb"><a href="https://www.kaggle.com/c/mercedes-benz-greener-manufacturing" rel="noopener  ugc nofollow" target="_blank"><div class="lc ab dw"><div class="ld ab le cl cj lf"><h2 class="bd hj fi z dy lg ea eb lh ed ef hh bi translated">梅赛德斯-奔驰绿色制造</h2><div class="li l"><h3 class="bd b fi z dy lg ea eb lh ed ef dx translated">你能减少一辆奔驰花在测试台上的时间吗？</h3></div><div class="lj l"><p class="bd b fp z dy lg ea eb lh ed ef dx translated">www.kaggle.com</p></div></div><div class="lk l"><div class="ll l lm ln lo lk lp ik lb"/></div></div></a></div><p id="69f6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 2。现有方法:</strong></p><p id="e580" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae lq" href="https://github.com/Danila89/kaggle_mercedes/blob/master/solution_11_place.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="lr">该内核建立在观察到目标变量可能来自4个不同的高斯分布，并分别为其中4个聚类建立4个模型的基础上。建立的模型也很简单，max_depth为2，估计值&lt;为100，表明模型观察到数据的潜在趋势。</em>T9】</a></p><p id="7fc5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个内核在特性工程方面给出了很好的想法，并且只有一个xgboost模型。它还考虑了基于特征间相关性的特征移除。T13】</p><p id="e725" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae lq" href="https://www.kaggle.com/adityakumarsinha/stacked-then-averaged-models-private-lb-0-554" rel="noopener ugc nofollow" target="_blank"> </a></p><p id="b495" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae lq" href="https://www.kaggle.com/paulorzp/one-line-solution-in-python-lb-0-55453" rel="noopener ugc nofollow" target="_blank"> <em class="lr">这显示了特征X0在数据集中的重要性及其对目标</em> </a> <em class="lr">的影响。</em></p><p id="f4aa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">我的方法有什么不同:</strong></p><p id="895b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我测试了各种各样的模型，如LinearRegression、Xgboost、RandomForest、ExtraTreeRegressor、SVR和NeuralNet。这已经通过分类变量的各种编码完成了(one_hot、target、label)。降维技术，如TSVD和选择测试，也被用在这个过程中。贝叶斯优化用于超参数调整。模型的堆叠是在基于树的模型上完成的。已经进行了t检验，以检查ID特征是否为交叉验证提供了统计上的显著差异。一些功能工程也已经试用。</p><p id="04d3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lr">公制:</em> </strong></p><p id="13d6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这场比赛中的度量是R2 (R平方)得分，也称为决定系数。这个指标比MSE更容易理解。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/2cbdb8383867c4d0a2ab4093014e7c94.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*Kfhr9EvuVGkwnPXruMFhFg.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">R2与MSE的关系</figcaption></figure><p id="45a0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">R2解释了模型性能与基线模型的比较。这里的基线指的是目标变量的方差，也就是说，假设一个模型将目标的平均值作为每一列的输出，那么MSE将等于目标变量的方差。如果R2为-ve，则模型的表现比基线(每次观察的平均值)差。R2介于1和-无穷大之间。R2会像MSE一样受到异常值的影响。</p><p id="5f24" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 3。探索性数据分析:</strong></p><ul class=""><li id="2e20" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj mb jq jr js bi translated">368个二进制列，数据集中有8个分类列。</li><li id="5622" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj mb jq jr js bi translated">所有的列都是匿名的。</li><li id="340e" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj mb jq jr js bi translated">存在的分类变量具有从4到47的大范围基数。</li><li id="33d3" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj mb jq jr js bi translated">数据集中不存在缺失值。</li><li id="c575" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj mb jq jr js bi translated">数据集中有重复的列(56)。这些必须移除。</li><li id="929e" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj mb jq jr js bi translated">数据集中大约有20-30个异常值。</li></ul><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es mc"><img src="../Images/6923eb39efe135c25e935a78f817be06.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*03aZIYG_vuWiQJ3GVIEL1A.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">目标变量的箱线图</figcaption></figure><p id="76aa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">由此导致的问题是，如果你忽略离群值，将会发生以下情况——在训练的第一阶段，模型将会受到影响，因为离群值没有学习的趋势。让事情变得更糟的是(R2或MSE)对离群值来说是最糟糕的。所以当你继续推动模型时，它会记住异常值(<strong class="io hj">过度拟合！).</strong></p><p id="f929" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这可以简单地通过将异常值限制到一个阈值来解决。在这里，我们接受了离群值无法预测的事实，我们最好准备我们的模型在非离群点上工作良好。阈值选择为150。我没有尝试的另一种方法是在cv计算中忽略这些异常值造成的损失，这样模型不是基于它们的性能异常值而是基于非异常值进行排名，本质上与上述相同。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/32c0925ac88009c467081b02976b703d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*LedYo_P-7jcMhTryLLfFmg.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">目标的分布图显示了4个峰值(存在4个高斯分布)</figcaption></figure><p id="5f68" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">还存在具有不同目标的相同行(515)。这进一步证明了这一定是真实世界的数据集:-)。在这里，我试图用mean来代替targets，但这似乎让事情变得更糟。所以我保留了数据。</p><p id="06e1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">特征本身之间也有很好的相关性。现在可以有一些相互关联的特性，但却增加了模型的价值。但是这告诉我们，模型给出的特性重要性必须有所保留。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/6c3e7c85a9d4f9503d7aa7315d88dc51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*HlKgAIsUJJCqxCxTsTm0cQ.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">一瞥特征之间的相关性(最后100个)</figcaption></figure><p id="6689" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">PCA可视化(2个组件)也给出了一些有趣的结果。这虽然不好，但是通过预测每个聚类的平均值，可以充当最简单的回归变量。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/567a691abc66f886820e71a3a740c24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*o87JSQMmeij1PdfYC7XvgQ.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">PCA清楚地将数据分为3类</figcaption></figure><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/064ad997042a3c4449a1596c94d2a81a.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*EGPDRVXtpseZosyQRXdJpA.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">图表显示，含有200种成分的五氯苯甲醚足以解释99%以上的差异</figcaption></figure><p id="2186" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">有些特征(X127，X261，X314)与目标变量具有非常好的(. 65)相关性。</p><p id="69cb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">存在方差小于. 01的列，我们将删除所有这些列。X4似乎是唯一的分类变量，因此消除了这种差异。</p><p id="045b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">具有目标编码的X0(分类)是数据集中最相关的变量。8与目标的相关性。</p><p id="194b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 4。数据准备:</strong></p><p id="fef5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这里，我制作了3个具有统一格式的列的数据集(重复，方差<.01 removed="" and="" targets="" clipped="" at="" with="" :-=""/></p><ol class=""><li id="a9fe" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jp jq jr js bi translated">One-Hot Encoded Categorical</li><li id="ab9b" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">Label Encoded Categorical</li><li id="7d29" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">Target Encoded Categorical</li></ol><p id="761d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lr">)。折叠次数越多，CV越可靠。</em>T5】</strong></p><p id="6abe" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 5。造型:</strong></p><p id="13de" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我构建的第一个模型是添加了pca(12个组件)作为新特性的xgboost。该模型是具有16个模型的bagging模型，其中15个模型在各种CV折叠上训练，1个模型在完整的训练数据上训练。并输出这16个预测平均值。</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="4d4a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">第二个是对one_hot编码数据的简单LR模型。此处使用了Minmax Scaler来缩放ID列(只有数字列，而不是二进制列)，这给出了一个具有CV分数(. 58302)的合理结果。</p><p id="cbf8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">有很多kaggle内核在谈论ID特性的重要性。因此，我决定做t检验，看看这是不是真的。为此，我在250次折叠(5次折叠，50次重复)中使用了cross_val_score，以获得使用ID和不使用ID训练的模型的cv分数，这些分数成为我对scipy.stats.ttest_rel的输入，它输出p_value，如果该值&lt; . 05，则ID特征确实重要，否则不重要。</p><blockquote class="mn mo mp"><p id="1aaa" class="im in lr io b ip iq ir is it iu iv iw mq iy iz ja mr jc jd je ms jg jh ji jj hb bi translated">t检验要求样本服从高斯分布→这个条件在这里成立。</p></blockquote><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/34a5c1a7527d2389ffc08271602a1b4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*wEUH-XoizLJ0q28gv9H8ig.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">具有ID特征(橙色)和不具有ID特征(蓝色)的数据的CV分数之间的距离图</figcaption></figure><p id="96f3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">测试表明，由于p值为0.074，这在统计学上是不显著的，因此我们不能拒绝ID列改变CV值的假设。这可能是因为可以有任何其他特征传递与ID相同的信息，使得该特征冗余。</p><p id="b785" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我的第三个模型是在目标编码数据集上使用LR调优模型。这里还必须使用MinMaxScaler来将每个特征调整到相同的比例。</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="ml mm l"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">线性回归片段</figcaption></figure><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/37a0ac5d075e71719dbc376770edcb12.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*IbhkEMsJqrwiTWZrsTUU7A.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">超参数调谐图</figcaption></figure><p id="d374" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这里，我在2个不同的数据集(one_hot，label)上训练了3个基于树的模型(xgboost，RandomForest，ExtraTrees)。这些模型是手动调整的超参数，并且6个模型中的每一个都比上述线性模型表现得更好。在所有这些模型中，Xgboost表现最好。并且与目标编码数据集相比，标签数据集给出了更好的结果。</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="ml mm l"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">在ExtraTrees上进行手动调谐</figcaption></figure><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mv"><img src="../Images/c9fe9bc1653e8e6d91c2d1d810f4d83b.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*qP2ddW6LYrT431OylvMy_g.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">带有标签编码数据集的xgboost上的前10个要素重要性</figcaption></figure><p id="1031" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">接下来，我使用mlextend StackingCVRegressor对三个模型进行了堆叠，并将岭回归元作为元模型。这个模型在PrivateLB上给出了前5%的分数。这里，我在模型上保留了0 (alpha)正则化和0偏差。这实际上与取基本模型(树模型)的加权平均值相同，只是模型(山脊)用于寻找这些权重。</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="ml mm l"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">使用堆叠的片段</figcaption></figure><p id="2626" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这里，我试图按照这里的想法添加新的功能。这种内核似乎可以根据与目标变量的相关性创建新的特征，使用Spearman系数测量相关性，然后将它们相加。</p><p id="3cf7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我尝试的其他方法包括使用TSVD对one_hot数据集使用SVR，以及为FeatureSelection选择最佳测试。</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="ml mm l"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">选择此处使用最佳代码段。</figcaption></figure><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/d3e13ef25deefb583a9ed33834f4fad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*JPhV3Ieynb8g2SZ0jYL0hQ.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">230 n处的弯头_ TSVD海图中的分量</figcaption></figure><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/0055393f1fe37a066e074d775e1567e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*C2MlPyvmxuY_HzXAzJMpkA.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">超参数调谐图</figcaption></figure><p id="5810" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">已经在树模型(Xgb、RandomForest、ExtraTrees)上执行了贝叶斯优化，以查看是否可以实现分数的任何提升，但是它收敛到与手动调整超参数几乎相同的结果，除了花费非常少的时间。</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="ml mm l"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">需要将一个方法作为贝叶斯优化的强制方法来传递</figcaption></figure><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="ml mm l"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">这里是真正进行参数搜索的地方</figcaption></figure><p id="2627" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用了一个简单的深度学习模型。然而，这种架构真的很难，因为单个层就足以适应数据集。这就迫使用辍学来很好地概括。但是Dropout从来不会处理单个隐藏层，必须使用具有良好架构的多个层才能达到模型泛化良好的程度。但是这个模型没有之前的堆叠模型好。</p><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/ded43f441f19756924800d1092931270.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*P9lW-gCSLRoUFTNUBa0eLA.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">神经网络的新纪元vs R2</figcaption></figure><figure class="lt lu lv lw fd ij er es paragraph-image"><div class="er es my"><img src="../Images/5ea56a4b943ddcb6c293b23fbef9b6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*1ShLCuAKX1zdtrHI49_NuQ.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">神经网络的丢失与纪元</figcaption></figure><p id="8dd8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Gridsearch已经用于网络中的训练参数和模型参数，以找出哪一个效果最好。</p><figure class="lt lu lv lw fd ij"><div class="bz dy l di"><div class="ml mm l"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">神经网络网格搜索</figcaption></figure><p id="3ea0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 6。型号对比:</strong></p><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es mz"><img src="../Images/49bb1f7e33112797e42fe3c553b76804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JWA5sIJK8fWJ8NFA0lt4fw.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">所有试验过的模型的漂亮表格摘要</figcaption></figure><figure class="lt lu lv lw fd ij er es paragraph-image"><div role="button" tabindex="0" class="md me di mf bf mg"><div class="er es na"><img src="../Images/f2ee8d35bd14b0ff701c66322d8b0688.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*owgRC2ZKOYD8NyPbE0KzMg.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">最佳成绩截图</figcaption></figure><p id="87f1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 7。未来工作:</strong></p><ul class=""><li id="ce93" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj mb jq jr js bi translated"><strong class="io hj">好的特性&gt;聪明的模型</strong>使用交互发现好的特性总是可以改进模型。</li><li id="23ee" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj mb jq jr js bi translated">与kaggle标准相比，这里完成的堆叠非常简单，我们还可以在这方面进行改进。但是要注意小数据集的大小。</li><li id="f512" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj mb jq jr js bi translated">我看到一些kaggle内核有4个特性，并获得了前1%的结果，因此特性选择可以进一步改进。</li><li id="6a5d" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj mb jq jr js bi translated">使用噪声可以更好地完成这里使用的目标编码，以避免目标泄漏。它被称为影响编码。可以试试。</li><li id="46b5" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj mb jq jr js bi translated">神经网络架构可以进一步探索各种辍学和隐藏层，纪元在这里没有调整，所以肯定有改进的空间。</li></ul><blockquote class="mn mo mp"><p id="a85c" class="im in lr io b ip iq ir is it iu iv iw mq iy iz ja mr jc jd je ms jg jh ji jj hb bi translated">N <strong class="io hj">注:</strong>此处显示的代码已更改，以提高博客的可读性。请查看<a class="ae lq" href="https://github.com/level14taken/mercedes-benz-greener-manufacturing/blob/master/SelfCaseStudy.ipynb" rel="noopener ugc nofollow" target="_blank">代码</a>以了解完整的项目。</p></blockquote><p id="0d09" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 8。参考文献:</strong></p><p id="de0a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae lq" rel="noopener" href="/@george.drakos62/how-to-select-the-right-evaluation-metric-for-machine-learning-models-part-1-regrression-metrics-3606e25beae0">回归度量</a></p><p id="9b94" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae lq" href="https://github.com/GKarmakar/RegressionUsingNN/blob/master/RegressionUsingNeuralNetwork.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/GKarmakar/RegressionUsingNN/blob/master/regressionusingneuralnetwork . ipynb</a><br/>neural _ net:-<a class="ae lq" href="https://www.kaggle.com/frednavruzov/baseline-to-start-with-keras-lb-0-55" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/frednavruzov/baseline-to-start-with-keras-l b-0-55</a></p><p id="8458" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Bayesian _ optimization:-<a class="ae lq" href="https://www.kaggle.com/btyuhas/bayesian-optimization-with-xgboos" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/btyuhas/Bayesian-optimization-with-XG boos</a>t</p><p id="fd20" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">t-test:-<a class="ae lq" href="https://machinelearningmastery.com/statistical-significance-tests-in-python/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/statistical-significance-tests-in-python/</a></p><p id="5330" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">异常值检测:-<a class="ae lq" href="https://stackoverflow.com/questions/22354094/pythonic-way-of-detecting-outliers-in-one-dimensional-observation-data" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/22354094/python-way-of-detecting-outliers-in-one-dimension-observation-data</a></p><p id="1872" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">课程:-<a class="ae lq" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com</a></p><p id="7cfc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">Linkedin:-</strong><a class="ae lq" href="https://www.linkedin.com/in/manoj-guthikonda-5405b4112/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/manoj-guthikonda-5405b4112/</a></p><p id="1a76" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">Github</strong>:-<a class="ae lq" href="https://github.com/level14taken" rel="noopener ugc nofollow" target="_blank">https://github.com/level14taken</a></p></div></div>    
</body>
</html>