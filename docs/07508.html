<html>
<head>
<title>Easily Implement Different Transformers🤗🤗 through Hugging Face</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">轻松实现不同的变压器🤗🤗通过拥抱脸</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/easily-implement-different-transformers-through-hugging-face-e471035e9c86?source=collection_archive---------10-----------------------#2020-06-28">https://medium.com/analytics-vidhya/easily-implement-different-transformers-through-hugging-face-e471035e9c86?source=collection_archive---------10-----------------------#2020-06-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="1d47" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">变形金刚是最先进的模型，已经被用于以非常有效的方式解决从情感分析到问题/回答的新颖的NLP任务。然而，变形金刚最基本的功能只是注意力机制的编码器层的堆叠。即使使用Pytorch或Tensorflow之类的DL框架，从头实现它也是相当困难和具有挑战性的。然而拥抱脸使得实现各种类型的变形金刚变得非常容易。在本文中，我将向您展示如何通过拥抱人脸库在Tensorflow(Keras)中轻松实现变形金刚。</p></blockquote></div><div class="ab cl jh ji gp jj" role="separator"><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm"/></div><div class="hb hc hd he hf"><h1 id="d4f9" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">你需要什么:</strong></h1><p id="1b1f" class="pw-post-body-paragraph ii ij hi il b im km io ip iq kn is it ko kp iw ix kq kr ja jb ks kt je jf jg hb bi translated">首先你需要安装拥抱脸库，这真的很容易。只需简单地安装它:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="2407" class="ld jp hi kz b fi le lf l lg lh">pip install transformers </span></pre><p id="1f1e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">其次，您将需要最新的TensorFlow版本，该版本也可以通过pip轻松安装。</p><p id="a49d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated"><strong class="il hj">数据:</strong></p><p id="991f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">为了测试和实现不同的转换器，我使用了kaggle竞赛中的数据。这是最近的一个比赛，我参加了一个名为<a class="ae li" href="https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification" rel="noopener ugc nofollow" target="_blank">拼图-多语言-有毒-评论-分类</a>的比赛。但是，使用相同的数据并不是强制性的，因为下面的实现可以很容易地适应任何文本数据。</p><p id="46c4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">这场比赛给出了不同的评论，我们的任务是检测特定的评论是否有毒。因此，这是一个二元分类任务。</p><p id="33db" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated"><strong class="il hj">强大计算能力:</strong></p><p id="300e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">还要注意，变压器有数百万个参数，因此我利用Kaggle内核提供的TPU来训练我的模型。或者，如果您没有强大的本地机器，您可以使用google colab来跟踪本文的实现。</p></div><div class="ab cl jh ji gp jj" role="separator"><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm"/></div><div class="hb hc hd he hf"><h1 id="bdac" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">让我们享受实现变形金刚的乐趣:</strong></h1><figure class="ku kv kw kx fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lj"><img src="../Images/9cc09362cda60ea8bdcf31fe063246b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sQQ9KJMud6uGsv82XNU3TA.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">图片来自<a class="ae li" href="https://huggingface.co/front/thumbnails/models.png" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/front/thumbnails/models.png</a></figcaption></figure><p id="ecbc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated"><strong class="il hj">进口</strong></p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="1b1d" class="ld jp hi kz b fi le lf l lg lh">import numpy as np <em class="ik"># linear algebra</em><br/>import pandas as pd <em class="ik"># data processing, CSV file I/O (e.g. pd.read_csv)</em><br/>import tensorflow as tf<br/>import tensorflow_hub as hub<br/>from tqdm import tqdm<br/>from tqdm import tqdm_notebook<br/>from sklearn.metrics import auc<br/>from sklearn.metrics import classification_report<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/><br/>from transformers import AutoTokenizer,BertTokenizer,TFBertModel,TFOpenAIGPTModel,OpenAIGPTTokenizer,DistilBertTokenizer, TFDistilBertModel,XLMTokenizer, TFXLMModel<br/>from transformers import TFAutoModel, AutoTokenizer<br/>from kaggle_datasets import KaggleDatasets<br/>from sklearn.metrics import roc_curve,confusion_matrix,auc<br/>from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors<br/><em class="ik"># Input data files are available in the read-only "../input/" directory</em><br/><em class="ik"># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</em><br/>import matplotlib as mpl<br/><br/><br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.metrics import confusion_matrix, accuracy_score<br/>from sklearn.model_selection import train_test_split<br/><br/>from tensorflow.keras.preprocessing.text import Tokenizer<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import *<br/>from tensorflow.keras.initializers import Constant</span></pre><p id="c26f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated"><strong class="il hj">哪些变压器:</strong></p><p id="3409" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">以下变压器架构已在笔记本电脑中进行了测试</p><p id="e03c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">1-伯特</p><p id="cc81" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">2-OpenAIGPT</p><p id="952b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">3-蒸馏啤酒</p><p id="0af8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">4-XLM</p><p id="51f5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">5-xlmrobertalage</p><p id="1380" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">不用担心所有这些变形金刚的实现。实现简单且相似。</p><p id="933f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated"><strong class="il hj">使用的超参数:</strong></p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="24ed" class="ld jp hi kz b fi le lf l lg lh">EPOCHS=2</span><span id="8de1" class="ld jp hi kz b fi lv lf l lg lh">max_seq_length = 192<br/>LEARNING_RATE=1e-5<br/>early_stopping=early_stopping = tf.keras.callbacks.EarlyStopping(<br/>    monitor='val_loss', <br/>    verbose=1,<br/>    patience=10,<br/>    mode='max',<br/>    restore_best_weights=True)</span></pre><p id="5c8b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated"><strong class="il hj">编码功能:</strong></p><p id="d5de" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">每一个变形金刚都对每一句话进行编码。我希望你能理解这句话的含义。如果没有，那么他们在互联网上有许多了解编码的好资源。在一个非常基本的层面上，编码意味着通过为我们语料库中的每个单词(标记)分配一个唯一的整数来将原始文本数据转换为数字数据。然而，transformer编码稍微复杂一点，因为它也使用字符级编码，将未知单词分解成单个字符，然后进行编码。然而，我不会进一步深入变压器编码如何工作的细节，因为它相当详细。可以肯定地说，下一个函数基本上将数据中的每个句子转换成各种转换器可以理解的特殊整数列表:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="7336" class="ld jp hi kz b fi le lf l lg lh">def single_encoding_function(text,tokenizer,name='BERT'):<br/>    input_ids=[]<br/>    if name=='BERT':<br/>        tokenizer.pad_token ='[PAD]'<br/>    elif name=='OPENAIGPT2':<br/>        tokenizer.pad_token='&lt;unk&gt;'<br/>    elif name=='Transformer XL':<br/>        print(tokenizer.eos_token)<br/>        tokenizer.pad_token= tokenizer.eos_token<br/>    elif name=='DistilBert':<br/>        tokenizer.pad_token='[PAD]'<br/>    <br/>for sentence <strong class="kz hj">in</strong> tqdm(text):       encoded=tokenizer.encode(sentence,max_length=max_seq_length,<br/>pad_to_aax_length=True)## this is inside the loop<br/>        input_ids.append(encoded)<br/>    return input_ids</span></pre><p id="caca" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated"><strong class="il hj">制作数据管道:</strong></p><p id="b981" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">a)制作阵列:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="5e6b" class="ld jp hi kz b fi le lf l lg lh">X_train=np.array(single_encoding_function(train_raw['comment_text'].values.tolist(),tokenizer,name="BERT"))<br/>y_train=np.array(train_raw['toxic'])<br/>X_valid=np.array(single_encoding_function(valid_raw['comment_text'].values.tolist(),tokenizer,name="BERT"))<br/>y_valid=np.array(valid_raw['toxic'])<br/>X_test=np.array(single_encoding_function(test_raw['content'].values.tolist(),tokenizer,name="BERT"))</span><span id="1343" class="ld jp hi kz b fi lv lf l lg lh">steps_per_epoch = X_train.shape[0] // BATCH_SIZE</span></pre><p id="437f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">上面的代码是不言自明的，我只是将原始文本数据作为输入提供给单个编码函数，然后将结果转换为编码令牌的数组，这是提供给TensorFlow管道的最终数据。</p><p id="97e0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">b)制作张量流管道:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="82db" class="ld jp hi kz b fi le lf l lg lh">def make_data():<br/>    train = (<br/>        tf.data.Dataset<br/>        .from_tensor_slices((X_train, y_train))<br/>        .repeat()<br/>        .shuffle(2048)<br/>        .batch(BATCH_SIZE)<br/>        .prefetch(AUTO))<br/><br/>    valid = (<br/>        tf.data.Dataset<br/>        .from_tensor_slices((X_valid, y_valid))<br/>        .batch(BATCH_SIZE)<br/>        .cache()<br/>        .prefetch(AUTO)<br/>    )<br/><br/>    test = (<br/>        tf.data.Dataset<br/>        .from_tensor_slices(X_test)<br/>        .batch(BATCH_SIZE)<br/>    )<br/>    return train,valid,test </span></pre><p id="b903" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated"><strong class="il hj">与Keras的实际实施:</strong></p><p id="7de8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">下一步真的很重要，所以仔细看看:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="a871" class="ld jp hi kz b fi le lf l lg lh">def build_model(transformer_layer,max_len=max_seq_length):<br/>    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")<br/>    sequence_output = transformer_layer(input_word_ids)[0]<br/>    <br/>    cls_token = sequence_output[:, 0, :]<br/>    out = tf.keras.layers.Dense(1, activation='sigmoid')(cls_token)<br/>    <br/>    model = tf.keras.Model(inputs=input_word_ids, outputs=out)<br/>    <br/>    <br/>    return model</span></pre><p id="c5f4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">代码块很重要，因此让我进一步阐述它。首先有一个输入层，它为一个特定的实例接受给定变换器的<strong class="il hj">编码输入。然后，输入令牌被输入到主transformer层(从即将到来的代码块中定义的名为compile_model的函数中加载)。我想让这些代码对所有的变形金刚都是可重用的，因此不是复制和粘贴每个变形金刚的整个模型，唯一不同的层是变形金刚的变形金刚层。然后，转换器层输出序列输出。然而，这是一个分类任务，因此从序列输出中，我们将只提取给定句子中每个单词的CLS(分类标记)。这个cls_token然后被馈送到用于区分给定句子的毒性的sigmoid层。</strong></p><p id="3c23" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">请注意，您可以通过添加更多的层来使模型更加复杂，但我没有这样做，因为这会使我们的模型更加复杂，并且会花费更多的训练时间。</p><h2 id="89bf" class="ld jp hi bd jq lw lx ly ju lz ma mb jy ko mc md kc kq me mf kg ks mg mh kk mi bi translated"><strong class="ak">下一节仅展示如何绘制各变压器性能的相关有用图表，以便对不同模型进行对比分析。它们与任何转换器的主要实现都有关系。</strong></h2><p id="27f8" class="pw-post-body-paragraph ii ij hi il b im km io ip iq kn is it ko kp iw ix kq kr ja jb ks kt je jf jg hb bi translated"><strong class="il hj">绘制有用的图表来比较性能:</strong></p><p id="0b1e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">a)绘制损耗和度量图:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="d90c" class="ld jp hi kz b fi le lf l lg lh">mpl.rcParams['figure.figsize'] = (12, 10)<br/>colors = plt.rcParams['axes.prop_cycle'].by_key()['color']<br/><br/>def plot_loss(history):<br/><em class="ik"># Use a log scale to show the wide range of values.</em><br/>    plt.semilogy(history.epoch,  history.history['loss'],<br/>               color='red', label='Train Loss')<br/>    plt.semilogy(history.epoch,  history.history['val_loss'],<br/>          color='green', label='Val Loss',<br/>          linestyle="--")<br/>    plt.xlabel('Epoch')<br/>    plt.ylabel('Loss')<br/>  <br/>    plt.legend()<br/>    <br/>    <br/>def plot_metrics(history):<br/>    metrics =  ['loss', 'auc', 'precision', 'recall']<br/>    for n, metric <strong class="kz hj">in</strong> enumerate(metrics):<br/>        name = metric.replace("_"," ").capitalize()<br/>        plt.subplot(2,2,n+1)<br/>        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')<br/>        plt.plot(history.epoch, history.history['val_'+metric],<br/>                 color=colors[0], linestyle="--", label='Val')<br/>        plt.xlabel('Epoch')<br/>        plt.ylabel(name)<br/>        if metric == 'loss':<br/>            plt.ylim([0, plt.ylim()[1]])<br/>        elif metric == 'auc':<br/>            plt.ylim([0.8,1])<br/>        else:<br/>            plt.ylim([0,1])<br/><br/>        plt.legend()</span></pre><p id="70fb" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">这两个函数都采用训练的历史，然后绘制丢失和度量的相关函数，即AUC、召回和多个时期的精度。</p><p id="78c8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">b)绘制混淆矩阵和ROC曲线:</p><p id="5f07" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">下一个代码块从模型和基础事实中获取y_predicted，为模型创建混淆矩阵和ROC曲线。</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="16d6" class="ld jp hi kz b fi le lf l lg lh">def plot_cm(y_true, y_pred, title):<br/>    <em class="ik">''''</em><br/><em class="ik">    input y_true-Ground Truth Labels</em><br/><em class="ik">          y_pred-Predicted Value of Model</em><br/><em class="ik">          title-What Title to give to the confusion matrix</em><br/><em class="ik">    </em><br/><em class="ik">    Draws a Confusion Matrix for better understanding of how the model is working</em><br/><em class="ik">    </em><br/><em class="ik">    return None</em><br/><em class="ik">    </em><br/><em class="ik">    '''</em><br/>    <br/>    figsize=(10,10)<br/>    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))<br/>    cm_sum = np.sum(cm, axis=1, keepdims=True)<br/>    cm_perc = cm / cm_sum.astype(float) * 100<br/>    annot = np.empty_like(cm).astype(str)<br/>    nrows, ncols = cm.shape<br/>    for i <strong class="kz hj">in</strong> range(nrows):<br/>        for j <strong class="kz hj">in</strong> range(ncols):<br/>            c = cm[i, j]<br/>            p = cm_perc[i, j]<br/>            if i == j:<br/>                s = cm_sum[i]<br/>                annot[i, j] = '<strong class="kz hj">%.1f%%\n%d</strong>/<strong class="kz hj">%d</strong>' % (p, c, s)<br/>            elif c == 0:<br/>                annot[i, j] = ''<br/>            else:<br/>                annot[i, j] = '<strong class="kz hj">%.1f%%\n%d</strong>' % (p, c)<br/>    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))<br/>    cm.index.name = 'Actual'<br/>    cm.columns.name = 'Predicted'<br/>    fig, ax = plt.subplots(figsize=figsize)<br/>    plt.title(title)<br/>    sns.heatmap(cm, cmap= "YlGnBu", annot=annot, fmt='', ax=ax)<br/><br/>def roc_curve_plot(fpr,tpr,roc_auc):<br/>    plt.figure()<br/>    lw = 2<br/>    plt.plot(fpr, tpr, color='darkorange',<br/>             lw=lw, label='ROC curve (area = <strong class="kz hj">%0.2f</strong>)' %roc_auc)<br/>    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')<br/>    plt.xlim([0.0, 1.0])<br/>    plt.ylim([0.0, 1.05])<br/>    plt.xlabel('False Positive Rate')<br/>    plt.ylabel('True Positive Rate')<br/>    plt.title('Receiver operating characteristic example')<br/>    plt.legend(loc="lower right")<br/>    plt.show()</span></pre><p id="fed8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated"><strong class="il hj">最后:D训练不同的变形金刚:</strong></p><p id="2dd4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">a)编译模型:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="1e2b" class="ld jp hi kz b fi le lf l lg lh">def compile_model(name):<br/>    with strategy.scope():<br/>        METRICS = [<br/>          tf.keras.metrics.TruePositives(name='tp'),<br/>          tf.keras.metrics.FalsePositives(name='fp'),<br/>          tf.keras.metrics.TrueNegatives(name='tn'),<br/>          tf.keras.metrics.FalseNegatives(name='fn'), <br/>          tf.keras.metrics.BinaryAccuracy(name='accuracy'),<br/>          tf.keras.metrics.Precision(name='precision'),<br/>          tf.keras.metrics.Recall(name='recall'),<br/>          tf.keras.metrics.AUC(name='auc')]<br/>        if name=='bert-base-uncased':<br/>            transformer_layer = (<br/>                TFBertModel.from_pretrained(name)<br/>            )<br/>        elif name=='openai-gpt':<br/>            transformer_layer = (<br/>                TFOpenAIGPTModel.from_pretrained(name)<br/>            )<br/>        elif name=='distilbert-base-cased':<br/>            transformer_layer = (<br/>                TFDistilBertModel.from_pretrained(name)<br/>            )<br/>        elif name=='xlm-mlm-en-2048':<br/>            transformer_layer = (<br/>                TFBertModel.from_pretrained(name)<br/>            )<br/>        elif name=='jplu/tf-xlm-roberta-large':<br/>            transformer_layer = (<br/>                TFAutoModel.from_pretrained(name)<br/>            )<br/>        model = build_model(transformer_layer, max_len=max_seq_length)<br/>        model.compile(optimizer=tf.keras.optimizers.Adam(<br/>        learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=METRICS)<br/>    return model</span></pre><p id="3151" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">上面的代码块真的很简单。如图所示，它采用您要编译模型的变压器的名称，然后从hugging face library加载相关的变压器层。然后，它将加载的transformer层提供给函数build_model(上面定义的),然后我们编译这个模型。请注意，我还创建了一个名为METRICS的列表，因为我想检查不同指标的模型性能，而不是限制自己只关注准确性。</p><p id="6917" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">d)实际培训:</p><p id="6b19" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">现在，每个变压器的实际训练过程是相同的。你只需要输入相关的名字并调用期望的函数来适应这个模型。</p><p id="65eb" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">在本文中，我不会显示每个模型的输出图，因为这将占用大量的空间，但是，我将只显示一个变压器的图形，即提取的BERT。可以以类似的方式为其他模型生成相同的图形。</p><p id="3d6b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">接下来的代码块展示了如何使用上面为提取的BERT定义的函数。为了训练任何其他转换器，您只需要将名为“distilt-base-cased”和“DistilBert”(粗体突出显示)的字符串更改为相关的转换器名称。</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="042c" class="ld jp hi kz b fi le lf l lg lh"><em class="ik"># # First load the real tokenizer</em><br/>tokenizer = DistilBertTokenizer.from_pretrained(<strong class="kz hj">'distilbert-base-cased'</strong>)</span><span id="a1a7" class="ld jp hi kz b fi lv lf l lg lh">X_train=np.array(single_encoding_function(train_raw['comment_text'],tokenizer,<strong class="kz hj">'DistilBert'</strong>))#change the name<br/>y_train=np.array(train_raw['toxic'])<br/>X_valid=np.array(single_encoding_function(valid_raw['comment_text'],tokenizer,<strong class="kz hj">'DistilBert'</strong>))#change the name<br/>y_valid=np.array(valid_raw['toxic'])<br/>X_test=np.array(single_encoding_function(test_raw['content'],tokenizer,<strong class="kz hj">'DistilBert'</strong>))#change the name</span><span id="08db" class="ld jp hi kz b fi lv lf l lg lh">train,valid,test=make_data()</span><span id="9688" class="ld jp hi kz b fi lv lf l lg lh">steps_per_epoch = X_train.shape[0] // BATCH_SIZE</span><span id="df1e" class="ld jp hi kz b fi lv lf l lg lh">model=compile_model(<strong class="kz hj">'distilbert-base-cased'</strong>)#change the name<br/>print(model.summary())<br/><br/>history=model.fit(<br/>    train,steps_per_epoch=steps_per_epoch,<br/>    epochs=EPOCHS,callbacks=[early_stopping], validation_data=valid<br/>)</span></pre><figure class="ku kv kw kx fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mj"><img src="../Images/bb6b097781b3c8a80651fabc066ba0d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kQegPh6HQ8B-ieONnB5xCg.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">模型总结和培训信息</figcaption></figure><p id="1d0a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">该模型将开始训练，并需要很长时间，这取决于您的数据和计算能力。最后，获取历史记录并使用上面定义的相关函数来生成图表:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="f6a4" class="ld jp hi kz b fi le lf l lg lh">plot_loss(history)</span></pre><figure class="ku kv kw kx fd lk er es paragraph-image"><div class="er es mk"><img src="../Images/0622602c26af4a6e4d692a588e969a0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*N-KP-5zSDoJ3JYP0lsr1Ow.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">distilt-BERT两个时期的损失</figcaption></figure><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="e573" class="ld jp hi kz b fi le lf l lg lh">plot_metrics(history)</span></pre><figure class="ku kv kw kx fd lk er es paragraph-image"><div class="er es ml"><img src="../Images/5d45fd7f521f145c7f09195aaaf178cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*mMX5azrJFp_PBlta6msWWw.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">各时期的损失、AUC、精确度和召回</figcaption></figure><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="1ad1" class="ld jp hi kz b fi le lf l lg lh">y_predict=model.predict(valid, verbose=1)<br/>y_predict[ y_predict&gt; 0.5] = 1<br/>y_predict[y_predict &lt;= 0.5] = 0<br/>plot_cm(y_valid, y_predict, 'Distil BERT Performance-Confusion Matrix')</span></pre><figure class="ku kv kw kx fd lk er es paragraph-image"><div class="er es mm"><img src="../Images/499f002b4dc58e6562124fc739bf2c9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*-uJRAurMINyIC_o-p5zHkA.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">蒸馏水混淆矩阵</figcaption></figure><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="0414" class="ld jp hi kz b fi le lf l lg lh">y_predict_prob=model.predict(valid, verbose=1)<br/>fpr, tpr, _ = roc_curve(y_valid,y_predict_prob)<br/>roc_auc = auc(fpr, tpr)<br/>roc_curve_plot(fpr,tpr,roc_auc)</span></pre><figure class="ku kv kw kx fd lk er es paragraph-image"><div class="er es mn"><img src="../Images/7a9017abac03ac9ece4f812e19b73938.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*cEMpoJE_RThl-5dannOgWw.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">一种蒸馏啤酒用摇床</figcaption></figure><h1 id="bed0" class="jo jp hi bd jq jr mo jt ju jv mp jx jy jz mq kb kc kd mr kf kg kh ms kj kk kl bi translated">结论:</h1><p id="3e47" class="pw-post-body-paragraph ii ij hi il b im km io ip iq kn is it ko kp iw ix kq kr ja jb ks kt je jf jg hb bi translated">嗯，拥抱脸真的让transformer的实现变得非常容易。然而，理解转换器的底层工作也非常重要，因为否则上述实现将只是一个黑盒，您将无法进一步调整和优化您的模型。我的kaggle账户上也有完整的代码:<a class="ae li" href="https://www.kaggle.com/keenborder/comparing-different-transformers-lstms" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/keen border/comparing-different-transformers-lstms</a>如果你想更详细地了解变形金刚，这里也有链接。</p></div><div class="ab cl jh ji gp jj" role="separator"><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm jn"/><span class="jk bw bk jl jm"/></div><div class="hb hc hd he hf"><p id="28e2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ko iv iw ix kq iz ja jb ks jd je jf jg hb bi translated">我为这篇文章真的很努力，因此，请鼓掌，如果你喜欢它，并想看到更多可怕的NLP内容。</p></div></div>    
</body>
</html>