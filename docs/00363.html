<html>
<head>
<title>Ridge Regression with Multicollinearity in Pyhton</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Pyhton多重共线性岭回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ridge-regression-with-multicollinearity-in-pyhton-c8f6f3385e4c?source=collection_archive---------0-----------------------#2019-04-27">https://medium.com/analytics-vidhya/ridge-regression-with-multicollinearity-in-pyhton-c8f6f3385e4c?source=collection_archive---------0-----------------------#2019-04-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/80db452e5dea779dd78277b876658086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gJqw6ULeA4YL5vXWn9j0OQ.png"/></div></div></figure><div class=""/><p id="4a2f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">定义岭回归</strong></p><p id="d67f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi jo translated"><span class="l jp jq jr bm js jt ju jv jw di"> R </span> idge回归是一种用于分析多重回归数据的技术，该数据存在多重共线性，岭回归使用的特殊类型被称为L2正则化。在岭回归中，惩罚是系数的平方和。L2正则化又名岭正则化-这在模型中添加了正则化项，它是参数系数平方的函数。参数的系数可以接近零，但永远不会变为零，因此。</p><p id="0b12" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">多重共线性</strong></p><p id="a349" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">多重共线性是指独立变量之间存在近似线性的关系。</p><p id="8dc8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">多重共线性的影响</strong></p><p id="97b1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">多重共线性会产生不准确的回归系数估计值，增大<br/>回归系数的标准误差，缩小回归系数的部分t检验，给出错误的、不显著的p值，并降低模型的可预测性(这只是开始)。</p><p id="6f6f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">多重共线性的来源</strong></p><p id="ac02" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要处理多重共线性，您必须能够识别其来源。多重共线性的来源会影响线性模型的分析、校正和解释。有五个来源(详见Montgomery [1982]:</p><ol class=""><li id="2c7d" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated">数据收集。在这种情况下，数据是从独立<br/>变量的狭窄子空间中收集的。多重共线性是由抽样方法创建的-它不存在于<br/>总体中。在扩大的范围内获得更多的数据可以解决多重共线性问题。最极端的例子是当你试图用一条线来拟合一个点的时候。</li><li id="04a4" class="jx jy ht is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">线性模型或总体的物理约束。无论使用何种取样技术，多重共线性的来源都将存在。许多制造或服务流程对<br/>独立变量(就其范围而言)有物理、政治或法律上的限制，这将产生<br/>多重共线性。</li><li id="fdc1" class="jx jy ht is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">过度定义的模型。这里，变量比观察值多。这种情况应该避免。</li><li id="d710" class="jx jy ht is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">型号选择或规格。多重共线性的来源是使用独立变量<br/>,这些变量是一组原始变量的幂或相互作用。需要注意的是，如果独立变量的采样<br/>子空间很窄，那么这些变量的任何组合都会进一步增加多重共线性问题。</li><li id="881a" class="jx jy ht is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">离群值。X空间中的极值或异常值会导致多重共线性，也会隐藏多重共线性。我们称之为<br/>这种离群值导致的多重共线性。这应该通过在应用岭<br/>回归之前移除异常值来校正</li></ol><p id="cebe" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">多重共线性检测</strong></p><p id="03fd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">检测多重共线性有几种方法。我们举几个例子。</p><ol class=""><li id="fa68" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated">从研究独立变量对的成对散点图开始，寻找近乎完美的关系。也可以看一下相关性矩阵，了解高相关性。不幸的是，当一次考虑两个变量时，多重共线性并不总是出现。</li><li id="d5b9" class="jx jy ht is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">考虑方差通货膨胀因素(VIF)。波动率指数超过10表示共线变量。</li><li id="5ec0" class="jx jy ht is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">自变量的相关矩阵的特征值接近零表示多重共线性。不要看特征值的数值大小，使用条件数。大条件表示多重共线性。</li><li id="6bf6" class="jx jy ht is b it kg ix kh jb ki jf kj jj kk jn kc kd ke kf bi translated">调查回归系数的符号。回归系数符号与预期相反的变量可能表示多重共线性。</li></ol><p id="6363" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">多重共线性校正</strong></p><p id="c22f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">根据多重共线性的来源，解决方案会有所不同。如果多重共线性是由数据收集产生的，则在更宽的X子空间上收集额外的数据。如果线性模型的选择增加了多重共线性，请使用变量选择技术简化模型。如果一个或两个观察值导致了多重共线性，则删除这些观察值。最重要的是，一开始就要小心选择变量。当这些步骤不可行时，您可以尝试岭回归。</p><p id="710f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">岭回归模型<br/> </strong>按照通常的符号，假设我们的回归方程以矩阵形式写成</p><figure class="km kn ko kp fd hk er es paragraph-image"><div class="er es kl"><img src="../Images/7f30008e819db71c6e98b214691e24cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*4LAWVCcRHkL9S7mdk9zjIA.png"/></div></figure><p id="c285" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中<strong class="is hu"> Y </strong>为因变量，<strong class="is hu"> X </strong>代表自变量，<strong class="is hu"> B </strong>为待估计的回归系数，<strong class="is hu"> e </strong>代表误差为残差。</p><p id="0c0b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">转到岭回归</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="8c58" class="kv kw ht kr b fi kx ky l kz la">import mglearn<br/>from sklearn.model_selection import train_test_split<br/>X, y = mglearn.datasets.load_extended_boston()<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)</span></pre><p id="6406" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">出局:</p><p id="05c1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">训练集得分:0.89 <br/>测试集得分:0.75</p><p id="dbe8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">岭是一个更受限制的模型，所以这个模型过拟合。不太复杂的模型意味着在训练集上的较差性能，但如果模型过于复杂，这是不好的，因为可能会过度拟合。用户可以使用alpha参数来指定模型对简单性与训练集性能的重视程度。在前面的例子中，我们使用默认参数alpha=1.0。尽管如此，没有理由说这会给我们最好的权衡。<br/>alpha的最佳设置取决于我们使用的特定数据集。<br/>增加alpha迫使系数更趋向于零，这降低了训练集的性能，但可能有助于泛化。例如:</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="9ef3" class="kv kw ht kr b fi kx ky l kz la">ridge10 = Ridge(alpha=10).fit(X_train, y_train)</span><span id="f4c5" class="kv kw ht kr b fi lb ky l kz la">print("Training set score: {:.2f}".format(ridge10.score(X_train, y_train)))</span><span id="2c88" class="kv kw ht kr b fi lb ky l kz la">print("Test set score: {:.2f}".format(ridge10.score(X_test, y_test)))</span></pre><p id="5618" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">出局:</strong></p><p id="bf3d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">训练集得分:0.79 <br/>测试集得分:0.64</p><p id="2f68" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">减小alpha值可以减少系数的限制。对于非常小的α值，系数几乎不受任何限制</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="921d" class="kv kw ht kr b fi kx ky l kz la">ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)</span><span id="c19a" class="kv kw ht kr b fi lb ky l kz la">print("Training set score: {:.2f}".format(ridge01.score(X_train, y_train)))</span><span id="82c2" class="kv kw ht kr b fi lb ky l kz la">print("Test set score: {:.2f}".format(ridge01.score(X_test, y_test)))</span></pre><p id="7e1c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">出局:</strong></p><p id="1844" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">训练集得分:0.93 <br/>测试集得分:0.77</p><p id="0c63" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过检查具有不同alpha值的模型的coef_ attribute，我们可以更定性地了解alpha参数如何改变模型。更高的alpha意味着更受限制的模型，因此我们期望coef_的条目对于高alpha值比对于低alpha值具有更小的量值。为了便于比较，我们使用线性回归。这在图中得到证实:</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="8b7a" class="kv kw ht kr b fi kx ky l kz la">from sklearn.linear_model import LinearRegression<br/>lr = LinearRegression().fit(X_train, y_train)</span><span id="b483" class="kv kw ht kr b fi lb ky l kz la">import matplotlib.pyplot as plt</span><span id="cf5a" class="kv kw ht kr b fi lb ky l kz la">plt.plot(ridge.coef_, 's', label="Ridge alpha=1")<br/>plt.plot(ridge10.coef_, '^', label="Ridge alpha=10")<br/>plt.plot(ridge01.coef_, 'v', label="Ridge alpha=0.1")<br/>plt.plot(lr.coef_, 'o', label="LinearRegression")</span><span id="a9bd" class="kv kw ht kr b fi lb ky l kz la">plt.xlabel("Coefficient index")<br/>plt.ylabel("Coefficient magnitude")<br/>plt.hlines(0, 0, len(lr.coef_))<br/>plt.ylim(-25, 25)<br/>plt.legend()</span></pre><p id="ee16" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">出局:</strong></p><figure class="km kn ko kp fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lc"><img src="../Images/b1120326576380f6cbd0e45bb646346c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mzaZzHaBErkZ_rIOwmcoGQ.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">用不同的α和线性回归的<br/>值比较岭回归的系数大小</figcaption></figure><p id="adcf" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里，x轴枚举coef_ : x=0表示与第一个特征相关的系数，x=1表示与第二个特征相关的系数，以此类推，直到x=100。y轴显示系数的相应值的数值。这里的主要要点是，对于alpha=10，系数大多在–3和3之间。α= 1的脊模型的系数稍大。对应于alpha=0.1的点仍然具有较大的量值，并且对应于没有任何正则化的线性回归(alpha=0)的许多点非常大，以至于它们在图表之外。</p><p id="d75b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">理解正则化影响的另一种方法是固定alpha值，但改变可用的训练数据量。对于图2，我们对波士顿住房数据集进行了二次抽样，并对规模不断增加的子集评估了线性回归和岭(alpha=1)(显示模型性能作为数据集规模的函数的图称为学习曲线):</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="7108" class="kv kw ht kr b fi kx ky l kz la">mglearn.plots.plot_ridge_n_samples()</span></pre><figure class="km kn ko kp fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lc"><img src="../Images/dd074dd072da2a44aaaa605829f2012a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TbHff9JCeCBeS4gRKTRJ4w.png"/></div></div><figcaption class="ld le et er es lf lg bd b be z dx translated">图二。波士顿<br/>住房数据集的岭回归和线性回归的学习曲线</figcaption></figure><p id="9c49" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如所料，对于所有数据集大小，无论是岭回归还是线性回归，训练分数都高于测试分数。因为岭是正则化的，所以岭的训练分低于全线线性回归的训练分。然而，岭的测试分数更好，特别是对于小的数据子集。对于少于400个数据点，线性回归不能学习任何东西。随着越来越多的数据可供模型使用，两个模型都得到了改进，最终线性回归赶上了岭回归。这里的教训是，有了足够的训练数据，正则化就变得不那么重要了，给定足够的数据，岭和线性回归就会有相同的性能(在这里使用完整数据集时出现这种情况只是偶然的)。图2的另一个有趣的方面是线性回归的训练性能下降。如果添加了更多的数据，模型就更难适应或记忆数据。</p><h2 id="ebfe" class="kv kw ht bd lh li lj lk ll lm ln lo lp jb lq lr ls jf lt lu lv jj lw lx ly lz bi translated">目标= RSS + α *(系数的平方和)</h2><p id="382a" class="pw-post-body-paragraph iq ir ht is b it ma iv iw ix mb iz ja jb mc jd je jf md jh ji jj me jl jm jn hb bi translated">这里，α (alpha)是平衡最小化RSS和最小化系数平方和的强调量的参数。α可以取不同的值:</p><ol class=""><li id="4f9f" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated"><strong class="is hu"> α = 0: </strong></li></ol><p id="9c53" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">目标变得与简单线性回归相同。</p><p id="9d76" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将得到与简单线性回归相同的系数。</p><p id="2d0e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> 2。α = ∞: </strong></p><p id="5ed3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">系数将为零。为什么？因为系数平方的权重是无限的，所以任何小于零的值都会使目标无限。</p><p id="1f32" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> 3。0 &lt; α &lt; ∞: </strong></p><p id="c432" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">α的大小将决定给予目标不同部分的权重。</p><p id="3ce1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于简单的线性回归，系数将介于0和1之间。</p><p id="642c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我希望这能对α如何影响系数的大小有所帮助。有一点是肯定的，任何非零值给出的值都小于简单线性回归的值。差多少？我们很快就会知道了。把数学细节留到后面，让我们看看岭回归在上述相同问题上的作用。</p><h2 id="0db3" class="kv kw ht bd lh li lj lk ll lm ln lo lp jb lq lr ls jf lt lu lv jj lw lx ly lz bi translated">目标= RSS + α *(系数的平方和)</h2><p id="c971" class="pw-post-body-paragraph iq ir ht is b it ma iv iw ix mb iz ja jb mc jd je jf md jh ji jj me jl jm jn hb bi translated">这里，α (alpha)是平衡最小化RSS和最小化系数平方和的强调量的参数。α可以取不同的值:</p><ol class=""><li id="41c4" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated"><strong class="is hu"> α = 0: </strong></li></ol><ul class=""><li id="2c0c" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn mf kd ke kf bi translated">目标变得与简单线性回归相同。</li><li id="59d4" class="jx jy ht is b it kg ix kh jb ki jf kj jj kk jn mf kd ke kf bi translated">我们将得到与简单线性回归相同的系数。</li></ul><ol class=""><li id="d2df" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated"><strong class="is hu"> α = ∞: </strong></li></ol><ul class=""><li id="c29a" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn mf kd ke kf bi translated">系数将为零。为什么？因为系数平方的权重是无限的，所以任何小于零的值都会使目标无限。</li></ul><ol class=""><li id="1ab6" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated"><strong class="is hu"> 0 &lt; α &lt; ∞: </strong></li></ol><ul class=""><li id="0be5" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn mf kd ke kf bi translated">α的大小将决定给予目标不同部分的权重。</li><li id="e752" class="jx jy ht is b it kg ix kh jb ki jf kj jj kk jn mf kd ke kf bi translated">对于简单的线性回归，系数将介于0和1之间。</li></ul><p id="a76b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我希望这能对α如何影响系数的大小有所帮助。有一点是肯定的，任何非零值给出的值都小于简单线性回归的值。差多少？我们很快就会知道了。把数学细节留到后面，让我们看看岭回归在上述相同问题上的作用。</p><h2 id="05c6" class="kv kw ht bd lh li lj lk ll lm ln lo lp jb lq lr ls jf lt lu lv jj lw lx ly lz bi translated">目标= RSS + α *(系数的平方和)</h2><p id="5c87" class="pw-post-body-paragraph iq ir ht is b it ma iv iw ix mb iz ja jb mc jd je jf md jh ji jj me jl jm jn hb bi translated">这里，α (alpha)是平衡最小化RSS和最小化系数平方和的强调量的参数。α可以取不同的值:</p><ol class=""><li id="26c2" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated"><strong class="is hu"> α = 0: </strong></li></ol><ul class=""><li id="c9cd" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn mf kd ke kf bi translated">目标变得与简单线性回归相同。</li><li id="7038" class="jx jy ht is b it kg ix kh jb ki jf kj jj kk jn mf kd ke kf bi translated">我们将得到与简单线性回归相同的系数。</li></ul><ol class=""><li id="c54f" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated"><strong class="is hu"> α = ∞: </strong></li></ol><ul class=""><li id="4f30" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn mf kd ke kf bi translated">系数将为零。为什么？因为系数平方的权重是无限的，所以任何小于零的值都会使目标无限。</li></ul><ol class=""><li id="8106" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn kc kd ke kf bi translated"><strong class="is hu"> 0 &lt; α &lt; ∞: </strong></li></ol><ul class=""><li id="cade" class="jx jy ht is b it iu ix iy jb jz jf ka jj kb jn mf kd ke kf bi translated">α的大小将决定给予目标不同部分的权重。</li><li id="9459" class="jx jy ht is b it kg ix kh jb ki jf kj jj kk jn mf kd ke kf bi translated">对于简单的线性回归，系数将介于0和1之间。</li></ul><p id="8de4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我希望这能对α如何影响系数的大小有所帮助。有一点是肯定的，任何非零值给出的值都小于简单线性回归的值。差多少？我们很快就会知道了。把数学细节留到后面，让我们看看岭回归在上述相同问题上的作用。</p><p id="3070" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">希望这能有用，别忘了鼓掌，不懂的请评论</p><p id="53a7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">参考</p><p id="e14d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">安德烈亚斯·穆勒和萨拉·圭多。2017.pyhton机器学习简介</p><p id="70ad" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">NCSS统计软件第335章，岭回归</p></div></div>    
</body>
</html>