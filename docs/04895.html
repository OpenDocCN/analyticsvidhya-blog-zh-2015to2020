<html>
<head>
<title>Scaling — Zero to Hero</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">扩展—从零到英雄</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/scaling-zero-to-hero-45668c6d58f6?source=collection_archive---------18-----------------------#2020-04-04">https://medium.com/analytics-vidhya/scaling-zero-to-hero-45668c6d58f6?source=collection_archive---------18-----------------------#2020-04-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/4b047d1b2ad853765403092c62aa16ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YXxOQUcXx2Im_ERdpNpQEQ.jpeg"/></div></div></figure><p id="2295" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">据说哈奴曼勋爵有山那么大，飞过狭窄的海峡到达兰卡。着陆后，他发现了一个居住着国王罗波那和他的恶魔追随者的城市，所以他缩小到一只蚂蚁的大小&amp;潜入城市；根据印度教神话。它明确指出了缩放的相关性，这也是我们今天要讨论的内容。<em class="jo">‘特征缩放及其在机器学习中的重要性’。</em></p><h2 id="ab94" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated"><strong class="ak">什么是特征缩放？</strong></h2><p id="9cc0" class="pw-post-body-paragraph iq ir hi is b it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj ko jl jm jn hb bi translated">要素缩放是一种用于缩放数据集的独立变量(要素)的方法。在开发用于预测的机器学习模型之前，它作为数据预处理的一部分被执行。</p><p id="47de" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我们进一步探索如何之前，我认为更重要的是获得一种直觉，并澄清“为什么”</p><h2 id="595a" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated"><strong class="ak">我们为什么要缩放特征？</strong></h2><p id="e449" class="pw-post-body-paragraph iq ir hi is b it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj ko jl jm jn hb bi translated">如果你想让你的机器(计算机)从提供的数据中学习并准确预测，你需要一种方式向系统传达100公斤的体重并不比10公里/小时的速度好，仅仅100并不意味着大于/好于10(因为你的系统对体重和速度没有直觉，只能理解数字)。真实世界数据集包含在量值、单位和范围上有很大差异的要素。假设数据集包含100个特征/变量，您不希望某些特征对算法的决策过程产生过大的影响。因为机器学习模型总是给较高或较大的数字较高的权重，给较小的数字较低的权重。</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/220b1c7e7ccb4e97e05d4efd9d0fb2c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*e0BdgK0-hOnqmXeAXoHp4w.jpeg"/></div></figure><p id="7be8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">特征缩放的其他好处包括在算法的<a class="ae ku" href="https://www.quora.com/How-can-feature-scaling-affect-regularization" rel="noopener ugc nofollow" target="_blank">正则化</a>过程中进行适当的惩罚(防止过度拟合&amp;帮助一般化】)(特别是如果模型对异常值敏感的话)。它还<em class="jo">加速收敛</em>，例如在随机梯度下降中，特征缩放有时可以提高算法的收敛速度。在<a class="ae ku" href="https://scikit-learn.org/stable/modules/svm.html" rel="noopener ugc nofollow" target="_blank">支持向量机</a>中，可以减少寻找支持向量的时间。</p><h2 id="5f56" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated"><strong class="ak">什么时候进行特征缩放？</strong></h2><p id="f682" class="pw-post-body-paragraph iq ir hi is b it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj ko jl jm jn hb bi translated">某些算法，如SVM、逻辑回归和KNN，使用基于距离的度量(比如<a class="ae ku" href="https://en.wikipedia.org/wiki/Euclidean_distance" rel="noopener ugc nofollow" target="_blank">欧几里德距离</a>)需要特征缩放来提供正确的预测。而决策树、随机森林、XG-Boost等基于树的算法不需要<a class="ae ku" href="https://stats.stackexchange.com/questions/244507/what-algorithms-need-feature-scaling-beside-from-svm" rel="noopener ugc nofollow" target="_blank">特征缩放</a>。</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kv"><img src="../Images/ded4c0ba97ae9253b89d68eac5315298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7dW-37OSEPj6ajbUKK0rQ.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">特征缩放和PCA</figcaption></figure><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es la"><img src="../Images/a3fad70e13fde753279737354b8198ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*mK9k0bbfgFGzHed3cHFTxg.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">何时扩展</figcaption></figure><h2 id="decb" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated"><strong class="ak">你怎么能以规模为特征？</strong></h2><p id="a20b" class="pw-post-body-paragraph iq ir hi is b it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj ko jl jm jn hb bi translated">有三种主要的定标器可以完成这项工作(阅读更多<a class="ae ku" href="https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02" rel="noopener" target="_blank">这里</a>或者<a class="ae ku" href="https://becominghuman.ai/demystifying-feature-scaling-baff53e9b3fd" rel="noopener ugc nofollow" target="_blank">这里</a>——很棒的文章)</p><ol class=""><li id="cde1" class="lb lc hi is b it iu ix iy jb ld jf le jj lf jn lg lh li lj bi translated"><strong class="is hj"> Min-max scaler: </strong>顾名思义，从你的特征中减去最小值，再除以范围(见下图)。它保留了原始分布的形状，并且不改变原始数据中嵌入的信息。它不会降低极值的权重，即异常值不受影响。</li></ol><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lk"><img src="../Images/3f41311242765c691c8e0d89935df4db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GmFes4H0-rk6zdI5YQAM1w.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">最小-最大缩放器</figcaption></figure><p id="0a26" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.<strong class="is hj">标准化:</strong>通过减去平均值然后缩放至单位方差来标准化特征。如果您的要素值呈正态分布，并且您希望减少异常值的影响，并且您不介意要素内相对间距的变化，那么这是一个不错的选择。深度学习和某些回归器受益于标准缩放。</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/715643d53b91b7dc40646edb773818ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*_xSuxV8nDAKCjB_Ft3Ewqw.jpeg"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">标准缩放器</figcaption></figure><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/3ab64f0278698e7d5c8d047eba88b3df.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*KrYzFzhLBjiQ89OvDld8Tw.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">分布</figcaption></figure><p id="a40a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 3。鲁棒定标器:</strong>该定标器移除中值，并根据分位数范围(Q3，75%值— Q1，25%值)对数据进行定标。它减少了异常值的影响。</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/41344a62477f3078ec1ff81f1b595e77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*8zcKB6p5aMa59aBc7FqDlA.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">鲁棒定标器</figcaption></figure><p id="81e1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae ku" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing" rel="noopener ugc nofollow" target="_blank">链接到Sklearn预处理API</a>(python标准机器学习库)</p><h2 id="a909" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated"><strong class="ak">结束语</strong></h2><p id="8ab7" class="pw-post-body-paragraph iq ir hi is b it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj ko jl jm jn hb bi translated">如果您希望模型有效，特征缩放是一种非常有用的技术，但同时并不总是必需的。某些机器学习算法，尤其是Tree，不需要您缩放您的要素。</p><p id="a41d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">希望这篇文章给你一个关于什么是缩放，为什么和如何使用它的简要介绍。我试图用通俗的语言来描述它，在必要的时候提供外部资源链接。感谢阅读。</p><p id="54c3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">想动手试试的请查看<a class="ae ku" href="https://www.kaggle.com/rtatman/data-cleaning-challenge-scale-and-normalize-data" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/rtatman/data-cleaning-challenge-scale-and-normalize-data</a></p></div></div>    
</body>
</html>