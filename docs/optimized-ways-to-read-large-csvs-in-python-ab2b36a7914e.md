# åœ¨ Python ä¸­è¯»å–å¤§å‹ CSV çš„ä¼˜åŒ–æ–¹æ³•

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/optimized-ways-to-read-large-csvs-in-python-ab2b36a7914e?source=collection_archive---------0----------------------->

ä½ å¥½ã€‚ğŸ™‹

![](img/e1b4fcd74f153adf84446250e81cdf78.png)

[æ¥æº](https://leaderonomics.com/business/love-innovation-or-die)

åœ¨å½“å‰çš„æ—¶ä»£ï¼Œæ•°æ®åœ¨åˆ†æå’Œå»ºç«‹ ML/AI æ¨¡å‹ä¸­èµ·ç€éå¸¸é‡è¦çš„ä½œç”¨ã€‚æ•°æ®å¯ä»¥åœ¨ CSVã€å¹³é¢æ–‡ä»¶ã€JSON ç­‰å„ç§æ ¼å¼ä¸­æ‰¾åˆ°ï¼Œå½“æ•°æ®å¾ˆå¤§æ—¶ï¼Œå¾ˆéš¾è¯»å…¥å†…å­˜ã€‚è¿™ä¸ªåšå®¢å›´ç»•ç€å¤„ç† CSV æ ¼å¼çš„è¡¨æ ¼æ•°æ®ï¼Œè¿™äº›æ•°æ®æ˜¯é€—å·åˆ†éš”çš„æ–‡ä»¶ã€‚

**é—®é¢˜:**å¯¼å…¥(è¯»å–)å¤§å‹ CSV æ–‡ä»¶å¯¼è‡´å†…å­˜ä¸è¶³é”™è¯¯ã€‚æ²¡æœ‰è¶³å¤Ÿçš„å†…å­˜æ¥ä¸€æ¬¡è¯»å–æ•´ä¸ª CSV æ–‡ä»¶ï¼Œå¯¼è‡´è®¡ç®—æœºå´©æºƒã€‚

è¿™é‡Œæœ‰ä¸€äº›åœ¨ Python ä¸­å¯¼å…¥ CSV çš„æœ‰æ•ˆæ–¹æ³•ã€‚

![](img/b282c8dbaeb8843eff23baebe7d9854f.png)

å›¾ 1

ç°åœ¨æ€ä¹ˆåŠï¼Ÿå¥½å§ï¼Œè®©æˆ‘ä»¬å‡†å¤‡ä¸€ä¸ªåº”è¯¥å¾ˆå¤§çš„æ•°æ®é›†ï¼Œç„¶åæ¯”è¾ƒå®ç°å›¾ 1 æ‰€ç¤ºé€‰é¡¹çš„æ€§èƒ½(æ—¶é—´)ã€‚è®©æˆ‘ä»¬å¼€å§‹å§..ğŸƒ

ç”¨éšæœºæ•°å’Œå­—ç¬¦ä¸²åˆ›å»ºä¸€ä¸ª 15 åˆ— 1000 ä¸‡è¡Œçš„æ•°æ®æ¡†æ¶ã€‚å°†å…¶å¯¼å‡ºä¸ºå¤§çº¦ 1 GB å¤§å°çš„ CSV æ ¼å¼ã€‚

```
df = pd.DataFrame(data=np.random.randint(99999, 99999999, size=(10000000,14)),columns=['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14'])df['C15'] = pd.util.testing.rands_array(5,10000000)
df.to_csv("huge_data.csv")
```

ç°åœ¨è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹å¯¼å…¥é€‰é¡¹ï¼Œæ¯”è¾ƒä¸€ä¸‹å°† CSV è¯»å…¥å†…å­˜æ‰€ç”¨çš„æ—¶é—´ã€‚

# ç†ŠçŒ«

pandas python åº“æä¾› read_csv()å‡½æ•°ï¼Œå°† csv ä½œä¸ºæ•°æ®å¸§ç»“æ„å¯¼å…¥ï¼Œä»¥ä¾¿äºè®¡ç®—æˆ–åˆ†æã€‚è¿™ä¸ªå‡½æ•°æä¾›äº†ä¸€ä¸ªå‚æ•°(å°†åœ¨åé¢çš„éƒ¨åˆ†ä¸­æè¿°)æ¥æ›´å¿«åœ°å¯¼å…¥æ‚¨çš„å·¨å¤§æ–‡ä»¶ã€‚

## 1.pandas.read_csv()

***è¾“å…¥*** :è¯»å– CSV æ–‡ä»¶
***è¾“å‡º*** :ç†ŠçŒ«æ•°æ®å¸§

pandas.read_csv()å°†æ•´ä¸ª csv æ–‡ä»¶ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜ä¸­çš„å•ä¸ªæ•°æ®å¸§ä¸­ã€‚

```
start = time.time()
df = pd.read_csv('huge_data.csv')
end = time.time()
print("Read csv without chunks: ",(end-start),"sec")Read csv without chunks:  26.88872528076172 sec
```

å¦‚æœ CSV å¤§å°è¶…è¿‡æ‚¨çš„å†…å­˜å¤§å°(RAM ),è¿™æœ‰æ—¶å¯èƒ½ä¼šç”±äº OOM(å†…å­˜ä¸è¶³)é”™è¯¯è€Œä½¿æ‚¨çš„ç³»ç»Ÿå´©æºƒã€‚é€šè¿‡ä¸‹ä¸€ä¸ªå¯¼å…¥æ–¹æ³•æ”¹è¿›äº†è§£å†³æ–¹æ¡ˆã€‚

## 2.pandas.read_csv(chunksize)

***è¾“å…¥*** :è¯»å– CSV æ–‡ä»¶
***è¾“å‡º*** :ç†ŠçŒ«æ•°æ®å¸§

ä¸æ˜¯ä¸€æ¬¡è¯»å–æ•´ä¸ª CSVï¼Œ**CSV çš„å¤§å—è¢«è¯»å…¥å†…å­˜**ã€‚ä½¿ç”¨ chunksize å‚æ•°æŒ‡å®šå—çš„å¤§å°ï¼Œè¯¥å‚æ•°æŒ‡çš„æ˜¯è¡Œæ•°ã€‚è¿™ä¸ªå‡½æ•°è¿”å›ä¸€ä¸ªè¿­ä»£å™¨æ¥éå†è¿™äº›å—ï¼Œç„¶åéšå¿ƒæ‰€æ¬²åœ°å¤„ç†å®ƒä»¬ã€‚å› ä¸ºä¸€æ¬¡åªèƒ½è¯»å–ä¸€ä¸ªå¤§æ–‡ä»¶çš„ä¸€éƒ¨åˆ†ï¼Œæ‰€ä»¥ä½å†…å­˜è¶³ä»¥å®¹çº³æ•°æ®ã€‚ç¨åï¼Œè¿™äº›å—å¯ä»¥è¿æ¥æˆä¸€ä¸ªæ•°æ®å¸§ã€‚

```
start = time.time()
#read data in chunks of 1 million rows at a time
chunk = pd.read_csv('huge_data.csv',chunksize=1000000)
end = time.time()
print("Read csv with chunks: ",(end-start),"sec")
pd_df = pd.concat(chunk)Read csv with chunks:  0.013001203536987305 sec
```

æ­¤é€‰é¡¹é€Ÿåº¦æ›´å¿«ï¼Œæœ€é€‚åˆåœ¨ RAM æœ‰é™çš„æƒ…å†µä¸‹ä½¿ç”¨ã€‚æˆ–è€…ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ä¸€ä¸ªæ–°çš„ python åº“ DASKï¼Œå¦‚ä¸‹æ‰€è¿°ã€‚

# è¾¾æ–¯å…‹

***è¾“å…¥*** :è¯»å– CSV æ–‡ä»¶
***è¾“å‡º*** : Dask dataframe

åœ¨è¯»å–å¤§çš„ CSV æ–‡ä»¶æ—¶ï¼Œå¦‚æœå®ƒä¸é€‚åˆä½ çš„ RAMï¼Œä½ å¯èƒ½ä¼šé‡åˆ°å†…å­˜ä¸è¶³çš„é”™è¯¯ï¼Œå› æ­¤ DASK å‡ºç°äº†ã€‚

*   Dask æ˜¯ä¸€ä¸ª**å¼€æº** python åº“ï¼Œå…·æœ‰ python ä¸­çš„å¹¶è¡Œæ€§å’Œå¯ä¼¸ç¼©æ€§ç‰¹æ€§ï¼Œé»˜è®¤åŒ…å«åœ¨ Anaconda å‘è¡Œç‰ˆä¸­ã€‚
*   å®ƒé€šè¿‡é‡ç”¨**ç°æœ‰çš„ Python åº“**ï¼Œå¦‚ pandasã€numpy æˆ– sklearnï¼Œæ‰©å±•äº†å…¶å¯ä¼¸ç¼©æ€§å’Œå¹¶è¡Œæ€§çš„ç‰¹æ€§ã€‚è¿™å¯¹é‚£äº›å·²ç»ç†Ÿæ‚‰è¿™äº› Python åº“çš„äººæ¥è¯´å¾ˆæ–¹ä¾¿ã€‚
*   æ€ä¹ˆå…¥æ‰‹å‘¢ï¼Ÿæ‚¨å¯ä»¥é€šè¿‡ pip æˆ– conda å®‰è£…ã€‚æˆ‘ä¼šæ¨è condaï¼Œå› ä¸ºé€šè¿‡ pip å®‰è£…å¯èƒ½ä¼šäº§ç”Ÿä¸€äº›é—®é¢˜ã€‚

```
pip install dask
```

å—¯ï¼Œå½“æˆ‘å°è¯•ä¸Šé¢çš„æ–¹æ³•æ—¶ï¼Œå®ƒäº§ç”Ÿäº†ä¸€äº›é—®é¢˜ï¼Œä½¿ç”¨ä¸€äº› GitHub é“¾æ¥ä»å¤–éƒ¨æ·»åŠ  dask path ä½œä¸ºç¯å¢ƒå˜é‡è§£å†³äº†è¿™äº›é—®é¢˜ã€‚ä½†æ˜¯å½“æœ‰æ›´ç®€å•çš„é€‰æ‹©æ—¶ï¼Œä¸ºä»€ä¹ˆè¦å¤§æƒŠå°æ€ªå‘¢ï¼Ÿ

```
conda install dask
```

*   ä»£ç å®ç°:

```
from dask import dataframe as ddstart = time.time()
dask_df = dd.read_csv('huge_data.csv')
end = time.time()
print("Read csv with dask: ",(end-start),"sec")Read csv with dask:  0.07900428771972656 sec
```

Dask åœ¨è¯»å–è¿™ä¸ªå¤§å‹ CSV æ—¶ä¼¼ä¹æ˜¯æœ€å¿«çš„ï¼Œä¸ä¼šä½¿è®¡ç®—æœºå´©æºƒæˆ–å˜æ…¢ã€‚å“‡ï¼é‚£æœ‰å¤šå¥½ï¼Ÿï¼ï¼ä¸€ä¸ªæ–°çš„ Python åº“ï¼Œä¿®æ”¹äº†ç°æœ‰çš„åº“ä»¥å¼•å…¥å¯ä¼¸ç¼©æ€§ã€‚

## ä¸ºä»€ä¹ˆè¾¾æ–¯å…‹æ¯”ç†ŠçŒ«å¼ºï¼Ÿ

*   Pandas ä½¿ç”¨å•ä¸ª CPU å†…æ ¸ï¼Œè€Œ **Dask é€šè¿‡å†…éƒ¨åˆ†å—æ•°æ®å¸§å’Œå¹¶è¡Œå¤„ç†æ¥ä½¿ç”¨å¤šä¸ª CPU å†…æ ¸**ã€‚ç®€è€Œè¨€ä¹‹ï¼Œä¸€ä¸ªå¤§æ•°æ®å¸§çš„å¤šä¸ªå°æ•°æ®å¸§åŒæ—¶è¢«å¤„ç†ï¼Œè€Œåœ¨ pandas ä¸‹ï¼Œæ“ä½œä¸€ä¸ªå¤§æ•°æ®å¸§éœ€è¦å¾ˆé•¿æ—¶é—´ã€‚
*   DASK å¯ä»¥åœ¨å•ä¸ª CPU ä¸Šå¤„ç†å¤§å‹æ•°æ®é›†ï¼Œåˆ©ç”¨å®ƒçš„å¤šä¸ªæ ¸å¿ƒæˆ–æœºå™¨é›†ç¾¤ï¼Œç§°ä¸ºåˆ†å¸ƒå¼è®¡ç®—ã€‚å®ƒæä¾›äº†ä¸€ç§**è§„æ¨¡çš„ pandas å’Œ numpy åº“**ã€‚
*   ä¸ä»…ä»…æ˜¯ dataframeï¼Œdask è¿˜æä¾›äº†æ•°ç»„å’Œ scikit-learn åº“æ¥åˆ©ç”¨å¹¶è¡Œæ€§ã€‚

ä¸€äº› DASK æä¾›çš„åº“å¦‚ä¸‹æ‰€ç¤ºã€‚

*   **Dask é˜µåˆ—**:å¹¶è¡Œæ•°å­—é˜µåˆ—
*   **Dask æ•°æ®å¸§**:å¹³è¡Œç†ŠçŒ«
*   **Dask ML** :å¹¶è¡Œ Scikit-Learn

**æˆ‘ä»¬å°†åªå…³æ³¨æ•°æ®å¸§ï¼Œå› ä¸ºå…¶ä»–ä¸¤ä¸ªä¸åœ¨è®¨è®ºèŒƒå›´å†…ã€‚ä½†æ˜¯ï¼Œè¦æƒ³å¼„è„ä½ çš„æ‰‹ï¼Œæœ€å¥½è€ƒè™‘ä¸€ä¸‹** [**è¿™ä¸ªåšå®¢**](https://www.analyticsvidhya.com/blog/2018/08/dask-big-datasets-machine_learning-python/) **ã€‚**

## Dask å¦‚ä½•å­˜å‚¨å¤§äºå†…å­˜(RAM)çš„æ•°æ®ï¼Ÿ

å½“æˆ‘ä»¬å¯¼å…¥æ•°æ®æ—¶ï¼Œå®ƒè¢«è¯»å…¥æˆ‘ä»¬çš„ RAMï¼Œè¿™çªå‡ºäº†å†…å­˜é™åˆ¶ã€‚
æ¯”æ–¹è¯´ï¼Œä½ æƒ³åœ¨ä½ çš„ 4 GB RAM ä¸­å¯¼å…¥ 6 GB æ•°æ®ã€‚è¿™ä¸èƒ½é€šè¿‡ç†ŠçŒ«æ¥å®ç°ï¼Œå› ä¸ºä¸€ä¸ªé•œå¤´ä¸­çš„å…¨éƒ¨æ•°æ®éƒ½ä¸é€‚åˆå†…å­˜ï¼Œä½† Dask å¯ä»¥ã€‚æ€ä¹ˆä¼šï¼Ÿ
*Dask é¦–å…ˆä»£æ›¿è®¡ç®—ï¼Œ* ***åˆ›å»ºä¸€ä¸ªä»»åŠ¡å›¾*** *å…¶ä¸­è¯´çš„æ˜¯å…³äºå¦‚ä½•æ‰§è¡Œé‚£ä¸ªä»»åŠ¡ã€‚å®ƒç›¸ä¿¡æ‡’æƒ°è®¡ç®—ï¼Œè¿™æ„å‘³ç€ dask çš„ä»»åŠ¡è°ƒåº¦ç¨‹åºé¦–å…ˆåˆ›å»ºä¸€ä¸ªå›¾ï¼Œç„¶ååœ¨è¯·æ±‚æ—¶* ***è®¡ç®—è¯¥å›¾*** *ã€‚ä¸ºäº†æ‰§è¡Œä»»ä½•è®¡ç®—ï¼Œcompute()è¢«æ˜¾å¼è°ƒç”¨ï¼Œå®ƒè°ƒç”¨ä»»åŠ¡è°ƒåº¦å™¨æ¥åˆ©ç”¨æ‰€æœ‰æ ¸å¤„ç†æ•°æ®ï¼Œæœ€åå°†ç»“æœç»„åˆæˆä¸€ä¸ªã€‚*

å¯¹äºé‚£äº›å·²ç»ç†Ÿæ‚‰ç†ŠçŒ«çš„äººæ¥è¯´ï¼Œè¿™å¹¶ä¸éš¾ç†è§£ã€‚

> [](/@shachikaul35/dask-for-python-and-machine-learning-dbe1356b5d7a)

# ç»“è®º

ä½¿ç”¨å„ç§å¯¼å…¥é€‰é¡¹åœ¨å†…å­˜ä¸­è¯»å–çº¦ 1 GB CSV å¯ä»¥é€šè¿‡åŠ è½½åˆ°å†…å­˜ä¸­æ‰€ç”¨çš„æ—¶é—´æ¥è¯„ä¼°ã€‚

*pandas.read_csv* åœ¨è¯»å–æ¯” RAM å¤§çš„ csv æ—¶**æœ€å·®**ã€‚
*pandas . read _ CSV(chunksize)*æ¯”ä¸Šé¢çš„**æ‰§è¡Œå¾—æ›´å¥½ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡è°ƒæ•´ chunk size æ¥è¿›ä¸€æ­¥æ”¹è¿›ã€‚
*dask.dataframe* è¢«è¯æ˜æ˜¯**æœ€å¿«çš„**ï¼Œå› ä¸ºå®ƒå¤„ç†å¹¶è¡Œå¤„ç†ã€‚**

**å› æ­¤ï¼Œæˆ‘ä¼šå»ºè®®ä½ èµ°å‡ºä½¿ç”¨ç†ŠçŒ«çš„èˆ’é€‚åŒºï¼Œå°è¯• daskã€‚ä½†æ˜¯ä»…ä¾›å‚è€ƒï¼Œæˆ‘åªæµ‹è¯•äº† DASK è¯»å–å¤§ CSV çš„èƒ½åŠ›ï¼Œè€Œæ²¡æœ‰æµ‹è¯•æˆ‘ä»¬åœ¨ç†ŠçŒ«èº«ä¸Šåšçš„è®¡ç®—ã€‚**

> ****ä½ å¯ä»¥æŸ¥çœ‹æˆ‘çš„** [**github ä»£ç **](https://github.com/shachi01/dask_in_python_ml/blob/master/efficient_read_csv.ipynb) **æ¥è®¿é—®è¦†ç›–æœ¬åšå®¢ç¼–ç éƒ¨åˆ†çš„ç¬”è®°æœ¬ã€‚****

# **å‚è€ƒ**

*   **[Dask æœ€æ–°æ–‡æ¡£](https://docs.dask.org/en/latest/)**
*   **[å€¼å¾—ä¸€è¯»çš„ä¹¦](https://www.amazon.in/Data-Science-Scale-Python-Dask/dp/1617295604)**
*   **è¿™ç¯‡åšå®¢ä¸­æ²¡æœ‰åŒ…æ‹¬çš„å…¶ä»– CSV è¯»å†™é€‰é¡¹ã€‚**

**[](https://realpython.com/python-csv/) [## ç”¨ Python è¯»å†™ CSV æ–‡ä»¶-çœŸæ­£çš„ Python

### ç«‹å³è§‚çœ‹æœ¬æ•™ç¨‹æœ‰ä¸€ä¸ªç”±çœŸæ­£çš„ Python å›¢é˜Ÿåˆ›å»ºçš„ç›¸å…³è§†é¢‘è¯¾ç¨‹ã€‚å’Œä¹¦é¢çš„ä¸€èµ·çœ‹â€¦

realpython.com](https://realpython.com/python-csv/) 

3.æƒ³åœ¨ DASK é‡Œå¼„è„ä½ çš„æ‰‹ï¼Œåº”è¯¥æµè§ˆä¸€ä¸‹ä¸‹é¢çš„é“¾æ¥ã€‚

[](https://pythonspeed.com/articles/faster-pandas-dask/) [## ä»åˆ†å—åˆ°å¹¶è¡Œ:æ›´å¿«çš„ç†ŠçŒ«ä¸ Dask

### å½“æ•°æ®ä¸é€‚åˆå†…å­˜æ—¶ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨åˆ†å—:åŠ è½½æ•°æ®ï¼Œç„¶ååˆ†å—å¤„ç†ï¼Œè¿™æ ·åªæœ‰ä¸€ä¸ªå­é›†â€¦

pythonspeed.com](https://pythonspeed.com/articles/faster-pandas-dask/) 

å¦‚æœä½ å–œæ¬¢è¿™ä½ä½œè€…çš„åšå®¢ï¼Œè¯·éšæ„å…³æ³¨ï¼Œå› ä¸ºè¿™ä½ä½œè€…ä¿è¯ä¼šå¸¦æ¥æ›´å¤šæœ‰è¶£çš„äººå·¥æ™ºèƒ½ç›¸å…³å†…å®¹ã€‚
æ„Ÿè°¢ï¼Œ
å­¦ä¹ æ„‰å¿«ï¼ğŸ˜„

***å¯ä»¥é€šè¿‡***[***LinkedIn***](https://www.linkedin.com/in/kaul-shachi)***å–å¾—è”ç³»ã€‚*****