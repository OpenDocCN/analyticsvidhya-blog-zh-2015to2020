<html>
<head>
<title>A brief idea on Ensemble Models-II (Gradient Boosting Decision Tree)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于集成模型II(梯度推进决策树)的一点想法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/brief-idea-on-ensemble-models-ii-gradient-boosting-decision-tree-7ed996fd4085?source=collection_archive---------18-----------------------#2020-08-20">https://medium.com/analytics-vidhya/brief-idea-on-ensemble-models-ii-gradient-boosting-decision-tree-7ed996fd4085?source=collection_archive---------18-----------------------#2020-08-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="acee" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">机器学习中Boosting技术的集成模型综合指南。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/5d31be0f18fa6689e04f4bae8e8d5488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yG7HQhDYU6IqyTKQ4ccyQA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">用MS PowerPoint创建</figcaption></figure><p id="700b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在这篇博客中，我们将了解Boosting，这是建立集合模型的策略之一。在我之前的博客中，我已经简单地讨论了打包策略，如果你还没有，请浏览一下。这里是<a class="ae kj" rel="noopener" href="/@bhanuprakash193/brief-idea-on-ensemble-models-i-random-forest-3aa3dd5b9bdb">环节</a>。</p><blockquote class="kk kl km"><p id="fbbc" class="jn jo kn jp b jq jr ij js jt ju im jv ko jx jy jz kp kb kc kd kq kf kg kh ki hb bi translated">简单地说，我们可以将助推技术与我们生活中的一句名言或教训联系起来:“<strong class="jp hj"> <em class="hi">从错误中吸取教训</em> </strong>”</p><p id="fe0b" class="jn jo kn jp b jq jr ij js jt ju im jv ko jx jy jz kp kb kc kd kq kf kg kh ki hb bi translated">错误+纠正=学习</p></blockquote><p id="05c6" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这是升压工作原理的核心思想。让我们深入了解一下。</p><p id="c5fd" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">理解偏差:</strong></p><p id="793c" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这个模型的思想是减少偏差。偏差误差是由于简化假设而产生的误差，如假设平面将正负点分开，但曲线(非线性表面)实际上将它们分开。因此，这些建模假设将导致欠拟合的高偏差模型。因此，我们可以直观地将高偏差视为高训练误差。</p><h2 id="982f" class="kr ks hi bd kt ku kv kw kx ky kz la lb jw lc ld le ka lf lg lh ke li lj lk ll bi translated">工作原理:</h2><p id="fc5e" class="pw-post-body-paragraph jn jo hi jp b jq lm ij js jt ln im jv jw lo jy jz ka lp kc kd ke lq kg kh ki hb bi translated">在这篇博客中，我将帮助你直观地了解Boosting是如何用更少的数学和更多的理论来工作的。我们将基础模型作为低方差和高偏差模型(浅深度通常为1或2。深度为1的树称为决策树桩)。</p><p id="b342" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj"> <em class="kn">第一步:</em> </strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/a7cd6391969ce413bf8be94fa7567313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EMcmFrOs3DdDUGoKcbsjWg.jpeg"/></div></div></figure><p id="96ff" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在第一步中，我们在全部训练数据上训练一个模型。模型在此基础上进行训练，并试图找到正确的函数，即根据数据拟合函数。所以我们得到了预测的产量。我们有实际值和预测值，实际值和预测值之间的简单差异就会产生误差。获得的这个误差用于下一阶段，使得模型非常适合以进一步减小误差。</p><p id="fcb8" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj"> <em class="kn">第二步:</em> </strong></p><p id="add6" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">不是用完整的数据点和Yi进行训练，而是仅在和在第一阶段或步骤1或先前阶段中获得的误差上进行训练(在，error_i上训练模型)。所以我们尝试用误差函数来代替Yi。因此，这个阶段结束时的模型会是这样的，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ls"><img src="../Images/97b2aa06d263efa17f9e82a82cc1c367.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/0*QbVm74rzh4ojdeEa"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">两个基本模型h0和h1的加权和</figcaption></figure><p id="e68a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">伽马值通过损失最小化函数获得(请参考维基。)</p><p id="944a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj"> <em class="kn">第三步:</em> </strong></p><p id="54c1" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">现在再次训练Xi和在步骤2(前一阶段)中获得的误差，并在其上拟合一个模型。同样，我们对Xi和前一阶段结束时留下的误差(这是一个残留误差)重复步骤m次，直到我们得到一个低残留误差。随着阶段的数量(=基础学习者的数量，因为在每个阶段我们都在训练一个基础模型)增加，在每个阶段，我们都在拟合误差，以使残差逐阶段减小。这个误差是有偏差的。因此，我们能够完成减少偏差的任务。</p><p id="31c7" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在求解损失最小化函数时，我们得到的残差等于损失函数在每个阶段-m的负梯度。因此，我们用<strong class="jp hj">伪残差代替残差，这有助于我们最小化任何损失函数</strong>，只要它是可微分的。这个伪残差就是梯度提升的核心思想。请参考维基百科的梯度推进算法，了解数学更深层次的见解。在这篇博客中，我主要集中在它如何工作的理论部分。</p><p id="e160" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">因此最终的模型会是这样的，</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lt"><img src="../Images/9c35ead4dfd74f1d30b610502b321490.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/0*0cu-_BwLnaq2mPRx"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">m是基础学习者的数量</figcaption></figure><p id="39ff" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">上面的等式可以被准确地认为是逻辑回归，而γ代表逻辑回归中的权重，而在这里，它可以被认为是使损失函数最小化的常数。</p><p id="8853" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">过度配合</strong>:</p><p id="0b99" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">随着基础学习者数量的增加，我们经常在GBDT(梯度推进决策树)中过度拟合，因为我们试图拟合误差函数，因此我们在训练数据上更加精确以避免误差。这将导致模型的高方差和过拟合。因此，为了避免这种情况，有一个被称为收缩的概念，它通常位于0和1之间。</p><p id="c62a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">较小的收缩值允许我们部分考虑以前的模型输出。因此，这控制了模型的过度拟合。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lu"><img src="../Images/19417fdf984b5a4d184f5e8495fdeeda.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/0*RnVn7tT74h3Lahz5"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">v值控制过度拟合</figcaption></figure><blockquote class="kk kl km"><p id="fd04" class="jn jo kn jp b jq jr ij js jt ju im jv ko jx jy jz kp kb kc kd kq kf kg kh ki hb bi translated">超参数:基础模型的数量和收缩率(v)</p></blockquote><p id="e00d" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">代码</strong> : <code class="du lv lw lx ly b"><a class="ae kj" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble" rel="noopener ugc nofollow" target="_blank">sklearn.ensemble</a></code>。梯度推进分级机</p><h2 id="8a86" class="kr ks hi bd kt ku kv kw kx ky kz la lb jw lc ld le ka lf lg lh ke li lj lk ll bi translated">XGBoost:</h2><p id="d7ca" class="pw-post-body-paragraph jn jo hi jp b jq lm ij js jt ln im jv jw lo jy jz ka lp kc kd ke lq kg kh ki hb bi translated">有没有一种方法可以将装袋技术(行抽样和列抽样)和boosting(GBDT)的优点结合起来？是的，XGBoost是GBDT的实现，与单独使用GBDT相比，我们可以使用相同的数据实现更高的性能。</p><blockquote class="kk kl km"><p id="f23f" class="jn jo kn jp b jq jr ij js jt ju im jv ko jx jy jz kp kb kc kd kq kf kg kh ki hb bi translated">GBDTs不容易并行化，因为所有的基本模型都是顺序相关的，即前一阶段的数据用于下一阶段的训练。</p></blockquote><p id="fa39" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这都是关于GBDTs如何工作的简单直觉。可以很容易地用于低延迟应用程序，因为每个决策树的深度都很浅，所以存储和运行它们是很简单的。</p><p id="7dac" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">请查看以下参考资料，以了解更深层次的数学直觉。谢谢:)</p><p id="4c05" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">请参考这个<a class="ae kj" rel="noopener" href="/@bhanuprakash193/a-brief-idea-on-ensemble-models-iii-stacking-classifier-564a81a855e4?sk=fddcca18a4130de438235d37ac67b149">链接</a>来看下一个关于叠加的整体模型的博客。</p><p id="02b2" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">T3】参考文献:T5】</strong></p><p id="719f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><a class="ae kj" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Gradient_boosting</a></p><div class="lz ma ez fb mb mc"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" rel="noopener  ugc nofollow" target="_blank"><div class="md ab dw"><div class="me ab mf cl cj mg"><h2 class="bd hj fi z dy mh ea eb mi ed ef hh bi translated">3.2.4.3.5.sk learn . ensemble . gradientboostingclassifier-sci kit-learn 0 . 23 . 2文档</h2><div class="mj l"><h3 class="bd b fi z dy mh ea eb mi ed ef dx translated">class sk learn . ensemble . GradientBoostingClassifier(*，loss='deviance '，learning_rate=0.1，n_estimators=100…</h3></div><div class="mk l"><p class="bd b fp z dy mh ea eb mi ed ef dx translated">scikit-learn.org</p></div></div><div class="ml l"><div class="mm l mn mo mp ml mq jh mc"/></div></div></a></div><div class="lz ma ez fb mb mc"><a href="https://xgboost.readthedocs.io/en/latest/python/python_api.html" rel="noopener  ugc nofollow" target="_blank"><div class="md ab dw"><div class="me ab mf cl cj mg"><h2 class="bd hj fi z dy mh ea eb mi ed ef hh bi translated">Python API参考-xgboost 1 . 2 . 0-快照文档</h2><div class="mj l"><h3 class="bd b fi z dy mh ea eb mi ed ef dx translated">本页给出了xgboost的Python API参考，更多内容请参考Python包介绍…</h3></div><div class="mk l"><p class="bd b fp z dy mh ea eb mi ed ef dx translated">xgboost.readthedocs.io</p></div></div></div></a></div></div></div>    
</body>
</html>