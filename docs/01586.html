<html>
<head>
<title>Build a simple predictive keyboard using python and Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用python和Keras构建一个简单的预测键盘</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/build-a-simple-predictive-keyboard-using-python-and-keras-b78d3c88cffb?source=collection_archive---------0-----------------------#2019-11-02">https://medium.com/analytics-vidhya/build-a-simple-predictive-keyboard-using-python-and-keras-b78d3c88cffb?source=collection_archive---------0-----------------------#2019-11-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2cff00107cf3fd521529447ef9816fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z66oSuX8suov6WMB"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@kaitlynbaker?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯特琳·贝克</a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="f927" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">键盘是我们生活的一部分。我们在每个计算环境中都使用它。为了减少我们打字的工作量，今天大多数键盘都提供了先进的预测工具。它预测下一个字符，或下一个单词，甚至可以自动完成整个句子。因此，让我们讨论一些使用python中的Keras构建简单的下一个单词预测键盘应用程序的技术。本教程的灵感来自于<a class="jt ju ge" href="https://medium.com/u/102e34a0beb1?source=post_page-----b78d3c88cffb--------------------------------" rel="noopener" target="_blank">维尼林·瓦尔科夫</a>在<a class="ae iu" rel="noopener" href="/@curiousily/making-a-predictive-keyboard-using-recurrent-neural-networks-tensorflow-for-hackers-part-v-3f238d824218">下一个字符预测键盘</a>上写的博客。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jv"><img src="../Images/f40b81dffca206fdb0ead751e66f500a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_j4jGasd9TpuF8VGay91ng.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">下一个单词的预测</figcaption></figure><p id="383f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为此，我们使用递归神经网络。选择这个模型是因为它提供了一种检查先前输入的方法。LSTM，一种特殊的RNN也用于此目的。LSTM提供了保存误差的机制，这些误差可以通过时间和层反向传播，这有助于减少<a class="ae iu" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem##targetText=In%20machine%20learning%2C%20the%20vanishing,based%20learning%20methods%20and%20backpropagation.&amp;targetText=The%20problem%20is%20that%20in,weight%20from%20changing%20its%20value." rel="noopener ugc nofollow" target="_blank">消失梯度</a>问题。</p><h2 id="2455" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">我们来编码吧！</h2><p id="e663" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">首先，我们需要安装几个库。</p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="59ba" class="ka kb hi lb b fi lf lg l lh li">pip install numpy<br/>pip install tensorflow<br/>pip install keras<br/>pip install nltk</span></pre><p id="d354" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们导入所需的库。</p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="e9bd" class="ka kb hi lb b fi lf lg l lh li">import numpy as np<br/>from nltk.tokenize import RegexpTokenizer<br/>from keras.models import Sequential, load_model<br/>from keras.layers import LSTM<br/>from keras.layers.core import Dense, Activation<br/>from keras.optimizers import RMSprop<br/>import matplotlib.pyplot as plt<br/>import pickle<br/>import heapq</span></pre><p id="b88c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">加载数据集</strong>是下一个要做的重要步骤，这里我们使用<a class="ae iu" href="https://www.gutenberg.org/files/1661/1661-0.txt" rel="noopener ugc nofollow" target="_blank"> <em class="lj">《福尔摩斯探案集》</em> </a> <em class="lj">作为数据集。</em></p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="21a9" class="ka kb hi lb b fi lf lg l lh li">path = '1661-0.txt'<br/>text = open(path).read().lower()<br/>print('corpus length:', len(text))</span><span id="d8ed" class="ka kb hi lb b fi lk lg l lh li"><strong class="lb hj">Output<br/></strong>corpus length: 581887</span></pre><p id="dde6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们希望将整个数据集按顺序拆分成每个单词，而不要出现特殊字符。</p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="ba8b" class="ka kb hi lb b fi lf lg l lh li">tokenizer = RegexpTokenizer(r'\w+')<br/>words = tokenizer.tokenize(text)</span><span id="1a28" class="ka kb hi lb b fi lk lg l lh li"><strong class="lb hj">Output</strong><br/>['project', 'gutenberg', 's', 'the', 'adventures', 'of', 'sherlock', 'holmes', 'by', ............................... , 'our', 'email', 'newsletter', 'to', 'hear', 'about', 'new', 'ebooks']</span></pre><p id="f027" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，对于特征工程部分，我们需要有唯一的排序单词列表。我们还需要一个字典(<key: value="">)，将unique_words列表中的每个单词作为键，将其对应的位置作为值。</key:></p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="c75b" class="ka kb hi lb b fi lf lg l lh li">unique_words = np.unique(words)<br/>unique_word_index = dict((c, i) for i, c in enumerate(unique_words))</span></pre><p id="88c0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">特色工程</strong></p><p id="59fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据维基百科的说法，<strong class="ix hj">特征工程</strong>是使用数据的<a class="ae iu" href="https://en.wikipedia.org/wiki/Domain_knowledge" rel="noopener ugc nofollow" target="_blank">领域知识</a>来创建<a class="ae iu" href="https://en.wikipedia.org/wiki/Feature_(machine_learning)" rel="noopener ugc nofollow" target="_blank">特征</a>的过程，这些特征使<a class="ae iu" href="https://en.wikipedia.org/wiki/Machine_learning" rel="noopener ugc nofollow" target="_blank">机器学习</a>算法工作。特征工程是机器学习应用的基础，既困难又昂贵。</p><p id="d97d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们定义了一个单词长度，这意味着前一个单词的数量决定了下一个单词。此外，我们创建一个名为prev_words的空列表来存储一组五个前面的单词及其在next_words列表中对应的下一个单词。我们通过在小于单词长度的范围内循环来填充这些列表。</p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="ff88" class="ka kb hi lb b fi lf lg l lh li">WORD_LENGTH = 5<br/>prev_words = []<br/>next_words = []<br/>for i in range(len(words) - WORD_LENGTH):<br/>    prev_words.append(words[i:i + WORD_LENGTH])<br/>    next_words.append(words[i + WORD_LENGTH])<br/>print(prev_words[0])<br/>print(next_words[0])</span><span id="c0af" class="ka kb hi lb b fi lk lg l lh li"><strong class="lb hj">Output<br/></strong>['project', 'gutenberg', 's', 'the', 'adventures']<br/>of</span></pre><p id="8bda" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，是生成特征向量的时候了。为了生成特征向量，我们使用<strong class="ix hj">一键编码</strong>。</p><figure class="jw jx jy jz fd ij"><div class="bz dy l di"><div class="ll lm l"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">解释:一键编码</figcaption></figure><p id="c151" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，我们创建两个numpy数组X(用于存储特性)和Y(用于存储相应的标签(这里是下一个单词))。我们迭代X和Y，如果单词存在，那么相应的位置为1。</p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="2da5" class="ka kb hi lb b fi lf lg l lh li">X = np.zeros((len(prev_words), WORD_LENGTH, len(unique_words)), dtype=bool)<br/>Y = np.zeros((len(next_words), len(unique_words)), dtype=bool)</span><span id="abb2" class="ka kb hi lb b fi lk lg l lh li">for i, each_words in enumerate(prev_words):<br/>    for j, each_word in enumerate(each_words):<br/>        X[i, j, unique_word_index[each_word]] = 1<br/>    Y[i, unique_word_index[next_words[i]]] = 1</span></pre><p id="3280" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看一个单一的序列:</p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="8913" class="ka kb hi lb b fi lf lg l lh li">print(X[0][0])</span><span id="2b26" class="ka kb hi lb b fi lk lg l lh li">Output<br/>[False False False … False False False]</span></pre><h2 id="7f6d" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated">构建模型</h2><p id="18cb" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">我们使用具有128个神经元的单层LSTM模型、全连接层和用于激活的softmax函数。</p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="65cd" class="ka kb hi lb b fi lf lg l lh li">model = Sequential()<br/>model.add(LSTM(128, input_shape=(WORD_LENGTH, len(unique_words))))<br/>model.add(Dense(len(unique_words)))<br/>model.add(Activation('softmax'))</span></pre><h2 id="4dee" class="ka kb hi bd kc kd ke kf kg kh ki kj kk jg kl km kn jk ko kp kq jo kr ks kt ku bi translated"><strong class="ak">培训</strong></h2><p id="1105" class="pw-post-body-paragraph iv iw hi ix b iy kv ja jb jc kw je jf jg kx ji jj jk ky jm jn jo kz jq jr js hb bi translated">该模型将用RMSprop优化器用20个时期来训练。</p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="3498" class="ka kb hi lb b fi lf lg l lh li">optimizer = RMSprop(lr=0.01)<br/>model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])<br/>history = model.fit(X, Y, validation_split=0.05, batch_size=128, epochs=2, shuffle=True).history</span></pre><p id="f75a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">训练成功后，我们将保存训练好的模型，并在需要时重新加载。</p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="4b23" class="ka kb hi lb b fi lf lg l lh li">model.save('keras_next_word_model.h5')<br/>pickle.dump(history, open("history.p", "wb"))</span><span id="1001" class="ka kb hi lb b fi lk lg l lh li">model = load_model('keras_next_word_model.h5')<br/>history = pickle.load(open("history.p", "rb"))<br/></span></pre><p id="a767" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">评估</strong></p><p id="72cb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">模型在训练成功后输出训练评估结果，我们也可以从历史变量中访问这些评估。</p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="6143" class="ka kb hi lb b fi lf lg l lh li">{‘val_loss’: [6.99377903472107, 7.873811178441364], ‘val_accuracy’: [0.1050897091627121, 0.10563895851373672], ‘loss’: [6.0041207935270124, 5.785401324014241], ‘accuracy’: [0.10772078, 0.14732216]}</span><span id="727b" class="ka kb hi lb b fi lk lg l lh li"># sample evaluation ---- # only 2 epochs</span></pre><p id="9935" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">预测</strong></p><p id="6366" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们需要使用这个模型来预测新单词。为此，我们输入样本作为特征向量。我们将输入字符串转换成一个单一的特征向量。</p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="2b5f" class="ka kb hi lb b fi lf lg l lh li">def prepare_input(text):<br/>    x = np.zeros((1, WORD_LENGTH, len(unique_words)))<br/>    for t, word in enumerate(text.split()):<br/>        print(word)<br/>        x[0, t, unique_word_index[word]] = 1<br/>    return x</span><span id="af6a" class="ka kb hi lb b fi lk lg l lh li">prepare_input("It is not a lack".lower())</span><span id="dd05" class="ka kb hi lb b fi lk lg l lh li"><strong class="lb hj">Output</strong><br/>array([[[ 0.,  0.,  0., ...,  0.,  0.,  0.],<br/>        [ 0.,  0.,  0., ...,  0.,  0.,  0.],<br/>        [ 0.,  0.,  0., ...,  0.,  0.,  0.],<br/>        ..., <br/>        [ 0.,  0.,  0., ...,  0.,  0.,  0.],<br/>        [ 0.,  0.,  0., ...,  0.,  0.,  0.],<br/>        [ 0.,  0.,  0., ...,  0.,  0.,  0.]]])</span></pre><p id="ca24" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在通过样本函数从模型中进行预测之后，选择最佳可能的n个单词。</p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="8376" class="ka kb hi lb b fi lf lg l lh li">def sample(preds, top_n=3):<br/>    preds = np.asarray(preds).astype('float64')<br/>    preds = np.log(preds)<br/>    exp_preds = np.exp(preds)<br/>    preds = exp_preds / np.sum(exp_preds)<br/><br/>    return heapq.nlargest(top_n, range(len(preds)), preds.take)</span></pre><p id="deeb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，对于预测，我们使用函数predict_completions，它使用模型来预测并返回n个预测单词的列表。</p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="ac9a" class="ka kb hi lb b fi lf lg l lh li">def predict_completions(text, n=3):<br/>    if text == "":<br/>        return("0")<br/>    x = prepare_input(text)<br/>    preds = model.predict(x, verbose=0)[0]<br/>    next_indices = sample(preds, n)<br/>    return [unique_words[idx] for idx in next_indices]</span></pre><p id="aabf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们看看它是如何预测的，我们使用tokenizer.tokenize来删除标点符号，我们还选择了前5个单词，因为我们的预测基于前5个单词。</p><pre class="jw jx jy jz fd la lb lc ld aw le bi"><span id="adca" class="ka kb hi lb b fi lf lg l lh li">q =  "Your life will never be the same again"<br/>print("correct sentence: ",q)<br/>seq = " ".join(tokenizer.tokenize(q.lower())[0:5])<br/>print("Sequence: ",seq)<br/>print("next possible words: ", predict_completions(seq, 5))</span><span id="5793" class="ka kb hi lb b fi lk lg l lh li"><strong class="lb hj">Output</strong><br/>correct sentence:  Your life will never be the same again<br/>Sequence:  your life will never be<br/>next possible words:  ['the', 'of', 'very', 'no', 'in']</span></pre><p id="78d4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">弊端</strong></p><ul class=""><li id="a754" class="ln lo hi ix b iy iz jc jd jg lp jk lq jo lr js ls lt lu lv bi translated">这里，在准备唯一的单词时，我们只从输入数据集中收集唯一的单词，而不是从英语词典中收集。因为这个原因，很多都被忽略了。(要创建如此大的输入集(根据nltk，英语词典包含约23000个单词，我们需要执行<strong class="ix hj">批处理)</strong></li></ul><p id="c677" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">参考文献</strong></p><ul class=""><li id="aa73" class="ln lo hi ix b iy iz jc jd jg lp jk lq jo lr js ls lt lu lv bi translated"><a class="ae iu" href="https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/" rel="noopener ugc nofollow" target="_blank">tensor flow中的递归网络</a></li><li id="36fb" class="ln lo hi ix b iy lw jc lx jg ly jk lz jo ma js ls lt lu lv bi translated"><a class="ae iu" rel="noopener" href="/@curiousily/making-a-predictive-keyboard-using-recurrent-neural-networks-tensorflow-for-hackers-part-v-3f238d824218">使用递归神经网络制作预测键盘</a></li><li id="415d" class="ln lo hi ix b iy lw jc lx jg ly jk lz jo ma js ls lt lu lv bi translated"><a class="ae iu" href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">递归神经网络的不合理有效性</a></li><li id="5c35" class="ln lo hi ix b iy lw jc lx jg ly jk lz jo ma js ls lt lu lv bi translated"><a class="ae iu" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">了解LSTM网络</a></li><li id="9a7b" class="ln lo hi ix b iy lw jc lx jg ly jk lz jo ma js ls lt lu lv bi translated"><a class="ae iu" href="https://peterroelants.github.io/posts/rnn_implementation_part01/" rel="noopener ugc nofollow" target="_blank">如何用Python实现RNN</a></li><li id="e61d" class="ln lo hi ix b iy lw jc lx jg ly jk lz jo ma js ls lt lu lv bi translated"><a class="ae iu" href="http://deeplearning.net/tutorial/lstm.html" rel="noopener ugc nofollow" target="_blank">用于情感分析的LSTM网络</a></li><li id="9618" class="ln lo hi ix b iy lw jc lx jg ly jk lz jo ma js ls lt lu lv bi translated"><a class="ae iu" href="http://cs231n.stanford.edu/slides/2016/winter1516_lecture10.pdf" rel="noopener ugc nofollow" target="_blank"> cs231n —递归神经网络</a></li></ul></div></div>    
</body>
</html>