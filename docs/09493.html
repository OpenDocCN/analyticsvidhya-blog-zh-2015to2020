<html>
<head>
<title>Sequence to Sequence Learning — Paper Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">序列对序列学习—论文说明</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sequence-to-sequence-learning-paper-explained-ce346158e0a6?source=collection_archive---------6-----------------------#2020-09-08">https://medium.com/analytics-vidhya/sequence-to-sequence-learning-paper-explained-ce346158e0a6?source=collection_archive---------6-----------------------#2020-09-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="a4c3" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">seq2seq模型中的编码器和解码器到底是什么？</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/a52e029795a70960f5ac19e01583765f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6KSJoc7JEOeUU5G3X_ooPA.png"/></div></div></figure><p id="af21" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这个博客给出了Seq2Seq模型的高级直觉，所以你不需要成为“深度学习者”来理解它的全部，基本的神经网络知识就足以理解这个博客。</p><p id="cb8f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">研究论文<a class="ae kg" href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="kf">用神经网络</em> </a>进行序列对序列学习，被认为是谷歌在<em class="kf">神经信息处理系统(NIPS) </em>会议上发布该论文后，在自然语言处理领域取得的突破。在理解Seq2Seq之后，人们可以继续阅读变形金刚、注意力概念以及NLP领域的其他最新突破。</p><p id="4e35" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">从一开始就已经有了以下rnn:</p><ol class=""><li id="1f34" class="kh ki hi jl b jm jn jp jq js kj jw kk ka kl ke km kn ko kp bi translated"><strong class="jl hj">矢量对矢量(Vec2Vec) RNN </strong></li></ol><p id="574a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">该模型将单个向量作为输入，并产生单个向量作为输出。</p><p id="0473" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">一个例子可以是字对字的翻译。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kq"><img src="../Images/cc5267266f1c782e5bf331bccedf04ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/format:webp/1*x4_OHnfVl2oBjJ4mREXAww.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">Vec2Vec模型</figcaption></figure><p id="8a54" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 2。序列到向量(Seq2Vec) RNN </strong></p><p id="769b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在RNN模型中，我们给出输入，输出的序列是一个单一的向量。<br/>这样的一个例子可以是语言预测器，</p><p id="65cb" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi"><em class="kf">我爱计算机科学</em> (input sequence) gives <em class="kf">Chinese</em>(output vector)</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kv"><img src="../Images/b2e4710421f32c8662e99c97c0f10c14.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*EpxbcNRWdtEHhxzxcMV1-A.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">Seq2Vec模型</figcaption></figure><p id="4a1f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 3。向量到序列(Vec2Sec) RNN </strong></p><p id="e2c5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">RNN模型，其中输入是单个向量，输出一个序列。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kw"><img src="../Images/a803f369289fb1c2393037d9c87cfb4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*vHQ3O5guCMqla7oMYVvVVw.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">Vec2Seq模型</figcaption></figure><p id="243f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 4。序列对序列(Seq2Seq) RNN </strong></p><p id="cb70" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">是啊！我们之前也有序列对序列模型，它将输入作为一个序列，也按序列提供输出。<br/> <em class="kf">注意:它完全不同于本文中描述的编码器-解码器架构，本文稍后将对此进行解释</em></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kx"><img src="../Images/2d06b753a916730e61e52a749e3098f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*pkcvvNv26vogdU0ZYEf3cw.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">传统Seq2Seq RNN模型。</figcaption></figure><p id="35b8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">尽管有这个传统的seq2seq RNN模型。这篇论文怎么可能是一个突破？<br/>在这里，<strong class="jl hj">在上图中，我们可以看到输出序列的长度与输入序列相同。语言翻译模型也是如此。如果我们试图将英语转换为印地语，这并不一定意味着英语序列的长度与印地语序列的长度相同。这就是编码器和解码器有用的地方。</strong></p><h1 id="f14f" class="ky kz hi bd la lb lc ld le lf lg lh li io lj ip lk ir ll is lm iu ln iv lo lp bi translated">编码器和解码器</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lq"><img src="../Images/e15136e37df74bb1d332c0a018905044.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TD1pHfYgXU5xaBXrN-jNTA.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">Seq2Seq的编码器解码器架构</figcaption></figure><p id="85cc" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">上面显示的是Seq2Seq -编码器解码器研究论文中提出的相同架构。它由2个基本成分组成——<br/>1。编码器<br/> 2。下面显示的解码器<br/>是编码器和解码器的详细图解。使用编码器/解码器模型的主要优点是输入长度和输出长度可以不同。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/a2a3fdce7b8a50964140a6ca02ed0b22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*13CteA43Soi6dZFp7vYI2Q.png"/></div></div></figure><ul class=""><li id="c2d6" class="kh ki hi jl b jm jn jp jq js kj jw kk ka kl ke ls kn ko kp bi translated">该架构中的每个单元都是一个RNN网络<em class="kf">(可以是LSTM网络，也可以是GRU网络)。</em></li><li id="495b" class="kh ki hi jl b jm lt jp lu js lv jw lw ka lx ke ls kn ko kp bi translated">编码器接收输入<em class="kf"> (A，B，C，&lt; eos &gt; ) </em>，每个单元格接收来自<strong class="jl hj">隐藏状态</strong> <em class="kf">的反馈(每个单元格之间的链接)。</em></li><li id="2e35" class="kh ki hi jl b jm lt jp lu js lv jw lw ka lx ke ls kn ko kp bi translated"><em class="kf"> W </em>是<strong class="jl hj">的上下文向量</strong>。</li><li id="fc34" class="kh ki hi jl b jm lt jp lu js lv jw lw ka lx ke ls kn ko kp bi translated">解码器是产生输出<em class="kf"> (X，Y，Z，&lt; eos &gt;的部分。</em></li></ul><p id="317d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们试着详细了解每个组件，这样我们就可以了解编码器、解码器的整体功能，以及信息如何从编码器传递到解码器。</p><h2 id="a4fd" class="ly kz hi bd la lz ma mb le mc md me li js mf mg lk jw mh mi lm ka mj mk lo ml bi translated">编码器</h2><p id="b764" class="pw-post-body-paragraph jj jk hi jl b jm mm ij jo jp mn im jr js mo ju jv jw mp jy jz ka mq kc kd ke hb bi translated">编码器是每个单元中的递归RNN网络。每个单元接收单个向量，后跟一个语句结束标记，<eos>标记输入序列的结束，并生成<strong class="jl hj">上下文向量，</strong> <strong class="jl hj"> W </strong>。</eos></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lq"><img src="../Images/86c464d0c26c1be3584d0990fc96eadd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mvYUaNkDwHKB1t-PZlN1LQ.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">编码器-解码器模型中的典型编码器，显示了RNN块、输入以及上下文向量w之间的关系</figcaption></figure><p id="1ebd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">它负责接收输入并将信息传递给下一个细胞。这个被传递的信息被称为<strong class="jl hj">反馈(隐藏状态)</strong>，并通过连续单元格之间的链接显示出来。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mr"><img src="../Images/9dcd1f9721a46b51b0e87a14a7b84b37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*3bMt1jxpB7Gdh79T6JhcRQ.png"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">隐藏状态(反馈)计算公式</figcaption></figure><p id="f566" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这里提到的公式用于计算隐藏状态的反馈值，该值包含直到最后一个单元格输入的所有信息。这里，<em class="kf"> Wʰʰ </em>是在时间戳<em class="kf"> (t-1) </em>赋予前一隐藏状态的权重，<em class="kf"> Wʰˣ </em>是在时间戳<em class="kf"> t. </em>赋予当前输入的权重</p><h2 id="3b16" class="ly kz hi bd la lz ma mb le mc md me li js mf mg lk jw mh mi lm ka mj mk lo ml bi translated"><strong class="ak">上下文向量——编码器和解码器之间的链接</strong></h2><p id="f5b5" class="pw-post-body-paragraph jj jk hi jl b jm mm ij jo jp mn im jr js mo ju jv jw mp jy jz ka mq kc kd ke hb bi translated">上下文向量<strong class="jl hj">包含来自编码器内部所有隐藏状态的所有信息</strong>,它仅充当解码器部分的输入。</p><h2 id="ba2d" class="ly kz hi bd la lz ma mb le mc md me li js mf mg lk jw mh mi lm ka mj mk lo ml bi translated">解码器</h2><p id="bef9" class="pw-post-body-paragraph jj jk hi jl b jm mm ij jo jp mn im jr js mo ju jv jw mp jy jz ka mq kc kd ke hb bi translated">解码器负责产生输出。每个单元在时间戳<em class="kf"> (t-1) </em>接收从前一个单元生成的输出，作为当前单元在时间戳<em class="kf"><br/></em>的输入。由编码器的最后一个单元生成的上下文向量充当解码器中RNN单元的初始输入。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ms"><img src="../Images/d117fa1bde033080be7f3e7ff642d555.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f1HywyX6etAULoKu3578EA.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">编码器-解码器模型中的典型解码器，显示RNN块、输入和输出之间的关系</figcaption></figure><p id="0ab7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">需要注意的关键点是，单元在时间标记<em class="kf"> t </em>产生的输出被用作单元在时间标记<em class="kf"> t+1的输入。</em></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mt"><img src="../Images/5b4478d71aa90fd9aa7841df9246fa45.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*tCxkRlnO6LKybJHXBta4Ng.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mu"><img src="../Images/e9353583d1817d90e856ca8c48d0ea48.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*KnO0NsONpQvzigm1i9AeMg.png"/></div></figure><p id="9bea" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这两个公式用于计算隐藏状态反馈和每个单元的输出。</p><p id="5d26" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这是关于编码器和解码器功能的所有基本知识。<strong class="jl hj">这篇论文成为自然语言处理(NLP)研究和应用领域的一项突破，因为现在输出序列的长度可以不同于输入序列的长度</strong>，这使得它在机器翻译、问答系统、对话生成模型和许多其他领域中非常健壮和有用。</p></div></div>    
</body>
</html>