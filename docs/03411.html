<html>
<head>
<title>Webscraping movie sessions in Python using Scrapy and pymongo</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Scrapy和pymongo在Python中抓取电影会话</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/webscraping-movie-sessions-in-python-using-scrapy-and-pymongo-1ab0ab8e4bd6?source=collection_archive---------5-----------------------#2020-01-30">https://medium.com/analytics-vidhya/webscraping-movie-sessions-in-python-using-scrapy-and-pymongo-1ab0ab8e4bd6?source=collection_archive---------5-----------------------#2020-01-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="463e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">一个如何用python将电影片段抓取并保存到数据库的例子。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/70ae2722bd7af09b61c94058a7201675.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xYy3dKsfdBAUcF8CQ4660w.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">穆维。一个web应用程序项目的电影会议和门票计划在巴西。</figcaption></figure></div><div class="ab cl jn jo gp jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="hb hc hd he hf"><p id="af38" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在本文中，我将展示一个简单的例子，说明如何使用Python中的Scrapy和pymongo库轻松地从电影网站获取电影会话数据，并将其保存到数据库中。我在一个个人项目中使用了这个程序，在那里我从Cinermark的网站上抓取了会话，并将数据保存在mongoDB数据库中。</p><p id="0dd3" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">因此，让我们首先设置我们的python环境来开始这个例子(我使用anaconda)，如果你还没有安装它，我建议在这里查看<a class="ae kq" href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html" rel="noopener ugc nofollow" target="_blank"><strong class="jw hj"/></a>并遵循指导方针。在您的终端中进行如下设置:</p><pre class="iy iz ja jb fd kr ks kt ku aw kv bi"><span id="79c9" class="kw kx hi ks b fi ky kz l la lb">conda create --name webscraping</span></pre><p id="6202" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">现在让我们安装我们的依赖项:</p><pre class="iy iz ja jb fd kr ks kt ku aw kv bi"><span id="afb7" class="kw kx hi ks b fi ky kz l la lb">conda activate webscraping<br/>conda install -c conda-forge scrapy<br/>conda install pymongo</span></pre><p id="ebd3" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">就这样，我们的环境可以开始编码了。(如果您愿意，您可以稍后将其他有用的抓取库安装到该环境中)。</p><h1 id="ef7c" class="lc kx hi bd ld le lf lg lh li lj lk ll io lm ip ln ir lo is lp iu lq iv lr ls bi translated">蜘蛛</h1><p id="532b" class="pw-post-body-paragraph ju jv hi jw b jx lt ij jz ka lu im kc kd lv kf kg kh lw kj kk kl lx kn ko kp hb bi translated">为了成功抓取我们的电影网站，我们必须写下Scrapy spiders，这是python类，包含的方法和属性将明确告诉Scrapy您想要抓取哪些URL以及您想要从中提取什么。所以让我们从这个开始一个基本的零碎的工作空间:</p><pre class="iy iz ja jb fd kr ks kt ku aw kv bi"><span id="3387" class="kw kx hi ks b fi ky kz l la lb">mkdir myproject<br/>scrapy startproject cinemarkbot myproject/</span></pre><p id="52cd" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">这将在myproject目录中创建一个名为cinemarkbot的工作区，看起来如下所示:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/2e7d927b9c95924ee3b417bac6748eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*JlOWIer5z9SY6k3w_5Fa2g.png"/></div></figure><p id="9518" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在这里，我在spiders文件夹中创建了一个名为<strong class="jw hj">的python文件。在这个文件夹中，你可以创建所有你想让Scrapy运行的蜘蛛。稍后我们必须告诉Scrapy用<strong class="jw hj"> crawl </strong>命令运行哪个蜘蛛。不要担心其他文件，我们稍后会详细讨论它们。</strong></p><p id="fea9" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">首先，让我们把注意力放在我们的蜘蛛上。这就是我如何写我的<strong class="jw hj"> scrapemovies </strong> spider:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="3eb5" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">是的，我知道，很多东西，让我们来看看部分:</p><ul class=""><li id="7a61" class="mb mc hi jw b jx jy ka kb kd md kh me kl mf kp mg mh mi mj bi translated"><strong class="jw hj"> name </strong>变量是您将从<strong class="jw hj"> crawl </strong>命令调用这个蜘蛛的字符串。</li><li id="1e05" class="mb mc hi jw b jx mk ka ml kd mm kh mn kl mo kp mg mh mi mj bi translated">方法<strong class="jw hj"> start_requests </strong>返回Scrapy将开始抓取的所有页面请求。在这个例子中，因为除了城市名称之外，Cinemark电影会话的所有URL都是相似的，所以我使用了一个循环来为每个城市产生不同的请求。</li><li id="1b47" class="mb mc hi jw b jx mk ka ml kd mm kh mn kl mo kp mg mh mi mj bi translated"><strong class="jw hj"> parse </strong>方法是在抓取每个请求时调用的方法。这里是所有神奇事情发生的地方，在这里您定义要提取什么数据，在这里您可能会花费大量时间来寻找正确的CSS选择器。<a class="ae kq" href="https://docs.scrapy.org/en/latest/topics/selectors.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jw hj">这个教程</strong> </a> <strong class="jw hj"> </strong>在数据提取上帮了我大忙。我在终端上运行scrapy shell来下载一个电影会话页面，并尝试了许多不同的选择器来准确找到我需要的内容(电影名称、电影院名称、城市名称、会话日期和会话时间)。使用浏览器的开发工具来检查HTML也是必不可少的。</li></ul><h1 id="6577" class="lc kx hi bd ld le lf lg lh li lj lk ll io lm ip ln ir lo is lp iu lq iv lr ls bi translated">项目</h1><p id="1039" class="pw-post-body-paragraph ju jv hi jw b jx lt ij jz ka lu im kc kd lv kf kg kh lw kj kk kl lx kn ko kp hb bi translated">使用element.css()从CSS选择器中提取数据。get()方法。在这里，我将每个会话数据存储在一个名为session的结构化Scrapy项中。您可以通过编辑<strong class="jw hj"> items.py </strong>文件来定义这些项目，如下所示:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="4191" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">然后在parse方法上，我为每个会话实例化了一个条目，并用提取的数据填充了所有字段。当每个会话项完成时，您返回一个带有<strong class="jw hj"> yield项的生成器。</strong></p><h1 id="8418" class="lc kx hi bd ld le lf lg lh li lj lk ll io lm ip ln ir lo is lp iu lq iv lr ls bi translated">项目管道</h1><p id="8d73" class="pw-post-body-paragraph ju jv hi jw b jx lt ij jz ka lu im kc kd lv kf kg kh lw kj kk kl lx kn ko kp hb bi translated">每个返回的项目将被传递到项目管道，以存储在我们的mongo数据库。Pipelines是一个Scrapy工具，它定义了一系列定制方法，在抓取过程中将为每个项目调用这些方法，因此您可以运行任何python脚本来在项目管道文件中操作这些项目，这正是我们将这些项目作为单个文档保存到MongoDB集合中所要做的。以下是我如何编写我的管道:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="4087" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">同样，让我们一步步深入了解:</p><ul class=""><li id="ffb6" class="mb mc hi jw b jx jy ka kb kd md kh me kl mf kp mg mh mi mj bi translated">有一个__init__方法可以从MongoDB设置中构造我们的管道对象，这些变量取自<strong class="jw hj"> settings.py </strong>文件。您应该用正确的值编辑这个文件，MONGO_DB是您想要连接的数据库的名称，MONGO_URI是建立到MONGO的连接的URI，它根据您是使用本地还是基于云的MONGO而不同，我使用的是mongoDB Atlas。</li><li id="d761" class="mb mc hi jw b jx mk ka ml kd mm kh mn kl mo kp mg mh mi mj bi translated"><strong class="jw hj"> from_crawler </strong>被调用，从crawler创建我们的管道实例。它访问我们的设置文件中的变量，并返回管道的新实例。</li><li id="1246" class="mb mc hi jw b jx mk ka ml kd mm kh mn kl mo kp mg mh mi mj bi translated"><strong class="jw hj">打开_蜘蛛</strong>和<strong class="jw hj">关闭_蜘蛛</strong>在我们的蜘蛛打开和关闭时被调用。它们只被调用一次，这里我们使用pymongo用它来连接和断开我们的mongoDB数据库。MongoClient() 。我还使用<strong class="jw hj"> deleteMany({}) </strong>清除movie_sessions集合中的任何现有文档，因此每次蜘蛛运行时都会更新所有会话。</li><li id="2d7f" class="mb mc hi jw b jx mk ka ml kd mm kh mn kl mo kp mg mh mi mj bi translated"><strong class="jw hj"> process_item </strong>是进入此管道的每个项目将被调用的内容。这里我们使用pymongo将movie_sessions集合中的项目保存为python字典，使用<strong class="jw hj">。insertOne(dict(item)) </strong>。注意，我必须添加一个if语句来检查该项目是否已经在我的集合中(不知何故，它保存了重复的项目)。此方法必须返回该项以将其发送到其他管道，或者引发DropItem异常以将其从其他进程中排除。</li></ul><p id="0778" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">为了使此管道工作，您需要将其包含在设置文件中的ITEM_PIPELINES对象内。值300指定您希望运行管道的顺序，它从0到1000。</p><p id="a50a" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">这是我的设置文件:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="lz ma l"/></div></figure><p id="d90c" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">有关项目管道如何工作的更多信息，请参见此处的<a class="ae kq" href="https://docs.scrapy.org/en/latest/topics/item-pipeline.html" rel="noopener ugc nofollow" target="_blank"><strong class="jw hj"/></a><strong class="jw hj">。</strong></p><h1 id="522d" class="lc kx hi bd ld le lf lg lh li lj lk ll io lm ip ln ir lo is lp iu lq iv lr ls bi translated">运转</h1><p id="5580" class="pw-post-body-paragraph ju jv hi jw b jx lt ij jz ka lu im kc kd lv kf kg kh lw kj kk kl lx kn ko kp hb bi translated">现在所有的工作都完成了，我们已经为运行我们的蜘蛛做好了一切准备。所以让我们开始吧！在顶级目录(myproject文件夹)中，运行以下命令:</p><pre class="iy iz ja jb fd kr ks kt ku aw kv bi"><span id="e253" class="kw kx hi ks b fi ky kz l la lb">scrapy crawl scrapecinemark</span></pre><p id="1cb2" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">你应该会看到很多事情发生的非常快，Scrapy发送很多日志到终端，包括你正在抓取的物品。它会在完成时发送一条消息，还会记录过程中可能发生的任何错误(我经常遇到这种情况)，所以如果第一次尝试没有成功，也不要沮丧。</p><p id="bfc5" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在它运行之后，如果您访问mongoDB数据库，您应该已经看到了存储在您的集合中的所有电影会话项目。像这样:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mp"><img src="../Images/fdc963f9e5c4e093b83b62afe6e4bcfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Px84mMyNoUMMbk9mqrkHsQ.png"/></div></div></figure><p id="5b58" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">我们完事了。来自巴西所有城市的所有Cinemark会话都以分钟的方式存储在我们的数据库中，Scrapy真的很神奇！但这只是Scrapy所能做的事情的冰山一角，对于更多的爱好者，可以在他们的详细文档<a class="ae kq" href="https://docs.scrapy.org/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jw hj">这里</strong> </a>中找到更多的功能。</p><p id="1e5c" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在我的项目中，我在heroku 中部署了这个程序，并添加了一个调度程序来每天运行我的蜘蛛，这非常容易和简单，也许将来我会就此发表另一篇文章。</p><p id="1ba3" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">祝所有人编码快乐！</p><p id="0db4" class="pw-post-body-paragraph ju jv hi jw b jx jy ij jz ka kb im kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">再见。</p></div></div>    
</body>
</html>