<html>
<head>
<title>Neural Networks Part 3: Understanding Back propagation &amp; Learning Rates</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络第3部分:理解反向传播和学习率</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-networks-part-3-understanding-back-propagation-learning-rates-3482a981a2f0?source=collection_archive---------6-----------------------#2020-06-20">https://medium.com/analytics-vidhya/neural-networks-part-3-understanding-back-propagation-learning-rates-3482a981a2f0?source=collection_archive---------6-----------------------#2020-06-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a474" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将了解如何计算梯度，以及网络如何通过反向传播进行学习。根据<a class="ae jd" href="https://link.medium.com/2BL0UqqMm7" rel="noopener">上一篇关于我们讨论神经网络和梯度下降的文章</a>。我们讨论了梯度，以及它如何减少损失，直到我们收敛到全局最小值。</p><p id="dc36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来看看一个只有一个隐藏神经元和一个输出的网络。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/d476334553cf3d2fc8d333c3a972fb6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*IFA4JPtvqPA5XZwuumv12g.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">一个小小的改变。:w2)影响最终损失J(W)？</figcaption></figure><p id="a156" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算我们的损失相对于权重W2(它是输出和隐藏层之间的第二个权重)的梯度。我们的输出可以告诉我们W2的微小变化会对我们的损失产生多大的影响。如果我们小幅度改变W2，我们的损失变化有多大？它往下走吗？它改变了多少，改变了多少？让我们看看这个损失相对于W2的导数，为了评估它，我们可以应用微积分中的链式法则。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jq"><img src="../Images/1a21b411276637c44d62e2da13766ad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*QhNb6UlSm3s5H-6rvPX__g.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">通过应用链式法则</figcaption></figure><p id="f913" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以将其分解为我们的损耗相对于输出的梯度y_hat乘以我们的输出相对于我们的重量的梯度(W2)。假设你想对一个神经网络重复这个过程，比如说W1而不是W2，那么我们在两边都替换W1 &amp;我们也应用链式法则。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jr"><img src="../Images/2780b7613aae1bc9baada0e1d815857f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*ty5QL8PschFxE3w7zZ7I0w.png"/></div></figure><p id="b74c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是现在你会注意到，y相对于W1的梯度是不可直接计算的。我们必须再次应用链式法则来评估它。再次应用链式法则后，我们把第二项分解成y相对于状态z1的梯度和状态z1相对于W1的梯度。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es js"><img src="../Images/35c0177db301e949c327bbfb6b4b6319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*YQ2qLfd6ogbGNCrL-ewaOQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">将所有这些梯度从输出反向传播到输入</figcaption></figure><p id="9ff7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jt">因此，我们将所有这些梯度从输出端一路反向传播到输入端，从而允许误差信号从输出端传播到输入端，并允许计算这些梯度。</em>现在在实践中，理解这里的复杂数学并不像在许多流行的深度学习框架中那样重要，我们有所谓的自动微分，它在引擎盖下完成所有这些反向传播，而我们甚至从未看到它……令人难以置信的是，您不必再从头开始实现反向传播，但理解这些如何在底层工作仍然很重要，这就是本文的目的。</p><p id="04f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，对网络中的每一层重复上述过程。这里我们只展示了W1和W2，它们是一个单层，但是当你有更多层时，你可以再重复一次。继续应用输出、输入和计算的链式法则。这个过程就是反向传播。这就是网络如何从这种错误或损失中学习，并试图纠正那些旨在减少损失的错误。</p><p id="a2b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实践中，神经网络的优化是极其困难的，它不像第一张图所示的那样简单。如果我们尝试可视化一个景观，一个具有各种参数的神经网络，并尝试将其压缩到一个二维空间，这样你就可以可视化它。你可以看到，在现实中，景观是复杂的，可能会有许多局部最小值，梯度下降算法可能会陷入其中，在这些类型的环境中应用梯度下降在神经网络中是非常标准的，可能非常具有挑战性。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es ju"><img src="../Images/c6835cfbd301c0c93103d6cee085d31a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aiS1idlQhAiQJLkZmKidYg.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">梯度下降算法会陷入局部极小值</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es jz"><img src="../Images/2004595b18ffbb3eafb3bcd9468d1bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*UZTNv1ZAglhUnmoxdEvoFQ.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">根据梯度下降算法更新方程(第三点)</figcaption></figure><p id="a949" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在回想一下我们之前在梯度下降算法中定义的更新方程。这是同一个方程，我们将在梯度的相反方向上更新我们的权重。如果我们看看这个<strong class="ih hj"> η </strong> (eta)，它是决定我们应该在那个梯度的方向上走多少步的学习速率，在实践中设置这个学习速率会对你的表现产生巨大的影响。</p><p id="048d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你把学习率设置得太小，这意味着你没有真正相信你每一步的梯度，所以如果你的学习率太小，你只会向梯度的相反方向移动一点点，或者只是以很小的增量移动。可能发生的是，你会陷入这些局部极小值，因为你没有足够的进取心去逃避这些局部极小值。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ka"><img src="../Images/7940231d717f8ebbf4f9855e2172c0ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*GjLAgjRNpbbwJPtRdmmh7w.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">a.)小学习率收敛慢，卡在伪局部极小。而b .)大的学习速率超调，变得不稳定和发散。</figcaption></figure><p id="562a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你把学习速率设置得太大，实际上你可能会完全超调并发散，这是更不可取的。因此，在实践中设定学习率可能相当具有挑战性。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kb"><img src="../Images/45e0a03ce50a6c6af6747ac2134c89c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*dWAm0f6UlGEzVnUoukMrBQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">稳定的学习速率平稳地收敛并避免局部极小值。</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kc"><img src="../Images/3068d195e0349dced3a206b6c7b014a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*azPI8vqG9VkFf8TPAzR-JA.png"/></div></figure><p id="0cf2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，你一定会问，我们如何设定合适的学习速度？选项1是我们尝试一系列的学习率，看看什么效果最好。第二个选择是，我们可以尝试变得更聪明一点，尝试拥有一个<em class="jt">自适应学习速率</em>，它随着我们的环境或学习速度的变化而变化，或者随着我们网络优化中的一系列其他想法而变化。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kd"><img src="../Images/fcf52fc24d9642d725db8be7b67b90e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*3Bv-qv0buR_HzzgnmTzEJQ.png"/></div></figure><p id="a817" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这意味着我们的学习速度不再是固定的，而是可以在整个训练过程中增加或减少。因此，随着训练的进行，你的学习速度可能会加快，你可以采取更积极的步骤，你可以采取更小的步骤，更接近局部最小值，这样你就可以真正收敛到那个点。关于如何设计这种自适应算法，这里有许多选择，这是ml &amp; dl损失优化理论中广泛研究的领域。对于这些不同类型的自适应学习率，tensorflow中有不同类型的论文和实现。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ke"><img src="../Images/f21b415cc3704341d529fe159db3569e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*TZyxTtXFTrhG-FFxIOhnCA.png"/></div></figure><p id="a6f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SGD是我之前展示过的普通梯度下降。所有其他的都是适应性学习率，这意味着它们在训练过程中改变它们的学习率。因此它们可以根据优化的进展而增加或减少</p><p id="97f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总之，我们可以在这个顺序包装器中定义我们的模型。在这个顺序包装器中，我们有所有由<a class="ae jd" href="https://link.medium.com/TOSFsnlxo7" rel="noopener">感知器或单个神经元</a>组成的层。然后，我们可以使用我们的优化随机梯度下降，也可以是任何自适应学习率。</p><p id="90cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的训练循环中，我们<em class="jt">通过那个模型向前传递</em>我们所有的输入，我们得到我们的预测。使用这些预测，我们可以评估它们，计算我们的损失&amp;我们的损失告诉我们我们的网络在那次迭代中有多错误，它还告诉我们如何计算梯度，以及我们如何改变我们网络中的所有权重以在未来改进。</p><p id="592f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，采用这些梯度，实际上允许我们的优化器更新权重&amp;训练变量，这样在下一次迭代中，它们会做得更好一点。随着时间的推移，如果我们继续循环，这将收敛，希望应该适合你的数据。</p><p id="2f1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是神经网络中的反向传播和学习率。</p><p id="ccb6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jt">快乐学习。</em></p></div><div class="ab cl kf kg gp kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="hb hc hd he hf"><p id="d982" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你想了解神经网络中单个神经元内部发生了什么(本文的第一部分)，请点击这里。T9】</p><p id="bcc3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你想了解梯度下降是如何工作的，以及我们如何利用神经元构建神经网络(本文的第二部分)，请点击这里。T13】</p></div></div>    
</body>
</html>