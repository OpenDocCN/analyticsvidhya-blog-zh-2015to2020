# 集成学习——装袋和随机森林(下)

> 原文：<https://medium.com/analytics-vidhya/ensemble-learning-bagging-random-forest-part-2-6cc81eb3470d?source=collection_archive---------15----------------------->

![](img/e5f0911fa004f6fa6498e8e1794a7993.png)

由罗曼·贝斯

*这是一系列以简单易懂的方式解释系综方法的帖子。在这篇文章中，我们将讨论同构系综打包。*

1/ [异质合奏](https://irenepham-45233.medium.com/ensemble-learning-your-machine-learning-savoir-and-here-is-why-part-1-78ef52c8c365)

2/同质套装—装袋

3/同质系综—增强

## 均匀系综

当异构集成利用不同算法的优点时，同构集成只使用一种类型的算法。这种方法的两个主要技术是装袋和增压。

## 装袋(助推器聚集)

Bagging，即 bootstrap aggregation，是一种机器学习方法，它通过组合在数据集的 bootstrap 子集上训练的几个分类器来进行预测。

## 引导程序

首先，我们使用 bootstrap 采样从我们的训练数据中创建一些数据子集。这里强调的是，这些数据子集是随机抽样的产品*并带有替换。*这与我们的模型有什么关系？答案是，它确保了随机性和独立性对我们模型的影响。由于随机性允许算法试图避免过度拟合，并在新的观察值上更好地概括，bootstrap 有效地帮助对抗高方差，这是决策树等算法的缺点。使用 Bagging 技术的算法的一个例子是众所周知的使用决策树作为基本分类器的随机森林。

## 聚合

在创建了在自举数据子集上训练的多个分类器之后，然后使用投票或平均方法聚集输出以实现最终预测。由于这一特性，我们也可以说 Bagging 是投票集成方法的姐妹，而 Bagging 的强大的 bootstrap 技术替代了投票中微调分类器的强度。

## 量词的特征

Bagging 的另一个值得一提的特点是学习器的数量应该很大，通常在 100 到 500 个分类器之间，同时，每个分类器需要达到 50%以上的准确率。Bagging 将孔多塞陪审团定理应用于多数决定的概率分析，该定理指出:

> “如果 p 是*大于*(每个投票者更有可能正确投票)，那么增加*更多的投票者增加了多数决定是正确的概率*。极限中，随着投票人数的增加，主要票数正确趋近于 1 的概率”(维基百科，2020)。

基于这个理论，只要我们的 Bagging 模型满足那些条件，最终的预测准确率可能高达 100%。

当我们需要对抗方差，创建可以并行运行的更稳定和健壮的模型时，Bagging 无疑是一个星形分类器。唯一的限制是计算量大，因为它需要大量的估计器。出于这个原因，我们需要记住，我们的估计器集合应该是轻量级的、快速的，并且能够提供比随机更好的预测(超过 50%)。

## 随机森林

让我们看看 Random Forest 如何采用 Bagging 技术来升级其基于树的模型。

步骤 1:在数据集的不同自举子集上训练一些树。

第二步:让那些学习者做预测。

第 3 步:使用 Mode(用于分类)或 Mean(用于回归)来聚合预测并返回最终结果。

尽管决策树非常直观，易于使用和解释结果，但它最大的局限性之一是不稳定性。如果我们的模型达到了一个高的精确度，就不能保证我们可以在部署中有这样的度量。这可能是任何其他建模技术的普遍问题，但对决策树来说更是如此。由于决策树基于我们的特定数据集(真实世界数据人口的子集)选择它的分裂点，所以不能确定分裂点将在真实世界数据上工作。

随机森林通过在训练数据的不同随机样本上运行许多树来解决这个问题。使用 bootstrap 采样是 Random Forest 所说的方式:让我们基于我们的数据集模拟一个更大的数据子集，这将更能代表真实世界的数据。通过在*更大的*数据子集上训练许多树，我们可以有更多不同的分裂点，因此，可以决定更多的观点，并提出一个具有更好概括能力的模型。

除此之外，随机森林还在每个节点采样的随机特征子集中搜索最佳特征。这一有趣的特性可用于高维数据集的特征选择，以快速得出对我们的模型更重要的特征。

使用 Scikit-learn 可以很容易地调用这个强大的技术，您可以根据自己的喜好查看下面的代码。

总的来说，Bagging Ensemble 是一种强大的机器学习建模技术。通过实施 bootstrap，方差减少，而偏倚水平不一定由于聚集基本模型的效应而增加。除此之外，我们的模型还具有较高的稳定性和健壮性，这些都是成功部署模型不可或缺的特征。即使如果估计器的数量非常高，Bagging 在计算上可能非常昂贵，但为了创建更好的机器学习模型，这似乎是一个公平的牺牲。

感谢阅读！

不要害羞，如果你对我的更多帖子感兴趣，让我们联系一下:

中:【https://medium.com/@irenepham_45233】T2

insta gram:[https://www.instagram.com/funwithmachinelearning/](https://www.instagram.com/funwithmachinelearning/)