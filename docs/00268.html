<html>
<head>
<title>Understanding the Search Query — Part II</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解搜索查询—第二部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-the-search-query-part-ii-44d18892283f?source=collection_archive---------0-----------------------#2019-02-11">https://medium.com/analytics-vidhya/understanding-the-search-query-part-ii-44d18892283f?source=collection_archive---------0-----------------------#2019-02-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="fb87" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">机器学习</h2></div><blockquote class="ix iy iz"><p id="d5de" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">“人工智能比我们的智能小吗？”—斯派克·琼斯</p></blockquote><p id="00b8" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">现在，在这一部分，我们将从云基础设施的角度讨论ML模型和部署。</p><h1 id="bdad" class="ka kb hi bd kc kd ke kf kg kh ki kj kk io kl ip km ir kn is ko iu kp iv kq kr bi translated">ML模型，我们称之为【鳄鱼模型】:)，</h1><p id="32f1" class="pw-post-body-paragraph ja jb hi jd b je ks ij jg jh kt im jj jx ku jm jn jy kv jq jr jz kw ju jv jw hb bi translated">由人工神经网络组成，特别是时间序列NN，即基于<a class="ae kx" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank"> LSTM(长期短期记忆)</a>和CRF(卷积随机场)作为概率模型的递归神经网络。参考—<a class="ae kx" href="https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html" rel="noopener ugc nofollow" target="_blank">https://guillaumegenthial . github . io/sequence-tagging-with-tensor flow . html</a></p><p id="8ac9" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">大体上，它可以分为三个部分:</p><p id="5439" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jd hj"> 1。单词表示</strong>:首先，我们需要以特征向量的形式表示搜索查询中的单词，为此我们使用了预先训练的单词嵌入模型，该模型是作为斯坦福大学的开源项目开发的，名为<a class="ae kx" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> GloVe </a>。它提供了嵌入单词的300维向量。我们将把ElMo嵌入和这个连接在一起。</p><p id="da1f" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jd hj"> 2。上下文单词表示:</strong>为了找到查询中单词之间的上下文信息，它通过一个双向LSTM，然后是一个密集层，产生与标签数量相等的维数的输出。</p><p id="7d10" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jd hj"> 3。解码</strong>:现在，我们有了每个单词的向量表示，我们应用CRF来寻找标签的最佳可能组合。</p><p id="aedd" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">现有技术模型有三种变体，如下所示:</p><p id="384c" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jd hj"> 1。same通用报告格式:</strong>由与上述相同的三部分组成。</p><p id="0556" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jd hj"> 2。CharLSTM-LSTM-CRF: </strong>我们使用LSTM在字符级别创建了一个单词嵌入，因为可能有一些单词(如品牌)可能不在预训练单词嵌入模型(如GloVe)的字典中。</p><figure class="kz la lb lc fd ld er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ky"><img src="../Images/2fbe517231905c500ec78e3995e217f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JiEJgtWyxVgZGWirDaEEAw.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">模型网络架构</figcaption></figure><p id="b837" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jd hj"> 3。CharConv-LSTM-CRF </strong>:与上述模型相同，唯一不同的是用于生成字符级嵌入的神经网络，这里是CNN而不是LSTM。</p><blockquote class="lo"><p id="e361" class="lp lq hi bd lr ls lt lu lv lw lx jw dx translated">4.具有Elmo嵌入的查理斯姆-LSTM-CRF:</p></blockquote><figure class="lz ma mb mc md ld er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ly"><img src="../Images/0c6b200536b0ee8adf4e300269062643.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kKKWGO9LPSZCYl3kmnk0lQ.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">模型2的扩展</figcaption></figure><p id="2657" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">我们使用第二个模型CharLSTM-LSTM-CRF作为我们的用例，因为它比任何其他模型都表现得更好。我们也在努力实现模型4。</p><h1 id="4317" class="ka kb hi bd kc kd ke kf kg kh ki kj kk io kl ip km ir kn is ko iu kp iv kq kr bi translated"><strong class="ak">tensor flow中Model-2【鳄鱼模型】的实现:</strong></h1><p id="b2bf" class="pw-post-body-paragraph ja jb hi jd b je ks ij jg jh kt im jj jx ku jm jn jy kv jq jr jz kw ju jv jw hb bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> T </span> <a class="ae kx" href="https://www.tensorflow.org/api_docs/python/tf/data" rel="noopener ugc nofollow" target="_blank"> <strong class="jd hj">传感器流量tf。数据API </strong> </a> <strong class="jd hj"> </strong>是一个很好的选择，当使用高级API类估算器时，可以将数据输入到您的模型中。它引入了tf.data.Dataset类，该类创建一个输入管道来读取数据。它有一个从生成器生成元素的方法<code class="du mn mo mp mq b">from_generator</code>。您也可以使用<code class="du mn mo mp mq b">map</code>方法进行特征工程。</p><pre class="kz la lb lc fd mr mq ms mt aw mu bi"><span id="0bd2" class="mv kb hi mq b fi mw mx l my mz">#Accepting a method which yields the generator after reading each line from csv file<br/>dataset = tf.data.Dataset.from_generator( functools.partial(generator_fn, words, tags), output_shapes=shapes, output_types=types)</span></pre><p id="b642" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">得到数据集后，我们可以做更多的事情，比如:</p><pre class="kz la lb lc fd mr mq ms mt aw mu bi"><span id="44dc" class="mv kb hi mq b fi mw mx l my mz"># it will shuffle the the dataset first sampled from 100 elements, and then repeat the same dataset for 5 times- which can be used for iterating through number of epochs<br/>dataset = dataset.shuffle(100).repeat(5)</span><span id="1cc0" class="mv kb hi mq b fi na mx l my mz">#creates the batch which is padded in the defaults shape and buffer 2500 records for the next iteration<br/>dataset = (dataset.padded_batch(2500, shapes, defaults).prefetch(2500))</span></pre><p id="08fe" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">另一个扩展tf.data.Dataset的类名为tf.data.TextLineDataset，它以csv文件名作为参数并为您读取。当您使用基于文件的数据集时，这个API将为您做大量的内存管理工作。例如，您可以读入比内存大得多的数据集文件，或者通过指定一个列表作为参数读入多个文件。</p><p id="54a9" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di">T</span><a class="ae kx" href="https://www.tensorflow.org/guide/estimators" rel="noopener ugc nofollow" target="_blank"><strong class="jd hj">ensor flow Estimator API</strong></a><strong class="jd hj"/>使用自定义估值器，Tensorflow tf.data已用于编写所有训练、建模和评估的代码。Tensorflow中的自定义估算器有<strong class="jd hj"> tf.estimator.Estimator </strong>类，它包装了由<code class="du mn mo mp mq b">model_fn</code>和<strong class="jd hj">TF . Estimator . train _ and _ evaluate</strong>实用函数指定的模型，该实用函数通过使用给定的<strong class="jd hj">估算器来训练、评估和(可选地)导出模型。</strong>该型号的<code class="du mn mo mp mq b">model_fn</code>如下:</p><ol class=""><li id="e1d5" class="nb nc hi jd b je jf jh ji jx nd jy ne jz nf jw ng nh ni nj bi translated">对于单词表示:<strong class="jd hj">首先，生成字符嵌入:</strong></li></ol><pre class="kz la lb lc fd mr mq ms mt aw mu bi"><span id="224d" class="mv kb hi mq b fi mw mx l my mz">#For each sentence and words, we have a list of characters<br/>#We find the index of character present in the dictionary of all characters<br/>char_ids = vocab_chars.lookup(chars) #[sentence, words, chars] </span><span id="c0ea" class="mv kb hi mq b fi na mx l my mz">#Initialize a variable [total_number_of_chars, dimension of char_embedding=100] storing the initial embedding of all characters with some random floating point numbers<br/>variable = tf.get_variable(<br/> ‘chars_embeddings’, [num_chars, params[‘dim_chars’]], tf.float32)</span><span id="d86d" class="mv kb hi mq b fi na mx l my mz">#Lookup the embeddings of the chars in char_ids <br/>char_embeddings = tf.nn.embedding_lookup(variable, char_ids, validate_indices=False)  #[sentence, word, chars, char_dim=100]</span><span id="a50d" class="mv kb hi mq b fi na mx l my mz">#Adding a dropout in the layer<br/>char_embeddings = tf.layers.dropout(char_embeddings, rate=dropout,<br/> training=training) </span><span id="05ae" class="mv kb hi mq b fi na mx l my mz">#[max_length of sentences in batch]<br/>dim_words = tf.shape(char_embeddings)[1]</span><span id="7a08" class="mv kb hi mq b fi na mx l my mz">#[max_length of words in all the sentences]<br/>dim_chars = tf.shape(char_embeddings)[2]</span><span id="a43e" class="mv kb hi mq b fi na mx l my mz">flat = tf.reshape(char_embeddings, [-1, dim_chars, params['dim_chars']])  #[sentence*max_words_in_sentence ,max_chars_in_all_words, char_dim=100]</span><span id="f9fe" class="mv kb hi mq b fi na mx l my mz">#making time major from batch major as required by tf.contrib.rnnt = tf.transpose(flat, perm=[1, 0, 2])</span><span id="8f48" class="mv kb hi mq b fi na mx l my mz">#Initializing LSTM each having 25 units<br/>lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(25)<br/>lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(25)</span><span id="01c8" class="mv kb hi mq b fi na mx l my mz">#Creating backward dir LSTM<br/>lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)</span><span id="4941" class="mv kb hi mq b fi na mx l my mz">#output having dim [max_chars_in_all_words, sentence*max_words_in_sentence, char_embd_size=25]</span><span id="d676" class="mv kb hi mq b fi na mx l my mz">#Here time_steps i.e.[sequence_length] = number of chars in each words<br/>_, (_, output_fw) = lstm_cell_fw(t, dtype=tf.float32, sequence_length=tf.reshape(nchars, [-1]))</span><span id="a070" class="mv kb hi mq b fi na mx l my mz">#Reverse Bi-LSTM output<br/>_, (_, output_bw) = lstm_cell_bw(t, dtype=tf.float32, sequence_length=tf.reshape(nchars, [-1]))</span><span id="3a20" class="mv kb hi mq b fi na mx l my mz">output = tf.concat([output_fw, output_bw], axis=-1) # [max_chars_in_all_words, sentence*max_words_in_sentence, char_embd_size=25+25=50]</span><span id="9675" class="mv kb hi mq b fi na mx l my mz">#Reshape to [num_of_sentences, max_num_of_words, 50]<br/>char_embeddings = tf.reshape(output, [-1, dim_words, 50])</span></pre><p id="7937" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jd hj">现在，同样生成单词嵌入:</strong></p><pre class="kz la lb lc fd mr mq ms mt aw mu bi"><span id="aaac" class="mv kb hi mq b fi mw mx l my mz">#For each sentence, we have a list of words<br/>#We find the index of words present in the dictionary of all words<br/>word_ids = vocab_words.lookup(words) #[sentence, words]</span><span id="dc48" class="mv kb hi mq b fi na mx l my mz">#Getting the glove embeddings of all the words<br/>glove = np.load(params[‘glove’])[‘embeddings’]</span><span id="f5aa" class="mv kb hi mq b fi na mx l my mz">#Appending an extra embeddings to return if some word is not found<br/>variable = np.vstack([glove, [[0.] * params[‘dim’] ]])variable = tf.Variable(variable, dtype=tf.float32, trainable=False)</span><span id="fecd" class="mv kb hi mq b fi na mx l my mz">#Look up the word embeddings in the dictionary we created as non-trainable<br/>word_embeddings = tf.nn.embedding_lookup(variable, word_ids) #[sentence, word, glove_word_dim = 300]</span><span id="68c4" class="mv kb hi mq b fi na mx l my mz"># Concatenate Word and Char Embeddings<br/>embeddings = tf.concat([word_embeddings, char_embeddings], axis=-1)<br/>#[sentence, word, 300+50=350]</span></pre><p id="c94c" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jd hj"> 2。上下文单词表示法</strong></p><pre class="kz la lb lc fd mr mq ms mt aw mu bi"><span id="a8a0" class="mv kb hi mq b fi mw mx l my mz">#Time major, input shape= [sentences, words, 350]<br/>t = tf.transpose(embeddings, perm=[1, 0, 2]) </span><span id="7bd1" class="mv kb hi mq b fi na mx l my mz">#Forward and Backward lstm each of 100 units<br/>lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(100)<br/>lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(100)<br/>lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)</span><span id="55ab" class="mv kb hi mq b fi na mx l my mz"># time steps i.e. [sequence_length] having number of words in each sentence<br/>output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=nwords) #[sentence, words, 100]<br/>output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=nwords) #[sentence, words, 100]</span><span id="cb3e" class="mv kb hi mq b fi na mx l my mz"># Concatenate the both forward and backword encoding <br/>output = tf.concat([output_fw, output_bw], axis=-1) #[sentence, words, 100+100=200]<br/>output = tf.transpose(output, perm=[1, 0, 2]) <br/>#transponse to original shape</span><span id="1262" class="mv kb hi mq b fi na mx l my mz">#Create a dense layer to reduce the output to num of tags<br/>logits = tf.layers.dense(output, num_tags) # [sentence, word, num_of_tag=6]</span></pre><p id="710e" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jd hj"> 3。使用CRF解码:</strong></p><pre class="kz la lb lc fd mr mq ms mt aw mu bi"><span id="f2cb" class="mv kb hi mq b fi mw mx l my mz">#Create a variable and initialize as a transition score from one tags to another tags in determining the score of a particular combination of tags<br/>crf_params = tf.get_variable(“crf”, [num_tags, num_tags], dtype=tf.float32)</span><span id="8b82" class="mv kb hi mq b fi na mx l my mz"># determining the tags for each sentence # [sentence, no_of_tags]<br/>pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, [length_of_tags])</span></pre><p id="480e" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">计算损耗并优化损耗:</p><pre class="kz la lb lc fd mr mq ms mt aw mu bi"><span id="4b74" class="mv kb hi mq b fi mw mx l my mz">#Using Log likelihood as the loss function<br/>log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(logits, correct_tags, [length_of_tags], crf_params)<br/>loss = tf.reduce_mean(-log_likelihood)<br/></span><span id="8b5f" class="mv kb hi mq b fi na mx l my mz">#Using adam optimizer to minimize the loss<br/>if mode == tf.estimator.ModeKeys.TRAIN:<br/>     train_op = tf.train.AdamOptimizer().minimize(loss, global_step=tf.train.get_or_create_global_step())</span></pre><h1 id="31f4" class="ka kb hi bd kc kd ke kf kg kh ki kj kk io kl ip km ir kn is ko iu kp iv kq kr bi translated">TensorFlow服务:</h1><p id="07f8" class="pw-post-body-paragraph ja jb hi jd b je ks ij jg jh kt im jj jx ku jm jn jy kv jq jr jz kw ju jv jw hb bi translated">上述估计器模型可以保存在dir中，该dir使用方法<code class="du mn mo mp mq b">export_saved_model </code>将推理图作为<code class="du mn mo mp mq b">SavedModel</code>导出到给定的dir中，该方法可以用于称为<a class="ae kx" href="https://www.tensorflow.org/serving" rel="noopener ugc nofollow" target="_blank"> Tensorflow服务的生态系统。</a></p><pre class="kz la lb lc fd mr mq ms mt aw mu bi"><span id="d285" class="mv kb hi mq b fi mw mx l my mz">estimator.export_savedmodel(‘/content/model’, serving_input_receiver_fn)</span></pre><p id="63bf" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">上述方法采用保存模型参数的目录路径和类型为<a class="ae kx" href="https://www.tensorflow.org/api_docs/python/tf/estimator/export/ServingInputReceiver" rel="noopener ugc nofollow" target="_blank"> ServingInputReceiver </a>的input_fn，其中输入特征作为字典传递。</p><p id="ca0d" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">在对几个场景中创建的数据集进行训练之后，以上述方式导出SavedModel。相同的模型被加载到T <a class="ae kx" href="https://www.tensorflow.org/serving/docker" rel="noopener ugc nofollow" target="_blank"> ensorflow服务API (docker-version) </a>中，该API公开REST API并返回对应于特定搜索查询的预测标签。</p><pre class="kz la lb lc fd mr mq ms mt aw mu bi"><span id="ca2b" class="mv kb hi mq b fi mw mx l my mz">docker run -p 8501:8501 \<br/>--mount type=bind,source=/path/to/my_model/,target=/models/my_model\<br/>--mount type=bind,source=/path/to/my/models.config,target=/models/models.config \<br/>-t tensorflow/serving --model_config_file=/models/models.config</span></pre><p id="22ed" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">TF-Serving模型对查询“无坚果巧克力”的响应如下所示:</p><pre class="kz la lb lc fd mr mq ms mt aw mu bi"><span id="d46e" class="mv kb hi mq b fi mw mx l my mz">{<br/>    "outputs": [<br/>        [<br/>            "NV-S",<br/>            "PR-S",<br/>            "BQ-S"<br/>        ]<br/>    ]<br/>}</span></pre><p id="73ab" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">标签“NV-S”:表示营养标签，“PR-S”:表示介词标签，“BQ-S”:表示基本查询。</p><p id="a3ca" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">关于部署部分的讨论将在<a class="ae kx" rel="noopener" href="/@sonusharma.mnnit/understanding-the-search-query-part-iii-a0c5637a639">第三部分继续。</a></p></div></div>    
</body>
</html>