<html>
<head>
<title>Reinforcement Learning and Game Theory- An Intuitive Understanding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习和博弈论——一种直观的理解</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-and-game-theory-28208ee795f8?source=collection_archive---------5-----------------------#2019-10-16">https://medium.com/analytics-vidhya/reinforcement-learning-and-game-theory-28208ee795f8?source=collection_archive---------5-----------------------#2019-10-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="33a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">近年来，机器学习和深度学习技术在语音处理、预测、计算机视觉、机器翻译、预测、机器人等不同领域都有着令人瞩目的表现..各种机器学习概念的本质如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/889a3c7f240dfe52aa0d20cd41344101.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qN0tmUihoaP7beftvHLhMw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">机器学习概念一瞥(<a class="ae jt" href="https://www.analyticsvidhya.com/blog/2016/12/artificial-intelligence-demystified/" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><h1 id="1ae9" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">1.强化学习</h1><p id="d4e0" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated"><em class="kx">是什么让RL独一无二？</em>强化学习通过利用定义的参数，帮助机器学习决定等同于或在某些情况下远远超出人类理解的行动。</p><p id="d141" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">强化学习(RL)是一种面向目标的学习，在这种学习中，一个主体在一个环境中被训练，通过选择一个最佳的可能动作来达到一个目标。对于每一个行动，可以定义积极或消极的奖励。这些奖励作为注释，我们的模型可以从错误中学习，并在每次迭代中继续改进。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ky"><img src="../Images/f413dc2b5005f79833f72ba2f4cf0bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*au4Ejqos7R6V9upB_Bnj4w.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><a class="ae jt" href="https://perfectial.com/blog/reinforcement-learning-applications/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h2 id="59ce" class="kz jv hi bd jw la lb lc ka ld le lf ke iq lg lh ki iu li lj km iy lk ll kq lm bi translated">1.1平台</h2><p id="cfa4" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">这些平台用于模拟RL环境，RL环境通常是用于训练代理的有限且固定的空间。基于需求，代理在环境中的范围可以是有限的或无限的。但是，环境通常被建模为一个随机的有限机器空间，其中的代理人观察并采取行动，以最大化的回报。</p><p id="cecc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">常用的开源RL平台对比如下。有关详细信息，请查看<a class="ae jt" href="https://www.analyticsvidhya.com/blog/2016/12/getting-ready-for-ai-based-gaming-agents-overview-of-open-source-reinforcement-learning-platforms/" rel="noopener ugc nofollow" target="_blank">开源RL平台概述</a>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ln"><img src="../Images/20d42075a3b059fdfe97e5fca44d6c57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SSHgpnoWPk9VO67qe_XZbw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">训练RL模型的各种平台的比较(<a class="ae jt" href="https://www.analyticsvidhya.com/blog/2016/12/getting-ready-for-ai-based-gaming-agents-overview-of-open-source-reinforcement-learning-platforms/" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><h2 id="7540" class="kz jv hi bd jw la lb lc ka ld le lf ke iq lg lh ki iu li lj km iy lk ll kq lm bi translated">1.2参数</h2><p id="5950" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">用于训练RL模型的必要参数如下:</p><p id="b6a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理:单个或多个代理<br/>状态:S <br/>动作:A(S)，A <br/>模型:T(s，A，s') ~ Pr(s'|s，a) <br/>奖励:r(s)，R(s，A)，R(s，A ')<br/>策略:π(s) = a，π*(s)</p><h2 id="6f24" class="kz jv hi bd jw la lb lc ka ld le lf ke iq lg lh ki iu li lj km iy lk ll kq lm bi translated">1.3 RL-API</h2><p id="54ac" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">RL中一些常用的接口如下:</p><p id="47c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">规划器:模型(T，R) —策略<br/>学习器:转换(s，a，R，s’)—策略<br/>建模器:转换—模型<br/>模拟器:模型—转换<br/>基于RL的规划器:<br/>模型—模拟器—学习器—策略<br/>基于模型的规划器:<br/>转换—建模器—规划器—策略</p><h1 id="c1cc" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">2.强化学习算法</h1><h2 id="1aa3" class="kz jv hi bd jw la lb lc ka ld le lf ke iq lg lh ki iu li lj km iy lk ll kq lm bi translated">2.1马尔可夫决策过程</h2><p id="8840" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">MDP是一种决策算法，为有限环境空间中的机器提供最佳可能的动作状态，以使其报酬最大化。MDP通常只考虑现状。如果我们需要考虑过去的行为来采取进一步的行动，我们必须将这些信息与当前状态结合起来。</p><p id="c82b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了求解MDP，我们考虑以下两个假设:<br/> ●静止世界中的无限视界(注:视界或时间有限的情况也许是可能的。但是，对于本文，我们只考虑了无限)<br/> ●序列的效用，即偏好状态(有助于选择最佳策略)</p><p id="2574" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.1.1价值函数</strong></p><p id="2001" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">效用(价值)可以用贝尔曼方程来计算。其中，𝛾是将无限时间戳序列更改为有限值的<br/>折扣因子。<br/>值迭代可以如下进行:<br/> ●从任意效用开始<br/> ●使用<br/>基于邻居更新效用●重复直到收敛</p><p id="292f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.1.2策略迭代</strong></p><p id="ca0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">策略迭代的伪代码如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lo"><img src="../Images/83c0084dde726cbabee161e9b1bf6625.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gbR2kuX2RCNJCuy9L0mPTQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><a class="ae jt" rel="noopener" href="/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa">来源</a></figcaption></figure><p id="c69c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.1.3 MDP方法</strong></p><p id="e435" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">三种常用的MDP方法是:<br/> ●政策搜索:国家-政策-行动<br/> ●基于价值:国家-效用-价值<br/> ●基于模型:(s，a)-(过渡，奖励)-(s '，r)</p><p id="b8de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这三种方法可以联系如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lp"><img src="../Images/18b7662200cf41cf008f36125af5b777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*fIDIVFNpln_RzcmHbYxvug.png"/></div></figure><h2 id="aa1a" class="kz jv hi bd jw la lb lc ka ld le lf ke iq lg lh ki iu li lj km iy lk ll kq lm bi translated">2.2 Q-学习</h2><p id="2bfd" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">Q-Learning是动作-值函数，它被引入来进一步优化代理选择并计算最优效用和最优策略，而不需要学习转移概率或奖励函数。它可以被定义为到达状态的代理人的价值，同时采取行动(a)并在其后以最佳方式进行。</p><p id="5f9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Q学习的伪码如下(其中，α-学习率):</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lq"><img src="../Images/b50eeb9834cae18c26259310e9ae923e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*gZAdJp1jjkIJgL4l3Ux5wg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><a class="ae jt" href="http://incompleteideas.net/book/first/ebook/node65.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="ad06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">2 . 2 . 1 ɛ-greedy勘探</strong></p><p id="9617" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ɛ-greedy探索是一种随机选择的方式。它使用贪婪极限无限探索(GLIE)来随时间衰减ɛ。这可以被认为是RL中减少次优遗憾的基本权衡。</p><p id="2955" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，随着时间的推移，最佳Q(学习-探索)和π(使用-开发)将被计算出来。像老虎机这样的探索开发问题是理解ɛ-greedy探索的最好例子。</p><h2 id="c9b9" class="kz jv hi bd jw la lb lc ka ld le lf ke iq lg lh ki iu li lj km iy lk ll kq lm bi translated">2.3.博弈论</h2><p id="4cc9" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">博弈论通常被定义为冲突的数学，并被应用于各种领域，如经济学、心理学、人工智能、社会学等..在博弈论w.r.t RL中，策略就是战略，映射出所有可能的行动状态，与游戏中的一个参与者有关。</p><p id="b02f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">多智能体RL (MARL)中的博弈类型有:<br/> ●静态博弈:玩家独立且同时决策<br/> ●阶段博弈:规则取决于具体阶段<br/> ●重复博弈:当一个博弈按顺序进行时</p><p id="1577" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理解博弈论的基本博弈是“两人零和完全信息有限确定性博弈”。通过修改各种参数，如非确定性、隐藏信息、非零和，可以获得更深入的了解。安德鲁摩尔材料有广泛的游戏，提供了对各种博弈论概念的见解。</p><p id="2100" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">纳什均衡</strong></p><p id="4983" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑一个博弈中的n个玩家，策略分别为s1，s2，…sn。当且仅当所有参与人的策略对应于该参与人的最优策略时，这些策略被称为在NE中。ne的3个基本定理是:<br/> ●在n人纯策略博弈中，如果消除严格劣势<br/>策略，除了一个组合之外消除了所有组合，那么它是唯一的NE。任何NE都将在严格劣势策略的淘汰中幸存。<br/> ●若n是有限的，s(i)是有限的，则至少存在一个ne。</p><p id="5f11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.3.2国内流离失所者战略</strong></p><p id="1b16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">针锋相对和冷酷触发是处理<br/>重复囚徒困境(重复博弈)的一些著名策略。</p><p id="c414" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TFT中玩家的行动状态由以下策略决定:<br/> ●第一轮合作<br/> ●此后复制对手先前的移动</p><p id="f6ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个玩家在冷酷扳机中的行动状态如下:<br/> ●继续与其他玩家合作<br/> ●如果任何一个玩家叛逃一次，则继续叛逃直到永远</p><p id="6ed4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.3.3最小最大轮廓</strong></p><p id="4da6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">●最小最大值档案通常代表一对收益，每个<br/>玩家一个，代表玩家通过<br/>防御恶意随机对手可以获得的收益。所以，这基本上就像零和游戏一样，每个玩家都试图减少另一个玩家的T21奖励。<br/> ●最小最大值曲线用于纯策略来寻找收益，在<br/>混合策略的情况下，我们将使用安全级别收益。</p><p id="4c3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.3.4民俗定理</strong></p><p id="e9d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">●民间定理的总体思路是“在重复博弈中，<br/>报复的可能性打开了合作的大门”。<br/> ●博弈论中，指特定的结果。它可以被描述为一组<br/>收益，可以从重复博弈的纳什策略中得到。<br/> ●我们可以获得可行区域(平均。联合策略的收益)和<br/>可接受区域(优选—最小最大剖面)<br/> ●任何严格支配最小最大/安全<br/>水平的可行收益剖面都可以实现为纳什均衡收益，具有足够大的<br/>贴现因子。因为，如果它严格控制了最小最大值剖面，它可以<br/>用它作为威胁。</p><p id="37f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.3.5子游戏完美</strong></p><p id="e5a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">●如果一个参与者总是选择最佳<br/>反应，一个策略被称为子博弈完美，与历史无关。<br/> ●简单来说，子博弈完美避免了不可信的威胁，而<br/>随着时间的推移继续选择最佳行动状态。<br/> ● TFT和Grim trigger处于纳什均衡，但不是子博弈<br/>完美。</p><p id="7b0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.3.6巴甫洛夫策略</strong></p><p id="02d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">巴甫洛夫是另一种IDP策略，参与者同意就合作，不同意就有缺陷。巴甫洛夫同时满足纳什均衡和子博弈完美。计算民俗定理可以用来建立类似巴甫洛夫的机器。CFT还可以在多项式时间内建立任何博弈的子博弈完美纳什均衡。使用CFT的一些优势是:<br/> ●巴甫洛夫，如果可能的话<br/> ●零和游戏(2人游戏)<br/> ●至少一个玩家提高</p><h1 id="1a6e" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">3.致谢:</h1><p id="eb12" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">本文是对RL的一个基本概述。目前，我正在编写一个RL教程系列，它将让读者简要了解RL中的各种参数和技术，以及使用python的实际实现。这一系列教程是根据我对佐治亚理工学院的RL材料、国立研究大学高等经济学院的实用RL以及该领域专家的各种博客(我已经提到了相同内容的链接)的理解而创建的。</p></div></div>    
</body>
</html>