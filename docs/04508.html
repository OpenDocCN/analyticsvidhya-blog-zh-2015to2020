<html>
<head>
<title>Random Forest Fundamentals &amp; Beyond — ML for coders by Fast.ai (Lesson 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">《随机森林基础与超越——fast . ai为编码人员提供的ML 》(第1课)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/random-forests-fundamentals-beyond-ml-for-coders-by-fast-ai-lesson-1-5e9003811515?source=collection_archive---------13-----------------------#2020-03-22">https://medium.com/analytics-vidhya/random-forests-fundamentals-beyond-ml-for-coders-by-fast-ai-lesson-1-5e9003811515?source=collection_archive---------13-----------------------#2020-03-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d3cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">这篇文章深入讨论了支持Fast.ai提供的“程序员ML入门”课程第一课学习的细节</em></p><p id="bc2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">与传统的课程笔记不同，我的重点是用外部来源的信息丰富讨论的主题。要访问我在本系列中讨论课程背景的第一篇文章，并浏览所有课程，请单击此处的</em><a class="ae je" rel="noopener ugc nofollow" target="_blank" href="/@alexrobwong/ml-for-coders-beyond-572ae05448"><em class="jd"/></a><em class="jd">。</em></p><h1 id="c4de" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">摘要</h1><p id="af6f" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">在第一课中，Jeremy介绍了决策树和随机森林的高级概念。他还讨论了fast.ai的关键功能，以快速准备建模数据。</p><h1 id="9e71" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">目录</h1><ol class=""><li id="af70" class="ki kj hi ih b ii kd im ke iq kk iu kl iy km jc kn ko kp kq bi translated">什么是随机森林？</li><li id="5eb6" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">什么是决策树？</li><li id="4c9c" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">驱动决策树的数学</li><li id="e264" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">从决策树到随机森林(集成学习)</li><li id="4d72" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">随机森林是如何工作的？(伪代码)</li><li id="f895" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">随机森林利弊</li><li id="ed9b" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">从本课中学到的其他知识</li></ol><h1 id="6dea" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">感谢</h1><p id="3eab" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">特别感谢<a class="ae je" href="https://github.com/fastai/fastai" rel="noopener ugc nofollow" target="_blank">杰瑞米·霍华德</a>发布他的课程和源代码。还要特别感谢<a class="ae je" rel="noopener" href="/@hiromi_suenaga">上原广美·末永</a>出版了她详细的课程笔记。</p><p id="fd92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章是我综合了许多来源的信息。我已经尽力提供了原始资料的链接，并强调了我引用他人作品的地方。</p><h1 id="15ab" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">什么是随机森林？</h1><blockquote class="kw kx ky"><p id="3e71" class="if ig jd ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated"><strong class="ih hj">随机森林</strong>或<strong class="ih hj">随机决策森林</strong>是一种用于分类、回归和其他任务的集成学习方法，它通过在训练时构建大量决策树并输出类来运行，该类是各个树的类(分类)或均值预测(回归)的模式(<a class="ae je" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank">来源:维基百科</a></p></blockquote><p id="46a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">换句话说，随机森林是一种用于回归和分类的机器学习算法，由许多决策树组成。每个决策树是在原始数据的随机抽样部分上开发的，并且来自各个树的预测被聚合以创建来自森林的最终预测。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es lc"><img src="../Images/f9d57dc29277ea44c5600573e1763a37.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/0*o_a5dlJwvRpqyUl2.jpg"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">随机森林的简化说明(<a class="ae je" href="https://www.kdnuggets.com/2017/10/random-forests-explained.html" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="e2c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了理解随机森林是如何工作的，首先要理解什么是决策树。</p><p id="c48d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意，在本文中，假设读者熟悉将可用数据分成训练/验证/测试数据的概念和重要性。关于这个主题的更多信息，请查看机器学习大师的这篇伟大的<a class="ae je" href="https://machinelearningmastery.com/difference-test-validation-datasets/" rel="noopener ugc nofollow" target="_blank">文章。</a></p><h1 id="4208" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><strong class="ak">什么是决策树？</strong></h1><blockquote class="kw kx ky"><p id="f97b" class="if ig jd ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated">决策树是用于回归和分类问题的大量使用的非参数有效机器学习建模技术。为了找到解决方案，决策树根据预测数据对结果变量做出连续的、分层的决策(<a class="ae je" href="https://towardsdatascience.com/what-is-a-decision-tree-22975f00f3e1" rel="noopener" target="_blank">来源:走向数据科学—媒体</a>)。</p></blockquote><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lo"><img src="../Images/a2a42b75e4be903766e58eb8ef68d5b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*exaN79p5v5_smM0K"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">一个简单的天气是否适合打网球的决策树(<a class="ae je" href="https://towardsdatascience.com/decision-tree-in-machine-learning-e380942a4c96" rel="noopener" target="_blank">来源:亚达夫王子— Medium </a></figcaption></figure><h2 id="db15" class="lt jg hi bd jh lu lv lw jl lx ly lz jp iq ma mb jt iu mc md jx iy me mf kb mg bi translated">决策树是如何工作的？</h2><p id="7df0" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">决策树由节点和分支组成。在培训期间，采取以下步骤:</p><ol class=""><li id="1b16" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc kn ko kp kq bi translated">从整组观察值开始，决策树基于“<em class="jd">最优</em>”特征和值的组合，在其<strong class="ih hj">根节点</strong>将数据拆分成两个互斥的分支。</li><li id="0c60" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">从根节点延伸的每个分支进一步导向另外的<strong class="ih hj">决策节点</strong>，在那里基于每个决策节点可用的数据子集的“<em class="jd">最优</em>”特征和值组合进行进一步的分割。</li><li id="2cac" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">该过程以迭代的方式继续，直到到达<strong class="ih hj">叶节点</strong>并做出预测。当满足“<em class="jd">停止标准</em>”或节点中只剩下一个观察值时，出现叶节点。</li></ol><p id="66f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在根节点和每个决策节点，分割标准的类型取决于被分割的要素的类型:</p><ul class=""><li id="7607" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated">当分割的最佳特征是连续的时，分割标准是一个不等式，例如大于或小于最佳值。</li><li id="91f4" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">当分裂的最佳特征是类别时，分裂标准是类别成员的布尔值。请注意，当一个分类变量可以有两个以上的类别时，在适当的情况下会反复使用二元分割来进一步分割类别。(例如，对于包含太阳、雨和云的分类变量，节点可能是“太阳或不是太阳”，然后每个分支可能最终导致另一个分支“雨或不是雨”)</li></ul><p id="2273" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，术语“最佳”和“停止标准”将在本节稍后定义。有关决策树相关关键术语的摘要，请参见下图:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es ml"><img src="../Images/3b4140593a6792e64ff66a560518a108.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jJIqDIZwXzawH_XGHhETOQ.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来源:https://clearpredictions.com/Home/DecisionTree</figcaption></figure><h2 id="e3c7" class="lt jg hi bd jh lu lv lw jl lx ly lz jp iq ma mb jt iu mc md jx iy me mf kb mg bi translated">为什么决策树是非参数化的？</h2><p id="78b5" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">决策树是非参数化的，因为它没有假设或要求数据中的基本特征(变量)是均匀分布的。<a class="ae je" href="https://www.investopedia.com/terms/n/nonparametric-statistics.asp" rel="noopener ugc nofollow" target="_blank">根据Investopedia </a>的定义:</p><ul class=""><li id="a592" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated">非参数统计是指不要求数据符合正态分布的统计方法。</li><li id="98cb" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">非参数统计使用通常是有序的数据，这意味着它不依赖于数字，而是依赖于排序或排序。</li><li id="bc59" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">非参数模型的模型结构不是事先指定的<em class="jd"/>，而是从数据中确定的。</li><li id="b35d" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">术语<em class="jd">非参数</em>并不意味着这种模型完全缺少参数，而是参数的数量和性质是灵活的，不是预先固定的。</li></ul><h1 id="b378" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">驱动决策树的数学</h1><p id="4201" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">有不同类型的决策树算法，每种算法使用不同的杂质测量来进行分裂。<strong class="ih hj">基尼指数和信息增益常用于分类树</strong>和<strong class="ih hj">标准差用于回归树。</strong></p><h2 id="ed13" class="lt jg hi bd jh lu lv lw jl lx ly lz jp iq ma mb jt iu mc md jx iy me mf kb mg bi translated"><strong class="ak">基尼指数(用于分类树)</strong></h2><p id="130d" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">以下是reg的基尼系数公式，其中Pi表示对象被分类到特定类别的概率。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mm"><img src="../Images/af2eec49cfa66711658c3a153f360855.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*KPoPB7MsUNlBcf-o.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">基尼指数公式</figcaption></figure><p id="edab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基尼指数是对随机选取的特定变量被错误分类的程度或概率的衡量:</p><ul class=""><li id="c28f" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated">基尼系数为0表示所有要素都属于某一类，或者只有一类</li><li id="a9e4" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">基尼指数为1表示这些要素在各个阶层中随机分布</li><li id="d5df" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">基尼指数为0.5表示某些阶层的要素分布均等</li></ul><p id="e0af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最佳分割是使基尼指数最小化的分割。一般来说，当类的数量很大时，Gini会有困难，并且倾向于产生具有纯度的大小相等的分区的测试。</p><p id="e18a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基尼指数用于经典的CART(分类和回归树)算法，该算法于1984年由<a class="ae je" href="https://en.wikipedia.org/wiki/Leo_Breiman" rel="noopener ugc nofollow" target="_blank">利奥·布雷曼</a>、<a class="ae je" href="http://www.kdd.org/node/362" rel="noopener ugc nofollow" target="_blank">杰罗姆·弗里德曼</a>、<a class="ae je" href="http://www-stat.stanford.edu/~olshen/" rel="noopener ugc nofollow" target="_blank">理查德·奥尔申</a>和<a class="ae je" href="http://vcresearch.berkeley.edu/charles-stone" rel="noopener ugc nofollow" target="_blank">查尔斯·斯通</a>提出。虽然它最初是由Corrado Gini在1912年为衡量经济不平等而开发的，但它也是使用决策树的sklearn实现的默认标准。</p><p id="aa9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">基尼指数示例</strong></p><p id="1e37" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面这个例子来自<a class="ae je" href="http://www.learnbymarketing.com/481/decision-tree-flavors-gini-info-gain/" rel="noopener ugc nofollow" target="_blank"> LearnByMarketing </a>。下表中有10个观察值，用<strong class="ih hj">变量1 </strong>和<strong class="ih hj">变量2 </strong>计算<strong class="ih hj">类:</strong></p><ul class=""><li id="4dbd" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated">对于分类变量1，考虑二进制分裂0，1</li><li id="45fe" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">对于数值Var2，您可以从不同的值到不同的值，并将分割检查为小于和大于或等于。</li></ul><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es mn"><img src="../Images/35d43bafdfcc224436ea8288dbaceb01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dhagLTxUt3YUhN5tZzG35g.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来源:<a class="ae je" href="http://www.learnbymarketing.com/481/decision-tree-flavors-gini-info-gain/" rel="noopener ugc nofollow" target="_blank"> LearnByMarketing </a></figcaption></figure><p id="8640" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">拆分考虑#1: Var1 == 1 </em> </strong></p><p id="2760" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分割的基线:Var1有4个实例(4/10)等于1，有6个实例(6/10)等于0。</p><ul class=""><li id="63e3" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated">对于Var1 == 1 &amp; Class == A: 1 / 4实例的类等于A。</li><li id="e04d" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">对于Var1 == 1 &amp; Class == B: 3 / 4的实例具有等于B的类。</li></ul><p id="acea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→ <em class="jd">这里的基尼指数是1-((1/4) + (3/4) ) = 0.375 </em></p><ul class=""><li id="e593" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated">对于Var1 == 0 &amp; Class== A: 4 / 6实例的类等于A。</li><li id="3865" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">对于Var1 == 0 &amp; Class == B: 2 / 6实例的Class等于B。</li></ul><p id="33cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→ <em class="jd">这里的基尼指数是1-((4/6) + (2/6) ) = 0.4444 </em></p><p id="d2d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们根据每个拆分所占数据的基线/比例，对每个拆分进行加权和求和。</p><p id="9272" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→<em class="jd">4/10 * 0.375+6/10 * 0.444 =</em><strong class="ih hj"><em class="jd">0.41667</em></strong></p><p id="c8cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">拆分考虑#2: Var2 ≥ 32 </em> </strong></p><p id="cbb8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分割的基线:Var2有8个实例(8/10)，其中它等于或大于32，当它小于32时有2个实例(2/10)。</p><ul class=""><li id="3929" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated">对于变量2 &gt;= 32 &amp; Class == A: 5 / 8的实例的类等于A。</li><li id="f78f" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">对于变量2 &gt;= 32 &amp; Class == B: 3 / 8的实例的类等于B。</li></ul><p id="ca8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→这里的基尼指数是1-((5/8) + (3/8) ) = 0.46875</p><ul class=""><li id="58dd" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated">对于变量2 &lt; 32 &amp; Class == A: 0 / 2 instances have class equal to A.</li><li id="41b9" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">For Var2 &lt; 32 &amp; Class == B: 2 / 2 instances have class equal to B.</li></ul><p id="7e30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→ Gini Index here is 1-((0/2)² + (2/2)²) = 0</p><p id="5eb5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">We then weight and sum each of the splits based on the baseline / proportion of the data each split takes up.</p><p id="dc50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→ 8/10 * 0.46875 + 2/10 * 0 = <strong class="ih hj"> 0.375 </strong></p><p id="2bd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于分割考虑#1和#2之间的结果，您将选择Var2&gt;=32作为分割，因为其加权基尼指数最小。下一步是从分割和进一步划分中获取结果。</p><h2 id="7556" class="lt jg hi bd jh lu lv lw jl lx ly lz jp iq ma mb jt iu mc md jx iy me mf kb mg bi translated"><strong class="ak">信息增益(针对分类树)</strong></h2><p id="adda" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">以下是使用熵作为计算一部分的信息增益公式:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es mo"><img src="../Images/a7c2e13fcee5c61bdcca4d1e59a1b3dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sEkAiQyuBhuCkVVIoI6dsg.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来源:<a class="ae je" rel="noopener" href="/datadriveninvestor/decision-tree-and-random-forest-e174686dd9eb">数据驱动投资者</a></figcaption></figure><p id="e94f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">信息增益是起始节点和它的两个子节点的加权杂质之间的不确定性(熵)的差异。最佳分裂是导致给定节点的总熵最大程度降低的分裂。请参见下面的信息论和熵的正式定义:</p><blockquote class="kw kx ky"><p id="70a8" class="if ig jd ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated"><strong class="ih hj">信息熵</strong>，通常就是<strong class="ih hj">熵</strong>，是<a class="ae je" href="https://en.wikipedia.org/wiki/Information_theory" rel="noopener ugc nofollow" target="_blank">信息论</a>中与任何<a class="ae je" href="https://en.wikipedia.org/wiki/Random_variable" rel="noopener ugc nofollow" target="_blank">随机变量</a>相关的一个基本量，可以解释为变量可能结果中固有的“信息”、“惊喜”或“不确定性”的平均水平。信息熵的概念是由Claude Shannon在他1948年的论文“<a class="ae je" href="https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication" rel="noopener ugc nofollow" target="_blank">A Mathematical Theory of Communication</a>”中引入的……熵也可以被解释为一个<a class="ae je" href="https://en.wikipedia.org/wiki/Stochastic" rel="noopener ugc nofollow" target="_blank">随机</a>数据源产生<a class="ae je" href="https://en.wikipedia.org/wiki/Information" rel="noopener ugc nofollow" target="_blank">信息</a>的<a class="ae je" href="https://en.wikipedia.org/wiki/Expected_value" rel="noopener ugc nofollow" target="_blank">平均</a>速率。当数据源产生低概率值时(即，当低概率事件发生时)，事件比数据源产生高概率值时携带更多的“信息”。(来源:<a class="ae je" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="noopener ugc nofollow" target="_blank">维基百科</a>)</p></blockquote><p id="7fae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">信息增益用于<strong class="ih hj"> ID3 </strong> ( <strong class="ih hj">迭代二分法3 </strong> ) <a class="ae je" href="https://en.wikipedia.org/wiki/Algorithm" rel="noopener ugc nofollow" target="_blank">算法</a>由<a class="ae je" href="https://en.wikipedia.org/wiki/Ross_Quinlan" rel="noopener ugc nofollow" target="_blank">罗斯·昆兰</a>发明。</p><p id="5d3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不像基尼指数的范围在0和1之间，<a class="ae je" href="http://www.learnbymarketing.com/481/decision-tree-flavors-gini-info-gain/" rel="noopener ugc nofollow" target="_blank">熵的最大值</a>取决于类的数量。它以2为基数，所以如果你有…</p><ul class=""><li id="0acc" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated">两类:最大熵为1。</li><li id="1f2a" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">四类:最大熵为2。</li><li id="5536" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">八类:最大熵为3。</li></ul><p id="21de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">信息增益示例</strong></p><p id="ccf1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以与基尼指数相似的方式，让我们继续使用来自<a class="ae je" href="http://www.learnbymarketing.com/481/decision-tree-flavors-gini-info-gain/" rel="noopener ugc nofollow" target="_blank"> LearnByMarketing </a>的相同示例，并基于信息增益计算最佳分割(回想一下，原始数据在Var2 &gt; 32上被分割)。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es mp"><img src="../Images/a00419cc029923a4523cf439e0861dd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pEJN6jT4L5x4cRz3iQ76PQ.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来源:</figcaption></figure><p id="1bda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">拆分考虑#1: </em> </strong> Var2 &lt; 45.5</p><p id="7850" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分割基线:Var2有2个实例(2/8)，其中它的&lt; 45.5 and 6 instances (6/8) when it’s &gt; =45.5。</p><ul class=""><li id="9e8d" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated">对于Var2 &lt; 45.5 &amp; Class == A: 2 / 2 instances have class equal to A.</li><li id="6148" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">For Var2 &lt; 45.5 &amp; Class == B: 0 / 2 instances have class equal to B.</li></ul><p id="21cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→ Entropy here is -1 * ((2/2)*log(2/2, 2)) = 0</p><ul class=""><li id="2fde" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated">For Var2 &gt; = 45.5 &amp; Class == A: 3 / 6的实例的类等于A。</li><li id="4e35" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">对于Var2 &gt;= 45.5 &amp; Class == B: 3 / 6实例的Class等于B。</li></ul><p id="4a94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→熵这里是-1 *(3/6)* log(3/6，2) +(3/6)*log(3/6，2)) = 1</p><p id="d604" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们根据每个拆分所占数据的基线/比例，对每个拆分进行加权和求和。</p><p id="e99e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→ 2/8 * 0 + 6/8 * 1 = <strong class="ih hj"> 0.75 </strong></p><p id="0b51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">拆分考虑#2: </em> </strong> Var2 &lt; 65.5</p><p id="4d51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分割基线:Var2有7个实例(7/8)，其中它的&lt; 65.5 and 1 instance (1/8) when it’s &gt; =65.5。</p><ul class=""><li id="4ff1" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated">对于Var2 &lt; 65.5 &amp; Class == A: 5 / 7 instances have class equal to A.</li><li id="63d1" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">For Var2 &lt; 65.5 &amp; Class == B: 2 / 7 instances have class equal to B.</li></ul><p id="aeee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→ Entropy here is -1 * ((5/7)*log(5/7, 2) +(2/7)*log(2/7, 2)) = 0.8631</p><ul class=""><li id="49cc" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated">For Var2 &gt; = 65.5 &amp; Class == A: 0 / 1个实例的类等于A。</li><li id="e288" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">对于Var2 &gt;= 65.5 &amp; Class == B: 1 / 1实例的Class等于B。</li></ul><p id="9645" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→这里的熵是-1 *(1/1)* log(1/1，2)) = 0</p><p id="584f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们根据每个拆分所占数据的基线/比例，对每个拆分进行加权和求和。</p><p id="dd56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→7/8 * 0.8631+1/8 * 0 =<strong class="ih hj">0.7552</strong></p><p id="13bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于信息增益，我们将选择具有较低熵值的分割(因为它将最大化信息增益)。我们会选择Var2 &lt; 45.5 as the next split to use in the decision tree.</p><h2 id="8e0d" class="lt jg hi bd jh lu lv lw jl lx ly lz jp iq ma mb jt iu mc md jx iy me mf kb mg bi translated"><strong class="ak">标准差(对于回归树)</strong></h2><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mq"><img src="../Images/b6d16c3d8369371c442f6022e5b1229e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*79BhscMoGrOkTIzT8C4zOA.png"/></div></figure><p id="5e33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当响应变量是分类变量时，可以使用基尼系数和信息增益，而当响应变量是连续变量时，则使用标准差作为杂质标准。</p><p id="b18b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要确定分割的最佳变量(和值)，首先要确定每个潜在分支的标准差。一旦计算完所有分支，就可以确定加权平均标准偏差(基于观察次数)。具有最低加权标准偏差的变量(和值)被选择作为给定节点的分割。</p><p id="1913" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">标准差示例</strong></p><p id="3b4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面的例子来自<a class="ae je" href="https://sefiks.com/" rel="noopener ugc nofollow" target="_blank"> Sefik Ilkin Serengil </a>。在下表中，有14个观测值(天)和四个天气特征可用于预测高尔夫球员的数量。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es mr"><img src="../Images/9fd18d90062e72c33a604f2d4484f112.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ObjW2l9TMGvqRXfz944dA.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来源:(<a class="ae je" href="https://sefiks.com/2018/08/28/a-step-by-step-regression-decision-tree-example/" rel="noopener ugc nofollow" target="_blank"> Sefiks </a>)</figcaption></figure><p id="b004" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→首先，计算目标变量的标准偏差:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es ms"><img src="../Images/dd420b6787b41fc3e1b8d488d813e7da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OA6ILr3f5w4shdc0oz0QQA.png"/></div></div></figure><p id="6b55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→接下来，从第一个预测器<em class="jd">前景</em>开始，计算目标变量的标准偏差，一旦该预测器将目标变量分成其各自的分支</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es mt"><img src="../Images/1c50463023b6f506fc85637f3c5e5681.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qJBQEzFvIQiplp4idWZXUA.png"/></div></div></figure><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es mu"><img src="../Images/66a092774c4bc8db5d097a122eb193c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*InCJ3w1K5bnfUXs4LSMCTw.png"/></div></div></figure><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es mv"><img src="../Images/878ef9021f819d75a57793a945ed5ef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w5273uyEH9AHRq3V-f5oaw.png"/></div></div></figure><p id="d62a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">→计算所有分支后，确定该特征的加权标准偏差</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es mw"><img src="../Images/487870525d262ebe8a6f3ea3b69063b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xVcevicVmT_weYAPOhRWeA.png"/></div></div></figure><p id="ffb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据上述步骤，对其他预测要素(温度、湿度和风)重复上述步骤。对于该节点(本例中的根节点)，选择具有最低加权标准差并因此标准差减少最大的特征进行分割。</p><h1 id="ff80" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">从决策树到随机森林</h1><p id="f40a" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">回想一下文章开头提供的维基百科对随机森林的定义:</p><blockquote class="mx"><p id="a470" class="my mz hi bd na nb nc nd ne nf ng jc dx translated"><strong class="ak">随机森林</strong>或<strong class="ak">随机决策森林</strong>是一种用于分类、回归和其他任务的集成学习方法，通过在训练时构建大量决策树并输出类(即类的模式)或个体树的均值预测(回归)来进行操作</p></blockquote><h2 id="c69c" class="lt jg hi bd jh lu nh lw jl lx ni lz jp iq nj mb jt iu nk md jx iy nl mf kb mg bi translated">什么是集成学习方法？</h2><p id="485e" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">简而言之，集成学习方法将许多单个模型的输出结合起来，用于回归或分类，理想地导致模型性能的提高。</p><blockquote class="kw kx ky"><p id="8c7d" class="if ig jd ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated">在统计和机器学习中，<strong class="ih hj">集成方法</strong>使用多种学习算法来获得比单独使用任何组成学习算法都更好的预测性能(来源:<a class="ae je" href="https://en.wikipedia.org/wiki/Ensemble_learning" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></blockquote><p id="e189" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">集成学习方法有很多种。基本集成技术包括<strong class="ih hj">最大投票</strong>、<strong class="ih hj">平均</strong>、<strong class="ih hj">加权平均</strong>。更先进的成套技术包括<strong class="ih hj">堆垛</strong>、<strong class="ih hj">混合</strong>、<strong class="ih hj">装袋</strong>和<strong class="ih hj">增压</strong>。虽然每种集成技术的细节超出了本文的范围，但更多信息可以在Analytics Vidhya 的<a class="ae je" href="https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/" rel="noopener ugc nofollow" target="_blank">帖子中找到。</a></p><p id="d5ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如您将在下一节中看到的，描述随机森林如何工作，随机森林使用<strong class="ih hj">装袋，</strong>也称为<strong class="ih hj">引导聚集</strong>。</p><blockquote class="kw kx ky"><p id="1d91" class="if ig jd ih b ii ij ik il im in io ip kz ir is it la iv iw ix lb iz ja jb jc hb bi translated">Bootstrapping是一种采样技术，其中我们从原始数据集创建观察值子集，<strong class="ih hj">替换为</strong>。子集的大小与原始集合的大小相同(来源:<a class="ae je" href="https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/" rel="noopener ugc nofollow" target="_blank"> Analytics Vidhya </a>)</p></blockquote><h1 id="5b8c" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">随机森林是如何工作的？</h1><p id="833b" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">总之，随机森林模型是从预定数量的决策树创建的，这些决策树是在原始数据集的不同子集上训练的。每个子集都是通过随机采样并替换原始数据集来创建的，直到该子集包含与原始数据集相同数量的观察值。对于决策树的sklearn实现，仅使用随机选择的特征的子集在每个节点进行分割。一般的随机森林伪代码*过程如下:</p><ol class=""><li id="aa1c" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc kn ko kp kq bi translated">从总共“m”个特征中随机选择“K”个特征，其中K&lt; m</li><li id="35d2" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">Among the “K” features, calculate the node “d” using the best split point</li><li id="db50" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">Split the node into daughter nodes using the best split method</li><li id="43e4" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">Repeat the previous steps until “l” number of nodes has been reached</li><li id="bc70" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">Build forest by repeating all steps for “n” number times to create “n” number of trees</li><li id="129a" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">After the random forest decision trees and classifiers are created, predications can be made with the following steps (7–9):</li><li id="93fc" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">Run the test features through the rules of each decision tree to predict the outcome, then stores that predicted target outcome.</li><li id="0305" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">Calculate the votes for each predicted target</li><li id="9f0b" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">Choose the most highly voted predicted target as the final prediction</li></ol><p id="eef6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">*Source: <a class="ae je" href="https://deepai.org/machine-learning-glossary-and-terms/random-forest" rel="noopener ugc nofollow" target="_blank">deepai.org</a></p><h2 id="42f1" class="lt jg hi bd jh lu lv lw jl lx ly lz jp iq ma mb jt iu mc md jx iy me mf kb mg bi translated"><strong class="ak">随机森林超参数</strong></h2><p id="5049" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">随机森林具有以下超参数，这些参数是用户在模型训练之前定义的*。</p><ul class=""><li id="c61c" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated"><strong class="ih hj"> n_estimators: <br/> </strong>它定义了要在随机森林中创建的决策树的数量。通常，较高的数值会使预测更强、更稳定，但是非常大的数值会导致更长的训练时间。</li><li id="dd8e" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated"><strong class="ih hj">判据:<br/> </strong>定义了要用于拆分的函数。该函数测量每个要素的分割质量，并选择最佳分割。</li><li id="6097" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated"><strong class="ih hj"> max_features </strong> : <br/>它定义了每个决策树中分裂所允许的最大特征数。增加最大特征数通常会提高性能，但是很大的数量会降低每个树的多样性。</li><li id="b560" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated"><strong class="ih hj"> max_depth </strong>:随机森林有多个决策树。此参数定义了树的最大深度。</li><li id="36ba" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated"><strong class="ih hj"> min_samples_split: </strong>用于定义在尝试拆分之前，叶节点中所需的最小样本数。如果样本数量小于所需数量，则不分割节点。</li><li id="a127" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated"><strong class="ih hj"> min_samples_leaf: </strong>定义一个叶节点所需的最小样本数。较小的叶子尺寸使得模型更容易捕捉训练数据中的噪声。</li><li id="b3b6" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated"><strong class="ih hj"> max_leaf_nodes: </strong>该参数指定每棵树的最大叶节点数。当叶节点的数量等于最大叶节点时，树停止分裂。</li><li id="f662" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated"><strong class="ih hj"> n_jobs </strong>:表示并行运行的作业数量。如果希望它在系统中的所有核心上运行，请将值设置为-1。</li></ul><p id="34f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">*来源:<a class="ae je" href="https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/" rel="noopener ugc nofollow" target="_blank">分析Vidhya </a></p><h1 id="9aa6" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">随机森林的利与弊</h1><p id="dc23" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">一般来说，随机森林可以用于几乎任何表格数据集，只需要很少的预处理和特征工程步骤，并且具有高度的可解释性。然而，它们不针对回归任务进行外推，并且除非并行化，否则在训练和进行预测时会很慢。</p><p id="c1f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">随机森林赞成*: </strong></p><ol class=""><li id="1fee" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc kn ko kp kq bi translated">非参数算法。参数模型具有关于数据分布的参数(推断它们)或假设，而RF、神经网络或提升树具有与算法本身相关的参数，但它们不需要关于数据分布的假设或将数据分类为理论分布(不依赖于均匀分布的底层数据)</li><li id="ee90" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">可以处理高维数据集并识别最重要的特征，这可以被认为是一种降维方法</li><li id="0dc3" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">特征重要性可以很容易地从模型中提取出来</li><li id="d7a8" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">可以轻松处理异常值</li><li id="5f21" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">它有一个有效的方法来估计缺失数据，并在大部分数据缺失时保持准确性</li><li id="14ad" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">它有方法来平衡数据集中类不平衡的错误</li><li id="0ae4" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">上述能力可以扩展到未标记的数据，导致无监督的聚类、数据视图和异常值检测。</li><li id="88ec" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">不一定需要测试组，因为袋外样品可以用作测试组。由于随机森林涉及替换输入数据的采样(自举采样)，每棵树大约有三分之一的数据没有用于训练。对没有看到特定观测值的所有树的预测进行平均，对所有观测值重复这一过程，并与实际目标变量值进行比较，这就是如何确定袋外误差样本的。</li></ol><h2 id="2981" class="lt jg hi bd jh lu lv lw jl lx ly lz jp iq ma mb jt iu mc md jx iy me mf kb mg bi translated">随机林配置*:</h2><ol class=""><li id="2879" class="ki kj hi ih b ii kd im ke iq kk iu kl iy km jc kn ko kp kq bi translated">对于回归任务，树不能完美地模拟线性关系，只能使用无限小的步长变化来近似它</li><li id="388f" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">对于回归任务，预测可能缺乏平滑性。这是因为输入变量的微小变化可能导致遍历树中的不同路径，最终导致非常不同的结果</li><li id="762f" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">树是不稳定的，因为训练集中的微小变化可能会对最终模型产生很大影响</li><li id="f0ae" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">对于回归任务，随机森林不会预测超出训练数据范围的数据，并且它们可能会过度拟合噪声特别大的数据集</li><li id="a369" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">一个集合模型本质上比一个单独的决策树更难解释</li><li id="176a" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">训练大量的深度树会有很高的计算成本(但是可以并行化),并且使用大量的内存</li><li id="da8e" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">预测速度较慢，这可能会给应用程序带来挑战</li><li id="a1ae" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">对于统计建模者来说，随机森林就像是一种黑盒方法，我们对模型的功能几乎没有控制</li></ol><p id="8bd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">*以上利弊来自以下来源:</p><ul class=""><li id="f447" class="ki kj hi ih b ii ij im in iq mh iu mi iy mj jc mk ko kp kq bi translated">第一课上原广美·苏娜加关于人工智能的快速人工智能课程笔记</li><li id="7e29" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated"><a class="ae je" href="https://learning.oreilly.com/library/view/hands-on-machine-learning/9781789346411/" rel="noopener ugc nofollow" target="_blank">斯特凡·詹森的算法交易机器实践学习</a></li><li id="e45d" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated"><a class="ae je" href="https://github.com/ctufts/Cheat_Sheets/wiki/Classification-Model-Pros-and-Cons" rel="noopener ugc nofollow" target="_blank">克里斯塔夫斯分类模式利弊</a></li><li id="ff7d" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated">陈爱龙·普拉丹的Quora Q</li><li id="5209" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc mk ko kp kq bi translated"><a class="ae je" href="https://stats.stackexchange.com/questions/147587/are-random-forest-and-boosting-parametric-or-non-parametric" rel="noopener ugc nofollow" target="_blank">堆叠交换</a></li></ul><h1 id="7656" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated">从Fast.ai课程1中获得的其他知识</h1><p id="b5db" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated">虽然不一定特定于随机森林，但以下是视频讲座中涉及的其他关键主题:</p><h2 id="a542" class="lt jg hi bd jh lu lv lw jl lx ly lz jp iq ma mb jt iu mc md jx iy me mf kb mg bi translated">Jupyter笔记本命令</h2><ol class=""><li id="84ac" class="ki kj hi ih b ii kd im ke iq kk iu kl iy km jc kn ko kp kq bi translated">在单元格中键入<code class="du nm nn no np b">display(<em class="jd">function</em>)</code>并按shift+enter——它会告诉你<em class="jd">函数</em>的来源及其接受的参数(即定义它的脚本或笔记本)。这有助于导航到源代码。</li><li id="d959" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">在单元格中键入<code class="du nm nn no np b">?<em class="jd">function</em></code>并按shift+enter——它将显示给定<em class="jd">函数</em>的文档</li><li id="7ef8" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">在单元格中键入<code class="du nm nn no np b">??<em class="jd">function</em></code>,然后按shift+enter——它将显示<em class="jd">函数</em>的源代码。这对fast.ai库特别有用，因为大部分函数都像fast.ai创建者杰里米所说的那样“易于阅读，长度不超过5行”。</li><li id="66ed" class="ki kj hi ih b ii kr im ks iq kt iu ku iy kv jc kn ko kp kq bi translated">在一个命令中输入<code class="du nm nn no np b">%time</code>,查看该命令执行需要多长时间。在单元格顶部键入<code class="du nm nn no np b">%%time</code>,查看执行整个单元格需要多长时间。</li></ol><h2 id="29cd" class="lt jg hi bd jh lu lv lw jl lx ly lz jp iq ma mb jt iu mc md jx iy me mf kb mg bi translated">额外快速。人工智能功能有助于数据准备</h2><p id="3a0d" class="pw-post-body-paragraph if ig hi ih b ii kd ik il im ke io ip iq kf is it iu kg iw ix iy kh ja jb jc hb bi translated"><code class="du nm nn no np b">add_datepart</code>方法从一个数据帧中获取一个日期时间列，并返回一个相同的数据帧，该数据帧带有特定于日期时间的特性的附加列(下面是源代码)。与下面的函数相比，这可能是最有用的fast.ai内置函数，因为它非常python化，并且利用了许多鲜为人知的python/pandas内置函数。</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="nq nr l"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来源:Fast.ai</figcaption></figure><p id="429b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du nm nn no np b">train_cats</code>方法为所有字符串创建分类变量:</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="nq nr l"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来源:Fast.ai</figcaption></figure><p id="70b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du nm nn no np b">fix_missing</code>方法首先检查是否有空列。如果是这样，它将创建一个新列，其名称在末尾附加有' _na '，如果它不存在，则将它设置为1；否则为0(布尔值)。然后，它会用中间值替换缺失的值</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="nq nr l"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来源:Fast.ai</figcaption></figure><p id="0360" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du nm nn no np b">numericalize</code>方法检查一个列是否是分类类型，如果分类的数量超过特定的阈值，我们将用它的代码加1替换这个列。默认情况下，pandas使用-1表示缺失:</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="nq nr l"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来源:Fast.ai</figcaption></figure><p id="220d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du nm nn no np b">proc_df</code>方法复制数据框，获取因变量的值，并从数据框中删除因变量。然后它将<code class="du nm nn no np b">fix_missing</code>，调用<code class="du nm nn no np b">numericalize,</code>，然后在没有被数值化转换的分类列上创建虚拟列</p><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="nq nr l"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来源:Fast.ai</figcaption></figure><h1 id="b22d" class="jf jg hi bd jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc bi translated"><span class="l ns nt nu bm nv nw nx ny nz di"> T </span></h1></div></div>    
</body>
</html>