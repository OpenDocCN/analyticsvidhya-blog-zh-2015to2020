<html>
<head>
<title>A Simple Introduction to Dropout Regularization (With Code!)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">辍学正规化的简单介绍(附代码！)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-simple-introduction-to-dropout-regularization-with-code-5279489dda1e?source=collection_archive---------0-----------------------#2020-04-22">https://medium.com/analytics-vidhya/a-simple-introduction-to-dropout-regularization-with-code-5279489dda1e?source=collection_archive---------0-----------------------#2020-04-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="6915" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">阿曼·奥贝罗伊和妮莎·麦克尼利斯</p></blockquote><p id="bdc2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><strong class="il hj">什么是辍学？</strong></p><p id="1daa" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">机器学习中的“辍学”是指在训练过程中随机忽略某一层中的某些节点的过程。</p><p id="b0cd" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">在下图中，左侧的神经网络代表了一个典型的神经网络，其中所有单元都被激活。在右边，红色单元已被从模型中删除-它们的权重值和偏差在训练中不被考虑。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es jl"><img src="../Images/7b98ebfea445205b650b67a6afedaf8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/0*EY8R7nS10y5kQzOx"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">改编自Srivastava，Nitish等人，“辍学:防止神经网络<br/>过度拟合的简单方法”，JMLR，2014年</figcaption></figure><p id="f89a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">Dropout被用作一种正则化技术——它通过确保没有单元是相互依赖的来防止过度拟合(稍后将详细介绍)。</p><p id="60f6" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><strong class="il hj">其他常见的正则化方法</strong></p><p id="4d0c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">当谈到打击过度拟合，辍学肯定不是唯一的选择。常见的正则化技术包括:</p><ol class=""><li id="cba0" class="jx jy hi il b im in iq ir ji jz jj ka jk kb jg kc kd ke kf bi translated"><a class="ae jh" href="https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/" rel="noopener ugc nofollow" target="_blank">提前停止</a>:当一个特定的绩效指标(如验证损失、准确性)停止改善时，自动停止培训</li><li id="af5b" class="jx jy hi il b im kg iq kh ji ki jj kj jk kk jg kc kd ke kf bi translated"><a class="ae jh" href="https://machinelearningmastery.com/introduction-to-weight-constraints-to-reduce-generalization-error-in-deep-learning/" rel="noopener ugc nofollow" target="_blank">权重衰减</a>:通过向损失函数添加惩罚来激励网络使用较小的权重(这确保了权重的范数相对均匀地分布在网络中的所有权重中，从而防止少数权重严重影响网络输出)</li><li id="040b" class="jx jy hi il b im kg iq kh ji ki jj kj jk kk jg kc kd ke kf bi translated"><a class="ae jh" href="https://towardsdatascience.com/how-to-use-noise-to-your-advantage-5301071d9dc3" rel="noopener" target="_blank">噪声</a>:通过增强允许数据中的一些随机波动(这使得网络对更大的输入分布具有鲁棒性，从而提高泛化能力)</li><li id="1f90" class="jx jy hi il b im kg iq kh ji ki jj kj jk kk jg kc kd ke kf bi translated">模型组合:平均单独训练的神经网络的输出(需要大量的计算能力、数据和时间)</li></ol><p id="7282" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">尽管有过多的选择，辍学仍然是一个非常受欢迎的防止过度适应的保护措施，因为它的效率和效果。</p><p id="d490" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><strong class="il hj">辍学是如何进行的？</strong></p><p id="7ec6" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">当我们将dropout应用于神经网络时，我们正在创建一个“稀疏”的网络，隐藏层中的单元的独特组合在训练期间的不同时间点被随机丢弃。每次更新我们模型的梯度时，我们基于概率超参数<em class="ik"> p </em>生成一个新的稀疏神经网络，其中不同的单元被丢弃。因此，使用dropout训练网络可以被视为训练不同稀疏神经网络的负载，并将它们合并到一个网络中，该网络拾取每个稀疏网络的关键属性。</p><p id="e737" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">该过程允许放弃以减少模型对训练数据的过度拟合。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es kl"><img src="../Images/10a7e40c3f40cb59d5e510b1002b7072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/0*DhRPJZKX6Wyzn1if"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">Srivastava，Nitish等人，“辍学:防止神经网络<br/>过度拟合的简单方法”，JMLR，2014年</figcaption></figure><p id="88f9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">该图摘自Srivastava等人的论文“dropout:一种防止神经网络过度拟合的简单方法”，比较了没有dropout的模型与具有Dropout的相同模型(保持所有其他超参数不变)的分类误差的变化。所有的模型都在MNIST数据集上进行了训练。</p><p id="4eae" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">观察到在任何给定的时间点，具有脱落的模型比没有脱落的相同模型具有更低的分类误差。当这些模型用于训练vision中的其他数据集以及语音识别和文本分析时，也观察到了类似的趋势。</p><p id="aab0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">较低的误差是因为下降有助于通过减少隐藏层中的每个单元对隐藏层中的其他单元的依赖来防止对训练数据的过度拟合。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es km"><img src="../Images/999d88d90641ef739bdaa77af97ccc2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5M-4yLH56uPs0aBy"/></div></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">Srivastava，Nitish等人，“辍学:防止神经网络<br/>过度拟合的简单方法”，JMLR，2014年</figcaption></figure><p id="19db" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">这些取自同一篇论文的图表显示了MNIST上的自动编码器在一层256个单元没有丢失(<em class="ik"> a </em>)的情况下学习到的特征，以及使用p = 0.5的丢失(<em class="ik"> b </em>)的相同自动编码器学习到的特征。在图<em class="ik"> a </em>中可以观察到，单元似乎没有拾取任何有意义的特征，而在图<em class="ik"> b </em>中，单元似乎拾取了提供给它们的数据中的明显边缘和斑点。</p><p id="a348" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">这表明，退出有助于打破单元之间的协同适应，当使用退出正则化时，每个单元可以更加独立地行动。换句话说，如果没有退出，网络将永远无法捕捉到一个单元<em class="ik"> A </em>补偿另一个单元<em class="ik"> B </em>的缺陷。由于辍学，在某些点上单元<em class="ik"> A </em>将被忽略，结果训练精度将降低，暴露单元<em class="ik"> B </em>的不准确性。</p><p id="c429" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><strong class="il hj">如何使用辍学</strong></p><p id="e3e3" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">没有退出的CNN可以用如下代码表示:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kr"><img src="../Images/c0ece0a06b053080008b891429a0086d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fZn_-RoHMXFfr_4C"/></div></div></figure><p id="67b1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">要添加一个dropout层，程序员可以添加这样一行:</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es ks"><img src="../Images/748e9630ae26ff32fd9ca2babe0710e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Yq4eFeBo6P9LIhHu"/></div></div></figure><p id="1636" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">第一个参数，用橙色圈起来的，是给定单元退出的概率<em class="ik"> p </em>。在这个例子中，概率是0.5，这意味着大约一半的给定单元将会退出。值0.5已经通过实验确定为接近各种模型的最佳概率，但是可以随意实验其他概率！</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es kt"><img src="../Images/e142ce0c3472e6163e67aa9f793f2605.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*AyXXu6Fd8jigIc_M"/></div></figure><p id="85e1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><strong class="il hj">在测试过程中调整重量</strong></p><p id="c5aa" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">由于dropout从层中移除了一些单元，因此具有dropout的网络将在每次训练运行期间加重剩余单元的权重，以补偿丢失的输入。然而，在测试时，使用训练模型在其夸大状态下的权重是不可行的，因此通过乘以超参数<em class="ik"> p </em>来缩小每个权重。这种现象可以在下面的例子中观察到。</p><p id="6787" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">我们来看一个网络，一层有四个单元(图a)。每个单元上的权重最初为= 0.25。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es ku"><img src="../Images/062695682d2ed8b2369eee7226cdbb56.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/0*iO-MDUkVFBL939o-"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">(图片a)</figcaption></figure><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es kv"><img src="../Images/999660ba98d00227c7f378314e99b933.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/0*qMRUNsLyGLnUf6Ir"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">(图片b)</figcaption></figure><p id="06c7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">如果我们将p = 0.5的dropout应用于该层，它最终可能看起来像图像b。因为只考虑了两个单元，所以它们的初始权重都= 0.5。然而，dropout只用于训练，所以我们不希望这些权重在测试期间固定在这么高的数字。</p><p id="0704" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">为了解决这个问题，当我们进入测试阶段时，我们将权重乘以<em class="ik"> p </em>(如下图所示)，最终得到0.5*0.5 = 0.25，这是正确的初始权重。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es kw"><img src="../Images/322e907837c176c17b766bbeedb58466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/0*PfYLhhrVRP60A_VV"/></div><figcaption class="jt ju et er es jv jw bd b be z dx translated">Srivastava，Nitish等人，“辍学:防止神经网络<br/>过度拟合的简单方法”，JMLR，2014年</figcaption></figure><p id="acc9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><strong class="il hj">退出正则化中的超参数</strong></p><p id="72dd" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">已经发现，与退出正则化一起工作得很好的超参数设置包括大的衰减学习率和高动量。这是因为使用dropout限制我们的权重向量使我们能够使用大的学习速率，而不用担心权重爆炸。辍学产生的噪声加上我们大幅度下降的学习率帮助我们探索损失函数的不同区域，并有希望达到更好的最小值。</p><p id="3e20" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><strong class="il hj">辍学的坏处</strong></p><p id="15ab" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated">虽然辍学显然是一个非常有效的工具，但它也有一定的缺点。与标准网络相比，有中断的网络可能需要2-3倍的时间来训练。在不降低训练速度的情况下获得下降优势的一种方法是找到一个基本上相当于下降层的正则化子。对于线性回归，这种正则化已被证明是L2正则化的修改形式。对于更复杂的模型，一个等价的正则化子还有待于确定。在那之前，如果有疑问:退学。</p><p id="3069" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ji iv iw ix jj iz ja jb jk jd je jf jg hb bi translated"><strong class="il hj">自己试试！</strong></p><figure class="jm jn jo jp fd jq"><div class="bz dy l di"><div class="kx ky l"/></div></figure><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kz"><img src="../Images/9f7407aa15a482846e89f4118d3728a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WyqWF-JglMsvv4vXyfKX0Q.jpeg"/></div></div></figure></div></div>    
</body>
</html>