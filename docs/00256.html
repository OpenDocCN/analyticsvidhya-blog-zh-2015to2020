<html>
<head>
<title>Bellman Equation and dynamic programming</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝尔曼方程和动态规划</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bellman-equation-and-dynamic-programming-773ce67fc6a7?source=collection_archive---------0-----------------------#2019-01-28">https://medium.com/analytics-vidhya/bellman-equation-and-dynamic-programming-773ce67fc6a7?source=collection_archive---------0-----------------------#2019-01-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a4e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">报名参加我的计算机视觉直播课程:<a class="ae jd" href="https://bit.ly/cv_coursem" rel="noopener ugc nofollow" target="_blank">https://bit.ly/cv_coursem</a></p><p id="b6ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一系列关于强化学习的文章，如果你是新手并且没有学习过之前的文章，请务必阅读(本文最后的链接)。到目前为止，我们只讨论了强化学习的基础知识，以及如何使用马尔可夫决策过程(MDP)来描述强化学习问题。从现在开始，我们将致力于解决MDP问题。</p><p id="bc7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">贝尔曼方程</strong></p><p id="7119" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你读过任何与强化学习相关的东西，你一定在什么地方遇到过贝尔曼方程。贝尔曼方程是解决强化学习的基本块，在强化学习中无处不在。这有助于我们解决MDP。求解意味着找到最优策略和价值函数。</p><p id="cba0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最佳值函数V*(S)是产生最大值的函数。</p><p id="f832" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给定状态的<em class="je"> </em>值等于给定状态下最优动作的<strong class="ih hj"> </strong>奖励的最大动作(使值最大化的动作)并加上一个乘以贝尔曼方程中的<strong class="ih hj"> </strong>下一状态值的折扣因子。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jf"><img src="../Images/bbaa94ed9ff4075a125b3da61a64a68c.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*0ZXbechPD73VlGWtWkrvbQ.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">确定性环境的贝尔曼方程</figcaption></figure><p id="745f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来理解这个等式，V(s)是处于某一状态的值。V(s ')是我们在采取行动a后将处于下一个状态的值，R(s，a)是我们在状态s采取行动a后得到的回报，因为我们可以采取不同的行动，所以我们使用最大值，因为我们的代理人希望处于最佳状态。γ是前面讨论过的贴现因子。这就是确定性环境中的贝尔曼方程(在第1部分中讨论过)。对于非确定性环境或随机环境，情况会略有不同。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jr"><img src="../Images/0cf8215347342ea507a9f7dc45ef5d3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CiDCpUjj_3mGm3vdGrxu4g.png"/></div></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">随机环境中的贝尔曼方程</figcaption></figure><p id="9b05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在随机环境中，当我们采取一项行动时，并不确定我们会在特定的下一个状态结束，而是有可能在特定的状态结束。P(s，a，s ')是通过采取行动a从s结束is状态s '的概率。这是未来状态总数的总和。例如，如果通过采取一个行动，我们可以以0.2，0.2和0.6的概率在3个州s₁,s₂和s州s₃结束。贝尔曼方程将会是</p><p id="f0e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">v(s)= maxₐ(r(s,a)+γ(0.2*v(s₁)+0.2*v(s₂)+0.6*v(s₃)</p><p id="b469" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以用一种叫做动态规划的特殊技术来求解贝尔曼方程。</p><p id="9ea2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">动态编程</strong></p><p id="1e3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">动态规划是一种解决复杂问题的技术。在DP中，我们不是一次解决一个复杂的问题，而是将问题分解成简单的子问题，然后针对每个子问题，我们计算并存储解决方案。如果出现同样的子问题，我们不会重新计算，而是使用已经计算过的解。</p><p id="b2f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用两种强大的算法求解贝尔曼方程:</p><ul class=""><li id="63d7" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated">价值迭代</li><li id="56ea" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">策略迭代</li></ul><p id="39fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">值迭代</strong></p><p id="2435" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将通过图表和程序来学习它。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es kk"><img src="../Images/5ca7a65bf7d2f2c94562f723de721045.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*t7MnTsmL8ZQwoJN0SmE2vg.jpeg"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">价值迭代</figcaption></figure><p id="ce5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在值迭代中，我们从随机值函数开始。因为如果随机初始化，值表不会被优化，所以我们迭代地优化它。</p><p id="6521" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们从编程开始，我们将使用开放的人工智能健身房和numpy。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="kl km l"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">价值迭代</figcaption></figure><p id="ad80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">政策迭代</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es kk"><img src="../Images/eca7b59cc4cdb1f8ca198e80d67f0bfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*YxHvFU_HR1B-73rkyYlHxQ.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">策略迭代</figcaption></figure><p id="96e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在策略迭代中，首先决定或初始化代理需要采取的动作，并根据策略创建值表。</p><p id="1408" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">策略迭代的代码:</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="kl km l"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">策略迭代</figcaption></figure><h1 id="660c" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">强化学习系列</h1><ol class=""><li id="17cb" class="jw jx hi ih b ii ll im lm iq ln iu lo iy lp jc lq kc kd ke bi translated"><a class="ae jd" rel="noopener" href="/@sanchittanwar75/introduction-to-reinforcement-learning-dc49e5c04310">强化学习简介。</a></li><li id="212f" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc lq kc kd ke bi translated"><a class="ae jd" rel="noopener" href="/@sanchittanwar75/markov-chains-and-markov-decision-process-e91cda7fa8f2">马尔可夫链和马尔可夫决策过程。</a></li><li id="749e" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc lq kc kd ke bi translated">贝尔曼方程和动态规划→你在这里。</li></ol><p id="233f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献</strong></p><ol class=""><li id="07f4" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc lq kc kd ke bi translated">Sudarshan Ravichandran的python强化学习实践</li><li id="abaa" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc lq kc kd ke bi translated"><a class="ae jd" rel="noopener" href="/@taggatle/02-reinforcement-learning-move-37-the-bellman-equation-254375be82bd">https://medium . com/@ taggatle/02-reinforcement-learning-move-37-the-bellman-equation-254375 be 82 BD</a></li></ol></div></div>    
</body>
</html>