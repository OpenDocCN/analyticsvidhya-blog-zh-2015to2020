<html>
<head>
<title>Paper Review- DeshuffleGAN: A Self-Supervised GAN to Improve Structure Learning(ICIP 2020)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文综述——de shuffle GAN:改进结构学习的自我监督GAN(ICIP 2020)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/paper-review-deshufflegan-a-self-supervised-gan-to-improve-structure-learning-1d601f3d95f8?source=collection_archive---------28-----------------------#2020-07-13">https://medium.com/analytics-vidhya/paper-review-deshufflegan-a-self-supervised-gan-to-improve-structure-learning-1d601f3d95f8?source=collection_archive---------28-----------------------#2020-07-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="6016" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">内容</h2></div><ol class=""><li id="4b06" class="ix iy hi iz b ja jb jc jd je jf jg jh ji jj jk jl jm jn jo bi translated">摘要</li><li id="9721" class="ix iy hi iz b ja jp jc jq je jr jg js ji jt jk jl jm jn jo bi translated">方法</li><li id="48d1" class="ix iy hi iz b ja jp jc jq je jr jg js ji jt jk jl jm jn jo bi translated">结果和实验</li></ol><h2 id="ac89" class="ju jv hi bd jw jx jy jz ka kb kc kd ke je kf kg kh jg ki kj kk ji kl km kn ko bi translated">1.摘要</h2><p id="db98" class="pw-post-body-paragraph kp kq hi iz b ja kr ij ks jc kt im ku je kv kw kx jg ky kz la ji lb lc ld jk hb bi translated"><a class="ae le" href="https://arxiv.org/abs/2006.08694" rel="noopener ugc nofollow" target="_blank">本文</a>被ICIP 2020接受。</p><blockquote class="lf lg lh"><p id="9bb6" class="kp kq li iz b ja jb ij ks jc jd im ku lj lk kw kx ll lm kz la ln lo lc ld jk hb bi translated">作者认为，就真实性和与原始数据分布的相似性而言，提高<a class="ae le" href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="noopener ugc nofollow" target="_blank"> GAN </a>性能的关键点之一是能够为模型提供学习数据空间结构的能力。</p></blockquote><p id="fc2d" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">解决七巧板学习空间表征的想法已经存在。(<a class="ae le" href="https://arxiv.org/abs/1603.09246" rel="noopener ugc nofollow" target="_blank">【1】</a>，<a class="ae le" href="https://arxiv.org/abs/1903.06864" rel="noopener ugc nofollow" target="_blank">【2】</a>)但是，本文提出用上述思想来提高甘的学习能力。</p><p id="d5f9" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">作者将该模型实现为pytorch。但据我所知，还没有落实。</p><h1 id="1b66" class="lp jv hi bd jw lq lr ls ka lt lu lv ke io lw ip kh ir lx is kk iu ly iv kn lz bi translated">2.方法</h1><figure class="mb mc md me fd mf er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es ma"><img src="../Images/ba63b7bc8d7eb5b719ff39df38c7a50f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5wvr3alTr5wnuBWYVwzZ9w.png"/></div></div><figcaption class="mm mn et er es mo mp bd b be z dx translated">图一。模型的结构</figcaption></figure><p id="a442" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">作者介绍了Shuffler，它像拼图一样混合输入图像，以改善空间表达学习。</p><p id="320d" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">Shuffler将输入图像划分为9个图块，H和W的输入大小为/3，然后以随机顺序洗牌。此时，可能的排列数是9！，根据<a class="ae le" href="https://en.wikipedia.org/wiki/Hamming_distance" rel="noopener ugc nofollow" target="_blank">汉明距离</a>选择30种排列。同样的程序适用于所有样品。</p><p id="6242" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">在DeshuffleGAN中，鉴别器D不仅能区分标准GAN与X_real或X_fake，还能预测S_fake和S_real的排列顺序。d在执行两个任务时共享权重，输出层除外。</p><p id="560d" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">因此，去混洗的任务是用d来解决S_fake和S_real的拼图。如果生成器G生成的图像质量不好，拼图块将不会相互关联。在这种情况下，D向G给出负反馈以提高生成图像质量</p><h2 id="04c1" class="ju jv hi bd jw jx jy jz ka kb kc kd ke je kf kg kh jg ki kj kk ji kl km kn ko bi translated">看一看[1]，[2]之后</h2><p id="4a10" class="pw-post-body-paragraph kp kq hi iz b ja kr ij ks jc kt im ku je kv kw kx jg ky kz la ji lb lc ld jk hb bi translated">作者说，根据海明距离选择排列顺序。对我来说这看起来模棱两可。所以，我看一看[1]，[2]。有一个表格和算法解释选择排列顺序。</p><figure class="mb mc md me fd mf er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mq"><img src="../Images/cbf41641ad099ffebf0080edcd1a75a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*--yHBphxnibFJQqMUdU0aQ.png"/></div></div><figcaption class="mm mn et er es mo mp bd b be z dx translated">表1。置换集影响的烧蚀研究(来自[1])</figcaption></figure><figure class="mb mc md me fd mf er es paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="er es mr"><img src="../Images/53e5017fa2e49c3950a35f3262d101bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TkP1210g_fU078iwbm6J7Q.png"/></div></div><figcaption class="mm mn et er es mo mp bd b be z dx translated">来自[1]</figcaption></figure><p id="6046" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">尽管如此，我还是不知道他们为什么选择30种排列。有什么原因吗？..</p><h2 id="2067" class="ju jv hi bd jw jx jy jz ka kb kc kd ke je kf kg kh jg ki kj kk ji kl km kn ko bi translated">在作者的评论之后。</h2><p id="e590" class="pw-post-body-paragraph kp kq hi iz b ja kr ij ks jc kt im ku je kv kw kx jg ky kz la ji lb lc ld jk hb bi translated">上述问题的答案是，他们的目标不是解决具有挑战性的去混洗问题，他们只想选择数量少但有效的排列。</p><h2 id="47c6" class="ju jv hi bd jw jx jy jz ka kb kc kd ke je kf kg kh jg ki kj kk ji kl km kn ko bi translated">2比1的不利损失</h2><figure class="mb mc md me fd mf er es paragraph-image"><div class="er es ms"><img src="../Images/c5fac7453cf416317eb2dd47c9a2b8be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*VioG1QqdYGbzOH3c-jSlOg.png"/></div><figcaption class="mm mn et er es mo mp bd b be z dx translated">图2经典GAN训练损耗</figcaption></figure><p id="5274" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">注意，P是实数据分布，Q是生成数据分布，C(x)是x的实性的度量，L_D和L_G是D和G的平均损失函数。</p><p id="45bf" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">作者将拉甘的理论描述如下。</p><blockquote class="lf lg lh"><p id="48d4" class="kp kq li iz b ja jb ij ks jc jd im ku lj lk kw kx ll lm kz la ln lo lc ld jk hb bi translated">经典的GAN训练导致训练中的问题，因为对于真实和虚假数据，G将D推到输出1，而事实上鉴别器应该收敛到0.5，以实现输入和生成的数据分布之间的JS发散</p></blockquote><p id="03f1" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">作者还说，</p><blockquote class="lf lg lh"><p id="b901" class="kp kq li iz b ja jb ij ks jc jd im ku lj lk kw kx ll lm kz la ln lo lc ld jk hb bi translated">训练的目的应该是不仅增加假数据是真实的概率，而且降低真实数据是真实的概率</p></blockquote><p id="0c6c" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">因此，RaGAN提出了一个新的目标，即相对主义，即鉴别器将估计输入数据比生成数据更真实的概率。</p><p id="4e50" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">在这篇论文中，据说使用了RaGAN损耗和DCGAN结构。作者说，他们只增加了一个conv层的输出D的排列。</p><figure class="mb mc md me fd mf er es paragraph-image"><div class="er es mt"><img src="../Images/08e2d6a7fd2274042a167b82a7f2a5b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*63jj4V-OLobCnpeY_JlsZw.png"/></div><figcaption class="mm mn et er es mo mp bd b be z dx translated">图三。射频损耗</figcaption></figure><p id="bc0b" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">d预测X_real和X_fake的r/f概率，因为混洗数据不影响敌对目标</p><h2 id="d8ec" class="ju jv hi bd jw jx jy jz ka kb kc kd ke je kf kg kh jg ki kj kk ji kl km kn ko bi translated">2–2.去混洗损失</h2><p id="2dbc" class="pw-post-body-paragraph kp kq hi iz b ja kr ij ks jc kt im ku je kv kw kx jg ky kz la ji lb lc ld jk hb bi translated">对于鉴别器，目标是最小化S_real的真实洗牌顺序和洗牌顺序预测之间的误差。作者认为只根据S_real更新D的原因是因为使用S_fake有可能学习到无意义的数据。</p><p id="2744" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">相反，对于生成器，目标是最小化真实洗牌顺序和对S_fake的洗牌顺序的预测之间的误差。如果G很好地生成样本，训练有素的D使用真实数据将能够对生成的样本进行去洗牌。在这种情况下，D给予G以正反馈<br/>相比之下，如果G生成的样本很糟糕，训练有素的D使用真实数据将无法对生成的样本进行洗牌。这种情况下，D给g负反馈。</p><p id="1d7b" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">D和G的去混洗目标被给定为交叉熵损失。</p><figure class="mb mc md me fd mf er es paragraph-image"><div class="er es mu"><img src="../Images/52e478c3f15ceeea282b7b8519a037f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/1*pG3HUMXtImERue9hqxMnug.png"/></div><figcaption class="mm mn et er es mo mp bd b be z dx translated">图4。D和G的交叉熵损失</figcaption></figure><p id="a9be" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">其中N表示样本数量，y_d是S_real的大小为30×1的独热编码标签向量，y_bar_d是S_real的置换索引的预测向量。</p><p id="b905" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">具有30×1独热码编码向量的原因似乎是通过上述汉明距离选择了30个排列。</p><h2 id="27b1" class="ju jv hi bd jw jx jy jz ka kb kc kd ke je kf kg kh jg ki kj kk ji kl km kn ko bi translated">2–3.完全目标</h2><figure class="mb mc md me fd mf er es paragraph-image"><div class="er es mv"><img src="../Images/bc6a945ce83d190f105ddce1cfe121db.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*kcUBcRyw3UyT1Pf3xrScFg.png"/></div><figcaption class="mm mn et er es mo mp bd b be z dx translated">图五。完全目标函数</figcaption></figure><p id="37a3" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">本文选取α为1，β为0.2</p><h1 id="7cab" class="lp jv hi bd jw lq lr ls ka lt lu lv ke io lw ip kh ir lx is kk iu ly iv kn lz bi translated">3.结果和实验</h1><h2 id="3f82" class="ju jv hi bd jw jx jy jz ka kb kc kd ke je kf kg kh jg ki kj kk ji kl km kn ko bi translated">3–1.目标函数</h2><p id="3a17" class="pw-post-body-paragraph kp kq hi iz b ja kr ij ks jc kt im ku je kv kw kx jg ky kz la ji lb lc ld jk hb bi translated">在<a class="ae le" href="https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" rel="noopener ugc nofollow" target="_blank"> [ </a> 3]中具有标准对抗性训练损失的RaSGAN，在[ <a class="ae le" href="https://arxiv.org/abs/1611.04076" rel="noopener ugc nofollow" target="_blank"> 4 </a>中具有最小平方损失的RaLSGAN，以及在[ <a class="ae le" href="https://arxiv.org/abs/1705.02894" rel="noopener ugc nofollow" target="_blank"> 5 </a>中具有铰链损失的RaHingeGAN被用作基线方法。基线的去混洗版本增加了去混洗损失，如图5所示。</p><h2 id="0ce6" class="ju jv hi bd jw jx jy jz ka kb kc kd ke je kf kg kh jg ki kj kk ji kl km kn ko bi translated">3–2.结果</h2><figure class="mb mc md me fd mf er es paragraph-image"><div class="er es mw"><img src="../Images/d191e312a2041bb9ba5f4335aa4dffeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*bf3HBZw6r4FDtSJZZQM6Uw.png"/></div><figcaption class="mm mn et er es mo mp bd b be z dx translated">表二。评估结果(FID)</figcaption></figure><p id="00ab" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">除了在CAT数据集上的RaSGAN之外，在所有设置中，去DeshuffleGANs相对于基线实现了较低的FID。</p><figure class="mb mc md me fd mf er es paragraph-image"><div class="er es mx"><img src="../Images/80350190db598659fb5749d1b5b7eef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*XiFoUFp9NoGwGaJlxnjvJQ.png"/></div><figcaption class="mm mn et er es mo mp bd b be z dx translated">图CAT数据集的生成结果。</figcaption></figure><figure class="mb mc md me fd mf er es paragraph-image"><div class="er es ms"><img src="../Images/023cc6d3d1dbfc134533fb5c17a35972.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*gJ431CkQxPVN8MVbIegSHw.png"/></div><figcaption class="mm mn et er es mo mp bd b be z dx translated">图7。LSUN教堂数据集的生成结果。</figcaption></figure><p id="aa89" class="pw-post-body-paragraph kp kq hi iz b ja jb ij ks jc jd im ku je lk kw kx jg lm kz la ji lo lc ld jk hb bi translated">从正态分布中采样5个不同的向量，并作为输入提供给6个不同的GAN模型，如下所示。<br/> (a)拉斯甘，(b)拉斯甘，(c)拉欣根，(d)德舒夫雷(拉斯)甘，(e)德舒夫雷(拉斯)甘，(f)德舒夫雷(拉欣格)甘</p><h1 id="35ee" class="lp jv hi bd jw lq lr ls ka lt lu lv ke io lw ip kh ir lx is kk iu ly iv kn lz bi translated">我的看法</h1><p id="d2b6" class="pw-post-body-paragraph kp kq hi iz b ja kr ij ks jc kt im ku je kv kw kx jg ky kz la ji lb lc ld jk hb bi translated">就我个人而言，我认为在基本GAN框架没有显著变化的情况下，通过增加一个洗牌损失项来提高性能是非常好的。</p></div></div>    
</body>
</html>