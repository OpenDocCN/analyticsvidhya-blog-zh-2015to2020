<html>
<head>
<title>Feature Engineering Method with code Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带有代码示例的特征工程方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-engineering-method-with-code-examples-b016fc9f1e9a?source=collection_archive---------8-----------------------#2019-10-31">https://medium.com/analytics-vidhya/feature-engineering-method-with-code-examples-b016fc9f1e9a?source=collection_archive---------8-----------------------#2019-10-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/65b851ba1426dbcf68a05239cbcceeb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*DchtxV_HDCg6gFasCfNUkA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://interestingengineering.com/craft-a-successful-career-with-these-inspiring-success-stories-by-mechanical-engineers" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h1 id="47d3" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">介绍</h1><p id="5fa8" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">特征工程是机器学习的一个非常重要的方面。特征工程是充分利用数据的可靠方法。这通常比找到最佳模型和超参数更有效。</p><p id="1f53" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">最近发现kaggle提供的一个非常好的关于特征工程师的<a class="ae iu" href="https://www.kaggle.com/learn/feature-engineering" rel="noopener ugc nofollow" target="_blank">教程</a>。所以这篇文章基本上是我从本教程和我在参考资料部分列出的一些其他资源中学到的东西。</p><h1 id="7d73" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">分类特征</h1><h2 id="d4bd" class="kw iw hi bd ix kx ky kz jb la lb lc jf ke ld le jj ki lf lg jn km lh li jr lj bi translated">编码分类特征</h2><p id="481d" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">要将分类特征转换为数值，我们可以进行因式分解或热编码。</p><ul class=""><li id="5604" class="lk ll hi jv b jw kr ka ks ke lm ki ln km lo kq lp lq lr ls bi translated"><strong class="jv hj">因式分解</strong></li></ul><p id="30b4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">因式分解意味着用数字替换唯一值。我们可以使用Pandas <code class="du lt lu lv lw b">factorize</code>或scikit-learn <code class="du lt lu lv lw b">LabelEncoder</code>来获得相同的结果。</p><figure class="lx ly lz ma fd ij"><div class="bz dy l di"><div class="mb mc l"/></div></figure><p id="8a44" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">或者在scikit-learn的情况下<code class="du lt lu lv lw b">LabelEncoder</code></p><pre class="lx ly lz ma fd md lw me mf aw mg bi"><span id="cab4" class="kw iw hi lw b fi mh mi l mj mk">from sklearn.preprocessing import LabelEncoder<br/>encoder = LabelEncoder()<br/><em class="ml"># Apply the label encoder to each column</em><br/>df[categorical_col] = df[categorical_col].apply(encoder.fit_transform)</span></pre><ul class=""><li id="f7ee" class="lk ll hi jv b jw kr ka ks ke lm ki ln km lo kq lp lq lr ls bi translated"><strong class="jv hj">一个热编码</strong></li></ul><p id="c18a" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在一个热编码的情况下，特征被扩展到唯一值的数量。</p><figure class="lx ly lz ma fd ij"><div class="bz dy l di"><div class="mb mc l"/></div></figure><h2 id="d7a3" class="kw iw hi bd ix kx ky kz jb la lb lc jf ke ld le jj ki lf lg jn km lh li jr lj bi translated">相互作用</h2><p id="2b02" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">生成新分类特征的一个简单方法是将两个分类特征相加。</p><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/b8a545680e8877cc56cfc0dd6ae08ce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FuYYW-c2egGGl3K0uAy4Og.png"/></div></div></figure><h2 id="06e9" class="kw iw hi bd ix kx ky kz jb la lb lc jf ke ld le jj ki lf lg jn km lh li jr lj bi translated">分类特征的计数编码</h2><p id="003b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">计数编码将每个分类值替换为它在数据集中出现的次数。</p><p id="7b19" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们可以简单地使用group-transform-count方法或<code class="du lt lu lv lw b"><a class="ae iu" href="https://github.com/scikit-learn-contrib/categorical-encoding" rel="noopener ugc nofollow" target="_blank">categorical-encodings</a></code> <a class="ae iu" href="https://github.com/scikit-learn-contrib/categorical-encoding" rel="noopener ugc nofollow" target="_blank">包</a>来获得这种编码。</p><pre class="lx ly lz ma fd md lw me mf aw mg bi"><span id="a695" class="kw iw hi lw b fi mh mi l mj mk">import category_encoders as ce<br/>cat_features = ['category', 'currency', 'country']<br/><strong class="lw hj">count_enc = ce.CountEncoder()</strong><br/>count_encoded = count_enc.fit_transform(ks[cat_features])<br/><br/>data = baseline_data.join(count_encoded.add_suffix("_count"))<br/><br/><em class="ml"># Training a model on the baseline data</em><br/>train, valid, test = get_data_splits(data)<br/>bst = train_model(train, valid)</span></pre><p id="ca9e" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">或者简单地像这样:</p><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/1cd38af49ddc4ed392facb88b403c48b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kKhW5Bbn7eoT5K-TS7X83g.png"/></div></div></figure><h2 id="4fb2" class="kw iw hi bd ix kx ky kz jb la lb lc jf ke ld le jj ki lf lg jn km lh li jr lj bi translated">目标编码</h2><p id="02a3" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">目标编码将分类值替换为该特征值的目标平均值。</p><p id="399a" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">然而，使用这种方法，我们需要非常小心“目标泄漏”问题。我们应该只从训练数据集中学习目标编码，并将其应用到其他数据集中。</p><pre class="lx ly lz ma fd md lw me mf aw mg bi"><span id="5af2" class="kw iw hi lw b fi mh mi l mj mk">import category_encoders as ce<br/>cat_features = ['category', 'currency', 'country']<br/><br/><em class="ml"># Create the encoder itself</em><br/>target_enc = ce.TargetEncoder(cols=cat_features)<br/><br/>train, valid, _ = get_data_splits(data)<br/><br/><em class="ml"># Fit the encoder using the categorical features and target</em><br/>target_enc.fit(train[cat_features], train['outcome'])<br/><br/><em class="ml"># Transform the features, rename the columns with _target suffix, and join to dataframe</em><br/>train = train.join(target_enc.transform(train[cat_features]).add_suffix('_target'))<br/>valid = valid.join(target_enc.transform(valid[cat_features]).add_suffix('_target'))<br/><br/>train.head()<br/>bst = train_model(train, valid)</span></pre><p id="4a9d" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">或者我们可以使用<a class="ae iu" href="https://mattmotoki.github.io/beta-target-encoding.html" rel="noopener ugc nofollow" target="_blank"> beta目标编码</a>来进一步减少泄漏。</p><h2 id="9cd7" class="kw iw hi bd ix kx ky kz jb la lb lc jf ke ld le jj ki lf lg jn km lh li jr lj bi translated">CatBoost编码</h2><p id="84ca" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">根据<a class="ae iu" href="https://www.kaggle.com/matleonard/categorical-encodings" rel="noopener ugc nofollow" target="_blank">本笔记本</a>:</p><blockquote class="mo mp mq"><p id="ce99" class="jt ju ml jv b jw kr jy jz ka ks kc kd mr kt kg kh ms ku kk kl mt kv ko kp kq hb bi translated">catboost类似于目标编码，因为它基于给定值的目标概率。然而，对于CatBoost，对于每一行，目标概率只从它前面的行计算。</p></blockquote><p id="b3bd" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">而我们可以从这里的公文<a class="ae iu" href="https://catboost.ai/docs/concepts/algorithm-main-stages_cat-to-numberic.html" rel="noopener ugc nofollow" target="_blank">中了解到。然而，我还没有找出文件中的例子。所以我暂时先不说了。如果你明白了，请留言。</a></p><p id="6c43" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated"><a class="ae iu" href="https://github.com/scikit-learn-contrib/categorical-encoding" rel="noopener ugc nofollow" target="_blank">分类编码</a>中提供了很多其他方法，我还没有经历过:</p><figure class="lx ly lz ma fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/e579738dbf2faca13cebba4b4360170b.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*QANo7YfqDz3lyTcOKsxEaQ.png"/></div></figure><h2 id="ae2c" class="kw iw hi bd ix kx ky kz jb la lb lc jf ke ld le jj ki lf lg jn km lh li jr lj bi translated">分类特征的组特征</h2><p id="471f" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">另一种常用的方法是按分类特征对数据进行分组，并计算其他特征的聚合值。在这种情况下，转换方法非常有用。</p><figure class="lx ly lz ma fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mv"><img src="../Images/6b4eb02be81ebb9d1c310c294c562a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y9ts-EeoPt-auauGzJ1FAA.png"/></div></div></figure><h1 id="240d" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">连续特征</h1><h2 id="b2a7" class="kw iw hi bd ix kx ky kz jb la lb lc jf ke ld le jj ki lf lg jn km lh li jr lj bi translated">改变</h2><p id="f49e" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">关于连续特征的一种非常常见的技术是用一些函数对其进行变换。因为当特征是正态分布时，一些模型工作得更好，所以它可能有助于转换目标值。</p><p id="9249" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">常见的选择是平方根和自然对数。这些转换也有助于约束离群值。但是，这不会影响树模型。</p><h2 id="cc01" class="kw iw hi bd ix kx ky kz jb la lb lc jf ke ld le jj ki lf lg jn km lh li jr lj bi translated">旋转</h2><p id="6548" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">对于具有时间序列索引的值，尝试一些滚动功能是个好主意。比如滚动平均，计数等等。</p><h1 id="e81b" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">最后，特征选择</h1><p id="20f3" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在我们创建了大量的新特征之后，选择那些最能提高计算效率和减少过度拟合问题的特征是至关重要的。我写过两篇关于特性选择的文章:</p><h2 id="3857" class="kw iw hi bd ix kx ky kz jb la lb lc jf ke ld le jj ki lf lg jn km lh li jr lj bi translated"><a class="ae iu" href="https://towardsdatascience.com/next-level-feature-selection-method-with-code-examples-2ba4edb40cd3?source=your_stories_page---------------------------" rel="noopener" target="_blank">下一级特征选择方法及代码示例</a></h2><h2 id="6a4c" class="kw iw hi bd ix kx ky kz jb la lb lc jf ke ld le jj ki lf lg jn km lh li jr lj bi translated"><a class="ae iu" rel="noopener" href="/analytics-vidhya/feature-selection-methods-with-code-examples-a78439477cd4?source=your_stories_page---------------------------">特征选择方法及代码示例</a></h2><p id="a18d" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">希望我们会得到一个好的模型^-^</p><h1 id="f492" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">参考</h1><div class="mw mx ez fb my mz"><a href="http://contrib.scikit-learn.org/categorical-encoding/index.html" rel="noopener  ugc nofollow" target="_blank"><div class="na ab dw"><div class="nb ab nc cl cj nd"><h2 class="bd hj fi z dy ne ea eb nf ed ef hh bi translated">类别编码器-类别编码器最新文档</h2><div class="ng l"><h3 class="bd b fi z dy ne ea eb nf ed ef dx translated">一套scikit-learn风格的转换器，用于使用不同的技术将分类变量编码成数字…</h3></div><div class="nh l"><p class="bd b fp z dy ne ea eb nf ed ef dx translated">contrib.scikit-learn.org</p></div></div></div></a></div><div class="mw mx ez fb my mz"><a href="https://www.kaggle.com/matleonard/categorical-encodings" rel="noopener  ugc nofollow" target="_blank"><div class="na ab dw"><div class="nb ab nc cl cj nd"><h2 class="bd hj fi z dy ne ea eb nf ed ef hh bi translated">分类编码</h2><div class="ng l"><h3 class="bd b fi z dy ne ea eb nf ed ef dx translated">下载数千个项目的开放数据集+在一个平台上共享项目。探索热门话题，如政府…</h3></div><div class="nh l"><p class="bd b fp z dy ne ea eb nf ed ef dx translated">www.kaggle.com</p></div></div><div class="ni l"><div class="nj l nk nl nm ni nn io mz"/></div></div></a></div><div class="mw mx ez fb my mz"><a href="https://www.kaggle.com/kyakovlev/ieee-fe-for-local-test" rel="noopener  ugc nofollow" target="_blank"><div class="na ab dw"><div class="nb ab nc cl cj nd"><h2 class="bd hj fi z dy ne ea eb nf ed ef hh bi translated">用于本地测试的IEEE - FE</h2><div class="ng l"><h3 class="bd b fi z dy ne ea eb nf ed ef dx translated">下载数千个项目的开放数据集+在一个平台上共享项目。探索热门话题，如政府…</h3></div><div class="nh l"><p class="bd b fp z dy ne ea eb nf ed ef dx translated">www.kaggle.com</p></div></div></div></a></div><figure class="lx ly lz ma fd ij"><div class="bz dy l di"><div class="no mc l"/></div></figure></div></div>    
</body>
</html>