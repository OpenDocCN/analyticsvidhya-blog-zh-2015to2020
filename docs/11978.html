<html>
<head>
<title>Analyzing Fleet Foxes New Album ‘Shore’ Using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python分析狐狸乐队新专辑《海岸》</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/analyzing-fleet-foxes-new-album-shore-using-python-e737fc40f3ef?source=collection_archive---------19-----------------------#2020-12-26">https://medium.com/analytics-vidhya/analyzing-fleet-foxes-new-album-shore-using-python-e737fc40f3ef?source=collection_archive---------19-----------------------#2020-12-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="4ed2" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">深入分析</h2><div class=""/><div class=""><h2 id="c8e6" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">(另)使用Python编程语言对狐狸乐队的歌曲进行文本分析</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/b4f92050bfa24f7dbe0ff983a9e5ac4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mEJ24t9_DUElZHmk"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">海岸专辑黑胶</figcaption></figure><p id="28a7" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi ks translated">还记得我们都认为2020年会比2019更好的时候吗？原来今年差很多。从二月份新冠肺炎病毒开始在世界各地传播到最后几个月，这是艰难的一年。今年发生了很多事情，病毒；人们生病和死亡，乔治·弗洛伊德，科比，格伦·弗雷德利，美国总统选举，等等。</p><p id="da15" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">我们都同意2020年是一场噩梦，但每个噩梦都有一个休息的时间。2020年可能一点也不坏，尤其是对我和你这些狐狸乐队的粉丝来说。2020年9月22日，我最喜欢的乐队，发行了他们的新专辑《海岸》。《海岸》是狐狸乐队的第四张专辑，也是该乐队在中断三年后于2016年重组后的第二张专辑。</p><p id="24cd" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">我是狐狸乐队的超级粉丝。如果你查看我的2020 Spotify Wrapped，你可以看到四首热门歌曲是蓝岭山脉(我一直最喜欢的)，粗糙的木头，无助的布鲁斯和白色冬季赞美诗。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es lb"><img src="../Images/2a3a5464e1eeb09845d3af4295270d1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a1Kd0KGSk4oyKaNApe0t9Q.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">我的Spotify包装2020</figcaption></figure><p id="e035" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">之前写过一篇关于<a class="ae lc" rel="noopener" href="/analytics-vidhya/analyzing-fleet-foxess-top-5-favorite-songs-using-r-7953daa4e403"> <strong class="jy hs">的文章，分析了狐狸乐队最喜欢的五首歌</strong> </a>。在这篇文章中，我将做同样的事情，但我不会分析他们最喜欢的五首歌曲，而是我想分析他们的新专辑“海岸”。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ld"><img src="../Images/5c50815f1a2b0427b355256af3b29ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*6KOS9xQy1On705s6"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">Shore专辑曲目</figcaption></figure><p id="2edf" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">在前面的分析中，我使用了R编程语言，但是在本文中，我将使用Python。该分析的目的是探索包含在专辑中每首歌曲的文本文档(歌词)中的信息。专辑包含15首歌曲，最长曲目约5分钟，最短曲目约2分钟。</p><p id="d381" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">让我们跳到代码！</p></div><div class="ab cl le lf gp lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="hb hc hd he hf"><p id="2640" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">还是和上一篇文章一样，我使用Genius library从Genius网站数据库中获取数据，也就是歌词。我发现Genius不仅可以提取歌词，还可以从每首歌曲或专辑中提取元数据。这确实是一个有用的库，但是对于这个分析，我只需要每首歌的歌词和标题。</p><p id="f7f8" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">我想做的第一件事是导入我将用于分析的所有库。</p><h2 id="bd80" class="ll lm hi bd ln lo lp lq lr ls lt lu lv kf lw lx ly kj lz ma mb kn mc md me ho bi translated">导入库</h2><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="2f8a" class="ll lm hi mg b fi mk ml l mm mn">import lyricsgenius<br/>import json <br/>import ijson<br/>import pandas as pd<br/>import nltk<br/>nltk.download('stopwords')<br/>import re<br/>import string<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/>from nltk.tokenize.treebank import TreebankWordDetokenizer<br/>from PIL import Image<br/>from wordcloud import WordCloud<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from nltk.corpus import opinion_lexicon<br/>nltk.download('opinion_lexicon')<br/>from nltk.sentiment.vader import SentimentIntensityAnalyzer<br/>nltk.download('vader_lexicon')</span></pre><p id="e2b4" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">这是一个非常标准的文本文档情感分析库。</p><h2 id="7f62" class="ll lm hi bd ln lo lp lq lr ls lt lu lv kf lw lx ly kj lz ma mb kn mc md me ho bi translated">生成数据</h2><p id="bfc4" class="pw-post-body-paragraph jw jx hi jy b jz mo is kb kc mp iv ke kf mq kh ki kj mr kl km kn ms kp kq kr hb bi translated">在使用Genius库提取数据之前，我们需要从Genius网站获取令牌。您可以注册，然后生成您的令牌，之后您可以使用您的令牌，这样API就可以从Genius数据库中提取数据。</p><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="8b7a" class="ll lm hi mg b fi mk ml l mm mn">token = "insert your token here"<br/>genius = lyricsgenius.Genius(token)</span></pre><p id="53e6" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">Genius library附带了很多提取数据的函数。你可以在他们的文档<a class="ae lc" href="https://github.com/johnwmillr/LyricsGenius" rel="noopener ugc nofollow" target="_blank">这里</a>查看所有的功能。由于我想从专辑中获取每首歌的所有歌词，所以我会使用<code class="du mt mu mv mg b">search_album</code>函数，然后将提取的所有数据保存为JSON格式。</p><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="0f52" class="ll lm hi mg b fi mk ml l mm mn">f = open('Lyrics_Shore.json') <br/>data = json.load(f)</span><span id="db10" class="ll lm hi mg b fi mw ml l mm mn">g = open('Lyrics_Shore.json', 'r')<br/>parser = ijson.parse(g)<br/>paths = sorted(set(prefix for prefix, event, value in parser if prefix))</span><span id="fab4" class="ll lm hi mg b fi mw ml l mm mn">#to check the paths<br/>for path in paths:<br/>    print(path)</span></pre><p id="d8d9" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">如果您打印路径，您会看到Genius不仅提取歌词，还提取歌曲的元数据。在这种情况下，我们必须从JSON格式中提取歌词和标题，并将它们都插入dataframe格式中。</p><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="ec48" class="ll lm hi mg b fi mk ml l mm mn">df = []<br/>for i in data['songs']:<br/>    df.append({'title': i["title"], 'lyrics': i["lyrics"]})</span><span id="f796" class="ll lm hi mg b fi mw ml l mm mn">shore = pd.DataFrame(df)</span></pre><p id="9cf2" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">完成后，您将得到一个包含两列的dataframe。一个是每首歌的歌名，一个是每首歌的歌词。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mx"><img src="../Images/518c0a3ac245791e245a91ca4e60a734.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*yjRA5uapbe2xY6JM0BYS7Q.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">数据帧包含标题和歌词</figcaption></figure><p id="533c" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">由于歌词仍然是一个<em class="my">肮脏的</em>格式，那么我们的下一步是做清理，删除歌词中不必要的字符串。</p><h2 id="4f51" class="ll lm hi bd ln lo lp lq lr ls lt lu lv kf lw lx ly kj lz ma mb kn mc md me ho bi translated">清洗</h2><p id="793a" class="pw-post-body-paragraph jw jx hi jy b jz mo is kb kc mp iv ke kf mq kh ki kj mr kl km kn ms kp kq kr hb bi translated">我将删除标点符号，删除不必要的文本，如<em class="my">诗句，合唱，结尾</em>，将文本转换为小写，拆分单词，然后删除停用词。</p><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="af64" class="ll lm hi mg b fi mk ml l mm mn">stopword = nltk.corpus.stopwords.words('english')<br/>def remove_stopwords(text):<br/>    text=[word for word in text if word not in stopword]<br/>    return text</span><span id="3126" class="ll lm hi mg b fi mw ml l mm mn">def remove_punctuation(text):<br/>    no_punct=[words for words in text if words not in string.punctuation]<br/>    words_wo_punct=''.join(no_punct)<br/>    return words_wo_punct</span><span id="dc86" class="ll lm hi mg b fi mw ml l mm mn">def detokenize(text):<br/>    de = TreebankWordDetokenizer().detokenize(text)<br/>    return de</span><span id="ff68" class="ll lm hi mg b fi mw ml l mm mn">shore['lyrics']=shore['lyrics'].apply(lambda x: remove_punctuation(x))<br/>shore['lyrics'] = shore['lyrics'].str.replace(r'Verse', '')<br/>shore['lyrics'] = shore['lyrics'].str.replace(r'Chorus', ' ')<br/>shore['lyrics'] = shore['lyrics'].str.replace(r'Outro', ' ')<br/>shore['lyrics'] = shore['lyrics'].str.replace(r'\n', ' ')<br/>shore['lyrics'] = shore['lyrics'].str.replace(r'\u2005', ' ')<br/>shore['lyrics'] = shore['lyrics'].str.replace('\d+', '')<br/>shore['lyrics'] = shore['lyrics'].str.lower()<br/>shore['lyrics'] = shore['lyrics'].str.split()<br/>shore['filtered_words'] = shore['lyrics'].apply(lambda x: remove_stopwords(x))<br/>shore['clean_lyrics'] = shore['filtered_words'].apply(lambda x: detokenize(x))</span></pre><p id="b988" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">完成所有工作后，现在你可以使用数据进行分析，因为现在歌词是干净的，可以使用了。我想做的第一件事是统计每首歌的字数。</p><h2 id="ad2d" class="ll lm hi bd ln lo lp lq lr ls lt lu lv kf lw lx ly kj lz ma mb kn mc md me ho bi translated">字数统计</h2><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="463f" class="ll lm hi mg b fi mk ml l mm mn">shore['words_count'] = shore['lyrics'].str.len()<br/>shore = shore.sort_values('words_count')</span></pre><p id="24fe" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">之后，我使用Matplotlib将其可视化。</p><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="4f37" class="ll lm hi mg b fi mk ml l mm mn">shore.plot.barh(x='title', y='words_count', figsize=(9,6), colormap='summer', title='Number of Words for each Title');</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mz"><img src="../Images/8cf84222426cf9b1cadd4f61954ac7b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xyRTpRQ218OU-qZcgUaE7w.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">每个标题的字数</figcaption></figure><p id="4fab" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">你可以看到，Sunblind和专辑中的其他歌曲相比字数最多，而胸腺嘧啶的字数最少。接下来我要做的是使用词汇来查找每首歌曲使用的独特单词的数量(词汇丰富度)。</p><h2 id="cb73" class="ll lm hi bd ln lo lp lq lr ls lt lu lv kf lw lx ly kj lz ma mb kn mc md me ho bi translated">词汇丰富度</h2><p id="bc8e" class="pw-post-body-paragraph jw jx hi jy b jz mo is kb kc mp iv ke kf mq kh ki kj mr kl km kn ms kp kq kr hb bi translated">将词汇中使用的单词数相除，我们就能衡量出每位艺术家的词汇丰富程度(即歌曲中使用的单词有多大比例是独特的)。</p><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="60ca" class="ll lm hi mg b fi mk ml l mm mn">shore['filtered_words_count'] = shore['filtered_words'].str.len()<br/>shore['lexical_richness'] = shore['filtered_words_count']/shore['words_count']*100<br/>shore = shore.sort_values('lexical_richness')<br/>shore.plot.barh(x='title', y='lexical_richness', figsize=(9,6), colormap='Wistia', title='Lexical Richness for each Title')</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mz"><img src="../Images/78f04ac3ba63aea5473c611108635462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QaB_n9gKjDDATVWM8ttLPA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">每个标题的词汇丰富度</figcaption></figure><p id="ce95" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">如你所见，划分词汇使用的字数后的结果是，<em class="my"> Quiet Air/Gioia </em>拥有最丰富的词汇，而<em class="my"> Sunblind </em>虽然拥有最多的单词，但却下降到了第四位。另一方面，即使<em class="my">胸腺嘧啶</em>有最少的词，它实际上比专辑中的其他歌曲有更多的词汇丰富性。</p><h2 id="bfa6" class="ll lm hi bd ln lo lp lq lr ls lt lu lv kf lw lx ly kj lz ma mb kn mc md me ho bi translated">Wordcloud</h2><p id="2a6f" class="pw-post-body-paragraph jw jx hi jy b jz mo is kb kc mp iv ke kf mq kh ki kj mr kl km kn ms kp kq kr hb bi translated">一旦我们知道了每首歌中的单词数量和词汇丰富程度，现在我想可视化专辑中所有最常用的单词(所有歌曲)。我将使用wordcloud来可视化所有的单词，但在此之前，我必须计算每首歌的所有单词，并将其转换为一个向量。</p><p id="d600" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">我正在使用sci-kit learn countvectorizer来计算每个单词的频率。</p><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="883e" class="ll lm hi mg b fi mk ml l mm mn">X = shore['clean_lyrics']<br/>vectorizer = CountVectorizer()<br/>text_vec = vectorizer.fit_transform(X.values.astype('U'))<br/>word_count = pd.DataFrame(text_vec.toarray(), columns=vectorizer.get_feature_names())</span><span id="7d04" class="ll lm hi mg b fi mw ml l mm mn">word_list = vectorizer.get_feature_names()<br/>count_list = text_vec.toarray().sum(axis=0) <br/>word_freq = dict(zip(word_list,count_list))</span></pre><p id="0179" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">然后，我用wordcloud把它可视化。</p><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="7190" class="ll lm hi mg b fi mk ml l mm mn">wcp = WordCloud(background_color="white",width=1000,height=1000, max_words=100,relative_scaling=0.5,normalize_plurals=False).generate_from_frequencies(word_freq)<br/>plt.figure(figsize=(9,6))<br/>plt.imshow(wcp)<br/>plt.axis('off')<br/>plt.show()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es na"><img src="../Images/fe74a242e818ff32cb879eda6f758994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*4A2tvGa8WGs5bKHeGhVH8w.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">相册中最常用的单词</figcaption></figure><p id="eb9e" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">“一个”和“想要”是用得最多的词，其次是“水”、“走”、“从不”。这是显而易见的，因为“水”这个词经常在前两首曲目中被提及，<em class="my">涉过齐腰高的水</em>和<em class="my">窗帘</em>。</p><h2 id="0b0c" class="ll lm hi bd ln lo lp lq lr ls lt lu lv kf lw lx ly kj lz ma mb kn mc md me ho bi translated">情感分析</h2><p id="765e" class="pw-post-body-paragraph jw jx hi jy b jz mo is kb kc mp iv ke kf mq kh ki kj mr kl km kn ms kp kq kr hb bi translated">我将使用两种情感分析技术。首先，我将使用Vader情绪分析来计算极性分数，这将返回四个分数:正面、中性、负面和复合。在这个分析中，我将忽略复合分数，更多地关注正面、中性和负面分数。</p><p id="19f1" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">对于第二个分析，我将使用NRC意见词典根据每个词的权重或类别来计算歌词。与朴素贝叶斯相似，这种情感分析将每个单词作为独立的特征进行计算，并忽略单词的整个上下文。</p><p id="5680" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated"><strong class="jy hs">维德情绪分析</strong></p><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="d032" class="ll lm hi mg b fi mk ml l mm mn">analyzer = SentimentIntensityAnalyzer()<br/>shore['compound'] = [analyzer.polarity_scores(x)['compound'] for x in shore['clean_lyrics']]<br/>shore['neg'] = [analyzer.polarity_scores(x)['neg'] for x in shore['clean_lyrics']]<br/>shore['neu'] = [analyzer.polarity_scores(x)['neu'] for x in shore['clean_lyrics']]<br/>shore['pos'] = [analyzer.polarity_scores(x)['pos'] for x in shore['clean_lyrics']]</span><span id="12e8" class="ll lm hi mg b fi mw ml l mm mn">sentiment = pd.DataFrame()<br/>sentiment = shore[['title', 'neg', 'neu', 'pos']]<br/>sentiment.plot.bar(x='title', stacked=True, figsize=(9,6), color={'pos': 'green', 'neg': 'red', 'neu': 'blue'})<br/>plt.show()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nb"><img src="../Images/871f5eb14ee94bd948cc071d956411d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*9VwxB2KmDQnA0oxTW-YR1A.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">维德情感分析</figcaption></figure><p id="d4b0" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">我使用分析仪的三个分数。正如你在上面的堆叠条形图上看到的，红色条代表负面情绪的得分，蓝色条代表中性情绪的得分，绿色条代表正面情绪的得分。</p><p id="1f0b" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">Sunblind的积极情绪得分最高，而go-to-the-sun Road的得分最低。相反，宁静空气/Gioia的负面情绪得分最高，而Sunblind，我不是我的季节，我能相信你也有同样低的负面情绪得分。</p><p id="eaaf" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated"><strong class="jy hs"> NRC意见词典</strong></p><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="4562" class="ll lm hi mg b fi mk ml l mm mn">pos_list=set(opinion_lexicon.positive())<br/>neg_list=set(opinion_lexicon.negative())</span><span id="911e" class="ll lm hi mg b fi mw ml l mm mn">def sentiment(sentence):<br/>  senti=0<br/>  words = [word for word in sentence]<br/>  for word in words:<br/>    if word in pos_list:<br/>      senti += 1<br/>    elif word in neg_list:<br/>      senti -= 1<br/>  return senti</span><span id="310e" class="ll lm hi mg b fi mw ml l mm mn">shore['sentiment']=shore['filtered_words'].apply(sentiment)<br/>shore['is_positive'] = shore['sentiment'] &gt; -0<br/>shore[['title', 'sentiment']].plot(x='title', kind='bar',  title='Sentiment for each Title using NRC', figsize=(9,6), legend= False, color=[shore.is_positive.map({True: 'yellow', False: 'red'})])</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es nc"><img src="../Images/95efc6e3105ce3cfe001c0eb70107bb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*8oMdaVDm3HWGVzHkj6spWQ.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">NRC观点词典</figcaption></figure><p id="198d" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">正如你在上面的柱状图中看到的，因为NRC将每个单词作为一个独立的特征进行计算，所以它并不关心单词或歌词本身的上下文。从人类语言学的角度来看，句子中出现的负面分类词越多，NRC就会假设整个句子实际上是负面的，即使实际上不是负面的。</p><p id="ef94" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">NRC意见词典的结果显示，专辑中有10首歌曲被归类为负面情绪较高的歌曲，而其余的歌曲则具有更积极的情绪。消极情绪歌曲包括《宁静的空气/Gioia》、《捷瑞》、《Maestranza》、《我能相信你吗》、《涉过齐腰高的水》以及其他几首歌曲。</p><h2 id="49fb" class="ll lm hi bd ln lo lp lq lr ls lt lu lv kf lw lx ly kj lz ma mb kn mc md me ho bi translated">积极和消极的话</h2><p id="09d3" class="pw-post-body-paragraph jw jx hi jy b jz mo is kb kc mp iv ke kf mq kh ki kj mr kl km kn ms kp kq kr hb bi translated">我最不想做的事情就是对专辑中每首歌里出现的正面和负面词汇进行比较。这种比较实际上是通过对正面词和负面词进行分类，然后用wordcloud可视化这两种分类来建立的。</p><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="6dfd" class="ll lm hi mg b fi mk ml l mm mn"># pos neg words classified<br/>pos_dic = list(pos_list)<br/>neg_dic = list(neg_list)</span><span id="b2b0" class="ll lm hi mg b fi mw ml l mm mn">a = shore['filtered_words']<br/>pos_word = []<br/>neg_word = []<br/>all_words = []<br/>for sublist in a:<br/>    for item in sublist:<br/>        all_words.append(item)</span><span id="a6dc" class="ll lm hi mg b fi mw ml l mm mn">for word in all_words:<br/>    if word in pos_dic:<br/>        pos_word.append(word)<br/>    elif word in neg_dic:<br/>        neg_word.append(word)</span></pre><p id="53d9" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">Wordcloud为<strong class="jy hs">正面词</strong>:</p><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="4be1" class="ll lm hi mg b fi mk ml l mm mn"># wordcloud for positive words<br/>unique_string=(" ").join(pos_word)<br/>wcp = WordCloud(background_color="white",width=2000,height=1000, max_words=1000,relative_scaling=0.5,normalize_plurals=False, colormap="summer").generate(unique_string)<br/>plt.figure(figsize=(10,8))<br/>plt.imshow(wcp)<br/>plt.axis('off')<br/>plt.show()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es na"><img src="../Images/65397e575b5f77e265c64a33e119430c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*mcFCtLqmt8gfvyfJX5N_-Q.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">积极词汇的词云</figcaption></figure><p id="d68f" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">Wordcloud为<strong class="jy hs">否定词</strong>:</p><pre class="jh ji jj jk fd mf mg mh mi aw mj bi"><span id="286a" class="ll lm hi mg b fi mk ml l mm mn"># wordcloud for negative words<br/>unique_string=(" ").join(neg_word)<br/>wcp = WordCloud(background_color="white",width=2000,height=1000, max_words=1000,relative_scaling=0.5,normalize_plurals=False, colormap="RdGy").generate(unique_string)<br/>plt.figure(figsize=(10,8))<br/>plt.imshow(wcp)<br/>plt.axis('off')<br/>plt.show()</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es na"><img src="../Images/b0363fbd653682621bdaf2dbb15edf4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*dwB13lkZ1vbd-Jz0BjXr-A.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">负面词汇的词云</figcaption></figure><p id="92e2" class="pw-post-body-paragraph jw jx hi jy b jz ka is kb kc kd iv ke kf kg kh ki kj kk kl km kn ko kp kq kr hb bi translated">正如你所看到的，狐狸乐队用“错误”、“魔鬼”、“软弱”和“无情”这样的词来描绘他们新专辑的负面。另一方面，像“温暖”、“嗯”、“安静”和“爱”这样的词在他们的新专辑中产生了积极的影响。</p><figure class="jh ji jj jk fd jl"><div class="bz dy l di"><div class="nd ne l"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">狐狸乐队——我能相信你吗(圣安&amp;圣三一教堂现场直播)</figcaption></figure></div></div>    
</body>
</html>