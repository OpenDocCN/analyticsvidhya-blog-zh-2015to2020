<html>
<head>
<title>Text Classification Using Naive Bayes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于朴素贝叶斯的文本分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/text-classification-using-naive-bayes-e889dbf1edd?source=collection_archive---------7-----------------------#2019-10-18">https://medium.com/analytics-vidhya/text-classification-using-naive-bayes-e889dbf1edd?source=collection_archive---------7-----------------------#2019-10-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="51ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我敢打赌，我们大多数人在阅读这篇文章时，都已经解决了一个涉及贝叶斯定理的问题，无论是在不久前还是在遥远的过去。像这样的问题，</p><p id="4ff8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一种致命的疾病被发现了(姑且称之为DiseaseXities)，其诊断测试准确率达99%。众所周知，1%的人口已经患有这种疾病。你已经做了测试，结果呈阳性。你真的得了这种病的可能性有多大？</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/7e9cdfe7299a0f0e749c4f5a9904ed8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YvV8y-HMZfWhUuAN0k2X8g.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">贝叶斯定理</figcaption></figure><p id="b4f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在理清这一点之前，让我们先看看基础知识，然后我们将解决上述问题，然后尝试看看贝叶斯如何成为朴素贝叶斯并用于分类。</p><h2 id="1adc" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">基础知识:三种概率</h2><p id="03bc" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">概率是对事件发生的可能性进行量化的度量。这个事件可以是任何事情，从掷硬币时得到一条尾巴，到掷骰子时得到6。掷硬币或掷骰子，两者都是<strong class="ih hj">实验</strong>，而得到一个<code class="du ku kv kw kx b">tail</code>或6是那些实验的<strong class="ih hj">事件</strong>(一组结果)。</p><p id="8ec7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常<code class="du ku kv kw kx b">X</code>被用作变量来表示事件。由此"<em class="jd">掷骰子得到6的概率是多少？</em>”将被表示为“<strong class="ih hj"> P(X=6) </strong>”。</p><p id="bc85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个实验中所有结果的集合称为该实验的<strong class="ih hj"> <em class="jd">样本空间</em> </strong>。因此，集合<code class="du ku kv kw kx b">{1, 2, 3, 4, 5, 6}</code>是掷骰子的样本空间。形式上，一个<strong class="ih hj"> <em class="jd">事件</em> </strong>是<strong class="ih hj"> <em class="jd">样本空间</em> </strong>的子集。</p><p id="5515" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">概率最简单的形式是</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ky"><img src="../Images/e2b903dd4437c42d1fab14422ea5273e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*9k3HgaLQBMWAlS2itPBBaw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图1:事件E的概率</figcaption></figure><p id="7564" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">边际概率</em> </strong>是一个事件发生的概率，与另一个变量的结果无关。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kz"><img src="../Images/f517c9507cd4c3aa7a1f288840076c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*m8IozU3Bo9qs1OGwPj1-eg.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图2:联合概率</figcaption></figure><p id="280e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一<strong class="ih hj"> <em class="jd">联合概率</em> </strong>是两个或两个以上事件同时发生的概率。<strong class="ih hj">P(X =</strong>T3<strong class="ih hj">)</strong>？？当从一副牌中抽出一张牌时，抽出的牌是<code class="du ku kv kw kx b">Heart.</code>的<code class="du ku kv kw kx b">Ace</code> &amp;的概率是多少</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es la"><img src="../Images/249351b2bb7d2baf57e844a5fb7aefd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*6lyZzUMDu3dA9so7ru63-w.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图3:给定B已经发生的概率</figcaption></figure><p id="cc07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">条件概率</em> </strong>(理解贝叶斯定理的关键要素)是给定某个其他事件已经发生的情况下，某个事件发生的概率。<br/>例如<em class="jd">假设一个罐子里装着5个两种颜色的球(2红3黑)，你挑两个球，发现其中一个是红色的。另一个球也是红色的概率是多少？</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lb"><img src="../Images/84a08e14356cbf24cd568e445276f007.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*wUUaveQMpcXb1dyHxpicvg.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图4</figcaption></figure><p id="2d7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">样本空间= <code class="du ku kv kw kx b">{RR, RB, BB}</code></p><p id="9b54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">P(red|red) = 1/2 ie <code class="du ku kv kw kx b">{RR}/{RR,RB}</code></p><p id="5e12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们考虑一下<strong class="ih hj"> <em class="jd"> P(A|B) </em> </strong>。这里我们特别感兴趣的是<strong class="ih hj"> <em class="jd">一</em> </strong>。由于B已经发生，可能的结果必须来自图4中的阴影区域，将范围限制在A和B共享的区域(蓝色阴影区域),即P(A ∩ B ),根据经典概率理论P =相关/总，因此</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es la"><img src="../Images/ce3ea7bdebd3d660e3cd00c5cd146732.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*p5jhHSMy5KdqfUimYM-Mbw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图5:条件概率。(蓝色区域/黄色区域)</figcaption></figure><p id="0856" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lc"><img src="../Images/f794c10656c2e02c87a12542fb81b603.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*9h7pIJjv_3acNFr4EhzEng.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图6</figcaption></figure><p id="3ba2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样的，</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ld"><img src="../Images/64ce318e32421726207cc28581048d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*XF2ufBk6GjarXO0zKU__UQ.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图7</figcaption></figure><p id="bc22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，从图6和图7可以看出</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es le"><img src="../Images/00afea1ff1195536e0c1cc7ff187498f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*COFZ_P6woW6MbWB-5nWM8w.png"/></div></div></figure><p id="8d55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是贝叶斯定理。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lf"><img src="../Images/5a45451c8bd688d7d000adacf4ec2aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*0JyuHfU_oIEiWDeEtbAaXQ.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图8</figcaption></figure><h2 id="9bb5" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">解决问题</h2><p id="7f92" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">既然我们已经得出了公式，让我们用它来解决上面的问题。让我们再次重申这个问题</p><p id="9d8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">一种致命的疾病被发现了(姑且称之为DiseaseXities)，其诊断测试准确率达99%。众所周知，1%的人口已经患有这种疾病。你已经做了测试，结果呈阳性。你真的得了这种病的可能性有多大？</em></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lg"><img src="../Images/6d4892ad1b45103a7bb03f21608a9990.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*EjZ4aB4gvRFhh5JK70p7zA.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图9:图像src:<a class="ae lh" href="https://brilliant.org/wiki/bayes-theorem/" rel="noopener ugc nofollow" target="_blank">https://brilliant.org/wiki/bayes-theorem/</a></figcaption></figure><p id="476e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们用数学方法来表述这个问题。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es li"><img src="../Images/86329aa573debf09ef5da40351a34587.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*zNhPIMZZt-sugoQSerV9dg.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图10</figcaption></figure><p id="4096" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于该测试有99%的准确性——受感染的人有99%的时间被检测为阳性，而对于健康的人，该测试有99%的时间被检测为阴性。众所周知，1%的人口患有这种疾病。因此，一个随机的人患有疾病P(+ve)的概率是</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lj"><img src="../Images/60edf275075253caf55d1fd1eb1d08df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*huxXrQ5IhUCSJ6yflYfSqQ.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图11</figcaption></figure><p id="c74c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为测试有99%的准确性</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lk"><img src="../Images/2c5a8bd872cc227eb375bbaa43ef6e54.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*MLTTaGKuizsdJ8kXSX-wxw.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图12</figcaption></figure><p id="096c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据贝叶斯定理</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ll"><img src="../Images/1e4bdb05ce1abd5eb760caca0e3459c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JY2fKLhfhP6DDUyu3tZVqQ.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图13</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lm"><img src="../Images/20c22df4cb32e7369cad15e6c25fbf1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nDnsmZtPi-GnM7TPHvbJBg.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图14</figcaption></figure><p id="83df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，你得这种病的可能性是50%。</p></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h2 id="0e08" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">文本分类和情感分析</h2><p id="6a75" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">文本分类</em> </strong>将预定义的类别分配给文本文档的任务。情感分析可以被认为是文本分类中的一个领域，其重点在于识别文本中的主观信息，并将每条数据分类为正面、负面或中性。通常被称为<strong class="ih hj">意见挖掘</strong>。</p><p id="b4c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">文本分类的一个经典例子是检测电子邮件是否是垃圾邮件。在这里，我们将使用餐馆评论样本。</p><p id="ca00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们继续讨论之前，让我们再来看看贝叶斯定理，不过符号有所改变。</p><p id="ed10" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用数学术语来说，文本分类的问题是</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lu"><img src="../Images/abc2935234c19730cbfa540159545192.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z8IEgRDtYYBFJ99uR3Q9UA.png"/></div></div></figure><p id="ad5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">应用贝叶斯定理，我们得到</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lv"><img src="../Images/a8152ef4ccccee7cf3c520191398f86f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lj-pO7eLPKkAHNXpO5gYBw.png"/></div></div></figure><p id="bb78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为分母在所有项中都是相同的，所以明智的做法是将其写成</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lw"><img src="../Images/affdf54bdd08bdc8999b44cd1b7d66cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mOGLBu_uMZxkaBdGdBxEmQ.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lx"><img src="../Images/3f50adb9ff9fca4520b27f050aca6b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*53GfeP13xGRDd5GHmRznGA.png"/></div></div></figure><p id="4e83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于单个术语，必须计算P(H)先验和P(D|H)在假设下看到文本的概率。先验帮助你把假设的先验信念。P(+ve)，任何新评论是正面评论的概率。因此</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ly"><img src="../Images/780235007207201a312996d45eeb4aa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NKIQa_jb9vPg98YN6PeDCQ.png"/></div></div></figure><p id="6c3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其他类别也是如此。现在我们有了获得先验的方法，让我们找出在特定假设下获得文本的概率。p(文本|假设)。</p><p id="676b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果说一个句子中的单词不相互依赖，那就太天真了。但是，我们在计算P(text|Hypothesis)时也做了同样的假设。因而得名<strong class="ih hj"> <em class="jd">朴素贝叶斯</em> </strong>。</p><blockquote class="lz"><p id="2cea" class="ma mb hi bd mc md me mf mg mh mi jc dx translated">假设特性是独立的，是非常天真的。因此有了朴素贝叶斯这个名字。</p></blockquote><p id="45bb" class="pw-post-body-paragraph if ig hi ih b ii mj ik il im mk io ip iq ml is it iu mm iw ix iy mn ja jb jc hb bi translated">现在在<em class="jd">天真的假设下，</em> P(Text|Hypothesis)是</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mo"><img src="../Images/3a8c7f8cda501e1daac04386f3f0319c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cRxEwwyQgZdXwrLmQrBleg.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">P(A ∩ B) = P(A) * P(B)如果A和B是独立的</figcaption></figure><p id="4b51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您现在知道了朴素贝叶斯分类器是如何工作的。</p></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h2 id="7aab" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">弄脏我们的手</h2><p id="7aa3" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">现在，让我们尝试将我们的知识用于构建一个分类器，我们将在真实数据集上训练该分类器。</p><p id="19e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Yelp评论数据集随时可用，只需一次谷歌搜索。要获得数据的概述，您可以参考本<a class="ae lh" href="https://www.kaggle.com/suzanaiacob/sentiment-analysis-of-the-yelp-reviews-data/data" rel="noopener ugc nofollow" target="_blank"> kaggle </a>笔记本中的探索性数据分析(EDA)部分。</p><p id="cf72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">评论数据包含许多列，但是，我们将只挑选相关的列，即<code class="du ku kv kw kx b">text</code>和<code class="du ku kv kw kx b">stars</code>。<code class="du ku kv kw kx b">text</code>包含来自用户的个人评论以及他们的<code class="du ku kv kw kx b">stars</code>评分(1到5分)。为了简化任务，可以对恒星进行重新标记。星数大于3标记为3(正)，星数小于3标记为1(负)，3标记为2(中性)。让我们看看个人评论是什么样的。</p><pre class="jf jg jh ji fd mp kx mq mr aw ms bi"><span id="439d" class="ju jv hi kx b fi mt mu l mv mw"><em class="jd">“Love the staff, love the meat, love the place. Prepare for a long line around lunch or dinner hours. \n\nThey ask you how you want you meat, lean or something maybe, I can’t remember. Just say you don’t want it too fatty. \n\nGet a half sour pickle and a hot pepper. Hand cut french fries too.”</em></span></pre><p id="4e03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们首先将文本转换成小写，然后删除停用词。将单词规范化也是明智的，单词规范化就像词干化(将单词简化为它的<strong class="ih hj"> <em class="jd">词干</em> </strong>)，词汇化(将两个单词简化为它的词根，单词<code class="du ku kv kw kx b">is</code>、<code class="du ku kv kw kx b">am,</code> <code class="du ku kv kw kx b">are</code>属于同一个词根<code class="du ku kv kw kx b">be.</code></p><p id="bd0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与字符串相比，处理数字更快。我们将把文本内容转换成数字特征向量。<strong class="ih hj"> <em class="jd">词袋</em> </strong>模型通常用在文档分类方法中，其中每个词的(出现频率)被用作训练分类器的特征。请参考<a class="ae lh" href="https://stackabuse.com/python-for-nlp-creating-bag-of-words-model-from-scratch/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>上的包话。</p><p id="b824" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简而言之，单词包是两个数据结构的组合，一个包含语料库(所有文档/文本中)中的单词列表及其索引，另一个是矩阵M，其中M[i，j]存储第<code class="du ku kv kw kx b">j</code>个文档中第<code class="du ku kv kw kx b">j</code>个单词(在第一个ds中以j作为索引的单词)的计数。第一个数据结构(即单词列表)很容易包含数千个单词，然而，一个句子很少超过100个单词。因此，使得矩阵非常稀疏，因为大多数条目将为零。</p><p id="40c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du ku kv kw kx b">scipy.sparse</code>矩阵是数据结构，存储相同的信息非常容易(内存优化)，并且<code class="du ku kv kw kx b">scikit-learn</code>内置了对这些结构的支持。</p><pre class="jf jg jh ji fd mp kx mq mr aw ms bi"><span id="e93a" class="ju jv hi kx b fi mt mu l mv mw"># train_df is a pandas dataframe containing "text", "stars" as<br/># columns (stars being revised stars positive, neutral or negative)</span><span id="469d" class="ju jv hi kx b fi mx mu l mv mw"># test_df is similar to the train_df, however it will be used for <br/># testing(evaluating the classifier)</span><span id="b366" class="ju jv hi kx b fi mx mu l mv mw">from sklearn.feature_extraction.text import CountVectorizer<br/>bog_d = CountVectorizer()<br/>bog_m = bog_d.fit_transform(train_df.text.get_values())</span><span id="3479" class="ju jv hi kx b fi mx mu l mv mw"># get the index of the word good<br/>&gt;&gt;&gt; bog_d.vocabulary_.get(u"good")<br/>&gt;&gt;&gt; 85492</span><span id="2fb9" class="ju jv hi kx b fi mx mu l mv mw"># thus the number 85492 represents "good"</span></pre><p id="64ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果所有文档的字数或多或少相同，则出现次数(频率)是好的。然而，评论文本长度变化很大。对于较长的文档，它将比较短的文档具有更高的平均计数值，即使它们可能谈论相同的主题。</p><p id="b5ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了避免这些潜在的差异，将文档中每个单词的出现次数除以文档中单词的总数就足够了:这些新特征被称为<strong class="ih hj">词频(tf) </strong>。</p><p id="b7bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<code class="du ku kv kw kx b">tf</code>之上的另一个改进是降低出现在语料库的许多文档中的单词的权重，因此比那些只出现在语料库的一小部分中的单词信息量少。这种缩减被称为<strong class="ih hj">TF–IDF</strong>，表示<strong class="ih hj">“术语频率乘以逆文档频率”。</strong></p><p id="04ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以让我们找到<code class="du ku kv kw kx b">tf-idfs.</code></p><pre class="jf jg jh ji fd mp kx mq mr aw ms bi"><span id="26f3" class="ju jv hi kx b fi mt mu l mv mw">from sklearn.feature_extraction.text import TfidfTransformer<br/>tfidf_transformer = TfidfTransformer()<br/>train_tfidf = tfidf_transformer.fit_transform(bog_m)</span></pre><p id="dbc5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有了正确格式的特征，让我们训练分类器。</p><pre class="jf jg jh ji fd mp kx mq mr aw ms bi"><span id="ea46" class="ju jv hi kx b fi mt mu l mv mw">from sklearn.naive_bayes import MultinomialNB</span><span id="097f" class="ju jv hi kx b fi mx mu l mv mw">clf = MultinomialNB().fit(train_tfidf, train_df.stars.get_values())</span></pre><p id="d151" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是的，用<code class="du ku kv kw kx b">sklearn</code>训练分类器就是这么简单。</p><p id="5b34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们来测试一下。</p><pre class="jf jg jh ji fd mp kx mq mr aw ms bi"><span id="6346" class="ju jv hi kx b fi mt mu l mv mw">bog_tm = bog_d.transform(test_texts.get_values())<br/>test_tfidf = tfidf_transformer.transform(bog_tm)</span><span id="cf6e" class="ju jv hi kx b fi mx mu l mv mw">predicted = clf.predict(test_tfidf)</span><span id="adf8" class="ju jv hi kx b fi mx mu l mv mw">print("the accuracy of the classifier is", <br/>             np.mean(predicted == test_targets))</span></pre><p id="be13" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">朴素贝叶斯的一个显著特征是它在训练时快如闪电。超级好理解。</p></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><h2 id="9117" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">引用和参考文献</h2><ol class=""><li id="8fbc" class="my mz hi ih b ii kp im kq iq na iu nb iy nc jc nd ne nf ng bi translated"><a class="ae lh" href="https://brilliant.org/wiki/bayes-theorem/" rel="noopener ugc nofollow" target="_blank">https://brilliant.org/wiki/bayes-theorem/</a></li><li id="4016" class="my mz hi ih b ii nh im ni iq nj iu nk iy nl jc nd ne nf ng bi translated"><a class="ae lh" href="https://en.wikipedia.org/wiki/Probability" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Probability</a></li><li id="5465" class="my mz hi ih b ii nh im ni iq nj iu nk iy nl jc nd ne nf ng bi translated"><a class="ae lh" href="https://stackabuse.com/python-for-nlp-creating-bag-of-words-model-from-scratch/" rel="noopener ugc nofollow" target="_blank">https://stack abuse . com/python-for-NLP-creating-bag-of-words-model-from-scratch/</a></li></ol></div><div class="ab cl ln lo gp lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="hb hc hd he hf"><p id="600d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">特别感谢<a class="nm nn ge" href="https://medium.com/u/2fbc374a08b5?source=post_page-----e889dbf1edd--------------------------------" rel="noopener" target="_blank">prajval gurumurthy</a>校对了这篇文章。</p></div></div>    
</body>
</html>