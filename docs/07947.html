<html>
<head>
<title>Gradient Descent — A deep dive</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降——深度潜水</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-a-deep-dive-355135841269?source=collection_archive---------25-----------------------#2020-07-12">https://medium.com/analytics-vidhya/gradient-descent-a-deep-dive-355135841269?source=collection_archive---------25-----------------------#2020-07-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="0c09" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">深入探讨最重要的算法梯度下降与实际例子使用JAX。</h2></div><p id="91b0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最初，当我接触这种算法时，我花了一些时间来回答以下问题:</p><blockquote class="jt ju jv"><p id="b1cc" class="ix iy jw iz b ja jb ij jc jd je im jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">梯度下降算法是如何工作的？</p><p id="fcac" class="ix iy jw iz b ja jb ij jc jd je im jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">梯度是什么意思？</p><p id="a094" class="ix iy jw iz b ja jb ij jc jd je im jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">如何计算一个梯度？</p><p id="3891" class="ix iy jw iz b ja jb ij jc jd je im jf jx jh ji jj jy jl jm jn jz jp jq jr js hb bi translated">参数是如何学习的，梯度对学习有什么帮助？</p></blockquote><p id="4c57" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">理论上，我明白它是做什么的，但我找不到一个足够好的实际例子来让我理解和回答上述问题。有人向我介绍了JAX库，它可以帮助我计算梯度。我想放一个玩具示例，用它来做实验，以便更好地理解算法。</p><p id="dc9e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下文章的结构是这样的，它首先解释梯度下降算法及其变体。稍后它以一个使用<a class="ae ka" href="https://github.com/google/jax" rel="noopener ugc nofollow" target="_blank"> JAX </a>的实际例子结束。在实际例子中，我试图展示梯度下降算法如何工作以及网络如何学习参数。让我们开始吧。</p><h1 id="310a" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated"><strong class="ak">梯度下降算法—理论</strong></h1><p id="2596" class="pw-post-body-paragraph ix iy hi iz b ja kt ij jc jd ku im jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">为了使用神经网络来解决现实生活中的问题，神经网络必须利用给定的数据自己学习参数。为此，使用了学习算法。</p><p id="ca85" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">考虑<strong class="iz hj">预测房价</strong>的任务，其中网络的任务是输出与房屋相关的特定信息的价格，例如房屋的大小、房屋的位置等。向网络提供数据<code class="du ky kz la lb b">x</code>和<code class="du ky kz la lb b">y</code>。输入的<code class="du ky kz la lb b">x</code>是与房屋相关的信息，<code class="du ky kz la lb b">y</code>是其相应的价格。</p><p id="dde3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里，网络必须通过学习映射函数将<code class="du ky kz la lb b">x</code>映射到<code class="du ky kz la lb b">y</code>。映射函数具有作为参数的权重，并且这些参数必须使用优化器来学习。随着计算单元(神经元)数量的增加，网络将有更多的参数。首先，用随机值初始化权重，并且在训练期间，使用诸如梯度下降优化器的优化器来学习权重。</p><p id="65b5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们考虑一个例子，其中我们的网络具有以下参数:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es lc"><img src="../Images/8dac9f6027dec6934e0efef0e05ae598.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*t6HmtXmuXUacmeKwdf3EwA.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">网络参数</figcaption></figure><p id="69bb" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了提供更好的结果，必须学习这些参数。输入为<code class="du ky kz la lb b">x </code>，输出为<code class="du ky kz la lb b">y </code>。如果网络给出输出<code class="du ky kz la lb b">y'</code>，我们可以通过实现一个损失函数来计算网络的表现。我们可以使用不同的损失函数，但是让我们考虑使用均方误差(MSE)。使用这个函数，我们可以计算我们的网络在预测房价时产生的误差。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es lo"><img src="../Images/2a235d9a4aa117e38924cee5085a5ff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*TXRqteJvmpKHVg6QSK9IbQ.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">均方差[1]</figcaption></figure><p id="b4ce" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">计算MSE的代码如下所示:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="26cf" class="lt kc hi lb b fi lu lv l lw lx"># preds = prediction made by our model. targets = actual ground #truth. n = number of samples.</span><span id="895f" class="lt kc hi lb b fi ly lv l lw lx">loss = np.sum((preds - targets) ** 2) / n</span></pre><p id="e46e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里<code class="du ky kz la lb b"><em class="jw">n </em></code>表示样本的数量。优化器的任务是尽可能减少误差，为了达到这个目的，我们对误差函数<code class="du ky kz la lb b">E</code>进行偏导数。这用符号∇表示，这叫做梯度。对每个参数取偏导数，如下所示:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es lz"><img src="../Images/a85045a9f8cbca73377d3fbdee17bc40.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*XD7gdE1McT0SwEQKBOolRw.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">偏导数</figcaption></figure><p id="69a8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用计算出的梯度，然后如下所示更新每个参数:</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es ma"><img src="../Images/47762760bdf85bffdbf11f34a7969668.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*af4HL6f6UeJx6T-HlqbXWw.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">梯度下降优化器[4]</figcaption></figure><p id="e49c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以看到，时间<code class="du ky kz la lb b">t + 1</code>处的参数<code class="du ky kz la lb b">w</code>将基于时间t处的误差函数的偏导数进行更新。α表示参数将被学习的学习速率。学习率是超参数之一。在开始实验之前设置的参数称为超参数。在开始实验之前，必须确定学习速率和优化器的选择。</p><p id="4804" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">用于更新该参数的代码将如下所示:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="0d31" class="lt kc hi lb b fi lu lv l lw lx"># here the parameter W will be updated with the gradient and learning<br/>#rate. for example the learning rate here is 0.01.<br/>W = W - ((0.01) * W_grad)</span></pre><p id="6393" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">梯度下降优化器的性能很大程度上取决于它用来一次性优化误差函数的数据量。根据梯度下降优化器处理的数据量，有三种不同的方法来使用它。批量梯度下降、随机梯度下降和小批量梯度下降是变体[3]。</p><h1 id="f84a" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated"><strong class="ak">批量梯度下降</strong></h1><p id="0393" class="pw-post-body-paragraph ix iy hi iz b ja kt ij jc jd ku im jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">这种变体也称为香草梯度下降。在这个变体中，考虑整个训练数据集来最小化误差函数和计算梯度。由于整个训练数据集仅被考虑用于参数的一次更新，这带来了时间和存储器问题。更新参数需要很多时间，此外，对存储器的依赖性很高，这使得很难在存储器中容纳整个训练数据集。它不能用于在线学习，因为更新发生在整个训练数据集，而不是单个训练数据[3]。</p><h1 id="8baf" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated"><strong class="ak">随机梯度下降</strong></h1><p id="c0c1" class="pw-post-body-paragraph ix iy hi iz b ja kt ij jc jd ku im jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">该变体更新每个训练数据的参数，因此也可以用于在线训练。这种变体速度更快，并且不需要大量内存，因为在计算误差函数的梯度时，内存中只有单个训练数据[3]。一方面，这种变体是有希望的，但另一方面，可以确定误差收敛到局部/全局最小值是不稳定的[3]。如果学习率继续下降，这种变体似乎达到了与普通梯度下降相同的行为[3]。</p><h1 id="382e" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated"><strong class="ak">小批量梯度下降</strong></h1><p id="0a44" class="pw-post-body-paragraph ix iy hi iz b ja kt ij jc jd ku im jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">它使用了随机和普通梯度下降算法的优点。它最小化了小批量训练数据集的误差函数。通常，小批量由这些值中的<code class="du ky kz la lb b">8,32,64,.. and so on</code>组成。因此，不稳定收敛的问题得到解决，因为参数不是针对单个训练数据而是针对小型训练数据集更新的。它确实给出了与普通梯度下降[3]相似的收敛行为。</p><h1 id="86f0" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated"><strong class="ak">梯度下降算法——在实践中</strong></h1><p id="c882" class="pw-post-body-paragraph ix iy hi iz b ja kt ij jc jd ku im jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">现在，我们可以在实践中检查梯度如何帮助参数以正确的方式学习。我觉得用一个实际的例子来形象化我们所学的理论更好。</p><p id="ddb2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">考虑下面的玩具数据集[2]。</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="f4c6" class="lt kc hi lb b fi lu lv l lw lx">xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)</span><span id="0083" class="lt kc hi lb b fi ly lv l lw lx">ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)</span></pre><p id="e091" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面的数据集可以通过一个简单的函数<code class="du ky kz la lb b">y=2x-1</code>进行映射。这是一个线性函数。如果我们想预测给定的<code class="du ky kz la lb b">x</code>的<code class="du ky kz la lb b">y</code>,‘预测’函数看起来像这样:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="caba" class="lt kc hi lb b fi lu lv l lw lx"># From the function y = 2x - 1. Here x = W and -1 is b. A linear function.<br/>def predict(W, b, inputs):<br/>   return (inputs * W) + b</span></pre><p id="65d5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，该函数在给定参数<code class="du ky kz la lb b">w</code>和<code class="du ky kz la lb b">b</code>的情况下预测<code class="du ky kz la lb b">inputs</code>的结果。我们希望我们的预测精确到与函数<code class="du ky kz la lb b">y=2x-1</code>相匹配。</p><p id="169a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，让我们创建损失函数，即MSE函数:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="c1fb" class="lt kc hi lb b fi lu lv l lw lx"># Here W and b are the parameters and 6 is actually n. From our toy # example you can see that n is 6 for our dataset.<br/>def loss(W, b):<br/>   preds = predict(W, b, inputs)<br/>   loss = np.sum((preds - targets) ** 2) / 6<br/>   return loss</span></pre><p id="8d80" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们画出损失函数。如前所述<code class="du ky kz la lb b">inputs</code>就是我们的<code class="du ky kz la lb b">xs</code>。为了简单起见，我们将保持<code class="du ky kz la lb b">b</code>与<code class="du ky kz la lb b">-1</code>不变:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="9b09" class="lt kc hi lb b fi lu lv l lw lx">import matplotlib.pyplot as plt<br/>import numpy as np</span><span id="dcc1" class="lt kc hi lb b fi ly lv l lw lx"># let us check for differnt values of W how our loss function looks like.<br/>w = np.asarray([-8, -3, -2, -1, 0, 1, 2, 3, 8, 10])<br/>l = []</span><span id="b993" class="lt kc hi lb b fi ly lv l lw lx">for i in range(10):<br/>   # Keeping b constant to -1. <br/>   l.append(loss(w[i], -1))</span><span id="7881" class="lt kc hi lb b fi ly lv l lw lx">plt.plot(w, l, ‘-r’, label=’Loss Function’)<br/>plt.title(‘Graph of Loss Function’)<br/>plt.xlabel(‘Weight’, color=’#1C2833')<br/>plt.ylabel(‘Loss’, color=’#1C2833')<br/>plt.legend(loc=’upper left’)<br/>plt.grid()<br/>plt.show()</span></pre><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mb"><img src="../Images/eed8532461304e204fe6fc0ac22403f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*STMKR9jeq7aV6Crd6allVA.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图—损失函数W = (-8，10)和b = -1</figcaption></figure><p id="0540" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">保持<code class="du ky kz la lb b">b</code>恒定在<code class="du ky kz la lb b">-1</code>并取<code class="du ky kz la lb b">W</code>的范围<code class="du ky kz la lb b">(-8,10)</code>，我们得到上面的损失函数图。从图中可以清楚的看到，如果<code class="du ky kz la lb b">W</code>是<code class="du ky kz la lb b">2</code>而<code class="du ky kz la lb b">b</code>是<code class="du ky kz la lb b">-1</code>；我们得到最小的损失<code class="du ky kz la lb b">0</code>。</p><p id="7bf2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以通过直接传递给形成<code class="du ky kz la lb b">y=2x-1</code>的损失函数<code class="du ky kz la lb b">(2,1)</code>来证明；我们将收到的损失称为<code class="du ky kz la lb b">0</code>。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mc"><img src="../Images/638e1d6b2a42e228f748df23aafb46b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*wnRmUXz1ikROKuDrqPet0g.png"/></div></figure><p id="31fa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上面的图像和情节，你可以得到一个我们想要实现的想法。我们想学习参数<code class="du ky kz la lb b">W</code>和<code class="du ky kz la lb b">b</code>，这样我们接收到的损失为<code class="du ky kz la lb b">0</code>。最符合我们数据集的参数值是<code class="du ky kz la lb b">(2, -1)</code>。这是学习的目标，以确定最适合数据集的参数。</p><blockquote class="md"><p id="54ba" class="me mf hi bd mg mh mi mj mk ml mm js dx translated">为简单起见，我们将保持参数<code class="du ky kz la lb b">b</code>恒定为<code class="du ky kz la lb b">-1</code>，并通过梯度下降算法学习参数<code class="du ky kz la lb b">W</code>。</p></blockquote><p id="e19e" class="pw-post-body-paragraph ix iy hi iz b ja mn ij jc jd mo im jf jg mp ji jj jk mq jm jn jo mr jq jr js hb bi translated">现在，一旦我们明确了我们的目标，我们将看看如何通过编程来实现它。</p><p id="d26e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们的计算机中没有JAX软件包，我们将安装它:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="f25d" class="lt kc hi lb b fi lu lv l lw lx">pip install --upgrade jax jaxlib</span></pre><p id="bad0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">弄清楚我们的进口:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="ea93" class="lt kc hi lb b fi lu lv l lw lx">import numpy as np<br/>import jax.numpy as np<br/>from jax import grad<br/>from jax import value_and_grad</span></pre><p id="59d9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">初始化我们的玩具数据集:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="b97b" class="lt kc hi lb b fi lu lv l lw lx">xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)</span><span id="07d6" class="lt kc hi lb b fi ly lv l lw lx">ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)</span></pre><p id="49e2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">创建<code class="du ky kz la lb b">predict</code>、<code class="du ky kz la lb b">loss</code>功能。初始化参数<code class="du ky kz la lb b">W</code>和<code class="du ky kz la lb b">b</code>。如前所述，我们将保持参数<code class="du ky kz la lb b">b</code>恒定为<code class="du ky kz la lb b">-1</code>。我们将参数<code class="du ky kz la lb b">W</code>初始化为<code class="du ky kz la lb b">8</code>，并使用梯度下降算法使其收敛到<code class="du ky kz la lb b">2</code>:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="1415" class="lt kc hi lb b fi lu lv l lw lx">def predict(W, b, inputs):<br/>   return (inputs * W) + b</span><span id="45f3" class="lt kc hi lb b fi ly lv l lw lx"># Setting the inputs and targets.<br/>inputs = xs<br/>targets = ys</span><span id="faea" class="lt kc hi lb b fi ly lv l lw lx"># Creating the loss function. n = 6 as our dataset as 6 elements.<br/>def loss(W, b):<br/>   preds = predict(W, b, inputs)<br/>   loss = np.sum((preds - targets) ** 2) / 6<br/>   return loss</span><span id="b631" class="lt kc hi lb b fi ly lv l lw lx"># Finally initialize our parameters.<br/># Initalizing W to 8.<br/>W = numpy.array(8, dtype=numpy.float32)<br/># keeping b constant to -1.<br/>b = numpy.array(-1, dtype=numpy.float32)</span></pre><p id="88c3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面的代码是代码中最重要的部分。我们现在将计算相对于<code class="du ky kz la lb b">W</code>的梯度，并用新值更新参数。我们将在一个循环中重复这样做:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="7036" class="lt kc hi lb b fi lu lv l lw lx"># Keeping the W parameter history.<br/>W_array = []<br/># Keeping the loss history<br/>L_array = []</span><span id="d05a" class="lt kc hi lb b fi ly lv l lw lx"># Looping 50 times.<br/>for i in range(50):<br/>  print('Loop starts -----------------------------------')<br/>  print('W_old', W)<br/>  # Appending the W to a list for visualization later.<br/>  W_array.append(W)<br/>  # Calculting the gradient with respect to W.<br/>  loss_value, W_grad = value_and_grad(loss, 0)(W, b)<br/>  L_array.append(loss_value)</span><span id="ed7b" class="lt kc hi lb b fi ly lv l lw lx">if loss_value != 0:<br/>    # learning rate is 0.01.<br/>    W = W - ((0.01) * W_grad)<br/>    <br/>    print('W_new', W)<br/>    print('loss is ', loss_value)<br/>    print('Loop ends ------------------------------------')<br/>  else:<br/>    print('W', W)<br/>    print('loss is ', loss_value)<br/>    print('Loop ends ------------------------------------')<br/>    break</span></pre><p id="031a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">API <code class="du ky kz la lb b">value_and_grad</code>为我们提供了损失函数w.r.t <code class="du ky kz la lb b">W</code>的梯度。它还为我们提供了损失值:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="6677" class="lt kc hi lb b fi lu lv l lw lx"># Calculting the gradient with respect to W.<br/>loss_value, W_grad = value_and_grad(loss, 0)(W, b)</span></pre><p id="9b42" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此<code class="du ky kz la lb b">W_grad</code>用于更新参数:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="ddae" class="lt kc hi lb b fi lu lv l lw lx">W = W - ((0.01) * W_grad)</span></pre><p id="6bc1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">就这么简单。现在让我们看看如何接收输出。为了清楚起见，我将只显示前3个循环结果和后3个循环结果。</p><p id="1537" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">前三个循环的结果如下:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="7976" class="lt kc hi lb b fi lu lv l lw lx">Loop starts -----------------------------------<br/>W_old 8.0<br/>W_new 7.38<br/>loss is  186.0<br/>Loop ends ------------------------------------<br/>Loop starts -----------------------------------<br/>W_old 7.38<br/>W_new 6.8240666<br/>loss is  149.54607<br/>Loop ends ------------------------------------<br/>Loop starts -----------------------------------<br/>W_old 6.8240666<br/>W_new 6.3255796<br/>loss is  120.236694<br/>Loop ends ------------------------------------</span></pre><p id="c1d9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du ky kz la lb b">W</code>以值<code class="du ky kz la lb b">8</code>开始，经过三个循环后，它达到值<code class="du ky kz la lb b">6.32</code>。损失值从<code class="du ky kz la lb b">186.0</code>开始，三个循环后损失达到<code class="du ky kz la lb b">120.23</code>。可以看到两者都有减少的趋势。</p><p id="ad41" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们看看最后三个循环:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="24ac" class="lt kc hi lb b fi lu lv l lw lx">Loop starts -----------------------------------<br/>W_old 2.0356295<br/>W_new 2.0319479<br/>loss is  0.0065588956<br/>Loop ends ------------------------------------<br/>Loop starts -----------------------------------<br/>W_old 2.0319479<br/>W_new 2.0286465<br/>loss is  0.005273429<br/>Loop ends ------------------------------------<br/>Loop starts -----------------------------------<br/>W_old 2.0286465<br/>W_new 2.0256863<br/>loss is  0.004239871<br/>Loop ends ------------------------------------</span></pre><p id="72f7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里<code class="du ky kz la lb b">W</code>趋向于达到<code class="du ky kz la lb b">2</code>的值，损失趋向于达到<code class="du ky kz la lb b">0</code>的值。</p><p id="e164" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的参数已经被学习了。我们已经到达了从<code class="du ky kz la lb b">8</code>开始的点，现在到达了参数<code class="du ky kz la lb b">W</code>的<code class="du ky kz la lb b">2.025</code>。这就是梯度下降算法如何帮助学习参数并减少损失。</p><p id="7cd4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们的学习过程更加清晰的最后一个情节。下图显示了学习是如何进行的。<strong class="iz hj">红色</strong>线显示<code class="du ky kz la lb b">loss</code>功能，而<strong class="iz hj">蓝色</strong>线显示参数<code class="du ky kz la lb b">W</code>的学习过程:</p><pre class="ld le lf lg fd lp lb lq lr aw ls bi"><span id="b690" class="lt kc hi lb b fi lu lv l lw lx">import matplotlib.pyplot as plt<br/>import numpy as np<br/>w = np.asarray([-3, -2, -1, 0, 1, 2, 3])<br/>l = []</span><span id="5aa4" class="lt kc hi lb b fi ly lv l lw lx">for i in range(7):<br/>  l.append(loss(w[i], -1))</span><span id="e7d8" class="lt kc hi lb b fi ly lv l lw lx">plt.plot(w, l, '-r', label='Loss Function')<br/>plt.plot(W_array, L_array, '-b', label='Parameter learning')<br/>plt.title('Plot learning and loss function')<br/>plt.xlabel('Weight', color='#1C2833')<br/>plt.ylabel('Loss', color='#1C2833')<br/>plt.legend(loc='upper left')<br/>plt.grid()<br/>plt.show()</span></pre><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es ms"><img src="../Images/38110558bb5eee9e19ac1ec903d08706.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*FkU7RZkWnzn3-VWihFptyg.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">学习达到了0的损失。</figcaption></figure><p id="c6e1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇博客用一个实例展示并解释了梯度下降算法。我也上传了代码到Github上。随意试验不同的参数初始值。</p><p id="87b7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我希望你在创作这些内容的时候和我一样喜欢阅读。我感谢你的时间，并对你如何喜欢这篇文章给予反馈，这让我有动力写更多这些。</p><p id="15ac" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">参考资料:</p><ol class=""><li id="25aa" class="mt mu hi iz b ja jb jd je jg mv jk mw jo mx js my mz na nb bi translated"><a class="ae ka" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Mean_squared_error</a></li><li id="7741" class="mt mu hi iz b ja nc jd nd jg ne jk nf jo ng js my mz na nb bi translated"><a class="ae ka" href="https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%202%20-%20Lesson%202%20-%20Notebook.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/lmor oney/dlai Course/blob/master/Course % 201% 20-% 20 part % 202% 20-% 20 lesson % 202% 20-% 20 note book . ipynb</a></li><li id="59e6" class="mt mu hi iz b ja nc jd nd jg ne jk nf jo ng js my mz na nb bi translated">塞巴斯蒂安·鲁德。“梯度下降优化算法概述”。载于:CoRR abs/1609.04747 (2016年)。网址:<a class="ae ka" href="http://arxiv.org/abs/" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/</a>1609.04747。</li><li id="d053" class="mt mu hi iz b ja nc jd nd jg ne jk nf jo ng js my mz na nb bi translated">Marcin Andrychowicz等《通过梯度下降学习梯度下降学习》。载于:CoRR abs/1606.04474 (2016年)。网址:http://arxiv . org/ABS/1606.04474。</li></ol></div></div>    
</body>
</html>