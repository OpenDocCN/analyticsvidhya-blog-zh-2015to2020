<html>
<head>
<title>Why are LSTMs struggling to matchup with Transformers?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么LSTMs苦苦与变形金刚对决？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/why-are-lstms-struggling-to-matchup-with-transformers-a1cc5b2557e3?source=collection_archive---------2-----------------------#2020-10-11">https://medium.com/analytics-vidhya/why-are-lstms-struggling-to-matchup-with-transformers-a1cc5b2557e3?source=collection_archive---------2-----------------------#2020-10-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/20aff1ef842a1c4397ba05972eb2169b.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*a2y4YKLPebiJewIzABixOg.png"/></div></figure><p id="2252" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这篇文章揭示了长短期记忆(LSTM)和变压器网络的性能。我们将从了解LSTM和变形金刚的信息开始，然后继续了解它们工作的内部机制。让我们来了解一下幕后发生了什么，以及为什么《变形金刚》能够比《LSTM》表现得更好。</p><h1 id="d26c" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">什么是RNN，它是如何工作的？</h1><p id="bfa9" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">在学习LSTM网络之前，让我们知道递归神经网络(RNN)是如何工作的。RNN是一种神经网络，前一时间步的输出作为输入输入到当前时间步。与前馈神经网络不同，rnn可以使用其内部状态(记忆)来处理输入序列。因此，神经rnn擅长对序列数据建模。</p><p id="6caf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在传统的神经网络中，所有的输入和输出是相互独立的；而在需要连续信息来预测句子的下一个单词的情况下，需要前面的单词，因此需要记住前面的单词。于是，RNN诞生了。他们在循环结构的帮助下解决了这个问题。RNN最主要也是最重要的特征是“隐藏”状态，这种状态能记住关于序列的信息。这使得它们适用于诸如未分段的、连接的手写识别或语音识别之类的任务。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es kn"><img src="../Images/765bb856f436bbc6303fcf258ed8f5f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*zxAM1SWse00x7B228-d_jg.jpeg"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">来源:colah的博客</figcaption></figure><p id="4fb8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这种循环结构允许神经网络接受输入序列。如果你看到展开的版本，你会更好地理解它。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es kw"><img src="../Images/c8b8bef929fa04a824b5a137a04d8ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DZEDYZrsqW3N3M4eijQIZg.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">来源:colah的博客</figcaption></figure><p id="17fa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">香草RNN的主要问题是，他们不能照顾长期的依赖性(即，如果来自初始时间步长的输入的信息被要求在网络的后面部分产生输出，则记忆会因为梯度的消失或爆炸而丢失。我们希望RNN能够跨多个时间步长存储信息，并在相关时检索信息——但普通的rnn通常很难做到这一点。更多解释请查看<a class="ae lb" href="https://towardsdatascience.com/understanding-rnn-and-lstm-f7cdf6dfc14e" rel="noopener" target="_blank">本出</a>。</p><h1 id="b029" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">长短期记忆(LSTM)网络是如何工作的？</h1><p id="2687" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">LSTM网络是递归神经网络的高级版本，它使记忆过去的数据变得更容易。最后，LSTMs是一种特殊类型的RNNs，其中您以特定的方式连接这些单元，以避免常规RNNs中出现的某些问题(如解决RNNs长期依赖性问题的消失和爆炸梯度)。给定未知持续时间的时滞，LSTM非常适合于分类、处理和预测时间序列。它通过随时间推移使用反向传播来训练模型。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/ccb5c7d302c6787e4224f06dbefb2b65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*wx5L43peUVde7mrhC41-rg.png"/></div></figure><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es ld"><img src="../Images/d4bdaa79e3a4e19e4528dd2c1dac8bb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gu0SeGRj6t6Vhf0g2JT0nA.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">来源:colah的博客</figcaption></figure><p id="cc99" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里，“A”是接受输入“X”，输出“h”(一个向量)的单位。此外，还有一些向量从前面的单元反馈到A(由从一个A到另一个A的右箭头所示)。除了来自前一个单元的输入和单元状态(穿过单元顶部的水平线)，我们还有来自前一个单元的另一个向量(即前一个单元的输出)。因此，每个单元总共有三个输入和两个输出。注意，这里所有的A指的是内存中的一个对象——为了便于解释，它们是以展开的方式绘制的。实际情况是，从A右侧出来的输出从左侧反馈到同一个A模块。</p><p id="d7fa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">您可以通过多种方式将LSTMs用于序列间任务。如上图所示，您可以将输入序列输入到一个LSTM中，并获得一个h向量序列。然后，您可以根据需要进一步处理这些h向量，以执行分类或回归任务。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es le"><img src="../Images/315b06669286a86dbab272e616b5ef94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HGY8vT3iSpCgqUAR_abe_Q.png"/></div></div></figure><p id="343f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">LSTMs的主要思想是单元格状态，即贯穿单元格顶部的水平线。</p><p id="2db1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">细胞状态有点像传送带。它直接贯穿整个网络，只有很少的线性交互作用。信息很容易沿着它流动——保持不变。这就解决了我们长期依赖的问题。关于LSTM氏症的更多详细解释，请浏览Colah的博客。</p><p id="60c6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在最大的问题来了。当进行实验分析时，基于LSTM网络或LSTM的编码器-解码器模型不适用于长句子，这是因为这种句子具有单个潜在向量作为输出，并且最后的LSTM单元可能不能捕捉句子的全部本质。由于长句中的所有单词都被捕获到一个向量中，如果输出单词依赖于特定的输入单词，那么在简单的基于LSTM的编码器-解码器模型中没有给予它适当的关注(就像我们人类通常所做的那样)。为了解决这些问题，基于注意力的模型应运而生。</p><h1 id="2eb1" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">什么是变压器？</strong></h1><p id="0bb4" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">变形金刚模型本质上是基于注意力的模型。如果解码器不仅仅依赖于上下文向量，而是可以访问编码器过去的所有状态，会怎么样？这正是注意力在做的事情。在每个解码步骤中，解码器都会查看编码器的任何特定状态。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/7f9cfb3ea45501031ba5221c36709ed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*VEvZxLeg3zqn6Y2fo2oEnA.png"/></div></figure><p id="a154" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这种情况下，使用双向输入，其中向前和向后提供输入序列，然后在传递到解码器之前连接到内容向量(Ci)。注意力模型不是将输入序列编码到单个固定的上下文向量中，而是开发一个上下文向量，该向量专门针对每个输出时间步长进行过滤。每个输出时间步长的内容向量基于其对输入时间步长的依赖性而改变，即由注意力权重(alpha)来照顾。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/73ee0e1502e8b19fd5324efeb656b1bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*jg1ZXljcLZl6oEvPNFf32A.png"/></div></figure><p id="8919" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从下图中，我们可以看到基线模型(没有注意)的BLEU分数(这是衡量准确性的尺度之一)随着句子长度的增加而急剧下降。但是，随着注意机制的引入，这个问题得到了缓解。自我注意模型的表现略低于具有注意的RNN编解码器，它们处理长句的能力也略低。这些结果表明，基于注意力的模型比传统的编码器-解码器模型具有显著的优势，传统的编码器-解码器模型甚至在长句上也表现出很好的性能。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es lh"><img src="../Images/81d2edc945cde44ddfdc52fff0c6482d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HJ8S155kQ10SwXIr2p35og.png"/></div></div></figure><p id="03d0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">变压器网络完全建立在自我关注机制上，不使用递归网络架构。变形金刚是用多头自关注模型制作的。他们的灵感来自CNN的类似邻居的概念。更多详细解释请阅读本博客。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es li"><img src="../Images/54c9c1eece91680ccb5197de8a8dd2c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bTNcS9h5zIzL0e5tobkIRg.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">杰伊·阿拉玛的博客</figcaption></figure><h1 id="399c" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated"><strong class="ak">结论:</strong></h1><p id="59f2" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">如前所述，变压器比基于RNN的模型更快，因为所有输入都是一次性接收的。与变压器网络相比，训练LSTM更困难，因为LSTM网络中的参数数量要多得多。此外，在LSTM网络中进行迁移学习是不可能的。seq2seq型号的变压器现在是最先进的网络。因此，我们的结论是，变压器网络具有最佳精度，并且复杂性和计算成本较低。</p></div></div>    
</body>
</html>