<html>
<head>
<title>Linear Regression, Decision Tree and Ensemble Learning applied to Seoul housing prices</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归、决策树和集成学习在首尔房价中的应用</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-decision-tree-and-ensemble-learning-applied-to-seoul-housing-prices-830d3493cfdb?source=collection_archive---------3-----------------------#2020-01-24">https://medium.com/analytics-vidhya/linear-regression-decision-tree-and-ensemble-learning-applied-to-seoul-housing-prices-830d3493cfdb?source=collection_archive---------3-----------------------#2020-01-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="52ec" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">实现回归模型以对真实数据集进行预测</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/46e7d8ca88f1f5bec597a1afd3a8bbc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1symXvipy9O8sdz-OCFA8w.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">《寄生虫》，奉俊昊，2019。</figcaption></figure><p id="5a08" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这篇文章阐述了用Python准备数据集并通过scikit-learn库中实现的不同机器学习算法(线性回归、决策树、随机森林和三种提升算法— AdaBoost、梯度提升、XGBoost)运行数据集以解决回归问题的步骤。</p><p id="9051" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><a class="ae kj" href="https://data.seoul.go.kr/dataList/OA-15549/S/1/datasetView.do" rel="noopener ugc nofollow" target="_blank">数据集</a>包含2016年至2019年间首尔市记录的超过76万宗租赁交易。目标是预测承租人为每笔交易支付的保证金(或额外租金)的金额。被认为与预测相关的特征是:1)楼层面积，2)楼层数，3)建造年份，以及4)房产的位置。数据集的介绍见B部分。</p><p id="a01f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">将讨论以下几点:</p><ul class=""><li id="3cdb" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">如何塑造、可视化和清理数据</li><li id="3a94" class="kk kl hi jp b jq kt jt ku jw kv ka kw ke kx ki kp kq kr ks bi translated">如何建立和训练模型(或预测器)</li><li id="0bb4" class="kk kl hi jp b jq kt jt ku jw kv ka kw ke kx ki kp kq kr ks bi translated">如何评价他们的表现</li></ul><p id="799b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">Jupyter笔记本在https://github.com/thierrylaplanche/SeoulHousingPrediction<a class="ae kj" href="https://github.com/thierrylaplanche/SeoulHousingPrediction" rel="noopener ugc nofollow" target="_blank">有售</a></p></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><h1 id="3566" class="lf lg hi bd lh li lj lk ll lm ln lo lp io lq ip lr ir ls is lt iu lu iv lv lw bi translated">A.总体结果</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/4cb50ec9fa90455a1a796f7027a0ef37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Vjbp3S30W4-HxuE3kB23Q.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">模型评估的不同指标</figcaption></figure><p id="5dc4" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们用于评估和比较不同模型(预测值)的度量是R (R平方，或决定系数)，它评估预测值与原始值相比的拟合程度。范围从0到1，值接近1表示模型更好。在上表中:</p><ul class=""><li id="0f52" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated"><em class="ly"> R^2验证</em>是在交叉验证期间对验证集计算的平均r分数。</li><li id="4233" class="kk kl hi jp b jq kt jt ku jw kv ka kw ke kx ki kp kq kr ks bi translated"><em class="ly"> R^2测试</em>是对最终模型的评估，在包含训练期间未见过的数据的测试集上执行(在我们的例子中，占总数据的30 %)。它通常低于<em class="ly"> R^2验证</em>，但是太大的差异将是模型“过度拟合”训练数据并且不能很好地概括新数据的迹象。</li></ul><p id="a6ac" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">就我们问题的预测准确性而言，基于决策树的算法远远领先于更简单的回归模型。最好的成绩是由XGBoost取得的。然而，当与简单的决策树(其超参数通过随机搜索CV进行调整)相比时，差异并不显著。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lz"><img src="../Images/c5ec41b129f56606f1e31ca63354cd6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nav0l3V9x88ah49mSU0FjQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">绘制核密度估计值可以看出预测值与实际值有多接近</figcaption></figure><p id="f058" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">上图表示目标变量(关键货币)的<em class="ly">核密度估计值</em>。实际值用红色表示，预测值用蓝色表示。我们可以看到，与简单的线性回归相比，使用XGBoost训练的模型的预测值更接近实际值。这证实了上述指标。</p><p id="f92f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">作为说明，下图显示了测试集的前5个目标值(租赁保证金金额)和每个模型的预测值(单位为10，000韩元，即8.5美元左右)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ma"><img src="../Images/4251ad03bd6757dff0c32bbb59a0f803.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SNNVQikwLcPAk2M_5J_y3w.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">每个模型的实际值与预测值</figcaption></figure></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><h1 id="239e" class="lf lg hi bd lh li lj lk ll lm ln lo lp io lq ip lr ir ls is lt iu lu iv lv lw bi translated">B.数据集的呈现</h1><p id="adb7" class="pw-post-body-paragraph jn jo hi jp b jq mb ij js jt mc im jv jw md jy jz ka me kc kd ke mf kg kh ki hb bi translated">数据集从首尔开放数据广场网站下载为Excel文件:<a class="ae kj" href="https://data.seoul.go.kr/dataList/OA-15549/S/1/datasetView.do" rel="noopener ugc nofollow" target="_blank">https://Data . Seoul . go . kr/dataList/OA-15549/S/1/dataset view . do</a>。它包含了首尔25个区从2011年到2019年的200多万笔租赁交易。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mg"><img src="../Images/2a0ee0ea35994b64e4633815cd855b34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NPauh4zT3gyJsVb6r4L_Lw.png"/></div></div></figure><p id="1f8b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">数据集由20多列组成，其中我们保留了以下内容:</p><ul class=""><li id="ac51" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated"><em class="ly">楼层面积:</em>确定押金数额的最明显标准</li><li id="6f62" class="kk kl hi jp b jq kt jt ku jw kv ka kw ke kx ki kp kq kr ks bi translated"><em class="ly">楼层数:</em>我们预计随着我们上楼，押金会增加</li><li id="cf74" class="kk kl hi jp b jq kt jt ku jw kv ka kw ke kx ki kp kq kr ks bi translated"><em class="ly">建造年份:</em>最近的建筑通常更贵</li><li id="e616" class="kk kl hi jp b jq kt jt ku jw kv ka kw ke kx ki kp kq kr ks bi translated"><em class="ly">地区名称:</em>位置是另一个重要的标准</li><li id="500f" class="kk kl hi jp b jq kt jt ku jw kv ka kw ke kx ki kp kq kr ks bi translated"><em class="ly">保证金</em>，或保证金:我们要预测的目标</li></ul><p id="5ea0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">为了提高计算效率，并最大限度地减少通货膨胀的影响，我们将数据过滤为2016年至2019年之间执行的交易。</p></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><h1 id="df63" class="lf lg hi bd lh li lj lk ll lm ln lo lp io lq ip lr ir ls is lt iu lu iv lv lw bi translated">C.数据准备</h1><p id="1fc9" class="pw-post-body-paragraph jn jo hi jp b jq mb ij js jt mc im jv jw md jy jz ka me kc kd ke mf kg kh ki hb bi translated">执行以下操作为建模准备数据。朱庇特笔记本中提供了更多的细节。</p><ul class=""><li id="2c40" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">使用正确的编码导入数据集</li></ul><p id="41f8" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">方法<code class="du mh mi mj mk b">read_csv</code>将逗号分隔的值文件解析成pandas数据帧，指定参数<code class="du mh mi mj mk b">encoding</code>来解释韩文字符。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="c32a" class="mp lg hi mk b fi mq mr l ms mt">import pandas as pd</span><span id="303e" class="mp lg hi mk b fi mu mr l ms mt">df_transactions = pd.read_csv('seoul_lease_transactions_20190916.csv', encoding = 'EUC-KR', dtype = str)</span></pre><ul class=""><li id="d2aa" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">根据交易日期筛选行</li></ul><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="2ea2" class="mp lg hi mk b fi mq mr l ms mt">df_transactions = df_transactions[(df_transactions['Transaction year'] &lt; '2020') &amp; (df_transactions['Transaction year'] &gt; '2015')]</span></pre><ul class=""><li id="2be2" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">删除不必要的列</li></ul><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="c462" class="mp lg hi mk b fi mq mr l ms mt">df_transactions = df_transactions.drop(<br/>                  columns = {'Lot code', 'Institution code',<br/>                            'Serial number', 'Collection year',<br/>                            'District code', 'Neighborhood code',...<br/>                            })</span></pre><ul class=""><li id="f9f5" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">输入缺失数据</li></ul><p id="b2e7" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">超过200 000行有未指定的楼层数。我们可以删除相应的行，删除整个属性“楼层数”，或者用最常见的值或中间值来估算缺失值。我们决定将缺失的楼层设置为数据集(2)中最常见的楼层。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="791b" class="mp lg hi mk b fi mq mr l ms mt"># Find the most common values for column ‘Floor’<br/>df_transactions['Floor'].value_counts().head()</span><span id="6436" class="mp lg hi mk b fi mu mr l ms mt"># Fill ‘null’ values with 2<br/>df_transactions['Floor'] = df_transactions['Floor'].fillna(2)</span></pre><ul class=""><li id="7692" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">删除损坏的数据</li></ul><p id="7d26" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">消除畸形或损坏的数字数据的一个简单方法是强制输入列的类型。我们将所有预期包含数值的列转换为数值类型，将参数<code class="du mh mi mj mk b">error</code>设置为‘强制’。任何无效值将被设置为<code class="du mh mi mj mk b">null</code>。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="c9d7" class="mp lg hi mk b fi mq mr l ms mt"># Set numerical type to specific columns<br/>df_transactions[['Floor', 'Floor area', 'Key money', 'Construction year']] = df_transactions[['Floor', 'Floor area', 'Key money', 'Construction year']].apply(pd.to_numeric, errors='coerce')</span><span id="e8ea" class="mp lg hi mk b fi mu mr l ms mt"># Remove null values<br/>df_transactions = df_transactions.dropna()</span><span id="a043" class="mp lg hi mk b fi mu mr l ms mt"># Convert floats to integers to remove decimals<br/>df_transactions[['Floor', 'Floor area', 'Key money', 'Construction year']] = df_transactions[['Floor', 'Floor area', 'Key money', 'Construction year']].astype(int)</span></pre><ul class=""><li id="f987" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">寻找相关性</li></ul><p id="9733" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">用方法<code class="du mh mi mj mk b">corr</code>查看相关系数给了我们特征之间任何线性关系的暗示。接近1的值表示强正相关，接近-1的值表示强负相关。识别更复杂关系的另一种方法是通过调用方法<code class="du mh mi mj mk b">scatter_matrix</code>来绘制每个数字属性与其他属性的关系。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="55bf" class="mp lg hi mk b fi mq mr l ms mt"># Pearson correlation coefficient between each pair of attributes<br/>df_transactions.corr()</span><span id="fb54" class="mp lg hi mk b fi mu mr l ms mt"># Plot every numerical attribute against every other<br/>pd.plotting.scatter_matrix(df_transactions, figsize =(20,15), alpha = 0.01)</span><span id="b514" class="mp lg hi mk b fi mu mr l ms mt"># Visualize correlation between the attribute ‘Floor area’ and the target ‘Key money’<br/>df_transactions.plot.scatter('Floor area', 'Key money', alpha = 0.01)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mv"><img src="../Images/ca8a16b738f1d35765d3b35903b52028.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*wKcxVWMJiY0IIRAuKKUk7Q.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">“楼面面积”与“租金”:上升趋势是明显的，但不明显</figcaption></figure><ul class=""><li id="bf7b" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">消除异常值</li></ul><p id="2fd9" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">为了帮助线性回归模型对看不见的数据进行归纳，从定型数据中移除极端样本是一种很好的做法。我们应用<a class="ae kj" href="https://www.thoughtco.com/what-is-the-interquartile-range-rule-3126244" rel="noopener ugc nofollow" target="_blank"> 1.5倍IQR法则</a>来隔离第一个四分位数以下低于1.5倍IQR(四分位数间距)和第三个四分位数以上高于1.5倍IQR的数据。这对于基于决策树的预测器来说是不必要的，因为基于决策树的预测器对于异常值是鲁棒的。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="e3b2" class="mp lg hi mk b fi mq mr l ms mt"># 1.5*IQR rule: function to compute the lower range and upper range<br/>def range_keep(column):<br/>   sorted(column)<br/>   Q1,Q3 = np.percentile(column , [25,75]) # 25th to 75th percentile<br/>   IQR = Q3 - Q1<br/>   lower_range = Q1 - (1.5 * IQR)<br/>   upper_range = Q3 + (1.5 * IQR)<br/>   return lower_range, upper_range</span><span id="7c00" class="mp lg hi mk b fi mu mr l ms mt"># Compute the lower range and upper range for column 'Key money'<br/>lower_range, upper_range = range_keep(df_transactions['Key money'])</span><span id="8a9e" class="mp lg hi mk b fi mu mr l ms mt"># Keep only values that lie between that range<br/>df_transactions =  df_transactions.drop(df_transactions[ (df_transactions['Key money'] &gt; upper_range) |<br/>(df_transactions['Key money'] &lt; lower_range)].index)</span><span id="e5f5" class="mp lg hi mk b fi mu mr l ms mt"># Compute the lower range and upper range for column 'Floor area'<br/>lower_range, upper_range = range_keep(df_transactions['Floor area'])</span><span id="0bcd" class="mp lg hi mk b fi mu mr l ms mt"># Keep only values that lie between that range<br/>df_transactions = df_transactions.drop(df_transactions[ (df_transactions['Floor area'] &gt; upper_range) | (df_transactions['Floor area'] &lt; lower_range)].index)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mw"><img src="../Images/c6fa30120ca9a58e49fbfd83b8249f8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6igckh0uiJHYE7QhXWbubg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">移除异常值前后每个数值属性的直方图</figcaption></figure><ul class=""><li id="e34d" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">将分类特征转换为二元(虚拟)变量</li></ul><p id="490f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">非数字特征通常需要在被馈送到机器学习算法之前被转换成二进制变量。列“区名”包含25个不同的文本值，每个值代表首尔的一个区，它被转换为25个二进制列。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="e89a" class="mp lg hi mk b fi mq mr l ms mt"># Automatically generate dummy variables from column 'District name'<br/>dummy_district = pd.get_dummies(df_transactions['District name'])</span><span id="4d2b" class="mp lg hi mk b fi mu mr l ms mt"># Concatenate the original dataframe with the dummy variables<br/>df_transactions_dummy = pd.concat([df_transactions, dummy_district], axis = 1)</span><span id="abe0" class="mp lg hi mk b fi mu mr l ms mt"># Drop the original 'District name' column<br/>df_transactions_dummy = df_transactions_dummy.drop(columns={'District name'}).reset_index(drop=True)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mx"><img src="../Images/78324dd27e6cc9e6da8af4a7ca46813a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zqTZj0VmVOhwhr83igWAyg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">准备好的数据集</figcaption></figure></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><h1 id="5f59" class="lf lg hi bd lh li lj lk ll lm ln lo lp io lq ip lr ir ls is lt iu lu iv lv lw bi translated">D.数据建模步骤</h1><p id="959e" class="pw-post-body-paragraph jn jo hi jp b jq mb ij js jt mc im jv jw md jy jz ka me kc kd ke mf kg kh ki hb bi translated">有了干净的数据集，是时候为算法塑造它了。</p><ul class=""><li id="5ce4" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">定义输入和目标变量</li></ul><p id="f64d" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">输入矩阵(X)包含这些特征。输出向量(y)是我们要预测的目标(存款额，或关键金额)。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="5ad3" class="mp lg hi mk b fi mq mr l ms mt">y = df_transactions_dummy['Key money']</span><span id="72f6" class="mp lg hi mk b fi mu mr l ms mt">X = df_transactions_dummy.drop(columns={'Key money'})</span></pre><ul class=""><li id="5806" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">分割训练/测试集</li></ul><p id="44c9" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">模型在原始数据的子集(训练集)上学习，并在不同的子集(测试集)上评估。我们把比例定为70%-30%。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="2bfb" class="mp lg hi mk b fi mq mr l ms mt">from sklearn.model_selection import train_test_split</span><span id="19ca" class="mp lg hi mk b fi mu mr l ms mt">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 2)</span></pre><ul class=""><li id="d9ae" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">缩放功能</li></ul><p id="5aec" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">线性回归模型对缩放后的要素学习得更好。我们将所有特征的范围定为0到1。对于基于决策树的模型，我们将使用原始(未缩放)数据。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="32ed" class="mp lg hi mk b fi mq mr l ms mt">from sklearn.preprocessing import MinMaxScaler</span><span id="9330" class="mp lg hi mk b fi mu mr l ms mt">scaler_x = MinMaxScaler(feature_range = (0,1))</span><span id="4e4d" class="mp lg hi mk b fi mu mr l ms mt">X_train_scaled = scaler_x.fit_transform(X_train)<br/>X_test_scaled = scaler_x.transform(X_test)</span></pre><p id="cd08" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">下一步是拟合模型(详见E部分)并评估它们的性能。</p><ul class=""><li id="a787" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">模型评估</li></ul><p id="30f4" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">预测值的<code class="du mh mi mj mk b">score</code>方法返回决定系数(R)。我们使用这个指标来评估每个模型并比较它们的性能。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="c3a4" class="mp lg hi mk b fi mq mr l ms mt"># Coefficient of determination (R^2) on the test set<br/>tree_reg.score(X_test, y_test)</span></pre><ul class=""><li id="980e" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">预测的可视化</li></ul><p id="7edb" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在同一张图上绘制实际值和预测值的核密度估计值，暗示了预测的准确性。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="2040" class="mp lg hi mk b fi mq mr l ms mt">import seaborn as sns</span><span id="010f" class="mp lg hi mk b fi mu mr l ms mt">ax1 = sns.distplot(y_test, hist = False, color = 'r', label = 'actual')</span><span id="ca8c" class="mp lg hi mk b fi mu mr l ms mt">sns.distplot(y_pred, hist = False, color = 'b', label = 'prediction', ax = ax1)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es my"><img src="../Images/d0bce73844bc0a2c6ff4ce64062dc360.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*32miDa2VVtv0qKSARjtFjg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><em class="mz">实际值和预测值的核密度估计</em></figcaption></figure><ul class=""><li id="9ff5" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">使用<code class="du mh mi mj mk b">RandomizedSearchCV</code>进行超参数调谐</li></ul><p id="ad0e" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><code class="du mh mi mj mk b">RandomizedSearchCV</code>评估算法参数(超参数)的不同组合，并用找到的最佳组合重新训练数据集。</p><p id="4db2" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">它采用<em class="ly">交叉验证分裂策略</em>，将训练集分裂成一定数量的子集(默认为5个)，然后每次在不同的折叠上训练和评估模型(4个作为较小的训练集，剩下的一个作为验证集)。这通常导致比简单的训练/测试分割更少的偏差结果。我们之前构建的测试集并不包含在这个过程中，它是为模型的最终评估而保留的。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="d707" class="mp lg hi mk b fi mq mr l ms mt">from sklearn.model_selection import RandomizedSearchCV</span><span id="cd0e" class="mp lg hi mk b fi mu mr l ms mt"># Hyperparameter values to test<br/>param_grid = {'max_features': ['auto', 'sqrt'],<br/>              'max_depth': np.arange(5, 36, 5),<br/>              'min_samples_split': [5, 10, 20, 40],<br/>              'min_samples_leaf': [2, 6, 12, 24],<br/>             }</span><span id="c531" class="mp lg hi mk b fi mu mr l ms mt"># RandomizedSearchCV with a limit of 10 combinations to test<br/>tree_reg = RandomizedSearchCV(estimator = DecisionTreeRegressor(), param_distributions = param_grid, n_iter = 10, verbose = 2, n_jobs = -1)</span><span id="b083" class="mp lg hi mk b fi mu mr l ms mt">tree_reg.fit(X_train, y_train)</span></pre></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><h1 id="bdc4" class="lf lg hi bd lh li lj lk ll lm ln lo lp io lq ip lr ir ls is lt iu lu iv lv lw bi translated">E.模型评论</h1><ul class=""><li id="8d6a" class="kk kl hi jp b jq mb jt mc jw na ka nb ke nc ki kp kq kr ks bi translated">线性回归</li></ul><p id="2370" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">最简单的模型有时会产生最好的结果。当每个特征和目标变量之间的关系基于单个系数时，这就是线性回归的情况。然而，数据通常以更复杂的方式相互关联。对我们的数据集的探索没有揭示变量之间的任何明确的线性关系。因此，我们并不指望这种模式会有很大的成效。然而，我们可以用它作为基线来判断其他更复杂的预测器的性能。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="8488" class="mp lg hi mk b fi mq mr l ms mt">from sklearn.linear_model import LinearRegression</span><span id="f68b" class="mp lg hi mk b fi mu mr l ms mt">lin_reg = LinearRegression()</span><span id="e986" class="mp lg hi mk b fi mu mr l ms mt">lin_reg_score = cross_val_score(lin_reg, X_train_scaled, y_train, verbose = 2)</span><span id="b880" class="mp lg hi mk b fi mu mr l ms mt">lin_reg.fit(X_train_scaled, y_train)</span><span id="38c1" class="mp lg hi mk b fi mu mr l ms mt">y_pred = lin_reg.predict(X_test_scaled)</span><span id="2b75" class="mp lg hi mk b fi mu mr l ms mt">lin_reg.score(X_test_scaled, y_test)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/5f59b423b080dcf320cb4708106fdf86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kbIpQ82UX60rlaSZBQl7qg.png"/></div></div></figure><ul class=""><li id="07c9" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">多项式回归</li></ul><p id="9532" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">多项式回归允许表示变量之间更复杂(非线性)的关系。该方法基于与上述相同的线性回归算法，但对输入要素进行了预先数学变换，并创建了组合现有要素的新属性。这可能会导致功能数量激增，并大大减慢学习过程。此外，当我们增加多项式变换的次数时，这种方法容易过度拟合。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="e874" class="mp lg hi mk b fi mq mr l ms mt">from sklearn.linear_model import LinearRegression<br/>from sklearn.preprocessing import PolynomialFeatures</span><span id="ee19" class="mp lg hi mk b fi mu mr l ms mt"># Polynomial transformation of degree 2<br/>poly_features = PolynomialFeatures(degree = 2)</span><span id="7b96" class="mp lg hi mk b fi mu mr l ms mt"># Returns a transformed version of X with new features<br/>X_train_scaled_poly = poly_features.fit_transform(X_train_scaled)<br/>X_test_scaled_poly = poly_features.fit_transform(X_test_scaled)</span><span id="916b" class="mp lg hi mk b fi mu mr l ms mt">poly_reg = LinearRegression()</span><span id="da4d" class="mp lg hi mk b fi mu mr l ms mt">poly_reg_score = cross_val_score(poly_reg, X_train_scaled_poly, y_train, verbose = 2)</span><span id="e94f" class="mp lg hi mk b fi mu mr l ms mt">poly_reg.fit(X_train_scaled_poly, y_train)</span><span id="99d7" class="mp lg hi mk b fi mu mr l ms mt">y_pred = poly_reg.predict(X_test_scaled_poly)</span><span id="4c4c" class="mp lg hi mk b fi mu mr l ms mt">poly_reg.score(X_test_scaled_poly, y_test)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/dd13f9025a7747098b958a16e116951f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yGjd8zr9fVGtJHKoYX0Ktw.png"/></div></div></figure><ul class=""><li id="fb49" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">决策图表</li></ul><p id="87af" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">决策树能够对数据中复杂的非线性关系进行建模，既适用于分类任务，也适用于回归任务。位于树的较高层的特征对预测有较大的影响，这使得模型易于解释。众所周知，决策树容易过度拟合。然而，这可以通过调整算法的超参数来防止，例如树的最大深度。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="de1a" class="mp lg hi mk b fi mq mr l ms mt">from sklearn.model_selection import RandomizedSearchCV<br/>from sklearn.tree import DecisionTreeRegressor</span><span id="e6f9" class="mp lg hi mk b fi mu mr l ms mt">param_grid = {'max_features': ['auto', 'sqrt'],<br/>              'max_depth': np.arange(5, 36, 5),<br/>              'min_samples_split': [5, 10, 20, 40],<br/>              'min_samples_leaf': [2, 6, 12, 24],<br/>             }</span><span id="5d93" class="mp lg hi mk b fi mu mr l ms mt">tree_reg = RandomizedSearchCV(estimator = DecisionTreeRegressor(), param_distributions = param_grid, n_iter = 100, verbose = 2, n_jobs = -1)</span><span id="9ec5" class="mp lg hi mk b fi mu mr l ms mt">tree_reg.fit(X_train, y_train)</span></pre><p id="fcab" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">对超参数的随机搜索产生了以下最佳组合。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="d743" class="mp lg hi mk b fi mq mr l ms mt">tree_reg.best_params_</span><span id="c70d" class="mp lg hi mk b fi mu mr l ms mt">{'min_samples_split': 40,<br/> 'min_samples_leaf': 2,<br/> 'max_features': 'auto',<br/> 'max_depth': 40}</span></pre><p id="0504" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">决策树模型带有一个方便的<code class="du mh mi mj mk b">feature_importances_</code>属性，它返回模型中每个特征的重要性。也称为“基尼重要度”，它是包括该特征的所有树的分裂数之和，与它分裂的样本数成比例。在我们的案例中，预测存款金额的最具决定性的特征如下:</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="2376" class="mp lg hi mk b fi mq mr l ms mt">sorted(dict(zip(X_train.columns, tree_reg.best_estimator_.feature_importances_)).items(), key=lambda x: x[1], reverse=True)</span><span id="ea15" class="mp lg hi mk b fi mu mr l ms mt">[('Floor area', 0.6650461951919517),<br/> ('Construction year', 0.1020058438278795),<br/> ('Floor', 0.09984360254893668),<br/> ('Gangnam', 0.02888171812885576),<br/> ('Seocho', 0.018332026175877324),<br/> ('Songpa', 0.013463453343552612),<br/> ('Yangcheon', 0.007425564646474106), ...</span></pre><p id="4c47" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">正如所料，建筑面积是预测存款的最决定性因素，其次是建筑年份和楼层数。接下来的地区列表或多或少按每个地区的平均存款额排序，最贵的地区权重最大。我们可以尝试去掉最不重要的特征，因为它们似乎对预测没有太大贡献。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/314974de36e4cb1b8b0ff77c46fd338d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eCwmeHM9aYTOwU9SQu_WRQ.png"/></div></div></figure><ul class=""><li id="2705" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">随机森林</li></ul><p id="4910" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">随机森林是一种集成学习方法，它包括并行生长几个决策树，并组合它们的输出来进行预测。每个决策树(称为“弱学习者”)都建立在训练集的不同随机子集上，从而学习不同的模式。组合一组树的预测通常会产生比最佳单个树更好的结果。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="5c55" class="mp lg hi mk b fi mq mr l ms mt">from sklearn.model_selection import RandomizedSearchCV<br/>from sklearn.ensemble import RandomForestRegressor</span><span id="685b" class="mp lg hi mk b fi mu mr l ms mt">param_grid = {'max_features': ['auto', 'sqrt'],<br/>              'max_depth': np.arange(5, 36, 5),<br/>              'min_samples_split': [5, 10, 20, 40],<br/>              'min_samples_leaf': [2, 6, 12, 24],<br/>             }</span><span id="402a" class="mp lg hi mk b fi mu mr l ms mt">rfor_reg = RandomizedSearchCV(RandomForestRegressor(), param_distributions = param_grid, n_iter = 100, verbose = 2, n_jobs = -1)</span><span id="645d" class="mp lg hi mk b fi mu mr l ms mt">rfor_reg.fit(X_train, y_train)</span></pre><p id="03ef" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">随机搜索产生了以下超参数的最佳组合:</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="80f5" class="mp lg hi mk b fi mq mr l ms mt">rfor_reg.best_params_</span><span id="e15b" class="mp lg hi mk b fi mu mr l ms mt">{'min_samples_split': 5,<br/> 'min_samples_leaf': 2,<br/> 'max_features': 'sqrt',<br/> 'max_depth': 35}</span></pre><p id="870a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">最具决定性的特征如下。楼层数被赋予了更大的权重:</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="924b" class="mp lg hi mk b fi mq mr l ms mt">sorted(dict(zip(X_train.columns, rfor_reg.best_estimator_.feature_importances_)).items(), key=lambda x: x[1], reverse=True)</span><span id="6e79" class="mp lg hi mk b fi mu mr l ms mt">[('Floor area', 0.5355792921620066),<br/> ('Floor', 0.19361661010178893),<br/> ('Construction year', 0.12539123364405722),<br/> ('Gangnam', 0.02845832530359601),<br/> ('Seocho', 0.018817437372265773),<br/> ('Songpa', 0.01287372002727185),<br/> ('Seongdong', 0.007682300699672825), ...</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/e50fa7e9172fca4a889d61a68e63f00e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j9P7QdxGs85T2vKyiyj4fQ.png"/></div></div></figure><ul class=""><li id="1c7a" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">助推算法</li></ul><p id="b7d6" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">作为一种集成学习，boosting包括依次训练几个模型，每个模型都试图提高其前任的性能。AdaBoost、Gradient Boosting和XGBoost是一些最常见的增强算法，每种算法的表现都略有不同。</p><p id="750f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><a class="ae kj" href="https://hackernoon.com/boosting-algorithms-adaboost-gradient-boosting-and-xgboost-f74991cad38c" rel="noopener ugc nofollow" target="_blank"> AdaBoost </a>专注于通过对前一个模型最不适合(预测不佳)的样本增加权重来依次改进每个模型。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="babf" class="mp lg hi mk b fi mq mr l ms mt">from sklearn.model_selection import RandomizedSearchCV<br/>from sklearn.ensemble import AdaBoostRegressor<br/>from sklearn.tree import DecisionTreeRegressor</span><span id="8f0e" class="mp lg hi mk b fi mu mr l ms mt">param_grid = {"learning_rate" : [0.01, 0.1, 0.3],<br/>              "loss"          : ['linear', 'square', 'exponential']<br/>             }</span><span id="138f" class="mp lg hi mk b fi mu mr l ms mt">ada_reg = RandomizedSearchCV(AdaBoostRegressor( DecisionTreeRegressor(), n_estimators=100), param_distributions = param_grid, n_iter = 100, verbose = 2, n_jobs = -1)</span><span id="4c3f" class="mp lg hi mk b fi mu mr l ms mt">ada_reg.fit(X_train, y_train)</span></pre><p id="42a8" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><a class="ae kj" rel="noopener" href="/hackernoon/gradient-boosting-and-xgboost-90862daa6c77">梯度推进</a>试图解决一个优化问题:其目标是通过在每一步增加一个弱学习器来减少损失函数。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="eb85" class="mp lg hi mk b fi mq mr l ms mt">from sklearn.model_selection import RandomizedSearchCV<br/>from sklearn.ensemble import GradientBoostingRegressor</span><span id="fce2" class="mp lg hi mk b fi mu mr l ms mt">param_grid = {"learning_rate"    : [0.01, 0.1, 0.3],<br/>              "subsample"        : [0.5, 1.0],<br/>              "max_depth"        : [3, 4, 5, 10, 15, 20],<br/>              "max_features"     : ['auto', 'sqrt'],<br/>              "min_samples_split": [5, 10, 20, 40],<br/>              "min_samples_leaf" : [2, 6, 12, 24]<br/>             }</span><span id="8425" class="mp lg hi mk b fi mu mr l ms mt">grad_reg = RandomizedSearchCV(GradientBoostingRegressor(), param_distributions = param_grid, n_iter = 100, verbose = 2, n_jobs = -1)</span><span id="a4f9" class="mp lg hi mk b fi mu mr l ms mt">grad_reg.fit(X_train, y_train)</span></pre><p id="5d24" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><a class="ae kj" rel="noopener" href="/hackernoon/gradient-boosting-and-xgboost-90862daa6c77"> XGBoost </a>是梯度增强算法的高级实现，采用正则化技术进一步减少过拟合，并通过并行处理加快计算速度。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="3f15" class="mp lg hi mk b fi mq mr l ms mt">from sklearn.model_selection import RandomizedSearchCV<br/>from xgboost import XGBRegressor</span><span id="bc13" class="mp lg hi mk b fi mu mr l ms mt">param_grid = {"learning_rate"    : [0.01, 0.1, 0.3] ,<br/>              "max_depth"        : [3, 4, 5, 10, 15, 20],<br/>              "min_child_weight" : [1, 3, 5, 7],<br/>              "gamma"            : [0.0, 0.1, 0.2, 0.3, 0.4],<br/>              "colsample_bytree" : [0.3, 0.4, 0.5, 0.7]<br/>             }</span><span id="0b73" class="mp lg hi mk b fi mu mr l ms mt">xgb_reg = RandomizedSearchCV(XGBRegressor(), param_distributions = param_grid, n_iter = 100, verbose = 2, n_jobs = -1)</span><span id="cb67" class="mp lg hi mk b fi mu mr l ms mt">xgb_reg.fit(X_train, y_train)</span></pre><p id="ccbe" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">XGBoost附带了一个很酷的特性来可视化每个特性的重要性，其特征是一个特性被用来在所有树之间分割数据的次数。</p><pre class="iy iz ja jb fd ml mk mm mn aw mo bi"><span id="6fd4" class="mp lg hi mk b fi mq mr l ms mt">from xgboost import plot_importance</span><span id="83a4" class="mp lg hi mk b fi mu mr l ms mt">plot_importance(xgb_reg.best_estimator_)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nd"><img src="../Images/9dc344e8e9701e9dc16a46d54024f577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*W-v4-FNCR4JuBSUQD-YkwA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">XGBoost模型中每个特性的重要性</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/5d2a907b1a809263708c78f534dca442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7YrIgzobOUXWB2XB5jPMpg.png"/></div></div></figure></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><h1 id="aabc" class="lf lg hi bd lh li lj lk ll lm ln lo lp io lq ip lr ir ls is lt iu lu iv lv lw bi translated">F.几处改动</h1><ul class=""><li id="1606" class="kk kl hi jp b jq mb jt mc jw na ka nb ke nc ki kp kq kr ks bi translated">删除“地区名称”</li></ul><p id="c08a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">“地区名称”属性在功能重要性方面排名较低。然而，由于它的非数值性质，它在我们的数据集中占据了不少于25列(总共28列)。我们怀疑这个变量对训练时间有很大的影响。</p><p id="0857" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">事实上，取消这一功能可以大大减少培训时间。例如，随机森林训练时间从252分钟下降到80分钟。然而，时间上的收益伴随着预测性能的下降，如下面的核密度估计图所示。除此之外，测试的R分数从0.878降到了0.755。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ne"><img src="../Images/cd660ecd0d260dd786794cf356e45211.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*elzNfeFEEjf23epCYWbBMw.png"/></div></figure><p id="90e2" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">一个原因可能是，虽然每个二元属性单独使用一般不会对所有预测产生很大影响，但所有25个属性的组合有助于模型产生更好的预测。</p><p id="41c2" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">保持训练时间短和性能高的一个可能的解决方案是用一个数字特征代替这个分类特征，例如相应地区的中值收入。</p><ul class=""><li id="2458" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">删除楼层号</li></ul><p id="a251" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">超过四分之一的样本没有指定楼层数。作为一种变通方法，我们用数据集中最常见的楼层数来估算缺失值。这引入了许多错误的值，并且可能阻碍了我们模型的性能。</p><p id="fa32" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">还是有？重新训练没有这个特征的模型没有产生任何显著的变化。这证实了基于决策树的模型对该特征的低重要性。</p></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><h1 id="9d3b" class="lf lg hi bd lh li lj lk ll lm ln lo lp io lq ip lr ir ls is lt iu lu iv lv lw bi translated">G.可能的改进</h1><ul class=""><li id="5a0a" class="kk kl hi jp b jq mb jt mc jw na ka nb ke nc ki kp kq kr ks bi translated">添加更多功能</li></ul><p id="854b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在现实生活中，房产的价值(或保证金的数量)取决于更多的因素，而不仅仅是建筑面积和我们在模型中使用的其他属性。有了更多样化的特征，比如卧室的数量、到最近地铁站的距离，或者从房产看风景的评级(0表示垃圾场，10表示山或河)，我们可以期待更好的结果。</p><ul class=""><li id="bcc6" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">标准化数据集</li></ul><p id="2731" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在绘制数字特征的直方图时，我们注意到一些分布是重尾的:它们向一侧延伸得比另一侧远。这对于一些机器学习算法来说并不理想，因为这可能会使它们更难检测到模式。我们可能想要转换这些属性，例如通过计算它们的对数，以便获得更“类似高斯”的分布。</p><ul class=""><li id="ce45" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">执行分层洗牌</li></ul><p id="3438" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">正如所料,“地板面积”属性是预测存款金额的最重要因素。我们可能希望确保该特征的不同间隔(例如，从10米到30米、从30米到50米、从50米到70米等)。)在测试集中的比例与它们在整个数据集中的比例相同。为此，我们将在数据集中创建一个新列，存储每个样本所属的区间(地层)，然后使用scikit-learn的函数<code class="du mh mi mj mk b">StratifiedShuffleSplit</code>执行分层采样，而不是使用<code class="du mh mi mj mk b">train_test_split</code>执行纯粹的随机采样。</p><ul class=""><li id="43a3" class="kk kl hi jp b jq jr jt ju jw km ka kn ke ko ki kp kq kr ks bi translated">尝试其他型号</li></ul><p id="8709" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们可以投入更多时间，训练其他机器学习算法:回归支持向量机(SVR)、KNN或神经网络。</p></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><p id="ab35" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">最后，这里是首尔的choropleth地图，代表25个区的平均保证金金额。我们可以证实，最昂贵的地区(在东南部)是那些被基于决策树的模型赋予最大重要性(权重)的地区。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nf"><img src="../Images/d04c02680d91920ab752820fd72de173.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CMDZ3s8su5PYVMNnPrxzdQ.png"/></div></div></figure></div></div>    
</body>
</html>