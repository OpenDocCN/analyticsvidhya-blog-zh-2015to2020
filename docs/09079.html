<html>
<head>
<title>DECISION TREE</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策图表</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decision-tree-2855f7e198f0?source=collection_archive---------18-----------------------#2020-08-24">https://medium.com/analytics-vidhya/decision-tree-2855f7e198f0?source=collection_archive---------18-----------------------#2020-08-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8cc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树属于监督机器学习技术的范畴，也被称为<strong class="ih hj"> CART </strong> <strong class="ih hj">(分类和回归树)</strong>。它利用一个树形结构<strong class="ih hj"> <em class="jd">来模拟<strong class="ih hj">特征</strong>和<strong class="ih hj">结果</strong>之间的关系</em> </strong>。它由代表<strong class="ih hj"> <em class="jd">决策函数</em> </strong>的<strong class="ih hj"> <em class="jd">节点和代表</em> </strong>决策函数<strong class="ih hj"> <em class="jd">输出的<strong class="ih hj"> <em class="jd">分支</em> </strong>组成。因此，这是决定如何对新数据点进行分类的流程图。</em></strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/4edcb084b2a69f7af92bbc378cac8011.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*7cyzrfuh9hKqz2lZxi_8ug.gif"/></div></div></figure><p id="ee4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该决策使用属性选择度量(ASM)来选择最佳属性以拆分记录。树标准将数据分成子集，子集又分成更小的子集。当子集内的数据足够相似时<strong class="ih hj"> <em class="jd">算法停止分割数据</em> </strong>。决策树分割所有可用变量上的节点，然后选择产生最相似子节点的分割。</p><h2 id="0deb" class="jq jr hi bd js jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">决策树可用于分类和回归问题，但它们的工作方式不同。</h2></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><h2 id="be54" class="jq jr hi bd js jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">分类问题的决策树；</h2><ul class=""><li id="caa0" class="ks kt hi ih b ii ku im kv iq kw iu kx iy ky jc kz la lb lc bi translated">所有类别的<strong class="ih hj">后验概率</strong>反映在叶节点中，叶节点属于多数类别。在执行之后，数据点的类别由它到达的叶节点决定。</li><li id="9467" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">目标是<strong class="ih hj">尽可能减少叶节点处的杂质</strong>。</li><li id="b253" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">损失函数是属于父节点的节点</strong>的目标列中的杂质 <strong class="ih hj">的度量。节点处的杂质是节点的目标列中不同类别的混合的度量。</strong></li></ul><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="li lj l"/></div></figure><h2 id="0c56" class="jq jr hi bd js jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">回归问题的决策树；</h2><ul class=""><li id="d3bd" class="ks kt hi ih b ii ku im kv iq kw iu kx iy ky jc kz la lb lc bi translated">目标属性的<strong class="ih hj">平均值或中间值</strong>被分配给查询变量。</li><li id="3fc3" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">目标是<strong class="ih hj">最小化每个节点的目标列中的方差</strong>(数据点与中心值的不相似性)。</li><li id="d224" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">方差的减少相当于同质性或纯度的增加。</li></ul><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="li lj l"/></div></figure><h1 id="2446" class="lk jr hi bd js ll lm ln jw lo lp lq ka lr ls lt kd lu lv lw kg lx ly lz kj ma bi translated">分割树的精确度的测量:</h1><h2 id="b75d" class="jq jr hi bd js jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">杂质:</h2><p id="c69c" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">该树将数据分割成不够<em class="jd">同质的子集，称为不纯。</em></p><p id="d318" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为什么这很重要？根据使用的杂质测量，树分类结果可能会有所不同。这可能会对您的模型产生或大或小的影响。</p><h2 id="0531" class="jq jr hi bd js jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">熵:</h2><p id="9fcd" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">熵控制决策树如何决定在哪里分割数据。它<em class="jd">是测量</em> <strong class="ih hj"> <em class="jd">杂质</em> </strong> <em class="jd">或数据点中的随机性。</em></p><p id="763c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">熵是在0和1之间计算的<strong class="ih hj"> <em class="jd">。</em></strong><strong class="ih hj"><em class="jd">熵值越小</em> </strong>越好。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es me"><img src="../Images/4b4356ab6a5df0b6fd6037517a8b5c98.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*D1AYsJYDYQ_-Sbbd5HOBpQ.png"/></div></figure><p id="2083" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">比如</em> </strong>，假设我们只有两个类，一个正类和一个负类。因此'<strong class="ih hj"> <em class="jd"> i </em> </strong>'在这里可能是(+)也可能是(-)。因此，如果我们的数据集中总共有100个数据点，其中30个数据点属于正类，70个数据点属于负类，那么'<strong class="ih hj"><em class="jd">【P+】</em></strong>'将是3/10，而'<strong class="ih hj"> <em class="jd"> P- </em> </strong>'将是7/10。因此，在这个例子中使用上面的公式计算类的熵。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mf"><img src="../Images/4a0aed2048e4816d30013c69567af3f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KhB_PhE50tJUE3XCsjkp2g.png"/></div></figure><p id="61b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里的熵大约是0.88。这被认为是一种高熵，一种高水平的无序(意味着低水平的纯度或<strong class="ih hj">高度不纯的分裂</strong>)。</p><h2 id="0485" class="jq jr hi bd js jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">信息增益:</h2><p id="2226" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">信息增益根据给定的属性值计算数据集分割前的熵和分割后的平均熵之差。</p><ul class=""><li id="daea" class="ks kt hi ih b ii ij im in iq mg iu mh iy mi jc kz la lb lc bi translated">基于熵的概念，即通过量化 <strong class="ih hj"> <em class="jd">杂质</em> </strong>的大小来量化哪个特征提供关于分类的最大信息，目的是从根节点到叶节点减少熵<strong class="ih hj">的量。</strong></li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mf"><img src="../Images/7f4742e6a5bc1de7bf9e2ddf13372120.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N-NLrXAfyqZo8ijxySUBAg.png"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">我们只需从Y的熵中减去给定X中Y的熵，就可以计算出给定一条关于Y的额外信息X时，关于Y的不确定性的减少，这称为信息增益。这种不确定性减少得越多，从x获得的关于Y的信息就越多。</figcaption></figure><h2 id="c1b2" class="jq jr hi bd js jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">基尼指数或基尼系数:</h2><p id="632a" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">它计算随机选择时某个特定要素被错误分类的概率。<strong class="ih hj"> </strong>如果所有的元素都与一个单一的类相联系那么它就可以被称为纯粹的。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mn"><img src="../Images/3bb037468c32ff2979f1ba897821bf3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*UWJ1P7dX7AoMXtK8WNBKSg.png"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">其中Pj表示元素被分类为不同类别的概率。</figcaption></figure><p id="4455" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">基尼指数</em> <strong class="ih hj"> <em class="jd">在数值0和1 </em> </strong> <em class="jd">之间变化，其中0表示分类的纯度或完美分类。</em>分类和回归树<strong class="ih hj"> (CART) </strong>算法采用基尼指数的方法来产生二元分裂。</p><h2 id="06ca" class="jq jr hi bd js jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">基尼指数与信息增益:</h2><ul class=""><li id="5025" class="ks kt hi ih b ii ku im kv iq kw iu kx iy ky jc kz la lb lc bi translated">基尼指数<strong class="ih hj">有利于更大的分布</strong>以便于实施，而信息增益<strong class="ih hj">有利于具有多个特定值的小计数的较小分布</strong>。</li><li id="922a" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">基尼指数<strong class="ih hj">根据“成功”或“失败”对分类目标变量</strong>进行操作，并且<strong class="ih hj">仅执行二元分裂</strong>，相反，信息增益<strong class="ih hj">计算分裂</strong>之前和之后的熵 <strong class="ih hj">之间的差异，并指示元素类别中的杂质。</strong></li></ul><h2 id="7a4f" class="jq jr hi bd js jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">使用基尼熵与熵的误差差异:</h2><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mo"><img src="../Images/f66c52490e2359f58141420a3328d384.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*Ro0ocRxf3lg4HUC6UA5Tfw.png"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">杂项错误不用于决策树，图只适用于二进制分类。</figcaption></figure><h1 id="28a9" class="lk jr hi bd js ll lm ln jw lo lp lq ka lr ls lt kd lu lv lw kg lx ly lz kj ma bi translated">优点和缺点:</h1><h2 id="6ea5" class="jq jr hi bd js jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">优势:</h2><ul class=""><li id="8ab0" class="ks kt hi ih b ii ku im kv iq kw iu kx iy ky jc kz la lb lc bi translated">处理简单快速且有效。</li><li id="76ae" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">对有噪声的数据和丢失的数据处理得很好。</li><li id="e591" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">处理数字和分类变量。</li><li id="d6f7" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">解释结果不需要数学或统计知识。</li></ul><h2 id="89f8" class="jq jr hi bd js jt ju jv jw jx jy jz ka iq kb kc kd iu ke kf kg iy kh ki kj kk bi translated">缺点:</h2><ul class=""><li id="fd0e" class="ks kt hi ih b ii ku im kv iq kw iu kx iy ky jc kz la lb lc bi translated">很容易过度拟合。</li><li id="191d" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">通常偏向于分裂或具有大量级别(树深度)的特征。</li><li id="9855" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">训练数据中的小变化会导致逻辑的大变化。</li><li id="29b9" class="ks kt hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">大树可能很难解释。</li></ul></div><div class="ab cl kl km gp kn" role="separator"><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq kr"/><span class="ko bw bk kp kq"/></div><div class="hb hc hd he hf"><p id="2173" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望这篇博客能帮助你更好地理解决策树。如果你喜欢，请鼓掌支持。  <strong class="ih hj"> <em class="jd">【快乐学习……:)</em></strong></p></div></div>    
</body>
</html>