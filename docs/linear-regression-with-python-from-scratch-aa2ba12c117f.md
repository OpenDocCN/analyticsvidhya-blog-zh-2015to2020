# ç”¨ Python ä»å¤´å¼€å§‹çº¿æ€§å›å½’

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/linear-regression-with-python-from-scratch-aa2ba12c117f?source=collection_archive---------15----------------------->

![](img/b1d65de89e5eadabcc478e430653908f.png)

ç”¨ Python è¿›è¡Œæœºå™¨å­¦ä¹ 

æœ‰ä¸‰ç§ç±»å‹çš„æœºå™¨å­¦ä¹ ç±»åˆ«:

1.  ç›‘ç£å­¦ä¹ 
2.  æ— ç›‘ç£å­¦ä¹ 
3.  å¼ºåŒ–å­¦ä¹ 

çº¿æ€§å›å½’ç®—æ³•å±äºç›‘ç£å­¦ä¹ çš„èŒƒç•´ï¼Œåœ¨è¿™ä¸€èŒƒç•´ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æ•°æ®ä¸­æ¯ä¸ªä¾‹å­çš„â€œæ­£ç¡®ç­”æ¡ˆâ€ã€‚çº¿æ€§å›å½’æ˜¯æœ€ç®€å•çš„å­¦ä¹ ç®—æ³•ï¼Œå®ƒé€šè¿‡ä»ç»™å®šçš„å…ˆå‰æ•°æ®ä¸­å­¦ä¹ æ¥å¸®åŠ©é¢„æµ‹æ­£ç¡®çš„ç­”æ¡ˆã€‚

![](img/0e66b55d4eeff751163b35c589413d38.png)

çº¿æ€§å›å½’æ¡†å›¾(å›¾ 1)

è¿™é‡ŒåŸºæœ¬ä¸Šâ€œhâ€æ˜¯æˆ‘ä»¬çš„å‡è®¾å‡½æ•°ï¼Œè¯¥å‡½æ•°ç»™å‡ºå¦‚ä¸‹:

![](img/9dcdbd8cb9815cd578867b950960fd0e.png)

å›¾ 2

è¿™ä¸ªå‡½æ•°åŸºæœ¬ä¸Šé¢„æµ‹ Y æ˜¯ X è½´åœ¨ X ä¸Šçš„æŸä¸ªç›´çº¿å‡½æ•°ã€‚è¿™ä¸ªæ¨¡å‹è¢«ç§°ä¸ºä¸€å…ƒçº¿æ€§å›å½’æˆ–ä¸€å…ƒçº¿æ€§å›å½’ï¼Œå› ä¸ºè¿™é‡Œåªæœ‰ X ä¸ªå˜é‡ã€‚æˆ‘ä»¬å¿…é¡»æ‰¾åˆ°Î¸_ 0 å’ŒÎ¸_ 1 çš„æœ€ä½³å¯èƒ½å€¼ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥åœ¨æ•°æ®ä¸Šæ‹Ÿåˆæœ€ä½³å¯èƒ½çš„çº¿æ€§çº¿ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚

![](img/cc31cb2689d09291a9019ddcd222a31a.png)

å›¾ 3

## æ¨¡å‹è¡¨ç¤º

![](img/cfc91d53898f3613d7b88baa86db8f54.png)

å›¾ 4

**ç¬¦å·:**

m =è®­ç»ƒç¤ºä¾‹çš„æ€»æ•°[ä¾‹å¦‚:2104ï¼Œ1416ï¼Œ1534ï¼Œ852ï¼Œâ€¦]

n =ç‰¹æ€§æ€»æ•°[ä¾‹å¦‚:ä»¥è‹±å°ºä¸ºå•ä½çš„å°ºå¯¸(X)ï¼Œåƒç‰‡è®¢é‡çš„ä»·æ ¼(Y)]

**å‚æ•°:**

![](img/9dcdbd8cb9815cd578867b950960fd0e.png)

è¿™é‡ŒÎ¸_ 0 å’ŒÎ¸_ 1 æ˜¯å‚æ•°

> æˆ‘ä»¬çš„åº§å³é“­æ˜¯é€‰æ‹©Î¸_ 0 å’ŒÎ¸_ 1ï¼Œå› æ­¤å¯¹äºæˆ‘ä»¬çš„è®­ç»ƒç¤ºä¾‹(xï¼Œy)ï¼Œè¯¥å‡è®¾æ¥è¿‘ y

ä¸ºäº†è·å¾—å‡è®¾å‡½æ•°çš„æœ€ä½³å€¼ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªæˆæœ¬å‡½æ•°æ¥æœ€å°åŒ–Î¸çš„å€¼ï¼Œè¿™å°†ç»™å‡ºæœ€ç¬¦åˆç»™å®šæ•°æ®çš„Î¸çš„æœ€ä½³å€¼ã€‚

## æˆæœ¬å‡½æ•°/å¹³æ–¹è¯¯å·®å‡½æ•°

![](img/51fd7f544fa24bfff6d4a58d78fbeb8c.png)

çº¿æ€§å›å½’çš„ä»£ä»·å‡½æ•°(J)æ˜¯é¢„æµ‹ y å€¼(pred)å’ŒçœŸå® y å€¼(y)ä¹‹é—´çš„**å‡æ–¹æ ¹è¯¯å·®(RMSE)** ã€‚

![](img/667e5a4fb36dca205a156881b6c6dbad.png)

å›¾ 5

åœ¨ä¸Šé¢å›¾åƒçš„å·¦ä¾§ï¼Œåœ¨**æœ€å°åŒ–Î¸[Theta]çš„å€¼**ä¹‹åï¼Œè¯¥çº¿å¯¹æ•°æ®æ‹Ÿåˆå¾—ä¸å¥½ã€‚åœ¨å›¾åƒçš„å³ä¾§ï¼Œæˆ‘ä»¬å¾—åˆ°è¯¥çº¿å¯¹æ•°æ®æ‹Ÿåˆå¾—å¾ˆå¥½çš„å›¾å½¢ã€‚

## æ¢¯åº¦ä¸‹é™

æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºæ‰¾åˆ°ä½¿æˆæœ¬å‡½æ•°(cost)æœ€å°åŒ–çš„å‡½æ•°(f)çš„å‚æ•°å€¼ã€‚

ç”¨äºæœ€å°åŒ– J(Î¸_ 0ï¼ŒÎ¸_ 1)

**æ¦‚è¦:**

1.  ä»Î¸_ 0 å’ŒÎ¸_ 1 çš„æŸä¸ªå€¼å¼€å§‹[ **æ¯”å¦‚Î¸_ 0 = 0&Î¸_ 1 = 0**
2.  ä¸æ–­æ”¹å˜Î¸_ 0 &Î¸_ 1 ä»¥å‡å° J(Î¸_ 0ï¼ŒÎ¸_ 1)ï¼Œç›´åˆ°æˆ‘ä»¬æœ‰å¸Œæœ›è¾¾åˆ°æœ€å°å€¼ã€‚

![](img/f1a4e24a9596b609530083776c67a24b.png)

å›¾ 6

åœ¨ä¸Šå›¾ä¸­ï¼Œæ˜Ÿå·æ˜¯Î¸éšæœºå€¼ã€‚æœ‰æ—¶ï¼Œæˆ‘ä»¬ä»é›¶å€¼å¼€å§‹ï¼Œè¿™åªæ˜¯ä¸€ä¸ªä¾‹å­ã€‚åœ¨å°† aÎ¸çš„å€¼éšæœºåˆå§‹åŒ–æˆ–åˆå§‹åŒ–ä¸ºé›¶åï¼Œå®ƒå°†ç¯é¡¾å¹³é¢ï¼Œç®—æ³•å°†å‘æ”¶æ•›æ–¹å‘è¿ˆä¸€å°æ­¥ã€‚åœ¨è¿ˆå‡ºä¸€å°æ­¥åï¼Œå®ƒå°†ä»æ–°çš„ä½ç½®å†æ¬¡è¿ˆå‡ºä¸€å°æ­¥ï¼Œè¿™æ ·ï¼Œå®ƒå°†æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ã€‚è®©æˆ‘ä»¬ç›´è§‚åœ°çœ‹çœ‹è¿™ä¸ªã€‚

![](img/d5564f3ffb03a5368e17854670734ffd.png)

å›¾ 7

è¿™æ ·ï¼Œå®ƒç°åœ¨æ”¶æ•›åˆ°å±€éƒ¨æœ€ä¼˜ï¼Œçœ‹ç®—æ³•ã€‚

![](img/48e18ce5e1ead3f007b289e04322c746.png)![](img/1265650cc14c91d01d7b0f69bad8eb3e.png)

**ä¸Šè¿°æ–¹ç¨‹çš„æ¨å¯¼**:

![](img/4b52f563b48cb4b13105e0e52c246d01.png)

> *è¿™é‡ŒÎ±[alpha]æ˜¯æˆ‘ä»¬çš„å­¦ä¹ ç‡ã€‚*

å¦‚æœÎ±[alpha]å¤ªå°ï¼Œæ¸å˜ä¼šå¾ˆæ…¢ï¼Œå› ä¸ºå®ƒéœ€è¦ä¸€æ­¥ä¸€æ­¥åœ°æ›´æ–°ã€‚å¦‚æœÎ±[alpha]å¤ªå¤§ï¼Œæ¢¯åº¦ä¸‹é™å¯èƒ½è¶…è¿‡å±€éƒ¨æœ€ä¼˜ï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•æ”¶æ•›ç”šè‡³å‘æ•£ã€‚

![](img/98a0975b88fcf01d6a7840ec4a8be046.png)

å›¾ 8

æ‰€ä»¥å°½é‡é€‰æ‹©Î±[alpha]çš„å€¼ï¼Œæ¯”å¦‚ 0.001ï¼Œ0.003ï¼Œ0.01ï¼Œ0.1ï¼Œâ€¦

## **ç”¨ python å®ç°**

æˆ‘ä»¬å°†çœ‹åˆ°å¹¶å°è¯•é¢„æµ‹å‡ºç§Ÿè½¦çš„å‡ºç§Ÿè½¦è´¹æ•°æ®é›†ã€‚

**å¯¼å…¥åŒ…**

```
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
```

**å¯¼å…¥æ•°æ®é›†**

```
data = pd.read_csv("cab_fare.csv")
data.head()ID  Distance  fare_amount  passenger_count          
       "0        0  5.393882         15.0              1.0  
       "1        1  0.014134         52.0              6.0  
       "2        2  5.523719         15.5              1.0  
       "3        3  1.392056          6.5              1.0  
       "4        4  4.150647         11.0              1.0  
       "...    ...       ...          ...              ...                      
       "16044  122  1.962983          8.1              1.
       "16045  123  3.400518         12.5              2.0                      
       "16046  124  1.617977          8.5              1.0                     
       "16047  125  2.466481          8.0              1.0                      
       "16048  127  1.498280          8.5              1.0
```

**å°†æ•°æ®æ‹†åˆ†æˆ X å’Œ Y** å’Œ**å¹¶è½¬æ¢æˆçŸ©é˜µ**

```
X_1 = data.drop(['fare_amount','pickup_datetime','ID'],axis=1)
y_1 = data['fare_amount']
Y_1 = np.expand_dims(Y_1,axis =1 )X = np.matrix(X_1)
X.shape = (16049, 2)Y = np.matrix(Y_1)
Y.shape = (16049, 1)m,n = X.shape           #m =16049 n =2X0 = np.ones((m,1))
X = np.hstack((X0,X))   #Concatenating X0 is our bias and our X
```

**åˆå§‹åŒ–Î¸[theta]çš„å€¼**

```
theta = np.zeros((n+1,1))  # initially assigning the value to zeros
print(theta)[[0.]
 [0.]
 [0.]]
```

**åˆ›å»ºæˆ‘ä»¬å°†åœ¨ç¨‹åºä¸­ä½¿ç”¨çš„å‡½æ•°**

```
def prediction(X,theta): prediction = X*theta
    return predictiondef cost(y,pred): m,n = y.shape
    J = (1/(2*m))*(np.sum(np.square(pred - y)))
    return Jdef grad(X,prediction,y):

    g = (1/m) * (X.T * (prediction - y))
    return gdef LinearRegression(X,y,theta,alpha,epoch):

    j_history=[]

    for i in range(epoch):

        pred = prediction(X,theta)
        J = cost(y,pred)
        g = grad(X,pred,y)

        theta = theta - alpha*g

        j_history = np.append(j_history,J)

        print("Epoch : ",i," ","cost : ",J)

    x = np.linspace(0,epoch,epoch)
    plt.ylabel("cost function")
    plt.plot(x,j_history,color='r')
    plt.xlabel("No. of iterations")
    plt.title("Decreasing of cost function")

    return theta,pred
```

**è°ƒç”¨å‡½æ•°**

```
theta,pred = LinearRegression(X,y,theta,alpha = 0.03,epoch = 1000)Epoch :  0   cost :  107.22753436974266
Epoch :  1   cost :  14.868966072066558
Epoch :  2   cost :  10.872687041778397
Epoch :  3   cost :  10.66332230545673
Epoch :  4   cost :  10.61995377513655
Epoch :  5   cost :  10.586946000902037
Epoch :  6   cost :  10.557104107786772
Epoch :  7   cost :  10.529680767063443
Epoch :  8   cost :  10.504281786250193
Epoch :  9   cost :  10.480591591287318
Epoch :  10  cost :  10.458350239402089
Epoch :  11  cost :  10.437343188331878
Epoch :  12  cost :  10.417393249074857
Epoch :  13  cost :  10.3983539796047
Epoch :  14  cost :  10.380104249118626
Epoch :  15  cost :  10.362543764862313
Epoch :  16  cost :  10.345589391029318
Epoch :  17  cost :  10.329172119456064
Epoch :  18  cost :  10.3132345766786
Epoch :  19  cost :  10.297728972361188
Epoch :  20  cost :  10.282615410929923
Epoch :  21  cost :  10.267860502088192
Epoch :  22  cost :  10.253436217282786
Epoch :  23  cost :  10.239318948563623
.
.
.
..
Epoch :  999 cost :  9.357530142476707
```

![](img/b46e062424321832804c4bd9cc44b6a9.png)

æˆ‘ä»¬å¯ä»¥åœ¨ä¸Šé¢çš„å›¾åƒä¸­çœ‹åˆ°ï¼Œéšç€çºªå…ƒçš„è¿›è¡Œï¼Œå€¼åœ¨å‡å°‘ã€‚

![](img/924a8ec87fc331dbe35e9cc869fceff0.png)

å¦‚æœä½ æ²¡æœ‰çœ‹è¿‡æˆ‘ä¹‹å‰å…³äº K-means èšç±»çš„åšå®¢ï¼Œç‚¹å‡»è¿™ä¸ªé“¾æ¥:[https://medium . com/analytics-vid hya/K-means-clustering-with-python-77 b 20 c 2d 538d](/analytics-vidhya/k-means-clustering-with-python-77b20c2d538d)

å¦‚æœä½ è§‰å¾—è¿™ç¯‡æ–‡ç« æœ‰å¸®åŠ©ï¼Œè¯·é¼“æŒğŸ‘æ‹æ‰‹å¯ä»¥è®©æ›´å¤šçš„äººçœ‹åˆ°ä¸€ä¸ªå¸–å­ã€‚