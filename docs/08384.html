<html>
<head>
<title>Convolutional Neural Networks-An Intuitive approach-Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卷积神经网络-直观的方法-第二部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/convolutional-neural-networks-an-intuitive-approach-part-2-729bfb5e4d87?source=collection_archive---------20-----------------------#2020-07-27">https://medium.com/analytics-vidhya/convolutional-neural-networks-an-intuitive-approach-part-2-729bfb5e4d87?source=collection_archive---------20-----------------------#2020-07-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="af75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">前一篇文章的续篇</p><p id="78eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请找到第1部分的链接</p><div class="jd je ez fb jf jg"><a rel="noopener follow" target="_blank" href="/@nikethnarasimhan/convolutional-neural-networks-an-intuitive-approach-part-1-c8d526a43e3e"><div class="jh ab dw"><div class="ji ab jj cl cj jk"><h2 class="bd hj fi z dy jl ea eb jm ed ef hh bi translated">卷积神经网络-直观的方法-第1部分</h2><div class="jn l"><h3 class="bd b fi z dy jl ea eb jm ed ef dx translated">简单而全面的概念方法</h3></div><div class="jo l"><p class="bd b fp z dy jl ea eb jm ed ef dx translated">medium.com</p></div></div><div class="jp l"><div class="jq l jr js jt jp ju jv jg"/></div></div></a></div><p id="850d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们已经知道了卷积到底是什么，让我们看看如何使用它来完成各种任务，如图像分类等</p><p id="e922" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> CNN架构</strong></p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="ab fe cl kb"><img src="../Images/c491042e8c67cc7551d9089572b0f69e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*jGp7MieTNCjmtrJwml8rQQ.png"/></div></figure><p id="9a3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上所述，CNN有两个组成部分:</p><ul class=""><li id="8d6e" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc ki kj kk kl bi translated"><strong class="ih hj">隐藏层/特征提取部分</strong></li></ul><p id="3bc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这一部分中，网络将执行一系列的<strong class="ih hj">卷积</strong>和<strong class="ih hj">汇集</strong>操作，在此期间检测<strong class="ih hj">特征</strong>。如果你有一张老虎的照片，这是网络可以识别条纹、四条腿、两只眼睛、一个鼻子、独特的橙色等的部分。</p><ul class=""><li id="6289" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc ki kj kk kl bi translated"><strong class="ih hj">分类部分</strong></li></ul><p id="9c9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，完全连接的层将在这些提取的特征之上充当<strong class="ih hj">分类器</strong>。他们将为图像上的物体分配一个<strong class="ih hj">概率</strong>，该物体就是算法预测的物体。</p><p id="6be5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输入馈入卷积层</p><p id="558a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">卷积层:</p><p id="0b75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们前面已经了解了卷积的基本概念。</p><p id="fce9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们学习两个新的重要概念</p><h1 id="7f65" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">步幅:</h1><p id="8c5b" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">跨距是在输入矩阵上移动的像素数。当步幅为1时，我们一次移动一个像素的过滤器。当步幅为2时，我们每次将过滤器移动2个像素，依此类推。<strong class="ih hj">步幅</strong>是卷积滤波器每次移动的步长。步幅大小通常为1，这意味着过滤器逐像素滑动。通过增加步长，滤波器以更大的间隔在输入上滑动，从而减少像元之间的重叠。这意味着激活中的每个输出值将更加独立于相邻值。从下面可以看出</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es lp"><img src="../Images/a46f376a04a69cb7b3962c13a9989a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/0*CbYD3cXrvb2tTEzY"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">内核在输入上移动，大小为1，填充为1(边界处为1层)</figcaption></figure><h1 id="cc68" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">填充:</h1><p id="8f56" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">有时，滤波器并不完全适合输入图像。我们有两个选择:</p><ul class=""><li id="7664" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc ki kj kk kl bi translated">用零填充图片，使其适合</li><li id="d524" class="kd ke hi ih b ii lu im lv iq lw iu lx iy ly jc ki kj kk kl bi translated">删除图像中不适合滤镜的部分。这称为有效填充，仅保留图像的有效部分。</li></ul><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es lp"><img src="../Images/a46f376a04a69cb7b3962c13a9989a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/0*CbYD3cXrvb2tTEzY"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">用零填充5x5图像以获得5x5x1输出</figcaption></figure><p id="5f8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们将5x5x1图像放大为6x6x1图像，然后对其应用3x3x1内核时，我们发现卷积矩阵的维数为5x5x1。因此得名— <strong class="ih hj">同填充</strong>。</p><p id="22ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一方面，如果我们在没有填充的情况下执行相同的操作，我们会看到一个矩阵，它具有内核(3x3x1)本身的维度— <strong class="ih hj">有效填充</strong>。</p><p id="aa14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意:添加填充会增加输出音量，而stride会降低输出音量。</strong></p><h1 id="adbe" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated"><strong class="ak">特征图</strong></h1><p id="629b" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">在CNN的情况下，使用<strong class="ih hj">滤波器</strong>或<strong class="ih hj">核</strong>(两者指同一事物)<strong class="ih hj"> </strong>对输入数据执行卷积，然后产生<strong class="ih hj">特征图(输出)</strong>。</p><p id="bded" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">计算参数个数:</strong></p><p id="9d55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是计算参数的步骤:</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es lz"><img src="../Images/99d6aff5bebe1e98ce3570dfa715d89b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SdsifdTST93VJYeU7hD8hQ.png"/></div></div></figure><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es me"><img src="../Images/757420941f48cbbd4976cc2cbcfe6571.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*wZ-tt8X79Gzb2m9A9LadoA.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">上述公式中的输入和输出维度相同</figcaption></figure><p id="890c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">非线性(ReLU) </strong></p><p id="924e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ReLU代表用于非线性操作的整流线性单元。输出为<strong class="ih hj"><em class="mf">(x)= max(0，x)。</em> </strong></p><p id="bd02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为什么ReLU很重要:ReLU的目的是在我们的ConvNet中引入非线性。因为，真实世界的数据需要我们的ConvNet学习非负线性值。</p><p id="e2b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ReLU很重要，因为它不会饱和；如果神经元激活，梯度总是高的(等于1)。只要不是死神经元，连续更新还是相当有效的。ReLU评价也很快。</p><p id="8327" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与sigmoid或tanh相比，两者都是饱和的(如果输入非常高或非常低，梯度非常非常小)。</p><p id="ea75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更一般地说，非线性激活函数很重要，因为你试图学习的函数通常是非线性的。如果不使用非线性激活函数，网络将是一个大的线性分类器，并且可以通过简单地将权重矩阵相乘来简化(考虑偏差)。它不能做任何有趣的事情，比如图像分类或文本预测。</p><h2 id="0e62" class="mg kn hi bd ko mh mi mj ks mk ml mm kw iq mn mo la iu mp mq le iy mr ms li mt bi translated">池层:</h2><p id="633c" class="pw-post-body-paragraph if ig hi ih b ii lk ik il im ll io ip iq lm is it iu ln iw ix iy lo ja jb jc hb bi translated">当图像太大时，池层部分将减少参数的数量。空间池也称为二次采样或下采样，可减少每个地图的维度，但保留重要信息。空间池可以有不同的类型:</p><ul class=""><li id="c374" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc ki kj kk kl bi translated">最大池化</li><li id="de74" class="kd ke hi ih b ii lu im lv iq lw iu lx iy ly jc ki kj kk kl bi translated">平均池</li></ul><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es mu"><img src="../Images/2775aab2ff1d49e139e7fc416d56a769.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/0*jZPmSYnxeZ78Ws43.gif"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">最大池化</figcaption></figure><p id="c79a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">左边是最大池的示例，其中最大值选自每个slde期间形成的矩阵。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es mv"><img src="../Images/30ed2088f28733346c60e0cd917151b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*lgyPgaPtQJG5Yfv_.png"/></div></figure><p id="2bd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如左图所示，平均池选择幻灯片中形成的矩阵中所有数字的平均值。</p><p id="c34f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如12，20，8和12的平均数是13。</p><p id="4e1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">;</p><p id="924a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">联营的优势:</p><ol class=""><li id="fe36" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc mw kj kk kl bi translated"><strong class="ih hj">降维:</strong>在深度学习中，当我们训练一个模型时，由于数据量过大，模型可能需要花费大量的时间进行训练。现在考虑使用5x5大小的最大池，1步。它将给定图像的大小为5×5的连续区域缩小为1×1区域，最大值为5×5区域。在这里，池化将25 (5x5)个像素减少到单个像素(1x1)，以避免“维数灾难”(参见该术语，机器学习中的重要概念)。</li><li id="3946" class="kd ke hi ih b ii lu im lv iq lw iu lx iy ly jc mw kj kk kl bi translated"><strong class="ih hj">旋转/位置不变特征提取:</strong> Pooling也可以用于提取旋转和位置不变特征。考虑使用大小为5x5的池的相同示例。池化从给定的5x5区域提取最大值。基本上从给定区域提取主要特征值(最大值),而不管特征值的位置。最大值将来自区域内的任何位置。池化不捕获最大值的位置，因此提供了旋转/位置不变的特征提取。</li><li id="983c" class="kd ke hi ih b ii lu im lv iq lw iu lx iy ly jc mw kj kk kl bi translated"><strong class="ih hj">减少过度拟合:</strong>输入中更高的维数意味着我们需要使用更多的参数，这会导致过度拟合。因此，我们需要一种方法来减少这种维数，以便我们可以避免过度拟合，这是由池层执行的。</li></ol><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es mx"><img src="../Images/e9127e1d52b794d8788d443e529c02df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*txiZIsLfO_3O6_HOup5RHA.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">联营业务。</figcaption></figure><p id="54d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分类:</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es my"><img src="../Images/0bfa5e2202ba4d0568fdb4a88012b4c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7HouFuOFHC8G447X.jpeg"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">CNN架构</figcaption></figure><p id="b387" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">全连接图层:</strong></p><p id="f622" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上图中可以看出，一个14x14x3的矩阵被展平成一个588x1的单层</p><p id="659c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然我们已经将输入图像转换成适合多层感知器的形式，我们应该将图像展平成一个列向量。平坦化的输出被馈送到前馈神经网络，并且<strong class="ih hj">反向传播</strong>被应用于训练的每次迭代。在一系列时期内，该模型能够区分图像中的主要特征和某些低级特征，并使用<strong class="ih hj"> Softmax分类</strong>技术(使用Softmax激活功能)对其进行分类。</p><p id="b038" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">总结</strong></p><ul class=""><li id="c54e" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc ki kj kk kl bi translated">将输入图像提供给卷积层</li><li id="4b35" class="kd ke hi ih b ii lu im lv iq lw iu lx iy ly jc ki kj kk kl bi translated">选择参数，如果需要，使用步长、填充来应用过滤器。对图像执行卷积，并对矩阵应用ReLU激活。</li><li id="917b" class="kd ke hi ih b ii lu im lv iq lw iu lx iy ly jc ki kj kk kl bi translated">执行池化以减少维度大小</li><li id="4ab0" class="kd ke hi ih b ii lu im lv iq lw iu lx iy ly jc ki kj kk kl bi translated">添加尽可能多的卷积层，直到满意为止</li><li id="b6a3" class="kd ke hi ih b ii lu im lv iq lw iu lx iy ly jc ki kj kk kl bi translated">展平输出并馈入完全连接的层(FC层)</li><li id="768a" class="kd ke hi ih b ii lu im lv iq lw iu lx iy ly jc ki kj kk kl bi translated">使用激活函数(带有成本函数的逻辑回归)输出类并对图像进行分类。</li></ul><p id="f3c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">CNN的架构有多种类型，下面列出了一些，感兴趣的人可以研究一下！！！我会试着在后续的文章中介绍它们！！</p><ol class=""><li id="b722" class="kd ke hi ih b ii ij im in iq kf iu kg iy kh jc mw kj kk kl bi translated">LeNet</li><li id="3718" class="kd ke hi ih b ii lu im lv iq lw iu lx iy ly jc mw kj kk kl bi translated">AlexNet</li><li id="1da8" class="kd ke hi ih b ii lu im lv iq lw iu lx iy ly jc mw kj kk kl bi translated">VGGNet</li><li id="1cc9" class="kd ke hi ih b ii lu im lv iq lw iu lx iy ly jc mw kj kk kl bi translated">谷歌网</li><li id="fdbd" class="kd ke hi ih b ii lu im lv iq lw iu lx iy ly jc mw kj kk kl bi translated">雷斯内特</li><li id="fb75" class="kd ke hi ih b ii lu im lv iq lw iu lx iy ly jc mw kj kk kl bi translated">ZFNet</li></ol></div></div>    
</body>
</html>