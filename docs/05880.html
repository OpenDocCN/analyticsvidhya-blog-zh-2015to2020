<html>
<head>
<title>Understanding the KNN-Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解KNN算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-the-knn-algorithm-1a69fa4e9617?source=collection_archive---------27-----------------------#2020-05-04">https://medium.com/analytics-vidhya/understanding-the-knn-algorithm-1a69fa4e9617?source=collection_archive---------27-----------------------#2020-05-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6fce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">KNN是一种监督学习算法，用于回归和分类问题。虽然主要用于分类。KNN试图通过计算测试数据和所有训练点之间的距离来预测测试数据的正确类别。然后，它选择最接近数据的k点。</p><ol class=""><li id="80e5" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">给定如下所示的训练数据集。我们有一个新的测试数据，需要分配给两个类中的一个。</li></ol><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es jm"><img src="../Images/c76261c26f073ee43ac83174347f6ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*pmpishR_2AEr3DhjLu4TZg.png"/></div></figure><p id="af6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2)现在，k-NN算法计算测试数据和给定训练数据之间的距离。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es ju"><img src="../Images/155226fee13a37e3e891a7ca850fbea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*ZLGjoSZUKWBgIziZ7mlSQA.png"/></div></figure><p id="9db6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3)在计算距离之后，它将选择与测试数据最近的k个训练点。在我们的例子中，假设k的值是3。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es jv"><img src="../Images/1dfe6e51ce14f2a99b51afff6591fad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*FXszMT74i3Bfb6Ie8nilcQ.png"/></div></figure><p id="d382" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以这里K=3，绿色=2，红色=1。KNN将此点归类为绿色。这是一个分类示例，对于回归，预测值是k个选定点的平均值。这种方法被称为计算距离的蛮力KNN，有许多计算距离的方法，如欧几里德，曼哈顿和汉明等。</p><p id="e4b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">欧几里德距离:</p><ul class=""><li id="760f" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc jw jj jk jl bi translated">最常见的方法。</li><li id="9a60" class="jd je hi ih b ii jx im jy iq jz iu ka iy kb jc jw jj jk jl bi translated">考虑两点P(p1，p2)和Q(q1，q2)欧几里得距离将为ED(p，q)=平方根[(q1-p1)**2 + (q2-p2)**2]</li></ul><p id="d530" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">曼哈顿距离:</p><ul class=""><li id="5f91" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc jw jj jk jl bi translated">距离表示向量中绝对值之间的绝对差之和。</li><li id="8f33" class="jd je hi ih b ii jx im jy iq jz iu ka iy kb jc jw jj jk jl bi translated">MD(p，q) = |q1-p1|+|q2-p2|</li><li id="f947" class="jd je hi ih b ii jx im jy iq jz iu ka iy kb jc jw jj jk jl bi translated">受异常值的影响比欧几里德距离小。优选使用非常高维的数据集。</li></ul><p id="7aaa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">蛮力KNN在计算上非常昂贵，所以有其他方法，如kd树和球树(它们更有效，可以在高维数据集中更好地工作)。</p><p id="47e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择K的值:</p><ul class=""><li id="6bae" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc jw jj jk jl bi translated">低k值具有高方差和低偏差。k值较低时，数据可能会过度拟合。</li><li id="5e1f" class="jd je hi ih b ii jx im jy iq jz iu ka iy kb jc jw jj jk jl bi translated">高k值具有减少的方差和增加的偏差。对于高k值，数据中存在欠拟合的可能性。</li></ul><p id="c35b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">KNN算法是一个懒惰的学习者？是的，它是。与其他算法不同，如SVM、逻辑和贝叶斯算法，它们是求知欲强的学习者，在获得测试数据集之前，在训练数据集上进行归纳。换句话说，他们根据训练数据集创建模型，然后根据测试数据集进行预测。KNN是一个懒惰的学习者，所以它等待测试数据集，在获得测试数据集后，它开始在训练数据集上进行推广。懒惰算法在训练时工作较少，在测试时工作较多。</p><p id="6fbc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将向大家展示一个使用UCI数据库中的糖尿病数据集的KNN算法的例子。我将要执行的步骤是:</p><ol class=""><li id="2846" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">数据清理</li><li id="7b1e" class="jd je hi ih b ii jx im jy iq jz iu ka iy kb jc ji jj jk jl bi translated">数据可视化</li><li id="0872" class="jd je hi ih b ii jx im jy iq jz iu ka iy kb jc ji jj jk jl bi translated">模型创建</li><li id="84be" class="jd je hi ih b ii jx im jy iq jz iu ka iy kb jc ji jj jk jl bi translated">使用GridSearchCV选择k值</li><li id="4778" class="jd je hi ih b ii jx im jy iq jz iu ka iy kb jc ji jj jk jl bi translated">使用交叉值评分的模型验证</li></ol><pre class="jn jo jp jq fd kc kd ke kf aw kg bi"><span id="8c24" class="kh ki hi kd b fi kj kk l kl km">#importing libraries<br/>import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV<br/>from statsmodels.stats.outliers_influence import variance_inflation_factor <br/>from sklearn.metrics import accuracy_score<br/>%matplotlib inline</span><span id="460b" class="kh ki hi kd b fi kn kk l kl km">#reading data<br/>data=pd.read_csv('diabetes.csv')<br/>data.head()</span></pre><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ko"><img src="../Images/7216d2fd4dbb1f557f8999c424f87f30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nxWeJd9YtksHEsfEHNdUYA.png"/></div></div></figure><p id="f047" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据集中没有缺失值，但是，存在异常值，并且一些变量是倾斜的。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es kt"><img src="../Images/c296847162ff5a917fe7af6a6c695831.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gRFO6eLov1MzOH9q2uuH_w.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">检查变量的分布</figcaption></figure><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ky"><img src="../Images/7dca6fe896322b8e6b9159b13afe7fb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pdFzNFTXSlcGiMiLzZkbSA.png"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">极端值</figcaption></figure><p id="04ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在清理数据集并从中移除离群值后，我们接着继续创建KNN模型。首先，我们将数据分为训练和测试，之后我们将训练模型并进行预测。然后，我们将执行GridSearch操作来找到最佳参数，并检查我们的初始模型是否过拟合。</p><pre class="jn jo jp jq fd kc kd ke kf aw kg bi"><span id="e8cc" class="kh ki hi kd b fi kj kk l kl km">x_train,x_test,y_train,y_test = train_test_split(df,y, test_size= 0.3,random_state=42)</span><span id="a16a" class="kh ki hi kd b fi kn kk l kl km">knn = KNeighborsClassifier()<br/>knn.fit(x_train,y_train)</span><span id="025e" class="kh ki hi kd b fi kn kk l kl km">KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',metric_params=None, n_jobs=None, <strong class="kd hj">n_neighbors=5</strong>, p=2,weights='uniform')</span><span id="6b38" class="kh ki hi kd b fi kn kk l kl km">y_pred = knn.predict(x_test)</span><span id="09a5" class="kh ki hi kd b fi kn kk l kl km">knn.score(x_train,y_train)<br/><strong class="kd hj">0.8435754189944135</strong></span><span id="c22c" class="kh ki hi kd b fi kn kk l kl km">print("The accuracy score is : ", accuracy_score(y_test,y_pred))<br/><strong class="kd hj">The accuracy score is :  0.7272727272727273</strong></span><span id="6393" class="kh ki hi kd b fi kn kk l kl km">param_grid = { 'algorithm' : ['ball_tree', 'kd_tree', 'brute'],<br/>               'leaf_size' : [18,20,25,27,30,32,34],<br/>               'n_neighbors' : [3,5,7,9,10,11,12,13]<br/>              }</span><span id="46ee" class="kh ki hi kd b fi kn kk l kl km">gridsearch = GridSearchCV(knn, param_grid).fit(x_train,y_train)</span><span id="788a" class="kh ki hi kd b fi kn kk l kl km">gridsearch.best_params_<br/><strong class="kd hj">{'algorithm': 'ball_tree', 'leaf_size': 18, 'n_neighbors': 11}</strong></span><span id="3573" class="kh ki hi kd b fi kn kk l kl km">knn = KNeighborsClassifier(algorithm = 'ball_tree', leaf_size =18, n_neighbors =11)</span><span id="f997" class="kh ki hi kd b fi kn kk l kl km">knn.fit(x_train,y_train)</span><span id="4d0c" class="kh ki hi kd b fi kn kk l kl km">y_pred = knn.predict(x_test)</span><span id="a2fd" class="kh ki hi kd b fi kn kk l kl km">knn.score(x_train,y_train)<br/><strong class="kd hj">0.8063314711359404</strong></span><span id="7f79" class="kh ki hi kd b fi kn kk l kl km">print("The accuracy score is : ", accuracy_score(y_test,y_pred))<br/><strong class="kd hj">The accuracy score is :  0.7445887445887446</strong></span><span id="1214" class="kh ki hi kd b fi kn kk l kl km">cv_results = cross_val_score(knn, x_test, y_test)<br/>msg = "%s: %f (%f)" % ('KNN', cv_results.mean(), cv_results.std())<br/>print(msg)<br/><strong class="kd hj">KNN: 0.807018 (0.040441)</strong></span></pre><p id="6ab9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你所看到的，KNN最初过度适应了，因为我们得到了84分的高训练分数，但只有72分的低测试分数<strong class="ih hj"/>。后来使用GridSearch参数，我们得到了一个低的训练分数<strong class="ih hj"> 80 </strong>，但是一个更高的测试分数<strong class="ih hj"> 74 </strong>，所以可以说KNN过度拟合了模型。过度拟合的一个原因是因为我们没有提供n _ neighbours值，KNN采用了默认值5。回想一下上面我提到的小n值可能会导致过拟合。</p><p id="4f5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我试图通过这篇文章用最简单的形式来解释KNN。对于初学者来说，KNN是一个很好的算法，它简单易懂，它的数学部分也容易掌握，可以很容易地向任何人解释。</p></div></div>    
</body>
</html>