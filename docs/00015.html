<html>
<head>
<title>An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对单词嵌入的直观理解:从计数向量到Word2Vec</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/an-intuitive-understanding-of-word-embeddings-from-count-vectors-to-word2vec-8231e18dbe92?source=collection_archive---------1-----------------------#2017-06-04">https://medium.com/analytics-vidhya/an-intuitive-understanding-of-word-embeddings-from-count-vectors-to-word2vec-8231e18dbe92?source=collection_archive---------1-----------------------#2017-06-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6415" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们开始之前，看看下面的例子。</p><ol class=""><li id="7211" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">你打开谷歌搜索一篇关于正在进行的冠军奖杯的新闻文章，得到数百个关于它的搜索结果。</li><li id="21fd" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">内特·西尔弗分析了数百万条推文，并正确预测了2008年美国总统选举50个州中49个州的结果。</li><li id="d33b" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">你在谷歌翻译中输入一个英语句子，就会得到一个对等的中文翻译。</li></ol><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es jr"><img src="../Images/62e5894398bc5d08493d66f8a842eadb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*QgTeWquPRMRiUKQR.png"/></div></div></figure><p id="ed52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么以上例子有什么共同点呢？</p><p id="3c57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可能猜对了—<strong class="ih hj">文本处理</strong>。以上三种场景都需要处理海量的文本来执行不同的任务，比如谷歌搜索中的聚类，第二种情况下的分类，第三种情况下的机器翻译。</p><p id="05ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">人类可以非常直观地处理文本格式，但是假设我们在一天内生成了数百万个文档，我们不可能让人类执行上述三项任务。它既不可扩展也不有效。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es kd"><img src="../Images/6f65f7d22538657490d9a0058a1dd626.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/0*InbYRgEVT56P4Mxp.jpg"/></div></figure><p id="5fa6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，既然我们知道当今的计算机在处理字符串或文本以获得任何有成效的输出时通常效率低下，那么我们如何让它们对文本数据执行聚类、分类等操作呢？</p><p id="a837" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当然，计算机可以匹配两个字符串，并告诉你它们是否相同。但是我们如何让计算机在你搜索梅西的时候告诉你足球或者c罗？如何让一台电脑明白《苹果是一种美味的水果》中的“苹果”是一种可以吃的水果而不是一家公司？</p><p id="017c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上问题的答案在于创建一个单词的表示法，该表示法捕捉它们的<em class="ke">含义</em>、<em class="ke">语义关系</em>以及它们在不同类型的上下文中的使用。</p><p id="a247" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有这些都是通过使用单词嵌入或文本的数字表示来实现的，以便计算机可以处理它们。</p><p id="288f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面，我们将正式看到什么是单词嵌入及其不同类型，以及我们如何实际实现它们来执行任务，如返回高效的Google搜索结果。</p><h1 id="c1e6" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">目录</h1><ol class=""><li id="0450" class="jd je hi ih b ii ld im le iq lf iu lg iy lh jc ji jj jk jl bi translated">什么是单词嵌入？</li><li id="6b4d" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">不同类型的单词嵌入<br/> 2.1基于频率的嵌入<br/> 2.1.1计数向量<br/> 2.1.2 TF-IDF <br/> 2.1.3同现矩阵<br/> 2.2基于预测的嵌入<br/>2 . 2 . 1 CBOW<br/>2 . 2 . 2 Skip-Gram</li><li id="07ce" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">单词嵌入用例场景(使用单词嵌入可以做什么？例如:相似性、独特性等。)</li><li id="8401" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">使用预先训练的单词向量</li><li id="dbb7" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">训练你自己的单词向量</li><li id="33b8" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">结束注释</li></ol><h1 id="2120" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">1.什么是单词嵌入？</h1><p id="a1a2" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">简单来说，单词嵌入是将文本转换成数字，同一文本可能有不同的数字表示。但是在我们深入到单词嵌入的细节之前，应该问下面的问题——我们为什么需要单词嵌入？</p><p id="cf17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">事实证明，许多机器学习算法和几乎所有的深度学习架构都无法处理原始形式的<em class="ke">字符串</em>或<em class="ke">纯文本</em>。他们需要数字作为输入来执行任何种类的工作，无论是分类、回归等等。广义而言。随着大量数据以文本格式呈现，从中提取知识并构建应用程序势在必行。文本应用的一些真实世界的应用是——亚马逊评论的情感分析等。通过Google等进行文档或新闻分类或聚类。</p><p id="cb19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们正式定义单词嵌入。单词嵌入格式通常尝试使用字典将单词映射到向量。让我们把这个句子分解成更细的细节，以便看得更清楚。</p><p id="7f74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看看这个例子——<strong class="ih hj">句子</strong> =“单词嵌入是将单词转换成数字”</p><p id="1ad3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个<strong class="ih hj">句</strong>中的一个<em class="ke">字</em>可能是“嵌入”或“数字”等。</p><p id="9c20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个<em class="ke">字典</em>可以是<strong class="ih hj">句子中所有唯一单词的列表。</strong>因此，字典可能看起来像——<br/>['单词'，'嵌入'，'是'，'转换'，'转换'，'数字']</p><p id="2fb8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个字的ve <em class="ke"> ctor </em>表示可以是一个独热编码的向量，其中1代表该字存在的位置，0代表其它任何位置。根据上述字典，这种格式的“数字”<strong class="ih hj"> </strong>的向量表示为[0，0，0，0，0，1]，转换后的向量表示为[0，0，0，1，0，0]。</p><p id="1564" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这只是一个非常简单的方法来表示一个矢量形式的单词。让我们看看不同类型的单词嵌入或单词向量，以及它们相对于其他类型的优缺点。</p><h1 id="7e5b" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">2.不同类型的单词嵌入</h1><p id="3842" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">不同类型的单词嵌入可以大致分为两类</p><ol class=""><li id="cfe6" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">基于频率的嵌入</li><li id="e431" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">基于预测的嵌入</li></ol><p id="eaf4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们试着详细理解这些方法。</p><h1 id="e343" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">2.1基于频率的嵌入</h1><p id="36ac" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">在这一类别下，我们通常会遇到三种类型的病媒。</p><ol class=""><li id="aa42" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">计数向量</li><li id="256d" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">TF-IDF载体</li><li id="b7b9" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">共现向量</li></ol><p id="9156" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们详细研究一下这些矢量化方法。</p><h2 id="77a8" class="ll kg hi bd kh lm ln lo kl lp lq lr kp iq ls lt kt iu lu lv kx iy lw lx lb ly bi translated">计数向量</h2><p id="85d3" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">考虑D个文档的语料库{ d1，d2…..这N个记号将形成我们的字典，并且计数向量矩阵M的大小将由D×N给出。矩阵M中的每一行包含文档D(i)中记号的频率。</p><p id="ebd1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们用一个简单的例子来理解这一点。</p><p id="3edc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">D1:他是一个懒惰的男孩。她也很懒。</p><p id="84bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">D2:尼拉杰是个懒惰的人。</p><p id="1603" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所创建的字典可以是语料库中的唯一记号(单词)的列表=['He '，' She '，' lazy '，' boy '，' Neeraj '，' person']</p><p id="4207" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，D=2，N=6</p><p id="9ef9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大小为2 X 6的计数矩阵M将表示为</p><p id="b17f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他她懒惰的男孩Neeraj人D1 1 1 2 1 0 0 D2 0 0 1 0 1 1</p><p id="bcce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，一列也可以理解为矩阵m中对应词的词向量，比如上面矩阵中‘lazy’的词向量是[2，1]等等。这里，<em class="ke">行</em>对应于语料库中的<em class="ke">文档</em>，而<em class="ke">列</em>对应于词典中的<em class="ke">标记</em>。上面矩阵中的第二行可以读作——D2包含“懒惰”:once，“Neeraj”:once和“人”once。</p><p id="6727" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，在准备上述矩阵m时，可能会有相当多的变化。这些变化通常在-</p><ol class=""><li id="3bb4" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">字典的编写方式。<br/>为什么？因为在现实世界的应用中，我们可能有一个包含数百万文档的语料库。有了数百万份文件，我们可以提取数亿个独特的单词。所以基本上，像上面那样准备的矩阵是非常稀疏的，对于任何计算都是低效的。因此，除了使用每个独特的单词作为字典元素之外，另一种方法是根据频率选择前10，000个单词，然后准备一本字典。</li><li id="8ceb" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">对每个单词计数的方式。我们可以采用频率(一个单词在文档中出现的次数)或存在(这个单词在文档中出现过吗？)作为计数矩阵m中的条目。但是通常，频率方法优于后者。</li></ol><p id="54f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是矩阵M的代表性图像，以便于理解。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es lz"><img src="../Images/fbf75d8ccf9f0533e9a3b67c702a9d48.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/0*LhbaO8UpdndNUE0e.png"/></div></figure><h2 id="654e" class="ll kg hi bd kh lm ln lo kl lp lq lr kp iq ls lt kt iu lu lv kx iy lw lx lb ly bi translated">TF-IDF矢量化</h2><p id="4990" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">这是另一种基于频率方法的方法，但它不同于计数矢量化，因为它不仅考虑单个文档中的单词出现，还考虑整个语料库中的单词出现。那么，这背后的逻辑是什么呢？让我们试着去理解。</p><p id="4006" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像“is”、“the”、“a”等常用词。与对文档重要的单词相比，往往出现得相当频繁。例如，与其他文档相比，关于莱昂内尔·梅西的文档<strong class="ih hj"> A </strong>将包含更多出现的单词“梅西”。但是像“the”这样的常用词。也会在几乎所有文件中以更高的频率出现。</p><p id="c111" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理想情况下，我们想要的是降低在几乎所有文档中出现的常见单词的权重，并赋予出现在文档子集中的单词更多的重要性。</p><p id="75ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TF-IDF的工作原理是，在某个特定的文档中，通过给这些普通的单词分配较低的权重来惩罚它们，同时给像Messi这样的单词赋予重要性。</p><p id="2e25" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，TF-IDF到底是怎么运作的呢？</p><p id="7ed8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑下面的示例表，它给出了两个文档中的术语(标记/单词)的计数。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es ma"><img src="../Images/7647db1da62f42ce5adbe0f88f7169ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/0*ivI7EwX14DOXuSmw.png"/></div></figure><p id="3cbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们定义几个与TF-IDF相关的术语。</p><p id="fd7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TF =(术语t在文档中出现的次数)/(文档中的术语数)</p><p id="87fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，TF(This，Document1) = 1/8</p><p id="1245" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TF(This，文档2)=1/5</p><p id="5184" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它表示单词对文档的贡献，即与文档相关的单词应该是频繁出现的。一份关于梅西的文件应该包含大量的“梅西”一词。</p><p id="d174" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">IDF = log(N/n)，其中，N是文档数，N是术语t出现的文档数。</p><p id="b757" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中N是文档的数量，N是术语t出现过的文档的数量。</p><p id="ffaa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，IDF(This) = log(2/2) = 0。</p><p id="f406" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，我们如何解释IDF背后的推理呢？理想情况下，如果一个单词出现在所有文档中，那么这个单词可能与特定文档不相关。但是如果它出现在文档的子集中，那么该单词可能与其出现的文档有某种关联。</p><p id="e9d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们计算一下‘梅西’这个词的IDF。</p><p id="cf2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">IDF(梅西)= log(2/1) = 0.301。</p><p id="6d58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们比较TF-IDF中的一个常用词“This”和一个似乎与文献1相关的词“Messi”。</p><p id="70f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TF-IDF(This，Document1) = (1/8) * (0) = 0</p><p id="b2b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TF-IDF(This，Document2) = (1/5) * (0) = 0</p><p id="40a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TF-IDF(梅西，Document1) = (4/8)*0.301 = 0.15</p><p id="0422" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你在文献1中看到的，TF-IDF方法对单词“This”进行了严重的惩罚，但对“Messi”赋予了更大的权重。因此，从整个语料库的上下文来看，这可以理解为“梅西”是Document1的一个重要单词。</p><h2 id="842c" class="ll kg hi bd kh lm ln lo kl lp lq lr kp iq ls lt kt iu lu lv kx iy lw lx lb ly bi translated">2.1.3具有固定上下文窗口的共生矩阵</h2><p id="b0fa" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated"><strong class="ih hj">大意</strong>—相似的单词往往会一起出现，并且会有相似的上下文，例如——苹果是一种水果。芒果是一种水果。<br/>苹果和芒果往往有相似的语境，即水果。</p><p id="4bf8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我深入研究如何构建共现矩阵的细节之前，有两个概念需要澄清——共现和上下文窗口。</p><p id="55aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">共现-对于给定的语料库，一对词如w1和w2的共现是它们在上下文窗口中一起出现的次数。</p><p id="bdc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上下文窗口–上下文窗口由一个数字和方向指定。那么2(左右)的上下文窗口是什么意思呢？让我们看看下面的例子，</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es mb"><img src="../Images/4ad70749a923085a9e8e824def5b578a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*kPmF9UDCWorwOjCuLvvHww.png"/></div></figure><p id="7d37" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">绿色单词是单词“Fox”的2(周围)上下文窗口，并且为了计算共现，将只计算这些单词。让我们看看单词“Over”的上下文窗口。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es mc"><img src="../Images/4d9402f7516229d15f83f06f79ff416b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*dvmvf1TQfqdsb4u7-BRAKQ.png"/></div></figure><p id="bfa1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们以一个示例语料库来计算共现矩阵。</p><p id="5b91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他不懒。他很聪明。他很聪明。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es md"><img src="../Images/3da977206dd4471aa2c8fc039421e1b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*APgmsc9v4iUnNjEeAvQkUg.png"/></div></figure><p id="7f5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们通过上表中的两个例子来理解这个共现矩阵。红色和蓝色的盒子。</p><p id="60da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">红框——它是“他”和“是”在上下文窗口2中出现的次数，可以看到计数结果是4。下表将帮助您直观地了解计数。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es me"><img src="../Images/0f7a5800ec3d347f0a8a999909865bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AKWF8LevFq33rnreoRHlbw.png"/></div></div></figure><p id="b575" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">而单词“lazy”从未与“intelligent”一起出现在上下文窗口中，因此在蓝框中被赋值为0。</p><p id="2563" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">共生矩阵的变化</strong></p><p id="0dfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设语料库中有V个唯一词。所以词汇大小= V。共现矩阵的列形成了上下文单词<em class="ke">s。共现矩阵的不同变化是</em></p><ol class=""><li id="f390" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">一个大小为V×V的共现矩阵。现在，即使一个像样的语料库V也会变得非常大，难以处理。所以一般来说，这种架构在实践中从来都不是首选。</li><li id="430d" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">一个大小为V X N的共生矩阵，其中N是V的子集，可以通过移除像停用词等不相关的词来获得。比如说。这仍然非常大，并且存在计算上的困难。</li></ol><p id="1974" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是，记住这个共现矩阵不是通常使用的单词向量表示。相反，使用诸如PCA、SVD等技术来分解这个共生矩阵。这些因素的组合形成了单词向量表示。</p><p id="eaad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我更清楚地说明这一点。例如，您对上述大小为VXV的矩阵执行PCA。你将获得V个主分量。你可以从这些V分量中选择k分量。所以，新矩阵的形式是V X k。</p><p id="b458" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">并且，一个单词，不是在V维中表示，而是在k维中表示，同时仍然捕捉几乎相同的语义。k通常是几百的数量级。</p><p id="da09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，PCA在后面做的是将共生矩阵分解成三个矩阵，U，S和V，其中U和V都是正交矩阵。重要的是，U和S的点积给出了单词向量表示，而V给出了单词上下文表示。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es mf"><img src="../Images/6e2101f441b762c52f93d7400d42b6e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aD6PfKXnT6KFN1fq.png"/></div></div></figure><p id="f44d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">共生矩阵的优势</strong></p><ol class=""><li id="8955" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">它保留了单词之间的语义关系。也就是说，男人和女人比男人和苹果更亲近。</li><li id="ac0c" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">它在其核心使用SVD，这产生了比现有方法更精确的单词向量表示。</li><li id="29ae" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">它使用因式分解，因式分解是一个定义明确的问题，可以有效地解决。</li><li id="4fd7" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">它只需计算一次，一旦计算出来就可以随时使用。从这个意义上说，它比其他的要快。</li></ol><p id="8322" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">共生矩阵的缺点</strong></p><ol class=""><li id="cefe" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">它需要巨大的内存来存储共生矩阵。<br/>但是，这个问题可以通过在系统外分解矩阵来规避，例如在Hadoop集群等中。并且可以被保存。</li></ol><h1 id="1126" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">2.2基于向量的预测</h1><p id="6150" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated"><strong class="ih hj">先决条件</strong>:这一部分假设你对神经网络的工作原理和更新神经网络权重的机制有一定的了解。如果你是神经网络的新手，我建议你浏览一下Sunil的这篇很棒的文章，以便很好地理解神经网络是如何工作的。</p><p id="c40d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，我们已经看到了确定单词向量的确定性方法。但是这些方法被证明是有限的，直到米托洛夫等。el向NLP社区介绍了word2vec。这些方法是基于预测的，从某种意义上说，它们提供了单词的概率，并被证明是像单词类比和单词相似性这样的任务的最先进水平。他们还能够完成像国王-男人+女人=王后这样的任务，这被认为是一个几乎不可思议的结果。因此，让我们看看目前用于生成单词向量的word2vec模型。</p><p id="34ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Word2vec不是一个单一的算法，而是两种技术的结合——CBOW(连续单词包)和Skip-gram模型。这两种都是浅层神经网络，将单词映射到也是单词的目标变量。这两种技术都学习作为单词向量表示的权重。让我们分别讨论这两种方法，并获得它们工作的直觉。</p><h1 id="f31f" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">2.2.1连续单词包</h1><p id="e86b" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">CBOW的工作方式是，它倾向于预测给定上下文中单词出现的概率。上下文可以是单个单词或一组单词。但是为了简单起见，我将采用单个上下文单词并尝试预测单个目标单词。</p><p id="a44c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设，我们有一个语料库C =“嘿，这是只使用一个上下文单词的样本语料库。”并且我们已经定义了1的上下文窗口。该语料库可以如下转换成CBOW模型的训练集。输入如下所示。下图中右侧的矩阵包含左侧输入的一个热码编码形式。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es mh"><img src="../Images/c87800c91678bd8687da95f86a111ec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*L_MBRQmZkR8WHkch.png"/></div></div></figure><p id="faad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">单一数据点(比如数据点4)的目标如下所示</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es mi"><img src="../Images/8c64e13c70e4e46fa85b0c7e077e199f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G77CHYkEGlfimL6vZtjuIQ.png"/></div></div></figure><p id="4db3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图所示的这个矩阵被发送到一个具有三层的浅层神经网络中:输入层、隐藏层和输出层。输出层是softmax层，用于将输出层中获得的概率总和为1。现在让我们看看正向传播如何计算隐藏层激活。</p><p id="445b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们首先来看看CBOW模型的图示。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es mj"><img src="../Images/39040faf1a4c5eed3375dafc93c04ce3.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/0*jxLfI-8sLdan2OBU.png"/></div></figure><p id="8a7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图中单一数据点的矩阵表示如下。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es mf"><img src="../Images/ce5a7024ad32955ae725740e324a8338.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oMYGAbhk1ZsRGmRC.png"/></div></div></figure><p id="c443" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">流程如下:</p><ol class=""><li id="71d3" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">输入层和目标层都是大小为[1 X V]的一键编码。在上面的例子中，V=10。</li><li id="b0d1" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">有两组砝码。一个是在输入层和隐藏层之间，第二个是在隐藏层和输出层之间。<br/>输入-隐藏层矩阵大小=[V X N]，隐藏-输出层矩阵大小=[N X V]:其中N是我们选择用来表示我们的单词的维数。它是神经网络的任意参数和超参数。另外，N是隐藏层中神经元的数量。这里，N=4。</li><li id="aa4e" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">任何层之间都没有激活功能。(更具体地说，我指的是线性激活)</li><li id="249c" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">输入乘以输入隐藏权重，称为隐藏激活。它只是被复制的输入隐藏矩阵中的相应行。</li><li id="8db6" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">隐藏输入乘以隐藏输出权重，然后计算输出。</li><li id="185f" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">计算输出和目标之间的误差，并传播回去以重新调整权重。</li><li id="9f61" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">隐藏层和输出层之间的权重作为单词的单词向量表示。</li></ol><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es mk"><img src="../Images/66db6fd2e0ea9d0283fd767844639c73.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/0*SbXBs-rxGPNzwTEr.png"/></div></figure><p id="ca91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们看到了单个上下文单词的上述步骤。现在，如果我们有多个上下文单词呢？下图描述了多个上下文单词的架构。</p><p id="c284" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是上述架构的矩阵表示，以便于理解。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es mf"><img src="../Images/7c8127531e70c4becb8a4ef05aac4903.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*09c4wiml26ZZxnUM.png"/></div></div></figure><p id="8a1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图采用了3个上下文单词，并预测了目标单词的概率。可以假设输入采用输入层中的三个独热编码矢量，如上面的红色、蓝色和绿色所示。</p><p id="cdc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，如上所示，输入层在输入中将有3个[1 X V]矢量，在输出层中有1个[1 X V]。架构的其余部分与单上下文CBOW相同。</p><p id="55f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤保持不变，只是隐藏激活的计算发生了变化。不是仅仅将输入隐藏权重矩阵的相应行复制到隐藏层，而是对矩阵的所有相应行取平均值。我们可以通过上图来理解这一点。计算出的平均向量成为隐藏激活。因此，如果我们对于单个目标单词有三个上下文单词，我们将有三个初始隐藏激活，然后对它们进行元素平均以获得最终激活。</p><p id="7d19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在单个上下文单词和多个上下文单词中，我都显示了图像，直到计算隐藏激活，因为这是CBOW不同于简单MLP网络的部分。隐层计算后的步骤与本文提到的MLP相同——<a class="ae mg" href="https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/" rel="noopener ugc nofollow" target="_blank">从零开始理解和编码神经网络</a>。</p><p id="72b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MLP和CBOW之间的差异如下:</p><ol class=""><li id="d392" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">在MLP中的目标函数是MSE(均方误差)，而在CBOW中，它是给定一组上下文的单词的负对数似然性，即log(p(wo/wi))，其中p(wo/wi)被给出为</li></ol><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es ml"><img src="../Images/2cf2798cdc5c70434dba9863c59a1c02.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*L34RjsvgZ0F_vuHe.jpg"/></div></figure><p id="b74d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">wo:输出单词<br/> wi:上下文单词</p><p id="2475" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.关于隐藏输出权重和输入隐藏权重的误差梯度是不同的，因为MLP具有sigmoid激活(通常)而CBOW具有线性激活。然而，计算梯度的方法与MLP相同。</p><p id="86e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">CBOW的优势:</strong></p><ol class=""><li id="a4fe" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">概率性是自然的，它被认为比确定性方法表现更好(一般来说)。</li><li id="989f" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">内存不足。它不需要像共生矩阵那样具有巨大的RAM需求，在共生矩阵中它需要存储三个巨大的矩阵。</li></ol><p id="e4cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">CBOW的缺点:</strong></p><ol class=""><li id="aa25" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">CBOW取一个词的上下文的平均值(如上文计算隐藏激活中所见)。例如，苹果既可以是水果，也可以是公司，但CBOW取两者的平均值，将其放在水果和公司之间。</li><li id="e5a2" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">如果没有适当的优化，从头开始训练CBOW可能要花很长时间。</li></ol><h1 id="b664" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">跳过Gram模型</h1><p id="4c93" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">skip–gram遵循与CBOW相同的拓扑结构。它完全颠覆了CBOW的架构。跳格的目的是预测给定单词的上下文。让我们以构建CBOW模型所基于的相同语料库为例。C= "嘿，这是只使用一个上下文单词的样本语料库。"让我们构建训练数据。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es ml"><img src="../Images/9ae88c020efa6e65b7c84ab72c2ca32e.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*YMPOxXn2saUSTXji.png"/></div></figure><p id="8d1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">skip-gram的输入向量将类似于1-context CBOW模型。此外，直到隐藏层激活的计算将是相同的。不同之处在于目标变量。由于我们在两侧都定义了1的上下文窗口，因此将会有“<strong class="ih hj">两个”一个热编码目标变量</strong>和“<strong class="ih hj">两个”相应的输出</strong>，如图中蓝色部分所示。</p><p id="3a39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相对于两个目标变量计算两个单独的误差，并且将获得的两个误差向量逐元素相加以获得最终误差向量，该最终误差向量被传播回去以更新权重。</p><p id="a9e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输入层和隐含层之间的权重作为训练后的单词向量表示。损失函数或目标与CBOW模型的类型相同。</p><p id="c97c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">跳跃图结构如下所示。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es mm"><img src="../Images/480d908a7ba550b94cf03df1f5da7f1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/0*wdDOum0ndxSZxv6M.png"/></div></figure><p id="b1a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好的理解，下面显示了矩阵风格的结构和计算。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="er es mn"><img src="../Images/6a842166df1e2be2da7380e08f0f2e46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*DDHpUfGh-5CPmQEI.png"/></div></div></figure><p id="e413" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来分解一下上面的图像。</p><p id="0438" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输入层大小—[1 X V]，输入隐藏权重矩阵大小—[V X N]，隐藏层中的神经元数量—N，隐藏输出权重矩阵大小—[N X V]，输出层大小—C[1 X V]</p><p id="eaa0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的例子中，C是上下文单词的数量=2，V= 10，N=4</p><ol class=""><li id="84e4" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">红色行是对应于输入独热码编码向量的隐藏激活。基本上就是复制了输入隐藏矩阵的对应行。</li><li id="981f" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">黄色矩阵是隐藏层和输出层之间的权重。</li><li id="220d" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">蓝色矩阵通过隐藏激活和隐藏输出权重的矩阵乘法来获得。将为两个目标(上下文)单词计算两行。</li><li id="250f" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">蓝色矩阵的每一行都被单独转换为其softmax概率，如绿色框中所示。</li><li id="70a5" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">灰色矩阵包含两个上下文单词(目标)的一个热编码向量。</li><li id="7301" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">通过从绿色矩阵(输出)的第一行中按元素减去灰色矩阵(目标)的第一行来计算误差。对下一行重复这一过程。因此，对于<strong class="ih hj"> n个</strong>目标上下文单词，我们将有<strong class="ih hj"> n个</strong>错误向量。</li><li id="7a93" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">对所有误差向量进行逐元素求和，以获得最终误差向量。</li><li id="78ca" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">该误差向量被传播回来以更新权重。</li></ol><h1 id="1166" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">跳格模型的优点</h1><ol class=""><li id="e847" class="jd je hi ih b ii ld im le iq lf iu lg iy lh jc ji jj jk jl bi translated">跳格模型可以捕捉一个单词的两种语义。也就是说，它将有两个苹果的矢量表示。一个给公司，一个给水果。</li><li id="a1cd" class="jd je hi ih b ii jm im jn iq jo iu jp iy jq jc ji jj jk jl bi translated">负子采样Skip-gram通常优于所有其他方法。</li></ol><p id="3c6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个是一个很好的交互工具，可以可视化CBOW和skip gram。我建议你仔细阅读这个链接，以便更好地理解。</p><h1 id="9378" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">3.单词嵌入用例场景</h1><p id="4bc3" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">由于单词嵌入或单词向量是单词之间上下文相似性的数字表示，它们可以被操纵并执行令人惊奇的任务，如-</p><ol class=""><li id="4569" class="jd je hi ih b ii ij im in iq jf iu jg iy jh jc ji jj jk jl bi translated">寻找两个词之间的相似程度。</li></ol><pre class="js jt ju jv fd mo mp mq mr aw ms bi"><span id="efa9" class="ll kg hi mp b fi mt mu l mv mw"> <!-- -->model.similarity('woman','man')<br/> <!-- -->0.73723527</span></pre><p id="e101" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.找出奇怪的一个。</p><pre class="js jt ju jv fd mo mp mq mr aw ms bi"><span id="f797" class="ll kg hi mp b fi mt mu l mv mw">model.doesnt_match('breakfast cereal dinner lunch';.split())<br/> <!-- -->'cereal'</span></pre><p id="1d9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.像女人+国王-男人=王后这样令人惊奇的事情</p><pre class="js jt ju jv fd mo mp mq mr aw ms bi"><span id="d9bc" class="ll kg hi mp b fi mt mu l mv mw"> <!-- -->model.most_similar(positive=['woman','king'],negative=['man'],topn=1)<br/> <!-- -->queen: 0.508</span></pre><p id="4b52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.模型下文本的概率</p><pre class="js jt ju jv fd mo mp mq mr aw ms bi"><span id="393d" class="ll kg hi mp b fi mt mu l mv mw"> <!-- -->model.score(['The fox jumped over the lazy dog'.split()])<br/> <!-- -->0.21</span></pre><p id="bac3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是word2vec的一个有趣的可视化。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es ml"><img src="../Images/9ad42199da104e0f7e2b1ffbe00a938c.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*hCVBREr0xM2LZ1LP.jpg"/></div></figure><p id="3008" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图是二维单词向量的t-SNE表示，你可以看到苹果的两个上下文被捕捉到了。一个是水果，一个是公司。</p><p id="4a5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.它可以用来执行机器翻译。</p><figure class="js jt ju jv fd jw er es paragraph-image"><div class="er es ml"><img src="../Images/b33f2f00370b90e7285183c36174a183.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*KDEz48sANDwcbopM.png"/></div></figure><p id="7b5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图是双语嵌入，中文用绿色，英文用黄色。如果我们知道在中文和英文中有相似意思的词，上述双语嵌入可以用于将一种语言翻译成另一种语言。</p><h1 id="aac1" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">4.使用预先训练的单词向量</h1><p id="49cf" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">我们将使用谷歌的预训练模型。它包含300万个单词的词汇向量，这些词汇是根据谷歌新闻数据集中的大约1000亿个单词训练的。模型的下载链接是<a class="ae mg" href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit" rel="noopener ugc nofollow" target="_blank">这个</a>。小心这是一个1.5 GB的下载。</p><pre class="js jt ju jv fd mo mp mq mr aw ms bi"><span id="ab22" class="ll kg hi mp b fi mt mu l mv mw">from gensim.models import Word2Vec</span><span id="e45f" class="ll kg hi mp b fi mx mu l mv mw">#loading the downloaded model<br/> <!-- -->model = Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, norm_only=True)</span><span id="98ed" class="ll kg hi mp b fi mx mu l mv mw">#the model is loaded. It can be used to perform all of the tasks mentioned above.</span><span id="9bd7" class="ll kg hi mp b fi mx mu l mv mw"># getting word vectors of a word<br/> <!-- -->dog = model['dog']</span><span id="5686" class="ll kg hi mp b fi mx mu l mv mw">#performing king queen magic<br/> <!-- -->print(model.most_similar(positive=['woman', 'king'], negative=['man']))</span><span id="8e2b" class="ll kg hi mp b fi mx mu l mv mw">#picking odd one out<br/> <!-- -->print(model.doesnt_match("breakfast cereal dinner lunch".split()))</span><span id="a06c" class="ll kg hi mp b fi mx mu l mv mw">#printing similarity index<br/> <!-- -->print(model.similarity('woman', 'man')</span></pre><h1 id="ce42" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">5.训练你自己的单词向量</h1><p id="5ecd" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">我们将在自定义语料库上训练我们自己的word2vec。为了训练模型，我们将使用gensim，步骤如下所示。</p><p id="fffa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">word2Vec要求用于训练列表的列表格式，其中每个文档都包含在列表中，且每个列表都包含文档的标记列表。我不会在这里讨论预处理部分。所以让我们用一个list的例子来训练我们的word2vec模型。</p><p id="762a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">句子=[' Neeraj '，' Boy']，['Sarwan '，' is']，['good '，' boy']]</p><pre class="js jt ju jv fd mo mp mq mr aw ms bi"><span id="29f4" class="ll kg hi mp b fi mt mu l mv mw">#training word2vec on 3 sentences<br/> <!-- -->model =<!-- --> <!-- -->gensim.models.Word2Vec(sentence, min_count=1,size=300,workers=4)</span></pre><p id="fd92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们试着理解这个模型的参数。</p><p id="d1b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">句子—我们的语料库列表的列表<br/>min _ count = 1—单词的阈值。只有频率高于此值的单词才会被包含到模型中。<br/>size = 300——我们希望用来表达我们的话语的维度数量。这是单词向量的大小。<br/> workers=4 —用于并行化</p><pre class="js jt ju jv fd mo mp mq mr aw ms bi"><span id="3a4b" class="ll kg hi mp b fi mt mu l mv mw">#using the model<br/> #The new trained model can be used similar to the pre-trained ones.</span><span id="186d" class="ll kg hi mp b fi mx mu l mv mw">#printing similarity index<br/> <!-- -->print(model.similarity('woman', 'man'))</span></pre><h1 id="30ac" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">6.结束注释</h1><p id="e876" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">单词嵌入是一个活跃的研究领域，试图找出比现有的更好的单词表示。但是，随着时间的推移，它们的数量越来越多，也越来越复杂。本文旨在简化这些嵌入模型的一些工作，而不会带来数学开销。如果你认为我能够消除你的一些困惑，请在下面评论。欢迎任何改变或建议。</p><p id="4e60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ke">原载于2017年6月4日</em><a class="ae mg" href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" rel="noopener ugc nofollow" target="_blank"><em class="ke">【www.analyticsvidhya.com】</em></a><em class="ke">。</em></p></div></div>    
</body>
</html>