<html>
<head>
<title>A Survey Of Deep Learning based Object detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于深度学习的目标检测综述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-survey-of-deep-learning-based-object-detection-816204e970ac?source=collection_archive---------15-----------------------#2020-11-01">https://medium.com/analytics-vidhya/a-survey-of-deep-learning-based-object-detection-816204e970ac?source=collection_archive---------15-----------------------#2020-11-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex if ig ih ii"><div class="bz dy l di"><div class="ij ik l"/></div><figcaption class="il im et er es in io bd b be z dx translated">目标检测视频</figcaption></figure><p id="6aac" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">这是“深度学习的对象检测:综述”</p><p id="464a" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">论文:<a class="ae jn" href="https://arxiv.org/pdf/1807.05511.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1807.05511.pdf</a></p><p id="fbc6" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated"><em class="jo">作者:</em></p><p id="d121" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">焦立成，【研究员】，IEEE，，，刘芳，<em class="jo">资深会员，IEEE，</em>杨淑媛，<em class="jo">资深会员，IEEE，</em>，<em class="jo">会员，IEEE，</em>冯，<em class="jo">会员，IEEE，</em>和荣曲，<em class="jo">资深会员，IEEE </em></p></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="304b" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">数据集</strong></h1><p id="f726" class="pw-post-body-paragraph ip iq hi ir b is ku iu iv iw kv iy iz ja kw jc jd je kx jg jh ji ky jk jl jm hb bi translated">快速介绍用于生成目标检测模型的两个数据集。</p><h2 id="dbec" class="kz jx hi bd jy la lb lc kc ld le lf kg ja lg lh kk je li lj ko ji lk ll ks lm bi translated"><em class="ln"> PASCAL VOC 数据集</em></h2><p id="4931" class="pw-post-body-paragraph ip iq hi ir b is ku iu iv iw kv iy iz ja kw jc jd je kx jg jh ji ky jk jl jm hb bi translated">链接到数据集:<a class="ae jn" href="http://host.robots.ox.ac.uk/pascal/VOC/" rel="noopener ugc nofollow" target="_blank">此处</a></p><p id="a3f7" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">PASCAL VOC 数据集包含 20 个对象类别(在 VOC2007 中，如人、自行车、鸟、瓶子、狗等。)传播了超过 11000 张图片。这 20 个类别可以被认为是 4 个主要分支——车辆、动物、家用物品和人。</p><p id="ce00" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">超过 27，000 个对象实例边界框被标记，其中近 7，000 个具有详细的分割。VOC2007 数据集中存在不平衡的数据集。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es lo"><img src="../Images/b8c0f4bddf51977a525d5eb0416c5d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pjqBC-5e2TK7EkLe5Be6Dw.png"/></div></div></figure><p id="eee8" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated"><strong class="ir hj"> MS COCO 数据集</strong></p><p id="9c68" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">链接到数据集:<a class="ae jn" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank">此处</a></p><p id="fbf2" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">用于检测和分割在日常生活中在其自然环境中发现的对象的数据集包含 91 个常见对象类别，其中 82 个具有超过 5000 个标记实例。这些类别涵盖了 PASCAL VOC 数据集中的 20 个类别。该数据集在 328，000 幅图像中总共有 2，500，000 个标记实例。MS COCO 数据集还关注不同的视点，所有对象都在自然环境中，这给了我们丰富的上下文信息。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es lz"><img src="../Images/80544c541e731bf934ef611d8b4049dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZMPUDT-raGixthBr6HfpRw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">椰子树</figcaption></figure><p id="2934" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">还有其他各种数据集，如 ImageNet、OpenImage 等。它们是不断增长的数据集。</p></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><p id="5418" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi ma translated">计算机视觉是一个科学领域，致力于使用图像或视频来解释和理解视觉世界。图像或视频是由物体组成的。图像分类通常是计算机视觉的直接应用，但识别图像中的每个对象揭示了许多无限的可能性。行人检测、盗窃检测、可疑活动检测、自动搜索结果等等。随着深度学习的快速增加，对象检测有了很大的提高。</p><p id="45ea" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">物体检测通常有两种</p><ol class=""><li id="1b60" class="mj mk hi ir b is it iw ix ja ml je mm ji mn jm mo mp mq mr bi translated">两级检测器</li><li id="5982" class="mj mk hi ir b is ms iw mt ja mu je mv ji mw jm mo mp mq mr bi translated">一级检测器</li></ol></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="4ed9" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">两级检测器</h1><h1 id="84c3" class="jw jx hi bd jy jz mx kb kc kd my kf kg kh mz kj kk kl na kn ko kp nb kr ks kt bi translated">美国有线电视新闻网:</h1><p id="5453" class="pw-post-body-paragraph ip iq hi ir b is ku iu iv iw kv iy iz ja kw jc jd je kx jg jh ji ky jk jl jm hb bi translated">R-CNN 是一个基于区域的 CNN 检测器。这包括 4 个模块。</p><p id="af53" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">第一个模块生成与类别无关的区域建议。第二模块从每个区域提议中提取固定长度的特征向量。第三个模块是一组特定类别的线性支持向量机，用于对一幅图像中的对象进行分类。最后一个模块是用于精确包围盒预测的包围盒回归器。</p><p id="0129" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">通常，建议在大型数据集上进行预训练。经过预训练后，我们可以使用该模型为特定数据集进行训练。这有助于快速收敛。</p><p id="5468" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">因此，R-CNN 的想法是将每张图片分成 2000 个区域，称为选择性区域，并尝试处理这些独立的区域。</p><p id="1fc6" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">关于这个主题的详细解释可以在这个<a class="ae jn" href="https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf" rel="noopener ugc nofollow" target="_blank">链接</a>中找到</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nc"><img src="../Images/4e2b81897e5b5690019b134cc0949f2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CP3X0CXeCIUki5NB4cjsTA.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">RCNN: <a class="ae jn" href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e" rel="noopener" target="_blank">来源</a></figcaption></figure><h1 id="10f1" class="jw jx hi bd jy jz mx kb kc kd my kf kg kh mz kj kk kl na kn ko kp nb kr ks kt bi translated">快速 R-CNN</h1><p id="6b47" class="pw-post-body-paragraph ip iq hi ir b is ku iu iv iw kv iy iz ja kw jc jd je kx jg jh ji ky jk jl jm hb bi translated">R-CNN 的缺点是 Conv 网络之间没有信息共享，计算每个区域的支持向量机需要大量时间。因此引入了快速 R-CNN。</p><p id="2be9" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">Fast R-CNN 不是生成 2000 个区域建议，而是使用最大池层生成感兴趣区域(RoI)。并且在进一步的操作中仅使用这些区域。因为我们最小化了用于分析的图像的面积，所以我们将具有更快的收敛。</p><p id="e7d3" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">快速 R-CNN 是一个单阶段的端到端训练过程，在每个标记的 RoI 上使用多任务损失来联合训练网络。此外，SVM 层被替换为<a class="ae jn" href="https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491\" rel="noopener"> SVD </a>，进一步加快了进程。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nd"><img src="../Images/d6fe72e41a7f6e2dcbcdae506590596c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1hupDiwxsvnZKQVo75bVIQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">快速 R-CNN:来源:<a class="ae jn" href="https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491" rel="noopener">链接</a></figcaption></figure><h1 id="6fdb" class="jw jx hi bd jy jz mx kb kc kd my kf kg kh mz kj kk kl na kn ko kp nb kr ks kt bi translated"><em class="ln">更快的 R-CNN: </em></h1><p id="045e" class="pw-post-body-paragraph ip iq hi ir b is ku iu iv iw kv iy iz ja kw jc jd je kx jg jh ji ky jk jl jm hb bi translated">对快速 R-CNN 感兴趣的区域是基于使用最大池层的选择性搜索，这是缓慢的。因此，在 Faster 中，R-CNN 用一种新颖的 RPN(区域建议网络)取代了区域选择方法，RPN 是一种完全卷积的网络，可以有效地预测各种规模和纵横比的区域建议。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es ne"><img src="../Images/4f3cdf24e7b8602f47daf9a1cb164a6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bGTEGqwD5mbxsPobvDnjLw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">更快 R-CNN 来源:<a class="ae jn" href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e" rel="noopener" target="_blank">链接</a></figcaption></figure><h1 id="dcd4" class="jw jx hi bd jy jz mx kb kc kd my kf kg kh mz kj kk kl na kn ko kp nb kr ks kt bi translated">屏蔽 R-CNN</h1><p id="3c84" class="pw-post-body-paragraph ip iq hi ir b is ku iu iv iw kv iy iz ja kw jc jd je kx jg jh ji ky jk jl jm hb bi translated">更快的 R-CNN 表现不错，但是它有一个<a class="ae jn" href="https://missinglink.ai/guides/neural-network-concepts/instance-segmentation-deep-learning/" rel="noopener ugc nofollow" target="_blank">实例分割问题</a>。屏蔽 R-CNN 克服了这个问题。首先，它基于输入图像生成关于可能存在物体的区域的建议。其次，它预测对象的类别，细化包围盒，并基于第一阶段的提议在对象的像素级生成遮罩。</p><p id="d11c" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">由于上述过程，它创建了一个非常高度可靠的对象检测架构。</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/47e8841b30168715209f98430dae757a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*YvO33cQvx87bt3CtatahRA.png"/></div><figcaption class="il im et er es in io bd b be z dx translated">面具 R-CNN，来源:<a class="ae jn" href="https://arxiv.org/pdf/1703.06870.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><p id="9afa" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">最终分割的区域将如下所示:</p><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es ng"><img src="../Images/889d2b355c6b45b18553a654c27caa58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X4LIKG7w3ekNMas2k53Isw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">蒙版图片，来源:<a class="ae jn" href="https://arxiv.org/pdf/1703.06870.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><h1 id="549b" class="jw jx hi bd jy jz mx kb kc kd my kf kg kh mz kj kk kl na kn ko kp nb kr ks kt bi translated">一级检测器</h1><h1 id="6289" class="jw jx hi bd jy jz mx kb kc kd my kf kg kh mz kj kk kl na kn ko kp nb kr ks kt bi translated">你只活一次</h1><p id="ee43" class="pw-post-body-paragraph ip iq hi ir b is ku iu iv iw kv iy iz ja kw jc jd je kx jg jh ji ky jk jl jm hb bi translated">基于流行的“你只活一次，你只看一次”的意识形态。</p><p id="fe31" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">在这种情况下，没有区域创建，然后再在此基础上进行处理，而是有一个卷积网络来创建盒子和每个盒子的类预测。</p><blockquote class="nh ni nj"><p id="f784" class="ip iq jo ir b is it iu iv iw ix iy iz nk jb jc jd nl jf jg jh nm jj jk jl jm hb bi translated">我们将对象检测重新定义为一个单一的回归问题，直接从图像像素到边界框坐标和类别概率。</p></blockquote><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nn"><img src="../Images/21cd52f62dd4dd9f3d20efa80ab3e8f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gkZ-Od-aF2rGPmBuE3f6jA.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">Yolo，来源:<a class="ae jn" href="https://arxiv.org/pdf/1506.02640.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><p id="4e31" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">实验表明，YOLO 不擅长精确定位，定位误差是预测误差的主要组成部分。</p><h1 id="c1f4" class="jw jx hi bd jy jz mx kb kc kd my kf kg kh mz kj kk kl na kn ko kp nb kr ks kt bi translated">约洛·V2</h1><p id="1c08" class="pw-post-body-paragraph ip iq hi ir b is ku iu iv iw kv iy iz ja kw jc jd je kx jg jh ji ky jk jl jm hb bi translated">YoLo 只看了一次，但没有给出完美的结果，也有许多本地化问题。</p><p id="7abf" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">因此，在 YoLo v2 中，架构被重新审视，变得非常快速和可靠。介绍了以下内容</p><ol class=""><li id="6a64" class="mj mk hi ir b is it iw ix ja ml je mm ji mn jm mo mp mq mr bi translated"><a class="ae jn" href="https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c" rel="noopener" target="_blank">批量归一化</a></li><li id="436a" class="mj mk hi ir b is ms iw mt ja mu je mv ji mw jm mo mp mq mr bi translated"><a class="ae jn" href="https://hal.archives-ouvertes.fr/hal-01660754/document" rel="noopener ugc nofollow" target="_blank">高分辨率分类器</a></li><li id="2344" class="mj mk hi ir b is ms iw mt ja mu je mv ji mw jm mo mp mq mr bi translated"><a class="ae jn" href="https://www.mathworks.com/help/vision/ug/anchor-boxes-for-object-detection.html#:~:text=Anchor%20boxes%20are%20a%20set,sizes%20in%20your%20training%20datasets." rel="noopener ugc nofollow" target="_blank">使用锚定框作为边界框</a></li></ol><figure class="lp lq lr ls fd ii er es paragraph-image"><div class="er es no"><img src="../Images/6fd2889e3e102132a7e8543d5bb20c6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*3gk2YUgbKrq3RztoKCLDJA.jpeg"/></div><figcaption class="il im et er es in io bd b be z dx translated">YoLo v2:来源:<a class="ae jn" href="https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" rel="noopener">链接</a></figcaption></figure><h1 id="9b87" class="jw jx hi bd jy jz mx kb kc kd my kf kg kh mz kj kk kl na kn ko kp nb kr ks kt bi translated">YoLo V3</h1><p id="bb61" class="pw-post-body-paragraph ip iq hi ir b is ku iu iv iw kv iy iz ja kw jc jd je kx jg jh ji ky jk jl jm hb bi translated">这是约洛·V2 的改良版。</p><p id="09b1" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">这有以下更新更改:</p><ol class=""><li id="63e1" class="mj mk hi ir b is it iw ix ja ml je mm ji mn jm mo mp mq mr bi translated">多标签分类</li><li id="16ec" class="mj mk hi ir b is ms iw mt ja mu je mv ji mw jm mo mp mq mr bi translated">使用特征图预测包围盒</li><li id="2bbe" class="mj mk hi ir b is ms iw mt ja mu je mv ji mw jm mo mp mq mr bi translated">使用暗网作为最终特征提取器</li></ol><p id="f8be" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">详细的论文可以在这里找到<a class="ae jn" href="https://pjreddie.com/darknet/yolo/" rel="noopener ugc nofollow" target="_blank"/></p><figure class="lp lq lr ls fd ii"><div class="bz dy l di"><div class="np ik l"/></div></figure><h1 id="0b3e" class="jw jx hi bd jy jz mx kb kc kd my kf kg kh mz kj kk kl na kn ko kp nb kr ks kt bi translated"><em class="ln">单发探测器(SSD) </em></h1><p id="790f" class="pw-post-body-paragraph ip iq hi ir b is ku iu iv iw kv iy iz ja kw jc jd je kx jg jh ji ky jk jl jm hb bi translated">这种架构就像 Yolo 一样，它可以在一次拍摄中检测到物体。使用多盒进行检测。</p><p id="e2c2" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">这种架构速度更快，精度更高。</p><p id="9b24" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">这种架构有三个重要特征</p><ul class=""><li id="d54a" class="mj mk hi ir b is it iw ix ja ml je mm ji mn jm nq mp mq mr bi translated"><strong class="ir hj">单次:</strong>这意味着物体定位和分类<em class="jo"> </em>的任务是在网络的<em class="jo">单次</em> <em class="jo">前向传递</em>中完成的</li><li id="697b" class="mj mk hi ir b is ms iw mt ja mu je mv ji mw jm nq mp mq mr bi translated"><strong class="ir hj">多框:</strong>这是 Szegedy 等人开发的边界框回归技术的名称(我们将很快简要介绍)</li><li id="480f" class="mj mk hi ir b is ms iw mt ja mu je mv ji mw jm nq mp mq mr bi translated"><strong class="ir hj">检测器:</strong>网络是一个对象检测器，它还对那些检测到的对象进行分类</li></ul><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nn"><img src="../Images/992ea66040273b0d0f59e5dbb4a2b6c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZaQ3JVGW5kLVEZIWvb_MVA.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">SSD，来源:<a class="ae jn" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5" rel="noopener" target="_blank">链接</a></figcaption></figure><p id="0dcc" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">详细的纸张可以在本 l <a class="ae jn" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank">油墨</a>中找到</p><h1 id="a4ab" class="jw jx hi bd jy jz mx kb kc kd my kf kg kh mz kj kk kl na kn ko kp nb kr ks kt bi translated">去卷积单触发检测器(<em class="ln"> DSSD) </em></h1><p id="fd26" class="pw-post-body-paragraph ip iq hi ir b is ku iu iv iw kv iy iz ja kw jc jd je kx jg jh ji ky jk jl jm hb bi translated">这是对 SSD 的改进。</p><p id="0513" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">DSSD 有这两个重要特征:</p><ul class=""><li id="cb96" class="mj mk hi ir b is it iw ix ja ml je mm ji mn jm nq mp mq mr bi translated"><strong class="ir hj">逐步反卷积</strong>放大特征图</li><li id="c273" class="mj mk hi ir b is ms iw mt ja mu je mv ji mw jm nq mp mq mr bi translated"><strong class="ir hj">特征组合</strong>来自卷积路径和反卷积路径</li></ul><figure class="lp lq lr ls fd ii er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nn"><img src="../Images/434e939b60ac136599f9c567958e1854.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AgodE5XfauJrqlTSrE7T8A.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated">DSSD，来源:<a class="ae jn" href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5" rel="noopener" target="_blank">链接</a></figcaption></figure><p id="a323" class="pw-post-body-paragraph ip iq hi ir b is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm hb bi translated">还有一些其他对象检测架构，如 RetinaNet、M2Det 和 RefineNet，这些架构变得更加复杂，需要更详细的解释才能基本理解。因此，我们将在下一篇文章中保留它们。</p><div class="nr ns ez fb nt nu"><a href="https://pjreddie.com/darknet/yolo/" rel="noopener  ugc nofollow" target="_blank"><div class="nv ab dw"><div class="nw ab nx cl cj ny"><h2 class="bd hj fi z dy nz ea eb oa ed ef hh bi translated">YOLO:实时目标检测</h2><div class="ob l"><h3 class="bd b fi z dy nz ea eb oa ed ef dx translated">你只看一次(YOLO)是一个最先进的，实时对象检测系统。在 Pascal Titan X 上，它处理…</h3></div><div class="oc l"><p class="bd b fp z dy nz ea eb oa ed ef dx translated">pjreddie.com</p></div></div><div class="od l"><div class="oe l of og oh od oi lx nu"/></div></div></a></div><div class="nr ns ez fb nt nu"><a href="http://host.robots.ox.ac.uk/pascal/VOC/" rel="noopener  ugc nofollow" target="_blank"><div class="nv ab dw"><div class="nw ab nx cl cj ny"><h2 class="bd hj fi z dy nz ea eb oa ed ef hh bi translated">PASCAL 可视对象类主页</h2><div class="ob l"><h3 class="bd b fi z dy nz ea eb oa ed ef dx translated">为对象类别识别提供标准化的图像数据集，为访问……</h3></div><div class="oc l"><p class="bd b fp z dy nz ea eb oa ed ef dx translated">主机.机器人. ox.ac.uk</p></div></div><div class="od l"><div class="oj l of og oh od oi lx nu"/></div></div></a></div><div class="nr ns ez fb nt nu"><a href="https://cocodataset.org/#home" rel="noopener  ugc nofollow" target="_blank"><div class="nv ab dw"><div class="nw ab nx cl cj ny"><h2 class="bd hj fi z dy nz ea eb oa ed ef hh bi translated">COCO -上下文中的常见对象</h2><div class="ob l"><h3 class="bd b fi z dy nz ea eb oa ed ef dx translated">编辑描述</h3></div><div class="oc l"><p class="bd b fp z dy nz ea eb oa ed ef dx translated">cocodataset.org</p></div></div><div class="od l"><div class="ok l of og oh od oi lx nu"/></div></div></a></div><div class="nr ns ez fb nt nu"><a href="https://towardsdatascience.com/review-dssd-deconvolutional-single-shot-detector-object-detection-d4821a2bbeb5" rel="noopener follow" target="_blank"><div class="nv ab dw"><div class="nw ab nx cl cj ny"><h2 class="bd hj fi z dy nz ea eb oa ed ef hh bi translated">综述:DSSD —解卷积单次探测器(目标探测)</h2><div class="ob l"><h3 class="bd b fi z dy nz ea eb oa ed ef dx translated">反卷积层:引入额外的大规模背景，提高小对象的准确性</h3></div><div class="oc l"><p class="bd b fp z dy nz ea eb oa ed ef dx translated">towardsdatascience.com</p></div></div><div class="od l"><div class="ol l of og oh od oi lx nu"/></div></div></a></div><div class="nr ns ez fb nt nu"><a href="https://hackernoon.com/understanding-yolo-f5a74bbc7967" rel="noopener  ugc nofollow" target="_blank"><div class="nv ab dw"><div class="nw ab nx cl cj ny"><h2 class="bd hj fi z dy nz ea eb oa ed ef hh bi translated">了解 YOLO</h2><div class="ob l"><h3 class="bd b fi z dy nz ea eb oa ed ef dx translated">这篇文章解释了 YOLO 对象检测体系结构，从某人的角度谁想要实现…</h3></div><div class="oc l"><p class="bd b fp z dy nz ea eb oa ed ef dx translated">hackernoon.com</p></div></div><div class="od l"><div class="om l of og oh od oi lx nu"/></div></div></a></div><div class="nr ns ez fb nt nu"><a href="https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" rel="noopener follow" target="_blank"><div class="nv ab dw"><div class="nw ab nx cl cj ny"><h2 class="bd hj fi z dy nz ea eb oa ed ef hh bi translated">使用 YOLO、YOLOv2 和现在的 YOLOv3 进行实时物体检测</h2><div class="ob l"><h3 class="bd b fi z dy nz ea eb oa ed ef dx translated">你只看一次(YOLO)是一个目标为实时处理的对象检测系统。我们将介绍 YOLO…</h3></div><div class="oc l"><p class="bd b fp z dy nz ea eb oa ed ef dx translated">jonathan-hui.medium.com</p></div></div><div class="od l"><div class="on l of og oh od oi lx nu"/></div></div></a></div><div class="nr ns ez fb nt nu"><a href="https://towardsdatascience.com/understanding-ssd-multibox-real-time-object-detection-in-deep-learning-495ef744fab" rel="noopener follow" target="_blank"><div class="nv ab dw"><div class="nw ab nx cl cj ny"><h2 class="bd hj fi z dy nz ea eb oa ed ef hh bi translated">了解 SSD 多盒—深度学习中的实时对象检测</h2><div class="ob l"><h3 class="bd b fi z dy nz ea eb oa ed ef dx translated">这篇文章旨在对 SSD 多框对象检测技术进行直观的解释。我试过了…</h3></div><div class="oc l"><p class="bd b fp z dy nz ea eb oa ed ef dx translated">towardsdatascience.com</p></div></div><div class="od l"><div class="oo l of og oh od oi lx nu"/></div></div></a></div><figure class="lp lq lr ls fd ii"><div class="bz dy l di"><div class="op ik l"/></div></figure><figure class="lp lq lr ls fd ii"><div class="bz dy l di"><div class="op ik l"/></div></figure><figure class="lp lq lr ls fd ii"><div class="bz dy l di"><div class="op ik l"/></div></figure></div></div>    
</body>
</html>