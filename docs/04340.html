<html>
<head>
<title>Review CMUPose &amp; OpenPose — Winner in COCO KeyPoint Detection Challenge 2016 (Human Pose Estimation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">回顾CMU Pose &amp; open Pose—2016年COCO关键点检测挑战赛(人体姿势估计)的获胜者</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/review-cmupose-openpose-winner-in-coco-keypoint-detection-challenge-2016-human-pose-ccbdbc72b7dd?source=collection_archive---------7-----------------------#2020-03-15">https://medium.com/analytics-vidhya/review-cmupose-openpose-winner-in-coco-keypoint-detection-challenge-2016-human-pose-ccbdbc72b7dd?source=collection_archive---------7-----------------------#2020-03-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="5eee" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">第一个用于多人2D姿势检测的开源实时系统</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/a3ce07ca7ebf935921ea041aaa32c90e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*HO9GV_nSDUueu7qh.gif"/></div></figure><p id="9403" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi kb translated"><span class="l kc kd ke bm kf kg kh ki kj di">在</span>这个故事中，CMUPose &amp; OpenPose，进行了回顾。<strong class="jh hj"> CMUPose </strong>是卡耐基梅隆大学参加并<strong class="jh hj">赢得2016年可可关键点检测挑战赛</strong>的团队名称。该方法作为<strong class="jh hj"> 2017 CVPR </strong>出版，引用<strong class="jh hj"> 2000余篇</strong>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kk"><img src="../Images/d05aed6423c6f3373630cf3860e28c18.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*Qb7uJjKwTBoTHa6RRxPTYA.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">CMU pose:2016可可关键点检测挑战赛冠军(</strong><a class="ae kq" href="http://cocodataset.org/#keypoints-leaderboard" rel="noopener ugc nofollow" target="_blank"><strong class="bd kp">)http://cocodataset.org/#keypoints-leaderboard</strong></a><strong class="bd kp">)</strong></figcaption></figure><p id="ee2f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">后来，加州大学、卡耐基梅隆大学和脸书现实实验室提出了更加增强的<strong class="jh hj"> OpenPose </strong>，其中<strong class="jh hj">第一次组合了身体和脚部关键点数据集和检测器</strong>。它也是第一个用于多人2D姿势检测的开源实时系统。最终发布为<strong class="jh hj"> 2019 TPAMI </strong>，引用<strong class="jh hj"> 300余次</strong>。(<a class="kr ks ge" href="https://medium.com/u/aff72a0c1243?source=post_page-----ccbdbc72b7dd--------------------------------" rel="noopener" target="_blank"> Sik-Ho Tsang </a> @中)</p><p id="3c79" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">由于OpenPose是CMUPose的加强版，所以在这个故事中主要提到OpenPose。</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="30f5" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated">概述</h1><ol class=""><li id="0ae3" class="lr ls hi jh b ji lt jl lu jo lv js lw jw lx ka ly lz ma mb bi translated"><strong class="jh hj">整体管线</strong></li><li id="1c7b" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka ly lz ma mb bi translated"><strong class="jh hj"> CMUPose网络架构</strong></li><li id="8d53" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka ly lz ma mb bi translated"><strong class="jh hj"> OpenPose网络架构</strong></li><li id="8790" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka ly lz ma mb bi translated"><strong class="jh hj"> OpenPose损失函数和其他细节</strong></li><li id="3ac5" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka ly lz ma mb bi translated"><strong class="jh hj"> OpenPose伸展脚检测</strong></li><li id="a7b3" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka ly lz ma mb bi translated"><strong class="jh hj">结果(在OpenPose论文中)</strong></li></ol></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="0d20" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated"><strong class="ak"> 1。整体管道</strong></h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mh"><img src="../Images/fbd00a2af86c0b45e34bfda5b02a5114.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xbthshlyj0V0_2sKqzeHwQ.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">整体管道</strong></figcaption></figure><ul class=""><li id="2766" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated"><strong class="jh hj"> (a) </strong> : <strong class="jh hj">尺寸为w×h的彩色图像</strong>作为输入图像。</li><li id="f299" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated"><strong class="jh hj"> (b) </strong>:前馈网络同时<strong class="jh hj">预测一组身体部位位置</strong>的2D置信图(CM)<em class="mq"/>，以及</li><li id="3c4e" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated"><strong class="jh hj"> (c) </strong> : <strong class="jh hj">零件亲缘关系的一组2D矢量场<em class="mq">L</em></strong>，或<strong class="jh hj">零件亲缘关系场(PAF) </strong>，其编码零件之间的关联程度</li><li id="3380" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">集合<em class="mq"> S </em> = ( <em class="mq"> S </em> 1、<em class="mq"> S </em> 2、…、<em class="mq"> SJ </em>)具有<em class="mq"> J </em>置信图，每个零件一个。</li><li id="709c" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">集合<em class="mq"> L </em> = ( <em class="mq"> L </em> 1、<em class="mq"> L </em> 2、…、<em class="mq"> LC </em>)有<em class="mq"> C </em>个向量场，每个肢体一个。<em class="mq"> LC </em>中的每个图像位置编码一个2D矢量。</li><li id="b61f" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated"><strong class="jh hj"> (d) </strong>:然后，通过贪婪推理来解析置信图和亲和域，并且</li><li id="47b1" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated"><strong class="jh hj"> (e) </strong>:输出图像中所有人的2D关键点。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="2f60" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated">2.<strong class="ak"> CMUPose网络架构</strong></h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es mr"><img src="../Images/4fd5894c55b839c461494e22d607eb67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qi8w4KWj8sfsoOJBpALe8Q.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp"> CMUPose网络架构</strong></figcaption></figure><ul class=""><li id="643a" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">图像首先由<a class="ae kq" rel="noopener" href="/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11"> VGG-19 </a>的前10层进行分析，生成一组特征图<em class="mq"> F </em>输入到每个分支的第一级。</li><li id="30d8" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">第一阶段，网络产生一组CMs，<em class="mq">S</em>1 =<em class="mq">ρ</em>1(<em class="mq">F</em>)，一组PAF，<em class="mq">L</em>1 =<em class="mq">φ</em>1(<em class="mq">F</em>)。</li><li id="bb30" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated"><em class="mq"> S </em>和<em class="mq"> L </em>可以迭代细化以改善检测结果。</li><li id="68c0" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">在阶段t，它变成:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ms"><img src="../Images/b120f73ce8a4061d61c423d040169d3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*_LCLm8leldApPa3SGryVBg.png"/></div></figure><ul class=""><li id="0012" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">基于S和L，可以检测人体姿态。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="eb01" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated">3.开放式网络体系结构</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mt"><img src="../Images/eeaeaba72522e5c12f299c3577cfa9c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*hT3ICTN_eHY3rvuL4O8Phw.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp"> OpenPose网络架构</strong></figcaption></figure><ul class=""><li id="03d6" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">OpenPose的架构与CMUPose不同。</li><li id="e977" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">该网络首先制作一套PAFS，<em class="mq"> Lt. </em></li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mu"><img src="../Images/9d7f96eaeabe005b42fe26ffce7dc4db.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*q_NQDxOwoopGUQaBBml4tw.png"/></div></figure><ul class=""><li id="e005" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">然后制作一套CMs，<em class="mq"> St </em>。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mv"><img src="../Images/6607b99aa879b414458c2e0cfb461d6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*KV1BBcCV9zir_QwxxL0wfA.png"/></div></figure><ul class=""><li id="8fd0" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">发现<strong class="jh hj">提炼PAFs比提炼CMs </strong>更重要。因此，PAF细化对于高精度来说更关键且足够，在增加网络深度的同时移除身体部位置信图细化。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mw"><img src="../Images/4612b66a18c97bfe40d133ddbd99cc60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*z10YHlhK8jkP7zgCCw3NuQ.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp"> PAF跨阶段细化</strong></figcaption></figure><ul class=""><li id="53af" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">在CMUPose中，网络架构包括几个7x7卷积层。<strong class="jh hj">在OpenPose中，每个7×7卷积核被3个连续的3×3核代替。</strong>保留感受野并减少操作次数。前者的手术次数是97次，而后者只有51次。</li><li id="1626" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">此外，<strong class="jh hj">3个卷积核中的每一个的输出被连接</strong>，遵循类似于<a class="ae kq" href="https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803?source=post_page---------------------------" rel="noopener" target="_blank"> DenseNet </a>的方法。非线性层的数量增加了三倍，并且网络可以保持较低级和较高级的特征。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="91f0" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated"><strong class="ak"> 4。损失函数和其他细节</strong></h1><h2 id="41f4" class="mx lb hi bd kp my mz na lf nb nc nd lj jo ne nf ll js ng nh ln jw ni nj lp nk bi translated">4.1.损失函数</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nl"><img src="../Images/7c32d2a56d21bcc1ffeb99d68b48ba75.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*KAZhwWRURv8oKA6uTr0tTw.png"/></div></figure><ul class=""><li id="3676" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">在估计的预测和地面实况图和场之间使用L2损失。</li><li id="bc1c" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated"><em class="mq"> Lc </em> *是地面真相PAF，S <em class="mq"> j </em> *是地面真相部分置信图，<em class="mq"> W </em>是像素<em class="mq"> p </em>处注释缺失时<em class="mq"> W </em> ( <em class="mq"> p </em> ) = 0的二进制掩码。</li><li id="086d" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">每个阶段的中间监督通过周期性地补充梯度来解决消失梯度问题。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nm"><img src="../Images/99f3cf7ff00a3a6548516a7e62624d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*xUsX6aqiRi1o9Gev9xqN3Q.png"/></div></figure><h2 id="2169" class="mx lb hi bd kp my mz na lf nb nc nd lj jo ne nf ll js ng nh ln jw ni nj lp nk bi translated">4.2.置信图</h2><ul class=""><li id="83ec" class="lr ls hi jh b ji lt jl lu jo lv js lw jw lx ka mp lz ma mb bi translated">每个人<em class="mq"> k </em>和每个身体部位<em class="mq"> j </em>的置信图<em class="mq"> S </em> *为:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nn"><img src="../Images/39fef42a8614b9dd7730b59e9ce133d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*OMIOGyXMg3F-rrOLYGYT2Q.png"/></div></figure><ul class=""><li id="e5a5" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">如上式所示，它是一个逐渐变化的高斯点，峰值位于点的中心，σ控制峰值的扩散。</li><li id="aa43" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">这些CMs实际上类似于<a class="ae kq" href="https://towardsdatascience.com/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c" rel="noopener" target="_blank">Tompson NIPS’14</a>中使用的热图。</li></ul><h2 id="371c" class="mx lb hi bd kp my mz na lf nb nc nd lj jo ne nf ll js ng nh ln jw ni nj lp nk bi translated">4.3.<strong class="ak">部分亲和领域(</strong> PAF)</h2><ul class=""><li id="d64b" class="lr ls hi jh b ji lt jl lu jo lv js lw jw lx ka mp lz ma mb bi translated">对于多人关键点检测，我们需要知道哪个身体部位链接到哪个身体部位。</li><li id="81a5" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">因为举例来说，如果有多人，则有多个头部和肩部。特别是，它们紧密地组合在一起，很难区分哪个头部和肩部属于同一个人。</li><li id="48d8" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">因此，需要一个链接来指定属于同一个人的特定头部和肩部之间的联系。</li><li id="99d2" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated"><strong class="jh hj">这种联系是以PAF为代表的。两个身体部分之间的PAF越强，这两个部分联系在一起并且属于同一个人的信心就越高。</strong></li><li id="5149" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">如果p位于肢上，则PAF <em class="mq"> L </em> *为单位矢量，否则为0:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es no"><img src="../Images/553e0618f7c09943a8134296ac72ca48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*lib4a07Ly507L-ZyGI_zSg.png"/></div></figure><ul class=""><li id="decf" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">沿着线段的预测零件亲和场<em class="mq"> Lc </em>用于测量两个候选零件位置dj1和dj2的置信度:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es np"><img src="../Images/e66a0066ee89be129e670a49dbd64acb.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*bwjGtGfArKcYZzcfdRjhCw.png"/></div></figure><ul class=""><li id="88ba" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">对于多人，总的<em class="mq"> E </em>需要最大化:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nq"><img src="../Images/1b1015140a010c48f819cce92f1e7051.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*C86TguDNT9y8d7CvAQcw3w.png"/></div></figure><ul class=""><li id="cfc9" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">连接身体部位有多种方法:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nr"><img src="../Images/39029159f6343e5b7d8997c3c466a719.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*8Z5JJGl1wkZM9X8QDMgTyA.png"/></div></figure><ul class=""><li id="9a12" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated"><strong class="jh hj"> (a) </strong>:双人身体部位。</li><li id="eaef" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated"><strong class="jh hj"> (b) </strong>:考虑所有边缘进行匹配。</li><li id="ad75" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated"><strong class="jh hj"> (c) </strong>:最小树边匹配。</li><li id="e65f" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated"><strong class="jh hj">(d)</strong>:open pose使用的贪婪算法，只学习最小的边。</li><li id="d2b6" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">事实上，有很多关于CMs和PAFS的细节，请阅读文件了解更多细节。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="1249" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated">5.<strong class="ak"> OpenPose伸展脚检测</strong></h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ns"><img src="../Images/f608152fb8d9fc9c1fa3a857288cf3bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*aai6Zy4gZrCoXP8MFgtZUQ.png"/></div></figure><ul class=""><li id="9481" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">OpenPose已经提出了<strong class="jh hj"/><strong class="jh hj">第一个组合的身体和脚部关键点数据集和检测器</strong>如上图所示。</li><li id="a95e" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">通过包含脚部关键点，它能够正确地检测脚踝，如上图右侧所示。</li></ul></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h1 id="93fe" class="la lb hi bd kp lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated">6.结果<strong class="ak">(在OpenPose纸中)</strong></h1><h2 id="8d8a" class="mx lb hi bd kp my mz na lf nb nc nd lj jo ne nf ll js ng nh ln jw ni nj lp nk bi translated">6.1.MPII多人</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nt"><img src="../Images/e756553497f60b131fb70cee6890c9fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*weKSFQjHyU126WxhfqRuQA.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp"> MPII数据集</strong></figcaption></figure><ul class=""><li id="c76d" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">对于288个图像子集以及完整的测试集，OpenPose也获得了高mAP，优于或相当于<a class="ae kq" rel="noopener" href="/@sh.tsang/review-deepcut-deepercut-multi-person-pose-estimation-human-pose-estimation-da5b469cbbc3"> DeepCut </a>、<a class="ae kq" rel="noopener" href="/@sh.tsang/review-deepcut-deepercut-multi-person-pose-estimation-human-pose-estimation-da5b469cbbc3"> DeeperCut </a>和<a class="ae kq" href="https://towardsdatascience.com/review-newell-eccv16-and-newell-pocv-16-stacked-hourglass-networks-human-pose-estimation-a9eeb76d40a5" rel="noopener" target="_blank">纽维尔ECCV 16</a>。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mw"><img src="../Images/6c3a39eacbec814b1c2b12bd237d746a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*Y6jY4h772nCcvnT2Ai14vA.png"/></div></figure><ul class=""><li id="350d" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated"><strong class="jh hj">左</strong>:如果地面实况关键点位置与所提出的解析算法一起使用:88.3% mAP。</li><li id="5e42" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">使用GT连接与建议的关键点检测81.6%地图。</li><li id="0d0b" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated"><strong class="jh hj">右</strong>:更多阶段，更高地图。</li></ul><h2 id="532a" class="mx lb hi bd kp my mz na lf nb nc nd lj jo ne nf ll js ng nh ln jw ni nj lp nk bi translated">6.2.可可要点挑战</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nu"><img src="../Images/59ad46977307cfa43d2051e1bb415c00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*jSfpjIx-wq0jchGaSmfiLw.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">测试开发排行榜</strong></figcaption></figure><ul class=""><li id="004f" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">存在首先检测人然后检测关键点的自顶向下的方法，而自底向上的方法是首先检测关键点以形成人的骨架。</li><li id="0cd9" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">上表中，OpenPose表现不太好。这主要是因为当只考虑高等级的人时，准确性下降得更快(AP^L).</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es nv"><img src="../Images/42f23f5eac719b967d9fe8f1ccf66325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*4TMhtxt4E_0G8i946gIo1A.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">验证设置</strong></figcaption></figure><ul class=""><li id="e51c" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated"><strong class="jh hj"> 5PAF — 1CM </strong> : 5级PAF，1级CM最高地图65.3。</li><li id="ffcb" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated"><strong class="jh hj"> 3CM-3PAF </strong>:仅61.0%地图。当使用PAF作为先验时，零件置信度图的精确度大大增加。</li></ul><h2 id="4a49" class="mx lb hi bd kp my mz na lf nb nc nd lj jo ne nf ll js ng nh ln jw ni nj lp nk bi translated">6.3.推理时间</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nw"><img src="../Images/75df05e3f5a9df5bd2ad514cbc368699.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*uQlnsZQb4ehzOSp372vhyQ.png"/></div></figure><ul class=""><li id="9c4b" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">OpenPose几乎保持相同的运行时间，不管每张图片有多少人。</li><li id="fc4c" class="lr ls hi jh b ji mc jl md jo me js mf jw mg ka mp lz ma mb bi translated">虽然自上而下的方法如Alpha-Pose和Mark R-CNN，但运行时间与每张图像的人数成正比。</li></ul><h2 id="58f0" class="mx lb hi bd kp my mz na lf nb nc nd lj jo ne nf ll js ng nh ln jw ni nj lp nk bi translated">6.4.足部关键点数据集</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nx"><img src="../Images/64dc842c090ebb9368e707d28cd3a740.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*w1kX0JHKoQi76e4YjeNSXg.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">脚部验证设置</strong></figcaption></figure><ul class=""><li id="e220" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">通过OpenPose获得高AP和AR。</li></ul><h2 id="44ed" class="mx lb hi bd kp my mz na lf nb nc nd lj jo ne nf ll js ng nh ln jw ni nj lp nk bi translated">6.5.车辆姿态估计</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es ny"><img src="../Images/3c666f94ecfae18d24f120a9e608db6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bG58h-QtqpFml-i3JuJaFA.png"/></div></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nz"><img src="../Images/df60d19bbe83eb1006e953d7c5cde90f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*Nlt8afNUA_cgdVjl-emIkw.png"/></div></figure><ul class=""><li id="6066" class="lr ls hi jh b ji jj jl jm jo mm js mn jw mo ka mp lz ma mb bi translated">车辆关键点也可以用高AP和AR来检测。</li></ul><h2 id="5481" class="mx lb hi bd kp my mz na lf nb nc nd lj jo ne nf ll js ng nh ln jw ni nj lp nk bi translated">6.6.定性结果</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es oa"><img src="../Images/12a5b96e07f7399fbf6921bf45b40488.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*roE2zfB_BRL8sII1dktjng.png"/></div></div></figure><h2 id="a9b5" class="mx lb hi bd kp my mz na lf nb nc nd lj jo ne nf ll js ng nh ln jw ni nj lp nk bi translated">6.7.失败案例</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es ob"><img src="../Images/2eb4eb56e72e8e3e0124e6c4f5b12624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JmnSrkqnWGGHCM2weipUoA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">常见故障案例(a) </strong>罕见姿势或外观、<strong class="bd kp"> (b) </strong>缺失或错误零件检测、<strong class="bd kp"> (c) </strong>重叠零件，即两个人共享零件检测、<strong class="bd kp"> (d) </strong>错误连接关联两个人的零件、<strong class="bd kp"> (e-f) </strong>雕像或动物的假阳性。</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mi mj di mk bf ml"><div class="er es oc"><img src="../Images/43e326f934e2078285fbb26cfcb7aabf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UOQ_4Qvq8ehEJEofT7pzIA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated"><strong class="bd kp">脚失灵案例</strong></figcaption></figure></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><p id="c29e" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">事实上，在OpenPose论文中还有许多我在这里没有提到的结果。请随意阅读他们的论文。:)</p></div><div class="ab cl kt ku gp kv" role="separator"><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky kz"/><span class="kw bw bk kx ky"/></div><div class="hb hc hd he hf"><h2 id="9659" class="mx lb hi bd kp my mz na lf nb nc nd lj jo ne nf ll js ng nh ln jw ni nj lp nk bi translated">参考</h2><p id="205f" class="pw-post-body-paragraph jf jg hi jh b ji lt ij jk jl lu im jn jo od jq jr js oe ju jv jw of jy jz ka hb bi translated">【2017 CVPR】【CMU Pose】<br/><a class="ae kq" href="https://arxiv.org/abs/1611.08050" rel="noopener ugc nofollow" target="_blank">利用部位亲和场的实时多人2D姿态估计</a></p><p id="dbea" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">【2019 TPAMI】【open Pose】<br/><a class="ae kq" href="https://arxiv.org/abs/1812.08008" rel="noopener ugc nofollow" target="_blank">open Pose:利用部位亲和场的实时多人2D姿态估计</a></p><p id="ef0d" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">【GitHub】<a class="ae kq" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank">https://github.com/CMU-Perceptual-Computing-Lab/openpose</a></p><h1 id="cb7e" class="la lb hi bd kp lc og le lf lg oh li lj io oi ip ll ir oj is ln iu ok iv lp lq bi translated">我以前对人体姿态估计的评论</h1><p id="fd39" class="pw-post-body-paragraph jf jg hi jh b ji lt ij jk jl lu im jn jo od jq jr js oe ju jv jw of jy jz ka hb bi translated"><strong class="jh hj">人体姿态估计</strong><a class="ae kq" href="https://towardsdatascience.com/review-deeppose-cascade-of-cnn-human-pose-estimation-cf3170103e36?source=post_page---------------------------" rel="noopener" target="_blank">deep Pose</a>】<a class="ae kq" href="https://towardsdatascience.com/review-tompson-nips14-joint-training-of-cnn-and-graphical-model-human-pose-estimation-95016bc510c?source=post_page---------------------------" rel="noopener" target="_blank">汤普森NIPS ' 14</a>】<a class="ae kq" href="https://towardsdatascience.com/review-tompson-cvpr15-spatial-dropout-human-pose-estimation-c7d6a5cecd8c?source=post_page---------------------------" rel="noopener" target="_blank">汤普森CVPR ' 15</a>]<a class="ae kq" rel="noopener" href="/@sh.tsang/review-cpm-convolutional-pose-machines-human-pose-estimation-224cfeb70aac?source=post_page---------------------------">CPM</a>]<a class="ae kq" rel="noopener" href="/@sh.tsang/review-fcgn-fully-convolutional-google-net-human-pose-estimation-52022a359cb3">FCGN</a>]<a class="ae kq" rel="noopener" href="/towards-artificial-intelligence/review-ief-iterative-error-feedback-human-pose-estimation-a56add160fa5">IEF</a>]<a class="ae kq" rel="noopener" href="/@sh.tsang/review-deepcut-deepercut-multi-person-pose-estimation-human-pose-estimation-da5b469cbbc3">deep cut&amp;DeeperCut</a><a class="ae kq" href="https://towardsdatascience.com/review-newell-eccv16-and-newell-pocv-16-stacked-hourglass-networks-human-pose-estimation-a9eeb76d40a5" rel="noopener" target="_blank">纽维尔ECCV'16 &amp;纽维尔POCV '</a></p><h2 id="c34b" class="mx lb hi bd kp my mz na lf nb nc nd lj jo ne nf ll js ng nh ln jw ni nj lp nk bi translated"><a class="ae kq" rel="noopener" href="/@sh.tsang/overview-my-reviewed-paper-lists-tutorials-946ce59fbf9e">我之前的其他评论</a></h2></div></div>    
</body>
</html>