<html>
<head>
<title>Intuition behind L1-L2 Regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">L1-L2正则化背后的直觉</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/intuition-behind-l1-l2-regularisation-2ac1e6a1bd81?source=collection_archive---------1-----------------------#2018-08-16">https://medium.com/analytics-vidhya/intuition-behind-l1-l2-regularisation-2ac1e6a1bd81?source=collection_archive---------1-----------------------#2018-08-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/46d61b6e85f6c9b381513452144e8101.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FQJdEl_AovVpXGUyoAAvsA.png"/></div></div></figure><div class=""/><p id="4fda" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正则化是使预测函数不太适合训练数据的过程，希望它能更好地概括新数据。这是正规化的一般定义。今天，我们将深入挖掘机器学习的两个著名的正则化，即L1和L2正则化，用纯数学的眼光。</p><p id="2ad3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在深入研究正则化技术之前，我们将建立一个基础，我们首先要考虑的是假设空间。为了捕捉ML中从X到Y ( <em class="jo"> f:X→Y) </em>的学习，我们通常会提出一些假设h1，…，hn，其中h∈H。这里，H是您的假设空间或集合。所有这些假设或预测函数都有一定程度的复杂性。复杂程度可以用特征的数量、多项式的次数或决策树的深度来衡量。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es jp"><img src="../Images/9401fe28a006933f02c09100ab38fb83.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*NXDvuOqBd5qSOfMi7aE4RA.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图1:复杂性的不同度量</figcaption></figure><p id="94e7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">给定假设空间F，考虑F中复杂度至多为r的所有函数:</p><p id="136b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">Fr</em>= { F∈F |ω(F)≤r }</p><p id="ef37" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> Fr </em>是复杂度≤ r的假设空间<em class="jo"> F </em>的子集，这里r是一个超调参数，我们可以从交叉验证阶段学习，并为我们的预测函数计算出最佳r。</p><p id="00ed" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们跳到称为伊万诺夫正则化的第一种形式的约束正则化。在下面的等式中，我们最小化经验风险，即我们的损失函数被约束在假设空间上，其复杂性不大于r。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es jy"><img src="../Images/9463018586e7bd30148842936065d7f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*5VWG2vnCpDV0J9YXkifYJw.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图2:伊万诺夫的正规化形式</figcaption></figure><p id="e88b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第二种形式的惩罚正则化被称为吉洪诺夫正则化，其中我们将乘以超参数𝝀的预测函数的复杂性添加到我们的目标函数中。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es jz"><img src="../Images/3227d968d369275550889948b00d20f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*d--0_ltjcUxrHvlGEL-ivg.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图3:吉洪诺夫正则化形式</figcaption></figure><p id="af80" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以直观地看到，当我们得到一个高度复杂的假设时，它会将目标函数射得更高，其主要目的是最小化。𝝀的大小平衡了我们要找到的假设的复杂性与我们拟合数据的程度或经验风险的大小之间的权衡。约束和惩罚形式的正则化之间的主要区别在于，在前者中，我们已经约束了假设空间，这使得优化函数更容易最小化损失，而在后者中，我们必须运行许多批次来使其正则化和最小化。如果我们通过调整r得到的所有可能值等于我们通过调整𝝀.得到的所有可能值，我们可以说这两种形式是等价的</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es ka"><img src="../Images/73b1ae8bb196ba0b699d2e290e4b421a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*QFaOTwpGig8_4Jp6V2nxcA.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图4:吉洪诺夫形式的岭回归</figcaption></figure><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es kb"><img src="../Images/709b01fb31ac51bd8792eed93c345d69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*40I649oYquFwSGzPiGQ-3A.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图Ivanov形式的岭回归</figcaption></figure><p id="0251" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以看到脊线回归方程的吉洪诺夫和伊万诺夫形式，同样适用于套索回归。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es kc"><img src="../Images/ebaac57bdb6cca60c2cc1af974048e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*u6Z5wExfIKL9IZVVGIM2Ew.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图6:岭回归的正则化路径</figcaption></figure><p id="2b3b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在图6的第一部分中，我们可以看到特征系数从更正则化到更不正则化的路径。约束和非约束系数之间的比率告诉我们我们在模型上做的正则化的数量。x轴上的值来源于我们采用不同的<em class="jo"> r. </em>值的比率。最初，系数非常小，这是因为它的约束边界受到严重惩罚，随着我们进一步移动，系数变得不受约束，经验风险处于最小值。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es kd"><img src="../Images/56e336c7082744c5852d150462c13711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*fthBA8fVa92CDDiKPdEshQ.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图7:套索回归的正则化路径</figcaption></figure><p id="03aa" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于Lasso回归，我们可以看到，当模型被大量正则化时，一些特征的系数在一段时间内为0。这是Lasso回归的USP，它提供了系数的稀疏矩阵，可用于多种方式，如识别较慢的非线性模型的重要特征，存储特征的内存较少，有时可以提供更好的预测。为什么套索回归给出稀疏模型，我们将在下一节讨论它</p><p id="b94e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">为什么lasso回归给出稀疏解？</strong></p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es ke"><img src="../Images/9712a64c8bfce9902e66b2675fd44281.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*7WaTnGibkHp7Hm_ed9PBQg.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图8:以w1和w2为轴的L1正则化的参数空间</figcaption></figure><p id="27ea" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">约束菱形给出了系数w1+w2 ≤ r，而w-hat是经验风险最小的无约束系数空间。当我们进一步移动无约束空间，使其更具约束性时，它会演变成一个椭球体，并在轴上或边线上触及我们的约束空间。第一个接触点给出了我们系数的解，如果它接触轴，我们得到稀疏解，如果它接触边，我们得不到稀疏解。现在的问题是，它给出稀疏解的概率有多大。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es kf"><img src="../Images/8998cb7f3df65992ea1f6a89bc410621.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*KZEi-f1Au59YC7gUFKMMJA.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图9:套索回归稀疏背后的原因</figcaption></figure><p id="f9a7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以从图9中看到，无论何时w-hat无约束空间位于红色和绿色之间的区域，它将接触边线上的约束空间，并给我们非稀疏解，但无论何时它位于红色或绿色空间，它将接触轴上的约束空间，并给我们稀疏解。这适用于参数空间的所有四个象限。并且w-hat的中心位于𝛂- <strong class="is hu"> 1 </strong>和𝛂- <strong class="is hu"> 2 </strong>之间的区域的概率比𝛂- <strong class="is hu"> 1 </strong>和𝛂- <strong class="is hu"> 2所覆盖的区域要小得多。</strong></p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es kg"><img src="../Images/fdecccb7bdee002c44b400428fe5274e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*CQBeUjx8ApcIbbTBQoZWzg.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图10:不同形式的约束空间</figcaption></figure><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es kh"><img src="../Images/5b2f3fd5d27b971018a7f9ed9c74c003.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*I2ZsJzFO7v0VKiTD-otigg.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图11:当q≤1时矩阵甚至是稀疏的</figcaption></figure><p id="b3b5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们涵盖了正则化的不同形式，以及套索回归给出稀疏矩阵的原因。敬请关注更多关于机器学习的深度文章。</p><p id="63d6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">礼貌:大卫·罗森博格(彭博)</p></div></div>    
</body>
</html>