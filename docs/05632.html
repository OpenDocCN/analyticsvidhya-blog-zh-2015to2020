<html>
<head>
<title>Ensemble Learning: Data Science</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成学习:数据科学</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ensemble-learning-data-science-3b0a3832c479?source=collection_archive---------18-----------------------#2020-04-27">https://medium.com/analytics-vidhya/ensemble-learning-data-science-3b0a3832c479?source=collection_archive---------18-----------------------#2020-04-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/6eb107d7b2a8ed0224f159541d3826fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tax3Kk7aLW4MIzdmR8Ke1Q.jpeg"/></div></div></figure><h1 id="b694" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">什么是集成学习？</h1><p id="ea11" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">集成学习是一种技术或过程，其中生成并组合多个模型来解决特定的机器学习问题。您可以将它们视为元算法，将多个模型结合在一起，主要是为了提高模型的性能并减少结果的方差。</p><p id="d70f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在任何回归或分类问题中，选择使用哪种模型都是极其重要的，选择取决于许多变量，如数据量、数据分布及其类型。</p><p id="1ae3" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">如果我们看看有监督的机器学习，一种算法从训练数据中创建一个模型，目标是在给定数据(X)的情况下最好地估计输出变量(y)。为了高度准确地预测这个输出变量，我们需要理解关于训练数据的特定算法的偏差-方差权衡。这些算法(基础模型)的<strong class="jq hj">偏差-方差权衡</strong>将告诉我们要执行什么类型的集成学习方法，因为一些方法将用于增加偏差，而其他方法将用于减少方差。因此，在我们进入不同的集成方法之前，我想先了解一下偏差和方差的含义，以了解它们的权衡如何影响我们选择的方法。</p><h2 id="7b06" class="kr ir hi bd is ks kt ku iw kv kw kx ja jz ky kz je kd la lb ji kh lc ld jm le bi translated">理解偏差-方差权衡:</h2><p id="f6e1" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">偏差和方差是模型中两个最基本的特征，我们的想法是以这样一种方式调整我们的参数，即我们的偏差和方差达到平衡，以创建良好的预测性能。不同类型的数据需要不同类型的算法，这些独特的算法带来了不同的权衡。看待这种权衡的另一种方式是，我们希望我们的模型有足够的灵活性来解决新数据集的任何潜在复杂性，但又不至于使模型在训练和测试数据上的准确性降低。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/3cebabafb111e68a1da592f42f1b4ab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*n0t9AG2CpFN1TSrKbGshmg.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来源:机器学习中的偏差-方差权衡。(2019年10月19日)。检索自:<a class="ae lo" href="https://ai-pool.com/a/s/bias-variance-tradeoff-in-machine-learning" rel="noopener ugc nofollow" target="_blank">https://ai-pool . com/a/s/bias-variance-trade-off-in-machine-learning</a></figcaption></figure><h2 id="b987" class="kr ir hi bd is ks kt ku iw kv kw kx ja jz ky kz je kd la lb ji kh lc ld jm le bi translated">偏移误差</h2><p id="b75e" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">偏差误差是模型为使目标更容易预测而做出的假设，换句话说，它是我们的平均预测与实际正确值的接近程度。偏差误差越低，我们对训练和测试数据的预测就越准确，偏差误差越高，我们的预测就越不准确。(偏差是对数据的近似能力)。</p><ul class=""><li id="6c77" class="lp lq hi jq b jr km jv kn jz lr kd ls kh lt kl lu lv lw lx bi translated"><strong class="jq hj">低偏差</strong>意味着对目标函数形式的假设更少。</li><li id="d8bc" class="lp lq hi jq b jr ly jv lz jz ma kd mb kh mc kl lu lv lw lx bi translated"><strong class="jq hj">高偏差</strong>意味着更多关于目标函数形式的假设，或者换句话说，高偏差意味着模型<strong class="jq hj">很少关注<strong class="jq hj">训练数据</strong>和<strong class="jq hj">过度简化</strong>模型。这将导致训练和测试数据中的高误差。如果试图拟合二次关系，线性回归模型会有很高的偏差。</strong></li></ul><h2 id="7f6e" class="kr ir hi bd is ks kt ku iw kv kw kx ja jz ky kz je kd la lb ji kh lc ld jm le bi translated">方差误差</h2><p id="ee9c" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">方差误差是如果使用不同的训练数据，目标函数的估计将改变的量。我们应该预料到算法无论如何都会有一些变化，如果我们引入新的训练集，我们应该预料到变化，但理想情况下，我们不希望看到一个集与下一个集之间有太多的变化。如果我们看不到太多的变化，我们知道基础模型擅长挑选输入和输出变量之间隐藏的潜在映射。</p><p id="0851" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">具有<strong class="jq hj">高方差</strong>的算法受到训练数据集的强烈影响，并且在涉及新数据集时不会很好地推广。具有很大灵活性的算法也有很高的方差，这些往往是非线性算法，如决策树或K-最近邻，而线性算法往往有很低的方差。(方差表示模型的稳定性)。</p><ul class=""><li id="7323" class="lp lq hi jq b jr km jv kn jz lr kd ls kh lt kl lu lv lw lx bi translated"><strong class="jq hj">低方差</strong>表示对目标函数估计值的微小变化</li><li id="8b92" class="lp lq hi jq b jr ly jv lz jz ma kd mb kh mc kl lu lv lw lx bi translated"><strong class="jq hj">高方差</strong>表示目标函数的估计值有较大变化。高方差模型会非常关注训练数据，并且不会对以前没有见过的数据进行很好的概括。</li></ul><p id="3614" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">低偏差算法</strong>:树、K近邻和支持向量机</p><p id="914a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">高偏差算法:</strong>线性回归、线性判别分析、逻辑回归。</p><p id="fc8a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">低方差算法:</strong>线性回归、线性判别分析和逻辑回归</p><p id="c188" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">高方差算法</strong>:决策树、K近邻和支持向量机。</p><p id="e60a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">整体目标是实现低偏差和低方差。《出埃及记》线性算法具有低方差和高偏差，而非线性算法具有高方差和低偏差，反过来，两者将需要不同的集成方法来使预测值更加准确。这些不同的集成方法将有助于通过组合几个模型来改善误差结果。</p><h2 id="6410" class="kr ir hi bd is ks kt ku iw kv kw kx ja jz ky kz je kd la lb ji kh lc ld jm le bi translated">整体方法:装袋、助推和堆叠</h2><h2 id="b54f" class="kr ir hi bd is ks kt ku iw kv kw kx ja jz ky kz je kd la lb ji kh lc ld jm le bi translated">装袋:</h2><p id="7de3" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">Bagging使用随机选择的训练数据子集来训练集合中的每个模型。通过在随机选择的子集上训练我们的模型，我们可以对原始集中的分布有一个很好的概括想法，从而创建强有力的预测。Bagging使用并行集成方法，在随机选择的子集上相互独立地训练每个模型。这方面的一个例子是随机森林算法。装袋通常旨在减少差异。</p><figure class="lg lh li lj fd ij er es paragraph-image"><div class="er es md"><img src="../Images/88053857ca035ad2b4208336c7073e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*ZGo-uKn5Ly8O6c86xmW-gQ.jpeg"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">来源:数据科学基础:集合学习者介绍。(2016).检索自:<a class="ae lo" href="https://www.kdnuggets.com/2016/11/data-science-basics-intro-ensemble-learners.html" rel="noopener ugc nofollow" target="_blank">https://www . kdnugges . com/2016/11/data-science-basics-intro-ensemble-learners . html</a></figcaption></figure><h2 id="f8b4" class="kr ir hi bd is ks kt ku iw kv kw kx ja jz ky kz je kd la lb ji kh lc ld jm le bi translated">增压:</h2><p id="c8a1" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">Boosting是一种将弱学习者组合成强学习者的顺序集成方法。它集合了多个模型，每个模型都弥补了上一个模型的不足。Boosting将采用弱模型，如回归或基于树的模型，并对其进行改进。例如，XGBoost是一种基于决策树的算法，它使用<strong class="jq hj">梯度推进</strong>来改进自身，其主要焦点是通过将精力放在最困难的观察上来减少偏差。</p><p id="08ac" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">流行的助推技术:</p><ol class=""><li id="1136" class="lp lq hi jq b jr km jv kn jz lr kd ls kh lt kl me lv lw lx bi translated"><strong class="jq hj"> AdaBoost </strong>:专为分类问题设计的AdaBoost。当训练决策树时，它从训练观察值并给每个观察值分配相等的权重开始。在对第一组进行评估后，它将提高难以分类的权重，降低容易分类的权重。它将重复这些过程，直到得到好的预测值。</li></ol><figure class="lg lh li lj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/f0646692faf14cb9a5216af625c2ef33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G1XCy_m08UVvXfYSJSBBOw.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">资料来源:系综方法:装袋、助推和堆叠。(2019 . 4 . 22)检索自:<a class="ae lo" href="https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205" rel="noopener" target="_blank">https://towards data science . com/ensemble-methods-bagging-boosting-and-stacking-c 9214 a 10 a 205</a></figcaption></figure><ol class=""><li id="17ad" class="lp lq hi jq b jr km jv kn jz lr kd ls kh lt kl me lv lw lx bi translated">梯度增强:梯度增强不是集中在数据点上，而是通过使用其向量梯度来执行相同的调整损失函数的权重的方法。损失函数是表示模型系数与基础数据拟合程度(实际值与预测值之差)的一种度量。梯度增强适用于分类和回归。</li></ol><h2 id="b0ec" class="kr ir hi bd is ks kt ku iw kv kw kx ja jz ky kz je kd la lb ji kh lc ld jm le bi translated">堆叠:</h2><p id="0c63" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">堆叠将多个分类或回归模型与元分类器或元回归器结合起来。基础模型在完整的训练集上被训练，而元模型在作为特征的基础级模型的输出上被训练。</p></div><div class="ab cl mg mh gp mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="hb hc hd he hf"><p id="6af0" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">要点:</strong></p><ul class=""><li id="f8f7" class="lp lq hi jq b jr km jv kn jz lr kd ls kh lt kl lu lv lw lx bi translated">集成学习是一种技术或过程，其中生成并组合多个模型来解决特定的机器学习问题。</li><li id="8eeb" class="lp lq hi jq b jr ly jv lz jz ma kd mb kh mc kl lu lv lw lx bi translated">在任何回归或分类问题中，选择使用哪种模型都是极其重要的，选择取决于许多变量，如数据量、数据分布及其类型。</li><li id="5263" class="lp lq hi jq b jr ly jv lz jz ma kd mb kh mc kl lu lv lw lx bi translated">偏差和方差是模型中两个最基本的特征，我们的想法是以这样一种方式调整我们的参数，即我们的偏差和方差达到平衡，以创建良好的预测性能。</li><li id="9861" class="lp lq hi jq b jr ly jv lz jz ma kd mb kh mc kl lu lv lw lx bi translated">不同类型的数据需要不同类型的算法，这些独特的算法带来了不同的权衡。</li><li id="ac73" class="lp lq hi jq b jr ly jv lz jz ma kd mb kh mc kl lu lv lw lx bi translated">线性算法方差小，偏差大，非线性算法方差大，偏差小。</li><li id="8a5c" class="lp lq hi jq b jr ly jv lz jz ma kd mb kh mc kl lu lv lw lx bi translated">Bagging使用随机选择的训练数据子集来训练集合中的每个模型。</li><li id="05dd" class="lp lq hi jq b jr ly jv lz jz ma kd mb kh mc kl lu lv lw lx bi translated">装袋通常旨在减少差异。</li><li id="42f2" class="lp lq hi jq b jr ly jv lz jz ma kd mb kh mc kl lu lv lw lx bi translated">Boosting是一种将弱学习者组合成强学习者的顺序集成方法。它集合了多个模型，每个模型都弥补了上一个模型的不足。</li><li id="9d4b" class="lp lq hi jq b jr ly jv lz jz ma kd mb kh mc kl lu lv lw lx bi translated">助推通常旨在减少偏差。</li></ul><p id="31d1" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">要了解选择哪种方法，重要的是要了解我们正在处理什么类型的数据，并了解不同回归和分类模型的缺点。一旦我们从这些模型中了解了偏差和方差的权衡，我们就可以开始选择一种能给出最佳结果的集成方法。</p><p id="724d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">如果你喜欢这篇文章，请随意点击“鼓掌”按钮，不要害羞，伸出手在下面留下评论，我会尽我所能回答任何评论。</p></div></div>    
</body>
</html>