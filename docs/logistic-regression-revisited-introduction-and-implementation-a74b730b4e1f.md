# 逻辑回归再探

> 原文：<https://medium.com/analytics-vidhya/logistic-regression-revisited-introduction-and-implementation-a74b730b4e1f?source=collection_archive---------10----------------------->

逻辑回归是分类问题最基本的机器学习算法之一。彻底理解它变得更加重要，因为它为更复杂的算法(如深度神经网络)奠定了坚实的基础。从技术上讲，我们可以说逻辑回归是神经网络的最简单形式。

## **什么是逻辑回归**

逻辑回归是一种经典的二元分类算法。当所需输出为 0 或 1 时使用。如果输入由 X 表示，输出由 y 表示，那么逻辑回归的输出可以表示为:

```
y^ = Probability(y=1 | X)
```

这里 y^ (y-hat)是预测值，介于 0 和 1 之间。在逻辑回归中使用 Sigmoid 激活函数来生成这种类型的输出。让我们接下来讨论这个。

## **乙状结肠激活功能**

如果您熟悉线性回归，输入 x 和预测输出 y^之间的关系定义为:

```
y^ = W*X + b 
```

其中 W 是权重，b 是偏差。上述公式返回 y^.的连续值。我们在逻辑回归中的目的是得到一个介于 0 和 1 之间的值。为了实现这一点，我们在线性输出之上应用 sigmoid 激活。所以输出变成了:

```
y^ = Sigmoid (W*X +b),
where Sigmoid(z) = 1 / (1+e^-z)
```

当 Z 是非常大的数时，Sigmoid(z)变得接近 1。当 Z 很小时(大负数)，Sigmoid(z)变得接近 0。因此，Sigmoid 函数的输出始终保持在 0 和 1 之间。

给定一个二元分类问题，我们的目标是找到 w 和 b 的最佳值，以最小化地面真实值 y 和预测值之间的差异。y^.损失函数用于此目的。让我们来了解如何。

## **损失函数**

损失函数被定义来衡量预测相对于地面真实值有多好。我们的目标是最小化损失函数。为了计算单个训练示例的损失 L，我们使用如下定义的对数损失:

```
L( y^, y) = -(y.log(y^) + (1-y).log(1-y^))
```

(1)如果 y = 0，那么损失将等于:-log(1-y^).y^应该接近 0，这样 log 的输出就变成 0。(因为 log(1) = 0)

(2)如果 y = 1，那么损失变成:-log(y^).y^应该接近 1，这样输出就变成 0。

对于多个训练样本，我们计算成本函数，这基本上是所有这些样本的平均损失。如果我们有 m 个训练样本，成本 J 定义为:

```
J(W,b) = 1/m (Σi=1 to m L(y^(i), y(i))
```

现在我们的目标是最小化为所有训练样本计算的成本函数。为此，我们使用梯度下降算法。让我们看看如何。

## **梯度下降**

在梯度下降算法中，相对于成本函数 J 计算参数 W 和 b 导数，以使其最小化。现在什么是导数。

> 导数基本上是函数的斜率，它决定了向哪个方向移动以使函数最小。换句话说，导数是当从属特征被更新时，函数改变的程度。

在逻辑回归中，我们根据 W 和 b 参数最小化成本函数。J W r t W 和 b 的导数分别表示为 dW 和 d b。如果α是学习率，下面是在梯度下降中更新参数的等式:

```
W = W - α.dW
b = b - α.db
```

让我们通过一个例子来理解这一点。假设我们有两个输入特征 x1，x2。相应的权重为 w1、w2，偏差为 b。因此，逻辑回归的输出可以写成以下语句:

```
Z = w1.x1 + w2.x2 +b
y^ = Sigmoid(z)
```

并且参数将被更新为:

```
w1 = w1 - α.dw1
w2 = w2 - α.dw2
b = b - α.db
```

现在让我们检查一下这个例子中梯度下降的单个步骤:

```
J=0, dw1=0, dw2=0, db=0for i=1 to m:
  z(i) = W.x(i) +b
  y^(i) = Sigmoid(z(i))
  J+= -[y(i).log(y^(i) + (1-y(i).log(1-y^(i)]
  dz(i) = y^(i) - y(i)
  dw1+= x1(i).dz(i) #repated for each input feature
  dw2+= x2(i).dz(i)
  db+= dz(i)J/= m  #averaing for m training samples
dw1/= m
dw2/= m
db/ = m 
```

在上面的实现中，有两个地方我们需要使用显式 for 循环。第一个循环用于 m 个训练示例，第二个循环用于 n 个输入特征。(在这个例子中我们只有两个特性，因此我们没有写 for 循环)。

在实现中使用显式 for 循环会随着训练数据变大而使梯度下降变得极其缓慢。我们使用向量化矩阵乘法技术来解决这个问题。

## **矢量化实现**

> 数组乘法的矢量化版本比使用 for 循环快大约 300 倍。

Numpy dot()函数用于将两个数组相乘。我们将使用以下矢量化版本进行实施:

x:跨 m 个训练样本的输入特征

y:m 个训练样本的基础真实向量

w:权重矩阵

a:m 个训练样本的预测向量

实施的胜利梯度下降将被定义为:

```
import numpy as npZ = np.dot(W,X) + b
A = Sigmoid(Z)
dZ = A - Y
dW = 1/m (X.dZ)
db = 1/m (np.sum(dZ))W = W - α.dW
b = b- α.db
```

尽管我们已经移除了训练样本(m)和特征数量(n)的循环，但我们仍然需要梯度下降中迭代次数的 for 循环。

最后，让我们总结一下实施逻辑回归梯度下降的步骤:

1.  计算当前损失
2.  计算当前梯度
3.  使用梯度和学习率更新参数

总之，我们在本文中讨论了逻辑回归的工作。这是我即将发表的关于深度学习的文章中的第一篇。希望你觉得有用。

如果你觉得这篇文章有用，请在评论中告诉我。我是一名数据科学爱好者和博客作者。你可以通过我的 LinkedIn [个人资料](https://www.linkedin.com/in/sawan-saxena-640a4475/)联系我。

感谢阅读。