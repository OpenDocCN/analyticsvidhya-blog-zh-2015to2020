<html>
<head>
<title>Big Data File Formats Explained Using Spark Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Spark解释大数据文件格式第2部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/big-data-formats-explained-using-spark-on-azure-gcp-part-2-b93bf59118e7?source=collection_archive---------11-----------------------#2019-12-04">https://medium.com/analytics-vidhya/big-data-formats-explained-using-spark-on-azure-gcp-part-2-b93bf59118e7?source=collection_archive---------11-----------------------#2019-12-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="bdb2" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用Azure Databricks &amp; Cloud Dataproc的演示</h2></div><h2 id="6c9c" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">在Azure &amp; GCP上使用Spark的演示</h2><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/68e6dcf8108451645e1e76b9ef4f4991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6izWZ3bi4RRfrQrm5s2r-A.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated"><em class="kl">图片来源:</em><a class="ae km" href="https://www.ellicium.com/orc-parquet-avro/" rel="noopener ugc nofollow" target="_blank"><em class="kl">【https://www.ellicium.com/orc-parquet-avro/】</em></a></figcaption></figure><p id="34b8" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">本文是第1部分的延续，第1部分讨论了所使用的各种大数据文件格式。</p><p id="df9e" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">使用此处<a class="ae km" href="https://www.kaggle.com/crailtap/taxi-trajectory" rel="noopener ugc nofollow" target="_blank">找到的滑行轨迹数据集</a>，我们将使用<a class="ae km" rel="noopener" href="/@aqeelahamed17/big-data-formats-explained-using-spark-on-azure-gcp-part-1-a83d153c4e66">第1部分</a>中描述的文件格式演示简单和更复杂查询的速度。</p><p id="863c" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">演示将在运行Azure或谷歌云平台的PySpark笔记本上进行，因此您可以在您选择的平台上运行该练习。这个练习使用的管道如图1所示。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es lg"><img src="../Images/e028d62c7626c541f7f0aab86678ba5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*pHc3DSNovLgk8huCG99xTQ.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated"><em class="kl">图1显示了用于将数据从本地存储加载到云中的管道。我们首先将数据集从本地存储加载到云存储“桶”中，然后从支持PySpark的笔记本中调用存储在该桶中的数据。</em></figcaption></figure><p id="ed99" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">一旦我们在本地机器上保存了数据集，我们就需要将它挂载到存储在云中的存储blob上。此后，我们将从PySpark笔记本中调用那个存储桶/容器，以便对其进行查询。(注:对于存储，GCP使用“桶”这个词，Azure使用“容器”。我们将交替使用它们)</p><h1 id="e8ed" class="lh iy hi bd iz li lj lk jd ll lm ln jh io lo ip jl ir lp is jp iu lq iv jt lr bi translated">使用Azure数据块的演示</h1><p id="186d" class="pw-post-body-paragraph kn ko hi kp b kq ls ij ks kt lt im kv ji lu kx ky jm lv la lb jq lw ld le lf hb bi translated">我们将使用Azure Databricks来调用存储在存储blob中的数据。要尝试这一点，你需要使用Azure版本的Databricks(点击<a class="ae km" href="https://docs.microsoft.com/en-us/azure/databricks/getting-started/try-databricks" rel="noopener ugc nofollow" target="_blank">此处</a>注册免费试用)。一旦你启动并运行了Databricks，首先要做的就是创建一个存储帐户来存储你的数据。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/03ffe142275382be5649af227bee3607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I3ng2L_07ii9C7dd5UoN2Q.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图2:在Azure门户上创建存储帐户</figcaption></figure><p id="2488" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">点击azure门户中的存储帐户，并创建一个新的存储帐户。此后，在存储帐户中创建一个容器来保存您的数据。</p><p id="3e0e" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">为此演示创建的存储帐户名为'<strong class="kp hj"><em class="lx">taxi storage 2019</em></strong>'，容器名为'<strong class="kp hj"> <em class="lx"> taxicab </em> </strong>'。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/5d74b40bf3b4ece093fd762faf692374.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l61qpJJBj10RZs783bpJyQ.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图3:显示了名为“taxistorage2019”的存储帐户和名为“taxicab”的容器。</figcaption></figure><p id="0d40" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">点击容器，我们就可以上传数据。本演示的数据集名为“train.csv”。在图3中，请注意在“设置”下，我们有“访问键”。请记下这一点，因为以后会用到它。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/e7b6ad3ebb28d42da886540c39baae3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6M-s-CtorON0UZNtClp2IQ.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图4:显示了在Azure的存储容器中上传的数据集“train.csv”。</figcaption></figure><p id="8a42" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">现在，我们可以登录Databricks，启动集群，并在新笔记本上查询数据集。(查看数据块<a class="ae km" href="https://docs.databricks.com/getting-started/quick-start.html" rel="noopener ugc nofollow" target="_blank">快速启动</a>页面，了解如何创建集群和加载新笔记本)</p><p id="948e" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">一旦创建了集群，我们就需要使用Azure门户上的帐户名和访问密钥连接到存储帐户。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/ba7797ff821a289273e5abb913b98322.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LRc03mBxkWPcrq-Shm85Mg.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图5:显示了在数据块上加载数据的步骤。</figcaption></figure><p id="ed10" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">在笔记本上设置存储帐户和访问密钥凭据后，我们需要使用“wabss://<container-name>@<storage-account-name>. blob . core . windows . net/<file-location>”格式从存储帐户中读入数据。</file-location></storage-account-name></container-name></p><p id="2e9d" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">读取数据后，我们需要将其保存为适当的文件格式。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/ae7d1bc1d145198628ec839b4bcf5fc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jOrBY_wgELVdvsnowQbcaA.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图6</figcaption></figure><p id="1dd1" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">然后，我们可以查询数据并比较性能。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/1630ef0e856bd0dda40b35a641c8482f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TA0q1NteccF-Z7wOVvHFqg.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图7</figcaption></figure><p id="9873" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">正如所料，我们可以看到，对于这个简单的查询，Avro没有Parquet和ORC快，但性能比CSV好得多。</p><p id="7ceb" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">让我们尝试一个更复杂的查询:</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/ce0fa118f82ee2bab5f67d798fe02410.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JQWcVxZyiW-Q7LHWtJ0YEA.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图8</figcaption></figure><p id="5687" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">对于仅在数据集的几列上运行聚合的查询，我们看到ORC的性能最好(如预期的那样),大约比Avro快10倍，比CSV快20倍！</p><h1 id="d2ea" class="lh iy hi bd iz li lj lk jd ll lm ln jh io lo ip jl ir lp is jp iu lq iv jt lr bi translated">使用云Dataproc的演示</h1><p id="672d" class="pw-post-body-paragraph kn ko hi kp b kq ls ij ks kt lt im kv ji lu kx ky jm lv la lb jq lw ld le lf hb bi translated">和Azure一样，你也可以在GCP上注册一个为期12个月的免费账户。一旦帐户设置完毕，我们将使用云存储创建一个存储“桶”来保存我们的数据。在我们访问GCP上的资源之前，我们首先需要创建一个<a class="ae km" href="https://cloud.google.com/appengine/docs/standard/nodejs/building-app/creating-project" rel="noopener ugc nofollow" target="_blank">项目</a>。完成后，我们可以从左侧面板导航到<strong class="kp hj">存储</strong>并创建我们的存储桶。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/871b352a5544aa4bb677c0872bf5f06f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SYZLeZGBJKjqJzb4234yIA.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图9: <em class="kl">导航到云存储页面。</em></figcaption></figure><p id="d9fa" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">我们已经创建了一个名为默认项目ID的存储桶，并上传了滑行轨迹数据集。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/72e285b96bd216c640c2078877a6e8c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mT6FekSxo7j6NecVqflR-w.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图10:“train . CSV”数据集已经上传到存储桶。</figcaption></figure><p id="5258" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">为了能够访问pyspark笔记本，我们首先需要使用Cloud Dataproc创建一个集群。导航到左边的Cloud Dataproc选项卡，并单击“Clusters”。点击<a class="ae km" href="https://cloud.google.com/dataproc/docs/tutorials/jupyter-notebook" rel="noopener ugc nofollow" target="_blank">此处</a>按照说明设置pyspark笔记本。</p><p id="3543" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">在这次演示中，我们将使用Jupyter实验室推出的笔记本电脑。一旦我们启动并运行了笔记本，就像Databricks一样，我们首先需要从google云存储中读取数据，使用适当的格式保存数据并查询数据。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/6ee634d4df7fbc966cc0e429c4baf3f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tpKo8Rrpq1BMlzYoHYcz_g.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图11</figcaption></figure><p id="3e8f" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">为了从云存储中加载数据，我们需要在文件位置前面加上前缀‘GS://’。因此格式应该是<strong class="kp hj">“GS://&lt;文件位置&gt;”。</strong></p><p id="dd84" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">此后，我们可以使用所讨论的文件格式查询数据并比较速度上的差异。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/cf7624f38488244ba4eda1903288bed2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JWril0E5sly1oE-oUDlctA.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">图12</figcaption></figure><p id="8a0a" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">Azure Databricks和GCP上使用的笔记本都可以在我的<a class="ae km" href="https://github.com/Akeelahamed17/Projects/tree/master/Big%20Data%20File%20Formats" rel="noopener ugc nofollow" target="_blank"> github </a>上找到。</p><h1 id="36d2" class="lh iy hi bd iz li lj lk jd ll lm ln jh io lo ip jl ir lp is jp iu lq iv jt lr bi translated">参考</h1><p id="da33" class="pw-post-body-paragraph kn ko hi kp b kq ls ij ks kt lt im kv ji lu kx ky jm lv la lb jq lw ld le lf hb bi translated">Nexla白皮书:<a class="ae km" href="https://www.nexla.com/resource/introduction-big-data-formats-understanding-avro-parquet-orc/" rel="noopener ugc nofollow" target="_blank">大数据格式介绍</a>。</p><p id="966a" class="pw-post-body-paragraph kn ko hi kp b kq kr ij ks kt ku im kv ji kw kx ky jm kz la lb jq lc ld le lf hb bi translated">Datanami: <a class="ae km" href="https://www.datanami.com/2018/05/16/big-data-file-formats-demystified/" rel="noopener ugc nofollow" target="_blank">大数据文件格式揭秘</a>。</p></div></div>    
</body>
</html>