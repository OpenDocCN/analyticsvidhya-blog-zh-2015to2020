<html>
<head>
<title>Math behind Linear, Ridge and Lasso Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性、脊形和套索回归背后的数学</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/math-behind-linear-ridge-and-lasso-regression-b9de216ebdf8?source=collection_archive---------1-----------------------#2019-08-23">https://medium.com/analytics-vidhya/math-behind-linear-ridge-and-lasso-regression-b9de216ebdf8?source=collection_archive---------1-----------------------#2019-08-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="51eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回归模型用于根据自变量/变量的值预测因变量的值。最常用的线性模型是线性回归、岭回归和套索回归。这篇文章不是要讨论这三个模型的应用，而是这些模型背后的直觉和数学。如果你想了解这些模型，可以看看我写的关于它们的其他文章。</p><p id="b1a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有两种众所周知的方法来说明线性模型如何通过数据点拟合直线。</p><ul class=""><li id="52f2" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">普通最小二乘法</li><li id="6ffc" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">梯度下降法。</li></ul><p id="795f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我们将探讨梯度下降法。用于预测因变量的值的数学方程</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es js"><img src="../Images/3469bf18f24233fab70450d662090d52.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/1*YSCAncjdh0gw40K9UP9lLQ.gif"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">线性模型方程</figcaption></figure><p id="68f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面提到的等式是机器学习模型试图优化的。它试图为每个参数(x)选择最佳的权重集(w)。</p><p id="6b45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然线性回归模型试图优化上述方程，那么优化必须基于特定的标准，该值必须告诉算法一组权重与其他组权重相比是最佳的。该值称为成本函数，由以下等式给出</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es ke"><img src="../Images/5402cd1404cc1590c8b18a3778b61da3.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/1*GTVK5SSt79dBlCP8ar03Lw.gif"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">成本函数，n为观察次数</figcaption></figure><p id="43de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中y的实际值和预测值之间的差异称为误差项。</p><p id="b0d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">均方误差原因(假设一个自变量):</strong></p><p id="2d58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们用代数方法展开平方误差项时，我们得到</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kf"><img src="../Images/b852f5e468883fa3a747e8f3dbeeaf75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/1*2At9OmuP_3HV_H9mgs0Muw.gif"/></div></figure><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kg"><img src="../Images/026194fef2f611ba6f04f00a05458104.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/1*lVJeAipYUF62ZozGUr3owQ.gif"/></div></figure><p id="7686" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">再次展开平方项，并对我们得到的相似项进行分组，</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kh"><img src="../Images/358b525f9f6cb2b3725e235d9847f140.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/1*dylryH7Ne3OFCtrCQNh2GA.gif"/></div></figure><p id="638b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这之后，一旦我们取括号中各项的平均值，我们就得到了方程。对于<em class="ki"> y </em>，我们将看到，其余的条款是类似地达成</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kj"><img src="../Images/5a71f7dc94d5bc69c7c5049cf1a4d3f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/1*PDZ3TCfsE6thV35B7AaGcA.gif"/></div></figure><p id="3deb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们得到了等式</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kk"><img src="../Images/6b95186c3b9b58120464ae687367f484.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*Q2bB3GlzVhh5io69eUSw_g.gif"/></div></figure><p id="851b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你真的观察上面的等式，很明显，除了权重(w0，w1)或系数，其余各项都是常数。因此，我们需要找到使上述等式最小化的(w0，w1)的值。为此，想象绘制w0和w1，对于满足等式的w0和w1的值，将得到在最低点具有最小值的凸曲线</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kl"><img src="../Images/27a3f910feb220f3b3636eebf9dd62cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*Rxn7PH2Hdu6S7CxJ_dQ8dQ.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">最低点的最小值，红点是与w0、w1的不同值相关的成本</figcaption></figure><p id="cef7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这条曲线很重要，你会在下面的章节中知道为什么。</p><p id="7591" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">整个想法很简单，从权重的随机初始化开始，不断将其与每个要素相乘，然后对它们求和以获得预测，计算成本项，并尝试根据迭代次数或容差值迭代最小化成本项，低于该容差值迭代将停止。因此，我们为特定的数据点生成一定数量的回归线，并选择成本最低的一条。</p><p id="98fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，我们已经为回归模型建立了一个成本函数，并且我们已经看到了如何选择成本最低的权重作为最佳拟合线。一切看起来都很好，但这里有一个小问题——随机选择100次迭代的权重可以给我们100组不同的权重和100条不同的线。机器可以从中挑选出最好的线，但很难说这条线是最佳拟合线，因为可能有许多组合比我们挑选的100条更好。</p><p id="07cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们需要找到一种方法来系统地减少重量，以达到最低的成本，并确保它创建的线确实是最佳拟合线，无论您选择其他什么线。这一点在下图中可以更好的理解。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es km"><img src="../Images/73808a0258b808fc79bcb833536d7f92.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*sKxRBbdaLzydTOWcAsv47Q.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">在红点处有全局最小值的凸曲线。</figcaption></figure><p id="5b1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们将上述曲线视为与每个权重相关的一组成本，则最低成本位于红色曲线所示的最底部。我们的算法必须确保它到达那个点，而这个任务在只有有限的一组权重的情况下是困难的。因此，为了克服这一点，我们使用了机器学习中最受欢迎的优化算法之一，即梯度下降算法。</p><p id="5a1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们从一组权重开始，计算其成本，并逐步向最低点移动(用数学术语来说，全局最小值)。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kn"><img src="../Images/d5d87dea7a9018780b4e49cb77d1f2c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:494/format:webp/1*mI6S9j-1Ot1aIE23mCUF-g.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">向全局最小值移动的梯度下降(凸优化，曲线很重要的原因)</figcaption></figure><p id="0d35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">红点是与不同权重集相关联的成本，并且值不断最小化以达到全局最小值。这种最小化成本以达到最低值的方式被称为梯度下降，这是一种优化技术。</p><p id="7b3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降通过获取成本函数的导数，将其乘以学习速率(下面解释的步长)并将其减去前面步骤中的权重，来完成向最陡下降(全局最小值)移动的任务。权重更新的公式为</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es ko"><img src="../Images/b41241ea969ba9f06fc7bf054aed1533.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/1*Bo1GHtcjraPVj3CHXHs5Og.gif"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">权重更新。</figcaption></figure><p id="655d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">学习率:</strong></p><p id="b3c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上一段中，我们谈到了一个叫做学习率的术语(上式中的α)。这个学习率决定了我们需要沿着曲线走多远才能到达全局最小值。这个超参数的大值将确保我们的算法将超过最低成本，而非常小的值将花费时间收敛到最低成本。所以选择这个值对于机器学习模型整体来说是极其重要的。</p><p id="6430" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">衍生品:</strong></p><p id="e194" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ki"> dw </em>项是成本函数的一阶导数。它的计算方法是</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kp"><img src="../Images/574ff992f5f0f85e0987a676fb16aca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/1*lUSfRSm8fE6hAhuBM1P70w.gif"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">一次导数</figcaption></figure><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kq"><img src="../Images/b245c9ae71738fde890c7f6bade8bd12.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/0*lGC-Gwem5viXYGbx"/></div></figure><p id="a4a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是线性回归背后的数学。均方误差也是优选的，因为它对具有较高差异的点的惩罚比对具有较低差异的点的惩罚大得多，并且它还确保了当相等比例的负值和正值被相加时它们不会被抵消，因为在没有平方的情况下相加误差项确保了这一点。</p><p id="ad61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归的GitHub要点如下</p><figure class="jt ju jv jw fd jx"><div class="bz dy l di"><div class="kr ks l"/></div></figure><p id="5143" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">山脊和套索:</strong></p><p id="0879" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">岭回归和套索回归与线性回归非常相似。唯一的区别是在套索回归中增加了<em class="ki">L1</em>罚分，在岭回归中增加了<em class="ki">L2</em>罚分。添加这些惩罚项的主要原因是为了确保正则化，将模型的权重缩小到零或接近零，以确保模型不会过度拟合数据。</p><p id="f7e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">过度拟合是当回归模型对训练数据调整得太多而不能很好地概括时出现的问题。它也称为高方差模型，因为测试集中因变量的实际值和预测值之间的差异会很大。</p><p id="585c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么Ridge和Lasso是如何克服过拟合问题的呢？这两种技术在它们的成本函数中都使用了一个额外的术语叫做惩罚。下面给出了这两种技术方程式</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es kt"><img src="../Images/40879eb5f78c57f5d9b00bd15d0ad2db.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/1*XN9hxyk82UySDAvQ_9w76Q.gif"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">岭回归成本函数</figcaption></figure><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es ky"><img src="../Images/54800ba57fa43efa3b8eef2592306a1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/1*PJav7bnRliTqNaeDVOjLWQ.gif"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">套索回归成本</figcaption></figure><p id="77a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么以上两个方程是什么，又是如何解决过拟合问题的呢？</p><p id="0dd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要回答这个问题，我们需要了解这两个方程的实际推导方式。山脊的方程式是</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es kz"><img src="../Images/900a7e14f389274edba2901d9e620678.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/format:webp/1*AQcMZkJEzrUqV_KdrJQY4Q.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">山脊约束</figcaption></figure><p id="cc32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将从扩展约束开始，这是产生约束的范数，</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es la"><img src="../Images/99fc851ae2c6a72f10e34d050234351d.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*q0Q6Rzm17Z_8r1TMJ6cdxw.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">2个参数w0和w1约束扩展</figcaption></figure><p id="162f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上式右手边的项可以是任何常数值。这为我们提供了一个圆的方程，原点为(0，0)，半径为c。因此，岭回归的基本作用是创建一个最小化成本函数的解决方案，使得<em class="ki"> w0 </em>和<em class="ki"> w1 </em>的值只能来自圆内或圆周上的点。</p><p id="bcf3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">人们试图最小化的等式变成:</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es lb"><img src="../Images/0375b899562301ff9504b150a1835cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/1*lfM_BhxvUIRo8aBYD-MI2g.gif"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">我们需要最小化的等式</figcaption></figure><p id="ef3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于第二项中λ的固定值，λ与<em class="ki"> c </em>的乘积产生一个常数项。所以本质上我们将最小化上面山脊的方程。Lambda是一个超参数，我们可以根据自己的选择将其设置为特定值。如果它被设置为零，那么岭的方程被转换成正常的线性回归方程。</p><p id="8f59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从那时起，就使用梯度下降进行优化而言，该过程类似于正常线性回归的过程。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es lc"><img src="../Images/6cd67b9e43c772a80409ece3371c3381.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/1*5d9Nn0-QbeHJCLfboxvgnQ.gif"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">脊函数的导数</figcaption></figure><p id="f202" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Ridge的Github要点是</p><figure class="jt ju jv jw fd jx"><div class="bz dy l di"><div class="kr ks l"/></div></figure><p id="3dc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">套索回归:</strong></p><p id="9a15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当涉及到特征选择时，岭回归作为算法有一个小缺陷，即当有两个彼此高度相关的特征时，权重在这两个特征之间平均分布，这意味着将有两个具有较小系数值的特征，而不是一个具有强系数的特征。当您希望基于阈值选择某些要素时，这就成了问题。如果值较高的单个要素是单独的，则可能会被选中，但由于多重共线性，这两个要素都不会被选中，因为它们的权重是分开的。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es ld"><img src="../Images/47b6d773cc1c25f6f8e1d01da03cb0d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*SD9gXUS25axCKkaEXtlj7Q.png"/></div></figure><p id="136c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像岭回归一样，套索回归进行正则化，即将系数缩小为零。当有大量特征来对机器学习算法建模时，这是很重要的。其方式是通过尝试最小化成本函数，即受制于约束的残差平方和。</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es le"><img src="../Images/fa51fd084f2601a1813044d6be35f65b.png" data-original-src="https://miro.medium.com/v2/resize:fit:186/1*Zstaco2-yAYBmHDCbsQstQ.gif"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">套索约束</figcaption></figure><p id="9805" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将这个等式扩展为<em class="ki"> w0 </em>和<em class="ki"> w1 </em>，我们得到</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es lf"><img src="../Images/7ee70611b877288fd44b8051243d8ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/1*jIOc7kzmSbDdEZoBBUnQqQ.gif"/></div></figure><p id="4f7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与岭模型类似，套索回归将约束的成本降至最低，但对于套索，当我们绘制约束点时，将有一个以(0，0)为中心创建的菱形。这背后的直觉是，我们将得到具有剩余平方和值的等高线图，这些值必定在钻石的圆周内或圆周上。因为等高线图接触菱形的端点的机会非常高，从而使某些特征的权重为零。</p><p id="298e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑到Lasso回归使用<em class="ki"> l1 </em>范数，当我们尝试更新成本函数时，其导数要么为负1，要么为正1，并且在点0处无法确定。因此，使用闭合形式方法或梯度下降来更新要素的权重是不可行的，因此Lasso使用一种称为坐标下降的方法来更新权重。因为它使用软阈值来获得与特征相关联的权重值。下面的代码可以更好地解释这一点:</p><figure class="jt ju jv jw fd jx"><div class="bz dy l di"><div class="kr ks l"/></div></figure><blockquote class="lg lh li"><p id="1f81" class="if ig ki ih b ii ij ik il im in io ip lj ir is it lk iv iw ix ll iz ja jb jc hb bi translated">请在评论部分指出任何错误。</p><p id="4821" class="if ig ki ih b ii ij ik il im in io ip lj ir is it lk iv iw ix ll iz ja jb jc hb bi translated">关于实施<a class="ae jd" rel="noopener" href="/@sidharths758/polynomial-regression-overfitting-and-ridge-regression-an-overview-70de53f0ccab">脊</a>或<a class="ae jd" rel="noopener" href="/@sidharths758/least-absolute-shrinkage-and-selection-operator-lasso-regression-9132dc80654b">套索</a>回归的其他文章。</p></blockquote></div></div>    
</body>
</html>