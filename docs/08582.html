<html>
<head>
<title>How to select the perfect CNN Back-bone for Object Detection? — A simple test</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何选择完美的CNN脊梁进行物体检测？—一个简单的测试</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-select-the-perfect-cnn-back-bone-for-object-detection-a-simple-test-b3f9e9519174?source=collection_archive---------5-----------------------#2020-08-04">https://medium.com/analytics-vidhya/how-to-select-the-perfect-cnn-back-bone-for-object-detection-a-simple-test-b3f9e9519174?source=collection_archive---------5-----------------------#2020-08-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="340a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗨！因此，您有兴趣使用卷积神经网络(CNN)来解决计算机视觉(CV)问题。如果您正在使用分类、检测或细分模型，请继续。理想情况下，我在这里描述的技巧(用代码)应该适用于任何使用卷积特征提取器(CONV-FE)的深度学习(DL)模型。(“特征提取器”有时也被称为“骨架”或“编码器”)</p><p id="3ead" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随着CNN日益深入和广泛，可视化通常是开发人员解释其模型的唯一方式。无论您是试图找出您的模型不工作的原因，还是试图验证您的模型的性能，或者试图为您的管道选择最佳的conv-费，可视化通常是教科书解决方案的最接近的替代品。</p><p id="ac72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">今天，我们将看到一个巧妙的技巧来可视化任何CV任务的CONV-FE。我们将使用Keras和TensorFlow后端来演示这个技巧。这个技巧可以用来“调试”没有正确训练的模型，或者“检查”/“解释”一个训练过的模型。</p><p id="b5fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于新读者来说，值得一提的是建筑和conv-费之间的区别。SSD、YOLO和F-RCNN是当今一些流行的对象检测架构。如你所知，架构是具有某种形式的<em class="jd">灵活性</em>的<em class="jd">模板</em>，用户可以选择<em class="jd">为他的用例调整</em>架构。对于新读者来说，值得一提的是架构和主干的区别。CONV-FE是一种大多数架构都将其视为插件的东西。一些受欢迎的conv-Fe是VGG-16、ResNet50和MobileNetV2。经常可以看到，CONV-FEs的选择将极大地影响用特定架构构建的模型的性能(速度和准确性)。出于这个原因，从业者提到他们在模型名称中使用的CONV-FE。F-RCNN(ResNet50)是使用ResNet50 CONV-FE构建的具有F-RCNN论文中描述的体系结构的模型。换句话说:F-RCNN(MobileNetV2)和F-RCNN(VGG-16)遵循相同的网络架构，但是使用不同的conv-Fe。因此，我们可以预期每个系统会有不同的性能。conv-菲斯也有他们的建筑，这就是为什么我们有VGG-16和VGG-19，但这是另一天的故事！</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/3036bdbc3e7739b9673ce992bcf73631.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*1M3TgW9ADqe8J5YGnM-oLw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><strong class="bd jq"> <em class="jr">图1: </em> </strong>模板为通用的物体检测算法。这适用于YOLO、F-RCNN、SSD等。</figcaption></figure><p id="1fa3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图1显示了DL供电对象检测系统的高级架构。众所周知的架构，如YOLO，F-RCNN和SSD可以用这个模板来概括。[ <em class="jd"> W，H，</em> 3]表示图像的输入宽度和高度，也表示输入图像具有3个通道(<em class="jd">又名</em> RGB输入)。【<em class="jd"> w，h，N </em>表示CONV铁的输出尺寸。请注意，由于<em class="jd"> N </em>是特征提取器的最后一个CONV层中的滤波器数量，因此它通常是一个很大的数字(对于VGG-16，<em class="jd"> N = </em> 512)。<em class="jd"> w </em>和<em class="jd"> h </em>几乎总是分别小于<em class="jd"> W，H </em>，因为在当今的大多数架构中，CONV块之后通常是池层。</p><p id="60a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上所述，CONV-FE的选择对于我们的CV系统实现最佳性能至关重要。然而，问题在于<em class="jd"> N的高值。不可能将所有维度都任意大的三维张量可视化！这就是我们今天要解决的问题。让我们开始吧，别再废话了。为了简单起见，我们将使用Google Colaboratoy。</em></p><p id="f845" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们选择一个CONV吧。VGG 16号怎么样？让我们试着加载它。</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="js jt l"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><strong class="ak">代码片段1: </strong>加载CONV-FE的脚本</figcaption></figure><p id="7c7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦模型被加载，我们需要做一些模板脚本来加载我们想要试验的图像。我在这里不描述这些步骤，因为它们会因您的特定用例而有很大的不同。(但是，如果您坚持的话，它们是在带有注释的代码中实现的)。现在，我们假设所有需要的图像都存储在一个名为“images”的目录中，该目录直接位于我们的工作目录中。现在让我们使用这个技巧吧！</p><figure class="jf jg jh ji fd jj"><div class="bz dy l di"><div class="js jt l"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">片段2: 加载图像、对它们进行推理并可视化conv-费的代码</figcaption></figure><p id="ca53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您将看到类似下面的输出(这些输出也将保存在直接在工作目录中创建的“结果”目录中)</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es ju"><img src="../Images/a51416b44e60a35baa773ff075559de1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pZD5BwvmntesiBGAFRM_8Q.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><strong class="bd jq">图2.1: </strong>结果1 _大象</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es ju"><img src="../Images/1887953e8a334b2361dde2d781a3124e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vpu-g5IHNo3NKZfSUdSvCQ.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><strong class="bd jq">图2.2: </strong>结果2 _汽车</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es ju"><img src="../Images/f90918a909f3572af6981fa35f725ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mwlEJobf9v7OHl0W1DOEBg.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><strong class="bd jq">图2.3: </strong>结果3_Man</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es ju"><img src="../Images/5392470e3a43b6087685c777e5787734.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FAGvLkvGFi2eEfdTGVLwgw.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated"><strong class="bd jq">图2.4 </strong>结果4_PCB</figcaption></figure><p id="b279" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二列显示了任何通道在由CONV-FE产生的输出特征图中可以达到的像素最大值。第三列显示所有频道的平均值。如果结果中的第二列和第三列显示出清晰的特征分离，我们说CONV-FE适合于我们的数据集。</p><p id="f065" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所见，1和2中的结果非常好，但3和4中的结果表明，如果您有全屏PCB图像或穿着传统服装的印度男子的图像，则当前模型并不真正合适。但是，如果数据集包含野生动物或汽车/车辆的图片，此模型似乎是一个不错的选择。</p><p id="7ca1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们尝试为代码片段2中的第25行和第26行添加一个小的调整。根据图1，输出特征图可以有任意数量(N)的输出通道。这是我们不能想象和比较conv-菲斯的主要原因。我们使用常见的聚合技术，即。求和并平均，以获得特征图质量的总体近似值。我没有使用通道的像素中值进行聚合。你能在评论中解释为什么吗？如果我添加第四个中间列，您能告诉我输出会是什么样子吗？(<strong class="ih hj">提示:</strong>您可以使用np.median()沿着任意numpy数组的任意轴查找中值)</p><p id="dbd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Github链接完整代码:<a class="ae jz" href="https://github.com/IshanBhattOfficial/Vizualize-CONV-FeatureExtractors" rel="noopener ugc nofollow" target="_blank">https://github . com/IshanBhattOfficial/vizulize-conv-FeatureExtractors</a></p><p id="95df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望这是有帮助的！</p></div></div>    
</body>
</html>