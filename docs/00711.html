<html>
<head>
<title>All about Gradient Descent and its variants</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">所有关于梯度下降及其变体</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/all-about-gradient-descent-and-its-variants-d095be1a833b?source=collection_archive---------7-----------------------#2019-08-30">https://medium.com/analytics-vidhya/all-about-gradient-descent-and-its-variants-d095be1a833b?source=collection_archive---------7-----------------------#2019-08-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="e0bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章向您解释了梯度下降算法的变体，这些变体用于优化任何深度学习问题的解决方案。如果你想深入了解梯度下降，那么我推荐你<a class="ae jd" href="https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><p id="44a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设你在一座山的山顶上，你想到达山的最低点的一个湖。你会往哪边走？？？</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/8f8d7cfa6b9f8f959317f62cb2ac279c.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*OXn92-UYqQvo1jP0S0cAmQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">梯度下降</figcaption></figure><p id="eb2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最简单的方法是检查你站的地方，找到地面下降最多的地方，然后开始向那个方向移动。这条路很有可能会带你去湖边。这就是上图所描绘的。从图形上看，它可以如下所示:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jq"><img src="../Images/0da52b80dbac39abcc1931a276854407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*09kq2L23D9XM_9Xtr8gc8Q.png"/></div></div></figure><p id="869b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">峰值用红色区域表示成本高的区域，而最低点用蓝色区域表示成本或损失最小的区域。在任何深度学习问题中，我们都试图找到一个模型函数，它给出的预测与实际值相比损失最小。假设我们的模型函数有两个参数，那么在数学上，我们希望找到参数θ1和θ2的最佳值，使我们的损失最小。上图中显示的损失(J(θ))空间告诉我们，如果我们为参数选择一个特定值，我们的算法将如何执行。这里，θ1和θ2是我们的x和y轴，而损耗对应于z轴绘制。</p><p id="e963" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降法则指出，我们应该移动的方向应该与梯度成180度。如果有人想知道为什么要朝那个特定的方向前进，我已经在<a class="ae jd" rel="noopener" href="/@anjana7718/why-we-move-opposite-to-gradients-in-gradient-descent-9077b9aa68e4"> <strong class="ih hj">这篇文章</strong> </a>中回答了。换句话说，与梯度方向相反。这里梯度是偏导数的向量。我们的模型函数相对于θ1和θ2或权重(w)的δ。以下是普通梯度下降算法的更新规则。这里η是学习率，它是常数。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jv"><img src="../Images/f119ff9551ae74020ec70c4bb3d722cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*nO7UCBCk1UjsJvzB3VbgSg.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">普通梯度下降的更新规则</figcaption></figure><p id="2d0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">很多时候，我们会遇到这样的情况，即使模型没有达到全局最小值，它也会在训练过程中停滞不前并停止学习。这通常发生在训练参数位于缓坡区域从而难以导航的时候。这是因为这些区域的梯度非常小。那么我们如何解决这个问题呢？？？</p><p id="bf89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">基于动量的梯度下降</strong></p><p id="4f93" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设你被反复要求朝同一个方向移动，那么你对自己的移动有了一些信心，开始朝那个方向加快步伐。类似于球在斜坡上滚动时获得动量。如果我们在训练中使用相同的类比，我们可以假设，如果我们的梯度在特定方向上持续下降，那么我们可能正在向正确的方向移动，并开始获得信心，在那个方向上迈出更大的步伐。这种获得动量的类比用于基于动量的梯度下降。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jw"><img src="../Images/407c52d7e47fd36367e4ed1e477fff04.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*H3c5niGx5bKUgV00FrgOwA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">基于动量梯度下降的更新规则</figcaption></figure><p id="e7d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更新参数负责先前的更新，并且如果梯度连续地在相同的方向上，则给我们动量。超参数γ和η控制过去历史和当前梯度的贡献。与普通梯度下降相比，这有助于模型更快地收敛。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jx"><img src="../Images/b28a61b8c23c497dfdf6aff62a2f6021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QzuTMjcr7gzZoh8nZ8bnkA.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">基于动量的梯度下降</figcaption></figure><p id="c08c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里出现的问题是，动量逐渐增加到梯度开始在极小值谷来回振荡的程度。正如我们在上面看到的，当动量带着它走出谷底时，它每次都超过全局最小值。因此，在最终收敛之前，需要进行许多U形转弯。尽管有这些u形转弯，它仍然比普通梯度下降法收敛得更快。怎么才能解决这个振荡问题呢？？</p><p id="df73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">内斯特罗夫梯度下降</strong></p><p id="eb34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">内斯特罗夫加速梯度下降建议，与其通过动量和梯度向前移动，为什么不先使用动量移动，检查该点的梯度，然后从初始点相应地向前移动。这解决了极小值谷中的多次振荡问题。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jy"><img src="../Images/87e1e42df5da3e350ea82ff65b2be79d.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*4bM4LQZUMWi7LeWS2xQIjQ.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">内斯特罗夫加速梯度下降的更新规则</figcaption></figure><p id="02b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如下图所示，我们使用以前的更新历史计算W-look ahead，并计算该点的梯度。如果梯度方向反转，说明我们已经超越了极小值。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es jz"><img src="../Images/dba104835242849b847a78afb6a0145f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*xBycsEpCf290UC2vukahbw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">NAG前瞻计算</figcaption></figure><p id="beb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，最终的权重更新将在展望未来时考虑历史和梯度。如果它们的符号相反(当最小值被超过时发生)，权重的更新量将会减少，从而降低振荡的程度，如下所示。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jx"><img src="../Images/5f03933bf3d81d48a03f1ea679b8bb80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Eoy80Zn6iLsUNwiE1P-2UA.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">内斯特罗夫加速梯度下降</figcaption></figure><p id="9405" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，我们已经看到了单特征问题的梯度优化。假设我们有多维特征，那么可以利用相同的方法同时找到每个特征的优化。</p><p id="53f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在你能说出这里梯度下降的行为表明了什么吗:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jx"><img src="../Images/373640580ba14fdd12a57b1a738ffd59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ej9WAbbge1XNw8WOuDAU9w.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">x1特征稀疏的多变量数据上的梯度下降</figcaption></figure><p id="f59a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以观察到，最初在x1的方向上有许多更新。在点模型了解到没有必要在这个方向上继续移动，因为这里没有最小值。因此，它决定向x2的方向前进，并最终达到最小值。</p><p id="ddef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">x1方向上的频繁更新表明该模型接收了大量具有x1值的数据点和较少的具有x2值的数据点。因此，与特征x1相比，特征x2似乎是稀疏的。这种稀疏性在包含多达1000个特征的神经网络中很常见。</p><p id="43bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们可以看到，稀疏特征给模型的学习带来了问题，因为没有足够的值供模型学习。我们可以肯定地说，稀疏特征可能没有密集特征重要，因为它们被记录的次数很少。这在大多数情况下可能是正确的，但在某些情况下，密集要素可能会携带一些有用的信息。其中一个例子是阿米尔·汗的电影。虽然他两年拍一部电影，但是这部电影成功的几率非常高。因此，稀疏数据可能也是一个重要的特性。那么我们如何解决学习稀疏特征的问题呢？？？？</p><p id="2a6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是否有可能为每个参数设置不同的学习速率，以考虑到特性的频率？？？</p><p id="f19f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> AdaGrad —自适应梯度下降</strong></p><p id="7fb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">AdaGrad代表自适应渐变。它与参数的更新历史成比例地衰减参数的学习速率。因此更多的更新意味着更多的衰退。通过使用特定的学习率，我们可以确保尽管稀疏，x2也能获得更高的学习率，从而获得更多的更新。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ka"><img src="../Images/7e6cc63d9b2a8fc5c4c7efb9fc31f69a.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*FdbytjWMwN3Mrwt30WjEmA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">AdaGrad的更新规则</figcaption></figure><p id="8a8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，它确保了如果w经历了大量更新，则其有效学习率会因为分母的增长而降低。</p><p id="8b65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然这有助于学习稀疏参数，但是梯度有可能变得太低，从而完全停止学习。考虑下图:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jx"><img src="../Images/c75f2cbe3e834019065755efa9303ad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ckxbj9Ft9QCr-DJRF3az5A.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">阿达格拉德因渐变消失而被卡住</figcaption></figure><p id="4e31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里学习已经停止，因为学习率已经减少到模型根本没有梯度的程度。随着时间的推移，有效学习率将降低，从而不再对x2进行更新。我们能避免这种情况吗？？</p><p id="1324" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RMS — Prop</p><p id="59c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">阿达格拉德非常积极地降低学习率。结果，一段时间后，由于学习速率的衰减，频繁参数将开始接收非常小的更新。那么为什么不衰减衰减参数呢？为了做到这一点，我们衰减分母，以防止其快速增长。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kb"><img src="../Images/c67769026deef242042625d91bf5aec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*CQf0Mat10IcjK7hlpfHZ5w.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">RMS的更新规则-正确梯度下降</figcaption></figure><p id="888d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以在下图中观察到，与梯度下降相比，RMS-Prop的训练时间要少得多。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jx"><img src="../Images/715bd8366879df2b521fcf95f6d2d5ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4cJ4HNjWQ639tnqV4y5-hQ.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">RMS-Prop优化</figcaption></figure><p id="683b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，当AdaGrad由于学习速率衰减而停滞不前时，RMS-Prop通过降低衰减来解决这个问题。</p><p id="1326" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">亚当梯度下降</strong></p><p id="3884" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，考虑到我们在上面学到的一切，使用动量来加快收敛，并衰减关于特征的参数，我们为什么不结合这两种方法并使用它。</p><p id="6fff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们尽RMS-Prop所能解决AdaGrad问题，并使用梯度的累积历史。这就是亚当优化算法所做的。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kc"><img src="../Images/1ad48e3531fe4c7eb445c9e32c182f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*tznYPvGRYxFIxGSDZ0_8jw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">Adam优化的更新规则</figcaption></figure><p id="fd76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Adam代表自适应矩，因为它利用一阶和二阶矩进行模型收敛。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jx"><img src="../Images/0839484dd007be3aec6dadb3ab7005a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Scw-B56AFT1VRhHUyvJ7Zg.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">亚当优化</figcaption></figure><p id="a2b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上图所示，与上述方法相比，Adam收敛所需的时间要少得多。Adam是目前最好的优化算法，适用于当今几乎所有的问题。</p><p id="0f94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上述算法定义了可用于实现优化或全局最小值的各种方法。它们考虑了参数和超参数，用于优化学习和权重更新。要记住的另一件事是在训练期间每个时期要传递多少数据。</p><p id="1abe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">香草渐变下降</strong></p><p id="4174" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更新权重的理想方式是在传递一次完整的数据集之后。这就是香草梯度下降所做的。尽管仅在计算完整数据集的损失之后执行权重更新是理想的，但是对于一个较小的权重更新，我们正在执行数千次计算。这导致时间减少，并使用大量内存进行模型优化。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jx"><img src="../Images/210db35f7a04714d3dd62b63e1035b81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xx4l7ZMRh1ulSAV5VqOXVg.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">N(数据集大小)= 5的普通梯度下降</figcaption></figure><p id="ce62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">克服这种巨大计算量的一种方法是在通过每个数据点并计算损失之后更新权重。这样，权重在一个时期中被更新N次，其中N是数据集的大小。这被称为<strong class="ih hj">随机梯度下降</strong>。</p><p id="98ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下图显示了随机梯度下降算法。我们可以清楚地看到，每次权重更新后都会出现大量振荡。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jx"><img src="../Images/9c5259d5230ec4b8ef855e821ffe095b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1RULslyUZKFOIO__Dv5Cuw.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">N = 5的随机梯度下降</figcaption></figure><p id="cf66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于模型使用一个数据点计算整个数据集的梯度，因此会出现随机梯度下降的问题。这就好比说，如果扔一枚硬币，正面朝上，正面的概率是1，反面的概率是0。我们从总体中抽取单个数据，并假设整个总体的行为与该点相似。</p><p id="7918" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">执行梯度更新的理想方式是考虑权重更新所有点的损失。那么我们如何实现这一点呢？？？</p><p id="3137" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以在传递较小的一组或一批数据点之后执行更新，而不是在每个数据点执行更新。这就是<strong class="ih hj">小批量梯度下降</strong>的作用。它对一批数据点进行采样，计算这些点的损失，最后更新权重。因此，如果批大小是k个数据点，则在每次迭代中通过N/k批k个数据点，并且在每个时期结束时执行N/k次权重更新。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jx"><img src="../Images/78d5682b7820660462a4533d459034c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ItRWSMsDK8gf4DFDNT8Klw.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">N = 5和K = 3的小批量梯度下降</figcaption></figure><p id="f85f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们可以清楚地看到振荡的减少和更快的优化。</p><p id="9055" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">至此，我们已经了解了对传统梯度下降算法的改进。我希望你喜欢阅读这篇文章，并精通梯度下降及其变种。</p><p id="e651" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里是<a class="ae jd" href="https://github.com/anjana7/Gradient-Descent-Variants" rel="noopener ugc nofollow" target="_blank">代码</a>的git hub链接。</p></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><p id="4a30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习— NPTEL by米特什·哈普拉<br/>T5】https://www.youtube.com/watch?v=aPfkYu_qiF4&amp;list = plyqspqzte 6m 9 gcgajvqbc 68 hk _ JKGBAYT</p><p id="64de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">菲利浦·萨尔瓦托<br/> <a class="ae jd" href="https://gist.github.com/felipessalvatore/c2e1c09dfcb8710b847e2457620f8204" rel="noopener ugc nofollow" target="_blank">绘制3d渐变图https://gist . github . com/feli pes Salvatore/C2 E1 c 09 dfcb 8710 b 847 e 2457620 f 8204</a></p></div></div>    
</body>
</html>