<html>
<head>
<title>Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/natural-language-processing-bb8de5fc902?source=collection_archive---------7-----------------------#2020-10-25">https://medium.com/analytics-vidhya/natural-language-processing-bb8de5fc902?source=collection_archive---------7-----------------------#2020-10-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3610" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><strong class="ak">社交媒体分析— </strong>分布语义</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/e8f2003ce8f6816c1014b6690cbe1d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wrKeIkPbaGz1ymH-nNBOWQ.png"/></div></div></figure><p id="dcae" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">英国语言学家约翰·弗斯在 1957 年说过——“<strong class="jl hj">你可以通过一个人交什么样的朋友来了解这个人”。</strong></p><p id="3ba1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">最常用的单词表示法是使用“单词向量”。有两种技术可以将单词表示为向量:</strong></p><blockquote class="kf kg kh"><p id="b7fe" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><strong class="jl hj"> <em class="hi">术语-文档出现矩阵，</em> </strong> <em class="hi">其中每行是词汇表中的术语，每列是文档(如网页、推文、书籍等。)</em></p><p id="954e" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><strong class="jl hj"> <em class="hi">术语-术语共现矩阵</em> </strong> <em class="hi">，其中第 I 行和第 j 列表示第 j 个单词在第 j 个单词的上下文中的出现。</em></p></blockquote><p id="c2cb" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">“现在，每个单词和文档都有相应的向量表示——每行是一个表示单词的向量，而每列是一个表示文档(或上下文，如推文、书籍等)的向量。)".</p><blockquote class="kf kg kh"><p id="5a0d" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><strong class="jl hj">术语-文档矩阵</strong></p><p id="c18d" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">考虑四个文档，每个文档都是从一部电影中截取的一段。假设你的词汇只有以下几个词:恐惧、啤酒、乐趣、魔法、巫师。</p><p id="c5d1" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">下表总结了术语-文档矩阵，每个条目代表一个术语在电影中的使用频率::</em></p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es km"><img src="../Images/4531db73a005e53e98bdb3675768e710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1LToQIV0QMCbqD8PPqh7rg.png"/></div></div></figure><p id="4ade" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">创建共生矩阵有两种方式:</p><blockquote class="kf kg kh"><p id="2f59" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><strong class="jl hj"> <em class="hi">利用发生上下文(如句子):</em> </strong></p><p id="55bc" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">每个句子被表示为一个上下文(也可以有其他定义)。如果两个术语出现在同一个上下文中，我们说它们出现在同一个上下文中。</p><p id="ec68" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><strong class="jl hj"><em class="hi">【x-Skip-n-grams】:</em></strong></p><p id="4e1b" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">滑动窗口将包括(x+n)个单词。这个窗口现在将作为上下文。在这个上下文中共同出现的术语被称为已经共同出现。</em></p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es kn"><img src="../Images/7a453d73479e6f5aba32ed83ba28b128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*ZPkvsXp2YGsxzBMs9-GzVw.png"/></div></div></figure><p id="4eb3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">最后，让我们谈谈我们开始做的事-</p><h1 id="de42" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">从一个人交的朋友，你就可以知道这个人说的话。</h1><h1 id="d66a" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated"><strong class="ak">词向量</strong></h1><p id="2134" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">假设你从《哈利·波特与魔法石》中得到以下一段话:</p><p id="7d2c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">“对不起，”当这个瘦小的老人绊了一下，差点摔倒时，他咕哝道。过了几秒钟，德思礼先生才意识到那个人穿着一件紫色斗篷。对于差点被撞倒在地，他似乎一点也不难过。”T41】</p><p id="dfda" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们假设我们的词汇表只包含下面列出的几个单词。删除停用词和标点符号，只保留词汇表中的词后，段落变成:</p><p id="e336" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">男子绊倒秒钟德思礼男子披风打翻被撞倒地</strong></p><blockquote class="kf kg kh"><p id="2553" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">使用 3-skip-2-gram 技术创建一个共现矩阵，并回答下列问题(选择一个相似性度量)。</p><p id="cae5" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">词汇应该是:</p><p id="4924" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">(人，绊倒，秒，德思礼，斗篷，打翻，撞倒，落地)</em></p><p id="81be" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">你得到的共现对将是(左右单词的位置无关紧要，它们也可以互换):</p><p id="b160" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">(男子，绊倒)(男子，秒)(男子，德思礼)(男子，男子)</p><p id="29c9" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">(绊倒，秒)(绊倒，德思礼)(绊倒，伙计)(绊倒，斗篷)</p><p id="0101" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">(秒，德思礼)(秒，人)(秒，斗篷)(秒，心烦)</p><p id="d542" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">(德思礼，伙计)(德思礼，斗篷)(德思礼，心烦意乱)(德思礼，敲门)</p><p id="2952" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">(人，斗篷)(人，心烦意乱)(人，敲)(人，地面)</p><p id="3bfa" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">(斗篷，不安)(斗篷，被撞)(斗篷，地面)</p><p id="514c" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">(心烦意乱，敲门)(心烦意乱，地面)</p><p id="635d" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">(敲，地面)</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ll"><img src="../Images/af076babcdb9e50bfcc2ff46c7ee3a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sf-qePLwhnbXrwtNlC6iMA.png"/></div></div></figure><p id="717a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">单词嵌入:</strong></p><p id="156c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">出现和共现矩阵是稀疏的(真的稀疏！)和高维。谈论高维——为什么不使用矩阵分解技术(如奇异值分解等)来降低维数呢？？</strong></p><blockquote class="kf kg kh"><p id="ad98" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">这正是单词嵌入的目的。单词嵌入是庞大的出现和共现矩阵的压缩、低维版本。</p><p id="4faa" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">每一行(即单词)都有一个更短的向量(大小为 100，而不是数万)，并且是密集的，即大多数条目都是非零的(并且您仍然可以保留完整大小的稀疏矩阵所包含的大部分信息)。</em></p></blockquote><p id="07cd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">可以使用以下两种广泛的方法来生成单词嵌入:</strong></p><blockquote class="kf kg kh"><p id="9e78" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><strong class="jl hj"> <em class="hi">基于频率的方法</em> </strong> <em class="hi">:减少术语-文档矩阵(也可以是 tf-idf、关联矩阵等。)使用维数减少技术，例如 SVD。</em></p><p id="6f42" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><strong class="jl hj"> <em class="hi">基于预测的方法</em> </strong> <em class="hi">:在这种方法中，输入是单个单词(或单词的组合)，输出是上下文单词的组合(或单个单词)。浅层神经网络学习嵌入，使得可以使用输入单词来预测输出单词。</em></p></blockquote><h1 id="ce09" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated"><strong class="ak">潜在语义分析</strong></h1><blockquote class="kf kg kh"><p id="5981" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">在 LSA，你把一个单词的嘈杂的高维向量投影到一个低维空间。低维空间是单词语义的更丰富的表示。</p><p id="71f9" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi"> LSA 广泛用于处理大量文档，用于各种目的，例如文档聚类和分类(在低维空间中)，比较文档之间的相似性(例如，推荐用户喜欢的相似书籍)，寻找术语之间的关系(例如同义词和多义词)等。</em></p><p id="919b" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi"/><strong class="jl hj"><em class="hi"/></strong><em class="hi">LSA 除了有很多优点外，也有一些缺点。一个是产生的维数是不可解释的(任何基于矩阵分解的技术如 PCA 的典型缺点)。此外，LSA 不能处理多义词等问题。例如，我们之前提到过，术语“Java”有三种含义，在低维空间中该术语的表示将代表该术语的某种“平均含义”，而不是三种不同的含义。</em></p></blockquote><h1 id="7ea6" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated"><strong class="ak"> Word2Vec </strong></h1><p id="80f8" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated"><strong class="jl hj">“语义规则和相似性是词向量最重要的部分，可以由 word2vec 模型捕获，而不是像词模型包那样由词汇/句法处理捕获”。</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lm"><img src="../Images/273eb736ea7091a2d4db13cebba3e616.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xXWW85DIwx8lfwWf"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated"><strong class="bd kq">未定义维度的词向量</strong></figcaption></figure><blockquote class="kf kg kh"><p id="668c" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><strong class="jl hj">使用余弦相似度来测量词向量之间的相似度。</strong></p><p id="c7bf" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><strong class="jl hj">最近邻居</strong></p><p id="c760" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">两个单词向量之间的欧几里德距离(或余弦相似度)提供了一种测量相应单词的语言或语义相似度的有效方法。有时，根据这一标准，最近的邻居揭示了普通人词汇之外的罕见但相关的单词。</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lr"><img src="../Images/bc112cb30d1af70e7e62e473a9d9456e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*XUB4zjhq6UtVUCV6_NzukQ.png"/></div></figure><p id="f8ac" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">跳跃图和 CBOW </strong></p><p id="395d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">skip-gram 将目标/给定单词作为输入，并预测上下文单词(在窗口中)，而 CBOW 将上下文术语作为输入，并预测目标/给定术语。</p><p id="050a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">简单来说，CBOW 模型通过基于上下文预测当前单词来学习嵌入。skip-gram 模型通过预测给定当前单词的周围单词来学习。</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ls"><img src="../Images/75439b7784f9b7f0c855a65d6342e265.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1QEnyAj1Izq96Fi2.png"/></div></div></figure><p id="e0d5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">对于不太频繁的单词，使用跳跃图训练的单词嵌入比使用 CBOW 训练的单词嵌入略“好”。所谓“更好”，我们只是指与不常用词相似的词也将是不常用词。</p><p id="e62e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">手套嵌入</strong></p><p id="d016" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们之前提到过，除了 Word2Vec 之外，不同的团队还开发了其他几个单词嵌入。其中最流行的是斯坦福研究小组开发的<a class="ae lt" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"> <em class="ki"> GloVe(单词的全局向量)</em> </a>。这些嵌入在大约 60 亿个独特的标记上进行训练，并作为预训练的词向量可用于文本应用。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lu"><img src="../Images/d090fd0ff5ea7d1366e99a2d6bd2a4be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K2Q5ukfTyV1JDVL9ianwrg.png"/></div></div></figure><p id="3dc9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">概率潜在语义分析(PLSA) </strong></p><p id="2c4d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">PLSA 是一种用于主题建模的概率技术。首先，我们固定任意数量的主题，这是一个超参数(例如，所有文档中有 10 个主题)。我们假设的基本模型是这样的——每个文档是一些主题的集合，每个主题是一些术语的集合。</p><p id="4e42" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">PLSA 算法</strong>的任务是算出题目集合 c. <strong class="jl hj"> PLSA </strong>通常被表示为图形模型，阴影节点表示观察到的随机变量(d，w)，无阴影节点表示未观察到的随机变量。建立优化例程的基本思想是找到最大化联合概率<strong class="jl hj"> P(d，w) </strong>的主题集合 c。</p><h1 id="8b0e" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">无监督学习——主题建模:我们为什么需要它？</h1><p id="e418" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">让我假设你是亚马逊的一名产品经理，想要了解客户在评论中谈论的最近发布的产品(比如亚马逊 Echo Dot)的功能。</p><p id="f8f0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们假设你能够确定 50%的人谈论硬件，30%谈论与音乐相关的功能，而 20%谈论产品的包装。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lv"><img src="../Images/a9ef8f3862274dd7ef9870c68f26a96b.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/0*wEXw4V2SmNoPTMXG.jpg"/></div></figure><p id="03a7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">另一个场景可能是，我们有一些研究论文的大型语料库，我们希望构建一个能够进行特定主题搜索的应用程序。</p><p id="e688" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">什么是主题——文章描述的主要观点</strong></p><p id="5c47" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">主题背后的高层次直觉:</strong>假设一名教师每天给一班学生讲授技术。</p><p id="e77c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我是他的一个学生，转录了他所有的<strong class="jl hj"> Gyan </strong>并得到了如下几个关键词</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lw"><img src="../Images/44e220357b6906aa0cce94a185fabd87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PktwsJUPZstnf4gLNHfvqg.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated"><strong class="bd kq">猜猜题目讲的是什么！！</strong></figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lx"><img src="../Images/d4f0e43b440a572ee96c4937abc117dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*448N2ZeZrEcFyZx3DjmxXA.png"/></div></div></figure><blockquote class="kf kg kh"><p id="ef26" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><strong class="jl hj">主题分布::(o.5，o.5，0)::‘这是主题建模的核心思想’。</strong></p><p id="63f1" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">主题模型的输入是文档集，例如一组客户评论、推文、研究论文、书籍等。</em></p><p id="2498" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">主题模型有两个输出——1。文档中主题的分布和 2。主题中的单词分布。</p></blockquote><p id="6190" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">同样，在主题建模中有两个<strong class="jl hj">任务</strong></p><ol class=""><li id="a6de" class="ly lz hi jl b jm jn jp jq js ma jw mb ka mc ke md me mf mg bi translated">定义一个主题——很简单，每个术语就是一个词汇表中的主题分布</li><li id="a834" class="ly lz hi jl b jm mh jp mi js mj jw mk ka ml ke md me mf mg bi translated">估计覆盖率</li></ol><p id="9dee" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">假设我们有两个主题魔术和科学—</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mm"><img src="../Images/7d103f807b8af3c8650aa9714fbcef02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YKYuA141qu6-E5pze1L4Rw.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated"><strong class="bd kq">词汇的分布</strong></figcaption></figure><p id="fc8d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">概率潜在语义分析(PLSA) </strong></p><p id="f67e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">PLSA 可以被表示为具有随机变量文档 d、主题 c 和单词 w 的图形模型</p><p id="dc7f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">用于推断参数的基本思想，即优化目标，是最大化观察文档和单词的联合概率 p(w，d )(因为这两个是仅有的观察变量)。注意，你正在做一件非常聪明(也很困难)的事情——使用观察到的随机变量(d，w)来推断未观察到的随机变量 c。</p><p id="53d3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">使用贝叶斯法则，你可以把 p(w，d)写成:</p><p id="1e0d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> p(w，d) = p(d) x p(w|d) </strong></p><p id="1362" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">“推理</strong> <strong class="jl hj">任务</strong>是计算 M×k 个文档-主题概率和 k×N 个主题-术语概率。”</p><p id="e547" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">假设有 M 个文档(由下图中的外板表示)，为了简单起见，假设每个文档(内板)中有 N 个单词。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mn"><img src="../Images/e93a5c88282d39e739ec639b1a7924a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/0*3KvGvqq9xYsghDg8.png"/></div></figure><blockquote class="kf kg kh"><p id="6d9c" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">术语 p(w|d)代表单词 w 从文档 d 中生成的概率，但是我们的模型假设单词从主题中生成，而主题又从文档中生成，所以我们可以将 p(w|d)写成 p(w|c)。p(c|d)对所有 k 个主题求和:</em></p><p id="646d" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi"> P(w|d) = ∑p(c|d) x p(w|c) </em></p><p id="9853" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">所以，我们有</em> <strong class="jl hj"> <em class="hi"> P(w，d)= P(d)x∈【P(c | d)x P(w | c)】</em></strong></p></blockquote><p id="ff28" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">PLSA 的局限性</strong>:“PLSA”的主要问题是它有大量参数，这些参数随文档线性增长。"</p><h1 id="b031" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated"><strong class="ak">潜在狄利克雷分配(LDA)——</strong></h1><blockquote class="kf kg kh"><p id="3865" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">在 LDA 中，我们假设</em> <strong class="jl hj"> <em class="hi">文档-主题</em> </strong> <em class="hi">和</em> <strong class="jl hj"> <em class="hi">主题-术语</em> </strong> <em class="hi">分布是</em> <strong class="jl hj"> <em class="hi">狄利克雷分布</em> </strong> <em class="hi">(由一些变量参数化)，我们要推断这两个分布。</em></p><p id="9222" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">下面我们有 LDA 板图- </em></p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mo"><img src="../Images/5689f7ee0efde7cca756d8a64e166460.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/0*ZiAIEUDlriuyM-nr.png"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated"><strong class="bd kq"> LDA 板</strong></figcaption></figure><blockquote class="kf kg kh"><p id="0220" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">“与 PLSA 不同，LDA 是一个参数模型，也就是说，我们不必知道所有的个体概率。相反，我们假设概率来自一个基本的概率分布(‘狄利克雷’分布),我们可以使用一些参数对其进行建模。”</em></p><p id="d326" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">例如，在正态分布中，我们有两个参数，即均值和标准差。在 LDA 中，</em> <strong class="jl hj"> <em class="hi">我们假设文档-主题和主题-术语分布是狄利克雷分布(由一些变量参数化)</em> </strong> <em class="hi">，我们想要推断这两个分布。</em></p></blockquote><p id="148b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如上图所示；</p><p id="2ae4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> Alpha </strong> —“决定文档主题分布的狄利克雷分布参数。”<br/> <strong class="jl hj"> Eta </strong> —“决定主题词分布的参数。</p><h1 id="1ad3" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated"><strong class="ak"> <em class="mp">狄利克雷分布</em> : </strong></h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mq"><img src="../Images/06ea9c18e8ecf3093a1fac89d14c6681.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kDik7PVCsuMT6z4ygwHqug.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated"><strong class="bd kq">对称狄利克雷分布</strong></figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mr"><img src="../Images/f7e5bf60bf95c9705253c142afd67471.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*03Z-maCva5dKrzRKkeiD2Q.png"/></div></div></figure><blockquote class="kf kg kh"><p id="df1c" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">在</em><strong class="jl hj"><em class="hi">alpha&lt;1</em></strong><em class="hi">的值下(图-4)，除了少数位于中心的点，大部分点向边缘分散(a </em> <strong class="jl hj"> <em class="hi">稀疏分布</em></strong><em class="hi">——大部分主题概率低，少数占主导地位)。</em></p><p id="8402" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">在</em> <strong class="jl hj"> <em class="hi"> alpha=1 </em> </strong> <em class="hi">(图-1)点是</em> <strong class="jl hj"> <em class="hi">均匀分布在整个单纯形上的</em> </strong> <em class="hi">。</em></p><p id="cdf2" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">在</em><strong class="jl hj"><em class="hi">alpha&gt;1</em></strong><em class="hi">(图 2，右上)点围绕中心分布(即所有主题具有可比较的概率，例如(t1=0.32，t2=0.33，t3=0.35))。</em></p><p id="6f0a" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">左下图显示了一个</em> <strong class="jl hj"> <em class="hi">非对称分布</em></strong><em class="hi">(LDA 中未使用)，大部分点靠近 topic-2。</em></p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ms"><img src="../Images/8a79a3f860b4d58fd1298ae9fe923f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*RMwBU_BPVx5a-P_D.png"/></div></figure><p id="0c50" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">关键推论</strong>:</p><blockquote class="kf kg kh"><p id="e8a5" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><strong class="jl hj"> <em class="hi">稀疏主题—非常低的 alpha 表示文档中只有一个主题或者给定主题中只有一个术语—。<br/> —如果 alpha 很高，你甚至没有在听数据，但是如果它适中，你就混合了给定的数据和估计值。</em> </strong></p></blockquote><h1 id="d5c8" class="ko kp hi bd kq kr ks kt ku kv kw kx ky io kz ip la ir lb is lc iu ld iv le lf bi translated">工作进展</h1><p id="046b" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">作为一个用例，我正在使用 Twitter Api 实时抓取推文，并应用所有的 NLP 技术来创建 LDA 模型、word 到 vec，以及提取叙述并对它们进行聚类。</p><p id="6334" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">请继续关注此工作区以获取更多更新…</p></div></div>    
</body>
</html>