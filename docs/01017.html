<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-boost-decomposition-pytorch-optimization-sklearn-decision-tree-regressor-41a3d0cb9bb7?source=collection_archive---------6-----------------------#2019-09-25">https://medium.com/analytics-vidhya/gradient-boost-decomposition-pytorch-optimization-sklearn-decision-tree-regressor-41a3d0cb9bb7?source=collection_archive---------6-----------------------#2019-09-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="b7ee" class="hg hh hi bd hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie bi translated"><strong class="ak">梯度推进实施= pytorch优化+ sklearn </strong>决策树回归器</h2><p id="e25c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip hs iq ir is hw it iu iv ia iw ix iy iz hb bi translated">为了理解梯度推进算法，已经努力使用<strong class="ih ja"> pytorch </strong>执行必要的优化(最小化损失函数)并计算损失函数和来自sklearn 的<strong class="ih ja">决策树回归器的残差(相对于预测值的偏导数)以创建回归决策树。</strong></p><figure class="jc jd je jf fd jg er es paragraph-image"><div role="button" tabindex="0" class="jh ji di jj bf jk"><div class="er es jb"><img src="../Images/397d6cde0f2e579fd80eb7a2d3368f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*omQKD1B6QAHxe23mswi-WA.jpeg"/></div></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">一排树</figcaption></figure></div><div class="ab cl jr js gp jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hb hc hd he hf"><p id="7f53" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip hs ka ir is hw kb iu iv ia kc ix iy iz hb bi translated">中故事的更新版本可以在以下kaggle笔记本中找到:<br/><a class="ae kd" href="https://www.kaggle.com/code/igtzolas/inventing-gradient-boosting-regression" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/code/igtzolas/inventing-gradient-boosting-regression</a><br/><a class="ae kd" href="https://www.kaggle.com/code/igtzolas/inventing-gradient-boosting-classification" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/code/igtzolas/inventing-gradient-boosting-class ification</a></p><h1 id="03fa" class="ke hh hi bd hj kf kg kh hn ki kj kk hr kl km kn hv ko kp kq hz kr ks kt id ku bi translated">算法概要</h1><p id="e75f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip hs iq ir is hw it iu iv ia iw ix iy iz hb bi translated">算法的轮廓可以在下图中看到:</p><figure class="jc jd je jf fd jg er es paragraph-image"><div role="button" tabindex="0" class="jh ji di jj bf jk"><div class="er es kv"><img src="../Images/8f69d2d0cc9a98f2a7914614211eceab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xQXPUxSnBd1lOxjLoePoVQ.png"/></div></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">算法的步骤</figcaption></figure><p id="0eb4" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip hs ka ir is hw kb iu iv ia kc ix iy iz hb bi translated">我们将试着一步一步地实现它，同时也试着理解为什么算法所采取的步骤是有意义的。</p></div><div class="ab cl jr js gp jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hb hc hd he hf"><h1 id="437b" class="ke hh hi bd hj kf kw kh hn ki kx kk hr kl ky kn hv ko kz kq hz kr la kt id ku bi translated"><strong class="ak"> <em class="lb">解释+代码片段</em> </strong></h1><h2 id="7288" class="hg hh hi bd hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie bi translated">损失函数最小化</h2><p id="29b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip hs iq ir is hw it iu iv ia iw ix iy iz hb bi translated">重要的是，算法试图最小化损失函数，无论是回归情况下的平方距离还是分类情况下的二元交叉熵。</p><p id="b12b" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip hs ka ir is hw kb iu iv ia kc ix iy iz hb bi translated">损失函数参数是我们对训练样本所做的预测:<strong class="ih ja">L(prediction _ for _ train _ point _ 1，prediction_for_train_pont_2，…。，预测_对于_训练_点_m)。</strong></p><p id="f751" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip hs ka ir is hw kb iu iv ia kc ix iy iz hb bi translated">pytorch用于最小化损失函数，如下所示:</p><figure class="jc jd je jf fd jg"><div class="bz dy l di"><div class="lc ld l"/></div></figure></div><div class="ab cl jr js gp jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hb hc hd he hf"><h2 id="2556" class="hg hh hi bd hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie bi translated">计算残差</h2><p id="9de4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip hs iq ir is hw it iu iv ia iw ix iy iz hb bi translated">什么是所谓的残差？</p><p id="bbcd" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip hs ka ir is hw kb iu iv ia kc ix iy iz hb bi translated">它们是损失函数相对于数据点示例的当前预测的一阶偏导数。</p><p id="965f" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip hs ka ir is hw kb iu iv ia kc ix iy iz hb bi translated">简单地说，残差是向量(<strong class="ih ja">方向+范数</strong>)，表示到目前为止预测值应该在哪里变化，以使损失函数最小化。在我们的例子中，方向由残差的符号提供(因为它是一维标量)。<strong class="ih ja">有意义的是，具有相同损失函数最小化趋势的数据点以完全相同的方式被分组和处理(进入与树叶相同的处理桶)</strong>！这是算法最重要的概念直觉。为什么<strong class="ih ja">决策树是在相对于损失函数/目标的当前预测的部分梯度上构建的。</strong></p><p id="3404" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip hs ka ir is hw kb iu iv ia kc ix iy iz hb bi translated">计算导数/残差的代码如下:</p><figure class="jc jd je jf fd jg"><div class="bz dy l di"><div class="lc ld l"/></div></figure></div><div class="ab cl jr js gp jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hb hc hd he hf"><h2 id="874d" class="hg hh hi bd hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie bi translated"><strong class="ak">构建决策回归树</strong></h2><p id="87a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip hs iq ir is hw it iu iv ia iw ix iy iz hb bi translated">决策树回归器用于创建回归树。保留决策路径是因为将为落入每个决策路径/叶桶内的数据计算单个预测。</p><figure class="jc jd je jf fd jg"><div class="bz dy l di"><div class="lc ld l"/></div></figure></div><div class="ab cl jr js gp jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hb hc hd he hf"><h2 id="edc4" class="hg hh hi bd hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie bi translated"><strong class="ak">将所有的片段放在一起，创建一个sklearn like类</strong></h2><p id="228f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip hs iq ir is hw it iu iv ia iw ix iy iz hb bi translated">下面利用已经定义的代码提供算法实现。在每一步，残差被馈送到回归树，然后基于使同一桶(叶)的数据上的损失函数最小化的值来更新数据点的预测。</p><figure class="jc jd je jf fd jg"><div class="bz dy l di"><div class="lc ld l"/></div></figure><p id="698c" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip hs ka ir is hw kb iu iv ia kc ix iy iz hb bi translated">完整的代码可以在下面的<a class="ae kd" href="https://github.com/igtzolas/ml_algorithms" rel="noopener ugc nofollow" target="_blank"> <strong class="ih ja"> <em class="le"> github链接</em> </strong> </a>中找到。</p></div><div class="ab cl jr js gp jt" role="separator"><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw jx"/><span class="ju bw bk jv jw"/></div><div class="hb hc hd he hf"><h1 id="72cd" class="ke hh hi bd hj kf kw kh hn ki kx kk hr kl ky kn hv ko kz kq hz kr la kt id ku bi translated"><strong class="ak">在示例中使用定制实现，并与传统算法结果进行比较</strong></h1><p id="a5a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip hs iq ir is hw it iu iv ia iw ix iy iz hb bi translated">以下示例使用自定义和普通sklearn实现来执行分类任务并比较结果:</p><figure class="jc jd je jf fd jg"><div class="bz dy l di"><div class="lc ld l"/></div></figure><p id="3ca8" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip hs ka ir is hw kb iu iv ia kc ix iy iz hb bi translated">该程序的输出是:</p><p id="bf63" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip hs ka ir is hw kb iu iv ia kc ix iy iz hb bi translated">定制实现:训练数据的准确度分数:0.7661290322580645 <br/>定制实现:测试数据的准确度分数:0.625</p><p id="fd05" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip hs ka ir is hw kb iu iv ia kc ix iy iz hb bi translated">普通实现:训练数据的准确度分数:0.7580645161290323 <br/>普通实现:训练数据的准确度分数:0.625</p><p id="55cb" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip hs ka ir is hw kb iu iv ia kc ix iy iz hb bi translated">非常感谢你花时间通读这个帖子！</p></div></div>    
</body>
</html>