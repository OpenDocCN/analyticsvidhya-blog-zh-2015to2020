<html>
<head>
<title>2 ways to train a Linear Regression Model-Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">训练线性回归模型的两种方法-第2部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/2-ways-to-train-a-linear-regression-model-part-2-fdeb50fc58fa?source=collection_archive---------15-----------------------#2020-10-26">https://medium.com/analytics-vidhya/2-ways-to-train-a-linear-regression-model-part-2-fdeb50fc58fa?source=collection_archive---------15-----------------------#2020-10-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f1a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在之前的<a class="ae jd" href="https://jeffyjacob.medium.com/2-ways-to-train-a-linear-regression-model-part-1-e643dbef3df1" rel="noopener">帖子</a>中，我们讨论了如何使用<strong class="ih hj">正态方程直接计算最适合训练集<em class="je">的线性回归模型的模型参数(θ)。</em></strong>在本帖中，我们将讨论使用<strong class="ih hj">梯度下降的第二种训练方式。</strong></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jf"><img src="../Images/2038eb945d6123ed5a8060d0b7b9cbac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*81ijUZgwcCOO-_cLjVSRHg.png"/></div></div></figure><p id="6127" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算正常方程听起来很简单，它给我们的模型参数接近用于生成数据的原始函数的参数。<em class="je">那么，我们为什么需要第二种培训方法呢？</em></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jr"><img src="../Images/351aac5259f1b5d79f2666dfe35ffe29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xvlZmp1g_384D4vb8DYJyA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">法线方程计算θ值1.9和4.0，接近原始参数2和4</figcaption></figure><p id="a203" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">原因是，当数据集包含大量实例或大量特征时，<em class="je">法线方程</em>和<em class="je"> SVD </em>方法非常慢。如果数据集太大，甚至无法放入内存，该怎么办？你必须采用一种<em class="je">非核心学习</em>技术。幸运的是，<strong class="ih hj">梯度下降</strong>就是其中之一。</p><h1 id="fafb" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">梯度下降</strong></h1><p id="6e2e" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq kw is it iu kx iw ix iy ky ja jb jc hb bi kz translated"><span class="l la lb lc bm ld le lf lg lh di"> G </span>梯度下降法是一种优化技术(并不局限于线性回归)，能够找到一系列问题的最佳解决方案。</p><blockquote class="li"><p id="e188" class="lj lk hi bd ll lm ln lo lp lq lr jc dx translated">“梯度下降的一般思想是反复调整参数，以最小化成本函数”Aurelien Geron的机器实践学习</p></blockquote><p id="bd0b" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq lu is it iu lv iw ix iy lw ja jb jc hb bi translated"><strong class="ih hj">梯度下降的步骤</strong></p><ol class=""><li id="bc21" class="lx ly hi ih b ii ij im in iq lz iu ma iy mb jc mc md me mf bi translated">从给θ分配随机值开始——这一步称为<em class="je">随机初始化</em>步骤。</li><li id="c266" class="lx ly hi ih b ii mg im mh iq mi iu mj iy mk jc mc md me mf bi translated">接下来，反复调整这些值，直到算法收敛到最小成本函数。</li></ol><p id="40c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降有几种不同的变体。各有利弊。它们列举如下:</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ml"><img src="../Images/f7496e87fef0d75edc12cfd9be2688a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IS5Ipy-TdHepPbm2_N6esg.png"/></div></div></figure><p id="80ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们稍后将讨论更多细节。但是现在让我们来谈谈成本函数MSE(均方误差)以及它如何影响我们的搜索。根据成本函数，对最优解的搜索变得更有挑战性或不那么有挑战性。例如，MSE成本函数看起来像一个漂亮、规则的碗形。其他成本函数可能具有平台、脊和洞，使得梯度下降更难找到最优解。</p><p id="4572" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="je">2 GD面临的挑战</em>(我从现在开始这样称呼梯度下降)<em class="je">有:</em></p><ol class=""><li id="6f7a" class="lx ly hi ih b ii ij im in iq lz iu ma iy mb jc mc md me mf bi translated">收敛到局部最小值而不是全局最小值。</li><li id="50e9" class="lx ly hi ih b ii mg im mh iq mi iu mj iy mk jc mc md me mf bi translated">长时间被困在高原上</li></ol><p id="5034" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下图描述了这两种挑战:</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ml"><img src="../Images/080581e56990708eb3f1e6a4eca12676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BxYHC7v1JxtW1jaL_BOryQ.png"/></div></div></figure><p id="07f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">幸运的是，线性回归的成本函数MSE是碗形的，因此最优解位于碗的底部。它就是我们所说的一个<em class="je">凸</em>和<em class="je">连续</em>的函数。这对我们意味着什么？我们保证只有一个全局最优解！</p><p id="d3c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们重申我们在这里试图做的事情:我们试图找到最佳的模型参数，使训练集上的成本函数最小化。我们使用一种叫做<em class="je">梯度下降</em>的优化技术来迭代搜索模型参数空间。跟我到目前为止？好的，太好了！</p><h1 id="f96f" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">批量梯度下降</strong></h1><p id="d566" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq kw is it iu kx iw ix iy ky ja jb jc hb bi kz translated"><span class="l la lb lc bm ld le lf lg lh di"> B </span> <em class="je"> atch GD在每个GD步骤使用整批训练数据。</em>如果您有大量实例，这可能会非常慢，但是它可以很好地扩展大量的特性(比普通方程快得多)。</p><blockquote class="li"><p id="54b9" class="lj lk hi bd ll lm ln lo lp lq lr jc dx translated"><em class="mm">批量GD在每个GD步骤使用整批训练数据。</em></p></blockquote><p id="5c34" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq lu is it iu lv iw ix iy lw ja jb jc hb bi translated">我还告诉你一个梯度下降中很重要的参数叫做<em class="je">“学习率”。</em>选择一个好的学习率很重要，因为小的学习率会导致算法收敛得太晚，而大的学习率可能会错过全局最优。您可以将学习率视为超参数，并使用<em class="je">网格搜索来确定它。</em></p><p id="b73b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要实现梯度下降，您需要计算成本函数相对于每个模型参数的偏导数。MSE相对于θ0和θ1偏导数如下所示:</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ml"><img src="../Images/841a5c1493c91d38af6e8b62d836a668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3hMDLql-tYvp7U4tT-lGbQ.png"/></div></div></figure><p id="d50a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些被计算并存储在称为<em class="je">梯度矢量</em>的矢量中。梯度向量如下所示:</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ml"><img src="../Images/5f2d8f215b4d6b75074369a15240d047.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hI68hCy01mpowtoDGJ-Weg.png"/></div></div></figure><p id="72b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，梯度矢量指向上坡。为了下山(碗的尽头)，我们需要从学习率中减去它。看起来我们有所有的方程式来实现它。所以我们来试试吧！</p><pre class="jg jh ji jj fd mn mo mp mq aw mr bi"><span id="7125" class="ms jx hi mo b fi mt mu l mv mw">eta = 0.1<br/>n_iterations = 1000<br/>m = 100 # number of instances</span><span id="666e" class="ms jx hi mo b fi mx mu l mv mw">theta = np.random.randn(2,1)<br/>for itr in range(n_iterations):<br/>    gradient_vector = 2/m *X_b.T.dot(X_b.dot(theta) - y)<br/>    theta = theta - eta *gradient_vector<br/>print(theta)</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es my"><img src="../Images/66f13b3183492801b83a39d6d04f189f.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*AuJkF2tcrf6dRxCnd-_Bgg.png"/></div></figure><p id="1bb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们得到了与<strong class="ih hj">正规方程</strong>完全相同的结果。在线性回归中使用批量GD的一个优点是，我们可以保证在正确的学习速率下，算法最终会收敛到全局最优。</p><p id="e95e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">使用梯度下降的缺点</strong></p><ol class=""><li id="ffa1" class="lx ly hi ih b ii ij im in iq lz iu ma iy mb jc mc md me mf bi translated">我们知道批处理GD在每一步都需要整个数据集。因此，如果你有一个很大的训练集，它会变得非常慢。</li><li id="1c09" class="lx ly hi ih b ii mg im mh iq mi iu mj iy mk jc mc md me mf bi translated">其次，批量GD通常会陷入局部最小值，因为成本函数下降非常缓慢，并且与其他变量(如随机GD)相比<em class="je">非常规则</em>，随机GD可以跳出局部最小值，因为如果其<em class="je">不规则</em>。当然，这对我们来说不是问题，因为我们的成本函数没有局部最小值。</li></ol><h1 id="34f1" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">随机梯度下降</strong></h1><p id="10c9" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq kw is it iu kx iw ix iy ky ja jb jc hb bi kz translated">术语“随机”的意思是<em class="je">随机决定。</em>随机GD正是这么做的。它在每一步的训练集中随机选取一个实例。渐变是基于该单个实例计算的。因此，使用这种方法在巨大的数据集上进行训练是可能的。</p><blockquote class="li"><p id="486c" class="lj lk hi bd ll lm ln lo lp lq lr jc dx translated">它在每一步的训练集中随机选取一个实例。</p></blockquote><p id="bbca" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq lu is it iu lv iw ix iy lw ja jb jc hb bi translated">随机GD是<em class="je">非常不规则，</em>意思是到处都是！成本函数不断上下跳动。最终它会接近最小值，但即使这样，它也会继续反弹，而不是稳定下来。因此，我们不能保证模型参数值是最佳的。但是他们可能非常接近。</p><p id="5cc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但由于其不规则性，不太容易陷入局部最优。它会从那里弹开的！让我们看看随机GD的sklearn实现:</p><blockquote class="li"><p id="98d1" class="lj lk hi bd ll lm ln lo lp lq lr jc dx translated">随机性是好事也是坏事！</p></blockquote><pre class="mz na nb nc nd mn mo mp mq aw mr bi"><span id="f675" class="ms jx hi mo b fi mt mu l mv mw">#sklearn implementation of Stochastic GD</span><span id="0f88" class="ms jx hi mo b fi mx mu l mv mw">from sklearn.linear_model import SGDRegressor<br/>sgd_reg = SGDRegressor(max_iter = 500, tol = 1e-3, penalty = None, eta0 = 0.1)<br/>sgd_reg.fit(X, y.ravel())<br/>print(sgd_reg.intercept_)<br/>print(sgd_reg.coef_)</span></pre><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es my"><img src="../Images/00c462c82ed5748f99575db652222235.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*A8F2HkTYbfzQ_iGCIdtIxg.png"/></div></figure><h1 id="9ff5" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">小批量梯度下降</strong></h1><p id="923b" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq kw is it iu kx iw ix iy ky ja jb jc hb bi translated">如果你理解了批量和随机GD，那么迷你批量就没那么难了。它会随机挑选一组称为“小批量”的实例。小批量的优势在于它比随机批量更有规律，这意味着它更接近全局最优。</p><blockquote class="li"><p id="ef1c" class="lj lk hi bd ll lm ln lo lp lq lr jc dx translated">它会随机挑选一组称为“小批量”的实例。</p></blockquote><p id="dd93" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq lu is it iu lv iw ix iy lw ja jb jc hb bi translated">这些类型之间唯一的主要区别是它们遍历参数空间的方式。批次GD可以被认为是一条朝向最小值的平滑线，它在那里停止。随机和小批量GD在遍历的方式上比较曲折，继续走来走去。</p><p id="1325" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以使用SGDRegressor的<em class="je"> partial_fit </em>方法实现小批量GD。有关实现的更多详细信息，请查看此<a class="ae jd" href="https://adventuresindatascience.wordpress.com/2014/12/30/minibatch-learning-for-large-scale-data-using-scikit-learn/" rel="noopener ugc nofollow" target="_blank">文章</a>。</p><h1 id="3e75" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">结论</strong></h1><p id="b2ea" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq kw is it iu kx iw ix iy ky ja jb jc hb bi translated">在这篇博客中，我们了解到:</p><ol class=""><li id="152a" class="lx ly hi ih b ii ij im in iq lz iu ma iy mb jc mc md me mf bi translated">训练线性回归模型的第二种方法(当数据集很大时)-使用梯度下降。第一种方法，参考我之前的<a class="ae jd" href="https://jeffyjacob.medium.com/2-ways-to-train-a-linear-regression-model-part-1-e643dbef3df1" rel="noopener">博客</a>。</li><li id="cbae" class="lx ly hi ih b ii mg im mh iq mi iu mj iy mk jc mc md me mf bi translated">MSE代价函数的性质——<em class="je">凸</em>和<em class="je">连续</em>函数</li><li id="0237" class="lx ly hi ih b ii mg im mh iq mi iu mj iy mk jc mc md me mf bi translated">梯度下降的不同挑战。</li><li id="9fb5" class="lx ly hi ih b ii mg im mh iq mi iu mj iy mk jc mc md me mf bi translated">批量GD的简单实现</li><li id="6b2c" class="lx ly hi ih b ii mg im mh iq mi iu mj iy mk jc mc md me mf bi translated">随机GD的Scikit-Learn实现</li><li id="7a2f" class="lx ly hi ih b ii mg im mh iq mi iu mj iy mk jc mc md me mf bi translated">批量、随机、小批量梯度下降——其优点和缺点。</li></ol><p id="301b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你喜欢这个博客，请留下掌声或评论。它让我知道我的博客是有帮助的:)祝大家学习愉快！</p><h1 id="270b" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">参考文献</strong></h1><p id="4108" class="pw-post-body-paragraph if ig hi ih b ii ku ik il im kv io ip iq kw is it iu kx iw ix iy ky ja jb jc hb bi translated">1.用Scikit-Learn和TensorFlow进行机器学习:构建智能系统的概念、工具和技术。奥雷连·杰龙的书。</p></div></div>    
</body>
</html>