<html>
<head>
<title>A Comprehensive Guide to Ensemble Learning (with Python codes)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成学习综合指南(带Python代码)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-comprehensive-guide-to-ensemble-learning-with-python-codes-5261650bb531?source=collection_archive---------2-----------------------#2018-06-18">https://medium.com/analytics-vidhya/a-comprehensive-guide-to-ensemble-learning-with-python-codes-5261650bb531?source=collection_archive---------2-----------------------#2018-06-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="996c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当你想购买一辆新车时，你会根据经销商的建议去第一家汽车商店购买吗？可能性极小。</p><p id="6be7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可能会浏览一些门户网站，人们在那里发布评论，比较不同的汽车型号，检查它们的功能和价格。你也可能会询问你的朋友和同事的意见。简而言之，你不会直接得出结论，而是会考虑其他人的意见来做决定。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/f017154d4cf456b244e666f411aee479.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*Dw5mNcoOtZm0uPnV.jpg"/></div></figure><p id="d2ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">机器学习中的集成模型基于类似的想法运行。它们将来自多个模型的决策结合起来，以提高整体性能。这可以通过多种方式实现，您将在本文中发现这些方式。</p><p id="a752" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文的目的是介绍集成学习的概念，并理解使用这种技术的算法。为了巩固您对这个多样化主题的理解，我们将使用一个实际问题的动手案例研究来解释Python中的高级算法。</p><p id="485b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jl">注意:本文假设对机器学习算法有基本的了解。我会推荐大家通过</em> <a class="ae jm" href="https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/" rel="noopener ugc nofollow" target="_blank"> <em class="jl">这篇文章</em> </a> <em class="jl">来熟悉这些概念。</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jn"><img src="../Images/0e9c99543ab00ffde606c038cb0bbd41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*7kCWW2xJDAbt2oz_hwq8yg.png"/></div></figure><h1 id="9f5b" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">目录</h1><ol class=""><li id="3768" class="km kn hi ih b ii ko im kp iq kq iu kr iy ks jc kt ku kv kw bi translated">集成学习简介</li><li id="8728" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">基本集成技术<br/> 2.1最大投票<br/> 2.2平均<br/> 2.3加权平均</li><li id="5d63" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">先进的综合技术<br/> 3.1堆垛<br/> 3.2混合<br/> 3.3装袋<br/> 3.4增压</li><li id="c78c" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">基于Bagging和Boosting的算法<br/> 4.1 Bagging元估计器<br/> 4.2随机森林<br/>4.3 AdaBoost<br/>4.4 GBM<br/>4.5 XGB<br/>4.6 Light GBM<br/>4.7 CatBoost</li></ol><h1 id="e687" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">1.集成学习简介</h1><p id="3c88" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">我们用一个例子来理解集成学习的概念。假设你是一名电影导演，你创作了一部关于一个非常重要和有趣的主题的短片。现在，您希望在电影公开之前获得初步反馈(评级)。你能做那件事的可能方法是什么？</p><p id="baea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jl">答:</em> </strong> <em class="jl">你可以让你的一个朋友帮你给这部电影打分。</em> <br/>现在完全有可能你选择的那个人很爱你，不想让你心碎，给你创作的恐怖作品提供1星评价。</p><p id="da94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jl"> B: </em> </strong> <em class="jl">另一种方式可以是让你的5个同事给电影评级。这应该能让你对这部电影有更好的了解。这种方法可以为您的电影提供真实的评级。但是一个问题仍然存在。这5个人可能不是你电影主题的“主题专家”。当然，他们可能理解电影摄影、镜头或音频，但同时可能不是黑色幽默的最佳评判者。</em></p><p id="6ba9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jl"> C: </em> </strong> <em class="jl">让50个人给电影打分怎么样？有些人可能是你的朋友，有些人可能是你的同事，有些人甚至可能是完全陌生的人。</em></p><p id="107e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这种情况下，回答会更加一般化和多样化，因为现在你有了拥有不同技能的人。事实证明，这是一种比我们之前看到的案例更好的获得诚实评级的方法。</p><p id="a7fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过这些例子，你可以推断出，与个人相比，一个多样化的群体可能会做出更好的决定。与单一模型相比，多样化的模型也是如此。机器学习中的这种多样化是通过一种称为集成学习的技术实现的。</p><p id="3e9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在你已经知道了什么是集成学习的要点——让我们看看集成学习中的各种技术及其实现。</p><h1 id="e347" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">2.简单的集成技术</h1><p id="71bb" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">在本节中，我们将了解一些简单但功能强大的技术，即:</p><ol class=""><li id="9325" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">最大投票</li><li id="2c3e" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">求平均值</li><li id="443f" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">加权平均</li></ol><h1 id="45fe" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">2.1最大投票</h1><p id="3cc5" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">最大投票法一般用于分类问题。在这种技术中，使用多个模型来预测每个数据点。每个模型的预测被认为是一次“投票”。我们从大多数模型中得到的预测被用作最终预测。</p><p id="acc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，当你让你的5位同事给你的电影打分时(满分5分)；我们假设其中三人给它打了4分，而两人给了5分。由于大多数人给了4分，所以最终的评分将为4分。<strong class="ih hj">你可以认为这是采用了所有预测的模式。</strong></p><p id="446a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最大值投票的结果将是这样的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es li"><img src="../Images/d2a3c5152c856c8e4c11e2fdac2d8732.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*uOA1tNcKPotBsCKujNANcw.png"/></div></figure><p id="4cde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">样本代码:</strong></p><p id="0dde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里x_train由训练数据中的独立变量组成，y_train是训练数据的目标变量。验证集是x_test(自变量)和y_test(目标变量)。</p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="d2ce" class="lo jp hi lk b fi lp lq l lr ls">model1 = tree.DecisionTreeClassifier() <br/>model2 = KNeighborsClassifier() <br/>model3= LogisticRegression() </span><span id="c124" class="lo jp hi lk b fi lt lq l lr ls">model1.fit(x_train,y_train) <br/>model2.fit(x_train,y_train) <br/>model3.fit(x_train,y_train) </span><span id="f459" class="lo jp hi lk b fi lt lq l lr ls">pred1=model1.predict(x_test) <br/>pred2=model2.predict(x_test) <br/>pred3=model3.predict(x_test) <br/>final_pred = np.array([]) </span><span id="0052" class="lo jp hi lk b fi lt lq l lr ls">for i in range(0,len(x_test)):     <br/>      final_pred = np.append(final_pred, mode([pred1[i],pred2[i], pred3[i]]))</span></pre><p id="8fe8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">或者，您可以使用<em class="jl"> sklearn </em>中的“VotingClassifier”模块，如下所示:</p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="f235" class="lo jp hi lk b fi lp lq l lr ls">from sklearn.ensemble import VotingClassifier </span><span id="9ab7" class="lo jp hi lk b fi lt lq l lr ls">model1 = LogisticRegression(random_state=1) <br/>model2 = tree.DecisionTreeClassifier(random_state=1) <br/>model = VotingClassifier(estimators=[('lr', model1), ('dt', model2)], voting='hard') </span><span id="ed5e" class="lo jp hi lk b fi lt lq l lr ls">model.fit(x_train,y_train) model.score(x_test,y_test)</span></pre><h1 id="420d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">2.2平均</h1><p id="c652" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">类似于最大投票技术，在平均中对每个数据点进行多次预测。在这种方法中，我们取所有模型预测的平均值，并用它来进行最终预测。平均可用于回归问题中的预测或计算分类问题的概率。</p><p id="bea1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">For example, in the below case, the averaging method would take the average of all the values.</p><p id="95e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">i.e. (5+4+5+4+4)/5 = 4.4</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/18d2cfe15ef4610dac14219e70f351d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*eji0dGBINAWLJQepIQ7MMQ.png"/></div></figure><p id="24b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">Sample Code:</strong></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="d437" class="lo jp hi lk b fi lp lq l lr ls">model1 = tree.DecisionTreeClassifier()<br/>model2 = KNeighborsClassifier() <br/>model3= LogisticRegression() </span><span id="7d67" class="lo jp hi lk b fi lt lq l lr ls">model1.fit(x_train,y_train) <br/>model2.fit(x_train,y_train) <br/>model3.fit(x_train,y_train) </span><span id="2d1b" class="lo jp hi lk b fi lt lq l lr ls">pred1=model1.predict_proba(x_test) pred2=model2.predict_proba(x_test) pred3=model3.predict_proba(x_test) </span><span id="b7f8" class="lo jp hi lk b fi lt lq l lr ls">finalpred=(pred1+pred2+pred3)/3</span></pre><h1 id="dcb4" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">2.3 Weighted Average</h1><p id="e896" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">This is an extension of the averaging method. All models are assigned different weights defining the importance of each model for prediction. For instance, if two of your colleagues are critics, while others have no prior experience in this field, then the answers by these two friends are given more importance as compared to the other people.</p><p id="1111" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">The result is calculated as [(5*0.23) + (4*0.23) + (5*0.18) + (4*0.18) + (4*0.18)] = 4.41.</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es lv"><img src="../Images/4f4271ca1727067f0a27a900869c71c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1fEmHTYAFcuVFKI3MFOCAg.png"/></div></div></figure><p id="956c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">Sample Code:</strong></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="af2c" class="lo jp hi lk b fi lp lq l lr ls">model1 = tree.DecisionTreeClassifier() <br/>model2 = KNeighborsClassifier() <br/>model3= LogisticRegression() </span><span id="8023" class="lo jp hi lk b fi lt lq l lr ls">model1.fit(x_train,y_train) <br/>model2.fit(x_train,y_train) <br/>model3.fit(x_train,y_train) </span><span id="51e6" class="lo jp hi lk b fi lt lq l lr ls">pred1=model1.predict_proba(x_test) pred2=model2.predict_proba(x_test) pred3=model3.predict_proba(x_test) </span><span id="0c1d" class="lo jp hi lk b fi lt lq l lr ls">finalpred=(pred1*0.3+pred2*0.3+pred3*0.4)</span></pre><h1 id="759c" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">3. Advanced Ensemble techniques</h1><p id="8d60" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">Now that we have covered the basic ensemble techniques, let’s move on to understanding the advanced techniques.</p><h1 id="a65f" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">3.1 Stacking</h1><p id="deff" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">Stacking is an ensemble learning technique that uses predictions from multiple models (for example decision tree, knn or svm) to build a new model. This model is used for making predictions on the test set. Below is a step-wise explanation for a simple stacked ensemble:</p><ol class=""><li id="2322" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">The train set is split into 10 parts.</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es jd"><img src="../Images/3d458f01e859e9ee30f7fda24290bf83.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*sopF4MpRwjIg46OU.png"/></div></div></figure><p id="6d2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2. A base model (suppose a decision tree) is fitted on 9 parts and predictions are made for the 10th part. This is done for each part of the train set.</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/d69daafb0e82640b36da5e34891f1487.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*w16MhoMbVLjR5U2w.png"/></div></figure><p id="b6b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3. The base model (in this case, decision tree) is then fitted on the whole train dataset.</p><p id="7a3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4. Using this model, predictions are made on the test set.</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/c1e67438e9f8eb164b2ce7f29d00ee4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*fx911siDqo9K2pTp.png"/></div></figure><p id="45d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5. Steps 2 to 4 are repeated for another base model (say knn) resulting in another set of predictions for the train set and test set.</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/4cd0c33ca760f4da16d96201677cadfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*DkvPg1m_lonzyDp9.png"/></div></figure><p id="2325" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6. The predictions from the train set are used as features to build a new model.</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es ma"><img src="../Images/f34b485dd161b911bb0f6b03345b2fdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/0*o9x0BU30xM51wYZo.png"/></div></div></figure><p id="7946" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7. This model is used to make final predictions on the test prediction set.</p><p id="1f39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">Sample code:</strong></p><p id="b6a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">We first define a function to make predictions on n-folds of train and test dataset. This function returns the predictions for train and test for each model.</p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="350e" class="lo jp hi lk b fi lp lq l lr ls">def Stacking(model,train,y,test,n_fold):<br/>  folds=StratifiedKFold(n_splits=n_fold,random_state=1)<br/>  test_pred=np.empty((test.shape[0],1),float)<br/>  train_pred=np.empty((0,1),float)</span><span id="c839" class="lo jp hi lk b fi lt lq l lr ls">  for train_indices,val_indices in folds.split(train,y.values):<br/>   x_train,x_val=train.iloc[train_indices],train.iloc[val_indices]<br/>   y_train,y_val=y.iloc[train_indices],y.iloc[val_indices]<br/><br/>   model.fit(X=x_train,y=y_train)<br/>   train_pred=np.append(train_pred,model.predict(x_val))<br/>   test_pred=np.append(test_pred,model.predict(test))<br/>  return test_pred.reshape(-1,1),train_pred</span></pre><p id="0534" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Now we’ll create two base models — decision tree and knn.</p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="80ba" class="lo jp hi lk b fi lp lq l lr ls">model1 = tree.DecisionTreeClassifier(random_state=1)<br/><br/>test_pred1 ,train_pred1=Stacking(model=model1,n_fold=10, train=x_train,test=x_test,y=y_train)<br/><br/>train_pred1=pd.DataFrame(train_pred1)<br/>test_pred1=pd.DataFrame(test_pred1)</span><span id="eb4a" class="lo jp hi lk b fi lt lq l lr ls">model2 = KNeighborsClassifier() </span><span id="dc47" class="lo jp hi lk b fi lt lq l lr ls">test_pred2,train_pred2=Stacking(model=model2,n_fold=10,train=x_train,test=x_test,y=y_train) </span><span id="a2e9" class="lo jp hi lk b fi lt lq l lr ls">train_pred2=pd.DataFrame(train_pred2) test_pred2=pd.DataFrame(test_pred2)</span></pre><p id="fe87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Create a third model, logistic regression, on the predictions of the decision tree and knn models.</p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="b1fa" class="lo jp hi lk b fi lp lq l lr ls">df = pd.concat([train_pred1, train_pred2], axis=1)<br/>df_test = pd.concat([test_pred1, test_pred2], axis=1)<br/><br/>model = LogisticRegression(random_state=1)<br/>model.fit(df,y_train)<br/>model.score(df_test, y_test)</span></pre><p id="7115" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">In order to simplify the above explanation, the stacking model we have created has only two levels. The decision tree and knn models are built at level zero, while a logistic regression model is built at level one. Feel free to create multiple levels in a stacking model.</p><h1 id="e273" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">3.2 Blending</h1><p id="04d0" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">Blending follows the same approach as stacking but uses only a holdout (validation) set from the train set to make predictions. In other words, unlike stacking, the predictions are made on the holdout set only. The holdout set and the predictions are used to build a model which is run on the test set. Here is a detailed explanation of the blending process:</p><ol class=""><li id="df42" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">The train set is split into training and validation sets.</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/c6d2c5c80a609612c10300992d5f3b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*p88Qc04am4-ygtA-.png"/></div></figure><p id="34bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2. Model(s) are fitted on the training set.</p><p id="f97e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3. The predictions are made on the validation set and the test set.</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/052c961fd3bdc3d2c1d7d27eac05d546.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*rRfX3rMGqBxJkLqV.png"/></div></figure><p id="e20d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4. The validation set and its predictions are used as features to build a new model.</p><p id="49e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.该模型用于对测试和元特征进行最终预测。</p><p id="b94a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">样本代码:</strong></p><p id="ffec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将在训练集上建立两个模型，决策树和knn，以便在验证集上进行预测。</p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="fbb7" class="lo jp hi lk b fi lp lq l lr ls">model1 = tree.DecisionTreeClassifier()<br/>model1.fit(x_train, y_train)</span><span id="fe7a" class="lo jp hi lk b fi lt lq l lr ls">val_pred1=model1.predict(x_val)<br/>test_pred1=model1.predict(x_test)<br/>val_pred1=pd.DataFrame(val_pred1)<br/>test_pred1=pd.DataFrame(test_pred1)<br/><br/>model2 = KNeighborsClassifier()<br/>model2.fit(x_train,y_train)</span><span id="6be2" class="lo jp hi lk b fi lt lq l lr ls">val_pred2=model2.predict(x_val)<br/>test_pred2=model2.predict(x_test)<br/>val_pred2=pd.DataFrame(val_pred2)<br/>test_pred2=pd.DataFrame(test_pred2)</span></pre><p id="ac22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结合元特征和验证集，建立逻辑回归模型来对测试集进行预测。</p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="09f9" class="lo jp hi lk b fi lp lq l lr ls">df_val=pd.concat([x_val, val_pred1,val_pred2],axis=1)<br/>df_test=pd.concat([x_test, test_pred1,test_pred2],axis=1)<br/><br/>model = LogisticRegression()<br/>model.fit(df_val,y_val)<br/>model.score(df_test,y_test)</span></pre><h1 id="cc9d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">3.3包装</h1><p id="7f1c" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">bagging背后的思想是将多个模型(例如，所有决策树)的结果结合起来，得到一个一般化的结果。这里有一个问题:如果你把所有的模型都建立在同一套数据上，然后组合起来，会有用吗？这些模型很有可能给出相同的结果，因为它们得到了相同的输入。那么如何才能解决这个问题呢？其中一项技术是自举。</p><p id="1bfb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Bootstrapping是一种采样技术，在这种技术中，我们用替换的从原始数据集<strong class="ih hj">中创建观察值的子集。子集的大小与原始集的大小相同。</strong></p><p id="bfe9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Bagging(或Bootstrap Aggregating)技术使用这些子集(包)来获得分布的公平概念(完整集)。为打包创建的子集的大小可能小于原始集。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mb"><img src="../Images/3f0183962d814e8f1ecd45e988674c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Klwy9_nsbJ8c_1nJ.png"/></div></div></figure><ol class=""><li id="06ea" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">从原始数据集创建多个子集，选择带有替换的观测值。</li><li id="ba34" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">在这些子集的每一个上创建一个基础模型(弱模型)。</li><li id="29bf" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">这些模型并行运行，相互独立。</li><li id="778e" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">通过组合所有模型的预测来确定最终预测。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mc"><img src="../Images/158c41c6edf347474cbcfeba3b62f939.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xBoYn5ouALKj86Ra.png"/></div></div></figure><h1 id="408f" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">3.4增压</h1><p id="1375" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">在我们继续之前，这里有另一个问题:如果第一个模型错误地预测了一个数据点，然后是下一个(可能是所有模型)，组合预测会提供更好的结果吗？这种情况通过升压来解决。</p><p id="66ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">升压是一个连续的过程，其中每个后续模型都试图纠正前一个模型的错误。后续模型依赖于前一个模型。让我们在下面的步骤中了解升压的工作方式。</p><ol class=""><li id="ed50" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">从原始数据集创建一个子集。</li><li id="640c" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">最初，所有数据点被赋予相同的权重。</li><li id="b08e" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">在该子集上创建基础模型。</li><li id="b197" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">该模型用于对整个数据集进行预测。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es md"><img src="../Images/3deaa0d1d1b566b9c5cd269a85895c8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/0*GEkUBthvCBsz2kc0.png"/></div></figure><p id="278d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.使用实际值和预测值计算误差。</p><p id="ac12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.被错误预测的观察值被给予较高的权重。<br/>(此处，三个错误分类的蓝色加分将被赋予更高的权重)</p><p id="77c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7.创建另一个模型，并对数据集进行预测。(这个模型试图纠正前一个模型的错误)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es me"><img src="../Images/47ad84cd187748d70f7f24ea0a5e8147.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/0*b9l325fN-HAM3EC4.png"/></div></figure><p id="0311" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">8.类似地，创建多个模型，每个模型纠正前一个模型的错误。</p><p id="ff66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">9.最终模型(强学习者)是所有模型(弱学习者)的加权平均值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/e4734b297c4f730c20617ea2a99e9892.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*KSEItrR75VeBscJ8.png"/></div></figure><p id="c12f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，增强算法将多个弱学习器结合起来形成一个强学习器。单个模型在整个数据集上表现不佳，但在数据集的某些部分上表现良好。因此，每个模型实际上都提高了整体的性能。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mf"><img src="../Images/9bba8685aecb04cfb3c5527ca52c73ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/0*tOGrxEJkl0UVDOjZ.png"/></div></figure><h1 id="3e33" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">4.基于打包和提升的算法</h1><p id="4ed0" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">打包和提升是机器学习中最常用的两种技术。在这一节中，我们将详细讨论它们。以下是我们将重点关注的算法:</p><p id="e8bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">打包算法:</p><ul class=""><li id="1d3c" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated">Bagging元估计量</li><li id="ae76" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated">随机森林</li></ul><p id="ac5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">升压算法:</p><ul class=""><li id="17dc" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated">adaboost算法</li><li id="c178" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated">马恩岛</li><li id="84b0" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated">XGBM</li><li id="5ea9" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated">轻型GBM</li><li id="d81e" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated">CatBoost</li></ul><p id="6425" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于本节中讨论的所有算法，我们将遵循以下步骤:</p><ul class=""><li id="bdec" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated">算法简介</li><li id="ecae" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated">示例代码</li><li id="a811" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated">因素</li></ul><p id="77a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于本文，我使用了贷款预测问题。你可以从<a class="ae jm" href="https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">这里</strong> </a>下载数据集。请注意，一些代码行(读取数据，分成训练测试集，等等。)对于每个算法都是一样的。为了避免重复，我在下面写了相同的代码，并且只进一步讨论算法的代码。</p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="65cc" class="lo jp hi lk b fi lp lq l lr ls">#importing important packages<br/>import pandas as pd<br/>import numpy as np<br/><br/>#reading the dataset<br/>df=pd.read_csv("/home/user/Desktop/train.csv")<br/><br/>#filling missing values<br/>df['Gender'].fillna('Male', inplace=True)</span></pre><p id="3cd6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，为所有列填充值。EDA、缺失值和异常值的处理在本文中已经跳过。要了解这些话题，可以通读这篇文章:<a class="ae jm" href="https://www.analyticsvidhya.com/blog/2015/04/comprehensive-guide-data-exploration-sas-using-python-numpy-scipy-matplotlib-pandas/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">使用NumPy、Matplotlib和Pandas </strong>在Python中进行数据探索的终极指南。</a></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="56f1" class="lo jp hi lk b fi lp lq l lr ls">#split dataset into train and test<br/><br/>from sklearn.model_selection import train_test_split<br/>train, test = train_test_split(df, test_size=0.3, random_state=0)<br/><br/>x_train=train.drop('Loan_Status',axis=1)<br/>y_train=train['Loan_Status']<br/><br/>x_test=test.drop('Loan_Status',axis=1)<br/>y_test=test['Loan_Status']<br/><br/>#create dummies<br/>x_train=pd.get_dummies(x_train)<br/>x_test=pd.get_dummies(x_test)</span></pre><p id="23cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们进入装袋和增压算法！</p><h1 id="7105" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">4.1 Bagging元估计量</h1><p id="6aea" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">Bagging元估计器是一种集成算法，可用于分类(BaggingClassifier)和回归(BaggingRegressor)问题。它遵循典型的装袋技术进行预测。以下是bagging元估计算法的步骤:</p><ol class=""><li id="b1c2" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">从原始数据集创建随机子集(引导)。</li><li id="d3d8" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">数据集的子集包括所有要素。</li><li id="e4d5" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">用户指定的基本估计器被安装在这些较小的集合中的每一个上。</li><li id="3e9f" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">来自每个模型的预测被组合以得到最终结果。</li></ol><p id="6b9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">代码:</strong></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="94f0" class="lo jp hi lk b fi lp lq l lr ls">from sklearn.ensemble import BaggingClassifier<br/>from sklearn import tree<br/>model = BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))<br/>model.fit(x_train, y_train)<br/>model.score(x_test,y_test)<br/>0.75135135135135134</span></pre><p id="0114" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">回归问题示例代码:</strong></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="98f3" class="lo jp hi lk b fi lp lq l lr ls">from sklearn.ensemble import BaggingRegressor<br/>model = BaggingRegressor(tree.DecisionTreeRegressor(random_state=1))<br/>model.fit(x_train, y_train)<br/>model.score(x_test,y_test)</span></pre><p id="c0f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">算法中使用的参数:</strong></p><ul class=""><li id="d92f" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated"><strong class="ih hj"> base_estimator </strong>:它定义了适合数据集随机子集的基本估计量。当没有指定时，基本估计器是一个决策树。</li><li id="c2bf" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> n_estimators </strong>:是要创建的基本估值器的数量。估计器的数量应该仔细调整，因为大的数量将花费很长时间来运行，而非常小的数量可能不会提供最好的结果。</li><li id="270a" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> max_samples </strong>:该参数控制子集的大小。它是训练每个基本估计量的最大样本数。</li><li id="ebfb" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> max_features </strong>:控制从整个数据集中提取的特征数量。它定义了训练每个基本估计量所需的最大特征数。</li><li id="f8f1" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> n_jobs </strong>:并行运行的作业数量。将该值设置为系统中的核心数。如果为-1，作业的数量将设置为核心的数量。</li><li id="4e5c" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> random_state </strong>:指定随机拆分的方法。当两个模型的随机状态值相同时，两个模型的随机选择是相同的。当您想要比较不同的模型时，此参数很有用。</li></ul><h1 id="024c" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">4.2随机森林</h1><p id="7503" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">随机森林是另一种遵循bagging技术的集成机器学习算法。它是bagging估计算法的扩展。随机森林中的基本估计量是决策树。与bagging元估计器不同，随机森林随机选择一组特征，用于决定决策树每个节点的最佳分裂。</p><p id="93ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一步一步来看，这就是随机森林模型的作用:</p><ol class=""><li id="d0e7" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">从原始数据集创建随机子集(引导)。</li><li id="d450" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">在决策树的每个节点上，只考虑一组随机的特征来决定最佳分割。</li><li id="a437" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">决策树模型适用于每个子集。</li><li id="ee7b" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">通过平均所有决策树的预测来计算最终预测。</li></ol><p id="4257" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jl">注意:随机森林中的决策树可以建立在数据和特征的子集上。特别地，随机森林的sklearn模型使用决策树的所有特征，并且在每个节点随机选择一个特征子集用于分裂。</em></p><p id="ebd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">综上所述，随机森林<strong class="ih hj">随机</strong>选取数据点和特征，构建<strong class="ih hj">多棵树(森林)。</strong></p><p id="0512" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">代码:</strong></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="3b16" class="lo jp hi lk b fi lp lq l lr ls">from sklearn.ensemble import RandomForestClassifier<br/>model= RandomForestClassifier(random_state=1)<br/>model.fit(x_train, y_train)<br/>model.score(x_test,y_test)<br/>0.77297297297297296</span></pre><p id="c2a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在随机森林中使用<em class="jl">model . feature _ importances _</em>可以看到特征重要性。</p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="7cd3" class="lo jp hi lk b fi lp lq l lr ls">for i,j in sorted(zip(x_train.columns, model.feature_importances_)):<br/>    print(i, j)</span></pre><p id="6b63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结果如下:</p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="458b" class="lo jp hi lk b fi lp lq l lr ls">ApplicantIncome 0.180924483743<br/>CoapplicantIncome 0.135979758733<br/>Credit_History 0.186436670523<br/>.<br/>.<br/>.<br/>Property_Area_Urban 0.0167025290557<br/>Self_Employed_No 0.0165385567137<br/>Self_Employed_Yes 0.0134763695267</span></pre><p id="092e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">回归问题示例代码:</strong></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="0ebf" class="lo jp hi lk b fi lp lq l lr ls">from sklearn.ensemble import RandomForestRegressor<br/>model= RandomForestRegressor()<br/>model.fit(x_train, y_train)<br/>model.score(x_test,y_test)</span></pre><p id="839f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参数</strong></p><ul class=""><li id="d812" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated"><strong class="ih hj"> n_estimators: </strong>它定义了要在随机森林中创建的决策树的数量。通常，较高的数值会使预测更强、更稳定，但是非常大的数值会导致更长的训练时间。</li><li id="7292" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj">标准</strong>:定义用于拆分的功能。该函数测量每个要素的分割质量，并选择最佳分割。</li><li id="1a1c" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> max_features </strong>:定义了每个决策树中允许拆分的最大特征数。增加最大特征数通常会提高性能，但是很大的数量会降低每个树的多样性。</li><li id="c32c" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> max_depth </strong>:随机森林有多个决策树。此参数定义了树的最大深度。</li><li id="0c80" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> min_samples_split: </strong>用于定义一个叶节点在尝试拆分之前所需的最小样本数。如果样本数量小于所需数量，则不分割节点。</li><li id="8bdd" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> min_samples_leaf: </strong>定义一个叶节点所需的最小样本数。较小的叶子尺寸使得模型更容易捕捉训练数据中的噪声。</li><li id="7dea" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> max_leaf_nodes: </strong>该参数指定每棵树的最大叶节点数。当叶节点的数量等于最大叶节点时，树停止分裂。</li><li id="e5d8" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> n_jobs </strong>:表示并行运行的作业数量。如果希望它在系统中的所有核心上运行，请将值设置为-1。</li><li id="8158" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> random_state </strong>:该参数用于定义随机选择。它用于各种模型之间的比较。</li></ul><h1 id="d596" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">4.3 AdaBoost</h1><p id="7b77" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">自适应增强或AdaBoost是最简单的增强算法之一。通常，决策树用于建模。创建多个顺序模型，每个模型纠正上一个模型的错误。AdaBoost为错误预测的观测值分配权重，随后的模型工作以正确预测这些值。</p><p id="90d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是执行AdaBoost算法的步骤:</p><ol class=""><li id="ad57" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">最初，数据集中的所有观察值被赋予相同的权重。</li><li id="4d6d" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">模型建立在数据子集的基础上。</li><li id="84c2" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">使用此模型，可以对整个数据集进行预测。</li><li id="42f0" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">通过比较预测值和实际值来计算误差。</li><li id="4c1d" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">在创建下一个模型时，对预测不正确的数据点给予较高的权重。</li><li id="eca0" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">可以使用误差值来确定权重。例如，误差越高，分配给观察值的权重就越大。</li><li id="9288" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">重复这个过程，直到误差函数不变，或者达到估计器数量的最大极限。</li></ol><p id="b290" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">代码:</strong></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="1f4b" class="lo jp hi lk b fi lp lq l lr ls">from sklearn.ensemble import AdaBoostClassifier<br/>model = AdaBoostClassifier(random_state=1)<br/>model.fit(x_train, y_train)<br/>model.score(x_test,y_test)<br/>0.81081081081081086</span></pre><p id="b2f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">回归问题示例代码:</strong></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="c541" class="lo jp hi lk b fi lp lq l lr ls">from sklearn.ensemble import AdaBoostRegressor <br/>model = AdaBoostRegressor() <br/>model.fit(x_train, y_train) model.score(x_test,y_test)</span></pre><p id="7f61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参数</strong></p><ul class=""><li id="fa09" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated"><strong class="ih hj"> base_estimators </strong>:它有助于指定基本估计器的类型，即用作基本学习器的机器学习算法。</li><li id="fb36" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> n_estimators: </strong>定义基本估计量的个数。默认值为10，但是您应该保留更高的值以获得更好的性能。</li><li id="2ae6" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj">learning_rate: </strong>This parameter controls the contribution of the estimators in the final combination. There is a trade-off between learning_rate and n_estimators.</li><li id="5d69" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj">max_depth</strong>: Defines the maximum depth of the individual estimator. Tune this parameter for best performance.</li><li id="a869" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj">n_jobs : </strong>Specifies the number of processors it is allowed to use. Set value to -1 for maximum processors allowed.</li><li id="bf09" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj">random_state</strong> : An integer value to specify the random data split. A definite value of random_state will always produce same results if given with same parameters and training data.</li></ul><h1 id="8aa7" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">4.4 Gradient Boosting (GBM)</h1><p id="8828" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">Gradient Boosting or GBM is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner. Regression trees used as a base learner, each subsequent tree in series is built on the errors calculated by the previous tree.</p><p id="7623" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">We will use a simple example to understand the GBM algorithm. We have to predict the age of a group of people using the below data:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mh"><img src="../Images/2ddaea41b010a85fc30d832503297e61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YGAS7CMrlL56MWhX.png"/></div></div></figure><ol class=""><li id="1fe3" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated">The mean age is assumed to be the predicted value for all observations in the dataset.</li><li id="25b0" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc kt ku kv kw bi translated">The errors are calculated using this mean prediction and actual values of age.</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mi"><img src="../Images/e475e3b87698d6d4d9717102c710a845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IsMRFM_v9mVCIkOp.png"/></div></div></figure><p id="ca7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3. A tree model is created using the errors calculated above as target variable. Our objective is to find the best split to minimize the error.</p><p id="dd38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4. The predictions by this model are combined with the predictions 1.</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mj"><img src="../Images/dd19eb2df5f56245e31a0b649e1c7c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JDxRFUUhGj5KkFzk.png"/></div></div></figure><p id="dd34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5. This value calculated above is the new prediction.</p><p id="224d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6. New errors are calculated using this predicted value and actual value.</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mk"><img src="../Images/61eb701f68b21c9b3b6d47d52d336a7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/0*61gv6GRGBttSUbO6.png"/></div></figure><p id="4418" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7. Steps 2 to 6 are repeated till the maximum number of iterations is reached (or error function does not change).</p><p id="2356" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">Code:</strong></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="6719" class="lo jp hi lk b fi lp lq l lr ls">from sklearn.ensemble import GradientBoostingClassifier<br/>model= GradientBoostingClassifier(learning_rate=0.01,random_state=1)</span><span id="1b92" class="lo jp hi lk b fi lt lq l lr ls">model.fit(x_train, y_train)<br/>model.score(x_test,y_test)<br/>0.81621621621621621</span></pre><p id="feee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">Sample code for regression problem:</strong></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="4678" class="lo jp hi lk b fi lp lq l lr ls">from sklearn.ensemble import GradientBoostingRegressor <br/>model= GradientBoostingRegressor() </span><span id="c57a" class="lo jp hi lk b fi lt lq l lr ls">model.fit(x_train, y_train) <br/>model.score(x_test,y_test)</span></pre><p id="2442" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">Parameters</strong></p><ul class=""><li id="e2cc" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated"><strong class="ih hj">min_samples_split : </strong>Defines the minimum number of samples (or observations) which are required in a node to be considered for splitting. Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.</li><li id="ae0f" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj">min_samples_leaf : </strong>Defines the minimum samples required in a terminal or leaf node. Generally, lower values should be chosen for imbalanced class problems because the regions in which the minority class will be in the majority will be very small.</li><li id="b0c3" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj">min_weight_fraction_leaf : </strong>Similar to min_samples_leaf but defined as a fraction of the total number of observations instead of an integer.</li><li id="03d4" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj">max_depth : </strong>The maximum depth of a tree. Used to control over-fitting as higher depth will allow the model to learn relations very specific to a particular sample. Should be tuned using CV.</li><li id="5963" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> max_leaf_nodes : </strong>一棵树中终端节点或叶子的最大数量。可以代替max_depth进行定义。由于二叉树被创建，深度“n”将产生最大的2^n叶。如果这样定义，GBM将忽略max_depth。</li><li id="93ff" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> max_features : </strong>搜索最佳分割时要考虑的特征数量。这些将是随机选择的。根据经验，特性总数的平方根很有用，但是我们应该检查特性总数的30-40%。较高的值会导致过度拟合，但这通常取决于具体情况。</li></ul><h1 id="3a3f" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">4.5 XGBoos</h1><p id="84cd" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">XGBoost(极限梯度增强)是梯度增强算法的高级实现。XGBoost已被证明是一种高效的ML算法，广泛用于机器学习竞赛和黑客马拉松。XGBoost具有很高的预测能力，比其他梯度增强技术快近10倍。它还包括各种正则化，减少过度拟合，提高整体性能。因此也被称为“<strong class="ih hj">正则化增强</strong>技术。</p><p id="f844" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来看看XGBoost相对于其他技术的优势:</p><ol class=""><li id="45d6" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc kt ku kv kw bi translated"><strong class="ih hj">正规化:</strong></li></ol><ul class=""><li id="e643" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated">标准的GBM实现没有XGBoost那样的规范化。</li><li id="4ac4" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated">因此XGBoost也有助于减少过度拟合。</li></ul><p id="d3e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。并行处理:</strong></p><ul class=""><li id="5667" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated">XGBoost实现并行处理，比GBM快。</li><li id="312b" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated">XGBoost还支持在Hadoop上实现。</li></ul><p id="fee5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。高灵活性:</strong></p><ul class=""><li id="0050" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated">XGBoost允许用户定义定制的优化目标和评估标准，为模型增加了一个全新的维度。</li></ul><p id="3e18" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。处理缺失值:</strong></p><ul class=""><li id="2efc" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated">XGBoost有一个内置的例程来处理丢失的值。</li></ul><p id="f7a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。树木修剪:</strong></p><ul class=""><li id="5186" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated">XGBoost进行拆分，直到指定的max_depth，然后开始向后修剪树，并删除没有正增益的拆分。</li></ul><p id="2906" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 6。内置交叉验证:</strong></p><ul class=""><li id="d6ec" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated">XGBoost允许用户在升压过程的每次迭代中运行交叉验证，因此很容易在一次运行中获得升压迭代的精确最佳次数。</li></ul><p id="7ff3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">代码:</strong></p><p id="aa19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为XGBoost会自己处理丢失的值，所以您不必估算丢失的值。您可以跳过上述代码中缺失值插补的步骤。像往常一样遵循剩余的步骤，然后如下应用xgboost。</p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="ba33" class="lo jp hi lk b fi lp lq l lr ls">import xgboost as xgb<br/>model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)<br/>model.fit(x_train, y_train)<br/>model.score(x_test,y_test)<br/>0.82702702702702702</span></pre><p id="bd6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">回归问题示例代码:</strong></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="0716" class="lo jp hi lk b fi lp lq l lr ls">import xgboost as xgb <br/>model=xgb.XGBRegressor() </span><span id="4342" class="lo jp hi lk b fi lt lq l lr ls">model.fit(x_train, y_train) <br/>model.score(x_test,y_test)</span></pre><p id="814f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参数</strong></p><ul class=""><li id="5eaf" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated"><strong class="ih hj"> nthread : </strong>用于并行处理，需要输入系统内核的数量。如果您希望在所有内核上运行，请不要输入该值。算法会自动检测出来。</li><li id="f220" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> eta : </strong>类似于以GBM为单位的学习率。通过缩小每一步的权重，使模型更加健壮。</li><li id="568b" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> min_child_weight : </strong>定义一个孩子所需的所有观察的最小权重和。用于控制过度拟合。较高的值会阻止模型学习可能高度特定于为树选择的特定样本的关系。</li><li id="669c" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj">最大深度:</strong>用于定义最大深度。更高的深度将允许模型学习特定样本的特定关系。</li><li id="655e" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> max_leaf_nodes : </strong>树中终端节点或叶子的最大数量。可以代替max_depth进行定义。由于二叉树被创建，深度“n”将产生最大的2^n叶。如果这样定义，GBM将忽略max_depth。</li><li id="04b7" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> gamma : </strong>仅当结果分裂给出损失函数的正减少时，节点才被分裂。Gamma指定进行分割所需的最小损失减少量。使算法保守。这些值可能因损失函数而异，应该进行调整。</li><li id="76f5" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj">子样本:</strong>同GBM子样本。表示每棵树随机抽样的观察值的分数。较低的值使算法更加保守，并防止过度拟合，但太小的值可能会导致拟合不足。</li><li id="07c6" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> colsample_bytree : </strong>类似于GBM中的max_features。表示对每棵树随机抽样的列的分数。</li></ul><h1 id="c500" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">4.6轻型GBM</h1><p id="21a2" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">在讨论Light GBM是如何工作的之前，我们先来了解一下，当我们有这么多其他算法(就像上面看到的那些)的时候，为什么我们还需要这个算法。<strong class="ih hj">当数据集非常大的时候，Light GBM胜过所有其他算法</strong>。与其他算法相比，轻量级GBM在大型数据集上运行所需的时间更少。</p><p id="4401" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LightGBM是一个梯度推进框架，它使用基于树的算法，并遵循逐叶方法，而其他算法以逐层方法模式工作。下面的图片将帮助你更好地理解其中的区别。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/902e4b28100f4ec8139b9fd5713202bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*LMTIVKZfswkcvaix.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mm"><img src="../Images/5d2b00c61e02dae67c7c781bdcdbd447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_IlpS758Wp3jaDlH.png"/></div></div></figure><p id="e431" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逐叶增长可能会在较小的数据集上导致过拟合，但这可以通过使用“max_depth”参数进行学习来避免。你可以在<a class="ae jm" href="https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">这篇</strong> </a>文章中了解更多关于Light GBM及其与XGB的比较。</p><p id="58dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">代号:</strong></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="06a5" class="lo jp hi lk b fi lp lq l lr ls">import lightgbm as lgb<br/>train_data=lgb.Dataset(x_train,label=y_train)<br/>#define parameters<br/>params = {'learning_rate':0.001}<br/>model= lgb.train(params, train_data, 100) <br/>y_pred=model.predict(x_test)<br/>for i in range(0,185):<br/>   if y_pred[i]&gt;=0.5: <br/>   y_pred[i]=1<br/>else: <br/>   y_pred[i]=0<br/>0.81621621621621621</span></pre><p id="0b62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">回归问题示例代码:</strong></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="97d3" class="lo jp hi lk b fi lp lq l lr ls">import lightgbm as lgb train_data=lgb.Dataset(x_train,label=y_train) params = {'learning_rate':0.001} </span><span id="6d1c" class="lo jp hi lk b fi lt lq l lr ls">model= lgb.train(params, train_data, 100) from sklearn.metrics import mean_squared_error rmse=mean_squared_error(y_pred,y_test)**0.5</span></pre><p id="cd9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参数</strong></p><ul class=""><li id="b9cc" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated"><strong class="ih hj"> num_iterations </strong>:定义要执行的升压迭代次数。</li><li id="03ec" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj">叶子数量</strong>:该参数用于设置一棵树要形成的叶子数量。在轻型GBM的情况下，由于分裂发生在叶片方向而不是深度方向，叶片数量必须小于2^(max_depth)，否则，可能会导致过度拟合。</li><li id="c877" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> min_data_in_leaf </strong>:非常小的值可能会导致过拟合。这也是处理过度拟合时最重要的参数之一。</li><li id="ad80" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> max_depth </strong>:指定一棵树可以生长到的最大深度或者级别。此参数的值非常高会导致过度拟合。</li><li id="0612" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> bagging_fraction </strong>:用于指定每次迭代使用的数据的分数。该参数通常用于加速训练。</li><li id="b768" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> max_bin </strong>:定义特征值将被存储的最大箱数。较小的max_bin值可以节省大量时间，因为它将特征值存储在离散的箱中，计算开销很小。</li></ul><h1 id="34cb" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">4.7催化增强</h1><p id="bd4d" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">处理分类变量是一个乏味的过程，尤其是当你有大量这样的变量时。当您的分类变量有太多标签时(即，它们是高度基数的)，对它们执行一次热编码会以指数方式增加维度，并且处理数据集变得非常困难。</p><p id="9c1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">CatBoost可以自动处理分类变量，不需要像其他机器学习算法那样进行大量的数据预处理。<a class="ae jm" href="https://www.analyticsvidhya.com/blog/2017/08/catboost-automated-categorical-data/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">这里的</strong> </a>是一篇详细讲解CatBoost的文章。</p><p id="f984" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">代码:</strong></p><p id="cd2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">CatBoost算法有效地处理分类变量。因此，您不应该对分类变量执行一次性编码。只需加载文件，估算缺失值，就可以了。</p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="0663" class="lo jp hi lk b fi lp lq l lr ls">from catboost import CatBoostClassifier<br/>model=CatBoostClassifier()<br/>categorical_features_indices = np.where(df.dtypes != np.float)[0]</span><span id="0281" class="lo jp hi lk b fi lt lq l lr ls">model.fit(x_train,y_train,cat_features=([ 0,  1, 2, 3, 4, 10]),eval_set=(x_test, y_test))<br/>model.score(x_test,y_test)<br/>0.80540540540540539</span></pre><p id="467d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">回归问题示例代码:</strong></p><pre class="je jf jg jh fd lj lk ll lm aw ln bi"><span id="f9c2" class="lo jp hi lk b fi lp lq l lr ls">from catboost import CatBoostRegressor <br/>model=CatBoostRegressor() categorical_features_indices = np.where(df.dtypes != np.float)[0] </span><span id="06a6" class="lo jp hi lk b fi lt lq l lr ls">model.fit(x_train,y_train,cat_features=([ 0,  1, 2, 3, 4, 10]),eval_set=(x_test, y_test)) <br/>model.score(x_test,y_test)</span></pre><p id="6d63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参数</strong></p><ul class=""><li id="9290" class="km kn hi ih b ii ij im in iq lf iu lg iy lh jc mg ku kv kw bi translated"><strong class="ih hj"> loss_function: </strong>定义用于训练的度量。</li><li id="899e" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj">迭代次数:</strong>可以构建的最大树数。树的最终数量可能小于或等于该数量。</li><li id="a25c" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> learning_rate: </strong>定义学习率。用于减少梯度步长。</li><li id="835a" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> border_count: </strong>指定数字特征的分割数。它类似于max_bin参数。</li><li id="3397" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj">深度:</strong>定义了树的深度。</li><li id="dec6" class="km kn hi ih b ii kx im ky iq kz iu la iy lb jc mg ku kv kw bi translated"><strong class="ih hj"> random_seed: </strong>这个参数类似于我们之前看到的‘random _ state’参数。它是一个整数值，用于定义训练的随机种子。</li></ul><p id="882b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就把我们带到了整体算法部分的结尾。我们在这篇文章中已经涵盖了相当多的内容！</p><h1 id="bf65" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">结束注释</h1><p id="eb2d" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq lc is it iu ld iw ix iy le ja jb jc hb bi translated">集合建模可以指数级提升你的模型的性能，有时会成为第一名和第二名之间的决定性因素！在本文中，我们介绍了各种集成学习技术，并了解了这些技术如何应用于机器学习算法。此外，我们在贷款预测数据集上实现了这些算法。</p><p id="2c8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章会让你对这个话题有一个坚实的理解。如果你有任何建议或问题，请在下面的评论区分享。此外，我鼓励您在自己的终端实现这些算法，并与我们分享您的结果！</p></div><div class="ab cl mn mo gp mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="hb hc hd he hf"><p id="0448" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jl">原载于2018年6月18日</em><a class="ae jm" href="https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/" rel="noopener ugc nofollow" target="_blank"><em class="jl">【www.analyticsvidhya.com】</em></a><em class="jl">。</em></p></div></div>    
</body>
</html>