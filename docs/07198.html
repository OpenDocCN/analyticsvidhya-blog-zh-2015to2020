<html>
<head>
<title>Chapter 4.1 — Linear Regression Model using PyTorch Built-ins</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">第4.1章—使用PyTorch内置的线性回归模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/chapter-4-1-linear-regression-model-using-pytorch-built-ins-53e8be20fb96?source=collection_archive---------28-----------------------#2020-06-16">https://medium.com/analytics-vidhya/chapter-4-1-linear-regression-model-using-pytorch-built-ins-53e8be20fb96?source=collection_archive---------28-----------------------#2020-06-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3d3f" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">PyTorch内置介绍。</h2></div><p id="8110" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在之前的博客中，我们从头开始构建了一个线性回归模型，没有使用任何PyTorch内置函数。在使用PyTorch内置之前，我们应该了解一些关键概念，并熟悉常用的包和模块。让我们开始吧。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/5104e6e37339d2eb8b2aa0b479334a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DIvJyK2-1TsZif3mcokm1A.jpeg"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated"><a class="ae kj" href="https://ak.picdn.net/shutterstock/videos/3921440/thumb/1.jpg" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><ol class=""><li id="b02f" class="kk kl hi iz b ja jb jd je jg km jk kn jo ko js kp kq kr ks bi translated"><strong class="iz hj"> torch.nn包</strong></li></ol><p id="4349" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个torch包提供了一组标准的神经网络模块和一组容器模块，可以用来定义任意方向(循环或非循环)的图形。实际上，大多数网络要么是顺序的，要么具有简单的分支模式或递归。</p><p id="0f32" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> 2。torch . utils . data . tensordataset</strong></p><p id="fc94" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个类帮助我们使用数组索引来访问一小部分训练数据。它返回一个元组(或元组对)，其中第一个元素包含所选行的输入变量，第二个元素包含目标。</p><p id="0361" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> 3。torch . utils . data . random _ split</strong></p><p id="3a69" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们使用这个函数将我们手头的数据集分成训练、验证和测试数据集(我们将很快讨论这些)。</p><p id="c188" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它需要两个参数:—</p><ul class=""><li id="3c14" class="kk kl hi iz b ja jb jd je jg km jk kn jo ko js kt kq kr ks bi translated">数据集:-要拆分的数据集。</li><li id="706a" class="kk kl hi iz b ja ku jd kv jg kw jk kx jo ky js kt kq kr ks bi translated">长度:-每个子集不同长度的列表。</li></ul><p id="9da2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> 4。torch.utils.data.DataLoader </strong></p><p id="7e62" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个类用于将我们的数据分成预定义大小的批。它还提供了数据的随机混洗。它使计算更快更有效。因为我们在列表中有两个值，所以在调用这个函数时，我们有两个分配给它的对象。</p><p id="555e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> 5。torch.nn.Linear </strong></p><p id="f292" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个类对输入的数据进行线性变换。我们使用神经网络定义模型，而不是像在第3章中那样手动初始化权重和偏差。线性类自动。</p><p id="1444" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个类有以下参数:—</p><ul class=""><li id="1d3e" class="kk kl hi iz b ja jb jd je jg km jk kn jo ko js kt kq kr ks bi translated">in_features :-每个输入样本的大小。</li><li id="1ce3" class="kk kl hi iz b ja ku jd kv jg kw jk kx jo ky js kt kq kr ks bi translated">out_features :-每个输出样本的大小。</li><li id="74c9" class="kk kl hi iz b ja ku jd kv jg kw jk kx jo ky js kt kq kr ks bi translated">偏差:-如果设置为false，该层将不会学习附加偏差。默认情况下，它被设置为true。</li></ul><p id="148d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">权重的形状为(输入要素，输出要素)，偏差的形状为(输出要素)。它们是随机初始化的，可以更改。它有一个参数方法，将返回模型中所有权重和偏差矩阵的列表。对于线性回归模型，我们只有一个权重矩阵和一个偏差矩阵。</p><p id="bd76" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> 6。torch.nn.Functional </strong></p><p id="927b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个火炬包包括许多损失函数和其他几个实用程序。</p><p id="ce57" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> 7。torch.optim </strong></p><p id="839f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以使用优化器，而不是使用梯度手动操作模型的权重和偏差。这个torch包实现了各种优化技术。要使用torch.optim，我们必须构建一个优化器对象，它将保存当前状态，并根据计算出的梯度更新参数。所有的优化技术都实现了一个step()方法来更新参数。</p><p id="ea9f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这些是我们将在下一篇博客中用来构建线性回归模型的一些包。接下来，让我们来看看我们将用于实现该模型的一些术语和概念。</p><p id="fbfe" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在构建真实世界的机器学习模型时，我们将把可用的数据集分成三部分:—</p><ul class=""><li id="25f1" class="kk kl hi iz b ja jb jd je jg km jk kn jo ko js kt kq kr ks bi translated">训练集:这是一组用于训练模型的数据，即计算损失，使用优化技术调整模型的权重。</li><li id="66b7" class="kk kl hi iz b ja ku jd kv jg kw jk kx jo ky js kt kq kr ks bi translated">验证集:-该集用于在训练时评估模型，调整超参数(学习率、批量大小等。)并挑选模型的最佳版本。</li><li id="a5e4" class="kk kl hi iz b ja ku jd kv jg kw jk kx jo ky js kt kq kr ks bi translated">测试集:这是用来比较不同的模型，或不同类型的建模方法，并报告模型的最终准确性。</li></ul><p id="45ef" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">梯度下降算法</strong></p><p id="2089" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个术语在<a class="ae kj" rel="noopener" href="/analytics-vidhya/chapter-3-linear-regression-from-scratch-474a795ea70f">第3章</a>的最后部分使用，当时我们正在训练模型以减少损失函数。让我们详细讨论一下这个概念。</p><p id="71e0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">定义导数有很多种方法。两个常见的定义是:-</p><ul class=""><li id="9633" class="kk kl hi iz b ja jb jd je jg km jk kn jo ko js kt kq kr ks bi translated">函数图形切线的斜率。</li><li id="2221" class="kk kl hi iz b ja ku jd kv jg kw jk kx jo ky js kt kq kr ks bi translated">函数变化率。</li></ul><p id="ab92" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">梯度是几个变量的导数的概括。让我们用x作为函数J(x1，x2，x3)中的变量</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es kz"><img src="../Images/00d7302547588bc2cd4cfc112d48dcce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wNJmTAm0xSjQI6qE7fCSGA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">由作者生成</figcaption></figure><p id="7567" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里∇是符号表示，表示我们在取函数的梯度，而&lt; and &gt;内的梯度用来表示梯度是一个向量。</p><p id="b7d8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于标量(一个变量)，梯度给出切线的斜率。在矢量的情况下，梯度指向函数增长最快的方向。</p><p id="33bd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，让我们建立在上面的梯度定义，让我们尝试和理解梯度下降算法。这是一个迭代过程，用于寻找误差或成本函数的最小值。</p><p id="5e00" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们想减少一个函数，我们要朝着与最快增加相反的方向移动。也就是说，如果我们在点x0，并想移动到附近最低的点(局部最小值)，我们的下一步应该是:—</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es la"><img src="../Images/04a190402e7253ac33f76d54c19ea39e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xGmhvU-u0R5afoSejfOseA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">由作者生成</figcaption></figure><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lb"><img src="../Images/060d978d7ee800d6862f7def89d0cac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lYJnfMhD5tEPr83x_xZ9Pw.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated"><a class="ae kj" href="https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/1200px-Gradient_descent.svg.png" rel="noopener ugc nofollow" target="_blank">来源</a> -梯度下降算法</figcaption></figure><p id="99cd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">学习率是一个非常小的值，乘以梯度，然后从当前位置减去。学习速度的选择是至关重要的，尤其是当我们接近最小值的时候。如果学习率很低，达到最低点所用的时间将会很长。另一方面，如果选择的值太高，则会导致过冲，并可能错过局部最小值。</p><p id="487f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">等式中的负号表示我们正朝着与梯度相反的方向运动。</p><p id="b2ff" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">随机梯度下降</strong></p><p id="1f7b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将在我们的模型中使用随机梯度下降作为优化器。之所以称之为随机，是因为样本是成批选择的(通常是随机洗牌)，而不是作为一个单一的群体。它也被称为增量梯度下降。在该算法中，我们重复运行训练集，并且每当我们遇到数据点时，我们仅根据该单个数据点集(输入，目标)的误差梯度来更新参数。而批量梯度下降必须在采取单个步骤之前扫描整个训练集，如果数据点的数量非常大，这可能证明是昂贵的。</p><p id="1520" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作为博客的总结，我们现在可以简单解释一下我们将使用PyTorch内置的不同torch包来构建线性回归模型。我们对梯度下降算法和随机梯度下降算法有很好的理解。</p></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><p id="b8b1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在下一篇博客中，我们将以此为基础，使用PyTorch内置函数构建一个线性回归模型。</p></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><p id="dae0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="lj">感谢阅读，下期再见！</em></p></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><blockquote class="lk ll lm"><p id="4a84" class="ix iy lj iz b ja jb ij jc jd je im jf ln jh ji jj lo jl jm jn lp jp jq jr js hb bi translated">对于博客的进一步阅读，</p></blockquote><p id="42e4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae kj" href="https://github.com/cleor41/CS229_Notes/blob/master/lectures/cs229-notes1.pdf" rel="noopener ugc nofollow" target="_blank">https://github . com/cleor 41/cs 229 _ Notes/blob/master/lections/cs 229-Notes 1 . pdf</a></p><p id="c027" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请看<a class="ae kj" href="https://jovian.ml/outlink?url=https%3A%2F%2Fstorage.googleapis.com%2Fsupplemental_media%2Fudacityu%2F315142919%2FGradient%2520Descent.pdf" rel="noopener ugc nofollow" target="_blank">这些来自Udacity课程</a>的笔记。</p></div></div>    
</body>
</html>