<html>
<head>
<title>ReLU Activation : Increase accuracy by being Greedy!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ReLU激活:靠贪心增加准确率！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/relu-activation-increase-accuracy-by-being-greedy-6b93c7c40882?source=collection_archive---------7-----------------------#2019-09-29">https://medium.com/analytics-vidhya/relu-activation-increase-accuracy-by-being-greedy-6b93c7c40882?source=collection_archive---------7-----------------------#2019-09-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/87e6dac9af2f82aecdb7ec2e14bcd623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jqaW5OEqSVF6xjftzRkNnw.jpeg"/></div></div></figure><p id="923b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本文将帮助您决定在哪里使用ReLU(校正线性单位),以及它如何在提高模型的准确性方面发挥作用。使用这个<a class="ae jo" href="https://github.com/divyanshuraj6815/experiments/blob/master/activation_relu/Experiment_with_Activation_(Relu).ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub链接</a>查看源代码。</p><p id="98c5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下表是本文在cifar10数据集上训练50个时期的最终结果。我们对基本模型有64%的准确率。激活层的优化使用将准确率提高到80%。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="920b" class="jy jz hi ju b fi ka kb l kc kd">+---------------------------+-----------------------------------+<br/>|         <strong class="ju hj">model_name        </strong>| <strong class="ju hj">validation accuracy </strong>| <strong class="ju hj">time taken  </strong>|<br/>+---------------------------+-----------------------------------+<br/>|  without_any_activation   |               64.00 |  1171.98    |<br/>|  with_relu_activation     |               78.52 |  1300.42    |<br/>|  with_relu_activation_2   |               78.12 |  1305.67    |<br/>|  with_relu_activation_2_3 |               77.55 |  1303.76    |<br/>|  with_relu_activation_7   |               78.80 |  1349.62    |<br/>|  with_relu_activation_7_10|               80.56 |  1368.53    |<br/>+---------------------------+-----------------------------------+</span></pre><h2 id="b02b" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">内容</h2><ol class=""><li id="dc24" class="kx ky hi is b it kz ix la jb lb jf lc jj ld jn le lf lg lh bi translated">ReLU的定义</li><li id="ddcd" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">模型的定义</li><li id="62a5" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">定义启发法</li><li id="fbe9" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">实验和结果</li><li id="7625" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">未来的工作</li></ol><h2 id="0932" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">1.ReLU的定义:</h2><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/61feb87faaae38518f47d08b8658e26c.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*njuH4XVXf-l9pR_RorUOrA.png"/></div></figure><ul class=""><li id="3185" class="kx ky hi is b it iu ix iy jb lo jf lp jj lq jn lr lf lg lh bi translated">它从神经网络中代表该层的层或矩阵中删除所有负值。</li></ul><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="246b" class="jy jz hi ju b fi ka kb l kc kd">def relu (x):<br/> return x if x &gt; 0 else 0</span></pre><ul class=""><li id="35ba" class="kx ky hi is b it iu ix iy jb lo jf lp jj lq jn lr lf lg lh bi translated">我们也可以使用泄漏ReLU。</li><li id="8948" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn lr lf lg lh bi translated">我们使用了<a class="ae jo" href="https://en.wikipedia.org/wiki/Batch_normalization" rel="noopener ugc nofollow" target="_blank">批量标准化</a>，这是通过一个标准化步骤来实现的，该步骤固定了每一层输入的均值和方差。</li></ul><h2 id="667b" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">2.本实验所用模型的定义</h2><p id="f5f8" class="pw-post-body-paragraph iq ir hi is b it kz iv iw ix la iz ja jb ls jd je jf lt jh ji jj lu jl jm jn hb bi translated">下面的例子是一个非常简单的模型，因为我们只想关注<strong class="is hj"> ReLU激活的作用。</strong></p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="b209" class="jy jz hi ju b fi ka kb l kc kd">_________________________________________________________________<br/><strong class="ju hj">Layer (type)                 Output Shape              Param #   </strong><br/>=================================================================<br/>conv2d_1 (Conv2D)           (None, 30, 30, 48)          1344      <br/>_________________________________________________________________<br/>activation_1 (Activation)    (None, 30, 30, 48)            0         <br/>_________________________________________________________________<br/>batch_normalization_1 (Batc (None, 30, 30, 48)           192       <br/>_________________________________________________________________<br/>conv2d_2 (Conv2D)           (None, 28, 28, 48)         20784     <br/>_________________________________________________________________<br/>activation_2 (Activation)    (None, 28, 28, 48)            0         <br/>_________________________________________________________________<br/>batch_normalization_2 (Batc (None, 28, 28, 48)           192       <br/>_________________________________________________________________<br/>conv2d_3 (Conv2D)           (None, 26, 26, 96)         41568     <br/>_________________________________________________________________<br/>activation_3 (Activation)    (None, 26, 26, 96)            0         <br/>_________________________________________________________________<br/>batch_normalization_3 (Batc (None, 26, 26, 96)           384       <br/>_________________________________________________________________<br/>conv2d_4 (Conv2D)           (None, 24, 24, 96)         83040     <br/>_________________________________________________________________<br/>activation_4 (Activation)    (None, 24, 24, 96)            0         <br/>_________________________________________________________________<br/>batch_normalization_4 (Batc (None, 24, 24, 96)           384       <br/>_________________________________________________________________<br/>conv2d_5 (Conv2D)           (None, 22, 22, 96)         83040     <br/>_________________________________________________________________<br/>activation_5 (Activation)    (None, 22, 22, 96)            0         <br/>_________________________________________________________________<br/>batch_normalization_5 (Batc (None, 22, 22, 96)           384       <br/>_________________________________________________________________<br/>conv2d_6 (Conv2D)           (None, 20, 20, 96)         83040     <br/>_________________________________________________________________<br/>activation_6 (Activation)    (None, 20, 20, 96)            0         <br/>_________________________________________________________________<br/>batch_normalization_6 (Batc (None, 20, 20, 96)           384       <br/>_________________________________________________________________<br/>max_pooling2d_3 (MaxPooling2 (None, 10, 10, 96)            0         <br/>_________________________________________________________________<br/>conv2d_7 (Conv2D)           (None, 8, 8, 192)         166080    <br/>_________________________________________________________________<br/>activation_7 (Activation)    (None, 8, 8, 192)             0         <br/>_________________________________________________________________<br/>batch_normalization_7 (Batc (None, 8, 8, 192)            768       <br/>_________________________________________________________________<br/>conv2d_8 (Conv2D)           (None, 6, 6, 192)         331968    <br/>_________________________________________________________________<br/>activation_8 (Activation)    (None, 6, 6, 192)             0         <br/>_________________________________________________________________<br/>batch_normalization_8 (Batc (None, 6, 6, 192)            768       <br/>_________________________________________________________________<br/>max_pooling2d_4 (MaxPooling2 (None, 3, 3, 192)             0         <br/>_________________________________________________________________<br/>conv2d_9 (Conv2D)           (None, 1, 1, 192)         331968    <br/>_________________________________________________________________<br/>activation_9 (Activation)   (None, 1, 1, 192)              0         <br/>_________________________________________________________________<br/>batch_normalization_9 (Batc (None, 1, 1, 192)            768       <br/>_________________________________________________________________<br/>conv2d_10 (Conv2D)           (None, 1, 1, 10)           1930      <br/>_________________________________________________________________<br/>activation_10 (Activation)   (None, 1, 1, 10)              0         <br/>_________________________________________________________________<br/>batch_normalization_10 (Batc (None, 1, 1, 10)             40        <br/>_________________________________________________________________<br/>flatten_1 (Flatten)          (None, 10)                    0         <br/>_________________________________________________________________<br/>activation_11 (Activation)   (None, 10)                    0         <br/>=================================================================<br/>Total params: 1,149,026<br/>Trainable params: 1,146,894<br/>Non-trainable params: 2,132</span></pre><p id="da15" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在此模型中，我们将尝试通过使用这些层或根据一些启发不使用它们来提高准确性:</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="1c35" class="jy jz hi ju b fi ka kb l kc kd">activation_1, activation_2, activation_3, activation_4, activation_5, activation_6, activation_7, activation_8, activation_9 and activation_10</span></pre><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/970b5f3f148dca2ee52905eb4e3aa128.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*QPeVEifBmEGGRYPZGf6Pxg.png"/></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">CIFAR10数据集</figcaption></figure><h2 id="cbdd" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">3.定义启发法</h2><p id="445b" class="pw-post-body-paragraph iq ir hi is b it kz iv iw ix la iz ja jb ls jd je jf lt jh ji jj lu jl jm jn hb bi translated">我们来看一个3x3的矩阵:</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="bc39" class="jy jz hi ju b fi ka kb l kc kd">          A                                            B<br/>+------+------+------+                      +------+------+------+<br/>| -0.2 |  0.5 |  0.3 |                      |  <strong class="ju hj">0.0</strong> |  0.5 |  0.3 |<br/>+------+------+------+                      +------+------+------+<br/>|  0.4 | -0.6 |  0.4 |      =========&gt;      |  0.4 |  <strong class="ju hj">0.0</strong> |  0.4 |<br/>+------+------+------+         ReLU         +------+------+------+<br/>| -0.3 |  0.9 |  0.1 |                      |  <strong class="ju hj">0.0</strong> |  0.9 |  0.1 |<br/>+------+------+------+                      +------+------+------+</span></pre><ol class=""><li id="cdb9" class="kx ky hi is b it iu ix iy jb lo jf lp jj lq jn le lf lg lh bi translated"><strong class="is hj"> ratio_num : </strong>负数数量/正数数量</li><li id="d035" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated"><strong class="is hj"> ratio_sum : </strong>负数之和/正数之和</li></ol><p id="a018" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">在给定矩阵</strong>中:</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="160d" class="jy jz hi ju b fi ka kb l kc kd">+-----------------+----------------------+-------------------+ <br/>|     Heuristic   |       Matrix A       |     Matrix B      |<br/>+-----------------+----------------------+-------------------+<br/>|    ratio_num    |      3 / 6 = 0.5     |     0 / 6 = 0     |<br/>+-----------------+----------------------+-------------------+<br/>|    ratio_sum    |  -1.1 / 2.6 = -0.423 |    0 / 2.6 = 0    |<br/>+-----------------+----------------------+-------------------+</span></pre><p id="d970" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们来看看图层<strong class="is hj"> conv2d_8 </strong>和<strong class="is hj"> activation_8 </strong>。它们分别代表<strong class="is hj">矩阵A </strong>和<strong class="is hj">矩阵B </strong>。它的尺寸是6x6x192。这意味着它有192个6×6的矩阵。对于我们的用例，我们将它视为一维数组。</p><p id="5714" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们如何知道将矩阵A (conv2d_8)转换为矩阵B (activation_8)是否有利于提高精度？</p><p id="ed65" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们为<strong class="is hj"> conv2d_8 </strong>找到<strong class="is hj"> ratio_num </strong>和<strong class="is hj"> ratio_sum </strong>，看看它是否能帮助我们做出任何决定。</p><h2 id="cc3f" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">4.实验</h2><h2 id="66b0" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated"><strong class="ak"> <em class="ma"> 1。无_任何_激活(模型1): </em> </strong></h2><ol class=""><li id="ca78" class="kx ky hi is b it kz ix la jb lb jf lc jj ld jn le lf lg lh bi translated">我们在不包括任何激活层的情况下训练该模型，即在定义该模型时删除了所有激活1、激活2、激活3、激活4、激活5、激活6、激活7、激活8、激活9和激活10层。</li><li id="5ab6" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">在第10个纪元中，我们获得了64.07%的最佳验证准确率。</li><li id="8365" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">我们从所有10个类中选择一个图像，并在训练模型50个时期后，找出每个类和每个层的启发法(之后我们应该使用ReLU激活)。我们还找出了启发式的平均值。</li><li id="bd18" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">在此查看结果:</li></ol><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="mb mc l"/></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">所有类的所有卷积图层的ratio_num和ratio_sum</figcaption></figure><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/e73c3db7cc99f3d68b3dc29ee48da6ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q4MnQvJK_A9rtA7IiJ42bA.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">精确度和损耗图</figcaption></figure><h2 id="65c5" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated"><strong class="ak"> 2。<em class="ma"> with_relu_activation(模型2):</em>T3】</strong></h2><ol class=""><li id="ee4a" class="kx ky hi is b it kz ix la jb lb jf lc jj ld jn le lf lg lh bi translated">我们用所有激活层训练该模型，即所有激活_1、激活_2、激活_3、激活_4、激活_5、激活_6、激活_7、激活_8、激活_9和激活_10层。</li><li id="e74f" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">在第48个历元中，我们获得了78.52%的最佳验证准确率。</li><li id="895b" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">我们从所有10个类中选择一个图像，并在训练模型50个时期后的激活层之前的层中找出每个类的启发法。我们还找出了启发式的平均值。</li><li id="4df3" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">在此查看结果:</li></ol><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="mb mc l"/></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">所有类的所有卷积图层的ratio_num和ratio_sum</figcaption></figure><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/2d6aba3c7c9076d7e249c27bd6a18405.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ESOPbJ0okdQhlmCsm7o_xA.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">精确度和损耗图</figcaption></figure><blockquote class="me mf mg"><p id="7a32" class="iq ir mh is b it iu iv iw ix iy iz ja mi jc jd je mj jg jh ji mk jk jl jm jn hb bi translated">我们观察到，通过添加ReLU激活，我们将准确率从64%提高到了78%。</p><p id="c5f1" class="iq ir mh is b it iu iv iw ix iy iz ja mi jc jd je mj jg jh ji mk jk jl jm jn hb bi translated">我们将尝试移除所有ReLU激活层的20%(即2层)，看看我们是否可以进一步提高准确性。</p><p id="5f34" class="iq ir mh is b it iu iv iw ix iy iz ja mi jc jd je mj jg jh ji mk jk jl jm jn hb bi translated">有两种方法可以去除激活层:<br/> 1。移除ratio_num和ratio_sum值最小的图层。(表中的索引2)</p><p id="20b8" class="iq ir mh is b it iu iv iw ix iy iz ja mi jc jd je mj jg jh ji mk jk jl jm jn hb bi translated">2.移除ratio_num和ratio_sum值最大的图层。(表中的索引7)</p></blockquote><h2 id="b5eb" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">3.with_relu_activation_2(型号3):</h2><ol class=""><li id="23f1" class="kx ky hi is b it kz ix la jb lb jf lc jj ld jn le lf lg lh bi translated">我们用激活_1、激活_3、激活_4、激活_5、激活_6、激活_7、激活_8、激活_9和激活_10层来训练模型。请注意，我们已经从该模型中移除了<strong class="is hj">activation _ 2</strong>。</li><li id="2925" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">我们在第45个纪元达到了78.12%的最佳验证准确率。</li><li id="750b" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">我们从所有10个类中选择一个图像，并在训练模型50个时期后的激活层之前的层中找出每个类的启发法。我们还找出了启发式的平均值。</li><li id="8d16" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">在此查看结果:</li></ol><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="mb mc l"/></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">所有类的所有卷积图层的ratio_num和ratio_sum</figcaption></figure><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/410824a6ca014410d4892a5a25a3c077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OyDzJhZ9wFVVK5aad8ITTg.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">精确度和损耗图</figcaption></figure><blockquote class="me mf mg"><p id="1e75" class="iq ir mh is b it iu iv iw ix iy iz ja mi jc jd je mj jg jh ji mk jk jl jm jn hb bi translated">我们观察到验证准确率从78.52%下降到78.12%</p><p id="725b" class="iq ir mh is b it iu iv iw ix iy iz ja mi jc jd je mj jg jh ji mk jk jl jm jn hb bi translated">现在在这个模型中，我们可以从表中看到<strong class="is hj">索引3 </strong>现在有最低的ratio_num和ratio_sum。</p><p id="aecf" class="iq ir mh is b it iu iv iw ix iy iz ja mi jc jd je mj jg jh ji mk jk jl jm jn hb bi translated">所以我们将重新定义没有<strong class="is hj"> activation_2 </strong>和<strong class="is hj"> activation_3 </strong>层的模型，重新训练。</p></blockquote><h2 id="90e5" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">4.with_relu_activation_2_3(型号4):</h2><ol class=""><li id="beeb" class="kx ky hi is b it kz ix la jb lb jf lc jj ld jn le lf lg lh bi translated">我们用激活_1、激活_4、激活_5、激活_6、激活_7、激活_8、激活_9和激活_10层来训练该模型。请注意，我们已经从该模型中移除了<strong class="is hj">activation _ 2</strong>和<strong class="is hj"> activation_3 </strong>。</li><li id="f429" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">我们在第48个纪元达到了77.55%的最佳验证准确率。</li></ol><blockquote class="me mf mg"><p id="0f68" class="iq ir mh is b it iu iv iw ix iy iz ja mi jc jd je mj jg jh ji mk jk jl jm jn hb bi translated">整体精度下降了1%，所以这不是有效的方法。</p><p id="06a4" class="iq ir mh is b it iu iv iw ix iy iz ja mi jc jd je mj jg jh ji mk jk jl jm jn hb bi translated">我们现在将尝试移除具有最大比率_数量和比率_总和(即激活_7)的层</p></blockquote><h2 id="d83d" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">5.with_relu_activation_7(型号5):</h2><ol class=""><li id="7e56" class="kx ky hi is b it kz ix la jb lb jf lc jj ld jn le lf lg lh bi translated">我们用激活_1、激活_2、激活_3、激活_4、激活_5、激活_6、激活_8、激活_9和激活_10层来训练该模型。请注意，我们已经从这个模型中移除了activation_7。</li><li id="bb49" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">在第42个历元中，我们获得了78.80%的最佳验证准确率。</li><li id="e1a4" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">我们从所有10个类中选择一个图像，并在训练模型50个时期后的激活层之前的层中找出每个类的启发法。我们还找出了启发式的平均值。</li><li id="0b59" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">在此查看结果:</li></ol><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="mb mc l"/></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">所有类的所有卷积图层的ratio_num和ratio_sum</figcaption></figure><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/1222b04d07682153925c013cb997b66a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TItaSmqvfbcoxrC7_pDDXA.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">精确度和损耗图</figcaption></figure><blockquote class="me mf mg"><p id="4bc2" class="iq ir mh is b it iu iv iw ix iy iz ja mi jc jd je mj jg jh ji mk jk jl jm jn hb bi translated">我们观察到验证准确率从78.52%上升到78.80%</p><p id="f458" class="iq ir mh is b it iu iv iw ix iy iz ja mi jc jd je mj jg jh ji mk jk jl jm jn hb bi translated">现在在这个模型中，我们可以从表中看到<strong class="is hj">指数10 </strong>现在具有最高的ratio_num和ratio_sum。</p><p id="448a" class="iq ir mh is b it iu iv iw ix iy iz ja mi jc jd je mj jg jh ji mk jk jl jm jn hb bi translated">所以我们将重新定义没有<strong class="is hj">激活_7 </strong>和<strong class="is hj">激活_10 </strong>层的模型，重新训练。</p></blockquote><h2 id="368f" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">6.with_relu_activation_7_10(型号6):</h2><ol class=""><li id="d34e" class="kx ky hi is b it kz ix la jb lb jf lc jj ld jn le lf lg lh bi translated">我们使用激活1、激活2、激活3、激活4、激活5、激活6、激活8和激活9层来训练该模型。请注意，我们已经从该模型中删除了<strong class="is hj">activation _ 7</strong>和<strong class="is hj"> activation_10 </strong>。</li><li id="9e7c" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">在第45个历元中，我们获得了80.56%的最佳验证准确率。</li></ol><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/333b38c9e173700eceb73f423daa036d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KpyKgEkt-MYe4nCbPaJI5A.png"/></div></div><figcaption class="lw lx et er es ly lz bd b be z dx translated">精确度和损耗图</figcaption></figure><blockquote class="me mf mg"><p id="cdb3" class="iq ir mh is b it iu iv iw ix iy iz ja mi jc jd je mj jg jh ji mk jk jl jm jn hb bi translated">总体精度提高了2%，因此这似乎是一种有效的方法。</p></blockquote><p id="3033" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，我们得出这个结果:</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="4c5c" class="jy jz hi ju b fi ka kb l kc kd">+---------------------------+-----------------------------------+<br/><strong class="ju hj">|         model_name        | validation accuracy | time taken  |<br/></strong>+---------------------------+-----------------------------------+<br/>|  without_any_activation   |               64    |  1171.98    |<br/>|  with_relu_activation     |               78.52 |  1300.42    |<br/>|  with_relu_activation_2   |               78.12 |  1305.67    |<br/>|  with_relu_activation_2_3 |               77.55 |  1303.76    |<br/>|  with_relu_activation_7   |               78.80 |  1349.62    |<br/>|  with_relu_activation_7_10|               80.56 |  1368.53    |<br/>+---------------------------+-----------------------------------+</span></pre><blockquote class="me mf mg"><p id="e8bb" class="iq ir mh is b it iu iv iw ix iy iz ja mi jc jd je mj jg jh ji mk jk jl jm jn hb bi translated"><strong class="is hj">负值与正值的幅度比很高的层，我们不应应用ReLU激活。</strong></p></blockquote><p id="30a0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以在其他数据集上测试，让我知道你是否能够提高你的模型的准确性。</p><h2 id="e72b" class="jy jz hi bd ke kf kg kh ki kj kk kl km jb kn ko kp jf kq kr ks jj kt ku kv kw bi translated">5.未来的工作</h2><ol class=""><li id="9152" class="kx ky hi is b it kz ix la jb lb jf lc jj ld jn le lf lg lh bi translated">更好的试探法:这里我们采用了简单的比率，这样我们就可以提出更好的试探法。例如，将标准偏差作为一个决定因素。</li><li id="9685" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">不同的数据集:我们可以在不同的数据集上测试相同的原理，看看我们是否可以推广解决方案。</li><li id="4267" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">我们可以选择其他激活功能，并尝试找到一种启发式方法来帮助我们决定是否要使用激活功能。</li><li id="df69" class="kx ky hi is b it li ix lj jb lk jf ll jj lm jn le lf lg lh bi translated">这是一种贪婪的解决方案，可能不是最佳解决方案。我们需要找到一种有效的方法来达到最佳解决方案。</li></ol></div></div>    
</body>
</html>