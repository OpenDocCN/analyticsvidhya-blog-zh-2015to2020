<html>
<head>
<title>Feature Selection Methods for Data Science (just a few)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学的特征选择方法(仅几个)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-selection-methods-for-data-science-just-a-few-fca3086eb445?source=collection_archive---------2-----------------------#2020-01-31">https://medium.com/analytics-vidhya/feature-selection-methods-for-data-science-just-a-few-fca3086eb445?source=collection_archive---------2-----------------------#2020-01-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d9b9b7b2242c9b7b4db0519cf9b7bb35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VtEDMGhLiENNabmLdNrwBg.jpeg"/></div></div></figure><h2 id="c427" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">为什么特性选择很重要</h2><p id="3743" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">在我们开始之前，让我们看看为什么特性选择应该是你的模型的一部分。特征选择有两个主要原因。首先，更简单的模型(通常)更好。它们更容易理解和扩展，并且从长远来看运行起来更便宜。通过选择要素来减少要素的数量，您就能够识别出模型在减小整个数据集的同时需要良好运行的重要要素。第二，您可能会使用太多的特征导致过度拟合，这意味着您的模型在训练数据集上学习得很好，但在以前没有见过的新数据上表现不佳。</p><p id="b3e1" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">一些基本的特征选择方法可以使用简单的皮尔逊相关或卡方方法，但出于本文的目的，我们将重点关注其他一些方法。</p><h1 id="c3b4" class="ko ir hi bd is kp kq kr iw ks kt ku ja kv kw kx je ky kz la ji lb lc ld jm le bi translated">方法</h1><h2 id="90d4" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">1.<a class="ae lf" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank">递归特征消除</a> (RFE)</h2><p id="1973" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">RFE的目标是“通过递归地考虑越来越小的特征集来选择特征”本质上，这种方法根据原始数量的特征训练模型，并赋予每个特征一个重要性。最不重要的特征被剔除，然后对指定数量的特征重复该过程。请参见图1，了解RFE是如何工作的。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/d2ef670282d858e4c3554aeb7151b208.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*QLJIAU2bT92WN6Mpln-tqQ.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">图1:递归特征消除方法</figcaption></figure><h2 id="cffb" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">例子</h2><p id="89ca" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">我们将查看一个简单模型的sklearn乳腺癌数据集。我的完整代码可以在我的GitHub上通过点击<a class="ae lf" href="https://github.com/svideloc/Feature-Selection-Blog" rel="noopener ugc nofollow" target="_blank">这里</a>找到，但是为了这篇博客，我将只展示部分代码。</p><p id="7430" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">让我们看看没有递归特征选择的逻辑回归的代码。</p><pre class="lh li lj lk fd lp lq lr ls aw lt bi"><span id="016e" class="iq ir hi lq b fi lu lv l lw lx">#No Feature Selection<br/>lr = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', random_state=42, n_jobs=-1, max_iter=500)</span><span id="4235" class="iq ir hi lq b fi ly lv l lw lx">lr.fit(X_train, y_train)<br/>y_guess = lr.predict(X_train)<br/>y_score = lr.predict(X_test)</span></pre><p id="bd24" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">现在让我们看看RFE的情况:</p><pre class="lh li lj lk fd lp lq lr ls aw lt bi"><span id="02e5" class="iq ir hi lq b fi lu lv l lw lx">from sklearn.feature_selection import RFE<br/>rfe = RFE(lr, n_features_to_select=7)<br/>rfe.fit(X_train, y_train)<br/>y_guess = rfe.predict(X_train)<br/>y_score = rfe.predict(X_test)</span></pre><p id="1e09" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">让我们看看这些的结果:</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/9adfbbd81d6b9fb7a37a84eacd3b993e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*duwknIpNPED_txInaNnOVA.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">原始模型与RFE选定特征模型</figcaption></figure><p id="e100" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">从这里你可以看到，我们实际上在RFE版本中得分更高，可能是因为在数据中丢弃了一些噪声。此外，RFE模型仅使用了7个特征，这比原始模型更有效。</p><p id="46ac" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj">好处</strong></p><p id="d532" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">这种方法对于大多数线性类型的模型来说应该工作得很好，并且运行起来相对较快，这是一个额外的好处。RFE的计算复杂度也比我们将看到的下一个特征选择方法要低。</p><p id="1288" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj">弊端</strong></p><p id="4c1e" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">请注意参数' n_reatures_to_select 'RFE不是最聪明的特征选择方法，所以你需要告诉模型你想要选择多少特征。它将运行其消除，直到它使用您指定的功能数量。这意味着你必须调整这个数字一点，看看功能的甜蜜点在哪里，这就是我如何得到7个功能。您还可以为此编写一个for循环来尝试不同数量的特性！</p><p id="d1b1" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">如果你不指定一些功能，RFE将删除一半的功能。这可能会有问题，因为您可能会删除太多或不够的功能。这就是为什么您必须谨慎选择参数的原因。</p><h2 id="4089" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">2.<a class="ae lf" href="http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/" rel="noopener ugc nofollow" target="_blank">顺序特征选择器</a> (SFS)</h2><p id="8209" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">来自文档:SFA的“基于分类器性能，一次删除或添加一个特征，直到达到所需大小的特征子集<em class="ma"> k </em>，k是小于原始维度特征空间的特征数量，d。在下面的图2中，模型首先将每个特征作为自己的模型运行，然后选择特征2作为最佳特征。然后，它将功能2与其他功能配对，并确定功能2和功能3的性能最佳。它将它们分组并再次添加每个特征，并发现2、3和1具有最佳模型。它将以这种方式不断迭代。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/f151fdf4afde88499ac77bda1feb1455.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*GtT1JXyPL7LNHa9EVrtcfg.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">图2:顺序特征选择方法</figcaption></figure><h2 id="be9f" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">例子</h2><p id="2adf" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">再一次，让我们看同一个例子，但是这次我们将使用顺序特征选择。</p><pre class="lh li lj lk fd lp lq lr ls aw lt bi"><span id="3a2b" class="iq ir hi lq b fi lu lv l lw lx"># Sequential Feature Selection<br/>from mlxtend.feature_selection import SequentialFeatureSelector<br/>sfs = SequentialFeatureSelector(lr, k_features='best', forward = False, n_jobs=-1)</span><span id="2ef1" class="iq ir hi lq b fi ly lv l lw lx">sfs.fit(X_train, y_train)</span><span id="2a38" class="iq ir hi lq b fi ly lv l lw lx">features = list(sfs.k_feature_names_)<br/>lr.fit(X_train[features], y_train)<br/>y_score = lr.predict(X_test[features])</span></pre><p id="535f" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">让我们来看看这个方法的结果:</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/66881ffacea4cb74ac7a85146e5319de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*AhoyPCWYNE28NQiCw133Yg.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">原始模型与SFS选定功能模型</figcaption></figure><p id="1945" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">好了，我们排除了7个特征，用SFS得到了一个稍微差一点的分数。然而，我不需要像在RFE那样手动尝试和优化功能的数量，所以我们现在有了一个更具可扩展性的模型。</p><p id="3055" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj">好处</strong></p><p id="ddfe" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">SFS的运行时间比RFE稍长，但易于管理。主要的好处是，这种方法可以自行选择，比RFE更智能。如果你懒，SFS比RFE做得多一点思考。</p><p id="c690" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj">弊端</strong></p><p id="e403" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">用户对这种方法的控制有点少，可能倾向于不选择绝对最佳的特性组合，正如我们在上面的例子中看到的那样。尽管如此，它仍然会很好地减少你的数据集的维数。</p><h2 id="0f7b" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">2.<a class="ae lf" href="http://rasbt.github.io/mlxtend/user_guide/feature_selection/ExhaustiveFeatureSelector/" rel="noopener ugc nofollow" target="_blank">详尽的特征选择器</a> (EFS)</h2><p id="d442" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">详尽的特征选择将是这篇博客中所涉及的三种方法中最健壮的。这是对每个功能子集的强力评估，意味着它会尝试所有可能的功能组合，并选择性能最佳的模型。在图3中，我们有四个特性。然后，EFS使用这些特性尝试每一种可能的组合，在图表中发现特性1、3和4具有最佳模型。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es md"><img src="../Images/4b6ebb88e0e507fbbaf0a08879378c40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*5uszHOfy1CaZz8OOzXNEwA.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">图3:详尽的特性选择方法</figcaption></figure><p id="3084" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj">好处</strong></p><p id="32b9" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">如果你有计算能力，你一定会优化你的功能选择使用详尽的功能选择。然而，由于运行时间的原因，我还没有在实践中使用这种方法。如果您使用一种更简单的要素选择方法来减少要素，然后在数据集中的要素少得多时尝试这种方法，这可能也很有用。</p><p id="c06c" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated"><strong class="jq hj">弊端</strong></p><p id="83a6" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">如前所述，EFS将需要大量的计算能力。对于一个只有四个特征的模型，这个方法需要模型运行15次，如图3所示。随着功能数量的增加，这将会变成数量极大的模型。</p><p id="7100" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">对于我们的30个特征乳腺癌数据集的例子，这意味着模型将必须尝试107，3741，823种不同的组合(2 ⁰-1).这可能不是一个好主意！</p><p id="181d" class="pw-post-body-paragraph jo jp hi jq b jr kj jt ju jv kk jx jy jb kl ka kb jf km kd ke jj kn kg kh ki hb bi translated">请注意，使用此方法时，您可以指定用作参数的最大要素数，但模型仍有许多迭代要运行。</p><h1 id="a4c4" class="ko ir hi bd is kp kq kr iw ks kt ku ja kv kw kx je ky kz la ji lb lc ld jm le bi translated">结论</h1><p id="176a" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki hb bi translated">在数据科学中，对于一个问题，从来没有放之四海而皆准的解决方案，特征选择也是如此。这篇文章中概述的方法提供了一些选项，可能有助于尝试和消除模型中过多的特征，但还有无数其他方法来尝试和减少特征。在调整模型时，结合使用多种方法也是非常合理的。</p></div></div>    
</body>
</html>