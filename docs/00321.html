<html>
<head>
<title>Reinforcement Learning: Introduction to Temporal Difference (TD) Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习:时间差异学习简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/nuts-and-bolts-of-reinforcement-learning-introduction-to-temporal-difference-td-learning-a0624eb3b985?source=collection_archive---------1-----------------------#2019-03-28">https://medium.com/analytics-vidhya/nuts-and-bolts-of-reinforcement-learning-introduction-to-temporal-difference-td-learning-a0624eb3b985?source=collection_archive---------1-----------------------#2019-03-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="df5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当DeepMind提出一种在雅达利游戏上达到超人水平的算法时，Q-learning成为了数据科学中一个家喻户晓的名字。这是强化学习的核心组成部分之一。每当我阅读关于RL的资料时，我经常会碰到Q-learning。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/42664d1da732a70e8184c5c4516d509c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8jLTvno2tQyYdypv.png"/></div></div></figure><p id="a4f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是Q-learning和我们的时间差异学习这个话题有什么关系呢？让我举个例子来直观地说明时间差异学习是怎么回事。</p><p id="b4e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Rajesh计划开车从德里去斋浦尔。在谷歌地图上快速查看显示，他的行程估计为5小时。不幸的是，由于路障，出现了意外的延迟(任何长途旅行的人都会有同感！).拉杰什的预计到达时间现在上升到了5小时30分钟。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jq"><img src="../Images/7b06925f11283c95be9701777fcd7c53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xbpo5TeMVdLo4ZuM.png"/></div></div></figure><p id="55e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">半路上，他找到了一条支路，缩短了他的到达时间。所以总的来说，他从德里到斋浦尔的旅程需要5小时10分钟。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jq"><img src="../Images/071d21bce3e8529c1f71ba719009a78c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MuBemKBMM6lOHhmA.png"/></div></div></figure><p id="fba8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你有没有注意到Rajesh的到达时间是如何根据不同的剧集不断变化和更新的？简而言之，这说明了时间差异学习概念。在本文中，我将向您详细介绍这种算法及其组件，包括Q-learning如何融入其中。我们还将挑选一个案例研究，并用Python来解决它。</p><p id="c541" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是最有趣的强化学习概念之一，所以让我们享受学习的乐趣吧！</p><h1 id="3afa" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">目录</h1><ul class=""><li id="b556" class="kp kq hi ih b ii kr im ks iq kt iu ku iy kv jc kw kx ky kz bi translated">时差学习简介</li><li id="d9d5" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">政策外学习与政策内学习</li><li id="80df" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">Q-learning:非策略时间差异学习</li><li id="6207" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">SARSA:基于策略的时间差异学习</li><li id="f8e5" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">案例研究:使用Python中的Q-learning进行出租车调度</li></ul><h1 id="8a4e" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">时差学习简介</h1><p id="ba52" class="pw-post-body-paragraph if ig hi ih b ii kr ik il im ks io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">我们在引言中发展了一种对时间差异学习的直觉。现在让我们更详细地理解它。</p><p id="cf86" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我的<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/?utm_source=blog&amp;utm_medium=reinforcement-learning-temporal-difference" rel="noopener ugc nofollow" target="_blank">上一篇关于蒙特卡罗学习</a>的文章中，我们学习了当环境的模型动态事先未知时，如何使用它来解决马尔可夫决策过程(MDP)。那么我们为什么不用那个代替TD呢？</p><p id="7f5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗯，虽然蒙特卡罗学习为无模型学习提供了一种有效而简单的方法，但它也有一些局限性。它只能应用于偶尔发生的任务。</p><p id="f44f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个阶段性的任务持续有限的时间。例如，下一盘棋是一个阶段性的任务，你会赢或输。如果一集很长(很多情况下都是这样)，那么我们必须等待很长时间来计算价值函数。</p><p id="35a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">时间差分(TD)学习是一种无模型学习算法，具有两个重要特性:</p><ul class=""><li id="7cc1" class="kp kq hi ih b ii ij im in iq li iu lj iy lk jc kw kx ky kz bi translated">它不需要预先知道模型动力学</li><li id="f8b2" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">它也可以应用于非偶发任务</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es jq"><img src="../Images/7f3d917546f387695ac675c89d0442d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Amo4-Mz8JUZgPkRD.png"/></div></div></figure><p id="9240" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TD学习算法是由伟大的Richard Sutton在1988年提出的。该算法同时考虑了蒙特卡罗方法和动态规划(DP)的优点:</p><ul class=""><li id="0975" class="kp kq hi ih b ii ij im in iq li iu lj iy lk jc kw kx ky kz bi translated">像蒙特卡罗方法一样，它不需要模型动力学，而且</li><li id="8be9" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">和动态规划一样，它不需要等到一集结束的时候才做出价值函数的估计</li></ul><p id="223e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相反，时间差异学习基于先前学习的估计来逼近当前估计。这种方法也称为引导。</p><h1 id="4a76" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">获得TD预测背后的直觉</h1><p id="1cea" class="pw-post-body-paragraph if ig hi ih b ii kr ik il im ks io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">我们试图在时间差异学习中预测状态值，就像我们在蒙特卡罗预测和动态规划预测中所做的那样。在蒙特卡洛预测中，我们通过简单地获取每个状态的平均回报来估计价值函数，而在动态规划和TD学习中，我们通过当前状态来更新先前状态的值。但是TD学习不像DP那样需要环境的模型。</p><p id="08be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们如何做到这一点？TD学习使用称为TD更新规则的东西来更新状态的值:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ll"><img src="../Images/03db325e1249347ab8e546d91ca97bb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*y_sUk0NzZb5gF4dO.png"/></div></div></figure><p id="ecc3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">前一状态值=前一状态值+ learning_rate *(奖励+折扣_因子(当前状态值)-前一状态值)</p><p id="823f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个等式实际上是什么意思？</p><p id="7b21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是<strong class="ih hj">实际报酬(r + Gamma * V(s')) </strong>和期望报酬<strong class="ih hj"> V(s) </strong>之差乘以学习率α。</p><p id="ca52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">学习率意味着什么？</p><p id="616f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">学习率，也称为步长，对收敛很有用。</p><p id="f3f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我们取实际值和预测值之间的差值，这就像一个误差。我们可以称之为<strong class="ih hj"> TD误差</strong>。注意，每次的TD误差是当时估计的误差。因为TD误差取决于下一个状态和下一个奖励，所以实际上直到一个时间步长之后才可用。迭代地，我们将尝试最小化这个误差。</p><h2 id="54a4" class="ln js hi bd jt lo lp lq jx lr ls lt kb iq lu lv kf iu lw lx kj iy ly lz kn ma bi translated">以冰湖为例理解TD预测</h2><p id="265a" class="pw-post-body-paragraph if ig hi ih b ii kr ik il im ks io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">让我们以冰湖为例来理解TD预测。接下来显示的是冰冻的湖泊环境。首先，我们将值函数初始化为0，如V(S)中所有状态的0，如下面的状态-值图所示:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mb"><img src="../Images/b1ecd89df5a92dd2c13b11fca314bc44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*r2KbXF1qbYTcZNhY.png"/></div></div></figure><p id="1eaf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们处于一个起始状态(1，1)，我们采取了一个正确的行动，并移动到下一个状态(1，2)，得到的回报(r)为-0.4。</p><p id="6fda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们如何使用这些信息来更新状态的值呢？回想一下TD更新公式:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ll"><img src="../Images/9cf9785e025d5f00b412c52bb1a40b93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*wlH7nDiya7J12IfQ.png"/></div></div></figure><p id="5ee7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑学习率(α)为0.1，贴现因子()为0.5；我们知道状态(1，1)的值，如在v(s)中，是0，下一个状态(1，2)的值，如在V(s’)中，也是0。我们获得的回报(r)是-0.3。我们在TD规则中替换如下:</p><p id="9b41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">V(s) = 0 + 0.1 [ -0.4 + 0.5 (0)-0]</p><p id="4c60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">V(s) = — 0.04</p><p id="2c86" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们在值表中将状态(1，1)的值更新为-0.04，如下图所示:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mb"><img src="../Images/919cddbc2bb7f3abd1a73b7a9daff4f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*V8SaN65kGWMZfJ7B.png"/></div></div></figure><p id="5cb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们在状态as (1，2)，我们采取一个行动权利，并移动到下一个状态(s’)(1，3)，并收到奖励(r) -0.4。我们现在如何更新state (1，2)的值？我们将替换TD更新等式中的值:</p><p id="36ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">V(s) = 0 + 0.1 [ -0.4 + 0.5(0)-0 ]</p><p id="e50f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">V(s) = -0.04</p><p id="5135" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">继续在值表中将state (1，2)的值更新为-0.04:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mb"><img src="../Images/ba8d5d5cab91150908168cca1d8dba9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xWRebSuIQ_VXHHb8.png"/></div></div></figure><p id="7322" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">跟我到目前为止？我们现在处于状态(1，3)。让我们向左走一步。</p><p id="93f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们再次回到状态(s’)(1，2)，我们得到奖励(r) -0.3。这里，在值表中，状态(1，3)的值是0，下一个状态(1，2)的值是-0.03。现在，我们可以更新state (1，3)的值，如下所示:</p><p id="f146" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">V(s) = 0 +0.1 [ -0.4 + 0.5 (-0.04)-0) ]</p><p id="256b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">V(s) = 0.1[-0.42]</p><p id="f4ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">V(s) = -0.042</p><p id="6f92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你知道现在该做什么。在值表中将state (1，3)的值更新为-0.042:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mb"><img src="../Images/192d989340f43b5e4d185cbce8b8c113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zhegEYDxmIIYHcGZ.png"/></div></div></figure><p id="60af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用TD更新规则以类似的方式更新所有状态的值。总而言之，TD预测算法中涉及的步骤是:</p><ol class=""><li id="6e32" class="kp kq hi ih b ii ij im in iq li iu lj iy lk jc mc kx ky kz bi translated">首先，将V(S)初始化为0或某个任意值</li><li id="a025" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc mc kx ky kz bi translated">然后，开始这一集。对于情节中的每一步，在状态S中执行动作A，并接收奖励R，然后移动到下一个状态(S’)</li><li id="3784" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc mc kx ky kz bi translated">使用TD更新规则更新先前状态的值</li><li id="dad1" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc mc kx ky kz bi translated">重复步骤2和3，直到我们到达终端状态</li></ol><h1 id="0831" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">理解时间差异控制</h1><p id="817a" class="pw-post-body-paragraph if ig hi ih b ii kr ik il im ks io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">在时间差预测中，我们<em class="lm">估计</em>价值函数。在TD控制中，我们<em class="lm">优化</em>价值函数。我们用于TD控制的算法有两种:</p><ul class=""><li id="b763" class="kp kq hi ih b ii ij im in iq li iu lj iy lk jc kw kx ky kz bi translated">非策略学习算法:Q学习</li><li id="abe0" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">基于策略的学习算法:SARSA</li></ul><p id="1ff7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">政策外vs政策内</strong></p><p id="5eca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">政策外学习和政策内学习有什么区别？答案就在他们的名字里:</p><ul class=""><li id="a59b" class="kp kq hi ih b ii ij im in iq li iu lj iy lk jc kw kx ky kz bi translated"><strong class="ih hj">非策略学习:</strong>代理根据从另一个策略采样的经验学习策略π</li><li id="8b25" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated"><strong class="ih hj">基于策略的学习:</strong>代理从相同策略π的经验样本中学习策略π</li></ul><p id="a2b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我用一个例子来说明这一点。假设你作为一名数据科学家加入了一家新公司。在这种情况下，您可以将政策学习等同于工作学习。你会尝试不同的事情，并且只从自己的经历中学习。</p><p id="31d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">政策外学习是指你可以完全了解其他员工的行为。在这种情况下，你要做的就是从员工的经历中学习，而不是重复员工失败的事情。</p><h1 id="ce30" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">q学习</h1><p id="f54d" class="pw-post-body-paragraph if ig hi ih b ii kr ik il im ks io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">Q-learning是一种非常流行和广泛使用的非策略TD控制算法。</p><p id="bfef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Q学习中，我们关注的是状态-动作值对-在状态s中执行动作a的效果。这告诉我们在特定状态下(Q(s，a))动作对代理有多好，而不是只看在那个状态下(V(s))有多好</p><p id="631e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将根据以下等式更新Q值:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es md"><img src="../Images/860d37866248a27a18dfe1141de4298b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*P_N7z06CkdOFmjIF.png"/></div></div></figure><p id="63ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为什么Q-learning被认为是一种非策略技术？这是因为它使用下一个状态𝑠′和贪婪动作𝑎′.的q值来更新其q值换句话说，它估计了假设遵循贪婪政策(<em class="lm">maxQ(</em>s ' a<em class="lm">)</em>)的状态-行动对的回报(总贴现未来报酬)，尽管事实上它并没有遵循贪婪政策！</p><p id="30e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的等式类似于TD预测更新规则，但有细微的区别。以下是Q-learning中涉及的步骤(我想让你注意到这里的区别):</p><ol class=""><li id="44cd" class="kp kq hi ih b ii ij im in iq li iu lj iy lk jc mc kx ky kz bi translated">首先，将Q函数初始化为某个任意值</li><li id="c73d" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc mc kx ky kz bi translated">使用epsilon-greedy策略()从一个状态采取一个动作，并将其移动到新的状态</li><li id="4700" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc mc kx ky kz bi translated">通过遵循更新规则来更新先前状态的Q值</li><li id="42c7" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc mc kx ky kz bi translated">重复步骤2和3，直到我们到达终端状态</li></ol><p id="5558" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们回到冰湖的例子。假设我们处于状态(3，2)，有两个动作(左和右)。请参考下图:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mb"><img src="../Images/4dba194109a64ea15a7c84f5953f50d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6weIJY794VKFPYxB.png"/></div></div></figure><p id="259d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用Q-learning中的ε-贪婪策略选择一个动作。<strong class="ih hj">我们要么探索一个概率为ε的新动作，要么选择概率为1-ε的最佳动作。</strong>假设我们选择一个概率ε，并选择一个特定的动作(向下移动):</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es me"><img src="../Images/2829e57dbe01ae5ecb0e8c671b27daf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mWNxAUlohAB1e6Ro.png"/></div></div></figure><p id="fa3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经在状态(3，2)中执行了向下的动作，并且使用ε-贪婪策略到达了新的状态(4，2)。我们如何使用更新规则更新先前状态(3，2)的值？这很简单！</p><p id="a428" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑α为0.1，贴现因子为1，奖励为0.4:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es md"><img src="../Images/c14a04cd95d901f71c787937665716fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*l4uyZjZBZLsPDk-u.png"/></div></div></figure><p id="85d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lm"> Q( (3，2) down) = Q( (3，2) down ) + 0.1 ( 0.4 + 1 max [Q( (4，2) action) ]- Q( (3，2)，down) </em></p><p id="4a68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以说，在Q表中，具有向下动作的状态(3，2)的值是0.6。对于状态(4，2)，max Q ( (4，2)，action)是多少？</p><p id="403d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经研究了三个动作(向上、向下和向右)，所以我们将只根据这些动作取最大值。这里不涉及探索——这是一个直截了当的贪婪政策。</p><p id="33cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于前面的Q表，我们可以插入值:</p><p id="0e78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Q( (3，2)，down) = 0.6 + 0.1 ( 0.4 + 1 * max [0.2，0.4，0.6] — 0.6)</p><p id="4843" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Q( (3，2)，down) = 0.64</p><p id="71a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们将Q的值((3，2)，down)更新为0.64。</p><p id="844b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们处于(4，2)状态。我们应该采取什么行动？基于ε-贪婪策略，我们可以探索概率为ε的新动作，或者选择概率为1-ε的最佳动作。假设我们选择了后者。因此，在(4，2)中，行动权有一个最大值，这就是我们要选择的:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es me"><img src="../Images/4b3ccfd2cdba2ab71fc0fc5c8167c580.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LhYnE5qxA6Iqb2h9.png"/></div></div></figure><p id="8d29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对，我们已经搬到了州(4，3)。到目前为止，事情进展顺利。但是等等——我们如何更新先前状态的值呢？</p><p id="4964" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Q( (4，2)，right) = Q( (4，2)，right ) + 0.1 ( 0.4 + 1*max [Q( (4，3) action) ]- Q( (4，2)，right)</p><p id="ae15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你看下面的Q表，我们只研究了状态(4，3)的两个动作(向上和向下)。因此，我们将仅基于这些动作来取最大值(这里我们不会执行ε-贪婪策略；我们只需选择具有最大价值的行动):</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es me"><img src="../Images/330558b311cf721ad057c022959746e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uNLdK4kffZgCk2cR.png"/></div></div></figure><p id="a771" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Q ( (4，2)，right) = Q((4，2)，right) + 0.1 (0.4 + 1 max [ (Q (4，3)，up)，(Q (4，3)，down) ] — Q ((4，2)，right)</p><p id="71a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Q ( (4，2)，右)= 0.6 + 0.1 (0.4 + 1 max [ 0.2，0.4] — 0.8)</p><p id="fab6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">= 0.6 + 0.1 (0.4 + 1(0.4) — 0.6)</p><p id="a089" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">= 0.62</p><p id="98d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">厉害！我们将状态Q的值((4，2)，右)更新为0.62。</p><blockquote class="mf mg mh"><p id="7f31" class="if ig lm ih b ii ij ik il im in io ip mi ir is it mj iv iw ix mk iz ja jb jc hb bi translated">这就是我们如何在Q-learning中获得状态-动作值。我们使用ε-贪婪策略来决定采取什么行动，并在更新Q值时简单地选择最大行动。</p></blockquote><h1 id="f725" class="jr js hi bd jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko bi translated">萨尔萨</h1><p id="a178" class="pw-post-body-paragraph if ig hi ih b ii kr ik il im ks io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">状态-动作-奖励-状态-动作(SARSA)是一种基于策略的TD控制算法。类似于我们在Q-learning中所做的，我们<strong class="ih hj">关注状态-动作值，而不是状态-值对。</strong>在SARSA中，我们根据以下更新规则更新Q值:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ml"><img src="../Images/04e389fce4977aca7be22378883e9434.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2Zs8APsaD1r6fU-n.png"/></div></div></figure><p id="eafd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可能已经注意到<strong class="ih hj">没有最大Q(s '，a' </strong>)(不像Q-learning)。这里，简单来说就是Q(s '，a ')。当您完成以下SARSA步骤时，您会明白这一点:</p><ol class=""><li id="17f9" class="kp kq hi ih b ii ij im in iq li iu lj iy lk jc mc kx ky kz bi translated">首先，将Q值初始化为一些任意值</li><li id="0ee8" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc mc kx ky kz bi translated">通过ε-greedy策略()选择一个操作，并从一个状态转移到另一个状态</li><li id="40f4" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc mc kx ky kz bi translated">通过遵循更新规则来更新前一状态中的Q值，其中a’是由ε-贪婪策略选择的动作()</li><li id="c115" class="kp kq hi ih b ii la im lb iq lc iu ld iy le jc mc kx ky kz bi translated">现在我们将一步一步地理解这个算法</li></ol><p id="2146" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑同一个冰湖的例子。假设我们处于状态(4，2)。我们根据ε-贪婪策略决定行动。假设我们使用概率1—ε并选择最佳行动(向右移动):</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es me"><img src="../Images/1a9ca68452bc8c862d41977d3606689d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xd5fAbden4nqLYEB.png"/></div></div></figure><p id="9b6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在州(4，3)着陆。我们如何更新先前状态(4，2)的值？我将调出上面的等式并代入数值。让我们考虑α为0.1，回报为0.4，贴现因子为1:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ml"><img src="../Images/eb590b49a7429d7535e6b30567359f67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5VRE7H-JMPuh2kdh.png"/></div></div></figure><p id="593a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Q( (4，2)，right) = Q( (4，2)，right) + 0.1 ( 0.4 + 1 *Q( (4，3)，action)) — Q((4，2)，right)</p><p id="3ac7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们如何选择<em class="lm"> Q ((4，3)，action) </em>的值？我们不能像在Q-learning中那样只拿起<em class="lm"> max ( Q(4，3)，action) </em>。在SARSA，我们使用ε贪婪策略。看看我下面展示的Q表。在状态(4，3)中，我们探讨了两个动作:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es me"><img src="../Images/8cc056cbed234a395bde1dfec09e7e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*w8MwOjBdfXOzT4MP.png"/></div></div></figure><p id="0c21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们要么以概率ε探索，要么以概率1-ε利用。假设我们选择了前一个选项，并探索了一个新的动作(向右移动):</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es me"><img src="../Images/0795dd86ca1350a89a7a439630d9b0fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*I9MdgjZ4uD-6Gcdr.png"/></div></div></figure><p id="e04b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Q ( (4，2)，右)= Q((4，2)，右)+ 0.1 (0.4 + 1 (Q (4，3)，右)— Q ((4，2)，右)</p><p id="f5fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Q ( (4，2)，right) = 0.6 + 0.1 (0.4 + 1(0.7) — 0.6)</p><p id="2e40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Q ( (4，2)，右)= 0.65</p><blockquote class="mf mg mh"><p id="2337" class="if ig lm ih b ii ij ik il im in io ip mi ir is it mj iv iw ix mk iz ja jb jc hb bi translated">这就是我们如何在SARSA中获得状态-动作值。我们使用ε-贪婪策略采取行动，并选择使用ε-贪婪策略更新Q值的行动。</p></blockquote><p id="7d77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在来看看实际情况，在<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/" rel="noopener ugc nofollow" target="_blank"> Analytics Vidhya博客</a>上有一个很棒的使用Q-Learning解决出租车调度的python案例研究。看看这个。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mm"><img src="../Images/066f25d363f7ff1ad6c967df48bf7e29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/0*jluhxenTxzBOz3M9.png"/></div></div></figure><p id="6b3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相关文章</p></div><div class="ab cl mn mo gp mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="hb hc hd he hf"><p id="154d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="lm">原载于2019年3月28日</em><a class="ae jd" href="https://www.analyticsvidhya.com/blog/2019/03/reinforcement-learning-temporal-difference-learning/" rel="noopener ugc nofollow" target="_blank"><em class="lm">https://www.analyticsvidhya.com</em></a><em class="lm">。</em></p></div></div>    
</body>
</html>