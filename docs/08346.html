<html>
<head>
<title>Vanishing and Exploding Gradient Problems</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">消失和爆炸梯度问题</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/vanishing-and-exploding-gradient-problems-c94087c2e911?source=collection_archive---------9-----------------------#2020-07-26">https://medium.com/analytics-vidhya/vanishing-and-exploding-gradient-problems-c94087c2e911?source=collection_archive---------9-----------------------#2020-07-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4e0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated">训练超深层神经网络的一个问题是，梯度会消失，呈爆炸式增长。(即，当训练非常深的神经网络时，有时导数变得非常非常小或非常非常大，这使得训练困难)。在这个博客中，我们将通过两个消失和爆炸梯度的细节。</p><h1 id="6d3a" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated"><strong class="ak">消失梯度问题:</strong></h1><figure class="kl km kn ko fd kp er es paragraph-image"><div class="er es kk"><img src="../Images/00e332d0fc1ad25f49036f7a8f0d8052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*dZ8A3ezzzn1q-Ybpikqxsw.gif"/></div></figure><blockquote class="ks kt ku"><p id="09a0" class="if ig kv ih b ii ij ik il im in io ip kw ir is it kx iv iw ix ky iz ja jb jc hb bi translated">当损耗函数相对于网络的早期层中的参数的梯度非常小时，它学习慢，并且由于许多梯度非常小，它们对学习贡献不大，并且可能导致较差的性能。</p></blockquote><p id="7cd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在神经网络中，权重在输入层随机初始化。将输入要素的总和乘以权重以及偏差传递到隐藏层。它与输出权重相乘，并被传递到激活函数，以输出它给出预测输出的层。然后，计算成本函数(即实际输出和预测输出之间的差异)。</p><figure class="kl km kn ko fd kp er es paragraph-image"><div class="er es kz"><img src="../Images/221b89a18508dc16a5fa69bb0f41b383.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/1*QHm9nfNYqApecv36vtPtLg.gif"/></div></figure><p id="50e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了最小化这种损失，我们向网络反向传播并更新新的权重。</p><p id="cc00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在早期，最常用的激活函数是sigmoid激活函数。</p><figure class="kl km kn ko fd kp er es paragraph-image"><div class="er es la"><img src="../Images/297ca42c120fc7b66b0f3fcf9f94f669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*RJ2k1KsoraEsEKeoO7r4hQ.png"/></div></figure><p id="1a10" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在sigmoid中，值在0和1之间转换，它们的导数在0和0.25之间。权重上升的公式是，</p><figure class="kl km kn ko fd kp er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lb"><img src="../Images/d656bb67cb6f2f0ec7ea4637b6114b27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vxW9Wwdx7ZsfJKXX6qKtyA.png"/></div></div></figure><p id="8b4c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">学习率必须非常小，以使模型逐渐学习并最终收敛到全局最小值。</p><p id="3d19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随着神经网络层数的增加，导数不断减少。因此，更多层的加入将导致几乎为零的导数</p><figure class="kl km kn ko fd kp er es paragraph-image"><div class="er es lg"><img src="../Images/d5a1a4a70da6d5e5fe2d8dc5bd30b310.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*8a7-3zbmXI3vCLndlAKauw.png"/></div></figure><p id="7891" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果旧权重近似等于新权重，则不能进一步反向传播，因为导数为0。这就是为什么sigmoid不再用作隐藏层中的激活函数的原因。激活函数tanh也没有被使用，因为它的导数介于0和1之间。因此，这也导致了消失梯度问题。</p><h1 id="e80b" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated"><strong class="ak">消失梯度问题的解:</strong></h1><ul class=""><li id="9784" class="lh li hi ih b ii lj im lk iq ll iu lm iy ln jc lo lp lq lr bi translated">最简单的解决方案是使用其他激活函数，如ReLU、leakerrelu。这种激活仅在一个方向上饱和，因此对梯度的消失更有弹性。</li></ul><p id="a2b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其他解决办法是，</p><ul class=""><li id="ff43" class="lh li hi ih b ii ij im in iq ls iu lt iy lu jc lo lp lq lr bi translated">使用多层层次结构</li><li id="55e4" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated">使用长期短期记忆(LSTM)网络</li><li id="6a44" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated">使用更快的硬件</li><li id="44f6" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated">使用剩余网络(ResNets)</li><li id="2348" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated">权重搜索的遗传算法</li></ul><h1 id="ddf7" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated"><strong class="ak">爆发梯度问题:</strong></h1><figure class="kl km kn ko fd kp er es paragraph-image"><div class="er es ma"><img src="../Images/a9f1969ddbcfc7dc30d1be8565a73bf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*dtk5MwFgW3P8f33f2FDE7Q.gif"/></div></figure><blockquote class="ks kt ku"><p id="d53b" class="if ig kv ih b ii ij ik il im in io ip kw ir is it kx iv iw ix ky iz ja jb jc hb bi translated">当大的误差梯度累积并在训练期间导致神经网络模型权重的非常大的更新时，爆发梯度是一个问题。梯度计算训练神经网络期间的方向和幅度，并用于以正确的量在正确的方向上教导网络权重。当存在误差梯度时，部件的爆炸可能呈指数增长。</p></blockquote><p id="ca8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当较大的误差梯度累积时，模型可能变得不稳定，并且无法从训练数据中学习。在极端情况下，权重值可能变得过大，以至于溢出并导致NaN值。</p><h1 id="1d28" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated"><strong class="ak">怎么知道？</strong></h1><p id="2ff6" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">在网络训练期间，您可以使用一些细微的迹象来确定模型是否遭受爆炸梯度</p><ul class=""><li id="b570" class="lh li hi ih b ii ij im in iq ls iu lt iy lu jc lo lp lq lr bi translated">该模型在训练数据上学习不多，因此导致了较差的损失。</li><li id="8693" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated">该模型不稳定，导致每次更新的损失变化很大。</li><li id="00f9" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated">过了一段时间，南在训练中失去了模型。</li></ul><p id="7c14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有一些迹象可以用来确认你有爆炸梯度，</p><ul class=""><li id="4ad3" class="lh li hi ih b ii ij im in iq ls iu lt iy lu jc lo lp lq lr bi translated">模型重量在训练期间去NaN。</li><li id="3e21" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated">在训练期间，模型权重呈指数增长，变得非常大。</li><li id="5858" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated">误差梯度值总是大于1。</li></ul><h1 id="66d8" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated"><strong class="ak">如何解决这个问题？</strong></h1><p id="43a7" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">有许多方法可以修复爆炸渐变，但是一些最好的方法是，</p><ul class=""><li id="9f73" class="lh li hi ih b ii ij im in iq ls iu lt iy lu jc lo lp lq lr bi translated">使用LSTM网络</li><li id="142d" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated">使用渐变剪辑</li><li id="d4ab" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated">使用正则化(如L2范数)</li><li id="c92c" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated">重新设计神经网络</li></ul><h1 id="f9b9" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated"><strong class="ak">结论:</strong></h1><p id="01a2" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">在这篇文章中，我们讨论了在深度神经网络的训练过程中出现的消失和爆炸梯度问题，并且还公开了一些以有效方式解决这些问题的方法。</p><h1 id="f3fd" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated"><strong class="ak">参考文献:</strong></h1><ul class=""><li id="3e6c" class="lh li hi ih b ii lj im lk iq ll iu lm iy ln jc lo lp lq lr bi translated"><a class="ae me" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Vanishing_gradient_problem</a></li><li id="bead" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated"><a class="ae me" href="http://www.cs.toronto.edu/~rgrosse/courses/csc321_2017/readings/L15%20Exploding%20and%20Vanishing%20Gradients.pdf" rel="noopener ugc nofollow" target="_blank">http://www . cs . Toronto . edu/~ rgrosse/courses/CSC 321 _ 2017/readings/L15 % 20 expanding % 20 and % 20 vanishing % 20 gradients . pdf</a></li><li id="0f92" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated"><a class="ae me" href="https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/slides/cs224n-2019-lecture07-fancy-rnn.pdf" rel="noopener ugc nofollow" target="_blank">https://web . Stanford . edu/class/archive/cs/cs 224n/cs 224n . 1194/slides/cs 224n-2019-lecture 07-fancy-rnn . pdf</a></li><li id="e9dd" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated"><a class="ae me" href="https://machinelearningmastery.com/exploding-gradients-in-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/expanding-gradients-in-neural-networks/</a></li><li id="2fd5" class="lh li hi ih b ii lv im lw iq lx iu ly iy lz jc lo lp lq lr bi translated"><a class="ae me" href="https://neptune.ai/blog/understanding-gradient-clipping-and-how-it-can-fix-exploding-gradients-problem" rel="noopener ugc nofollow" target="_blank">https://Neptune . ai/blog/understanding-gradient-clipping-and-how-it-can-fix-explosing-gradients-problem</a></li></ul></div></div>    
</body>
</html>