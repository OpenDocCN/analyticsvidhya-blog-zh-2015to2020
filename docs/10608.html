<html>
<head>
<title>Traditional Text Vectorization Techniques in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的传统文本矢量化技术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/traditional-text-vectorization-techniques-in-nlp-4e99218e7efe?source=collection_archive---------11-----------------------#2020-10-26">https://medium.com/analytics-vidhya/traditional-text-vectorization-techniques-in-nlp-4e99218e7efe?source=collection_archive---------11-----------------------#2020-10-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/29d43604216ef02097173d19185d6e27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_NOSRORFvtaAUcfb02kKXg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">作者图片(男性，马尔代夫)</figcaption></figure><blockquote class="iu"><p id="5e40" class="iv iw hi bd ix iy iz ja jb jc jd je dx translated"><em class="jf">矢量化是</em>将来自数据集的词汇单词或记号映射到相应的实数向量。这些向量被用作机器学习(ML)模型的输入。<em class="jf">现在，最近的单词嵌入方法被用于执行大多数下游 NLP 任务。在这篇文章中，让我们看看文字嵌入时代之前的文本矢量化方法。</em></p></blockquote><h1 id="e46d" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd bi translated"><strong class="ak">基于统计的矢量化方法</strong></h1><p id="d061" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la je hb bi translated">在单词嵌入时代之前，基于统计单词矢量化的方法，如计数单词共现、加权矩阵，被用来从文本中提取特征，以便稍后用作机器学习算法的输入。(特尼博士和潘特尔出版社，2010 年)。</p><h1 id="1639" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr lb jt ju jv lc jx jy jz ld kb kc kd bi translated"><strong class="ak"> 1。一键编码</strong></h1><p id="a668" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la je hb bi translated">表 1。通过在向量的其他位置设置值为 1、其余为 0 的唯一标记，提供了一种表示词汇表中每个唯一单词的方法。</p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es le"><img src="../Images/cf8447012500315183b271d41de7b368.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*z62zIx0NUV4lwzbc4hx6vw.png"/></div></figure><p id="9f8e" class="pw-post-body-paragraph ke kf hi kg b kh lj kj kk kl lk kn ko kp ll kr ks kt lm kv kw kx ln kz la je hb bi translated"><strong class="kg hj"> <em class="lo">例</em></strong><em class="lo"><br/></em><strong class="kg hj"><em class="lo">差了。1 </em> </strong> <em class="lo">:他们在踢足球。<br/> </em> <strong class="kg hj"> <em class="lo">派。2 </em> </strong> <em class="lo">:他们在打板球。<br/> </em> <strong class="kg hj"> <em class="lo"> Vocab。</em> </strong> <em class="lo">:【他们，正在，玩，足球，板球】</em></p><p id="1ad4" class="pw-post-body-paragraph ke kf hi kg b kh lj kj kk kl lk kn ko kp ll kr ks kt lm kv kw kx ln kz la je hb bi translated">向量大小的缺点等于计算词汇表中的唯一单词。1-热编码遗漏了单词之间的关系，并且不传达关于上下文的信息。</p><h1 id="9c71" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr lb jt ju jv lc jx jy jz ld kb kc kd bi translated"><strong class="ak"> 2。文字袋(蝴蝶结)</strong></h1><p id="816a" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la je hb bi translated">BoW 是一种将文本内容转换成数字特征向量的矢量化技术(P. D. Turney。2002).BoW 模型保存关于它出现的文档的字数，这里每个向量充当 ML 模型的特征列。表二。演示每个文档的功能示例。</p><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/7be108be208681b1a231a60b048babe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*rtsc3GfEQNMjv7nXGtEJeg.png"/></div></figure><p id="3469" class="pw-post-body-paragraph ke kf hi kg b kh lj kj kk kl lk kn ko kp ll kr ks kt lm kv kw kx ln kz la je hb bi translated"><strong class="kg hj"> <em class="lo">举例</em></strong><em class="lo"><br/></em><strong class="kg hj"><em class="lo">D1</em></strong><em class="lo">:他们在踢足球。<br/></em><strong class="kg hj"><em class="lo">D2</em></strong><em class="lo">:他们在打板球。</em></p><p id="7125" class="pw-post-body-paragraph ke kf hi kg b kh lj kj kk kl lk kn ko kp ll kr ks kt lm kv kw kx ln kz la je hb bi translated">BoW 的缺点是它不保留单词顺序，并且不允许对下游的 NLP 任务进行有用的推断。</p><h1 id="25bb" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr lb jt ju jv lc jx jy jz ld kb kc kd bi translated"><strong class="ak"> 3。n-gram </strong></h1><p id="e7a6" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la je hb bi translated">n-grams 考虑文本中 n 个单词的序列；其中 n 是(1，2，3..)例 1 克，2 克。对于令牌对。与 BoW 不同，n-gram 保持词序。</p><p id="5b58" class="pw-post-body-paragraph ke kf hi kg b kh lj kj kk kl lk kn ko kp ll kr ks kt lm kv kw kx ln kz la je hb bi translated"><strong class="kg hj"> <em class="lo">例题</em> </strong> <em class="lo">:一个游泳运动员在游泳池里游泳。<br/>单字母(1-gram): A，游泳者，is，游泳，在，游泳，游泳池…… <br/>双字母(2-gram): A，游泳者，游泳者，正在游泳，正在游泳中…… <br/>三字母(3-gram):A，游泳者，游泳者正在游泳，正在游泳中…… </em></p><p id="3ee1" class="pw-post-body-paragraph ke kf hi kg b kh lj kj kk kl lk kn ko kp ll kr ks kt lm kv kw kx ln kz la je hb bi translated">n-gram 的缺点是特征太多。特征集变得过于稀疏，并且计算量很大。</p><h1 id="c6dd" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr lb jt ju jv lc jx jy jz ld kb kc kd bi translated"><strong class="ak"> 4。术语频率-逆文档频率(TF-idf) </strong></h1><p id="a271" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la je hb bi translated">TF-idf 对罕见事件给予更大的权重，而对预期事件给予较小的权重。TF- idf 惩罚在文档中频繁出现的频繁出现的单词，如“the”、“is”，但对不太频繁或罕见的单词赋予更大的权重。</p><p id="faa0" class="pw-post-body-paragraph ke kf hi kg b kh lj kj kk kl lk kn ko kp ll kr ks kt lm kv kw kx ln kz la je hb bi translated"><strong class="kg hj">公式<br/></strong></p><p id="b1e1" class="pw-post-body-paragraph ke kf hi kg b kh lj kj kk kl lk kn ko kp ll kr ks kt lm kv kw kx ln kz la je hb bi translated">一个单词的 TF×IDF 的乘积表示在文档中找到该标记(t)的频率，以及该标记对于整个文档集来说有多独特。</p><h1 id="f736" class="jg jh hi bd ji jj jk jl jm jn jo jp jq jr lb jt ju jv lc jx jy jz ld kb kc kd bi translated"><strong class="ak"> 5。逐点互信息(PMI) </strong></h1><p id="37a1" class="pw-post-body-paragraph ke kf hi kg b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la je hb bi translated">PMI 通常识别文本中的配对模式(Turney，P. D .，&amp; Pantel，P. 2010)。<strong class="kg hj"> <em class="lo">公式</em> </strong> <em class="lo">:出现次数(word1 和 word2) /计数(word1) *计数(word2)) </em></p><p id="2e01" class="pw-post-body-paragraph ke kf hi kg b kh lj kj kk kl lk kn ko kp ll kr ks kt lm kv kw kx ln kz la je hb bi translated"><strong class="kg hj">举例:<em class="lo"> </em> </strong> <em class="lo">假设，</em> <strong class="kg hj"> <em class="lo"> </em> </strong> <em class="lo">在一个文档中 word1(汽车)和 word2(驾驶)出现的概率可能较低。相反，出现概率比共现概率高得多的一对单词会得到小的 PMI 分数，如单词 1 (that)和单词 2 (is)。</em></p></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><p id="3a4d" class="pw-post-body-paragraph ke kf hi kg b kh lj kj kk kl lk kn ko kp ll kr ks kt lm kv kw kx ln kz la je hb bi translated">我们在这里看到的所有这些方法都存在向量稀疏问题，因此不能处理复杂的单词关系，也不能对长文本序列建模。在下一篇文章中，我们将尝试研究新时代的文本矢量化技术。</p></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><p id="21a9" class="pw-post-body-paragraph ke kf hi kg b kh lj kj kk kl lk kn ko kp ll kr ks kt lm kv kw kx ln kz la je hb bi translated"><strong class="kg hj">更新链接至:</strong> <a class="ae lx" href="https://saurabhk30.medium.com/word-embedding-new-age-text-vectorization-in-nlp-3a2db1db2f5b" rel="noopener">单词嵌入:NLP 中的新时代文本矢量化</a></p></div></div>    
</body>
</html>