<html>
<head>
<title>Deep Learning — Hyperparameter Tuning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习—超参数调整</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/hyperparameters-f7f112d93941?source=collection_archive---------13-----------------------#2019-12-23">https://medium.com/analytics-vidhya/hyperparameters-f7f112d93941?source=collection_archive---------13-----------------------#2019-12-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7a3c8c4eb1ddc1e68111a99ef83d815d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2A7cOl1aB8WSVUGAReTSFA.jpeg"/></div></div></figure><p id="4d90" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">超参数</strong>是一个<em class="jo">静态</em>参数或变量，需要在对数据应用算法之前赋值。例如，像<strong class="is hj">学习率、时期等参数</strong>。是在训练模型之前设置的。</p><p id="2ca0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">优化超参数:</strong>这些参数与优化过程相关，如梯度下降(学习率)、训练过程、小批量等。</p><p id="810e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">模型超参数:</strong>这些参数与模型有关，如隐含层数或每层神经元数等。</p><h1 id="8807" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">学习率</h1><p id="7b24" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">它是所有超参数中最重要的。即使我们使用预先训练的模型，我们也应该尝试学习率的多个值。最常用的<strong class="is hj">学习率有0.1，0.01，0.001，0.0001，0.00001等。</strong></p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ks"><img src="../Images/057a029a4d8f28bf37c64e9d0f5c6f36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1wboF3ig3DNtZ1alxuEKxw.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><a class="ae lb" href="https://datascience.stackexchange.com/questions/50948/gradient-descent" rel="noopener ugc nofollow" target="_blank">学习率</a></figcaption></figure><p id="1829" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">学习率的<strong class="is hj">大值</strong>倾向于<strong class="is hj">超过梯度值，使得难以将权重收敛到全局最小值。</strong></p><p id="136d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">学习率的<strong class="is hj">小值</strong>使得向<strong class="is hj">全局最小值的进展非常缓慢，从验证和训练损失中可以认识到</strong>。</p><p id="9ccf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">学习率的一个<strong class="is hj">最优值</strong>会导致全局最小，这可以通过<strong class="is hj">不断减少损失来看。</strong></p><p id="6302" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">学习率衰减:</strong>有时仅保持一个学习率可能无法帮助我们达到全局最小值，因此在一定数量的历元之后改变学习率的值，使得如果梯度停留在局部最小值时发生收敛。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/80016c503e37fcf580ec319308b87c13.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*lh7R5nWJKln2bzSQbCP7fg.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><a class="ae lb" href="https://cs231n.github.io/neural-networks-3/" rel="noopener ugc nofollow" target="_blank">学习率衰减</a></figcaption></figure><p id="b514" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">自适应学习率:</strong>有时理解问题并相应地改变学习率是至关重要的，比如增加或减少学习率。像Adam Optimizer和Adagrad Optimizer这样的算法。</p><p id="94ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">指数衰减:</strong><a class="ae lb" href="https://www.tensorflow.org/api_docs/python/tf/train/exponential_decay" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/train/指数衰减</a></p><p id="1d92" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> Adam优化器:</strong><a class="ae lb" href="https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/train/AdamOptimizer</a></p><p id="4b55" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">Adagrad Optimizer:</strong><a class="ae lb" href="https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/train/Adagrad Optimizer</a></p><h1 id="809e" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">迷你批次大小:</h1><p id="3799" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">它是深度学习中最常调整的参数之一。如果我们有1000个记录用于训练模型，那么我们可以有三组不同的小批量大小。</p><p id="7765" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第一个</strong>:如果我们保持<strong class="is hj"> <em class="jo"> Minibatch size = 1 </em> </strong>，那么反向传播后每条记录的权重都会更新。称之为<strong class="is hj"> <em class="jo">随机分批梯度下降。</em>T13】</strong></p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/48ca01cddf89f78e9e94a0cb34b3ecab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9calCrrqS9opiytuA--7AA.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><a class="ae lb" href="https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network" rel="noopener ugc nofollow" target="_blank">批量大小</a></figcaption></figure><p id="a8c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第二个</strong>:如果我们保持<strong class="is hj"><em class="jo">mini batch Size =</em></strong>数据集中的记录数，那么在所有记录通过正向传播后，权重被更新。称之为<strong class="is hj"> <em class="jo">批量梯度下降。</em>T25】</strong></p><p id="0367" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第三个</strong>:如果我们保持<strong class="is hj"> <em class="jo"> Minibatch Size =值在1到记录总数</em> </strong>之间，那么在所有设定的记录数通过正向传播后，权重被更新。称之为<strong class="is hj"> <em class="jo">小批量梯度下降。</em> </strong></p><p id="e772" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最常用的<strong class="is hj"> <em class="jo">迷你批次尺寸值</em> </strong>为<strong class="is hj"> 32、64、128、256。</strong>大于256的值需要更多的内存和计算效率。</p><p id="862c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">【https://arxiv.org/abs/1606.02228】批量大小如何影响模型性能:<a class="ae lb" href="https://arxiv.org/abs/1606.02228" rel="noopener ugc nofollow" target="_blank"/></p><p id="abd8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">stack exchange</strong>:<a class="ae lb" href="https://stats.stackexchange.com/questions/49528/batch-gradient-descent-versus-stochastic-gradient-descent" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/49528/batch-gradient-descent-vs-random-gradient-descent</a></p><p id="de02" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">BGD vs SGD</strong>:<a class="ae lb" href="https://towardsdatascience.com/difference-between-batch-gradient-descent-and-stochastic-gradient-descent-1187f1291aa1" rel="noopener" target="_blank">https://towards data science . com/difference-between-batch-gradient-descent-and-random-gradient-descent-1187 f 1291 aa 1</a></p><h1 id="26bd" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">迭代次数</h1><p id="2888" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">迭代或历元的数量可以基于验证误差来决定，只要验证误差保持减小，我们就可以假设我们的模型正在积极地学习和更新权重。有一种技术叫做<strong class="is hj">提前停止</strong>，这有助于确定迭代次数。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/a366e82e69df21714a5e6a3903fe51ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yILMZ4gelBDqe9z5zwRhjA.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">提前停止</figcaption></figure><p id="a4e9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">验证监视器</strong>:<a class="ae lb" href="https://www.tensorflow.org/get_started/monitors#early_stopping_with_validationmonitor" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/get _ started/monitors # early _ stopping _ with _ Validation Monitor</a></p><pre class="kt ku kv kw fd lf lg lh li aw lj bi"><span id="fac9" class="lk jq hi lg b fi ll lm l ln lo">validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(<br/>      test_set.data,<br/>      test_set.target,<br/>      every_n_steps=50,<br/>      metrics=validation_metrics,<br/>      early_stopping_metric="loss",<br/>      early_stopping_metric_minimize=True,<br/>      early_stopping_rounds=200)</span></pre><p id="2a6c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后一个参数指示ValidationMonitor，如果在200步(轮)的训练中损失没有减少，它应该停止训练过程。</p><p id="5066" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">会话运行钩子:它是tf.train的一个不断发展的部分，向前看似乎是实现早期停止的合适地方。</p><pre class="kt ku kv kw fd lf lg lh li aw lj bi"><span id="98ff" class="lk jq hi lg b fi ll lm l ln lo"><a class="ae lb" href="https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/train/SessionRunHook</a></span></pre><p id="d4e6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> StopAtStepHook </strong>:在一定步数后请求停止训练的监视器。</p><pre class="kt ku kv kw fd lf lg lh li aw lj bi"><span id="4805" class="lk jq hi lg b fi ll lm l ln lo"><a class="ae lb" href="https://www.tensorflow.org/api_docs/python/tf/train/StopAtStepHook" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/train/StopAtStepHook</a></span></pre><p id="7743" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> NanTensorHook </strong>:监控丢失的监视器，如果遇到NaN丢失，则停止训练。</p><pre class="kt ku kv kw fd lf lg lh li aw lj bi"><span id="a538" class="lk jq hi lg b fi ll lm l ln lo"><a class="ae lb" href="https://www.tensorflow.org/api_docs/python/tf/train/NanTensorHook" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/train/NanTensorHook</a></span></pre><h1 id="e247" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">隐藏单元/层的数量</h1><p id="c4e0" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">高度神秘的参数决定了隐藏单元和层的数量。我们试图在深度学习中实现的是在特征和目标之间建立一个复杂的映射函数。<strong class="is hj">开发复杂的函数，其复杂度与隐藏单元的数量成正比，隐藏单元越大意味着函数的复杂度越大。</strong>需要注意的一点是，如果我们创建的模型过于复杂，那么它会过度拟合训练数据，这可以从训练时的验证错误中看出，在这种情况下，我们应该减少隐藏单元。</p><p id="3dd0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">总之，在增加隐藏单元数量的同时，跟踪验证错误。</strong></p><p id="e198" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如Andrej Karpathy所说，3层网络比2层网络性能更好，但超出这个范围对网络没有什么帮助。而在CNN中，层数越多，网络的性能越好。</p><p id="c131" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">安德烈·卡帕西:<a class="ae lb" href="https://cs231n.github.io/neural-networks-1/" rel="noopener ugc nofollow" target="_blank">https://cs231n.github.io/neural-networks-1/</a></p><p id="190a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">深度学习书籍【http://www.deeplearningbook.org/contents/ml.html】:<a class="ae lb" href="http://www.deeplearningbook.org/contents/ml.html" rel="noopener ugc nofollow" target="_blank"/></strong></p><h1 id="ad4b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">RNN的超参数</h1><p id="b269" class="pw-post-body-paragraph iq ir hi is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">对于单词嵌入，嵌入大小是要设置的关键参数。尺寸越大，性能越好。常用的嵌入大小是200。</p><p id="5421" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">与更传统的循环单元相比，使用门控单元具有明显的优势。收敛往往更快，最终解往往更好。然而，在比较LSTM和GRU时，结果并不是决定性的，这表明门控循环单位类型的选择可能严重依赖于数据集和相应的任务。除了语言建模之外，GRU在所有任务上都超过了LSTM。</p><p id="1a50" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们一致的发现是，至少两个深度是有益的。然而，在两到三层之间，我们的结果是混合的。此外，LSTM和GRU之间的结果参差不齐，但都明显优于RNN。</p><p id="8268" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在[神经机器翻译]实验中，LSTM细胞始终胜过GRU细胞。由于我们架构中的计算瓶颈是softmax运算，因此我们没有观察到LSTM单元和GRU单元之间的训练速度有很大差异。令人有点惊讶的是，我们发现普通解码器的学习能力不如门控解码器。</p><p id="76b1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">网络可视化</strong>:<a class="ae lb" href="http://jalammar.github.io/" rel="noopener ugc nofollow" target="_blank">http://jalammar . github . io</a></p><p id="7712" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">深度架构基于梯度训练实用推荐</strong>:<a class="ae lb" href="https://arxiv.org/abs/1206.5533" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1206.5533</a></p><p id="317d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">深度学习书作者伊恩·古德菲勒</strong>:<a class="ae lb" href="http://www.deeplearningbook.org/contents/guidelines.html" rel="noopener ugc nofollow" target="_blank">http://www.deeplearningbook.org/contents/guidelines.html</a>，<a class="ae lb" href="http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters" rel="noopener ugc nofollow" target="_blank">http://neuralnetworksanddeeplearning . com/chap 3 . html # how _ to _ choose _ a _ neural _ network ' s _ hyper-parameters</a></p><p id="5454" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">扬·勒村</strong>:<a class="ae lb" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" rel="noopener ugc nofollow" target="_blank">http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf</a></p><p id="936c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">生成好字嵌入</strong>:<a class="ae lb" href="https://arxiv.org/abs/1507.05523" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1507.05523</a></p><p id="a01f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你学到了什么，别忘了鼓掌！</p></div></div>    
</body>
</html>