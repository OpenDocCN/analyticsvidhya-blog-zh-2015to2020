<html>
<head>
<title>Seq2seq Model and The Exposure Bias Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Seq2seq模型与暴露偏倚问题</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/seq2seq-model-and-the-exposure-bias-problem-962bb5607097?source=collection_archive---------3-----------------------#2019-09-03">https://medium.com/analytics-vidhya/seq2seq-model-and-the-exposure-bias-problem-962bb5607097?source=collection_archive---------3-----------------------#2019-09-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c4b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Seq2seq模型或通常称为编码器-解码器模型，是用于机器翻译、图像字幕、文本摘要等的技术。在下图中，我们可以看到用于机器翻译的编码器-解码器模型。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/43d89d14465142ade5b75eed7bfdbe68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x4wsJobiSC7zlTkP8yv40A.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">(机器翻译模型)</figcaption></figure><p id="16ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以使用LSTM或其变体，如GRU或双向LSTM作为编码器解码器单元。假设我们想要创建一个将英语句子转换成法语句子的模型。然而，我们不能直接将原始的英语句子作为输入提供给编码器。因此，我们需要将这些单词转换成向量。我们可以使用各种预训练的模型，如google的word2vec、glove或fasttext，将这些单词转换为矢量，或者我们可以使用嵌入层来获得每个单词的矢量表示。</p><p id="a259" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在后一种情况下，我们的模型将在训练模型时学习单词的有用表示。首先，我们可以将每个单词转换成一个整数，然后应用嵌入查找操作来找到给定单词的相应表示。</p><p id="2eb5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里有一个我们在进行机器翻译时应该遵循的技巧:在将整个句子作为编码器模型的输入时，我们应该首先反转它，然后将其馈送给模型，使得第一个单词应该出现在最后，因为这是应该首先翻译的单词。</p><p id="eb27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们试着分析解码器的输入。在训练时间中，解码器被给予输入，该输入应该是在先前步骤中输出的，除了第一个单词，该单词具有作为代表句子开始的输入的标记(假设为<go>)。除此之外，解码器单元被给予先前的输出作为输入。</go></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jt"><img src="../Images/29f80c4f1e70ed356519fd1f7fea7bd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*paBPTcKP0U4anF6IP9qb9Q.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">(用于自动响应生成的seq2seq模型)</figcaption></figure><p id="bca1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在每个时间步中，解码器将输出每个单词的分数，该分数通过softmax函数转化为概率。我们还选择概率最高的单词作为输出(不同之处在于波束搜索策略)。在这种情况下，假设单词good的概率为0.3，“I”的概率为0.4，“am”的概率为0.3，那么根据贪婪搜索策略，我们选择“I”作为输出。</p><p id="8dfe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在推理时间或测试时间，我们没有目标句子作为输入提供给解码器。在这里，模型完全依赖于它自己，因为实际摘要的不可用性，我们提供来自先前输出的输入。这个问题被称为<strong class="ih hj"> <em class="ju">曝光偏差问题。</em> </strong>如果模型在某一步输出一个坏的输出，反过来影响前面的整个序列，这个问题就变得很尖锐。</p><p id="64c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为这个问题的第一步解决方案，我们可以利用课程学习，试图消除解码器对实际摘要或基础事实的依赖。这利用了所谓的<strong class="ih hj"> <em class="ju">计划抽样</em> </strong>的概念，其中模型将随机选择从哪里抽样。也就是说，是将地面实况作为输入提供给下一个时间步，还是将前一个时间步中产生的输出作为两个输入提供给推断时间。这种方法大大减少了模型对原始摘要的依赖。</p><p id="21fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下一篇文章中，我们将看到使用<strong class="ih hj"> <em class="ju">强化学习</em> </strong>来抑制曝光偏差的问题。</p></div></div>    
</body>
</html>