<html>
<head>
<title>Optimization in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的优化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/optimization-in-neural-networks-d315dc2c8e46?source=collection_archive---------8-----------------------#2020-02-01">https://medium.com/analytics-vidhya/optimization-in-neural-networks-d315dc2c8e46?source=collection_archive---------8-----------------------#2020-02-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/fc6d935d6c7cb1f379617bee32cccf24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/0*nV8hO7oarRGirMQI.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">来源:pix abay[<a class="ae iq" href="https://pixabay.com/vectors/neural-network-thought-mind-mental-3816319/" rel="noopener ugc nofollow" target="_blank">https://pix abay . com/vectors/neural-network-thought-mind-mental-3816319/]</a></figcaption></figure><p id="36a8" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">读者们好，在这里我将讨论神经网络中的优化器和误差反向传播。</p><p id="c4b7" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">什么是优化器？</strong></p><p id="0c2f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">神经网络中的优化器有助于在误差E的相反方向上更新权重，以实现具有最小可能误差的最佳结果。</p><p id="3bf2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">每个优化器在优化权重方面都有自己的策略。</p><p id="8abd" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">如果一切都很好，我们以尽可能少的错误获得了伟大的成果，<strong class="it hj"/>错误反向传播的概念是在哪里以及为什么<strong class="it hj">出现的？</strong></p><p id="8203" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">因为当优化器试图通过记住误差来更新权重时，他们通过使用从最终层流向初始层的反向传播来这样做，这就是为什么这种方法被称为误差反向传播。现在，这种方法也存在一些问题，这些问题可能会遇到，将在本文中进一步讨论。首先，让我们来看看优化器。我们将要讨论的优化器是:</p><h1 id="7681" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak"> 1。梯度下降:</strong></h1><p id="7118" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">梯度下降是最基本的，但在优化算法中使用最多。它在回归和分类算法中被大量使用。这是一种一阶优化算法，依赖于损失函数的一阶导数。</p><p id="004f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">为什么叫渐变下降？</strong></p><p id="fd16" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">想象一下，我们正从山顶向一个低谷行进。众所周知，只有当我们到达最底的可到达点时，这个旅程才算完成。这就是梯度下降的工作原理，它将误差从很高的值降低到尽可能小的值。</p><p id="817c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">它计算出应该改变权重的方式，以便函数可以达到最小值<strong class="it hj"/>。在反向传播过程中，损耗从一层转移到另一层，并且模型的参数(也称为权重)根据损耗进行修改，从而可以从最终层到初始层最小化损耗/误差。</p><p id="a68a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">算法:<strong class="it hj">δw = -α⋅∇e</strong></p><p id="8e3e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">有不同类型的梯度下降，它们在算法的工作方式上有所不同。</p><p id="5895" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">梯度下降算法是有利的，因为它易于计算、易于实现并且易于理解。</p><p id="20fb" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">但是坏处呢？</p><p id="eabe" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">1.)首先，梯度下降找到的<strong class="it hj">最小值</strong>有很多机会，<strong class="it hj">不是保证的最小值，</strong>可以被称为<strong class="it hj">坏局部最小值，因为一旦到达它，它将被卡在那里。</strong></p><p id="e326" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">2.)由于权重是在对整个数据集进行计算之后计算的，因此在处理大型数据集时这是耗时的，需要很长时间才能收敛到最佳结果。</p><p id="170d" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">3.)由于权重对于一个周期/ <strong class="it hj">历元</strong>仅更新一次，因此计算梯度下降算法需要大的存储器。</p><p id="86b7" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">(Epoch是神经网络在训练过程中查看我们整个数据集的次数)。</p><p id="1be7" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">那么如何克服上述陷入坏的局部极小值和巨大的计算时间的风险呢？</p><p id="9174" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">随机梯度下降</strong> <strong class="it hj"> (SGD) </strong>来救援了！</p><p id="7d22" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">随机=随机。它从随机点开始，并开始更新权重，在这种情况下，与每个时期仅更新一次的普通梯度下降不同，它会更频繁地更新权重。例如，如果我们有<strong class="it hj"> 100个数据点</strong>，则<strong class="it hj"> SGD在每个时期更新权重100次。</strong></p><p id="de5c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">但是就整组数据来看，哪个是正确的？</strong></p><p id="daca" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">显示一个输入-&gt;调整权重，显示另一个输入-&gt;再次调整权重，以此类推，直到覆盖最后一个点，然后，如果需要，从输入1开始。</p><p id="a968" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">同样，显示输入训练直到收敛，显示第二个，训练直到收敛。</p><p id="2c6b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">观察到当模型参数频繁更新时，由于数据包含在一些地方具有一些高值的数据点，在不同强度的损失函数的权重更新和波动期间，参数将具有高方差，这是一个缺点。</p><p id="e5bb" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">优点</strong>:</p><p id="0ac8" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">1.)模型参数的频繁更新因此，在更短的时间内收敛。</p><p id="c9c3" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">2.)需要较少的存储器，因为不需要存储损失函数值。</p><p id="71e4" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">3.)可能会得到新的更好的极小值。</p><p id="814c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">缺点</strong>:</p><p id="f586" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">1.2)模型参数中的高变化，这可能延迟全局最小值的快速实现。</p><p id="0766" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">2.)甚至可能在达到全局极小值后拍摄。</p><p id="70fa" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">3.)要得到和梯度下降一样的收敛性需要慢慢降低学习率的值。</p><p id="58f6" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">由于训练一个历元需要很多次的权重更新，可以做些什么来减少时间消耗吗？当然是的…</p><h1 id="eb90" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">2.<strong class="ak">批量训练:</strong></h1><p id="8ebb" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">使用一批样本来更新权重，而不是一次使用一个样本。</p><p id="108c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这个过程就是<strong class="it hj">批量SGD。</strong></p><p id="e74c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">优势</strong>:</p><p id="ed0a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">1.)频繁地更新模型参数，并且也具有较小的变化，因为这些是作为批量更新的一部分完成的。</p><p id="0b03" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">2.)由于面向批处理的更新，需要中等大小的内存。</p><p id="0c1f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">所有类型的梯度下降都有一些挑战:</strong></p><p id="ec29" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">1.2)选择学习率的最佳值。如果学习率太小，梯度下降可能需要很长时间才能收敛。</p><p id="9a87" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">2.)对于所有参数都具有恒定的学习速率。可能有些参数我们不想以同样的速度改变。</p><p id="26c0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">3.)可能陷入局部最小值。</p><p id="5ed9" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">但是，您是否一直想知道重量更新和流程？</p><p id="28ca" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">为了更好地理解，我们必须了解动量。</p><p id="4d16" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">动量是对梯度方向变化的阻力。</strong></p><p id="cc67" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">复杂语句？好吧…</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/c1c22d08cf0c66ebbef4b495851c4cba.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/0*frHoWvyMKmLLN023.gif"/></div><figcaption class="im in et er es io ip bd b be z dx translated">《黑客帝国》中的三位一体倾斜的自行车</figcaption></figure><p id="830e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">当你驾驶摩托车时，当你将它倾斜到一个角度(最好是向左)，观察至少1秒钟，当转弯时，你感觉到摩托车向倾斜角度方向的拉力，使你感觉速度增加了一秒钟，你试图抵抗这种拉力，以避免失去与摩托车的平衡，最终在你转弯后，向角度方向的拉力停止。所以你最后转了一个弯，但不是90度。这是力学中的一个经验法则的应用，我现在记不起它的名字了，为此道歉。:(</p><p id="c5f1" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">同样，在SGD/Batch SGD中，动量通过加速向相关方向的收敛来软化收敛，减少向无关方向的波动。(希望以上段落中的所有行现在对您有一些意义:)</p><p id="ecee" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">动量通常设定为0.9。</p><p id="d297" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">优点</strong>:</p><p id="b2a5" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">1.)动量减少了参数的振荡和高方差。</p><p id="23a3" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">2.)由于上述原因，它比梯度下降收敛得更快。</p><p id="d31b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">缺点</strong>:</p><p id="f70f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">1.)增加了一个需要手动精确选择的超参数，这就是动量，我们必须面对避免动量过低或过高的风险。</p><h1 id="31ea" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">3.内斯特罗夫加速梯度:</h1><p id="a88f" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">动量可能是一个好方法，但如果动量太高，算法可能会错过局部最小值，可能会继续上升。因此，为了解决这个问题，开发了<strong class="it hj"> NAG </strong>算法。这是一种避免上升势头的前瞻性方法。</p><p id="5713" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">优点</strong>:</p><p id="e10b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">1.)不会错过局部最小值。</p><p id="bfce" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">2.)如果出现最小值，速度会减慢。</p><p id="6ee8" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">缺点</strong>:</p><p id="9e11" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">1.)不过，Hyper参数需要手动选择。</p><h1 id="911f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">训练神经网络时的其他考虑事项:</strong></h1><h1 id="b19b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">学习率:</strong></h1><p id="e09c" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">我们应该从每个输入样本中学习多少就是学习率。</p><p id="36cd" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">典型值为0.1到0.4</p><h2 id="5eb3" class="kx jq hi bd jr ky kz la jv lb lc ld jz jc le lf kd jg lg lh kh jk li lj kl lk bi translated"><strong class="ak">抖动:</strong></h2><p id="23d3" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">将<strong class="it hj">小</strong>量的<strong class="it hj"> </strong>噪声添加到输入数据中。</p><h2 id="a5d5" class="kx jq hi bd jr ky kz la jv lb lc ld jz jc le lf kd jg lg lh kh jk li lj kl lk bi translated"><strong class="ak">重量衰减:</strong></h2><p id="6410" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">权重乘以小系数0 <e/></p><h2 id="16f3" class="kx jq hi bd jr ky kz la jv lb lc ld jz jc le lf kd jg lg lh kh jk li lj kl lk bi translated"><strong class="ak">提前停止:</strong></h2><p id="24c4" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">当误差变化很低时，停止训练。</p><h1 id="f914" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">选择较高的初始学习率，并在各代间降低。为什么这是一个好的实践？</strong></h1><h2 id="c53c" class="kx jq hi bd jr ky kz la jv lb lc ld jz jc le lf kd jg lg lh kh jk li lj kl lk bi translated"><strong class="ak">变量选择:</strong></h2><p id="5d37" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">训练数据在同一个序列吗？</p><p id="d45c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">输入层需要有与输入一样多的节点。说说降维，数据预处理！</p><p id="2bd4" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">将数据分为训练、验证和测试模型。</p><h1 id="089b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">我们现在需要适应性！所以，我们有了更好的版本如<strong class="ak">阿达格拉德</strong>、<strong class="ak">阿达德尔塔</strong>、<strong class="ak">亚当</strong>。</h1><h2 id="9e9c" class="kx jq hi bd jr ky kz la jv lb lc ld jz jc le lf kd jg lg lh kh jk li lj kl lk bi translated">阿达格勒:</h2><p id="380a" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">这是一种二阶优化算法。它对误差函数的导数起作用。</p><p id="c749" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">上面提到的所有优化器的缺点之一是，对于所有参数和每个周期，学习率是恒定的。</p><p id="0e42" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这个优化器改变了学习率。它改变每个参数的学习率<strong class="it hj">‘η’</strong>和每个时间步长<strong class="it hj">‘t’。</strong></p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/45e5d9c439caa122add0a5a945e3d75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/0*sTXJXBIkRWwRfq9H.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">给定参数在给定时间t的损失函数的导数。</figcaption></figure><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/f7c8091c68bd2598c6fac6f427b74b90.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/0*5mmWxOotmPqNUNXD.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">给定输入I和时间/迭代t时的参数更新</figcaption></figure><p id="ee5a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> η </strong> =在给定时间，根据给定参数<strong class="it hj"> θ(i)计算的先前梯度，对给定参数<strong class="it hj"> θ(i) </strong>进行修改的学习率。</strong></p><p id="eb0a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们存储梯度的平方和w.r.t. <strong class="it hj"> θ(i) </strong>直到时间步长<strong class="it hj"> t </strong>，而<strong class="it hj"> ϵ </strong>是一个避免被零除的平滑项(通常约为1e 8)。有趣的是，如果没有平方根运算，该算法的性能会差得多。</p><p id="0263" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">它对不太频繁的参数进行大的更新，对频繁的参数进行小的更新。</p><p id="b071" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">优点</strong>:</p><ol class=""><li id="8912" class="ln lo hi it b iu iv iy iz jc lp jg lq jk lr jo ls lt lu lv bi translated">每个训练参数的学习率变化。</li><li id="7617" class="ln lo hi it b iu lw iy lx jc ly jg lz jk ma jo ls lt lu lv bi translated">无需手动调整学习速率。</li><li id="863f" class="ln lo hi it b iu lw iy lx jc ly jg lz jk ma jo ls lt lu lv bi translated">由于适应性，能够在稀疏数据上训练。</li></ol><p id="fea6" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">缺点</strong>:</p><ol class=""><li id="70d2" class="ln lo hi it b iu iv iy iz jc lp jg lq jk lr jo ls lt lu lv bi translated">由于需要计算二阶导数，计算成本很高。</li><li id="2b79" class="ln lo hi it b iu lw iy lx jc ly jg lz jk ma jo ls lt lu lv bi translated">学习率总是下降/衰减，导致训练缓慢。</li></ol><h2 id="2383" class="kx jq hi bd jr ky kz la jv lb lc ld jz jc le lf kd jg lg lh kh jk li lj kl lk bi translated">阿达德尔塔</h2><p id="157c" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">它是AdaGrad 的扩展，旨在消除AdaGrad的<strong class="it hj"> <em class="mb">衰减学习率</em> </strong>问题。<strong class="it hj"> AdaDelta </strong>将累积的过去梯度的窗口限制为某个固定大小<strong class="it hj"> w </strong>，而不是累积所有先前平方的梯度。在这种情况下，使用指数移动平均，而不是所有梯度的总和。</p><p id="74e9" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> E[g ](t)=γ。e[g](t1)+(1γ)。g (t) </strong></p><p id="1571" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们将<strong class="it hj"> γ </strong>设置为与动量项相似的值，大约为0.9。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/0bdd97077ac4fdd36c99ae02ec0389b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/0*PIgmaAyJwhuUUD0B.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">更新参数</figcaption></figure><p id="a9f8" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">优点</strong>:</p><ol class=""><li id="3568" class="ln lo hi it b iu iv iy iz jc lp jg lq jk lr jo ls lt lu lv bi translated">现在学习速度不衰减，训练不停止。</li></ol><p id="c797" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">缺点</strong>:</p><ol class=""><li id="fb42" class="ln lo hi it b iu iv iy iz jc lp jg lq jk lr jo ls lt lu lv bi translated">计算开销很大。</li></ol><h2 id="1e02" class="kx jq hi bd jr ky kz la jv lb lc ld jz jc le lf kd jg lg lh kh jk li lj kl lk bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</h2><p id="8e89" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated"><a class="ae iq" href="https://arxiv.org/pdf/1412.6980.pdf" rel="noopener ugc nofollow" target="_blank">亚当</a>(自适应矩估计)对一阶和二阶动量起作用。Adam背后的直觉是，我们不想因为可以跳过最小值而滚动得如此之快，而是希望稍微降低速度以进行仔细的搜索。除了存储像<strong class="it hj"> AdaDelta </strong>，<strong class="it hj"> <em class="mb"> Adam </em> </strong> <em class="mb"> </em>这样的过去平方梯度的指数衰减平均值之外，还保持过去梯度的指数衰减平均值<strong class="it hj"> M(t)。</strong></p><p id="10c2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj"> M(t)和V(t) </strong>分别是梯度<em class="mb"> </em>的<strong class="it hj"> <em class="mb">均值</em> </strong>和<strong class="it hj"> <em class="mb">无中心方差</em> </strong>的一阶矩值。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es md"><img src="../Images/636b093477131123b1ea7ad03ee669ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/0*KQYMeSIW-lTBGBWO.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">一阶和二阶动量</figcaption></figure><p id="8918" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这里，我们取<strong class="it hj"> M(t) </strong>和<strong class="it hj"> V(t) </strong>的平均值，使得<strong class="it hj">E[M(t)】</strong>可以等于<strong class="it hj"> E[g(t)] </strong>其中，<strong class="it hj">E[f(x)】</strong>是<strong class="it hj"> f(x) </strong>的期望值。</p><p id="9aa5" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">要更新参数:</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es me"><img src="../Images/cb82c8fa4e5984a142e9b09d829ec8c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:120/0*ppAd7TOIAe4j0TtU"/></div></figure><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/1ba2fb3eaa72e0962c70cd92bdfc7857.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/0*WtL2BtFLeg0H0fQB.png"/></div></figure><p id="9228" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">更新参数</p><p id="4630" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">β1的值是0.9，β2的值是0.999，而'<strong class="it hj"> ϵ' </strong>的值是(10 x exp(-8))。</p><p id="258b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">优点</strong>:</p><ol class=""><li id="7e49" class="ln lo hi it b iu iv iy iz jc lp jg lq jk lr jo ls lt lu lv bi translated">方法太快，收敛很快。</li><li id="e914" class="ln lo hi it b iu lw iy lx jc ly jg lz jk ma jo ls lt lu lv bi translated">纠正消失学习率，高方差。</li></ol><p id="b1e9" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><strong class="it hj">缺点</strong>:</p><p id="dca9" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">计算成本高。</p><h2 id="9251" class="kx jq hi bd jr ky kz la jv lb lc ld jz jc le lf kd jg lg lh kh jk li lj kl lk bi translated"><strong class="ak">雷达姆:</strong></h2><p id="42a3" class="pw-post-body-paragraph ir is hi it b iu kn iw ix iy ko ja jb jc kp je jf jg kq ji jj jk kr jm jn jo hb bi translated">是亚当的<a class="ae iq" href="https://arxiv.org/pdf/1908.03265.pdf" rel="noopener ugc nofollow" target="_blank">整改版。</a></p><p id="7bb1" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">Vanilla Adam和其他自适应学习率优化器在训练早期基于太少的数据做出错误的决定。因此，如果没有某种形式的热身(最初以非常小的学习率进行训练)，他们很可能一开始就陷入糟糕的局部最优，从而由于糟糕的开始而使训练曲线变得更长、更难。</p><p id="83f9" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">假设预热作为方差缩减器，但是所需预热的程度是未知的，并且随着数据集的不同而不同，于是人们开始确定一种数学算法作为动态方差缩减器。因此，他们建立了一个修正项，这将允许自适应动量缓慢但稳定地发展到作为潜在方差函数的完全表达。</p><p id="68c8" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">作者注意到，在某些情况下，RAdam可以退化为动量等价的SGD，这是由衰变率和潜在方差驱动的。</p><p id="0b97" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">总结是RAdam根据方差的潜在散度动态地打开或关闭自适应学习率。实际上，它提供了一个不需要可调参数的动态预热。</p><p id="4222" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">作者证实，RAdam优于传统的手动预热调谐，传统的手动预热调谐需要推测或猜测预热的步骤数。</p><p id="a3a9" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">你可以在我的另一篇文章中读到关于误差反向传播和消失梯度的问题。</p></div><div class="ab cl mg mh gp mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="hb hc hd he hf"><p id="2270" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">谢谢你读到这里！下一集再见！</p></div></div>    
</body>
</html>