<html>
<head>
<title>Guide to Deep Learning Concepts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习概念指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/beginners-guide-to-deep-learning-concepts-46a414b0e80a?source=collection_archive---------10-----------------------#2020-05-19">https://medium.com/analytics-vidhya/beginners-guide-to-deep-learning-concepts-46a414b0e80a?source=collection_archive---------10-----------------------#2020-05-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7f86b567113f22ef83c164df2a6b6f03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*lVuFwr5W59aG5uWwfAfTeA.gif"/></div></div></figure><p id="94d9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过经验学习，记忆所学的东西是由我们的大脑负责的技能…那么有没有人想过机器是否能像我们一样思考，像我们一样学习？是的，机器可以像我们一样思考，甚至比人类更能思考，通过使用一些算法像我们一样学习。这种现象被称为“<strong class="is hj"> <em class="jo">机器学习</em> </strong>”。</p><p id="eb2f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">深度学习是机器学习的子集，机器学习是AI的子集。基本上深度学习可以被称为对机器学习的改进。</p><blockquote class="jp jq jr"><p id="a2b6" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">机器学习:</strong>机器学习是人工智能(AI)的一种应用，它为系统提供了自动学习和根据经验改进的能力，而无需显式编程</p><p id="9a3d" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">深度学习:</strong>深度学习是模仿人脑处理数据的概念，用于决策和预测，用于图像识别等。这里出现了神经元的概念，神经元是大脑传输数据的主要部分，我们将在课程中进一步看到神经网络。</p></blockquote><p id="181f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在学习深度学习之前，我们首先需要了解“线性回归”和“逻辑回归”，这将是有用的。</p><blockquote class="jp jq jr"><p id="c147" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">线性回归:</strong>线性回归是一种机器学习算法，它执行的任务是根据给定的自变量(x)来预测因变量值(y)。因此，这种回归技术找出了x(输入)和y(输出)之间的线性关系。该方程的形式为“y = ax + b”。</p></blockquote><p id="504e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这种线性回归在两组数据点之间放置了一条分隔线，以区分其各自的类别。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es jv"><img src="../Images/aa381993269aad3642c30edf34d82530.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*15cGwIUfTZjZ9atRVax6Lw.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">使用线性回归的二元分类</figcaption></figure><blockquote class="jp jq jr"><p id="50c2" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">逻辑回归:</strong>逻辑回归是一种监督学习分类算法，用于预测目标变量的概率。这是一种统计方法，用于分析决定结果的一个或多个独立变量。线性变换(ax + b)通过激活函数，比如sigmoid，输出由sigmoid激活函数给出。</p></blockquote><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es ke"><img src="../Images/9019ce14e3da3708b1e0d624f4a70556.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*WodefjXOBp52lu87f_R8Ow.png"/></div></figure><h1 id="fc60" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated"><strong class="ak">线性回归与逻辑回归:</strong></h1><p id="1c48" class="pw-post-body-paragraph iq ir hi is b it ld iv iw ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn hb bi translated">线性回归不使用任何激活函数，但在逻辑回归中并非如此，在逻辑回归中，线性变换(ax + b)通过激活函数sigmoid并生成输出。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es li"><img src="../Images/7ab105cab47e2365d219cf6a4f224d95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DalqI0RtzyMjIQFpMhVeiA.png"/></div></div></figure><p id="0c6d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们将进入深度学习的主要概念，包括神经元、神经网络、权重、偏差、成本函数、反向传播、超参数、梯度下降等。</p><p id="1354" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 1)神经元:</strong>神经元是神经网络的主要模块之一，神经网络只不过是一个携带信息并将其传递到另一层的单元。在神经网络中，输入数据被传递给神经元，这是神经网络的起点。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lj"><img src="../Images/07dd295d7f75c17fdbe81e793f8c948a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oQd98NncOH5Kko-uJTsHcA.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">基本神经网络架构</figcaption></figure><p id="1075" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上面的架构中，输入层、隐藏层有一些节点，这些节点只是神经元，它们接收数据并进一步传递。</p><p id="b8b0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 2)神经网络:</strong>神经网络只不过是神经元和隐藏层的集合。</p><p id="fbb6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 3) Bias : </strong> Bias是神经网络中的一个附加参数，与权重一起用于调整输出，以及对<strong class="is hj">神经元</strong>的输入(X * W)的加权和。因此，偏差是一个常数，它以一种最适合给定数据的方式帮助模型。</p><p id="2d0f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 4)权重:</strong>权重是转换输入数据的<a class="ae lk" href="https://deepai.org/machine-learning-glossary-and-terms/neural-network" rel="noopener ugc nofollow" target="_blank">神经网络</a>中的参数。通常用‘W’来表示。没有具体的符号，但通常我们使用变量“W”。我们将权重与输入数据相乘，并添加偏差。假设“X”是输入数据,“W”是权重,“b”是偏差，那么将进入隐藏层的数据将是“激活函数(X * W + b )”,其中激活函数可以是tanh、sigmoid等等。基本上，我们在开始时为权重和偏差分配一个随机数，并通过称为反向传播的特殊概念定期改进权重，我们将进一步学习这一概念。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/53ba3045b914b77c125457862416f5c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vhu_4NtEBPrzlkIM-tnczg.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">使用输入数据、权重和偏差的基本数据流</figcaption></figure><p id="0d83" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在神经网络中，模型的训练是通过使用称为反向传播的概念来完成的。</p><p id="8835" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你熟悉机器学习和使用模型训练数据，我们就像“训练”一样调用模型api的方法，它使用反向传播。如果您有机会查看模型api代码，您将在培训方法中找到反向传播代码。</p><p id="53d8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 5)成本函数:</strong>成本函数是神经网络相对于其给定的训练样本和预期输出有多好的度量。它是单个误差的总和。求成本函数有各种各样的公式。其中一些是:</p><p id="a9d6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">成本=真实标签*日志(预测标签)</p><p id="9cf4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">损失可能性(成本)=总和(真实标签*对数(预测标签)-(1-真实标签)*对数(1-预测标签)</p><p id="e264" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 6)反向传播:</strong>由于我已经明确神经网络中的训练是反向传播，我们将讨论反向传播的主要概念。</p><p id="485c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">反向传播是训练模型的概念，其中调整随机分配的权重和偏差，使得使用权重和偏差的导数将成本降低到最小。当代码中的所有内容都是正确的，并且您选择的超级参数非常适合您的网络时，成本的斜率将如下图所示。</p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/ea0f3455478ad0ab039193930b589e40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WgXNsfCmtB885FK0ncMeoA.png"/></div></div></figure><p id="a908" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 7)超参数:</strong>超参数基本分为4类</p><p id="b0c4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> i)学习率:</strong>学习率<strong class="is hj"> </strong>是一个超参数，它控制着我们对网络权重的调整程度。学习率用于反向传播，其中学习率乘以权重和偏差的导数。(w +=学习率*导数_w，b +=学习率*导数_b)。基本上为了网络的最佳性能，我们需要选择学习率非常小。</p><p id="5e9b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> ii)正则化:</strong>拟合数据时的一个问题是过拟合，这个问题可以通过使用正则化来避免。</p><p id="2cbd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> iii)隐藏层:</strong>神经网络中的隐藏层是介于输入层和输出层之间的层，其中神经元接受一组加权输入，并通过激活函数产生输出。</p><p id="c8f6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> iv)激活函数:</strong>激活函数是非线性变换，它有自己的一套算法来生成输出。线性变换即。加权和与偏差一起通过激活函数来生成输出，如果没有它，那么在数学上计算如此复杂的映射将是无效的。神经网络中使用的一些激活函数有Tanh、Sigmoid、ReLu、Softmax <strong class="is hj">。</strong></p><blockquote class="jp jq jr"><p id="09e6" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj"> Sigmoid函数公式:</strong> 1 / (1 + e^-x)</p></blockquote><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/8ab6822daa7febb653bf7cf6524d4c26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*qBSXMevJFD2au9p1ONLRxw.png"/></div></figure><blockquote class="jp jq jr"><p id="a6be" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj"> Softmax函数公式:</strong> a = exp(x)，a / a.sum(axis = 1)</p><p id="7b8c" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">其中x是数据，假设加权和+偏差。</p><p id="f0a1" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj"> ReLu(整流线性单位)Softmax函数:</strong> <em class="hi"> y = max(0，x) </em></p></blockquote><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lo"><img src="../Images/4f58377e6a5e0d92ba64fbcdc22c6969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QkZ5OYHJvCnYPigMJ2SIyg.png"/></div></div></figure><blockquote class="jp jq jr"><p id="85fe" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj"> Tanh(双曲正切)激活函数:</strong>y = tanh(x)-&gt;(e^x -e^-x)/(e^x +e^-x)</p></blockquote><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/ee41ed0ad92d8ab18f8b70a8a2f18aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*IflzcEDYWUdv03e7mHnoZA.png"/></div></figure><p id="7587" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们以下列方式调整反向传播的权重和偏置</p><blockquote class="jp jq jr"><p id="79e2" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">用于更新从隐藏层到输出层的权重w2:</strong></p><p id="8638" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">w2 +=学习率* Z.T.dot(目标预测值)</p><p id="61f1" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">用于更新从隐藏层到输出层的偏差B2:</strong></p><p id="1af6" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">b2 +=学习率*(目标-预测)。总和()</p><p id="f398" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">用于更新从输入层到隐藏层的权重w1:</strong></p><p id="3c9a" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">对于Sigmoid激活函数:</strong> dZ = numpy.outer(目标预测，w2) * Z * (1 — Z)</p><p id="c7f1" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">对于双曲正切激活函数:</strong> dZ = numpy.outer(目标预测，w2) * (1 — Z * Z)</p><p id="a34e" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">对于ReLu激活功能:</strong> dZ =(目标-预测)。点(w2。t)* Z *(1-Z)</p><p id="a862" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">dz是隐藏节点的误差。因此，最终的权重更新公式为:</p><p id="8282" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">w1 +=学习率* X.T.dot(dZ)</p><p id="f918" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">用于更新从输入层到隐藏层的偏差B1:</strong></p><p id="5d99" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">b1的dZ将与w1的dZ相同，具有各自的激活功能。</p><p id="41e7" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">因此，最终偏差更新公式为:</p><p id="8bd2" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">b1 +=学习率* dZ.sum(轴= 0)</p></blockquote><p id="b377" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中Z是隐藏层神经元的输出，Target是实际标签，而predicted是预测标签。</p><blockquote class="jp jq jr"><p id="e1f0" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">这些权重和偏差的更新不过是<strong class="is hj">“梯度下降”</strong>。</p></blockquote><p id="f1cb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以我们想到的问题是，如何选择超参数？如果你问我是否有找到超参数的特殊方法，我的答案是否定的…基于输入和你正在做的分类类型，你必须做一个学习率和隐藏层的试错法。因此，对于哪种超参数配置，您可以获得完美的网络精度和最佳分类率，这些配置将是最好的..</p><p id="6397" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，对于二元分类，我们使用<strong class="is hj">“Sigmoid激活函数”</strong>，对于多类分类，我们使用<strong class="is hj">“soft max激活函数”。</strong></p><p id="1524" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我已经说过的，学习率应该是小的，如果学习率很高，那么成本函数将转到“INF”或“NaN”。</p><p id="8f3b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一些常用的神经网络架构有:</p><ul class=""><li id="2695" class="lq lr hi is b it iu ix iy jb ls jf lt jj lu jn lv lw lx ly bi translated"><strong class="is hj">卷积神经网络(CNN) </strong></li><li id="dae5" class="lq lr hi is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly bi translated"><strong class="is hj">递归神经网络(RNN) </strong></li><li id="cf47" class="lq lr hi is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly bi translated"><strong class="is hj">长短期记忆(LSTM) </strong></li><li id="ec7f" class="lq lr hi is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly bi translated"><strong class="is hj">玻尔兹曼机</strong></li><li id="0b5c" class="lq lr hi is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly bi translated"><strong class="is hj">深度信念网络</strong></li></ul><p id="503d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以如果你想知道这些神经网络是如何工作的，你需要练习和编码。除非你练习，否则你很难理解。所以我有一些免费的数据集供你练习。</p><ul class=""><li id="304c" class="lq lr hi is b it iu ix iy jb ls jf lt jj lu jn lv lw lx ly bi translated">MNIST</li><li id="e2e7" class="lq lr hi is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly bi translated">西法尔</li><li id="a3f7" class="lq lr hi is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly bi translated">SVNH</li><li id="aab0" class="lq lr hi is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly bi translated">虹膜</li></ul><p id="e94f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你也可以通过在浏览器中访问“谷歌的神经网络游乐场”来了解这些神经网络如何与不同的超参数一起工作。它是神经网络的可视化。</p><h1 id="9f6f" class="kf kg hi bd kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc bi translated">最终注释:</h1><p id="a2d8" class="pw-post-body-paragraph iq ir hi is b it ld iv iw ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn hb bi translated">我用简单的术语来表达概念，这样就不会变得复杂，更容易理解，你也会对学习产生兴趣。我希望我贡献了一些知识，你也学到了一些东西…！如果是的话，我很高兴发表这篇文章，如果你喜欢我的文章，请不要忘记鼓掌。感谢任何形式的反馈。谢谢各位！！！</p></div></div>    
</body>
</html>