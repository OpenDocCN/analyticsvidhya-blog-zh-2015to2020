# è½»æ¾å®ç°ä¸åŒçš„å˜å‹å™¨ğŸ¤—ğŸ¤—é€šè¿‡æ‹¥æŠ±è„¸

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/easily-implement-different-transformers-through-hugging-face-e471035e9c86?source=collection_archive---------10----------------------->

> å˜å½¢é‡‘åˆšæ˜¯æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œå·²ç»è¢«ç”¨äºä»¥éå¸¸æœ‰æ•ˆçš„æ–¹å¼è§£å†³ä»æƒ…æ„Ÿåˆ†æåˆ°é—®é¢˜/å›ç­”çš„æ–°é¢–çš„ NLP ä»»åŠ¡ã€‚ç„¶è€Œï¼Œå˜å½¢é‡‘åˆšæœ€åŸºæœ¬çš„åŠŸèƒ½åªæ˜¯æ³¨æ„åŠ›æœºåˆ¶çš„ç¼–ç å™¨å±‚çš„å †å ã€‚å³ä½¿ä½¿ç”¨ Pytorch æˆ– Tensorflow ä¹‹ç±»çš„ DL æ¡†æ¶ï¼Œä»å¤´å®ç°å®ƒä¹Ÿæ˜¯ç›¸å½“å›°éš¾å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚ç„¶è€Œæ‹¥æŠ±è„¸ä½¿å¾—å®ç°å„ç§ç±»å‹çš„å˜å½¢é‡‘åˆšå˜å¾—éå¸¸å®¹æ˜“ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•é€šè¿‡æ‹¥æŠ±äººè„¸åº“åœ¨ Tensorflow(Keras)ä¸­è½»æ¾å®ç°å˜å½¢é‡‘åˆšã€‚

# **ä½ éœ€è¦ä»€ä¹ˆ:**

é¦–å…ˆä½ éœ€è¦å®‰è£…æ‹¥æŠ±è„¸åº“ï¼Œè¿™çœŸçš„å¾ˆå®¹æ˜“ã€‚åªéœ€ç®€å•åœ°å®‰è£…å®ƒ:

```
pip install transformers 
```

å…¶æ¬¡ï¼Œæ‚¨å°†éœ€è¦æœ€æ–°çš„ TensorFlow ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬ä¹Ÿå¯ä»¥é€šè¿‡ pip è½»æ¾å®‰è£…ã€‚

**æ•°æ®:**

ä¸ºäº†æµ‹è¯•å’Œå®ç°ä¸åŒçš„è½¬æ¢å™¨ï¼Œæˆ‘ä½¿ç”¨äº† kaggle ç«èµ›ä¸­çš„æ•°æ®ã€‚è¿™æ˜¯æœ€è¿‘çš„ä¸€ä¸ªæ¯”èµ›ï¼Œæˆ‘å‚åŠ äº†ä¸€ä¸ªåä¸º[æ‹¼å›¾-å¤šè¯­è¨€-æœ‰æ¯’-è¯„è®º-åˆ†ç±»](https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification)çš„æ¯”èµ›ã€‚ä½†æ˜¯ï¼Œä½¿ç”¨ç›¸åŒçš„æ•°æ®å¹¶ä¸æ˜¯å¼ºåˆ¶æ€§çš„ï¼Œå› ä¸ºä¸‹é¢çš„å®ç°å¯ä»¥å¾ˆå®¹æ˜“åœ°é€‚åº”ä»»ä½•æ–‡æœ¬æ•°æ®ã€‚

è¿™åœºæ¯”èµ›ç»™å‡ºäº†ä¸åŒçš„è¯„è®ºï¼Œæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯æ£€æµ‹ç‰¹å®šçš„è¯„è®ºæ˜¯å¦æœ‰æ¯’ã€‚å› æ­¤ï¼Œè¿™æ˜¯ä¸€ä¸ªäºŒå…ƒåˆ†ç±»ä»»åŠ¡ã€‚

**å¼ºå¤§è®¡ç®—èƒ½åŠ›:**

è¿˜è¦æ³¨æ„ï¼Œå˜å‹å™¨æœ‰æ•°ç™¾ä¸‡ä¸ªå‚æ•°ï¼Œå› æ­¤æˆ‘åˆ©ç”¨ Kaggle å†…æ ¸æä¾›çš„ TPU æ¥è®­ç»ƒæˆ‘çš„æ¨¡å‹ã€‚æˆ–è€…ï¼Œå¦‚æœæ‚¨æ²¡æœ‰å¼ºå¤§çš„æœ¬åœ°æœºå™¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ google colab æ¥è·Ÿè¸ªæœ¬æ–‡çš„å®ç°ã€‚

# **è®©æˆ‘ä»¬äº«å—å®ç°å˜å½¢é‡‘åˆšçš„ä¹è¶£:**

![](img/9cc09362cda60ea8bdcf31fe063246b1.png)

å›¾ç‰‡æ¥è‡ª[https://huggingface.co/front/thumbnails/models.png](https://huggingface.co/front/thumbnails/models.png)

**è¿›å£**

```
import numpy as np *# linear algebra*
import pandas as pd *# data processing, CSV file I/O (e.g. pd.read_csv)*
import tensorflow as tf
import tensorflow_hub as hub
from tqdm import tqdm
from tqdm import tqdm_notebook
from sklearn.metrics import auc
from sklearn.metrics import classification_report
import seaborn as sns
import matplotlib.pyplot as plt

from transformers import AutoTokenizer,BertTokenizer,TFBertModel,TFOpenAIGPTModel,OpenAIGPTTokenizer,DistilBertTokenizer, TFDistilBertModel,XLMTokenizer, TFXLMModel
from transformers import TFAutoModel, AutoTokenizer
from kaggle_datasets import KaggleDatasets
from sklearn.metrics import roc_curve,confusion_matrix,auc
from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors
*# Input data files are available in the read-only "../input/" directory*
*# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory*
import matplotlib as mpl

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.initializers import Constant
```

**å“ªäº›å˜å‹å™¨:**

ä»¥ä¸‹å˜å‹å™¨æ¶æ„å·²åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸­è¿›è¡Œäº†æµ‹è¯•

1-ä¼¯ç‰¹

2-OpenAIGPT

3-è’¸é¦å•¤é…’

4-XLM

5-xlmrobertalage

ä¸ç”¨æ‹…å¿ƒæ‰€æœ‰è¿™äº›å˜å½¢é‡‘åˆšçš„å®ç°ã€‚å®ç°ç®€å•ä¸”ç›¸ä¼¼ã€‚

**ä½¿ç”¨çš„è¶…å‚æ•°:**

```
EPOCHS=2max_seq_length = 192
LEARNING_RATE=1e-5
early_stopping=early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', 
    verbose=1,
    patience=10,
    mode='max',
    restore_best_weights=True)
```

**ç¼–ç åŠŸèƒ½:**

æ¯ä¸€ä¸ªå˜å½¢é‡‘åˆšéƒ½å¯¹æ¯ä¸€å¥è¯è¿›è¡Œç¼–ç ã€‚æˆ‘å¸Œæœ›ä½ èƒ½ç†è§£è¿™å¥è¯çš„å«ä¹‰ã€‚å¦‚æœæ²¡æœ‰ï¼Œé‚£ä¹ˆä»–ä»¬åœ¨äº’è”ç½‘ä¸Šæœ‰è®¸å¤šäº†è§£ç¼–ç çš„å¥½èµ„æºã€‚åœ¨ä¸€ä¸ªéå¸¸åŸºæœ¬çš„å±‚é¢ä¸Šï¼Œç¼–ç æ„å‘³ç€é€šè¿‡ä¸ºæˆ‘ä»¬è¯­æ–™åº“ä¸­çš„æ¯ä¸ªå•è¯(æ ‡è®°)åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„æ•´æ•°æ¥å°†åŸå§‹æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ•°å­—æ•°æ®ã€‚ç„¶è€Œï¼Œtransformer ç¼–ç ç¨å¾®å¤æ‚ä¸€ç‚¹ï¼Œå› ä¸ºå®ƒä¹Ÿä½¿ç”¨å­—ç¬¦çº§ç¼–ç ï¼Œå°†æœªçŸ¥å•è¯åˆ†è§£æˆå•ä¸ªå­—ç¬¦ï¼Œç„¶åè¿›è¡Œç¼–ç ã€‚ç„¶è€Œï¼Œæˆ‘ä¸ä¼šè¿›ä¸€æ­¥æ·±å…¥å˜å‹å™¨ç¼–ç å¦‚ä½•å·¥ä½œçš„ç»†èŠ‚ï¼Œå› ä¸ºå®ƒç›¸å½“è¯¦ç»†ã€‚å¯ä»¥è‚¯å®šåœ°è¯´ï¼Œä¸‹ä¸€ä¸ªå‡½æ•°åŸºæœ¬ä¸Šå°†æ•°æ®ä¸­çš„æ¯ä¸ªå¥å­è½¬æ¢æˆå„ç§è½¬æ¢å™¨å¯ä»¥ç†è§£çš„ç‰¹æ®Šæ•´æ•°åˆ—è¡¨:

```
def single_encoding_function(text,tokenizer,name='BERT'):
    input_ids=[]
    if name=='BERT':
        tokenizer.pad_token ='[PAD]'
    elif name=='OPENAIGPT2':
        tokenizer.pad_token='<unk>'
    elif name=='Transformer XL':
        print(tokenizer.eos_token)
        tokenizer.pad_token= tokenizer.eos_token
    elif name=='DistilBert':
        tokenizer.pad_token='[PAD]'

for sentence **in** tqdm(text):       encoded=tokenizer.encode(sentence,max_length=max_seq_length,
pad_to_aax_length=True)## this is inside the loop
        input_ids.append(encoded)
    return input_ids
```

**åˆ¶ä½œæ•°æ®ç®¡é“:**

a)åˆ¶ä½œé˜µåˆ—:

```
X_train=np.array(single_encoding_function(train_raw['comment_text'].values.tolist(),tokenizer,name="BERT"))
y_train=np.array(train_raw['toxic'])
X_valid=np.array(single_encoding_function(valid_raw['comment_text'].values.tolist(),tokenizer,name="BERT"))
y_valid=np.array(valid_raw['toxic'])
X_test=np.array(single_encoding_function(test_raw['content'].values.tolist(),tokenizer,name="BERT"))steps_per_epoch = X_train.shape[0] // BATCH_SIZE
```

ä¸Šé¢çš„ä»£ç æ˜¯ä¸è¨€è‡ªæ˜çš„ï¼Œæˆ‘åªæ˜¯å°†åŸå§‹æ–‡æœ¬æ•°æ®ä½œä¸ºè¾“å…¥æä¾›ç»™å•ä¸ªç¼–ç å‡½æ•°ï¼Œç„¶åå°†ç»“æœè½¬æ¢ä¸ºç¼–ç ä»¤ç‰Œçš„æ•°ç»„ï¼Œè¿™æ˜¯æä¾›ç»™ TensorFlow ç®¡é“çš„æœ€ç»ˆæ•°æ®ã€‚

b)åˆ¶ä½œå¼ é‡æµç®¡é“:

```
def make_data():
    train = (
        tf.data.Dataset
        .from_tensor_slices((X_train, y_train))
        .repeat()
        .shuffle(2048)
        .batch(BATCH_SIZE)
        .prefetch(AUTO))

    valid = (
        tf.data.Dataset
        .from_tensor_slices((X_valid, y_valid))
        .batch(BATCH_SIZE)
        .cache()
        .prefetch(AUTO)
    )

    test = (
        tf.data.Dataset
        .from_tensor_slices(X_test)
        .batch(BATCH_SIZE)
    )
    return train,valid,test 
```

**ä¸ Keras çš„å®é™…å®æ–½:**

ä¸‹ä¸€æ­¥çœŸçš„å¾ˆé‡è¦ï¼Œæ‰€ä»¥ä»”ç»†çœ‹çœ‹:

```
def build_model(transformer_layer,max_len=max_seq_length):
    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name="input_word_ids")
    sequence_output = transformer_layer(input_word_ids)[0]

    cls_token = sequence_output[:, 0, :]
    out = tf.keras.layers.Dense(1, activation='sigmoid')(cls_token)

    model = tf.keras.Model(inputs=input_word_ids, outputs=out)

    return model
```

ä»£ç å—å¾ˆé‡è¦ï¼Œå› æ­¤è®©æˆ‘è¿›ä¸€æ­¥é˜è¿°å®ƒã€‚é¦–å…ˆæœ‰ä¸€ä¸ªè¾“å…¥å±‚ï¼Œå®ƒä¸ºä¸€ä¸ªç‰¹å®šçš„å®ä¾‹æ¥å—ç»™å®šå˜æ¢å™¨çš„**ç¼–ç è¾“å…¥ã€‚ç„¶åï¼Œè¾“å…¥ä»¤ç‰Œè¢«è¾“å…¥åˆ°ä¸» transformer å±‚(ä»å³å°†åˆ°æ¥çš„ä»£ç å—ä¸­å®šä¹‰çš„åä¸º compile_model çš„å‡½æ•°ä¸­åŠ è½½)ã€‚æˆ‘æƒ³è®©è¿™äº›ä»£ç å¯¹æ‰€æœ‰çš„å˜å½¢é‡‘åˆšéƒ½æ˜¯å¯é‡ç”¨çš„ï¼Œå› æ­¤ä¸æ˜¯å¤åˆ¶å’Œç²˜è´´æ¯ä¸ªå˜å½¢é‡‘åˆšçš„æ•´ä¸ªæ¨¡å‹ï¼Œå”¯ä¸€ä¸åŒçš„å±‚æ˜¯å˜å½¢é‡‘åˆšçš„å˜å½¢é‡‘åˆšå±‚ã€‚ç„¶åï¼Œè½¬æ¢å™¨å±‚è¾“å‡ºåºåˆ—è¾“å‡ºã€‚ç„¶è€Œï¼Œè¿™æ˜¯ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼Œå› æ­¤ä»åºåˆ—è¾“å‡ºä¸­ï¼Œæˆ‘ä»¬å°†åªæå–ç»™å®šå¥å­ä¸­æ¯ä¸ªå•è¯çš„ CLS(åˆ†ç±»æ ‡è®°)ã€‚è¿™ä¸ª cls_token ç„¶åè¢«é¦ˆé€åˆ°ç”¨äºåŒºåˆ†ç»™å®šå¥å­çš„æ¯’æ€§çš„ sigmoid å±‚ã€‚**

è¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥é€šè¿‡æ·»åŠ æ›´å¤šçš„å±‚æ¥ä½¿æ¨¡å‹æ›´åŠ å¤æ‚ï¼Œä½†æˆ‘æ²¡æœ‰è¿™æ ·åšï¼Œå› ä¸ºè¿™ä¼šä½¿æˆ‘ä»¬çš„æ¨¡å‹æ›´åŠ å¤æ‚ï¼Œå¹¶ä¸”ä¼šèŠ±è´¹æ›´å¤šçš„è®­ç»ƒæ—¶é—´ã€‚

## **ä¸‹ä¸€èŠ‚ä»…å±•ç¤ºå¦‚ä½•ç»˜åˆ¶å„å˜å‹å™¨æ€§èƒ½çš„ç›¸å…³æœ‰ç”¨å›¾è¡¨ï¼Œä»¥ä¾¿å¯¹ä¸åŒæ¨¡å‹è¿›è¡Œå¯¹æ¯”åˆ†æã€‚å®ƒä»¬ä¸ä»»ä½•è½¬æ¢å™¨çš„ä¸»è¦å®ç°éƒ½æœ‰å…³ç³»ã€‚**

**ç»˜åˆ¶æœ‰ç”¨çš„å›¾è¡¨æ¥æ¯”è¾ƒæ€§èƒ½:**

a)ç»˜åˆ¶æŸè€—å’Œåº¦é‡å›¾:

```
mpl.rcParams['figure.figsize'] = (12, 10)
colors = plt.rcParams['axes.prop_cycle'].by_key()['color']

def plot_loss(history):
*# Use a log scale to show the wide range of values.*
    plt.semilogy(history.epoch,  history.history['loss'],
               color='red', label='Train Loss')
    plt.semilogy(history.epoch,  history.history['val_loss'],
          color='green', label='Val Loss',
          linestyle="--")
    plt.xlabel('Epoch')
    plt.ylabel('Loss')

    plt.legend()

def plot_metrics(history):
    metrics =  ['loss', 'auc', 'precision', 'recall']
    for n, metric **in** enumerate(metrics):
        name = metric.replace("_"," ").capitalize()
        plt.subplot(2,2,n+1)
        plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')
        plt.plot(history.epoch, history.history['val_'+metric],
                 color=colors[0], linestyle="--", label='Val')
        plt.xlabel('Epoch')
        plt.ylabel(name)
        if metric == 'loss':
            plt.ylim([0, plt.ylim()[1]])
        elif metric == 'auc':
            plt.ylim([0.8,1])
        else:
            plt.ylim([0,1])

        plt.legend()
```

è¿™ä¸¤ä¸ªå‡½æ•°éƒ½é‡‡ç”¨è®­ç»ƒçš„å†å²ï¼Œç„¶åç»˜åˆ¶ä¸¢å¤±å’Œåº¦é‡çš„ç›¸å…³å‡½æ•°ï¼Œå³ AUCã€å¬å›å’Œå¤šä¸ªæ—¶æœŸçš„ç²¾åº¦ã€‚

b)ç»˜åˆ¶æ··æ·†çŸ©é˜µå’Œ ROC æ›²çº¿:

ä¸‹ä¸€ä¸ªä»£ç å—ä»æ¨¡å‹å’ŒåŸºç¡€äº‹å®ä¸­è·å– y_predictedï¼Œä¸ºæ¨¡å‹åˆ›å»ºæ··æ·†çŸ©é˜µå’Œ ROC æ›²çº¿ã€‚

```
def plot_cm(y_true, y_pred, title):
    *''''*
 *input y_true-Ground Truth Labels*
 *y_pred-Predicted Value of Model*
 *title-What Title to give to the confusion matrix*

 *Draws a Confusion Matrix for better understanding of how the model is working*

 *return None*

 *'''*

    figsize=(10,10)
    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))
    cm_sum = np.sum(cm, axis=1, keepdims=True)
    cm_perc = cm / cm_sum.astype(float) * 100
    annot = np.empty_like(cm).astype(str)
    nrows, ncols = cm.shape
    for i **in** range(nrows):
        for j **in** range(ncols):
            c = cm[i, j]
            p = cm_perc[i, j]
            if i == j:
                s = cm_sum[i]
                annot[i, j] = '**%.1f%%\n%d**/**%d**' % (p, c, s)
            elif c == 0:
                annot[i, j] = ''
            else:
                annot[i, j] = '**%.1f%%\n%d**' % (p, c)
    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))
    cm.index.name = 'Actual'
    cm.columns.name = 'Predicted'
    fig, ax = plt.subplots(figsize=figsize)
    plt.title(title)
    sns.heatmap(cm, cmap= "YlGnBu", annot=annot, fmt='', ax=ax)

def roc_curve_plot(fpr,tpr,roc_auc):
    plt.figure()
    lw = 2
    plt.plot(fpr, tpr, color='darkorange',
             lw=lw, label='ROC curve (area = **%0.2f**)' %roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic example')
    plt.legend(loc="lower right")
    plt.show()
```

**æœ€å:D è®­ç»ƒä¸åŒçš„å˜å½¢é‡‘åˆš:**

a)ç¼–è¯‘æ¨¡å‹:

```
def compile_model(name):
    with strategy.scope():
        METRICS = [
          tf.keras.metrics.TruePositives(name='tp'),
          tf.keras.metrics.FalsePositives(name='fp'),
          tf.keras.metrics.TrueNegatives(name='tn'),
          tf.keras.metrics.FalseNegatives(name='fn'), 
          tf.keras.metrics.BinaryAccuracy(name='accuracy'),
          tf.keras.metrics.Precision(name='precision'),
          tf.keras.metrics.Recall(name='recall'),
          tf.keras.metrics.AUC(name='auc')]
        if name=='bert-base-uncased':
            transformer_layer = (
                TFBertModel.from_pretrained(name)
            )
        elif name=='openai-gpt':
            transformer_layer = (
                TFOpenAIGPTModel.from_pretrained(name)
            )
        elif name=='distilbert-base-cased':
            transformer_layer = (
                TFDistilBertModel.from_pretrained(name)
            )
        elif name=='xlm-mlm-en-2048':
            transformer_layer = (
                TFBertModel.from_pretrained(name)
            )
        elif name=='jplu/tf-xlm-roberta-large':
            transformer_layer = (
                TFAutoModel.from_pretrained(name)
            )
        model = build_model(transformer_layer, max_len=max_seq_length)
        model.compile(optimizer=tf.keras.optimizers.Adam(
        learning_rate=LEARNING_RATE), loss='binary_crossentropy', metrics=METRICS)
    return model
```

ä¸Šé¢çš„ä»£ç å—çœŸçš„å¾ˆç®€å•ã€‚å¦‚å›¾æ‰€ç¤ºï¼Œå®ƒé‡‡ç”¨æ‚¨è¦ç¼–è¯‘æ¨¡å‹çš„å˜å‹å™¨çš„åç§°ï¼Œç„¶åä» hugging face library åŠ è½½ç›¸å…³çš„å˜å‹å™¨å±‚ã€‚ç„¶åï¼Œå®ƒå°†åŠ è½½çš„ transformer å±‚æä¾›ç»™å‡½æ•° build_model(ä¸Šé¢å®šä¹‰çš„),ç„¶åæˆ‘ä»¬ç¼–è¯‘è¿™ä¸ªæ¨¡å‹ã€‚è¯·æ³¨æ„ï¼Œæˆ‘è¿˜åˆ›å»ºäº†ä¸€ä¸ªåä¸º METRICS çš„åˆ—è¡¨ï¼Œå› ä¸ºæˆ‘æƒ³æ£€æŸ¥ä¸åŒæŒ‡æ ‡çš„æ¨¡å‹æ€§èƒ½ï¼Œè€Œä¸æ˜¯é™åˆ¶è‡ªå·±åªå…³æ³¨å‡†ç¡®æ€§ã€‚

d)å®é™…åŸ¹è®­:

ç°åœ¨ï¼Œæ¯ä¸ªå˜å‹å™¨çš„å®é™…è®­ç»ƒè¿‡ç¨‹æ˜¯ç›¸åŒçš„ã€‚ä½ åªéœ€è¦è¾“å…¥ç›¸å…³çš„åå­—å¹¶è°ƒç”¨æœŸæœ›çš„å‡½æ•°æ¥é€‚åº”è¿™ä¸ªæ¨¡å‹ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä¸ä¼šæ˜¾ç¤ºæ¯ä¸ªæ¨¡å‹çš„è¾“å‡ºå›¾ï¼Œå› ä¸ºè¿™å°†å ç”¨å¤§é‡çš„ç©ºé—´ï¼Œä½†æ˜¯ï¼Œæˆ‘å°†åªæ˜¾ç¤ºä¸€ä¸ªå˜å‹å™¨çš„å›¾å½¢ï¼Œå³æå–çš„ BERTã€‚å¯ä»¥ä»¥ç±»ä¼¼çš„æ–¹å¼ä¸ºå…¶ä»–æ¨¡å‹ç”Ÿæˆç›¸åŒçš„å›¾å½¢ã€‚

æ¥ä¸‹æ¥çš„ä»£ç å—å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ä¸Šé¢ä¸ºæå–çš„ BERT å®šä¹‰çš„å‡½æ•°ã€‚ä¸ºäº†è®­ç»ƒä»»ä½•å…¶ä»–è½¬æ¢å™¨ï¼Œæ‚¨åªéœ€è¦å°†åä¸ºâ€œdistilt-base-casedâ€å’Œâ€œDistilBertâ€(ç²—ä½“çªå‡ºæ˜¾ç¤º)çš„å­—ç¬¦ä¸²æ›´æ”¹ä¸ºç›¸å…³çš„è½¬æ¢å™¨åç§°ã€‚

```
*# # First load the real tokenizer*
tokenizer = DistilBertTokenizer.from_pretrained(**'distilbert-base-cased'**)X_train=np.array(single_encoding_function(train_raw['comment_text'],tokenizer,**'DistilBert'**))#change the name
y_train=np.array(train_raw['toxic'])
X_valid=np.array(single_encoding_function(valid_raw['comment_text'],tokenizer,**'DistilBert'**))#change the name
y_valid=np.array(valid_raw['toxic'])
X_test=np.array(single_encoding_function(test_raw['content'],tokenizer,**'DistilBert'**))#change the nametrain,valid,test=make_data()steps_per_epoch = X_train.shape[0] // BATCH_SIZEmodel=compile_model(**'distilbert-base-cased'**)#change the name
print(model.summary())

history=model.fit(
    train,steps_per_epoch=steps_per_epoch,
    epochs=EPOCHS,callbacks=[early_stopping], validation_data=valid
)
```

![](img/bb6b097781b3c8a80651fabc066ba0d0.png)

æ¨¡å‹æ€»ç»“å’ŒåŸ¹è®­ä¿¡æ¯

è¯¥æ¨¡å‹å°†å¼€å§‹è®­ç»ƒï¼Œå¹¶éœ€è¦å¾ˆé•¿æ—¶é—´ï¼Œè¿™å–å†³äºæ‚¨çš„æ•°æ®å’Œè®¡ç®—èƒ½åŠ›ã€‚æœ€åï¼Œè·å–å†å²è®°å½•å¹¶ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„ç›¸å…³å‡½æ•°æ¥ç”Ÿæˆå›¾è¡¨:

```
plot_loss(history)
```

![](img/0622602c26af4a6e4d692a588e969a0b.png)

distilt-BERT ä¸¤ä¸ªæ—¶æœŸçš„æŸå¤±

```
plot_metrics(history)
```

![](img/5d45fd7f521f145c7f09195aaaf178cb.png)

å„æ—¶æœŸçš„æŸå¤±ã€AUCã€ç²¾ç¡®åº¦å’Œå¬å›

```
y_predict=model.predict(valid, verbose=1)
y_predict[ y_predict> 0.5] = 1
y_predict[y_predict <= 0.5] = 0
plot_cm(y_valid, y_predict, 'Distil BERT Performance-Confusion Matrix')
```

![](img/499f002b4dc58e6562124fc739bf2c9e.png)

è’¸é¦æ°´æ··æ·†çŸ©é˜µ

```
y_predict_prob=model.predict(valid, verbose=1)
fpr, tpr, _ = roc_curve(y_valid,y_predict_prob)
roc_auc = auc(fpr, tpr)
roc_curve_plot(fpr,tpr,roc_auc)
```

![](img/7a9017abac03ac9ece4f812e19b73938.png)

ä¸€ç§è’¸é¦å•¤é…’ç”¨æ‘‡åºŠ

# ç»“è®º:

å—¯ï¼Œæ‹¥æŠ±è„¸çœŸçš„è®© transformer çš„å®ç°å˜å¾—éå¸¸å®¹æ˜“ã€‚ç„¶è€Œï¼Œç†è§£è½¬æ¢å™¨çš„åº•å±‚å·¥ä½œä¹Ÿéå¸¸é‡è¦ï¼Œå› ä¸ºå¦åˆ™ä¸Šè¿°å®ç°å°†åªæ˜¯ä¸€ä¸ªé»‘ç›’ï¼Œæ‚¨å°†æ— æ³•è¿›ä¸€æ­¥è°ƒæ•´å’Œä¼˜åŒ–æ‚¨çš„æ¨¡å‹ã€‚æˆ‘çš„ kaggle è´¦æˆ·ä¸Šä¹Ÿæœ‰å®Œæ•´çš„ä»£ç :[https://www . ka ggle . com/keen border/comparing-different-transformers-lstms](https://www.kaggle.com/keenborder/comparing-different-transformers-lstms)å¦‚æœä½ æƒ³æ›´è¯¦ç»†åœ°äº†è§£å˜å½¢é‡‘åˆšï¼Œè¿™é‡Œä¹Ÿæœ‰é“¾æ¥ã€‚

æˆ‘ä¸ºè¿™ç¯‡æ–‡ç« çœŸçš„å¾ˆåŠªåŠ›ï¼Œå› æ­¤ï¼Œè¯·é¼“æŒï¼Œå¦‚æœä½ å–œæ¬¢å®ƒï¼Œå¹¶æƒ³çœ‹åˆ°æ›´å¤šå¯æ€•çš„ NLP å†…å®¹ã€‚