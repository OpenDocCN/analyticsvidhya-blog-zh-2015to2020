<html>
<head>
<title>Faster RCNN on Indian Driving Dataset(IDD)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">印度驾驶数据集(IDD)上的快速RCNN</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/faster-rcnn-on-indian-driving-dataset-idd-ddfdaa0f02d7?source=collection_archive---------12-----------------------#2020-03-21">https://medium.com/analytics-vidhya/faster-rcnn-on-indian-driving-dataset-idd-ddfdaa0f02d7?source=collection_archive---------12-----------------------#2020-03-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="0c17" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">这个博客解释了更快的RCNN以及如何在IDD上应用它。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/72d19a2128f03b54e24b4fa788c31e09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*O-UoSBhBrcBDhPln"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">埃里克·韦伯在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h1 id="8fd9" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">概观</h1><p id="23df" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">IDD数据集由从连接到汽车的前置摄像头获得的图像组成。它由10，000幅图像组成，用从印度道路上的182个驾驶序列中收集的34个类别进行了精细注释。这辆车在海德拉巴、班加罗尔城市及其郊区行驶。这些图像大多是1080p分辨率，但也有一些图像具有720p和其他分辨率。这是由IIT海德拉巴和英特尔赞助的。</p><p id="d1b9" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated"><a class="ae jn" href="https://idd.insaan.iiit.ac.in/dataset/download/" rel="noopener ugc nofollow" target="_blank"> <em class="lh">官网</em> </a> <em class="lh">共有五个数据集。你可能需要注册才能使用它。我用过名为<strong class="ki hj">的数据集数据集名称:IDD- Detection (22.8 GB) </strong>。我已经在git中解释了整个文件夹结构和代码。为了简洁起见，我在这里就长话短说。给定的输入数据是图像和包含图像信息的xml。</em></p><p id="06a0" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">更快的RCNN(一种用于深度学习领域的对象检测技术)于2015年在NIPS发表。出版后，它经历了一些修改。更快的R-CNN是R-CNN的第三代。我相信RCNN在图像分割和边界框目标检测领域是革命性的。下图中带边界框的<strong class="ki hj">对象检测</strong>(通常称为定位)和<strong class="ki hj">分割</strong>的区别。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es li"><img src="../Images/d39aedcb626e3e994710091d551dca51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*FfwGJOHT-SEzzM0_f9pPBA.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">目标检测vs分割源(<a class="ae jn" href="https://miro.medium.com/max/600/0*VX1UNfrNgjRD24Kq.jpg" rel="noopener">https://miro.medium.com/max/600/0*VX1UNfrNgjRD24Kq.jpg</a>)</figcaption></figure><p id="0efa" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">对象检测输出对象和相应的边界框，而另一方面，分割标记对象的像素。</p></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><p id="3893" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">如果您想了解更多关于我如何格式化IDD数据的信息，从xml解析到图像复制再到为RCNN创建CSV，请访问我的<a class="ae jn" href="https://github.com/prabhudayala/IDD-data-set-Faster-RCNN" rel="noopener ugc nofollow" target="_blank"> git </a>并参考jupyter笔记本1和2。为了简洁起见，我将在这个博客上解释更快的RCNN。</p></div><div class="ab cl lj lk gp ll" role="separator"><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo lp"/><span class="lm bw bk ln lo"/></div><div class="hb hc hd he hf"><h1 id="c4fa" class="jo jp hi bd jq jr lq jt ju jv lr jx jy io ls ip ka ir lt is kc iu lu iv ke kf bi translated">体系结构</h1><p id="294a" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">RCNN架构有点复杂，因为它有许多部分。让我们了解它的所有活动部分，稍后我们将连接这些点。</p><h2 id="2767" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">输入和输出</h2><p id="f203" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">FRCNN的输入是一个图像，输出由以下三个部分组成。包围盒列表<br/> 2。分配给每个边界框<br/> 3的标签。每个标签和边界框的概率</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mj"><img src="../Images/3403f792aea405bfb931caac7afcac0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uQNculABX73lqJwESVHr5w.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">RCNN architechure来源(<a class="ae jn" href="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/fasterrcnn-architecture.b9035cba.png" rel="noopener ugc nofollow" target="_blank">https://tryo labs . com/images/blog/post-images/2018-01-18-faster-RCNN/fasterr CNN-architecture . b 9035 CBA . png</a>)</figcaption></figure><h2 id="a2b9" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">基础网络</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mk"><img src="../Images/33f73ce248e26f828ac097213c47b1f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*ifWuy38BnGgNavrNrmgJbQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">基网来源(<a class="ae jn" href="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/image-to-feature-map.89f5aecb.png" rel="noopener ugc nofollow" target="_blank">https://tryolabs . com/images/blog/post-images/2018-01-18-faster-rcnn/image-to-feature-map . 89 F5 aecb . png</a>)</figcaption></figure><p id="8bcb" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">输入图像通过预先训练的模型(在我们的例子中是VGG)向上传递，直到中间层，在那里它获得特征图，该特征图在下一阶段中进一步用作3d张量。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mm l"/></div></figure><h2 id="19e9" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">锚</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mn"><img src="../Images/256522039e17085dfa81bea13a85aab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FBGB1n_cvnKeIypzy1rvOA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图片来源上的锚点(<a class="ae jn" href="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/anchors-centers.141181d6.png" rel="noopener ugc nofollow" target="_blank">https://tryolabs . com/images/blog/post-images/2018-01-18-faster-rcnn/anchors-centers . 141181d 6 . png</a>)</figcaption></figure><p id="3cc3" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">锚点是固定的边界框，以不同的大小和比例放置在整个图像中，将在首次预测对象位置时用作参考。如果我们收到尺寸为<em class="lh"> conv </em> <em class="lh">宽度</em> × <em class="lh"> conv </em> <em class="lh">高度</em> × <em class="lh"> conv </em> <em class="lh">深度</em>的特征图，我们为<em class="lh"> conv </em> <em class="lh">宽度</em> × <em class="lh"> conv </em> <em class="lh">高度中的每个点创建一组锚点。</em>虽然锚点是在特征图上定义的，但它们指的是实际图像的比例。如果我们考虑所有的锚点，它看起来会像这样。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mj"><img src="../Images/2ef4369d6c141c199db46ca35d6faf9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZ4ekpBgRk4q9vKTxfnxyA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">锚点左:单锚点，中:图像上所有比例的单锚点，右:图像源上所有比例的所有锚点(<a class="ae jn" href="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/anchors-progress.119e1e92.png" rel="noopener ugc nofollow" target="_blank">https://tryo labs . com/images/blog/post-images/2018-01-18-faster-rcnn/anchors-progress . 119 e1e 92 . png</a>)</figcaption></figure><h2 id="cfa3" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">区域提案网络</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mj"><img src="../Images/e4f0ddbfdf1f2d85fd5c8599287e0ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mbbAiXG16T_j_Ag4u4DWZA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">RPN来源(<a class="ae jn" href="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/rpn-architecture.99b6c089.png" rel="noopener ugc nofollow" target="_blank">https://tryo labs . com/images/blog/post-images/2018-01-18-faster-rcnn/RPN-architecture . 99 b6c 089 . png</a>)</figcaption></figure><p id="f7d5" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">RPN取所有参考框(锚点)，输出一组好的对象建议。它通过为每个锚点提供两个不同的输出来实现这一点。<br/> 1。区域<br/> 2的客观性分数。用于调整锚点以更好地预测对象的边界框回归</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="edd8" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">上面的代码创建锚的数量(作为参数传递给方法的数量)并返回建议的客观性分数和实际边界框的delta误差。该客观性分数稍后用于接受或拒绝该提议。</p><h2 id="abed" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">利息池区域(ROI)</h2><p id="0b94" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">在RPN步骤之后，我们有一堆没有指定类的对象提议。我们下一个要解决的问题是如何把这些包围盒分类到我们想要的类别中。我们可以接受每一个建议，对其进行裁剪，并通过预先训练好的网络，然后用它来对图像进行分类，但是这种方法太慢了，因为我们有大量的建议。<br/>更快的RCNN通过重用我们从<strong class="ki hj">基网络</strong>输出得到的现有特征图解决了这个问题。它使用兴趣池区域为每个提议提取固定大小的特征地图。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mj"><img src="../Images/f21cb1ab971bcc01aa607fa33ad24f34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fqrmwuqhqwW9HkOrPC-8Jw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">RPN来源(<a class="ae jn" href="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/roi-architecture.7eaae6c2.png" rel="noopener ugc nofollow" target="_blank">https://tryo labs . com/images/blog/post-images/2018-01-18-faster-rcnn/ROI-architecture . 7 eaae 6 c 2 . png</a>)</figcaption></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="091e" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">收到建议后，此代码为ROI中选择的每个建议选择固定大小(7*7)的特征图。</p><h2 id="002a" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">基于区域的卷积神经网络</h2><p id="8e6a" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">这是整个模型的最后一层。在从最后一层(7x7x512)获得特征图之后，RCNN服务于两个目标。<br/> 1。将提议归入其中一个类别，外加一个背景类别(用于删除不良提议)。<br/> 2。更好地根据预测的类别调整提议的边界框。<br/>而他们就是这样的形象实现的。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mj"><img src="../Images/9b0259b9f73a21f53ea364a0cdeb7b59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZzK5vadpNE4h1OgEGYdvrw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">RCNN最终层-来源(<a class="ae jn" href="https://tryolabs.com/images/blog/post-images/2018-01-18-faster-rcnn/rcnn-architecture.6732b9bd.png" rel="noopener ugc nofollow" target="_blank">https://tryo labs . com/images/blog/post-images/2018-01-18-faster-RCNN/RCNN-architecture . 6732 b9bd . png</a>)</figcaption></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="b4ba" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">这段代码使用<strong class="ki hj"> RoiPoolingConv </strong>并从它那里接收7*7固定大小的输出作为特征映射。稍后，这些输出被展平并用于<strong class="ki hj">将图像分类到类别</strong>并且<strong class="ki hj">调整边界框以更符合实际</strong>。在这样做的同时，它输出两个操作的错误。</p><h2 id="9de5" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">借据</h2><p id="3c9d" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">IOU代表交集大于并集。这用于决定是否应该接受关于实际边界框的提议边界框。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es li"><img src="../Images/48e91c8b1b812862bc82532166a1c272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*2LPQLE87SJBRCSXhpow9sA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">IOU-source<a class="ae jn" href="https://www.pyimagesearch.com/wp-content/uploads/2016/09/iou_equation.png" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/WP-content/uploads/2016/09/IOU _ equation . png</a></figcaption></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="4cc7" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">上面的代码简单地计算两个矩形边界框的面积，并计算交集与并集。</p><h2 id="9e1b" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">非最大抑制</h2><p id="abf3" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">锚定提议在真实案例中经常重叠。为了解决这个问题，使用了NMS。NMS采用按分数排序的提案列表，并在排序的列表上迭代，用具有更高分数的提案丢弃那些IoU大于某个预定义阈值的提案。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mo"><img src="../Images/619de378923bb50f481a65afa65cd141.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HdAWr4t6AJ6dXLw9k4KBsQ.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">非最大抑制-来源(<a class="ae jn" href="https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2015/02/16/faster-non-maximum-suppression-python/</a>)</figcaption></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="cde3" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">这段代码将RPN提出的边界框和它们的概率(<strong class="ki hj"> objectness score </strong>我在前面描述过)作为输入——并删除重叠的提议。它通过将所有其他边界框的<strong class="ki hj"> IOU </strong>与具有最大概率的边界框进行比较来实现。如果任何框相对于具有最大概率的框的<strong class="ki hj"> IOU </strong>大于重叠阈值，则将其删除。</p><h2 id="72df" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">损耗</h2><p id="1e4a" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">在整个更快的RCNN中涉及4个损耗。<br/> 1。RPN分类损失(我前面描述过的客观性分数)</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="583a" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">2.RPN回归损失</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="44d0" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">3.最后一层的分类损失</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="e309" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">4.最后一层的回归损失</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mm l"/></div></figure><h1 id="07a5" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">让我们连接所有组件</h1><h2 id="b9f4" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">1.获取地面真实数据</h2><p id="6109" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">我们将采取一个图像的时间，并得到锚地的真相。当我说锚时，你现在必须明白它是边界框。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="6b86" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">这段代码采用一个图像路径，并在根据图像缩放比例对边界框进行缩放后返回边界框。例如，如果在1000*1000像素的图像中，对象在(10，10，30，30)处，如{xmin，ymin，xmax，ymax}，那么在将图像重新缩放到100*100像素之后，相对对象位置将是(1，1，3，3)，如{xmin，ymin，xmax，ymax}。该方法还创建RPN建议和损失(目标得分和边界框增量),稍后用于寻找ROI。</p><h2 id="1ad0" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">2.使用RPN模型并找到ROI</h2><p id="8cd2" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">用RPN模型预测一些方案，得到一些ROI。在本部分中,<strong class="ki hj">非最大抑制</strong>用于排除过度重叠的建议。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="2b3a" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">该方法使用来自RPN层的预先计算的建议，进行非最大值抑制，并选择那些不重叠的建议。</p><h2 id="cdb3" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">3.计算欠条</h2><p id="d579" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">计算IOU，并基于此选择一些正负样本。负样本在这里意味着没有对象的建议，换句话说，没有背景。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="ml mm l"/></div></figure><p id="eb6b" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">以上是为每个边界框计算的IOU，并返回相应的IOU。</p><h2 id="46d5" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">4.训练整个模型直到收敛。</h2><p id="dd6e" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">根据IOU值，我们将创建一些正样本(<strong class="ki hj">带对象</strong>)和一些负样本(<strong class="ki hj">不带对象—背景</strong>)，并将它们传递给模型分类器。我们将尝试最小化前面描述的所有4个误差的总和。训练直到收敛并保存最好的模型。</p><h2 id="2a24" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">5.一些误差图。</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mp"><img src="../Images/c591f74670082845ffbe200ad2ae4770.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vb81X7sOFiUt70Tt4d-eoQ.png"/></div></div></figure><h2 id="6af1" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">6.全损</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mq"><img src="../Images/d6975441944b58ddfc705500d69b3077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*YtEa9DpV1aBfpTOiTRTgxw.png"/></div></figure><h2 id="6518" class="lv jp hi bd jq lw lx ly ju lz ma mb jy kp mc md ka kt me mf kc kx mg mh ke mi bi translated">7.图像测试</h2><p id="b2af" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">在图像上测试代码与培训非常相似。在我的<a class="ae jn" href="https://github.com/prabhudayala/IDD-data-set-Faster-RCNN" rel="noopener ugc nofollow" target="_blank"> git </a>链接中，你可以找到一个名为Pipeline的Jupyter笔记本，在那里你可以自己测试图像。下面是一些结果。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mr"><img src="../Images/37367032e006034266f2dc10eb6a5f61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cdaYL75oCqXCjNg4HG7a2g.png"/></div></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mr"><img src="../Images/44e3466aa62655aab2a1376ed66361bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2bnOVHxSv2Ac4Cw-rU-sZQ.png"/></div></div></figure><p id="c6e0" class="pw-post-body-paragraph kg kh hi ki b kj lc ij kl km ld im ko kp le kr ks kt lf kv kw kx lg kz la lb hb bi translated">就结果而言，这里有很大的改进余地。<strong class="ki hj">我已经运行这个模型将近100个时期了</strong>。这需要大量的时间和大量的GPU资源。<strong class="ki hj">尝试跑更多的时代，一定会有更好的模式</strong>。</p><h1 id="04c1" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">结束注释</h1><p id="6d70" class="pw-post-body-paragraph kg kh hi ki b kj kk ij kl km kn im ko kp kq kr ks kt ku kv kw kx ky kz la lb hb bi translated">快速RCNN模型相当复杂，它是许多模型组合和多年研究的结果。如果你觉得代码很难理解，完全没关系。请访问我的<a class="ae jn" href="https://github.com/prabhudayala/IDD-data-set-Faster-RCNN" rel="noopener ugc nofollow" target="_blank"> git </a>获取完整代码，并尝试一段一段地运行代码。肯定会有帮助的。如果没有下面提到的参考链接，这个博客是不可能的。</p><h1 id="c286" class="jo jp hi bd jq jr js jt ju jv jw jx jy io jz ip ka ir kb is kc iu kd iv ke kf bi translated">参考</h1><ol class=""><li id="3b30" class="ms mt hi ki b kj kk km kn kp mu kt mv kx mw lb mx my mz na bi translated"><a class="ae jn" href="https://tryolabs.com/blog/2018/01/18/faster-r-cnn-down-the-rabbit-hole-of-modern-object-detection/" rel="noopener ugc nofollow" target="_blank">https://tryo labs . com/blog/2018/01/18/faster-r-CNN-down-the-rabbit-hole-of-modern-object-detection/</a></li><li id="a687" class="ms mt hi ki b kj nb km nc kp nd kt ne kx nf lb mx my mz na bi translated"><a class="ae jn" href="https://github.com/RockyXu66/Faster_RCNN_for_Open_Images_Dataset_Keras" rel="noopener ugc nofollow" target="_blank">https://github . com/rocky Xu 66/Faster _ RCNN _ for _ Open _ Images _ Dataset _ Keras</a></li></ol></div></div>    
</body>
</html>