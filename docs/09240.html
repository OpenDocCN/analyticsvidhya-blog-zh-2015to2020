<html>
<head>
<title>Pre-Processing and Model Training using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python进行预处理和模型训练</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pre-processing-and-model-training-using-python-174ed43dd8bb?source=collection_archive---------34-----------------------#2020-08-29">https://medium.com/analytics-vidhya/pre-processing-and-model-training-using-python-174ed43dd8bb?source=collection_archive---------34-----------------------#2020-08-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3010" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">了解数据预处理及其在模型训练中的重要性</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/e2417ac4e0697e1464262c2d42675059.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OxpYWxBSblfO-6kD"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">马库斯·温克勒在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="2ea6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在机器学习中，预处理和模型训练密切相关，这意味着二者缺一不可。问题是，我们人类与我们能够理解的数据进行交互，这些数据是用自然语言编写的，我们希望我们的机器学习模型能够接受相同的数据，并给我们一些见解。嗯，机器只能理解二进制语言(0和1 ),必须有一种方法让机器理解同样的数据。这就是预处理的用武之地。预处理基本上是将自然语言中的数据转换成机器可以理解的形式。这个过程也称为编码。</p><p id="03b9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">那么我们如何做预处理和模型训练，从数据中获得洞察呢？有多种方法可以预处理数据和训练机器学习模型，当然我不知道所有的方法。在这篇文章中，我们将看看一些预处理方法，并探索三种我们可以用来训练模型的机器学习算法。因此，我们将研究:</p><p id="7b92" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">预处理</strong></p><ul class=""><li id="fded" class="kk kl hi jq b jr js ju jv jx km kb kn kf ko kj kp kq kr ks bi translated">数据清理</li><li id="6dcb" class="kk kl hi jq b jr kt ju ku jx kv kb kw kf kx kj kp kq kr ks bi translated">使用OneHotEncoder的分类特征编码</li><li id="48f9" class="kk kl hi jq b jr kt ju ku jx kv kb kw kf kx kj kp kq kr ks bi translated">使用标准缩放器缩放数字特征</li><li id="8ac1" class="kk kl hi jq b jr kt ju ku jx kv kb kw kf kx kj kp kq kr ks bi translated">使用主成分分析、T-SNE和自动编码器进行降维</li><li id="2a0d" class="kk kl hi jq b jr kt ju ku jx kv kb kw kf kx kj kp kq kr ks bi translated">通过过采样平衡类</li><li id="c3b6" class="kk kl hi jq b jr kt ju ku jx kv kb kw kf kx kj kp kq kr ks bi translated">特征抽出</li></ul><p id="8fd9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">模特培训</strong></p><ul class=""><li id="b2ba" class="kk kl hi jq b jr js ju jv jx km kb kn kf ko kj kp kq kr ks bi translated">逻辑回归</li><li id="c8b7" class="kk kl hi jq b jr kt ju ku jx kv kb kw kf kx kj kp kq kr ks bi translated">随机森林</li><li id="2d05" class="kk kl hi jq b jr kt ju ku jx kv kb kw kf kx kj kp kq kr ks bi translated">决策树</li></ul></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><p id="557c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这篇文章中，我们将使用银行活动数据，这些数据可以在<a class="ae jn" href="http://archive.ics.uci.edu/ml/datasets/Bank+Marketing" rel="noopener ugc nofollow" target="_blank">这里</a>找到。除了数据之外，源还对数据中的要素进行了全面描述。简而言之，我们的数据由描述客户的20个特征、10个分类特征和10个数字特征以及目标变量组成。我们的目标变量是客户是否订阅定期存款的表示。这个项目的目标是预测哪些未来的客户将订阅定期存款。要对我们的数据集进行全面的探索性数据分析，您可以查看<a class="ae jn" href="https://dub01.online.tableau.com/t/multipleviews/views/Kiiru-Anastasia_Bank_Data_EDA/EDAforTermDepositSubscriptionsY?:showAppBanner=false&amp;:display_count=n&amp;:showVizHome=n&amp;:origin=viz_share_link" rel="noopener ugc nofollow" target="_blank"> my Tableau Dashboard </a>或在<a class="ae jn" href="https://github.com/Kiiru-Anastasia/10acadWeeklyChallenges/blob/master/Week6/notebooks/Term_Deposit.Predictive_Model.ipynb" rel="noopener ugc nofollow" target="_blank">这个GitHub存储库</a>上进行同样的操作。这篇文章将只关注预处理和模型训练。</p><h1 id="ed2b" class="lf lg hi bd lh li lj lk ll lm ln lo lp io lq ip lr ir ls is lt iu lu iv lv lw bi translated">预处理</h1><h2 id="e79e" class="lx lg hi bd lh ly lz ma ll mb mc md lp jx me mf lr kb mg mh lt kf mi mj lv mk bi translated">数据清理</h2><p id="6f46" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx mn jz ka kb mo kd ke kf mp kh ki kj hb bi translated">数据清理包括检查数据集中的缺失值，根据缺失值的数量及其在数据中的重要性，删除空行或输入缺失值。数据清理还包括在我们的数据中寻找重复数据并删除它们，因为它们可能会显著影响模型的有效性。数据清理还包括检查数据中的异常值，并根据异常值出现的频率用中值或平均值替换数据。这只是我们将在本帖中探讨的几种数据清理方法中的一部分。</p><p id="68bd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">缺失值</strong></p><p id="5984" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们可以通过调用<code class="du mq mr ms mt b">dataset.info()</code>并检查数据集中的所有特性来检查数据中的缺失值，或者简单地运行<code class="du mq mr ms mt b">dataset.isnull().values.any()</code>，如果数据集中有任何空值，它将返回True，如果没有，则返回False。然后，我们可以决定删除包含唯一数据的值，或者用模式数据估算分类特征，用均值数据估算数值特征。我们的银行数据集没有缺失值，因此我们继续下一步。</p><p id="2fdb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">重复数据</strong></p><p id="0aa1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">使用<code class="du mq mr ms mt b">dataset[dataset.duplicated()]</code>检查数据中的重复行。我们的银行数据有12个重复的行，因此，我们可以使用<code class="du mq mr ms mt b">dataset = dataset.drop_duplicates()</code>删除这些行，并将结果数据作为新数据集分配给我们的数据集变量。</p><p id="7890" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">异常值检测</strong></p><p id="1703" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">通过简单地绘制特征的方框图，很容易检测数值特征中的异常值。我已经定义了一个函数，可以帮助你绘制所有的数字特征的箱线图，你只需要把你的数据集和特征的名称传递给这个函数。该功能是:</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="f447" class="lx lg hi mt b fi my mz l na nb">def plot_outliers(data, features):<br/>    fig, ax = plt.subplots(len(features), figsize = (8, 40))<br/>    <br/>    for i, feature in enumerate(features):<br/>        sns.boxplot(y = data[feature], ax = ax[i])<br/>        ax[i].set_title('Box plot - {}'.format(feature), fontsize = 10)<br/>        ax[i].set_xlabel(feature, fontsize = 8)<br/>    <br/>    plt.show()</span></pre><p id="aed3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">然后，如果离群值很多，我们可以用中值替换离群值，如果离群值不多，我们可以用平均值替换离群值。下面的函数将为您自动完成这个过程。您只需传入您的数据、要替换的带有异常值的要素，并指明您是用平均值还是中值进行替换。</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="b110" class="lx lg hi mt b fi my mz l na nb">def replace_outlier(data, features, with_median=False, with_mean=False):<br/>    for feature in features:<br/>        Q1 = data[feature].quantile(0.25)<br/>        Q3 = data[feature].quantile(0.75)<br/>    <br/>        median = data[feature].quantile(0.50)<br/>        mean = data[feature].mean()<br/>    <br/>        IQR = Q3 - Q1<br/>    <br/>        upper_whisker = Q3 + (1.5 * IQR)<br/>        lower_whisker = Q1 - (1.5 * IQR)<br/>    <br/>        if with_median:<br/>            data[feature] = np.where(data[feature] &gt; upper_whisker, median, data[feature])<br/>            data[feature] = np.where(data[feature] &lt; lower_whisker, median, data[feature])<br/>    <br/>        if with_mean:<br/>            data[feature] = np.where(data[feature] &gt; upper_whisker, mean, data[feature])<br/>            data[feature] = np.where(data[feature] &lt; lower_whisker, mean, data[feature])</span></pre><h2 id="0e1c" class="lx lg hi bd lh ly lz ma ll mb mc md lp jx me mf lr kb mg mh lt kf mi mj lv mk bi translated">使用OneHotEncoder的分类特征编码</h2><p id="cee2" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx mn jz ka kb mo kd ke kf mp kh ki kj hb bi translated">分类特征的编码是通过将分类值转换成二进制值来完成的。OneHotEncoder通过为特性中的每个类别创建一个单独的二进制值列来实现这一点。在新列中，值1表示目录存在，0表示目录不存在。例如，在我们的银行数据中，一个分类特征是婚姻，它有四个可能的类别；已婚的，单身的，离婚的，未知的。对于这些类别中的每一个，OneHotEncoder都创建一个新列，并在该类别出现时使用1，否则使用0。</p><p id="be47" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">OneHotEncoding可以通过多种方式实现，比如在熊猫身上使用get_dummies。然而，这篇文章将使用CountVectorizer，这是一种比get_dummies更好的方法，因为get_dummies返回不同维度的数据。</p><p id="66bd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">同样，使用下面的函数，你可以很容易地对你的分类特征进行一次性编码，你只需要传入你的分类特征的名称，X_train集合和X_test集合。</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="2401" class="lx lg hi mt b fi my mz l na nb">from sklearn.feature_extraction.text import CountVectorizer</span><span id="1e20" class="lx lg hi mt b fi nc mz l na nb">def add_onehot_to_dataframe(sparse, df, vectorizer, name):<br/>  '''<br/>  This function will add the one hot encoded to the dataframe.<br/>  '''<br/>  for i, col in enumerate(vectorizer.get_feature_names()):<br/>    colname = name+"_"+col<br/>    # df[colname] = pd.SparseSeries(sparse[:, i].toarray().flatten(), fill_value=0)<br/>    df[colname] = sparse[:, i].toarray().ravel().tolist()<br/>  <br/>  return df</span><span id="db4f" class="lx lg hi mt b fi nc mz l na nb">def OneHotEncoder(categorical_cols, X_train, X_test):<br/>  '''<br/>    This function takes categorical column names as inputs. The objective of this function is to take the column names iteratively and encode the features using One hot Encoding mechanism and also adding the encoded feature to the respective dataframe.<br/>  '''</span><span id="9511" class="lx lg hi mt b fi nc mz l na nb">for i in categorical_cols:<br/>    Vectorizer = CountVectorizer(token_pattern="[A-Za-z0-9-.]+")<br/>    print("Encoding for feature: ", i)<br/>    # Encoding training dataset <br/>    temp_cols = Vectorizer.fit_transform(X_train[i])<br/>    X_train = add_onehot_to_dataframe(temp_cols, X_train, Vectorizer, i)</span></pre><p id="a092" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">#编码测试数据集<br/>temp _ cols = Vectorizer . transform(X_test[I])<br/>X _ Test = add _ onehot _ to _ data frame(temp _ cols，X _ Test，Vectorizer，I)</p><h2 id="5ce4" class="lx lg hi bd lh ly lz ma ll mb mc md lp jx me mf lr kb mg mh lt kf mi mj lv mk bi translated">使用StandardScaler()缩放数字特征</h2><p id="0964" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx mn jz ka kb mo kd ke kf mp kh ki kj hb bi translated">数据集中的每个数值特征通常在不同的尺度上进行测量，这可能会在训练模型时导致一些偏差。因此，建议总是通过使用<code class="du mq mr ms mt b">MinMaxScaler()</code>或<code class="du mq mr ms mt b">StandardScaler()</code>来规范化您的数据。在本文中，我们将使用<code class="du mq mr ms mt b">StandardScaler()</code>和下面的代码片段来实现我们的数字特征的规范化。</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="faa5" class="lx lg hi mt b fi my mz l na nb">from sklearn.preprocessing import StandardScaler<br/>    <br/>scaler = StandardScaler()</span><span id="0217" class="lx lg hi mt b fi nc mz l na nb">numerical_cols = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']<br/>    <br/>#normalizing train set<br/>X_train = X_train.reset_index()<br/>temp_df = X_train.filter(numerical_cols, axis = 1)<br/>X_train = X_train.drop(numerical_cols, axis = 1)<br/>   <br/>cols = temp_df.values<br/>    <br/>temp_cols = scaler.fit_transform(cols)<br/>temp_df = pd.DataFrame(temp_cols, columns = numerical_cols)<br/>X_train = X_train.merge(temp_df, left_index=True, right_index=True)<br/>X_train = X_train.drop(['index'], axis=1)<br/>    <br/>#normalizing test set<br/>X_test = X_test.reset_index()<br/>temp_df = X_test.filter(numerical_cols, axis = 1)<br/>X_test = X_test.drop(numerical_cols, axis = 1)<br/>    <br/>cols = temp_df.values<br/>    <br/>temp_cols = scaler.fit_transform(cols)<br/>temp_df = pd.DataFrame(temp_cols, columns = numerical_cols)<br/>X_test = X_test.merge(temp_df, left_index=True, right_index=True)<br/>X_test = X_test.drop(['index'], axis=1)</span></pre><h2 id="b218" class="lx lg hi bd lh ly lz ma ll mb mc md lp jx me mf lr kb mg mh lt kf mi mj lv mk bi translated">降维</h2><p id="6779" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx mn jz ka kb mo kd ke kf mp kh ki kj hb bi translated">这种预处理技术包括将高维数据转换为低维数据，同时仍然保持整个数据集的良好表示。有几种降维技术，但这篇文章将只通过PCA(主成分分析)，t-SNE(t-分布式随机邻居嵌入)和自动编码器。</p><p id="8b35" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj"> PCA(主成分分析)</strong></p><p id="a969" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">下面的代码将我们的数据降维为2D。<code class="du mq mr ms mt b">n_components</code>属性是用于指定数据应该减少到的组件数量的部分。</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="71b4" class="lx lg hi mt b fi my mz l na nb">from sklearn.decomposition import PCA</span><span id="dd3e" class="lx lg hi mt b fi nc mz l na nb">pca = PCA(n_components=2)<br/>X_pca_2train = pca.fit_transform(X_train)<br/>PCA_df_2train = pd.DataFrame(data = X_pca_2train, columns = ['PC1', 'PC2'])<br/>PCA_df_2train = pd.concat([PCA_df_2train, y_train], axis = 1)</span></pre><p id="ed75" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">PCA还提供了一种检查由所选组件保留的方差比率的方法，这可以通过:<code class="du mq mr ms mt b">pca.explained_variance_ratio_</code>对于上面的代码，该方差的输出是<code class="du mq mr ms mt b">out[]: [0.29620347 0.09104375]</code>，表示我们的数据的29.62%由组件1表示，只有9.10%由组件2表示。</p><p id="f31c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们还可以使用以下代码绘制PCA的结果分布图:</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="4fd1" class="lx lg hi mt b fi my mz l na nb">plt.figure(figsize=(8, 8))</span><span id="cff4" class="lx lg hi mt b fi nc mz l na nb">classes = [1, 0]<br/>colors = ['r', 'b']<br/>for clas, color in zip(classes, colors):<br/>    plt.scatter(PCA_df_2train.loc[PCA_df_2train['y'] == clas, 'PC1'], <br/>                PCA_df_2train.loc[PCA_df_2train['y'] == clas, 'PC2'], <br/>                c = color)<br/>    <br/>plt.xlabel('Principal Component 1', fontsize = 12)<br/>plt.ylabel('Principal Component 2', fontsize = 12)<br/>plt.title('2D PCA', fontsize = 15)<br/>plt.legend(['Subscription', 'No Subscription'])<br/>plt.grid()</span></pre><p id="1f47" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">上面的代码给了我们以下情节:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nd"><img src="../Images/7585d6850de60287e830d541f5650dfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*OQiz-wsla3dCpirAgLLgzA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="fc42" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">从上图可以看出，两个输出之间的区别并不明显，因此使用两个元件可能不是最佳选择。</p><p id="e4a9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">要获得用于降维的理想分量数，您可以使用PCA，而无需为n_components赋值，并绘制数据的解释方差比，如以下代码片段所示:</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="71f7" class="lx lg hi mt b fi my mz l na nb">pca = PCA()<br/>pca_data = pca.fit_transform(X_train)<br/>plt.plot(np.cumsum(pca.explained_variance_ratio_))<br/>plt.xlabel('number of components')<br/>plt.ylabel('cumulative explained variance')</span></pre><p id="7b44" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这段代码导致了以下情节:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ne"><img src="../Images/7abef248741733792c1a96d720e090a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*ZwwJrj21mgEyb3e1cFl5DQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="7843" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">从上面的图中我们可以看到，使用接近30个组件将会给我们100%的数据表示。10种成分也有很好的代表性，接近80%。分析该图后，您可以再次运行PCA，这一次将n_components分配给理想数量的组件，以获得更好的数据表示。</p><p id="2215" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">t-SNE(t-分布式随机邻居嵌入)</strong></p><p id="a6fd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">与PCA不同，t-SNE是一种非线性降维技术。要对数据应用t-SNE降维，请使用下面的代码片段，并使用n_components指定组件的数量。</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="f5fb" class="lx lg hi mt b fi my mz l na nb">tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)<br/>X_tsne_2train = tsne.fit_transform(X_train)</span></pre><p id="c802" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">要使用可视化来检查SNE霸王龙的分布，请使用以下代码:</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="45a8" class="lx lg hi mt b fi my mz l na nb">plt.figure(figsize=(8,8))<br/>plt.scatter(X_tsne_2train[:, 0], X_tsne_2train[:, 1], c=y_train.values)<br/>plt.show()</span></pre><p id="d6e4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">上面的代码给了我们一个如下图所示的情节:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nf"><img src="../Images/d245787a1e76b994af67a9df15bbbc17.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*E5bXpsO3E6tQmToO0HBHVA.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="0b8d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">正如PCA所示，这两类之间的区别也不明显，因此使用更多的组分可能会更好。</p><p id="c174" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">自动编码器</strong></p><p id="f2b7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">自动编码器是机器学习算法，可用于降维。他们使用非线性变换将数据从高维映射到低维。有不同类型的自动编码器，例如:变分自动编码器、卷积自动编码器、去噪自动编码器和稀疏自动编码器。在这篇文章中，我们将构建一个基本的自动编码器，它包含两个主要组件:编码器和解码器。</p><p id="f0d1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">自动编码器使用python中的Keras API，因此，在运行以下代码之前，请确保您的环境中安装了Keras和tensorflow。通过在python 3.7和anaconda3环境下运行<code class="du mq mr ms mt b">!pip install keras</code>和<code class="du mq mr ms mt b">!pip install tensorflow</code>，可以很容易地在Jupyter笔记本上安装这两个软件。</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="c64b" class="lx lg hi mt b fi my mz l na nb">from keras.layers import Input, Dense<br/>from keras.models import Model</span><span id="cf94" class="lx lg hi mt b fi nc mz l na nb">X = pd.concat([X_train, X_test], ignore_index=True)</span><span id="ee36" class="lx lg hi mt b fi nc mz l na nb">input_layer = Input(shape=(X.shape[1],))<br/>encoded = Dense(3, activation='relu')(input_layer)<br/>decoded = Dense(X.shape[1], activation='softmax')(encoded)<br/>autoencoder = Model(input_layer, decoded)<br/>autoencoder.compile(optimizer='adam', loss='binary_crossentropy')</span><span id="2d6e" class="lx lg hi mt b fi nc mz l na nb">X1, X2, Y1, Y2 = train_test_split(X, X, test_size=0.3, random_state=101)</span><span id="8d8b" class="lx lg hi mt b fi nc mz l na nb">autoencoder.fit(X1, Y1,<br/>                epochs=100,<br/>                batch_size=300,<br/>                shuffle=True,<br/>                verbose = 30,<br/>                validation_data=(X2, Y2))</span><span id="3467" class="lx lg hi mt b fi nc mz l na nb">encoder = Model(input_layer, encoded)<br/>X_ae3 = encoder.predict(X)</span></pre><p id="ca5a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">请注意，我连接了我的训练集和测试集，因为自动编码器将输入特征(X)作为特征和标签。我们还为编码器使用了ReLu activation函数，为解码器使用了Softmax来实现非线性转换。</p><h2 id="94dd" class="lx lg hi bd lh ly lz ma ll mb mc md lp jx me mf lr kb mg mh lt kf mi mj lv mk bi translated">通过过采样平衡类</h2><p id="9925" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx mn jz ka kb mo kd ke kf mp kh ki kj hb bi translated">从我们的数据探索中，我们注意到我们的目标变量是不平衡的，无订阅与订阅的比例为89:11。类别不平衡也会影响模型训练，因此我们必须平衡我们的目标变量。一些最大似然算法，如逻辑回归算法，带有平衡类的属性；也就是说<code class="du mq mr ms mt b">LogisticRegression(class_weight='balanced')</code>,我们可以将此选项用于此类模型，否则，我们必须通过欠采样和过采样等其他方法来平衡我们的clas。在这种情况下，将使用SMOTE对我们的类进行过采样，如下面的代码所示:</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="95e8" class="lx lg hi mt b fi my mz l na nb">from imblearn.over_sampling import SMOTE</span><span id="6ae6" class="lx lg hi mt b fi nc mz l na nb">os = SMOTE(random_state = 0)</span><span id="21eb" class="lx lg hi mt b fi nc mz l na nb">columns = X_train.columns<br/>os_data_X, os_data_y = os.fit_sample(X_train, y_train)<br/>os_data_X = pd.DataFrame(data=os_data_X, columns=columns)<br/>os_data_y = pd.DataFrame(data=os_data_y, columns=['y'])</span></pre><h2 id="1338" class="lx lg hi bd lh ly lz ma ll mb mc md lp jx me mf lr kb mg mh lt kf mi mj lv mk bi translated">特征抽出</h2><p id="6c96" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx mn jz ka kb mo kd ke kf mp kh ki kj hb bi translated">特征提取包括从整个数据集中选择一些特征用于模型训练。基于这些特征在预测目标变量中的重要性来选择这些特征。这篇文章将探讨特征提取的RFE方法。下面的代码通过重复拟合训练数据来选择前20个重要的特性，选择最重要的特性，然后在下一轮中放弃它，直到选择了20个特性。</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="1ba6" class="lx lg hi mt b fi my mz l na nb">from sklearn.feature_selection import RFE</span><span id="1a13" class="lx lg hi mt b fi nc mz l na nb">rfe = RFE(logreg, n_features_to_select=20)<br/>rfe = rfe.fit(X_train, y_train)<br/>rank = rfe.ranking_</span></pre><h1 id="663d" class="lf lg hi bd lh li lj lk ll lm ln lo lp io lq ip lr ir ls is lt iu lu iv lv lw bi translated">模特培训</h1><p id="d173" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx mn jz ka kb mo kd ke kf mp kh ki kj hb bi translated">接下来，我们继续进行模型训练，我们将探索前面列出的三种机器学习算法。</p><h2 id="460a" class="lx lg hi bd lh ly lz ma ll mb mc md lp jx me mf lr kb mg mh lt kf mi mj lv mk bi translated">逻辑回归</h2><p id="e74d" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx mn jz ka kb mo kd ke kf mp kh ki kj hb bi translated">首先，需要注意的是，对于我们的数据集,“duration”列不应用于模型训练，因此，我们通过使用以下代码将其从训练数据中删除:</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="44a7" class="lx lg hi mt b fi my mz l na nb">#Removing duration feature from dataset</span><span id="8a83" class="lx lg hi mt b fi nc mz l na nb">#From train<br/>X_train = X_train.drop('duration', axis=1)</span><span id="fe9f" class="lx lg hi mt b fi nc mz l na nb">#From test<br/>X_test = X_test.drop('duration', axis=1)</span></pre><p id="cf19" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">然后，我们对数据应用逻辑回归算法。记住，这个模型有一个处理不平衡数据的内置方法，因此，我们将使用它。</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="ff08" class="lx lg hi mt b fi my mz l na nb">from sklearn.linear_model import LinearRegression</span><span id="0d21" class="lx lg hi mt b fi nc mz l na nb">logreg = LogisticRegression(class_weight='balanced')<br/>logreg.fit(X_train, y_train)<br/>y_pred_logreg = logreg.predict_proba(X_test)</span></pre><p id="2966" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们可以通过使用ROC(受试者操作特征)分数来测试我们模型的准确性</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="a25b" class="lx lg hi mt b fi my mz l na nb">from sklearn.metrics import roc_auc_score</span><span id="65e6" class="lx lg hi mt b fi nc mz l na nb">print('AUC score without duration columns : ', roc_auc_score(y_test, y_pred_logreg[:, 1]))</span></pre><p id="83fd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们还可以使用下面的代码绘制ROC曲线，从模型中获得更多信息。</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="6d31" class="lx lg hi mt b fi my mz l na nb">from sklearn.metrics import roc_curve</span><span id="bcfa" class="lx lg hi mt b fi nc mz l na nb">logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))<br/>fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])<br/>plt.figure()<br/>plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)<br/>plt.plot([0, 1], [0, 1],'r--')<br/>plt.xlim([0.0, 1.0])<br/>plt.ylim([0.0, 1.05])<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')<br/>plt.title('Receiver operating characteristic')<br/>plt.legend(loc="lower right")<br/>plt.savefig('Log_ROC')<br/>plt.show()</span></pre><p id="6e51" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">上述代码生成了以下图形:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ng"><img src="../Images/41bed654c05319ffc47a3c3d31bd0e2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*FdfpWz5TNzzVa9YSeus-oQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">作者图片</figcaption></figure><p id="9655" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">虚线是纯随机分类器的ROC曲线的表示。好的分类器应该远离这条线。</p><h2 id="ad8d" class="lx lg hi bd lh ly lz ma ll mb mc md lp jx me mf lr kb mg mh lt kf mi mj lv mk bi translated">随机森林</h2><p id="406c" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx mn jz ka kb mo kd ke kf mp kh ki kj hb bi translated">随机森林是scikit-learn库附带的分类器之一，因此，要运行此模型，我们只需调用库中的分类器，如下所示:</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="5263" class="lx lg hi mt b fi my mz l na nb">from sklearn.ensemble import RandomForestClassifier</span><span id="54a7" class="lx lg hi mt b fi nc mz l na nb">rfclf = RandomForestClassifier(class_weight='balanced', max_depth=2, random_state=0)<br/>rf_model = rfclf.fit(X_train, y_train)</span><span id="a727" class="lx lg hi mt b fi nc mz l na nb">y_pred_rf=rfclf.predict(X_test)</span></pre><p id="e560" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们可以通过生成混淆矩阵以及计算准确度、精确度、召回率、Fmeasure和支持度来测试模型的准确性，如下所示:</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="d24d" class="lx lg hi mt b fi my mz l na nb">target = np.array(y_test)<br/>rf_prediction = np.array(y_pred_rf)</span><span id="e0d3" class="lx lg hi mt b fi nc mz l na nb">print('Confusion matrix : ', confusion_matrix(target, rf_prediction))<br/>print(classification_report(y_test, y_pred_rf))</span></pre><h2 id="8652" class="lx lg hi bd lh ly lz ma ll mb mc md lp jx me mf lr kb mg mh lt kf mi mj lv mk bi translated">决策树</h2><p id="3b72" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx mn jz ka kb mo kd ke kf mp kh ki kj hb bi translated">决策树还附带了scikit-library，因此，实现决策树模型就像导入库并拟合您的训练数据一样简单，如下所示:</p><pre class="iy iz ja jb fd mu mt mv mw aw mx bi"><span id="ab68" class="lx lg hi mt b fi my mz l na nb">from sklearn.tree import DecisionTreeClassifier</span><span id="47b3" class="lx lg hi mt b fi nc mz l na nb">dt = DecisionTreeClassifier(class_weight='balanced')</span><span id="887a" class="lx lg hi mt b fi nc mz l na nb">dt_model = dt.fit(X_train, y_train)</span><span id="e2dd" class="lx lg hi mt b fi nc mz l na nb">y_pred_dt = dt.predict(X_test)</span></pre><p id="e310" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">就像随机森林一样，我们也可以通过生成混淆矩阵并计算准确性、召回率、精确度、Fmeasure和支持度来测试我们的决策树模型的准确性。</p></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><p id="0c9e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这篇文章概述了机器学习中的数据预处理和模型训练。它提供了许多对数据进行预处理的方法，并解释了如何以及何时使用它们。这篇文章还谈到了三种可用于模型训练的机器学习算法，使用这些算法运行模型的基础知识，以及获得模型准确性的不同方法。要了解代码，您可以下载提供的链接中使用的数据，或者访问提供的GitHub链接中的完整代码。</p><h1 id="1c8d" class="lf lg hi bd lh li lj lk ll lm ln lo lp io lq ip lr ir ls is lt iu lu iv lv lw bi translated">参考</h1><p id="5ea6" class="pw-post-body-paragraph jo jp hi jq b jr ml ij jt ju mm im jw jx mn jz ka kb mo kd ke kf mp kh ki kj hb bi translated">[1]: L. Susan，<em class="nh">用python构建逻辑回归，一步一步</em> (2017)，<a class="ae jn" href="https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8" rel="noopener" target="_blank">https://towardsdatascience . com/Building-a-logistic-regression-in-python-step-by-step-becd 4d 56 c 9 c 8</a></p><p id="7cc5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[2]: R.Sukanta，<em class="nh">机器学习案例研究:预测银行电话营销成功的数据驱动方法</em> (2019)，<a class="ae jn" href="https://towardsdatascience.com/machine-learning-case-study-a-data-driven-approach-to-predict-the-success-of-bank-telemarketing-20e37d46c31c" rel="noopener" target="_blank">https://towards data science . com/Machine-Learning-case-study-A-data-driven-approach-to-predict-the-success-of-bank-tele marketing-20 e 37d 46 c 31 c</a></p></div></div>    
</body>
</html>