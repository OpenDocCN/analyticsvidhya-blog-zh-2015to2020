<html>
<head>
<title>Recurrent Neural Networks(RNN’s) and Time Series Forecasting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">递归神经网络(RNN)和时间序列预测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/recurrent-neural-networks-rnns-and-time-series-forecasting-d9ea933426b3?source=collection_archive---------3-----------------------#2020-03-13">https://medium.com/analytics-vidhya/recurrent-neural-networks-rnns-and-time-series-forecasting-d9ea933426b3?source=collection_archive---------3-----------------------#2020-03-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="7ef1" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><strong class="ak">动机</strong></h2></div><p id="70bc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">普通神经网络非常适合于许多简单的任务，如输入被分配一个类别或标签的分类问题，给定一组输入参数预测实数值的回归问题。这类网络的主要问题是它们不记得它们所学的东西，也就是说，在训练网络的每一次迭代之后，它会重新开始，而不记得它在之前的迭代中所学的任何东西。当数据是连续的时，这一特性使它们处于不利地位。</p><p id="abca" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">顺序数据意味着数据点是相互依赖的(时间t的数据依赖于时间t-1的数据)。例如时间序列数据，如股票价格、会话等。因此，RNNs(递归神经网络)出现了。你可以想象rnn有一种记忆感，这种记忆感有助于他们跟踪序列数据中早期发生的事情，从而帮助他们获得上下文并识别相关性和模式。</p><h2 id="4390" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak">建筑</strong></h2><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es ko"><img src="../Images/1da769f8a810793bcead6c945ae7d8e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/0*bdaHfQb0JFAx239E"/></div></figure><p id="0b14" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">RNN看起来非常像前馈神经网络，唯一的区别是它有向后指向的连接。在每个时间步长t，递归层接收输入x(t)以及来自前一时间步长的输出。上图显示了最简单的RNN，由单个神经元接收输入、计算输出并将其发送回自身组成</p><p id="ce86" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">每个递归神经元有2组权重，一组用于当前时间步长的输入(wx)，另一组用于先前时间步长的输出(wy)。当考虑一层这样的神经元时，我们将每个权重放在矩阵Wx和Wy中。</p><p id="fa2d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">整个循环层的输出可以计算如下</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es kw"><img src="../Images/6039572dce67d6fee28dc56f1d61c987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vP6EPkcxJygNjr-9L1ENCQ.png"/></div></div></figure><p id="00ae" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">训练rnn的方法是将它们在时间上展开(就像我们在示例图像中所做的那样)，然后使用常规反向传播。这个过程被称为穿越时间的反向传播(BPTT)。</p><h2 id="c34e" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak">实际实施(时间序列预测)</strong></h2><p id="060d" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">为简单起见，我们将使用下面给出的函数生成我们自己的时间序列数据。</p><pre class="kp kq kr ks fd lg lh li lj aw lk bi"><span id="92ba" class="jt ju hi lh b fi ll lm l ln lo">import numpy as np<br/>def generate_time_series(batch_size, n_steps):<br/>    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)<br/>    time = np.linspace(0, 1, n_steps)<br/>    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))    # +wave 1<br/>    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))            # +wave 2<br/>    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)     # +noise<br/>    return series[..., np.newaxis].astype(np.float32)</span></pre><p id="a7d4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此函数通过batch_size参数根据需要创建尽可能多的单变量(每个时间步长一个值)时间序列。该函数返回一个NumPy数组，它是两个固定振幅、随机相位和频率的正弦波加上一点噪声的和。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es lp"><img src="../Images/b9b31d51431a0b4f48f6b34b8255ef06.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/0*MqH3qRMHdlEhBR_F"/></div></figure><p id="3cd9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上图显示了由我们的函数生成的单变量时间序列，它有50个时间步长。我们的目标是预测下一时间步的值(用X表示)</p><p id="cf3a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们将根据上述数据创建一个训练集、一个验证集和一个测试集。</p><pre class="kp kq kr ks fd lg lh li lj aw lk bi"><span id="56f6" class="jt ju hi lh b fi ll lm l ln lo">n_steps = 50<br/>series = generate_time_series(10000, n_steps + 1)<br/>X_train, y_train = series[:7000, :n_steps], series[:7000, -1]<br/>X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]<br/>X_test, y_test = series[9000:, :n_steps], series[9000:, -1]</span></pre><p id="7ec0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将构建两个模型，一个具有单个RNN层和单个神经元，另一个是具有多层的深度RNN<br/>为了编译我们的模型，我们将使用默认的adam优化器和MSE损失，使其适合我们20个时期的训练数据，并在我们的验证集上对其进行评估</p><p id="b9e4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">简单的RNN </strong></p><pre class="kp kq kr ks fd lg lh li lj aw lk bi"><span id="5cf6" class="jt ju hi lh b fi ll lm l ln lo">import tensorflow as tf<br/>from tensorflow import keras</span><span id="12ce" class="jt ju hi lh b fi lq lm l ln lo">model = keras.models.Sequential([<br/>    keras.layers.SimpleRNN(1, input_shape=[None, 1])<br/>])</span><span id="d7b4" class="jt ju hi lh b fi lq lm l ln lo">optimizer = keras.optimizers.Adam(lr=0.005)<br/>model.compile(loss="mse", optimizer=optimizer)<br/>history = model.fit(X_train, y_train, epochs=20,<br/>                    validation_data=(X_valid, y_valid))<br/>model.evaluate(X_valid, y_valid)</span></pre><p id="77d4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">*请注意，RNN图层使用的默认激活函数是双曲正切(tanh)</p><p id="b458" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于简单的RNN，我们得到的损耗为0.0108，这很好，但我们可以做得更好</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es lr"><img src="../Images/90463e4cc2167c6adab35de9ff22a554.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/0*udpmavOfm8lUUjTa"/></div></figure><p id="0d74" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">深RNN </strong></p><pre class="kp kq kr ks fd lg lh li lj aw lk bi"><span id="cc56" class="jt ju hi lh b fi ll lm l ln lo">model = keras.models.Sequential([<br/>    keras.layers.SimpleRNN(10, return_sequences=True, input_shape=[None, 1]),<br/>    keras.layers.SimpleRNN(10, return_sequences=True),<br/>    keras.layers.SimpleRNN(1)<br/>])</span><span id="8de2" class="jt ju hi lh b fi lq lm l ln lo">model.compile(loss="mse", optimizer="adam")<br/>history = model.fit(X_train, y_train, epochs=20,<br/>                    validation_data=(X_valid, y_valid))</span><span id="1ed9" class="jt ju hi lh b fi lq lm l ln lo">model.evaluate(X_valid, y_valid)</span></pre><p id="0496" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们的前2层中，我们已经将返回序列参数设置为True，因为默认情况下，RNN层仅返回最终输出，但是对于我们的深层rnn层(除了最后一层，我们只关心最终预测)，我们需要处理每个时间步的输出。</p><p id="fdc0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的深度RNN模型达到0.0026的mse，这比我们的简单RNN几乎好10倍。</p><figure class="kp kq kr ks fd kt er es paragraph-image"><div class="er es ls"><img src="../Images/cfd60284d9519cad06b1afce5687238d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/0*0E_BGDiYRwGO2pLM"/></div></figure><p id="d839" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇博客中，我们只使用了简单的递归层，但在实践中，我们通常使用其他类型的递归层，如LSTM或GRU，这有助于缓解像消失梯度的问题。我希望你们都觉得rnn很有趣，并且很想了解更多。我将为任何想更深入了解rnn及其用途的人添加参考资料和链接。请在评论区自由发表你的意见或任何关于博客的问题。</p><h2 id="bd22" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak">参考资料/进一步阅读</strong></h2><ul class=""><li id="be70" class="lt lu hi iz b ja lb jd lc jg lv jk lw jo lx js ly lz ma mb bi translated"><a class="ae mc" href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes05-LM_RNN.pdf" rel="noopener ugc nofollow" target="_blank">http://web . Stanford . edu/class/cs 224n/readings/cs 224n-2019-notes 05-LM _ rnn . pdf</a></li><li id="9d81" class="lt lu hi iz b ja md jd me jg mf jk mg jo mh js ly lz ma mb bi translated"><a class="ae mc" href="https://www.youtube.com/watch?v=iWea12EAu6U&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=7&amp;t=3633s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=iWea12EAu6U&amp;list = ploromvodv 4 rohcuxmzknm 7j 3 fvwbby 42 z&amp;index = 7&amp;t = 3633s</a></li><li id="7f8b" class="lt lu hi iz b ja md jd me jg mf jk mg jo mh js ly lz ma mb bi translated">第15章，使用Scikit-Learn、Keras和TensorFlow进行机器学习:构建智能系统的概念、工具和技术</li></ul></div></div>    
</body>
</html>