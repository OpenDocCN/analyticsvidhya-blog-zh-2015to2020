<html>
<head>
<title>Timeline of Transfer Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">迁移学习模型的时间线</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/timeline-of-transfer-learning-models-db2a0be39b37?source=collection_archive---------7-----------------------#2019-10-01">https://medium.com/analytics-vidhya/timeline-of-transfer-learning-models-db2a0be39b37?source=collection_archive---------7-----------------------#2019-10-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9c74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本帖中，我们将列出一些用于图像分类的最强大的迁移学习模型的时间表，以及它们在过去几年中是如何发展的。在迁移学习中，我们可以使用预先训练的模型，并添加一些紧密连接的层，以便从新的数据集中识别对象。在Keras 的<a class="ae jd" href="https://keras.io/applications/" rel="noopener ugc nofollow" target="_blank">文档中，我们可以找到一些可以导入的预训练模型，如VGG、ResNet和Inception。在进入每个模型的更多细节之前，让我们检查这些模型来自哪里。这些模型来自一个名为ImageNet的年度竞赛。</a></p><h1 id="5ca7" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">ImageNet</h1><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/7399530d03e52088d6b1960745e00118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*617Ro-6NO3oGQkSYBaxXKg.png"/></div></div></figure><p id="4fb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ImageNet是一个大型视觉数据库，用于研究视觉对象识别软件。该项目手动注释了超过1400万张图像，以将对象分为20，000多个类别。自2010年以来，ImageNet项目发起了一年一度的软件竞赛，名为ImageNet的大规模视觉识别挑战(ILSVRC)，其中软件程序竞争正确分类和检测对象和场景。该挑战使用原始数据集的子集，其中120万幅图像作为输入，1000个类别作为输出。</p><p id="8846" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在过去几年中，平均误差一直在不断下降。自2012年以来，从经典计算机视觉技术到深度学习出现了重大转变。AlexNet是第一个使用深度学习技术的获胜者，与前一年使用计算机视觉的获胜者相比，它的错误减少了10%以上。让我们进一步讨论其中的一些模型。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ko"><img src="../Images/9bbf0126123c58e5afa8f00a6a9bb907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*z8oR8SyTdkDNBOiP1TVf6A.png"/></div></figure><h1 id="9d65" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">2012年— AlexNet</h1><p id="005e" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">2012年，AlexNet能够以10%的巨大差距超越经典计算机视觉模型。这是深度神经网络第一次赢得这种类型的比赛，从那以后，深度学习成为图像分类问题的标准方法。它由五对卷积层和两对全连接层组成，如图2所示。由于当时的计算限制，训练是在两个GPU上进行的，这就是为什么他们的网络被分成两个管道的原因。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es ku"><img src="../Images/860214ba262109b0bdb1b4200c107024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3P1SSSeMASDxK2IanhM7KA.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图2: AlexNet模型</figcaption></figure><p id="a4bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">AlexNet帮助推广和标准化了以下实践:</p><ul class=""><li id="53ed" class="kz la hi ih b ii ij im in iq lb iu lc iy ld jc le lf lg lh bi translated"><strong class="ih hj"> ReLu(校正线性单元)</strong>作为隐藏层的默认激活函数，而不是早期标准的Tanh或Sigmoid。ReLu的优点是它可以训练，因为它的导数对于正值是常数。这也有助于解决渐变消失的问题。</li><li id="1271" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated"><strong class="ih hj">在每个完全连接的层之后，去除</strong>层，以减少过拟合。它随机关闭作为参数给出的一部分单位的激活(默认为50%)。其工作的原因是，dropout通过迫使单元从先前单元的不同子集学习，帮助网络开发独立的功能。</li></ul><p id="b405" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2013年的获胜者是ZFNet。它的架构类似于AlexNet，但在过滤器和超参数的数量上有所改进。总的来说，想法是一样的。</p><h1 id="4a96" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">2014年——VGG和谷歌网</h1><p id="3dfd" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">2014年，得益于更深层次的网络，性能再次跃升。从前一年的8层，到现在的2014年，有两个势均力敌的赢家，VGG和谷歌网，分别有19层和22层。让我们先更详细地看看VGG。</p><h2 id="b194" class="ln jf hi bd jg lo lp lq jk lr ls lt jo iq lu lv js iu lw lx jw iy ly lz ka ma bi translated">VGG</h2><p id="26b3" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">牛津大学VGG小组的VGG有两个版本，一个16层，另一个19层。它由五组卷积层组成，每组卷积层都有一个最大池。卷积层使用3x 3内核大小的过滤器，而不是AlexNet使用的11个过滤器，这有助于减少参数数量，同时仍能捕捉前一层的特征。由于其简单性，这是一个被用作起点的流行模型。然而，该模型的参数数量相当大，并且加载到内存中的量很大。当时，由于消失梯度问题，具有大约20层的模型被认为是非常深的模型。正如我们将很快看到的，批量标准化和残差网络的发明使得训练具有更多层的模型成为可能。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es mb"><img src="../Images/2a76f667d8b2edd16aca40cf021daaac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*740e0iZMjHXEU_f8-AQEmQ.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">VGG建筑(<a class="ae jd" href="http://webia.lip6.fr/~cord/pdfs/news/TalkDeepCordI3S.pdf" rel="noopener ugc nofollow" target="_blank">图片来源</a></figcaption></figure><p id="f339" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">VGG帮助推广和规范了以下做法:</p><ul class=""><li id="053c" class="kz la hi ih b ii ij im in iq lb iu lc iy ld jc le lf lg lh bi translated"><strong class="ih hj">大小为3×3的卷积滤波器</strong></li></ul><h2 id="09b1" class="ln jf hi bd jg lo lp lq jk lr ls lt jo iq lu lv js iu lw lx jw iy ly lz ka ma bi translated">谷歌网(盗梦空间)</h2><p id="f73d" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">GoogLeNet也被称为Inception，其参数比AlexNet少12倍，是ILSVRC 2014年分类冠军。它由22层组成。它的主要关键点是，他们设计了一个名为inception的模块，以处理计算效率的问题。初始模块将许多这样的模块堆叠在一起，以便更有效地计算网络。他们基本上是在进入同一层的相同输入之上并行应用几种不同类型的过滤操作。然后，它们将所有这些滤波器输出在深度方向上连接在一起，因此这在最后创建了一个更紧张的输出，该输出将传递到下一层。关于该模块的另一点是瓶颈层，它是一个接一个内核大小的过滤器。它们通过减少滤波器的数量来降低计算复杂度。最后，在卷积滤波器之后的网络输出中没有完全连接的层。相反，它们使用全局平均池层，允许使用更少的参数。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es mc"><img src="../Images/0add1d0c22c9dfdebbf82f86b801ba7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_16dRc3Bpbi_bM1pm8epSw.png"/></div></div></figure><p id="59c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">GoogLeNet帮助传播和标准化了以下实践:</p><ul class=""><li id="188c" class="kz la hi ih b ii ij im in iq lb iu lc iy ld jc le lf lg lh bi translated"><strong class="ih hj">初始模块</strong>具有不同的并行操作和深度连接。</li><li id="2f29" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated"><strong class="ih hj">全局平均池</strong>代替卷积滤波器后的末端全连接层，以降低复杂度。</li></ul><h1 id="060e" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">2015年— ResNet</h1><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es md"><img src="../Images/a0aedf01b521ba0c2b594dbb2d50a207.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*G7yAHh530xXs6B_5xg8sbQ.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">ResNet模型(<a class="ae jd" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank">图片来源</a>)</figcaption></figure><p id="8e34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ResNet凭借152层的模型赢得了2015年的比赛。他们能够获得3.57%的分类准确度的前5名误差，这优于人类的表现。层数量的急剧增加是可能的，因为有两个主要的关键点:使用他们所谓的剩余连接，以及使用批量标准化来处理早期层中的梯度流。剩余连接是具有先前层的身份映射的连接。这些联系背后的直觉是，他们注意到由于梯度消失问题，非常深的模型比浅的网络表现得更差。作为一个解决方案，他们假设更深的模型应该至少和更浅的模型表现一样好。这可以通过使用与先前层的身份映射的连接来实现，跳过一些层，使网络相当于一个较浅的模型，但也具有学习更复杂的决策边界的潜力。他们还使用全局平均池，而不是在完全连接的层之后进行扁平化。网络中的另一个要点是批量标准化的使用，这避免了梯度的传播或者爆炸或者趋向于零。</p><p id="820b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ResNet帮助推广下列做法并使之标准化:</p><ul class=""><li id="f276" class="kz la hi ih b ii ij im in iq lb iu lc iy ld jc le lf lg lh bi translated"><strong class="ih hj">剩余连接</strong>以使模型至少与较浅的网络一样好，但也有可能变得更好。</li><li id="c870" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated"><strong class="ih hj">在每个卷积层后进行批量标准化</strong>，以提高训练过程的稳定性、速度和性能。</li></ul><h1 id="21c4" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">摘要</h1><p id="007d" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">总之，我们已经看到了不同种类的CNN架构。我们看了四种广泛使用的主要架构。AlexNet，早期非常流行的网络之一。VGG和谷歌网，它们仍然被广泛使用。但是ResNet有点接管了你最应该看的东西。有一种趋势是设计我们如何连接层，跳过连接，什么连接到什么，并使用这些来设计您的架构，以改善梯度流。提到了每个模型的以下关键点:</p><ul class=""><li id="bc69" class="kz la hi ih b ii ij im in iq lb iu lc iy ld jc le lf lg lh bi translated"><strong class="ih hj"> AlexNet: </strong> ReLu为激活函数，每FC层后Dropout。</li><li id="d0f7" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated"><strong class="ih hj"> VGG: </strong>我们通常会看到的最常见的架构，包括最大池、3x3内核大小的过滤器、卷积过滤器以及全连接层。</li><li id="b916" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated"><strong class="ih hj"> Inception/GoogLeNet: </strong>不同操作并行的Inception模块；全球平均池而不是完全连接的层；“瓶颈”层使用一个接一个内核大小的过滤器来减少卷积层的深度。</li><li id="49ae" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated"><strong class="ih hj"> ResNet: </strong>剩余连接，以便使模型等效于较浅的网络；提高性能的批处理规范化。</li></ul><h1 id="6a09" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">进一步阅读</h1><ul class=""><li id="be34" class="kz la hi ih b ii kp im kq iq me iu mf iy mg jc le lf lg lh bi translated"><a class="ae jd" href="https://en.wikipedia.org/wiki/ImageNet" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/ImageNet</a></li><li id="4771" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated"><a class="ae jd" href="https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/" rel="noopener ugc nofollow" target="_blank">https://cv-tricks . com/CNN/understand-resnet-Alex net-vgg-inception/</a></li><li id="b46b" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated"><a class="ae jd" href="https://www.youtube.com/watch?v=DAOcjicFr1Y" rel="noopener ugc nofollow" target="_blank">第9讲| CNN建筑|斯坦福大学</a></li></ul></div></div>    
</body>
</html>