<html>
<head>
<title>sklearn-Linear Regression under the hood</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">sk learn-引擎盖下的线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sklearn-linear-regression-under-the-hood-31ee71aec00?source=collection_archive---------5-----------------------#2020-09-09">https://medium.com/analytics-vidhya/sklearn-linear-regression-under-the-hood-31ee71aec00?source=collection_archive---------5-----------------------#2020-09-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/476d6be68fef7681f1d35e832c47a873.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*pSNZX8y_8v0hxBufEqonRw.gif"/></div><figcaption class="im in et er es io ip bd b be z dx translated">使用梯度下降收敛到最优解的线性回归模型。然而，sklearn 线性回归不使用梯度下降。</figcaption></figure><p id="ed42" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“线性回归”这个术语肯定会让数据科学和统计学领域的每个人都有所触动。这是最简单和“第一次教”的算法。</p><p id="dc44" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，它看起来是多么简单，几百年的数学在勒让德和高斯等人的贡献下发展了它。</p><p id="23bf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性回归所做的，或者我应该这样说，它试图做的很简单，不是很有创造性。简单地说，它试图找到使给定数据集的方差最小化的最佳拟合超平面。线性回归模型的“好”取决于它捕捉和解释数据集变化的能力。做一个 n 维的超平面，需要 n 个系数和 1 个截距系数。</p><p id="c698" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性回归的奇特之处在于，许多人知道它做了什么，但没有多少人知道它是如何做的。这正是我们想要揭示的。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h1 id="3de0" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">稍微了解一下 sklearn 线性回归的工作原理:</h1><p id="a9e3" class="pw-post-body-paragraph iq ir hi is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">我们首先使用 sklearn 的 make_regression 方法创建一个合成数据集，并在此基础上创建一个线性回归模型。参数“噪声”决定了点的标准偏差，而“n _ 特征”，顾名思义，决定了特征的数量。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/7f90cf2a9791f523100cd6ebee898155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1266/format:webp/1*IG_bGM3xoGcImWq_nnuO9w.png"/></div></figure><p id="4141" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，继续在上述数据集上创建模型，</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/af2c21dd000a18fff1d5d7ee278644ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*dve7fWtCgiTxARapB2uRzQ.png"/></div></figure><p id="791f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这条直线是算法得出的。</p><p id="29a7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该算法最小化损失函数，在线性回归的情况下，损失函数是均方根误差(RMSE)。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es le"><img src="../Images/fe0d90e6da0357b15fe6d9d2616141e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*st1zZN7rHX8KYy_BZWYETg.png"/></div></figure></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h1 id="2d4a" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">进入 OLS。</h1><p id="41ab" class="pw-post-body-paragraph iq ir hi is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">n 维空间中的(n-1)维超平面有无限种可能，所以蛮力永远找不到我们的最优解。对我们有利的一件事是，RMSE 的导数形成了一个凸函数，没有局部极小值和一个全局极小值。因此，梯度下降是第一件想到的事情，以制定出最佳解决方案。</p><p id="1793" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">除了梯度下降法之外，还有许多寻找最佳超平面系数的方法。sklearn 用的是<strong class="is hj">普通最小二乘法，</strong>由高斯-马尔可夫定理<strong class="is hj">推导而来。</strong>我们将超平面定义为:</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/707a4049de66a3d8910fddae7ff53760.png" data-original-src="https://miro.medium.com/v2/resize:fit:572/0*bGlJ2AQmQu_6qm7W"/></div></figure><p id="37eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在方程中，贝塔系数(βs)是 OLS 估计的参数。ε是随机误差。</p><p id="9a6f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以，我们可以通过找到β <em class="lg"> 0 </em>，β <em class="lg"> 1，…，</em> β <em class="lg"> k. </em>的值来完全定义我们的超平面，β的值告诉我们一件重要的事情。如果我们的模型预测了|βi|的高值，这意味着我们的模型在决定预测值时给予了特征 X <em class="lg"> i </em>很高的重要性。β <em class="lg"> i = </em> 0 意味着我们的模型完全忽略了特征 X <em class="lg"> i. </em></p><p id="b74a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以将上面的等式转换成矩阵形式，并写成:</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/683b3f29a5dbdd07866847d514f4518b.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*Puq3LMGTdVqtrADlN188Dw.png"/></div></figure><p id="4bce" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中<strong class="is hj"> <em class="lg"> x </em> </strong> <em class="lg"> i </em>是所有解释变量的第<em class="lg"> i </em>个观测值的列向量；β是未知参数的 k×1 向量。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es li"><img src="../Images/675f9821233da0f50e9a80b97a42ba95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mzu6duvamwJv7cVpmr761g.png"/></div></div></figure><p id="c0c6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在数学中，当使用正规方程求解时，给定的问题有唯一的解，</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/8bc1a76cb624bc93b77f94eb352041fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*BxFH9OvF63XqtW-TYqax0w.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">正态方程</figcaption></figure><p id="2d1b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这最终将我们引向:</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es lo"><img src="../Images/99b96bce342bf2c359657940c58dcf86.png" data-original-src="https://miro.medium.com/v2/resize:fit:854/format:webp/1*nLyKB2O_9YvuoM_XMnN5IQ.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">我们开始寻找的系数向量可以用一组矩阵运算来计算</figcaption></figure></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h1 id="b16b" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">证明 sklearn 线性回归遵循 OLS 方法并使用正规方程来确定系数:</h1><p id="1211" class="pw-post-body-paragraph iq ir hi is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">如前所述，再次创建一个简单的合成数据集，在线 y=9x + 10 周围有 20，000 个正态分布的数据点，我们得到</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es lp"><img src="../Images/79491d0c8c2e4e57a0259f5667d2d63b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bB28Z3gsKDyrIIXFagS9ag.png"/></div></div></figure><p id="1956" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了便于观察，我们选择了一个简单的模型，其中有一个因变量和一个自变量。由于我们已经知道生成这些点所使用的线，最佳拟合线(由 sklearn 线性回归预测)必须具有非常接近 9°的<strong class="is hj">斜率和非常接近 10°的 y 截距。</strong></p><p id="6cb4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在将线性回归付诸实施，并将结果与我们之前看到的正态方程进行比较，</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es lp"><img src="../Images/b1959fcb7d28ff07772ea87441afd2f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wuNdo2OXlLEO6Tn7nO2D3A.jpeg"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">建议使用 linalg.pinv 而不是 linalg.inv，因为它也可以处理不可逆矩阵。</figcaption></figure><p id="7e90" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">结果匹配！我们刚刚揭开了复杂的线性回归的幕后！虽然 sklearn 在其基础上使用了额外的数学技巧，如奇异值分解和 Moore-Penrose 伪逆来降低计算要求，但基本思想仍然是相同的。</p><p id="583d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">发现有用就留个掌声！</p></div></div>    
</body>
</html>