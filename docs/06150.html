<html>
<head>
<title>Stepping into Recurrent Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">步入递归神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/stepping-into-recurrent-neural-networks-54b2a0190239?source=collection_archive---------34-----------------------#2020-05-12">https://medium.com/analytics-vidhya/stepping-into-recurrent-neural-networks-54b2a0190239?source=collection_archive---------34-----------------------#2020-05-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="859d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大家好！这是我在一个月内完成<em class="jd">深度学习纳米学位</em>的旅程中的第九篇文字！我已经完成了该学位总共六个模块中第四个模块的33%。今天主要修改了<em class="jd">前馈</em>和<em class="jd">反向传播</em>。</p><h2 id="cb93" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">第14天</h2><p id="968b" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">因此，学位中的这个模块是关于RNNs的，为了开始，他们建议修改前馈和反向传播如何工作的概念。这正是我们今天要讨论的。</p><blockquote class="ke"><p id="e6ec" class="kf kg hi bd kh ki kj kk kl km kn jc dx translated">CNN擅长于在训练数据中寻找空间和可见模式的任务。</p></blockquote><figure class="ko kp kq kr ks kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es ca"><img src="../Images/e967b10455c81b688078f1927f9cc416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WZ-5RHn5CFdDJO5ApkbljA.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">RNNs</figcaption></figure><p id="101b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">来，我们现在复习一下基础。</p><blockquote class="le lf lg"><p id="c3ab" class="if ig jd ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated">神经网络本质上充当非线性函数激活。</p></blockquote><h2 id="a29d" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated"><em class="lk"> NN </em>应用</h2><ul class=""><li id="58d0" class="ll lm hi ih b ii jz im ka iq ln iu lo iy lp jc lq lr ls lt bi translated"><strong class="ih hj">分类</strong>:例如，我们输入一只猫，模型告诉我们这是一只猫。</li><li id="9c35" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><strong class="ih hj">回归</strong>:例子可以是预测股票价格。我们输入过去5天的股票价格，然后模型预测未来5天的股票价格。</li></ul><blockquote class="ke"><p id="35c4" class="kf kg hi bd kh ki lz ma mb mc md jc dx translated">我们最终会找到产生最佳输出的最佳砝码组。</p></blockquote><h2 id="058a" class="je jf hi bd jg jh me jj jk jl mf jn jo iq mg jq jr iu mh jt ju iy mi jw jx jy bi translated">培训和评估流程</h2><p id="afba" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">在训练阶段，我们获取数据集(也称为训练集)，它包括许多对输入和它们对应的目标(输出)。我们的目标是找到一组将输入映射到期望输出的最佳权重。在评估阶段，我们使用在培训阶段创建的网络，应用我们的新输入，并期望获得期望的输出。</p><p id="65d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">再来说说<strong class="ih hj"> <em class="jd">权重矩阵</em> </strong>，它会有等于隐藏层节点的列和等于输入层节点的行。我们通过将激活函数应用于输入向量和权重矩阵的乘积来获得输出。</p><h2 id="03ce" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">误差函数</h2><p id="9b90" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">最常用的两个误差函数是<strong class="ih hj">均方误差</strong> (MSE)，通常用于回归问题，以及<strong class="ih hj">交叉熵，</strong>通常用于分类问题。</p><h2 id="7cea" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">正向输送</h2><p id="9f9f" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">基本上，前馈过程与我们对CNN或MLP所做的一样。我们通过模型的架构运行输入，然后计算误差。</p><pre class="mj mk ml mm fd mn mo mp mq aw mr bi"><span id="7601" class="je jf hi mo b fi ms mt l mu mv">y = F(x, w)</span></pre><p id="73cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中'<em class="jd"> x </em>为输入，<em class="jd"> w </em>为权重&amp; ' <em class="jd"> F </em>为激活函数。在使用神经网络的情况下，我们有从输入到输出的静态映射。<em class="jd">使用静态</em>，因为模型没有记忆，它只取决于输入和权重。我们看到每个节点都乘以它的权重，我们加上bias，然后再加到下一层节点。然后，激活函数作用于它，然后它传递到下一层。</p><h2 id="ed5f" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">反向传播</h2><p id="8bcd" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">在这方面，我们的主要目标是找到一组权重，使我们在前向传递完成后计算的误差最小化。反向传播滚动到计算误差相对于所有权重的偏导数，然后相应地改变权重。</p><p id="020f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，RNNs的反向传播有点不同。我们不仅得到该层的偏导数，而且还将前面节点的偏导数结果相加。</p><p id="9a07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在通过时间 、<em class="jd"> BPTT </em>的<strong class="ih hj"> <em class="jd">反向传播中，我们在时间步长’<em class="jd">t</em>训练网络，并且考虑所有先前的时间步长。</em></strong></p><figure class="mj mk ml mm fd kt er es paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="er es mw"><img src="../Images/b1ee2a22fdd08cd3c1238af5c8f5fbd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9qwD9gKkTiU9fnsJ3cLUyg.png"/></div></div><figcaption class="la lb et er es lc ld bd b be z dx translated">RNN反向传播方程</figcaption></figure><blockquote class="le lf lg"><p id="fe85" class="if ig jd ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated">反向传播实际上是链式法则下的随机梯度下降。</p></blockquote></div><div class="ab cl mx my gp mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="hb hc hd he hf"><h1 id="bf35" class="ne jf hi bd jg nf ng nh jk ni nj nk jo nl nm nn jr no np nq ju nr ns nt jx nu bi translated">递归神经网络</h1><p id="6c0b" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">这些网络为我们提供了一种将<em class="jd">记忆</em>整合到我们的神经网络中的方法，并将在分析序列数据中发挥关键作用。<em class="jd"> RNN的</em>通常与文本处理和文本生成有关，因为句子是由一系列单词组成的！CNN系统没有任何记忆元素。<em class="jd">rnn</em>通过在产生当前输出时使用<em class="jd">存储器</em>(即网络的过去输入)来解决这个非常基本且重要的问题。这些可以<em class="jd">叠加</em>依赖关系。<em class="jd">记忆</em>定义为隐含层神经元的输出，它将在下一个训练步骤中作为网络的附加输入。</p><blockquote class="le lf lg"><p id="a127" class="if ig jd ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated">系统将自我训练并学习如何优化权重矩阵来实现网络。</p><p id="654e" class="if ig jd ih b ii ij ik il im in io ip lh ir is it li iv iw ix lj iz ja jb jc hb bi translated">我最喜欢的RNN用例之一是生成图纸。<a class="ae nv" href="https://magenta.tensorflow.org/assets/sketch_rnn_demo/index.html" rel="noopener ugc nofollow" target="_blank">素描RNN </a>是一个程序，一旦你给它一些东西(一条线或一个圆等)，它就学会完成一幅画。)要开始了！把画画想象成一个连续的行为很有趣，但事实的确如此！这个网络采用一条起始线或曲线，然后，在经过多种类型的草图训练后，根据你输入的曲线尽最大努力完成绘图。</p></blockquote><figure class="mj mk ml mm fd kt er es paragraph-image"><div class="er es nw"><img src="../Images/030e167c0c06436e66c5dc837fa0313d.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*DJYP1vRFtmzOJlxWBzuMwg.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">RNNs</figcaption></figure><h2 id="5829" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">裂缝</h2><p id="c4cb" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">rnn有一个关键缺陷，因为捕捉跨越8或10步以上的关系实际上是不可能的。这个缺陷源于信息的贡献随时间几何衰减的“<strong class="ih hj"> <em class="jd">消失梯度</em> </strong>”问题。您可能还记得，在训练我们的网络时，我们使用反向传播。在<em class="jd">反向传播</em>过程中，我们使用梯度来调整权重矩阵。在此过程中，<em class="jd">梯度</em>通过导数的连续乘法来计算。这些导数的值可能非常小，以至于这些连续的乘法可能导致梯度实际上“<em class="jd">消失</em>”。</p><p id="9be0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"><em class="jd"/></strong>是克服<em class="jd"> RNNs </em>中消失梯度问题的一个选项。<br/> <strong class="ih hj">长短期记忆细胞</strong> (LSTMs)和<strong class="ih hj">门控循环单元</strong> (GRUs)通过帮助我们应用具有时间依赖性的网络，给出了消失梯度问题的解决方案。在以后的文章中会有更多的介绍。</p><h2 id="3106" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">折叠模型</h2><p id="320c" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">这是当我们有一个正常的前馈，但与连续层，我们有一个' S '层，考虑到内存。“s”是隐藏状态向量。<br/>前馈时，一切完全相同，但我们只需将正常电流<code class="du nx ny nz mo b">weights * inputs</code>和先前的<code class="du nx ny nz mo b">weights * inputs</code>相加，然后将其传递给激活函数。</p><figure class="mj mk ml mm fd kt er es paragraph-image"><div class="er es oa"><img src="../Images/e4663da679be2fae316ccc1243afd95d.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*vtqBfyfPWFE3h7YUX2Qlnw.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">折叠模型</figcaption></figure><h2 id="8ca1" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">展开模型</h2><p id="84db" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">模型也可以“及时展开”。展开的模型通常是我们在使用<em class="jd"> RNNs </em>时使用的。当我们训练<em class="jd"> RNNs </em>时，我们也使用<em class="jd">反向传播</em>，但是在概念上有所改变。这个过程类似于前馈神经网络，除了我们需要考虑以前的时间步骤，因为系统有记忆。这个过程被称为<em class="jd">穿越时间的反向传播(BPTT) </em>。</p><figure class="mj mk ml mm fd kt er es paragraph-image"><div class="er es ob"><img src="../Images/85390bd1261ad15b33e94dc6b215c617.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*fuEn59O-4ZuSK03ZBzvR0w.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">展开模型</figcaption></figure></div><div class="ab cl mx my gp mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="hb hc hd he hf"><figure class="mj mk ml mm fd kt er es paragraph-image"><div class="er es oc"><img src="../Images/40f67a883d49fa8861c61fe21c63fb81.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*8nLU4kJsgWnD0ZiHOUtQ9w.png"/></div><figcaption class="la lb et er es lc ld bd b be z dx translated">项目2</figcaption></figure><p id="0192" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是我今天能做的。Ik这似乎是不完整的，但我今天没有感觉，不得不取消它。无论如何，狗品种分类项目获得批准，该项目是可怕的。下一集再见！</p></div></div>    
</body>
</html>