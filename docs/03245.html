<html>
<head>
<title>Predicting user churn with PySpark (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用PySpark预测用户流失(第1部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/predicting-user-churn-with-pyspark-part-1-f13befbf04c3?source=collection_archive---------27-----------------------#2020-01-21">https://medium.com/analytics-vidhya/predicting-user-churn-with-pyspark-part-1-f13befbf04c3?source=collection_archive---------27-----------------------#2020-01-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="f8f4" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">这是一个三部分系列的第一部分，我们开始探索来自虚拟音乐流媒体平台Sparkify的用户数据，并定义它意味着什么</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/a92a08fca47b533eb37bcb046d2af257.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zHsRLXWzeNYHtanEDeY3YQ.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">虚拟流媒体平台Sparkify的徽标</figcaption></figure><h1 id="b632" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">介绍</h1><blockquote class="kf kg kh"><p id="c883" class="ki kj kk kl b km kn ij ko kp kq im kr ks kt ku kv kw kx ky kz la lb lc ld le hb bi translated"><em class="hi">这一系列文章是我为完成</em> <a class="ae lf" href="https://www.udacity.com/course/data-scientist-nanodegree--nd025" rel="noopener ugc nofollow" target="_blank"> <em class="hi"> Udacity的数据科学家Nanodegree </em> </a> <em class="hi">所要求的最终项目工作的成果，本质上是有教育意义的。</em></p></blockquote><p id="6420" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">Sparkify是一个类似于<a class="ae lf" href="https://en.wikipedia.org/wiki/Spotify" rel="noopener ugc nofollow" target="_blank"> Spotify </a>的虚拟服务。在高层次上，用户可以作为客人或登录用户来播放大量艺术家的歌曲。他们也可以决定为服务付费以获得更多利益。他们也可以随时自由退订这项服务。</p><p id="c278" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated"><a class="ae lf" href="https://www.udacity.com/" rel="noopener ugc nofollow" target="_blank"> Udacity的</a>慷慨地提供了一个中型(128MB)和大型(12GB)数据集，并提供人工用户活动。在这个数据集中，行代表特定用户在某个时间点的动作，例如播放艺术家“Metallica”的歌曲的动作。</p><p id="51fa" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">在三篇文章中，我将向您展示我如何使用<a class="ae lf" href="https://pypi.org/project/pyspark/" rel="noopener ugc nofollow" target="_blank"> pyspark </a>构建一个受监督的机器学习模型，用于预测用户是否会在不久的将来离开该平台(在这种情况下，取消订阅该服务)</p><p id="2641" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">预测客户流失是数据科学家和分析师在任何面向客户的业务中经常遇到的一个具有挑战性的常见问题。此外，使用Spark高效操作大型数据集的能力是数据领域需求最高的技能之一。</p><p id="451c" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">下面是你将在每篇文章中学到的东西的分类:</p><ul class=""><li id="3c11" class="lj lk hi kl b km kn kp kq lg ll lh lm li ln le lo lp lq lr bi translated"><strong class="kl hj">第1部分(本文)</strong>:我们将对128MB的数据集进行数据探索，然后从用户事件开始反向工作以定义流失。</li><li id="baff" class="lj lk hi kl b km ls kp lt lg lu lh lv li lw le lo lp lq lr bi translated"><strong class="kl hj">第二部分(</strong> <a class="ae lf" rel="noopener" href="/@jcm.orlando/predicting-user-churn-with-pyspark-part-2-90874e6807bd"> <strong class="kl hj"> <em class="kk">链接</em> </strong> </a> <strong class="kl hj"> ): </strong>利用探索阶段的知识，我们将精心制作一些预测特征，并将数据输入到有监督的机器学习模型中。</li><li id="5f90" class="lj lk hi kl b km ls kp lt lg lu lh lv li lw le lo lp lq lr bi translated"><strong class="kl hj">第3部分(</strong> <a class="ae lf" rel="noopener" href="/@jcm.orlando/predicting-user-churn-with-pyspark-part-3-89536234fa79"> <strong class="kl hj">链接</strong> </a> <strong class="kl hj"> ): </strong>最后，我们将介绍如何建立一个<a class="ae lf" href="https://aws.amazon.com/emr/" rel="noopener ugc nofollow" target="_blank"> AWS EMR </a>集群的过程，以使用12GB数据集来训练和评估我们的模型。</li></ul><p id="fed6" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">我们开始吧！</p><blockquote class="kf kg kh"><p id="5d68" class="ki kj kk kl b km kn ij ko kp kq im kr ks kt ku kv kw kx ky kz la lb lc ld le hb bi translated">如果您愿意，您可以跳过这些教程，只需访问<a class="ae lf" href="https://github.com/ojcastillo/sparkify" rel="noopener ugc nofollow" target="_blank"> github repo </a>，其中有文章中给出的结果的所有代码和说明(以及更多)</p></blockquote><h1 id="19ad" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">先决条件</h1><p id="5e88" class="pw-post-body-paragraph ki kj hi kl b km lx ij ko kp ly im kr lg lz ku kv lh ma ky kz li mb lc ld le hb bi translated">我假设您已经熟悉PySpark SQL的基础知识，如果不熟悉，我建议您先查看官方的<a class="ae lf" href="https://spark.apache.org/docs/latest/sql-getting-started.html" rel="noopener ugc nofollow" target="_blank">入门指南</a>，然后再回来。</p><p id="db16" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">如果您想跟随并在本地执行代码，您需要下载中等大小的数据集，您可以在这里找到<a class="ae lf" href="http://udacity-dsnd.s3.amazonaws.com/sparkify/mini_sparkify_event_data.json" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="d267" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">我也强烈推荐在Jupyter笔记本会话中运行代码。如果你想得到介绍，请查阅本指南。</p><h2 id="f093" class="mc jo hi bd jp md me mf jt mg mh mi jx lg mj mk jz lh ml mm kb li mn mo kd mp bi translated">Python依赖性</h2><p id="f6cf" class="pw-post-body-paragraph ki kj hi kl b km lx ij ko kp ly im kr lg lz ku kv lh ma ky kz li mb lc ld le hb bi translated">我建议设置一个虚拟环境来安装依赖项。我个人喜欢<a class="ae lf" href="https://en.wikipedia.org/wiki/Conda_(package_manager)" rel="noopener ugc nofollow" target="_blank">康达</a>，这个你可以在这里找到<a class="ae lf" href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html" rel="noopener ugc nofollow" target="_blank">的安装说明。</a></p><p id="037b" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">有了环境后，打开终端运行以下命令，这将安装所有必需的python依赖项:</p><pre class="iy iz ja jb fd mq mr ms mt aw mu bi"><span id="3319" class="mc jo hi mr b fi mv mw l mx my">pip install jupyterlab==1.2.4 \<br/>    pyspark==2.4.4 \<br/>    numpy==1.18.1 \<br/>    matplotlib==3.1.2 \<br/>    pandas==0.25.3</span></pre><h1 id="e08a" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">加载数据</h1><p id="9628" class="pw-post-body-paragraph ki kj hi kl b km lx ij ko kp ly im kr lg lz ku kv lh ma ky kz li mb lc ld le hb bi translated">让我们从导入所有必需的包开始(其中一些将在以后的文章中用到):</p><pre class="iy iz ja jb fd mq mr ms mt aw mu bi"><span id="2601" class="mc jo hi mr b fi mv mw l mx my">import datetime</span><span id="2e62" class="mc jo hi mr b fi mz mw l mx my">import matplotlib.pyplot as plt<br/>from pyspark.ml import Pipeline<br/>from pyspark.ml.classification import LogisticRegression<br/>from pyspark.ml.evaluation import MulticlassClassificationEvaluator<br/>from pyspark.ml.feature import StandardScaler, VectorAssembler<br/>from pyspark.ml.tuning import CrossValidator, ParamGridBuilder<br/>from pyspark.mllib.evaluation import MulticlassMetrics<br/>import pyspark.sql.functions as sqlF<br/>from pyspark.sql import SparkSession<br/>from pyspark.sql import Window<br/>from pyspark.sql.types import IntegerType</span><span id="1017" class="mc jo hi mr b fi mz mw l mx my">%matplotlib inline</span></pre><p id="9ece" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">接下来，让我们创建从现在开始将使用的SparkSession，并加载用于分析的中型数据集(输出代码以<strong class="kl hj">粗体</strong>显示):</p><pre class="iy iz ja jb fd mq mr ms mt aw mu bi"><span id="a1a5" class="mc jo hi mr b fi mv mw l mx my"><em class="kk"># Create the spark session that will be used for the whole notebook</em><br/>spark = SparkSession \<br/>    .builder \<br/>    .appName("Sparkify") \<br/>    .getOrCreate()</span><span id="dc39" class="mc jo hi mr b fi mz mw l mx my"><em class="kk"># Read the medium sized sparkify dataset for the initial exploration<br/># This assumes the json file was downloaded and is on the same <br/># directory in which you're running the code<br/></em>file_path = 'mini_sparkify_event_data.json'<br/>df = spark.read.json(file_path)<br/>df.head()</span><span id="768d" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj"><em class="kk">Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')</em></strong></span></pre><h1 id="c759" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">探索数据</h1><p id="7ddb" class="pw-post-body-paragraph ki kj hi kl b km lx ij ko kp ly im kr lg lz ku kv lh ma ky kz li mb lc ld le hb bi translated">让我们从收集关于数据集的一些高级事实开始:</p><pre class="iy iz ja jb fd mq mr ms mt aw mu bi"><span id="feef" class="mc jo hi mr b fi mv mw l mx my">def nice_describe(df, jump_size=5):<br/>    <em class="kk">"""Wrapper around describe that prints columns at a time"""</em><br/>    ncols = len(df.columns)<br/>    for idx in range(0, ncols, jump_size):<br/>        col_list = df.columns[idx:idx+jump_size]<br/>        print(f'Summary statistics for {col_list}')<br/>        df.describe(col_list).show()</span><span id="3c0a" class="mc jo hi mr b fi mz mw l mx my"><em class="kk"># Print the schema per entry in the json</em><br/>df.printSchema()</span><span id="38fe" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">root<br/> |-- artist: string (nullable = true)<br/> |-- auth: string (nullable = true)<br/> |-- firstName: string (nullable = true)<br/> |-- gender: string (nullable = true)<br/> |-- itemInSession: long (nullable = true)<br/> |-- lastName: string (nullable = true)<br/> |-- length: double (nullable = true)<br/> |-- level: string (nullable = true)<br/> |-- location: string (nullable = true)<br/> |-- method: string (nullable = true)<br/> |-- page: string (nullable = true)<br/> |-- registration: long (nullable = true)<br/> |-- sessionId: long (nullable = true)<br/> |-- song: string (nullable = true)<br/> |-- status: long (nullable = true)<br/> |-- ts: long (nullable = true)<br/> |-- userAgent: string (nullable = true)<br/> |-- userId: string (nullable = true)</strong></span><span id="f55b" class="mc jo hi mr b fi mz mw l mx my"><em class="kk"># Print how many rows and columns are in the dataset</em><br/>df.count(), len(df.columns)</span><span id="43c7" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">(286500, 18)</strong></span><span id="a2d7" class="mc jo hi mr b fi mz mw l mx my"><em class="kk"># Print descriptive statistics 2 columns at a time</em><br/>nice_describe(df, 2)</span><span id="2094" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Summary statistics for ['artist', 'auth']<br/>+-------+------------------+----------+<br/>|summary|            artist|      auth|<br/>+-------+------------------+----------+<br/>|  count|            228108|    286500|<br/>|   mean| 551.0852017937219|      null|<br/>| stddev|1217.7693079161374|      null|<br/>|    min|               !!!| Cancelled|<br/>|    max| ÃÂlafur Arnalds|Logged Out|<br/>+-------+------------------+----------+<br/><br/>Summary statistics for ['firstName', 'gender']<br/>+-------+---------+------+<br/>|summary|firstName|gender|<br/>+-------+---------+------+<br/>|  count|   278154|278154|<br/>|   mean|     null|  null|<br/>| stddev|     null|  null|<br/>|    min| Adelaida|     F|<br/>|    max|   Zyonna|     M|<br/>+-------+---------+------+<br/><br/>Summary statistics for ['itemInSession', 'lastName']<br/>+-------+------------------+--------+<br/>|summary|     itemInSession|lastName|<br/>+-------+------------------+--------+<br/>|  count|            286500|  278154|<br/>|   mean|114.41421291448516|    null|<br/>| stddev|129.76726201141085|    null|<br/>|    min|                 0|   Adams|<br/>|    max|              1321|  Wright|<br/>+-------+------------------+--------+<br/><br/>Summary statistics for ['length', 'level']<br/>+-------+------------------+------+<br/>|summary|            length| level|<br/>+-------+------------------+------+<br/>|  count|            228108|286500|<br/>|   mean|249.11718197783722|  null|<br/>| stddev| 99.23517921058324|  null|<br/>|    min|           0.78322|  free|<br/>|    max|        3024.66567|  paid|<br/>+-------+------------------+------+<br/><br/>Summary statistics for ['location', 'method']<br/>+-------+-----------------+------+<br/>|summary|         location|method|<br/>+-------+-----------------+------+<br/>|  count|           278154|286500|<br/>|   mean|             null|  null|<br/>| stddev|             null|  null|<br/>|    min|       Albany, OR|   GET|<br/>|    max|Winston-Salem, NC|   PUT|<br/>+-------+-----------------+------+<br/><br/>Summary statistics for ['page', 'registration']<br/>+-------+-------+--------------------+<br/>|summary|   page|        registration|<br/>+-------+-------+--------------------+<br/>|  count| 286500|              278154|<br/>|   mean|   null|1.535358834085557E12|<br/>| stddev|   null| 3.291321616328068E9|<br/>|    min|  About|       1521380675000|<br/>|    max|Upgrade|       1543247354000|<br/>+-------+-------+--------------------+<br/><br/>Summary statistics for ['sessionId', 'song']<br/>+-------+-----------------+--------------------+<br/>|summary|        sessionId|                song|<br/>+-------+-----------------+--------------------+<br/>|  count|           286500|              228108|<br/>|   mean|1041.526554973822|            Infinity|<br/>| stddev|726.7762634630834|                 NaN|<br/>|    min|                1|ÃÂg ÃÂtti Gr...|<br/>|    max|             2474|ÃÂau hafa slopp...|<br/>+-------+-----------------+--------------------+<br/><br/>Summary statistics for ['status', 'ts']<br/>+-------+------------------+--------------------+<br/>|summary|            status|                  ts|<br/>+-------+------------------+--------------------+<br/>|  count|            286500|              286500|<br/>|   mean|210.05459685863875|1.540956889810471...|<br/>| stddev| 31.50507848842202|1.5075439608187113E9|<br/>|    min|               200|       1538352117000|<br/>|    max|               404|       1543799476000|<br/>+-------+------------------+--------------------+<br/><br/>Summary statistics for ['userAgent', 'userId']<br/>+-------+--------------------+------------------+<br/>|summary|           userAgent|            userId|<br/>+-------+--------------------+------------------+<br/>|  count|              278154|            286500|<br/>|   mean|                null| 59682.02278593872|<br/>| stddev|                null|109091.94999910519|<br/>|    min|"Mozilla/5.0 (Mac...|                  |<br/>|    max|Mozilla/5.0 (comp...|                99|<br/>+-------+--------------------+------------------+</strong></span></pre><p id="da80" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">让我们也绘制一些可视化图形:</p><pre class="iy iz ja jb fd mq mr ms mt aw mu bi"><span id="e1f6" class="mc jo hi mr b fi mv mw l mx my"># Show a bar chart with proportions of visits per page<br/>page_counts_pd = df.groupby('page').count().sort('count').toPandas()<br/>page_counts_pd['count'] = page_counts_pd['count'].astype(float)<br/>total_visits = page_counts_pd['count'].sum() <br/>page_counts_pd['prop'] = page_counts_pd['count'] / total_visits</span><span id="dd29" class="mc jo hi mr b fi mz mw l mx my">plt.figure(figsize=(16, 6))<br/>plt.barh(page_counts_pd['page'], page_counts_pd["prop"])<br/>plt.title("Proportions of visits per page")<br/>plt.xlabel("Proportion")<br/>plt.ylabel("Page");</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es na"><img src="../Images/eda6fbc2486ce207c2f7fc2666b7069b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wL1qn-9yYpCRv3gypTJQCQ.png"/></div></div></figure><pre class="iy iz ja jb fd mq mr ms mt aw mu bi"><span id="83e5" class="mc jo hi mr b fi mv mw l mx my"><em class="kk"># Show a bar chart with proportions of auth types</em><br/>auth_counts_pd = df.groupby('auth').count().sort('count').toPandas()<br/>auth_counts_pd['count'] = auth_counts_pd['count'].astype(float)<br/>total_auths = auth_counts_pd['count'].sum() <br/>auth_counts_pd['prop'] = auth_counts_pd['count'] / total_auths</span><span id="0aef" class="mc jo hi mr b fi mz mw l mx my">plt.figure(figsize=(16, 6))<br/>plt.barh(auth_counts_pd['auth'], auth_counts_pd["prop"])<br/>plt.title("Proportions of auth types")<br/>plt.xlabel("Proportion")<br/>plt.ylabel("auth");</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nb"><img src="../Images/04f069c560e0856bbb1e7194963b3fb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nPMANel0GycVYG0vrUQxrA.png"/></div></div></figure><pre class="iy iz ja jb fd mq mr ms mt aw mu bi"><span id="1825" class="mc jo hi mr b fi mv mw l mx my"><em class="kk"># Distribution of user actions per session</em><br/>action_counts_pd = df.groupby('userId', 'sessionId') \<br/>    .max() \<br/>    .withColumnRenamed('max(itemInSession)', 'session_actions') \<br/>    .toPandas()</span><span id="3b68" class="mc jo hi mr b fi mz mw l mx my">plt.figure(figsize=(16, 6))<br/>plt.hist(action_counts_pd['session_actions'])<br/>plt.title("Distribution of user actions per session (computed from itemInSession)")<br/>plt.xlabel("Amount of actions");</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nc"><img src="../Images/217ec1a41eb8784712fab8588c86035d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kmR5ISLj7CjmCali431VCA.png"/></div></div></figure><h1 id="ff2d" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">清理数据集</h1><p id="1417" class="pw-post-body-paragraph ki kj hi kl b km lx ij ko kp ly im kr lg lz ku kv lh ma ky kz li mb lc ld le hb bi translated">我们没有遗漏任何<code class="du nd ne nf mr b">userId</code>，但是看起来我们有空值的行。既然我们对用户流失感兴趣，那么理想情况下，我们希望能够将每一行追溯到某个用户的行为。让我们研究这些行，然后决定如何处理它们:</p><pre class="iy iz ja jb fd mq mr ms mt aw mu bi"><span id="fe56" class="mc jo hi mr b fi mv mw l mx my">def show_unique_stats(df, columns, sample_size=5):<br/>    <em class="kk">"""Function to print unique value stats of specific columns"""</em><br/>    for col in columns:<br/>        print(f'\nColumn "{col}":')<br/>        uniques = df.select(col).dropDuplicates()<br/>        nuniques = uniques.count()<br/>        print(f'\tNumber of unique values: {nuniques}')<br/>        print(f'\tSample: {uniques.head(sample_size)}')</span><span id="0e46" class="mc jo hi mr b fi mz mw l mx my"><em class="kk"># Explore rows with an empty user_id</em><br/>no_user_df = df.filter('userId == ""')<br/>print(f'Number of rows with empty userId: {no_user_df.count()}')<br/>print('Sample of rows:')<br/>no_user_df.head(1)</span><span id="4deb" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Number of rows with empty userId: 8346<br/>Sample row:<br/>[Row(artist=None, auth='Logged Out', firstName=None, gender=None, itemInSession=100, lastName=None, length=None, level='free', location=None, method='GET', page='Home', registration=None, sessionId=8, song=None, status=200, ts=1538355745000, userAgent=None, userId='')]</strong></span><span id="fe0a" class="mc jo hi mr b fi mz mw l mx my"><em class="kk"># Print unique value statistics of all categorical columns<br/># for rows with no user_id defined</em><br/>categorical_cols = [<br/>    'artist', 'auth', 'firstName', 'gender', 'lastName', 'level', <br/>    'location', 'method', 'page', 'song', 'userAgent'<br/>]<br/>show_unique_stats(no_user_df, categorical_cols, 10)</span><span id="6742" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Column "artist":<br/>	Number of unique values: 1<br/>	Sample: [Row(artist=None)]</strong></span><span id="d5b4" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Column "auth":<br/>	Number of unique values: 2<br/>	Sample: [Row(auth='Logged Out'), Row(auth='Guest')]</strong></span><span id="9463" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Column "firstName":<br/>	Number of unique values: 1<br/>	Sample: [Row(firstName=None)]</strong></span><span id="ff36" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Column "gender":<br/>	Number of unique values: 1<br/>	Sample: [Row(gender=None)]</strong></span><span id="ba69" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Column "lastName":<br/>	Number of unique values: 1<br/>	Sample: [Row(lastName=None)]</strong></span><span id="7a4b" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Column "level":<br/>	Number of unique values: 2<br/>	Sample: [Row(level='free'), Row(level='paid')]</strong></span><span id="2582" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Column "location":<br/>	Number of unique values: 1<br/>	Sample: [Row(location=None)]</strong></span><span id="1428" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Column "method":<br/>	Number of unique values: 2<br/>	Sample: [Row(method='PUT'), Row(method='GET')]</strong></span><span id="7614" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Column "page":<br/>	Number of unique values: 7<br/>	Sample: [Row(page='Home'), Row(page='About'), Row(page='Submit Registration'), Row(page='Login'), Row(page='Register'), Row(page='Help'), Row(page='Error')]</strong></span><span id="24ac" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Column "song":<br/>	Number of unique values: 1<br/>	Sample: [Row(song=None)]</strong></span><span id="5b0a" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Column "userAgent":<br/>	Number of unique values: 1<br/>	Sample: [Row(userAgent=None)]</strong></span></pre><p id="b1df" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">因此，无论是来宾还是已注销的个人都没有定义<code class="du nd ne nf mr b">userId</code>，这是有意义的。考虑到这一点，我认为继续使用定义了用户id的行是安全的:</p><pre class="iy iz ja jb fd mq mr ms mt aw mu bi"><span id="4686" class="mc jo hi mr b fi mv mw l mx my"><em class="kk"># Remove rows with an empty user_id</em><br/>df = df.filter('userId != ""')</span></pre><h1 id="c5e7" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">定义流失</h1><p id="88df" class="pw-post-body-paragraph ki kj hi kl b km lx ij ko kp ly im kr lg lz ku kv lh ma ky kz li mb lc ld le hb bi translated">我们可以将客户流失定义为用户取消订阅Sparkify服务的行为。在最初的探索中，<code class="du nd ne nf mr b">auth</code>字段显示它可以接受值<code class="du nd ne nf mr b">Cancelled</code>,我预计这些行将允许我们识别用户。让我们来看一行这样的状态:</p><pre class="iy iz ja jb fd mq mr ms mt aw mu bi"><span id="79a6" class="mc jo hi mr b fi mv mw l mx my">df.where('auth == "Cancelled"').head(1)</span><span id="4576" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">[Row(artist=None, auth='Cancelled', firstName='Adriel', gender='M', itemInSession=104, lastName='Mendoza', length=None, level='paid', location='Kansas City, MO-KS', method='GET', page='Cancellation Confirmation', registration=1535623466000, sessionId=514, song=None, status=200, ts=1538943990000, userAgent='"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.77.4 (KHTML, like Gecko) Version/7.0.5 Safari/537.77.4"', userId='18')]</strong></span></pre><p id="439b" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">用户访问了<code class="du nd ne nf mr b">Cancellation Confirmation</code>页面，所以听起来它确实在那一点上发生了变化。让我们探究一下该用户在该特定会话中的时间线，以帮助我们更好地理解发生了什么:</p><pre class="iy iz ja jb fd mq mr ms mt aw mu bi"><span id="15ed" class="mc jo hi mr b fi mv mw l mx my">def user_timeline(df, user_id, session_id, cols, n=5):<br/>    <em class="kk">"""Print rows the last n actions of an user in a session"""</em><br/>    user_df = df.where(f'userId={user_id} AND sessionId={session_id}')<br/>    print(f'Number of rows for user with id {user_id} and session id {session_id}: {user_df.count()}')<br/>    user_df.select(cols).sort(sqlF.desc('ts')).show(n)</span><span id="b280" class="mc jo hi mr b fi mz mw l mx my"><em class="kk"># Timeline for the user with id 18 and session with id 514</em><br/>user_timeline(df, 18, 514, ['ts', 'sessionId', 'auth', 'page'])</span><span id="c834" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Number of rows for user with id 18 and session id 514: 102<br/>+-------------+---------+---------+--------------------+<br/>|           ts|sessionId|     auth|                page|<br/>+-------------+---------+---------+--------------------+<br/>|1538943990000|      514|Cancelled|Cancellation Conf...|<br/>|1538943740000|      514|Logged In|              Cancel|<br/>|1538943739000|      514|Logged In|           Downgrade|<br/>|1538943726000|      514|Logged In|            NextSong|<br/>|1538943440000|      514|Logged In|            NextSong|<br/>+-------------+---------+---------+--------------------+<br/>only showing top 5 rows</strong></span></pre><p id="2ac4" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">这幅图现在开始变得有意义了:当用户在某个时候访问了<code class="du nd ne nf mr b">Cancellation Confirmation</code>页面，那么随之而来的就是用户不再是<code class="du nd ne nf mr b">Logged In</code>。我们可以证明:</p><pre class="iy iz ja jb fd mq mr ms mt aw mu bi"><span id="a1d3" class="mc jo hi mr b fi mv mw l mx my"><em class="kk"># Validate that when an user visits the `Cancellation Confirmation` page, then is no longer `Logged In`</em><br/>cancel_subset_df = df.where('page="Cancellation Confirmation"')<br/>show_unique_stats(cancel_subset_df, ['auth'])</span><span id="d343" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Column "auth":<br/>	Number of unique values: 1<br/>	Sample: [Row(auth='Cancelled')]</strong></span><span id="5ec1" class="mc jo hi mr b fi mz mw l mx my"><em class="kk"># Does a user with a `Cancelled` auth means it can only have visited the `Cancellation Confirmation` page?</em><br/>auth_subset_df = df.where('auth="Cancelled"')<br/>show_unique_stats(auth_subset_df, ['page'])</span><span id="474b" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Column "page":<br/>	Number of unique values: 1<br/>	Sample: [Row(page='Cancellation Confirmation')]</strong></span></pre><p id="a72e" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">因此，鉴于以上所有情况，我认为可以肯定地说，任何具有auth值<code class="du nd ne nf mr b">Cancelled</code>的用户都可以被认为在这一点上受到了干扰。</p><p id="0246" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">让我们在dataframe中添加一个<code class="du nd ne nf mr b">churned</code>列，如果用户在某个时候离开了平台，则该列标记为1，否则标记为0:</p><pre class="iy iz ja jb fd mq mr ms mt aw mu bi"><span id="7d25" class="mc jo hi mr b fi mv mw l mx my">def add_label_churned(df):<br/>    <em class="kk">"""Add the `churned` to indicate if the user churned"""</em><br/>    <br/>    <em class="kk"># Identify the rows with a cancelled auth state and mark those <br/>    # with 1, then use a window function that groups</em><br/>    <em class="kk"># by users and puts the cancel event at the top (if any) so <br/>    # every row gets a one after that when we sum</em><br/>    cancelled_udf = sqlF.udf(<br/>        lambda x: 1 if x == 'Cancelled' else 0, IntegerType())<br/>    current_window = Window.partitionBy('userId') \<br/>        .orderBy(sqlF.desc('cancelled')) \<br/>        .rangeBetween(Window.unboundedPreceding, 0)<br/>    churned_df = df.withColumn('cancelled', cancelled_udf('auth')) \<br/>        .withColumn("churned",<br/>            sqlF.sum('cancelled').over(current_window))<br/>    return churned_df.drop('cancelled')</span><span id="5653" class="mc jo hi mr b fi mz mw l mx my"><em class="kk"># Add the `churned` label</em><br/>df = add_label_churned(df)</span><span id="2b85" class="mc jo hi mr b fi mz mw l mx my"><em class="kk"># Show once again the timeline of actions for the user with id 18 <br/># and session id 514<br/></em>user_timeline(add_label_churned(df), 18, 514, <br/>              ['ts', 'sessionId', 'page', 'churned'])</span><span id="bf07" class="mc jo hi mr b fi mz mw l mx my"><strong class="mr hj">Number of rows for user with id 18 and session id 514: 102<br/>+-------------+---------+--------------------+-------+<br/>|           ts|sessionId|                page|churned|<br/>+-------------+---------+--------------------+-------+<br/>|1538943990000|      514|Cancellation Conf...|      1|<br/>|1538943740000|      514|              Cancel|      1|<br/>|1538943739000|      514|           Downgrade|      1|<br/>|1538943726000|      514|            NextSong|      1|<br/>|1538943440000|      514|            NextSong|      1|<br/>+-------------+---------+--------------------+-------+<br/>only showing top 5 rows</strong></span></pre><p id="85e7" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">我们现在已经正确标记了所有行！</p><h1 id="3b19" class="jn jo hi bd jp jq jr js jt ju jv jw jx io jy ip jz ir ka is kb iu kc iv kd ke bi translated">未完待续…</h1><p id="06f7" class="pw-post-body-paragraph ki kj hi kl b km lx ij ko kp ly im kr lg lz ku kv lh ma ky kz li mb lc ld le hb bi translated">在本系列的<a class="ae lf" rel="noopener" href="/@jcm.orlando/predicting-user-churn-with-pyspark-part-2-90874e6807bd">下一篇文章</a>中，我将分享我制作一些预测功能的过程，将数据成形为一种形式，其中每个用户由一行表示，然后我如何将数据输入到一个受监督的机器学习模型中。</p><p id="93e5" class="pw-post-body-paragraph ki kj hi kl b km kn ij ko kp kq im kr lg kt ku kv lh kx ky kz li lb lc ld le hb bi translated">正如文章开头提到的，如果您对可用于重现所有工作结果的实际代码感兴趣，您也可以访问我的<a class="ae lf" href="https://github.com/ojcastillo/sparkify" rel="noopener ugc nofollow" target="_blank"> github repo </a>。</p></div></div>    
</body>
</html>