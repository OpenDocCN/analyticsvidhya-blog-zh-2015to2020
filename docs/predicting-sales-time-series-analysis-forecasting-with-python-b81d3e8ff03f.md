# é¢„æµ‹é”€å”®:ç”¨ Python è¿›è¡Œæ—¶é—´åºåˆ—åˆ†æå’Œé¢„æµ‹

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/predicting-sales-time-series-analysis-forecasting-with-python-b81d3e8ff03f?source=collection_archive---------0----------------------->

![](img/324fc4005642e691b080830156250c86.png)

ä»»ä½•é›¶å”®å•†åº—å…¬å¸æœ€é‡è¦çš„ä»»åŠ¡ä¹‹ä¸€æ˜¯åˆ†æå…¶å•†åº—çš„ä¸šç»©ã€‚ä»»ä½•é›¶å”®å•†åº—é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯æå‰é¢„æµ‹æ¯ä¸ªå•†åº—æ‰€éœ€çš„é”€å”®é¢å’Œåº“å­˜ï¼Œä»¥é¿å…åº“å­˜è¿‡å¤šå’Œä¸è¶³ã€‚è¿™æœ‰åŠ©äºä¼ä¸šæä¾›æœ€ä½³çš„é¡¾å®¢ä½“éªŒï¼Œé¿å…äºæŸï¼Œä»è€Œç¡®ä¿å•†åº—çš„å¯æŒç»­ç»è¥ã€‚

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨ Kaggle ä¸Šçš„ Rossmann store [æ•°æ®](https://www.kaggle.com/c/rossmann-store-sales)ã€‚

![](img/2896f23ad8a4b2efd77a818e7d679f10.png)

Rossmann åœ¨ 7 ä¸ªæ¬§æ´²å›½å®¶ç»è¥ç€ 3000 å¤šå®¶è¯åº—ã€‚æŒ‘æˆ˜åœ¨äºæå‰é¢„æµ‹ä»–ä»¬é•¿è¾¾å…­å‘¨çš„æ—¥é”€å”®é¢ã€‚å•†åº—é”€å”®å—è®¸å¤šå› ç´ å½±å“ï¼ŒåŒ…æ‹¬ä¿ƒé”€ã€ç«äº‰ã€å­¦æ ¡å’Œå›½å®¶å‡æœŸã€å­£èŠ‚æ€§å’Œåœ°ç‚¹ã€‚

è¿™ç¯‡æ–‡ç« åˆ†ä¸ºä¸¤éƒ¨åˆ†:EDA å’Œé¢„æµ‹

# **éƒ¨åˆ† A)æ¢ç´¢æ€§æ•°æ®åˆ†æ(EDA)**

è®©æˆ‘ä»¬é¦–å…ˆå¯¼å…¥æ‰€éœ€çš„åº“ï¼Œç„¶åè¿›è¡Œæ•°æ®æ¢ç´¢ã€‚

```
# Importing required libraries
import numpy as np
import pandas as pd, datetime
import seaborn as sns
from statsmodels.tsa.stattools import adfuller
import matplotlib.pyplot as plt
get_ipython().run_line_magic('matplotlib', 'inline')
from time import time
import os
from math import sqrt
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import itertools
import statsmodels.api as sm
from statsmodels.tsa.stattools import acf,pacf
from statsmodels.tsa.arima_model import  ARIMA
from sklearn import model_selection
from sklearn.metrics import mean_squared_error, r2_score
from pandas import DataFrame
import xgboost as xgb
from fbprophet import Prophet
import warnings
warnings.filterwarnings('ignore')# Importing store data
store = pd.read_csv('./data/store.csv')
store.head()
```

![](img/ef636e6953812b6ae935511bf907b192.png)

ä¸Šè¡¨ç»™å‡ºäº†ç½—æ–¯æ›¼æ‹¥æœ‰çš„ 1115 å®¶å•†åº—çš„ä¿¡æ¯ã€‚

```
# Importing train data
train = pd.read_csv('./data/train.csv', index_col='Date', parse_dates = True)
train.head()
```

![](img/85384a827e53d07a3e554c65df443f18.png)

ä»ä¸Šè¡¨ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæ—¥æœŸæ˜¯å…¶ä¸­çš„ä¸€åˆ—ã€‚è¿™ä¸€åˆ†æçš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äº,â€œæ—¥æœŸâ€æ˜¯å½±å“é”€å”®çš„é‡è¦å› ç´ ä¹‹ä¸€ï¼Œå®ƒæ˜¯ä¸€ä¸ªç‹¬ç«‹å˜é‡ã€‚ç®€è€Œè¨€ä¹‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¶é—´åºåˆ—æ•°æ®ï¼Œå³æŒ‰æ—¶é—´é¡ºåºæ’åˆ—çš„ä¸€ç³»åˆ—æ•°æ®ç‚¹ã€‚

1.  **è¶‹åŠ¿&å­£èŠ‚æ€§**

è®©æˆ‘ä»¬çœ‹çœ‹é”€å”®é¢å¦‚ä½•éšæœˆä»½ã€ä¿ƒé”€ã€ä¿ƒé”€ 2(ç¬¬äºŒæ¬¡ä¿ƒé”€ä¼˜æƒ )å’Œå¹´ä»½è€Œå˜åŒ–ã€‚

```
# Sales trend over the months and year
sns.factorplot(data = train_store_joined, x ="Month", y = "Sales", 
               col = 'Promo', # per store type in cols
               hue = 'Promo2',
               row = "Year")
```

![](img/4cc809131f51bee58d4c168b533f0d22.png)

ä¸Šå›¾å‘Šè¯‰æˆ‘ä»¬ï¼Œé”€å”®å¾€å¾€ä¼šåœ¨ 12 æœˆè¾¾åˆ°å³°å€¼ï¼Œè¿™æ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºåœ£è¯èŠ‚å’Œå‡æœŸæ˜¯æ—ºå­£ã€‚å› æ­¤ï¼Œè¿™è¯å®äº†é”€å”®éšç€â€œæ—¥æœŸâ€(æ—¶é—´)è€Œå˜åŒ–ï¼Œå¹¶ä¸”åœ¨æˆ‘ä»¬çš„æ•°æ®ä¸­å­˜åœ¨å­£èŠ‚æ€§å› ç´ ã€‚

```
# Sales trend over days
sns.factorplot(data = train_store_joined, x = "DayOfWeek", y = "Sales", hue = "Promo")
```

![](img/9203bea0e905d1a6d1b7b5ea462f057c.png)

ä»ä¸Šè¿°è¶‹åŠ¿ä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œå‘¨æœ«ï¼Œå³å‘¨å…­å’Œå‘¨æ—¥æ²¡æœ‰ä¿ƒé”€æ´»åŠ¨ï¼Œè¿™æ˜¯æœ‰é“ç†çš„ï¼Œå› ä¸ºå•†åº—å¸Œæœ›åœ¨äººä»¬åšå®¶åŠ¡çš„æ—¶å€™èµšå–æœ€å¤§åˆ©æ¶¦ã€‚

é”€å”®é¢å¾€å¾€åœ¨å‘¨æ—¥å¢åŠ ï¼Œå› ä¸ºäººä»¬åœ¨å‘¨æœ«è´­ç‰©ã€‚æˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°ï¼Œæœ€å¤§çš„é”€å”®å‘ç”Ÿåœ¨æœ‰ä¿ƒé”€æ´»åŠ¨çš„å‘¨ä¸€ã€‚

**2ã€‚æ—¶é—´åºåˆ—çš„å¹³ç¨³æ€§**

ä¸ºäº†ä½¿ç”¨æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æˆ‘ä»¬çš„æ—¶é—´åºåˆ—æ•°æ®æ˜¯å¹³ç¨³çš„ï¼Œå³æ’å®šå‡å€¼ã€æ’å®šæ–¹å·®å’Œæ’å®šåæ–¹å·®ã€‚

æœ‰ä¸¤ç§æ–¹æ³•å¯ä»¥æ£€éªŒæ—¶é—´åºåˆ—çš„å¹³ç¨³æ€§:

**a)æ»šåŠ¨å¹³å‡å€¼**:æ—¶é—´åºåˆ—æ¨¡å‹çš„æ»šåŠ¨åˆ†æé€šå¸¸ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨ä¸€æ®µæ—¶é—´å†…çš„ç¨³å®šæ€§ã€‚çª—å£ä»¥å‘¨ä¸ºå•ä½æ»šåŠ¨(æ»‘è¿‡æ•°æ®)ï¼Œå…¶ä¸­ä»¥å‘¨ä¸ºå•ä½å–å¹³å‡å€¼ã€‚æ»šåŠ¨ç»Ÿè®¡æ˜¯ä¸€ç§å¯è§†åŒ–æµ‹è¯•ï¼Œæˆ‘ä»¬å¯ä»¥å°†åŸå§‹æ•°æ®ä¸æ»šåŠ¨æ•°æ®è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶æ£€æŸ¥æ•°æ®æ˜¯å¦ç¨³å®šã€‚

**b) Dicky -Fuller æ£€éªŒ**:è¯¥æ£€éªŒä¸ºæˆ‘ä»¬æä¾› p å€¼ç­‰ç»Ÿè®¡æ•°æ®ï¼Œä»¥äº†è§£æˆ‘ä»¬æ˜¯å¦å¯ä»¥æ‹’ç»é›¶å‡è®¾ã€‚é›¶å‡è®¾æ˜¯æ•°æ®ä¸æ˜¯é™æ€çš„ï¼Œå¦ä¸€ä¸ªå‡è®¾æ˜¯æ•°æ®æ˜¯é™æ€çš„ã€‚å¦‚æœ p å€¼å°äºä¸´ç•Œå€¼(æ¯”å¦‚ 0.5)ï¼Œæˆ‘ä»¬å°†æ‹’ç»é›¶å‡è®¾ï¼Œå¹¶è¯´æ•°æ®æ˜¯å¹³ç¨³çš„ã€‚

è®©æˆ‘ä»¬æ£€æŸ¥' a 'å‹å•†åº—çš„å¹³ç¨³æ€§ã€‚

```
# Data Preparation: input should be float type
train['Sales'] = train['Sales'] * 1.0# Assigning one store from each category
sales_a = train[train.Store == 2]['Sales']# Trend
sales_a.resample('W').sum().plot(ax = ax1)
```

![](img/d29533d67da38b3529f7fb4baeecd39b.png)

ä¸Šå›¾å‘Šè¯‰æˆ‘ä»¬ï¼Œé”€å”®å¾€å¾€åœ¨å¹´åº•è¾¾åˆ°é«˜å³°ã€‚

```
# Function to test the stationarity
def test_stationarity(timeseries):

    # Determing rolling statistics
    roll_mean = timeseries.rolling(window=7).mean()
    roll_std = timeseries.rolling(window=7).std()# Plotting rolling statistics:
    orig = plt.plot(timeseries.resample('W').mean(), color='blue',label='Original')
    mean = plt.plot(roll_mean.resample('W').mean(), color='red', label='Rolling Mean')
    std = plt.plot(roll_std.resample('W').mean(), color='green', label = 'Rolling Std')
    plt.legend(loc='best')
    plt.show(block=False)

  # Performing Dickey-Fuller test:
    print('Results of Dickey-Fuller Test:')
    result = adfuller(timeseries, autolag='AIC')
    print('ADF Statistic: %f' % result[0])
    print('p-value: %f' % result[1])
    print('Critical Values:')
    for key, value in result[4].items():
           print(key, value)# Testing stationarity of store type a
test_stationarity(sales_a)
```

![](img/8f4f30eae006ebdc64e412a85242ff51.png)

ä»ä¸Šé¢çš„å›¾å’Œç»Ÿè®¡æ£€éªŒä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œå‡å€¼å’Œæ–¹å·®å¹¶ä¸éšæ—¶é—´å˜åŒ–å¾ˆå¤§ï¼Œå³å®ƒä»¬æ˜¯å¸¸æ•°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ‰§è¡Œä»»ä½•è½¬æ¢(å½“æ—¶é—´åºåˆ—ä¸ç¨³å®šæ—¶éœ€è¦)ã€‚

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ç”¨åˆ†è§£å›¾æ¥çœ‹çœ‹å­£èŠ‚æ€§å’Œè¶‹åŠ¿ã€‚

```
# Plotting seasonality and trend
def plot_timeseries(sales,StoreType):fig, axes = plt.subplots(2, 1, sharex=True, sharey=False)
    fig.set_figheight(10)
    fig.set_figwidth(15)decomposition= seasonal_decompose(sales, model = 'additive',freq=365)estimated_trend = decomposition.trend
    estimated_seasonal = decomposition.seasonal
    estimated_residual = decomposition.resid

    axes[1].plot(estimated_seasonal, 'g', label='Seasonality')
    axes[1].legend(loc='upper left');

    axes[0].plot(estimated_trend, label='Trend')
    axes[0].legend(loc='upper left');plt.title('Decomposition Plots')
```

![](img/145a4ffd3b4e6f6d02896cdd92f01289.png)

ä»ä¸Šé¢çš„å›¾ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æˆ‘ä»¬çš„æ•°æ®ä¸­å­˜åœ¨å­£èŠ‚æ€§å’Œè¶‹åŠ¿æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è€ƒè™‘åˆ°è¿™ä¸¤ä¸ªå› ç´ çš„é¢„æµ‹æ¨¡å‹ã€‚

# **B)éƒ¨åˆ†é¢„æµ‹:é¢„æµ‹æ¨¡å‹**

1.  **è¯„ä¼°æŒ‡æ ‡**

æœ‰ä¸¤ä¸ªæµè¡Œçš„æŒ‡æ ‡ç”¨äºè¡¡é‡å›å½’(è¿ç»­å˜é‡)æ¨¡å‹çš„æ€§èƒ½ï¼Œå³æ¢…ä¼Šå’Œ RMSEã€‚

**å¹³å‡ç»å¯¹è¯¯å·®(MAE)** :é¢„æµ‹å€¼ä¸è§‚æµ‹å€¼ç»å¯¹å·®çš„å¹³å‡å€¼ã€‚

**å‡æ–¹æ ¹è¯¯å·®(RMSE)** :æ˜¯é¢„æµ‹å€¼ä¸è§‚æµ‹å€¼çš„å¹³æ–¹å·®çš„å¹³å‡å€¼çš„å¹³æ–¹æ ¹ã€‚

MAE æ›´å®¹æ˜“ç†è§£å’Œè§£é‡Šï¼Œä½†æ˜¯ RMSE åœ¨ä¸å¸Œæœ›å‡ºç°å¤§é”™è¯¯çš„æƒ…å†µä¸‹å·¥ä½œå¾—å¾ˆå¥½ã€‚è¿™æ˜¯å› ä¸ºè¯¯å·®åœ¨è¢«å¹³å‡ä¹‹å‰è¢«å¹³æ–¹ï¼Œå› æ­¤æƒ©ç½šäº†å¤§çš„è¯¯å·®ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼ŒRMSE éå¸¸é€‚åˆï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›ä»¥æœ€å°çš„è¯¯å·®é¢„æµ‹é”€å”®(å³æƒ©ç½šé«˜è¯¯å·®)ï¼Œä»¥ä¾¿å¯ä»¥æ­£ç¡®åœ°ç®¡ç†åº“å­˜ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å°†é€‰æ‹© RMSE ä½œä¸ºè¡¡é‡æ¨¡å‹æ€§èƒ½çš„æŒ‡æ ‡ã€‚

**2ã€‚é¢„æµ‹å»ºæ¨¡**

**æ¨¡å‹ 1:è‡ªå›å½’ç»¼åˆç§»åŠ¨å¹³å‡çº¿(ARIMA)**

æˆ‘ä»¬å°†ä½¿ç”¨æœ€å¸¸ç”¨çš„æ—¶é—´åºåˆ—é¢„æµ‹æ–¹æ³•ä¹‹ä¸€ï¼Œå³ ARIMAã€‚

ARIMA æ¨¡å‹ç”¨ ARIMA(pï¼Œdï¼Œq)æ¥è¡¨ç¤ºã€‚

pã€d å’Œ q åˆ†åˆ«ä»£è¡¨æ•°æ®ä¸­çš„å­£èŠ‚æ€§ã€è¶‹åŠ¿å’Œå™ªå£°ã€‚æˆ‘ä»¬å°†é¦–å…ˆåˆ›å»º pã€d å’Œ q çš„æ‰€æœ‰å¯èƒ½ç»„åˆï¼Œå¦‚ä¸‹æ‰€ç¤º:

```
# Define the p, d and q parameters to take any value between 0 and 3
p = d = q = range(0, 2)# Generate all different combinations of p, q and q triplets
pdq = list(itertools.product(p, d, q))# Generate all different combinations of seasonal p, q and q triplets
seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]print('Examples of parameter combinations for Seasonal ARIMA: ')
print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))
print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))
print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))
print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))
```

![](img/c19e32af17713bc1ab2216485642cba9.png)

**ARIMA è¶…å‚æ•°è°ƒè°**

ä¸ºäº†é€‰æ‹©ä¸Šè¿°å‚æ•°çš„æœ€ä½³ç»„åˆï¼Œæˆ‘ä»¬å°†ä½¿ç”¨**ç½‘æ ¼æœç´¢**ã€‚å‚æ•°çš„æœ€ä½³ç»„åˆå°†ç»™å‡ºæœ€ä½çš„èµ¤æ± ä¿¡æ¯æ ‡å‡†(AIC)åˆ†æ•°ã€‚AIC å‘Šè¯‰æˆ‘ä»¬ä¸€ç»„ç»™å®šæ•°æ®çš„ç»Ÿè®¡æ¨¡å‹çš„è´¨é‡ã€‚

```
# Determing p,d,q combinations with AIC scores.
for param in pdq:
    for param_seasonal in seasonal_pdq:
        try:
            mod = sm.tsa.statespace.SARIMAX(train_arima,
                                            order=param,
                                            seasonal_order=param_seasonal,
                                            enforce_stationarity=False,
                                            enforce_invertibility=False)results = mod.fit()print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))
        except:
            continue
```

![](img/6095a0b2c7bd9f075e387494b492ceec.png)

æˆ‘åªåŒ…æ‹¬äº†ç½‘æ ¼æœç´¢çš„å¿«ç…§ã€‚ä»¥ä¸Šè¿­ä»£å»ºè®® **SARIMAX(1ï¼Œ1ï¼Œ1)x(0ï¼Œ1ï¼Œ1ï¼Œ12)12** ä¸ºæœ€ä½ **AIC: 1806.29 çš„æœ€ä½³å‚æ•°ç»„åˆã€‚**

**æ‹Ÿåˆ ARIMA æ¨¡å‹**

```
# Fitting the data to ARIMA model 
model_sarima = sm.tsa.statespace.SARIMAX(train_arima,
                                order=(1, 1, 1),
                                seasonal_order=(0, 1, 1, 12),
                                enforce_stationarity=False,
                                enforce_invertibility=False)results_sarima = model_sarima.fit()print(results_sarima.summary().tables[1])
```

![](img/6e06d4dbadc756991dd81f1613fe6162.png)

è®©æˆ‘ä»¬æ£€æŸ¥è¯Šæ–­å›¾æ¥å¯è§†åŒ–æˆ‘ä»¬æ¨¡å‹çš„æ€§èƒ½ã€‚

```
# Checking diagnostic plots
results_sarima.plot_diagnostics(figsize=(10, 10))
plt.show()
```

![](img/a44a189d1ed1d7526f22103e58ceedae.png)

æ­£æ€ Q-Q å›¾æ˜¾ç¤ºæ®‹å·®çš„æœ‰åºåˆ†å¸ƒéµå¾ªç±»ä¼¼æ­£æ€åˆ†å¸ƒçš„åˆ†å¸ƒã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„æ¨¡å‹ä¼¼ä¹ç›¸å½“ä¸é”™ã€‚

**æ¨¡å‹é¢„æµ‹**

```
# Model Prediction
# Predictions are performed for the 11th Jan' 2015 onwards of the train data.pred = results_sarima.get_prediction(start=pd.to_datetime('2015-01-11'), dynamic = False)# Get confidence intervals of forecasts
pred_ci = pred.conf_int()ax = train_arima["2014":].plot(label = "observed", figsize=(15, 7))
pred.predicted_mean.plot(ax = ax, label = "One-step ahead Forecast", alpha = 1)
ax.fill_between(pred_ci.index, 
                pred_ci.iloc[:, 0], 
                pred_ci.iloc[:, 1], 
                color = "k", alpha = 0.05)ax.set_xlabel("Date")
ax.set_ylabel("Sales")plt.legend
plt.show()train_arima_forecasted = pred.predicted_mean
train_arima_truth = train_arima["2015-01-11":]# Calculating the error
rms_arima = sqrt(mean_squared_error(train_arima_truth, train_arima_forecasted))
print("Root Mean Squared Error: ", rms_arima)
```

![](img/05b212edee0bbe8cd2eda66cb0b9dbf6.png)![](img/ae0f697403dd1b88848ed60e3d414c7c.png)

ä¸Šå›¾æ˜¾ç¤ºï¼Œæˆ‘ä»¬çš„é¢„æµ‹å€¼èµ¶ä¸Šäº†æ•°æ®é›†ä¸­çš„è§‚å¯Ÿå€¼ã€‚æˆ‘ä»¬çš„é¢„æµ‹ä¼¼ä¹ä¸å®é™…æƒ…å†µéå¸¸å»åˆï¼Œå¹¶å¦‚é¢„æœŸçš„é‚£æ ·åœ¨ 12 æœˆä»½å‡ºç°å³°å€¼ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼ŒRMSE ä¹Ÿç›¸å½“ä½ã€‚

å› æ­¤ï¼Œæœ€ç»ˆçš„ ARIMA æ¨¡å‹å¯ä»¥è¡¨ç¤ºä¸º **SARIMAX(1ï¼Œ1ï¼Œ1)x(0ï¼Œ1ï¼Œ1ï¼Œ12)12ã€‚è¿™æ˜¯æˆ‘ä»¬èƒ½å¯¹ ARIMA åšçš„æœ€å¥½çš„ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å°è¯•å¦ä¸€ä¸ªæ¨¡å‹ï¼Œçœ‹çœ‹æˆ‘ä»¬æ˜¯å¦èƒ½å‡å°‘ RMSEã€‚**

**æ¨¡å¼äºŒ:å…ˆçŸ¥**

Prophet æ˜¯è„¸ä¹¦çš„å¼€æºå·¥å…·ã€‚æ­¤è¿‡ç¨‹ç”¨äºé¢„æµ‹åŸºäºåŠ æ³•æ¨¡å‹çš„æ—¶é—´åºåˆ—æ•°æ®ï¼Œåœ¨åŠ æ³•æ¨¡å‹ä¸­ï¼Œéçº¿æ€§è¶‹åŠ¿ä¸æ¯å¹´ã€æ¯å‘¨å’Œæ¯å¤©çš„å­£èŠ‚æ€§ä»¥åŠå‡æ—¥å½±å“ç›¸é€‚åº”ã€‚

**åŸºçº¿æ¨¡å‹**

æˆ‘ä»¬çš„åŸºçº¿(åˆå§‹)æ¨¡å‹å°†ä½¿ç”¨é»˜è®¤å‚æ•°ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ã€‚

```
# Creating a train dataset
train_prophet = train.copy()
train_prophet.reset_index(level=0, inplace=True)# Converting col names to specific names as required by Prophet library
train_prophet = train_prophet.rename(columns = {'Date': 'ds',
                                'Sales': 'y'})# Downsampling to week because modelling on daily basis takes a lot of time
ts_week_prophet = train_prophet.set_index("ds").resample("W").sum()
```

![](img/3a43d1ed4d7e9040b875deb166927c7f.png)

**æ‹Ÿåˆå…ˆçŸ¥æ¨¡å‹**

```
# Fitting data to Prophet model
prophet_1 = Prophet() 
prophet_1.fit(ts_week_prophet_train)
```

**æ¨¡å‹é¢„æµ‹**

```
future_1 = prophet_1.make_future_dataframe(periods = 52, freq = "W") 
forecast_1 = prophet_1.predict(future)
forecast_1[["ds", "yhat", "yhat_lower", "yhat_upper"]].tail()
```

![](img/f5ad466c80833cdce18d89d0399071c2.png)

```
# Visualizing predicions of forecast
prophet.plot(forecast_1);
```

![](img/072d375f3a9c9f3c98b8f92c46c5fcf4.png)

ä»ä¸Šé¢çš„å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œé¢„æµ‹æ˜¯ç›¸å½“ä¸é”™çš„ï¼Œä½†è®©æˆ‘ä»¬çœ‹çœ‹ RMSEï¼Œä»¥è·å¾—æ›´å¥½çš„æƒ³æ³•ã€‚

```
# Checking the RMSE of Prophet model
metric_prophet_1 = forecast_1.set_index('ds')[['yhat']].join(ts_week_prophet_train.set_index('ds').y).reset_index()
metric_prophet_1.dropna(inplace=True)
rms_prophet_1 = mean_squared_error(metric_prophet_1.y, metric_prophet_1.yhat)
rms_prophet_1
```

![](img/421a861f5904f39d1777305d16a81f2c.png)

å“‡ï¼åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒRMSE å¤ªå¤§äº†ï¼Œæˆ‘ä»¬éœ€è¦åšç‚¹ä»€ä¹ˆã€‚è®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦å¯ä»¥é€šè¿‡æ“çºµä¸€äº›å‚æ•°æ¥é™ä½å®ƒã€‚

## **å…ˆçŸ¥çš„è¶…å‚æ•°è°ƒæ•´**

åœ¨ Prophet æ¨¡å‹ä¸­æœ‰è®¸å¤šå¯ç”¨çš„å‚æ•°ã€‚å…¶ä¸­æœ€é‡è¦çš„æ˜¯**ã€å‡æ—¥ã€‘**ã€‚è¿™è®©æˆ‘ä»¬å¯ä»¥åœ¨è®­ç»ƒæ¨¡å‹æ—¶æ˜¾å¼è§£æå‡æ—¥ã€‚æˆ‘ä»¬å°†é€šè¿‡åœ¨å­¦æ ¡æˆ–å·æ”¾å‡æ—¶è¿›è¡Œè§‚å¯Ÿæ¥åˆ›å»ºä¸€ä¸ªæ–°çš„â€œå‡æœŸâ€æ•°æ®æ¡†ã€‚

æˆ‘ä»¬è¿˜å°†ä½¿ç”¨å¦å¤–ä¸‰ä¸ªå‚æ•°ï¼Œå³ã€‚

*   **interval_width** :å®šä¹‰è¿›è¡Œé¢„æµ‹çš„ä¸ç¡®å®šç¨‹åº¦ã€‚é»˜è®¤å€¼ä¸º 0.8ï¼Œä½†æˆ‘ä»¬å°†é‡‡ç”¨ 0.95ï¼Œå› ä¸ºæˆ‘ä»¬å¸Œæœ›åœ¨é¢„æµ‹ä¸­æ›´åŠ ç¡®å®šã€‚
*   **å¢é•¿**:æˆ‘ä»¬çŸ¥é“â€˜é”€å”®â€™å¯ä»¥å–ä»»ä½•å€¼ï¼Œä¸å­˜åœ¨é¥±å’Œç‚¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†é‡‡ç”¨â€œçº¿æ€§â€å¢é•¿ï¼Œè€Œä¸æ˜¯â€œå¯¹æ•°â€å¢é•¿ã€‚
*   **yearly _ seasonity**:æˆ‘ä»¬å°†æ˜ç¡®åœ°æŠŠå®ƒä½œä¸ºâ€˜Trueâ€™ä¼ é€’ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“åœ¨æˆ‘ä»¬çš„æ•°æ®ä¸­å­˜åœ¨ä¸€ä¸ªå¹´åº¦å­£èŠ‚æ€§(å¦‚ä¸Šæ‰€è¿°)ã€‚

```
# Encoding state holiday categorical variable
train_prophet["StateHoliday_cat"] = train_prophet["StateHoliday"].map({0:0, "0": 0, "a": 1, "b": 1, "c": 1})# Choosing only required cols
train_prophet = train_prophet[['ds', 'y', 'SchoolHoliday', 'StateHoliday_cat']]
train_prophet.head()
```

![](img/460b5fafbadf30e9a8cdc026369bc7a0.png)

```
# Modelling holidays - creating holidays dataframe
state_dates = train_prophet[(train_prophet.StateHoliday_cat == 1)].loc[:, "ds"].values
school_dates = train_prophet[(train_prophet.SchoolHoliday == 1)].loc[:, "ds"].valuesstate = pd.DataFrame({"holiday": "state_holiday",  "ds": pd.to_datetime(state_dates)})
school = pd.DataFrame({"holiday": "school_holiday", "ds": pd.to_datetime(school_dates)})holidays = pd.concat((state, school))
holidays.head()# Dropping holiday columns because not needed any more
train_prophet_clean = train_prophet.drop(["SchoolHoliday", "StateHoliday_cat"], axis = 1)# Downsampling to week because modelling on daily basis takes a lot of time
ts_week_prophet = train_prophet_clean.set_index("ds").resample("W").sum()# Resetting the index
ts_week_prophet_train = ts_week_prophet.reset_index() 
```

![](img/7ea1bdaf6b75591ccc85ae0a6e3dfcad.png)

**æ‹Ÿåˆè¶…è°ƒå…ˆçŸ¥æ¨¡å‹**

```
# Fitting data to Prophet model
prophet_2 = Prophet(holidays = holidays, interval_width = 0.95, growth='linear', yearly_seasonality = True) 
prophet_2.fit(ts_week_prophet_train)
print("done")
```

**æ¨¡å‹é¢„æµ‹**

```
future_2 = prophet_2.make_future_dataframe(periods = 52, freq = "W") 
forecast_2 = prophet_2.predict(future)
forecast_2[["ds", "yhat", "yhat_lower", "yhat_upper"]].tail() # We have a new dataframe, which includes, the forecast and the uncertainity invervals.
```

![](img/1be9c13ee8c0f03ad17f723787a6bd12.png)

```
# Visualizing predicions of forecast
prophet.plot(forecast_2);
```

![](img/7e496a0b33c9023f07fcb4cc8c9948a8.png)

```
# Visualizing trend and seasonality components
prophet.plot_components(forecast_2);
```

![](img/e825f818d53e15b9c5811fd23482e4d0.png)

ç¬¬ä¸€å¹…å›¾æ˜¾ç¤ºæ¯å‘¨çš„æ€»é”€å”®é¢åœ¨å¢åŠ ã€‚ç¬¬äºŒä¸ªå›¾æ˜¾ç¤ºäº†æ•°æ®é›†ä¸­çš„å‡æœŸå·®è·ï¼Œç¬¬ä¸‰ä¸ªå›¾æ˜¾ç¤ºäº†å•†åº—åœ¨ 12 æœˆæœ€åä¸€å‘¨çš„é”€å”®é¢éå¸¸é«˜(å› ä¸ºåœ£è¯èŠ‚å‡æœŸ)ã€‚

```
# Checking the RMSE of Prophet model
metric_prophet_2 = forecast_2.set_index('ds')[['yhat']].join(ts_week_prophet_train.set_index('ds').y).reset_index()
metric_prophet_2.dropna(inplace=True)
rms_prophet_2 = mean_squared_error(metric_prophet_2.y, metric_prophet_2.yhat)
rms_prophet_2
```

![](img/cae45ec541709cd53083335b8185c711.png)

æˆ‘ä»¬çš„åŸºçº¿ Prophet æ¨¡å‹ä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œæˆ‘ä»¬å¾—åˆ°çš„ RMSE ä¸º 53782649094881.14ï¼Œç»è¿‡è¶…è°ƒåï¼Œæˆ‘ä»¬å¾—åˆ°çš„ RMSE ä¸º 52478331938232.15ã€‚è™½ç„¶æœ€ç»ˆçš„æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œä½†ä¸ ARIMA ç›¸æ¯”ï¼Œå®ƒä»ç„¶è¡¨ç°ä¸ä½³ã€‚é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬è¯•è¯•å¦ä¸€ç§æ¨¡å¼ã€‚

**æ¨¡å‹ 3: XGBoost**

XGBoost æ˜¯ä¸€ä¸ªä¼˜åŒ–çš„åˆ†å¸ƒå¼æ¢¯åº¦å¢å¼ºåº“ï¼Œè®¾è®¡ä¸ºé«˜åº¦**é«˜æ•ˆ**ã€**çµæ´»**å’Œ**ä¾¿æº**ã€‚è™½ç„¶å®ƒä¸æ˜¯ä¸“é—¨ä¸ºæ—¶é—´åºåˆ—æ•°æ®è®¾è®¡çš„ï¼Œä½†ä¼—æ‰€å‘¨çŸ¥ï¼Œå®ƒåœ¨å„ç§å›å½’é—®é¢˜ä¸Šéƒ½è¡¨ç°å¾—éå¸¸å¥½ã€‚

```
# Dropping Customers and Sale per customer
ts_xgboost = train_store_joined.copy()
ts_xgboost = ts_xgboost.drop(['Customers', 'SalePerCustomer', 'PromoInterval'], axis=1)# Combining similar columns into one column and dropping old columns
ts_xgboost['CompetitionOpen'] = 12 * (ts_xgboost.Year - ts_xgboost.CompetitionOpenSinceYear) + (ts_xgboost.Month - ts_xgboost.CompetitionOpenSinceMonth)
ts_xgboost['PromoOpen'] = 12 * (ts_xgboost.Year - ts_xgboost.Promo2SinceYear) + (ts_xgboost.WeekOfYear - ts_xgboost.Promo2SinceWeek) / 4.0
ts_xgboost = ts_xgboost.drop(["CompetitionOpenSinceMonth", "CompetitionOpenSinceYear"], axis = 1)
ts_xgboost = ts_xgboost.drop(["Promo2SinceWeek", "Promo2SinceYear"], axis = 1)# Converting categorical cols to numerical cols and removing old cols
mappings = {0:0, "0": 0, "a": 1, "b": 1, "c": 1}
ts_xgboost["StateHoliday_cat"] = ts_xgboost["StateHoliday"].map(mappings)
ts_xgboost["StoreType_cat"] = ts_xgboost["StoreType"].map(mappings)
ts_xgboost["Assortment_cat"] = ts_xgboost["Assortment"].map(mappings)
ts_xgboost = ts_xgboost.drop(["StateHoliday", "StoreType", "Assortment"], axis = 1)# Splitting the data
features = ts_xgboost.drop(["Sales"], axis = 1)
target = ts_xgboost["Sales"]X_train, X_test, y_train, y_test = model_selection.train_test_split(features, target, test_size = 0.20)
```

**åŸºçº¿æ¨¡å‹**

æˆ‘ä»¬çš„åŸºçº¿(åˆå§‹)æ¨¡å‹å°†ä½¿ç”¨é»˜è®¤å‚æ•°ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ã€‚

```
# Tuning parameters - using default metrics
params = {'max_depth':6, "booster": "gbtree", 'eta':0.3, 'objective':'reg:linear'}dtrain = xgb.DMatrix(X_train, y_train)
dtest = xgb.DMatrix(X_test, y_test)
watchlist = [(dtrain, 'train'), (dtest, 'eval')]# Training the model
xgboost = xgb.train(params, dtrain, 100, evals=watchlist,early_stopping_rounds= 100, verbose_eval=True)

# Making predictions
preds = xgboost.predict(dtest)
```

![](img/98173df498aa5af19609d25a6e23636c.png)

```
# RMSE of model
rms_xgboost = sqrt(mean_squared_error(y_test, preds))
print("Root Mean Squared Error for XGBoost:", rms_xgboost)
```

![](img/4791800c82ddc523d5ed02574cbd91c0.png)

è‡³å°‘ä¸ Prophet ç›¸æ¯”ï¼Œå®ƒçš„è¡¨ç°ç›¸å½“ä¸é”™ã€‚è®©æˆ‘ä»¬çœ‹çœ‹èƒ½å¦è¿›ä¸€æ­¥é™ä½ RMSEã€‚

## XGBoost çš„è¶…è°ƒ

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°è¯•é€šè¿‡ä¸º XGBoost æ¨¡å‹ä¸­çš„è¶…å‚æ•°ä¼ é€’ä¸åŒçš„å€¼æ¥é™ä½ XGBoost çš„ RMSEã€‚

*   **eta** :å®šä¹‰å­¦ä¹ ç‡ï¼Œå³æ¢¯åº¦ä¸‹é™å»ºæ¨¡ä¸­å­¦ä¹ æ•°æ®çš„æ­¥é•¿(XGBoost çš„åŸºç¡€)ã€‚é»˜è®¤å€¼ä¸º 0.3ï¼Œä½†æˆ‘ä»¬å¸Œæœ›ä¿æŒè¾ƒä½çš„å­¦ä¹ é€Ÿç‡ï¼Œä»¥é¿å…è¿‡åº¦æ‹Ÿåˆã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬é€‰æ‹© 0.2 ä½œä¸º etaã€‚
*   **max_depth** :ä¸€æ£µæ ‘çš„æœ€å¤§æ·±åº¦ã€‚é»˜è®¤å€¼æ˜¯ 6ï¼Œä½†æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„æ¨¡å‹æ›´å¤æ‚ï¼Œå¹¶æ‰¾åˆ°æ›´å¥½çš„é¢„æµ‹ã€‚æ‰€ä»¥ï¼Œè®©æˆ‘ä»¬é€‰æ‹© 10 ä½œä¸ºæœ€å¤§æ·±åº¦ã€‚
*   **gamma** :åœ¨æ ‘çš„å¶å­èŠ‚ç‚¹ä¸Šåšè¿›ä¸€æ­¥åˆ’åˆ†æ‰€éœ€çš„æœ€å°æŸå¤±å‡å°‘ã€‚ä¼½ç›è¶Šå¤§ï¼Œç®—æ³•å°±è¶Šä¿å®ˆã€‚é»˜è®¤å€¼æ˜¯ 0ï¼Œè®©æˆ‘ä»¬é€‰æ‹©ä¸€ä¸ªç¨å¾®é«˜ä¸€ç‚¹çš„å€¼ï¼Œä»¥ä¾¿å¾—åˆ°å¥½çš„é¢„æµ‹ã€‚

```
# Tuning parameters
params_2 = {'max_depth':10, 'eta':0.1,  'gamma': 2}dtrain = xgb.DMatrix(X_train, y_train)
dtest = xgb.DMatrix(X_test, y_test)
watchlist = [(dtrain, 'train'), (dtest, 'eval')]# Training the model
xgboost_2 = xgb.train(params_2, dtrain, 100, evals=watchlist,early_stopping_rounds= 100, verbose_eval=True)

# Making predictions
preds_2 = xgboost_2.predict(dtest)
```

![](img/a9fa7601899f5ac4bc2e6c5fa72198d3.png)

```
# RMSE of model
rms_xgboost_2 = sqrt(mean_squared_error(y_test, preds_2))
print("Root Mean Squared Error for XGBoost:", rms_xgboost_2)
```

![](img/26b01cb4ddc675294f923952fe744198.png)

è¶…è°ƒåï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬çš„æ¨¡å‹çš„ RMSE ä¸‹é™ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹ä»€ä¹ˆç‰¹å¾ä¼šå½±å“å•†åº—çš„é”€å”®ã€‚

```
# Let's see the feature importance
fig, ax = plt.subplots(figsize=(10,10))
xgb.plot_importance(xgboost_2, max_num_features=50, height=0.8, ax=ax)
plt.show()
```

![](img/62fbdce9fcff2f4b39bedfcbcea2171c.png)

ä¸å‡ºæ‰€æ–™ï¼Œå½±å“å•†åº—é”€å”®çš„ä¸»è¦åŸå› æœ‰äº”ä¸ªï¼Œå³ã€‚**å•†åº—æ•°é‡ã€æ¯”èµ›è·ç¦»ã€æ˜ŸæœŸå‡ ã€æ¯”èµ›å¼€å§‹å’Œä¿ƒé”€**ã€‚

æˆ‘ä»¬æœ€ç»ˆçš„ XGBoost æ¨¡å‹åœ¨è¶…è°ƒåæ˜¯ä¸€ä¸ªå…·æœ‰**â€œmax _ depthâ€:10ï¼Œâ€œetaâ€:0.1ï¼Œâ€œgammaâ€:2 å’Œ 1191.90 çš„ RMSE åˆ†æ•°**çš„æ¨¡å‹ï¼Œéå¸¸æ£’ï¼ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ¯”è¾ƒä¸€ä¸‹æ‰€æœ‰å‹å·çš„æ€§èƒ½

**3ã€‚ç»“æœ**

æˆ‘ä»¬ä½¿ç”¨å‡æ–¹æ ¹è¯¯å·®(RMSE)æ¥è¯„ä¼°å’ŒéªŒè¯å„ç§æ¨¡å‹çš„æ€§èƒ½ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å“ªä¸ªæ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œä¸ºä»€ä¹ˆ/ä¸ºä»€ä¹ˆä¸ã€‚

```
# Comparing performance of above three models - through RMSE
rms_arima = format(float(rms_arima))
rms_prophet_2 = format(float(rms_prophet_2))
rms_xgboost_2 = format(float(rms_xgboost_2))model_errors = pd.DataFrame({
    "Model": ["SARIMA", "Prophet", "XGBoost"],
    "RMSE": [rms_arima, rms_prophet_2, rms_xgboost_2]
})model_errors.sort_values(by = "RMSE")
```

![](img/c3726c7fd6f7cd1e0825b4638f8120c2.png)

**4ã€‚å‹å·å¯¹æ¯”&é€‰æ‹©**

a)ä»ä¸Šè¡¨æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼ŒSARIMA è¡¨ç°æœ€å¥½ï¼Œå…¶æ¬¡æ˜¯ XGBoost å’Œ Prophetã€‚

b)è¿™æ˜¯æœ‰æ„ä¹‰çš„ï¼Œå› ä¸º SARIMA æ˜¯ä¸“é—¨ä¸ºå­£èŠ‚æ€§æ—¶é—´åºåˆ—æ•°æ®è®¾è®¡çš„ï¼Œè€Œ XGBoost æ˜¯ä¸€ç§é€šç”¨çš„(å°½ç®¡åŠŸèƒ½å¼ºå¤§)æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå…·æœ‰å„ç§åº”ç”¨ã€‚

c) Prophet æ˜¯åˆ¶ä½œå¿«é€Ÿé¢„æµ‹çš„å¥½é€‰æ‹©ï¼Œå› ä¸ºå®ƒä¸éœ€è¦å¾ˆå¼ºçš„æŠ€æœ¯æŠ€èƒ½ã€‚è¿™å¾ˆå®¹æ˜“å¤§è§„æ¨¡å®æ–½ã€‚å®ƒåœ¨è¿™é‡Œè¡¨ç°ä¸ä½³çš„åŸå› å¯èƒ½æ˜¯å› ä¸ºç¼ºä¹æ•°æ®ã€‚å®ƒæœ€é€‚ç”¨äºå…·æœ‰å¼ºçƒˆå­£èŠ‚æ•ˆåº”çš„æ—¶é—´åºåˆ—å’Œå‡ ä¸ªå­£èŠ‚çš„å†å²æ•°æ®ã€‚Prophet å¯¹ç¼ºå¤±æ•°æ®å’Œè¶‹åŠ¿å˜åŒ–éå¸¸ç¨³å¥ï¼Œé€šå¸¸èƒ½å¤Ÿå¾ˆå¥½åœ°å¤„ç†å¼‚å¸¸å€¼ã€‚

åŸºäºä¸Šè¿°åˆ†æï¼Œæˆ‘ä»¬å°†é€‰æ‹© ARIMA ä½œä¸ºæˆ‘ä»¬é¢„æµ‹é”€å”®çš„æœ€ç»ˆæ¨¡å‹ï¼Œå› ä¸ºå®ƒç»™æˆ‘ä»¬çš„ RMSE æœ€å°ï¼Œå¹¶ä¸”éå¸¸é€‚åˆæˆ‘ä»¬é¢„æµ‹æ—¶é—´åºåˆ—å­£èŠ‚æ€§æ•°æ®çš„éœ€è¦ã€‚æˆ‘ä»¬é€‰æ‹© **ARIMA(1ï¼Œ1ï¼Œ1)x(0ï¼Œ1ï¼Œ1ï¼Œ12)12** ä¸**AIC 1806.29 å’Œ RMSE 739.06 ä½œä¸ºæœ€ç»ˆå‚æ•°ç»„åˆã€‚**

**5ã€‚ç»“è®º**

**å€’å½±**

*   æ•°æ®ä¸­æœ€æœ‰è¶£çš„æ˜¯ï¼Œé”€å”®é¢æœ€é«˜çš„å•†åº—ç±»åˆ«å¹¶æ²¡æœ‰æœ€é«˜çš„æ¯ä½é¡¾å®¢é”€å”®é¢ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºè¿™äº›å•†åº—å‡ºå”®æ—¥å¸¸æ‰€éœ€çš„å°å•†å“ã€‚
*   å¦ä¸€ä»¶æœ‰è¶£çš„äº‹æƒ…æ˜¯ï¼Œç¬¬äºŒæ¬¡è¿›è¡Œä¿ƒé”€å¯¹å¢åŠ é”€å”®é¢æ²¡æœ‰å¸®åŠ©ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºé¡¾å®¢åœ¨ç¬¬ä¸€æ¬¡ä¿ƒé”€æ´»åŠ¨ä¸­å·²ç»è´­ä¹°äº†ä»–ä»¬æƒ³è¦çš„ä¸œè¥¿ã€‚

**è‡´è°¢**

[https://machine learning mastery . com/ARIMA-for-time-series-forecasting-with-python/](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/)

[https://www . digital ocean . com/community/tutorials/a-guide-to-time-series-forecasting-with-ARIMA-in-python-3](https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-forecasting-with-arima-in-python-3)

[https://xgboost . readthedocs . io/en/latest/python/python _ intro . html](https://xgboost.readthedocs.io/en/latest/python/python_intro.html)

[https://facebook.github.io/prophet/docs/quick_start.html](https://facebook.github.io/prophet/docs/quick_start.html)

æ›´å¤šè¯¦æƒ…ï¼Œè¯·æŸ¥çœ‹ Github ä¸Šçš„ [**æºä»£ç ã€‚**](https://github.com/bisman16/Kaggle_Rossmann_Store_Sales_Forecasting)

å¦‚æœä½ å–œæ¬¢é˜…è¯»è¿™ä¸ªæ•…äº‹ï¼Œè¯·ç‚¹å‡»ğŸ‘æŒ‰é’®å¹¶åˆ†äº«å‡ºæ¥ï¼Œå¸®åŠ©å…¶ä»–äººå­¦ä¹ æœºå™¨å­¦ä¹ ã€‚æˆ‘å¾ˆæƒ³å¬å¬ä½ çš„æƒ³æ³•ï¼Œæ¬¢è¿åœ¨ä¸‹é¢ç•™ä¸‹ä½ çš„è¯„è®ºã€‚