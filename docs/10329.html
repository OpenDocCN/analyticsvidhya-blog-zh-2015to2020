<html>
<head>
<title>OOP + MachineLearning = Powerful</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OOP +机器学习=强大</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/oop-machinelearning-powerful-a9b936a8db48?source=collection_archive---------9-----------------------#2020-10-14">https://medium.com/analytics-vidhya/oop-machinelearning-powerful-a9b936a8db48?source=collection_archive---------9-----------------------#2020-10-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7d83bbd18660306b4962263c3239b924.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*otRVGX9lsiMPU9CsTqQgjQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://javatutorial.net/java-oop" rel="noopener ugc nofollow" target="_blank">图像</a></figcaption></figure><p id="083f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">成为一名数据科学家并不容易，有时还会让人精疲力尽。这个领域有如此多的方面，对它们中的每一个都进行标记可能是乏味的。对于刚开始接触数据科学、python 编程或机器学习概念的人，尤其是那些没有编程背景的人，事情可能会困难得多。</p><p id="8641" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我开始的时候，甚至现在在某种程度上，我还在为 OOP(面向对象编程)概念而挣扎。它的有用性和有效性让我很想学习它，但我总是想要一些有趣的例子来掌握一个概念。这正是我在这里的意图。我不得不尝试用一个简单的线性回归的例子来演示 OOP 的核心概念。所以让我们开始吧:</p><p id="0d68" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们先试着得到一个数据集和一些预测:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="03c5" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">from </strong>numpy <strong class="jy hj">import </strong>array<br/><strong class="jy hj">from </strong>numpy.linalg <strong class="jy hj">import </strong>inv<br/><strong class="jy hj">import </strong>matplotlib.pyplot <strong class="jy hj">as </strong>plt<br/>data = array([<br/>   [0.05, 0.12],<br/>   [0.18, 0.22],<br/>   [0.31, 0.35],<br/>   [0.42, 0.38],<br/>   [0.5, 0.49],<br/>   ])<br/>#separate out X and y and reshape<br/>X = data[:,0]<br/>y = data[:,-1]<br/>X = X.reshape(-1,1)<br/>y = y.reshape(-1,1)</span><span id="b0ee" class="kc kd hi jy b fi ki kf l kg kh">#let's try to calculate coef using linear algebra, to predict the y #which is = coef*X (not including the intercept at this moment)</span><span id="a64f" class="kc kd hi jy b fi ki kf l kg kh">coef_ = inv(X.T.dot(X)).dot(X.T).dot(y)<br/>yhat = X.dot(coef_)</span><span id="e0bc" class="kc kd hi jy b fi ki kf l kg kh">#Finally let's plot the data</span><span id="51ba" class="kc kd hi jy b fi ki kf l kg kh">plt.scatter(X, y)<br/>plt.plot(X, yhat, color=<strong class="jy hj">'red'</strong>)<br/>plt.xlabel(<strong class="jy hj">'X'</strong>)<br/>plt.ylabel(<strong class="jy hj">'Y'</strong>)<br/>plt.show()</span></pre><p id="0ccb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上面的代码导致了下面的情节:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kj"><img src="../Images/4281e94a7e094b981d3eaafc4ca8da0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Oxx4gbyYWxPuyyOnOg9xQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">其中散点图表示实际数据，直线表示预测值</figcaption></figure><p id="4b5a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">添加这个简单的线性回归实现的全部意义在于，现在我将通过上面的线性回归实现来探索 OOP 概念。</p><p id="a812" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们从创建我们的线性回归类开始，我们将从头开始编写:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="a7ec" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">import </strong>numpy <strong class="jy hj">as </strong>np<br/><strong class="jy hj">from </strong>numpy <strong class="jy hj">import </strong>array<br/><strong class="jy hj">from </strong>numpy.linalg <strong class="jy hj">import </strong>inv<br/><strong class="jy hj">import </strong>matplotlib.pyplot <strong class="jy hj">as </strong>plt</span><span id="1372" class="kc kd hi jy b fi ki kf l kg kh"><strong class="jy hj">class </strong>LinearRegression():<br/>    <strong class="jy hj">def </strong>__init__(self):<br/>       '''initializes the variables coef and pred'''<br/>        self.coef = <strong class="jy hj">None<br/>        </strong>self.pred = <strong class="jy hj">None<br/><br/>    def </strong>fit(self,X,y):<br/>        '''calculate the coef ''<br/>        self.X = X<br/>        self.y = y<br/>        <strong class="jy hj">if </strong>len((self.X).shape) == 1:<br/>            self.X = (self.X).reshape(-1, 1)<br/><br/>       self.coef=inv(self.X.T.dot(self.X)).dot(self.X.T).dot(self.y)<br/><br/>    <strong class="jy hj">def </strong>predict(self):<br/>        '''predict the y values using the coef calculated above'''<br/>        <strong class="jy hj">if </strong>len((self.X).shape) == 1:<br/>            self.X = (self.X).reshape(-1, 1)<br/><br/>        self.pred=  self.X.dot(self.coef)<br/>        <strong class="jy hj">return </strong>self.pred<br/><br/>    <strong class="jy hj">def </strong>plt_prediction(self):<br/>        '''generates some plot'''<br/>        plt.scatter(self.X, self.y)<br/>        plt.plot(self.X, self.pred, color  = <strong class="jy hj">"red"</strong>)<br/>        plt.show()</span></pre><p id="5349" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> __init__ </strong>:默认的构造函数，当我们试图创建 LinearRegression 类的实例时，这个函数就会被调用。在这种情况下，它将初始化两个占位符:<strong class="ix hj"> coef </strong>和<strong class="ix hj"> pred </strong>，稍后当我们调用 fit 方法和 predict 方法时，它们将具有值。</p><p id="dc54" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> fit(X，y) </strong>:这是一个使用 X 和 y 值计算<strong class="ix hj"> coef </strong>的实际工作的方法。</p><p id="b1b3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> predict() </strong>:该方法在被调用时，预测值，并将它们存储在之前初始化的<strong class="ix hj"> pred </strong>变量中。</p><p id="4af3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> plt_predict() </strong>:最后，该方法生成如上图所示的相同图形。</p><p id="cde5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在有趣的部分，让我们做一个例子，看看神奇之处:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="ec4a" class="kc kd hi jy b fi ke kf l kg kh">mylinearreg = LinearRegression()<br/><br/>mylinearreg.fit(X,y)<br/>print(mylinearreg.predict())</span><span id="d414" class="kc kd hi jy b fi ki kf l kg kh"><strong class="jy hj">output</strong>:[0.05011661 0.18041981 0.310723   0.42097955 0.50116613]</span></pre><p id="8075" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，让我们创建一个基类，我的 LinearRegression 将从这个基类派生。我能把我的基类做成什么？？？一门叫做度量的课。？？我们确实需要评估我们的模型，对吗？？？？让我们这样做:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="2012" class="kc kd hi jy b fi ke kf l kg kh"><strong class="jy hj">class </strong>Metrics:<br/><br/>    <strong class="jy hj">def </strong>sse(self):<br/>        squared_errors = (self.y - self.pred) ** 2<br/>        self.sq_error_ = np.sum(squared_errors)<br/>        <strong class="jy hj">return </strong>self.sq_error_<br/><br/>    <strong class="jy hj">def </strong>sst(self):<br/>        <em class="kk">'''returns total sum of squared errors (actual vs avg(actual))'''<br/>        </em>avg_y = np.mean(self.y)<br/>        squared_errors = (self.y - avg_y) ** 2<br/>        self.sst_ = np.sum(squared_errors)<br/>        <strong class="jy hj">return </strong>self.sst_<br/><br/>    <strong class="jy hj">def </strong>r_squared(self):<br/>        <em class="kk">'''returns calculated value of r^2'''<br/>        </em>self.r_sq_ = 1 - self.sse() / self.sst()<br/>        <strong class="jy hj">return </strong>self.r_sq_<br/><br/><strong class="jy hj">class </strong>LinearRegression(Metrics):<br/>    <strong class="jy hj">def </strong>__init__(self):<br/>        self.coef = <strong class="jy hj">None<br/>        </strong>self.pred = <strong class="jy hj">None<br/><br/>    def </strong>fit(self,X,y):<br/>        self.X = X<br/>        self.y = y<br/>        <strong class="jy hj">if </strong>len((self.X).shape) == 1:<br/>            self.X = (self.X).reshape(-1, 1)<br/><br/>        self.coef = inv(self.X.T.dot(self.X)).dot(self.X.T).dot(self.y)<br/><br/>    <strong class="jy hj">def </strong>predict(self):<br/>        <strong class="jy hj">if </strong>len((self.X).shape) == 1:<br/>            self.X = (self.X).reshape(-1, 1)<br/><br/>        self.pred=  self.X.dot(self.coef)<br/>        <strong class="jy hj">return </strong>self.pred<br/><br/>    <strong class="jy hj">def </strong>plt_prediction(self):<br/>        plt.scatter(self.X, self.y)<br/>        plt.plot(self.X, self.pred, color  = <strong class="jy hj">"red"</strong>)<br/>        plt.show()</span></pre><p id="505a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里有几点需要注意:</p><ol class=""><li id="8ce5" class="kl km hi ix b iy iz jc jd jg kn jk ko jo kp js kq kr ks kt bi translated">我的基类(“Metrics”)没有 __init__ 方法，因为当我创建 LinearRegression 的实例时，它将自动获取基类(“Metrics”)中定义的所有方法，并且基类现在将使用派生类(“LinearRegression”)的 __init__ 方法。</li></ol><p id="2907" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.当我调用基类中的方法时，它会自动从派生类中获取变量。</p><p id="f707" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们现在打几个电话:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="f3f1" class="kc kd hi jy b fi ke kf l kg kh">mylinearreg = LinearRegression()<br/><br/>mylinearreg.fit(X,y)<br/>print(mylinearreg.predict())<br/>print(<strong class="jy hj">"The sse is: " </strong>, mylinearreg.r_squared())</span><span id="82c7" class="kc kd hi jy b fi ki kf l kg kh"><strong class="jy hj">output:</strong> The sse is:  0.8820779000238227</span></pre><p id="ba4c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这不是很有趣吗？现在，我们可以在 metrics 类中添加我们的定制度量，并通过创建单独的类并使它们都从基类继承来将它们用于其他模型，而不仅仅是线性回归。:)</p><p id="1fb2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请随意使用更大的数据集和更复杂的方法进行尝试，例如添加一个处理梯度下降的方法。</p><p id="26a1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，感谢你阅读这篇文章，如果你喜欢它，请留下评论或反馈。:)</p><p id="45b4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="kk">参考文献</em>:</p><div class="ku kv ez fb kw kx"><a href="https://dziganto.github.io/classes/data%20science/linear%20regression/machine%20learning/object-oriented%20programming/python/Understanding-Object-Oriented-Programming-Through-Machine-Learning/" rel="noopener  ugc nofollow" target="_blank"><div class="ky ab dw"><div class="kz ab la cl cj lb"><h2 class="bd hj fi z dy lc ea eb ld ed ef hh bi translated">通过机器学习理解面向对象编程</h2><div class="le l"><h3 class="bd b fi z dy lc ea eb ld ed ef dx translated">面向对象编程(OOP)并不容易理解。你可以一个接一个的看教程，筛选…</h3></div><div class="lf l"><p class="bd b fp z dy lc ea eb ld ed ef dx translated">dziganto.github.io</p></div></div><div class="lg l"><div class="lh l li lj lk lg ll io kx"/></div></div></a></div><p id="d21e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://machinelearningmastery.com/solve-linear-regression-using-linear-algebra/" rel="noopener ugc nofollow" target="_blank"><em class="kk">https://machine learning mastery . com/solve-linear-regression-using-linear-代数/ </em> </a></p></div></div>    
</body>
</html>