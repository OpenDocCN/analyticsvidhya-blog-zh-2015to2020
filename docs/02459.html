<html>
<head>
<title>Decoding the Best Machine Learning Papers from NeurIPS 2019</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解码NeurIPS 2019最佳机器学习论文</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decoding-the-best-machine-learning-papers-from-neurips-2019-7d2effdcc5ac?source=collection_archive---------16-----------------------#2019-12-17">https://medium.com/analytics-vidhya/decoding-the-best-machine-learning-papers-from-neurips-2019-7d2effdcc5ac?source=collection_archive---------16-----------------------#2019-12-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="3740" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="3f19" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">NeurIPS是世界上最重要的机器学习会议。没有其他的研究会议能在一个地方吸引超过6000人——这是真正的精英会议。如果你想沉浸在最新的机器学习研究进展中，你需要关注NeurIPS。</p><p id="63ba" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><a class="ae kg" href="https://nips.cc/" rel="noopener ugc nofollow" target="_blank"> NeurIPS 2019 </a>是第33届会议，于12月8日至14日在加拿大温哥华举行。我每年都会虔诚地关注这次会议，今年也不例外。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es kh"><img src="../Images/7d480e9e415dad5a2429758b0b637251.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*v_hNsoMlU6BJ9Vc4.jpg"/></div></div></figure><p id="4186" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">每年，NeurIPS都会为机器学习领域的顶级研究论文宣布一个奖项类别。鉴于这些论文的高级水平，大多数人可能很难理解。</p><p id="04d1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">但是不用担心！我把这些牛逼的论文都翻了一遍，总结了这篇文章的重点！我的目的是通过为我们的社区将关键的机器学习概念分解成易于理解的比特来帮助你理解每篇论文的本质。</p><p id="f659" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">以下是我将介绍的三个NeurIPS 2019最佳论文类别:</p><ul class=""><li id="7159" class="kt ku hi jf b jg kb jk kc jo kv js kw jw kx ka ky kz la lb bi translated">最佳论文奖</li><li id="1697" class="kt ku hi jf b jg lc jk ld jo le js lf jw lg ka ky kz la lb bi translated">杰出方向论文奖</li><li id="c09e" class="kt ku hi jf b jg lc jk ld jo le js lf jw lg ka ky kz la lb bi translated">测试时间论文奖</li></ul><p id="aaad" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们开始吧！</p><h1 id="5bbc" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">NeurIPS 2019最佳论文奖</h1><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lh"><img src="../Images/cd52f6115ecf04549ab6f54265c9b078.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/0*U2OFBbdOqq5KLj2B.jpg"/></div></figure><p id="96b2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">NeurIPS 2019最佳论文奖将授予:</p><h2 id="a54e" class="li ig hi bd ih lj lk ll il lm ln lo ip jo lp lq it js lr ls ix jw lt lu jb lv bi translated">具有Massart噪声的半空间的分布无关PAC学习</h2><p id="b2dc" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这真是一篇伟大的论文！这让我想到了机器学习中的一个基本概念:<strong class="jf hj">噪声和分布</strong>。这也需要对论文本身进行大量的研究，我会尽量解释论文的主旨，而不会使其变得复杂。</p><p id="89c3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">先把标题改一下。本文研究的是一种在带<strong class="jf hj"> Massart噪声的<strong class="jf hj">分布独立PAC模型</strong>中学习<strong class="jf hj">半空间</strong>的算法。该算法是该领域迄今为止最有效的算法。</strong></p><p id="2f38" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们理解这里的关键术语。</p><p id="3318" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">回想一下布尔函数和二进制分类的概念。基本上，</p><blockquote class="lw lx ly"><p id="dfd5" class="jd je lz jf b jg kb ji jj jk kc jm jn ma kd jq jr mb ke ju jv mc kf jy jz ka hb bi translated"><em class="hi">半空间是一个布尔函数，其中两个类(正样本和负样本)被一个超平面分开。由于超平面是线性的，所以又称为</em><strong class="jf hj"><em class="hi">【LTF】</em></strong><em class="hi">。</em></p></blockquote><p id="213d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">从数学上讲，线性阈值函数或半空间是一种阈值函数，它可以由某个阈值<em class="lz"> T </em>所限定的输入参数的线性方程来表示。如果布尔函数f(x)具有以下形式，则它是线性阈值函数:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es md"><img src="../Images/c3535c050fda8c7239f322bfcac12ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*Hj2yT_yaW0ks40TR.png"/></div></figure><p id="8d19" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">其中:</p><p id="365e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们也可以把LTFs称为感知器(在这里利用你的<a class="ae kg" href="https://www.analyticsvidhya.com/blog/2018/10/introduction-neural-networks-deep-learning/?utm_source=blog&amp;utm_medium=neurips-2019-best-papers" rel="noopener ugc nofollow" target="_blank">神经网络的</a>知识！).</p><p id="bdfd" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">PAC(可能近似正确)模型是二元分类的标准模型之一。</p><blockquote class="lw lx ly"><p id="6d5d" class="jd je lz jf b jg kb ji jj jk kc jm jn ma kd jq jr mb ke ju jv mc kf jy jz ka hb bi translated"><em class="hi"> Massart噪声条件，或者仅仅是Massart噪声，是当每个样本/记录的标签以学习算法未知的某个小概率翻转时。</em></p></blockquote><p id="5782" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">翻转的概率受到某个因子<em class="lz"> n </em>的限制，该因子总是小于1/2。由于主要目标是找到具有小的错误分类误差的假设，在以前的论文中已经进行了各种尝试来限制误差和与数据中的噪声相关联的风险。</p><p id="8218" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在这项研究中，虽然样本复杂性得到了很好的确定，但证明了多项式时间(1/ε)的超额风险等于Massart噪声水平加上ε。</p><p id="3e3d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这篇论文是朝着实现仅ε的超额风险迈出的一大步。你可以在这里阅读全文<a class="ae kg" href="https://papers.nips.cc/paper/8722-distribution-independent-pac-learning-of-halfspaces-with-massart-noise" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="7683" class="li ig hi bd ih lj lk ll il lm ln lo ip jo lp lq it js lr ls ix jw lt lu jb lv bi translated">NeurIPS对杰出论文奖的其他荣誉奖包括:</h2><ol class=""><li id="a84f" class="kt ku hi jf b jg jh jk jl jo me js mf jw mg ka mh kz la lb bi translated"><a class="ae kg" href="https://papers.nips.cc/paper/9109-nonparametric-density-estimation-convergence-rates-for-gans-under-besov-ipm-losses" rel="noopener ugc nofollow" target="_blank"> <em class="lz">非参数密度估计【Besov IPM损失下GANs的收敛速度</em> </a></li><li id="e376" class="kt ku hi jf b jg lc jk ld jo le js lf jw lg ka mh kz la lb bi translated"><a class="ae kg" href="https://papers.nips.cc/paper/9040-fast-and-accurate-least-mean-squares-solvers" rel="noopener ugc nofollow" target="_blank"> <em class="lz">快速准确的最小均方解算器</em> </a></li></ol><h1 id="3854" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">NeurIPS 2019上的杰出新方向论文</h1><p id="c336" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">NeurIPS 2019今年还有一个新的获奖论文类别，称为<strong class="jf hj">杰出新方向论文奖。</strong>用他们的话说:</p><blockquote class="lw lx ly"><p id="4c45" class="jd je lz jf b jg kb ji jj jk kc jm jn ma kd jq jr mb ke ju jv mc kf jy jz ka hb bi translated">该奖项旨在表彰为未来研究开辟新道路的杰出工作。</p></blockquote><p id="60d5" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">而这个奖项的获得者是——<strong class="jf hj">“一致收敛可能无法解释深度学习中的泛化”</strong>。</p><p id="734f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">今年我最喜欢的论文之一！本文继续从理论和经验证据两方面解释，当前的深度学习算法不能声称解释深度神经网络中的泛化。让我们更详细地理解这一点。</p><h2 id="661e" class="li ig hi bd ih lj lk ll il lm ln lo ip jo lp lq it js lr ls ix jw lt lu jb lv bi translated">一致收敛可能无法解释深度学习中的泛化</h2><p id="0e9b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">大型网络对看不见的训练数据进行很好的概括，尽管被训练成完全适合随机标记的数据。但是当特征的数量大于训练样本的数量时，这些网络应该不会工作得很好，对吗？</p><p id="37b7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">然而，它们仍然为我们提供了最先进的性能指标。这也表明，这些过度参数化的模型过度依赖于参数计数，并且没有考虑可变的批量大小。如果我们按照概括的基本等式:</p><p id="4b35" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">测试误差—训练误差&lt;= Generalisation bound</p><p id="82bb" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">For the above equation, we take the set of all hypotheses and attempt to minimize the complexity and keep these bounds as tight as possible.</p><p id="53e6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">The previous and current research so far has focused on tightening these bounds by concentrating on taking a relevant subset of the hypothesis class. There has also been a lot of pathbreaking research on refining these bounds, all based on the concept of <strong class="jf hj">一致收敛</strong>。</p><p id="489a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">然而，本文解释说，这些算法要么是:</p><ul class=""><li id="8ad9" class="kt ku hi jf b jg kb jk kc jo kv js kw jw kx ka ky kz la lb bi translated">太大，并且它们的复杂性随着参数计数而增加，或者</li><li id="7fd5" class="kt ku hi jf b jg lc jk ld jo le js lf jw lg ka ky kz la lb bi translated">很小，但是是在一个改进的网络上开发的</li></ul><blockquote class="lw lx ly"><p id="f3d1" class="jd je lz jf b jg kb ji jj jk kc jm jn ma kd jq jr mb ke ju jv mc kf jy jz ka hb bi translated"><em class="hi">该论文定义了一组泛化界限的标准，并展示了一组实验来证明一致收敛如何不能完全解释深度学习中的泛化。</em></p></blockquote><p id="be7b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">概化界限:</p><ol class=""><li id="81de" class="kt ku hi jf b jg kb jk kc jo kv js kw jw kx ka mh kz la lb bi translated">必须是理想的&lt;1 (vacuous)</li><li id="5fa4" class="kt ku hi jf b jg lc jk ld jo le js lf jw lg ka mh kz la lb bi translated">Become smaller with increasing width</li><li id="c68b" class="kt ku hi jf b jg lc jk ld jo le js lf jw lg ka mh kz la lb bi translated">Apply to the network learned by <a class="ae kg" href="https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/?utm_source=blog&amp;utm_medium=neurips-2019-best-papers" rel="noopener ugc nofollow" target="_blank"> SGD(随机梯度下降)</a></li><li id="12ed" class="kt ku hi jf b jg lc jk ld jo le js lf jw lg ka mh kz la lb bi translated">随着随机翻转的训练标签的比例而增加</li><li id="db00" class="kt ku hi jf b jg lc jk ld jo le js lf jw lg ka mh kz la lb bi translated">应与数据集大小成反比</li></ol><p id="62f4" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我前面提到的实验是在MNIST数据集上进行的，有三种过度参数化的模型(都是在SGD算法上训练的):</p><ol class=""><li id="0487" class="kt ku hi jf b jg kb jk kc jo kv js kw jw kx ka mh kz la lb bi translated">线性分类器</li><li id="f005" class="kt ku hi jf b jg lc jk ld jo le js lf jw lg ka mh kz la lb bi translated">一种带ReLU的宽神经网络</li><li id="edf1" class="kt ku hi jf b jg lc jk ld jo le js lf jw lg ka mh kz la lb bi translated">具有冻结隐权的无限宽神经网络</li></ol><p id="583c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">该论文继续展示了不同训练集大小的不同超参数设置。</p><blockquote class="lw lx ly"><p id="92cf" class="jd je lz jf b jg kb ji jj jk kc jm jn ma kd jq jr mb ke ju jv mc kf jy jz ka hb bi translated">我看到的一个非常有趣的观察结果是，尽管测试集误差随着训练集大小的增加而减小，但推广范围实际上却在增加。</p></blockquote><p id="238c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">如果网络只是记住我们不断添加到训练集中的数据点，会怎么样？</p><p id="48fd" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们举一个研究人员给出的例子。对于具有1000个维度的数据集上的分类任务，使用SGD训练具有1个隐藏层ReLU和100k个单元的过参数化模型。增加训练集大小可以提高泛化能力并减少测试集错误。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es mi"><img src="../Images/31553a10cdc099cd7b88197588386d61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5AJ_WbBa-FNPLijG.png"/></div></div></figure><p id="381c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">然而，尽管一般化，他们证明决策边界是相当复杂的。这就是他们违背一致收敛思想的地方。</p><p id="fca4" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">因此，即使对于线性分类器，一致收敛也不能完全解释泛化。事实上，当增加样本量时，它实际上可以被认为是导致边界增加的一个因素！</p><p id="c567" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">虽然以前的研究已经将开发深度网络的方向推向了算法相关(为了坚持一致收敛)，但本文提出了开发算法无关技术的需求，这些技术不会将自己限制为一致收敛来解释泛化。</p><p id="cb87" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们可以清楚地看到为什么这篇机器学习研究论文在NeurIPS 2019上获得了杰出新方向论文奖。</p><p id="bcf7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">研究人员表明，仅仅一致收敛不足以解释深度学习中的泛化。此外，不可能实现满足所有5个标准的小范围。这开启了一个全新的研究领域，探索可能解释泛化的其他工具。</p><p id="64d3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">你可以访问和阅读全文<a class="ae kg" href="https://papers.nips.cc/paper/9336-uniform-convergence-may-be-unable-to-explain-generalization-in-deep-learning" rel="noopener ugc nofollow" target="_blank">在这里</a>。</p><h2 id="43b2" class="li ig hi bd ih lj lk ll il lm ln lo ip jo lp lq it js lr ls ix jw lt lu jb lv bi translated">NeurIPS颁发的杰出新方向论文奖的其他荣誉奖包括:</h2><ol class=""><li id="d079" class="kt ku hi jf b jg jh jk jl jo me js mf jw mg ka mh kz la lb bi translated"><a class="ae kg" href="https://papers.nips.cc/paper/8568-putting-an-end-to-end-to-end-gradient-isolated-learning-of-representations" rel="noopener ugc nofollow" target="_blank"> <em class="lz">端到端的终结:表征的梯度隔离学习</em> </a></li><li id="0ac9" class="kt ku hi jf b jg lc jk ld jo le js lf jw lg ka mh kz la lb bi translated"><a class="ae kg" href="https://papers.nips.cc/paper/8396-scene-representation-networks-continuous-3d-structure-aware-neural-scene-representations" rel="noopener ugc nofollow" target="_blank"> <em class="lz">场景表示网络:连续3D结构感知神经场景表示</em> </a></li></ol><h1 id="ea34" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">2019年NeurIPS的时间测试奖</h1><p id="0cc6" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">每年，NeurIPS还为10年前在会议上发表的论文颁奖，该论文对该领域的贡献产生了持久的影响(也是一篇广受欢迎的论文)。</p><p id="72ad" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">今年，名副其实的时间检验奖授予了林晓的“正则化随机学习和在线优化的双重平均方法”。这项研究是基于基本概念，正如我们所知，这些概念奠定了现代机器学习的基础。</p><h2 id="ecfa" class="li ig hi bd ih lj lk ll il lm ln lo ip jo lp lq it js lr ls ix jw lt lu jb lv bi translated">正则化随机学习和在线优化的双重平均方法</h2><p id="840b" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们来分解这篇精彩论文中的四个关键概念:</p><p id="c7d1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 1 —随机梯度下降:</strong> <a class="ae kg" href="https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/?utm_source=blog&amp;utm_medium=neurips-2019-best-papers" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a>已经被正式确立为机器学习中的优化方法。这可以通过下面的随机优化来实现。回想一下SGD和针对非常大样本的随机优化方程，这里，w是权重向量，z是输入特征向量。对于t = 0，1，2…</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mj"><img src="../Images/93858faff196f0d46889c93c4c367d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/0*EuADCtX5lhA3dLWV.png"/></div></figure><p id="ef53" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 2 —在线凸优化:</strong>又一项开创性的研究。这被模拟为一个游戏，玩家将试图预测一个重量向量，并在每个<em class="lz"> t </em>计算最终的损失。主要目的是最小化这种损失——结果非常类似于我们如何使用随机梯度下降进行优化</p><p id="9113" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj"> 3 —压缩学习</strong>:这包括<a class="ae kg" href="https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/?utm_source=blog&amp;utm_medium=neurips-2019-best-papers" rel="noopener ugc nofollow" target="_blank">拉索回归</a>、L1正则化最小二乘法和其他混合正则化方案</p><p id="d2e1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">4——近似梯度法:</strong>与早期技术相比，这是一种更快的方法，可减少损失并保持凸性</p><p id="c649" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">虽然以前的研究开发了一个收敛到O(1/t)的有效算法，但数据的稀疏性是一个被忽略的因素。<strong class="jf hj">提出了一种新的正则化技术，称为正则化对偶平均法(RDA ),用于求解在线凸优化问题。</strong></p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es mk"><img src="../Images/a3d47231751a9be6ad2bc55d37dc4090.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/0*Aci-qC611cuJ9E7c.png"/></div></figure><p id="d921" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">当时，这些凸优化问题效率不高，尤其是在可伸缩性方面。</p><p id="a4a6" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">本研究提出了一种新颖的<a class="ae kg" href="https://www.analyticsvidhya.com/blog/2018/11/neural-networks-hyperparameter-tuning-regularization-deeplearning/?utm_source=blog&amp;utm_medium=neurips-2019-best-papers" rel="noopener ugc nofollow" target="_blank"> <strong class="jf hj">批量优化方法</strong> </a>。这意味着最初只有一些独立的样本可用，并且权重向量是基于那些样本计算的(在当前时间<em class="lz"> t </em>)。相对于当前权重向量的损失与次梯度一起计算。这在迭代中再次使用(在时间t+1)。</p><p id="c9da" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">具体来说，在RDA中，考虑的是平均次梯度，而不是当前次梯度。</p><blockquote class="lw lx ly"><p id="feb6" class="jd je lz jf b jg kb ji jj jk kc jm jn ma kd jq jr mb ke ju jv mc kf jy jz ka hb bi translated">当时，该方法对于稀疏MNIST数据集已经取得了比SGD和其他流行技术好得多的结果。事实上，随着稀疏度的增加，RDA方法也有明显更好的结果。</p></blockquote><p id="0bb2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这被授予时间测试论文的原因在进一步研究上述方法的不同论文中是显而易见的，如流形识别、加速RDA等。</p><p id="987c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">你可以在这里找到完整的论文<a class="ae kg" href="https://papers.nips.cc/paper/3882-dual-averaging-method-for-regularized-stochastic-learning-and-online-optimization" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="af46" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结束注释</h1><p id="7ca7" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">NeurIPS 2019又是一次极具教育意义和启发性的会议。我对新方向论文奖以及它如何解决深度学习中的泛化问题特别感兴趣。</p><p id="5433" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">哪篇机器学习研究论文引起了你的注意？除了你想尝试的或者真正给你灵感的论文，你还喜欢其他的吗？请在下面的评论区告诉我。</p><p id="173b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">所有的演讲，包括聚光灯和展示，都由NeurIPS团队现场直播。你可以在这里找到<a class="ae kg" href="https://slideslive.com/neurips" rel="noopener ugc nofollow" target="_blank">的链接。</a></p></div><div class="ab cl ml mm gp mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="hb hc hd he hf"><p id="6aad" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="lz">原载于2019年12月17日</em><a class="ae kg" href="https://www.analyticsvidhya.com/blog/2019/12/neurips-2019-best-papers/" rel="noopener ugc nofollow" target="_blank"><em class="lz">https://www.analyticsvidhya.com</em></a><em class="lz">。</em></p></div></div>    
</body>
</html>