<html>
<head>
<title>What is Gradient Descent? How does it work?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是梯度下降？它是如何工作的？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-is-gradient-descent-how-does-it-work-b713fab88349?source=collection_archive---------10-----------------------#2019-12-14">https://medium.com/analytics-vidhya/what-is-gradient-descent-how-does-it-work-b713fab88349?source=collection_archive---------10-----------------------#2019-12-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="0773" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><strong class="il hj">目的— </strong>了解梯度下降以及梯度下降是如何逐步进行的。</p></blockquote><h1 id="e83e" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">梯度下降</h1><p id="916d" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated">梯度下降是一种<strong class="il hj">优化算法</strong>，用于通过沿最陡下降方向缓慢移动来最小化函数，最陡下降方向由梯度的负值定义。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es kn"><img src="../Images/7cd0ca64781c9e8c16995e9a4ac3a973.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/0*fzqtBJ00jecGtbN2.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">梯度下降算法</figcaption></figure><p id="953c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">用于更新机器学习中的参数，例如<strong class="il hj">线性回归</strong>中的<strong class="il hj">回归系数</strong>和<strong class="il hj">神经网络</strong>中的<strong class="il hj">权重</strong>。</p><p id="c9be" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">让我们举一个简单的线性回归问题的例子，我们的目的是在只给定一个自变量的情况下预测因变量(y)。对于上述线性回归模型，直线方程如下。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es kz"><img src="../Images/c6e0b6bbdfc0eff70671ec460817379a.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*YjxFsmDJsIlMpzlPKpdDrw.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">线性回归的直线方程</figcaption></figure><p id="7563" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">在上式中，y是因变量，<br/> x是自变量，<br/> m是直线的斜率，<br/> c是直线在y轴上的截距。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es la"><img src="../Images/bd01d38a8dedf62272cb33624fe688b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*WzEqErjVmWMxBQOc31e4Jg.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">线性回归线不同分量的可视化</figcaption></figure><p id="1da1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">为此，我们将损失函数视为误差平方和。损失函数的方程如下。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es lb"><img src="../Images/16e8b871396dce6b31035d7a299783b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*UKHFok-LAnNdCjr2YY8b5g.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">损失函数</figcaption></figure><blockquote class="if ig ih"><p id="45e5" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">这里，我们需要优化“m”和“c”的值，以便最小化损失函数。</p></blockquote><p id="259c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">由于y_predicted是线性回归方程给出的输出，因此任何给定点的损耗可由下式给出</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es lc"><img src="../Images/c93650707a75ece8afa4f76faf3f0c6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*wH-WaVfe2-a_JVhmmPMW0w.png"/></div></figure><p id="3224" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">为了找出斜率的负值，我们先求出m和c的偏导数</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ld"><img src="../Images/35c23498c108d6a43839f02cc1440592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0eomlkCM5YHbbA8RQoLZtQ.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">偏导数w . r . t . m和c</figcaption></figure><blockquote class="if ig ih"><p id="5380" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">当两个或两个以上的偏导数在同一个方程上对两个或两个以上的不同变量进行求导时，称为梯度。</p></blockquote><p id="15f5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">在对‘m’和‘c’进行w.r.t .的偏导数后，我们得到上面给出的2个方程，当‘m’和‘c’的某个值给定并在所有数据点上求和时，我们得到斜率的负侧。</p><p id="203c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">下一步是假设一个学习率，通常用“α”(alpha)表示。在大多数情况下，学习率被设置为非常接近0，例如0.001或0.005。</p><blockquote class="if ig ih"><p id="f1c3" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">小的学习速率将导致梯度下降算法的步骤过多，并且如果选择大的“α”值，则可能导致模型永远不会收敛于最小值。</p></blockquote><p id="1a7f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">接下来是根据我们的学习速度来确定步长。步长可以定义为</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es li"><img src="../Images/018ca73734d4574bd81bb2c07849e2bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*6CmCay8reVlMwlwaI4xJWg.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">使用学习率寻找下一个点</figcaption></figure><p id="6f5f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">这将给我们2个点，它们将代表‘m’和‘c’的更新值。</p><p id="e771" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">我们迭代寻找斜率的负值的步骤，然后更新‘m’和‘c’的值，直到我们达到或收敛于我们的最小值。</p><h1 id="e646" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">梯度下降怎么知道什么时候停止算法的迭代？</h1><p id="56b3" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated">通常，当两个条件中的一个满足时，梯度下降将停止。<br/> 1。当步长很小时，它不会对‘m’和‘c’的值产生太大影响。<br/> 2。如果由于某种原因，算法无法收敛，它将在1000次迭代后自动停止，该参数可以在编程期间更改。</p><h1 id="b581" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">梯度下降的类型</h1><p id="a1e3" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated">有3种类型的梯度下降即，<br/> 1。随机梯度下降<br/> 2。批次梯度下降<br/> 3。小批量梯度下降</p><blockquote class="if ig ih"><p id="761b" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">当数据集太大时，执行n步的计算变得非常困难，因此使用随机和小批量梯度下降。</p></blockquote><p id="f9af" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated"><strong class="il hj">随机梯度下降— </strong>在极端情况下，梯度下降在每一步选取一个训练数据实例，并基于该数据点实例进行更新</p><p id="660d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated"><strong class="il hj">批量梯度下降</strong> —在批量梯度下降中，算法使用整个数据集来更新系数值。</p><p id="9b7c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated"><strong class="il hj">小批量梯度下降— </strong>在小批量梯度下降中，算法在每一步从整个数据集中挑选一个小批量，并更新系数的值。该方法兼有随机梯度下降法和分批梯度下降法的优点。</p></div></div>    
</body>
</html>