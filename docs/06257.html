<html>
<head>
<title>An Oversimplified Introduction to PySpark for Programmers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">程序员对PySpark过于简单的介绍</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pyspark-in-15-minutes-49bcde83f6b?source=collection_archive---------12-----------------------#2020-05-16">https://medium.com/analytics-vidhya/pyspark-in-15-minutes-49bcde83f6b?source=collection_archive---------12-----------------------#2020-05-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b0756d6e4c9ad1631f7aa303d886de92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bqHf2WCDUlm7_hU3.png"/></div></div></figure><h1 id="aa2b" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">什么是阿帕奇火花？</h1><p id="9797" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">Apache Spark是一个快速的通用集群计算系统。它提供了Java、Scala、Python和R的高级API，以及支持通用执行图的优化引擎。它还支持一套丰富的高级工具，包括用于SQL和结构化数据处理的<a class="ae km" href="https://spark.apache.org/docs/latest/sql-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> Spark SQL </a>，用于机器学习的<a class="ae km" href="https://spark.apache.org/docs/latest/ml-guide.html" rel="noopener ugc nofollow" target="_blank"> MLlib </a>，用于图形处理的<a class="ae km" href="https://spark.apache.org/docs/latest/graphx-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> GraphX </a>，以及<a class="ae km" href="https://spark.apache.org/docs/latest/streaming-programming-guide.html" rel="noopener ugc nofollow" target="_blank"> Spark Streaming </a>。</p><p id="3503" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">这是技术领域最热门的新趋势之一。它运行速度快(由于内存操作，比传统的<a class="ae km" href="https://www.tutorialspoint.com/hadoop/hadoop_mapreduce.htm" rel="noopener ugc nofollow" target="_blank"> Hadoop MapReduce </a>快100倍)，提供健壮的分布式容错数据对象(称为<a class="ae km" href="https://www.tutorialspoint.com/apache_spark/apache_spark_rdd.htm" rel="noopener ugc nofollow" target="_blank"> RDD </a>)，并与机器学习和图形分析的世界完美集成。</p><p id="5fea" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">Spark在<a class="ae km" href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html" rel="noopener ugc nofollow" target="_blank"> Hadoop/HDFS </a>上实现，大部分用<a class="ae km" href="https://www.scala-lang.org/" rel="noopener ugc nofollow" target="_blank"> Scala </a>编写，一种类似Java的函数式编程语言。幸运的是，Spark提供了一个奇妙的Python集成，称为<strong class="jq hj"> PySpark </strong>，它允许Python程序员与Spark框架进行交互，并学习如何大规模操作数据，以及如何在分布式文件系统上处理对象和算法。Spark以简单接口库的形式支持多种编程语言:Java、Python、Scala和r。</p><p id="98f2" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">需要记住的一点是，Spark不是像Python或Java那样的编程语言。它是一个通用的分布式数据处理引擎，适用于各种环境。它对于大规模和高速的大数据处理特别有用。</p><p id="f8e3" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">PySpark的下载和安装请参考<a class="ae km" href="https://spark.apache.org/docs/latest/" rel="noopener ugc nofollow" target="_blank">官方文档</a>。</p><p id="7544" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">在本文中，我们将学习PySpark的基础知识。有很多概念(不断发展和引入)，因此，我们只关注简单例子的基本原理。</p><p id="8b80" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj">本文必备:</strong></p><ol class=""><li id="d734" class="ks kt hi jq b jr kn jv ko jz ku kd kv kh kw kl kx ky kz la bi translated">基本Python语法</li><li id="9994" class="ks kt hi jq b jr lb jv lc jz ld kd le kh lf kl kx ky kz la bi translated">熟悉<a class="ae km" href="https://python-reference.readthedocs.io/en/latest/docs/operators/lambda.html" rel="noopener ugc nofollow" target="_blank">λ函数</a></li></ol><h1 id="f036" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">弹性分布式数据集(RDD)</h1><p id="3c5f" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">许多Spark程序都围绕着弹性分布式数据集(RDD)的概念，这是一个可以并行操作的容错元素集合。SparkContext驻留在驱动程序中，通过集群管理器管理工作节点上的分布式数据。使用PySpark的好处是，所有这些复杂的数据分区和任务管理都在后台自动处理，程序员可以专注于特定的分析或机器学习工作本身。</p><p id="d436" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">创建rdd有两种方法——在驱动程序中并行化现有集合，或者引用外部存储系统中的数据集，例如共享文件系统、HDFS、HBase或任何提供Hadoop InputFormat的数据源。</p><p id="9007" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">为了说明基于Python的方法，我们将在这里给出第一种类型的例子。我们可以使用Numpy random.randint()创建一个包含20个随机整数(0到10之间)的简单Python数组，然后创建一个RDD对象，如下所示:</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="4fbc" class="lp ir hi ll b fi lq lr l ls lt">from pyspark import SparkContext<br/>import numpy as np<br/>sc=SparkContext(master="local[4]")<br/>lst=np.random.randint(0,10,20)<br/>A=sc.parallelize(lst)</span></pre><p id="50be" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj"> <em class="lu">注意论点</em>中的‘4’。它表示4个计算核心(在您的本地机器中)将用于这个SparkContext对象</strong>。如果我们检查RDD对象的类型，我们得到如下结果，</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="d6aa" class="lp ir hi ll b fi lq lr l ls lt">type(A)<br/>&gt;&gt; pyspark.rdd.RDD</span></pre><p id="0e3a" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">与并行化相对的是集合(使用collect())，它将所有分布的元素集合起来并返回给头节点。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="e222" class="lp ir hi ll b fi lq lr l ls lt">A.collect()<br/>&gt;&gt; [5, 1, 1, 6, 9, 4, 2, 2, 3, 2, 2, 7, 7, 7, 8, 6, 9, 9, 6, 1]</span></pre><p id="826a" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">但是A不再是简单的Numpy数组。我们可以使用glom()方法来检查分区是如何创建的。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="3353" class="lp ir hi ll b fi lq lr l ls lt">A.glom().collect()<br/>&gt;&gt; [[5, 1, 1, 6, 9], [4, 2, 2, 3, 2], [2, 7, 7, 7, 8], [6, 9, 9, 6, 1]]</span></pre><p id="ae40" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">现在停止SC，用2个内核重新初始化它，看看重复这个过程会发生什么。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="da6b" class="lp ir hi ll b fi lq lr l ls lt">sc.stop()<br/>sc=SparkContext(master=”local[2]”)<br/>A = sc.parallelize(lst)<br/>A.glom().collect()<br/>&gt;&gt; [[5, 1, 1, 6, 9, 4, 2, 2, 3, 2], [2, 7, 7, 7, 8, 6, 9, 9, 6, 1]]</span></pre><p id="9f60" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">RDD现在分布在两个块上，而不是四个！T11】</p><p id="781e" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">您已经了解了分布式数据分析的第一步，即控制如何将数据划分为更小的数据块以供进一步处理。</p><h1 id="1e1a" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">深入研究代码</h1><h2 id="9d90" class="lp ir hi bd is lv lw lx iw ly lz ma ja jz mb mc je kd md me ji kh mf mg jm mh bi translated">数数元素</h2><p id="83dc" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">让我们数一下元素的数量。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="2b03" class="lp ir hi ll b fi lq lr l ls lt">A.count()<br/>&gt;&gt; 20</span></pre><h2 id="a000" class="lp ir hi bd is lv lw lx iw ly lz ma ja jz mb mc je kd md me ji kh mf mg jm mh bi translated">“first()”和“take()”运算</h2><p id="aa6b" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">记住，<strong class="jq hj"> take() </strong>总是带一个参数。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="5f31" class="lp ir hi ll b fi lq lr l ls lt">A.first()   #Gives the first element<br/>&gt;&gt; 5</span><span id="a205" class="lp ir hi ll b fi mi lr l ls lt">A.take(4)   #Gives the first few elements<br/>&gt;&gt; [5, 1, 1, 6]</span></pre><h2 id="6d6e" class="lp ir hi bd is lv lw lx iw ly lz ma ja jz mb mc je kd md me ji kh mf mg jm mh bi translated">使用“distinct()”删除重复项</h2><p id="1e65" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><strong class="jq hj">注意</strong>:这个操作需要一个<strong class="jq hj">洗牌</strong>来检测跨分区的重复。所以，这是一个缓慢的操作。不要过度。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="ae5a" class="lp ir hi ll b fi lq lr l ls lt">A.distinct().collect()<br/>&gt;&gt; [6, 4, 2, 8, 5, 1, 9, 3, 7]</span></pre><h2 id="ce5e" class="lp ir hi bd is lv lw lx iw ly lz ma ja jz mb mc je kd md me ji kh mf mg jm mh bi translated">将所有元素相加</h2><p id="ab15" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">这里，我们将通过两种方式找到总和:一种使用<strong class="jq hj"> sum() </strong>，另一种使用<strong class="jq hj"> reduce() </strong>。注意在后者中使用了<a class="ae km" href="https://python-reference.readthedocs.io/en/latest/docs/operators/lambda.html" rel="noopener ugc nofollow" target="_blank">λ函数</a>。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="1cb6" class="lp ir hi ll b fi lq lr l ls lt">A.sum()<br/>&gt;&gt; 97</span><span id="e2a9" class="lp ir hi ll b fi mi lr l ls lt">A.reduce(lambda x,y:x+y)<br/>&gt;&gt; 97</span></pre><h2 id="ceeb" class="lp ir hi bd is lv lw lx iw ly lz ma ja jz mb mc je kd md me ji kh mf mg jm mh bi translated">使用“reduce()”查找最大元素</h2><p id="3a38" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">使用lambda函数，查找最大值元素。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="5042" class="lp ir hi ll b fi lq lr l ls lt">A.reduce(lambda x,y: x if x &gt; y else y)<br/>&gt;&gt; 9</span></pre><h2 id="f960" class="lp ir hi bd is lv lw lx iw ly lz ma ja jz mb mc je kd md me ji kh mf mg jm mh bi translated">基本统计</h2><p id="688a" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在我们的RDD上应用基本统计函数。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="dcd7" class="lp ir hi ll b fi lq lr l ls lt">print("Maximum: ",A.max())<br/>print("Minimum: ",A.min())<br/>print("Mean (average): ",A.mean())<br/>print("Standard deviation: ",A.stdev())</span><span id="bbf9" class="lp ir hi ll b fi mi lr l ls lt">&gt;&gt; Maximum:  9<br/>&gt;&gt; Minimum:  1<br/>&gt;&gt; Mean (average):  4.850000000000001<br/>&gt;&gt; Standard deviation:  2.8332843133014376</span></pre><h2 id="ec45" class="lp ir hi bd is lv lw lx iw ly lz ma ja jz mb mc je kd md me ji kh mf mg jm mh bi translated">在文本块中查找最长的单词</h2><p id="f661" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">看，我们是如何使用<strong class="jq hj"> reduce() </strong>让我们的生活变得简单。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="9350" class="lp ir hi ll b fi lq lr l ls lt">words = ‘An Oversimplified Introduction to PySpark for Programmers’.split(‘ ‘)<br/>wordRDD = sc.parallelize(words)<br/>wordRDD.reduce(lambda w,v: w if len(w)&gt;len(v) else v)<br/>&gt;&gt; ‘Oversimplified’</span></pre><h2 id="9e47" class="lp ir hi bd is lv lw lx iw ly lz ma ja jz mb mc je kd md me ji kh mf mg jm mh bi translated">映射操作</h2><p id="37ee" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><strong class="jq hj"> map() </strong>通过对RDD的每个元素应用一个函数来返回一个新的RDD。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="a721" class="lp ir hi ll b fi lq lr l ls lt">B=A.<strong class="ll hj">map</strong>(lambda x:x*x)<br/>B.<strong class="ll hj">collect</strong>()<br/>&gt;&gt; [25, 1, 1, 36, 81, 16, 4, 4, 9, 4, 4, 49, 49, 49, 64, 36, 81, 81, 36, 1]</span></pre><h2 id="012b" class="lp ir hi bd is lv lw lx iw ly lz ma ja jz mb mc je kd md me ji kh mf mg jm mh bi translated">使用常规Python函数进行映射</h2><p id="a013" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">以下函数返回奇数元素的平方，并保持偶数参数不变。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="37e5" class="lp ir hi ll b fi lq lr l ls lt">def square_if_odd(x):<br/>    if x%2==1:<br/>        return x*x<br/>    else:<br/>        return x<br/>A.map(square_if_odd).collect()<br/>&gt;&gt; [25, 1, 1, 6, 81, 4, 2, 2, 9, 2, 2, 49, 49, 49, 8, 6, 81, 81, 6, 1]</span></pre><h1 id="5ce4" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">惰性评估(和缓存)</h1><p id="5e32" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">惰性评估是一种评估/计算策略，它为计算任务准备了详细的执行管道的逐步内部图，但将最终执行延迟到绝对需要的时候。这一策略是Spark加速许多并行化大数据操作的核心。</p><p id="666c" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">让我们在这个例子中使用两个CPU内核，</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="ffb2" class="lp ir hi ll b fi lq lr l ls lt">sc = SparkContext(master=”local[2]”)</span></pre><p id="c2f1" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">用一百万个元素做一个RDD，</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="9138" class="lp ir hi ll b fi lq lr l ls lt">%%time<br/>rdd1 = sc.parallelize(range(1000000))<br/>&gt;&gt; CPU times: user 316 µs, sys: 5.13 ms, total: 5.45 ms, Wall time: 24.6 ms</span></pre><p id="d2ff" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">现在，创建一个Python函数，比如“taketime”，</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="a9b5" class="lp ir hi ll b fi lq lr l ls lt">from math import cos<br/>def taketime(x):<br/>    [cos(j) for j in range(100)]<br/>    return cos(x)</span></pre><p id="7b3c" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">检查“花费时间”功能花费了多少时间</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="a36a" class="lp ir hi ll b fi lq lr l ls lt">%%time<br/>taketime(2)<br/>&gt;&gt; CPU times: user 21 µs, sys: 7 µs, total: 28 µs, Wall time: 31.5 µs<br/>&gt;&gt; -0.4161468365471424</span></pre><p id="f35d" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">记住这个结果，taketime()函数用了31.5 us的墙时间。当然，确切的数字将取决于您正在使用的机器。</p><p id="4a17" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">现在，对函数执行映射操作，</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="526c" class="lp ir hi ll b fi lq lr l ls lt">%%time<br/>interim = rdd1.map(lambda x: taketime(x))<br/>&gt;&gt; CPU times: user 23 µs, sys: 8 µs, total: 31 µs, Wall time: 34.8 µs</span></pre><p id="cce8" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">为什么每个taketime函数需要34.8 us，但是一百万元素RDD的地图操作也需要类似的时间？</p><p id="21e9" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated"><strong class="jq hj">因为懒评估，即上一步什么都没计算，只做了一个执行计划</strong>。变量<em class="lu"> interim </em>并不指向数据结构，相反，它指向一个执行计划，用依赖图表示。依赖图定义了rdd如何相互计算。</p><p id="ae3e" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">reduce()方法的实际执行，</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="838e" class="lp ir hi ll b fi lq lr l ls lt">%%time<br/>print(‘output =’,interim.reduce(lambda x,y:x+y))<br/>&gt;&gt; output = -0.28870546796843666<br/>&gt;&gt; CPU times: user 11.6 ms, sys: 5.56 ms, total: 17.2 ms, Wall time: 15.6 s</span></pre><p id="57ef" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">所以，这里的墙时间是15.6秒。还记得吗，taketime()函数的墙时间是31.5 us？因此，对于一百万个阵列，我们预计总时间约为31秒。因为在两个内核上并行操作，所以花费了大约15秒。</p><p id="5dc6" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">现在，我们在过渡期间没有保存(物化)任何中间结果，所以另一个简单的操作(例如计数元素&gt; 0)将花费几乎相同的时间。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="5334" class="lp ir hi ll b fi lq lr l ls lt">%%time<br/>print(interim.filter(lambda x:x&gt;0).count())<br/>&gt;&gt; 500000<br/>&gt;&gt; CPU times: user 10.6 ms, sys: 8.55 ms, total: 19.2 ms, Wall time: 12.1 s</span></pre><h2 id="df28" class="lp ir hi bd is lv lw lx iw ly lz ma ja jz mb mc je kd md me ji kh mf mg jm mh bi translated">缓存以减少类似操作的计算时间(消耗内存)</h2><p id="6d46" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">还记得我们在上一步中构建的依赖图吗？我们可以像以前一样使用cache方法运行相同的计算，告诉依赖图规划缓存。</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="0954" class="lp ir hi ll b fi lq lr l ls lt">%%time<br/>interim = rdd1.map(lambda x: taketime(x)).cache()</span></pre><p id="d880" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">第一次计算不会改进，但是它缓存了中间结果，</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="770a" class="lp ir hi ll b fi lq lr l ls lt">%%time<br/>print(‘output =’,interim.reduce(lambda x,y:x+y))<br/>&gt;&gt; output = -0.28870546796843666<br/>&gt;&gt; CPU times: user 16.4 ms, sys: 2.24 ms, total: 18.7 ms, Wall time: 15.3 s</span></pre><p id="d72c" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">现在在缓存结果的帮助下运行相同的过滤方法，</p><pre class="lg lh li lj fd lk ll lm ln aw lo bi"><span id="2f36" class="lp ir hi ll b fi lq lr l ls lt">%%time<br/>print(interim.filter(lambda x:x&gt;0).count())<br/>&gt;&gt; 500000<br/>&gt;&gt; CPU times: user 14.2 ms, sys: 3.27 ms, total: 17.4 ms, Wall time: 811 ms</span></pre><p id="ed93" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">哇！计算时间从之前的12秒下降到不到1秒！这样，延迟执行的缓存和并行化是Spark编程的核心特性。</p><h1 id="eceb" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">下一步做什么？</h1><p id="af77" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">本文介绍了Apache Spark的基础知识，以及它如何使用Python接口PySpark实现核心数据结构RDD的一些基本示例。</p><p id="fefd" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">关于Apache Spark，还有很多东西需要学习和实验。PySpark网站是一个很好的参考网站，他们会定期更新和改进，所以请密切关注。</p><p id="6ae1" class="pw-post-body-paragraph jo jp hi jq b jr kn jt ju jv ko jx jy jz kp kb kc kd kq kf kg kh kr kj kk kl hb bi translated">而且，如果您对使用Apache Spark进行大规模分布式机器学习感兴趣，那么可以查看一下<a class="ae km" href="https://spark.apache.org/mllib/" rel="noopener ugc nofollow" target="_blank"> MLlib </a>。</p></div></div>    
</body>
</html>