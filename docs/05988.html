<html>
<head>
<title>Hyper parameter Tuning — Deep Neural Networks.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">超参数调整-深度神经网络。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/hyper-parameter-tuning-deep-neural-networks-592414c0a85c?source=collection_archive---------35-----------------------#2020-05-07">https://medium.com/analytics-vidhya/hyper-parameter-tuning-deep-neural-networks-592414c0a85c?source=collection_archive---------35-----------------------#2020-05-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2a2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来谈谈如何调整超参数、正则化和优化来改进深度神经网络，而不需要太多的数学细节，只需要理解概念。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/60dd3a1b7f15791a1ca3fef2269cd398.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*TvFHPlU3seLmKUC1YQjKMQ.jpeg"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><a class="ae jp" href="https://www.kdnuggets.com/2020/02/deep-neural-networks.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="90f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">应用ML是一个非常迭代的过程，包括层、隐藏单元、学习速率、激活函数。</p><p id="99b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">众所周知，在训练神经网络之前，我们必须将数据分成<strong class="ih hj">训练</strong>、<strong class="ih hj">测试</strong>和<strong class="ih hj">验证</strong>(开发|维持)。许多人认为这没什么必要。但的确有影响。根据您拥有的数据数量，将其分为训练、测试和验证是非常重要的，这应该以这样的方式进行，即测试中必须有足够的数据，以确保网络的适当偏差和方差。为了找到适合您的数据和用例的最佳模型，您必须保留验证数据。有人用60%、20%、20%作为比例。但这是基于数据的数量。在大数据的情况下，您不必坚持这个比率，但您可以选择获取数据，以便有足够的信心使用最佳模型。通常会出现一个常见的错误，即训练数据中的数据与您真正想要测试的数据完全不同。例如猫和狗的分类，应用程序中的猫图片可能是您想要处理的数据。但是你的训练数据将包含来自网页的图像。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jq"><img src="../Images/c60ee5d589b0f0b521ae050786f39a9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*ZiYvylk60EY2XG7ck1lqJA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><a class="ae jp" href="https://dziganto.github.io/cross-validation/data%20science/machine%20learning/model%20tuning/python/Model-Tuning-with-Validation-and-Cross-Validation/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="a946" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二件事是偏差，方差权衡。我们先来了解一下，在我们的神经网络中，什么是偏差和方差。</p><p id="a115" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当模型试图给你<strong class="ih hj">训练错误</strong>，<strong class="ih hj">验证错误</strong>为:</p><ol class=""><li id="12f0" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated">1% | 11% —高方差。也就是说，你的模型能够识别训练集中的几乎每一个数据，并且它确实过度适合训练数据。并且它不会给测试数据带来太多的性能。你可以做的是向训练集添加更多的一般数据，或者进行正则化处理，如L1正则化、L2正则化或辍学正则化。此外，神经网络的选择也很重要。</li><li id="1183" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">15% | 16% —高偏差。这是你的模型不能对你的训练数据和验证数据进行分类的情况。这基本上就是fit中提到的内容。在这种情况下我们能做的是？增加隐藏层的数量，也就是让你的网络更大，以使网络了解更多。你也可以训练更长的时间和更多的纪元来消除偏见。</li><li id="859e" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">0.5% | 1% —低偏差低方差，真的很好！</li><li id="c09a" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">15% | 30% —高偏差和高方差，这真的很糟糕，您必须使用上述方法在两者之间进行权衡，以创建一个好的模型。</li></ol><p id="0331" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">专业人士提到的一个基本渠道是:</p><p id="3114" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">偏高</strong>(训练数据表现？)&gt; &gt; <strong class="ih hj">更大的网络</strong> | <strong class="ih hj">训练更大的</strong> | <strong class="ih hj">神经网络选择</strong> &gt; &gt;如果再次偏高:重复这个else: &gt; &gt; <strong class="ih hj">高方差</strong>(测试数据表现？)&gt; &gt; <strong class="ih hj">更多数据</strong> | <strong class="ih hj">规则化</strong> | <strong class="ih hj">神经网络选择</strong> &gt; &gt;如果再次出现高方差:重复此句:&gt; &gt; <strong class="ih hj">搞定</strong></p><p id="4da0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:你必须小心权衡偏差和方差。</p><h2 id="6733" class="kf kg hi bd kh ki kj kk kl km kn ko kp iq kq kr ks iu kt ku kv iy kw kx ky kz bi translated">L1和L2正则化</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es la"><img src="../Images/0d71d4b6df0a48205af94a2375073df0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*BFJx_uzxiKqQ6C78lDAoQg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><a class="ae jp" rel="noopener" href="/analytics-vidhya/l1-vs-l2-regularization-which-is-better-d01068e6658c">来源</a></figcaption></figure><blockquote class="lb lc ld"><p id="43b6" class="if ig le ih b ii ij ik il im in io ip lf ir is it lg iv iw ix lh iz ja jb jc hb bi translated"><em class="hi">L1正则化和L2正则化之间的主要直观区别在于，L1正则化试图估计数据的中值，而L2正则化试图估计数据的平均值以避免过拟合。</em></p></blockquote><p id="83b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以看到<strong class="ih hj"> L1正则化</strong>通过添加权重(Wj)参数的绝对值在成本函数中添加惩罚项，而<strong class="ih hj"> L2正则化</strong>在成本函数中添加权重(Wj)的平方值。</p><p id="93e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">两者之间的区别在于，当采用梯度下降来更新权重以便更好地拟合时，损失函数被减少了上面针对相应正则化示出的因子。这将减少损失并影响权重的更新，从而不会过度适应训练数据。L1试图通过从数据分布的中位数中减去损失来减少损失，而L2通过从数据分布的平均数中减去损失来减少损失。</p><blockquote class="lb lc ld"><p id="ef40" class="if ig le ih b ii ij ik il im in io ip lf ir is it lg iv iw ix lh iz ja jb jc hb bi translated"><strong class="ih hj">退学正规化</strong></p><p id="dc16" class="if ig le ih b ii ij ik il im in io ip lf ir is it lg iv iw ix lh iz ja jb jc hb bi translated">通过在训练期间随机删除节点，可以使用单个模型来模拟具有大量不同的网络架构。这被称为dropout，并提供了一种计算成本非常低且非常有效的正则化方法，以<a class="ae jp" href="https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/" rel="noopener ugc nofollow" target="_blank">减少过拟合并改善各种深度神经网络中的泛化误差</a>。</p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es li"><img src="../Images/3148a19792dee8716f07fb5c10ae9265.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*FhmHcHZaR8nfwlVetqEyXw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><a class="ae jp" rel="noopener" href="/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5">来源</a></figcaption></figure><p id="97f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在训练期间，一些节点的一些输出被忽略。这使得每一层每次都具有不同类型和数量的节点。实际上，训练期间对层的每次更新都是用不同的视图来执行的。</p><p id="9e3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以在网络中的任何或所有隐藏层以及可见层或输入层上实现丢弃。它不用于输出层。</p><p id="06c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">标准化输入</strong>:</p><ol class=""><li id="3110" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated">归一化平均值:归一化平均值是用全部数据的平均值减去每个数据的过程。</li><li id="6add" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">归一化方差:用每个特征的方差划分数据的过程。</li></ol><p id="edb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用平均值和方差来规范化训练集和测试集。这对我们有什么帮助？当您的要素处于归一化比例时，成本函数很容易收敛到某一点或进行优化，从而为您提供更好的预测。</p><h2 id="7c20" class="kf kg hi bd kh ki kj kk kl km kn ko kp iq kq kr ks iu kt ku kv iy kw kx ky kz bi translated">消失和爆炸渐变:</h2><p id="6cd5" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">从名字本身我们就知道，当我们试图训练一个深度神经网络时，有时梯度变大，有时梯度成指数级变小。加入一个与激活函数相对应的合理的比例初始化权重，将有助于减少消失梯度和爆炸梯度的影响，使你能够长时间训练神经网络。</p><p id="f0b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你有兴趣检查一下部分的<strong class="ih hj">梯度检查</strong>方法来检查你的神经网络。</p><p id="2d21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">思考<strong class="ih hj">超参数优化</strong>应该选择哪个<strong class="ih hj">库</strong>？看看这个博客。值得一读..</p><div class="lo lp ez fb lq lr"><a href="https://neptune.ai/blog/optuna-vs-hyperopt" rel="noopener  ugc nofollow" target="_blank"><div class="ls ab dw"><div class="lt ab lu cl cj lv"><h2 class="bd hj fi z dy lw ea eb lx ed ef hh bi translated">Optuna vs Hyperopt:应该选择哪个超参数优化库？- neptune.ai</h2><div class="ly l"><h3 class="bd b fi z dy lw ea eb lx ed ef dx translated">思考应该选择哪个库进行超参数优化？使用远视有一段时间了，感觉像…</h3></div><div class="lz l"><p class="bd b fp z dy lw ea eb lx ed ef dx translated">海王星. ai</p></div></div><div class="ma l"><div class="mb l mc md me ma mf jj lr"/></div></div></a></div></div><div class="ab cl mg mh gp mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="hb hc hd he hf"><p id="1cec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><ol class=""><li id="a5b6" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated"><a class="ae jp" href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/dropout-for-regulating-deep-neural-networks/</a></li><li id="c076" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated"><a class="ae jp" rel="noopener" href="/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5">https://medium . com/@ amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334 da 4 bfc 5</a></li></ol></div></div>    
</body>
</html>