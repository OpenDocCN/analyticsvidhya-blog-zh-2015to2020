<html>
<head>
<title>Understanding the Bellman Equation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解贝尔曼方程</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-the-bellman-equation-c711e531a2e5?source=collection_archive---------13-----------------------#2020-07-27">https://medium.com/analytics-vidhya/understanding-the-bellman-equation-c711e531a2e5?source=collection_archive---------13-----------------------#2020-07-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="04bf" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">免费RL课程:第2部分</h2></div><p id="cc20" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是我的强化学习课程中的第二篇文章。上一篇文章可以在这里找到<a class="ae jt" rel="noopener" href="/@NathanWeatherly/introduction-to-reinforcement-learning-53b9caee364c">。它涵盖了基本概念，如奖励和政策，所以如果你不熟悉这些概念，你可能应该回去读一读，然后再继续这一个。本文将涵盖Q学习中使用的方程和概念，Q学习是几乎所有现代强化学习算法的基础。</a></p><p id="bd5c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">回顾一下我们上次学到的内容，代理遵循一个策略在其环境中采取行动，并根据这些行动的结果获得奖励。上次我们方法的问题是代理没有办法学习。为了帮助我们的代理学习，我们必须给它一些方法来评估来自给定状态的动作的值。然后，基于采取特定行动时的回报，它将能够调整其政策，以更好地代表真正的价值观。本文将通过三个部分来帮助解释如何做到这一点:</p><ul class=""><li id="f3ee" class="ju jv hi iz b ja jb jd je jg jw jk jx jo jy js jz ka kb kc bi translated">价值函数</li><li id="51f1" class="ju jv hi iz b ja kd jd ke jg kf jk kg jo kh js jz ka kb kc bi translated">最佳值函数</li><li id="52d8" class="ju jv hi iz b ja kd jd ke jg kf jk kg jo kh js jz ka kb kc bi translated">贝尔曼方程</li></ul><p id="a039" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">值函数是在遵循特定策略时向动作或状态分配值的函数。最显而易见的方法是将价值作为奖励来分配:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es ki"><img src="../Images/2980b4895fa845859bcda544b8d67f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9ihfgzAWvrXIdfu6_5prRw.png"/></div></div></figure><p id="29fe" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个方程有一些奇特的数学语法，但它很容易理解。左手边表示一个状态的值，<strong class="iz hj"> <em class="ku"> s </em> </strong>，当代理遵循一个策略时，<strong class="iz hj"> <em class="ku"> 𝜋 </em> </strong>。这是然后设置等于收到的奖励，<strong class="iz hj"> <em class="ku"> Rₜ ₊ </em> </strong> ₁，如果我们从一个州开始，<strong class="iz hj"><em class="ku"/></strong>，并遵循一个政策，<strong class="iz hj"><em class="ku"/></strong>。然而，这种方法有一个明显的缺陷。它不考虑未来状态和行动的回报。没有前瞻性，代理将无法准确评估价值。要解决这个问题，你可以建议以这样的方式将所有未来的奖励加起来:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es kv"><img src="../Images/ed7e9846eb80264fabde4f74c356a17a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m3ICfsSHt0-gKPbDGaQ_dQ.png"/></div></div></figure><p id="0b08" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，这种方法会导致值经常趋向于无穷大，特别是如果环境确实有一个结束状态。为了解决这个问题，我们可以用符号<strong class="iz hj"><em class="ku"/></strong>(gamma)来表示随后的每个奖励乘以一个“折扣”。这个变量控制我们希望我们的代理人对未来奖励的估价，通常设置为<strong class="iz hj"> <em class="ku"> 0.99 </em> </strong>或<strong class="iz hj"> <em class="ku"> 0.999 </em> </strong>。更新后的等式将如下所示:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es kw"><img src="../Images/319fbab1e3f9888dfe1fe10b7d98347e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oknG1huqc58d4wxaw-Zdpw.png"/></div></div></figure><p id="3ac0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">所以现在我们有了特定状态下的值，<strong class="iz hj"><em class="ku"/></strong>，下策<strong class="iz hj"><em class="ku"/></strong>。为了将此扩展到动作，我们可以使用相同的等式，但是对于第一个动作，使用一个输入，<strong class="iz hj"><em class="ku"/></strong>，而不是策略，<strong class="iz hj"><em class="ku">【𝜋】</em></strong>。这个函数现在将给出我们所知的在状态<strong class="iz hj"> <em class="ku"> s </em> </strong>的动作<strong class="iz hj"> <em class="ku"> a </em> </strong>的Q值。该函数称为Q函数，定义如下:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es kx"><img src="../Images/1d6fcea24f1786ce9c43d26850530ba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tjnDxnrU_1kXPEX9xjFcNQ.png"/></div></div></figure><p id="aeaf" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们已经了解了Q函数，我们可以使用它来创建最优Q值函数，该函数找到使每个状态、<strong class="iz hj"> <em class="ku"> s </em> </strong>和动作、<strong class="iz hj"> <em class="ku"> a </em> </strong>的值最大化的策略。我们可以先将其定义为:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es ky"><img src="../Images/6f7746b509f6bb2e4f3877fbccc192af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VNR9nvtAEgriiLzQm5QfhA.png"/></div></div></figure><p id="eb39" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这种符号意味着最佳q函数等于遵循策略<strong class="iz hj"><em class="ku"/></strong>的q函数，该策略使每个q值最大化。我们可以进一步了解如何使用这个最佳Q函数，方法是将它应用到我们先前定义的Q函数，以获得这个属性:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es kz"><img src="../Images/b401354baedb2e2b0b478c02edef6567.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKF4KA1RvBsQR5KnDDGPVw.png"/></div></div></figure><p id="5e40" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这仅仅意味着最优q-函数也可以定义为奖励，<strong class="iz hj"><em class="ku">【rₜ₊₁】</em></strong>，一个动作，<strong class="iz hj"> <em class="ku">一个</em> </strong>，在一个状态，<strong class="iz hj"><em class="ku"/></strong>，加到下一个状态的贴现最优q-值，<strong class="iz hj"><em class="ku">【s’</em></strong>，其中最优动作，<strong class="iz hj"> <em class="ku">一个’</em></strong>，是这个方程被称为贝尔曼方程，可能是强化学习中最重要的方程。</p><p id="a71d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有了最优Q函数和我们当前策略的Q函数的定义，我们将能够比较这两者来慢慢地将我们的策略转向最优策略。这是Q-Learning背后的基本思想，也是本课程的下一篇文章将涉及的内容。</p></div><div class="ab cl la lb gp lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="hb hc hd he hf"><p id="c835" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="ku">方程式改编自理查德·萨顿的“</em><a class="ae jt" href="http://incompleteideas.net/book/RLbook2020.pdf" rel="noopener ugc nofollow" target="_blank"><em class="ku">RLbook</em></a><em class="ku">”</em>中的概念</p></div></div>    
</body>
</html>