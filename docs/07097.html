<html>
<head>
<title>An overview of gradient descent optimisation algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降优化算法综述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/an-overview-of-gradient-descent-optimisation-algorithms-eb75302b5d84?source=collection_archive---------23-----------------------#2020-06-13">https://medium.com/analytics-vidhya/an-overview-of-gradient-descent-optimisation-algorithms-eb75302b5d84?source=collection_archive---------23-----------------------#2020-06-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/12456cd1ef6c41d4bf3944ffe2e85646.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HJifIvvsZpQdGEenC8w-Qg.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">梯度下降(<a class="ae hv" href="https://datascience-enthusiast.com/DL/Optimization_methods.html" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><div class=""/><h1 id="a08f" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">什么是梯度下降？</h1><p id="b7ca" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi kr translated"><span class="l ks kt ku bm kv kw kx ky kz di"> G </span>梯度下降(radient Descent)是一种优化算法，用于为机器学习模型寻找最佳参数(权重和偏差)，从而最小化用于评估模型性能的损失/成本函数。当不能解析地计算参数并且因此必须在巨大的参数空间中搜索时，使用梯度下降。</p><h1 id="e641" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">梯度下降程序</h1><p id="8cbd" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">梯度下降是一个迭代过程，从一组随机的参数开始，并继续缓慢地改进它们。为了改进给定的一组权重，我们尝试使用当前权重(通过计算梯度)来获得成本函数的值，并在成本函数减小的方向上移动。在大多数情况下，重复这个步骤数千次，给我们一组最小化成本函数的权重。</p><p id="701c" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">梯度下降程序如下:</p><ol class=""><li id="15ed" class="lf lg hy jv b jw la ka lb ke lh ki li km lj kq lk ll lm ln bi translated">随机初始化砝码w。</li><li id="9fbe" class="lf lg hy jv b jw lo ka lp ke lq ki lr km ls kq lk ll lm ln bi translated">计算成本函数相对于权重的梯度G。</li><li id="9282" class="lf lg hy jv b jw lo ka lp ke lq ki lr km ls kq lk ll lm ln bi translated">使用更新规则<em class="lt"> w </em> := <em class="lt"> w-αG </em>更新权重</li><li id="f033" class="lf lg hy jv b jw lo ka lp ke lq ki lr km ls kq lk ll lm ln bi translated">重复步骤2和3，直到重量停止变化或满足终止标准。</li></ol><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es lu"><img src="../Images/cb79b995cfd236ca84ce39e8e868f08e.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*lMFVkOrWot0ymEAt9NlRPw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">梯度下降的基本方程</figcaption></figure><p id="4f57" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">在步骤3中，学习率<em class="lt"> α </em>决定了我们达到最小值所采取的步长。必须仔细选择学习速率，因为太低的学习速率会导致非常慢的学习，或者太高的学习速率会导致参数超过最小值。一旦达到最优，就说模型已经收敛。</p><h1 id="6298" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">梯度下降的直觉</h1><p id="a4e2" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">现在想象你被蒙上眼睛放在一个多维平面上(类似于一个崎岖的山区地形)。你的目标是找到并到达那个地形的最低点。随机分配的起始位置在机器学习领域被称为初始化。现在唯一的方法是，你可以通过感觉周围的地形/坡度开始向最低点移动。感觉斜率的想法类似于在机器学习中计算成本函数的导数。一旦决定了运动的方向，你将采取一个类似于梯度下降算法中权重更新的步骤。这个过程会一遍又一遍地重复，直到你到达最低点或者成本最小的点。</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lz"><img src="../Images/39cd5d3e189ef6161c99845fbfbc818e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AC-5oNFKLqNdl7EoxwFJNQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">梯度下降(<a class="ae hv" rel="noopener" href="/@DBCerigo/on-why-gradient-descent-is-even-needed-25160197a635">源</a>)</figcaption></figure><h1 id="ff7a" class="iv iw hy bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">香草梯度下降的问题</h1><h2 id="721b" class="ma iw hy bd ix mb mc md jb me mf mg jf ke mh mi jj ki mj mk jn km ml mm jr mn bi translated">局部最小值</h2><p id="83bb" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">实际上，神经网络是涉及非线性变换的复杂函数。得到的损失函数看起来不像一个只有一个最小值的凸函数。损失函数要复杂得多，可能看起来像这样。</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mo"><img src="../Images/9967456d5509fd22ce8fda935066e3af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vpCRPR2o3tTl6R3JiRcvyQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">深度神经网络的损失函数轮廓</figcaption></figure><p id="944f" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">在上面的图像中，存在一个梯度为零的局部最小值。但是，我们知道，它们并不是我们所能达到的最低点，这是与全局极小值相对应的点。如果权重被初始化到点A，那么模型很可能会收敛到局部最小值。由于局部最小值处的梯度为零，模型将不能从该点出来。这将导致次优的模型参数，这将导致机器学习模型的不良概括。</p><h2 id="1d5f" class="ma iw hy bd ix mb mc md jb me mf mg jf ke mh mi jj ki mj mk jn km ml mm jr mn bi translated">鞍点</h2><figure class="lv lw lx ly fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mp"><img src="../Images/b42a237e5a6c875728901b0f2c20126e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WGsb0YroD46lcqK58QpFMw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">鞍点(<a class="ae hv" href="https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><p id="736c" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">鞍点的名字来源于一匹马的马鞍，因为它很像它。虽然它在一个方向上是最小值，但在另一个方向上是局部最大值，如果轮廓在x方向上更平坦，GD将在y方向上来回振荡，给我们一种已经收敛到最小值的错觉。</p><p id="7a8c" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">为了避免这样的问题，我们可以对梯度下降算法进行细微的调整，以改善优化过程。我现在将谈论最常用的方法。</p></div><div class="ab cl mq mr gp ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="hb hc hd he hf"><h1 id="7bc2" class="iv iw hy bd ix iy mx ja jb jc my je jf jg mz ji jj jk na jm jn jo nb jq jr js bi translated">梯度下降优化算法</h1><p id="de6b" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在这一节中，我将描述各种梯度下降优化算法。</p><h2 id="9d2e" class="ma iw hy bd ix mb mc md jb me mf mg jf ke mh mi jj ki mj mk jn km ml mm jr mn bi translated">记号</h2><ul class=""><li id="7774" class="lf lg hy jv b jw jx ka kb ke nc ki nd km ne kq nf ll lm ln bi translated"><em class="lt"> w </em>:我们要更新的重量/参数</li><li id="c8f8" class="lf lg hy jv b jw lo ka lp ke lq ki lr km ls kq nf ll lm ln bi translated"><em class="lt"> α </em>:学习率</li><li id="db59" class="lf lg hy jv b jw lo ka lp ke lq ki lr km ls kq nf ll lm ln bi translated"><em class="lt"> t </em>:时间步长</li><li id="cbc3" class="lf lg hy jv b jw lo ka lp ke lq ki lr km ls kq nf ll lm ln bi translated"><em class="lt"> ∂L/∂w </em>:损失函数相对于权重/参数的梯度<em class="lt"> w </em></li></ul></div><div class="ab cl mq mr gp ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="hb hc hd he hf"><h2 id="bc97" class="ma iw hy bd ix mb mc md jb me mf mg jf ke mh mi jj ki mj mk jn km ml mm jr mn bi translated">1.随机梯度下降</h2><p id="392a" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">随机梯度下降(SGD)使用根据当前权重乘以学习率计算的梯度来更新权重。</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es ng"><img src="../Images/d029f9912d8f3d77bd6596a71f2cf770.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*eziENq9G5_nZKB_AOICNMQ.png"/></div></figure><h2 id="8f36" class="ma iw hy bd ix mb mc md jb me mf mg jf ke mh mi jj ki mj mk jn km ml mm jr mn bi translated">2.动力</h2><p id="7065" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">动量不是仅依赖于当前梯度来更新权重，而是用聚合的<em class="lt"> m </em>来代替梯度，该聚合是在直到时间<em class="lt"> t </em>的所有梯度上计算的。该集合是计算的当前和过去梯度的指数移动平均值，即所有到时间步长<em class="lt"> t </em>的梯度。</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es nh"><img src="../Images/8e36168b58b5fd2f9cbc7605357e1c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*bQpfhcKrUJPSK-yAW8W9Mw.png"/></div></figure><p id="877d" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">在哪里，</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es ni"><img src="../Images/fcf8683e0e0c068e99a57dd9599a1f87.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*Zcmk7okOngTnm2g31Ewa3Q.png"/></div></figure><p id="655a" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated"><em class="lt"> m </em>初始化为0，β为超参数。β的常见默认值为0.9。</p><p id="73f7" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated"><strong class="jv hz"> 3。阿达格勒</strong></p><p id="675e" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">AdaGrad或自适应梯度算法作用于权重更新方程的学习率部分。它将学习率除以当前和过去梯度的平方和的平方根。</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es nj"><img src="../Images/83550c4596cf1dce59d24e8fb17064b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*R913PinWp6oV7FM6SYcXIA.png"/></div></figure><p id="2f5b" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">在哪里，</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nk"><img src="../Images/1324dd8d01af0c49a9a82912c55aeb33.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*8Mhm1QO6Itnl2Hlu6oBc4w.png"/></div></div></figure><p id="6471" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated"><em class="lt"> ε </em>是一个小的浮点常数，确保我们永远不会被零除。</p><p id="9866" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated"><em class="lt"> v </em>初始化为0，<em class="lt"> ε </em>的默认值设置为1e-7。</p><p id="7e75" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated"><strong class="jv hz"> 4。RMSProp </strong></p><p id="0dae" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">均方根prop或RMSProp是对AdaGrad的修改。像AdaGrad一样，它也处理更新方程中的学习率部分。然而，RMSProp不像AdaGrad那样采用梯度的累积和，而是像动量一样计算以前和当前梯度的指数移动平均值。生成的更新方程看起来就像在AdaGrad中一样。</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es nj"><img src="../Images/83550c4596cf1dce59d24e8fb17064b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*R913PinWp6oV7FM6SYcXIA.png"/></div></figure><p id="ab77" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">在哪里，</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es nl"><img src="../Images/c8f6c64e9287d9c9f09a763a15b8ace8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*8-mASESXxQ-7N1-Q42urjw.png"/></div></figure><p id="df16" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated"><em class="lt"> v </em>被初始化为0，<em class="lt"> ε </em>的默认值被设置为1e-6。</p><h2 id="97e4" class="ma iw hy bd ix mb mc md jb me mf mg jf ke mh mi jj ki mj mk jn km ml mm jr mn bi translated">5.阿达德尔塔</h2><p id="b646" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">Adaptive Delta是AdaGrad的另一个改进，侧重于学习率部分。术语<em class="lt">δ</em>在这里指的是当前和新更新的权重之间的差。AdaDelta和RMSProp/AdaGrad的区别在于，它用一个参数<em class="lt"> D </em>代替了学习率，即平方Delta的指数移动平均值。</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es nm"><img src="../Images/6a4dc50f94674cad9485f3a8b80c7433.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*uMXoLjqS-VFUnexyBSeUag.png"/></div></figure><p id="f19f" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">在哪里，</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es nl"><img src="../Images/c8f6c64e9287d9c9f09a763a15b8ace8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*8-mASESXxQ-7N1-Q42urjw.png"/></div></figure><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es nn"><img src="../Images/e4e83f9d86b308190eead6d621cdfbf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*PoVqW3AN9Lk59KWfSbXIWA.png"/></div></figure><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es no"><img src="../Images/4d5338eb11b2d8cd3de647f90114f740.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*PFnZx4B5qnVwQro2MxStbA.png"/></div></figure><p id="06c7" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated"><em class="lt"> D </em>和<em class="lt"> v </em>被初始化为0。<em class="lt"> β </em> =0.95和<em class="lt"> ε </em> =1e-6的默认值。</p><p id="0635" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">6。内斯特罗夫加速梯度(NAG) </p><p id="dc4e" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">NAG利用动量原理，但是它以一种聪明的方式计算动量。NAG遵循“三思而后行”的原则。对于前者，NAG使用先前的动量向前迈一步并计算梯度。然后，这些梯度用于梯度的指数移动平均，以计算动量。因此，对于时间步长<em class="lt"> t </em>，我们必须在执行反向传播之前执行另一个正向传播。</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es nh"><img src="../Images/8e36168b58b5fd2f9cbc7605357e1c84.png" data-original-src="https://miro.medium.com/v2/resize:fit:610/format:webp/1*bQpfhcKrUJPSK-yAW8W9Mw.png"/></div></figure><p id="d838" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">在哪里，</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es np"><img src="../Images/d0b5da174490bdc2940b81959265c224.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*R4AaoWbpdBV0n8sLedQPlQ.png"/></div></figure><p id="39d7" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated"><em class="lt"> m </em>被初始化为0。</p><p id="a930" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">第二个等式中的最后一项是向前计算的梯度。</p><p id="e059" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">更新权重的程序:</p><ol class=""><li id="b714" class="lf lg hy jv b jw la ka lb ke lh ki li km lj kq lk ll lm ln bi translated">使用之前的动量将当前权重<em class="lt"> w </em>更新为新权重<em class="lt"> w* </em>。</li></ol><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es nq"><img src="../Images/3c05044190f610fbaa8b238f155fe11e.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*etMTWqCqC6jhQ_gr7wkE4A.png"/></div></figure><p id="9422" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">2.使用<em class="lt"> w* </em>进行正向传播。</p><p id="5e04" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">3.计算渐变，<strong class="jv hz"><em class="lt"/></strong>。</p><p id="e682" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">4.根据上述等式计算动量和更新的权重。</p><p id="c68f" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">β的默认值为0.9。</p><h2 id="aba1" class="ma iw hy bd ix mb mc md jb me mf mg jf ke mh mi jj ki mj mk jn km ml mm jr mn bi translated">7.圣经》和《古兰经》传统中）亚当（人类第一人的名字</h2><p id="dd7d" class="pw-post-body-paragraph jt ju hy jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">自适应矩估计是动量和RMSProp的融合。梯度分量的计算类似于动量，梯度的指数移动平均值，而学习率分量的计算类似于RMSProp，将学习率除以平方梯度的指数移动平均值的平方根。</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es nr"><img src="../Images/935f37bb501703c56671932641715237.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*8qxtGlMX6_qAPYLXoxvxwA.png"/></div></figure><p id="f7e8" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">在哪里，</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es ns"><img src="../Images/3a74da1354c94174215c593b21051099.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*u-5ZvASSJEyZ0LlDojQ6NQ.png"/></div></figure><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es nt"><img src="../Images/dec7cf118567c85275b42e330535aef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*XnMlmQJw1IwcMcNK9etyRQ.png"/></div></figure><p id="2e8d" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">是偏差修正，和</p><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es nu"><img src="../Images/78d0bb9f75090a8162cbd21e4240daaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*FkdRzFELcc0jYjYoYwE-GQ.png"/></div></figure><figure class="lv lw lx ly fd hk er es paragraph-image"><div class="er es nv"><img src="../Images/06693a14f91577d20f5c26aa6204f002.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*41tfFsKud5Hih_OeT-7jIQ.png"/></div></figure><p id="3636" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">m和v被初始化为0。缺省值α=0.001，β₁=0.9，β₂=0.999和ε=1e-8。</p></div><div class="ab cl mq mr gp ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="hb hc hd he hf"><p id="25b8" class="pw-post-body-paragraph jt ju hy jv b jw la jy jz ka lb kc kd ke lc kg kh ki ld kk kl km le ko kp kq hb bi translated">这个帖子到此为止。如果你喜欢这篇文章，请鼓掌并与他人分享。你可以在<a class="ae hv" href="http://.com/in/vidit-jain-10a828167/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上找到我，或者在<a class="ae hv" href="https://github.com/viditjain99" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上查看我的一些作品。如果您有任何问题或想法，请在下面留言。感谢你阅读这篇文章！！祝您有愉快的一天…🎉</p></div></div>    
</body>
</html>