<html>
<head>
<title>Machine Learning to Predict Taxi Fare — Part Two: Predictive Modelling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测出租车费用的机器学习—第二部分:预测模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-to-predict-taxi-fare-part-two-predictive-modelling-f80461a8072e?source=collection_archive---------1-----------------------#2018-09-21">https://medium.com/analytics-vidhya/machine-learning-to-predict-taxi-fare-part-two-predictive-modelling-f80461a8072e?source=collection_archive---------1-----------------------#2018-09-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="45ba" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">模型评估、模型调整</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/31501970e62bd11cb0d8c0da1566aa8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m4Kv3w4-PnWDG4H4ObJZfw.png"/></div></div></figure><p id="5c03" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">欢迎来到使用机器学习预测出租车费用系列的第二部分！这是一个独特的挑战，你说呢？我们定期乘坐出租车(有时甚至每天！)，然而当我们点击“立即预订”按钮时，我们依赖于手动即时计算，而不是核心的ML计算。这就是我在这里想要展示的。</p><p id="cc45" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在之前的<a class="ae kf" rel="noopener" href="/analytics-vidhya/machine-learning-to-predict-taxi-fare-part-one-exploratory-analysis-6b7e6b1fbc78">帖子</a>中，我们查看了清理数据和探索数据，以确定变量之间的关系，并了解将对出租车费用产生影响的各种功能。</p><p id="df6c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在本文中，我们将了解如何构建机器学习模型来预测出租车费用，并了解特征工程在此过程中发挥的重要影响。这篇文章的代码可以在<a class="ae kf" href="https://github.com/AiswaryaSrinivas/DataScienceWithPython/blob/master/New%20York%20Taxi%20Fare%20Prediction/Modelling.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="e4d0" class="kg kh hi bd ki kj kk kl km kn ko kp kq io kr ip ks ir kt is ku iu kv iv kw kx bi translated"><strong class="ak">路线图</strong></h1><p id="a12f" class="pw-post-body-paragraph jj jk hi jl b jm ky ij jo jp kz im jr js la ju jv jw lb jy jz ka lc kc kd ke hb bi translated">在数据清理和探索性分析阶段之后，我们最终到达了模型构建阶段。这一阶段结束时的结果质量取决于数据质量和用于建模的特征。在本文中，我们将详细了解以下步骤:</p><ol class=""><li id="657d" class="ld le hi jl b jm jn jp jq js lf jw lg ka lh ke li lj lk ll bi translated">建模的数据准备</li><li id="26db" class="ld le hi jl b jm lm jp ln js lo jw lp ka lq ke li lj lk ll bi translated">创建基线预测</li><li id="8d5d" class="ld le hi jl b jm lm jp ln js lo jw lp ka lq ke li lj lk ll bi translated">不使用特征工程构建模型</li><li id="39fc" class="ld le hi jl b jm lm jp ln js lo jw lp ka lq ke li lj lk ll bi translated">使用特征工程构建模型</li></ol><h1 id="2102" class="kg kh hi bd ki kj kk kl km kn ko kp kq io kr ip ks ir kt is ku iu kv iv kw kx bi translated">数据准备</h1><p id="e73f" class="pw-post-body-paragraph jj jk hi jl b jm ky ij jo jp kz im jr js la ju jv jw lb jy jz ka lc kc kd ke hb bi translated">这一步包括清理数据，删除不需要的列，将分类变量转换为机器可理解的格式，最后将训练数据拆分为训练集和验证集。</p><p id="dedc" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们将删除所有负票价金额和大于7的乘客计数，正如我们在第1部分中所做的那样。</p><p id="4bbc" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">数据准备的下一步是将训练数据分为训练数据集和验证数据集。几乎所有机器学习项目都遵循这一步骤，并且这是允许我们评估模型的最关键步骤之一。</p><p id="6c43" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">验证数据集有助于我们了解使用训练数据拟合的模型如何对任何看不见的数据起作用。这有助于我们判断模型是过拟合还是欠拟合。过度拟合是当训练误差较低，但测试误差较高时使用的术语。这是复杂模型的一个常见问题，因为它们倾向于记住底层数据，因此对看不见的数据表现不佳。</p><p id="01b2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">像线性回归这样的简单模型不会记住基础数据，而是对数据进行简单的假设。因此，这些模型有很高的误差(高偏差)，但方差低。验证数据集将帮助我们评估基于方差和偏差的模型。在这个分析中，我们将保留25%的数据作为验证数据。</p><pre class="iy iz ja jb fd lr ls lt lu aw lv bi"><span id="983b" class="lw kh hi ls b fi lx ly l lz ma">from sklearn.model_selection import train_test_split<br/>X=train.drop([fare_amount],axis=1)<br/>y=train[fare_amount]<br/>X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25,random_state=123)#test_size is the proportion of data that is to be kept aside for validation</span></pre><h1 id="a7b4" class="kg kh hi bd ki kj kk kl km kn ko kp kq io kr ip ks ir kt is ku iu kv iv kw kx bi translated">基线模型</h1><p id="b292" class="pw-post-body-paragraph jj jk hi jl b jm ky ij jo jp kz im jr js la ju jv jw lb jy jz ka lc kc kd ke hb bi translated">基线模型是不应用任何机器学习技术的问题的解决方案。我们建立的任何模型都必须改进这个解决方案。建立基线模型的一些方法是在分类时采用最常见的值，在回归问题中计算平均值。在此分析中，由于我们预测票价金额(这是一个定量变量)，因此我们将预测平均票价金额。<strong class="jl hj">这导致了9.71的RMSE。所以我们建立的任何模型都应该有低于9.71的RMSE。</strong></p><pre class="iy iz ja jb fd lr ls lt lu aw lv bi"><span id="bd11" class="lw kh hi ls b fi lx ly l lz ma">avg_fare=round(np.mean(y_train),2) #11.31<br/>baseline_pred=np.repeat(avg_fare,y_test.shape[0])<br/>baseline_rmse=np.sqrt(mean_squared_error(baseline_pred, y_test))<br/>print("Basline RMSE of Validation data :",baseline_rmse)</span></pre><h1 id="13da" class="kg kh hi bd ki kj kk kl km kn ko kp kq io kr ip ks ir kt is ku iu kv iv kw kx bi translated">模型建立和评估</h1><h2 id="5347" class="lw kh hi bd ki mb mc md km me mf mg kq js mh mi ks jw mj mk ku ka ml mm kw mn bi translated">1.没有特征工程</h2><p id="6c3d" class="pw-post-body-paragraph jj jk hi jl b jm ky ij jo jp kz im jr js la ju jv jw lb jy jz ka lc kc kd ke hb bi translated">在这一步中，我们将仅使用日期时间要素，而不包括任何其他要素，如行程距离、机场距离或从行政区的上车距离。为了理解和评估模型，我们将考虑以下ML算法:</p><ul class=""><li id="5532" class="ld le hi jl b jm jn jp jq js lf jw lg ka lh ke mo lj lk ll bi translated">线性回归</li><li id="21ea" class="ld le hi jl b jm lm jp ln js lo jw lp ka lq ke mo lj lk ll bi translated">随机森林</li><li id="ff54" class="ld le hi jl b jm lm jp ln js lo jw lp ka lq ke mo lj lk ll bi translated">轻型GBM</li></ul><p id="96ad" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">除了线性回归，其余的模型都是决策树的集合，但它们在决策树的创建方式上有所不同。在我们进一步讨论之前，让我们先了解我们将使用的几个关键术语的定义。</p><ol class=""><li id="2132" class="ld le hi jl b jm jn jp jq js lf jw lg ka lh ke li lj lk ll bi translated"><strong class="jl hj"> Bagging </strong>:该方法创建多个模型，输出预测是不同模型的平均预测。在Bagging中，你获取数据集的引导样本(替换)，每个样本训练一个弱学习者。随机森林使用这种方法进行预测。在随机森林中，创建多个决策树，输出是这些树的平均预测。为了使这种方法有效，基线模型必须具有较低的偏差(错误率)。</li><li id="73d1" class="ld le hi jl b jm lm jp ln js lo jw lp ka lq ke li lj lk ll bi translated"><strong class="jl hj"> Boosting </strong>:在这种方法中，多个弱学习器被集成以创建强学习器。Boosting使用所有数据来训练每个学习者。但是被先前的学习者错误分类的实例被给予更多的权重，以便后续的学习者可以在训练期间给予它们更多的关注。XGBoost和Light GBM都是基于这种方法。它们都是梯度推进决策树(GBDTs)的变体。在GBDTs中，决策树是迭代训练的，即一次训练一棵树。XGBoost和Light GBM在生长决策树时使用<strong class="jl hj">逐叶</strong>生长策略。在训练每个决策树和拆分数据时，XGBoost遵循逐层策略，而Light GBM遵循逐叶策略。</li></ol><p id="4293" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">终于到了建立我们的模型的时候了！</p><ol class=""><li id="967f" class="ld le hi jl b jm jn jp jq js lf jw lg ka lh ke li lj lk ll bi translated"><strong class="jl hj">线性回归:</strong>用于寻找目标与一个或多个预测值之间的线性关系。主要想法是确定一条最符合数据的线。最佳拟合线是预测误差最小的线。这种算法不是很灵活，并且有很高的偏差。线性回归也很容易受到异常值的影响，因为它试图最小化误差平方和。</li></ol><pre class="iy iz ja jb fd lr ls lt lu aw lv bi"><span id="54d3" class="lw kh hi ls b fi lx ly l lz ma">lm = LinearRegression()<br/>lm.fit(X_train,y_train)<br/>y_pred=np.round(lm.predict(X_test),2)<br/>lm_rmse=np.sqrt(mean_squared_error(y_pred, y_test))<br/>lm_train_rmse=np.sqrt(mean_squared_error(lm.predict(X_train), y_train))<br/>lm_variance=abs(lm_train_rmse - lm_rmse)<br/>print("Test RMSE for Linear Regression is ",lm_rmse)<br/>print("Train RMSE for Linear Regression is ",lm_train_rmse)<br/>print("Variance for Linear Regression is ",lm_variance)</span></pre><p id="841d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">线性回归模型的检验RMSE为8.14，训练RMSE为8.20。</strong>该模型是对基线预测的改进。尽管方差很低(0.069)，但这个模型的错误率仍然很高。接下来让我们尝试一个更复杂的模型。</p><p id="9511" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">2.<strong class="jl hj">随机森林</strong></p><p id="40e3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">随机森林比线性回归模型灵活得多。这意味着更低的偏差，它可以更好地拟合数据。复杂的模型通常会记住底层数据，因此不能很好地概括。参数调整用于避免这个问题。</p><pre class="iy iz ja jb fd lr ls lt lu aw lv bi"><span id="34e4" class="lw kh hi ls b fi lx ly l lz ma">rf = RandomForestRegressor(n_estimators = 100, random_state = 883,n_jobs=-1)<br/>rf.fit(X_train,y_train)<br/>rf_pred= rf.predict(X_test)<br/>rf_rmse=np.sqrt(mean_squared_error(rf_pred, y_test))<br/>print("RMSE for Random Forest is ",rf_rmse)</span></pre><p id="63c8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">随机森林模型的验证数据RMSE为3.72，训练RMSE为1.41。</strong>在训练和验证RMSE中有巨大的变化，表明过度拟合。为了减少过度拟合，我们可以调整这个模型。</p><p id="bb2d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">3.<strong class="jl hj"> LightGBM </strong></p><p id="1e14" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">LightGBM是一种基于提升树的算法。轻量级GBM和其他基于树的算法的区别在于，轻量级GBM是逐叶增长的，而不是逐层增长的。该算法选择将导致最大增量损失的节点来分割。轻量级GBM速度非常快，运行时占用的内存很少，并且注重结果的准确性。</p><pre class="iy iz ja jb fd lr ls lt lu aw lv bi"><span id="ca70" class="lw kh hi ls b fi lx ly l lz ma">train_data=lgb.Dataset(X_train,label=y_train)<br/>param = {'num_leaves':31, 'num_trees':5000,'objective':'regression'}<br/>param['metric'] = 'l2_root'<br/>num_round=5000<br/>cv_results = lgb.cv(param, train_data, num_boost_round=num_round, nfold=10,verbose_eval=20, early_stopping_rounds=20,stratified=False)<br/>lgb_bst=lgb.train(param,train_data,len(cv_results['rmse-mean']))<br/>lgb_pred = lgb_bst.predict(X_test)<br/>lgb_rmse=np.sqrt(mean_squared_error(lgb_pred, y_test))<br/>print("RMSE for Light GBM is ",lgb_rmse)</span></pre><p id="de74" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">该模型在验证数据上给出了3.78的RMSE</strong>，但偏差高于随机森林。另一方面，这个模型的方差是0.48，而我们的随机森林模型是2.31。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mp"><img src="../Images/b9d440f5dd770e6128d29af7e7908edc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*u9pW8ozKHHoay6WHf4YzXg.png"/></div></div><figcaption class="mq mr et er es ms mt bd b be z dx translated">差分模型的偏差和方差</figcaption></figure><p id="bc2f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">由于LightGBM的错误率与Random Forest相当，并且方差更低，运行速度更快，因此我们将使用Light GBM作为我们的模型进行进一步分析</p><h2 id="a406" class="lw kh hi bd ki mb mc md km me mf mg kq js mh mi ks jw mj mk ku ka ml mm kw mn bi translated">2.特征工程和模型调整</h2><p id="ed07" class="pw-post-body-paragraph jj jk hi jl b jm ky ij jo jp kz im jr js la ju jv jw lb jy jz ka lc kc kd ke hb bi translated">特征工程是将原始数据转换为输入最终模型的特征的过程。目的是提高模型的准确性。拥有好的特性意味着我们可以使用简单的模型来产生更好的结果。好的特征描述了数据中固有的结构。</p><p id="da4c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">正如在第1部分中所讨论的，我们将使用在EDA阶段确定的特征，如离机场的接送距离、离每个区的接送距离(接送是来自还是去往机场，以及接送来自哪个区)。</p><p id="dd57" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">将上述相同的轻型GBM模型应用于该特征工程数据，得出RMSE为3.641(从3.78下降)，方差为0.48。</strong></p><p id="026b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">下一步是调整这个模型。好的模型具有较低的偏差和方差(以避免过度拟合)。LightGBM中可以优化以减少过拟合的几个重要特性是:</p><ol class=""><li id="8420" class="ld le hi jl b jm jn jp jq js lf jw lg ka lh ke li lj lk ll bi translated"><strong class="jl hj"> max_depth </strong>:表示树的最大深度。由于LightGBM在没有调优的情况下遵循逐叶增长，因此与其他基于树的算法相比，其深度要高得多。</li><li id="c7d1" class="ld le hi jl b jm lm jp ln js lo jw lp ka lq ke li lj lk ll bi translated"><strong class="jl hj">子样本</strong>:这表示在每次迭代中使用数据的多少部分，它用于加速算法和控制过度拟合。</li><li id="eab0" class="ld le hi jl b jm lm jp ln js lo jw lp ka lq ke li lj lk ll bi translated"><strong class="jl hj"> colsample_bytree </strong>:这是在树构建过程的每次迭代中要使用的部分特征。</li><li id="9b24" class="ld le hi jl b jm lm jp ln js lo jw lp ka lq ke li lj lk ll bi translated"><strong class="jl hj"> min_child_samples: </strong>这表示一个叶节点中可以出现的最小样本数。这有助于控制过度拟合。</li></ol><p id="fa4b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我已经用Python中的<em class="mu"> hyperopt </em>库调优了模型。<strong class="jl hj"> <em class="mu"> Hyperopt </em>是一个超参数搜索包，它实现了各种搜索算法，以在搜索空间内找到最佳的超参数集。</strong>要使用<em class="mu"> Hyperopt，</em>我们必须指定一个目标/损失函数来最小化搜索空间和试验数据库(可选，MongoTrials可用于并行搜索)。对于我们的问题，目标是最小化RMSE和确定最佳参数集。我们将使用hyperopt调整max_depth、subsample和colsample_bytree参数。</p><p id="3b5a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在给定一组参数的情况下，用于调整的目标函数是最小化LightGBM回归器中的RMSE。</p><p id="7306" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">搜索空间定义了给定参数可以取的一组值。在定义了目标函数和搜索空间之后，我们运行100次试验，并对验证数据的试验结果进行评估，以确定最佳参数。</p><pre class="iy iz ja jb fd lr ls lt lu aw lv bi"><span id="1a36" class="lw kh hi ls b fi lx ly l lz ma">def objective(space):</span><span id="9c64" class="lw kh hi ls b fi mv ly l lz ma">clf = lgb.LGBMRegressor(<br/>          objective = 'regression',<br/>          n_jobs = -1, # Updated from 'nthread'<br/>          verbose=1,<br/>          boosting_type='gbdt',<br/>        num_leaves=60,<br/>        bagging_freq=20,<br/>       subsample_freq=100,<br/>    max_depth=int(space['max_depth']),<br/>    subsample=space['subsample'],<br/>        n_estimators=5000,<br/>    colsample_bytree=space['colsample'])<br/>          #metric='l2_root')</span><span id="20bf" class="lw kh hi ls b fi mv ly l lz ma">eval_set=[( X_train, y_train), ( X_test,y_test)]</span><span id="3cdf" class="lw kh hi ls b fi mv ly l lz ma">clf.fit(X_train, np.array(y_train),<br/>            eval_set=eval_set,eval_metric='rmse',<br/>            early_stopping_rounds=20)</span><span id="b3df" class="lw kh hi ls b fi mv ly l lz ma">pred = clf.predict(X_test)<br/>    rmse = np.sqrt(mean_squared_error(y_test, pred))<br/>    print("SCORE:", rmse)</span><span id="76b6" class="lw kh hi ls b fi mv ly l lz ma">return{'loss':rmse, 'status': STATUS_OK }</span><span id="417c" class="lw kh hi ls b fi mv ly l lz ma">space ={<br/>        'max_depth': hp.quniform("x_max_depth", 5, 30, 3),<br/>       <br/>        'subsample': hp.uniform ('x_subsample', 0.8, 1),<br/>        'colsample':hp.uniform ('x_colsample', 0.3, 1)<br/>    }</span><span id="f2ab" class="lw kh hi ls b fi mv ly l lz ma">trials = Trials()<br/>best = fmin(fn=objective,<br/>            space=space,<br/>            algo=tpe.suggest,<br/>            max_evals=100,<br/>            trials=trials)</span><span id="f70f" class="lw kh hi ls b fi mv ly l lz ma">print(best)</span></pre><p id="86de" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们通过特征工程为LightGBM获得的最佳参数是:</p><pre class="iy iz ja jb fd lr ls lt lu aw lv bi"><span id="0452" class="lw kh hi ls b fi lx ly l lz ma">{'max_depth': 24.0, 'subsample': 0.9988461076307639, 'colsample_bytree': 0.38429620148564814}</span></pre><p id="aac8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">使用这些参数，LightGBM模型得出的RMSE为3.633，方差为0.44。</strong>我们可以通过调整其他参数(如<em class="mu"> num_leaves </em>)以及添加L1和L2正则化参数来进一步改进该模型。</p><h1 id="2e50" class="kg kh hi bd ki kj kk kl km kn ko kp kq io kr ip ks ir kt is ku iu kv iv kw kx bi translated"><strong class="ak">结尾注释</strong></h1><p id="4550" class="pw-post-body-paragraph jj jk hi jl b jm ky ij jo jp kz im jr js la ju jv jw lb jy jz ka lc kc kd ke hb bi translated">特征工程显著提高了我们机器学习模型的预测能力。另一种提高模型准确性的方法是增加训练数据量，和/或建立集合模型。如果数据中有很多维度(特征)，降维技术也可以帮助提高模型的准确性。</p><p id="b034" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我希望这篇文章对你有所帮助。如果您有任何问题或反馈，请随时在下面的评论区联系我们。</p></div></div>    
</body>
</html>