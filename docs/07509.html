<html>
<head>
<title>Feature Selection For Dimensionality Reduction(Wrapper Method)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维的特征选择(包装方法)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-selection-for-dimensionality-reduction-wrapper-method-9979fffd0166?source=collection_archive---------11-----------------------#2020-06-28">https://medium.com/analytics-vidhya/feature-selection-for-dimensionality-reduction-wrapper-method-9979fffd0166?source=collection_archive---------11-----------------------#2020-06-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/390274081e50036bc9c8829c0eefce7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*xZIkF_FzxMW77JR0.jpeg"/></div></div></figure><p id="adfa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi jo translated"><span class="l jp jq jr bm js jt ju jv jw di">在</span>机器学习中选择数据中的重要特征是整个周期的重要部分。</p><p id="3f2f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">传递带有不相关要素的数据可能会影响模型的性能，因为模型会学习传递给它的不相关要素。</p><h1 id="914d" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">特征选择的需要:</h1><ul class=""><li id="3c1f" class="kv kw hi is b it kx ix ky jb kz jf la jj lb jn lc ld le lf bi translated">它有助于简化模型，使它们更容易和更快地训练。</li><li id="3288" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lc ld le lf bi translated">减少培训次数。</li><li id="126d" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lc ld le lf bi translated">帮助避免维度的<a class="ae ll" href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" rel="noopener ugc nofollow" target="_blank">诅咒</a>，</li><li id="da38" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lc ld le lf bi translated">通过减少<a class="ae ll" rel="noopener" href="/analytics-vidhya/over-fitted-and-under-fitted-models-f5c96e9ac581">过拟合</a>(形式上，减少<a class="ae ll" rel="noopener" href="/analytics-vidhya/bias-variance-tradeoff-2b19a4926e7d">方差</a>)来增强通用性</li></ul><h1 id="0ede" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">特征选择方法</h1><h1 id="fcd9" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku bi translated">有三种一般的特征选择方法:</h1><ol class=""><li id="ef9e" class="kv kw hi is b it kx ix ky jb kz jf la jj lb jn lm ld le lf bi translated"><a class="ae ll" rel="noopener" href="/analytics-vidhya/feature-selection-for-dimensionality-reduction-filter-method-201cc9eaa3b5">过滤方法</a></li><li id="2fb0" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lm ld le lf bi translated">包装方法</li><li id="77db" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lm ld le lf bi translated"><a class="ae ll" rel="noopener" href="/@abhigyan.singh282/feature-selection-for-dimensionality-reduction-embedded-method-e05c74014aa">嵌入方法</a></li></ol><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ln"><img src="../Images/a2e5e4541fb45fb3092ea60bf69c9d9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*49Hg1ygU7fzCGbJq.jpeg"/></div></div></figure><h2 id="ee44" class="ls jy hi bd jz lt lu lv kd lw lx ly kh jb lz ma kl jf mb mc kp jj md me kt mf bi translated">包装方法</h2><ol class=""><li id="dd13" class="kv kw hi is b it kx ix ky jb kz jf la jj lb jn lm ld le lf bi translated">包装器方法基于贪婪搜索算法，因为它们评估所有可能的特征组合，并为特定的机器学习算法选择产生最佳结果的组合。</li><li id="5f04" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lm ld le lf bi translated">这种方法的缺点是测试所有可能的特征组合在计算上非常昂贵，特别是当特征集非常大时。</li><li id="817e" class="kv kw hi is b it lg ix lh jb li jf lj jj lk jn lm ld le lf bi translated">不利的一面是，这些特征集可能并不是所有其他机器学习算法的最佳选择。</li></ol><p id="ce34" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">特征选择的包装方法可分为三类:</strong></p><ul class=""><li id="6af9" class="kv kw hi is b it iu ix iy jb mg jf mh jj mi jn lc ld le lf bi translated"><strong class="is hj">向前一步特征选择:<br/> → </strong>向前一步特征选择从评估每个单独的特征开始，并选择导致最佳执行的所选算法模型。<br/>→最佳完全取决于定义的评估标准(AUC、预测准确性、RMSE等)。).<br/>→前向选择是一种迭代方法，我们从模型中没有特征开始。<br/>→在每次迭代中，我们不断添加最能改善我们模型的特性，直到添加新变量不能改善模型的性能。<br/> *在第一步中，针对每个特征单独评估分类器的性能，并从所有特征中选择性能最佳的特征。<br/> *在第二步中，结合所有剩余特征尝试模型的第一个选定特征，并选择产生最佳算法性能的两个特征的组合。<br/> *从不同的特征组合中评估和选择最佳执行方法的过程重复进行，直到选定一定数量的特征。</li></ul><pre class="lo lp lq lr fd mj mk ml mm aw mn bi"><span id="505e" class="ls jy hi mk b fi mo mp l mq mr">from mlxtend.feature_selection import SequentialFeatureSelector<br/>from sklearn.linear_model import LinearRegression,LogisticRegression</span><span id="165c" class="ls jy hi mk b fi ms mp l mq mr">#FOR REGRESSION MODEL<br/>feature_select = SequentialFeatureSelector(LinearRegression(),<br/>                                           k_features=,<br/>                                           forward=True,<br/>                                           floating=False,<br/>                                           scoring='r2',<br/>                                           cv=)<br/>feature_select.fit(x,y)<br/>feature_select.k_feature_names_<br/></span><span id="c525" class="ls jy hi mk b fi ms mp l mq mr">#FOR CLASSIFICATION MODEL<br/>feature_select = SequentialFeatureSelector(LogisticRegression(),<br/>                                           k_features=,<br/>                                           forward=True,<br/>                                           floating=False,<br/>                                           scoring='roc_auc',<br/>                                           cv=)<br/>feature_select.fit(x,y)<br/>feature_select.k_feature_names_</span></pre><blockquote class="mt"><p id="9640" class="mu mv hi bd mw mx my mz na nb nc jn dx translated">可以使用任何模型来代替线性和逻辑回归。<br/> <strong class="ak"> k_features: </strong>为需要选择的特征数量的数值。<br/> <strong class="ak">评分:</strong>需要通过模型评估的指标。<br/> <strong class="ak"> cv: </strong> k倍交叉验证</p></blockquote><ul class=""><li id="4603" class="kv kw hi is b it nd ix ne jb nf jf ng jj nh jn lc ld le lf bi translated"><strong class="is hj">后退特征选择:<br/>→后退特征选择</strong>与前进特征选择相反，正如您可能已经猜到的那样，从整个特征集开始向后工作，删除特征以找到预定义大小的最佳子集。<br/> *在第一步中，移除一个特征，这是通过移除一个特征并计算模型的性能来完成的。基本上，这些功能是以循环方式删除的。<br/> *这个过程一直持续到剩下指定数量的特征。</li></ul><pre class="lo lp lq lr fd mj mk ml mm aw mn bi"><span id="7d19" class="ls jy hi mk b fi mo mp l mq mr">from mlxtend.feature_selection import SequentialFeatureSelector<br/>from sklearn.linear_model import LinearRegression,LogisticRegression</span><span id="8583" class="ls jy hi mk b fi ms mp l mq mr">#FOR REGRESSION MODEL<br/>feature_select = SequentialFeatureSelector(LinearRegression(),<br/>                                           k_features=,<br/>                                           forward=False,<br/>                                           floating=False,<br/>                                           scoring='r2',<br/>                                           cv=)<br/>feature_select.fit(x,y)<br/>feature_select.k_feature_names_<br/></span><span id="cb66" class="ls jy hi mk b fi ms mp l mq mr">#FOR CLASSIFICATION MODEL<br/>feature_select = SequentialFeatureSelector(LogisticRegression(),<br/>                                           k_features=,<br/>                                           forward=False,<br/>                                           floating=False,<br/>                                           scoring='roc_auc',<br/>                                           cv=)<br/>feature_select.fit(x,y)<br/>feature_select.k_feature_names_</span></pre><blockquote class="mt"><p id="42ad" class="mu mv hi bd mw mx my mz na nb nc jn dx translated">仅仅将函数中的向前取为假就给出了向后选择</p></blockquote><ul class=""><li id="b612" class="kv kw hi is b it nd ix ne jb nf jf ng jj nh jn lc ld le lf bi translated"><strong class="is hj">穷举特征选择:<br/> → </strong>这是一种最贪婪的算法，旨在找到性能最佳的特征子集。<br/> →它重复创建模型，并在每次迭代中保留最佳或最差的性能特征。<br/>→针对数据集中所有可能的特征组合评估算法的性能。<br/>→然后根据消除的顺序对特征进行排序。因为它检查所有可能的组合，所以计算量非常大，并且通常避免使用。</li></ul><pre class="lo lp lq lr fd mj mk ml mm aw mn bi"><span id="b78a" class="ls jy hi mk b fi mo mp l mq mr">from mlxtend.feature_selection import ExhaustiveFeatureSelector<br/>from sklearn.linear_model import LinearRegression,LogisticRegression</span><span id="38ea" class="ls jy hi mk b fi ms mp l mq mr">#FOR REGRESSION MODELS<br/>feature_select = ExhaustiveFeatureSelector(LinearRegression(),             <br/>                                           min_features=,<br/>                                           max_features=,<br/>                                           scoring='R2',<br/>                                           print_progress=<strong class="mk hj">True</strong>,<br/>                                           cv=5)<br/>feature_select = feature_select.fit(X, y)<br/>print('Best accuracy score: %.2f' %feature_select.best_score_) <br/>print('Best subset (indices):', feature_select.best_idx_) <br/>print('Best subset (name):', feature_select.best_feature_names_)</span><span id="e2a9" class="ls jy hi mk b fi ms mp l mq mr">#FOR CLASSIFICATION MODELS<br/>feature_select = ExhaustiveFeatureSelector(LogisticRegression(),             <br/>                                           min_features=,<br/>                                           max_features=,<br/>                                           scoring='roc_auc',<br/>                                           print_progress=<strong class="mk hj">True</strong>,<br/>                                           cv=5)<br/>feature_select = feature_select.fit(X, y)<br/>print('Best accuracy score: %.2f' %feature_select.best_score_) <br/>print('Best subset (indices):', feature_select.best_idx_) <br/>print('Best subset (name):', feature_select.best_feature_names_)</span></pre><blockquote class="mt"><p id="abdd" class="mu mv hi bd mw mx my mz na nb nc jn dx translated">包装器方法在计算上非常昂贵，因为特性是以循环方式传递的。</p><p id="20f3" class="mu mv hi bd mw mx ni nj nk nl nm jn dx translated">要执行包装器方法，请确保您已经编码了所有的分类特征。</p></blockquote><p id="b221" class="pw-post-body-paragraph iq ir hi is b it nd iv iw ix ne iz ja jb nn jd je jf no jh ji jj np jl jm jn hb bi translated">下周我们将讨论特征选择的嵌入式方法。<br/> <strong class="is hj">快乐学习！！！！</strong></p></div><div class="ab cl nq nr gp ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="hb hc hd he hf"><p id="6d45" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">喜欢我的文章？请为我鼓掌并分享它，因为这将增强我的信心。此外，我每周日都会发布新文章，所以请保持联系，以了解数据科学和机器学习基础系列的未来文章。</p><p id="4f41" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另外，请务必通过LinkedIn 与我联系。</p><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nx"><img src="../Images/dba0b020b5afbae04f8b00aaf40950fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GLb96EsuEkqyoKp3"/></div></div><figcaption class="ny nz et er es oa ob bd b be z dx translated">马库斯·斯皮斯克在<a class="ae ll" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure></div></div>    
</body>
</html>