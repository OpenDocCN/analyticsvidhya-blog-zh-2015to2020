<html>
<head>
<title>Linear Regression Concepts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归概念</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-900a7f4ee2ba?source=collection_archive---------19-----------------------#2020-09-14">https://medium.com/analytics-vidhya/linear-regression-900a7f4ee2ba?source=collection_archive---------19-----------------------#2020-09-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="ab fe cl ij"><img src="../Images/2eff2fcce4ff6b2fc4fe9198dd5f255f.png" data-original-src="https://miro.medium.com/v2/format:webp/1*6F2Cb80RiJ1MmeRxzE10nw.png"/></div></figure><p id="1ac2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果你正在学习机器学习，那么线性回归是第一个也是最简单的机器学习算法。让我们开始吧。</p><p id="0ef9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">线性回归也称为普通最小二乘(OLS)算法。这是一个有监督的机器学习算法，预测连续的数值。</p><p id="e2e7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们借助1个特征和1个目标变量来理解线性回归，它也称为简单线性回归。在简单的线性回归中，模型试图将给定的特征映射到目标变量，并使用直线方程来映射:<strong class="io hj"> y = mX + C </strong></p><p id="aa8f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">m =斜率，C =常数，X=特征，y =目标变量</p><p id="a596" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以线性回归的最终目的是找到这条线，而找到这条线的模型需要找到m和c的值。</p><figure class="jl jm jn jo fd ii er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es jk"><img src="../Images/86bc0d77778bfbc6a36f67573cf730de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rMLYa1B6YcN1syN6.jpg"/></div></div></figure><p id="e092" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一旦模型找到m和C的值，它就可以绘制如上图所示的线。现在，对于x轴上的任何新数据点，我们可以使用下面这条线在y轴上找到该数据点的预测输出。</p><h2 id="3a3d" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ix ke kf kg jb kh ki kj jf kk kl km kn bi translated">剩余:</h2><p id="989f" class="pw-post-body-paragraph im in hi io b ip ko ir is it kp iv iw ix kq iz ja jb kr jd je jf ks jh ji jj hb bi translated">实际数据点和预测线之间的差异。我们可以说这是模型造成的错误。</p><h2 id="0bb9" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ix ke kf kg jb kh ki kj jf kk kl km kn bi translated">成本函数:</h2><p id="2613" class="pw-post-body-paragraph im in hi io b ip ko ir is it kp iv iw ix kq iz ja jb kr jd je jf ks jh ji jj hb bi translated">有一个代价函数的概念是机器学习找出在所有数据点上模型产生了多少误差。</p><p id="93ea" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">有许多成本函数来测量该模型产生的总误差。其中一些如下:</p><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es kt"><img src="../Images/657d17ea2d4fabaf5e1eda3b0e9b90c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*mqZh4u8QIYepCuDChIFE9Q.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">绝对平均误差</figcaption></figure><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es ky"><img src="../Images/f4fbe880492c5fba946d061e8ffb5af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*Y9Xlc9dqkkUakjAhhlhQrg.png"/></div></figure><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es kz"><img src="../Images/3b570d37d03ffe3f7d790a83c30fd818.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*GxCU3joquKknBiBut32hgQ.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">均方根误差</figcaption></figure><p id="76f3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">到目前为止，我们知道该模型的最终目标是找到m和C的值，使得均方误差最小。所以让我们试着理解模型是如何找到这些值的。</p><h1 id="2f8e" class="la ju hi bd jv lb lc ld jz le lf lg kd lh li lj kg lk ll lm kj ln lo lp km lq bi translated">梯度下降:</h1><p id="5006" class="pw-post-body-paragraph im in hi io b ip ko ir is it kp iv iw ix kq iz ja jb kr jd je jf ks jh ji jj hb bi translated">梯度下降是优化任何成本函数的优化算法。在线性回归的情况下，它优化(减少)成本函数(MSE)。</p><p id="790a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">步骤1:用任意小的随机值初始化m和C的值。</p><p id="ca55" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">步骤2:使用MSE计算成本。</p><p id="31e7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">步骤3:使用以下公式更新m和C的值。</p><p id="9cbc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">m(t+1) = m(t) - learning_rate *导数(MSE)<br/>C(t+1)= C(t)-Learning _ rate *导数(MSE)</p><p id="fe24" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">重复步骤2和3，直到时间MSE值变化不大。</p><figure class="jl jm jn jo fd ii er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es lr"><img src="../Images/4d0a0c045acc006f50b601263df955a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RJD2eC5oKv92w_IALDN8hA.png"/></div></div></figure><h1 id="be15" class="la ju hi bd jv lb lc ld jz le lf lg kd lh li lj kg lk ll lm kj ln lo lp km lq bi translated">线性回归的假设</h1><p id="1c85" class="pw-post-body-paragraph im in hi io b ip ko ir is it kp iv iw ix kq iz ja jb kr jd je jf ks jh ji jj hb bi translated">线性回归需要满足一些假设，否则线性模型给出的输出是不可信的。这是面试中很常见的问题。</p><p id="7361" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">简单线性回归:</strong>当数据只有一个独立特征时，称为简单线性回归。</p><p id="ff36" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">多元线性回归:</strong>当数据有1个以上的独立特征时，称为多元线性回归。</p><p id="30a2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将借助简单线性回归来理解线性回归的假设。</p><h1 id="4d00" class="la ju hi bd jv lb lc ld jz le lf lg kd lh li lj kg lk ll lm kj ln lo lp km lq bi translated">1.X和y变量之间存在线性关系。</h1><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es ls"><img src="../Images/5c10461108c979b99a49acf9db1d9e51.png" data-original-src="https://miro.medium.com/v2/resize:fit:54/0*MRKmDnz_Gy6oPt6t"/></div></figure><figure class="jl jm jn jo fd ii er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es lt"><img src="../Images/b31d41b4a46fe15b0431ad0d8e9d610e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JUyrCc_O1eIu3pM1.png"/></div></div></figure><p id="ae1c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这种假设认为独立和从属特征具有线性关系。为了检验这个假设，我们可以使用散点图，散点图应该看起来像上面的左图。</p><h1 id="40cb" class="la ju hi bd jv lb lc ld jz le lf lg kd lh li lj kg lk ll lm kj ln lo lp km lq bi translated">2.误差项呈正态分布。</h1><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es ls"><img src="../Images/195ce19e5b21f24c5b5aadd03cca8709.png" data-original-src="https://miro.medium.com/v2/resize:fit:54/0*0c8TLOrM0_neu95-"/></div></figure><figure class="jl jm jn jo fd ii er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es lu"><img src="../Images/b55ec8913dc369f3740f9dfb3ecd6a7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*IyW2IFQGnDAW4XjU.png"/></div></div></figure><p id="1cb6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这个假设认为误差项是正态分布的。这里预测误差减去实际目标。要检查这一假设，请根据数据拟合模型并进行预测。现在计算误差，并画出这个误差的分布(直方图)，这个分布应该看起来像正态分布。</p><p id="44e5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">3.<strong class="io hj">误差项相互独立</strong></p><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es ls"><img src="../Images/4f5f2d2e4666cd0f165850cf6fa3adc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:54/0*UMl8jyeDS6AJorxB"/></div></figure><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es lv"><img src="../Images/ea37ddb42f1949f6d7b8dca0d6611ce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*_9p3PRpzzAFrNChl.png"/></div></figure><p id="dfa9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了检验这个假设，在目标变量和误差项之间画一个散点图。散点图不应显示明显的模式。</p><h1 id="20f4" class="la ju hi bd jv lb lc ld jz le lf lg kd lh li lj kg lk ll lm kj ln lo lp km lq bi translated">4.同方差性:误差项具有恒定方差。</h1><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es ls"><img src="../Images/5965f9906034155543f97e765abb39bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:54/0*g9esqTUEZ0jBaXXS"/></div></figure><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es lw"><img src="../Images/3804ae179ec01b0601937beb8946c32e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/0*D72LaFnkkUFzZgUh.png"/></div></figure><p id="9d83" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">要检查这一点，在独立特征和目标特征之间绘制一个散点图，然后在同一轴上，在独立特征和预测之间绘制一个散点图。您应该会得到一个类似上面左图的图形。</p><h1 id="0895" class="la ju hi bd jv lb lc ld jz le lf lg kd lh li lj kg lk ll lm kj ln lo lp km lq bi translated">5.最小多重共线性:</h1><p id="0f8a" class="pw-post-body-paragraph im in hi io b ip ko ir is it kp iv iw ix kq iz ja jb kr jd je jf ks jh ji jj hb bi translated">多重共线性意味着1个要素与其他要素相关，我们希望最小化多重共线性。很明显，这个问题出现在多个线性回归中，因为它包含不止一个特征。为了检验这一假设，使用VIF(方差膨胀系数)</p><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es ls"><img src="../Images/869f81963f939e3e39d78460a6339446.png" data-original-src="https://miro.medium.com/v2/resize:fit:54/0*HQ6KZSutZk0awchK"/></div></figure><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es lx"><img src="../Images/08028188afe7cd3865b6b7018b35b9a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/0*R5DKI0ZIZyJpDj_i.png"/></div></figure><p id="4723" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">VIF值越高，多重共线性越高。在大多数情况下，VIF值不应大于10。</p><h1 id="d173" class="la ju hi bd jv lb lc ld jz le lf lg kd lh li lj kg lk ll lm kj ln lo lp km lq bi translated"><strong class="ak">模型评估:</strong></h1><p id="e2e1" class="pw-post-body-paragraph im in hi io b ip ko ir is it kp iv iw ix kq iz ja jb kr jd je jf ks jh ji jj hb bi translated">你训练了线性回归模型，现在如何检验这个模型好不好？</p><p id="a14a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">决定系数:</strong></p><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es ly"><img src="../Images/b929ffef5a1033c5fe1c687015d42f71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*_HbexqOfMWKn66BYhV7m1Q.png"/></div></figure><p id="281d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">r称为决定系数或拟合优度。这个值告诉我们模型与数据的吻合程度，或者模型解释了数据中的多少差异。R值越高，模型越好。</p><p id="23a7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">R的问题在于它的值随着特征数量的增加而增加。因此，如果您在数据集中添加一个新要素，并且如果该要素没有用，那么R的值会增加。</p><p id="2bc1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了解决这个问题，有了调整R的概念</p><figure class="jl jm jn jo fd ii er es paragraph-image"><div class="er es ly"><img src="../Images/26331c54c70db6872db0741cfb9572b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*Q1M00PX1g_gVbHxWffG1rA.png"/></div></figure><p id="844a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这些都是你应该知道的线性回归中的重要概念。如果我错过了任何概念，请在下面随意评论。</p><p id="0bdc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="lz">感谢您的配合！</em>T9】</strong></p></div></div>    
</body>
</html>