<html>
<head>
<title>What happens when you remove first or the last tree from your Gradient Boosting Ensemble?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">当你把第一棵树或者最后一棵树从你的梯度推进系综中移除时会发生什么？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/what-happens-when-you-remove-first-or-the-last-tree-from-your-gradient-boosting-ensemble-315efaa85d5f?source=collection_archive---------7-----------------------#2020-07-16">https://medium.com/analytics-vidhya/what-happens-when-you-remove-first-or-the-last-tree-from-your-gradient-boosting-ensemble-315efaa85d5f?source=collection_archive---------7-----------------------#2020-07-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/a62e6dd2f652ec40507d33bc367727ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EM2EGRiJbUcLrG_r"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">谢尔盖·阿库利奇在<a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="8b45" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">这一切都始于两位杰出的科学家迈克尔·科恩斯和莱斯利·加百利·瓦利安特早在1989年提出了一个简单的问题<em class="ka">“一组弱学习者能创造出一个强学习者吗？”</em>。这是非常直观的，有时一些弱球员变得不可或缺，互相补充，形成一支非常强大的球队，以弗格森爵士和他的曼联为例。这是否也适用于机器学习和统计学领域。然而，这个问题并没有长期得不到解答，因为一年后，Robert Schapire发表了一篇论文，以积极的方式回答了这个问题，正是他的工作奠定了这一奇妙技术的基础，这一技术深受全球kagglers和数据科学家的喜爱。</p><p id="9538" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">从形式上讲，强化是一种将许多弱学习者结合起来创造强学习者的技术。当你这样听的时候，就好像蚂蚁在组队搬石头什么的。这里的“学习者”是一个比随机猜测表现稍好的模型。所以在回归的情况下，它可能是数据的平均值，很简单，不是吗？</p><p id="f150" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">在这篇文章中，我们将讨论梯度推进决策树是如何工作的，以及如果我们从计算中移除第一棵树或最后一棵树的贡献，我们的模型会发生什么。</p><h2 id="9066" class="kb kc hy bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv bi translated"><strong class="ak">决策树更新器</strong></h2><p id="6723" class="pw-post-body-paragraph jc jd hy je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hb bi translated">决策树的核心是一个足够简单的分类器，它利用分治法，根据数据点在空间中的概率递归地分割特征空间。每次分割都平行于超维度中的一个轴。如果我们把我们的决策树模型画成流程图，我们会发现一个由'<em class="ka"> if和else' </em>语句组成的树，因此被命名为决策树。当我们使用线性模型并且我们有一个编码为数字的分类变量时，比如说一个描述假设数据集中位置的特征，编码可能看起来像{纽约:1，旧金山:2，德里:3}。线性模型将假设存在一个顺序，因为数字1、2和3具有序数依赖性。因此，为了将这些信息安全地合并到我们的线性模型中，我们必须“一次热编码”该特征，这似乎是一种可行的方法，但有时当我们的特征中有太多不同的值时，我们最终会有许多列，因此显著增加了我们的计算时间。这就是基于树的模型派上用场的地方，使用基于树的模型，我们可以很容易地摆脱顺序依赖。假设在我们假设的例子中，2映射到一个特定的类，1和3映射到另一个类。使用“if — else ”,我们可以轻松地创建一个简单、快速且相当准确的模型。</p><h2 id="3623" class="kb kc hy bd kd ke kf kg kh ki kj kk kl jn km kn ko jr kp kq kr jv ks kt ku kv bi translated"><strong class="ak">梯度推进决策树</strong></h2><p id="1cb0" class="pw-post-body-paragraph jc jd hy je b jf kw jh ji jj kx jl jm jn ky jp jq jr kz jt ju jv la jx jy jz hb bi translated">现在，与随机森林不同，随机森林的最终输出是由系综中的树产生的输出的模式或均值，其中每棵树都独立于另一棵树，梯度提升以逐阶段的方式工作，其中每个树学习器都依赖于其先前的树。随机森林生成大量试图捕捉不相关特征的树，而每个树学习者在梯度提升中的学习依赖于前一个树的输出。</p><figure class="lc ld le lf fd hk er es paragraph-image"><div class="er es lb"><img src="../Images/84d5a0543c14b75b49967513d2cf1928.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*KIL396FXuTS0kdZgYlRWnA.png"/></div></figure><p id="f838" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">现在让我们试着理解这个等式的含义。Fm表示模型的'm^th'阶段的输出。我们在模型的开始指定总m。这个等式假设如果我们有足够多的阶段，那么我们的输出将等于我们的目标“y”。hm(x)是我们试图拟合样本的模型(这里是决策树)。</p><figure class="lc ld le lf fd hk er es paragraph-image"><div class="er es lg"><img src="../Images/0fb35f61ad96e271bef8685c7da6c2d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*eOaKXqIMs_uJK3eN0QD5UA.png"/></div></figure><p id="658f" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">因此，梯度提升将使模型“h”与作为上述等式的RHS的剩余值相匹配，从而减少每个后续阶段中的误差。</p><p id="002c" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">所以最后的等式是</p><figure class="lc ld le lf fd hk er es paragraph-image"><div class="er es lh"><img src="../Images/bb47eda8e0cf4b6c8d368b391ae454c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*iYh-Ftgnn9QIqhmVhIR9Rg.png"/></div></figure><p id="f034" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">其中γ可以被认为是学习率，而常量值用于初始化模型。因此，在梯度增强中，最终模型被构建为一系列后续模型，其中每个模型试图拟合由它的先前模型计算的残差值，这简单地意味着每个模型拟合((目标)——(它的先前模型的误差))。</p><p id="5bd6" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">现在你可能已经预感到标题中问题的答案了。如果你认为砍掉第一棵树比砍掉最后一棵树更具灾难性，那么是的，你完全正确。让我们在下一节用一个简单的例子来检验我们的理论。</p><pre class="lc ld le lf fd li lj lk ll aw lm bi"><span id="e944" class="kb kc hy lj b fi ln lo l lp lq">import numpy as np<br/>from sklearn.model_selection import train_test_split</span><span id="990a" class="kb kc hy lj b fi lr lo l lp lq">X_all = np.random.randn(5000, 2)</span><span id="66c2" class="kb kc hy lj b fi lr lo l lp lq">y_all = ((X_all[:, 0]/X_all[:, 1]) &gt; 1.5)*2 - 1</span><span id="e460" class="kb kc hy lj b fi lr lo l lp lq">X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=0.5, random_state=42)</span></pre><p id="eb3f" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">因此，我们将创建一个从随机分布中采样的二维数据集。此外，我们将留出50%用于测试集。让我们首先尝试一个单一的决策树桩，并检查它的表现如何。</p><blockquote class="ls lt lu"><p id="b6c7" class="jc jd ka je b jf jg jh ji jj jk jl jm lv jo jp jq lw js jt ju lx jw jx jy jz hb bi translated">虚拟化数据集</p></blockquote><figure class="lc ld le lf fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ly"><img src="../Images/c532adaabbbe86f7be6316fa2893f679.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DSe-sQQVjHXLNe5td52xKQ.png"/></div></div></figure><pre class="lc ld le lf fd li lj lk ll aw lm bi"><span id="8412" class="kb kc hy lj b fi ln lo l lp lq">from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.metrics import log_loss<br/>clf = DecisionTreeClassifier(max_depth=1)</span><span id="b72c" class="kb kc hy lj b fi lr lo l lp lq">clf.fit(X_train, y_train)<br/>y_pred = clf.predict_proba(X_test)[:, 1]</span><span id="c845" class="kb kc hy lj b fi lr lo l lp lq">print ('Accuracy for a single decision stump: {}'.format(clf.score(X_test, y_test)))</span></pre><blockquote class="ls lt lu"><p id="38d1" class="jc jd ka je b jf jg jh ji jj jk jl jm lv jo jp jq lw js jt ju lx jw jx jy jz hb bi translated">单个决策树桩的准确度:0.8224 <br/>测试对数损失:0.00086868686</p></blockquote><p id="0472" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">现在，让我们安装一个梯度提升树分类器，并将其与我们的简单模型进行比较。</p><pre class="lc ld le lf fd li lj lk ll aw lm bi"><span id="1560" class="kb kc hy lj b fi ln lo l lp lq">from sklearn.ensemble import GradienBoostingClassifier<br/>clf = GradientBoostingClassifier(n_estimators=5000, learning_rate=0.01, max_depth=3, random_state=0)</span><span id="1368" class="kb kc hy lj b fi lr lo l lp lq">clf.fit(X_train, y_train)<br/>y_pred = clf.predict_proba(X_test)[:, 1]</span><span id="0635" class="kb kc hy lj b fi lr lo l lp lq">print('Accuracy for a GBM: {}'.format(clf.score(X_test, y_test)))<br/>print("Test logloss: {}".format(log_loss(y_test, y_pred)))</span></pre><blockquote class="ls lt lu"><p id="8ff3" class="jc jd ka je b jf jg jh ji jj jk jl jm lv jo jp jq lw js jt ju lx jw jx jy jz hb bi translated">GBM的精度:0.9824 <br/>测试测井曲线损失:0.00008600001</p></blockquote><p id="4d1d" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">很明显，梯度推进模型比单个树执行得更好，现在让我们检查如果我们移除第一个树或最后一个树的输出会发生什么。</p><pre class="lc ld le lf fd li lj lk ll aw lm bi"><span id="cd31" class="kb kc hy lj b fi ln lo l lp lq">def sigmoid(x):<br/>    return 1 / (1 + np.exp(-x))</span><span id="6135" class="kb kc hy lj b fi lr lo l lp lq">def compute_loss(y_true, scores_pred):<br/>    return log_loss(y_true, sigmoid(scores_pred))</span></pre><p id="f808" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">因为我们使用原始分数，所以在计算log_loss本身之前，我们将包装log_loss并将sigmoid应用于我们的预测。</p><p id="439e" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">获取树的*决策函数*的累积和(cum_preds)。第I个元素是树0…i-1的和。我们不能使用staged_predict_proba，因为我们想要操纵原始分数(而不是概率)。最后才使用sigmoid将分数转换成概率。</p><pre class="lc ld le lf fd li lj lk ll aw lm bi"><span id="8983" class="kb kc hy lj b fi ln lo l lp lq">cum_preds = np.array([x for x in clf.staged_decision_function(X_test)])[:, :, 0]</span><span id="edb0" class="kb kc hy lj b fi lr lo l lp lq">print ("Logloss using all trees:           {}".format(compute_loss(y_test, cum_preds[-1, :])))</span><span id="1eac" class="kb kc hy lj b fi lr lo l lp lq">print ("Logloss using all trees but last:  {}".format(compute_loss(y_test, cum_preds[-2, :])))</span><span id="4d10" class="kb kc hy lj b fi lr lo l lp lq">print ("Logloss using all trees but first: {}".format(compute_loss(y_test, cum_preds[-1, :] - cum_preds[0, :])))</span></pre><blockquote class="ls lt lu"><p id="6911" class="jc jd ka je b jf jg jh ji jj jk jl jm lv jo jp jq lw js jt ju lx jw jx jy jz hb bi translated">使用所有树的对数损失:0.04533302136082253 <br/>使用除最后一棵树之外的所有树的对数损失:0.0453406808646</p></blockquote><p id="3483" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">综上所述，移除最后一棵树没有太大影响的原因是因为我们的模型中有5000个阶段，对于这样一个简单的例子来说已经足够高了。即使级数较少，去除最后一级的贡献也不会有相当大的影响，因为前面的级是完整的。但是除去第一棵树的贡献，我们的损失增加了0.02，这看起来并不多，但是如果这被应用到现实世界的问题中，变化将是巨大的。</p><p id="0856" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">第一棵树对GBM输出有相当大的影响，因为所有后续阶段(树)都是基于它构建的。如果我们提高学习速度，去掉第一棵树，模型就会爆炸。</p><p id="411c" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">希望您现在对GBMs及其工作原理有了更好的理解。GBM通常是kagglers的<em class="ka">到</em>型号。</p><blockquote class="ls lt lu"><p id="337d" class="jc jd ka je b jf jg jh ji jj jk jl jm lv jo jp jq lw js jt ju lx jw jx jy jz hb bi translated">如果你喜欢这本书，请给它一个掌声或者你知道50！</p></blockquote></div></div>    
</body>
</html>