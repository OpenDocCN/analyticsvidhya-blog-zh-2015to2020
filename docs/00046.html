<html>
<head>
<title>Movie Ratings Prediction Server using Flask and LSTM in Keras (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Keras中使用Flask和LSTM的电影分级预测服务器(第1部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/movie-ratings-prediction-server-using-flask-and-lstm-in-keras-part-1-90933e8dbf88?source=collection_archive---------0-----------------------#2018-08-11">https://medium.com/analytics-vidhya/movie-ratings-prediction-server-using-flask-and-lstm-in-keras-part-1-90933e8dbf88?source=collection_archive---------0-----------------------#2018-08-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/cb3663828c09d4071c4cf55b842d9ee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R_K4k4veirKKl5W5HGtMfg.jpeg"/></div></div></figure><p id="6199" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">深度学习是目前最热门的流行语之一，其理由在于它的大量应用以及它解决从计算机视觉到NLP等复杂问题的能力。在这一系列的帖子中，我将向您简要介绍如何使用深度学习来解决NLP问题。我们试图根据电影评论来预测电影的评分(满分5分)。在这一节中，我们将了解文本数据的基本预处理，这些预处理是为了使我们的数据可以用作LSTM模型的输入而必须完成的。</p><p id="3aa6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">RNNs是一类适合学习序列数据的神经网络。由于我们的数据由电影评论组成，我们肯定知道这是一种序列数据——因为一个词的情感取决于它之前或之后的词。例如，“这部电影很好”和“这部电影不好”描绘了相反的情绪，但为了找出这一点，我们必须跟踪“好”之前的单词。已经制定了rnn来解决这个具体问题。LSTM只是RNN的一个变种，它在各种问题上被观察到比普通的RNNs给出更好的结果。关于LSTM的深入分析，请参考<a class="ae jo" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">这篇</a>牛逼的博客。</p><p id="a942" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">关于RNN的更多应用，请参考<a class="ae jo" href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener ugc nofollow" target="_blank">这篇</a>博客。</p></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="204f" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">预处理</strong></h1><p id="cbc2" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">让我们从用python导入必要的模块开始。</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="d869" class="li jx hi le b fi lj lk l ll lm">import numpy as np<br/>import pandas as pd<br/>import re</span><span id="cddd" class="li jx hi le b fi ln lk l ll lm">from collections import Counter<br/>from keras.preprocessing.text import Tokenizer<br/>from keras.utils import np_utils<br/>from keras.models import Sequential, load_model<br/>from keras.layers import Dense, Dropout, LSTM, Bidirectional<br/>from nltk.corpus import stopwords<br/>from nltk.tokenize import word_tokenize<br/>from sklearn.cross_validation import train_test_split</span></pre><p id="a57c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，让我们定义从文本中提取干净标记的函数，将其添加到python中作为计数器存储的词汇表中，将计数器保存在文本文件中，加载该文本文件，最后根据加载的标记清理数据。我们还必须删除一些在默认nltk停用词中作为停用词出现的词。我们还添加了一些空格作为停用词，在清理数据后可以在语料库中找到。</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="1012" class="li jx hi le b fi lj lk l ll lm"># removing some words and adding some to increase accuracy<br/>stopwords = stopwords.words('english')<br/>newStopWords = ['', ' ', '  ', '   ', '    ', ' s']<br/>stopwords.extend(newStopWords)<br/>stopwords.remove('no')<br/>stopwords.remove('not')<br/>stopwords.remove('very')<br/>stop_words = set(stopwords)</span><span id="c124" class="li jx hi le b fi ln lk l ll lm">def clean_doc(doc, vocab=None):<br/>    tokens = word_tokenize(doc)<br/>    # keeping only alphabets    <br/>    tokens = [re.sub('[^a-zA-Z]', ' ', word) for word in tokens] <br/>    # converting to lowercase<br/>    tokens = [word.lower() for word in tokens]<br/>    # removing stopwords<br/>    tokens = [w for w in tokens if not w in stop_words]<br/>    # removing single characters if any<br/>    tokens = [word for word in tokens if len(word) &gt; 1]<br/>    if vocab:<br/>        tokens = [w for w in tokens if w in vocab]<br/>        tokens = ' '.join(tokens)        <br/>    return tokens</span><span id="eaff" class="li jx hi le b fi ln lk l ll lm">def add_doc_to_vocab(text, vocab):<br/>    tokens = clean_doc(text)<br/>    vocab.update(tokens)</span><span id="cec3" class="li jx hi le b fi ln lk l ll lm">def save_list(lines, filename):<br/>    data = '\n'.join(lines)<br/>    file = open(filename, 'w')<br/>    file.write(data)<br/>    file.close()<br/>    <br/>def load_doc(filename):<br/>    file = open(filename, 'r')<br/>    text = file.read()<br/>    file.close()<br/>    return text</span></pre><p id="f451" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">数据集由156060行和4列组成，其中只有两列，即“短语”和“情感”对我们很重要。“短语”列包含各种短语，而“情感”列包含分配给相应短语的从0到4的情感编号。0表示非常消极的情绪，4表示非常积极的情绪。</p><p id="d855" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">加载数据集后，我们将它分成训练和测试数据，然后对加载的数据应用我们之前编写的各种函数。你可以在这里得到数据集<a class="ae jo" href="https://drive.google.com/file/d/1aw3cItu0Cs-54qoYa1CFB09wECNySDlN/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="b8c3" class="li jx hi le b fi lj lk l ll lm">df = pd.read_csv('train.tsv', delimiter='\t')<br/>X = df['Phrase']<br/>y = df['Sentiment']<br/>y = np_utils.to_categorical(y)</span><span id="7fbe" class="li jx hi le b fi ln lk l ll lm"># splitting into training and testing data<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)</span><span id="d33d" class="li jx hi le b fi ln lk l ll lm"># removing unnecessary data<br/>del df, X, y</span><span id="c583" class="li jx hi le b fi ln lk l ll lm"># creating a vocabulary of words<br/>vocab = Counter()<br/>len_train = len(X_train)<br/>for i in range(len_train):<br/>    text = X_train.iloc[i]<br/>    add_doc_to_vocab(text , vocab)</span><span id="f530" class="li jx hi le b fi ln lk l ll lm">print(len(vocab))<br/># print the 20 most common words<br/>print(vocab.most_common(20))</span><span id="067e" class="li jx hi le b fi ln lk l ll lm"># removing tokens which occur less than 3 times.<br/>min_occurance = 2<br/>tokens = [k for k,c in vocab.items() if (c &gt;= min_occurance &amp; len(k) &gt; 1)]</span><span id="42f0" class="li jx hi le b fi ln lk l ll lm"># saving the vocabulary for futute use<br/>save_list(tokens, 'vocab.txt')</span><span id="2ebd" class="li jx hi le b fi ln lk l ll lm"># loading the saved vocabulary<br/>vocab = load_doc('vocab.txt')<br/>vocab = vocab.split()<br/>vocab = set(vocab)</span><span id="3305" class="li jx hi le b fi ln lk l ll lm">train_doc = []<br/>for i in range(len_train):<br/>    text = X_train.iloc[i]<br/>    doc = clean_doc(text, vocab)<br/>    train_doc.append(doc)</span><span id="0b9f" class="li jx hi le b fi ln lk l ll lm">test_doc = []<br/>len_test = len(X_test)<br/>for i in range(len_test):<br/>    text = X_test.iloc[i]<br/>    doc = clean_doc(text, vocab)<br/>    test_doc.append(doc)</span></pre><p id="ef57" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们创建了一套词汇库，以消除重复。然后，我们使用这个集合来清理我们的数据，并准备将其输入到我们的LSTM模型中。</p><p id="dd32" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">数据清理后，我发现某些行中没有标记，因为数据清理后每个标记都被删除了。这是我们在训练模型之前应该注意的事情。</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="302e" class="li jx hi le b fi lj lk l ll lm"># storing indexes where no tokens are present<br/>index_train = []<br/>for i in range(len(train_doc)):<br/>    if len(train_doc[i]) == 0 :<br/>        index_train.append(i)<br/>    <br/>index_test = []<br/>for i in range(len(test_doc)):<br/>    if len(test_doc[i]) == 0 :<br/>        index_test.append(i)</span><span id="8cb6" class="li jx hi le b fi ln lk l ll lm"># dropping the unnecessary data<br/>train_doc = np.delete(train_doc, index_train, 0)<br/>test_doc = np.delete(test_doc, index_test, 0)<br/>y_train = np.delete(y_train, index_train, 0)<br/>y_test = np.delete(y_test, index_test, 0)</span></pre><p id="41b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">清理完所有数据后，我们必须将文本数据转换成适合LSTM模型的形式。为此，我们可以使用Keras Tokenizer类将数据转换成单词包模型。</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="07a6" class="li jx hi le b fi lj lk l ll lm">tokenizer = Tokenizer()<br/>tokenizer.fit_on_texts(train_doc)</span><span id="4675" class="li jx hi le b fi ln lk l ll lm">X_train = tokenizer.texts_to_matrix(train_doc, mode='binary')<br/>X_test = tokenizer.texts_to_matrix(test_doc, mode='binary')<br/>n_words = X_test.shape[1]</span></pre></div><div class="ab cl jp jq gp jr" role="separator"><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju jv"/><span class="js bw bk jt ju"/></div><div class="hb hc hd he hf"><h1 id="1cc4" class="jw jx hi bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">模型结构</h1><p id="3b2f" class="pw-post-body-paragraph iq ir hi is b it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj ky jl jm jn hb bi translated">现在是最后一步。让我们开始构建我们的模型，并在我们的测试数据中找到它的准确性。我使用双向LSTM作为我们的模型，因此该模型的性能甚至比普通的LSTM还要好。在双向LSTM中，在对当前输入进行预测时，会考虑以前和将来的输入。关于双向网络的详细解释，请参考这个<a class="ae jo" href="https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks" rel="noopener ugc nofollow" target="_blank">维基百科</a>页面。</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="d766" class="li jx hi le b fi lj lk l ll lm"># LSTM Model<br/>model = Sequential()<br/>model.add(Bidirectional(LSTM(100, activation='relu'), input_shape=(None,n_words)))<br/>model.add(Dropout(0.2))<br/>model.add(Dense(units=50, input_dim=100, activation='relu'))<br/>model.add(Dense(5, activation='softmax'))<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</span><span id="a284" class="li jx hi le b fi ln lk l ll lm"># fitting the LSTM model<br/>model.fit(X_train.reshape((-1, 1, n_words)), y_train, epochs=20, batch_size=100)</span><span id="8ccd" class="li jx hi le b fi ln lk l ll lm"># finding test loss and test accuracy<br/>loss_rnn, acc_rnn = model.evaluate(X_test.reshape((-1, 1, n_words)), y_test, verbose=0)</span><span id="dbdd" class="li jx hi le b fi ln lk l ll lm"># saving model weights<br/>model.model.save('rnn.h5')</span><span id="8501" class="li jx hi le b fi ln lk l ll lm"># loading saved weights<br/>model_rnn = load_model('rnn.h5')</span></pre><p id="4dad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以通过使模型更深，并在每层中添加不同数量的神经元来进行实验。使用这个模型，我们可以得到大约60-65%的准确率。我们还保存模型，以便我们可以进一步使用模型权重来进行预测，而无需为将来使用或当您想要在web应用程序上进行实时预测时进行培训。为了使用LSTM制作这样一个web应用程序，请继续关注本帖的第二部分<a class="ae jo" rel="noopener" href="/@animeshsharma97/movie-ratings-prediction-server-using-flask-and-lstm-in-keras-part-2-b0a84fb30ab7">，我们将探索如何将你的模型转换成web应用程序。</a></p><p id="6155" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以在这里找到完整的实现<a class="ae jo" href="https://github.com/animeshsharma97/Movie-ratings-server" rel="noopener ugc nofollow" target="_blank"/>。</p></div></div>    
</body>
</html>