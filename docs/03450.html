<html>
<head>
<title>Dimensionality Reduction- How to deal with the features in your dataset (Part 1).</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维——如何处理数据集中的特征(第1部分)。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/dimensionality-reduction-how-to-deal-with-the-features-in-your-dataset-part-1-fbd174d6dc2a?source=collection_archive---------5-----------------------#2020-02-02">https://medium.com/analytics-vidhya/dimensionality-reduction-how-to-deal-with-the-features-in-your-dataset-part-1-fbd174d6dc2a?source=collection_archive---------5-----------------------#2020-02-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9012" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated">真实世界的数据是杂乱的，大多包含不想要的和多余的特征。这些冗余的特征通常会使我们的预测模型很难达到预期的效果。因此<strong class="ih hj">降维</strong>，消除、包含和转换现有特征的过程成为数据预处理的关键步骤。</p></div><div class="ab cl jm jn gp jo" role="separator"><span class="jp bw bk jq jr js"/><span class="jp bw bk jq jr js"/><span class="jp bw bk jq jr"/></div><div class="hb hc hd he hf"><p id="e346" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们有了简明和相关的数据，它会帮助我们</p><ul class=""><li id="d859" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">更好地可视化和探索数据集</li><li id="78b4" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">占用更少的内存空间</li><li id="7ced" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">降低预测模型的复杂性，使其更易于解释</li><li id="6b1e" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">减少过度拟合</li><li id="b2ef" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">通过选择正确的特征来提高模型的性能</li></ul><p id="f2f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该过程可以大致分为两种方式:</p><ol class=""><li id="a1af" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc kh jz ka kb bi translated"><strong class="ih hj">特征选择</strong>-排除或包含数据中存在的属性而不改变它们的过程。</li><li id="bb03" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc kh jz ka kb bi translated"><strong class="ih hj">特征提取- </strong>通过对现有属性进行一些转换来创建新的属性组合的过程</li></ol><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es ki"><img src="../Images/eb7cc5260935827bda2290688723ec57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LkeFXPinKGIKrRyf7-u17Q.png"/></div></div></figure><p id="7130" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个故事中，我们将看看<strong class="ih hj">特征选择</strong>部分。<strong class="ih hj">特征提取</strong>将在本文的第2部分讨论。</p><p id="2bd1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">特征选择</strong>分为3个主要类别。</p><ol class=""><li id="8f72" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc kh jz ka kb bi translated"><strong class="ih hj">过滤方法</strong></li><li id="b4a0" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc kh jz ka kb bi translated"><strong class="ih hj"> 2。包装方法</strong></li><li id="b1b3" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc kh jz ka kb bi translated"><strong class="ih hj"> 3。嵌入式方法。</strong></li></ol><p id="324c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将详细讨论每个问题:</p><h1 id="f5f4" class="ku kv hi bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">1.过滤方法</h1><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es ls"><img src="../Images/5aa7442f0e765bf486e39dc492b93bf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*01nnVZHb-6OTnx2i.png"/></div></div></figure><p id="52d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，最相关的特征是基于它们与目标的<strong class="ih hj">相关性</strong>、特征的<strong class="ih hj">唯一性</strong>以及它们的<strong class="ih hj">统计显著性</strong>来选择的。用于训练数据集的<strong class="ih hj"> ML算法</strong>在选择特征时<strong class="ih hj">不涉及</strong>。</p><p id="ca21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一些常用的过滤方法:</p><p id="9cf0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> a .缺失值比率:</strong>我们经常会遇到某些列有很多缺失值的数据。如果列<em class="lt">大部分为空</em>，则<em class="lt">没有提供足够的相关信息</em>。因此，我们可以找到每一列的缺失值比率，并删除不符合约定阈值比率的列。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lu"><img src="../Images/9a3b028a677e3d8230342200b9ab42d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*0eAYhM9d-MPzCj-1iKVD9w.png"/></div></figure><p id="5ee4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> b .低方差滤波器:</strong>如果一列中数据的分布是<em class="lt">最均匀</em>那么它的<em class="lt">方差趋于零</em>。这样的列<em class="lt">对预测目标变量</em>没有太大贡献。因此，在决定了阈值方差值之后，我们可以删除方差低于该值的列。但是方差取决于数据的分布/范围。因此，在应用该方法之前，<a class="ae lv" href="https://stats.stackexchange.com/questions/10289/whats-the-difference-between-normalization-and-standardization?source=post_page-----e3f174b65dfc----------------------" rel="noopener ugc nofollow" target="_blank"> <em class="lt">标准化</em> </a>数据非常重要。</p><p id="ec56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> c .信息增益:</strong>这有助于测量两个变量之间的相关性。主要用于<em class="lt">分类</em>问题。IG着眼于每个特征，并衡量该特征在目标变量分类中的重要性。它涉及到熵的度量。(信息增益= 1-熵)。<em class="lt">信息增益越高，分类越好。</em></p><p id="b9e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑一个数据集，它有两个特征<strong class="ih hj"> x1 </strong>和<strong class="ih hj"> x2 </strong>以及一个值为0和1的目标变量<strong class="ih hj"> y </strong>。让我们对<strong class="ih hj"> y </strong>进行两次分类，一次基于<strong class="ih hj"> x1 </strong>一次基于<strong class="ih hj"> x2。</strong></p><p id="d00b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设特定分裂的1和0的百分比为p1和p2，因此该分裂的熵为</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lw"><img src="../Images/6d9c2470fc5b161493ad97bf544ff777.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*8ZezWwfdW8LsW7UjPH7NMg.png"/></div></figure><p id="2ddd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们找出<strong class="ih hj"> y </strong>每次分裂的熵和信息增益</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es lx"><img src="../Images/870202f9d65434ee3bd7e3146023f101.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fJpQwlrpwstdtovo1D0yTQ.png"/></div></div></figure><p id="c5f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们可以看到，<strong class="ih hj"> x2 </strong>上的分割IG更大，分类也更好。因此，x2是一个更好的特征，应该被选择。</p><p id="d174" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> d. Pearson相关性:</strong>此<strong class="ih hj"> </strong>度量具有<strong class="ih hj">连续</strong>值的目标列对另一个也包含<strong class="ih hj">连续</strong>值的列的依赖性。它测量两个变量之间的线性关联。</p><p id="ede9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果这个值接近1，那么它就是一个完美的相关性:当一个变量增加时，另一个变量也会增加(如果为正)或减少(如果为负)。</p><p id="4714" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当值接近零时，变量被称为没有相关性。更多关于这个<a class="ae lv" href="https://www.khanacademy.org/math/ap-statistics/bivariate-data-ap/correlation-coefficient-r/v/calculating-correlation-coefficient-r" rel="noopener ugc nofollow" target="_blank">在这里。</a></p><p id="5f44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> e. ANOVA : </strong>此<strong class="ih hj"> </strong>测量具有<strong class="ih hj">连续</strong>值的目标列对包含<strong class="ih hj">分类</strong>值的另一列的依赖性。在我们首先讨论之前，我将敦促读者从<a class="ae lv" href="https://www.khanacademy.org/math/statistics-probability/analysis-of-variance-anova-library#analysis-of-variance-anova" rel="noopener ugc nofollow" target="_blank">这里</a>理解方差分析的概念。</p><p id="0e43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑一个分类特征<strong class="ih hj"> x </strong>和一个目标列<strong class="ih hj"> y </strong>，前者具有可能的值A、B和C，后者具有不同的连续值。现在，我们将根据特征x的类别对这些连续目标值<strong class="ih hj"> y </strong>进行分组</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es ly"><img src="../Images/953b5bd4b9d577060545176ac0c2f091.png" data-original-src="https://miro.medium.com/v2/resize:fit:260/format:webp/1*TKgysNwkCaCCCynXZyfejQ.png"/></div></figure><p id="ff1d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将<strong class="ih hj"> y </strong>的值按<strong class="ih hj"> x </strong>的类别分组后，我们发现表格如下</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lz"><img src="../Images/ad923b9eefbee36bb4e3180fa2df4e4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*LhhEp83XB_E_Fl6o8hkPbg.png"/></div></figure><p id="a29b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在ANOVA将确定Y值的这些组(A，B，C)中的每一组的平均值是否基本相等(<strong class="ih hj">零假设</strong>)或者它们之间是否有显著性差异(<strong class="ih hj">零假设</strong>)。</p><p id="2117" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">如果我们的零假设为真，那么我们将得出结论，分类特征X对Y没有任何影响</strong>。</p><p id="2855" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">否则，如果无效假设被拒绝，那么我们将得出结论，X特征的不同类别确实影响Y，因此应该在我们的特征选择技术中进行选择。</p><ul class=""><li id="2a42" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated"><strong class="ih hj">如果统计量&lt;临界值</strong>:无显著结果，不拒绝零假设(Ho)，<strong class="ih hj">独立。</strong></li><li id="3b2e" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">如果统计&gt; =临界值</strong>:显著结果，拒绝零假设(Ho)，依赖<strong class="ih hj"/>。</li></ul><p id="7fea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">卡方检验:</strong>卡方检验确定两个分类变量之间是否存在显著关系。你可以通过<a class="ae lv" href="https://www.khanacademy.org/math/statistics-probability/inference-categorical-data-chi-square-tests/chi-square-goodness-of-fit-tests/v/goodness-of-fit-example" rel="noopener ugc nofollow" target="_blank">这些</a>来了解卡方分布，以及如何计算卡方统计量。现在我们如何在机器学习中使用这个来进行特征选择呢？</p><p id="79e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它基本上确定了特定分类变量X(<strong class="ih hj">男性，女性</strong>)的不同组(<strong class="ih hj">性别</strong>)在另一变量Y ( <strong class="ih hj">兴趣</strong>)的不同类别(<strong class="ih hj">科学、艺术、数学</strong>)中的频率分布是否相同。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es ma"><img src="../Images/c7bc3e12cdcaf2d156e0aaf9bbb358cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*-J8dZk1Qg1KCLK47SD2Rog.png"/></div></figure><p id="2dd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们有2个频率分布(男性和女性),按艺术、科学和数学分组。所以我们将确定这两组频率是<strong class="ih hj">相等</strong>(零假设)还是它们之间存在<strong class="ih hj">显著差异</strong>(替代假设)</p><ul class=""><li id="ad28" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated"><strong class="ih hj">如果统计量&lt;临界值</strong>:无显著结果，不拒绝零假设(Ho)，<strong class="ih hj">独立。</strong></li><li id="061a" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">如果统计量&gt; =临界值</strong>:显著结果，拒绝零假设(Ho)，依赖<strong class="ih hj"/>。</li></ul><p id="eab0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更多关于这个<a class="ae lv" href="https://machinelearningmastery.com/chi-squared-test-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="d905" class="ku kv hi bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">2.包装方法</h1><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es mb"><img src="../Images/2a491f294c8ee63cc03f3ae1420015a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PG63fjE5Qa1CJL6a.png"/></div></div></figure><p id="512b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">包装器方法基于贪婪搜索算法，因为它们评估所有可能的特征组合，并为特定的机器学习算法选择产生最佳结果的组合。它基于算法的性能从子集迭代地选择或丢弃特征。它测试所有可能的组合，因此计算量很大。</p><p id="009e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一些最常见的包装方法:</p><p id="24ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> a .正向特征选择</strong></p><p id="3c40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，从1个特征开始逐个选择特征。</p><p id="de0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第一阶段，用每个特征训练算法。从中选出最好的。</p><p id="b940" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第二阶段，将该特征与其他特征相结合，并选择2的最佳组合。</p><p id="7a11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这一直持续到选择了所需特征数量的最佳组合。</p><p id="e894" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> b .递归特征消除</strong></p><p id="c072" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">递归要素消除过程从数据集中的所有要素开始。它以循环的方式消除每个特性一次，并评估剩余子集的性能。选择性能最佳的子集。</p><p id="27be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个选定的(功能数-1)子集，剩余的每个功能都被删除一次，并对性能进行评估。选择性能最佳的特征子集(特征数量-2)。</p><p id="ffd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个过程一直持续到我们得到满足所需标准的最佳性能特征子集</p><h1 id="43ab" class="ku kv hi bd kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr bi translated">3.嵌入式方法</h1><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es mc"><img src="../Images/688a0dfdd9fe8fc5323e7a62292aae65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Q47f-7rXQM4DZycw.png"/></div></div></figure><p id="c5f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">拥有更多的功能有时会增加噪音。模型可能最终会记住噪音，而不是学习数据的趋势。如果没有仔细训练，这些不准确性会导致低质量的模型。这被称为<strong class="ih hj">过拟合</strong></p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es md"><img src="../Images/6b53302c50dffcea94fc3fab202d109c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*q5vA048xyV7magnPnuihyQ.png"/></div></figure><p id="8b0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">避免过度拟合背后的主要概念是尽可能简化模型。简单模型(通常)不会过度拟合。另一方面，我们需要注意模型过拟合和欠拟合之间的平衡。这是通过<strong class="ih hj">正则化实现的。</strong></p><p id="af33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正则化背后的基本思想可以理解为<strong class="ih hj">对学习权重(w) </strong>的<strong class="ih hj">较高值的损失函数</strong>进行惩罚。这防止了一些<strong class="ih hj">特征呈指数增长并导致过度拟合</strong>。</p><p id="717a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们详细理解这一点</p><p id="3d09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑一下训练空间中的一个记录</p><p id="7b95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> n个</strong>特征由<strong class="ih hj"> x[0]，x[1]，x[2]表示..x[n]。</strong></p><p id="9f63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> </strong>学习到的参数或权重为<strong class="ih hj"> w[0]，w[1]，w[2]…w[n] </strong>。</p><p id="c6d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">目标值为y</strong></p><p id="6ce5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让预测值为</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es me"><img src="../Images/0a7fd3d3385eee7ff5d5126baa7e6bc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-9C6LBwCKW2M1YA-.png"/></div></div></figure><p id="25cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在损失函数可以定义为:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es me"><img src="../Images/a4d3abb3877335f437eef5abc607fee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pW09rLBAcPVydtFy.png"/></div></div></figure><p id="7fc6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的全部动机是最小化1.2中定义的损失函数。</p><p id="acaa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，如果由于某些特征x[j]相应的权重w[j]爆炸，这可能导致过度拟合。为了避免这种情况，我们必须惩罚那些爆炸重量的损失函数。这可以通过以下方式实现:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es me"><img src="../Images/b87c22d54659e32fe5154ee24cb39827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gvACN-MFdALcMwky.png"/></div></div></figure><p id="83ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们将正则化参数<strong class="ih hj"> λ </strong>与权重一起添加，以便在最小化成本函数的同时，收缩一些权重，使模型不那么复杂。</p><p id="9e9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> a .岭回归</strong></p><p id="aeb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在岭回归中，权重 w[j]的<strong class="ih hj">平方与<strong class="ih hj"> λ一起取值。</strong></strong></p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es me"><img src="../Images/b87c22d54659e32fe5154ee24cb39827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gvACN-MFdALcMwky.png"/></div></div></figure><p id="5702" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，通过改变<strong class="ih hj"> λ </strong>，权重被调整和收缩，但永远不会达到0。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es mf"><img src="../Images/90c7913a53d60ea83603250fa39c25db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WR6SUQLO1Eqncg399DkBzQ.png"/></div></div></figure><p id="71f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更多关于岭回归<a class="ae lv" href="https://www.youtube.com/watch?v=Q81RR3yKn30&amp;t=894s" rel="noopener ugc nofollow" target="_blank">这里</a></p><p id="e69e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> b .拉索回归</strong></p><p id="1d23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在套索中，重量 w[j]的<strong class="ih hj">绝对值与<strong class="ih hj"> λ一起被采用。</strong></strong></p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es me"><img src="../Images/c46f7ecf9a32afd9bd47e321fbec330a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ge_yILxjs4D6cT1j.png"/></div></div></figure><p id="ac9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这可能导致权重为零，即某些特征在评估输出时被完全忽略，从而完全消除了某些特征。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es mg"><img src="../Images/fc3edab3b31d3300b3df47adb096151b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*85ua-vRcFGIys2ijC1tpyQ.png"/></div></div></figure><p id="0773" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更多关于套索回归<a class="ae lv" href="https://www.youtube.com/watch?v=NGf0voTMlcs&amp;t=2s" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="32fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们几乎涵盖了主要的降维技术，即去掉或缩小不太重要的特征来创建一个更简单的模型。提取新特征主题将在后面讨论。</p></div></div>    
</body>
</html>