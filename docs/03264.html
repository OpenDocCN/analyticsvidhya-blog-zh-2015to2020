<html>
<head>
<title>Linear Regression Deep Dive</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归深潜</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-deep-dive-f266480da10d?source=collection_archive---------14-----------------------#2020-01-22">https://medium.com/analytics-vidhya/linear-regression-deep-dive-f266480da10d?source=collection_archive---------14-----------------------#2020-01-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a758" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归是回归建模中最简单且广泛使用的模型之一。它通过将线性方程拟合到观察到的数据来模拟连续的因变量Y和自变量X之间的关系。</p><p id="163d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">简单线性回归</strong>模型确定独立特征和从属目标变量之间的关系。这种关系可以表示为直线<em class="jd">的方程式，即</em> <strong class="ih hj"> <em class="jd"> y = b1。x+B0</em>T7】。<strong class="ih hj">多元线性回归</strong>确定目标变量与一个以上独立特征的关系。参数<em class="jd"> b0，b1，…，bn </em>可以通过使用<strong class="ih hj">普通最小二乘(OLS) </strong>方法最小化<strong class="ih hj"><em class="jd"/></strong>残差平方和来识别。</strong></p><p id="2ac3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最小二乘法使预测值与实际值的垂直偏差最小。这些误差项被称为<strong class="ih hj">残差。</strong>平方误差确保正负偏差不会抵消。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/d6362a51c1bdde9356e9ab451a8fadd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4RH-MfF4eWIXiNEnhOVccQ.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">从预测值到最佳拟合线的垂直线是<strong class="bd ju">残差</strong> <em class="jv">(来源:维基百科)</em></figcaption></figure></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><p id="e567" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Python实现</strong></p><p id="891b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用来自<a class="ae kd" href="https://archive.ics.uci.edu/ml/datasets/Student+Performance" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> UCI-ML知识库</strong> </a>的<strong class="ih hj">学生成绩数据</strong>来探索Python中的线性回归。该数据集包含关于两所葡萄牙学校的中等教育学生成绩的信息。数据属性包括学生成绩、人口统计、社会和学校相关特征。有关详细的数据字典，请访问链接。这是数据集前几行的样子。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ke"><img src="../Images/a7ccd35455fd310401c4950983673c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rtOPLdgv46Eg_Q8ygNH87A.png"/></div></div></figure><p id="341c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于数据集包含分类列，如父母的教育水平、学生的性别等，我们将<a class="ae kd" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">一次性编码</strong> </a> <strong class="ih hj"> </strong>这些。这将自动对包含文本数据的所有列进行一次性编码，并创建(n-1)个新要素，其中n是要素的级数。</p><pre class="jf jg jh ji fd kf kg kh ki aw kj bi"><span id="b927" class="kk kl hi kg b fi km kn l ko kp">df_transformed = pd.get_dummies(df_raw, drop_first=True)</span></pre><p id="7950" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">学生的健康、旅行时间等一些要素是顺序要素，尽管它们显示为数字。所以我们<a class="ae kd" href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">序数编码</strong> </a>那些特性使用scikit-learn的序数编码器类。</p><pre class="jf jg jh ji fd kf kg kh ki aw kj bi"><span id="dfb5" class="kk kl hi kg b fi km kn l ko kp">ordinal_features = ['Medu', 'Fedu', 'traveltime', 'studytime', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health']</span><span id="2fa3" class="kk kl hi kg b fi kq kn l ko kp">from sklearn.preprocessing import OrdinalEncoder</span><span id="fe61" class="kk kl hi kg b fi kq kn l ko kp">ord_enc = OrdinalEncoder()</span><span id="f096" class="kk kl hi kg b fi kq kn l ko kp">df_ord_enc = df_transformed.copy()<br/>df_ord_enc[ordinal_features] = ord_enc.fit_transform(df_transformed[ordinal_features])</span></pre><p id="eaa6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在拟合模型之前，我们<strong class="ih hj">缩放数据集</strong>,以便所有的特征都在相同的范围内。由于它是一个线性模型，缩放模型可以提高预测准确性和可解释性。使用scikit-learn中的<strong class="ih hj">标准缩放器</strong>，我们缩放数据集并创建一个新的数据帧。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kr"><img src="../Images/5b137cb9aadf7acd61b7e97e296c3ff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AZ8wq-apWAmIzM-okJA8SA.png"/></div></div></figure><p id="64ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将数据集分成测试样本和训练样本，以<strong class="ih hj">期末成绩G3 </strong>为目标特征，得到线性回归模型的最终训练集。</p><pre class="jf jg jh ji fd kf kg kh ki aw kj bi"><span id="9bab" class="kk kl hi kg b fi km kn l ko kp">from sklearn.model_selection import train_test_split</span><span id="8822" class="kk kl hi kg b fi kq kn l ko kp">X_train, X_test, y_train, y_test = train_test_split(df_ord_enc.drop(columns=[‘G3’]), df_ord_enc.G3, test_size=0.1)</span></pre><p id="06b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用scikit-learn的线性模型中的<a class="ae kd" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> LinearRegression </strong> </a>类，我们在训练集上拟合模型。</p><pre class="jf jg jh ji fd kf kg kh ki aw kj bi"><span id="4275" class="kk kl hi kg b fi km kn l ko kp">from sklearn.linear_model import LinearRegression</span><span id="5f18" class="kk kl hi kg b fi kq kn l ko kp">lin_reg = LinearRegression()<br/>lin_reg.fit(X_train, y_train)</span></pre><p id="102e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我们可以对测试集进行预测。</p><pre class="jf jg jh ji fd kf kg kh ki aw kj bi"><span id="f336" class="kk kl hi kg b fi km kn l ko kp">y_pred = lin_reg.predict(X_test)</span></pre><p id="6013" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LinearRegression类也有使用<a class="ae kd" href="https://en.wikipedia.org/wiki/Coefficient_of_determination" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> R平方</strong> </a>度量来检查模型性能的方法。简单地说，R平方度量解释了目标变量中有多少方差是由独立特征解释的。对于完美预测目标值的模型，该值为1，对于总是预测常数值的模型，该值为0。对于我们的模型，这个度量是<strong class="ih hj"> ~0.78 </strong>，这是一个合适的值。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ks"><img src="../Images/5ed9f678a804913e10c105228af2725c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o_cFDHjDov2co2L7CyDe-Q.png"/></div></div></figure></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><p id="6bff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">假设</strong></p><p id="a20d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然这是最广泛使用的算法之一，但只有当数据不违反线性回归的假设时，这些关系才有效。这些假设包括:</p><ol class=""><li id="7932" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc ky kz la lb bi translated">独立特征和因变量之间的关系是<strong class="ih hj">线性</strong>。</li><li id="9d87" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">数据中没有<a class="ae kd" href="https://en.wikipedia.org/wiki/Autocorrelation" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">自相关</strong> </a>即误差项之间没有相关性。时间序列数据通常是自相关的。<a class="ae kd" href="https://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">杜宾-沃森检验</strong> </a>可以给我们一个数据中自共线性的估计。</li><li id="fc8c" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">独立特征不应该是相关的。<strong class="ih hj">数据中的多重共线性</strong>会扭曲某些特征的贡献。<strong class="ih hj">特征的相关性分析</strong>可以识别高度相关的特征。</li><li id="aa7d" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">误差项应该具有恒定的方差，即数据应该是<a class="ae kd" href="https://en.wikipedia.org/wiki/Homoscedasticity" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"/></a>。这通常是由于数据中存在异常值。残差与拟合值的关系图可用于检测数据集中的异方差。</li><li id="67d0" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">误差项应该是正态分布的。这是因为存在不寻常的数据点。这可以用<a class="ae kd" href="https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> QQ图</strong> </a>来识别。</li></ol></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><p id="854f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">验证指标</strong></p><p id="00dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有一些标准指标可以帮助评估模型预测新数据点的能力。这些指标中的大多数都可以在scikit-learn的指标包中找到。我们将讨论其中的一些。</p><ol class=""><li id="5993" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc ky kz la lb bi translated"><strong class="ih hj">平均绝对误差</strong> —预测值与观测值之间的平均绝对差值。这给出了预测值与实际值的平均偏差。这很容易解释，因为它与目标变量的单位相同。</li><li id="49f5" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated"><strong class="ih hj">均方误差</strong> —它是预测误差的平方的平均值。由于误差项是平方的，它不像MAE那样容易解释。</li><li id="3005" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated"><strong class="ih hj">均方根误差</strong>—MSE的平方根。当较大的误差不太理想时，它是优选的，因为它给予较大的误差较大的权重。</li><li id="5ca4" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated"><strong class="ih hj"> R平方</strong> —它测量模型的拟合优度。R-squared衡量目标变量中多大比例的方差可以由独立特征来解释。最大值可以是1，也可以是负值。</li><li id="2b4e" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated"><strong class="ih hj">调整后的R平方</strong>-随着数据中要素数量的增加，R平方将不断增加。因此，即使附加特征是噪声，R平方也会增加。校正的R平方值校正模型中使用的独立要素数量的R平方值。只有当新特性的添加对模型的改进超过随机机会的预期时，该值才会增加，否则该值会减少。对于完全拟合的模型，这可以是1，对于拟合不佳的模型，这可以是负的。</li></ol></div><div class="ab cl jw jx gp jy" role="separator"><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb kc"/><span class="jz bw bk ka kb"/></div><div class="hb hc hd he hf"><p id="d97e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下一篇文章中，我们将查看更多的统计数据和图表，帮助我们更好地理解回归模型和使用Statsmodel包的数据集。</p><p id="7c50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">展示这些概念的代码可以在Github 上找到。</p></div></div>    
</body>
</html>