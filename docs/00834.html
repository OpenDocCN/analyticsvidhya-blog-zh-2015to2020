<html>
<head>
<title>Intuitive Understanding of Seq2seq model &amp; Attention Mechanism in Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对Seq2seq模型的直观理解&amp;深度学习中的注意机制</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/intuitive-understanding-of-seq2seq-model-attention-mechanism-in-deep-learning-1c1c24aace1e?source=collection_archive---------1-----------------------#2019-09-12">https://medium.com/analytics-vidhya/intuitive-understanding-of-seq2seq-model-attention-mechanism-in-deep-learning-1c1c24aace1e?source=collection_archive---------1-----------------------#2019-09-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2efc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我将向您解释序列到序列模型，它最近在诸如机器翻译、图像字幕、视频字幕、语音识别等应用中显示出巨大需求。</p><h2 id="32d4" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">目录:</h2><ol class=""><li id="213d" class="jy jz hi ih b ii ka im kb iq kc iu kd iy ke jc kf kg kh ki bi translated">Seq2seq模型是什么？</li><li id="85bd" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">为什么我们需要seq2seq模型？</li><li id="d6ac" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">它是如何工作的？</li><li id="f71a" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">Seq2seq模型的局限性？</li><li id="7b20" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">注意机制是什么？</li><li id="9620" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">我们为什么需要它&amp;它如何解决问题？</li><li id="93dd" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">它是如何工作的？</li><li id="6771" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">代码实现</li><li id="25a6" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">结论</li><li id="1d26" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">参考</li></ol><h1 id="145f" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">Seq2seq型号:</h1><p id="ed42" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">谷歌在2014年首次推出了Sequence to sequence。让我们来看看seq2 seq模型是什么？序列到序列模型尝试将固定长度的输入文本映射到固定长度的输出文本，其中模型的输入和输出的长度可能不同。正如我们所知，循环神经网络的变体，如长短期记忆或门控循环神经网络(GRU)是我们最常用的方法，因为它们克服了消失梯度的问题。</p><p id="bcfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们举个例子来弄清楚它是什么:</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es li"><img src="../Images/349d351fd83ed464fcec33a96c8e05e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IwYX_a5neui46OLQCLSJdw.jpeg"/></div></div></figure><p id="b103" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们可以观察到一种语言被翻译成另一种语言。有许多例子如下:</p><ol class=""><li id="5c19" class="jy jz hi ih b ii ij im in iq lu iu lv iy lw jc kf kg kh ki bi translated">语音识别</li><li id="b52a" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">机器语言翻译</li><li id="db1d" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">命名实体/主题提取</li><li id="bac1" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">关系分类</li><li id="eb0a" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">路径查询应答</li><li id="9cb9" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">语音生成</li><li id="1d48" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">聊天机器人</li><li id="28e5" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">文本摘要</li><li id="1792" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">产品销售预测</li></ol><p id="49ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将seq2seq模型或编码器-解码器模型分为两个阶段:</p><ol class=""><li id="a0c9" class="jy jz hi ih b ii ij im in iq lu iu lv iy lw jc kf kg kh ki bi translated">训练阶段，即编码器和解码器下的过程。</li><li id="01aa" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">推理阶段，即测试期间的过程。</li></ol><p id="4d31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们进入第一阶段——培训阶段</p><h1 id="a0ab" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">培训阶段:</h1><p id="c6ba" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">因此，在训练阶段，我们设置我们的编码器和解码器模型。设置后，我们训练我们的模型，并通过逐字或逐字符读取输入来预测每个时间戳。</p><p id="9760" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在进行训练和测试之前，需要清理数据，我们需要添加标记来指定句子的开始和结束。因此该模型将理解何时结束。让我们来看看编码器架构！</p><h1 id="8212" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">编码器:</h1><p id="72fe" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">让我们通过图表来理解，这样我们就能清楚地看到编码器部分的流程。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es lx"><img src="../Images/124b05efa15bf5d57b1ab20d07f690ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FKymQTmoVMKIrSlEqG0IGw.jpeg"/></div></div></figure><p id="0f9f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，从编码器作为LSTM网络的图中，我们可以直观地看到，在每个时间戳，单词被读取或处理，并且它从传递给编码器模型的输入序列中捕获每个时间戳的上下文信息。</p><p id="ec8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在fig中，我们传递了一个类似“How are you <end>”的例子，这个例子是一个字一个字处理的。在编码器中需要注意的一点是，我们返回最终隐藏状态(h3)和单元状态(c3)作为解码器模型的初始化状态。</end></p><h1 id="4d89" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">解码器:</h1><p id="21bc" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">As decoder也是一个LSTM网络，它逐字读取整个目标序列或句子，并预测偏移一个时间步长的相同序列。</p><p id="3c1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解码器模型被训练来预测给定前一个字的序列中的下一个字。为了更好地理解解码器模型流程，让我们浏览一下fig:</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ly"><img src="../Images/90ecef5d834c6f2395e08647e0f30f5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_WwXiu8vgB5S-0Pr"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">图片来源— <a class="ae md" href="https://mc.ai/implement-of-seq2seq-model/" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><p id="25a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> &lt; Go &gt; </strong>和&lt; <strong class="ih hj"> end &gt; </strong>是在将目标序列送入解码器之前添加到目标序列中的特殊标记。解码测试序列时目标序列未知。因此，我们开始预测目标序列，只需将第一个字传入解码器，解码器通常是&lt; <strong class="ih hj"> Go &gt; </strong>标记，或者您可以指定任何想要的标记。和&lt; <strong class="ih hj"> end &gt; </strong> token信号指定要建模的句子的结尾。</p><h1 id="2c9b" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">推理阶段:</h1><p id="7076" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">在训练我们的编码器-解码器或Seq2seq模型之后，在目标序列未知的新的看不见的输入序列上测试该模型。因此，为了获得对给定句子的预测，我们需要建立推理架构来解码测试序列:</p><h2 id="4245" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">那么它是如何工作的呢？</h2><p id="8b20" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">以下是解码测试序列的步骤:</p><ol class=""><li id="50eb" class="jy jz hi ih b ii ij im in iq lu iu lv iy lw jc kf kg kh ki bi translated">首先，对整个输入序列进行编码，并用编码器的内部状态初始化解码器。</li><li id="c712" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">Pass &lt;<strong class="ih hj"> Go &gt; </strong> token或您指定作为解码器输入的token。</li><li id="3166" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">用内部状态运行解码器一个时间步长</li><li id="f19f" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">输出将是下一个单词的概率。将选择具有最大概率的单词</li><li id="b87f" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">将最大概率字作为输入传递给下一个时间步长中的解码器，并用当前时间步长更新内部状态</li><li id="9353" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">重复步骤3-5，直到生成&lt;<strong class="ih hj"> end &gt; </strong>令牌或达到目标序列的最大长度。</li></ol><p id="e162" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来看一张图，图中我们给出了输入，并展示了推理过程是如何完成的，以及一个句子的编码-解码是如何完成的:</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es me"><img src="../Images/2728cad7f8cf4eba498a81dca9f17f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/0*gx_ImENMlvoHm8bH"/></div></figure><h1 id="2dc0" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">那么seq2seq模型为什么会失败呢？</h1><p id="19b6" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">因此，我们了解了编码器-解码器模型的架构和流程，因为它非常有用，但也有一些限制。正如我们所看到的，编码器接收输入并将其转换为固定大小的向量，然后解码器进行预测并给出输出序列。它对短序列工作良好，但当我们有一个长序列时就失败了，因为编码器很难将整个序列存储到一个固定大小的向量中，并从序列中压缩所有的上下文信息。正如我们观察到的，随着序列大小的增加，模型性能开始下降。</p><h2 id="04a1" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">那么如何克服这个问题呢？</h2><h1 id="078f" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">注意:</h1><p id="7fe8" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">为了解决这个问题，我们引入了注意机制的概念。因此，在这种情况下，我们将重点放在序列的特定部分，而不是预测单词的整个序列。基本上，在注意中，我们不丢弃来自编码器状态的中间体，但是我们利用它来从所有状态生成上下文向量，以便解码器给出输出结果。</p><p id="e7ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们通过一个例子来理解注意力机制是如何工作的:</p><blockquote class="mf mg mh"><p id="1557" class="if ig mi ih b ii ij ik il im in io ip mj ir is it mk iv iw ix ml iz ja jb jc hb bi translated"><strong class="ih hj">来源序列:</strong>“你最喜欢哪项运动？</p><p id="a3ed" class="if ig mi ih b ii ij ik il im in io ip mj ir is it mk iv iw ix ml iz ja jb jc hb bi translated"><strong class="ih hj">目标序列:</strong>“我爱板球”</p></blockquote><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es mm"><img src="../Images/4ee8feb0c62e0bbdf7037e835767b725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/0*_tepCvp-6bxpMH15"/></div></figure><p id="f2b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以注意力的想法是利用输入序列的所有上下文信息，这样我们就可以解码我们的目标序列。让我们看看我们提到的例子。目标序列中的第一个字<strong class="ih hj">‘我’</strong>和源序列中的第四个字<strong class="ih hj">‘你’</strong>相连，对吗？类似地，目标序列中的第二个字<strong class="ih hj">‘love’</strong>与源序列中的第五个字<strong class="ih hj">‘like’</strong>相关联。正如我们可以观察到的，我们更加关注源序列中的上下文信息，比如体育，比如你。</p><p id="131d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，它不是遍历整个序列，而是关注序列中的特定单词，并基于这些单词给出结果。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es mn"><img src="../Images/5a98bede395c1d0c1276184043d52cdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*feWTfEQybtL2jJRP"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">来源:<a class="ae md" href="https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/" rel="noopener ugc nofollow" target="_blank">链接</a></figcaption></figure><h2 id="baea" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">它是如何工作的:</h2><p id="27c0" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">我们已经从理论上了解了注意力是如何工作的，现在是从技术上了解的时候了，因为我们将坚持我们的例子“你最喜欢哪项运动？”。</p><p id="5a1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逐步步骤:</p><h2 id="436b" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">1.计算每个编码器状态的分数:</h2><p id="f297" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">因此，我们将使用所有编码器状态来训练我们的前馈网络(编码器-解码器),因为我们记得我们正在用编码器最终状态来初始化解码器初始状态。因此，通过训练我们的网络，我们将为使用注意力的状态生成高分，而我们忽略了低值的分数。</p><p id="244a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设s1、s2、s3、s4、s5、s6和s7是对应于状态h1、h2、h3、h4、h5、h6和h7产生的分数。因为我们假设我们需要更多地关注状态h2、h4和h5，而忽略h1、h3、h6和h7，以便预测“I”，所以我们期望上述神经生成分数，使得s2、s4和s5高，而s1、s3、s6和s7相对低。</p><h2 id="f150" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">2.计算注意力权重:</h2><p id="1561" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">生成分数后，我们对这些分数应用softmax，以获得权重w1、w2、w3、w4、w5、w6和w7。因为我们知道softmax是如何工作的，所以它会给出0-1范围内所有权重值的总和的概率。</p><p id="ab98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如:</p><p id="04b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">w1=0.23，w2=0.11，w3=0.13..所有权重总和为1。</p><h2 id="024f" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">3.计算上下文向量:</h2><p id="f86a" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">在计算注意力权重之后，现在我们将计算上下文向量，解码器将使用该向量来预测序列中的下一个单词。</p><p id="b056" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">context vector = E1 * h1+E2 * H2+E3 * H3+E4 * H4+E5 * H5+E6 * h6+E7 * H7</p><p id="b4c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">显然，如果e2和e4的值高，而e1、e3、e5、e6和e7的值低，则上下文向量将包含来自状态h2和h4的更多信息和来自状态h1、h3、h6和h7的相对较少的信息。</p><h2 id="b112" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">4.使用先前时间戳的输出添加上下文向量:</h2><p id="2301" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">在这里简单地添加我们的上下文向量和<strong class="ih hj"> &lt; Go &gt; </strong>令牌，因为对于第一个时间戳，我们没有先前的时间戳输出。</p><p id="3a11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个解码器生成一个单词的序列输出后，同样地，我们将得到序列中每个单词的预测。一旦解码器输出<end>，我们就停止生成一个单词。</end></p><p id="d9cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注:</strong></p><ol class=""><li id="ebf9" class="jy jz hi ih b ii ij im in iq lu iu lv iy lw jc kf kg kh ki bi translated">与seq2seq模型不同，我们为所有解码器时间戳使用固定大小的向量，但是在注意机制的情况下，我们在每个时间戳生成上下文向量。</li><li id="f738" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">由于注意机制的优势，模型的性能得到了改善，我们观察到了更好的结果。</li></ol><p id="8e64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意力机制有两种不同的方式，我们不会深入探讨。所以这两个类取决于你需要如何从输入序列中压缩上下文向量信息:</p><ol class=""><li id="95d1" class="jy jz hi ih b ii ij im in iq lu iu lv iy lw jc kf kg kh ki bi translated">全球关注</li><li id="03da" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">当地的关注</li></ol><p id="c2a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们深入研究一下。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es mo"><img src="../Images/0455354da7960a44e4848bcf5a085cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*51DQPamCpoBNbb3U"/></div></div><figcaption class="lz ma et er es mb mc bd b be z dx translated">来源:链接</figcaption></figure><h2 id="7ad7" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">全球关注:</h2><p id="b293" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">这里，注意力放在所有的源位置上。换句话说，编码器的所有隐藏状态都被考虑用于导出关注上下文向量<strong class="ih hj">。这里我们将关注所有中间状态，并收集所有上下文信息，以便我们的解码器模型预测下一个单词。</strong></p><h2 id="f0cb" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">当地注意:</h2><p id="a48e" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">这里，注意力仅放在几个源位置上。仅考虑编码器的几个隐藏或中间状态来导出受关注的上下文向量，因为我们仅重视序列的特定部分。</p><h1 id="c672" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">用代码解释:</h1><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="0d23" class="jd je hi mq b fi mu mv l mw mx">import necessary package:</span><span id="336a" class="jd je hi mq b fi my mv l mw mx">#Packages</span><span id="6ee7" class="jd je hi mq b fi my mv l mw mx">import pandas as pd<br/>import re<br/>import string<br/>from string import digits<br/>import numpy as np<br/>from sklearn.utils import shuffle<br/>from keras.layers import Input, LSTM, Embedding, Dense,Dropout,TimeDistributed<br/>from keras.models import Model</span></pre><p id="4ae7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">读取数据:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="84ec" class="jd je hi mq b fi mu mv l mw mx">data = pd.read_csv('./mar.txt',sep='\t',names=['eng','mar'])</span><span id="edbf" class="jd je hi mq b fi my mv l mw mx">data.head()<br/>#Output<br/>eng mar<br/>0 Go. जा.<br/>1 Run! पळ!<br/>2 Run! धाव!<br/>3 Run! पळा!<br/>4 Run! धावा!</span></pre><p id="afeb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">预处理数据:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="8ec4" class="jd je hi mq b fi mu mv l mw mx"># lowercase all the character<br/>source['eng']=data.eng.apply(lambda x: x.lower())<br/>target['mar']=data.mar.apply(lambda x: x.lower())<br/># Remove quotes<br/>data.eng=data.eng.apply(lambda x: re.sub("'","",x))<br/>data.mar=data.mar.apply(lambda x: re.sub("'","",x))</span><span id="5a12" class="jd je hi mq b fi my mv l mw mx">#specifying to remove punctuation<br/>exclude = set(string.punctuation)<br/># Remove all special character<br/>data.eng=data.eng.apply(lambda x: "".join(ch for ch in x if x not in exclude))<br/>data.mar=data.mar.apply(lambda x:"".join(ch for ch in x if x not in exclude))</span><span id="0683" class="jd je hi mq b fi my mv l mw mx">#Remove all numbers from text<br/>remove_digits = str.maketrans('','',digits)<br/>data.eng=data.eng.apply(lambda x: x.translate(remove_digits))<br/>data.mar = data.mar.apply(lambda x: re.sub("[२३०८१५७९४६]", "", x))</span><span id="54a2" class="jd je hi mq b fi my mv l mw mx"># Remove extra spaces<br/>data.eng=data.eng.apply(lambda x: x.strip())<br/>data.mar=data.mar.apply(lambda x: x.strip())<br/>data.eng=data.eng.apply(lambda x: re.sub(" +", " ", x))<br/>data.mar=data.mar.apply(lambda x: re.sub(" +", " ", x))</span><span id="7c33" class="jd je hi mq b fi my mv l mw mx"># Add start and end tokens to target sequences<br/>data.mar = data.mar.apply(lambda x : 'START_ '+ x + ' _END')</span></pre><p id="5e2e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">构建词汇:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="c575" class="jd je hi mq b fi mu mv l mw mx"># Vocabulary of English storing all the words in a set and same for marathi vocab<br/>all_eng_words=set()<br/>for eng in data.eng:<br/>    for word in eng.split():<br/>        if word not in all_eng_words:<br/>            all_eng_words.add(word)</span><span id="c09b" class="jd je hi mq b fi my mv l mw mx"># Vocabulary of marathi <br/>all_marathi_words=set()<br/>for mar in data.mar:<br/>    for word in mar.split():<br/>        if word not in all_marathi_words:<br/>            all_marathi_words.add(word)</span><span id="0181" class="jd je hi mq b fi my mv l mw mx"># Max Length of source sequence to specify wat size will be there for input <br/>lenght_list=[]<br/>for l in data.eng:<br/>    lenght_list.append(len(l.split(' ')))<br/>max_source_length = np.max(lenght_list)<br/>max_source_length<br/>#35</span><span id="d509" class="jd je hi mq b fi my mv l mw mx"># Max Length of target sequence to specifying wat size will be for target output<br/>lenght_list=[]<br/>for l in data.mar:<br/>    lenght_list.append(len(l.split(' ')))<br/>max_target_size = np.max(lenght_list)<br/>max_target_size<br/>#37</span><span id="2fc8" class="jd je hi mq b fi my mv l mw mx">input_words = sorted(list(all_eng_words))<br/>target_words = sorted(list(all_marathi_words))</span><span id="3f50" class="jd je hi mq b fi my mv l mw mx">#storing the vocab size for encoder and decoder<br/>num_of_encoder_tokens = len(all_eng_words)<br/>num_of_decoder_tokens = len(all_marathi_words)<br/>print("Encoder token size is {} and decoder token size is {}".format(num_of_encoder_tokens,num_of_decoder_tokens))<br/>#output<br/>Encoder token size is 8882 and decoder token size is 14689</span><span id="693f" class="jd je hi mq b fi my mv l mw mx">#for zero padding<br/>num_of_decoder_tokens += 1 <br/>num_of_decoder_tokens<br/>#output<br/>14690</span><span id="1768" class="jd je hi mq b fi my mv l mw mx"># dictionary to index each english character - key is index and value is english character<br/>eng_index_to_char_dict = {}</span><span id="ecd2" class="jd je hi mq b fi my mv l mw mx"># dictionary to get english character given its index - key is english character and value is index<br/>eng_char_to_index_dict = {}</span><span id="86b5" class="jd je hi mq b fi my mv l mw mx">for key, value in enumerate(input_words):<br/>    eng_index_to_char_dict[key] = value<br/>    eng_char_to_index_dict[value] = key</span><span id="7d75" class="jd je hi mq b fi my mv l mw mx">#similary for target i.e marathi words<br/>mar_index_to_char_dict = {}<br/>mar_char_to_index_dict = {}</span><span id="65b6" class="jd je hi mq b fi my mv l mw mx">for key,value in enumerate(target_words):<br/>    mar_index_to_char_dict[key] = value<br/>    mar_char_to_index_dict[value] = key</span></pre><p id="66b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将数据分为训练和测试:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="936f" class="jd je hi mq b fi mu mv l mw mx"># Splitting our data into train and tes part</span><span id="84e1" class="jd je hi mq b fi my mv l mw mx">from sklearn.model_selection import train_test_split</span><span id="f987" class="jd je hi mq b fi my mv l mw mx">X, y = data.eng, data.mar<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)</span><span id="5e55" class="jd je hi mq b fi my mv l mw mx">print("Training data size is {} and testing data size is {}".format(X_train.shape, X_test.shape))<br/>#output<br/>Training data size is (23607,) and testing data size is (10118,)</span></pre><p id="40a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">批量训练数据:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="bee4" class="jd je hi mq b fi mu mv l mw mx">#we will not train on whole data at time instead we will train on batch so as to reduce computation and increase learning and performace of model</span><span id="56b1" class="jd je hi mq b fi my mv l mw mx">def generate_batch(X = X_train, y = y_train, batch_size = 128):<br/>    while True:<br/>        for j in range(0, len(X), batch_size):<br/>            <br/>            #encoder input<br/>            encoder_input_data = np.zeros((batch_size, max_source_length),dtype='float32')<br/>            #decoder input<br/>            decoder_input_data = np.zeros((batch_size, max_target_size),dtype='float32')<br/>            <br/>            #target <br/>            decoder_target_data = np.zeros((batch_size, max_target_size, num_of_decoder_tokens),dtype='float32')<br/>            <br/>            for i, (input_text, target_text) in enumerate(zip(X[j:j+batch_size], y[j:j+batch_size])):<br/>                for t, word in enumerate(input_text.split()):<br/>                    encoder_input_data[i, t] = eng_char_to_index_dict[word] # encoder input seq<br/>                    <br/>                for t, word in enumerate(target_text.split()):<br/>                    if t&lt;len(target_text.split())-1:<br/>                        decoder_input_data[i, t] = mar_char_to_index_dict[word] # decoder input seq<br/>                    if t&gt;0:<br/>                        # decoder target sequence (one hot encoded)<br/>                        # does not include the START_ token<br/>                        # Offset by one timestep since it is one time stamp ahead<br/>                        decoder_target_data[i, t - 1, mar_char_to_index_dict[word]] = 1<br/>                        <br/>            yield([encoder_input_data, decoder_input_data], decoder_target_data)</span></pre><p id="e3fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">定义编码器型号:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="a00c" class="jd je hi mq b fi mu mv l mw mx">latent_dim = 256<br/># Encoder<br/>encoder_inputs = Input(shape=(None,))<br/>enc_emb =  Embedding(num_of_encoder_tokens, latent_dim, mask_zero = True)(encoder_inputs)<br/>encoder_lstm = LSTM(latent_dim, return_state=True)<br/>encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)</span><span id="c7e0" class="jd je hi mq b fi my mv l mw mx"># We discard `encoder_outputs` and only keep the states.<br/>encoder_states = [state_h, state_c]</span></pre><p id="b68f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">定义解码器模型:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="a353" class="jd je hi mq b fi mu mv l mw mx"># Set up the decoder, using `encoder_states` as initial state.<br/>decoder_inputs = Input(shape=(None,))<br/>dec_emb_layer = Embedding(num_of_decoder_tokens, latent_dim, mask_zero = True)<br/>dec_emb = dec_emb_layer(decoder_inputs)<br/>decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)</span><span id="e943" class="jd je hi mq b fi my mv l mw mx">hidden_with_time_axis = tf.expand_dims(latent_dim, 1)                score = Dense(tanh(Dense(enc_output) + Dense(hidden_with_time_axis)))</span><span id="3d34" class="jd je hi mq b fi my mv l mw mx">attention_weights = softmax(score, axis=1)                #       <br/>context_vector = attention_weights * enc_output<br/>context_vector = tf.reduce_sum(context_vector, axis=1)</span><span id="07ad" class="jd je hi mq b fi my mv l mw mx">x = Concatenate([tf.expand_dims(context_vector, 1),dec_emb], axis=-1)</span><span id="cb99" class="jd je hi mq b fi my mv l mw mx">decoder_outputs, _, _ = decoder_lstm(x,<br/>                                     initial_state=encoder_states)</span><span id="03bc" class="jd je hi mq b fi my mv l mw mx">decoder_dense = TimeDistributed(Dense(num_of_decoder_tokens, activation='softmax'))<br/>decoder_outputs = decoder_dense(decoder_outputs)</span><span id="f7b2" class="jd je hi mq b fi my mv l mw mx">decoder_outputs = Reshape(decoder_outputs, (-1, decoder_outputs.shape[2]))</span><span id="6468" class="jd je hi mq b fi my mv l mw mx">model = Model([encoder_inputs, decoder_inputs], decoder_outputs)</span></pre><p id="0bf1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">编译模型:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="a501" class="jd je hi mq b fi mu mv l mw mx">model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</span><span id="d122" class="jd je hi mq b fi my mv l mw mx">train_samples = len(X_train)<br/>val_samples = len(X_test)<br/>batch_size = 64<br/>epochs = 50</span></pre><p id="bd35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">训练我们的模型:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="23d4" class="jd je hi mq b fi mu mv l mw mx">history=model.fit_generator(generator = generate_batch(X_train, y_train, batch_size = batch_size),<br/>                    steps_per_epoch = train_samples//batch_size,<br/>                    epochs=epochs,<br/>                    validation_data = generate_batch(X_test, y_test,    batch_size = batch_size),<br/>                    validation_steps = val_samples//batch_size)</span></pre><p id="377b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">保存我们的模型:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="3d21" class="jd je hi mq b fi mu mv l mw mx">model.save_weights('./weights.h5')</span></pre><p id="423e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">推理模型:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="05e9" class="jd je hi mq b fi mu mv l mw mx"># Inference model<br/>#storing encoder input and internal states so as to give to decoder part<br/>encoder_model = Model(encoder_inputs, encoder_states)</span><span id="9fe7" class="jd je hi mq b fi my mv l mw mx">#specifying hidden and cell state for decoder part as vector process it will get output predicted and again we add to decoder states</span><span id="9d3c" class="jd je hi mq b fi my mv l mw mx">decoder_state_input_h = Input(shape=(latent_dim,))<br/>decoder_state_input_c = Input(shape=(latent_dim,))<br/>decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]</span><span id="7d9a" class="jd je hi mq b fi my mv l mw mx">dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence</span><span id="305b" class="jd je hi mq b fi my mv l mw mx"># To predict the next word in the sequence, set the initial states to the states from the previous time step</span><span id="6f28" class="jd je hi mq b fi my mv l mw mx">decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)<br/>decoder_states2 = [state_h2, state_c2]<br/>decoder_outputs2 = decoder_dense(decoder_outputs2)<br/> # A dense softmax layer to generate prob dist. over the target vocabulary</span><span id="d41a" class="jd je hi mq b fi my mv l mw mx"># Final decoder model<br/>decoder_model = Model(<br/>    [decoder_inputs] + decoder_states_inputs,<br/>    [decoder_outputs2] + decoder_states2)</span></pre><p id="bb53" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解码顺序:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="2314" class="jd je hi mq b fi mu mv l mw mx">def decode_sequence(input_seq):<br/>    # Encode the input as state vectors.<br/>    states_value = encoder_model.predict(input_seq)<br/>    # Generate empty target sequence of length 1.<br/>    target_seq = np.zeros((1,1))<br/>    # Populate the first character of target sequence with the start character.<br/>    target_seq[0, 0] = mar_char_to_index_dict['START_']</span><span id="f72c" class="jd je hi mq b fi my mv l mw mx"># Sampling loop for a batch of sequences<br/>    # (to simplify, here we assume a batch of size 1).<br/>    stop_condition = False<br/>    decoded_sentence = ''<br/>    while not stop_condition:<br/>        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)</span><span id="4187" class="jd je hi mq b fi my mv l mw mx"># Sample a token<br/>        sampled_token_index = np.argmax(output_tokens[0, -1, :])<br/>        sampled_char = mar_index_to_char_dict[sampled_token_index]<br/>        decoded_sentence += ' '+sampled_char</span><span id="996a" class="jd je hi mq b fi my mv l mw mx"># Exit condition: either hit max length<br/>        # or find stop character.<br/>        if (sampled_char == '_END' or<br/>           len(decoded_sentence) &gt; 50):<br/>            stop_condition = True</span><span id="fe35" class="jd je hi mq b fi my mv l mw mx"># Update the target sequence (of length 1).<br/>        target_seq = np.zeros((1,1))<br/>        target_seq[0, 0] = sampled_token_index</span><span id="42c3" class="jd je hi mq b fi my mv l mw mx"># Update states<br/>        states_value = [h, c]</span><span id="35e5" class="jd je hi mq b fi my mv l mw mx">return decoded_sentence</span></pre><p id="110e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">列车数据测试:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="1f6e" class="jd je hi mq b fi mu mv l mw mx">train_gen = generate_batch(X_train, y_train, batch_size = 1)<br/>k=-1</span><span id="46ad" class="jd je hi mq b fi my mv l mw mx">k+=1<br/>(input_seq, actual_output), _ = next(train_gen)<br/>decoded_sentence = decode_sequence(input_seq)<br/>print('Input English sentence as per data:', X_train[k:k+1].values[0])</span><span id="3925" class="jd je hi mq b fi my mv l mw mx">print('Actual Marathi Translation as per data:', y_train[k:k+1].values[0][6:-4])</span><span id="d61b" class="jd je hi mq b fi my mv l mw mx">print('Predicted Marathi Translation predicted by model:', decoded_sentence[:-4])</span><span id="7d4c" class="jd je hi mq b fi my mv l mw mx">#output<br/>Input English sentence as per data: i want something to drink. Actual Marathi Translation as per data:  मला काहीतरी प्यायला हवंय.  Predicted Marathi Translation predicted by model:  मला काहीतरी प्यायला हवंय.</span></pre><p id="e55a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">测试数据测试:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="d1ea" class="jd je hi mq b fi mu mv l mw mx">val_gen = generate_batch(X_test, y_test, batch_size = 1)<br/>k=-1</span><span id="191e" class="jd je hi mq b fi my mv l mw mx">#Adam<br/>k+=1<br/>(input_seq, actual_output), _ = next(val_gen)<br/>decoded_sentence = decode_sequence(input_seq)</span><span id="7ef2" class="jd je hi mq b fi my mv l mw mx">print('Input English sentence as per data:', X_test[k:k+1].values[0])</span><span id="8a54" class="jd je hi mq b fi my mv l mw mx">print('Actual Marathi Translation as per data:', y_test[k:k+1].values[0][6:-4])</span><span id="bc42" class="jd je hi mq b fi my mv l mw mx">print('Predicted Marathi Translation predicted by model:', decoded_sentence[:-4])<br/>#output</span><span id="eddc" class="jd je hi mq b fi my mv l mw mx">Input English sentence as per data: i dont speak ukrainian. Actual Marathi Translation as per data:  मी युक्रेनियन बोलत नाही.  Predicted Marathi Translation predicted by model:  मी युक्रेनियन भाषा बोलत नाही.</span></pre><p id="ac07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于看不见的查询:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="74d8" class="jd je hi mq b fi mu mv l mw mx">loading our model and model weights , compiling it</span><span id="8bb9" class="jd je hi mq b fi my mv l mw mx">model.load_model('./model.h5<br/>model.load_weights('./weights.h5')</span><span id="a3a4" class="jd je hi mq b fi my mv l mw mx">model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</span><span id="9edb" class="jd je hi mq b fi my mv l mw mx">#pre-processing step<br/>from string import digits<br/>def pre_processing(sentence):<br/>    sentence = sentence.lower()<br/>    sentance = re.sub("'","",sentence).strip()<br/>    sentence = re.sub(" +", " ", sentence)<br/>    remove_digits = str.maketrans('','',digits)<br/>    sentence=sentence.translate(remove_digits)<br/>    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in exclude)</span><span id="4eb7" class="jd je hi mq b fi my mv l mw mx">return encoder_input_data</span></pre><p id="9806" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设置推理模型:</p><pre class="lj lk ll lm fd mp mq mr ms aw mt bi"><span id="8481" class="jd je hi mq b fi mu mv l mw mx"># Inference model<br/>#storing encoder input and internal states so as to give to decoder part<br/>encoder_model = Model(encoder_inputs, encoder_states)</span><span id="7dcf" class="jd je hi mq b fi my mv l mw mx">#specifying hidden and cell state for decoder part as vector process it will get output predicted and again we add to decoder states<br/>decoder_state_input_h = Input(shape=(latent_dim,))<br/>decoder_state_input_c = Input(shape=(latent_dim,))<br/>decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]</span><span id="d738" class="jd je hi mq b fi my mv l mw mx">dec_emb2= dec_emb_layer(decoder_inputs) # Get the embeddings of the decoder sequence</span><span id="1cd5" class="jd je hi mq b fi my mv l mw mx"># To predict the next word in the sequence, set the initial states to the states from the previous time step</span><span id="ac68" class="jd je hi mq b fi my mv l mw mx">decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)<br/>decoder_states2 = [state_h2, state_c2]<br/>decoder_outputs2 = decoder_dense(decoder_outputs2)<br/> # A dense softmax layer to generate prob dist. over the target vocabulary</span><span id="b411" class="jd je hi mq b fi my mv l mw mx"># Final decoder model<br/>decoder_model = Model(<br/>    [decoder_inputs] + decoder_states_inputs,<br/>    [decoder_outputs2] + decoder_states2)</span><span id="3090" class="jd je hi mq b fi my mv l mw mx">#decoding unseen query:<br/>def decode_sequence(input_seq):<br/>    # Encode the input as state vectors.<br/>    states_value = encoder_model.predict(input_seq)<br/>    # Generate empty target sequence of length 1.<br/>    target_seq = np.zeros((1,1))<br/>    target_seq[0, 0] = mar_char_to_index_dict['START_']</span><span id="b515" class="jd je hi mq b fi my mv l mw mx">stop_condition = False<br/>    decoded_sentence = ''<br/>    while not stop_condition:<br/>        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)</span><span id="37f5" class="jd je hi mq b fi my mv l mw mx"># Sample a token<br/>        sampled_token_index = np.argmax(output_tokens[0, -1, :])<br/>        sampled_char = mar_index_to_char_dict[sampled_token_index]<br/>        if (sampled_char == '_END'):<br/>            break;<br/>        decoded_sentence += ' '+sampled_char<br/>        <br/>        target_seq = np.zeros((1,1))<br/>        target_seq[0, 0] = sampled_token_index</span><span id="a2a4" class="jd je hi mq b fi my mv l mw mx"># Update states<br/>        states_value = [h, c]</span><span id="bc45" class="jd je hi mq b fi my mv l mw mx">return decoded_sentence</span></pre><h1 id="6464" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">结论:</h1><ol class=""><li id="0e36" class="jy jz hi ih b ii ka im kb iq kc iu kd iy ke jc kf kg kh ki bi translated">我们理解seq2seq模型背后的概念，它的工作原理和局限性。</li><li id="578b" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">我们也明白了如何使用注意机制来解决seq2seq模型的问题。因此，通过使用注意机制，该模型能够找到输入序列和输出序列之间的映射。</li><li id="6d00" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">通过增加训练数据集和使用双向LSTM获得更好的上下文向量，可以提高模型的性能。</li><li id="9e81" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">使用<strong class="ih hj">波束搜索策略</strong>解码测试序列，而不是使用贪婪方法(argmax)</li><li id="b9ad" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">根据<strong class="ih hj"> BLEU分数</strong>评估您的模型的性能</li><li id="59d2" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">注意力的唯一缺点是它很费时间。为了克服这个问题，谷歌引入了“变压器模型”，我们将在未来的博客中看到。</li></ol><h1 id="9513" class="ko je hi bd jf kp kq kr jj ks kt ku jn kv kw kx jq ky kz la jt lb lc ld jw le bi translated">参考:</h1><ol class=""><li id="5b19" class="jy jz hi ih b ii ka im kb iq kc iu kd iy ke jc kf kg kh ki bi translated"><a class="ae md" href="https://www.appliedaicourse.com/course/11/Applied-Machine-learning-course" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></li><li id="f4f0" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated"><a class="ae md" href="https://www.coursera.org/lecture/nlp-sequence-models/attention-model-lSwVa" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/lecture/NLP-sequence-models/Attention-model-lSwVa</a>(吴恩达关于注意力的解释)</li><li id="487d" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">https://arxiv.org/abs/1409.3215<a class="ae md" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank"/></li><li id="fbb0" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated">【https://keras.io/examples/lstm_seq2seq/ T4】</li><li id="7e72" class="jy jz hi ih b ii kj im kk iq kl iu km iy kn jc kf kg kh ki bi translated"><a class="ae md" href="https://arxiv.org/abs/1406.1078" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1406.1078</a></li></ol><h2 id="badd" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">代码:</h2><p id="d4c0" class="pw-post-body-paragraph if ig hi ih b ii ka ik il im kb io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">完整代码将更新到我的Github repo: <a class="ae md" href="https://github.com/Mrjaggu" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="mi">此处</em> </strong> </a></p></div></div>    
</body>
</html>