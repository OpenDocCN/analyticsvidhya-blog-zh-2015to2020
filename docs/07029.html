<html>
<head>
<title>Distributed Topic Modelling using Spark NLP and Spark MLLib(LDA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Spark NLP和Spark MLLib(LDA)的分布式主题建模</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/distributed-topic-modelling-using-spark-nlp-and-spark-mllib-lda-6db3f06a4da3?source=collection_archive---------6-----------------------#2020-06-11">https://medium.com/analytics-vidhya/distributed-topic-modelling-using-spark-nlp-and-spark-mllib-lda-6db3f06a4da3?source=collection_archive---------6-----------------------#2020-06-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2119462976f7fe9968cd54addf962730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M05oBB_4ZD5OS_NkIdIyKw.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:大英百科全书</figcaption></figure><p id="6329" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">主题建模是自然语言处理中最常见的任务之一。从数以百万计的文档中提取主题分布在许多方面都是有用的，例如识别对特定产品或所有产品的投诉的原因，或者识别新闻文章中的主题的更经典的例子。我们不会深究什么是主题建模或者它是如何工作的细节。互联网上有很多关于它的好文章，但我发现<a class="ae js" href="https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/" rel="noopener ugc nofollow" target="_blank">这篇</a>文章来自Analytics Vidhya comprehensive。因此，如果您不熟悉主题建模，或者需要刷新您的记忆，请继续查看。</p><p id="fd7e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这个博客的目的是熟悉主题建模的分布式方法。假设您的数据湖(例如Hadoop)中有数十亿个文档，您希望更好地理解其中的主题。用python处理数十亿个文档会遇到一些计算限制和瓶颈。幸运的是，Spark MLlib提供了LDA的优化版本，它是专门为在分布式环境中工作而设计的。我们将构建一个简单的主题建模管道，使用Spark NLP预处理数据，使用Spark MLlib的ld a从数据中提取主题。</p><p id="ca84" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将使用新闻文章数据。你可以从<a class="ae js" href="https://github.com/ravishchawla/topic_modeling/blob/master/data/abcnews-date-text.csv" rel="noopener ugc nofollow" target="_blank">这个</a>链接下载数据集。让我们开始写一些代码。</p><h2 id="aa2c" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">初始化Spark会话</h2><p id="8186" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated">首先，我们将导入所有需要的包并初始化spark会话。您还可以在使用spark-submit调用spark应用程序时传递配置。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="fbe7" class="jt ju hi ky b fi lc ld l le lf"><em class="lg"># Import Spark NLP</em><br/><strong class="ky hj">from</strong> <strong class="ky hj">sparknlp.base</strong> <strong class="ky hj">import</strong> *<br/><strong class="ky hj">from</strong> <strong class="ky hj">sparknlp.annotator</strong> <strong class="ky hj">import</strong> *<br/><strong class="ky hj">from</strong> <strong class="ky hj">sparknlp.pretrained</strong> <strong class="ky hj">import</strong> PretrainedPipeline<br/><strong class="ky hj">import</strong> <strong class="ky hj">sparknlp</strong><br/><strong class="ky hj">from</strong> <strong class="ky hj">pyspark.sql</strong> <strong class="ky hj">import</strong> SparkSession<br/><strong class="ky hj">from</strong> <strong class="ky hj">pyspark.ml</strong> <strong class="ky hj">import</strong> Pipeline</span><span id="8cd1" class="jt ju hi ky b fi lh ld l le lf">spark = SparkSession.builder \<br/>    .appName("Spark NLP")\<br/>    .config("spark.driver.memory","8G")\ #change accordingly<br/>    .config("spark.driver.maxResultSize", "2G") \<br/>    .config("spark.jars.packages", "com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5")\<br/>    .config("spark.kryoserializer.buffer.max", "1000M")\<br/>    .getOrCreate()</span></pre><h2 id="a3ae" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">读取数据</h2><p id="972e" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated">我们可以从三个来源读取数据，即。本地、HDFS和S3。如果你的数据存储在S3，我建议你使用分布式拷贝，使用<a class="ae js" href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/UsingEMR_s3distcp.html" rel="noopener ugc nofollow" target="_blank"> S3DistCp </a>将数据传输到HDFS，然后从HDFS加载数据。这种方法减少了从S3读取所有数据、将数据分发到所有工作节点以及将数据加载到内存中所需的网络IO。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="b2ab" class="jt ju hi ky b fi lc ld l le lf"># if you are reading file from local storage<br/>file_location = r'path\to\abcnews_date_txt.csv'</span><span id="b339" class="jt ju hi ky b fi lh ld l le lf"># if you are reading file from hdfs<br/>file_location = r'hdfs:\\\user\path\to\abcnews_date_txt.csv'</span><span id="00ff" class="jt ju hi ky b fi lh ld l le lf">file_type = "csv"</span><span id="01e5" class="jt ju hi ky b fi lh ld l le lf"><em class="lg"># CSV options</em><br/>infer_schema = "true"<br/>first_row_is_header = "true"<br/>delimiter = ","</span><span id="a581" class="jt ju hi ky b fi lh ld l le lf">df = spark.read.format(file_type) \<br/>  .option("inferSchema", infer_schema) \<br/>  .option("header", first_row_is_header) \<br/>  .option("sep", delimiter) \<br/>  .load(file_location)</span><span id="98ea" class="jt ju hi ky b fi lh ld l le lf"><em class="lg"># Verify the count</em><br/>df.count()</span></pre><p id="d290" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">数据由两列组成，即发布日期和标题文本。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es li"><img src="../Images/ab18c5ef866639b7725690acaae90856.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*_fnTHaot-NLdqeHF_IyFzA.png"/></div></figure><h2 id="53ba" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">使用Spark NLP的预处理流水线</h2><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="457f" class="jt ju hi ky b fi lc ld l le lf"><em class="lg"># Spark NLP requires the input dataframe or column to be converted to document. </em><br/>document_assembler = DocumentAssembler() \<br/>    .setInputCol("headline_text") \<br/>    .setOutputCol("document") \<br/>    .setCleanupMode("shrink")</span><span id="b4a9" class="jt ju hi ky b fi lh ld l le lf"><em class="lg"># Split sentence to tokens(array)</em><br/>tokenizer = Tokenizer() \<br/>  .setInputCols(["document"]) \<br/>  .setOutputCol("token")</span><span id="84b4" class="jt ju hi ky b fi lh ld l le lf"><em class="lg"># clean unwanted characters and garbage</em><br/>normalizer = Normalizer() \<br/>    .setInputCols(["token"]) \<br/>    .setOutputCol("normalized")</span><span id="54a9" class="jt ju hi ky b fi lh ld l le lf"><em class="lg"># remove stopwords</em><br/>stopwords_cleaner = StopWordsCleaner()\<br/>      .setInputCols("normalized")\<br/>      .setOutputCol("cleanTokens")\<br/>      .setCaseSensitive(<strong class="ky hj">False</strong>)</span><span id="fa81" class="jt ju hi ky b fi lh ld l le lf"><em class="lg"># stem the words to bring them to the root form.</em><br/>stemmer = Stemmer() \<br/>    .setInputCols(["cleanTokens"]) \<br/>    .setOutputCol("stem")</span><span id="0fda" class="jt ju hi ky b fi lh ld l le lf"><em class="lg"># Finisher is the most important annotator. Spark NLP adds its own structure when we convert each row in the dataframe to document. Finisher helps us to bring back the expected structure viz. array of tokens.</em></span><span id="7692" class="jt ju hi ky b fi lh ld l le lf">finisher = Finisher() \<br/>    .setInputCols(["stem"]) \<br/>    .setOutputCols(["tokens"]) \<br/>    .setOutputAsArray(<strong class="ky hj">True</strong>) \<br/>    .setCleanAnnotations(<strong class="ky hj">False</strong>)</span><span id="9ae7" class="jt ju hi ky b fi lh ld l le lf"><em class="lg"># We build a ml pipeline so that each phase can be executed in sequence. This pipeline can also be used to test the model. </em><br/>nlp_pipeline = Pipeline(<br/>    stages=[document_assembler, <br/>            tokenizer,<br/>            normalizer,<br/>            stopwords_cleaner, <br/>            stemmer, <br/>            finisher])</span><span id="14fa" class="jt ju hi ky b fi lh ld l le lf"><em class="lg"># train the pipeline</em><br/>nlp_model = nlp_pipeline.fit(df)</span><span id="d7ef" class="jt ju hi ky b fi lh ld l le lf"><em class="lg"># apply the pipeline to transform dataframe.</em><br/>processed_df  = nlp_model.transform(df)</span><span id="7037" class="jt ju hi ky b fi lh ld l le lf"># nlp pipeline create intermediary columns that we dont need. So lets select the columns that we need</span><span id="e355" class="jt ju hi ky b fi lh ld l le lf">tokens_df = processed_df.select('publish_date','tokens').limit(10000)</span><span id="d52d" class="jt ju hi ky b fi lh ld l le lf">tokens_df.show()</span></pre><p id="af64" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Spark NLP管道的输出是一个经过清理和词干处理的标记列表。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/977f3317a79f79028ea7d8ed1bcd3239.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*uQClFOHxtMmRq1bu4NoiVg.png"/></div></figure><h2 id="0f2a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">特征工程</h2><p id="14c0" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated">我们将使用Spark MLlib的<a class="ae js" href="https://spark.apache.org/docs/latest/ml-features#countvectorizer" rel="noopener ugc nofollow" target="_blank">计数矢量器</a>从文本数据中生成特征。潜在的狄利克雷分配需要特定于数据的词汇表来执行主题建模。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="5bb7" class="jt ju hi ky b fi lc ld l le lf"><strong class="ky hj">from</strong> <strong class="ky hj">pyspark.ml.feature</strong> <strong class="ky hj">import</strong> CountVectorizer</span><span id="4750" class="jt ju hi ky b fi lh ld l le lf">cv = CountVectorizer(inputCol="tokens", outputCol="features", vocabSize=500, minDF=3.0)</span><span id="ee94" class="jt ju hi ky b fi lh ld l le lf"># train the model<br/>cv_model = cv.fit(tokens_df)</span><span id="9123" class="jt ju hi ky b fi lh ld l le lf"># transform the data. Output column name will be features.<br/>vectorized_tokens = cv_model.transform(tokens_df)</span></pre><h2 id="c396" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">构建LDA模型</h2><p id="388d" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated">LDA模型至少需要2个超参数，即k(主题数)和maxIter(迭代次数)。尝试不同的k值和maxIter值，看看哪种组合最适合您的数据。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="f4e7" class="jt ju hi ky b fi lc ld l le lf"><strong class="ky hj">from</strong> <strong class="ky hj">pyspark.ml.clustering</strong> <strong class="ky hj">import</strong> LDA</span><span id="8dc6" class="jt ju hi ky b fi lh ld l le lf">num_topics = 3</span><span id="d4bb" class="jt ju hi ky b fi lh ld l le lf">lda = LDA(k=num_topics, maxIter=10)<br/>model = lda.fit(vectorized_tokens)</span><span id="4f43" class="jt ju hi ky b fi lh ld l le lf">ll = model.logLikelihood(vectorized_tokens)<br/>lp = model.logPerplexity(vectorized_tokens)</span><span id="1e72" class="jt ju hi ky b fi lh ld l le lf">print("The lower bound on the log likelihood of the entire corpus: " + str(ll))<br/>print("The upper bound on perplexity: " + str(lp))</span></pre><h2 id="bc1c" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">将主题可视化</h2><p id="b061" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated">完成训练后，我们可以使用下面的代码查看代表每个主题的单词；</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="5b56" class="jt ju hi ky b fi lc ld l le lf"># extract vocabulary from CountVectorizer<br/>vocab = cv_model.vocabulary</span><span id="f803" class="jt ju hi ky b fi lh ld l le lf">topics = model.describeTopics()   <br/>topics_rdd = topics.rdd</span><span id="4d80" class="jt ju hi ky b fi lh ld l le lf">topics_words = topics_rdd\<br/>       .map(<strong class="ky hj">lambda</strong> row: row['termIndices'])\<br/>       .map(<strong class="ky hj">lambda</strong> idx_list: [vocab[idx] <strong class="ky hj">for</strong> idx <strong class="ky hj">in</strong> idx_list])\<br/>       .collect()</span><span id="2847" class="jt ju hi ky b fi lh ld l le lf"><strong class="ky hj">for</strong> idx, topic <strong class="ky hj">in</strong> enumerate(topics_words):<br/>    print("topic: <strong class="ky hj">{}</strong>".format(idx))<br/>    print("*"*25)<br/>    <strong class="ky hj">for</strong> word <strong class="ky hj">in</strong> topic:<br/>       print(word)<br/>    print("*"*25)</span></pre><p id="7fd6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">以下是输出，</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lk"><img src="../Images/5d8b28c470d7ba008226b4dbbce35381.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hRJrA9f7IkkUvrykm0Ifng.png"/></div></div></figure><h2 id="cbd3" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jf ke kf kg jj kh ki kj jn kk kl km kn bi translated">结论</h2><p id="d255" class="pw-post-body-paragraph iu iv hi iw b ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn ks jp jq jr hb bi translated">从机器学习能力的单一角度来看，Spark MLlib不如通过Python直接获得的大量机器学习库丰富。Spark MLlib增加价值的地方是，通过为大型数据集上的基本ML任务提供分布式计算能力。随着每个版本的重大改进，Apache Spark正在慢慢弥合大数据和机器学习之间的差距。</p><p id="a142" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">感谢您的阅读！喜欢并留下评论，如果你觉得有趣和有用。</p></div></div>    
</body>
</html>