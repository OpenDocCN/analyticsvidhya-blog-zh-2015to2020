# 使用平均方法实现集成学习

> 原文：<https://medium.com/analytics-vidhya/implementing-ensemble-learning-using-averaging-methods-7185d572f47?source=collection_archive---------27----------------------->

![](img/3f7ecb51885b15f64ce29ebac1db97a9.png)

劳尔·瓦尔扎尔在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

这是我的续篇博客，如果你没有看过之前的博客，请务必参考之前的。

大家好！希望一切平安！我们来深入探讨一下这个概念。

**话题覆盖**

01.粘贴与装袋

02.作为决策树集合的随机森林

03.随机补丁

04.随机子空间

05.使用额外的树构建更多随机、多样的个体预测器

在这里，我们将讨论如何平均方法可以用来建立集合模型。平均涉及并行训练多个预测器，并且这些预测器在我们的训练数据的随机子集上训练。粘贴在所有方面都类似于打包，除了我们采样训练数据而不替换。随机森林是非常流行的集成模型，其中集成由决策树组成。我们将理解随机补丁来训练我们的模型和随机子空间。当我们有一组不同的个体预测器时，集成学习效果很好。我们要明确个别学习者的概念。个体学习者独立于其他学习者，并且这些个体学习者可以被并行训练，这就是允许平均方法很好地扩展的原因。

> 如果房间里的每个人都在想同样的事情，那么肯定有人没在想——巴顿将军

**打包粘贴:**模型的集合(collection)，其中在训练数据的不同随机子集上训练单个模型。

**打包和粘贴是并行训练单个模型的两种最常见的方法。别忘了我在之前的[博客](/analytics-vidhya/employing-ensemble-methods-with-scikit-learn-b6714384fed3)中讨论过关于发展集成学习的三个关键问题。集成学习的主要概念是个体学习者应该尽可能不同，但是对于大多数技术来说，很难生成大量非常不同的模型。这就是为什么决策树和随机森林集成是最常用的。**

**在这两者之间，bagging 通常是首选，因为它倾向于产生更多不相关的预测。Bagging 对训练数据的过拟合较少，建立的模型方差误差较低，与粘贴相比有较高的偏倚。打包和粘贴都是训练集合的平均方法的例子。**

****随机子空间:**决策树的集合(集合)，其中个体树在训练数据中的不同随机特征子集上被训练，但是保持训练数据中的所有点。**

****随机补丁:**决策树的集合(集合)，其中在训练数据中不同的随机特征子集以及训练数据点的随机子集上训练个体树。**

****极度随机化(Extra):** Trees 决策树的集合，其中使用随机分裂点(而不是寻找最佳阈值)在训练数据中的不同随机特征子集上训练个体树。**

****平均:**单个学习者是独立的，可以并行构建树，学习者不会从其他学习者的错误中学习。**

****Boosting:** 单个学习者链接到以前的学习者，需要按顺序构建树，单个学习者明确配置为从以前的错误中学习。**