<html>
<head>
<title>How I Built A Document Classification System using Deep Convolutional Neural Networks!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我如何使用深度卷积神经网络构建了一个文档分类系统！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-i-built-a-document-classification-system-using-deep-convolutional-neural-networks-e1d9a83cbabd?source=collection_archive---------0-----------------------#2019-11-01">https://medium.com/analytics-vidhya/how-i-built-a-document-classification-system-using-deep-convolutional-neural-networks-e1d9a83cbabd?source=collection_archive---------0-----------------------#2019-11-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex if ig ih ii"><div class="bz dy l di"><div class="ij ik l"/></div></figure><p id="89b7" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">随着深度学习的出现，OCR(光学字符识别)的工具改进了很多，变得更加高效。</p><p id="65df" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">你们知道谷歌文档本身有一个内置的OCR功能吗？</p><figure class="jj jk jl jm fd ii"><div class="bz dy l di"><div class="jn ik l"/></div></figure><h2 id="25f6" class="jo jp hi bd jq jr js jt ju jv jw jx jy iw jz ka kb ja kc kd ke je kf kg kh ki bi translated">那么，文档分类管道如何帮助改进OCR呢？</h2><p id="a65c" class="pw-post-body-paragraph il im hi in b io kj iq ir is kk iu iv iw kl iy iz ja km jc jd je kn jg jh ji hb bi translated">在内容检测OCR中，我们可以使用文档分类管道来索引不同的文档，以便OCR可以分别针对不同的布局高效地操作。为各种类型的文档，如“研究论文、新闻论文、报告、表格、简历”建立一个通用的OCR是一项非常艰巨的任务，但如果我们可以划分算法，它可以有效地完成所有任务。</p><p id="b0ab" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">在OCR运行之前进行文档分类可以帮助我们以不同的方式对不同种类的文档图像进行预处理，从而使OCR的任务变得更加容易。</p><p id="05c0" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们开始吧！</p><h2 id="f391" class="jo jp hi bd jq jr js jt ju jv jw jx jy iw jz ka kb ja kc kd ke je kf kg kh ki bi translated">数据集描述</h2><p id="0d72" class="pw-post-body-paragraph il im hi in b io kj iq ir is kk iu iv iw kl iy iz ja km jc jd je kn jg jh ji hb bi translated">RVL-CDIP(瑞尔森视觉实验室复杂文档信息处理)数据集由16类400，000幅灰度图像组成，每类25，000幅图像。有320，000幅训练图像、40，000幅验证图像和40，000幅测试图像。调整图像大小，使其最大尺寸不超过1000像素。</p><h2 id="a326" class="jo jp hi bd jq jr js jt ju jv jw jx jy iw jz ka kb ja kc kd ke je kf kg kh ki bi translated">让我们使用深度卷积神经网络来构建我们的文档分类系统，仅使用1/3的数据，准确率就超过90%!</h2><p id="7728" class="pw-post-body-paragraph il im hi in b io kj iq ir is kk iu iv iw kl iy iz ja km jc jd je kn jg jh ji hb bi translated"><strong class="in hj">这16个班级如下:</strong></p><ol class=""><li id="fe6f" class="ko kp hi in b io ip is it iw kq ja kr je ks ji kt ku kv kw bi translated">信</li><li id="33fc" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">形式</li><li id="b137" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">电子邮件</li><li id="3e4b" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">手写的</li><li id="6d00" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">广告</li><li id="d591" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">科学报告</li><li id="a907" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">科学出版物</li><li id="06de" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">规格</li><li id="2536" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">文件夹</li><li id="eb71" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">新闻文章</li><li id="51af" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">预算</li><li id="a359" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">发票</li><li id="b140" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">介绍会；展示会</li><li id="9efd" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">调查表</li><li id="d52a" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">简历</li><li id="e45c" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">备忘录</li></ol><h2 id="f865" class="jo jp hi bd jq jr js jt ju jv jw jx jy iw jz ka kb ja kc kd ke je kf kg kh ki bi translated">让我们来看看数据集中的一些样本:</h2><div class="jj jk jl jm fd ab cb"><figure class="lc ii ld le lf lg lh paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><img src="../Images/7f6f1d4b516e37daa22cd573a42786cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*5Q7slgXWyf8On22hgrhtzA.png"/></div></figure><figure class="lc ii lo le lf lg lh paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><img src="../Images/ba03b2ce6f0b27bf5d79641ca74c66df.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*hFgW1ryfXZmzXbvQgqta-g.png"/></div></figure><figure class="lc ii lp le lf lg lh paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><img src="../Images/491df3a28af646ed6b1f0986dc922929.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*O99GMGyhCKw4ES3I93_Thg.png"/></div></figure></div><h2 id="a11c" class="jo jp hi bd jq jr js jt ju jv jw jx jy iw jz ka kb ja kc kd ke je kf kg kh ki bi translated">创建结构化数据</h2><p id="0874" class="pw-post-body-paragraph il im hi in b io kj iq ir is kk iu iv iw kl iy iz ja km jc jd je kn jg jh ji hb bi translated">标签文件以下列格式列出图像及其类别:</p><p id="3c4b" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated"><code class="du lq lr ls lt b">path/to/the/image.tif category</code></p><p id="a25f" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们创建一个脚本，这样我们就可以得到两个单独的pickel文件，分别用于标签和图像路径</p><pre class="jj jk jl jm fd lu lt lv lw aw lx bi"><span id="e8b1" class="jo jp hi lt b fi ly lz l ma mb">import os<br/>import joblib<br/>count=0<br/>labels=[]</span><span id="b053" class="jo jp hi lt b fi mc lz l ma mb">directories=['C:/Users/sambal/Desktop/labels/train.txt','C:/Users/sambal/Desktop/labels/test.txt','C:/Users/sambal/Desktop/labels/val.txt']<br/>train_path=[]<br/>test_path=[]<br/>val_path=[]<br/>paths=[train_path,test_path,val_path]<br/>y_train=[]<br/>y_test=[]<br/>y_cv=[]<br/>labels=[y_train,y_test,y_cv]</span><span id="e305" class="jo jp hi lt b fi mc lz l ma mb">for i in range(3):<br/>        label=[]<br/>        with open(directories[i],'r') as f:<br/>            for line in f:<br/>                label.append(line)</span><span id="2681" class="jo jp hi lt b fi mc lz l ma mb">path=[]</span><span id="fe9a" class="jo jp hi lt b fi mc lz l ma mb">for x in label:</span><span id="85db" class="jo jp hi lt b fi mc lz l ma mb">category=x.split(" ")[1][-2::-1][::-1]<br/>            labels[i].append(category)<br/>            y=x.split(" ")[0]<br/>            paths[i].append(y)</span><span id="a2a0" class="jo jp hi lt b fi mc lz l ma mb">for i in range(3):<br/>    print(paths[i][0],labels[i][0])</span><span id="2425" class="jo jp hi lt b fi mc lz l ma mb">joblib.dump(labels,"labels")<br/>joblib.dump(paths,"paths")</span></pre><p id="e412" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">一旦我们将标签和路径分离到文件中，我们现在就可以将数据过滤成三个部分——Train、Test和Cv。</p><h2 id="299a" class="jo jp hi bd jq jr js jt ju jv jw jx jy iw jz ka kb ja kc kd ke je kf kg kh ki bi translated">让我们使用一些图像样本来检查图像大小是否有任何变化</h2><pre class="jj jk jl jm fd lu lt lv lw aw lx bi"><span id="9996" class="jo jp hi lt b fi ly lz l ma mb">train_width=[]</span><span id="63c1" class="jo jp hi lt b fi mc lz l ma mb">test_width=[]</span><span id="afe1" class="jo jp hi lt b fi mc lz l ma mb">cv_width=[]</span><span id="2d2e" class="jo jp hi lt b fi mc lz l ma mb">count=0</span><span id="c539" class="jo jp hi lt b fi mc lz l ma mb">for image in train[:100]:</span><span id="0b22" class="jo jp hi lt b fi mc lz l ma mb">im = cv2.imread(image,cv2.IMREAD_GRAYSCALE)</span><span id="b44f" class="jo jp hi lt b fi mc lz l ma mb">train_width.append(im.shape[1])</span><span id="1bb6" class="jo jp hi lt b fi mc lz l ma mb">for image in test[:100]:</span><span id="9139" class="jo jp hi lt b fi mc lz l ma mb">im = cv2.imread(image,cv2.IMREAD_GRAYSCALE)</span><span id="bdbb" class="jo jp hi lt b fi mc lz l ma mb">test_width.append(im.shape[1])</span><span id="6469" class="jo jp hi lt b fi mc lz l ma mb">count+=1</span><span id="06f6" class="jo jp hi lt b fi mc lz l ma mb">for image in cv[:100]:</span><span id="5a51" class="jo jp hi lt b fi mc lz l ma mb">im = cv2.imread(image,cv2.IMREAD_GRAYSCALE)</span><span id="4558" class="jo jp hi lt b fi mc lz l ma mb">cv_width.append(im.shape[1])</span></pre><p id="c01a" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">让我们形象化的图像宽度分布。</p><pre class="jj jk jl jm fd lu lt lv lw aw lx bi"><span id="1e62" class="jo jp hi lt b fi ly lz l ma mb">import seaborn as sb<br/>from matplotlib import pyplot as plt</span><span id="7025" class="jo jp hi lt b fi mc lz l ma mb">width=(train_width,test_width,cv_width)</span><span id="9c17" class="jo jp hi lt b fi mc lz l ma mb">for x in width:<br/>   sb.distplot(x,kde = True)<br/>   plt.show()</span></pre><figure class="jj jk jl jm fd ii er es paragraph-image"><div class="er es md"><img src="../Images/f97fec539446cf5fde08245335a728b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*cGAE1xN2UPBPefbQJNfQvg.png"/></div></figure><p id="d2be" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们看到，对于所有三个测试、训练和CV数据图像，图像的宽度存在很大差异。</p><p id="ca45" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">因此，调整图像大小将是这个项目的重要组成部分。</p><h2 id="2f8a" class="jo jp hi bd jq jr js jt ju jv jw jx jy iw jz ka kb ja kc kd ke je kf kg kh ki bi translated">检查阶级不平衡</h2><figure class="jj jk jl jm fd ii er es paragraph-image"><div class="er es me"><img src="../Images/63f166153e87409a60be89bd34bbf3dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*sNhAmUwi6-lRXcpAheYghA.png"/></div></figure><p id="87de" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">这是一个相当平衡的数据集，所有类别都具有相同的代表性。</p><h2 id="06c5" class="jo jp hi bd jq jr js jt ju jv jw jx jy iw jz ka kb ja kc kd ke je kf kg kh ki bi translated">构建生成器和文件结构</h2><p id="7424" class="pw-post-body-paragraph il im hi in b io kj iq ir is kk iu iv iw kl iy iz ja km jc jd je kn jg jh ji hb bi translated">我们将有3个文件夹，即培训、测试和简历。</p><p id="8394" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">每个文件夹将有16个文件，代表从0到15的每个类</p><p id="d54e" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们将所有与标签相关的数据移动到各自的文件夹中。</p><p id="39fe" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated"><strong class="in hj">由于计算能力的限制，现在我们只将每节课10k的训练数据转移到这些文件夹中！</strong></p><p id="f2b9" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">由于数据非常庞大，我们需要一个Keras生成器将数据批次从磁盘加载到模型中，以实现高效的I/O。因此，我们初始化了Keras数据生成器，还包括数据标准化方面，这样我们的数据范围为0到1，除以255。</p><figure class="jj jk jl jm fd ii er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mf"><img src="../Images/d55478492ba0560eac1bd4c710e6709f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0SBQ8377xam2I8guakGNA.png"/></div></div></figure><h1 id="b723" class="mg jp hi bd jq mh mi mj ju mk ml mm jy mn mo mp kb mq mr ms ke mt mu mv kh mw bi translated">建筑</h1><figure class="jj jk jl jm fd ii er es paragraph-image"><div class="er es mx"><img src="../Images/27b19bf99c61ac1c56e20a34b12442a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*h2eSV17mtE4FjGIUuSt2_Q.png"/></div></figure><p id="a3ec" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">哦！好吧，我上面说的这个东西看起来超级复杂，对吧？</p><p id="cfae" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">所以让我们把它一点一点的分解吧！</p><h1 id="62f8" class="mg jp hi bd jq mh mi mj ju mk ml mm jy mn mo mp kb mq mr ms ke mt mu mv kh mw bi translated">基础模型</h1><h2 id="b653" class="jo jp hi bd jq jr js jt ju jv jw jx jy iw jz ka kb ja kc kd ke je kf kg kh ki bi translated"><strong class="ak"> 1。型号_最终_结果</strong></h2><p id="d2aa" class="pw-post-body-paragraph il im hi in b io kj iq ir is kk iu iv iw kl iy iz ja km jc jd je kn jg jh ji hb bi translated">我们使用的是在imgenet上训练过的重量初始化的<strong class="in hj"> Inceptionresnet-V2 </strong>。我们将调整我们的图像大小为128x128，并训练这个模型。我们解冻所有层并训练。</p><p id="4d38" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated"><strong class="in hj">你可能听说过resnet和Inception-net，但是什么是inception_resnet_v2呢？</strong></p><p id="71a7" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">顾名思义，它基本上是一个来自Resnet的Inception-net结合剩余模块的概念。</p><p id="c2d8" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">剩余连接允许模型中的捷径，并允许研究人员成功地训练更深的神经网络，这导致了更好的性能。这也极大地简化了初始阶段。</p><figure class="jj jk jl jm fd ii er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es my"><img src="../Images/896e7e628153530afec5cd24f8e6b09b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cg32Mgd9mLx5SJ2dfJzvTQ.png"/></div></div></figure><figure class="jj jk jl jm fd ii er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mz"><img src="../Images/8890b5965af4eae39f6608d941902e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y62qQ3vtC4DSOP30CD30tg.png"/></div></div></figure><p id="2cc3" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们使用Keras应用程序API导入带有imagenet权重的inception_resnet_v2模型，并使用(None，None，3)将它设置为变量输入形状。稍后我们会在这个博客上做一些有趣的事情。</p><figure class="jj jk jl jm fd ii er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es na"><img src="../Images/0263e62d5449b90a6f1f87ce408c53de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zR1uK5AJTF3wLu5GPODEQQ.png"/></div></div></figure><p id="2df6" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们移除inception_resnet_v2模型的顶部，并附加2个具有下降的密集层，最后附加一个大小为16的Softmax层。</p><p id="7a3e" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们将使用旧的经典SGD，lr=0.1，momentum=0.0，nesterov=False </p><figure class="jj jk jl jm fd ii er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es nb"><img src="../Images/d0dfc2c83e64a65ffaca01536a71cc96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VTS31hTJGhmNOPmbWrQ6gA.png"/></div></div></figure><p id="1c00" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">如果验证精度在2个时期后没有提高10%,我们将惩罚我们的学习率，并最终用batch_size=128训练它30个时期。</p><p id="05eb" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated"><strong class="in hj">经过30个时期的训练，我们在测试数据上获得了相当不错的85%的准确率。</strong></p><h2 id="1e80" class="jo jp hi bd jq jr js jt ju jv jw jx jy iw jz ka kb ja kc kd ke je kf kg kh ki bi translated">2.模型_最终_结果_2</h2><p id="e849" class="pw-post-body-paragraph il im hi in b io kj iq ir is kk iu iv iw kl iy iz ja km jc jd je kn jg jh ji hb bi translated">在我们用128x128的图像大小训练了我们的基本模型之后，我们将把这些权重转移到另一个inception_resnet_v2模型，在那里我们将在256x256大小的图像上训练。</p><p id="1d7e" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">这是我从Fast.ai课程中学到的一个小技巧，杰瑞米·霍华德在较小的图像上训练，然后使用这些权重初始化相同的架构来训练较大的图像。</p><p id="bd47" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我认为这种技术可行的原因是，当一个较小尺寸的图像被提供给我们的架构时，它试图学习所有被较小图像尺寸限制的微小特征。然而，一旦它在约束条件下学习了这些特征，我们的模型最终将很好地处理较大尺寸的图像。这就像<strong class="in hj">数据扩充</strong>迫使你的模型学习更好的特性，有点像正则化。</p><p id="0753" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">所以一切都保持不变，除了我们改变我们的图像大小为256X256，并在上述相同的条件下训练20个时期。</p><p id="0db2" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated"><strong class="in hj">瓦拉！我们在测试数据上获得了89.37%的准确率</strong></p><p id="1024" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated"><strong class="in hj"> model_final_res_2现在将成为我们的基础模型。</strong></p><h1 id="759f" class="mg jp hi bd jq mh mi mj ju mk ml mm jy mn mo mp kb mq mr ms ke mt mu mv kh mw bi translated">域内迁移学习</h1><p id="061a" class="pw-post-body-paragraph il im hi in b io kj iq ir is kk iu iv iw kl iy iz ja km jc jd je kn jg jh ji hb bi translated">迁移学习涉及将机器学习模型在一个领域获得的经验转移到另一个相关领域。虽然文档分类和对象分类看起来似乎是不同的领域，但在1000类ImageNet数据集上训练的架构已被证明可以作为广义特征提取器。</p><p id="c5df" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">在这项工作中，在ImageNet上训练的inception_resnet_v2模型被用作我们的整体模型的初始权重，从而构成初始一级(L1)权重转移。当然，L1迁移源自不同的领域，是迁移学习的常规领域间形式。然而，在RVL-CDIP数据集的完整图像上训练的整体模型可以被认为是广义的文档特征提取器。基于区域的模型的训练集，尽管包含文档区域的图像并且具有不同的比例，但本质上仍然是文档的图像。因此，通过建立转移学习的另一层(L2)来利用这个概念，其中用来自整体模型而不是原始inception_resnet_v2模型的权重来初始化基于区域的模型。</p><p id="26aa" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">使用域内迁移学习的概念，我们现在将通过以不同方式裁剪我们的图像来训练<strong class="in hj">区域特定的</strong>模型。</p><h1 id="1fe0" class="mg jp hi bd jq mh mi mj ju mk ml mm jy mn mo mp kb mq mr ms ke mt mu mv kh mw bi translated">特定区域模型</h1><p id="e1df" class="pw-post-body-paragraph il im hi in b io kj iq ir is kk iu iv iw kl iy iz ja km jc jd je kn jg jh ji hb bi translated"><strong class="in hj"> 1。顶部裁剪</strong></p><p id="cbc7" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们从上到下取前256个像素，然后裁剪剩下的。</p><p id="c80d" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们通过构建一个裁剪函数并将该函数传递给生成器来实现这一点。</p><figure class="jj jk jl jm fd ii er es paragraph-image"><div class="er es nc"><img src="../Images/bf2cd8f15f3cbe44c652d880f9f10f82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*4M66l0u-glU7EqJ_WMk0pA.png"/></div></figure><p id="2c21" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们现在将权重从model_final_res_2加载到新的inception_resenet_v2模型中，并对其进行5个时期的训练。</p><p id="e658" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">仅使用图像的顶部区域，我们得到85.6%</p><p id="1063" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated"><strong class="in hj"> 2。底部裁剪</strong></p><p id="9185" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们从下到上取最后的256个像素，然后裁剪剩下的。</p><figure class="jj jk jl jm fd ii er es paragraph-image"><div class="er es nd"><img src="../Images/a849f4e6b53124cd42175d287ca695a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*AdkKCS_n2qGbBOMoKLbG_Q.png"/></div></figure><p id="bbb6" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">仅使用图像的底部区域，我们得到82.09%</p><p id="11f9" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated"><strong class="in hj"> 3。左侧裁剪</strong></p><p id="d57b" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们从左到右取最左边的256个像素，然后裁剪剩下的像素。</p><figure class="jj jk jl jm fd ii er es paragraph-image"><div class="er es ne"><img src="../Images/7dd70fd2c8c38ca348b1fd59adf18da4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*7mxgtCpzNXbZiduc-wAi9w.png"/></div></figure><p id="f102" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">仅使用图像的左侧区域，我们得到86.77%</p><p id="dfcc" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated"><strong class="in hj"> 4。右键单击</strong></p><p id="2573" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们从右到左取最右边的256个像素，然后裁剪剩下的像素。</p><figure class="jj jk jl jm fd ii er es paragraph-image"><div class="er es ne"><img src="../Images/7dd70fd2c8c38ca348b1fd59adf18da4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*7mxgtCpzNXbZiduc-wAi9w.png"/></div></figure><p id="b639" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">仅使用图像的区域，我们得到84.36%</p><p id="da6e" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们使用学习率为0.0001的Adam optimizer来训练所有区域特定的模型。</p><p id="adbf" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">到目前为止，我们已经训练好了所有的模型。</p><h1 id="4ebb" class="mg jp hi bd jq mh mi mj ju mk ml mm jy mn mo mp kb mq mr ms ke mt mu mv kh mw bi translated">堆叠概括</h1><p id="abdc" class="pw-post-body-paragraph il im hi in b io kj iq ir is kk iu iv iw kl iy iz ja km jc jd je kn jg jh ji hb bi translated">让我们使用堆叠的力量得到SOTA结果。</p><figure class="jj jk jl jm fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/08b6dce515b9bdf85ba31d2f11a17c65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*D6Vxo2BJqQegET9VsnN50A.png"/></div></figure><p id="1a80" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们带着我们所有的5个模特</p><ol class=""><li id="669e" class="ko kp hi in b io ip is it iw kq ja kr je ks ji kt ku kv kw bi translated">整体模型:模型_最终_结果_2</li><li id="52c1" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">顶部裁剪模型:模式_最终_顶部</li><li id="d0eb" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">底部裁剪模型:模式_最终_底部</li><li id="b2c2" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">左侧裁剪模型:模式_最终_左侧</li><li id="c565" class="ko kp hi in b io kx is ky iw kz ja la je lb ji kt ku kv kw bi translated">右裁剪模型:模式_最终_右</li></ol><p id="86c5" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">将所有验证数据和测试数据的每个模型的softmax输出连接在一起。</p><p id="9035" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">我们将使用从验证数据生成的softmax输出来训练一个简单的2层MLP元分类器，然后我们将在测试数据上测试它。</p><figure class="jj jk jl jm fd ii er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ng"><img src="../Images/446e505d1a87d754c221fe9adc9ad3b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*loHRTTc1A0M1V-3e59pJxg.png"/></div></div></figure><figure class="jj jk jl jm fd ii er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es nh"><img src="../Images/0759ca88999e78d9e9085fd296159852.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DlnLbrRIuObXzizE4fUmrw.png"/></div></div></figure><p id="11d0" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">经过20个时期的训练后，我们在测试和训练数据上都获得了90.5 %的准确度。</p><h1 id="48ce" class="mg jp hi bd jq mh mi mj ju mk ml mm jy mn mo mp kb mq mr ms ke mt mu mv kh mw bi translated"><strong class="ak">管道</strong></h1><pre class="jj jk jl jm fd lu lt lv lw aw lx bi"><span id="9369" class="jo jp hi lt b fi ly lz l ma mb">from matplotlib import cm</span><span id="050e" class="jo jp hi lt b fi mc lz l ma mb">cmap = cm.get_cmap('tab20')</span><span id="87d2" class="jo jp hi lt b fi mc lz l ma mb">def height_crop(path,type):</span><span id="c071" class="jo jp hi lt b fi mc lz l ma mb">    image=cv2.imread(path)</span><span id="60b7" class="jo jp hi lt b fi mc lz l ma mb">    image = cv2.resize(image, (512,256))</span><span id="c471" class="jo jp hi lt b fi mc lz l ma mb">    if type=='bottom':</span><span id="3bbb" class="jo jp hi lt b fi mc lz l ma mb">         image =image[-256:,:,:]</span><span id="0f51" class="jo jp hi lt b fi mc lz l ma mb">    else:</span><span id="430a" class="jo jp hi lt b fi mc lz l ma mb">          image =image[:256,:,:]</span><span id="9080" class="jo jp hi lt b fi mc lz l ma mb">     return image</span><span id="8b4b" class="jo jp hi lt b fi mc lz l ma mb">def width_crop(path,type):</span><span id="da1e" class="jo jp hi lt b fi mc lz l ma mb">   image=cv2.imread(path)<br/>   image = cv2.resize(image, (256,512)) <br/>   if type=='right':</span><span id="c8ca" class="jo jp hi lt b fi mc lz l ma mb">           image =image[:,:256,:]<br/>   else:</span><span id="c740" class="jo jp hi lt b fi mc lz l ma mb">           image =image[:,-256:,:]</span><span id="b485" class="jo jp hi lt b fi mc lz l ma mb">   return image</span><span id="ceca" class="jo jp hi lt b fi mc lz l ma mb">def full_image(path):</span><span id="9b69" class="jo jp hi lt b fi mc lz l ma mb">      image=cv2.imread(path)<br/>      image = cv2.resize(image, (256,256))<br/>      return image</span><span id="deb5" class="jo jp hi lt b fi mc lz l ma mb">def preprocess(im):</span><span id="756b" class="jo jp hi lt b fi mc lz l ma mb">     im = im/255<br/>     im = np.expand_dims(im, axis=0)<br/>     return im</span><span id="29ba" class="jo jp hi lt b fi mc lz l ma mb">def predictions(image):</span><span id="4a14" class="jo jp hi lt b fi mc lz l ma mb">   top_pred=top.predict(image[0])<br/>   bottom_pred=bottom.predict(image[1])<br/>   left_pred=left.predict(image[2])<br/>   right_pred=right.predict(image[3])<br/>   holistic_pred=main.predict(image[4])<br/>   total_features=np.hstack((top_pred,bottom_pred,<br/>   left_pred,right_pred,holistic_pred))<br/>   prediction=mlp_classifier.predict(total_features)</span><span id="e3ac" class="jo jp hi lt b fi mc lz l ma mb">   return prediction</span><span id="e846" class="jo jp hi lt b fi mc lz l ma mb">def plot_bar_x(axes,prediction,doc_type):</span><span id="3e9e" class="jo jp hi lt b fi mc lz l ma mb">       sort_index=np.argsort(prediction)[::-1]<br/>       df=pd.DataFrame({'Document_Type':doc_type,<br/>                         'Percentage':prediction})<br/>       df=df.sort_values('Percentage',ascending=True)<br/>       labels=df['Document_Type']  <br/>       df.plot(kind='barh',y='Percentage',x='Document_Type',       <br/>ax=axes,colormap=cmap)</span></pre><figure class="jj jk jl jm fd ii er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ni"><img src="../Images/db0f5d5d2dbd375c3f4c24a3ed60604a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zmCjmTU2dfB-MtCvrfvf0g.png"/></div></div></figure><p id="3969" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated"><strong class="in hj">我们对随机图像的输出非常准确！！</strong></p><h1 id="305f" class="mg jp hi bd jq mh mi mj ju mk ml mm jy mn mo mp kb mq mr ms ke mt mu mv kh mw bi translated"><strong class="ak">结论</strong></h1><p id="01a6" class="pw-post-body-paragraph il im hi in b io kj iq ir is kk iu iv iw kl iy iz ja km jc jd je kn jg jh ji hb bi translated">我们使用域内迁移学习，然后使用堆叠泛化，用1/3的数据实现了90.5%的准确率。</p><p id="2061" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">使用更多的数据可以产生更好的结果，我正在努力。</p><p id="1b55" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">一旦我们很快超过92.4基准，我会随时通知你！！</p><p id="9990" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">您可以从以下网址获得完整代码:<a class="ae nj" href="https://github.com/sambalshikhar/Document-Image-Classification-with-Intra-Domain-Transfer-Learning-and-Stacked-Generalization-of-Deep" rel="noopener ugc nofollow" target="_blank">https://github . com/sambalshikhar/Document-Image-class ification-with-Intra-Domain-Transfer-Learning-and-Stacked-Generalization-of-Deep</a></p><p id="3beb" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated">注意:笔记本显示最终结果为93%,这是没有重置发电机的错误。修复后，我们发现准确率为90.5%。</p><h1 id="da67" class="mg jp hi bd jq mh mi mj ju mk ml mm jy mn mo mp kb mq mr ms ke mt mu mv kh mw bi translated">学分:</h1><div class="nk nl ez fb nm nn"><a href="https://arxiv.org/abs/1801.09321" rel="noopener  ugc nofollow" target="_blank"><div class="no ab dw"><div class="np ab nq cl cj nr"><h2 class="bd hj fi z dy ns ea eb nt ed ef hh bi translated">基于域内迁移学习和层叠泛化的文档图像分类</h2><div class="nu l"><h3 class="bd b fi z dy ns ea eb nt ed ef dx translated">本文提出了一个基于区域的深度卷积神经网络框架，用于文档结构学习</h3></div><div class="nv l"><p class="bd b fp z dy ns ea eb nt ed ef dx translated">arxiv.org</p></div></div></div></a></div><p id="3199" class="pw-post-body-paragraph il im hi in b io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji hb bi translated"><a class="ae nj" href="https://ai.googleblog.com/2016/08/improving-inception-and-image.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2016/08/improving-inception-and-image . html</a></p></div></div>    
</body>
</html>