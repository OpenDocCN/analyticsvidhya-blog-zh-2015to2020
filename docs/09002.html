<html>
<head>
<title>Named Entity Recognition (NER) for CoNLL dataset with Tensorflow 2.2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Tensorflow 2.2.0 中 CoNLL 数据集的命名实体识别(NER)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ner-tensorflow-2-2-0-9f10dcf5a0a?source=collection_archive---------0-----------------------#2020-08-22">https://medium.com/analytics-vidhya/ner-tensorflow-2-2-0-9f10dcf5a0a?source=collection_archive---------0-----------------------#2020-08-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ce39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本博客详细介绍了使用 Tensorflow2.2.0 对句子(<a class="ae jd" href="https://www.clips.uantwerpen.be/conll2003/ner/" rel="noopener ugc nofollow" target="_blank"> CoNLL-2003 数据集</a>)进行命名实体识别(NER)标记的步骤</p><p id="04e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://www.clips.uantwerpen.be/conll2003/ner/" rel="noopener ugc nofollow" target="_blank"> CoNLL-2003 </a> <a class="ae jd" href="https://www.clips.uantwerpen.be/conll2003/ner/" rel="noopener ugc nofollow" target="_blank">数据集</a>包括 1393 篇英语和 909 篇德语新闻文章。我们将会关注英国的数据。CoNLL-2003 数据文件包含由一个空格分隔的四列。每个单词都被放在单独的一行，每个句子后面都有一个空行。每行的第一项是单词，第二项是词性(POS)标记，第三项是语法块标记，第四项是命名实体标记。组块标签和命名实体标签具有 I-TYPE 格式，这意味着单词在 TYPE 类型的短语内。只有当两个相同类型的短语紧随其后时，第二个短语的第一个单词才会有标签 B-TYPE，以表明它开始了一个新短语。标签为 O 的单词不是短语的一部分。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/35c13a24305d52a941b0b38478bd49b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*Djl00NBkYUNAUtuZ4l-3_g.png"/></div></figure><p id="d035" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据预处理</strong></p><p id="1d60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据文件夹中的 train.txt、valid.txt 和 test.txt 都有句子以及它们的标签。我们只需要命名的实体标签。我们将单词及其命名实体提取到一个数组中—[' EU '，' B-ORG']，['rejects '，' O']，['German '，' B-MISC']，['call '，' O']，['to '，' O']，[' bocoit '，' O']，['British '，' B-MISC']，['lamb '，' O']，[' . '，' O'] ]。参考下面的代码来提取单词和命名实体。我们为训练、有效和测试中的所有句子获取带有命名实体的单词。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="3144" class="jr js hi jn b fi jt ju l jv jw">def split_text_label(filename):<br/>  f = open(filename)<br/>  split_labeled_text = []<br/>  sentence = []<br/>  for line in f:<br/>    if len(line)==0 or line.startswith('-DOCSTART') or    <br/>    line[0]=="\n":<br/>       if len(sentence) &gt; 0:<br/>         split_labeled_text.append(sentence)<br/>         sentence = []<br/>       continue<br/>    splits = line.split(' ')<br/>    sentence.append([splits[0],splits[-1].rstrip("\n")])</span><span id="785c" class="jr js hi jn b fi jx ju l jv jw">  if len(sentence) &gt; 0:<br/>    split_labeled_text.append(sentence)<br/>    sentence = []<br/>  return split_labeled_text</span><span id="5d6f" class="jr js hi jn b fi jx ju l jv jw">split_train = split_text_label(os.path.join(args.data, "train.txt"))<br/>split_valid = split_text_label(os.path.join(args.data, "valid.txt"))<br/>split_test = split_text_label(os.path.join(args.data, "test.txt"))</span></pre><p id="871f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们为文件夹(train、valid 和 test)中的所有唯一单词和唯一标签(命名实体将被称为标签)构建词汇表。labelSet 包含标签中所有唯一的单词，即命名实体。wordSet 包含所有独特的单词。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="919c" class="jr js hi jn b fi jt ju l jv jw">labelSet = set()<br/>wordSet = set()<br/># words and labels<br/>for data in [split_train, split_valid, split_test]:<br/>  for labeled_text in data:<br/>    for word, label in labeled_text:<br/>      labelSet.add(label)<br/>      wordSet.add(word.lower())</span></pre><p id="ab61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们为词汇表中的每个单词/标签关联一个唯一的索引。我们为“填充 _ 令牌”分配索引 0，为“未知 _ 令牌”分配索引 1。当一批句子长度不等时,' PADDING_TOKEN '用于句末标记。“未知 _ 令牌”用于表示不存在于词汇表中的任何单词，</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="6f0f" class="jr js hi jn b fi jt ju l jv jw"># Sort the set to ensure '0' is assigned to 0<br/>sorted_labels = sorted(list(labelSet), key=len)<br/># Create mapping for labels<br/>label2Idx = {}<br/>for label in sorted_labels:<br/>  label2Idx[label] = len(label2Idx)<br/>idx2Label = {v: k for k, v in label2Idx.items()}</span><span id="cc91" class="jr js hi jn b fi jx ju l jv jw"># Create mapping for words<br/>word2Idx = {}<br/>if len(word2Idx) == 0:<br/>  word2Idx["PADDING_TOKEN"] = len(word2Idx)<br/>  word2Idx["UNKNOWN_TOKEN"] = len(word2Idx)</span><span id="0637" class="jr js hi jn b fi jx ju l jv jw">for word in wordSet:<br/>  word2Idx[word] = len(word2Idx)</span></pre><p id="4e74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们读取 split_train、split_valid 和 split_test 文件夹中的单词，并将其中的单词和标签转换为它们各自的索引。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="1c74" class="jr js hi jn b fi jt ju l jv jw">def createMatrices(data, word2Idx, label2Idx):<br/>  sentences = []<br/>  labels = []<br/>  for split_labeled_text in data:<br/>     wordIndices = []<br/>     labelIndices = []</span><span id="454c" class="jr js hi jn b fi jx ju l jv jw">     for word, label in split_labeled_text:<br/>       if word in word2Idx:<br/>          wordIdx = word2Idx[word]<br/>       elif word.lower() in word2Idx:<br/>          wordIdx = word2Idx[word.lower()]<br/>       else:<br/>          wordIdx = word2Idx['UNKNOWN_TOKEN']<br/>       wordIndices.append(wordIdx)<br/>       labelIndices.append(label2Idx[label])</span><span id="8b79" class="jr js hi jn b fi jx ju l jv jw">     sentences.append(wordIndices)<br/>     labels.append(labelIndices)</span><span id="6ceb" class="jr js hi jn b fi jx ju l jv jw">  return sentences, labels</span><span id="4725" class="jr js hi jn b fi jx ju l jv jw">train_sentences, train_labels = createMatrices(split_train, word2Idx, label2Idx)<br/>valid_sentences, valid_labels = createMatrices(split_valid, word2Idx, label2Idx)<br/>test_sentences, test_labels = createMatrices(split_test, word2Idx, label2Idx)</span></pre><p id="bda9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些句子长度不同。我们需要填充句子和标签，以使它们长度相等。max_seq_len 取 128。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="c567" class="jr js hi jn b fi jt ju l jv jw">def padding(sentences, labels, max_len, padding='post'):<br/>  padded_sentences = pad_sequences(sentences, max_len,       <br/>  padding='post')<br/>  padded_labels = pad_sequences(labels, max_len, padding='post')<br/>  return padded_sentences, padded_labels</span><span id="b4be" class="jr js hi jn b fi jx ju l jv jw">train_features, train_labels = padding(train_sentences, train_labels, max_seq_len, padding='post' )<br/>valid_features, valid_labels = padding(valid_sentences, valid_labels, max_seq_len, padding='post' )<br/>test_features, test_labels = padding(test_sentences, test_labels, max_seq_len, padding='post' )</span></pre><p id="6eb3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用预先训练的手套词嵌入。将手套嵌入— <a class="ae jd" href="http://nlp.stanford.edu/data/glove.6B.zip" rel="noopener ugc nofollow" target="_blank"> glove.6B.100d.txt </a>下载到嵌入文件夹。对于我们词汇表中的所有单词，我们得到单词的手套表示。embedding_vector 拥有我们词汇表中所有单词的手套表示。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="f4ef" class="jr js hi jn b fi jt ju l jv jw"># Loading glove embeddings<br/>embeddings_index = {}<br/>f = open('embeddings/glove.6B.100d.txt', encoding="utf-8")<br/>for line in f:<br/>  values = line.strip().split(' ')<br/>  word = values[0] # the first entry is the word<br/>  coefs = np.asarray(values[1:], dtype='float32') #100d vectors   <br/>  representing the word<br/>  embeddings_index[word] = coefs<br/>f.close()</span><span id="bad8" class="jr js hi jn b fi jx ju l jv jw">embedding_matrix = np.zeros((len(word2Idx), EMBEDDING_DIM))</span><span id="69c6" class="jr js hi jn b fi jx ju l jv jw"># Word embeddings for the tokens<br/>for word,i in word2Idx.items():<br/>  embedding_vector = embeddings_index.get(word)<br/>  if embedding_vector is not None:<br/>    embedding_matrix[i] = embedding_vector</span></pre><p id="a303" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">使用 TF . Data . dataset . from _ tensor _ slices 的数据管道</strong></p><p id="59e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用 TF . data . dataset . from _ tensor _ slices 对数据集进行批处理和混排。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="c600" class="jr js hi jn b fi jt ju l jv jw">train_batch_size = 32<br/>valid_batch_size = 64<br/>test_batch_size = 64</span><span id="76d3" class="jr js hi jn b fi jx ju l jv jw">train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))<br/>valid_dataset = tf.data.Dataset.from_tensor_slices((valid_features, valid_labels))<br/>test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))</span><span id="ec50" class="jr js hi jn b fi jx ju l jv jw">shuffled_train_dataset = train_dataset.shuffle(buffer_size=train_features.shape[0], reshuffle_each_iteration=True)</span><span id="7d1a" class="jr js hi jn b fi jx ju l jv jw">batched_train_dataset = shuffled_train_dataset.batch(train_batch_size, drop_remainder=True)<br/>batched_valid_dataset = valid_dataset.batch(valid_batch_size, drop_remainder=True)<br/>batched_test_dataset = test_dataset.batch(test_batch_size, drop_remainder=True)</span></pre><p id="09b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">型号采用双向 LSTM </strong></p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="b938" class="jr js hi jn b fi jt ju l jv jw">import tensorflow as tf<br/>from tensorflow.keras import layers</span><span id="6fe7" class="jr js hi jn b fi jx ju l jv jw">class TFNer(tf.keras.Model):<br/>def __init__(self, max_seq_len, embed_input_dim, embed_output_dim, num_labels, weights):<br/>   super(TFNer, self).__init__() </span><span id="c576" class="jr js hi jn b fi jx ju l jv jw">   self.embedding = layers.Embedding(input_dim=embed_input_dim, <br/>   output_dim=embed_output_dim, weights=weights,    <br/>   input_length=max_seq_len, trainable=False, mask_zero=True)        <br/>   <br/>   self.bilstm = layers.Bidirectional(layers.LSTM(128,  <br/>   return_sequences=True))</span><span id="437a" class="jr js hi jn b fi jx ju l jv jw">   self.dense = layers.Dense(num_labels)</span><span id="ebae" class="jr js hi jn b fi jx ju l jv jw">def call(self, inputs):</span><span id="8c9e" class="jr js hi jn b fi jx ju l jv jw">   x = self.embedding(inputs) # batchsize, max_seq_len,      <br/>   embedding_output_dim<br/>   x = self.bilstm(x) #batchsize, max_seq_len, hidden_dim_bilstm<br/>   logits = self.dense(x) #batchsize, max_seq_len, num_labels<br/>   return logits</span></pre><p id="181c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有一个嵌入层，它具有预训练的手套单词嵌入(不可训练)。我们在嵌入后使用双向 LSTM，我们有一个完全连接的层来转换 LSTM 的输出。</p><p id="d550" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们定义模型、优化器和损失。我们使用稀疏分类交叉熵损失和 Adam 优化器。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="2975" class="jr js hi jn b fi jt ju l jv jw">model = TFNer(max_seq_len=max_seq_len,embed_input_dim=len(word2Idx), embed_output_dim=EMBEDDING_DIM, weights=[embedding_matrix], num_labels=num_labels)</span><span id="76f4" class="jr js hi jn b fi jx ju l jv jw">optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)</span><span id="53c6" class="jr js hi jn b fi jx ju l jv jw">scce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</span></pre><p id="dc52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">自定义训练循环:</strong></p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="5bd6" class="jr js hi jn b fi jt ju l jv jw">train_loss_metric = tf.keras.metrics.Mean('training_loss', dtype=tf.float32)<br/>valid_loss_metric = tf.keras.metrics.Mean('valid_loss', dtype=tf.float32)</span><span id="df55" class="jr js hi jn b fi jx ju l jv jw">def train_step_fn(sentences_batch, labels_batch):<br/>  with tf.GradientTape() as tape:<br/>    logits = model(sentences_batch)<br/>    loss = scce(labels_batch, logits)<br/>  grads = tape.gradient(loss, model.trainable_variables)<br/>  optimizer.apply_gradients(list(zip(grads,   <br/>  model.trainable_variables)))</span><span id="41ef" class="jr js hi jn b fi jx ju l jv jw">  return loss, logits</span><span id="3dff" class="jr js hi jn b fi jx ju l jv jw">def valid_step_fn(sentences_batch, labels_batch):<br/>  logits = model(sentences_batch)<br/>  loss = scce(labels_batch, logits)<br/>  return loss, logits</span><span id="189c" class="jr js hi jn b fi jx ju l jv jw">for epoch in epoch_bar:<br/>  for sentences_batch, labels_batch in <br/>  progress_bar(batched_train_dataset, total=train_pb_max_len, <br/>  parent=epoch_bar) :<br/>    loss, logits = train_step_fn(sentences_batch, labels_batch)<br/>    train_loss_metric(loss)<br/>  train_loss_metric.reset_states()</span><span id="4ade" class="jr js hi jn b fi jx ju l jv jw">  for sentences_batch, labels_batch in <br/>  progress_bar(batched_valid_dataset, total=valid_pb_max_len, <br/>  parent=epoch_bar):<br/>    loss, logits = valid_step_fn(sentences_batch, labels_batch<br/>    valid_loss_metric.update_state(loss)<br/>  valid_loss_metric.reset_states()</span><span id="c035" class="jr js hi jn b fi jx ju l jv jw">model.save_weights(f"{args.output}/model_weights",save_format='tf')</span></pre><p id="7040" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">在测试数据集上评估模型性能</strong></p><p id="e064" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用精确度、召回率和 f1 值来评估模型。我们使用 seqeval 包。seqeval 是一个用于序列标签评估的 Python 框架。seqeval 可以评估命名实体识别、词性标注、语义角色标注等组块任务的性能。classification_report metric 构建显示主要分类指标的文本报告。</p><pre class="jf jg jh ji fd jm jn jo jp aw jq bi"><span id="c0cf" class="jr js hi jn b fi jt ju l jv jw">def idx_to_label(predictions, correct, idx2Label):<br/>  label_pred = []<br/>  for sentence in predictions:<br/>    for i in sentence:<br/>      label_pred.append([idx2Label[elem] for elem in i ])</span><span id="3303" class="jr js hi jn b fi jx ju l jv jw">  label_correct = []<br/>  if correct != None:<br/>    for sentence in correct:<br/>    for i in sentence:<br/>      label_correct.append([idx2Label[elem] for elem in i ])<br/>  return label_correct, label_pred</span><span id="15e4" class="jr js hi jn b fi jx ju l jv jw"><br/>test_model =  TFNer(max_seq_len=max_seq_len, embed_input_dim=len(word2Idx), embed_output_dim=EMBEDDING_DIM, weights=[embedding_matrix], num_labels=num_labels)</span><span id="998a" class="jr js hi jn b fi jx ju l jv jw">test_model.load_weights(f"{args.output}/model_weights")</span><span id="8a75" class="jr js hi jn b fi jx ju l jv jw">true_labels = []<br/>pred_labels = []</span><span id="c861" class="jr js hi jn b fi jx ju l jv jw">for sentences_batch, labels_batch in progress_bar(batched_test_dataset, total=test_pb_max_len):<br/>  logits = test_model(sentences_batch)<br/>  temp1 = tf.nn.softmax(logits)<br/>  preds = tf.argmax(temp1, axis=2)<br/>  true_labels.append(np.asarray(labels_batch))<br/>  pred_labels.append(np.asarray(preds))<br/>label_correct, label_pred = idx_to_label(pred_labels, true_labels, idx2Label)</span><span id="4939" class="jr js hi jn b fi jx ju l jv jw">report = classification_report(label_correct, label_pred, digits=4)</span></pre><p id="6537" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TensorBoard 提供了机器学习实验所需的酷可视化和工具。github repo 中的代码包含可视化培训和验证损失的代码。代码可在<a class="ae jd" href="https://github.com/bhuvanakundumani/NER_tensorflow2.2.0.git" rel="noopener ugc nofollow" target="_blank">https://github.com/bhuvanakundumani/NER_tensorflow2.2.0.git</a>获得</p></div></div>    
</body>
</html>