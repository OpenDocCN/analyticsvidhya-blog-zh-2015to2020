<html>
<head>
<title>Fake News Classifier using LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于 LSTM 的假新闻分类器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fake-news-classifier-using-lstm-280518f973f7?source=collection_archive---------11-----------------------#2020-10-23">https://medium.com/analytics-vidhya/fake-news-classifier-using-lstm-280518f973f7?source=collection_archive---------11-----------------------#2020-10-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="26dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">今天，我们将通过一个小项目来讨论自然语言处理(NLP)中最广泛使用的算法之一。</p><p id="e614" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">递归神经网络(RNN)克服了传统神经网络的问题。但是 RNN 也有它的问题，也就是说，它不能把以前的信息和现在的信息联系起来，这就是 LSTM 来帮忙的地方。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/77c453f4fdaf311ae11785c28ea55362.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9lfKUPAz37MvIUTWjQKj7Q.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><em class="jt"> LSTM </em>建筑</figcaption></figure><p id="a59d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了深入了解 LSTM，你可以访问由<a class="ae ju" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">科拉</a>撰写的最好的博客之一。</p><h1 id="4702" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">目录</strong></h1><ul class=""><li id="b33a" class="kt ku hi ih b ii kv im kw iq kx iu ky iy kz jc la lb lc ld bi translated">导入库和读取数据</li><li id="5b8d" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">预处理</li><li id="4e52" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">把...嵌入</li><li id="27a3" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">列车测试分离</li><li id="9893" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">LSTM</li><li id="3859" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">估价</li><li id="c254" class="kt ku hi ih b ii le im lf iq lg iu lh iy li jc la lb lc ld bi translated">结束注释</li></ul><h1 id="1abc" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">导入库并读取数据</strong></h1><p id="b372" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">首先，我们将导入所有需要的库。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="9e13" class="lr jw hi ln b fi ls lt l lu lv">import numpy as np<br/>import pandas as pd<br/>import re</span><span id="b7c7" class="lr jw hi ln b fi lw lt l lu lv">from sklearn.preprocessing import LabelEncoder<br/>from sklearn.model_selection import train_test_split</span><span id="355e" class="lr jw hi ln b fi lw lt l lu lv">from keras.utils import to_categorical<br/>from keras.models import Sequential <br/>from keras.layers import LSTM, Dense, Dropout, Bidirectional</span></pre><p id="dff4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您使用的是 tensorflow &lt; 2.0 than use keras and if you are have tensorflow &gt; 2.0，您可以如下图所示进行导入。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="fe53" class="lr jw hi ln b fi ls lt l lu lv">import numpy as np<br/>import pandas as pd<br/>import re</span><span id="a75e" class="lr jw hi ln b fi lw lt l lu lv">from sklearn.preprocessing import LabelEncoder<br/>from sklearn.model_selection import train_test_split</span><span id="a5e9" class="lr jw hi ln b fi lw lt l lu lv">from tensorflow.keras.utils import to_categorical<br/>from tensorflow.keras.models import Sequential <br/>from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional</span></pre><p id="b630" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦所有东西都被导入，我们将加载数据集。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="2e86" class="lr jw hi ln b fi ls lt l lu lv">train = pd.read_csv('./train.csv')<br/>train.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lx"><img src="../Images/1d419026944a7a34d1bb7e5040eea868.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E0rgRGxgvSbH5kAzKFxmLg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">数据</figcaption></figure><h1 id="3b81" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">预处理</strong></h1><p id="5d54" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">因为我们不需要 Unnamed:6 列，所以我们将删除它。参数 axis=1 表示删除所有行的列。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="935a" class="lr jw hi ln b fi ls lt l lu lv">new_train = train.drop(['Unnamed: 6'], axis = 1)</span></pre><p id="2679" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将运行<strong class="ih hj">。value_counts() </strong>获取每个类的计数。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="d255" class="lr jw hi ln b fi ls lt l lu lv">new_train['class'].value_counts()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/59005665b80bd961ca86d19273c3ee1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*4Gtf9eq8FVxOVrZ3u5zEeQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">输出</figcaption></figure><p id="d643" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们在输出中看到的，我们有一个名为“2017 年 2 月 5 日”的类，它不是分类所必需的，因此我们将删除它。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="9774" class="lr jw hi ln b fi ls lt l lu lv">new_train = new_train[new_train['class'] != 'February 5, 2017']</span></pre><p id="25b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们从数据集中删除一个条目时，索引变得混乱，因此我们需要将它们按顺序恢复。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="ba81" class="lr jw hi ln b fi ls lt l lu lv">new_train.reset_index(inplace=True)</span></pre><p id="614d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在接下来的步骤中，我们将创建一个语料库，基于这个语料库我们将对新闻进行分类。我们现在将从数据中提取文本列，并将其分配给语料库。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="9223" class="lr jw hi ln b fi ls lt l lu lv">corpus = new_train['text']</span></pre><p id="f942" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将清理数据。为了执行这个任务，我们将创建一个名为<strong class="ih hj"> clean()的函数。</strong>在此函数中，我们将创建一个空列表来存储转换/清理后的数据。我们将遍历所有数据点，并使用<strong class="ih hj">将其转换为小写。</strong>降低()。在下一步中，我们使用 re 模块(正则表达式)将除小写字符和数字之外的所有内容替换为空格，最后在列表中追加新的数据点。</p><p id="0358" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你也可以使用词干化和词汇化，但是由于准确性很好，我们在这里不需要它。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="00b4" class="lr jw hi ln b fi ls lt l lu lv">def clean(c):<br/>    C = []<br/>    for i in c:<br/>        i = i.lower()<br/>        k = re.sub(r"[^0-9a-z]"," ",i)<br/>        C.append(k)<br/>    return C</span></pre><p id="196e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步，我们将把语料库转换成一个系列，使任务更易于管理。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="e855" class="lr jw hi ln b fi ls lt l lu lv">corpus = pd.Series(corpus)</span></pre><p id="d8ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将预处理标签。我们将首先提取标签。的。方法将返回一个标签数组。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="9549" class="lr jw hi ln b fi ls lt l lu lv">y = new_train['class'].values</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lz"><img src="../Images/7ad84935c1109a544707418a4870d42d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*lrsJBltPoz04r2k5R_RA7A.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">变量 y</figcaption></figure><p id="4735" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于标签是单词的形式，我们需要将其转换为数字，因为深度学习模型无法在字符上训练，为此，我们将使用来自<strong class="ih hj"> Sklearn </strong>的<strong class="ih hj">标签编码器</strong>。</p><p id="991f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将创建一个对象并使用。fit_transform 方法获取数字形式的标签。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="5927" class="lr jw hi ln b fi ls lt l lu lv">le = Label Encoder()<br/>y = le.fit_transform(y)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ma"><img src="../Images/1b7e5090a7c2fb34397145bf8ca1a230.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*Lba3NhWGtLSS4kheMsWkrg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">变量 y</figcaption></figure><p id="bdc5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于该模型在<strong class="ih hj">单热编码</strong>的形式中获取标签值，我们将使用<strong class="ih hj">将其转换为 _ 分类</strong>。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="d231" class="lr jw hi ln b fi ls lt l lu lv">y = to_categorical(y)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mb"><img src="../Images/10f251a5d7ebc643943ec39da70ecb69.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*agQcfFR2xrMUUugPbuLvHA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">变量 y</figcaption></figure><h1 id="f944" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">嵌入</strong></h1><p id="c1d6" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">我们将使用一个名为<strong class="ih hj">单词表示全局向量(GloVe) </strong>的嵌入，你可以使用 Google 提供的嵌入，或者你也可以<a class="ae ju" href="https://www.youtube.com/watch?v=TsXR7_vtusQ&amp;t=611s" rel="noopener ugc nofollow" target="_blank">创建自己的嵌入</a>。</p><p id="3d0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将首先在记事本中打开该文件进行查看。旁边会有一个单词和一个嵌入，如下面图片中的“<strong class="ih hj"> the </strong>”。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mc"><img src="../Images/518713f8d80979f47bb244695612f9a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iOxqwYUOEOq40_UknPMNeA.png"/></div></div></figure><p id="8f28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在记事本中打开文件</p><p id="4f4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤 1 </strong>:打开文件</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="faf9" class="lr jw hi ln b fi ls lt l lu lv">f = open("./glove.6B.50d.txt", encoding='utf8')</span></pre><p id="68d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤 2 </strong>:我们将创建一个字典来存储单词及其作为键值对的嵌入。正如我们所知，对于每一行，第一件事是 word，其余的都是嵌入，我们将访问它，如下所示。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="4ad5" class="lr jw hi ln b fi ls lt l lu lv">embedding_index = {}</span><span id="a0c6" class="lr jw hi ln b fi lw lt l lu lv">for line in f:<br/>    values = line.split()<br/>    word = values[0]<br/>    emb = np.array(values[1:], dtype ='float')<br/>    <br/>    embedding_index[word] = emb</span></pre><p id="cf11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤 3 </strong>:我们现在将创建一个名为<strong class="ih hj"> get_embedding_output </strong>的函数来为我们的语料库创建嵌入。</p><p id="e900" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们要做的第一件事是分配一个最大长度变量(maxLen)。在下一步中，我们将创建一个名为<strong class="ih hj"> embedding_output </strong>的 NumPy 数组，所有值都为零，以便稍后用嵌入填充它。</p><p id="8f80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将遍历所有的数据点，我们将分割每个数据点，并将所有单词存储在列表 my_example 中。我们将遍历每个单词，并检查在我们之前创建的字典中是否有该单词的嵌入。如果有，我们会将它存储在<strong class="ih hj"> embedding_output 中。</strong></p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="f09c" class="lr jw hi ln b fi ls lt l lu lv">def get_embedding_output(X):<br/>    maxLen = 50<br/>    embedding_output = np.zeros((len(X), maxLen, 50))<br/>    <br/>    for ix in range(X.shape[0]):<br/>        my_example = X[ix].split()<br/>        <br/>     <br/>        for ij in range(len(my_example)): </span><span id="b9a4" class="lr jw hi ln b fi lw lt l lu lv">            if (embedding_index.get(my_example[ij].lower()) is not              <br/>                        None) and (ij&lt;maxLen):<br/>                <br/>                embedding_output[ix][ij]= <br/>                          embedding_index[my_example[ij].lower()]<br/>            <br/>    return embedding_output</span></pre><p id="1c4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">步骤 4 </strong>:在这一步中，我们将调用函数来为我们的语料库获取嵌入。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="f557" class="lr jw hi ln b fi ls lt l lu lv">embeddings = get_embedding_output(corpus)</span></pre><h1 id="c4f4" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">列车试分裂</strong></h1><p id="3c6d" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">我们将使用来自 Sklearn 的<strong class="ih hj"> train_test_split </strong>，因为一旦模型被训练，我们就需要一些数据来进行测试。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="df57" class="lr jw hi ln b fi ls lt l lu lv">X_train, X_test, y_train, y_test = train_test_split(embeddings, y, test_size = 0.2 , random_state=42)</span></pre><h1 id="f1a1" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak"> LSTM 模式</strong></h1><p id="c309" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">要创建顺序模型，我们将从 Keras 创建顺序对象。您也可以使用功能模型。我们将使用。add()方法向我们的模型添加层。</p><p id="a0e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们模型的第一层是双向 LSTM 层，单元(神经元)可以根据需要改变。我们使用 return_sequences=True 来堆叠 LSTM 层。</p><p id="5639" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们模型的第二层将是一个下降层，以防止过度拟合。下一层与上一层相似。</p><p id="5668" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的模型的最后一层将是具有 2 个单元(神经元)的密集层，因为我们有 2 个类别“假”或“真”，并且因为我们想要对其进行分类，我们将使用<strong class="ih hj"> softmax </strong>作为激活函数。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="37fb" class="lr jw hi ln b fi ls lt l lu lv">model = Sequential()<br/>model.add(Bidirectional(LSTM(units = 256 , return_sequences=True), input_shape = (50,50)))<br/>model.add(Dropout(0.3))<br/>model.add(Bidirectional(LSTM(units = 256)))<br/>model.add(Dropout(0.3))<br/>model.add(Dense(units= 2, activation='softmax'))<br/>model.summary()</span></pre><p id="f99e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">model.summary()用于获取我们刚刚构建的模型的摘要。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es md"><img src="../Images/1c469ba6e1b123b9524edbab08015c6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*1AEfcIdQVQMgsOLR9Xu90g.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">模型摘要</figcaption></figure><p id="d278" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将编译模型，我们将使用 adam 优化器，因为它是最好的优化器之一，您也可以使用其他优化器，如 sgd。我们将使用分类交叉熵来度量损失。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="3e76" class="lr jw hi ln b fi ls lt l lu lv">model.compile(optimizer='adam', loss='categorical_crossentropy', metrics =['accuracy'])</span></pre><p id="7dae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们将训练我们的模型。我们使用。fit()方法来训练我们的模型，我们传递从 train_test_split 获得的数据。validation_split 属性用于每个时期中模型验证。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="4c2b" class="lr jw hi ln b fi ls lt l lu lv">hist = model.fit(X_train, y_train, validation_split=0.2, shuffle=True, batch_size=32, epochs=5)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es me"><img src="../Images/fb7a0ffc9fbc2f518abf75f4c7c78b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ruLIMMwgtWClYZN_SpRujA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">培养</figcaption></figure><h1 id="a2da" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">估价</h1><p id="8762" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">随着模型被训练，我们现在将使用<strong class="ih hj">来评估我们的模型的准确性。evaluate() </strong>函数。我们刚刚创建的模型的准确率是 99.88%。</p><pre class="je jf jg jh fd lm ln lo lp aw lq bi"><span id="26d9" class="lr jw hi ln b fi ls lt l lu lv">model.evaluate(X_test, y_test)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mf"><img src="../Images/bbb1175bf355fe6e0e8016229727c6a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*23QOmYzASlwwZA_4vxQcVw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">输出</figcaption></figure><h1 id="293d" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">结尾注释</strong></h1><p id="54da" class="pw-post-body-paragraph if ig hi ih b ii kv ik il im kw io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">在这个模型中，我们使用文本来对新闻进行分类，你也可以使用标题或主题或者文本、标题、主题的组合来将准确率提高到 99.9%或者可能是 100%</p><p id="bf15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个使用长短期记忆的假新闻分类器(LSTM)。如果你想了解更多关于机器学习和数据科学的博客，请关注我，并告诉我你想了解的话题。</p><p id="d96f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嘿，读者们，感谢你们的时间。如果你喜欢这个博客，别忘了鼓掌欣赏它👏如果你喜欢❤，你可以给 50 英镑👏</p><p id="c1d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="mg">数据科学爱好者| ML 爱好者| TCS CA |编码块 CA | Blogger |社区成员|公共演讲者</em></p><p id="cb8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您有任何疑问或建议，请随时联系我</p><p id="0200" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ju" href="https://twitter.com/shah_naivedh" rel="noopener ugc nofollow" target="_blank">https://twitter.com/shah_naivedh</a></p><p id="3320" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae ju" href="https://www.linkedin.com/in/naivedh-shah/?originalSubdomain=in" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/naivedh-shah/</a></p></div></div>    
</body>
</html>