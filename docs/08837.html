<html>
<head>
<title>Creating Keras from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始创建 Keras</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/creating-keras-from-scratch-part-1-50599413ebc7?source=collection_archive---------18-----------------------#2020-08-15">https://medium.com/analytics-vidhya/creating-keras-from-scratch-part-1-50599413ebc7?source=collection_archive---------18-----------------------#2020-08-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="efc2" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">第一部分</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/b40d46785985130b5a6ad28d21c09325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pjjNXnRnmjhSddcfdC6ZUQ.jpeg"/></div></div></figure><h1 id="1dda" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">简介:</h1><p id="7c31" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">我认为，太多的机器学习工程师在没有太多了解什么是机器学习的情况下，就直接用 Keras 和 Tensorflow 等库应用机器学习模型。这方面的一个很好的练习是回去尝试从头创建一个机器学习算法。我选择更深入一层，试图从头开始重新创建 Keras 提供的功能。</p><h1 id="9a1c" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">目标:</h1><p id="1b67" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">这个程序的目标是创建一个灵活的用户界面来创建神经网络。为了简单起见，程序将只包括最简单的层类型。</p><h1 id="36c4" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">概念:</h1><p id="f88e" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">我的程序的结构非常类似于装配线。程序的每一部分都充当装配线上的工人。产品将从代码的每一部分传递到中央处理器，中央处理器进行所有的计算，并将相关信息发送回来。赋予每个程序这些“装配线”属性的最佳方式是将整个模型表示为一个类，并将每种类型的层表示为一个子类。</p><h1 id="de17" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">浏览代码:</h1><p id="c86a" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated"><strong class="kd hj">步骤 1|先决条件:</strong></p><pre class="iy iz ja jb fd kx ky kz la aw lb bi"><span id="e2c6" class="lc jk hi ky b fi ld le l lf lg">import numpy<br/>from matplotlib import pyplot as plt</span><span id="6001" class="lc jk hi ky b fi lh le l lf lg">def sigmoid(x):<br/>    return 1/(1+np.exp(-x))</span><span id="4d4d" class="lc jk hi ky b fi lh le l lf lg">def sigmoid_p(x):<br/>    return sigmoid(x)*(1 -sigmoid(x))</span><span id="70ab" class="lc jk hi ky b fi lh le l lf lg">def relu(x):<br/>    return np.maximum(x, 0)</span><span id="0bf9" class="lc jk hi ky b fi lh le l lf lg">def relu_p(x):<br/>    return np.heaviside(x, 0)</span><span id="c9be" class="lc jk hi ky b fi lh le l lf lg">def tanh(x):<br/>    return np.tanh(x)</span><span id="2310" class="lc jk hi ky b fi lh le l lf lg">def tanh_p(x):<br/>    return 1.0 - np.tanh(x)**2</span><span id="d146" class="lc jk hi ky b fi lh le l lf lg">def deriv_func(z,function):<br/>    if function == sigmoid:<br/>        return sigmoid_p(z)<br/>    elif function == relu:<br/>        return relu_p(z)<br/>    elif function == tanh:<br/>        return tanh_p(z)</span></pre><p id="5d4d" class="pw-post-body-paragraph kb kc hi kd b ke li ij kg kh lj im kj kk lk km kn ko ll kq kr ks lm ku kv kw hb bi translated">我导入了 Numpy 来进行矩阵操作，导入了 Matplotlib 来绘制损耗随时间的变化。激活功能描述如下。还有另一个函数，它将一个值和一个激活函数作为输入，并返回导数值。</p><p id="0f8f" class="pw-post-body-paragraph kb kc hi kd b ke li ij kg kh lj im kj kk lk km kn ko ll kq kr ks lm ku kv kw hb bi translated"><strong class="kd hj">步骤 2|创建主神经网络类:</strong></p><pre class="iy iz ja jb fd kx ky kz la aw lb bi"><span id="66cc" class="lc jk hi ky b fi ld le l lf lg">class NeuralNetwork:<br/>    def __init__(self):<br/>        self.layers = []<br/>        self.weights = []<br/>        self.loss = []<br/>    def add(self,layer_function):<br/>        self.layers.append(layer_function)<br/>        <br/>    def initialize_weights(self):<br/>        for layer in self.layers:<br/>            index = self.layers.index(layer)<br/>            weights = layer.initialize_weights(self.layers,index)<br/>            self.weights.append(weights)<br/>            <br/>    def propagate(self,X):<br/>        As,Zs = [],[]<br/>        input_data = X<br/>        for layer in self.layers:<br/>            a,z = layer.propagate(input_data)<br/>            As.append(a)<br/>            Zs.append(z)<br/>            input_data = a<br/>        return As,Zs</span></pre><p id="4e46" class="pw-post-body-paragraph kb kc hi kd b ke li ij kg kh lj im kj kk lk km kn ko ll kq kr ks lm ku kv kw hb bi translated">这是定义所有其他类和函数的类。这是操作所有其他工人的产品并返回相关信息的工人。因此，它可以访问嵌套类中包含的所有变量。</p><p id="c7a4" class="pw-post-body-paragraph kb kc hi kd b ke li ij kg kh lj im kj kk lk km kn ko ll kq kr ks lm ku kv kw hb bi translated"><strong class="kd hj">步骤 3|创建感知器类:</strong></p><pre class="iy iz ja jb fd kx ky kz la aw lb bi"><span id="599c" class="lc jk hi ky b fi ld le l lf lg">class Perceptron:<br/>        def __init__(self,nodes,input_shape= None,activation = None):<br/>            self.nodes = nodes<br/>            self.input_shape = input_shape<br/>            self.activation = activation<br/>        def initialize_weights(self,layers,index):<br/>            if self.input_shape:<br/>                self.weights = np.random.randn(self.input_shape[-1],self.nodes)<br/>            else:<br/>                self.weights = np.random.randn(layers[index-1].weights.shape[-1],self.nodes)<br/>            return self.weights<br/>        def propagate(self,input_data):<br/>            z = np.dot(input_data,self.weights)<br/>            if self.activation:<br/>                a = self.activation(z)<br/>            else:<br/>                a = z<br/>            return a,z<br/>        def network_train(self,gradient):<br/>            self.weights += gradient</span></pre><p id="c445" class="pw-post-body-paragraph kb kc hi kd b ke li ij kg kh lj im kj kk lk km kn ko ll kq kr ks lm ku kv kw hb bi translated">感知器在更大的网络中充当迷你神经网络。训练函数非常简单，因为不需要外部导数。</p><p id="89af" class="pw-post-body-paragraph kb kc hi kd b ke li ij kg kh lj im kj kk lk km kn ko ll kq kr ks lm ku kv kw hb bi translated"><strong class="kd hj">第 4 步|定义培训:</strong></p><pre class="iy iz ja jb fd kx ky kz la aw lb bi"><span id="f028" class="lc jk hi ky b fi ld le l lf lg">def train(self,X,y,iterations):<br/>        loss = []<br/>        for i in range(iterations):<br/>            As,Zs = self.propagate(X)<br/>            loss.append(np.square(sum(y - As[-1])))<br/>            As.insert(0,X)<br/>            g_wm = [0] * len(self.layers)<br/>            for i in range(len(g_wm)):<br/>                pre_req = (y-As[-1])*2<br/>                a_1 = As[-(i+2)]<br/>                z_index = -1<br/>                w_index = -1<br/>                if i == 0:<br/>                    range_value = 1<br/>                else:<br/>                    range_value = 2*i<br/>                for j in range(range_value):<br/>                    if j% 2 == 0:<br/>                        pre_req = pre_req * sigmoid_p(Zs[z_index])<br/>                        z_index -= 1<br/>                    else:<br/>                        pre_req = np.dot(pre_req,self.weights[w_index].T)<br/>                        w_index -= 1<br/>                gradient = np.dot(a_1.T,pre_req)<br/>                g_wm[-(i+1)] = gradient<br/>                for i in range(len(self.layers)):<br/>                    self.layers[i].network_train(g_wm[i])<br/>        return loss</span></pre><p id="ef0d" class="pw-post-body-paragraph kb kc hi kd b ke li ij kg kh lj im kj kk lk km kn ko ll kq kr ks lm ku kv kw hb bi translated">我创建的训练函数可能不是最简洁的，但它是健壮的，因此网络可以保持其灵活性。</p><h1 id="8d89" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">完整源代码:</h1><pre class="iy iz ja jb fd kx ky kz la aw lb bi"><span id="4c32" class="lc jk hi ky b fi ld le l lf lg">import numpy<br/>from matplotlib import pyplot as plt</span><span id="ca4c" class="lc jk hi ky b fi lh le l lf lg">def sigmoid(x):<br/>    return 1/(1+np.exp(-x))</span><span id="c1c7" class="lc jk hi ky b fi lh le l lf lg">def sigmoid_p(x):<br/>    return sigmoid(x)*(1 -sigmoid(x))</span><span id="a7ba" class="lc jk hi ky b fi lh le l lf lg">def relu(x):<br/>    return np.maximum(x, 0)</span><span id="b1fc" class="lc jk hi ky b fi lh le l lf lg">def relu_p(x):<br/>    return np.heaviside(x, 0)</span><span id="347c" class="lc jk hi ky b fi lh le l lf lg">def tanh(x):<br/>    return np.tanh(x)</span><span id="a675" class="lc jk hi ky b fi lh le l lf lg">def tanh_p(x):<br/>    return 1.0 - np.tanh(x)**2</span><span id="108e" class="lc jk hi ky b fi lh le l lf lg">def deriv_func(z,function):<br/>    if function == sigmoid:<br/>        return sigmoid_p(z)<br/>    elif function == relu:<br/>        return relu_p(z)<br/>    elif function == tanh:<br/>        return tanh_p(z)</span><span id="67d5" class="lc jk hi ky b fi lh le l lf lg">class NeuralNetwork:<br/>    def __init__(self):<br/>        self.layers = []<br/>        self.weights = []<br/>        self.loss = []<br/>    def add(self,layer_function):<br/>        self.layers.append(layer_function)<br/>        <br/>    def initialize_weights(self):<br/>        for layer in self.layers:<br/>            index = self.layers.index(layer)<br/>            weights = layer.initialize_weights(self.layers,index)<br/>            self.weights.append(weights)<br/>            <br/>    def propagate(self,X):<br/>        As,Zs = [],[]<br/>        input_data = X<br/>        for layer in self.layers:<br/>            a,z = layer.propagate(input_data)<br/>            As.append(a)<br/>            Zs.append(z)<br/>            input_data = a<br/>        return As,Zs<br/>    <br/>    def train(self,X,y,iterations):<br/>        loss = []<br/>        for i in range(iterations):<br/>            As,Zs = self.propagate(X)<br/>            loss.append(np.square(sum(y - As[-1])))<br/>            As.insert(0,X)<br/>            g_wm = [0] * len(self.layers)<br/>            for i in range(len(g_wm)):<br/>                pre_req = (y-As[-1])*2<br/>                a_1 = As[-(i+2)]<br/>                z_index = -1<br/>                w_index = -1<br/>                if i == 0:<br/>                    range_value = 1<br/>                else:<br/>                    range_value = 2*i<br/>                for j in range(range_value):<br/>                    if j% 2 == 0:<br/>                        pre_req = pre_req * sigmoid_p(Zs[z_index])<br/>                        z_index -= 1<br/>                    else:<br/>                        pre_req = np.dot(pre_req,self.weights[w_index].T)<br/>                        w_index -= 1<br/>                gradient = np.dot(a_1.T,pre_req)<br/>                g_wm[-(i+1)] = gradient<br/>                for i in range(len(self.layers)):<br/>                    self.layers[i].network_train(g_wm[i])<br/>        return loss<br/>        <br/>    class Perceptron:<br/>        def __init__(self,nodes,input_shape= None,activation = None):<br/>            self.nodes = nodes<br/>            self.input_shape = input_shape<br/>            self.activation = activation<br/>        def initialize_weights(self,layers,index):<br/>            if self.input_shape:<br/>                self.weights = np.random.randn(self.input_shape[-1],self.nodes)<br/>            else:<br/>                self.weights = np.random.randn(layers[index-1].weights.shape[-1],self.nodes)<br/>            return self.weights<br/>        def propagate(self,input_data):<br/>            z = np.dot(input_data,self.weights)<br/>            if self.activation:<br/>                a = self.activation(z)<br/>            else:<br/>                a = z<br/>            return a,z<br/>        def network_train(self,gradient):<br/>            self.weights += gradient<br/>                <br/>model = NeuralNetwork()</span><span id="e451" class="lc jk hi ky b fi lh le l lf lg">Perceptron = model.Perceptron</span><span id="6109" class="lc jk hi ky b fi lh le l lf lg">X = np.array([[0,1,1],[1,1,0],[1,0,1]])<br/>y = np.array([[0],[1],[1]])<br/>model.add(Perceptron(5,input_shape = (None,3),activation = sigmoid))<br/>model.add(Perceptron(10,activation = sigmoid))<br/>model.add(Perceptron(10,activation = sigmoid))<br/>model.add(Perceptron(1,activation = sigmoid))<br/>model.initialize_weights()<br/>loss = model.train(X,y,1000)<br/>plt.plot(loss)</span></pre><p id="f9f1" class="pw-post-body-paragraph kb kc hi kd b ke li ij kg kh lj im kj kk lk km kn ko ll kq kr ks lm ku kv kw hb bi translated">我希望你从这篇文章中学到了一些东西！请随意使用这段代码来加深您对机器学习的理解！</p></div></div>    
</body>
</html>