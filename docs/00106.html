<html>
<head>
<title>Nuts &amp; Bolts of Reinforcement Learning: Model Based Planning using Dynamic Programming</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的基础:基于模型的动态规划</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/nuts-bolts-of-reinforcement-learning-model-based-planning-using-dynamic-programming-d71d52011b53?source=collection_archive---------1-----------------------#2018-09-17">https://medium.com/analytics-vidhya/nuts-bolts-of-reinforcement-learning-model-based-planning-using-dynamic-programming-d71d52011b53?source=collection_archive---------1-----------------------#2018-09-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6671" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度强化学习是人工智能战胜人类专业人士的两个最大胜利的原因——Alpha Go和OpenAI Five。在谷歌和埃隆·马斯克的倡导下，近年来对这一领域的兴趣逐渐增加，现已成为一个蓬勃发展的研究领域。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/76711cdd72d84814d929f0c6202a3883.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YybMa_2IFeIW__lh.png"/></div></div></figure><p id="3efd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，在本文中，我们不会讨论典型的RL设置，而是探索动态编程(DP)。DP是可以解决问题<em class="jp">的算法集合，其中我们有完美的环境模型(即问题设置中发生的任何变化的概率分布是已知的),并且代理只能采取离散的行动。</em></p><p id="4e88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">DP本质上解决的是规划问题，而不是更一般的RL问题。如前所述，主要的区别在于，对于RL问题，环境可能非常复杂，并且最初根本不知道它的细节。</p><p id="d45e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是在我们深入研究所有这些之前，让我们首先使用一个直观的例子来理解为什么应该学习动态编程。</p><h1 id="4b42" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">为什么要学习动态编程？</h1><p id="e391" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">除了是掌握强化学习的良好起点之外，动态规划还可以帮助找到行业中面临的规划问题的最佳解决方案，其重要假设是环境的具体情况是已知的。DP为理解可以解决更复杂问题的RL算法提供了一个很好的起点。</p><h1 id="b606" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">桑尼摩托车租赁公司</h1><p id="e04f" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">桑尼在拉达克经营一家摩托车租赁公司。由于靠近世界上最高的公路，游客对出租摩托车有很大的需求。在镇上，他有两个地点，游客可以来租自行车。如果他在一个地方没有自行车，那么他就失去了生意。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kt"><img src="../Images/72427d2c05cec35a9e86290f58041f76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*F6Xg0ugUd9a838Iu.jpg"/></div></div></figure><ul class=""><li id="a490" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated">自行车以每天1200卢比的价格出租，并在归还后的第二天出租。</li><li id="0ff1" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">Sunny可以将自行车从一个位置移动到另一个位置，费用为100卢比</li><li id="9a3a" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">凭借经验，Sunny已经计算出需求和回报率的近似概率分布。</li><li id="27ac" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">每个位置返回和请求的自行车数量分别由函数g(n)和h(n)给出。确切地说，在两个地点租赁的自行车数量为n的概率由g(n)给出，在两个地点归还的自行车数量为n的概率由h(n)给出</li></ul><p id="5c03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Sunny试图解决的问题是找出他每天应该从一个地点向另一个地点移动多少辆自行车，以便他可以最大限度地增加收入。</p><p id="70d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们确切地知道环境(g(n)和h(n))，这就是动态规划可以派上用场的那种问题。类似地，如果您可以对问题的环境进行适当的建模，在这里您可以采取离散的行动，那么DP可以帮助您找到最优的解决方案。在本文中，我们将使用DP来训练一个使用Python遍历简单环境的代理，同时触及RL中的关键概念，如策略、奖励、价值函数等等。</p><h1 id="ef4b" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">使用井字游戏理解Agent环境界面</h1><p id="ef01" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">你们大部分人小时候肯定都玩过井字游戏。如果没有，你可以从它的<a class="ae li" href="https://en.wikipedia.org/wiki/Tic-tac-toe" rel="noopener ugc nofollow" target="_blank"> wiki页面</a>中掌握这个简单游戏的规则。假设井字游戏是你最喜欢的游戏，但是没有人陪你玩。所以你决定设计一个能和你一起玩这个游戏的机器人。一些关键问题是:</p><p id="b884" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你能定义一个基于规则的框架来设计一个高效的机器人吗？</p><p id="f836" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你当然可以，但是你必须为游戏中可能出现的每一种情况编写大量的规则。然而，一个更有趣的问题是:</p><p id="2438" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以通过和你对战几次来训练机器人学习吗？这也是在没有被明确编程为高效玩井字游戏的情况下吗？</p><p id="4595" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对此的一些考虑是:</p><ul class=""><li id="6a91" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated">首先，机器人需要了解它所处的情况。井字游戏有9个点来填充X或o。游戏中每种不同的可能组合对机器人来说都是不同的情况，基于这一点，它将采取下一步行动。下图所示的每个场景都是不同的<strong class="ih hj">州。</strong></li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lj"><img src="../Images/4572ac81a1952a2cf1a6523ed439f8c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KegpLSHGsxbHAifx.png"/></div></div></figure><ul class=""><li id="6348" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated">一旦知道了状态，机器人必须采取一个它认为最有可能赢得游戏的<strong class="ih hj">行动</strong>(<strong class="ih hj">策略</strong></li><li id="f250" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">这一步棋将产生一个新的场景，其中有O和X的新组合，这是一个<strong class="ih hj">新状态</strong>和一个数字<strong class="ih hj">奖励</strong>，将根据这步棋的质量给出，目标是赢得游戏(<strong class="ih hj">累积奖励</strong></li></ul><p id="b2d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更清楚地了解上述奖励，让我们考虑机器人O和X之间的匹配:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lk"><img src="../Images/d6eab34fc69d7fcc0a86730249921d09.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/0*qAeFLibqnm9D5C5s.png"/></div></figure><p id="9e56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑在井字游戏中遇到的以下情况:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ll"><img src="../Images/a6fbe8ff632b3d0a729d23fc7eb8e5f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/0*75fSc6ss2_DOkOjZ.png"/></div></figure><p id="59e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，如果机器人X将X放在右下角，会导致以下情况:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lm"><img src="../Images/b54b67905287f57f99c9d5108a84d128.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/0*wb8kLQgg8xHNEBCv.png"/></div></figure><p id="a74b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Bot O会很高兴(是的！它们被编程来表现情感)，因为它可以只用一个动作就赢得比赛。现在，我们需要教X不要再这样做了。因此，我们给予负面的奖励或惩罚，以在下一次试验中强化正确的行为。我们说，在给定的状态下，这一行动将对应于一个负的奖励，而不应该被认为是在这种情况下的最佳行动。</p><p id="c465" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类似地，如果X阻止O在下一步中获胜，它将获得正奖励:</p><h1 id="5ba2" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">马尔可夫决策过程简介</h1><p id="b1b0" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">现在我们已经理解了基本术语，让我们来讨论一下如何使用一个叫做马尔可夫决策过程或MDP的概念来将整个过程形式化。</p><p id="1d6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">马尔可夫决策过程(MDP)模型包含:</p><ul class=""><li id="4bbd" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated">一组可能的世界状态</li><li id="071c" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">一组可能的动作</li><li id="6e72" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">一个实值报酬函数R(s，a)</li><li id="d823" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">每个状态下每个动作效果的描述测试</li></ul><p id="7f4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们理解马尔可夫或“无记忆”性质。</p><p id="ff2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">处于给定状态的概率仅取决于前一状态的任何随机过程都是马尔可夫过程。</p><p id="4b2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">换句话说，在马尔可夫决策过程设置中，环境在时间t+1的响应只取决于时间t的状态和动作表示，而与过去发生的任何事情无关。</p><ul class=""><li id="4038" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated">ST:t时刻代理的状态</li><li id="e29e" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">At:座席在时间t采取的操作</li><li id="2ba0" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">Rt:在时间t获得的奖励</li></ul><p id="4e7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图清楚地说明了每个时间步的迭代，其中代理人收到奖励Rt+1，并根据其在特定状态St的动作结束于状态St+1。代理人的总体目标是使其长期收到的累积奖励最大化。任何时刻t的总报酬由下式给出:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ln"><img src="../Images/a64f84a74a0753aa5acf548e32c004df.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/0*LowKbtNzsl8HrrRl.png"/></div></figure><p id="1cff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中T是该集的最后一个时间步长。在上面的等式中，我们看到所有未来的奖励都有相同的权重，这可能是不可取的。这就是额外的折现概念出现的原因。基本上，我们将γ定义为一个贴现因子，即时奖励后的每个奖励用该因子贴现如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lo"><img src="../Images/efd90f617e1c08156d1d2f201e99fc75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Pw7-QmScYlOqRiQo.png"/></div></div></figure><p id="055d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于贴现因子&lt; 1, the rewards further in the future are getting diminished. This can be understood as a tuning parameter which can be changed based on how much one wants to consider the long term (γ close to 1) or short term (γ close to 0).</p><h1 id="7579" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">State Value Function: How good it is to be in a given state?</h1><p id="cecf" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">Can we use the reward function defined at each time step to define how good it is, to be in a given state for a given policy? <strong class="ih hj">,在策略π下表示为v(s)的价值函数表示代理所处的状态有多好。</strong>换句话说，在策略π下，代理人从当前状态开始会得到的平均回报是多少？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lp"><img src="../Images/ecd547cb32560fd623b3fc61550b9dd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*j2lue9YEBJBXOlgg.png"/></div></div></figure><p id="29c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上式中的e代表代理人遵循策略π时每个状态下的期望报酬，而<em class="jp"> S </em>代表所有可能状态的集合。</p><p id="34e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如前所述，策略是在每个状态下采取每个可能行动的概率的映射(π(a/s))。当策略确切地告诉你在每个状态下做什么而没有给出概率时，它也可能是确定性的。</p><p id="5633" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，直觉告诉我们，如果每个州的价值函数最大化，就能达到“最优政策”。该最优策略由下式给出:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lq"><img src="../Images/761d29e84bf34d49257ceb8e68405534.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*e5nixth6f_pfe3e20lzc-w.png"/></div></figure><h1 id="106f" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">状态-行动价值函数:在特定状态下行动有多好？</h1><p id="ed3c" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">上述值函数仅表征一种状态。我们也能知道一个动作在特定状态下有多好吗？一个状态-动作值函数，也称为q值，就是这样做的。我们将策略π下状态<em class="jp"> s、</em>中动作<em class="jp"> a、</em>的值定义为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lr"><img src="../Images/13d41c217f425d34b6e3b02a6338a872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*K5rJ0zsKgz4BN8VJ.png"/></div></div></figure><p id="32e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是代理人在给定状态St的情况下，如果在时间t采取行动，然后遵循策略π，将获得的预期回报。</p><h1 id="0a35" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">贝尔曼期望方程:来自后继状态的值信息被传递回当前状态</h1><p id="bbe1" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">贝尔曼是一位应用数学家，他推导出了有助于解决马尔可夫决策过程的方程。</p><p id="8fda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们回到状态值函数v和状态-动作值函数q。展开值函数方程得到:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ls"><img src="../Images/adb4aadc0193d636d507646716f16638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ciJhW8elBb24XFwS.png"/></div></div></figure><p id="9933" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个等式中，我们用下一个状态的价值函数来表示给定策略π的价值函数。</p><p id="8100" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择一个动作a，概率π(a/s)在状态s，导致状态s '有prob p(s'/s，a)。这给出了奖励[r + γ*vπ(s)]，如上面方括号中给出的。</p><p id="7903" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这被称为贝尔曼期望方程。来自后续状态的值信息被传输回当前状态，这可以通过称为备份图的东西来有效地表示，如下所示。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/32b57f99f6e3d27135b059588e6d83f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/0*3Er7YA24vxI7zYYE.png"/></div></figure><p id="d38e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jp">贝尔曼期望方程对所有可能性进行平均，并根据发生的概率对每种可能性进行加权。它指出，开始状态的值必须等于预期的下一个状态的(贴现)值，加上沿途预期的回报。</em></p><p id="ce17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有n(状态数)个线性方程，每个状态都有唯一的解。</p><h1 id="ade5" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">贝尔曼最优方程:寻找最优策略</h1><p id="ce02" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">这里的目标是找到最优策略，当代理人遵循该策略时，将获得最大的累积回报。换句话说，找到一个政策π，使得没有其他的π可以让代理人获得更好的预期收益。我们希望找到一种策略，为每个州实现最大价值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lq"><img src="../Images/761d29e84bf34d49257ceb8e68405534.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*e5nixth6f_pfe3e20lzc-w.png"/></div></figure><p id="f7bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，我们可能得不到唯一的策略，因为在任何情况下，都可能有两条或更多条路径具有相同的回报并且仍然是最优的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lu"><img src="../Images/1e7fc6f88d0cfb5c10f61f9af346a50c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XnSlGTJsqA2nbeoB.png"/></div></div></figure><p id="ff14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">最佳值函数可以通过寻找导致q*最大值的动作<em class="jp"> a </em>来获得。这被称为v*的贝尔曼最优方程。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/f0f25dc696aa4ff370bd0ddfb042dcf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/0*CCscZRIt3DNeVyZd.png"/></div></figure><p id="ac8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">直观地说，贝尔曼最优方程认为，最优策略下的每个状态的值必须是代理在遵循最优策略给出的最佳行动时获得的回报。对于最优策略π*，最优值函数由下式给出:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/5903f1f7325a69b2b0e23e157365e51e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/0*KcbxMGB4NfM9BR3X.png"/></div></figure><p id="1922" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">给定一个值函数q*，我们可以恢复一个最优策略如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/64b452b7188ce26e6db0c23286af9721.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/0*zeDYj8NPJA6nHOMy.png"/></div></figure><p id="8fe1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最优策略的价值函数可以通过非线性方程组求解。我们可以使用动态规划下的迭代方法有效地解决这些问题。</p><h1 id="58b8" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">动态规划</h1><p id="463c" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">动态规划算法解决一类称为规划问题的问题。在这里，给定环境(MDP)的完整模型和规范，我们可以成功地找到代理要遵循的最优策略。它包含两个主要步骤:</p><ol class=""><li id="8c9c" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc ly la lb lc bi translated">将问题分解成子问题并解决它</li><li id="c6bc" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc ly la lb lc bi translated">子问题的解决方案被缓存或存储以供重用，从而找到当前问题的整体最优解决方案</li></ol><p id="527b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要解决给定的MDP，解决方案必须包含以下组件:</p><ol class=""><li id="d586" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc ly la lb lc bi translated">找出任意政策有多好</li><li id="8412" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc ly la lb lc bi translated">找出给定MDP的最优策略</li></ol><h1 id="a2f0" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">政策评估:了解一个政策有多好？</h1><p id="e8de" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">政策评估回答了政策有多好的问题。给定一个MDP和一个任意的策略π，我们将计算状态值函数。在发展政策文献中，这被称为政策评估。这个想法是将前面讨论的贝尔曼期望方程更新。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lz"><img src="../Images/b6e6b6e29d3c08b221668ffcd8269916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1074/format:webp/0*Zb8PAYrpGggg0vgZ.png"/></div></figure><p id="c9d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了从vk产生每个逐次逼近vk+1，迭代策略评估对每个状态s应用相同的运算。它用从s的后继状态的旧值获得的新值替换s的旧值，以及期望的立即回报，以及在被评估的策略下可能的所有单步转换，直到它收敛到给定策略π的真实值函数。</p><p id="b67c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们用Gridworld这个非常流行的例子来理解政策评估。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ma"><img src="../Images/c9c900d190db07f5854a9244ef13d6fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MdBq66rAXWoLF0TG.png"/></div></div></figure><p id="e140" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">机器人需要穿过4×4维的网格才能到达目标(1或16)。每一步都与-1的奖励相关联。这里有2个终态:1和16以及14个非终态由[2，3，…]给出。,15].考虑一个随机策略，在每个状态下，每个行为{上、下、左、右}的概率等于0.25。我们将从将随机策略的v0初始化为全0开始。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mb"><img src="../Images/1f777a6d09fd7a689d664c6942e02aa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/0*ak5H8R1mB_Lv5T1d.png"/></div></figure><p id="4f03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个肯定用处不大。让我们计算6的所有状态的v2:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mc"><img src="../Images/351794ac0a0ca9fb109526ec415ebc11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2QFdu8G7DmjZ0wPG.png"/></div></div></figure><p id="09f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，对于所有非终结状态，v1(s) = -1。</p><p id="e22a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于终端状态p(s'/s，a) = 0，因此对于所有k，vk(1) = vk(16) = 0，因此随机策略的v1由下式给出:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es md"><img src="../Images/f85ec94294b83c4ba007cc9aa3296db6.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/0*iOFi95iwEtDyMWlv.png"/></div></figure><p id="b4f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，对于v2(s ),我们假设γ或折现因子为1:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es me"><img src="../Images/92842faf9743c4073f4d19b3c18de9aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3ARjv1CrVd_chrVa.png"/></div></div></figure><p id="f79e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你所看到的，上图中用红色标记的所有状态都等同于6，用于计算价值函数。因此，对于所有这些状态，v2(s) = -2。</p><p id="0815" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于所有剩余状态，即2、5、12和15，v2可计算如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mf"><img src="../Images/68256e9110d0bd8159fef4f89c50cbe5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/0*1oCsSiw-R13tnWl3.png"/></div></div></figure><p id="b82d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们重复这个步骤几次，我们得到vπ:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mg"><img src="../Images/2a46f110973378ca2b1b01d49252d06e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cVuPePUQ0qM4JrDh.png"/></div></div></figure><h1 id="204e" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">政策改进:改进任意的政策</h1><p id="58b5" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">使用策略评估，我们已经确定了任意策略π的值函数v。我们知道我们目前的政策有多好。现在对于一些状态s，我们想了解采取与政策π无关的行动<em class="jp"> a </em>会有什么影响。假设我们在<em class="jp">，</em>中选择<em class="jp"> a </em>，之后我们遵循原来的策略π。这种行为方式的价值表现为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mh"><img src="../Images/f16409cdc001f1a9a69d0792b2430916.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*U3jdJWAXRswPoYGk.png"/></div></div></figure><p id="fa30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果这恰好大于值函数vπ(s ),则意味着采取新的策略π'会更好。我们对所有状态重复这样做，以找到最佳策略。请注意，在这种情况下，代理将遵循一个贪婪的策略，因为它只向前看了一步。</p><p id="9338" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们回到gridworld的例子。使用为随机策略π获得的值函数vπ，我们可以通过遵循最高值的路径来改进π(如下图所示)。我们从一个任意的策略开始，对于每个状态，进行一步前瞻以找到导致具有最高值的状态的动作。这是对每个状态连续进行的。</p><p id="7ca0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如下所示，对于状态2，留下最佳动作，这导致终端状态具有值。这是接下来所有状态中最高的(0，-18，-20)。对所有状态重复这一过程以找到新的策略。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mi"><img src="../Images/9a966c9d9aceafd5103100f5ba4ce43b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HZPugYkGFoEgiKpP.png"/></div></div></figure><p id="5b8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">总的来说，在使用vπ的策略改进步骤之后，我们得到新的策略π':</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mj"><img src="../Images/62e2ed039a896c5746fba6e24d0716e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Y_SEyDc8Wkj6SWst.png"/></div></div></figure><p id="6488" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看新政策，很明显比随机政策好多了。但是，我们应该使用我们之前讨论的策略评估技术来计算vπ’,以验证这一点并更好地理解。</p><h1 id="4ba4" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">政策迭代:政策评估+政策改进</h1><p id="d874" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">一旦使用vπ改进了策略以产生更好的策略π’，我们就可以计算vπ’以将其进一步改进为π”。对于给定的策略π(策略评估)，进行重复迭代以近似收敛到真实值函数。按照策略改进一节中的描述改进策略称为策略迭代。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mk"><img src="../Images/5780a2abbe37065e574296563c261ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1ASoEICVLXyMAkBB.png"/></div></div></figure><p id="8dca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过这种方式，新策略肯定会比前一个策略有所改进，如果迭代次数足够多，它将返回最优策略。这听起来很神奇，但是有一个缺点——策略迭代中的每一次迭代本身都包括策略评估的另一次迭代，这可能需要多次扫描所有状态。下一节讨论的值迭代技术为此提供了一个可能的解决方案。</p><h1 id="8bb1" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">价值迭代</h1><p id="9010" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">我们在gridworld示例中看到，在k = 10左右，我们已经能够找到最优策略。因此，我们可以更早地停止，而不是等待策略评估步骤准确地收敛到值函数vπ。</p><p id="b262" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还可以通过仅1步策略评估，然后重复更新价值函数(但这次是使用从贝尔曼最优方程中导出的更新)来获得最优策略。让我们看看这是如何作为一个简单的备份操作来完成的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/10c80b49a36083af6a25ddd832af4ec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HHcswMb2z2l-QFwv.png"/></div></figure><p id="c861" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这与政策评估中的贝尔曼更新相同，不同之处在于我们在所有行动中取最大值。一旦更新足够小，我们可以将获得的价值函数作为最终值，并估计相应的最优策略。</p><p id="3aa8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与DP相关的一些要点:</p><ol class=""><li id="2149" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc ly la lb lc bi translated">只有在已知环境模型的情况下，才能使用DP。</li><li id="0e3a" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc ly la lb lc bi translated">具有非常高的计算开销，即，当状态的数量增加到很大数量时，它不能很好地扩展。另一种称为异步动态编程的方法在一定程度上有助于解决这个问题。</li></ol><h1 id="81b8" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">动态规划在实践中:使用Python寻找冰冻湖泊环境的最佳政策</h1><p id="4304" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">最重要的是，首先要有一个定义好的环境，以便测试任何一种有效解决MDP的策略。令人欣慰的是，OpenAI，一个非营利研究组织，提供了大量的环境来测试和玩各种强化学习算法。为了说明这里的动态编程，我们将使用它来导航冰冻的湖泊环境。</p><h1 id="1aab" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">冰冻湖泊环境</h1><p id="9fd6" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">代理控制网格世界中角色的移动。网格的一些瓦片是可行走的，其他的导致代理人掉进水里。此外，代理的移动方向是不确定的，并且仅部分取决于所选择的方向。代理人因找到一条通往目标方块的可行走路径而获得奖励。</p><p id="b684" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用如下所示的网格来描述表面:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mm"><img src="../Images/9e2d3277de271cd7c0add38cd1c89fc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*N-Jif6C1olH_WdSU.png"/></div></figure><p id="370b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(S:起点，安全)，(F:冰面，安全)，(H:洞，摔到你的末日)，(G:球门)</p><p id="6c40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个想法是从起点出发，只在冰冻的表面上行走，避开所有的洞，从而到达目标。安装细节和文档可通过此<a class="ae li" href="https://gym.openai.com/docs/" rel="noopener ugc nofollow" target="_blank">链接</a>获得。</p><p id="d249" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦安装了健身房图书馆，你就可以打开一个jupyter笔记本开始。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="e0c4" class="ms jr hi mo b fi mt mu l mv mw">import gym<br/>import numpy as np<br/>env = gym.make('FrozenLake-v0')</span></pre><p id="3dea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，env变量包含了关于冰冻湖泊环境的所有信息。在我们继续之前，我们需要了解什么是插曲。一个情节代表了代理人在追求达到目标的过程中所做的尝试。一旦代理人达到最终状态，一集就结束了，在这种情况下，要么是一个洞，要么是目标。</p><h1 id="a0d9" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">python中的策略迭代</h1><p id="2f48" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">策略迭代函数的参数描述</p><p id="58a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">策略:</strong>一个大小为n(S) x n(A)的2D数组，每个单元格代表一个在状态S采取行动A的概率</p><p id="1a1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">环境:</strong>初始化OpenAI健身房环境对象</p><p id="14f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">折扣因子:</strong> MDP折扣因子</p><p id="86f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">θ:</strong>一个值函数变化的阈值。一旦对值函数的更新低于这个数</p><p id="024e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> max_iterations </strong>:避免程序无限运行的最大迭代次数</p><p id="e421" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个函数将返回一个nS大小的向量，它代表每个状态的一个值函数。</p><p id="eb94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们从策略评估步骤开始。目标是收敛到给定策略π的真实值函数。我们将定义一个返回所需值的函数。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="93fa" class="ms jr hi mo b fi mt mu l mv mw">def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):<br/>        # Number of evaluation iterations<br/>        evaluation_iterations = 1<br/>        # Initialize a value function for each state as zero<br/>        V = np.zeros(environment.nS)<br/>        # Repeat until change in value is below the threshold<br/>        for i in range(int(max_iterations)):<br/>                # Initialize a change of value function as zero<br/>                delta = 0<br/>                # Iterate though each state<br/>                for state in range(environment.nS):<br/>                       # Initial a new value of current state<br/>                       v = 0<br/>                       # Try all possible actions which can be taken       from this state<br/>                       for action, action_probability in enumerate(policy[state]):<br/>                             # Check how good next state will be<br/>                             for state_probability, next_state, reward, terminated in environment.P[state][action]:<br/>                                  # Calculate the expected value<br/>                                  v += action_probability * state_probability * (reward + discount_factor * V[next_state])<br/>                       <br/>                       # Calculate the absolute change of value function<br/>                       delta = max(delta, np.abs(V[state] - v))<br/>                       # Update value function<br/>                       V[state] = v<br/>                evaluation_iterations += 1<br/>                <br/>                # Terminate if value change is insignificant<br/>                if delta &lt; theta:<br/>                        print(f'Policy evaluated in {evaluation_iterations} iterations.')<br/>                        return V</span></pre><p id="0109" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在进入策略迭代算法的<strong class="ih hj">策略改进</strong>部分。我们需要一个帮助器函数来计算状态值函数。这将返回一个长度为nA的数组，包含每个动作的预期值</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="f02a" class="ms jr hi mo b fi mt mu l mv mw">def one_step_lookahead(environment, state, V, discount_factor):<br/>        action_values = np.zeros(environment.nA)<br/>        for action in range(environment.nA):<br/>                for probability, next_state, reward, terminated in environment.P[state][action]:<br/>                        action_values[action] += probability * (reward + discount_factor * V[next_state])<br/>        return action_values</span></pre><p id="5738" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，整个策略迭代将如下所述。这将返回一个元组(policy，V ),它是每个状态的最佳策略矩阵和值函数。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="1245" class="ms jr hi mo b fi mt mu l mv mw">def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):<br/>        # Start with a random policy<br/>        #num states x num actions / num actions<br/>        policy = np.ones([environment.nS, environment.nA]) / environment.nA<br/>        # Initialize counter of evaluated policies<br/>        evaluated_policies = 1<br/>        # Repeat until convergence or critical number of iterations reached<br/>        for i in range(int(max_iterations)):<br/>                stable_policy = True<br/>                # Evaluate current policy<br/>                V = policy_evaluation(policy, environment, discount_factor=discount_factor)<br/>                # Go through each state and try to improve actions that were taken (policy Improvement)<br/>                for state in range(environment.nS):<br/>                        # Choose the best action in a current state under current policy<br/>                        current_action = np.argmax(policy[state])<br/>                        # Look one step ahead and evaluate if current action is optimal<br/>                        # We will try every possible action in a current state<br/>                        action_value = one_step_lookahead(environment, state, V, discount_factor)<br/>                        # Select a better action<br/>                        best_action = np.argmax(action_value)<br/>                        # If action didn't change<br/>                        if current_action != best_action:<br/>                                stable_policy = True<br/>                                # Greedy policy update<br/>                                policy[state] = np.eye(environment.nA)[best_action]<br/>                evaluated_policies += 1<br/>                # If the algorithm converged and policy is not changing anymore, then return final policy and value function<br/>                if stable_policy:<br/>                        print(f'Evaluated {evaluated_policies} policies.')<br/>                        return policy, V</span></pre><h1 id="214a" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">python中的值迭代</h1><p id="075d" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">对于值迭代，参数以相同的方式定义。值迭代算法可以类似地编码为:</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="19c8" class="ms jr hi mo b fi mt mu l mv mw">def value_iteration(environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):<br/>        # Initialize state-value function with zeros for each environment state<br/>        V = np.zeros(environment.nS)<br/>        for i in range(int(max_iterationsations)):<br/>                # Early stopping condition<br/>                delta = 0<br/>                # Update each state<br/>                for state in range(environment.nS):<br/>                        # Do a one-step lookahead to calculate state-action values<br/>                        action_value = one_step_lookahead(environment, state, V, discount_factor)<br/>                        # Select best action to perform based on the highest state-action value<br/>                        best_action_value = np.max(action_value)<br/>                        # Calculate change in value<br/>                        delta = max(delta, np.abs(V[state] - best_action_value))<br/>                        # Update the value function for current state<br/>                        V[state] = best_action_value<br/>                        # Check if we can stop<br/>                if delta &lt; theta:<br/>                        print(f'Value-iteration converged at iteration#{i}.')<br/>                        break<br/><br/>        # Create a deterministic policy using the optimal value function<br/>        policy = np.zeros([environment.nS, environment.nA])<br/>        for state in range(environment.nS):<br/>                # One step lookahead to find the best action for this state<br/>                action_value = one_step_lookahead(environment, state, V, discount_factor)<br/>                # Select best action based on the highest state-action value<br/>                best_action = np.argmax(action_value)<br/>                # Update the policy to perform a better action at a current state<br/>                policy[state, best_action] = 1.0<br/>        return policy, V</span></pre><p id="2879" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，让我们比较一下这两种方法，看看哪种方法在实际环境中效果更好。为此，我们将尝试使用上述两种技术来学习冰湖环境的最佳策略。稍后，我们将根据10000集后的平均回报来检查哪种技术的表现更好。</p><pre class="je jf jg jh fd mn mo mp mq aw mr bi"><span id="71ef" class="ms jr hi mo b fi mt mu l mv mw">def play_episodes(environment, n_episodes, policy):<br/>        wins = 0<br/>        total_reward = 0<br/>        for episode in range(n_episodes):<br/>                terminated = False<br/>                state = environment.reset()<br/>                while not terminated:<br/>                        # Select best action to perform in a current state<br/>                        action = np.argmax(policy[state])<br/>                        # Perform an action an observe how environment acted in response<br/>                        next_state, reward, terminated, info = environment.step(action)<br/>                        # Summarize total reward<br/>                        total_reward += reward<br/>                        # Update current state<br/>                        state = next_state<br/>                        # Calculate number of wins over episodes<br/>                        if terminated and reward == 1.0:<br/>                                wins += 1<br/>        average_reward = total_reward / n_episodes<br/>        return wins, total_reward, average_reward<br/><br/># Number of episodes to play<br/>n_episodes = 10000<br/># Functions to find best policy<br/>solvers = [('Policy Iteration', policy_iteration),<br/>           ('Value Iteration', value_iteration)]<br/>for iteration_name, iteration_func in solvers:<br/>        # Load a Frozen Lake environment<br/>        environment = gym.make('FrozenLake-v0')<br/>        # Search for an optimal policy using policy iteration<br/>        policy, V = iteration_func(environment.env)<br/>        # Apply best policy to the real environment<br/>        wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)<br/>        print(f'{iteration_name} :: number of wins over {n_episodes} episodes = {wins}')<br/>        print(f'{iteration_name} :: average reward over {n_episodes} episodes = {average_reward} \n\n')</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mx"><img src="../Images/cc7adb6b0a187acc70fd028ff4fda68b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*PqcIHPOSfajUka0W.jpg"/></div></div></figure><p id="d279" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们观察到，当价值迭代运行10，000集时，它具有更好的平均回报和更高的获胜次数。</p><h1 id="14a5" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">结束注释</h1><p id="91ee" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">在本文中，我们熟悉了使用动态编程的基于模型的规划，它给出了环境的所有规范，可以找到最佳策略。我想特别提到萨顿和巴尔托写的关于RL的杰出著作，它是这种技术的圣经，并鼓励人们参考它。更重要的是，你已经向掌握强化学习迈出了第一步。请继续关注这个令人兴奋的领域中涵盖不同算法的更多文章。</p><p id="5efa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你也可以在分析Vidhya的Android应用上阅读这篇文章</p><p id="e23e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jp">原载于2018年9月17日</em><a class="ae li" href="https://www.analyticsvidhya.com/blog/2018/09/reinforcement-learning-model-based-planning-dynamic-programming/" rel="noopener ugc nofollow" target="_blank"><em class="jp">【www.analyticsvidhya.com</em></a><em class="jp">。</em></p></div></div>    
</body>
</html>