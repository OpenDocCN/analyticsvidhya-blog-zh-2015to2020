<html>
<head>
<title>Named Entity Recognition (NER) for Turkish with BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带BERT的土耳其语命名实体识别(NER)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/named-entity-recognition-for-turkish-with-bert-f8ec04a31b0?source=collection_archive---------1-----------------------#2020-06-25">https://medium.com/analytics-vidhya/named-entity-recognition-for-turkish-with-bert-f8ec04a31b0?source=collection_archive---------1-----------------------#2020-06-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/3d53058a89c64bc6544b134e57586676.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IKLMYZuiQS-hXRAdPMzdPw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片鸣谢:<a class="ae iu" href="https://www.codemotion.com/magazine/dev-hub/machine-learning-dev/bert-how-google-changed-nlp-and-how-to-benefit-from-this/" rel="noopener ugc nofollow" target="_blank">https://www . code motion . com/magazine/dev-hub/machine-learning-dev/Bert-how-Google-changed-NLP-and-how-to-benefit-from-this/</a></figcaption></figure><h1 id="5a2d" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">介绍</h1><p id="39cc" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">谷歌人工智能的研究人员在2年前发布了论文“<a class="ae iu" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> BERT:用于语言理解的深度双向转换器的预训练</a>”。从那以后，它在机器学习/自然语言处理领域获得了很大的流行。BERT是建立在最近在NLP世界流行的许多成功和有前途的工作之上的。包括但不限于Seq2Seq架构、Transformer(来自“注意是你所需要的全部”一文)、ELMO、ULM-FIT和无监督语言建模。在BERT发布的时候，BERT在各种各样的NLP任务上展示了最先进的性能。在本文中，我们将在土耳其命名实体识别(NER)数据集上微调预训练的土耳其BERT模型。我们将使用流行的HuggingFace预训练变形金刚库进行微调。我们还将实现一个已知的解决方案，通过构建重叠子序列来处理BERT最大序列长度问题。</p><p id="7496" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在本文中，我假设读者已经了解了以下主题的背景信息:</p><ol class=""><li id="f8c8" class="kw kx hi jv b jw kr ka ks ke ky ki kz km la kq lb lc ld le bi translated">命名实体识别(NER)。</li><li id="a43a" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">来自变压器(BERT)的双向编码器表示。</li><li id="3b0f" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">HuggingFace(变形金刚)Python库。</li></ol><h1 id="b372" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">本文的重点:</h1><ol class=""><li id="0bb0" class="kw kx hi jv b jw jx ka kb ke lk ki ll km lm kq lb lc ld le bi translated">利用HuggingFace Trainer class轻松微调ner任务的BERT模型(适用于大多数变形金刚，而不仅仅是BERT)。</li><li id="e002" class="kw kx hi jv b jw lf ka lg ke lh ki li km lj kq lb lc ld le bi translated">处理比BERT的MAX_LEN = 512更长的序列</li></ol><h1 id="490c" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">拥抱脸培训师课程:</h1><p id="dfb8" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">变形金刚新的训练师职业提供了一个简单的方法来微调已知任务的变形金刚模型，比如康尔ner。<a class="ae iu" href="https://huggingface.co/transformers/examples.html" rel="noopener ugc nofollow" target="_blank">这里的</a>是其他支持的任务。本课程将关注培训/评估循环、日志记录、模型保存等。这使得切换到其他变压器模型非常容易。为此，我们将使用另一个类NerDataset来处理数据的加载和标记化。</p><h1 id="1b5c" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">预处理</h1><p id="8094" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">为了能够在ner任务中使用教练模型，我们将数据转换为如下所示的CoNLL格式:</p><figure class="lo lp lq lr fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/826a0934a0de0dd34b08e08bf6179037.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*Hs1Xw5IdYGdw4QXZtVp54g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">CoNLL文件格式</figcaption></figure><p id="b9f4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">其中每行有两列“标记标签”,由空格分隔，不同的句子由空行分隔。为了做到这一点，我们首先创建一个元组列表，其中每个元组具有(句子id，标记，标签),然后使用这些来初始化Pandas数据帧，其中每行表示(句子id，标记，标签)元组。不幸的是，我使用的数据集是私有的，因此我实际上不能共享数据集。我将只指出运行这段代码所需的预期数据格式。train_docs和test_docs是字符串列表。我们使用doc.split()执行简单的空白标记化，您可以跳过这一步(第6行和第11行)。train_labels和test_labels是令牌级IOB标签\labels的列表。</p><pre class="lo lp lq lr fd ls lt lu lv aw lw bi"><span id="1ea4" class="lx iw hi lt b fi ly lz l ma mb">print(train_docs)</span><span id="46c8" class="lx iw hi lt b fi mc lz l ma mb">###OUTPUT###<br/>["This is one sentence",<br/>"here's another one"]</span><span id="21ee" class="lx iw hi lt b fi mc lz l ma mb">print(train_labels)</span><span id="2500" class="lx iw hi lt b fi mc lz l ma mb">###OUTPUT###<br/>[['O','O','O','O'],<br/>['O','O','O']]</span></pre><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="md me l"/></div></figure><pre class="lo lp lq lr fd ls lt lu lv aw lw bi"><span id="4054" class="lx iw hi lt b fi ly lz l ma mb"># check the first 10 rows<br/>print(test_df.head(10))</span></pre><figure class="lo lp lq lr fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/839be5b62810b89bcad7b831f892b1f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*byLRdNE9ouvq3O1y0Y817A.png"/></div></figure><p id="0f16" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们还需要一个包含所有可能标签的列表，以及一个标签到整数的映射。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="md me l"/></div></figure><pre class="lo lp lq lr fd ls lt lu lv aw lw bi"><span id="c9fd" class="lx iw hi lt b fi ly lz l ma mb">print(labels) </span><span id="6ff3" class="lx iw hi lt b fi mc lz l ma mb">###OUTPUT###<br/>['O', 'B_PERSON', 'I_PERSON', 'B_LOCATION', 'I_LOCATION',        'B_ORGANIZATION', 'I_ORGANIZATION']</span><span id="7b2d" class="lx iw hi lt b fi mc lz l ma mb">print(label_map)</span><span id="5d14" class="lx iw hi lt b fi mc lz l ma mb">###OUTPUT###<br/>{0: 'O',  1: 'B_PERSON',  2: 'I_PERSON',  3: 'B_LOCATION',  4: 'I_LOCATION',  5: 'B_ORGANIZATION',  6: 'I_ORGANIZATION'}</span></pre><p id="1866" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">NerDataset希望每个培训/测试/开发集都有一个. txt文件。因此，我们的下一步是将这些CoNLL格式的文件创建到一个目录(“data”)中，我们将在这个目录中保存我们的培训和测试。txt文件:</p><h1 id="4ce5" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">NerDataset</h1><p id="3b13" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我们的trainer对象将通过NerDataset对象期待训练输入。我们已经准备好了train.txt文件，所以现在我们需要提供BERT记号化器并指定所需的参数(文件目录，最大序列长度..等等。).首先，我们必须下载包含NerDataset定义的utils_ner.py。</p><pre class="lo lp lq lr fd ls lt lu lv aw lw bi"><span id="96cb" class="lx iw hi lt b fi ly lz l ma mb">!wget <a class="ae iu" href="https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/utils_ner.py" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/huggingface/transformers/master/examples/token-classification/utils_ner.py</a></span></pre><p id="8439" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">这个脚本必须添加到python构建路径中，这样您就可以简单地导入它。现在我们必须指定模型参数，这些是初始化BERT记号化器和模型所需要的。我们将在两个独立的python字典中保存BERT模型参数，一个用于BERT模型参数，另一个用于数据相关参数。我们正在使用的模型是一个cased base BERT模型(<a class="ae iu" href="https://github.com/stefan-it/turkish-bert" rel="noopener ugc nofollow" target="_blank"> BERTurk </a>)，它是在一个大小为35GB和44，04976，662个标记的土耳其语料库上预先训练的。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="md me l"/></div></figure><p id="f825" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">接下来，我们初始化我们的配置和标记器。Config用于根据指定的参数(如类的数量)实例化BERT模型，并定义模型架构。Tokenizer负责准备模型的输入。通过提供正确的model_name_or_path参数，在这里使用auto model(auto config，AutoTokenizer)有助于使用其他的transformer模型(XLNet，RoBERTa…等等)。因为我们已经对句子进行了标记化，并且使用BERT Tokenizer对句子进行标记化会导致标签对齐问题。我们通过将do_basic_tokenize = False传递给标记器来跳过基本的空白标记化。然后，记号赋予器将只执行单词块记号化。在这里，我还定义了AutoModelForTokenClassification，它基本上是我们的BERT模型，在顶部有一个分类头用于标记分类。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="md me l"/></div></figure><p id="7fd2" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在，我们准备创建我们的训练NerDataset对象。Split用于指定我们正在创建的数据集的模式。它有三种状态:训练、测试、开发。通过指定模式，该对象将自动获取“data_dir”中的正确文件(在本例中为train.txt)。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="md me l"/></div></figure><h1 id="eea1" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">运动鞋</h1><p id="76ae" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">现在，我们几乎准备好创建我们的培训师，并开始培训过程。但是，首先我们需要指定我们的训练参数。训练器通过训练参数对象期望训练参数。我们将创建一个json文件，其中包含我们所有的训练参数。然后，我们将使用HfArgumentParser解析该文件，并将参数加载到TrainingArguments对象中。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="md me l"/></div></figure><p id="cb7d" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">在这里，我只指定了一些基本参数(output_dir是一个必需的参数)，其他参数(学习率，重量衰减…等)。)我只是使用了它们的默认值。<a class="ae iu" href="https://github.com/huggingface/transformers/blob/18177a1a60be16b2ff6749ecb5fe850ee28b49ff/src/transformers/training_args.py" rel="noopener ugc nofollow" target="_blank">这里是</a>可能的训练参数及其默认值的完整列表，可以在记录和保存模型检查点时提供更多信息。最后，我们可以创建我们的培训师对象，并通过调用。train()函数。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="md me l"/></div></figure><p id="ccea" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">使用Colab的GPU，训练需要大约1小时(每个纪元20分钟)。</p><p id="6ad4" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在，我们可以使用测试数据集来评估我们的模型性能。我们将从创建一个用于测试数据的NerDataset开始。然后，我们可以获得输入的最后一层输出/激活。这些激活具有以下形状(batch_size，seq_len，num_of_labels ),表示所有测试示例的类概率。然后，我们使用argmax(axis= 2)函数来获取每个示例中概率最高的标签。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="md me l"/></div></figure><p id="b156" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在，为了评估这些预测，我们可以使用seqeval库来计算精确度、召回率和F1分数度量。首先我们安装seqeval库:</p><pre class="lo lp lq lr fd ls lt lu lv aw lw bi"><span id="d48c" class="lx iw hi lt b fi ly lz l ma mb">!pip install seqeval</span></pre><p id="8d9b" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">然后，我们需要得到与预测形状相同的真实标签(num_of_examples，seq_len)。我们可以使用完全符合该形状的原始数据(在CoNNL变换之前)。但是我认为实际上从CoNLL格式(我们的测试数据帧)转换成(num_of_examples，seq_len)形式的标签列表可能是有用的。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="md me l"/></div></figure><p id="78c6" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">现在我们可以简单地通过计算F1值来评估我们的模型。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="md me l"/></div></figure><pre class="lo lp lq lr fd ls lt lu lv aw lw bi"><span id="ebb2" class="lx iw hi lt b fi ly lz l ma mb">F1-score: 95.2%</span></pre><p id="d4bb" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">我们还可以通过使用classification_report函数来查看类级别的性能。我们可以观察到支持度低的班级(I_ORGANIZATION，I_LOCATION)其实f1分最低。</p><figure class="lo lp lq lr fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/61ac001cbffa826d745d7b4783ab30a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*SVqASfXb8Zm2e7t6OxFeZg.png"/></div></figure></div><div class="ab cl mh mi gp mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="hb hc hd he hf"><h1 id="bb34" class="iv iw hi bd ix iy mo ja jb jc mp je jf jg mq ji jj jk mr jm jn jo ms jq jr js bi translated">用BERT处理长序列</h1><p id="0406" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">BERT模型的一个限制是它的最大长度限制。自然，你不能处理超过512个记号的序列。在我们的测试集中，实际上有8个序列没有被完全处理(裁剪)。这里需要注意的一点是，即使我们提供了512个单词的序列(在执行单词片段标记化之前),该序列也很可能不会被完全处理，因为在单词片段标记化期间，许多单词实际上被拆分成单词片段，例如:</p><pre class="lo lp lq lr fd ls lt lu lv aw lw bi"><span id="f2c7" class="lx iw hi lt b fi ly lz l ma mb">ORIGINAL: "Why isn't my text tokenizing"<br/>WordPiece TOKENIZED: ['why', 'isn', "'", 't', 'my', 'text', 'token', '##izing']</span></pre><p id="a130" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">记住这一点，实际检查一个序列是否将被完全处理的一种方法是实际尝试并标记该序列，并检查标记序列的长度是否等于或小于用于初始化BERT的最大长度。</p><pre class="lo lp lq lr fd ls lt lu lv aw lw bi"><span id="e3a0" class="lx iw hi lt b fi ly lz l ma mb"># simple example<br/>tokenizer = AutoTokenizer.from_pretrained("dbmdz/bert-base-turkish-cased",do_basic_tokenize = False)<br/>list_of_tokens = tokenizer.tokenize("Why isn't my text tokenizing")<br/>print(list_of_tokens)</span></pre><p id="3181" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">对于我们预测的情况，我们实际上可以将每个示例标签的长度与预测的标签进行比较。预测的标签不会超过最大值512，当两个长度不匹配时，这意味着该示例还没有被完全处理。</p><p id="50c2" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">为了克服这个问题，我们实际上可以创建重叠序列，将一个长序列分成两个较短的重叠序列。我们需要重叠来为分裂序列的两边提供上下文。然后，我们可以创建一个数据框架，重复我们之前完成的步骤，轻松地进行预测。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="md me l"/></div></figure><p id="464f" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">接下来，我们必须从重叠的标签中实际组合(重建)预测的(原始长度)标签序列。我已经尽可能地给代码添加了注释，我希望它足够清晰。重构序列后，我们用重构的标签替换旧的(未完全处理的)预测标签。这样，所有真实标签和预测标签的长度将匹配(举例来说)。</p><figure class="lo lp lq lr fd ij"><div class="bz dy l di"><div class="md me l"/></div></figure><p id="e9dd" class="pw-post-body-paragraph jt ju hi jv b jw kr jy jz ka ks kc kd ke kt kg kh ki ku kk kl km kv ko kp kq hb bi translated">然后，我们可以计算F1分数，检查是否有任何改进</p><pre class="lo lp lq lr fd ls lt lu lv aw lw bi"><span id="9320" class="lx iw hi lt b fi ly lz l ma mb">print("F1-score: {:.1%}".format(f1_score(test_labels, preds_list)))</span><span id="737c" class="lx iw hi lt b fi mc lz l ma mb">###OUTPUT###<br/>F1-score: 95.4%</span></pre></div></div>    
</body>
</html>