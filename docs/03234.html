<html>
<head>
<title>Understanding and Writing your own Machine Learning Algorithms: Linear and Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解并编写你自己的机器学习算法:线性和逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-and-writing-your-own-machine-learning-algorithms-linear-and-logistic-regression-6a26eb0a67cd?source=collection_archive---------16-----------------------#2020-01-21">https://medium.com/analytics-vidhya/understanding-and-writing-your-own-machine-learning-algorithms-linear-and-logistic-regression-6a26eb0a67cd?source=collection_archive---------16-----------------------#2020-01-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/4952e28f1f00c5098472ec0ab5752d96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cgP9_B69A6T91bwW"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://unsplash.com/@trille?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">托马斯·麦赫雷</a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</figcaption></figure><p id="9979" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你已经为一个连续变量(比如价格)或者一个分类问题建立了一个模型，那么你很可能遇到过线性回归和逻辑回归。这可能是当今许多编程语言(如Python中的Scikit-learn)中最流行的两种机器学习算法，只需几行代码就可以导入并运行。然而，这一过程的简单性意味着无需真正理解底层机制就可以非常容易地构建这些模型。</p><p id="6e15" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，在实现你自己的算法之前，你应该了解它们是如何工作的，在哪里可以使用它们，在哪里不可以。如果你知道这一点，实现你自己的算法将是帮助你理解他们做什么的一个非常好的方法。</p><p id="b88f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇博文中，我将解释这些众所周知的算法背后的直觉和数学，并为您提供一种自己编写和测试它们的方法，以便更好地理解正在发生的事情。这也意味着你将能够为特定的场景优化你的算法。</p><p id="3cf6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下内容可以用多种语言完成，具体取决于你最习惯的语言。我已经使用NumPy (Python)进行了计算，并在Octave中进行了初步实现——类似于MatLab，但适用于那些买不起许可证的人。Python中<a class="ae iu" href="https://www.gnu.org/software/octave/" rel="noopener ugc nofollow" target="_blank"> Octave </a>文件的运行可以用<a class="ae iu" href="https://oct2py.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> Oct2Py库实现。</a></p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="5a78" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak">线性回归</strong></h1><p id="faad" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">基于特征X和相应的权重/系数θ的组合，线性回归用于预测连续的结果变量y。我们称我们的预测为我们的假设h_θ(x)，并计算成本函数J(θ)，其中最常用的是均方误差。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/2e10bb64cbcd239426d23978451271ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*1YT2woZQhoPw7fbIUan_JA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">我们的预测h(x)和相关的成本函数/误差项。其中n是特征的数量，m是训练样本的数量。我们设置x0 =1，所以θ0是我们的截距。</figcaption></figure><p id="e608" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于我们的J(θ)是一个平方项，我们的成本函数呈现抛物线，其中最小值是误差最小的地方。当我们求导误差项的变化率时，1/2m是为了使数学更容易。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es li"><img src="../Images/a1a44b3c08ea01c2dac2412688a277e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*rxYrk5Rgs-MsORKcAhvMpQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">通过改变θ使成本函数最小化。</figcaption></figure><p id="f7d0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在机器学习来了。我们主要计算预测中的误差，并在使误差最小的方向上改变每个系数的值。这很好地解决了这个问题，因为我们可以利用成本函数对每个系数的微分来保持其他系数不变。这称为同步更新:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/905149d89fd611361b22ba3dbc507dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*2ObWvfv5cVIBBM64vUCxUw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">系数的同时更新，其中α是学习率，J是成本函数。</figcaption></figure><p id="3952" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是一个方便的等式，因为我们还可以看到，随着θ的减小，误差项也会减小，因此dJ/dθ项也会变小。这意味着当我们接近最小值时，我们会采取更小的步骤。我们的α值是学习率，必须适当选择，以便不超过最小值，也使我们的算法不需要太长时间运行。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lk"><img src="../Images/ed53c5ee081096aaaf6fc30848566b3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pu36X9W-i1Cl52ImC4Y9cg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">太大的α值(快速学习率)会导致成本函数膨胀，但是太小的α值(慢速学习率)会使我们达不到最小值。</figcaption></figure><h2 id="d67b" class="ll kb hi bd kc lm ln lo kg lp lq lr kk jg ls lt ko jk lu lv ks jo lw lx kw ly bi translated">矢量化</h2><p id="0670" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">为了预测多个变量，我们可以使用矢量化，只需对代码进行微小的调整，就可以轻松计算。这是将我们的特征X转换成矩阵，将我们的系数θ转换成向量的过程。</p><p id="24b8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是用Octave和Python写的代码，用于同步更新:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/b643ea6d7855c8ed3b2556429cbe03e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TM8XMXO2Xnz34oYstFX7TQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">整个梯度下降算法，向量化，在八度。</figcaption></figure><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/c965baddd906c78396da96888f392d39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f-JWJQRFNZFrd4unXLharQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Python中的同步更新，矢量化。</figcaption></figure><h1 id="c352" class="ka kb hi bd kc kd mb kf kg kh mc kj kk kl md kn ko kp me kr ks kt mf kv kw kx bi translated"><strong class="ak">逻辑回归</strong></h1><p id="f4fa" class="pw-post-body-paragraph iv iw hi ix b iy ky ja jb jc kz je jf jg la ji jj jk lb jm jn jo lc jq jr js hb bi translated">现在，当我们需要预测离散值时，我们使用逻辑回归。很多人会称逻辑回归为分类器，但实际上它不是。随着我对相关数学的研究，这一点会变得清晰，但本质上这是一个概率预测。</p><p id="217d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在逻辑回归的情况下，我们的假设采取不同的形式:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/436e423b8640b69318a0bd340bee5557.png" data-original-src="https://miro.medium.com/v2/resize:fit:516/format:webp/1*Rz4BhaaIUcv2CBvlJCTcpQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">sigmoid函数的方程。</figcaption></figure><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/a863d4ad959d3f42f96b693ad5a78813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*WzC4Z3VsylTWC-vj_PfVEg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">决策边界设置为0.5的sigmoid函数。</figcaption></figure><p id="b5ce" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于我们将使用sigmoid函数将我们的预测转换为0到1之间的概率，因此成本函数会发生变化。如果我们对此求平方，就像在线性回归的MSE中一样，我们将最终得到多个最小值，梯度下降将无法找到成本函数的全局最小值。</p><p id="aeaf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">相反，我们使用下面的交叉熵或对数损失函数:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/14d51e4b7f6aad7b0c524cd117c73459.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kRHUN3pXPdzE9BY9-Qs4CQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">成本函数可以合并成一个等式，其中，根据y是1还是0，该项的一侧消失。</figcaption></figure><p id="2060" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该等式还方便地为错误预测提供了更大的惩罚，使得成本函数更大。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/171aa6f9a0a74e31330461a980d5f213.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*Y_t5KT4tEVWWf64eY1WJjw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">对数损失在y轴上，我们的预测在x轴上。摘自吴恩达在Coursera上的斯坦福机器学习课程。</figcaption></figure><p id="d80e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们可以把它写成代码:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/fc8c2dba83001933adb2aa3359c0cc86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2_JaiXwbibCt_LPsXPKCTQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">正则化逻辑回归，矢量化，以倍频程表示。</figcaption></figure><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/b9396291a444e52c383128e811856714.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eseQVfuQSkxCbxdlSWRkfg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Python中正则化逻辑回归的矢量化算法</figcaption></figure><p id="eb66" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就是你自己的逻辑和线性回归算法！我仍然建议坚持使用预先构建的模型，因为这些模型已经针对许多不同的用例进行了尝试和测试——尽管您可能会找到一个用例来优化和使用您自己的用例！</p><p id="7021" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">希望这已经提供了对这些常用模型如何工作以及如何调试它们的有用理解。如果你觉得这有帮助或有趣，请留下评论！</p></div></div>    
</body>
</html>