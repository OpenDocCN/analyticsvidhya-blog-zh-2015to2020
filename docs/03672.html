<html>
<head>
<title>Entropy, Loss Functions and the Mathematical Intuition behind them</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">熵、损失函数及其背后的数学直觉</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/loss-functions-and-the-mathematical-intuition-behind-them-fec4ac95b117?source=collection_archive---------6-----------------------#2020-02-13">https://medium.com/analytics-vidhya/loss-functions-and-the-mathematical-intuition-behind-them-fec4ac95b117?source=collection_archive---------6-----------------------#2020-02-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/02daede02a831450c1a9a7ca9fa02657.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fUMsKHuYUdfuL6dTJMM8yg.jpeg"/></div></div></figure><p id="42c5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">损失函数是任何深度学习问题中必不可少的一步。首先，什么是损失函数？</p><blockquote class="jo jp jq"><p id="0364" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">损失函数只是一种评估方法，它提供了关于模型运行情况的信息。如果使用该模型的预测与真实值完全不同，那么损失函数输出更大的数字。当您进行更改以改进模型时，损失函数将告诉您模型实际上是否在给定数据集上显示出任何改进。</p></blockquote><p id="5f60" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">损失函数给出了我们的分类器有多好的想法，它量化了你对现有分类器有多满意</p><p id="b882" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在本次讨论中，我们将主要关注分类损失函数。在探索不同的损失函数之前，我们需要知道概率分布和熵</p><h1 id="742d" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">概率分布</h1><blockquote class="jo jp jq"><p id="f4f2" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">概率分布是实验中所有结果概率的集合。分布中的每个概率必须介于0和1之间(0和1包括在内)，并且分布中所有概率的总和必须等于1。</p></blockquote><p id="5f29" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果x，y，z是实验中3个不同结果类别的概率。它必须满足以下条件</p><blockquote class="kt"><p id="cba1" class="ku kv hi bd kw kx ky kz la lb lc jn dx translated">x + y + z = 1且0≤ x ≤ 1，0≤ y ≤ 1，0≤ z ≤ 1</p></blockquote><p id="ce0e" class="pw-post-body-paragraph iq ir hi is b it ld iv iw ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn hb bi translated">[0.5，0.89，0.6] →不是概率分布</p><p id="2fe6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">[0.05，0.89，0.06] →是一个概率分布</p><h1 id="2668" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">熵</h1><blockquote class="jo jp jq"><p id="a9f3" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">熵是信息随机性的量度。统计术语中的熵定义了信息概率分布的无序或不确定性的度量。熵越高，就越难从这些信息中得出任何具体的结果。</p></blockquote><p id="f3b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们用一个例子来探讨这个问题。考虑下图。</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es li"><img src="../Images/51442a8e13004090b10142165d9c9820.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*42qvXYcNjIJ3H6gDWGTtmw.png"/></div></div></figure><p id="0f7c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设有三个学生完成了考试，正在等待结果。</p><p id="6871" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">学生A知道他有90%的机会考试不及格，而通过的机会是10%</p><p id="68be" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">学生B知道他有70%的可能考试不及格</p><p id="ff1a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是学生C面临着50%的考试及格或不及格的可能性。</p><p id="35bb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在3名学生中，学生C面临着更高的不确定性。这意味着学生C的熵比其他两个高。让我们用数学方法来证明这一点</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/30ffc61782cfcd3cd6f2c0a4af72d005.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*9GnKGdLY1LFHkQJgGworUg.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">图一</figcaption></figure><p id="ad63" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">考虑学生A</p><p id="5a0e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过概率→ 10%，失败概率→ 90%</p><p id="711d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以把它看作一个10个结果的实验，其中9个是F，1个是P</p><p id="7079" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">学生FFFFFFFFFP</p><p id="30f5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">P(F) = 9/10，P(P) = 1/10</p><p id="ae54" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">A的结果= [ F，F，F，F，F，F，F，F，F，F，P]</p><p id="8372" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">学生B和学生C采用了类似的方法(图1)</p><p id="040f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所有学生所有结果概率的乘积</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ls"><img src="../Images/f776dc14bb0b193a964d90ba5886fbb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2fEIIj0ctW6sdYTgCGuH8w.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">图2</figcaption></figure><p id="8419" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在想象一下，如果我们有两个以上的结果，概率的乘积将产生一个非常小的数字。我认为大多数人更喜欢处理总数而不是乘积。将乘积转换为总和的一种方法是使用日志。</p><p id="fee2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> log(ab) = log(a) + log(b) </strong></p><p id="1c05" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所有学生的所有结果的概率乘积的对数</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/43cae00ba13bd5376b675dee4f38e792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BAO3Iq51TEHnwEIo0j64Iw.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">图3</figcaption></figure><p id="0e1e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所有学生的所有结果概率乘积的结果对数的平均值</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/9191a1c9be7fc609be327e213831a9f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5o5Zttp2x2zgczox5MHsig.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">图4</figcaption></figure><p id="ba26" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">熵是平均值的负值</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lv"><img src="../Images/230a0adaa68aab1c38cc7fe54e6a6761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wbvwu4Jmo1fpwtaU9itSQA.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">图5</figcaption></figure><p id="e6e3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">学生C的熵更大，这意味着学生C面临着所有学生中最高的不确定性。</p><p id="2c5a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">利用python中的scipy模块计算熵</p><pre class="lj lk ll lm fd lw lx ly lz aw ma bi"><span id="31c6" class="mb jw hi lx b fi mc md l me mf">from scipy.stats import entropy</span><span id="81fa" class="mb jw hi lx b fi mg md l me mf">entropy_st_a = entropy([9/10, 1/10])<br/>#entropy_st_a = 0.3250829733914482</span><span id="8d52" class="mb jw hi lx b fi mg md l me mf">entropy_st_b = entropy([7/10, 3/10])<br/>#entropy_st_b = 0.6108643020548935</span><span id="1706" class="mb jw hi lx b fi mg md l me mf">entropy_st_c = entropy([5/10, 5/10])<br/>#entropy_st_c = 0.6931471805599453</span></pre><h1 id="3565" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">什么是交叉熵？</h1><blockquote class="jo jp jq"><p id="2d37" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">交叉熵在深度学习中被用作损失函数。在熵中，我们只处理一种概率分布。在交叉熵中，我们处理两种不同的概率分布。</p></blockquote><p id="1119" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一个分布由真值组成。第二个分布由估计值组成。</p><p id="1337" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">交叉熵是两个概率分布之间差异的度量。</p><p id="eca0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们通过一个例子来了解交叉熵</p><p id="5ca5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">考虑包含4个不同结果的单个记录</p><p id="6a8b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该记录的4个结果的真实概率是[0.2，0.3，0.4，0.1]</p><p id="8bfc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你在一些分类器或模型的帮助下估计了该记录的4个结果的概率。</p><p id="2d8f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你估计的概率是[0.4，0.4，0.1，0.1]</p><p id="aee6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">真值t1，t2，t3，t4 = 0.2，0.3，0.4，0.1</p><p id="0f79" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">估计值p1，p2，p3，p4 = 0.4，0.4，0.1，0.1</p><p id="60fe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">熵(对于真值t) = -t*log(t) →熵只需要一个分布</p><p id="cfd7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">交叉熵(对于真概率t和估计概率p) = -t*log(p) →交叉熵需要两个分布来比较自己。</p><blockquote class="jo jp jq"><p id="bd73" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">交叉熵越高，两个概率分布之间的差异就越大</p></blockquote><blockquote class="kt"><p id="2d01" class="ku kv hi bd kw kx mh mi mj mk ml jn dx translated">交叉熵损失是机器学习或深度学习中分类问题中最常用的损失函数。</p></blockquote><p id="319d" class="pw-post-body-paragraph iq ir hi is b it ld iv iw ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn hb bi translated">一般来说，在机器学习中，他们使用不同的术语来表示交叉熵，它被称为<strong class="is hj">对数损失</strong></p><p id="55c3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在深度学习中，有3种不同类型的交叉熵损失</p><ol class=""><li id="1d06" class="mm mn hi is b it iu ix iy jb mo jf mp jj mq jn mr ms mt mu bi translated">二元交叉熵损失→当类别数或目标类别中的结果数为2时交叉熵的名称</li><li id="18a5" class="mm mn hi is b it mv ix mw jb mx jf my jj mz jn mr ms mt mu bi translated">分类交叉熵损失→当类别数或目标类别中的结果数大于2且结果的真值为1时交叉熵的名称。</li><li id="9423" class="mm mn hi is b it mv ix mw jb mx jf my jj mz jn mr ms mt mu bi translated">稀疏分类交叉熵损失→当目标类中的类数或结果数大于2并且结果的真值不是一个热点时的交叉熵的名称。</li></ol><p id="e7c0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们将研究所有3种不同的损失函数</p><h1 id="5a82" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">二元交叉熵损失</strong></h1><p id="4f31" class="pw-post-body-paragraph iq ir hi is b it na iv iw ix nb iz ja jb nc jd je jf nd jh ji jj ne jl jm jn hb bi translated">假设您正在处理一个只涉及两个类和三个记录的分类问题</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/51ee075b5483c2d48c532ce2a989ce6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*Gmzy36BAcXRqJjidr-DHrg.png"/></div></figure><p id="c944" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这三个记录的真实类别是[[1。, 0.], [1., 0.], [0., 1.]]</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/f347bfd59a15e1ebc4da1cd5eb60f066.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/1*4kP7Jpq5HPiWMg07yT09Ig.png"/></div></figure><p id="a80e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些记录的预测概率是[[0.9，. 1]，[0.7，. 3]，[0.4，. 6]]</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/c047d15a8532097c00aeb30ab587506f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*gMjaKAjZ4a7esjN7hTiFJQ.png"/></div></figure><p id="76b7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">导入NumPy以实现高效的数值计算</p><pre class="lj lk ll lm fd lw lx ly lz aw ma bi"><span id="fbb5" class="mb jw hi lx b fi mc md l me mf">import numpy as np</span><span id="c246" class="mb jw hi lx b fi mg md l me mf">true_values = np.array([[1.,0.],[1.,0.],[0.,1.]])</span><span id="7a6b" class="mb jw hi lx b fi mg md l me mf">predictions = np.array([[.9,.1], [.7,.3], [.4,.6]]</span><span id="6fbf" class="mb jw hi lx b fi mg md l me mf">true_values[0] = [1.,0.]<br/>predictions[0] = [.9,.1]</span><span id="1844" class="mb jw hi lx b fi mg md l me mf"><br/>bce_loss_first = -(1 * np.log(0.9))<br/>#bce_loss_first = 0.10536051565782628</span><span id="2686" class="mb jw hi lx b fi mg md l me mf">true_values[0] = [1.,0.]<br/>predictions[0] = [.7,.3]</span><span id="3bc0" class="mb jw hi lx b fi mg md l me mf">bce_loss_second = -(1 * np.log(0.7))<br/>#bce_loss_second = 0.35667494393873245</span><span id="614b" class="mb jw hi lx b fi mg md l me mf">true_values[0] = [0.,1.]<br/>predictions[0] = [.4,.6]</span><span id="c7b4" class="mb jw hi lx b fi mg md l me mf">bce_loss_third = -(1 * np.log(0.6))<br/>#bce_loss_third = 0.5108256237659907</span></pre><p id="31bc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们已经计算了各个记录的所有单个损失</p><p id="988c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">发生的最终损失或由此产生的成本通过取所有单个损失的平均值来计算。</p><pre class="lj lk ll lm fd lw lx ly lz aw ma bi"><span id="0eff" class="mb jw hi lx b fi mc md l me mf">loss = (bce_loss_first + bce_loss_second + bce_loss_third)/3<br/>#loss = 0.3242870277875165</span></pre><p id="6868" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，我们使用相同的记录和相同的预测，并通过使用Keras中内置的二元交叉熵损失函数来计算成本</p><pre class="lj lk ll lm fd lw lx ly lz aw ma bi"><span id="3d59" class="mb jw hi lx b fi mc md l me mf">import tensorflow as tf<br/>from tensorflow.keras.losses import BinaryCrossentropy</span><span id="45c3" class="mb jw hi lx b fi mg md l me mf">#tensorflow is imported to convert records into tensors</span><span id="0ca9" class="mb jw hi lx b fi mg md l me mf">#importing the Keras Binary Cross-Entropy function<br/>bce_loss = BinaryCrossentropy()</span><span id="05f7" class="mb jw hi lx b fi mg md l me mf">m = tf.cast([[1.,0.],[1.,0.],[0.,1.]], tf.float32)</span><span id="d983" class="mb jw hi lx b fi mg md l me mf">n = tf.cast([[.9,.1], [.7,.3], [.4,.6]], tf.float32)</span><span id="e434" class="mb jw hi lx b fi mg md l me mf">loss = bce_loss(m,n).numpy()<br/>#loss = 0.32428685</span></pre><p id="4773" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用和不使用高级功能API，我们都获得了相同的结果。</p><p id="8743" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在基于我们获得的直觉，我们将建立我们自己的二元交叉熵损失函数。这是我们到目前为止在函数中讨论的要点。</p><pre class="lj lk ll lm fd lw lx ly lz aw ma bi"><span id="bc5b" class="mb jw hi lx b fi mc md l me mf">def binary_cross_entropy(true_values,predictions):<br/>    y_true = tf.cast(true_values,dtype=tf.float32)<br/>    y_pred = tf.cast(predictions,dtype=tf.float32)<br/>    X = tf.multiply(y_true,tf.math.log(y_pred))<br/>    return (-tf.reduce_sum(X)/len(y_true)).numpy()</span><span id="0e6b" class="mb jw hi lx b fi mg md l me mf">true_values = [[1.,0.],[1.,0.],[0.,1.]]<br/>predictions = [[.9,.1], [.7,.3], [.4,.6]]</span><span id="8d03" class="mb jw hi lx b fi mg md l me mf">loss = binary_cross_entropy(true_values, predictions)<br/>#loss = 0.32428703</span></pre><h1 id="4a58" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">分类交叉熵损失</strong></h1><blockquote class="kt"><p id="6372" class="ku kv hi bd kw kx ky kz la lb lc jn dx translated">二元交叉熵是范畴交叉熵的一个特例</p></blockquote><p id="4e62" class="pw-post-body-paragraph iq ir hi is b it ld iv iw ix le iz ja jb lf jd je jf lg jh ji jj lh jl jm jn hb bi translated">假设您正在处理一个只涉及3个类别/结果和3个记录的分类问题。</p><p id="f6fb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">真实的结果是一个热编码</p><p id="77bc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些记录的真实类别或结果</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nh"><img src="../Images/e0d8e057720f3a955fd73da727b2f15e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OfnpaaEAfUSi0Y4cRynmOQ.png"/></div></div></figure><p id="6324" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些记录的预测概率</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/0b60cb3b52f15fb206c23a293b84b83d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3hB2M2WWH_nuK02nMFVefQ.png"/></div></div></figure><pre class="lj lk ll lm fd lw lx ly lz aw ma bi"><span id="4df6" class="mb jw hi lx b fi mc md l me mf">import numpy as np</span><span id="a97e" class="mb jw hi lx b fi mg md l me mf">true_values = [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]</span><span id="7f18" class="mb jw hi lx b fi mg md l me mf">predictions = [[.9, .05, .05], [.05, .89, .06], [.05, .01, .94]]</span><span id="bf47" class="mb jw hi lx b fi mg md l me mf">true_values[0] = [1., 0., 0.]<br/>predictions[0] = [.9, .05, .05]</span><span id="6323" class="mb jw hi lx b fi mg md l me mf">celoss_first = -(1 * np.log(0.9))<br/>#cce_loss_first = 0.10536051565782628</span><span id="4181" class="mb jw hi lx b fi mg md l me mf">true_values[0] = [0., 1., 0.]<br/>predictions[0] = [.05, .89, .06]</span><span id="bec1" class="mb jw hi lx b fi mg md l me mf">cce_loss_second = -(1 * np.log(0.89))<br/>#cce_loss_second = 0.11653381625595151</span><span id="63b6" class="mb jw hi lx b fi mg md l me mf">true_values[0] = [0., 0., 1.]<br/>predictions[0] = [.05, .01, .94]</span><span id="49f9" class="mb jw hi lx b fi mg md l me mf">cce_loss_third = -(1 * np.log(0.94))<br/>#cce_loss_third = 0.06187540371808753</span></pre><p id="8e2a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">发生的最终损失或由此产生的成本通过取所有单个损失的平均值来计算。</p><pre class="lj lk ll lm fd lw lx ly lz aw ma bi"><span id="ed84" class="mb jw hi lx b fi mc md l me mf">loss = (cce_loss_first + cce_loss_second + cce_loss_third)/3<br/>#loss = 0.09458991187728844</span></pre><p id="4dbd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用相同的记录和相同的预测，通过使用Keras中内置的分类交叉熵损失函数来计算成本</p><pre class="lj lk ll lm fd lw lx ly lz aw ma bi"><span id="57c3" class="mb jw hi lx b fi mc md l me mf">from tensorflow.keras.losses import CategoricalCrossentropy</span><span id="74bc" class="mb jw hi lx b fi mg md l me mf">true_values = [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]<br/>predictions = [[.9, .05, .05], [.05, .89, .06], [.05, .01, .94]]</span><span id="17c7" class="mb jw hi lx b fi mg md l me mf">cce = CategoricalCrossentropy()<br/>loss = cce(true_values, predictions).numpy()<br/>#loss = 0.09458993</span></pre><p id="c61b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，基于我们获得的直觉，我们将建立我们自己的分类交叉熵损失函数。这是我们到目前为止在函数中讨论的要点。</p><pre class="lj lk ll lm fd lw lx ly lz aw ma bi"><span id="8ba3" class="mb jw hi lx b fi mc md l me mf">def categorical_cross_entropy(true_values,predictions):<br/>    y_true = tf.cast(true_values,dtype=tf.float32)<br/>    y_pred = tf.cast(predictions,dtype=tf.float32)<br/>    X = tf.multiply(y_true,tf.math.log(y_pred))<br/>    return (-tf.reduce_sum(X)/len(y_true)).numpy()</span><span id="51ba" class="mb jw hi lx b fi mg md l me mf">true_values = [[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]<br/>predictions = [[.9, .05, .05], [.05, .89, .06], [.05, .01, .94]]</span><span id="de09" class="mb jw hi lx b fi mg md l me mf">loss = categorical_cross_entropy(true_values,predictions)<br/>#loss = 0.09458993</span></pre><h1 id="1bff" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">稀疏分类交叉熵</h1><p id="460c" class="pw-post-body-paragraph iq ir hi is b it na iv iw ix nb iz ja jb nc jd je jf nd jh ji jj ne jl jm jn hb bi translated">真实的结果<strong class="is hj">不是一个热编码</strong></p><p id="f9a8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">类别或结果编号从0开始。如果存在4个结果或4个类别，则类别标签为0、1、2、3</p><p id="a42c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">考虑一个由6条记录组成的例子，每条记录有3个结果</p><p id="5005" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">真实结果</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/ad1e2e9346a9c8e7c519730d0f00c79b.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*0jOy_IRwW7ZPgsppx6Mfiw.png"/></div></figure><p id="eaef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所有记录的结果估计概率</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nj"><img src="../Images/c6baead8292d52744d1b20c76286c15f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4IF2ZwtgNmLV65giqkswbQ.png"/></div></div></figure><p id="d13b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以采用两种方法来计算损失</p><p id="cdd8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一种方法是通过将真实标签或真实结果类别转换成一个热标签，并遵循与我们在分类交叉熵损失中遵循的相同过程</p><p id="3caa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将通过上面的例子探索第二种方法</p><pre class="lj lk ll lm fd lw lx ly lz aw ma bi"><span id="4d4c" class="mb jw hi lx b fi mc md l me mf">true_values = [1, 0, 2, 1, 2, 0]</span><span id="8b96" class="mb jw hi lx b fi mg md l me mf">predictions = [<br/>    [.05, .9, .05],<br/>    [.89, .05, .06],<br/>    [.05, .01, .94],<br/>    [.1, .8, .1],<br/>    [.7, .2, .1],<br/>    [.08, .05, .87]<br/>]</span></pre><p id="4fe6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们得到了预测值和真值，预测值和真值的数量是相同的</p><p id="8c54" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在考虑true_value中的每个项目和基于各自索引的预测中的每个项目</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es nk"><img src="../Images/411b0f08a9c7e1a2491e424de1a1d3ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*wtEh_rTKZIM-Yt655MhV6A.png"/></div></figure><p id="01f1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在从预测中提取真实类别的估计概率</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div class="er es nl"><img src="../Images/555a7168f13aee62eedf3375d16a1fbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*5P7nBIlXfJw7A-vzrhgvdQ.png"/></div></figure><p id="7b00" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">出现所有真实类别的概率，相对于真实类别= 1，以及出现所有估计类别的概率，相对于真实类别=预测[真]</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nm"><img src="../Images/247ede03f118a11698381a32f581af0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hkes_WB8BEzCafJBJ7HluA.png"/></div></div></figure><p id="f378" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">计算每条记录的交叉熵，将每条记录的P(真/真)和P(真/预测)中的每个值视为两个分布</p><figure class="lj lk ll lm fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nn"><img src="../Images/ff423386652648d06a005ec728d8cba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LRIG-bYb7BHzOzrdBYs0bg.png"/></div></div></figure><p id="92cd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该数据集的交叉熵损失=所有记录的单个交叉熵的平均值，它等于<strong class="is hj">0.8840413961</strong></p><p id="ce40" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">单项损失的计算</p><pre class="lj lk ll lm fd lw lx ly lz aw ma bi"><span id="1c9c" class="mb jw hi lx b fi mc md l me mf">individual_ce_losses = [-np.log(predictions[i][true_values[i]]) <br/>for i in range(len(true_values))]</span></pre><p id="2d9a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过取单个损失的平均值计算最终损失</p><pre class="lj lk ll lm fd lw lx ly lz aw ma bi"><span id="de90" class="mb jw hi lx b fi mc md l me mf">loss = np.mean(individual_ce_losses)<br/>#loss = 0.8892045040413961</span></pre><p id="929f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用相同的记录和相同的结果估计，使用Keras中的稀疏分类交叉熵函数计算稀疏分类交叉熵</p><pre class="lj lk ll lm fd lw lx ly lz aw ma bi"><span id="7c4f" class="mb jw hi lx b fi mc md l me mf">cce = tf.keras.losses.SparseCategoricalCrossentropy()<br/>loss = cce(<br/>    tf.cast(true_values,dtype=tf.float32),<br/>    tf.cast(predictions, dtype=tf.float32)<br/>).numpy()</span><span id="e1be" class="mb jw hi lx b fi mg md l me mf">#loss = 0.8892045</span></pre><p id="c75b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">基于我们获得的直觉，我们将构建我们自己的稀疏分类交叉熵损失函数。</p><pre class="lj lk ll lm fd lw lx ly lz aw ma bi"><span id="a70e" class="mb jw hi lx b fi mc md l me mf">def sparse_categorical_cross_entropy(true_values, predictions):<br/>    y_t = tf.cast(true_values,dtype=tf.int32)<br/>    y_p = tf.cast(predictions,dtype=tf.float32)<br/>    losses = [-np.log(y_p[i][y_t[i]]) for i in range(len(y_t))]<br/>    return np.mean(losses)</span><span id="bba3" class="mb jw hi lx b fi mg md l me mf">loss = sparse_categorical_cross_entropy(true_values, predictions)<br/>#loss = 0.8892045</span></pre><h1 id="ce25" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">结论</h1><p id="fa73" class="pw-post-body-paragraph iq ir hi is b it na iv iw ix nb iz ja jb nc jd je jf nd jh ji jj ne jl jm jn hb bi translated">这是关于一些分类损失函数如何工作的简短讨论。我们讨论了它们背后的数学原理，从头开始构建，以及如何通过使用API来使用它们。</p><blockquote class="jo jp jq"><p id="5a36" class="iq ir jr is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">您将在<a class="ae no" href="https://github.com/ambatiashok60/Deep-Learning/tree/master/Loss%20Functions%20and%20their%20Mathematical%20intuition%20" rel="noopener ugc nofollow" target="_blank"> github </a>找到与我们的讨论相关的完整代码和数据文件</p></blockquote></div></div>    
</body>
</html>