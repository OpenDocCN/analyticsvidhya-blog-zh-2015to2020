<html>
<head>
<title>Regularization in Machine Learning and Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习和深度学习中的正则化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/regularization-in-machine-learning-and-deep-learning-f5fa06a3e58a?source=collection_archive---------2-----------------------#2019-09-20">https://medium.com/analytics-vidhya/regularization-in-machine-learning-and-deep-learning-f5fa06a3e58a?source=collection_archive---------2-----------------------#2019-09-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="d1e2" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">机器学习具有有限的训练数据和无限数量的假设，因此选择正确的假设是一个巨大的挑战。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/b8fef6d9c72d72cc9698e4303eac520c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CmDTGlQyibHUORQ0.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">模型拟合场景</figcaption></figure><p id="3cf1" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在开始正则化之前，让我们试着理解什么是模型拟合。</p><p id="7518" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这张图片显示了偏差-方差权衡的必要性，这就像是甜蜜点，在进入技术术语之前，让我们试着从外行的角度理解这些概念。</p><p id="055a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">你的任务是抚养孩子，你的目标是把他抚养好。在孩子的成长过程中，你希望给孩子多大程度的灵活性？</p><p id="45fb" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">目标功能:儿童的全面发展。</p><p id="745c" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如果你只是让孩子为考试而学习，或者试图通过让他没有任何放松或缓冲时间来让他成为运动员，那么你就太适合了，这意味着你的孩子只能做特定类型的任务，某种程度上是某方面的专业，除了那一项任务，你无法对生活中的其他事情有一个概括的理解。</p><p id="c4ef" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">一般来说，孩子们想玩手机，吃巧克力和冰淇淋，过充满阳光和彩虹的生活。</p><p id="2ca1" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">介绍正规化:在这种情况下，你试图找到最佳时机</strong></p><ul class=""><li id="6643" class="kj kk hi jp b jq jr jt ju jw kl ka km ke kn ki ko kp kq kr bi translated">如果你想要巧克力，就平均分配给你的朋友和家人</li><li id="ef29" class="kj kk hi jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">如果你想玩游戏，完成作业。</li></ul><p id="6c5c" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这些是我们日常育儿中的一些常规。</p><blockquote class="kx ky kz"><p id="47eb" class="jn jo la jp b jq jr ij js jt ju im jv lb jx jy jz lc kb kc kd ld kf kg kh ki hb bi translated">偏差-方差权衡是过度拟合和欠拟合之间的拉锯战。过度健康就像通过复习前一年的问题来为考试做准备，而不健康就像阅读章节摘要并回答考试。在一般的测试案例中，这两种情况都会失败。为了解决这个问题，我们有了正规化。</p></blockquote><h1 id="fa16" class="le lf hi bd lg lh li lj lk ll lm ln lo io lp ip lq ir lr is ls iu lt iv lu lv bi translated"><strong class="ak">机器学习的正规化</strong></h1><p id="ae93" class="pw-post-body-paragraph jn jo hi jp b jq lw ij js jt lx im jv jw ly jy jz ka lz kc kd ke ma kg kh ki hb bi translated">正则化是“我们对学习算法进行的任何修改，目的是减少其泛化误差，而不是训练误差。”</p><p id="8f13" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如果模型在训练数据集中表现良好，则不能保证它在新数据集中表现良好。但通常来说，当我们使用网络时，网络遇到的是看不见的数据。最直接的策略是增加数据集，但这在大多数时候是不现实的。修改主要是一些数学上的改进，比如一个更合适的损失函数，提前停止，退出等等</p><p id="604c" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在这一节中，我们将研究L1和L2正则化。使用L1和L2范数进行的正则化。术语方面经常会出现混淆，因此下面是相同术语的等效术语:</p><p id="e0b6" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">线性模型中的正则化</strong></p><p id="51d4" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里提到的正则化类型是线性模型中使用的正则化类型，即分类器或回归变量可以表示为直线或超平面，如线性回归、逻辑回归和支持向量机。</p><p id="f076" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><code class="du mb mc md me b">Euclidean norm == Euclidean length == L2 norm == L2 distance</code></p><p id="8160" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><code class="du mb mc md me b">Manhattan norm == Manhattan length == L1 norm == L1 distance</code></p><p id="7282" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">https://medium.com/@kolwalkaramod96/hello-f4a9317ad<a class="ae mf" rel="noopener" href="/@kolwalkaramod96/hello-f4a9317ad">你可以通过这个链接查看距离度量。</a></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mg"><img src="../Images/2756230e17f4a03877abf844ee7de802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Rlo7wu9FIpy0FGHg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">L2 vs L1情节</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mh"><img src="../Images/13341d9eb582721e86d32d5af02b6e06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*oh-SfAfEGXHWDBs0.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">相同的三维等高线图</figcaption></figure><p id="ef62" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这个情节有助于区分L1和L2诺姆。蓝色曲线是L2范数，在零点不连续的红线是L1范数。L1范数是不连续的，因此它产生稀疏，L2是可微的，因此我们可以使用像随机梯度下降的技术。</p><p id="20f5" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">一般学习模型方程</p><p id="8853" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><em class="la">优化函数=损失+正则项</em></p><p id="3a37" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如果模型是逻辑回归，则损失是对数损失，如果模型是支持向量机，则损失是铰链损失。</p><p id="9c6e" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如果模型是神经网络，那么它将是某种形式的交叉熵损失。</p><p id="7b5b" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">L1和L2范数也适用于深度学习模型。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mi"><img src="../Images/0754543e95100bebe68257725a46711c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*cw1Xrkb5KbPccH0D.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">L1和L2一般优化方程</figcaption></figure><p id="08db" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里，<strong class="jp hj">λ</strong>是正则化参数。它是超参数，其值被优化以获得更好的结果。L2正则化也被称为<em class="la">权重衰减</em>，因为它迫使权重向零衰减(但不完全为零)。</p><p id="0ded" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如果λ= 0，该模型将倾向于过度拟合，因为优化方程将仅仅是损失函数。如果lambda =无穷大或某个较大的值，该模型将趋于不适合。我们可以通过模型的超参数调整来获得最佳λ。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mj"><img src="../Images/d1c0be239b9835111a6b218d21283cf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/0*Y-aEeCDUcHSV8IK6"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">L1正则化与L2正则化的比较</figcaption></figure><p id="6113" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如果你想两全其美，你可以使用弹性网，它在目标函数中使用L1和L2正则化子。</p><p id="4498" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">为了获得正则化的最佳结果，最好对正则化使用交叉验证。下面的链接是交叉验证的介绍。</p><p id="d15e" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><a class="ae mf" href="http://Link for Cross Validation" rel="noopener ugc nofollow" target="_blank">https://machinelearningmastery.com/k-fold-cross-validation/</a></p><p id="e821" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">让我们看看SVM的正则化，因为它是一个更广义的逻辑回归，具有边际最大化策略。</p><p id="967a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">正则化参数(λ)用作给予错误分类的重要程度。SVM提出了一个二次优化问题，该问题寻求最大化两个类之间的间隔并最小化错误分类的数量。然而，对于不可分离的问题，为了找到解决方案，必须放松误分类约束，并且这是通过设置提到的“正则化”来完成的。</p><p id="6241" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">因此，直觉上，随着λ变大，允许的错误分类的例子越少(或者损失函数中付出的代价越高)。然后，当λ趋于无穷大时，解趋于硬边界(不允许错误分类)。当λ趋于0(不为0)时，允许的错误分类越多。</p><p id="e5a5" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这两者之间肯定有一个权衡，通常较小的lambdas，但不是太小，概括得很好。以下是线性SVM分类(二元)的三个示例。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mk"><img src="../Images/7f7baef2dba9233ca27b47f6bcf3aa6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/0*925ynA4Ed-P2ICfl.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ml"><img src="../Images/f63e790304f14ba131f3f435868b75a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/0*2btTV73AF6veI0D5.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mm"><img src="../Images/a1284bb748ace179046a670dcb579065.png" data-original-src="https://miro.medium.com/v2/resize:fit:924/format:webp/0*3w2Bm_W7qSgBCg1H.png"/></div></figure><p id="c11c" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">对于非线性核SVM，想法是相似的。鉴于此，对于较高的λ值，过度拟合的可能性较高，而对于较低的λ值，欠拟合的可能性较高。</p><p id="f2a7" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">下图显示了RBF内核的行为，将sigma参数固定为1，并尝试λ= 0.01和λ= 10</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mn"><img src="../Images/c758624fe364e54176ec7cdd3c7fde46.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/0*TPE_t25Q7vPjEwwS.png"/></div></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mo"><img src="../Images/6f50e1da38b2993ef6b0da6dbd713cb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/0*35kkrYzEn80ZTteR.png"/></div></figure><p id="4780" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">你可以说，lambda较低的第一个数字比数据更精确拟合的第二个数字更“宽松”。</p><p id="4a8d" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">基于树的模型中的正则化</strong></p><p id="bb6e" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">基于树的模型是诸如决策树、随机森林等模型</p><p id="2137" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">除了L1和L2正则化，还有特定的正则化技术用于树模型，树模型通常过拟合，因此L1和L2可能不是这些模型的最佳正则化。</p><ol class=""><li id="869d" class="kj kk hi jp b jq jr jt ju jw kl ka km ke kn ki mp kp kq kr bi translated">限制树的最大深度</li><li id="a1c6" class="kj kk hi jp b jq ks jt kt jw ku ka kv ke kw ki mp kp kq kr bi translated">对何时进一步分割节点设置更严格的停止标准(例如，最小增益、样本数量等。)</li><li id="bc79" class="kj kk hi jp b jq ks jt kt jw ku ka kv ke kw ki mp kp kq kr bi translated">超参数调整用于训练和交叉验证的模型和图</li><li id="c497" class="kj kk hi jp b jq ks jt kt jw ku ka kv ke kw ki mp kp kq kr bi translated">如果你使用决策树作为基础模型，你可以使用一个像随机森林一样的集合</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mq"><img src="../Images/9d6cb91443f9e2e8104203e1f9975711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fzjkrE321rLVYdUs.JPG"/></div></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mr"><img src="../Images/5fd3b0cd8a18781a13a9d7536468f6e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jhJU3mKODpiCUe0u.png"/></div></div></figure><p id="2a64" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">梯度提升决策树</strong></p><p id="9d9a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这是一个使用决策树作为基础模型的集成模型，boosting是一种集成技术，但与random Forrest相比，它的工作方式有所不同。</p><p id="aed2" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">关于这方面的更多细节，请参考这个<a class="ae mf" href="https://stats.stackexchange.com/questions/173390/gradient-boosting-tree-vs-random-forest" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/173390/gradient-boosting-tree-vs-random-forest</a></p><p id="931c" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">梯度增强中使用的一些技术有:</p><ol class=""><li id="789b" class="kj kk hi jp b jq jr jt ju jw kl ka km ke kn ki mp kp kq kr bi translated">超参数调整-一个自然正则化参数是梯度推进迭代次数<em class="la"> M </em>(即当基学习器是决策树时模型中的树的数量)。增加<em class="la"> M </em>会减少训练集的误差，但将其设置得太高可能会导致过度拟合。<em class="la"> M </em>的最佳值通常是通过监控独立验证数据集的预测误差来选择的。除了控制<em class="la"> M </em>之外，还使用了其他几种正则化技术。</li><li id="a07f" class="kj kk hi jp b jq ks jt kt jw ku ka kv ke kw ki mp kp kq kr bi translated">另一个正则化参数是树的深度。该值越高，模型越有可能过度拟合训练数据。</li><li id="b62b" class="kj kk hi jp b jq ks jt kt jw ku ka kv ke kw ki mp kp kq kr bi translated"><strong class="jp hj">收缩</strong>-梯度推进方法的一个重要部分是收缩正则化，包括修改更新规则如下:</li></ol><p id="5f10" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">f m(x)= f m1(x)+ν⋅γm h m(x)，0 &lt; ν ≤ 1 ,</p><p id="e9d0" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">where parameter ν</p><p id="d5e5" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">is called the “learning rate”.</p><p id="c7ce" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">Empirically it has been found that using small learning rates (such as ν &lt;0.1) yields dramatic improvements in models’ generalization ability over gradient boosting without shrinking ( ν = 1). However, it comes at the price of increasing computational time both during training and querying: lower learning rate requires more iterations.</p><p id="ab17" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">Similar methods of limiting max-depth and tree pruning can be used</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ms"><img src="../Images/2c56450f4b384f93249f9b97f7c0a549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*PLE2-T-c0GQqjt6p.png"/></div></figure><p id="4837" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">朴素贝叶斯中的正则化</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mt"><img src="../Images/268d72a6f5f5339f2712bc365a2c5fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*LY62NEEgr0FR4-gJ.jpg"/></div></div></figure><p id="5bac" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">拉普拉斯平滑或加法平滑</p><p id="05af" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这是一种用于平滑分类数据的技术。给定一个观察值x =⟨x ^ 1，x ^ 2，…，x ^ d⟩</p><p id="1562" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">根据具有N次试验的多项式分布，数据的“平滑”版本给出估计量:</p><p id="1b7e" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">θ ^ i = x i + α N + α d ( i = 1，…，d)，</p><p id="d5b4" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">其中“伪计数”<em class="la"> α </em> &gt; 0是平滑参数。<em class="la"> α </em> = 0对应无平滑。加法平滑是一种收缩估计，因为得到的估计介于经验概率(相对频率)x i N和均匀分布1/ d之间。</p><p id="8a3f" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">你总是需要这种“万无一失”的概率。</p><p id="2c55" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">要了解原因，请考虑最坏的情况，即训练样本中的单词没有一个出现在测试句子中。在这种情况下，根据你的模型，我们会得出结论，这个句子是不可能的，但它显然存在，造成了矛盾。</p><p id="7bb6" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">另一个极端的例子是测试句子“亚历克斯遇见了史蒂夫。”其中“met”在训练样本中出现了几次，但“Alex”和“Steve”没有出现。你的模型会得出结论，这种说法很可能是不正确的。</p><h1 id="dd94" class="le lf hi bd lg lh li lj lk ll lm ln lo io lp ip lq ir lr is ls iu lt iv lu lv bi translated">深度学习中的正则化</h1><p id="4773" class="pw-post-body-paragraph jn jo hi jp b jq lw ij js jt lx im jv jw ly jy jz ka lz kc kd ke ma kg kh ki hb bi translated">L1和L2正则化也适用于深度学习，但是还有更多方法，例如</p><ul class=""><li id="0bad" class="kj kk hi jp b jq jr jt ju jw kl ka km ke kn ki ko kp kq kr bi translated">拒绝传统社会的人</li><li id="eaf7" class="kj kk hi jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">数据扩充</li><li id="fd4b" class="kj kk hi jp b jq ks jt kt jw ku ka kv ke kw ki ko kp kq kr bi translated">提前停止</li></ul><h2 id="535f" class="mu lf hi bd lg mv mw mx lk my mz na lo jw nb nc lq ka nd ne ls ke nf ng lu nh bi translated"><strong class="ak">辍学</strong></h2><p id="6691" class="pw-post-body-paragraph jn jo hi jp b jq lw ij js jt lx im jv jw ly jy jz ka lz kc kd ke ma kg kh ki hb bi translated">为了理解什么是辍学，让我们看看一个经典的神经网络。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ni"><img src="../Images/774bfb04e3fa1aea5b51f9c27c4944ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/0*DCjNcmHAU5GABSRA.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nj"><img src="../Images/1b00b6955ef68d13cae0e6313d4972ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/0*QK9k4P8PWQSIxdHM"/></div></figure><p id="3ece" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj"> Dropout </strong>是一种<a class="ae mf" href="https://en.wikipedia.org/wiki/Regularization_(mathematics)" rel="noopener ugc nofollow" target="_blank">正则化</a>技术，由Google<a class="ae mf" href="https://en.wikipedia.org/wiki/Dropout_(neural_networks)#cite_note-pat-1" rel="noopener ugc nofollow" target="_blank">【1】</a>申请专利，用于通过防止训练数据上的复杂协同适应来减少<a class="ae mf" href="https://en.wikipedia.org/wiki/Neural_networks" rel="noopener ugc nofollow" target="_blank">神经网络</a>中的<a class="ae mf" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank">过拟合</a>。这是用神经网络进行模型平均的一种非常有效的方法。<a class="ae mf" href="https://en.wikipedia.org/wiki/Dropout_(neural_networks)#cite_note-MyUser_Arxiv.org_July_26_2015c-2" rel="noopener ugc nofollow" target="_blank">【2】</a>术语“丢失”指的是在神经网络中丢失单元(隐藏的和可见的)。<a class="ae mf" href="https://en.wikipedia.org/wiki/Dropout_(neural_networks)#cite_note-MyUser_Jmlr.org_July_26_2015c-3" rel="noopener ugc nofollow" target="_blank">【3】</a><a class="ae mf" href="https://en.wikipedia.org/wiki/Dropout_(neural_networks)#cite_note-4" rel="noopener ugc nofollow" target="_blank">【4】</a></p><h2 id="ebdf" class="mu lf hi bd lg mv mw mx lk my mz na lo jw nb nc lq ka nd ne ls ke nf ng lu nh bi translated">数据扩充</h2><p id="93b9" class="pw-post-body-paragraph jn jo hi jp b jq lw ij js jt lx im jv jw ly jy jz ka lz kc kd ke ma kg kh ki hb bi translated">减少过度拟合的最简单方法是增加训练数据的大小。在机器学习中，我们无法增加训练数据的大小，因为标记的数据太昂贵了。</p><p id="1efe" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">但是，现在让我们考虑我们正在处理的图像。在这种情况下，有几种方法可以增加训练数据的大小-旋转图像、翻转、缩放、移动等。在下图中，手写数字数据集已经进行了一些转换。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nk"><img src="../Images/5d4b83d77c2c514271cdff7ad2d0e34a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/0*8u7siNLGHTlSFqAk"/></div></figure><h1 id="c809" class="le lf hi bd lg lh li lj lk ll lm ln lo io lp ip lq ir lr is ls iu lt iv lu lv bi translated">提前停止</h1><p id="c5f7" class="pw-post-body-paragraph jn jo hi jp b jq lw ij js jt lx im jv jw ly jy jz ka lz kc kd ke ma kg kh ki hb bi translated">早期停止是一种交叉验证策略，我们将训练集的一部分作为验证集。当我们看到验证集上的性能越来越差时，我们会立即停止对模型的训练。这就是所谓的提前停止。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nl"><img src="../Images/48b1c589f6a2505554c836ba2f2594e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/0*9aqGRF9UR9OYXCxl"/></div></figure><h1 id="1edd" class="le lf hi bd lg lh li lj lk ll lm ln lo io lp ip lq ir lr is ls iu lt iv lu lv bi translated">代码片段</h1><p id="3689" class="pw-post-body-paragraph jn jo hi jp b jq lw ij js jt lx im jv jw ly jy jz ka lz kc kd ke ma kg kh ki hb bi translated">既然我们知道了一些理论，让我们从一组数据开始。我们正在考虑的数据集是疟疾细胞图像数据。<a class="ae mf" href="https://www.kaggle.com/iarunava/cell-images-for-detecting-malaria" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/iarunava/cell-images-for-detecting-malaria</a></p><p id="2376" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">有两类图像感染疟疾和未感染疟疾的图像。</p><p id="93fd" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj"> L1和L2正规化</strong></p><p id="14f4" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在keras中，我们可以使用正则化直接将正则化应用于任何图层。我在有100个神经元和relu激活函数的密集层上应用了正则化。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nm nn l"/></div></figure><p id="c339" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">辍学</strong></p><p id="82a7" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">因此，每次迭代都有不同的节点集，这导致不同的输出集。它也可以被认为是机器学习中的一种集成技术。</p><p id="24bd" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">集成模型通常比单一模型表现更好，因为它们捕捉更多的随机性。类似地，dropout也比正常的神经网络模型表现得更好。</p><p id="fed5" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">选择应该丢弃多少节点的概率是丢弃函数的超参数。</p><p id="7d54" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">由于这些原因，当我们有一个大的神经网络结构时，为了引入更多的<strong class="jp hj">随机性，辍学通常是首选。</strong></p><p id="3fda" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在keras中，我们可以使用<strong class="jp hj"> keras层</strong>实现dropout。下面是Dropout实现。在我的神经网络架构中，在具有64个核的最后一个隐藏层之后和具有500个神经元的第一个密集层之后，我引入了0.2的丢弃率作为丢弃的概率。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nm nn l"/></div></figure><p id="a527" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">数据扩充</strong></p><p id="456a" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在keras中，我们可以使用<strong class="jp hj"> ImageDataGenerator </strong>来执行所有这些转换。它有一个很大的参数列表，你可以用它来预处理你的训练数据。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nm nn l"/></div></figure><p id="07ce" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jp hj">提前停止</strong></p><p id="1332" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在keras中，我们可以使用<strong class="jp hj">回调</strong>函数来应用提前停止。下面是它的实现代码。我已经应用了提前停止，这样如果验证错误在3个时期后没有减少，它将立即停止。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nm nn l"/></div></figure><p id="13db" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">结合所有这些元素</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="nm nn l"/></div></figure><p id="9d42" class="pw-post-body-paragraph jn jo hi jp b jq jr ij js jt ju im jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们得到的图像细胞是否含有疟疾的分类准确率约为95%+</p><h1 id="5612" class="le lf hi bd lg lh li lj lk ll lm ln lo io lp ip lq ir lr is ls iu lt iv lu lv bi translated">参考</h1><div class="no np ez fb nq nr"><a href="https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/" rel="noopener  ugc nofollow" target="_blank"><div class="ns ab dw"><div class="nt ab nu cl cj nv"><h2 class="bd hj fi z dy nw ea eb nx ed ef hh bi translated">深度学习中的正则化技术概述(带Python代码)</h2><div class="ny l"><h3 class="bd b fi z dy nw ea eb nx ed ef dx translated">简介数据科学专业人员面临的最常见问题之一是避免过度拟合。你来了吗…</h3></div><div class="nz l"><p class="bd b fp z dy nw ea eb nx ed ef dx translated">www.analyticsvidhya.com</p></div></div><div class="oa l"><div class="ob l oc od oe oa of jh nr"/></div></div></a></div></div></div>    
</body>
</html>