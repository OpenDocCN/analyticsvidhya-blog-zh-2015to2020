<html>
<head>
<title>Question and Answering System Using Bidirectional Attention Flow on SQuAD dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于SQuAD数据集的双向注意力流问答系统</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/question-and-answering-system-using-bidirectional-attention-flow-on-squad-dataset-ff88b454af14?source=collection_archive---------10-----------------------#2020-06-21">https://medium.com/analytics-vidhya/question-and-answering-system-using-bidirectional-attention-flow-on-squad-dataset-ff88b454af14?source=collection_archive---------10-----------------------#2020-06-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/4f12342657ac1230c05eae00353c4e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*-yJLG4VLlY7FiSj0ZIXbFg.png"/></div></div></figure><h1 id="5216" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">业务问题</strong></h1><p id="89ef" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">问题回答是自然语言处理领域中的一项任务，它涉及建立自动回答人类提出的问题的系统。</p><p id="6943" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们向系统提出一个问题以及包含该问题答案的摘录/文本片段，并且我们期望系统精确地提取作为该问题答案的那部分文本(也称为答案范围)。</p><blockquote class="kr ks kt"><p id="2064" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">举例:</strong></p></blockquote><p id="e7f7" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">语境:</strong>尼古拉·特斯拉(塞尔维亚西里尔文:николатесла；1856年7月10日-1943年1月7日)是一位塞尔维亚裔美国发明家、电气工程师、机械工程师、物理学家和未来学家，他最著名的贡献是设计了现代交流供电系统。</p><p id="4808" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">问题:尼古拉·特斯拉是什么种族？</p><p id="d7d7" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">回答:</strong>塞尔维亚语</p><blockquote class="kr ks kt"><p id="f4ae" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">应用:</strong></p></blockquote><p id="e5d9" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">然后，这可以扩展到其他应用程序，在这些应用程序中，您有大量数据需要回答一些问题。你可以简单地把数据输入系统，然后问一些问题，系统会为你解答。</p><p id="e7eb" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">推而广之，你也可以从网上刮下知识库(上下文)来回答你的问题。</p><p id="681c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">应用是多种多样的。我们每天都在寻找问题的答案。下面我将列举其中的几个:</p><ul class=""><li id="8e5f" class="ky kz hi jq b jr km jv kn jz la kd lb kh lc kl ld le lf lg bi translated"><strong class="jq hj">医疗</strong>:一个病人的所有医疗信息都可以汇编成一大段文字。然后，医生可以使用这种方法，他们可以简单地问一个问题(对虚拟助理类型的设置)，并且医生可以获得关于所请求的确切信息的相关细节。</li><li id="76f8" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated"><strong class="jq hj">合同</strong>:我们通常会忽略合同条款、申请等，但可能会有一些简单的问题。这个系统可以快速浏览大量的条款和条件，并为我们的查询提供答案。</li><li id="3515" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated"><strong class="jq hj">学术</strong>:这个区域广阔。学生可以使用这种设置来快速获得任何和所有与他感兴趣的主题相关的问题的答案(给定对材料的访问)。</li><li id="5257" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">还有更多…</li></ul><h1 id="88af" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">机器学习/深度学习的使用</strong></h1><blockquote class="kr ks kt"><p id="dcf9" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">机器学习类型问题</strong>:</p></blockquote><p id="d380" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">多类分类</p><p id="383c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">多输入多输出</p><p id="f4aa" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">直觉</strong>:上下文将有n个单词(n类)，我们必须确定答案范围的开始和结束单词。这可以被视为预测n个类别中的一个类别以获得答案范围的开始和结束单词。</p><p id="f3f3" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们正在进行两种不同的预测，一种是对开始字的预测，另一种是对结束字的预测。问题/模型是多输入(问题、上下文)和多输出(开始字、结束字)的问题/模型。</p><blockquote class="kr ks kt"><p id="d0b1" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated">E <strong class="jq hj">误差指标</strong>:</p></blockquote><p id="11f6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">F1分数</p><p id="b179" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">直觉</strong>:</p><p id="55f0" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">精度:在被确定为正的数字类中，有多少实际上是正的。</p><p id="d350" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">回忆:在实际为正的类的数量中，有多少被我们的模型正确分类。</p><p id="1fd2" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">直观地想一想:</p><p id="f8dd" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">对于上下文中作为答案一部分的单词，我们必须确定模型实际上提取了多少单词(回忆)，对于模型提取的单词，我们必须确定有多少单词实际上是答案的一部分(精确度)。</p><p id="36fd" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这有助于确定模型从给定问题的上下文中获取答案的能力。结合这两个指标(精确度和召回率)的度量是F1分数。</p><blockquote class="kr ks kt"><p id="9092" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated">B <strong class="jq hj">业务约束</strong>:</p></blockquote><ul class=""><li id="f77c" class="ky kz hi jq b jr km jv kn jz la kd lb kh lc kl ld le lf lg bi translated">在给定问题和上下文(包含答案的文本)的情况下，提供答案所需的时间应该非常短，通常在几毫秒的范围内。没有人会等上几秒或几分钟。相反，他们会简单地自己阅读文章/上下文来得到答案。</li><li id="e609" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">除了确切答案之外，答案范围不应包含额外的填充词。</li></ul><h1 id="515f" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">数据</strong></h1><p id="d769" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">该模型的数据来自斯坦福问答数据集。</p><p id="d679" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">有两个版本:</p><ol class=""><li id="9bf1" class="ky kz hi jq b jr km jv kn jz la kd lb kh lc kl lm le lf lg bi translated">包含有问题、上下文和答案的记录。</li><li id="47bc" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl lm le lf lg bi translated">除了包含问题、上下文和答案的记录之外，它还具有包含不可回答的问题但在上下文中有似是而非的错误答案的记录。这是为了迷惑系统。</li></ol><p id="872f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这里，我正在处理问题陈述1。</p><p id="5acf" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">有关数据集的更多信息，请访问:</p><div class="ln lo ez fb lp lq"><a href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener  ugc nofollow" target="_blank"><div class="lr ab dw"><div class="ls ab lt cl cj lu"><h2 class="bd hj fi z dy lv ea eb lw ed ef hh bi translated">斯坦福问答数据集</h2><div class="lx l"><h3 class="bd b fi z dy lv ea eb lw ed ef dx translated">斯坦福问题回答数据集(SQuAD)是一个新的阅读理解数据集，由…</h3></div><div class="ly l"><p class="bd b fp z dy lv ea eb lw ed ef dx translated">rajpurkar.github.io</p></div></div><div class="lz l"><div class="ma l mb mc md lz me io lq"/></div></div></a></div><blockquote class="kr ks kt"><p id="57d5" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">数据概述:</strong></p></blockquote><p id="0f44" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">数据集以JSON文件的形式提供。</p><p id="e9a6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我用一小段代码将数据集中的信息提取到熊猫数据帧中。</p><p id="ac57" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">被考虑用于建模的最终数据帧仅包括3列:</p><ul class=""><li id="952f" class="ky kz hi jq b jr km jv kn jz la kd lb kh lc kl ld le lf lg bi translated">问题</li><li id="2c4e" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">语境</li><li id="8fed" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">回答</li></ul><h1 id="d1a2" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">现有方法</strong></h1><p id="6564" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">解决这个问题的最有影响力的方法如下:</p><ul class=""><li id="3e3a" class="ky kz hi jq b jr km jv kn jz la kd lb kh lc kl ld le lf lg bi translated">由收集小队数据集的团队构建的初始逻辑回归模型。</li><li id="48ea" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">双向注意流模型。</li><li id="6433" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">基于BERT的模型。</li></ul><p id="c91c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这些方法有许多变体/副产品。但到目前为止，这些都是最有影响力的模型，以伯特为首。</p><p id="50e8" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我用过BiDAF(双向注意力流模型)模型。</p><h1 id="2f3a" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">探索性数据分析</h1><p id="9248" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">我对问题和上下文的句子/单词/字符长度进行了一些数据分析，这将有助于标记和填充输入。</p><blockquote class="kr ks kt"><p id="13e4" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">句子长度</strong></p></blockquote><pre class="mf mg mh mi fd mj mk ml mm aw mn bi"><span id="bf3a" class="mo ir hi mk b fi mp mq l mr ms">Min:  1<br/>Max:  5<br/>Mean:  1.0026952004699323<br/>Median:  1.0<br/>25th percentile:  1.0<br/>50th percentile:  1.0<br/>75th percentile:  1.0<br/>95th percentile:  1.0<br/>99th percentile:  1.0</span></pre><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/4802a40b4c8421174c2769334624baf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mmgUUa9R-uEGrnu85IRgBA.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">问题句子长度的PDF和方框图|图片由作者提供</figcaption></figure><p id="880b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">如你所见，几乎所有的问题都是句子长度为1的，这是意料之中的。你通常用一句话问一个问题。</p><pre class="mf mg mh mi fd mj mk ml mm aw mn bi"><span id="abb0" class="mo ir hi mk b fi mp mq l mr ms">Min:  1<br/>Max:  27<br/>Mean:  5.105435320947697<br/>Median:  5.0<br/>25th percentile:  4.0<br/>50th percentile:  5.0<br/>75th percentile:  6.0<br/>95th percentile:  9.0<br/>99th percentile:  13.0</span></pre><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es my"><img src="../Images/28bf60e6649b4331e2c167a8cc219c5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SMWvdiRLCqJqKZ1mX8DFzw.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">上下文句子长度的PDF和方框图|图片由作者提供</figcaption></figure><p id="fc1f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">上下文中的句子数量从1到最多27不等。但是大多数上下文(99%)的句子长度只有13个。</p><blockquote class="kr ks kt"><p id="c190" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">字长</strong></p></blockquote><pre class="mf mg mh mi fd mj mk ml mm aw mn bi"><span id="7798" class="mo ir hi mk b fi mp mq l mr ms">Min:  1<br/>Max:  60<br/>Mean:  11.291853353451353<br/>Median:  11.0<br/>25th percentile:  9.0<br/>50th percentile:  11.0<br/>75th percentile:  13.0<br/>95th percentile:  18.0<br/>99th percentile:  23.0</span></pre><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mz"><img src="../Images/8a697497707c440778c60512733fb7e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AhxiSY1rV2ymyr9xOaJJOw.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">问题单词长度的PDF和方框图|图片由作者提供</figcaption></figure><p id="f258" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">任何问题中的单词数都在1到60之间(极端的，可能的异常值)。大多数问题由不到30个单词组成。</p><pre class="mf mg mh mi fd mj mk ml mm aw mn bi"><span id="f52c" class="mo ir hi mk b fi mp mq l mr ms">Min:  22<br/>Max:  766<br/>Mean:  137.8945186072494<br/>Median:  127.0<br/>25th percentile:  102.0<br/>50th percentile:  127.0<br/>75th percentile:  164.0<br/>95th percentile:  245.0<br/>99th percentile:  325.0</span></pre><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/4b16aa59ac56fd8b6320797a40f032ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BiJgHHiD-XrnSoIps2vhyw.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">上下文单词长度的PDF和方框图|图片由作者提供</figcaption></figure><p id="b07e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">上下文中的单词数在325的22到99%之间，这意味着99%的上下文由少于325个单词组成。</p><blockquote class="kr ks kt"><p id="1c74" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">字符长度</strong></p></blockquote><pre class="mf mg mh mi fd mj mk ml mm aw mn bi"><span id="8a0d" class="mo ir hi mk b fi mp mq l mr ms">Min:  1<br/>Max:  37<br/>Mean:  4.606716632191931<br/>Median:  4.0<br/>25th percentile:  2.0<br/>50th percentile:  4.0<br/>75th percentile:  7.0<br/>95th percentile:  10.0<br/>99th percentile:  13.0<br/>99th percentile:  16.0</span></pre><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/53e13e07ab303d5f2996e507e89e9fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DwOwYjcpLzIBHU7a4ju0mg.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">单词字符长度的PDF和方框图|作者图片</figcaption></figure><p id="c9a7" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">出现在问题和上下文中的单词的字符长度的分布如上所述。它的范围从最常用的英文单词1、2和3(如I、is、the)到16的第99.9百分位(更复杂/更罕见的单词)。</p><p id="309e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">继续进行必要的预处理。让我们从基础开始。</p><h1 id="e10d" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">数据预处理:</strong></h1><p id="a01d" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">这个问题由字符串形式的输入(问题和上下文)组成。</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es nc"><img src="../Images/2de0c77179e7b716998ec3cadb9c38f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*lgZ_DBxOOcLXpDR2Le6dnw.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">输入|作者图片</figcaption></figure><p id="171b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">但是机器学习只处理数字。所以我们必须通过标记化将这些字符串转换成数字。</p><p id="ce62" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们在这里使用两种不同的标记化。单词级标记化和字符级标记化。至于我们为什么使用两种不同的标记化将在后面解释。</p><blockquote class="kr ks kt"><p id="d3ed" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">单词级标记化</strong></p></blockquote><p id="7503" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">有许多单词分词器，它们之间的功能有所不同。这里，我们使用PTBTokenizer(nltk . word _ token ize)。</p><p id="63e5" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这将输入字符串(问题和上下文)分割成单词和标点符号列表。</p><p id="3b2f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">然后，我们将每个唯一的单词/标点符号编码成一个唯一的数字(令牌)。并且这个数字到单词/标点的映射被保存在字典中。</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/ebdc2102923da392cbb44d53fd6a4800.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*SVCj9btBFAVvT_8Zh4OkxQ.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">单词分词器代码|作者图片</figcaption></figure><p id="42de" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">此外，对于每个记录，我们找出包含答案范围的开始和结束单词的单词级标记的标记索引。</p><blockquote class="kr ks kt"><p id="7ddd" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">字符级标记化</strong></p></blockquote><p id="b7ea" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这也用数字列表对字符串进行编码，但是是在字符级别。</p><p id="ad33" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">为此，我使用了Keras内置的标记器。</p><p id="8ec1" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这里，所有的标记和它们各自的字符都保存在一个字典中。</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/ddb391750468def682be917d25c872e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*5qsSIub8N9cxHT6gXQ8icw.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">字符标记器代码|作者图片</figcaption></figure><p id="4f64" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">经过所有这些处理后，我们的数据帧如下:</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/45bf0e279c2502139bb1f38ba94d8a09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*ScgUw2FAqnuMhj32jM8H1A.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">输入令牌|作者图片</figcaption></figure><blockquote class="kr ks kt"><p id="161c" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">填充</strong></p></blockquote><p id="b360" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">然后我们做一些分析，找出每个问题/上下文中有多少个单词，每个单词有多少个字符。</p><p id="dfbd" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这用于确定对任何问题/上下文进行编码所需的最大字数，以及对任何单词进行编码所需的最大字符数。</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/4ffcc189ba32398436c1ce2e305b2399.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*mV788JYONqi9TYgKmk8QWQ.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">问题字数|作者图片</figcaption></figure><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es nh"><img src="../Images/9ca4355d350a6ca1bea0986a95886696.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*Ape73BcHfgWdXNcVg30RKQ.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">上下文字数|作者图片</figcaption></figure><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/acadb8a19d234415387db40580c34a8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*PnP7RiuzK5NiWAJEiD6KJw.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">文字字符数|作者图片</figcaption></figure><p id="1615" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">因此，我们将每个问题编码为最多32(J)个单词标记，如果问题短于32个单词，则填充0(的单词标记)，如果问题长于32个单词，则截断。类似地，上下文的最大令牌数是340(T)。</p><p id="36cd" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们用来编码每个单词的最大字符数是15(K ),因为99.9%的单词接近这些字符。长于此长度的单词的字符将被截断，短于此长度的单词将用0填充(字符标记为)。</p><p id="722a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">单词和字符级标记化问题和上下文的填充序列最终如下:</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/325fd52a0ce97061010051dd2065dffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*tfN6iW8ur1vnpGBcy8Y1Kg.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">填充序列|作者图片</figcaption></figure><blockquote class="kr ks kt"><p id="cd6e" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">最终输入输出</strong></p></blockquote><p id="561f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">上述四个字段(问题和上下文的单词级和字符级标记)将作为模型的输入。</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es nk"><img src="../Images/be507224a08b43a0d57391cfa97aebd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*KvVJc0djP4NESh3MZHlbCw.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">开始和结束标记|作者图片</figcaption></figure><p id="73db" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">模型的输出将仅仅是答案范围的开始和结束指数(或者更具体地，它们的概率分布)。</p><p id="1eba" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">更具体地说，如果我们使用稀疏分类交叉熵损失，我们将使用这些指数作为输出。</p><p id="946b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">如果我们使用CategoricalCrossEntropy loss，我们将使用这些索引的一个热编码矢量(340维)作为输出。</p><p id="a4b6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在我的模型中，我使用CategoricalCrossEntropy作为损失，因此对输出开始和结束索引进行了热编码。</p><p id="05cf" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在我们已经有了模型的输入和输出，让我们继续讨论模型本身。</p><h1 id="0852" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">型号</strong></h1><p id="c085" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">我很大程度上要归功于下面提到的博客。</p><div class="ln lo ez fb lp lq"><a href="https://towardsdatascience.com/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b" rel="noopener follow" target="_blank"><div class="lr ab dw"><div class="ls ab lt cl cj lu"><h2 class="bd hj fi z dy lv ea eb lw ed ef hh bi translated">双向注意力流动图解指南</h2><div class="lx l"><h3 class="bd b fi z dy lv ea eb lw ed ef dx translated">这篇文章展示了BiDAF的工作原理，BiDAF是一个NLP模型，它在问答中拓展了范围…</h3></div><div class="ly l"><p class="bd b fp z dy lv ea eb lw ed ef dx translated">towardsdatascience.com</p></div></div><div class="lz l"><div class="nl l mb mc md lz me io lq"/></div></div></a></div><p id="637a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这是一本精彩的读物。</p><p id="b951" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这个模型可能看起来很复杂。但我会尽可能用最直观最简单的方式来解释。</p><p id="090c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">模型中有多个层，每个层都有自己的功能。我们会一个一个来。</p><ul class=""><li id="8402" class="ky kz hi jq b jr km jv kn jz la kd lb kh lc kl ld le lf lg bi translated">嵌入层(单词和字符)</li><li id="caf7" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">公路层</li><li id="4cda" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">上下文层</li><li id="91bb" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">注意层</li><li id="fc2e" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">建模层</li><li id="bfd6" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">输出图层</li></ul><p id="fcdb" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">注意:我已经使用单独的自定义类实现了每一层。</p><blockquote class="kr ks kt"><p id="8ae1" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">字嵌入层</strong></p></blockquote><p id="5954" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这一层将单词级标记序列转换成向量序列。</p><p id="e6a2" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">向量是预训练的手套字向量。他们在各种数据上接受训练(<a class="ae nm" href="http://dumps.wikimedia.org/enwiki/20140102/" rel="noopener ugc nofollow" target="_blank">维基百科2014 </a> + <a class="ae nm" href="https://catalog.ldc.upenn.edu/LDC2011T07" rel="noopener ugc nofollow" target="_blank"> Gigaword 5 </a>、普通抓取、Twitter)。此外，单词向量有各种尺寸(50、100、200和300)。</p><p id="40a1" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">对于这个模型，我使用了在维基百科+ Gigaword5上训练的100维单词向量。</p><p id="f218" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">你可以说问题/上下文中的每个单词/单词都被转换成了一个d(在本例中为100)维向量。</p><p id="7b82" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输入</strong>:单词记号的T，J维向量。</p><p id="ad59" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输出</strong> : dXT，dXJ维矩阵。</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nn"><img src="../Images/affb6ab37b8d90f90bfd51b39b41516d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_pSWtXkSY0kY1cqJ93V93A.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">文字嵌入层|作者图片</figcaption></figure><blockquote class="kr ks kt"><p id="7c0f" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">字符嵌入层</strong></p></blockquote><p id="4db0" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这一层将字符级标记序列转换为向量序列。</p><p id="4c4c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这里，对应于每个字符的向量是相对于字符数的随机向量或一次向量。我用过onehot vectors。</p><p id="c3cd" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">因此，每个单词的每个字符都被转换成c+1(对于<unk>为+1，即数据集中没有遇到的任何未知字符)维向量(假设向量是一次性编码的)，其中c是数据集中唯一字符的数量。</unk></p><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es no"><img src="../Images/2de8c2e1b5253489fe225e164c342f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tb-OLIH71CoLXvxsT_YC0w.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">字符嵌入|作者图片</figcaption></figure><p id="22ae" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输入</strong>:字符记号的KXT，KXJ矩阵(可以说是列表的列表)。</p><p id="706b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输出</strong> : KXTX(c+1)，KXJX(c+1)维矩阵。</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es np"><img src="../Images/2e7e059fc805785c6f4d73d063037dd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EXg2c6y5oyHD1MlhbtIc6w.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">字符嵌入层|作者图片</figcaption></figure><p id="bfb3" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在，为什么我们首先需要字符级嵌入？单词级嵌入还不够吗？</p><p id="8e7a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">理想情况下，如果预训练的手套向量包含我们数据集中所有唯一单词的向量表示，那么很可能。</p><p id="d211" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">但是，通常数据集中的所有单词都不会被手套向量覆盖。为了涵盖这种词汇表外单词的表示，我们使用字符级嵌入。</p><p id="2950" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在我们的例子中，在维基百科2014 + Gigaword5上训练的100维手套向量覆盖的单词百分比是:</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es nq"><img src="../Images/1bec4e653647390ab474445d079875e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*35lSol-LQ_MfvvAw-JzeHw.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">覆盖单词的百分比|作者图片</figcaption></figure><p id="a01b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">使用不同版本的手套向量可以提高这个数字。</p><p id="4caa" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">但是仅有字符级嵌入是不够的。它们只是代表一个单词的不同字符。但是要使用这样的字符嵌入获得单词级的表示，我们需要一个字符CNN。</p><blockquote class="kr ks kt"><p id="532e" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">人物CNN图层</strong></p></blockquote><p id="a9e9" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">字符嵌入层的输出通过字符CNN来获得单词级表示。</p><p id="4461" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">更具体地说，输入的每个时间步(单词)都通过字符CNN层。</p><p id="518e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">字符CNN层只不过是一个一维卷积神经网络。</p><p id="d932" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">让我们假设我们的1D卷积层有f个核/滤波器，每个都具有任意宽度w(这不必是常数，不同的核可以具有不同的宽度)和高度=输入的高度(这是常数，因为它是1D卷积，并且将等于输入的高度，即c+1)。</p><p id="4cd2" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">因此，对于每个滤波器，输入中的每个KX(c+1)矩阵将被变换为K-w+1维向量。</p><p id="673b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">然后，我们对此进行最大化，为这些向量中的每一个向量获取1个值。将此扩展到f过滤器，我们得到一个f维向量。</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nr"><img src="../Images/799d5d6f90a452608a864d652c8c1f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v_PBpEe9H6Xbc0izJkCxdA.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">人物CNN实现|作者图片</figcaption></figure><p id="be6a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">因此，最终在卷积和最大池化之后，每个KX(c+1)矩阵被转换成f维向量。</p><p id="2d0f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">为简单起见，我们固定f = d = 100</p><p id="b90b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输入</strong> : KXTX(c+1)，KXJX(c+1)维矩阵。</p><p id="c644" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输出</strong> : dXT，dXJ维矩阵。</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es np"><img src="../Images/8603570c14c6bb81ea53434355e16d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sQamKrGxz9qKFuPwUU-Xbg.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">人物CNN图层|作者图片</figcaption></figure><p id="e529" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在，我们有了每个单词的单词手套表示和每个单词的字符级表示。</p><p id="0160" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">接下来，我们将它们连接起来，得到分别代表上下文和问题的2dXT，2dXJ维向量。这些信息将作为输入输入到公路图层。</p><blockquote class="kr ks kt"><p id="f589" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated">H <strong class="jq hj">高速公路层</strong></p></blockquote><p id="14ba" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">高速公路网络的作用是调整单词嵌入和字符嵌入步骤的相对贡献。</p><p id="92e8" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">逻辑是，如果我们正在处理词汇之外的单词，我们会希望增加单词的1D-CNN表示的相对重要性，因为我们知道它的手套表示很可能是一些随机的胡言乱语或只是零。另一方面，当我们处理常见的和明确的英语单词时，我们可能希望从GloVe和1D-CNN获得更多的同等贡献。</p><p id="9908" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">简单地说，高速公路层只是一个有微小变化的前馈神经网络。</p><p id="ebee" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在给予网络的所有输入中，只有一小部分经历了权重相乘、偏置相加和激活。输入的剩余部分未经转换。这些分数的比率由变换门(t)和进位门(1-t)管理</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ns"><img src="../Images/24720dd4bf38ea2c5f384df654d0b5ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RiBr-HBdYTUIU25Vb58YfQ.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">高速公路图层实施|作者图片</figcaption></figure><p id="b597" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输入</strong> : 2dXT，2dXJ维矩阵</p><p id="1485" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输出</strong> : 2dXT，2dXJ维矩阵</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es nt"><img src="../Images/321ed4eae9f1e968fa53a691944d3193.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*XOLnoekK7Pw_PCt6V7jbmQ.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">公路图层|作者图片</figcaption></figure><p id="6c0e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在我们有了问题和上下文中每个单词的单词表示。但是它们没有上下文信息。即来自他们周围的信息。</p><blockquote class="kr ks kt"><p id="0661" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">上下文层</strong></p></blockquote><p id="7665" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在英语或任何语言中，我们可以有把握地说，单词的意思也是由它周围的单词(它的上下文)决定的。</p><p id="ee6d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">例如，银行可能是一个金融机构，也可能是一条河的两岸。这个词的实际意思可能是由它前面或后面的词表达出来的。</p><p id="9be0" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">因此，我们的下一个任务是获得每个单词的上下文表示。</p><p id="76e4" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这一层正是这样做的。它获得每个单词的上下文表示，即关于单词本身的信息和关于它周围单词的可能信息(上下文)。</p><p id="cdc1" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">处理时间序列数据(我们的问题和背景是时间序列数据)的模型类型是递归神经网络家族(RNN、LSTM和GRU)。</p><p id="2594" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">LSTM/GRU是一种可以记忆长期依赖关系的神经网络架构。当我们将输入序列(例如文本串)输入到正常的正向LSTM/GRU层中时，每个时间步长的输出序列将对来自该时间步长以及过去时间步长的信息进行编码。换句话说，每个单词的输出嵌入将包含来自之前单词的上下文信息。</p><p id="305c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在这里，我使用了GRUs。还有，单词的意义是由它两边的单词决定的。为此，我们使用双向GRU。</p><p id="9f0a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我将GRU的单元/隐藏单元/输出尺寸的数量设置为d(为了简单起见)。因此，双向GRU的输出数量是2d。</p><p id="f79b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输入</strong> : 2dXT，2dXJ维矩阵</p><p id="e673" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输出</strong> : 2dXT(姑且称这个矩阵H)，2dXJ维矩阵(姑且称这个矩阵U)</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nu"><img src="../Images/23745dc057e9c9ddaa01c2b9ac35cef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A9Y7i9KGZvvGtzkUt60gyQ.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">上下文嵌入层|作者图片</figcaption></figure><blockquote class="kr ks kt"><p id="11d1" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">关注层</strong></p></blockquote><p id="0cd3" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这些注意力层构成了这个模型的核心。</p><p id="93c9" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">注意力本身是一个巨大的概念。我试着用一个例子来解释一下——机器翻译。</p><p id="4e20" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">最简单的机器翻译模型由编码器解码器架构组成。</p><p id="c09a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">编码器读取源语言中的输入序列(通常是一个句子),并将其转换为向量。解码器使用这个向量来生成翻译语言的句子。</p><p id="7284" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">直觉地想到这一点。如果你是翻译，你必须阅读需要翻译的内容，记住它，然后翻译成另一种语言。短句就可以了。但是对于较长的文本，这几乎是不可能的。想象一下这样翻译一整本书。你也许能翻译后记。仅此而已。当你读完的时候，你可能已经忘记了这本书的大部分内容。</p><p id="877e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在让我们思考一下在现实生活中这个任务是如何完成的。译者阅读文本的一部分，翻译它。返回，阅读文本的下一部分，翻译它，等等。</p><p id="eec2" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">换句话说，在翻译过程中，我们只关注原文的相关部分。这就是注意力。</p><p id="1cbb" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">关于注意力在这类翻译器中是如何实现的，我就不赘述了。</p><p id="9651" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">将这一概念扩展到我们的任务，我们将集中在与问题相关的上下文中的那些单词和与上下文相关的那些正在讨论的单词上。这就是为什么它被称为双向注意力流。注意力是双向流动的，从问题到背景，从背景到问题。</p><p id="2338" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们在注意力层中有3个不同的步骤:</p><p id="0086" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">1.相似矩阵的形成</p><p id="e38b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">2.要查询(问题)注意的上下文</p><p id="0a2b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">3.质疑(问题)对语境的关注</p><p id="893f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> <em class="ku">相似度矩阵的形成</em> </strong></p><p id="3245" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">相似性矩阵是捕获上下文和查询中的单词之间的关系的矩阵。</p><p id="9cbe" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">很明显，这是一个TXJ矩阵。</p><p id="863f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">仅仅因为上下文单词和查询单词是相同的，并不一定意味着它们的向量表示非常相似。这是因为这些向量表示也包含了来自周围短语的信息。</p><p id="ed09" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这个TXJ矩阵的每个元素都是由它的代表字向量以下列方式连接而成的:</p><p id="659e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">S(t，j)= W .[h；u；h*u]</p><p id="a284" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">其中H，U是矩阵H，U *的列，表示逐元素乘法；表示串联，。是点积，W是6d可训练权重向量。</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nv"><img src="../Images/2cedf38b9f6436d1664cd93efd49f4fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TORi2e6lmHFjhVxFot2wBA.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">相似矩阵的形成|作者的图像</figcaption></figure><p id="74d6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">S(t，j)的值表示对应于第t列的上下文中的单词与对应于第j列的查询中的单词有多接近。</p><p id="8dd8" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输入</strong> : 2dXT，2dXJ维矩阵(H，U)</p><p id="a683" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输出</strong> : TXJ维矩阵</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es nw"><img src="../Images/c3d3eb6b51ce03d17a06a0597a393b76.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*Rn3-4rjUAwDKLQCRnwoJFQ.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">相似矩阵|作者图片</figcaption></figure><p id="3d46" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> <em class="ku">上下文查询关注</em> </strong></p><p id="e8ce" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这一步的目标是找到哪些查询单词与每个上下文单词最相关。</p><p id="ab61" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们将具有维度TXJ的相似性矩阵作为输入，并且沿着上下文的维度取一个softmax。</p><p id="6a31" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">然后，我们用一个矩阵来表示哪些查询词与每个上下文词最相关。</p><p id="d961" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">然后我们取这个矩阵的每一行1XJ，乘以矩阵U(2dXJ)并对列求和(有点像标量乘法和加权和)。</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nx"><img src="../Images/15ec4fd6d75ccc168a367c646096b598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j9ygaW19FO8QWWjOU2NWsw.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">查询关注度的上下文|按作者排序的图片</figcaption></figure><p id="6e95" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">对一行这样做将得到一个2dX1向量。因此，在T行中，我们得到一个2dXT矩阵作为结果。</p><p id="9785" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">结果矩阵(u `)封装了关于每个查询单词与每个上下文单词的相关性的信息。</p><p id="9359" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输入</strong> : TXJ维矩阵</p><p id="f59e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输出</strong> : 2dXT维矩阵(u `)</p><p id="075d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> <em class="ku">查询到上下文关注</em> </strong></p><p id="c5e2" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这里，我们的目标是找到哪些上下文单词与查询单词最相似，因此对于回答查询是至关重要的。</p><p id="66fc" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们将具有维度TXJ的相似性矩阵作为输入，并且沿着查询的维度取最大值。我们得到一个TX1向量。然后我们在这个向量上应用softmax。</p><p id="2ce8" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">然后，我们将这个向量TX1与矩阵H(2dXT)的列相乘，并对这些列求和(有点像标量乘法和加权求和)。</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nx"><img src="../Images/bc6cfa1695f2cd560465b22cbb0ba569.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BgFvF9brMaNUQhCAZjbBLw.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">对上下文关注|作者图片的查询</figcaption></figure><p id="c6c5" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这样做可以得到一个2dX1向量。我们简单地将它平铺/复制T次。我们得到一个2dXT矩阵作为结果。</p><p id="1ed9" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这个矩阵(h `)封装了关于查询的上下文中最重要的单词的信息。</p><p id="7ae5" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输入</strong> : TXJ维矩阵</p><p id="4447" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输出</strong> : 2dXT维矩阵(h `)</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es ny"><img src="../Images/d4bd2f6c056c4ed1a817e35e9d3a8dbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*XOv5oHIZ2D_4H7rnKtuGbw.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">双向注意力|作者的图像</figcaption></figure><p id="84db" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">综上所述:</p><p id="3384" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">h:在上下文中有语义、句法和语境意义的词。</p><p id="d27a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">u `:封装每个查询词与每个上下文词的相关性。</p><p id="a80c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">h `:封装关于查询的上下文中最重要的单词的信息</p><p id="8aa9" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们现在合并所有3个矩阵H(2dXT)，U`(2dXT)和H`(2dXT ),以下列方式形成矩阵G:</p><p id="d92a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">g =[h；uh * uh*h`]</p><p id="b6c9" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">其中H，u `，H `是H，u `和H `的列，*表示逐元素乘法；表示串联。</p><p id="3bde" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在，我们可以说G(8dXT)的每个列向量是上下文单词的表示，该上下文单词完全知道其含义、上下文含义，知道查询及其与查询的相关性。</p><blockquote class="kr ks kt"><p id="fe47" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">造型层</strong></p></blockquote><p id="1497" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">建模层只是一系列的两个双向gru，它使用信息丰富的矩阵G来决定哪些单词需要成为答案的一部分。</p><p id="67dd" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">第一建模层的输出是M1(2dXT)，第二建模层的输出是M2(2dXT)。</p><p id="c7e6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输入</strong> : 8dXT维矩阵(G)</p><p id="ff8e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输出</strong> : 2dXT，2dXT维矩阵(M1，M2)</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nu"><img src="../Images/9fc304146dd317eabfc7995ba67c23cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sadc6JVeoWoJ9EaeVLm4kw.png"/></div></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">建模层|作者图像</figcaption></figure><blockquote class="kr ks kt"><p id="af9e" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">输出层</strong></p></blockquote><p id="7fa6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">然后，输出层用于确定答案范围的开始和结束单词。</p><p id="7ec1" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">开始概率分布计算如下</p><p id="92e2" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">start = soft max(W .[G；M1])</p><p id="3083" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">其中[G；M1]是一个(10dXT)矩阵。是点积，W是10d可训练权重向量。</p><p id="8dfb" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">最终概率分布计算如下</p><p id="901c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">end = soft max(W .[G；M2])</p><p id="7876" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">其中[G；M2]是一个(10dXT)矩阵。是点积，W是10d可训练权重向量。</p><p id="176f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输入</strong>:10个维度矩阵(GM1，GM2)</p><p id="cbf1" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">输出</strong> : 1XT，1XT矢量(开始，结束)</p><p id="5f5e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这些概率分布告诉我们上下文中的每个单词作为答案范围的开始和结束的概率。</p><p id="f96b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">为了提取精确的开始和结束索引，简单地取对应于分布的最大值的索引，因为它指示该单词作为开始/结束的概率最高。</p><p id="b98a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在，您可以从开始索引到结束索引遍历上下文标记，以获得答案范围。</p><h1 id="2c8b" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">模型的直觉:</strong></h1><p id="10d9" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">让我们考虑一个新的学年开始了，一个班级的学生(上下文的单词)聚集在一起。</p><p id="161d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">他们每个人都了解自己(单词嵌入、字符嵌入和高速公路层)。</p><p id="0cb8" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">接下来，他们相互交流，了解类中的其他人(上下文嵌入层)。</p><p id="f228" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在，来了一群来自不同系的教授，他们说他们需要一些学生来完成一项特定的任务。</p><p id="e200" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在，学生们如何知道他们中谁能完成这个任务？</p><p id="2e5d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">因此，学生们与每位教授进行一对一的会谈，以了解任务，并了解他们如何融入到画面中(注意层)。</p><p id="cca6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在班上的每个学生都知道自己与手头任务的相关性。</p><p id="4fe3" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">但是，为了决定他们是否应该成为少数几个学生中的一员，所有的学生再次相互交流(建模层)。</p><p id="e43a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在，学生对自己的相关性以及班上其他学生的相关性有了一个概念，与完成任务(回答范围)最相关/最有能力的学生站出来完成任务。</p><blockquote class="kr ks kt"><p id="33d7" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">模型结果:</strong></p></blockquote><p id="2ab3" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我使用了Nadam优化器(具有内斯特罗夫动量的adam ),并运行了12个时期的模型，最小批量大小为32，初始学习率为0.0005，双向gru和密集层的退出率为0.2。</p><p id="6ea0" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">该模型有近180万个可训练参数。</p><p id="0988" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我在Google Colab上训练模型。使用特斯拉T4图形处理器，每个纪元需要将近25分钟。总训练时间接近6小时。</p><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/3455505fd7bd9a4002d2d00b06d95ff3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*ffgi5xjdhQE6joZMfgDvig.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">训练时期|作者图片</figcaption></figure><p id="d8b1" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">该模型在验证数据上获得了0.561的平均F1分数。</p><blockquote class="kr ks kt"><p id="3e94" class="jo jp ku jq b jr km jt ju jv kn jx jy kv ko kb kc kw kp kf kg kx kq kj kk kl hb bi translated"><strong class="jq hj">样本预测:</strong></p></blockquote><figure class="mf mg mh mi fd ij er es paragraph-image"><div class="er es nf"><img src="../Images/58f202173f8a19947c1951d1f77b811a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*4F-EXvMP_9xODVc_o-DHfQ.png"/></div><figcaption class="mu mv et er es mw mx bd b be z dx translated">样本预测|作者图片</figcaption></figure><h1 id="b700" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">结论</h1><p id="e811" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">像这样的方法展示了将人类行为模仿成算法的重要性，以便我们可以将人类智能转化为人工智能。</p><h1 id="98be" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">未来工作</strong></h1><ul class=""><li id="1059" class="ky kz hi jq b jr js jv jw jz nz kd oa kh ob kl ld le lf lg bi translated">尝试手套模型中的其他单词向量。</li><li id="0386" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">尝试字符嵌入层的随机矢量，而不是一个热点矢量。</li><li id="d5d1" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">使用最近的优化器，如RADAM(修正亚当)。</li><li id="f467" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated">尝试将脸书推论向量以某种方式整合到模型中。</li></ul><h1 id="d503" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">参考文献</strong></h1><ul class=""><li id="fdfa" class="ky kz hi jq b jr js jv jw jz nz kd oa kh ob kl ld le lf lg bi translated"><a class="ae nm" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">www.appliedaicourse.com</a></li><li id="e2d1" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated"><a class="ae nm" href="https://arxiv.org/abs/1606.05250" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1606.05250</a></li><li id="b55d" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated"><a class="ae nm" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank">https://rajpurkar.github.io/SQuAD-explorer/</a></li><li id="3bcf" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated"><a class="ae nm" href="https://arxiv.org/abs/1611.01603" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1611.01603</a></li><li id="7f99" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated"><a class="ae nm" href="https://towardsdatascience.com/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b" rel="noopener" target="_blank">https://towards data science . com/the-definitive-guide-to-bi-directional-attention-flow-d 0e 96 e 9e 666 b</a></li><li id="b066" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated"><a class="ae nm" href="https://towardsdatascience.com/character-level-cnn-with-keras-50391c3adf33" rel="noopener" target="_blank">https://towards data science . com/character-level-CNN-with-keras-50391 C3 ADF 33</a></li><li id="9b38" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated"><a class="ae nm" href="https://udai.gitbook.io/practical-ml/nn/training-and-debugging-of-nn" rel="noopener ugc nofollow" target="_blank">https://udai . git book . io/practical-ml/nn/training-and-debugging-of-nn</a></li><li id="c09c" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated"><a class="ae nm" href="https://towardsdatascience.com/building-a-question-answering-system-part-1-9388aadff507" rel="noopener" target="_blank">https://towards data science . com/building-a-question-answering-system-part-1-9388 aad ff 507</a></li><li id="dddf" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated"><a class="ae nm" href="https://towardsdatascience.com/nlp-building-a-question-answering-model-ed0529a68c54" rel="noopener" target="_blank">https://towards data science . com/NLP-building-a-question-answering-model-ed 0529 a68c 54</a></li><li id="824e" class="ky kz hi jq b jr lh jv li jz lj kd lk kh ll kl ld le lf lg bi translated"><a class="ae nm" href="https://arxiv.org/pdf/1506.03134.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1506.03134.pdf</a></li></ul><h1 id="322c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> Github库</strong></h1><div class="ln lo ez fb lp lq"><a href="https://github.com/th3darkprince/BiDAF_SQUAD" rel="noopener  ugc nofollow" target="_blank"><div class="lr ab dw"><div class="ls ab lt cl cj lu"><h2 class="bd hj fi z dy lv ea eb lw ed ef hh bi translated">th3马克普林斯/BiDAF_SQUAD</h2><div class="lx l"><h3 class="bd b fi z dy lv ea eb lw ed ef dx translated">面向小队数据集的问答系统双向注意流实现</h3></div><div class="ly l"><p class="bd b fp z dy lv ea eb lw ed ef dx translated">github.com</p></div></div><div class="lz l"><div class="oc l mb mc md lz me io lq"/></div></div></a></div><h1 id="a7eb" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">链接于</strong></h1><p id="3404" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><a class="ae nm" href="http://www.linkedin.com/in/yuvarajjanarthanan" rel="noopener ugc nofollow" target="_blank">www.linkedin.com/in/yuvarajjanarthanan</a></p></div></div>    
</body>
</html>