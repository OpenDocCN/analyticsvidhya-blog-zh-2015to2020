<html>
<head>
<title>Markov Decision Processes and Bellman Equations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">马尔可夫决策过程和贝尔曼方程</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/markov-decision-processes-and-bellman-equations-724abbd0a664?source=collection_archive---------9-----------------------#2020-04-17">https://medium.com/analytics-vidhya/markov-decision-processes-and-bellman-equations-724abbd0a664?source=collection_archive---------9-----------------------#2020-04-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="eb02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<a class="ae jd" rel="noopener" href="/@theperceptiveagent/reinforcement-learning-machines-learning-by-interacting-with-the-world-64e5862dbf19">之前的帖子</a>中，我们深入到强化学习的世界，了解了该领域一些非常基本但重要的术语。今天，我想讨论如何将一个任务框架化为一个RL问题，并讨论贝尔曼方程。当试图解决RL问题时，贝尔曼方程是绝对必要的。因此，我对这个话题的写作格外小心。</p><h1 id="e913" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">RL任务的类型</h1><p id="f0e3" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">所有的RL任务都可以分为两种:<br/> <strong class="ih hj"> 1。情景任务:</strong>通过上一篇文章中学习走路的例子，我们可以看到代理必须学会自己走到目的地。当代理成功到达目的地时会发生什么？由于这就是任务的全部内容，现在代理可以再次从起始位置开始，并尝试更有效地到达目的地。这是一个阶段性任务的例子。在这样的任务中，代理环境分解成一系列情节。情节任务在数学上更容易，因为每个动作只影响随后在情节中收到的有限数量的奖励。<br/> <strong class="ih hj"> 2。继续任务:</strong>我相信读者们会熟悉像地铁冲浪者和神庙逃亡这样没完没了的跑步游戏。现在，想象一个代理试图学习玩这些游戏来最大化分数。但是，这些游戏没有尽头。这是一个持续任务的例子。另一个例子是一个代理，它必须将传入的HTTP请求分配给世界各地的各种服务器。只要服务器在线，该任务就将继续，并且可以被认为是一个持续的任务。</p><h1 id="1a4d" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">马尔可夫决策过程——未来取决于我现在做什么！</h1><p id="ce64" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在强化学习中，所有问题都可以被框定为<strong class="ih hj">马尔可夫决策过程(MDPs) </strong>。所有MDP的一个基本属性是未来状态仅依赖于当前状态。这是因为当前状态应该拥有关于过去和现在的所有信息，因此，未来只取决于当前状态。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/1c70dd33798890eeceaa54121cca488d.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*hdWfDCtCADC5iVK_JNc-lA.png"/></div></figure><p id="98cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用更专业的术语来说，未来和过去是<strong class="ih hj">有条件独立的，</strong>给定现在。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kp"><img src="../Images/30d1f5fa348cb0049ecdd5d754982f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*6aBcQqKQ_gwqiTBlSt-uDg.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">马尔可夫决策过程中典型的主体-环境相互作用。(资料来源:萨顿和巴尔托)</figcaption></figure><p id="20ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看一个MDP的例子:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ku"><img src="../Images/86f93fd3138b44ee2ce43229df798963.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cTRFObiSE4UQCroYDDp2Ng.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">有三个状态(绿色圆圈)和两个动作(橙色圆圈)以及两个奖励(橙色箭头)的简单MDP的例子。<a class="ae jd" href="https://en.wikipedia.org/wiki/Markov_decision_process" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="ae7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，有三个州:S₀、S₁和S₂，每个州有两个可能的操作:a₀和a₁.箭头上的数字代表转移概率。例如，如果代理从S₀州开始并在a₀采取行动，则代理有50%的概率到达S₂州，还有50%的概率返回S₀.州在此MDP中，在S₂拿下a₁或在S₁.拿下a₀可获得2个奖励</p><h1 id="4ed3" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">贝尔曼方程</h1><p id="2ad6" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">强化学习和动态编程中使用的值函数的一个基本属性是它们满足如下所示的递归关系:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es kz"><img src="../Images/8d7d5c815a0cf6efc0f6e64244af86b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l5reww9fPqjpFeWzJD08oA.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">价值函数的贝尔曼方程。(资料来源:萨顿和巴尔托)</figcaption></figure><p id="410c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们知道，一个状态的价值是从该状态到最终状态的总预期回报。也可以这样想:如果我们在状态<em class="la"> s </em>采取一个动作<em class="la"> a </em>并在状态<em class="la">s’，</em>结束，那么状态<em class="la"> s </em>的值就是在状态<em class="la"> s </em>采取动作<em class="la"> a </em>所获得的奖励与状态<em class="la">s’</em>的值之和。</p><p id="2b66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类似地，对于Q值:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lb"><img src="../Images/d5f50257ea204e8faf42de8a658ec3bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*if3-TMCKxjFpnlU7yhYSZg.png"/></div></figure><h2 id="8f74" class="lc jf hi bd jg ld le lf jk lg lh li jo iq lj lk js iu ll lm jw iy ln lo ka lp bi translated">贝尔曼期望方程</h2><p id="20bc" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">贝尔曼方程的这种递归更新特性便于状态值和动作值函数的更新。当代理按照策略π从一个状态前进到另一个状态时:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lq"><img src="../Images/3c092c741d7857d07758e05c9c6774d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*__ycKTK6qnlDv5ZTA4qYoQ.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">递归属性允许状态值和动作值函数从一个状态到另一个状态。(<a class="ae jd" href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#key-concepts" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><h2 id="7dc2" class="lc jf hi bd jg ld le lf jk lg lh li jo iq lj lk js iu ll lm jw iy ln lo ka lp bi translated">贝尔曼最优方程</h2><p id="da89" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">如果我们只考虑最优值，那么我们只考虑最大值，而不是通过遵循策略π获得的值。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lr"><img src="../Images/e45a05071cd88fc08a008169c08177b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*XVP3wjoumnVUZRKn1jDP8w.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated"><a class="ae jd" href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#key-concepts" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="26b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">很明显，如果代理熟悉环境的动态，找到最佳值是可能的。但是，对于大多数问题，转移概率<strong class="ih hj"> Pᵃₛₛ' </strong>和<strong class="ih hj"> R(s，a) </strong>是未知的。尽管如此，贝尔曼方程构成了许多RL算法的基础。</p><h1 id="0a86" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">动态规划—在已知环境模型的情况下寻找最佳策略</h1><p id="f902" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">如果环境的模型是已知的，<a class="ae jd" href="https://en.wikipedia.org/wiki/Dynamic_programming" rel="noopener ugc nofollow" target="_blank">动态规划</a>可以与贝尔曼方程一起使用，以获得最佳策略。这需要两个基本步骤:</p><h2 id="f543" class="lc jf hi bd jg ld le lf jk lg lh li jo iq lj lk js iu ll lm jw iy ln lo ka lp bi translated">政策评价</h2><p id="ff44" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">计算策略<strong class="ih hj"> π </strong>的状态值<strong class="ih hj"> Vπ </strong>。这叫做<strong class="ih hj">政策评估</strong>。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es ls"><img src="../Images/38aff048923829fd3b62500c3959320f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k0uangvZcZpa994CjjI9fw.png"/></div></div></figure><p id="5226" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中<strong class="ih hj"> π(a|s) </strong>是在策略<strong class="ih hj"> π </strong>下，在状态<strong class="ih hj"> <em class="la"> s </em> </strong>中采取行动<strong class="ih hj"> <em class="la"> a </em> </strong>的概率，期望值由<strong class="ih hj"> π </strong>下标，表示它们以<strong class="ih hj"> π </strong>被遵循为条件。</p><h2 id="93be" class="lc jf hi bd jg ld le lf jk lg lh li jo iq lj lk js iu ll lm jw iy ln lo ka lp bi translated">政策改进</h2><p id="fa43" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">假设我们已经为任意确定性策略<strong class="ih hj"> π </strong>确定了价值函数<strong class="ih hj"> Vπ </strong>。对于某些状态<strong class="ih hj"><em class="la"/></strong>我们想知道是否应该改变策略，确定性地选择一个动作<strong class="ih hj"><em class="la"/></strong>≦<strong class="ih hj"><em class="la">【π(s)</em></strong>。<br/>一种方法是在<strong class="ih hj"><em class="la"/></strong>中选择<strong class="ih hj"> <em class="la"> a </em> </strong>，此后遵循现有策略<strong class="ih hj"> π。</strong></p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lt"><img src="../Images/2f731943722a39d7f3d64eacae17ce65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i-SlvPD-6hxkrRX-7qTauw.png"/></div></div></figure><p id="7db2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设选择一个动作<strong class="ih hj"><em class="la">a</em></strong>≦<strong class="ih hj"><em class="la">π(s)</em></strong>并遵循现有策略<strong class="ih hj"> <em class="la"> π </em> </strong>而不是选择当前策略建议的动作，那么可以预期，每次遇到状态<strong class="ih hj"> <em class="la"> s </em> </strong>时，选择动作<strong class="ih hj"> <em class="la"> a </em> </strong>将总是优于这导致了更好的总体政策。这就是<em class="la">政策改进定理</em>。</p><h2 id="bff5" class="lc jf hi bd jg ld le lf jk lg lh li jo iq lj lk js iu ll lm jw iy ln lo ka lp bi translated">策略迭代</h2><p id="41a1" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">一旦一个策略<strong class="ih hj"> <em class="la"> π </em> </strong>已经使用<strong class="ih hj"> V <em class="la"> π </em> </strong>进行了改进，以产生一个更好的策略<strong class="ih hj"><em class="la">【π’</em></strong>，然后我们可以计算<strong class="ih hj">V<em class="la">【π’</em></strong>，并再次对其进行改进，以产生一个更好的<strong class="ih hj"><em class="la">π】</em></strong>因此，我们可以获得一系列单调改进的策略和值函数:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="er es lu"><img src="../Images/b8298bac5ba0158f8f5f0c8051af433e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GEMEiQXu56zBa3vqA2YRpg.png"/></div></div><figcaption class="kq kr et er es ks kt bd b be z dx translated">策略迭代以获得最佳策略</figcaption></figure><p id="32d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">比方说，我们有一个策略π，然后通过贪婪地采取行动产生一个改进的版本π’。这个改进的π’的值肯定会更好，因为:</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lv"><img src="../Images/361e10eed33ccee2cc8a4299f5063ddf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*PJyb3ZoLOz6BPMkItwQcyw.png"/></div><figcaption class="kq kr et er es ks kt bd b be z dx translated"><a class="ae jd" href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#key-concepts" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="fd80" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是这一个。我没有详细讨论动态编程主题，因为本系列将更加关注无模型算法。<br/>在接下来的教程中，我们来说说<strong class="ih hj">蒙特卡罗</strong>方法。</p></div></div>    
</body>
</html>