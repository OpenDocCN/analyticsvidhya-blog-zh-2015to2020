# 人工智能和机器学习中的数据伦理

> 原文：<https://medium.com/analytics-vidhya/data-ethics-in-artificial-intelligence-machine-learning-72467b9c70f3?source=collection_archive---------5----------------------->

![](img/9c823e0f59df45bed8874f72f5ad6376.png)

照片来自 [Shutterstock](https://www.shutterstock.com/es/image-vector/integrity-word-cloud-1166439136)

道德是生活的一个重要方面，任何不道德的事情都是有害和可怕的。同样的原则在技术领域也是有效和合法的。随着大数据和高性能计算机器的发展，人工智能(AI)取得了飞跃性的进展。我们知道，为了制造一个高效的人工智能系统，我们需要精确的数据和一种能够以超高性能的方式处理未知数据的算法。因此，**数据**是人工智能或 ML(机器学习)算法的主要燃料，为这些目的收集的数据可能会有一些偏见和不道德的元素，这些元素可能会混淆或偏离算法，以不同的方式行事，并产生整个不道德的系统，这对社会可能是危险的。

有针对性的广告、社会偏见、假新闻是一些相关的例子，但是在过去发生了几个其他的例子，其中 ML 算法被误用(有时是无意的，因为模型的所有可能的行为都没有被测试)并且被证明以不期望的方式表现。下面是一些这样的使用案例的例子

1.  **英国的评分算法——**最近，英国教育部废弃了一种 [**算法**](https://rpubs.com/JeniT/ofqual-algorithm) 生成的成绩，该算法旨在预测年度 A(advance) Level 资格的表现。这一举措是由于 COVID 疫情和算法预测的结果被降级[超过三分之一的英国 A 级成绩。](https://www.bbc.com/news/explainers-53807730)开发的模型主要集中在两个特征**【学生的过去表现】**和**【学校的历史表现】**来预测学生的成绩。该算法的预测有利于私立学校和中学选择和六年级学校，这些学校的教师评估曾经受到严重影响。
2.  **不道德的面部识别—** 华盛顿邮报 [**文章**](https://www.washingtonpost.com/technology/2019/07/07/fbi-ice-find-state-drivers-license-photos-are-gold-mine-facial-recognition-searches/) 披露了美国移民和海关执法局如何不道德地收集大量数据来分析移民社区的日常活动。这是一个不道德地使用人工智能来滥用目标社区公民权利的例子。
3.  **亚马逊的人工智能招聘工具—** 亚马逊开发的招聘工具开始对女性求职者产生偏见。[见全文**这里**](https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G)
4.  **失业救济金欺诈—** MiDAS(密歇根集成数据自动化系统)一个失业系统，它的推出是为了取代其旧的基于 COBOL 的遗留系统，该系统记录了许多人申请失业救济金的欺诈行为。它错误地指控至少 20，000 名索赔者欺诈，错误率高达 93%。问题在于所谓的“机器人裁决”系统，它缺乏人的监督。该应用程序寻找索赔人文件中的差异，如果发现差异，个人会自动收到经济处罚，然后，他们会被标记为欺诈。请看这篇**[**metro times post**](https://www.metrotimes.com/detroit/criminalizing-the-unemployed/Content?oid=2353533)**了解更多详情。****
5.  ******微软** [**推出 Tay**](http://www.theverge.com/2016/3/23/11290200/tay-ai-chatbot-released-microsoft)**——**一个以“和 Tay 聊得越多，它就变得越聪明”为理念的 Twitter 机器人，在推出后的 24 小时内就因提供所有来自 Twitter 的厌恶女性、种族主义的信息而遭到破坏。查看此 [**帖子。**](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist)****
6.  ******谷歌的仇恨言论检测器—** 谷歌为捕捉仇恨言论而开发的 AI [**工具**](https://futurism.com/the-byte/google-hate-speech-ai-biased) 开始对黑人表现不同(偏见效应)。****

****所以，看着那些失灵的 AI/ML 工具，它们当然是由顶级开发者开发的，是由伟大的商业领袖设想的，突然变成了对社会的威胁。然后真正的问题出现了，如何在所有合作者群体(数据收集者、开发者、决策者、销售人员、市场营销人员、执行人员等)中建立一种道德的工作方式和合理的责任。)?在这个方向上已经发表了几篇论文，没有宗教上的黄金法则可以遵循，但是这个问题的几个重要方面可以总结。我想强调它们是本文的一部分，也是本文的目的。****

1.  ******5cs******

****![](img/5ba9ca07f017e8a0e75c2234ab6be48e.png)****

****数据伦理—5c****

*******a)*** ***同意—*** 数据提供者和数据服务者之间达成的一致同意。****

*******b)*** ***透明度—*** 透明度与同意告诉数据提供商他们正在提供什么直接相关。****

*******c)*** ***一致性&信任—*** 不可捉摸的人是不可信任的，因此信任需要一致性。这些事实很重要，应该成为数据伦理的一部分，因为我们已经看到了许多安全事件，这些事情被明确或隐含地破坏了。雅虎、Target、Anthem、当地医院和政府数据是这方面的几个例子。****

*******d)*** ***控制&透明度—*** 一旦提供了同意，了解数据是如何被使用的就变得很重要了。用户对它们有控制权吗？这些问题具有重要的方面，因为我们知道大公司通常如何使用公共数据为他们自己的目标做广告，并制造政治和宗教情绪。为了在一定程度上解决这些问题，欧洲的 [**通用数据保护条例**](http://bit.ly/2BnbBUF) (GDPR)就是一个很好的例子，它使用户能够同意从先前提交数据的系统中删除数据。****

*******e)*** ***后果* —** 风险永远不可能完全消除。使用 AI 和 ML 的产品得到构建，有时由于围绕数据使用的潜在问题，一些不可预见的后果会出现。已经制定了许多法规和指导方针来解决这类问题，例如，保护儿童及其数据的[儿童在线隐私保护法](http://bit.ly/2zYrL7q) (COPPA)和[基因信息非歧视法](http://bit.ly/2LzvRV1) (GINA)以应对日益增长的对基因检测可能被用于反对个人或其家庭的担忧。****

****实施**5**5**Cs**不是个人的责任，而是需要整个团队本着责任共担的理念。****

******2)** 是数据还是算法？当然，有不同的观点，但在大多数情况下，是人类开发了这些数学模型，或者为它们提供了数据集，这些数据集通常是由人类创建或收集的。因此，最终偏见是与人类相关的某个地方，我们需要在收集数据或设计科幻人工智能模型时展示责任和最佳实践。****

******3)** **语境—** 语境意识对于任何在人工智能和人工智能领域工作的人来说都起着重要的作用。理解数据以回答一些标准问题，如 ***我想做什么和为什么要做某些事情，这有助于*** 设计一种算法，它可以根据数据的上下文来感知决策。****

******4)模型的公平性&可解释性—** 模型的结果可信吗？他们是否解释了为什么特定特征及其权重对于可预测值很重要？我们能解释一下吗？这些问题与决定开发的模型的使用是否公平，以及它们对于开发它们的目的是否足够合理有关。****

******5)模型漂移—** 分析模型需要随时间进行修正，否则很有可能出现不稳定性和错误预测。在 ML/AI 中，这种行为被定义为模型漂移。它被分为两大类。****

*******i)概念漂移****——指的是模型试图预测的目标变量的统计属性，随着时间以不可预见的方式发生变化。*****

*******ii)数据漂移****——当预测因子的统计性质发生变化(自变量)时就会出现这种情况。这些变化会导致模型失败。数据漂移的典型例子是数据的季节性。黑色星期五这段时间总是比一年中的其他时间更畅销。*****

******6)道德和安全培训—** 作为教育课程的一部分学习或教授的理论缺乏实际实施，这就是为什么道德和安全培训对专业人士很重要，因为它使他们能够在相关领域实施这些原则。****

****因此，如果我们收集这些要点作为清单，并在做出任何决定时遵循它们，那么这将有助于避免常见错误。这些要点也能让我们对工作变得更加负责和敏感。 ***Mike Loukides、DJ Patil、Hilary Mason*** 在他们的书**伦理和数据科学**中汇编了以下清单，值得将它们放在我们的数据产品清单中。****

## ******清单—******

> ****我们列出了这项技术如何被攻击或滥用了吗？****
> 
> ****我们是否测试过我们的训练数据，以确保它的公平性和代表性？****
> 
> ****我们是否研究并理解了数据中可能的偏差来源？****
> 
> ****我们的团队是否反映了观点、背景和各种思想的多样性？****
> 
> ****我们需要收集哪种用户同意才能使用这些数据？****
> 
> ****我们有收集用户同意的机制吗？****
> 
> ****我们是否清楚地解释了用户同意什么？****
> 
> ****如果人们因结果而受到伤害，我们有补救机制吗？****
> 
> ****如果这个软件表现不好，我们可以在生产中关闭它吗？****
> 
> ****我们是否测试了不同用户群的公平性？****
> 
> ****我们测试过不同用户群中不同的错误率吗？****
> 
> ****我们是否测试和监控模型漂移，以确保我们的软件随着时间的推移保持公平？****
> 
> ****我们是否有保护用户数据的计划？****

****因此，简而言之，数据伦理原则可以帮助充分利用人工智能的好处，为社会造福，而不用担心，也可以在开发数据产品以解决关键问题的所有参与者中创造一种责任感。****

******参考文献—******

****[https://hub . packtpub . com/machine-learning-Ethics-what-you-need-to-know-and-what-you-can-do/#:~:text = Ethics % 20 needs % 20 to % 20 be % 20 seen % 20，and % 20 building % 20 machine % 20 learning % 20 systems。&text = But % 20 by % 20 专注于% 20 机器，robust % 20 和% 20 有% 20 更好的% 20 结果。](https://hub.packtpub.com/machine-learning-ethics-what-you-need-to-know-and-what-you-can-do/#:~:text=Ethics%20needs%20to%20be%20seen,and%20building%20machine%20learning%20systems.&text=But%20by%20focusing%20on%20machine,robust%20and%20have%20better%20outcomes.)****

****[https://learning . oreilly . com/library/view/ethics-and-data/9781492043898/](https://learning.oreilly.com/library/view/ethics-and-data/9781492043898/)****

****[https://www.bbc.com/news/explainers-53807730](https://www.bbc.com/news/explainers-53807730)****