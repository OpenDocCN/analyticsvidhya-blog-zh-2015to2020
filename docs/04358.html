<html>
<head>
<title>Text Mining: Extracting and Analyzing all my Blogs on Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本挖掘:提取和分析我所有关于机器学习的博客</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/text-mining-extracting-and-analyzing-all-my-blogs-on-machine-learning-b6983c7a608e?source=collection_archive---------1-----------------------#2020-03-16">https://medium.com/analytics-vidhya/text-mining-extracting-and-analyzing-all-my-blogs-on-machine-learning-b6983c7a608e?source=collection_archive---------1-----------------------#2020-03-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/52baf04fbeaa8b1754a807531bee8650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*InX9wI6vQTSl9pSm"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae iu" href="https://unsplash.com/@thoughtcatalog?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">思想目录</a>拍摄</figcaption></figure><p id="c36d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最近我开始在工作和家里从事自然语言处理。所以我决定应用一些我遇到过的技术，从我所有的博客中提取单词，创建一个单词云。</p><p id="fdad" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我在2019年2月15日发表了我的第一篇文章。写作两个月后，我被选为人工智能媒体的顶级作家。这种认可激励我继续与数据科学界分享我的知识。从那时起，我已经在<a class="ae iu" rel="noopener" href="/@niranjankumarc">媒体</a>和<a class="ae iu" href="https://www.marktechpost.com/author/niranjan-kumar/" rel="noopener ugc nofollow" target="_blank"> Marktechpost </a>上写了超过25篇关于机器学习、深度学习相关主题的文章。</p><p id="5e45" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们将删除所有文章的内容，并使用NLTK包处理数据以供进一步分析。为了丢弃数据，我们将使用<code class="du jt ju jv jw b">Selenium</code>和<code class="du jt ju jv jw b">BeautifulSoup</code>。</p><blockquote class="jx jy jz"><p id="1090" class="iv iw ka ix b iy iz ja jb jc jd je jf kb jh ji jj kc jl jm jn kd jp jq jr js hb bi translated">在您开始从任何网站废弃数据之前，请检查该网站的条款和条件。</p></blockquote><h1 id="54d2" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">设置环境</h1><p id="2b8e" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">下载并安装Anaconda包管理器。Anaconda发行版包含许多开源包的集合。除了Anaconda中现有的包，我们将使用新的包从网页中提取内容。</p><p id="0515" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要安装新的包，您可以使用Anaconda的包管理器、conda或pip来安装这些包。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="94ab" class="lp kf hi jw b fi lq lr l ls lt">pip install selenium<br/>pip install beautifulsoup4</span></pre><h1 id="dd72" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">检查网页</h1><p id="331a" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">为了找出我们需要使用python提取网页的哪些元素，我们需要首先检查网页。要检查任何<a class="ae iu" href="https://www.marktechpost.com/author/niranjan-kumar/" rel="noopener ugc nofollow" target="_blank">网页</a>,您需要右击网页并选择inspect或Fn + F12。这将打开HTML代码检查工具，在这里我们可以看到每个字段所包含的元素。</p><p id="7665" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">例如，如果我检查marktechpost <a class="ae iu" href="https://www.marktechpost.com/author/niranjan-kumar/" rel="noopener ugc nofollow" target="_blank"> author </a>页面，您可以看到文章链接和文章标题出现在<code class="du jt ju jv jw b">&lt;a&gt;</code>标签中。<code class="du jt ju jv jw b">&lt;a&gt;</code>标签有<code class="du jt ju jv jw b">href</code>属性，它指定链接到的页面的URL，它还有<code class="du jt ju jv jw b">title</code>属性，它定义文档的标题。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/2dcd60e8f1552d8fa854981e870a26e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VyaZYC5jWqmMsPkqQL1E_g.png"/></div></div></figure><p id="847a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过进一步检查，作者页面下的所有文章都遵循相同的模式<code class="du jt ju jv jw b">href</code>和<code class="du jt ju jv jw b">title</code>。所有文章的<code class="du jt ju jv jw b">&lt;a&gt;</code>标签都有<code class="du jt ju jv jw b">rel</code>属性，用于指定当前文档和链接文档之间的关系。利用这些属性，我们可以很容易地使用BeautifulSoup包从网页HTML代码中抓取相关信息。(我将在下面讨论如何使用BeautifulSoup进行提取)</p><p id="6562" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">类似地，我们可以检查<a class="ae iu" rel="noopener" href="/@niranjankumarc">媒体作者页面</a>，并找出相关信息(如文章链接和文章标题)出现在哪个标签下。</p><p id="b546" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">文章中讨论的所有代码都在我的GitHub上</p><div class="lv lw ez fb lx ly"><a href="https://github.com/Niranjankumar-c/DataScienceBlogAnalysis_NLP" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hj fi z dy md ea eb me ed ef hh bi translated">niranjankumar-c/DataScienceBlogAnalysis _ NLP</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">这个库包含了删除我所有文章的内容并使用NLTK包处理数据的代码…</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">github.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm io ly"/></div></div></a></div><h2 id="70f0" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">用硒刮除介质网页</h2><p id="2445" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">首先，我们将从媒体中抓取<a class="ae iu" rel="noopener" href="/@niranjankumarc">我的账号下所有已发表文章的链接。由于中型网站使用javascript加载页面，我们不能直接使用BeautifulSoup来抓取数据。我们将使用Selenium web驱动程序打开网页，搜索文章的链接并返回结果，而不是BeautifulSoup。</a></p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="49b1" class="lp kf hi jw b fi lq lr l ls lt">from selenium import webdriver<br/>import time</span><span id="b8ec" class="lp kf hi jw b fi na lr l ls lt">medium_profile_link = "<a class="ae iu" rel="noopener" href="/@niranjankumarc">https://medium.com/@niranjankumarc</a>"</span></pre><p id="dec5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本教程中，我将使用Chrome作为我的浏览器，所以我们需要下载Chrome WebDriver。你可以从<a class="ae iu" href="https://sites.google.com/a/chromium.org/chromedriver/home" rel="noopener ugc nofollow" target="_blank">这里</a>下载网络驱动。如果您使用任何其他浏览器，那么您需要下载特定的网络驱动程序。一旦你下载了网络驱动，运行Chrome网络驱动，将可执行文件路径指向下载文件路径。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="9e83" class="lp kf hi jw b fi lq lr l ls lt">#create the driver for chrome browser with executable.</span><span id="3314" class="lp kf hi jw b fi na lr l ls lt">driver = webdriver.Chrome(executable_path = 'C:\\Users\\NiranjanKumar\\Downloads\\chromedriver.exe')</span></pre><p id="774b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们可以连接到网页并搜索文章的链接。当我们在浏览器中加载网页时，通常需要一段时间来加载整个页面，甚至可能直到我们向下滚动到页面末尾时才加载。为了解决这个问题，我们将执行一段javascript代码来帮助加载整个网页。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="b7c0" class="lp kf hi jw b fi lq lr l ls lt">#load the webpage and scroll till the bottom of the page<br/>driver.get(medium_profile_link)</span><span id="74ec" class="lp kf hi jw b fi na lr l ls lt"># Get scroll height<br/>last_height = driver.execute_script("return document.body.scrollHeight")</span><span id="5514" class="lp kf hi jw b fi na lr l ls lt">while True:<br/>    # Scroll down to bottom<br/>    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")</span><span id="e03c" class="lp kf hi jw b fi na lr l ls lt"># Wait to load page<br/>    time.sleep(30)</span><span id="91a7" class="lp kf hi jw b fi na lr l ls lt"># Calculate new scroll height and compare with last scroll height<br/>    new_height = driver.execute_script("return document.body.scrollHeight")<br/>    if new_height == last_height:<br/>        break<br/>    last_height = new_height</span></pre><p id="1b1b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦你执行了上面的脚本，Chrome将打开指定的URL并向下滚动到页面底部。接下来，我们需要获取网页的内容。为了找到所有感兴趣的元素，我们将创建一个BeautifulSoup对象，并提取所有相关的<code class="du jt ju jv jw b">div</code>标签。</p><p id="3a7b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用“检查网页”选项找出文章和文章标题的相关链接。使用BeautifulSoup对象中的<code class="du jt ju jv jw b">findAll</code>命令，我们将提取相关的标签。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="105e" class="lp kf hi jw b fi lq lr l ls lt">#get the html content of the page<br/>html = driver.page_source</span><span id="0076" class="lp kf hi jw b fi na lr l ls lt">#create a soup object<br/>medium_soup = BeautifulSoup(html)</span><span id="f0ab" class="lp kf hi jw b fi na lr l ls lt">#finding the divs with class = 'r s y'<br/>soup_divs = medium_soup.findAll("div", {'class' : 'r s y'})</span></pre><p id="35c9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦我们获得了<code class="du jt ju jv jw b">div</code>标签，我们就遍历每个标签并获得文章链接和文章标题。文章链接出现在<code class="du jt ju jv jw b">a</code>标签中，文章链接在<code class="du jt ju jv jw b">h1</code>标签中。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="2e05" class="lp kf hi jw b fi lq lr l ls lt">title_lst = []<br/>links_lst = []</span><span id="b46f" class="lp kf hi jw b fi na lr l ls lt">for each_div in soup_divs:<br/>    <br/>    article_link = each_div.find("a").get("href").split("?")[0]<br/>    article_title = each_div.find("a").find("h1").text.strip()<br/>    <br/>    if article_link.startswith("https") == False:<br/>        article_link = "<a class="ae iu" rel="noopener" href="/">https://medium.com</a>" + article_link #appending the address for links (eg. hackernoon: moved out of medium)<br/>    <br/>    #append the values to list<br/>    title_lst.append(article_title)<br/>    links_lst.append(article_link)</span><span id="06ef" class="lp kf hi jw b fi na lr l ls lt">driver.quit() #stop the driver</span></pre><p id="2894" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们已经有了文章的所有链接和标题，我们将执行<code class="du jt ju jv jw b">driver.quit()</code>来确保浏览器关闭。</p><h2 id="f5ad" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">使用BeautifulSoup删除Marktechpost网页</h2><p id="630e" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">为了抓取marktechpost<a class="ae iu" href="https://www.marktechpost.com/author/niranjan-kumar/" rel="noopener ugc nofollow" target="_blank">author page</a>的数据，我们可以使用selenium，如前一步所示。在这一步，我们将使用<code class="du jt ju jv jw b">requests</code>和<code class="du jt ju jv jw b">BeautifulSoup</code>库来抓取数据。</p><p id="1f39" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦我们有了想要抓取数据的网页的URL。然后，我们使用<code class="du jt ju jv jw b">requests</code>连接到网页，并使用<code class="du jt ju jv jw b">BeautifulSoup</code>解析网页，将对象存储在变量‘mark _ soup’中。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="b426" class="lp kf hi jw b fi lq lr l ls lt">import requests</span><span id="e3e2" class="lp kf hi jw b fi na lr l ls lt">#creating a variable<br/>mark_url = requests.get('<a class="ae iu" href="https://www.marktechpost.com/author/niranjan-kumar/'" rel="noopener ugc nofollow" target="_blank">https://www.marktechpost.com/author/niranjan-kumar/'</a>)</span><span id="2d2c" class="lp kf hi jw b fi na lr l ls lt">#create a beautifulsoup object<br/>mark_soup = BeautifulSoup(mark_url.content)</span><span id="3f1d" class="lp kf hi jw b fi na lr l ls lt">#print soup<br/>print(mark_soup.prettify())</span></pre><p id="b99d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如上一节所讨论的——检查网页，我们需要搜索soup对象来找到相关的标签。一旦我们获得了<code class="du jt ju jv jw b">div</code>标签，我们就遍历每个标签并获得文章链接和文章标题。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="0fe5" class="lp kf hi jw b fi lq lr l ls lt">#iterate the articles and get the links and titles<br/>for eachitem in mark_soup.findAll("a", {'rel': 'bookmark'}):    <br/>    title_lst.append(eachitem.get("title"))<br/>    links_lst.append(eachitem.get("href"))</span></pre><p id="5f9e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将搜索操作的结果添加到文章链接和文章标题列表中。我们可以通过将收集到的数据保存到数据框中来利用这一点。我们可以打印数据框来查看内容。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="426a" class="lp kf hi jw b fi lq lr l ls lt">#create a dataframe<br/>titles_df = pd.DataFrame(list(zip(title_lst, links_lst)), columns = ["Title", "URL"])</span><span id="5099" class="lp kf hi jw b fi na lr l ls lt">titles_df.head()</span></pre><h1 id="603b" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">提取内容</h1><p id="f51b" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">现在我们已经有了所有发表文章的链接，让我们获取这些文章的内容。由于我们是从网页中提取整个内容，包括HTML内容，我们将使用<code class="du jt ju jv jw b">html2text</code>包将一页HTML转换成干净、易读的普通ASCII文本。</p><p id="789f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">安装<code class="du jt ju jv jw b">html2text</code>包</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="32f8" class="lp kf hi jw b fi lq lr l ls lt">!pip install html2text</span></pre><p id="e163" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du jt ju jv jw b">html2text</code>提供多个选项，如忽略链接、忽略图像、忽略提取数据中的表格，以获得清晰易读的文本。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="a250" class="lp kf hi jw b fi lq lr l ls lt">import html2text</span><span id="eaab" class="lp kf hi jw b fi na lr l ls lt">h = html2text.HTML2Text()</span><span id="df60" class="lp kf hi jw b fi na lr l ls lt">#ignoring all the links, tables and images in the blog<br/>h.ignore_links = True<br/>h.ignore_images = True<br/>h.ignore_tables = True</span></pre><p id="2384" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">迭代每个链接并提取HTML内容，将其添加到<code class="du jt ju jv jw b">titles_df</code>数据框中的新列。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="421d" class="lp kf hi jw b fi lq lr l ls lt">content_lst = []</span><span id="dcd2" class="lp kf hi jw b fi na lr l ls lt">#iterating through all the links <br/>for ind in titles_df.index: <br/>    <br/>    #content request object<br/>    request_content = requests.get(titles_df["URL"][ind])<br/>    <br/>    main_content = h.handle(str(request_content.content)) #get the text from the html content<br/>    <br/>    content_lst.append(str(main_content))</span></pre><p id="04c9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">生成的数据帧应该如下所示:</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/013511c2732ac716d1a6c7bae02e5d37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YaSmt5ZvFJH_ne22boHNvA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">最终数据帧</figcaption></figure></div><div class="ab cl nc nd gp ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="hb hc hd he hf"><h1 id="39cd" class="ke kf hi bd kg kh nj kj kk kl nk kn ko kp nl kr ks kt nm kv kw kx nn kz la lb bi translated">文本预处理—清理数据</h1><p id="e9a2" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">在本节中，我们将讨论如何预处理(清洗)网页的内容，以便我们可以从清洗后的数据中提取有用的信息。</p><h2 id="4610" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">导入包</h2><p id="0574" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">在我们开始挖掘数据之前，首先我们需要导入所需的库。我们将<code class="du jt ju jv jw b">nltk</code>对文本进行分词，并从语料库和<code class="du jt ju jv jw b">wordcloud</code>包中移除停用词，以生成词云。</p><figure class="lh li lj lk fd ij"><div class="bz dy l di"><div class="no np l"/></div></figure><p id="2c8c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们清理数据。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nq"><img src="../Images/e5ab0a3c008e70e2230163738d150720.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1bHCYpKfvl70XAwu"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">索菲·埃尔维斯在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h2 id="564f" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">1.移除链接</h2><p id="fb4e" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">尽管我们已经使用<code class="du jt ju jv jw b">html2text</code>包设置了忽略链接的选项，但某些链接仍有可能出现在网页内容中。第一个预处理步骤是删除内容中的链接。我们将通过使用regex来实现这一点。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="d6c5" class="lp kf hi jw b fi lq lr l ls lt">def remove_links(text):    <br/>    return re.sub(r'https?://[^\s]+', '', text)</span></pre><p id="530d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du jt ju jv jw b">remove_links</code>函数将文本作为输入，并用文本中的空值替换URL。我们将使用<code class="du jt ju jv jw b">lambda</code>函数对所有内容的数据帧应用此函数。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="acb8" class="lp kf hi jw b fi lq lr l ls lt"># 1. Remove all the links if present in the content</span><span id="143d" class="lp kf hi jw b fi na lr l ls lt">titles_df["content"] = titles_df.content.apply(lambda content: remove_links(content))<br/>titles_df.head()</span></pre><h2 id="e701" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">2.删除多余的空格和制表符</h2><p id="9cae" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">在移除多余的空格之前，我们将使用“\n\n”连接数据框中的所有内容，并使其成为单个字符串。使用正则表达式，我们将用一个空格替换多余的空格和制表符。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="c1be" class="lp kf hi jw b fi lq lr l ls lt"># 2. Join all the contents in the dataframe using "\n\n" and make it a single string</span><span id="b7ca" class="lp kf hi jw b fi na lr l ls lt">main_content_string =  "\n\n".join(titles_df.content)</span><span id="c6ee" class="lp kf hi jw b fi na lr l ls lt"># Remove all whitespaces (\n and \t) with space</span><span id="1f5a" class="lp kf hi jw b fi na lr l ls lt">main_content_string = re.sub('\s+',' ',main_content_string).strip()<br/>main_content_string[:300]</span></pre><h2 id="b792" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">3.扩张收缩</h2><p id="f801" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated"><a class="ae iu" href="https://www.thoughtco.com/contractions-commonly-used-informal-english-1692651" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj">缩写</strong> </a>是通过去掉一个或多个字母而被缩短的单词或短语。缩写的例子有像“不是”、“不是”、“不”这样的词。我们利用Rahul Agarwal编写的一组现有函数来扩展收缩。点击查看原帖<a class="ae iu" href="https://mlwhiz.com/blog/2019/01/17/deeplearning_nlp_preprocess/" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="eeff" class="lp kf hi jw b fi lq lr l ls lt"># 3. expand contractions</span><span id="517b" class="lp kf hi jw b fi na lr l ls lt">def _get_contractions(contraction_dict):<br/>    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))<br/>    return contraction_dict, contraction_re</span><span id="aad4" class="lp kf hi jw b fi na lr l ls lt">contractions, contractions_re = _get_contractions(contraction_dict)</span><span id="3ddb" class="lp kf hi jw b fi na lr l ls lt">def replace_contractions(text):<br/>    def replace(match):<br/>        return contractions[match.group(0)]<br/>    return contractions_re.sub(replace, text)</span><span id="80cb" class="lp kf hi jw b fi na lr l ls lt"># Usage<br/>main_content_string = replace_contractions(main_content_string)</span></pre><h2 id="6d88" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">4.删除标点符号和特殊字符</h2><p id="f9af" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">标点符号和特殊字符是非字母数字字符，在挖掘数据时不提供任何信息。因此，删除这些字符将为我们提供干净的数据进行分析。</p><p id="5ec6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了删除标点符号，我们将使用之前导入的<code class="du jt ju jv jw b">string</code>包。</p><figure class="lh li lj lk fd ij"><div class="bz dy l di"><div class="no np l"/></div></figure><h2 id="9200" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">5.小写字母</h2><p id="4634" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">我们需要将所有的推文转换为小写，以消除重复出现的单词，如'<strong class="ix hj"> USA </strong>'、<strong class="ix hj"> usa </strong>和'<strong class="ix hj"> Usa </strong>'。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="c9ae" class="lp kf hi jw b fi lq lr l ls lt"># 5. Lowercase</span><span id="6e90" class="lp kf hi jw b fi na lr l ls lt">main_content_string = main_content_string.lower().strip("b").strip()</span></pre><h2 id="fe1d" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">6.标记化</h2><p id="f09d" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">标记化是将文本分成单词或句子的过程。因为我们只有一个大句子，我们将使用NLTK包中的<code class="du jt ju jv jw b">word_tokenize</code>把这个句子拆分成单词。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="b70d" class="lp kf hi jw b fi lq lr l ls lt"># 6. Tokenization</span><span id="70db" class="lp kf hi jw b fi na lr l ls lt">word_tokens = word_tokenize(main_content_string)<br/>word_tokens[:10]</span></pre><h2 id="3dae" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">7.删除停用词</h2><p id="bd0b" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">停用词是在语料库中频繁出现的词，并且对从文本中提取的特征具有很小或没有意义。通常，这些可以是冠词、连词、介词等等。停用词的一些例子有:<em class="ka"> a </em>，<em class="ka">a</em>，<em class="ka">a</em>，<em class="ka">和。</em></p><p id="8e5e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将利用NLTK库中的停用词列表从我们的标记中删除这些词。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="8fc6" class="lp kf hi jw b fi lq lr l ls lt"># 7. remove stop words<br/>stop_words = stopwords.words('english')</span><span id="cf01" class="lp kf hi jw b fi na lr l ls lt">#remove stopwords<br/>main_content_tokens = [w for w in word_tokens if not w in stop_words]</span></pre><h2 id="d5ff" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">8.移除字母数字单词</h2><p id="f332" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">既包含字母又包含数字的单词不会给出关于文本的任何额外信息。因此，我们将删除所有包含数字的单词。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="c6fb" class="lp kf hi jw b fi lq lr l ls lt"># 8. Remove all words containing numbers</span><span id="624b" class="lp kf hi jw b fi na lr l ls lt">main_content_tokens = [word for word in main_content_tokens if word.isalpha()]</span></pre><h2 id="424d" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">9.去除语料库中频繁出现的单词</h2><p id="75d4" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">首先，我们将检查语料库中出现频率最高的单词。我觉得这些话不会影响分析，我们会在数据上做。所以我已经把这些词从语料库中删除了。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="001f" class="lp kf hi jw b fi lq lr l ls lt"># 9. Remove frequently occuring words in the corpus</span><span id="7aa7" class="lp kf hi jw b fi na lr l ls lt">freq = pd.Series(main_content_tokens).value_counts()[:10]<br/>freq</span><span id="b1eb" class="lp kf hi jw b fi na lr l ls lt">#output<br/>data        659<br/>network     483<br/>function    460<br/>learning    447<br/>nnnn        441<br/>xexx        394<br/>neural      355<br/>model       309<br/>input       306<br/>using       290</span><span id="8b56" class="lp kf hi jw b fi na lr l ls lt">#removing words<br/>main_content_tokens = [word for word in main_content_tokens if word not in list(freq.index)]</span></pre><h2 id="3125" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">10.移除生僻字移除</h2><p id="083a" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">我们将从语料库中删除1000个生僻字，这些生僻字不会对分析产生任何影响。这些词非常罕见，以至于它们成为分析的干扰。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="4124" class="lp kf hi jw b fi lq lr l ls lt"># 10. removing the least frequent words in the corpus</span><span id="93d3" class="lp kf hi jw b fi na lr l ls lt">freq = pd.Series(main_content_tokens).value_counts()[-1000:]<br/>freq</span><span id="35df" class="lp kf hi jw b fi na lr l ls lt">#removing words - less frequent words<br/>main_content_tokens = [word for word in main_content_tokens if word not in list(freq.index)]</span></pre><h2 id="fbb2" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">11.删除杂词</h2><p id="89ae" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">在进一步检查我们到目前为止获得的清理过的标记时，我发现在我们的语料库中有一些没有意义的杂词。例如'<strong class="ix hj"> rnrn </strong>'，'<strong class="ix hj"> rntt </strong>'和'<strong class="ix hj"> xcxb </strong>'。所以我删除了所有包含这些单词的标记。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="c19e" class="lp kf hi jw b fi lq lr l ls lt">#11. Removing miscellaneous words</span><span id="5153" class="lp kf hi jw b fi na lr l ls lt">main_content_tokens = [word for word in main_content_tokens if not <br/>any([phrase in word for phrase in ["rnrn", "rntt", "xcxb", "xx"]])]</span><span id="8993" class="lp kf hi jw b fi na lr l ls lt">#miscellaneous words<br/>main_content_tokens = [word for word in main_content_tokens if len(word) &gt;= 4]</span></pre><h2 id="452b" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">干净的语料库</h2><p id="6500" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">到目前为止，我们已经完成了所有的文本清理或预处理步骤，现在我们将清理后的标记合并到一个句子中，以便我们可以使用WordCloud清理语料库来识别热门话题。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="2663" class="lp kf hi jw b fi lq lr l ls lt">#merge all the tokens<br/>cleaned_content = " ".join(main_content_tokens)</span></pre></div><div class="ab cl nc nd gp ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="hb hc hd he hf"><h1 id="3044" class="ke kf hi bd kg kh nj kj kk kl nk kn ko kp nl kr ks kt nm kv kw kx nn kz la lb bi translated">WordCloud</h1><p id="79f7" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">现在我们准备创建一个词云来识别语料库中的热门话题。为了创建一个单词云，我们将利用<code class="du jt ju jv jw b">wordcloud</code>库。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="ec07" class="lp kf hi jw b fi lq lr l ls lt"># Create stopword list:<br/>stopwords = set(STOPWORDS)</span><span id="a399" class="lp kf hi jw b fi na lr l ls lt">wordcloud = WordCloud(width = 800, height = 800, max_font_size=50,min_font_size = 10,<br/>                stopwords = stopwords, background_color = "black", colormap="plasma").generate(cleaned_content) <br/>  <br/># plot the WordCloud image     <br/>plt.figure(figsize = (12, 12)) <br/>plt.imshow(wordcloud, interpolation="bilinear") <br/>plt.axis("off") <br/>plt.tight_layout()</span><span id="f02b" class="lp kf hi jw b fi na lr l ls lt"># store to file<br/>plt.savefig("av_wordcloud.png", dpi=150)<br/>  <br/>plt.show()</span></pre><p id="e219" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦我们执行了上面的脚本，WordCloud就会生成并保存到本地目录中，名为<code class="du jt ju jv jw b">av_wordcloud.png</code>。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nt"><img src="../Images/057c0f38d0c0a47fac8f37a1a43fcd58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N7-cAa92t-amMN0Cfh_kwA.png"/></div></div></figure><p id="943f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从词云中，我们可以清楚地看到一些热门词汇，如“梯度下降”、“输出”、“损失”、“决策边界”、“隐藏表示”等..表明语料库主要谈论神经网络和深度学习技术。这是意料之中的，因为我的大多数文章都是围绕深度学习展开的，从基础开始，如<a class="ae iu" rel="noopener" href="/hackernoon/perceptron-deep-learning-basics-3a938c5f84b6?source=---------26------------------">感知器</a>到<a class="ae iu" href="https://towardsdatascience.com/introduction-to-encoder-decoder-models-eli5-way-2eef9bbf79cb?source=---------4------------------" rel="noopener" target="_blank">编码器-解码器模型</a>。</p><h1 id="d315" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">最常用的词</h1><p id="77b9" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">另一种可视化热门词汇的方法是使用条形图。我们将使用<code class="du jt ju jv jw b">collections</code>包中的<code class="du jt ju jv jw b">Counter</code>来统计单词的频率，并绘制出前10个单词及其频率。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="0f86" class="lp kf hi jw b fi lq lr l ls lt">#frequent words</span><span id="cc3d" class="lp kf hi jw b fi na lr l ls lt">counted_words = Counter(main_content_tokens)<br/>most_common_df = pd.DataFrame(counted_words.most_common(10), columns=["words", "count"])</span><span id="8076" class="lp kf hi jw b fi na lr l ls lt">#plot the most common words<br/>sns.barplot(y = "words", x = "count", data = most_common_df, palette="viridis")<br/>plt.xlabel("Frequency")<br/>plt.ylabel("Words")<br/>plt.title("Top 15 Most Occuring Words in the Corpus")<br/>plt.show()</span></pre><p id="345c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">生成的柱状图如下所示，</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nu"><img src="../Images/5d3857d901868992def96f5a95e7cb27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WdjgHAnxngr01kmoC7L6rg.png"/></div></div></figure><h1 id="6f0c" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">风格云</h1><p id="7342" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">我们将使用<code class="du jt ju jv jw b">stylecloud</code>包来生成有风格的单词云，包括渐变和图标形状，而不是创建简单而枯燥的单词云！。</p><p id="4d44" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du jt ju jv jw b">stylecloud</code>是一个Python包，它利用了流行的<a class="ae iu" href="https://github.com/amueller/word_cloud" rel="noopener ugc nofollow" target="_blank"> word_cloud </a>包，添加了有用的功能来创建真正独特的word clouds！</p><p id="2bfa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要安装软件包</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="565d" class="lp kf hi jw b fi lq lr l ls lt">pip install stylecloud</span></pre><h2 id="f6d2" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">使用</h2><p id="9882" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">使用<code class="du jt ju jv jw b">stylecloud</code>,我们可以生成任何形状和大小的文字云。例如，让我们试着创建一个以深色为主题背景的狗形状的单词云。</p><p id="da14" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了得到狗的形状，我们只需要将<code class="du jt ju jv jw b">icon_name</code>改为“<strong class="ix hj"> fas fa-dog </strong>”，通过将<code class="du jt ju jv jw b">background_color</code>设置为“<strong class="ix hj"> black </strong>”，我们将得到黑暗主题。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="f60c" class="lp kf hi jw b fi lq lr l ls lt">import stylecloud</span><span id="4467" class="lp kf hi jw b fi na lr l ls lt">#dog wordcloud<br/>stylecloud.gen_stylecloud(text = cleaned_content,<br/>                          icon_name='fas fa-dog',<br/>                          background_color='black',<br/>                          gradient='horizontal', output_name='stylecloud_dog.png')</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es nv"><img src="../Images/f53ffa8b917a411a658aa89e60bf6cac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*3IeMxQV2j9QmatdEzJij1A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">可爱的狗</figcaption></figure><p id="07a1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你想把WordCloud变成twitter或LinkedIn的图标，我们只需要修改一行代码就可以做到。</p><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="bd7d" class="lp kf hi jw b fi lq lr l ls lt">#can also generate linkedln wordclouds</span><span id="d17f" class="lp kf hi jw b fi na lr l ls lt">stylecloud.gen_stylecloud(text = cleaned_content,<br/>                          icon_name="fab fa-linkedin-in",<br/>                          palette='colorbrewer.diverging.Spectral_11',<br/>                          background_color='white',<br/>                          gradient='vertical', output_name='stylecloud_li.png')</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es nv"><img src="../Images/6b6e86daacfb011d5fb4b0d055c032e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*9C0q3UcSjLkgfIeoTjrsXA.png"/></div></figure><pre class="lh li lj lk fd ll jw lm ln aw lo bi"><span id="87cf" class="lp kf hi jw b fi lq lr l ls lt">#can also generate twitter wordclouds</span><span id="1ac5" class="lp kf hi jw b fi na lr l ls lt">stylecloud.gen_stylecloud(text = cleaned_content,<br/>                          icon_name="fab fa-twitter",<br/>                          palette='colorbrewer.sequential.Blues_9',<br/>                          background_color='black',<br/>                          gradient='vertical', output_name='stylecloud_tw.png')</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es nv"><img src="../Images/f2c71cc5ea9a98ef4248cf9c853103f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*WlOxOhexJkS1hLbCDZANmA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">Twitter图标</figcaption></figure></div><div class="ab cl nc nd gp ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="hb hc hd he hf"><div class="lh li lj lk fd ly"><a href="https://github.com/Niranjankumar-c/DataScienceBlogAnalysis_NLP" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hj fi z dy md ea eb me ed ef hh bi translated">niranjankumar-c/DataScienceBlogAnalysis _ NLP</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">这个库包含了删除我所有文章的内容并使用NLTK包处理数据的代码…</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">github.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm io ly"/></div></div></a></div><h1 id="50ae" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">结论</h1><p id="5d83" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated">在本文中，我们讨论了两种不同的方法，根据网站是否使用javascript动态加载站点来废弃来自两个不同网站的数据。我们已经看到了如何使用selenium web驱动程序来废弃数据。之后，我们讨论了在分析数据以获得洞察力之前清理数据的各种技术。从那里，我们创建了一个WordCloud和Barplot来识别语料库中的热门话题。最后，我们已经看到了如何使用stylecloud包创建一个时尚的WordCloud。</p><p id="fb23" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你在实现我的GitHub库中的代码时遇到任何问题，请随时通过LinkedIn或twitter联系我。</p><p id="06ab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">直到下次和平:)</p><p id="4e90" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">NK。</p><h1 id="75cb" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">作者简介</h1><p id="d1d5" class="pw-post-body-paragraph iv iw hi ix b iy lc ja jb jc ld je jf jg le ji jj jk lf jm jn jo lg jq jr js hb bi translated"><a class="ae iu" rel="noopener" href="/@niranjankumarc"> Niranjan Kumar </a>是好事达印度公司的高级数据科学顾问。他对深度学习和人工智能充满热情。除了在媒体上写作，他还作为自由数据科学作家为Marktechpost.com写作。点击这里查看他的文章<a class="ae iu" href="https://www.marktechpost.com/author/niranjan-kumar/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="dc4b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可以在<a class="ae iu" href="https://www.linkedin.com/in/niranjankumar-c/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上与他联系，或者在<a class="ae iu" href="https://twitter.com/Nkumar_283" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上关注他，了解关于深度学习和机器学习的最新文章。</p><h2 id="1edb" class="lp kf hi bd kg mn mo mp kk mq mr ms ko jg mt mu ks jk mv mw kw jo mx my la mz bi translated">参考</h2><ul class=""><li id="bec0" class="nw nx hi ix b iy lc jc ld jg ny jk nz jo oa js ob oc od oe bi translated"><a class="ae iu" href="https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/" rel="noopener ugc nofollow" target="_blank">处理文本数据(使用Python)的终极指南——面向数据科学家和工程师</a></li><li id="83a7" class="nw nx hi ix b iy of jc og jg oh jk oi jo oj js ob oc od oe bi translated"><a class="ae iu" href="https://towardsdatascience.com/data-science-skills-web-scraping-using-python-d1a85ef607ed" rel="noopener" target="_blank">数据科学技能:使用python进行网页抓取</a></li><li id="fc8f" class="nw nx hi ix b iy of jc og jg oh jk oi jo oj js ob oc od oe bi translated"><a class="ae iu" href="https://towardsdatascience.com/data-science-skills-web-scraping-javascript-using-python-97a29738353f" rel="noopener" target="_blank">数据科学技能:使用python进行网页抓取javascript】</a></li></ul></div></div>    
</body>
</html>