<html>
<head>
<title>Creating Your Own Micro-Cluster</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">创建自己的微集群</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/creating-your-own-micro-cluster-5b75c1500ead?source=collection_archive---------8-----------------------#2020-01-08">https://medium.com/analytics-vidhya/creating-your-own-micro-cluster-5b75c1500ead?source=collection_archive---------8-----------------------#2020-01-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="fb8c" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用Docker，在纱线上试验Spark和Dask</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/869284fa2deae6364454458f1585b6b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uXC67m_cQRRZ7YPto2yXxQ.jpeg"/></div></div></figure><p id="9de4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在开始之前，我有一个简短的声明:这篇文章不是Docker教程，而是关于如何在你自己的机器上使用Docker创建集群的文章。如果你以前从未听说过它(这将是令人惊讶的)或者你从未有机会玩它，我强烈推荐你观看由<a class="ae kg" href="https://github.com/mmumshad" rel="noopener ugc nofollow" target="_blank"><em class="kf">Mumshad Mannambeth</em></a><em class="kf">制作的这个非常棒的入门</em> <a class="ae kg" href="https://www.youtube.com/watch?v=zJ6WbK9zFpI" rel="noopener ugc nofollow" target="_blank"> <em class="kf">视频</em> </a> <em class="kf">。我还做了一个非常简短一致的</em> <a class="ae kg" href="https://github.com/aminelemaizi/cheat-sheets/blob/master/Docker-Cheat-Sheet.md" rel="noopener ugc nofollow" target="_blank"> <em class="kf">小抄</em> </a> <em class="kf">你可以参考一下。</em></p><p id="b0a2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">除此之外，我假设你已经知道Dask、Spark、Yarn和Hadoop是什么了。同样，这不是一个介绍性的教程，而是一个“食谱”，可以这么说。</p><p id="f934" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><em class="kf">如果您是为了代码而来，并且想要在自己的机器上拥有一个交钥匙的集群，请不要犹豫，在</em><a class="ae kg" href="https://github.com/aminelemaizi/micro-cluster-lab" rel="noopener ugc nofollow" target="_blank"><em class="kf">my git repo</em></a><em class="kf">上使用我的代码。</em></p><p id="d3c3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">当第一次学习Spark或Dask时，我们通常在我们的本地机器上运行它们，这很方便，也很简单，但这远不是我们在许多节点上使用分布式计算的真实应用场景。有时我们只想在Spark或Dask上测试一两件事情，所以我们记得上次我们已经删除了独立的文件夹，或者我们已经丢弃了python虚拟环境，在那里我们已经很好地安装了Dask和Pyspark，重做配置是一件非常痛苦的事情。</p><p id="ebe8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">Docker是一个很好的工具，可以让我们的生活变得轻松一些。关于这一点，它可以在您自己的机器上模拟“真实生活”的环境，我们将利用它的操作系统级虚拟化能力，将Yarn、Spark和Dask放在1-Master 2-Slaves集群上(或多或少取决于您机器的能力)。</p><p id="4b82" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果您不太熟悉Docker，您的第一反应可能是问为什么不使用许多虚拟机来完成这项工作？！在我看来，这是一个有效的问题，对此我有两个答案:首先，Docker也进行虚拟化，但在低水平上，这意味着在资源消耗方面的高收益，更多的资源空间意味着更多的虚拟机(称为容器)，另外，Docker的虚拟化不是“目标”，而只是运行虚拟机上包含的应用程序的一种手段，这使我想到了我的第二点，即Docker是一种“平台即服务”， 或者用一种更酷的方式，PaaS，这意味着存在一个一致的API来运行我们的容器并毫不费力地配置它们。</p><h1 id="1174" class="kh ki hi bd kj kk kl km kn ko kp kq kr io ks ip kt ir ku is kv iu kw iv kx ky bi translated">集群架构</h1><p id="3f2f" class="pw-post-body-paragraph jj jk hi jl b jm kz ij jo jp la im jr js lb ju jv jw lc jy jz ka ld kc kd ke hb bi translated">我想象的架构是一个有三个节点的集群。一个是主节点，它不会做任何“工作”,但会作为其他节点的管理器，此外，它还会托管web接口(Jupyter笔记本和Yarn资源管理器UI ),我们将使用这些接口在集群上运行和检查我们的工作。另外两个节点是从节点(计算机科学有时可能很苛刻)，它们将完成繁重的工作，并执行任何我们希望它们做的计算-地图-冗余-机器-学习的事情。</p><p id="516d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如下图所示，Docker运行在主机(用不太时髦的话来说就是您自己的计算机)内部，并将运行与我之前讨论的三个节点相对应的三个容器。这种架构取决于您的资源(您的机器拥有的CPU数量和RAM数量)，如果您愿意，您可以增加到6个节点，如果您的资源有限，您可以减少到2个节点。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es le"><img src="../Images/81ba42ac7d5a8c18f3680b450cc9074f.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*srWtJ8-Oj1UgIqcB2kdG7w.png"/></div><figcaption class="lf lg et er es lh li bd b be z dx translated">一个1主2从架构，你可以看到集群被限制在Docker</figcaption></figure><h1 id="d9dc" class="kh ki hi bd kj kk kl km kn ko kp kq kr io ks ip kt ir ku is kv iu kw iv kx ky bi translated">要使用的文件树</h1><p id="dfd3" class="pw-post-body-paragraph jj jk hi jl b jm kz ij jo jp la im jr js lb ju jv jw lc jy jz ka ld kc kd ke hb bi translated">该<a class="ae kg" href="https://github.com/aminelemaizi/micro-cluster-lab" rel="noopener ugc nofollow" target="_blank">项目</a>将组织如下:</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="fa42" class="lo ki hi lk b fi lp lq l lr ls">├── docker-compose.yml<br/>├── Dockerfile<br/>├── confs<br/>│   ├── config<br/>│   ├── core-site.xml<br/>│   ├── hdfs-site.xml<br/>│   ├── mapred-site.xml<br/>│   ├── requirements.req<br/>│   ├── slaves<br/>│   ├── spark-defaults.conf<br/>│   └── yarn-site.xml<br/>├── datasets<br/>│   ├── alice_in_wonderland.txt<br/>│   └── iris.csv<br/>├── notebooks<br/>│   ├── Bash-Interface.ipynb<br/>│   ├── Dask-Yarn.ipynb<br/>│   ├── Python-Spark.ipynb<br/>│   └── Scala-Spark.ipynb<br/>└── script_files<br/>    └── bootstrap.sh</span></pre><ul class=""><li id="fa3c" class="lt lu hi jl b jm jn jp jq js lv jw lw ka lx ke ly lz ma mb bi translated"><strong class="jl hj"> docker-compose.yml </strong>:神奇的事情发生了，这个文件将包含docker用来创建整个集群、配置网络和为每个节点设置属性的所有指令集。</li><li id="d2b8" class="lt lu hi jl b jm mc jp md js me jw mf ka mg ke ly lz ma mb bi translated"><strong class="jl hj"> Dockerfile </strong>:这是Docker用来创建我们将用于容器的映像(OS +所需的应用程序)的文件。</li><li id="70ca" class="lt lu hi jl b jm mc jp md js me jw mf ka mg ke ly lz ma mb bi translated">这个文件夹将包含Hadoop、Yarn和Spark的配置文件以及python的需求。</li><li id="a527" class="lt lu hi jl b jm mc jp md js me jw mf ka mg ke ly lz ma mb bi translated"><strong class="jl hj"> datasets </strong>:该文件夹包含用于试验我们的集群的“starter”数据集。我选择了非常小的数据集来玩，记住这个集群不是为了“真正的”工作，而只是为了测试东西。</li><li id="197b" class="lt lu hi jl b jm mc jp md js me jw mf ka mg ke ly lz ma mb bi translated"><strong class="jl hj">笔记本</strong>:这些是笔记本“启动器”，每一个都包含启动我们的Scala-Spark集群的基本命令(你说的真正的Spark？！)、Python-Spark(对，我知道py Spark……)和Dask。除了这三个之外，还有一个运行Bash内核的笔记本，因此您可以拥有一个与命令shell的接口。</li><li id="8ab5" class="lt lu hi jl b jm mc jp md js me jw mf ka mg ke ly lz ma mb bi translated"><strong class="jl hj"> script_files </strong>:我会把所有要执行的bash脚本放在这里。现在，我只有一个名为<code class="du mh mi mj lk b">bootstrap.sh</code>的函数，它将在每次容器启动时执行。我将进一步解释它到底是干什么的。</li></ul><h1 id="4dc3" class="kh ki hi bd kj kk kl km kn ko kp kq kr io ks ip kt ir ku is kv iu kw iv kx ky bi translated">Hadoop、Yarn和Spark的资源和预设配置</h1><p id="ba4c" class="pw-post-body-paragraph jj jk hi jl b jm kz ij jo jp la im jr js lb ju jv jw lc jy jz ka ld kc kd ke hb bi translated">由于您拥有的资源，配置在这里非常重要。请记住，我们试图在一台机器上模拟许多节点，因此，如果您有例如4 GB的RAM和一个单独的CPU…我认为最好不要为此费心，坚持使用您机器上运行的任何东西。但是，如果您碰巧有16 GB的RAM和4个或更多的内核，这将使您能够模拟一个集群，如果您手头没有的话(即使8 GB也可以完成这项工作，但是您必须将自己限制在一个从节点上……)。</p><p id="26fa" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">也就是说，我将在这里介绍我用来运行这三个节点的默认配置，当然，您也可以更改它们，以适应您的需求和机器限制。</p><p id="94b4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><code class="du mh mi mj lk b">spark-defaults.conf</code>(火花)</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="433f" class="lo ki hi lk b fi lp lq l lr ls"># We inform Spark that it has to use Yarn<br/>spark.master                     yarn<br/># Client mode is important so we can interact with Spark using Jupyter<br/>spark.submit.deployMode          client<br/># Spark driver and executor memory<br/>spark.driver.memory              512m<br/>spark.executor.memory            512m<br/># This has to be lower than yarn.nodemanager.resource.memory-mb (inside yarn-site.xml)<br/>spark.yarn.am.memory             1G</span></pre><p id="c0f4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><code class="du mh mi mj lk b">slave</code> (Yarn/Hadoop)，这里我们定义了我们的托盘的名称，我们将进一步看到这些名称是在<code class="du mh mi mj lk b">docker-compose.yml</code>文件的层次上定义的。内部Docker DNS设法将这些名称与它们各自的机器相匹配。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="4cdb" class="lo ki hi lk b fi lp lq l lr ls">node-slave1<br/>node-slave2</span></pre><p id="d216" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><code class="du mh mi mj lk b">core-site.xml</code> (Yarn/Hadoop)，我们在这里定义主节点的名称(“主机”)和使用HDFS通信的端口。在这种情况下，我的主节点名称是<code class="du mh mi mj lk b">node-master</code>，HDFS端口是<code class="du mh mi mj lk b">9000</code>。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="7e8b" class="lo ki hi lk b fi lp lq l lr ls">&lt;?xml version="1.0" encoding="UTF-8"?&gt;<br/>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br/>​<br/>&lt;configuration&gt;<br/>        &lt;property&gt;<br/>            &lt;name&gt;fs.default.name&lt;/name&gt;<br/>            &lt;value&gt;hdfs://node-master:9000&lt;/value&gt;<br/>        &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="8025" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><code class="du mh mi mj lk b">hdfs-site.xml</code> (Hadoop)，我们在这里定义一些与HDFS系统相关的属性。我们已经将复制的数量(<code class="du mh mi mj lk b">dfs.replication</code>)设置为等于节点的数量，在本例中，2表示每个文件都将通过集群进行复制。除此之外，我们只是为namenode和datanode设置目录。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="3d65" class="lo ki hi lk b fi lp lq l lr ls">&lt;?xml version="1.0" encoding="UTF-8"?&gt;<br/>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br/>​<br/>&lt;configuration&gt;<br/>   &lt;property&gt;<br/>            &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;<br/>            &lt;value&gt;/opt/hadoop/data/nameNode&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>            &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;<br/>            &lt;value&gt;/opt/hadoop/data/dataNode&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>            &lt;name&gt;dfs.replication&lt;/name&gt;<br/>            &lt;value&gt;2&lt;/value&gt;<br/>    &lt;/property&gt;<br/>&lt;/configuration&gt;<br/>​</span></pre><p id="5567" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><code class="du mh mi mj lk b">yarn-site.xml</code> (Yarn)，这里我们设置Yarn的资源消耗，并指明谁是主节点。每个从机将只使用一个内核(<code class="du mh mi mj lk b">yarn.nodemanager.resource.cpu-vcores</code>)，最大内存为1536 MB ( <code class="du mh mi mj lk b">yarn.nodemanager.resource.memory-mb</code>)。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="7a3d" class="lo ki hi lk b fi lp lq l lr ls">&lt;?xml version="1.0"?&gt;<br/>&lt;configuration&gt;<br/>        &lt;property&gt;<br/>                &lt;name&gt;yarn.acl.enable&lt;/name&gt;<br/>                &lt;value&gt;0&lt;/value&gt;<br/>        &lt;/property&gt;<br/>        &lt;property&gt;<br/>                &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br/>                &lt;value&gt;node-master&lt;/value&gt;<br/>        &lt;/property&gt;<br/>        &lt;property&gt;<br/>                &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br/>                &lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br/>        &lt;/property&gt;<br/>        &lt;property&gt;<br/>                &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;<br/>                &lt;value&gt;1&lt;/value&gt;<br/>        &lt;/property&gt;<br/>        &lt;property&gt;<br/>                &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;<br/>                &lt;value&gt;1536&lt;/value&gt;<br/>        &lt;/property&gt;<br/>        &lt;property&gt;<br/>                &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;<br/>                &lt;value&gt;256&lt;/value&gt;<br/>        &lt;/property&gt;<br/>        &lt;property&gt;<br/>                &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;<br/>                &lt;value&gt;1536&lt;/value&gt;<br/>        &lt;/property&gt;<br/>        &lt;property&gt;<br/>                &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;<br/>                &lt;value&gt;false&lt;/value&gt;<br/>        &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="5629" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">(Yarn/Hadoop)，这次是我们正在配置的map和reduce处理。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="6967" class="lo ki hi lk b fi lp lq l lr ls">&lt;?xml version="1.0"?&gt;<br/>&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;<br/>​<br/>&lt;configuration&gt;<br/>  &lt;property&gt;<br/>      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br/>      &lt;value&gt;yarn&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>      &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;<br/>      &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>      &lt;name&gt;mapreduce.map.env&lt;/name&gt;<br/>      &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>      &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;<br/>      &lt;value&gt;HADOOP_MAPRED_HOME=$HADOOP_HOME&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>      &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt;<br/>      &lt;value&gt;1536&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>      &lt;name&gt;yarn.app.mapreduce.am.command-opts&lt;/name&gt;<br/>      &lt;value&gt;400&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>      &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;<br/>      &lt;value&gt;256&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>      &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;<br/>      &lt;value&gt;256&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>      &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;<br/>      &lt;value&gt;200&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>      &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;<br/>      &lt;value&gt;400&lt;/value&gt;<br/>  &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="4ffc" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果你不知道如何选择与你的资源相匹配的正确值，Cloudera有<a class="ae kg" href="https://docs.cloudera.com/HDPDocuments/HDP2/HDP-2.6.5/bk_command-line-installation/content/determine-hdp-memory-config.html" rel="noopener ugc nofollow" target="_blank">这篇文章</a>解释了一些要使用的启发法。</p><h1 id="05b9" class="kh ki hi bd kj kk kl km kn ko kp kq kr io ks ip kt ir ku is kv iu kw iv kx ky bi translated">创建基础映像</h1><p id="1c93" class="pw-post-body-paragraph jj jk hi jl b jm kz ij jo jp la im jr js lb ju jv jw lc jy jz ka ld kc kd ke hb bi translated">现在，我们需要一个特殊的“发行版”(在Docker行话中称为映像)，我们将在我们的三个容器上运行它，它必须包括必要的应用程序和配置，以便我们可以有一个包含Python、Spark、Hadoop &amp; Yarn和所有朋友的小可爱操作系统！</p><p id="baef" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">为此，我们将把必要的指令放在名为<code class="du mh mi mj lk b">Dockerfile</code>的文件中。我们首先指定要使用的基本操作系统/映像，在我们的例子中是Ubuntu Bionic Beaver，也称为Ubuntu 18.04 LTS。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="471e" class="lo ki hi lk b fi lp lq l lr ls">FROM ubuntu:bionic</span></pre><p id="79f8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">之后，我们将为(将要)运行的会话定义所有必要的环境变量。注释解释了每一行的作用。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="422a" class="lo ki hi lk b fi lp lq l lr ls"># showing to hadoop and spark where to find java!<br/>ENV JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-amd64/jre<br/>​<br/># after downloading hadoop (a bit further) we have to inform any concerned<br/># app where to find it<br/>ENV HADOOP_HOME /opt/hadoop<br/>​<br/># same for the hadoop configuration<br/>ENV HADOOP_CONF_DIR /opt/hadoop/etc/hadoop<br/>​<br/># and same for spark<br/>ENV SPARK_HOME /opt/spark<br/>​<br/># with this we can run all hadoop and spark scripts and commands directly from the shell<br/># without using the absolute path<br/>ENV PATH="${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}"<br/>​<br/># just informing the hadoop version, this isn't really necessary<br/>ENV HADOOP_VERSION 2.7.0<br/>​<br/># if you happend to run pyspark from shell, it will launch it on a Jupyter Notebook<br/># this is just two fancy lines, really no need for it<br/>ENV PYSPARK_DRIVER_PYTHON=jupyter<br/>ENV PYSPARK_DRIVER_PYTHON_OPTS='notebook'<br/>​<br/># showing pyspark which "python" command to use<br/>ENV PYSPARK_PYTHON=python3</span></pre><p id="c00c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们现在将更新操作系统；安装Java、SSH服务器和Python(包括pip和其他依赖项)。你也可以看到我们已经安装了wget(从URL下载)和nano(访问和修改文件)，这可能看起来很奇怪，但Docker <code class="du mh mi mj lk b">ubuntu:bionic</code>使用的基本发行版非常非常精简，只包含定义发行版本身所必需的内容。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="5c3d" class="lo ki hi lk b fi lp lq l lr ls">RUN apt-get update &amp;&amp; \<br/>    apt-get install -y wget nano openjdk-8-jdk ssh openssh-server<br/>RUN apt update &amp;&amp; apt install -y python3 python3-pip python3-dev build-essential libssl-dev libffi-dev libpq-dev</span></pre><p id="de1b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，我们将复制映像中的需求文件，然后将它们安装在Python上。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="5d1e" class="lo ki hi lk b fi lp lq l lr ls">COPY /confs/requirements.req /<br/>RUN pip3 install -r requirements.req<br/>RUN pip3 install dask[bag] --upgrade<br/>RUN pip3 install --upgrade toree<br/>RUN python3 -m bash_kernel.install</span></pre><p id="8bfb" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">是时候下载Hadoop和Spark了，并将它们分别提取到特定的文件夹中。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="5d2a" class="lo ki hi lk b fi lp lq l lr ls">RUN wget -P /tmp/ <a class="ae kg" href="https://archive.apache.org/dist/hadoop/common/hadoop-2.7.0/hadoop-2.7.0.tar.gz" rel="noopener ugc nofollow" target="_blank">https://archive.apache.org/dist/hadoop/common/hadoop-2.7.0/hadoop-2.7.0.tar.gz</a><br/>RUN tar xvf /tmp/hadoop-2.7.0.tar.gz -C /tmp &amp;&amp; \<br/>  mv /tmp/hadoop-2.7.0 /opt/hadoop<br/>​<br/>RUN wget -P /tmp/ <a class="ae kg" href="http://mirror.ibcp.fr/pub/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">http://mirror.ibcp.fr/pub/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz</a><br/>RUN tar xvf /tmp/spark-2.4.4-bin-hadoop2.7.tgz -C /tmp &amp;&amp; \<br/>    mv /tmp/spark-2.4.4-bin-hadoop2.7 ${SPARK_HOME}</span></pre><p id="8389" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们需要生成一个SSH密钥，以允许Hadoop和Spark所期望的容器之间的通信。只是为了澄清一些事情，安装Hadoop是运行Yarn和使用HDFS所必需的。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="44e8" class="lo ki hi lk b fi lp lq l lr ls">RUN ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa &amp;&amp; \<br/>  cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys &amp;&amp; \<br/>  chmod 600 ~/.ssh/authorized_keys<br/>COPY /confs/config /root/.ssh<br/>RUN chmod 600 /root/.ssh/config</span></pre><p id="689e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，我们将复制预设配置和<code class="du mh mi mj lk b">ENTRYPOINT</code>脚本(每次运行包含该图像的容器时将执行的脚本)。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="76c6" class="lo ki hi lk b fi lp lq l lr ls">COPY /confs/*.xml /opt/hadoop/etc/hadoop/<br/>COPY /confs/slaves /opt/hadoop/etc/hadoop/<br/>COPY /script_files/bootstrap.sh /<br/>COPY /confs/spark-defaults.conf ${SPARK_HOME}/conf</span></pre><p id="36a6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">公开端口以便容器可以监听它们，默认情况下它是一个TCP端口。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="b1ca" class="lo ki hi lk b fi lp lq l lr ls">EXPOSE 9000<br/>EXPOSE 7077<br/>EXPOSE 4040<br/>EXPOSE 8020<br/>EXPOSE 22</span></pre><p id="8d48" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">最后，我们将复制启动器(笔记本和数据集)并指定命令作为我们映像的<code class="du mh mi mj lk b">ENTRYPOINT</code>运行，在我们的例子中，它只是运行<code class="du mh mi mj lk b">bootstrap.sh</code>脚本。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="ba82" class="lo ki hi lk b fi lp lq l lr ls">RUN mkdir lab<br/>COPY notebooks/*.ipynb /root/lab/<br/>COPY datasets /root/lab/datasets<br/>​<br/>ENTRYPOINT ["/bin/bash", "bootstrap.sh"]</span></pre><p id="e04f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><code class="du mh mi mj lk b">bootstrap.sh</code>脚本做了四件事:</p><ul class=""><li id="13a9" class="lt lu hi jl b jm jn jp jq js lv jw lw ka lx ke ly lz ma mb bi translated">格式化namenode以支持HDFS。</li><li id="0eb2" class="lt lu hi jl b jm mc jp md js me jw mf ka mg ke ly lz ma mb bi translated">运行SSH客户端。</li><li id="ea32" class="lt lu hi jl b jm mc jp md js me jw mf ka mg ke ly lz ma mb bi translated">检查我们是否在主节点，如果是，它将启动Hadoop、Yarn并运行Jupypter(正确，因此不需要令牌或密码)。</li><li id="f27e" class="lt lu hi jl b jm mc jp md js me jw mf ka mg ke ly lz ma mb bi translated">启动一个空循环，这样我们的节点就可以在不终止容器的情况下重新进入。</li></ul><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="4ce7" class="lo ki hi lk b fi lp lq l lr ls">#!/bin/bash<br/>​<br/>hdfs namenode -format<br/>service ssh start<br/>if [ "$HOSTNAME" = node-master ]; then<br/>    start-dfs.sh<br/>    start-yarn.sh<br/>    cd /root/lab<br/>    jupyter trust Bash-Interface.ipynb<br/>    jupyter trust Dask-Yarn.ipynb<br/>    jupyter trust Python-Spark.ipynb<br/>    jupyter trust Scala-Spark.ipynb<br/>    jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token='' --NotebookApp.password='' &amp;<br/>fi<br/>​<br/>while :; do :; done &amp; kill -STOP $! &amp;&amp; wait $!</span></pre><p id="1de4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">使用所有这些块，最终的<code class="du mh mi mj lk b">Dockerfile</code>必须是这样的:</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="239f" class="lo ki hi lk b fi lp lq l lr ls">FROM ubuntu:bionic<br/>​<br/>ENV JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-amd64/jre<br/>ENV HADOOP_HOME /opt/hadoop<br/>ENV HADOOP_CONF_DIR /opt/hadoop/etc/hadoop<br/>ENV SPARK_HOME /opt/spark<br/>ENV PATH="${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin:${PATH}"<br/>ENV HADOOP_VERSION 2.7.0<br/>ENV PYSPARK_DRIVER_PYTHON=jupyter<br/>ENV PYSPARK_DRIVER_PYTHON_OPTS='notebook'<br/>ENV PYSPARK_PYTHON=python3<br/>​<br/>RUN apt-get update &amp;&amp; \<br/>    apt-get install -y wget nano openjdk-8-jdk ssh openssh-server<br/>RUN apt update &amp;&amp; apt install -y python3 python3-pip python3-dev build-essential libssl-dev libffi-dev libpq-dev<br/>​<br/>COPY /confs/requirements.req /<br/>RUN pip3 install -r requirements.req<br/>RUN pip3 install dask[bag] --upgrade<br/>RUN pip3 install --upgrade toree<br/>RUN python3 -m bash_kernel.install<br/>​<br/>RUN wget -P /tmp/ <a class="ae kg" href="https://archive.apache.org/dist/hadoop/common/hadoop-2.7.0/hadoop-2.7.0.tar.gz" rel="noopener ugc nofollow" target="_blank">https://archive.apache.org/dist/hadoop/common/hadoop-2.7.0/hadoop-2.7.0.tar.gz</a><br/>RUN tar xvf /tmp/hadoop-2.7.0.tar.gz -C /tmp &amp;&amp; \<br/>  mv /tmp/hadoop-2.7.0 /opt/hadoop<br/>​<br/>RUN wget -P /tmp/ <a class="ae kg" href="http://mirror.ibcp.fr/pub/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">http://mirror.ibcp.fr/pub/apache/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz</a><br/>RUN tar xvf /tmp/spark-2.4.4-bin-hadoop2.7.tgz -C /tmp &amp;&amp; \<br/>    mv /tmp/spark-2.4.4-bin-hadoop2.7 ${SPARK_HOME}<br/>​<br/>RUN ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa &amp;&amp; \<br/>  cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys &amp;&amp; \<br/>  chmod 600 ~/.ssh/authorized_keys<br/>COPY /confs/config /root/.ssh<br/>RUN chmod 600 /root/.ssh/config<br/>​<br/>COPY /confs/*.xml /opt/hadoop/etc/hadoop/<br/>COPY /confs/slaves /opt/hadoop/etc/hadoop/<br/>COPY /script_files/bootstrap.sh /<br/>COPY /confs/spark-defaults.conf ${SPARK_HOME}/conf<br/>​<br/>RUN jupyter toree install --spark_home=${SPARK_HOME}<br/>RUN echo "export JAVA_HOME=${JAVA_HOME}" &gt;&gt; /etc/environment<br/>​<br/>EXPOSE 9000<br/>EXPOSE 7077<br/>EXPOSE 4040<br/>EXPOSE 8020<br/>EXPOSE 22<br/>​<br/>RUN mkdir lab<br/>COPY notebooks/*.ipynb /root/lab/<br/>COPY datasets /root/lab/datasets<br/>​<br/>ENTRYPOINT ["/bin/bash", "bootstrap.sh"]</span></pre><p id="4a8a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在我们需要构建我们的映像，为此我们只需在与<code class="du mh mi mj lk b">Dockerfile</code>相同的文件夹级别运行以下命令:</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="0814" class="lo ki hi lk b fi lp lq l lr ls">docker build . -t cluster-base</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mk"><img src="../Images/50d551b46b72036cb2656725bf50e5a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rZubMb8MADBxmdgnf-6SVQ.png"/></div></div></figure><p id="bb8b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这里使用的图像名(<code class="du mh mi mj lk b">cluster-base</code>)与<code class="du mh mi mj lk b">docker-compose.yml</code>文件中指定的图像名匹配非常重要。</p><h1 id="8bdf" class="kh ki hi bd kj kk kl km kn ko kp kq kr io ks ip kt ir ku is kv iu kw iv kx ky bi translated">Docker作曲或如何让集群活起来</h1><p id="d6b4" class="pw-post-body-paragraph jj jk hi jl b jm kz ij jo jp la im jr js lb ju jv jw lc jy jz ka ld kc kd ke hb bi translated">Docker Compose不需要单独启动和配置每个节点，而是通过指定所有节点的特征来实现这一点，我所说的特征是指要使用的映像、要设置的主机名、要加入的网络以及您可以使用Docker命令行或多或少记下的任何内容。</p><p id="ab97" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">下面是我用过的YAML文件，我会给它添加一些注释，这样你就能理解它的作用了。</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="0780" class="lo ki hi lk b fi lp lq l lr ls">version: "3.3"<br/># Here we will list all nodes/containers!<br/>services:<br/># First container will be named node-master<br/>  node-master:<br/># We will tell Docker to run the slaves first before running the Master Node<br/>    depends_on:<br/>      - node-slave1<br/>      - node-slave2<br/># The image to be used is the image we've built before<br/>    image: cluster-base:latest<br/># It explains itself, what it the container name<br/>    container_name: node-master<br/># The hostname for this container<br/>    hostname: node-master<br/># Connecting your terminal to the stdin and stdout stream of the container so you can see<br/># all the messages it outputs<br/>    tty: true<br/># Ports redirection to Host<br/>    ports:<br/>      - "8088:8088"<br/>      - "50070:50070"<br/>      - "50030:50030"<br/>      - "8080:8080"<br/>      - "8042:8042"<br/>      - "8888:8888"<br/># Linking this container to the following network and defining an alias to be used<br/>    networks:<br/>      cluster-network:<br/>        aliases: <br/>          - node-master<br/># First Slave Node, same thing as previously seen<br/>  node-slave1:<br/>    image: cluster-base:latest<br/>    container_name: node-slave1<br/>    hostname: node-slave1<br/>    tty: true<br/>    ports:<br/>      - "7042:8042"<br/>    networks:<br/>      cluster-network:<br/>        aliases: <br/>          - node-slave1<br/># Second Slave Node<br/>  node-slave2:<br/>    image: cluster-base:latest<br/>    container_name: node-slave2<br/>    hostname: node-slave2<br/>    tty: true<br/>    ports:<br/>      - "9042:8042"<br/>    networks:<br/>      cluster-network:<br/>        aliases: <br/>          - node-slave2<br/># Defining a new network of Bridge type to be created<br/>networks:<br/>  cluster-network:<br/>    driver: bridge<br/>    ipam:<br/>      driver: default</span></pre><p id="6ccb" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">要运行集群并启动它，现在只需要在与<code class="du mh mi mj lk b">docker-compose.yml</code>相同的文件夹级别运行下面的命令:</p><pre class="iy iz ja jb fd lj lk ll lm aw ln bi"><span id="ba51" class="lo ki hi lk b fi lp lq l lr ls">docker-compose up</span></pre><p id="5579" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，您的集群已经启动并运行，您的shell上应该会显示如下内容:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mk"><img src="../Images/c69504c49665898c2af0447f98c4354f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zVNfde9LXFFnNkinALUj-Q.png"/></div></div></figure><p id="a004" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">你甚至可以访问Yarn resource manager UI来查看集群的从属节点部分(<code class="du mh mi mj lk b">localhost:8088/cluster/nodes</code>)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ml"><img src="../Images/2f70ffa7a404643d49bbbbd0b24d7d70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KAe4i0qtMb-XKu2WmcDFmA.png"/></div></div></figure><p id="9531" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在您可以在<code class="du mh mi mj lk b">localhost:8888</code>访问Jupyter笔记本。您可以使用入门笔记本，开始使用Spark (Scala或Python)和Dask:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ml"><img src="../Images/49c9adcf9f6a7248676803d3b329cb0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aa_6fIpl6lq2-AOuzWo5Pg.png"/></div></div></figure><p id="d5b3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">要停止集群，使用<code class="du mh mi mj lk b">Ctrl</code> + <code class="du mh mi mj lk b">C</code>，然后运行命令<code class="du mh mi mj lk b">docker-compose down</code>来完全删除容器。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mk"><img src="../Images/b997f625a39d1fe6c3d81bdfe8faa6c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CHJpU4-oTGUJuYEh1XqRxQ.png"/></div></div></figure><p id="9a1c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">就这样了，伙计们！我希望这篇文章对你有点用。</p><p id="d480" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">您可以关注我的<a class="ae kg" href="https://twitter.com/ALemaizi" rel="noopener ugc nofollow" target="_blank"> Twitter </a>以便在发布新帖子时得到通知！</p></div></div>    
</body>
</html>