<html>
<head>
<title>Random Forest — A Concise Technical Overview</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机森林—简明的技术概述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/random-forest-a-concise-technical-overview-cd1c5b565fa5?source=collection_archive---------14-----------------------#2020-08-18">https://medium.com/analytics-vidhya/random-forest-a-concise-technical-overview-cd1c5b565fa5?source=collection_archive---------14-----------------------#2020-08-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f48b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林是最流行和最强大的机器学习算法之一。它是可以用于分类和回归任务的算法之一，因此，它是机器学习领域中使用最多的算法之一。</p><p id="0731" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随机森林是一种监督学习算法。那么‘随机森林’到底是什么？顾名思义，这种算法创建了一个有许多树的“森林”。该算法的基本逻辑是在森林中有更多的树以产生高精度的结果。简而言之，随机森林构建多个(集合)决策树，并将它们合并在一起，以实现准确稳定的预测。</p><p id="9c1d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">构建的决策树不仅仅是简单的决策树，而是<em class="jd">袋装决策树</em>，它们在每次分裂时被分裂为特征的<em class="jd">子集。现在，越来越有趣了，不是吗？</em></p><p id="af07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们了解一下什么是袋装树。但在此之前，我们需要了解什么是<strong class="ih hj">自举。</strong></p><p id="5282" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Bootstrapping 是一种强大的统计方法，用于从数据样本(如均值)中估计值。例如，对数据进行采样并计算每个样本的平均值，以对所有计算的平均值进行平均，从而找到真实平均值的更好估计。在 bagging 技术中，使用了类似的方法，但是我们不是估计一个值，而是估计整个统计模型(决策树)。训练集被分成多个样本，并为每个样本构建模型。此外，每个这样的模型对新数据进行预测，并且这些预测被平均以给出真实输出值的更好估计。</p><p id="01f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常单个决策树会遭受过度拟合(高方差)，但是通过 bagging 技术，我们在预测过程中引入了许多决策树。这将弱学习者和强学习者结合起来，从而平均方差(消除过度拟合的风险)！</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/43c1dbf4eb9e019ddbb2b5089809f9e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YaG6aqXK0e3xWcrdHLxrSg.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">来源:谷歌图片</figcaption></figure><p id="ccaf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于 bagging 技术，随机森林的性能更好，因为它通过分割随机特征子集来解除树的相关性。在每次分割时，模型只考虑一小部分要素，而不是所有要素。这种在每次分割时随机选择较少特征的方法降低了方差。如果数据集包含几个强预测值，这些预测值将被一致地选择在树的顶部，从而形成非常相似的结构化树。因此，袋装树防止了少数选择强预测因子的支配，并在预测因子选择中引入随机性。</p><p id="f7fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看随机森林伪代码是如何进行预测的。</p><p id="1e5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤 1:采用<strong class="ih hj">测试特性</strong>，并使用每个随机创建的决策树的规则来预测结果，并将预测的结果存储为目标。</p><p id="b928" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二步:计算每个预测目标的<strong class="ih hj">票数</strong>。</p><p id="8ec1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第三步:将<strong class="ih hj">最高票</strong>预测目标作为随机森林算法的<strong class="ih hj">最终预测</strong>。</p><p id="0213" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了使用经过训练的随机森林算法来执行预测，我们需要通过每个随机创建的树的规则来传递测试特征。假设我们组成了 100 棵随机决策树。</p><p id="0c96" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个树将预测相同测试特征的不同目标(结果)。然后通过考虑每个预测目标，计算票数。假设 100 个随机决策树预测 3 个唯一目标<strong class="ih hj"> x，y，z </strong>，那么支持 x 的票数是 100 个随机决策树中有多少个树预测目标为 x. <strong class="ih hj"> </strong>同样，我们将计算其他 2 个目标- y 和 z 的票数。例如，如果从<strong class="ih hj"> 100 个</strong>随机决策树中，<strong class="ih hj"> 60 个</strong>树预测目标为<strong class="ih hj"> x </strong>，那么最后的随机树返回【T14</p><p id="726e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优点:</strong></p><ol class=""><li id="8564" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">即使存在不一致的数据，随机森林也能保持准确性。</li><li id="993f" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">非常方便和易于使用，因为它的默认超参数经常产生良好的预测输出。</li><li id="c5c1" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">它有平衡类总体不平衡数据集中的误差的方法。随机森林试图最小化整体错误率，因此当我们有一个不平衡的数据集时，较大的类将获得较低的错误率，而较小的类将具有较大的错误率。</li><li id="178e" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">随机森林对异常值和非线性数据不敏感。</li></ol><p id="8bd0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">局限性:</strong></p><ol class=""><li id="d705" class="ju jv hi ih b ii ij im in iq jw iu jx iy jy jc jz ka kb kc bi translated">就大型数据集而言，树的大小会使处理内存过载。</li><li id="b95d" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">解释是棘手的，因为随机森林就像黑盒，因此可能在很大程度上损害模型解释。</li><li id="c5c8" class="ju jv hi ih b ii kd im ke iq kf iu kg iy kh jc jz ka kb kc bi translated">它是一个预测性的建模工具，而不是描述性的工具。如果目标是找到数据中关系的描述，那么其他方法将是首选。</li></ol></div></div>    
</body>
</html>