<html>
<head>
<title>Imitation Learning: From Why to How!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">模仿学习:从为什么到如何！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/imitation-learning-from-why-to-how-7b713a079501?source=collection_archive---------9-----------------------#2020-05-05">https://medium.com/analytics-vidhya/imitation-learning-from-why-to-how-7b713a079501?source=collection_archive---------9-----------------------#2020-05-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/c672816267e0ae793b4e32e4559f5fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*6b-upgABCYURidOqeNjV4w.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><a class="ae iq" href="http://modules.ilabs.uw.edu/module/power-learning-imitation/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="6553" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">裘德是一名业余足球运动员，但他想成为一名伟大的运动员。裘德是克里斯蒂亚诺·罗纳尔多的铁杆粉丝，每次他在任何地方比赛，他都会观看。</p><p id="168d" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">裘德想知道他是否能像罗纳尔多一样。如果通过大量的练习，他可以<strong class="it hj">模仿</strong>罗纳尔多所拥有的所有技能。他不在乎在模仿时他是否进球，他只是像罗纳尔多那样踢球。</p><blockquote class="jp jq jr"><p id="87c8" class="ir is js it b iu iv iw ix iy iz ja jb jt jd je jf ju jh ji jj jv jl jm jn jo hb bi translated">如果我能像罗纳尔多一样踢球，那肯定会是一个进球！</p></blockquote><p id="d895" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">裘德对这个想法很着迷，但他也认为<strong class="it hj">太懒</strong>和<strong class="it hj">喜欢编程</strong>(永恒的结合)。他想为什么他不能在虚拟世界中做一些非常类似的事情，毕竟他可以在控制这个虚拟世界的同时快进这个训练过程。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es jw"><img src="../Images/d13178b99e5be0f8593965d5a728bd01.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*2aenqiOV1BlDE8_7kDJhng.jpeg"/></div><figcaption class="im in et er es io ip bd b be z dx translated"><a class="ae iq" href="https://hub.packtpub.com/google-research-football-environment-a-reinforcement-learning-environment-for-ai-agents-to-master-football/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="5cc7" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">裘德偶然发现了他正在寻找的东西！</p><h1 id="5c7c" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">模仿学习</h1><p id="7d03" class="pw-post-body-paragraph ir is hi it b iu kz iw ix iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo hb bi translated">继续这个故事，因为他经常观察罗纳尔多，他知道在许多情况下他会怎么做。比方说，裘德已经练习过，并且能够在完全相同的情况下模仿罗纳尔多。</p><p id="4913" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">由于罗纳尔多(或任何足球运动员)可能遇到的情况是无限的，所以无论他多么努力，裘德也无法了解所有的情况！由于裘德只是在复制罗纳尔多，当未知的情况出现时，他什么也没学到。</p><p id="296d" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">此外，因为裘德和我们一样也是人，所以没有什么像<strong class="it hj">和</strong>罗纳尔多一样。无论他多么努力，每次都会有一些错误！</p><p id="497a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们将在后面看到上述情况的数学投影。</p><blockquote class="jp jq jr"><p id="62b4" class="ir is js it b iu iv iw ix iy iz ja jb jt jd je jf ju jh ji jj jv jl jm jn jo hb bi translated">我们可以是错的，或者我们可以知道，但我们不能两者同时发生</p></blockquote><p id="c306" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">如果你理解了裘德，你就得到了模仿学习！😃</p><h2 id="3297" class="le kc hi bd kd lf lg lh kh li lj lk kl jc ll lm kp jg ln lo kt jk lp lq kx lr bi translated">不同的学习范式</h2><p id="7774" class="pw-post-body-paragraph ir is hi it b iu kz iw ix iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo hb bi translated">让我们快速定义机器学习的三个垂直领域，这将有助于定义我们的问题陈述。</p><ul class=""><li id="7560" class="ls lt hi it b iu iv iy iz jc lu jg lv jk lw jo lx ly lz ma bi translated">监督学习:基本上找到Xs和Ys，定义一个模型，拟合一个模型预测Ys。</li><li id="4b6b" class="ls lt hi it b iu mb iy mc jc md jg me jk mf jo lx ly lz ma bi translated">无监督学习:我们希望内在地学习数据的结构，而不需要明确地使用标签。</li><li id="f402" class="ls lt hi it b iu mb iy mc jc md jg me jk mf jo lx ly lz ma bi translated">强化学习:它是关于采取适当的行动，以便在一个环境中获得最大的回报。</li></ul><h2 id="a487" class="le kc hi bd kd lf lg lh kh li lj lk kl jc ll lm kp jg ln lo kt jk lp lq kx lr bi translated">为什么要模仿学习呢？</h2><p id="ade0" class="pw-post-body-paragraph ir is hi it b iu kz iw ix iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo hb bi translated">现在考虑一下，Jude现在不会使用模仿学习，他想在每种情况下训练自己，目标是使用强化学习射门。</p><p id="cf07" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在RL设置中，射门的奖励是<strong class="it hj"> +1 </strong>，休息的奖励是<strong class="it hj"> 0 </strong>。如你所知，这种奖励在本质上是非常稀少的。可能有一些时候代理人(在我们的例子中是裘德！)会得到积极的回报来学习。</p><blockquote class="mg"><p id="1734" class="mh mi hi bd mj mk ml mm mn mo mp jo dx translated"><em class="mq">奖励稀少是阻碍我们击败《蒙特祖马的复仇》的问题之一，这是一款众所周知的艰苦的Atari 2600游戏，至今尚未被破解。</em></p></blockquote><p id="06d1" class="pw-post-body-paragraph ir is hi it b iu mr iw ix iy ms ja jb jc mt je jf jg mu ji jj jk mv jm jn jo hb bi translated">另一个问题是我们上面讨论过的，我们需要虚拟模拟器来加快这个学习过程，但是如果我们没有虚拟模拟器或者我们想在现实生活中学习一些东西怎么办？</p><p id="eac0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">模仿学习有可能解决这些问题以及其他一些问题。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/654b0a77ce6401d860e87567f600d50b.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/1*7yX7EmQhN4Esd7_jcThvbg.gif"/></div><figcaption class="im in et er es io ip bd b be z dx translated">虚拟人形试图模仿人类奔跑(<a class="ae iq" href="https://gfycat.com/discover/siggraph-gifs" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><h2 id="13fc" class="le kc hi bd kd lf lg lh kh li lj lk kl jc ll lm kp jg ln lo kt jk lp lq kx lr bi translated">如何模仿学习？</h2><p id="23cc" class="pw-post-body-paragraph ir is hi it b iu kz iw ix iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo hb bi translated">一般来说，当专家更容易证明期望的行为时，模仿学习是有用的，这样我们就不必指定任何奖励函数。</p><p id="a514" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">假设我们收集专家演示(也称为RL中的轨迹)</p><p id="01a0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">τ = (s0，a0，s1，a1…..)其中动作(As)基于专家的策略(比如人脑)。在某些情况下，我们可能会在培训期间要求“<strong class="it hj">专家</strong>”。</p><p id="fb77" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">一旦我们得到了这个轨迹，我们将这个时间步骤切片，得到一对s和a。</p><p id="ec6b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">然后，我们将这些对视为独立样本，并应用<strong class="it hj">监督学习</strong>。</p><p id="d850" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在这种学习中改变损失函数和优化策略定义了各种模仿学习算法。让我们看看它们中的基本的。</p><h1 id="ddaf" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">行为克隆</h1><p id="6708" class="pw-post-body-paragraph ir is hi it b iu kz iw ix iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo hb bi translated">这是模仿学习的最简单的形式，你把这些对(Ss，As)作为i.i.d的例子，应用简单的监督学习。我们可以根据状态和动作空间的复杂性选择基于树的模型(随机森林、梯度推进)或神经网络。</p><h2 id="dce9" class="le kc hi bd kd lf lg lh kh li lj lk kl jc ll lm kp jg ln lo kt jk lp lq kx lr bi translated">说够了，让我们来看看实施吧！</h2><p id="0d5c" class="pw-post-body-paragraph ir is hi it b iu kz iw ix iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo hb bi translated">你可以去这个文件夹:</p><div class="mx my ez fb mz na"><a href="https://github.com/Shivanshmundra/reinforcement_learning_beginner/tree/master/imitation_learning_bc" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab dw"><div class="nc ab nd cl cj ne"><h2 class="bd hj fi z dy nf ea eb ng ed ef hh bi translated">shivanshmundra/强化_学习_初学者</h2><div class="nh l"><h3 class="bd b fi z dy nf ea eb ng ed ef dx translated">在这里，您可以运行expert_recorder.py来记录专家演示，专家就是您！。现在它使用…</h3></div><div class="ni l"><p class="bd b fp z dy nf ea eb ng ed ef dx translated">github.com</p></div></div><div class="nj l"><div class="nk l nl nm nn nj no ik na"/></div></div></a></div><p id="f4ba" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在这里，您可以运行<code class="du np nq nr ns b">expert_recorder.py</code>来记录专家演示，专家就是您！。现在它使用<code class="du np nq nr ns b">MountainCar-v0</code>一个你只有两个动作的环境——左或右，来爬山。您可以分别使用左右<code class="du np nq nr ns b">a</code>和<code class="du np nq nr ns b">d</code>进行引导。你也可以根据自己的心情改变环境！</p><p id="9fd2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">一旦您创建了“专家演示”，您就可以建立一个模型，并根据收集的数据进行训练。然后，经过训练的策略可以用于在相同的环境中进行测试。</p><p id="a102" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">详细说明在文件夹的<code class="du np nq nr ns b">README</code>中。另外，我借用了这段代码，所以要很好地理解代码，你可以看一下<a class="ae iq" href="https://www.youtube.com/watch?v=0rsrDOXsSeM&amp;feature=youtu.be&amp;t=1370" rel="noopener ugc nofollow" target="_blank">这段</a>视频。</p><p id="26ff" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">一旦经过训练，它看起来就像这样:</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es nt"><img src="../Images/c53bfc0c79972f1e13b91332fab5465f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*Q1d3QAY4SmskTpIGs5NKgg.gif"/></div><figcaption class="im in et er es io ip bd b be z dx translated">使用行为克隆训练山地车</figcaption></figure><h2 id="1a3c" class="le kc hi bd kd lf lg lh kh li lj lk kl jc ll lm kp jg ln lo kt jk lp lq kx lr bi translated">缺点</h2><p id="60ee" class="pw-post-body-paragraph ir is hi it b iu kz iw ix iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo hb bi translated">在某些情况下，行为克隆可以很好地发挥作用，尤其是在状态数量有限且一段时间很短的情况下。</p><p id="19bc" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">行为克隆通常不工作的主要原因是我们在上面的监督学习中采用的i.i.d .假设，而在像MDP(马尔可夫决策过程)这样的真实场景中，一个动作引发了下一个状态，这打破了i.i.d .假设。</p><p id="c022" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">此外，正如你所知，这种算法只在给定的状态下训练，因为我们之前说过，没有任何机器学习算法是100%准确的，在某个时间步长<em class="js"> t </em>上有可能出现错误，并且这种错误将随着<em class="js"> t </em>而不断增加。随着每个错误的决策，代理可能会陷入“更加未知”的状态，从而容易犯更多的错误。</p><p id="ba0b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在上述情况下，BC行为是未定义的，可能会导致灾难性的故障。</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="er es nu"><img src="../Images/e3205824b7e12ca96bc0e693e7fd16df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*Ewp0TqY9ymmvv37DGMoONA.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated"><em class="mq">来源:</em><a class="ae iq" href="https://web.stanford.edu/class/cs234/slides/lecture7.pdf" rel="noopener ugc nofollow" target="_blank"><em class="mq">https://web.stanford.edu/class/cs234/slides/lecture7.pdf</em></a></figcaption></figure><p id="5a56" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">DAgger(数据集聚合)算法</p><p id="2b1e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这基本上是我们上面讨论的行为克隆算法的改进版本。</p><p id="25fa" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">BC的主要缺点是用于训练的数据是静态的，并且代理可能会导致未知的状态，在这种状态下它没有专家演示。</p><p id="ddf5" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我们可以通过让expert参与训练循环并在每个循环中查询expert来收集更多数据，从而改进这一点。</p><blockquote class="jp jq jr"><p id="6f47" class="ir is js it b iu iv iw ix iy iz ja jb jt jd je jf ju jh ji jj jv jl jm jn jo hb bi translated">这里的“标记”意味着为先前策略中的新状态分布样本提供动作。</p></blockquote><figure class="jx jy jz ka fd ij er es paragraph-image"><div class="er es nz"><img src="../Images/69463c3c32de9c50ebee3aa63c84719e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*tJ2ska-MzpHfNnWXuMpg9Q.png"/></div></figure><figure class="jx jy jz ka fd ij er es paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="er es oa"><img src="../Images/e3aad4f59762815fc75234ec99ddfbd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JaAGZ40KZII2h08cIuzXNw.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">来源:https://shivanshmundra.github.io/post/imitation_learning/<a class="ae iq" href="https://shivanshmundra.github.io/post/imitation_learning/" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><p id="1198" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们看看DAgger的实现</p><p id="21c3" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">我使用了Sergey Levine的<a class="ae iq" href="http://rail.eecs.berkeley.edu/deeprlcourse/" rel="noopener ugc nofollow" target="_blank"> CS285深度强化学习课程</a>材料中的代码。如果你想深入到深度强化学习，这是一个金矿！</p><p id="881b" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">前往我的GitHub库:</p><div class="mx my ez fb mz na"><a href="https://github.com/Shivanshmundra/CS285-DeepRL-Solutions/tree/master/hw1" rel="noopener  ugc nofollow" target="_blank"><div class="nb ab dw"><div class="nc ab nd cl cj ne"><h2 class="bd hj fi z dy nf ea eb ng ed ef hh bi translated">shivanshmundra/cs 285-DeepRL-解决方案</h2><div class="nh l"><h3 class="bd b fi z dy nf ea eb ng ed ef dx translated">Sergey Levine课程CS285深度强化学习课程的解决方案(持续进行-到现在第一周)…</h3></div><div class="ni l"><p class="bd b fp z dy nf ea eb ng ed ef dx translated">github.com</p></div></div><div class="nj l"><div class="ob l nl nm nn nj no ik na"/></div></div></a></div><p id="08e9" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在不同的健身房环境中使用DAgger。</p><p id="056e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在这个解决方案中，我们有一个专家策略，当被查询时，在给定的状态上诱导动作，而不是人类专家标记状态空间。你可以看到，随着数据聚合的每一次迭代，阶段性的回报增加，代理人的方法趋向收敛。</p><p id="6acb" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">这个库还包含一个行为克隆的实现，用来比较BC和DAgger算法。</p><p id="b584" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">让我们来看一些在不同环境下经过培训的策略:</p><figure class="jx jy jz ka fd ij er es paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="er es oc"><img src="../Images/e53db7a824f1d730775e635b49df474c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*k_2IMOvUape2lvCvsRmHEA.gif"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">Ant-v2环境</figcaption></figure><figure class="jx jy jz ka fd ij er es paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="er es oc"><img src="../Images/9c3d2d8021fac26e9ab04a10cd14d50c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*DuFbhkaqve6P1NuP44kHhw.gif"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">Hopper-v2环境</figcaption></figure><figure class="jx jy jz ka fd ij er es paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="er es oc"><img src="../Images/9d05412144feca54dff9b0136d89773e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Nnx3wLJLY8ncx7ecfnUp1Q.gif"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">人形-v2环境</figcaption></figure><p id="c86e" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">在所有上述模拟中，左边的<strong class="it hj">是使用<strong class="it hj">匕首</strong>算法的训练策略，右边的<strong class="it hj">是在训练期间使用的专家策略。</strong></strong></p></div><div class="ab cl od oe gp of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="hb hc hd he hf"><h1 id="0048" class="kb kc hi bd kd ke ok kg kh ki ol kk kl km om ko kp kq on ks kt ku oo kw kx ky bi translated">其他算法</h1><p id="2e74" class="pw-post-body-paragraph ir is hi it b iu kz iw ix iy la ja jb jc lb je jf jg lc ji jj jk ld jm jn jo hb bi translated">上面解释的所有算法都被归类为模仿学习中的“基本”算法。还有其他几个算法要讨论，但我还没有完全理解它们。</p><p id="5e8a" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">其中一些是:</p><h2 id="1cbc" class="le kc hi bd kd lf lg lh kh li lj lk kl jc ll lm kp jg ln lo kt jk lp lq kx lr bi translated">逆向强化学习:</h2><ul class=""><li id="9f37" class="ls lt hi it b iu kz iy la jc op jg oq jk or jo lx ly lz ma bi translated">从专家轨迹中学习奖励函数，然后得出最优策略。</li><li id="8532" class="ls lt hi it b iu mb iy mc jc md jg me jk mf jo lx ly lz ma bi translated">运行起来非常昂贵。</li><li id="bb10" class="ls lt hi it b iu mb iy mc jc md jg me jk mf jo lx ly lz ma bi translated">从奖励函数中间接学习最优策略。</li></ul><h2 id="960a" class="le kc hi bd kd lf lg lh kh li lj lk kl jc ll lm kp jg ln lo kt jk lp lq kx lr bi translated">生成性对抗性模仿学习(GAIL):</h2><ul class=""><li id="04e5" class="ls lt hi it b iu kz iy la jc op jg oq jk or jo lx ly lz ma bi translated">它从数据中学习政策，而不是奖励函数。</li><li id="1473" class="ls lt hi it b iu mb iy mc jc md jg me jk mf jo lx ly lz ma bi translated">有时候，这比“专家”政策更好。</li><li id="bde2" class="ls lt hi it b iu mb iy mc jc md jg me jk mf jo lx ly lz ma bi translated">这一想法受到了生成性对抗网络的启发，在生成性对抗网络中，我们需要近似地面真实概率分布。</li><li id="630a" class="ls lt hi it b iu mb iy mc jc md jg me jk mf jo lx ly lz ma bi translated">这里我们需要近似“专家”状态-动作分布。</li><li id="b02e" class="ls lt hi it b iu mb iy mc jc md jg me jk mf jo lx ly lz ma bi translated">目标是找到一个策略pi-theta，使得鉴别器不能区分遵循pi-theta的状态和来自pi-expert的状态。</li></ul></div><div class="ab cl od oe gp of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="hb hc hd he hf"><p id="d3be" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated">所以这是模仿学习的一个弯路，希望你发现它很有见地！如果有反馈的话，我很乐意听到。❤️</p></div><div class="ab cl od oe gp of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="hb hc hd he hf"><h2 id="aebe" class="le kc hi bd kd lf lg lh kh li lj lk kl jc ll lm kp jg ln lo kt jk lp lq kx lr bi translated">参考资料:</h2><ul class=""><li id="3f4e" class="ls lt hi it b iu kz iy la jc op jg oq jk or jo lx ly lz ma bi translated"><a class="ae iq" rel="noopener" href="/@SmartLabAI/a-brief-overview-of-imitation-learning-8a8a75c44a9c"><em class="js">https://medium . com/@ SmartLabAI/a-brief-overview-of-模仿-学习-8a8a75c44a9c </em> </a></li><li id="e6f0" class="ls lt hi it b iu mb iy mc jc md jg me jk mf jo lx ly lz ma bi translated"><a class="ae iq" href="http://rail.eecs.berkeley.edu/deeprlcourse/" rel="noopener ugc nofollow" target="_blank"><em class="js">http://rail.eecs.berkeley.edu/deeprlcourse/</em></a></li><li id="c9ef" class="ls lt hi it b iu mb iy mc jc md jg me jk mf jo lx ly lz ma bi translated"><a class="ae iq" href="https://github.com/Shivanshmundra/CS285-DeepRL-Solutions/tree/master/hw1" rel="noopener ugc nofollow" target="_blank"><em class="js">https://github . com/Shivanshmundra/cs 285-DeepRL-Solutions/tree/master/hw1</em></a></li><li id="0a52" class="ls lt hi it b iu mb iy mc jc md jg me jk mf jo lx ly lz ma bi translated"><a class="ae iq" href="https://hollygrimm.com/rl_gail" rel="noopener ugc nofollow" target="_blank"><em class="js">https://hollygrimm.com/rl_gail</em></a></li><li id="fdde" class="ls lt hi it b iu mb iy mc jc md jg me jk mf jo lx ly lz ma bi translated"><a class="ae iq" rel="noopener" href="/@sanketgujar95/generative-adversarial-imitation-learning-266f45634e60"><em class="js">https://medium . com/@ sanketgujar 95/生殖-对抗-模仿-学习-266f45634e60 </em> </a></li><li id="4ae2" class="ls lt hi it b iu mb iy mc jc md jg me jk mf jo lx ly lz ma bi translated"><a class="ae iq" rel="noopener" href="/@jonathan_hui/rl-imitation-learning-ac28116c02fc"><em class="js">https://medium . com/@ Jonathan _ hui/rl-模仿-学习-AC 28116 c 02 fc</em>T29】</a></li></ul><p id="f174" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo hb bi translated"><em class="js">如有任何疑问或建议，欢迎联系我</em>😃。<em class="js">还有，在</em> <a class="ae iq" href="https://twitter.com/MundraShivansh" rel="noopener ugc nofollow" target="_blank"> <em class="js"> Twitter </em> </a> <em class="js">和</em><a class="ae iq" href="https://www.linkedin.com/in/shivansh-mundra-300849140/" rel="noopener ugc nofollow" target="_blank"><em class="js">Linkedin</em></a><em class="js">上找我。再见！！</em></p></div></div>    
</body>
</html>