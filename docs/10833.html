<html>
<head>
<title>A comprehensive guide to Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归综合指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-comprehensive-guide-to-linear-regression-2994b0d71fe3?source=collection_archive---------6-----------------------#2020-11-04">https://medium.com/analytics-vidhya/a-comprehensive-guide-to-linear-regression-2994b0d71fe3?source=collection_archive---------6-----------------------#2020-11-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex if ig ih ii"><div class="bz dy l di"><div class="ij ik l"/></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="ak">来源:</strong><a class="ae ip" href="https://www.youtube.com/watch?v=zPG4NjIkCjc" rel="noopener ugc nofollow" target="_blank"><strong class="ak">【https://www.youtube.com/watch?v=zPG4NjIkCjc】</strong></a></figcaption></figure><h1 id="bdc4" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">什么是回归？</h1><p id="e26c" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在了解什么是回归之前，让我们先了解两个重要的术语——相关性和因果关系。<br/> <strong class="jq hj">相关性— </strong>这种现象有助于我们理解两个变量之间的关系强度。它本质上表明了一个变量的变化如何影响另一个变量。它表示两个变量之间的关联程度。变量之间有 3 种关系，例如-</p><p id="f44e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> 1 —正相关:<br/> </strong>在这种情况下，一个变量的值增加往往会导致另一个变量<strong class="jq hj">的值持续增加</strong>。例如，当一个人摄入越来越多的卡路里时，他的体重就会增加。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="ab fe cl kv"><img src="../Images/f0f2535c57490842eb7dce2170b7e1aa.png" data-original-src="https://miro.medium.com/v2/format:webp/1*6ijlckSkxBJU4QF3mVqu4w.png"/></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">来源:</strong><a class="ae ip" href="https://www.investopedia.com/ask/answers/032515/what-does-it-mean-if-correlation-coefficient-positive-negative-or-zero.asp" rel="noopener ugc nofollow" target="_blank"><strong class="bd is">https://www . investopedia . com/ask/answers/032515/what-it-mean-if-correlation-coefficient-positive-negative-or-zero . ASP</strong></a></figcaption></figure><p id="3532" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> 2 —负相关:<br/> </strong>在这种情况下，一个变量数值的增加往往会导致另一个变量数值的减少<strong class="jq hj">一致</strong>。例如，一个学生的 GPA 随着他玩电子游戏时间的增加而下降。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="ab fe cl kv"><img src="../Images/2c8be2c08ec94be2df43ed3e05d71870.png" data-original-src="https://miro.medium.com/v2/format:webp/1*lssSe703KlI1kNL-X3wC5A.png"/></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">来源:</strong><a class="ae ip" href="https://www.investopedia.com/ask/answers/032515/what-does-it-mean-if-correlation-coefficient-positive-negative-or-zero.asp" rel="noopener ugc nofollow" target="_blank"><strong class="bd is">https://www . investopedia . com/ask/answers/032515/what-it-mean-if-correlation-coefficient-positive-negative-or-zero . ASP</strong></a></figcaption></figure><p id="e886" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> 3 —无相关性:<br/> </strong>非线性关系是指一个变量的值不会因<strong class="jq hj">常数因子</strong>而相对于另一个变量发生变化。例如，一个人的幸福并不取决于他/她拥有多少钱。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="ab fe cl kv"><img src="../Images/6a9ffab05f29b61b9834dfb2dc0a4673.png" data-original-src="https://miro.medium.com/v2/format:webp/1*GoQIsi9qO3SWE8gklYa7vQ.png"/></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">来源:</strong><a class="ae ip" href="https://www.investopedia.com/ask/answers/032515/what-does-it-mean-if-correlation-coefficient-positive-negative-or-zero.asp" rel="noopener ugc nofollow" target="_blank"><strong class="bd is">https://www . investopedia . com/ask/answers/032515/what-it-mean-if-correlation-coefficient-positive-negative-or-zero . ASP</strong></a></figcaption></figure><p id="6b9e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">因果关系或因果关系— </strong>是一个事件的发生引起第二个事件发生的现象。在这种情况下，这种现象也被称为因果关系，因为第一个事件被称为原因，第二个事件被称为结果。例如，学生花在学习上的时间越多，成绩就越高。<br/> <strong class="jq hj">注:</strong>相关性并不总是指因果关系，但对于因果关系，两个变量需要相互关联。例如，考试前的紧张会导致低分。这是负相关的一个例子，但不一定代表因果关系。分数低可能是因为其他原因，如学生学习不好，他们的健康状况不好等。因此，相关性不一定意味着因果关系。<br/>回到<strong class="jq hj">回归</strong>，它是一种有监督的机器学习技术，用来理解 2 个或更多变量之间的趋势或关系。回归来自术语<strong class="jq hj">‘回归’</strong>，意思是从一组变量中预测一个变量。被预测的变量称为<strong class="jq hj">因变量</strong>，预测中使用的变量称为<strong class="jq hj">自变量</strong>。回归通常涉及识别因变量和自变量之间的关系或相关性，但是它不能解释变量之间的任何因果关系。回归的一个例子是，我们试图根据不同的独立变量(如房子的大小和房间数量)来预测佛罗里达州的房价。</p><h1 id="d0e1" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">回归背后的数学</h1><p id="2ff3" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在进入回归的实际实现之前，让我们首先理解简单线性回归背后到底发生了什么。具有一个因变量/目标变量/预测变量和一个自变量/预测变量的线性回归方程如下所示:</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ky"><img src="../Images/5d6446b4fd97ae304957c10d82e2fb74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_kWj6wDKbsnxwJrqjNSLaA.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">回归的一般方程</strong></figcaption></figure><p id="cdc3" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">线性回归的主要目的是最小化方程中的误差项。下图给出了上述等式的一般概述</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ld"><img src="../Images/63a62c7dae8aff4162241ac233ad3f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LP3pOq2rjMolbflHX4CdSw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">线性回归的图形表示</strong></figcaption></figure><p id="5d3c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">y 的实际值与 y 的预测值之差称为<strong class="jq hj">‘残差’。</strong>线性回归的目标是在参数 a 和 b 的帮助下找到最佳拟合线，使得预测中的误差最小。为了理解这种关于最小化误差或如何选择影响回归的参数(a 和 b)值的直觉，让我们深入研究两个概念:-</p><p id="552a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> 1 —成本函数:- <br/> </strong>成本函数旨在找到估计值或参数值，以使这些参数产生最佳拟合线，该线将几乎拟合回归线上的所有数据点。为了找到这些值，可以将成本函数转换为最小化问题，以最小化实际值和预测值之间的误差。以下函数给出了线性回归的成本/损失函数:-</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es le"><img src="../Images/2ce70c084c8e57121d5b1da215ce7506.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JrQwDyxWAS1YNc_xaqpS0A.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">线性回归的成本函数</strong></figcaption></figure><p id="151e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">上述函数表示误差平方和，即各个点的所有残差平方和除以观察总数。它也被称为均方误差(MSE)。它表示所有数据点的平均平方误差。使用此成本函数，我们将计算θ0 和θ1 的值，以便最小化误差并找到最佳拟合线。</p><p id="cde7" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> 2 —梯度下降:- <br/> </strong>线性回归的下一步是使用梯度下降算法来最小化成本函数，并找到θ0 和θ1 的最佳值。该算法迭代地试图在域函数中找到θ0 和θ1 的不同值，直到达到不可能有比当前值更好的值的点。最初，该算法从θ0 和θ1 的一些任意值开始，然后在每次通过后更新这些值。简而言之，该算法迭代寻找θ0 和θ1 的值，直到它收敛于凸函数的最小值。为了直观地理解梯度下降算法，考虑下面的例子- <br/>想象 Adwait 在一个山顶上，他想要到达该山的最底部点。他环顾四周，然后决定他想使用一些路径下去。现在，这里的问题是 Adwait 只能进行离散步数。(示例在第节的图片之后继续— <strong class="jq hj"> α) </strong> <br/>为了更好地理解数学，考虑以下梯度下降算法-</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lf"><img src="../Images/9e15319c17179cb7c898f5f97ed40a6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yrwhbOVENfLW-Zd86Yri-Q.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">线性回归的梯度下降</strong></figcaption></figure><p id="7d09" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">如果我们考虑 y = θ0 + θ1*x，那么我们可以说，在算法中的每次迭代之后，我们更新θ0 和θ1 的值，以便找到最佳拟合线并使残差最小化。现在你可能想知道什么是阿尔法，因此这里有一个简短而甜蜜的解释！<br/> <strong class="jq hj"> α — </strong>梯度下降算法中的α也称为学习率。参考 Adwait 下山的例子，因为他只能采取离散的步骤，所以他必须决定最佳的步骤数，使得他不会因为采取大量的步骤而错过最底部的点，也不会因为采取较少的步骤而导致到达期望位置的大量时间。因此，α是学习速率或梯度下降算法的每次迭代所采用的步数。要在图示视图中理解这一点，请参考以下图像。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lg"><img src="../Images/706f481556b1fa33c98f3e5c8db594c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sb38mklSSDVZSoAoCVz0kw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">当学习率很小的时候</strong></figcaption></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es le"><img src="../Images/7342a6b626efd788bb3675e31d1b1e4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8C5UOGo6ZsJ2gRLrwm6dFg.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">当学习率较高时</strong></figcaption></figure><p id="6d87" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">继续，下一个问题是为什么在算法中对成本函数求导？这个问题的答案是，考虑函数曲线上的一点。如果我们画一条该点的切线并计算它的导数，我们就能找到它的斜率。求直线的斜率有两个目的</p><p id="82e5" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">1-学习率的大小和斜率将告诉我们要沿着曲线走多远才能到达最小值或最大值的收敛点。斜坡的标志会告诉我们前进的方向。<br/>回忆一下梯度下降算法——</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lf"><img src="../Images/9e15319c17179cb7c898f5f97ed40a6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yrwhbOVENfLW-Zd86Yri-Q.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">线性回归的梯度下降</strong></figcaption></figure><p id="68d8" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">为了推导出最终算法，我们使用偏导数来计算θ0 和θ1 的值，然后这些值将被同时更新，直到达到收敛点。下图显示了θ0 和θ1 的偏差</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lg"><img src="../Images/89d77c88620a67ea3e83db052033bd84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*riO22hiwdcNIC0Wo26-3gQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">相对于θ0 的偏导数</strong></figcaption></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lh"><img src="../Images/6d11c2aedce5e6cae5adbe55204107f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rJxZveXP5CIchMQRTsDqRg.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">相对于θ1 的偏导数</strong></figcaption></figure><p id="166d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">因此，在多次迭代之后，我们最终找到θ0 和θ1 的最佳值，从而使误差最小化，从而改进我们的预测。</p><p id="b3c6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在进入线性回归的实际实施之前，让我们看看另一个重要的概念，即线性回归的假设。</p><h1 id="e22d" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">线性回归的 5 个经典假设</h1><p id="4a3e" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><strong class="jq hj"> 1 —变量之间的线性关系<br/> </strong>线性回归算法对异常值很敏感，只有当预测变量或独立变量与预测变量或目标变量之间存在线性关系时，该算法才能正常工作。借助于散点图，这可以很容易地检测出来。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es li"><img src="../Images/7c1e218198387a0f7c05913ed990f5a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0IA_SpOTa2paZNjYCT1agg.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">来源:</strong><a class="ae ip" href="https://towardsdatascience.com/assumptions-of-linear-regression-algorithm-ed9ea32224e1" rel="noopener" target="_blank"><strong class="bd is">https://towards data science . com/assumptions-of-linear-regression-algorithm-ed 9ea 32224 e 1</strong></a></figcaption></figure><p id="124a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">2-避免要素间的多重共线性<br/> </strong>多重共线性是两个独立变量高度相关的现象。这导致在测试数据集上有很多不准确的预测。Pairplots 和相关矩阵(corr())可用于检测独立特征之间的相关性。一般的经验法则是移除相关系数小于-0.7 且大于 0.7 的特征。为什么要去除多重共线性？原因是线性回归依赖于预测值和目标变量之间的关系。它被解释为自变量的单位变化导致目标变量的平均值增加/减少系数量。因此，如果有两个高度相关的独立特征，则该算法不能在目标和预测器之间建立关系。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es lj"><img src="../Images/5b0ea95994f6f4816aa1f5a97c767a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/0*je1rbPRVhpo5y8PU.png"/></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">来源:</strong><a class="ae ip" href="https://towardsdatascience.com/assumptions-of-linear-regression-algorithm-ed9ea32224e1" rel="noopener" target="_blank"><strong class="bd is">https://towardsdatascience . com/assumptions-of-linear-regression-algorithm-ed 9ea 32224 e 1</strong></a></figcaption></figure><p id="a291" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">3 — <strong class="jq hj">同方差<br/> </strong>这是一种误差项或残差(预测值与实际值之间的差值)在所有数据点上保持不变，并且不遵循特定分布模式的现象。通过散点图检查残差与预测值的分布是一个很好的同质性指标。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lk"><img src="../Images/0d26a68ba1f7e8a0419cf20c3a9b639b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aXhuZrLl_am9VnGK.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">来源:</strong><a class="ae ip" href="https://towardsdatascience.com/assumptions-of-linear-regression-algorithm-ed9ea32224e1" rel="noopener" target="_blank"><strong class="bd is">https://towardsdatascience . com/assumptions-of-linear-regression-algorithm-ed 9ea 32224 e 1</strong></a></figcaption></figure><p id="e839" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">4 — <strong class="jq hj">误差项的正态分布<br/> </strong>数据集中的误差项应呈正态分布。这可以用 qq 图来验证。一条相当直的线表明残差是正态分布的。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es ll"><img src="../Images/f4462cebc7cf89501fbff3c58fed3b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/0*b-fW8KOmVWJOoqxg.png"/></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">来源:</strong><a class="ae ip" href="https://towardsdatascience.com/assumptions-of-linear-regression-algorithm-ed9ea32224e1" rel="noopener" target="_blank"><strong class="bd is">https://towardsdatascience . com/assumptions-of-linear-regression-algorithm-ed 9ea 32224 e 1</strong></a></figcaption></figure><p id="ac1d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">5—残差之间很少或没有自相关<br/> </strong>当残差相互依赖时，会出现自相关。误差项中相关性的存在大大降低了模型的准确性。自相关可以通过 Durbin-Watson 检验来检测。<br/>杜宾·沃森检验的假设是:<br/> H0 =没有一阶自相关。<br/> H1 =存在一阶相关性。</p><p id="3c29" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">Durbin Watson 测试报告一个测试统计值，取值范围为 0 到 4，其中:</p><ul class=""><li id="0ac0" class="lm ln hi jq b jr km jv kn jz lo kd lp kh lq kl lr ls lt lu bi translated">2 是没有自相关。</li><li id="3621" class="lm ln hi jq b jr lv jv lw jz lx kd ly kh lz kl lr ls lt lu bi translated">0 到&lt;2 is positive autocorrelation (common in time series data).</li><li id="a691" class="lm ln hi jq b jr lv jv lw jz lx kd ly kh lz kl lr ls lt lu bi translated">&gt; 2 到 4 为负自相关(在时间序列数据中不太常见)。</li></ul><p id="4bb0" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">所有上述假设都将在实际实施中得到检验。</p><p id="385f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">理解了这些假设之后，我们再来看看线性回归的实际实现。<br/>数据集—一个模拟数据集，其目标变量为薪水，预测变量为经验年限。这是数据的快照-</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div class="er es ma"><img src="../Images/644ed436f6e40955653a30f3c7d995b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*iHJcmzaztvpu6m7tAoE1tA.png"/></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">数据快照</strong></figcaption></figure><p id="d5dd" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">导入库</strong></p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mb"><img src="../Images/156fa0997ebcbc22b0747f1fd1e01103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YIbHyZ19xoLI3zmu1WslZw.png"/></div></div></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mc"><img src="../Images/6b5edcfc041bcc7046df4d37ba2faae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oRyuXzMHUrCQN-07d_XSsA.png"/></div></div></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es md"><img src="../Images/2742bf9108cac7fec71d33d5ea7cb766.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i07vIBEd8OwoJez5uxTuTg.png"/></div></div></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mc"><img src="../Images/bf5f32066060dfd75012be8ca799a169.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q7zmrWLh8vqgWh7vUJ03Nw.png"/></div></div></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es me"><img src="../Images/4fd978f5505a292354d3a20bdb7c1235.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zo5HUO8kBfLOKBgFq1MZfA.png"/></div></div></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mf"><img src="../Images/6d3dfbee55e94dc87377a18b3217b969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ag_EVg-sGMOjy7MnFLIeVQ.png"/></div></div></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mg"><img src="../Images/f3cae54df419f88ae760c9ab27f8010b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*erZSn3Y70xYjpwAleUT3Gw.png"/></div></div></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mh"><img src="../Images/aa3ee7982c06cebe4d959b2a25c45278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qoVi5kPmROKjo9wvvINudw.png"/></div></div></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mi"><img src="../Images/9fad0d25aab3f4b87fcb32189a0339d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vWOvOo_QujU1af1lVSHTqg.png"/></div></div></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mj"><img src="../Images/fc6f10314d2524c936e188ae41745b0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R9eV-Dt2-TV78GxVlWAnXA.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">工作经验每增加一个单位，一个人的工资就会增加 9312.57 美元</strong></figcaption></figure><p id="48bc" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">既然我们已经了解了 python 中线性回归的实现，那么让我们来看看衡量模型性能的不同指标。</p><h1 id="ed5d" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">评估指标:-</h1><p id="eb4c" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">现在让我们看看如何发现我们建立的模型是否足够好来预测未知数据的值。</p><p id="1f8d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> 1 — R 的平方<br/> </strong>这个指标基本上告诉我们预测值与实际值有多接近。它基本上告诉我们因变量的变化受自变量变化的影响有多大。范围从 0 到 1，其中 R 平方值接近 0 意味着模型拟合不足或有很多不正确的预测，值接近 1 表明自变量解释了因变量的大部分变化。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mk"><img src="../Images/6df094588ee9200d31dc7a02907f52f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*jnlNkeyN8tCKbtUM.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">来源:</strong><a class="ae ip" href="https://towardsdatascience.com/how-to-evaluate-your-machine-learning-models-with-python-code-5f8d2d8d945b" rel="noopener" target="_blank"><strong class="bd is">https://towardsdatascience . com/how-to-evaluate-your-machine-learning-models-with-python-code-5f 8d 2d 945 b</strong></a></figcaption></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lg"><img src="../Images/d92c6c5a4470324fc0cfe6f9b5b1f3d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dAX4lf622N1Viaa1YNSYrg.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">计算 R 平方值的公式</strong></figcaption></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lg"><img src="../Images/644f0ddc8513dd015072a4efc35ad8ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K1Nh-0yX6eJ4Oe_ty4vxBQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">SSR 和 SST 的值</strong></figcaption></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ml"><img src="../Images/009994dc0294b61256913d62db9e8a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i0dJ95blvyD8PUA1Hw2Fzg.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">Python 中的 R-square</strong></figcaption></figure><p id="80f0" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> 2 —调整后的 R 平方<br/> </strong>调整后的 R 平方告诉我们，与 R 平方类似的自变量对因变量的变化解释得有多好，但它惩罚了实际上对因变量的变化没有影响的自变量。R-square 值通常随着自变量数量的增加而增加，但调整后的 R-square 仅考虑那些实际影响因变量变化的变量。调整后的 R 平方始终≤ R 平方值。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es le"><img src="../Images/2b19b243d798169e93e96147ee5b6624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uHBuo3RCwP9LB4Lto3reNQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">计算调整后 R 平方的公式</strong></figcaption></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mm"><img src="../Images/5a06d8e3d717e1fd9ef20e359f144417.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*GxKTgxu0yWa0QeYOO0TpHQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">Python 中调整的 R-square</strong></figcaption></figure><p id="cd39" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> 3 —平均绝对误差(MAE) <br/> </strong> MAE 使用残差的绝对值，因此它不能指示模型是表现不佳还是过度拟合。每个残差对总误差的贡献是线性的，因为我们对单个残差求和。出于这个原因，small MAE 认为该模型在预测方面非常出色。类似地，一个大的 MAE 表明你的模型可能很难概括。MAE 为 0 意味着我们的模型输出完美的预测，但这在真实场景中不太可能发生。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lg"><img src="../Images/dde94ce262b629fa63b590e62adb34b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NgbnFGxLBYFgcAyxuQOrfw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">平均绝对误差公式</strong></figcaption></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mn"><img src="../Images/e39a8b98a73383c4eae7baa81d0747c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EJVL8F3zhedjFpYCBfVQFQ.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">Python 中的 MAE</strong></figcaption></figure><p id="762d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> 4 —均方误差和均方根误差<br/> </strong> MSE 或均方误差是回归任务最首选的指标之一。它只是目标值和回归模型预测值之间的平方差的平均值。当它计算差异的平方时，它惩罚了一个很小的错误，这个错误会导致高估模型的糟糕程度。它比其他指标更受青睐，因为它是可微分的，因此可以更好地优化。</p><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es le"><img src="../Images/9ab8f243256a26914734f204e871cefc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZZjsRcEZnQnyHDrZ5b6Lbw.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">计算均方误差的公式</strong></figcaption></figure><figure class="kr ks kt ku fd ii er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es mo"><img src="../Images/078c75dcbe21fc7955626bfcf66d17ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xT_kDMAMEWtZfTOJ-p-wLA.png"/></div></div><figcaption class="il im et er es in io bd b be z dx translated"><strong class="bd is">Python 中的均方误差</strong></figcaption></figure><h1 id="4b7b" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">结论</h1><p id="cf59" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">线性回归是一个强大的工具，也是预测建模中最基本和最被低估的工具之一。我希望我已经让人们更容易进入机器学习的世界，或者让有经验的人耳目一新。期待提供更多关于机器学习算法的信息！接下来，是逻辑回归，一个强大而基本的分类算法！敬请期待，在那之前，享受机器学习吧！</p><p id="0bb5" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">来源-</strong><a class="ae ip" href="https://www.varsitytutors.com/hotmath/hotmath_help/topics/correlation-and-causal-relation#:~:text=A%20causal%20relation%20between%20two,variables%20does%20not%20imply%20causation." rel="noopener ugc nofollow" target="_blank">https://www . varsitytutors . com/hotmath/hotmath _ help/topics/correlation-and-causal-relation #:~:text = A % 20 causal % 20 relation % 20 between % 20 two，variables % 20 does % 20 not % 20 implie % 20 因果关系。</a></p><p id="9de4" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><a class="ae ip" href="https://www.statisticssolutions.com/assumptions-of-linear-regression/" rel="noopener ugc nofollow" target="_blank">https://www . statistics solutions . com/assumptions-of-linear-regression/</a></p><p id="ef7b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><a class="ae ip" href="https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/" rel="noopener ugc nofollow" target="_blank">https://statistics byjim . com/regression/ols-linear-regression-assumptions/</a></p><p id="68d9" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><a class="ae ip" href="https://towardsdatascience.com/assumptions-of-linear-regression-algorithm-ed9ea32224e1" rel="noopener" target="_blank">https://towards data science . com/assumptions-of-linear-regression-algorithm-ed 9ea 32224 e 1</a></p><p id="dbce" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><a class="ae ip" href="https://www.statisticshowto.com/durbin-watson-test-coefficient/" rel="noopener ugc nofollow" target="_blank">https://www . statistics show to . com/durbin-Watson-test-coefficient/</a></p><p id="7c49" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><a class="ae ip" href="https://www.statisticshowto.com/adjusted-r2/#:~:text=Adjusted%20R2%20is%20a,of%20terms%20in%20a%20model." rel="noopener ugc nofollow" target="_blank">https://www . statistics show to . com/Adjusted-R2/#:~:text = Adjusted % 20r 2% 20 is % 20a，of%20terms%20in%20a%20model。</a></p><p id="0afa" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><a class="ae ip" href="https://towardsdatascience.com/how-to-evaluate-your-machine-learning-models-with-python-code-5f8d2d8d945b" rel="noopener" target="_blank">https://towards data science . com/how-to-evaluate-your-machine-learning-models-with-python-code-5f 8d 2d 8d 945 b</a></p></div></div>    
</body>
</html>