<html>
<head>
<title>Using RoBERTa with fast.ai for NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将RoBERTa与fast.ai一起用于NLP</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c?source=collection_archive---------1-----------------------#2019-09-02">https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c?source=collection_archive---------1-----------------------#2019-09-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="0cd5" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">在fast.ai中实现当前最先进的NLP模型</h2></div><p id="18f6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本教程将带你通过拥抱脸的变形金刚和fast.ai库来集成Fairseq的RoBERTa模型。我们将在Keita Kurita的 <a class="ae ju" href="https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/" rel="noopener ugc nofollow" target="_blank"> <em class="jt">文章的基础上，用快速AI </em> </a> <em class="jt">对BERT进行微调。最后，我们将使用</em> <a class="ae ju" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank"> <em class="jt"> IMDB数据集</em> </a> <em class="jt">。</em></p><p id="d55b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">更新2020.11: </em> <a class="ae ju" href="https://www.fast.ai/2020/08/21/fastai2-launch/" rel="noopener ugc nofollow" target="_blank"> <em class="jt">自本文发布后，fast.ai已升级至v2 </em> </a> <em class="jt">。为了与下面的步骤兼容，v2没有经过测试。因此，</em><a class="ae ju" href="https://pypi.org/project/fastai/1.0.61/" rel="noopener ugc nofollow" target="_blank"><em class="jt">v1</em></a><em class="jt">的用法推荐与本文一并关注。</em></p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/6c936079d372f013ae86dd0d38fa8349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0rzaJJmIWWxwmM5xTYvZGw.png"/></div></div></figure><p id="d37e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Fastai提供了一个简化的界面来构建数据集和训练模型。然而，它没有为当前最先进的NLP模型(如RoBERTa、BERT或XLNet)提供内置功能(截至2019年9月)。将这些整合到Fastai中，可以让您享受Fastai方法的便利，以及这些预训练模型的强大预测能力。</p><p id="f346" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">迁移学习的概念对于NLP来说还是一个相对较新的概念，并且正在以非常快的速度发展。因此，很有希望看到像RoBERTa这样的模型在几个不同的NLP任务中在<a class="ae ju" href="https://super.gluebenchmark.com/leaderboard" rel="noopener ugc nofollow" target="_blank"> SuperGLUE benchmark </a>上表现得非常好。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es kh"><img src="../Images/0f7461583c3753b345abd21911ee9e90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZcONvF6l-m8uurEPQEKxsQ.png"/></div></div><figcaption class="ki kj et er es kk kl bd b be z dx translated">罗伯塔与强力胶任务的其他模型。<a class="ae ju" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="3c99" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本质上，RoBERTa建立在BERT的基础上，通过更多数据、更大的批量进行更长时间的预训练，同时只对掩蔽语言建模进行预训练，而不是对下一句预测进行预训练。底层架构保持不变，因为两者都利用屏蔽语言模型预训练。你可以阅读<a class="ae ju" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank">这里的</a>了解更多关于差异的信息。</p><h1 id="ef51" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">0.先决条件</h1><p id="cdf1" class="pw-post-body-paragraph ix iy hi iz b ja le ij jc jd lf im jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">您需要安装Fastai和transformers库，最好能够访问GPU设备。对于Fastai，您可以遵循此处提供的<a class="ae ju" href="https://github.com/fastai/fastai#installation" rel="noopener ugc nofollow" target="_blank">说明。对于变压器:</a></p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="356f" class="lo kn hi lk b fi lp lq l lr ls">pip install transformers</span></pre><h1 id="a2b0" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">1.设置记号赋予器</h1><p id="99df" class="pw-post-body-paragraph ix iy hi iz b ja le ij jc jd lf im jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">首先，让我们导入相关的Fastai工具:</p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="7ddd" class="lo kn hi lk b fi lp lq l lr ls">from fastai.text import *<br/>from fastai.metrics import *</span></pre><p id="b6e5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还有《变形金刚》中罗伯塔的记号赋予器:</p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="885e" class="lo kn hi lk b fi lp lq l lr ls">from transformers import RobertaTokenizer<br/>roberta_tok = RobertaTokenizer.from_pretrained("roberta-base")</span></pre><p id="e003" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">RoBERTa使用与BERT不同的默认特殊标记。例如，不使用[CLS]和[SEP]作为开始和结束标记，而是分别使用<s>和</s>。例如，标记化的电影评论可能看起来像:</p><p id="df04" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">“电影很棒”→ [ <s>，movie，was，great，</s></p><p id="6afe" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们现在将围绕RobertaTokenizer创建一个Fastai包装器。</p><figure class="jw jx jy jz fd ka"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="c05e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们可以初始化我们的Fastai tokenizer:(注意:为了Fastai兼容性，我们必须将Fastai包装器包装在tokenizer类中)</p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="d864" class="lo kn hi lk b fi lp lq l lr ls">fastai_tokenizer = Tokenizer(tok_func = FastAiRobertaTokenizer(roberta_tok, max_seq_len=256), pre_rules=[], post_rules=[])</span></pre><p id="c6c3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我们将加载罗伯塔的词汇。</p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="d1b0" class="lo kn hi lk b fi lp lq l lr ls">path = Path()<br/>roberta_tok.save_vocabulary(path) </span><span id="5d60" class="lo kn hi lk b fi lv lq l lr ls">with open('vocab.json', 'r') as f:<br/>    roberta_vocab_dict = json.load(f)<br/>    <br/>fastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys()))</span></pre><h1 id="e730" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">2.设置数据中心</h1><p id="2211" class="pw-post-body-paragraph ix iy hi iz b ja le ij jc jd lf im jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">在构建Fastai数据束之前，我们需要为记号赋予器和词汇表创建适当的预处理器。</p><figure class="jw jx jy jz fd ka"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="7934" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们将专门为Roberta创建一个DataBunch类。</p><figure class="jw jx jy jz fd ka"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="3c4e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，我们还需要一个特定于Roberta的TextList类:</p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="7f3f" class="lo kn hi lk b fi lp lq l lr ls">class RobertaTextList(TextList):<br/>    _bunch = RobertaDataBunch<br/>    _label_cls = TextList</span></pre><h1 id="54cc" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">3.加载数据</h1><p id="12bc" class="pw-post-body-paragraph ix iy hi iz b ja le ij jc jd lf im jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">咻，现在我们已经完成了相关的设置过程，我们可以把它们放在一起读入我们的<a class="ae ju" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank"> IMDB数据</a>。</p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="52c5" class="lo kn hi lk b fi lp lq l lr ls">df = pd.read_csv("IMDB Dataset.csv")</span><span id="b16e" class="lo kn hi lk b fi lv lq l lr ls">feat_cols = "review"<br/>label_cols = "sentiment"</span></pre><p id="d96d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们现在可以简单地创建一个Fastai数据集群，使用:</p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="a28e" class="lo kn hi lk b fi lp lq l lr ls">processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)</span><span id="b98e" class="lo kn hi lk b fi lv lq l lr ls">data = RobertaTextList.from_df(df, ".", cols=feat_cols, processor=processor) \<br/>    .split_by_rand_pct(seed=2019) \<br/>    .label_from_df(cols=label_cols,label_cls=CategoryList) \<br/>    .databunch(bs=4, pad_first=False, pad_idx=0)</span></pre><h1 id="79e9" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">4.构建自定义Roberta模型</h1><p id="fdbc" class="pw-post-body-paragraph ix iy hi iz b ja le ij jc jd lf im jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">在这一步，我们将定义模型架构，并传递给我们的Fastai学习者。本质上，我们向RobertaModel的输出添加了一个新的最终层。这一层将被专门训练用于IMDB情感分类。</p><figure class="jw jx jy jz fd ka"><div class="bz dy l di"><div class="lt lu l"/></div></figure><p id="0b98" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">初始化模型:</p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="3c8a" class="lo kn hi lk b fi lp lq l lr ls">roberta_model = CustomRobertatModel()</span></pre><h1 id="a894" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">5.训练模型</h1><p id="8316" class="pw-post-body-paragraph ix iy hi iz b ja le ij jc jd lf im jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">初始化我们的Fastai学习器:</p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="c703" class="lo kn hi lk b fi lp lq l lr ls">learn = Learner(data, roberta_model, metrics=[accuracy])</span></pre><p id="772f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">开始训练:</p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="54c9" class="lo kn hi lk b fi lp lq l lr ls">learn.model.roberta.train() # set roberta into train mode</span><span id="3b0c" class="lo kn hi lk b fi lv lq l lr ls">learn.fit_one_cycle(1, max_lr=1e-5)</span></pre><p id="4049" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在仅仅一个时期之后并且没有解冻层的情况下，我们在验证集上实现了94%的准确度。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div class="er es lw"><img src="../Images/13ae29809a586d0e5e12019fc44d5fea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*DBBrkwCLr3-y5yzHYfdxcQ.png"/></div><figcaption class="ki kj et er es kk kl bd b be z dx translated">. 941900在一个训练时期内的准确度</figcaption></figure><p id="830b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您现在还可以利用其他Fastai方法，例如:</p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="69ce" class="lo kn hi lk b fi lp lq l lr ls"># find an appropriate lr<br/>learn.lr_find()<br/>learn.recorder.plot()</span><span id="9681" class="lo kn hi lk b fi lv lq l lr ls"># unfreeze layers<br/>learn.unfreeze()</span><span id="4836" class="lo kn hi lk b fi lv lq l lr ls"># train using half precision<br/>learn = learn.to_fp16()</span></pre><h1 id="c965" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">6.创建预测</h1><p id="bf0d" class="pw-post-body-paragraph ix iy hi iz b ja le ij jc jd lf im jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">由于Fastai的get_preds函数没有按顺序输出预测，我们可以使用下面的方法。</p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="1423" class="lo kn hi lk b fi lp lq l lr ls">def get_preds_as_nparray(ds_type) -&gt; np.ndarray:<br/><br/>    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()<br/>    sampler = [i for i in data.dl(ds_type).sampler]<br/>    reverse_sampler = np.argsort(sampler)<br/>    ordered_preds = preds[reverse_sampler, :]<br/>    pred_values = np.argmax(ordered_preds, axis=1)<br/>    return ordered_preds, pred_values</span><span id="211f" class="lo kn hi lk b fi lv lq l lr ls"># For Valid<br/>preds, pred_values = get_preds_as_nparray(DatasetType.Valid)</span></pre><p id="9fd9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">注意:如果我们有一个测试集，我们可以很容易地在前面的步骤3中通过初始化“数据”来添加一个测试集，如下所示:</p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="a27f" class="lo kn hi lk b fi lp lq l lr ls">data = RobertaTextList.from_df(df, ".", cols=feat_cols, processor=processor) \<br/>    .split_by_rand_pct(seed=2019) \<br/>    .label_from_df(cols=label_cols,label_cls=CategoryList) \<br/>    .add_test(RobertaTextList.from_df(test_df, ".", cols=feat_cols, processor=processor)) \<br/>    .databunch(bs=4, pad_first=False, pad_idx=0)</span></pre><p id="1163" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，如果我们有一个测试集，我们可以通过以下方式获得preds:</p><pre class="jw jx jy jz fd lj lk ll lm aw ln bi"><span id="4dd8" class="lo kn hi lk b fi lp lq l lr ls">test_preds = get_preds_as_nparray(DatasetType.Test)</span></pre></div><div class="ab cl lx ly gp lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="hb hc hd he hf"><p id="cf6a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，您可以使用RoBERTa和Fastai在几乎任何基于文本的数据集上进行训练，结合两种非常强大的工具来产生有效的结果。你可以在我的 <a class="ae ju" href="https://github.com/devkosal/fastai_roberta/tree/master/fastai_roberta_imdb" rel="noopener ugc nofollow" target="_blank"> <em class="jt"> github页面</em> </a> <em class="jt">或</em> <a class="ae ju" href="https://www.kaggle.com/ds3761/using-roberta-with-fastai-tutorial" rel="noopener ugc nofollow" target="_blank"> <em class="jt"> kaggle内核</em> </a> <em class="jt">上的数据一起访问本教程的jupyter笔记本。(如果在github上查看nb有困难，使用</em> <a class="ae ju" href="https://github.com/devkosal/fastai_roberta/tree/master/fastai_roberta_imdb" rel="noopener ugc nofollow" target="_blank"> <em class="jt">这个链接</em> </a> <em class="jt">)。如果你有兴趣看到强力胶任务的类似实现，请继续阅读我下面关于使用RoBERTa和Fastai完成强力胶任务CB </em>   <strong class="iz hj"> <em class="jt">的工作。</em>T29】</strong></p><div class="me mf ez fb mg mh"><a href="https://github.com/devkosal/fastai_roberta/tree/master/fastai_roberta_imdb" rel="noopener  ugc nofollow" target="_blank"><div class="mi ab dw"><div class="mj ab mk cl cj ml"><h2 class="bd hj fi z dy mm ea eb mn ed ef hh bi translated">德夫科萨尔/法斯泰_罗伯塔</h2><div class="mo l"><h3 class="bd b fi z dy mm ea eb mn ed ef dx translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="mp l"><p class="bd b fp z dy mm ea eb mn ed ef dx translated">github.com</p></div></div></div></a></div></div></div>    
</body>
</html>