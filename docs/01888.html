<html>
<head>
<title>Want to Build Machine Learning Pipelines? A Quick Introduction using PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">想要建立机器学习管道？PySpark的快速介绍</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/want-to-build-machine-learning-pipelines-a-quick-introduction-using-pyspark-ede288927ce3?source=collection_archive---------15-----------------------#2019-11-19">https://medium.com/analytics-vidhya/want-to-build-machine-learning-pipelines-a-quick-introduction-using-pyspark-ede288927ce3?source=collection_archive---------15-----------------------#2019-11-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="52db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">花点时间思考一下这个问题——一名有抱负的数据科学家需要具备哪些技能才能获得行业职位？</p><p id="7e9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个<a class="ae jd" href="https://courses.analyticsvidhya.com/courses/applied-machine-learning-beginner-to-professional?utm_source=blog&amp;utm_medium=build-machine-learning-pipelines-pyspark" rel="noopener ugc nofollow" target="_blank">机器学习</a>项目有许多移动的组件，在我们成功执行它之前，需要将它们捆绑在一起。知道如何建立端到端机器学习管道的能力是一项宝贵的资产。作为一名数据科学家(有志或已确立)，你应该知道这些机器学习管道是如何工作的。</p><p id="4f46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单地说，这是两个学科的融合——数据科学和软件工程。对于数据科学家来说，这两者密不可分。这不仅仅是关于构建模型——我们需要拥有构建企业级系统的软件技能。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/a68b6e4fc0e388f46e4a1d996ba16fb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fE89CgxJCiOV2R-W.jpg"/></div></div></figure><p id="9bd8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，在本文中，我们将重点关注使用PySpark构建这些机器学习管道背后的基本思想。这是一篇实践文章，所以启动您最喜欢的Python IDE，让我们开始吧！</p><h1 id="7b86" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">在火花数据帧上执行基本操作</h1><p id="4edb" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">任何数据科学项目中必不可少的(也是第一步)是在构建任何<a class="ae jd" href="https://courses.analyticsvidhya.com/courses/applied-machine-learning-beginner-to-professional?utm_source=blog&amp;utm_medium=build-machine-learning-pipelines-pyspark" rel="noopener ugc nofollow" target="_blank">机器学习</a>模型之前理解数据。大多数数据科学的追求者都会在这里遇到困难，他们只是没有花足够的时间去理解他们正在从事的工作。有一种冲进去建立模型的趋势——这是你<em class="kt">必须</em>避免的谬误。</p><p id="25a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将在本文中遵循这一原则。我将始终遵循结构化的方法，以确保我们不会错过任何关键步骤。</p><p id="ab34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，让我们花点时间，理解一下我们将要用到的每个变量。我们将使用最近结束的<a class="ae jd" href="https://drive.google.com/file/d/1wUcLfGZ2HxvJNstZwM1e2CeNDTrJLy9b/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">印度对孟加拉国板球比赛</a>的数据集。让我们看看数据集中的不同变量:</p><ul class=""><li id="9b91" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated"><strong class="ih hj">击球手:</strong>唯一击球手id(整数)</li><li id="9af6" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">击球手姓名:</strong>击球手的姓名(字符串)</li><li id="13da" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">保龄球员:</strong>唯一保龄球员id(整数)</li><li id="0d3d" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">投球手姓名:</strong>投球手的姓名(字符串)</li><li id="7f38" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">注释:</strong>广播事件的描述(字符串)</li><li id="1d3f" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj"> Detail: </strong>描述事件的另一个字符串，如wickets和extra deliveries (String)</li><li id="9331" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">被解雇:被解雇的击球手的唯一Id(字符串)</li><li id="0738" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj"> Id: </strong>唯一的行Id(字符串)</li><li id="2137" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj"> Isball: </strong>交货是否合法(布尔型)</li><li id="05c7" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj"> Isboundary: </strong>击球手是否触界(二进制)</li><li id="f7cf" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">击球手是否出局(二进制)</li><li id="8652" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">过:</strong>过数(双)</li><li id="4d80" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">运行:</strong>对特定交货运行(整数)</li><li id="0a74" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">时间戳:</strong>记录数据的时间(时间戳)</li></ul><p id="db75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们开始吧，好吗？</p><h1 id="aaf0" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">读取CSV文件</h1><p id="f2fd" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">当我们给Spark加电时，<strong class="ih hj"> SparkSession </strong>变量以“<strong class="ih hj"> spark </strong>”的名称适当地可用。我们可以用这个来读取多种类型的文件，比如CSV，JSON，TEXT等。这使我们能够将数据保存为Spark数据帧。</p><p id="9cab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">默认情况下，它将所有列的数据类型视为字符串。您可以使用<strong class="ih hj">数据帧:</strong>上的<strong class="ih hj"> printSchema </strong>功能检查数据类型</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="d34a" class="ln jr hi lj b fi lo lp l lq lr"># read a csv file<br/>my_data = spark.read.csv('ind-ban-comment.csv',header=True)</span><span id="ad90" class="ln jr hi lj b fi ls lp l lq lr"># see the default schema of the dataframe<br/>my_data.printSchema()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lt"><img src="../Images/fd94212555e1e6626f09364aa9786348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/0*1YYR6UJo1VzUQ1fO.png"/></div></figure><h1 id="44c8" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">定义模式</h1><p id="bbb6" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">现在，我们不希望数据集中的所有列都被视为字符串。那么我们能做些什么呢？</p><p id="093a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以在Spark中为我们的数据框架定义自定义模式。为此，我们需要创建一个<strong class="ih hj"> StructType </strong>的对象，它接受一个<strong class="ih hj"> StructField </strong>的列表。当然，我们应该用列名、列的数据类型以及特定列是否允许空值来定义<strong class="ih hj"> StructField </strong>。</p><p id="d833" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请参考下面的代码片段，了解如何创建此自定义模式:</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="9ba2" class="ln jr hi lj b fi lo lp l lq lr">import pyspark.sql.types as tp</span><span id="8c7b" class="ln jr hi lj b fi ls lp l lq lr"># define the schema<br/>my_schema = tp.StructType([<br/>    tp.StructField(name= 'Batsman',      dataType= tp.IntegerType(),   nullable= True),<br/>    tp.StructField(name= 'Batsman_Name', dataType= tp.StringType(),    nullable= True),<br/>    tp.StructField(name= 'Bowler',       dataType= tp.IntegerType(),   nullable= True),<br/>    tp.StructField(name= 'Bowler_Name',  dataType= tp.StringType(),    nullable= True),<br/>    tp.StructField(name= 'Commentary',   dataType= tp.StringType(),    nullable= True),<br/>    tp.StructField(name= 'Detail',       dataType= tp.StringType(),    nullable= True),<br/>    tp.StructField(name= 'Dismissed',    dataType= tp.IntegerType(),   nullable= True),<br/>    tp.StructField(name= 'Id',           dataType= tp.IntegerType(),   nullable= True),<br/>    tp.StructField(name= 'Isball',       dataType= tp.BooleanType(),   nullable= True),<br/>    tp.StructField(name= 'Isboundary',   dataType= tp.BinaryType(),   nullable= True),<br/>    tp.StructField(name= 'Iswicket',     dataType= tp.BinaryType(),   nullable= True),<br/>    tp.StructField(name= 'Over',         dataType= tp.DoubleType(),    nullable= True),<br/>    tp.StructField(name= 'Runs',         dataType= tp.IntegerType(),   nullable= True),<br/>    tp.StructField(name= 'Timestamp',    dataType= tp.TimestampType(), nullable= True)    <br/>])</span><span id="df67" class="ln jr hi lj b fi ls lp l lq lr"># read the data again with the defined schema<br/>my_data = spark.read.csv('ind-ban-comment.csv',schema= my_schema,header= True)</span><span id="b0c6" class="ln jr hi lj b fi ls lp l lq lr"># <br/>my_data.printSchema()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lu"><img src="../Images/b3432622e63f4b7512c94ddaf2aa3299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bHl_dm_oyX2FtlcZ.png"/></div></div></figure><h1 id="059e" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">从数据中删除列</h1><p id="5dae" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">在任何机器学习项目中，我们总会有一些解决问题不需要的列。我相信你以前也遇到过这种困境，无论是在行业内还是在在线黑客马拉松上。</p><p id="78c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的实例中，我们可以使用drop函数从数据中删除该列。使用列表前的<strong class="ih hj">星号(*) </strong>从数据集中删除多个列:</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="73b5" class="ln jr hi lj b fi lo lp l lq lr"># drop the columns that are not required<br/>my_data = my_data.drop(*['Batsman', 'Bowler', 'Id'])<br/>my_data.columns</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lv"><img src="../Images/2ab35250b80e29014ad0017b4b58a215.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/0*2gcFBG5muFRAc1HE.png"/></div></figure><h1 id="0cbc" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">检查数据维度</h1><p id="41ae" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">与Pandas不同，Spark数据帧没有shape函数来检查数据的维度。我们可以使用下面的代码来检查数据集的维度:</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="1e77" class="ln jr hi lj b fi lo lp l lq lr"># get the dimensions of the data<br/>(my_data.count() , len(my_data.columns))<br/># &gt;&gt; (605, 11)</span></pre><h1 id="7007" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">描述数据</h1><p id="1da2" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">Spark的<strong class="ih hj"> describe </strong>函数为我们提供了大多数统计结果，如平均值、计数、最小值、最大值和标准偏差。您也可以使用<strong class="ih hj"> summary </strong>函数获得数值变量的四分位数:</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="a576" class="ln jr hi lj b fi lo lp l lq lr"># get the summary of the numerical columns<br/>my_data.select('Isball', 'Isboundary', 'Runs').describe().show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lw"><img src="../Images/d078918dd90a0d10266c915754150f46.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/0*IfKziT9FHIQIN504.png"/></div></figure><h1 id="4631" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">缺失值计数</h1><p id="40fd" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">我们很少得到没有任何缺失值的数据集。你还记得上次发生这种事是什么时候吗？</p><p id="f8cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">检查所有列中缺失值的数量非常重要。了解计数有助于我们在使用该数据构建任何<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/?utm_source=blog&amp;utm_medium=build-machine-learning-pipelines-pyspark" rel="noopener ugc nofollow" target="_blank">机器学习模型</a>之前处理缺失值。</p><p id="39d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，您可以使用下面的代码来查找数据集中的空值计数:</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="1711" class="ln jr hi lj b fi lo lp l lq lr"># import sql function pyspark<br/>import pyspark.sql.functions as f</span><span id="c8af" class="ln jr hi lj b fi ls lp l lq lr"># null values in each column<br/>data_agg = my_data.agg(*[f.count(f.when(f.isnull(c), c)).alias(c) for c in my_data.columns])<br/>data_agg.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lx"><img src="../Images/af1c2869e4995552dfbfb77cab87e1a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*k0nGlNZSUmkjdJsL.png"/></div></div></figure><h1 id="111e" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">列的值计数</h1><p id="f24c" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">与熊猫不同，我们在Spark数据帧中没有<em class="kt"> value_counts() </em>函数。您可以使用<strong class="ih hj"> groupBy </strong>函数计算分类变量的唯一值计数:</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="0b79" class="ln jr hi lj b fi lo lp l lq lr"># value counts of Batsman_Name column<br/>my_data.groupBy('Batsman_Name').count().show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ly"><img src="../Images/561919c6cd42c626a88f5252132b7e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/0*ndQIkDW61k4xJ186.png"/></div></figure><h1 id="ea49" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">使用PySpark编码分类变量</h1><p id="e747" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">大多数机器学习算法只接受数字形式的数据。因此，<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/?utm_source=blog&amp;utm_medium=build-machine-learning-pipelines-pyspark" rel="noopener ugc nofollow" target="_blank">将数据集中出现的任何分类变量</a>转换成数字是至关重要的。</p><p id="73c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请记住，我们不能简单地将它们从数据集中删除，因为它们可能包含有用的信息。仅仅因为我们不想知道如何使用它们而失去它将是一场噩梦！</p><p id="429e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看使用PySpark编码分类变量的一些方法。</p><h1 id="7a3d" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">字符串索引</h1><p id="72e9" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">字符串索引类似于<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2015/11/easy-methods-deal-categorical-variables-predictive-modeling/?utm_source=blog&amp;utm_medium=build-machine-learning-pipelines-pyspark" rel="noopener ugc nofollow" target="_blank">标签编码</a>。它为每个类别分配一个唯一的整数值。0被分配给最频繁的类别，1被分配给下一个最频繁的值，依此类推。我们必须定义我们想要索引的输入列名和我们想要结果的输出列名:</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="58be" class="ln jr hi lj b fi lo lp l lq lr">from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator</span><span id="062c" class="ln jr hi lj b fi ls lp l lq lr"># create object of StringIndexer class and specify input and output column<br/>SI_batsman = StringIndexer(inputCol='Batsman_Name',outputCol='Batsman_Index')<br/>SI_bowler = StringIndexer(inputCol='Bowler_Name',outputCol='Bowler_Index')</span><span id="7f8c" class="ln jr hi lj b fi ls lp l lq lr"># transform the data<br/>my_data = SI_batsman.fit(my_data).transform(my_data)<br/>my_data = SI_bowler.fit(my_data).transform(my_data)</span><span id="003c" class="ln jr hi lj b fi ls lp l lq lr"># view the transformed data<br/>my_data.select('Batsman_Name', 'Batsman_Index', 'Bowler_Name', 'Bowler_Index').show(10)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lz"><img src="../Images/7924705d3c4fb645be8618e4b10b243c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/0*9dfR5iT69PmOSwwp.png"/></div></figure><h1 id="ce20" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">一键编码</h1><p id="3bae" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">一键编码是每个数据科学家都应该知道的概念。在处理缺失值时，我多次依赖于它。真是救命恩人！</p><p id="441e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里有一个警告——Spark的<em class="kt"> OneHotEncoder </em>不直接编码分类变量。</p><blockquote class="ma mb mc"><p id="3d06" class="if ig kt ih b ii ij ik il im in io ip md ir is it me iv iw ix mf iz ja jb jc hb bi translated"><em class="hi">首先，我们需要使用字符串索引器将变量转换成数字形式，然后使用</em><strong class="ih hj"><em class="hi">OneHotEncoderEstimator</em></strong><em class="hi">对数据集的多列进行编码。</em></p></blockquote><p id="538b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它为每一行创建一个稀疏向量:</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="95c1" class="ln jr hi lj b fi lo lp l lq lr"># create object and specify input and output column<br/>OHE = OneHotEncoderEstimator(inputCols=['Batsman_Index', 'Bowler_Index'],outputCols=['Batsman_OHE', 'Bowler_OHE'])</span><span id="65ed" class="ln jr hi lj b fi ls lp l lq lr"># transform the data<br/>my_data = OHE.fit(my_data).transform(my_data)</span><span id="f0c9" class="ln jr hi lj b fi ls lp l lq lr"># view and transform the data<br/>my_data.select('Batsman_Name', 'Batsman_Index', 'Batsman_OHE', 'Bowler_Name', 'Bowler_Index', 'Bowler_OHE').show(10)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mg"><img src="../Images/683a86c42883570b678202cfadd07222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TazEmbk-ajzZRbDD.png"/></div></div></figure><h1 id="2016" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">向量汇编程序</h1><blockquote class="ma mb mc"><p id="77e8" class="if ig kt ih b ii ij ik il im in io ip md ir is it me iv iw ix mf iz ja jb jc hb bi translated">向量组装器将给定的列列表组合成一个向量列。</p></blockquote><p id="60f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这通常在数据探索和预处理步骤的最后使用。在这个阶段，我们通常使用一些原始或转换的特征来训练我们的模型。</p><p id="8708" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">向量组装器将它们转换成单个特征列，以便训练机器学习模型</strong>(例如<a class="ae jd" href="https://www.analyticsvidhya.com/blog/2015/11/beginners-guide-on-logistic-regression-in-r/?utm_source=blog&amp;utm_medium=build-machine-learning-pipelines-pyspark" rel="noopener ugc nofollow" target="_blank">逻辑回归</a>)。它接受数字、布尔和向量类型的列:</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="4ccc" class="ln jr hi lj b fi lo lp l lq lr">from pyspark.ml.feature import VectorAssembler</span><span id="fc55" class="ln jr hi lj b fi ls lp l lq lr"># specify the input and output columns of the vector assembler<br/>assembler = VectorAssembler(inputCols=['Isboundary',<br/>                                       'Iswicket',<br/>                                       'Over',<br/>                                       'Runs',<br/>                                       'Batsman_Index',<br/>                                       'Bowler_Index',<br/>                                       'Batsman_OHE',<br/>                                       'Bowler_OHE'],<br/>                           outputCol='vector')</span><span id="e70f" class="ln jr hi lj b fi ls lp l lq lr"># fill the null values<br/>my_data = my_data.fillna(0)</span><span id="a111" class="ln jr hi lj b fi ls lp l lq lr"># transform the data<br/>final_data = assembler.transform(my_data)</span><span id="9565" class="ln jr hi lj b fi ls lp l lq lr"># view the transformed vector<br/>final_data.select('vector').show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mh"><img src="../Images/cf871142ff2169eaf34e9af9a408060a.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/0*RsXMqCBR-aXJTZQU.png"/></div></figure><h1 id="c8a5" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">使用PySpark构建机器学习管道</h1><p id="c5b4" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">机器学习项目通常包括数据预处理、特征提取、模型拟合和结果评估等步骤。我们需要按顺序对数据执行大量转换。可以想象，跟踪它们可能会成为一项乏味的任务。</p><p id="5437" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是机器学习管道的用武之地。</p><blockquote class="ma mb mc"><p id="1a55" class="if ig kt ih b ii ij ik il im in io ip md ir is it me iv iw ix mf iz ja jb jc hb bi translated"><em class="hi">管道允许我们维护所有相关转换的数据流，这些转换是达到最终结果所必需的。</em></p></blockquote><p id="5ea2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们需要定义作为Spark运行的命令链的管道阶段。这里，<strong class="ih hj">每一级要么是变换器，要么是估计器。</strong></p><h1 id="9d98" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">变压器和估算器</h1><p id="8a77" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">顾名思义，<a class="ae jd" href="https://spark.apache.org/docs/latest/ml-pipeline.html#pipeline-components" rel="noopener ugc nofollow" target="_blank"> Transformers </a>通过更新特定列的当前值(比如将分类列转换为数值)或者使用定义的逻辑将其映射到其他值，将一个数据帧转换为另一个数据帧。</p><p id="0ac7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">估计器在数据帧上实现<em class="kt"> fit() </em>方法，并生成模型。例如，<em class="kt"> LogisticRegression </em>是我们调用<em class="kt"> fit() </em>方法时训练分类模型的估计器。</p><p id="4f47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们借助一些例子来理解这一点。</p><h1 id="a529" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">管道示例</h1><p id="5562" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">让我们创建一个包含三列的示例数据框架，如下所示。在这里，我们将定义我们希望转换数据的一些阶段，并了解如何设置管道:</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="3cdb" class="ln jr hi lj b fi lo lp l lq lr">from pyspark.ml import Pipeline</span><span id="4e7d" class="ln jr hi lj b fi ls lp l lq lr"># create a sample dataframe<br/>sample_df = spark.createDataFrame([<br/>    (1, 'L101', 'R'),<br/>    (2, 'L201', 'C'),<br/>    (3, 'D111', 'R'),<br/>    (4, 'F210', 'R'),<br/>    (5, 'D110', 'C')<br/>], ['id', 'category_1', 'category_2'])</span><span id="6d9e" class="ln jr hi lj b fi ls lp l lq lr">sample_df.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mi"><img src="../Images/931f2bd19756f2b752ed6a1671200989.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/0*CwKM-t33grr7bpqo.png"/></div></figure><p id="819c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经创建了数据框架。假设我们必须按以下顺序转换数据:</p><ul class=""><li id="8274" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated">stage_1:标签编码或字符串索引列<em class="kt"> category_1 </em></li><li id="fce8" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">stage_2:标签编码或字符串索引列<em class="kt"> category_2 </em></li><li id="1e7d" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">stage_3:对索引列进行一次热编码<em class="kt"> category_2 </em></li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mj"><img src="../Images/6ae6ac1ba9f329d3a592b53d8f9e9b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*08-NCckflIK_xMbA.png"/></div></div></figure><p id="2225" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在每个阶段，我们将传递输入和输出列名，并通过传递在<strong class="ih hj">管道</strong>对象列表中定义的阶段来设置管道。</p><p id="8720" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，管道模型按顺序一个接一个地执行某些步骤，并给出最终结果。让我们看看如何实现管道:</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="b837" class="ln jr hi lj b fi lo lp l lq lr"># define stage 1 : transform the column category_1 to numeric<br/>stage_1 = StringIndexer(inputCol= 'category_1', outputCol= 'category_1_index')<br/># define stage 2 : transform the column category_2 to numeric<br/>stage_2 = StringIndexer(inputCol= 'category_2', outputCol= 'category_2_index')<br/># define stage 3 : one hot encode the numeric category_2 column<br/>stage_3 = OneHotEncoderEstimator(inputCols=['category_2_index'], outputCols=['category_2_OHE'])</span><span id="2621" class="ln jr hi lj b fi ls lp l lq lr"># setup the pipeline<br/>pipeline = Pipeline(stages=[stage_1, stage_2, stage_3])</span><span id="4eeb" class="ln jr hi lj b fi ls lp l lq lr"># fit the pipeline model and transform the data as defined<br/>pipeline_model = pipeline.fit(sample_df)<br/>sample_df_updated = pipeline_model.transform(sample_df)</span><span id="9c1b" class="ln jr hi lj b fi ls lp l lq lr"># view the transformed data<br/>sample_df_updated.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mk"><img src="../Images/1048d6bc94e3b6af1f83808130efa2d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/0*GkzmLvgU_zAcT1Gg.png"/></div></figure><p id="8893" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们举一个建立管道的更复杂的例子。在这里，我们将对数据进行转换，并构建一个逻辑回归模型。</p><p id="d5b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为此，我们将创建一个样本数据帧，它将成为我们的训练数据集，具有四个特征和目标标签:</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="7648" class="ln jr hi lj b fi lo lp l lq lr">from pyspark.ml.classification import LogisticRegression</span><span id="7b22" class="ln jr hi lj b fi ls lp l lq lr"># create a sample dataframe with 4 features and 1 label column<br/>sample_data_train = spark.createDataFrame([<br/>    (2.0, 'A', 'S10', 40, 1.0),<br/>    (1.0, 'X', 'E10', 25, 1.0),<br/>    (4.0, 'X', 'S20', 10, 0.0),<br/>    (3.0, 'Z', 'S10', 20, 0.0),<br/>    (4.0, 'A', 'E10', 30, 1.0),<br/>    (2.0, 'Z', 'S10', 40, 0.0),<br/>    (5.0, 'X', 'D10', 10, 1.0),<br/>], ['feature_1', 'feature_2', 'feature_3', 'feature_4', 'label'])</span><span id="0f63" class="ln jr hi lj b fi ls lp l lq lr"># view the data<br/>sample_data_train.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ml"><img src="../Images/10bb7e5cba0f8999d18f7884684beeeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/0*O9IYoo4XcRnTCB2a.png"/></div></figure><p id="5241" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，假设我们的管道顺序如下:</p><ul class=""><li id="6d3a" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated">stage_1:标签编码或字符串索引列<em class="kt"> feature_2 </em></li><li id="2660" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">stage_2:对列进行标签编码或字符串索引<em class="kt"> feature_3 </em></li><li id="c816" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">stage_3:对<em class="kt"> feature_2 </em>和<em class="kt"> feature_3 </em>的索引列进行一次热编码</li><li id="7957" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">阶段4:创建训练逻辑回归模型所需的所有特征的向量</li><li id="57f9" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated">阶段5:建立逻辑回归模型</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mm"><img src="../Images/4c91477a2aceb72638f8e97484da3079.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pYtEqYKexnop3BsE.png"/></div></div></figure><p id="1507" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们必须通过提供输入列名和输出列名来定义阶段。最后一步是建立逻辑回归模型。最后，当我们在训练数据集上运行管道时，它将按顺序运行步骤，并向数据帧添加新列(如rawPrediction、probability和Prediction)。</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="db68" class="ln jr hi lj b fi lo lp l lq lr"># define stage 1: transform the column feature_2 to numeric<br/>stage_1 = StringIndexer(inputCol= 'feature_2', outputCol= 'feature_2_index')<br/># define stage 2: transform the column feature_3 to numeric<br/>stage_2 = StringIndexer(inputCol= 'feature_3', outputCol= 'feature_3_index')<br/># define stage 3: one hot encode the numeric versions of feature 2 and 3 generated from stage 1 and stage 2<br/>stage_3 = OneHotEncoderEstimator(inputCols=[stage_1.getOutputCol(), stage_2.getOutputCol()], <br/>                                 outputCols= ['feature_2_encoded', 'feature_3_encoded'])<br/># define stage 4: create a vector of all the features required to train the logistic regression model <br/>stage_4 = VectorAssembler(inputCols=['feature_1', 'feature_2_encoded', 'feature_3_encoded', 'feature_4'],<br/>                          outputCol='features')<br/># define stage 5: logistic regression model                          <br/>stage_5 = LogisticRegression(featuresCol='features',labelCol='label')</span><span id="ba6e" class="ln jr hi lj b fi ls lp l lq lr"># setup the pipeline<br/>regression_pipeline = Pipeline(stages= [stage_1, stage_2, stage_3, stage_4, stage_5])</span><span id="ff53" class="ln jr hi lj b fi ls lp l lq lr"># fit the pipeline for the trainind data<br/>model = regression_pipeline.fit(sample_data_train)<br/># transform the data<br/>sample_data_train = model.transform(sample_data_train)</span><span id="408a" class="ln jr hi lj b fi ls lp l lq lr"># view some of the columns generated<br/>sample_data_train.select('features', 'label', 'rawPrediction', 'probability', 'prediction').show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mn"><img src="../Images/24f72a0d8d21e6f2e049e6cee626e044.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/0*ASK79gMAFYQkcJgy.png"/></div></figure><p id="0008" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">恭喜你。我们已经成功地建立了管道。让我们创建一个没有标签的样本测试数据集，这一次，我们不需要再次定义所有的步骤。我们只要通过管道传递数据，就大功告成了！</p><pre class="jf jg jh ji fd li lj lk ll aw lm bi"><span id="c1c2" class="ln jr hi lj b fi lo lp l lq lr"># create a sample data without the labels<br/>sample_data_test = spark.createDataFrame([<br/>    (3.0, 'Z', 'S10', 40),<br/>    (1.0, 'X', 'E10', 20),<br/>    (4.0, 'A', 'S20', 10),<br/>    (3.0, 'A', 'S10', 20),<br/>    (4.0, 'X', 'D10', 30),<br/>    (1.0, 'Z', 'E10', 20),<br/>    (4.0, 'A', 'S10', 30),<br/>], ['feature_1', 'feature_2', 'feature_3', 'feature_4'])</span><span id="64b0" class="ln jr hi lj b fi ls lp l lq lr"># transform the data using the pipeline<br/>sample_data_test = model.transform(sample_data_test)</span><span id="1f62" class="ln jr hi lj b fi ls lp l lq lr"># see the prediction on the test data<br/>sample_data_test.select('features', 'rawPrediction', 'probability', 'prediction').show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mo"><img src="../Images/f368bbd955d470aaf80d3a2788a2a5f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/0*MB4vwBrGZK4OwrpK.png"/></div></figure><p id="4c71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">完美！</p><p id="9a91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一篇关于如何使用PySpark构建机器学习管道的简短但直观的文章。我将再次重申这一点，因为它非常重要——你需要知道这些管道是如何工作的。这是您作为数据科学家的重要职责。</p><p id="300a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你以前做过端到端的机器学习项目吗？或者是在工业环境下建造这些管道的团队的一员？下面在评论区连线，一起讨论吧。</p><p id="2891" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将在PySpark初学者系列的下一篇文章中再见。快乐学习！</p></div><div class="ab cl mp mq gp mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="hb hc hd he hf"><p id="8cdf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kt">原载于2019年11月19日</em><a class="ae jd" href="https://www.analyticsvidhya.com/blog/2019/11/build-machine-learning-pipelines-pyspark/?utm_source=av&amp;utm_medium=feed-articles&amp;utm_campaign=feed" rel="noopener ugc nofollow" target="_blank"><em class="kt">https://www.analyticsvidhya.com</em></a><em class="kt">。</em></p></div></div>    
</body>
</html>