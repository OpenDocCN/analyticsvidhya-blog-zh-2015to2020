<html>
<head>
<title>Unsupervised Learning — Principal Component Analysis (PCA)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无监督学习—主成分分析(PCA)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/unsupervised-learning-principal-component-analysis-pca-dc94fecef09b?source=collection_archive---------5-----------------------#2020-03-29">https://medium.com/analytics-vidhya/unsupervised-learning-principal-component-analysis-pca-dc94fecef09b?source=collection_archive---------5-----------------------#2020-03-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="980a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">来自印度的Namaste！(电晕效应)。借我你的耳朵/眼睛来深入研究PCA。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/f56021f2f8d979af6029e7cf6b7cd930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*434U170St9pKULGy"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">随机图片(为了引起注意:)</figcaption></figure><p id="dccb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们以新冠肺炎数据集为例，与特征或变量的数量相比，数据点的数量非常少，这导致了<strong class="ih hj">维数灾难</strong>错误，PCA成了救星。主成分分析是一种降维技术，它通过识别数据集中的相关性和模式，从而可以将其转换为维度显著降低的数据集，而不会丢失任何重要信息。<br/>因此，这是一种以特定方式组合i/p变量/特征的特征提取技术，以便<strong class="ih hj">丢弃“最不重要”的变量</strong>，同时仍然保留所有变量中最有价值的部分。PCA后的每个“新”变量都是相互独立的。这是一个好处，因为线性模型的假设要求我们的自变量相互独立。如果我们决定用这些“新”变量来拟合一个线性回归模型，这个假设必然会得到满足。</p><p id="f455" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">事实上，每一个主分量总是与其他主分量正交。因为我们的主成分彼此正交，所以它们在统计上彼此线性独立。<strong class="ih hj">一般情况下，一个数据集的第n个主成分垂直于同一数据集的第(n — 1)个主成分。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jt"><img src="../Images/1cfb175c0c686506d9700315ea9b78bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5JM3hqmrmlkxaT4v"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">主成分分析</figcaption></figure><p id="7c0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">主成分分析是一种旋转数据集的方法，旋转后的要素在统计上不相关。这种旋转通常会根据新要素对解释数据的重要性，仅选择新要素的子集。<br/> PCA是一种减少数据集中独立变量数量的方法，尤其适用于数据点与独立变量的比率较低的情况。PCA变换变量的线性组合，使得结果变量表示变量组合内的最大方差。</p><p id="81ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">主成分分析(PCA)是一种统计过程，它使用正交变换将一组可能相关的变量的观察值转换为一组线性不相关的变量值，称为主成分。<br/>这种变换定义为第一主成分具有最大可能的方差(即尽可能多地解释数据中的可变性)，PCA主要用作探索性数据分析和建立预测模型的工具。它通常用于可视化种群间的遗传距离和亲缘关系。PCA可以通过数据协方差(或相关性)矩阵的特征值分解或数据矩阵的奇异值分解来完成，通常在初始数据的归一化步骤之后。每个属性的归一化包括均值中心-从变量的测量均值中减去每个数据值，使其经验均值(平均值)为零-以及，<br/>可能的话，归一化每个变量的方差，使其等于1；请参见Z分数。并且每个随后的分量又在与前面的分量正交的约束下具有可能的最高方差。</p><p id="d2ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">= &gt;高度相关的特征——意味着高偏差——去掉1个变量。</p><p id="23ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">=&gt;PCA去除不一致、冗余数据和高度相关的特征。</p><p id="9571" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">= &gt;非参数并解决过度拟合-由高维度引起。过滤噪声数据集，如图像压缩。</p><p id="cb8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><br/>PCA步骤:<strong class="ih hj"> 1。数据标准化:- </strong> <br/>缩放数据PCA中主要的预处理步骤是使用z-score或standardscalar缩放变量，以缩放每个变量，使两者具有相似的范围，从而使方差具有可比性。<br/> <strong class="ih hj"> z =变量值—均值/标准差</strong> <br/> <strong class="ih hj"> 2。计算协方差矩阵:- </strong>衡量每个变量如何相互关联。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ju"><img src="../Images/be0cf331be4e42005a329be35bcc4f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/0*_3O-6AkYObIHmSx8.png"/></div></figure><p id="bf34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。计算特征向量和特征值:- </strong> <br/>我们的数据分散的方向。(特征向量。)-线性变换后方向不变的向量。<br/>这些不同方向的相对重要性。(特征值。)—去除相关特征向量的标量<br/> PCA结合了我们的预测器，并允许我们去除相对不重要的特征向量。<br/> <strong class="ih hj"> 4。计算主成分:- </strong> <br/>主成分基本上是线性不相关且在数据中有方差的向量。从主成分中挑选出方差最大的前p个。<br/> PCA变换变量的线性组合，使得结果变量表达变量组合内的最大方差。<br/>通过以降序对特征向量进行排序来获得来自实际的新变量集，其中具有最高特征值的特征向量是最重要的，因此形成第一主分量。<br/> <strong class="ih hj"> 5。在不损失信息的情况下，通过选择最佳成分来降低数据的维数。</strong></p><p id="8a5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Python代码示例:- </strong></p><p id="a4ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"># scaling <br/>从scipy.stats导入zscore<br/>XScaled = x . apply(zscore)<br/>XScaled . head()<br/>#协方差<br/> covMatrix = np.cov(XScaled，row var = False)<br/>print(cov matrix)<br/># PCA<br/>PCA = PCA(n _ components = 6)<br/>PCA . fit(XScaled)<br/>#特征值<br/> print(pca.explained</p><p id="7e19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">#根据这些值选择3个最佳组件，然后使用这些组件特征进行回归<br/>PC a3 = PCA(n _ components = 3)<br/>PC a3 . fit(x scaled)<br/>print(PC a3 . components _)<br/>print(PC a3 . explained _ variance _ ratio _)<br/>xpca 3 = PC a3 . transform(x scaled)</p><p id="1819" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">regression _ model _ PCA = linear regression()<br/>regression _ model _ PCA . fit(xpca 3，y)<br/>regression _ model _ PCA . score(xpca 3，y)</p><p id="017e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">reference:-<br/><a class="ae jv" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn . org/stable/modules/generated/sk learn . decomposition . PCA . html</a><br/><a class="ae jv" rel="noopener" href="/@raghavan99o/principal-component-analysis-pca-explained-and-implemented-eeab7cb73b72">https://medium . com/@ raghavan 99 o/principal-component-analysis-PCA-explained-and-implemented-eeab 7 CB 73 b 72</a><br/><a class="ae jv" rel="noopener" href="/@aptrishu/understanding-principle-component-analysis-e32be0253ef0">https://medium . com/@ aptrishu/understanding-principle-component-analysis-e32be 0253 ef 0【参考</a></p><p id="a7dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关注我的其他博客:-</p><p id="2f9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jv" rel="noopener" href="/@nishu61988/dataset-sources-for-datascience-lovers-7484b3f78a8f?source=your_stories_page---------------------------">数据科学爱好者的数据集来源</a></p><p id="8984" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jv" href="https://www.facebook.com/NishuArt-109507937410651/?modal=admin_todo_tour" rel="noopener ugc nofollow" target="_blank">脸书</a> | <a class="ae jv" href="https://www.instagram.com/datasciencewithmemes/" rel="noopener ugc nofollow" target="_blank"> Instagram </a></p></div></div>    
</body>
</html>