<html>
<head>
<title>Zero-Shot Cross-Lingual Transfer with Meta-Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">元学习下的零元跨语言迁移</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/zero-shot-cross-lingual-transfer-with-meta-learning-de053b7c5215?source=collection_archive---------24-----------------------#2020-11-01">https://medium.com/analytics-vidhya/zero-shot-cross-lingual-transfer-with-meta-learning-de053b7c5215?source=collection_archive---------24-----------------------#2020-11-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1631" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">原作者:法尔哈德·诺拉拉扎德、詹尼斯·贝库里斯、约翰内斯·比耶瓦、伊莎贝尔·奥根斯坦</em></p><p id="2f15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【https://arxiv.org/pdf/2003.02739.pdf】文章链接:<a class="ae je" href="https://arxiv.org/pdf/2003.02739.pdf" rel="noopener ugc nofollow" target="_blank">T5T7】</a></p><p id="4109" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">发布日期:2020年10月5日</em></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jf"><img src="../Images/8a8e3ecc705460fd42e47fe92a694022.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HGVezN5_fFbDaE7m"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">图片:Unsplash/Andrew Stutesman</figcaption></figure><h1 id="154e" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">简介</strong></h1><p id="ccab" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">知识的战略性共享已经证明可以提高任务绩效。由于大多数语言资源不足，知识的战略性共享对于多语言应用变得非常重要。我们考虑建立一个除英语之外的多种语言的训练模型，在这种语言中没有数据或只有很少的数据可用。在本文中，我们将看到这个挑战可以通过使用元学习来解决。除了训练源语言模型，另一个模型学习选择哪个训练实例对第一个更有用。</p><p id="264a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所执行的实验是标准监督的、零镜头跨语言的，以及针对不同自然语言理解任务(自然语言推理、问题回答)的少量镜头跨语言设置。这些广泛的实验证明了使用总共15种语言的元学习的一致有效性。对于零炮、少炮NLI数据集，使用的是MultiNLI和XNLI，在MLQA数据集上进行了质量保证。</p><p id="d9d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">错误分析部分表明，语言之间的类型特征的相关性可以在一定程度上解释通过元学习学习的参数共享何时是有益的。</p><h1 id="89c5" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">文章的范围和背景</h1><p id="3281" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">世界上有7000种语言，尽管如此，当涉及到自然语言理解任务时，很少有语言具有适当的语言资源。大多数自然语言处理研究仍然考虑用英语进行研究，XNLI数据集证明了这一点。因此，这个问题的一个解决方案是收集带注释的数据，但这是一个耗时且不可行的过程。此外，用一种特定的语言为一项任务训练一个模型并将其直接应用于另一种语言也是很重要的，因为数据有限，所以探索允许使用大量训练数据(即英语)的策略变得至关重要，这样其他语言也可以受益。元学习最近被证明对机器学习任务是有用的。另外，对于NLP，最近的工作显示了任务和域之间共享的有用性。该条的范围</p><p id="654a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1)提出了一个跨语言元学习架构(X-MAML ),并针对以下两个自然语言理解任务进行了研究</p><p id="8fd0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a)自然语言推理</p><p id="7364" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b)提问和回答</p><p id="a183" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2)测试X-MAML在跨领域、跨语言、标准监督、少镜头以及零镜头学习方面的能力，总共涵盖15种语言。</p><p id="fa9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3)观察包括多语言BERT和XLM-罗伯塔在内的强大模型的持续改进。</p><p id="e3a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4)进行广泛的错误分析，发现跨语言趋势可以部分地由语言之间的类型共性来解释。</p><h1 id="53ed" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">元学习</h1><p id="a5c5" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">元学习是指在很少训练实例的情况下，能够快速学习新技能或适应新环境的过程。这是通过使用许多高资源任务重复模拟低资源任务的学习过程来实现的。有几种方法来执行元学习。</p><p id="7a14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-基于度量:目的是在给定相似性度量的情况下，学习来自不同训练集的实例的特征表示之间的相似性</p><p id="597c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-基于模型:重点是调整学习速度快的模型(如记忆网络)</p><p id="2c47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-基于优化:能够找到良好的初始化参数值，并快速适应新任务。本文考虑基于优化的方法。</p><p id="1b54" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模型不可知元学习(MAML)的使用已经在这篇文章中被开发，它使用梯度下降，并实现了对各种任务的良好推广。假设这些新的目标任务来自同一个分布，MAML能够通过在测试时只使用几个实例来快速适应新的目标任务。</p><p id="8547" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设任务{T1，T2，…，Tk}的分布p(T)在MAML。对于特定的任务，比如说Ti，从分布中采样的模型M的参数被更新为θi 0。具体来说，参数θ使用任务Ti的训练示例上的梯度下降步骤的一次或几次迭代来更新。例如，对于一个梯度更新，</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es ky"><img src="../Images/1b26df069e35b4440d4b1287c7a62fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8AkdC-nnVy5kxZZjrVQECA.png"/></div></div></figure><p id="9ecb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上式中，α是步长，Mθ是从神经网络学习的模型，LTi是特定任务Ti的损失。通过训练模型θ的参数来完成跨任务p(T)的看不见的测试示例(即，Dtest i)上的Mθ 0的性能优化。下面的等式表明了元学习的目标。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es kz"><img src="../Images/f8640dd664243746e859e0ce43ef912b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kSK__tr_AFWpkkrapzXC-A.png"/></div></div></figure><p id="7681" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MAML算法的目标是在一个新的也是元更新的任务上通过几个梯度步骤来优化模型参数。</p><p id="8c04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面的等式表明元更新在所有涉及的任务中使用随机梯度下降(SGD)对模型的θ参数进行更新。β是元更新步长。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es la"><img src="../Images/e5a76abd882fc453e5ef0b1789bc7fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HAjh3txEPC9hWRXBa26_vw.png"/></div></div></figure><h1 id="63b5" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">跨语言元学习</h1><p id="2d8e" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">这里使用MAML(或任何NLP任务)的主要思想是使用一堆高资源辅助任务/语言来找到最佳初始化，从该初始化学习目标任务/语言可以仅使用少量训练实例来完成。当只有英语数据集可用作高资源语言，并且只有少量实例可用于其他语言(跨语言设置，即XNLI，MLQA)时，用于MAML的训练过程需要相当大的改变。为此，跨语言元学习框架(XMAML)被引入。XMAML使用以下培训步骤:</p><p id="5551" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1)英语(高资源语言)上的预训练所有训练样本都是用高资源语言给出的，先在h上训练模型M，初始化模型参数θ。</p><p id="880f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2)使用低资源语言的元学习:</p><p id="4c50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-从低资源集中选择一种或多种辅助语言。</p><p id="e115" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-使用每个辅助语言的开发集构建随机抽样的任务批Ti。</p><p id="08d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-通过一个梯度下降步骤，使用Ti (Dtrain i)的K个数据点更新模型参数</p><p id="4ecf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-在每个任务中使用Q个示例(Dtest i)计算损失值。应当注意，用于训练的K个数据点(Dtrain i)不同于用于测试的Q个数据点(Dtest i)。</p><p id="2bac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-对所有任务的损失值求和，以最小化元目标函数，并使用第三个等式执行元更新。该步骤在多次迭代中执行。</p><p id="0d17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3)对目标语言的零投或少投学习:</p><p id="7f9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-这是X-MAML的最后一步，这里我们用元学习期间学习到的参数初始化模型参数。</p><p id="cfdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-通过在目标语言的测试集上评估模型(即零触发学习)或使用目标语言的开发集通过标准监督学习微调模型参数并在测试集上评估(即少触发学习)来继续。在下面的算法中给出了所提出的模型X-MAML的更正式的描述)</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lb"><img src="../Images/8ee86e91437941d93a41bcc98f96f703.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*w_ud6BGLq2-kycz_Bgl-1A.png"/></div></figure><p id="fce8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">自然语言推理(NLI): </strong></p><p id="32fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用NLI，我们可以预测一个假设句是中性的，真的，还是假的给定一个前提句。MultiNLI(多体裁自然语言推理)数据集具有433，000个用文本蕴涵信息标注的句子对。这支持跨类型评估，有近10种不同的类型:面对面，电话，逐字，国家，政府，小说，信件，911，旅游和牛津大学出版社。这些类型出现在测试和开发集中，其中五个包含在训练集中。每种体裁的NLI任务都被定义为Ti，以便更普遍地验证学习程序。正如本文前面提到的，MAML是在其原始设置中开发的。并调查元学习是否鼓励模型学习所有目标体裁的良好初始化，然后可以在较少监督的情况下对每个体裁开发实例进行微调，以在其测试集上实现良好的性能。XNLI数据集有5000个测试对和2500个开发对，带有英文文本标签，所有这些都提供了14种语言的翻译</p><p id="70ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">法语(fr)、西班牙语(es)、德语(de)、希腊语(el)、保加利亚语(bg)、俄语(ru)、土耳其语(tr)、阿拉伯语(ar)、越南语(vi)、泰语(th)、汉语(zh)、印地语(hi)、斯瓦希里语(sw)和乌尔都语(ur)。</p><p id="609d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个数据集还提供了一个基准来评估如何在低资源语言中执行推理。这个过程将允许我们研究使用一种低资源语言作为辅助语言的元学习的效果，并在XNLI测试集中提供的目标语言上评估由此产生的NLI模型。</p><p id="575d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">问题回答(QA): </strong></p><p id="d029" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">问答的任务是在回答问题的上下文中识别跨度。多语言问答数据集，包含7种语言的问答实例:英语(en)、阿拉伯语(ar)、德语(de)、西班牙语(es)、印地语(hi)、越南语(vi)和简体中文(zh)。它有超过12k的英语问答实例和5k的其他语言，每个问答实例可在4种语言。本文通过一种或两种辅助语言对问答系统的元学习进行了实验研究。</p><h1 id="cc7d" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">实验</h1><p id="e203" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">主要的焦点是调查元学习如何用于跨语言的分享。使用高级库2的XMAML与Adam optimizer一起使用。对于零投和少投学习，使用32的批量。试验了一些不同的架构，以验证该方法在它们之间的通用性。结果给出了少数镜头，零镜头跨领域，跨语言学习</p><p id="7f8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> NLI </strong>:使用两种设置进行实验。</p><p id="a196" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(I)对于跨类别数据集MultiNLI，采用了增强型顺序推理模型(ESIM)。ESIM注意使用逻辑推理来创造丰富的表达，捕捉前提和假设句子之间的关系。</p><p id="70aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(ii)对于XNLI，使用PyTorch版本的BERT，集成了Hugging Face的库作为底层模型M，但是所提出的元学习方法是模型竞争的，并且它也可以扩展到其他体系结构</p><p id="baf4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注:设置i) MAML适用，设置ii)X-MAML适用于原始英语BERT模型(En-BERT)和多语言BERT (Multi-BERT)模型。</p><p id="d7d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> QA </strong>:问答使用X-MAML的基础模型M，即XLM(和XLM-罗伯塔(XLM-R)。</p><p id="5d7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">提到的两个基线:(I)零测试基线:这个基线在目标语言的测试集上直接评估模型。(ii)少量基线:这个基线在开发集上微调模型，然后在低资源语言的测试集上评估。</p><h2 id="f4ee" class="lc jw hi bd jx ld le lf kb lg lh li kf iq lj lk kj iu ll lm kn iy ln lo kr lp bi translated"><strong class="ak"> 1。少拍跨域NLI </strong></h2><p id="a58b" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">在MultiNLI训练集上训练ESIM，以提供初始模型参数θ。在XNLI的英语测试集(因为MultiNLI测试集不是公开可用的)上评估预训练模型作为基线。在MAML把每一种体裁作为一项任务。包括元学习期间的训练集(5种类型)或发展集(10种类型)。然后首先用MAML学习的参数初始化模型参数。后来，实验继续使用MultiNLI的开发集对模型进行微调，然后在XNLI的英语测试集上报告准确性。随着更多实例的出现，所有模型(包括基线)的性能都会提高。</p><h2 id="9134" class="lc jw hi bd jx ld le lf kb lg lh li kf iq lj lk kj iu ll lm kn iy ln lo kr lp bi translated">2.零杆和少数杆跨语言NLI</h2><p id="9d07" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated"><strong class="ih hj">零触发学习:</strong>零触发学习:XMAML框架集成在零触发设置中。此后的元学习步骤没有针对每种目标语言的元学习的微调和影响，因为在测试集上报告了在基线模型(Multi-BERT)之上有和没有元学习的准确性差异。通过对每种目标语言使用一种辅助语言来报告平均和最高性能。通过在X-MAML中使用两种辅助语言，在我们的零射击实验中获得了最大的收益。</p><p id="96d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">少镜头学习:</strong>通过微调在目标语言的开发集上进行X-MAML的元学习，然后在测试集上进行评估。将X-MAML结果与一种或两种辅助语言的内部和外部基线进行比较。</p><h2 id="65aa" class="lc jw hi bd jx ld le lf kb lg lh li kf iq lj lk kj iu ll lm kn iy ln lo kr lp bi translated">3.零镜头跨语言问答</h2><p id="a72d" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">一种类似的方法用于MLQA数据集上的跨语言问答。MLQA的零炮结果如下所示。所有目标语言都受益于至少一种辅助语言的元学习。在X-MAML中使用两种辅助语言可以改善结果。简而言之，使用X-MAML的零起点学习模型优于内部和外部基线。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es lq"><img src="../Images/8ec615ce2d09a5084bc2ce9d005a852e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qXWXYLASDgVQK844uwqF6g.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">F1分数(平均超过10次运行)在MLQA测试集使用零杆X-MAML。列表示目标语言。avg列表示行平均F1分数。我们还报告了X-MAML在提高每种目标语言的F1测试中最有益的辅助语言。</figcaption></figure><h2 id="05ab" class="lc jw hi bd jx ld le lf kb lg lh li kf iq lj lk kj iu ll lm kn iy ln lo kr lp bi translated">相关著作</h2><p id="0125" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">这项任务的主要灵感来自世界上大多数语言的较少可用的标记训练数据集。为了解决这个问题，已经提出了许多方法，包括少量学习。少镜头学习方法最近被应用于自然语言处理任务。具体来说，在NLP中，这些少量学习方法包括将问题转化为不同的任务或元学习</p><p id="67f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">元学习:</strong>元学习或学会学习最近受到了NLP社区的广泛关注。</p><p id="1643" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">跨语言NLU: </strong>跨语言学习在自然语言处理中的历史相当短，并且主要局限于传统的自然语言处理任务，例如词性标注、形态变化和句法分析。</p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es lr"><img src="../Images/1f48580cfd65639f1ebec6b1d8078654.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*b2F6GyeKIoKQL6BQTA7VBQ.png"/></div></figure><h1 id="099c" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">讨论和分析</h1><p id="62ee" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">跨语言迁移:跨语言迁移和元学习产生了更好的结果，即使语言之间有很大的差异。这表明用X-MAML学习的元参数是充分的语言不可知的。这取决于能否获得预先训练的多语言模型，如BERT，尽管单语BERT (En-BERT)在一些目标/辅助设置中具有非常积极的增益。对于少量学习，发现仍然相似。</p><p id="bea7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类型学相关性:文章提到了类型学特征的研究，以及它们在目标语言和辅助语言之间的重叠，以便更好地解释跨语言零习得和少习得的结果。世界语言结构地图集(WALS)被评估(这是最大的公开可用的类型学数据库。文章提到从现有的工作中获得灵感，这些工作试图基于在各种NLP任务下学习的语言表征来预测类型学特征。在实验中使用了两个条件:(I)尝试使用X-MAML基于性能中的相互增益/损失来预测类型学特征(ii)在所研究的情况中，在两种类型学相似的语言之间的共享有助于使用X-MAML的性能。在某些情况下，发现语言共享WALS特征25A标记位点的特征值:全语言类型学通常互相帮助。这一特征描述了一种语言的形态句法标记是依赖于短语还是句法中心。例如，en、de、ru和zh在此功能中是“相关标记”。因此，人们发现，当使用X-MAML时，具有相似形态句法属性的语言可以相互受益。</p><h1 id="5f51" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">结论</h1><p id="5edc" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">在这篇文章中，我们看到元学习可以有效地利用来自辅助语言的训练数据进行零元和少量元的跨语言迁移。文章谈到了对两个具有挑战性的NLU任务(NLI和问答)和总共15种语言的评估。此外，我们还讨论了零炮XNLI和零炮QA在MLQA数据集上的最新基线模型的性能改进。</p><p id="22a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，文章表明，在类型学分析中，语言共享某些形态句法特征往往会从迁移类型中获得优势。本文将进一步将这一研究/工作扩展到其他跨语言的NLP任务和更多的语言。</p><h1 id="4154" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">参考</h1><p id="5bb3" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">更多资源可以在主文章中找到。</p><p id="7384" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Mostafa Abdou、Cezar Sas、Rahul Aralikatte、Isabelle Augenstein和Anders sgaard。2019.X-WikiRE:作为机器理解的关系抽取的大型多语言资源。《第二届低资源NLP深度学习方法研讨会论文集》(DeepLo 2019)，第265–274页，中国香港。计算语言学协会。</p><p id="e5a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">泽利科·阿吉奇和娜塔莉·施鲁特。2018.跨语言推理的基线和测试数据。在LREC。欧洲语言资源协会(ELRA)。</p><p id="32d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">戴维·m·埃伯哈德、加里·f·西蒙斯和查尔斯·d·芬尼格。2019.民族志:世界语言。<a class="ae je" href="https://www.ethnologue.com/" rel="noopener ugc nofollow" target="_blank">https://www.ethnologue.com/</a>统计/规模。访问时间:2019–05–25。</p></div></div>    
</body>
</html>