<html>
<head>
<title>Ensemble Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ensemble-learning-bb08c6cf4b59?source=collection_archive---------13-----------------------#2019-10-07">https://medium.com/analytics-vidhya/ensemble-learning-bb08c6cf4b59?source=collection_archive---------13-----------------------#2019-10-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="f1bb" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">在机器学习中，群体的表现真的比个体好吗？</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/75912597c881ab9ad11ffeae3a6f3c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HdYjILMB7kbJ_3FZ"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">https://your story . com/journal/the-wisdom-of-the-crowds-can-it-beat-or-match-expe-ZL 424 v9h4k</figcaption></figure><p id="ceb6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这篇文章中，我们将讨论一点关于集成学习。这种技术非常擅长为分类或回归任务建立模型。一般来说，集成学习技术的工作原理是假设将一些糟糕的机器学习算法结合在一起执行一项任务可能比使用单一算法更好。</p><p id="eb22" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">作为一个例子，最强大的机器学习算法之一，随机森林，是由一堆决策树建立起来的。它包括训练一组决策树，每一个决策树都基于训练集的一个随机子集，对所有单个的树进行预测，并从中挑选出获得最多投票的类。</p><h2 id="0969" class="kk kl hi bd km kn ko kp kq kr ks kt ku jx kv kw kx kb ky kz la kf lb lc ld le bi translated"><strong class="ak">投票分类器</strong></h2><p id="db64" class="pw-post-body-paragraph jo jp hi jq b jr lf ij jt ju lg im jw jx lh jz ka kb li kd ke kf lj kh ki kj hb bi translated">这种技术被称为<strong class="jq hj">硬投票</strong>分类器，也可以用于其他类别的算法，如SVM、逻辑回归等。此外，还有一种称为<strong class="jq hj">软投票</strong>分类器的技术，它包括从所有单个分类器的预测中平均选出具有最高分类概率的分类。</p><p id="dab2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在Python中，投票分类器可以用下图中的代码实现。它使用决策树、支持向量机和逻辑回归来建立预测模型。用于本演示的数据集将使用函数make_moons从sklearn.datasets创建。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lk"><img src="../Images/3660d6a298aad119bbc0b1d299992f70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*C1r1-IXlhJUwXm6HW8kOaw.png"/></div></figure><p id="30ba" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">使用创建的数据集，我们将把它分成训练和测试数据集，然后我们实现分类器和硬投票<strong class="jq hj"> </strong>集成，之后对其进行拟合和测试，如下所示:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ll"><img src="../Images/1a115ccf698135e82453982cbf8f4e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*6TlUZhUFTfNhf0EShvudzg.png"/></div></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lm"><img src="../Images/2e4a3ffb49822896d660b8ec574e70aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*9jIyIF6Nuj35C3HK7VIKJw.png"/></div></figure><p id="4967" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了创建软投票分类器，必须设置voting= "True "并且<br/>使所有分类器估计类别概率，就像在SVM分类器中所做的那样:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ln"><img src="../Images/db11a49a5466cdfa9706be4c7c5d1d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*0Wx2ohgq0MY3VbFm02x2UQ.png"/></div></figure><p id="1469" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">从结果中可以看出，软投票比硬投票的表现稍好，这种情况经常发生，因为软投票给予更有信心的投票更多的权重。</p><p id="d82f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">装袋粘贴</strong></p><p id="8afc" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">将一组不同的分类器组合成一组的另一种方式是对每个预测器使用相同的训练算法，然而，在训练集的随机样本上训练它们。在这种方法中，取样可以通过替换(<strong class="jq hj">装袋</strong>，或<strong class="jq hj">引导聚集</strong>)和不替换(<strong class="jq hj">粘贴</strong>)来完成。</p><p id="fbcc" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">替换采样意味着每次从训练集中取出一个样本进行训练时，它都会立即返回，以便可用于下一次采样。因此，同一个样本可以多次用于训练，因为它们是独立选择的(选择一个样本不影响选择另一个样本的概率)。另一方面，没有替换的采样意味着每次从用于训练的训练集中取出样本时，它不会回到袋子中，因此不再可用。因此，同一个样本只能用于一次训练，因为它们是独立选择的(选择一个样本增加了在下一轮采样中选择其他样本的可能性)。</p><p id="a1e3" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在Bagging分类器中，用于训练这些方法的训练集的样本数量可以被设置为超参数，并且由于样本是随机选择的，因此在训练阶段可以忽略其中的一部分。但是，如果使用这些样本来测试算法而不需要使用测试集，它们可以用作算法准确性的度量。这被称为<strong class="jq hj">外袋</strong>评估，在机器学习竞赛中特别有用。</p><p id="09e7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">Sci-kit learn提供了一种实现打包(bootstrap =“真”)和粘贴(bootstrap =“假”)的简单方法，从而可以轻松设置分类器(SVM、决策树、逻辑回归等)、想要训练多少个(n_estimators)、用于训练每个分类器的训练数据量(max_samples)以及是否想要进行开箱评估(oob_score)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lo"><img src="../Images/6fe149ccc0f8f6f53967b77c56256b55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*wExGi5hkZZykHY0XN3IT4Q.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/9413bf4f6dbaef6fd5f9dc7226e06a61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ln0DstYITQPZT_GAP0G9Lw.png"/></div></div></figure><p id="ba22" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">请注意，出袋得分高于装袋得分，表明该模型在看不见数据的情况下表现良好。此外，Bagging比Pasting具有稍好的准确性，这是意料之中的，因为bootstrapping使算法训练的子集更加多样化，导致更高的偏差，但也导致相关性更低的预测器，这降低了总体方差。总的来说，装袋提供了比粘贴更好的模型，这也说明了为什么它经常被首选。此外，该图将单个决策树的边界与由500个决策树组成的打包和粘贴进行了比较。可以看出，两种模型都比决策树具有更平滑的边界，这意味着更好的泛化性能。</p><p id="00fb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">随机森林</strong></p><p id="2f0b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如前所述，随机森林算法是决策树的集合。通常，它使用Bagging方法进行训练，max_samples设置为1.0，这意味着这些树将使用与训练集大小相同的样本进行训练。Sci-kit learn提供了一个直接设置随机Forrest分类器(或回归器，取决于目标)的函数，而不是将其构建在Bagging方法中。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lq"><img src="../Images/b3741d53be69b4ffc3ea78724ff7763f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*5ifj9iSJX__o2l8zFSydZA.png"/></div></figure><h2 id="702d" class="kk kl hi bd km kn ko kp kq kr ks kt ku jx kv kw kx kb ky kz la kf lb lc ld le bi translated">案例研究:使用1994年人口普查收入数据集比较装袋、粘贴和随机福利。</h2><p id="7e02" class="pw-post-body-paragraph jo jp hi jq b jr lf ij jt ju lg im jw jx lh jz ka kb li kd ke kf lj kh ki kj hb bi translated">作为一个案例研究，我们将使用1994年的收入数据集，也称为成人数据集，来展示所提出的方法在现实世界数据中是如何工作的。它可以在加州大学欧文分校的网站下载。该数据集给出了人们的一些特征(如性别、教育水平、工作阶级、婚姻状况)，通常用于创建将人们分为高收入(年收入5万美元或以上)和低收入(年收入低于5万美元)的模型。</p><p id="0ca9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">首先，让我们快速浏览一下数据集，以便理解每个特性的含义。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lr"><img src="../Images/bff7dd5c2625d5c57c765e0913142f66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Ywj_AQnyOepKskE0Z8z4A.png"/></div></div></figure><ul class=""><li id="4df9" class="ls lt hi jq b jr js ju jv jx lu kb lv kf lw kj lx ly lz ma bi translated">年龄(连续的):人的年龄。</li><li id="0407" class="ls lt hi jq b jr mb ju mc jx md kb me kf mf kj lx ly lz ma bi translated">工人阶级(分类的):工人阶级的范畴。(私营、自营企业、自营企业、联邦政府、地方政府、州政府、无薪、从未工作)。</li><li id="0642" class="ls lt hi jq b jr mb ju mc jx md kb me kf mf kj lx ly lz ma bi translated">教育(分类):教育资格的数字表示，范围从1到16；(学士、一些学院、11年级、HS-grad、Prof-school、Assoc-acdm、Assoc-voc、9年级、7-8年级、12年级、硕士、1-4年级、10年级、博士、5-6年级、学前班)；</li><li id="f670" class="ls lt hi jq b jr mb ju mc jx md kb me kf mf kj lx ly lz ma bi translated">婚姻状况(分类):个人的婚姻状况(已婚-同居-配偶、离婚、未婚、分居、丧偶、已婚-配偶不在、已婚-配偶)；</li><li id="ec7c" class="ls lt hi jq b jr mb ju mc jx md kb me kf mf kj lx ly lz ma bi translated">职业(分类):人的工作类型(技术支持、工艺修理、其他服务、销售、行政管理、专业教授、搬运工人、清洁工、机器操作员、检查员、行政文员、农业、渔业、运输、私人服务、保安服务、武装部队)；</li><li id="bef3" class="ls lt hi jq b jr mb ju mc jx md kb me kf mf kj lx ly lz ma bi translated">关系(分类):人的关系状态(妻子、亲生子女、丈夫、非家庭成员、其他亲属、未婚)；</li><li id="a202" class="ls lt hi jq b jr mb ju mc jx md kb me kf mf kj lx ly lz ma bi translated">种族(分类):代表人的种族(白人、亚洲太平洋岛民、美洲印第安爱斯基摩人、其他人、黑人)；</li><li id="2252" class="ls lt hi jq b jr mb ju mc jx md kb me kf mf kj lx ly lz ma bi translated">性别(分类):人的性别；</li><li id="0d2b" class="ls lt hi jq b jr mb ju mc jx md kb me kf mf kj lx ly lz ma bi translated">Fnlwgt:代表最终权重，即响应单位所代表的目标人群中的单位数；</li><li id="8fbe" class="ls lt hi jq b jr mb ju mc jx md kb me kf mf kj lx ly lz ma bi translated">资本收益(连续):该人的总资本收益；</li><li id="838f" class="ls lt hi jq b jr mb ju mc jx md kb me kf mf kj lx ly lz ma bi translated">资本损失(连续):他们的资本损失；</li><li id="7deb" class="ls lt hi jq b jr mb ju mc jx md kb me kf mf kj lx ly lz ma bi translated">每周小时数(连续):一周的总工作时间；</li><li id="aa51" class="ls lt hi jq b jr mb ju mc jx md kb me kf mf kj lx ly lz ma bi translated">国家(分类):该人居住的国家；</li><li id="d5c8" class="ls lt hi jq b jr mb ju mc jx md kb me kf mf kj lx ly lz ma bi translated">收入标签(分类):这个标签将人们分为年收入低于<strong class="jq hj"> 5万美元(假)</strong>的人和年收入超过<strong class="jq hj"> 5万美元(真)</strong>的人。</li></ul><p id="5817" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">使用相同的训练测试分割函数选择训练和测试数据，80%训练和20%测试。为了给算法提供数据，我们必须做一些预处理步骤。首先，将分类数据转换为数字类别(特别注意Country列，该列被转换为USA和NON_USA，因为原始数据集上有来自太多不同国家的人)并缩放连续数据。这是通过为每个任务使用一个管道来完成的，然后创建一个新的管道来合并它们的结果，从而产生一组数字特征:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mg"><img src="../Images/5db23e7146a9f312ed4f010418eb3369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XwLpvRAO9DURXI84cWiBGQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">最终要素数据框</figcaption></figure><p id="5e7d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了比较装袋、粘贴、随机森林和决策树的性能，我们使用sci-kit learn的GridSearchCV函数，创建一个强大的搜索空间，收集这三种具有不同超参数的算法:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mh"><img src="../Images/27db7b94bedcc35edf462a33a43c10f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*THRze6pIa7s0-YgV9Cf3pQ.png"/></div></figure><p id="5bae" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们找到了每种方法的最佳结果，并将其收集在两个表中，一个是AUC得分，另一个是准确性得分:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mi"><img src="../Images/cb21ccb5429344c9104174572ea2eca8.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*ffL0jrBgQKy0Elmgk-UaQQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">AUC评分表</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mj"><img src="../Images/07b07b90f9534791a6dae32954f7c825.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*7Rnkn5BpXBulkioaVQnDjw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">准确度得分表</figcaption></figure><p id="2f4e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">从表中，人们可以得出结论，装袋和粘贴模型过度拟合，具有较低的偏差，但在训练和准确性得分方面有很大的差异。然而，即使他们过拟合，他们也有比决策树方法更好的结果。随机森林在它们之间表现更好，具有相似的训练和测试结果。</p><p id="52fe" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">特征重要性技术</strong></p><p id="1607" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">随机森林分类器也有一种易于实现的方法来测量模型结果中每个特征的重要性，给出使用该特征的树节点减少森林中所有树的杂质的平均程度。以下代码给出了每个功能的名称及其重要程度:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mk"><img src="../Images/7f90012d46421c4776bd23b26ccd00e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*AWw7Ax3HP6fGPco4RD4wRg.png"/></div></figure><p id="c142" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">通过查看该列表，可以得出结论，特征fnlwgt、年龄、资本收益、关系和教育数量是模型中最重要的5个特征，而母国、种族、性别、教育和资本损失是最不重要的。</p><p id="8870" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了比较这种特征重要性方法，我们在分类器上实现了<a class="ae jn" href="https://eli5.readthedocs.io/en/latest/overview.html" rel="noopener ugc nofollow" target="_blank"> Eli5 </a> show_weights()函数。Eli5是一个Python包，它处理分类器，目的是解释它的预测。</p><p id="f865" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">要安装Eli5软件包:</p><pre class="iy iz ja jb fd ml mm mn mo aw mp bi"><span id="135d" class="kk kl hi mm b fi mq mr l ms mt">!pip install eli5</span></pre><p id="c7c0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在同一个随机森林模型上使用它:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mu"><img src="../Images/7359a97ecd49dce4ba57f55fd3245204.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*9Biduh1OVSliC_JUURVrpQ.png"/></div></figure><p id="ba32" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这两种方法显示了相似的要素重要性值，但是，Eli5方法提供了更多信息，以重要性等级显示值，还提供了重要性的偏差，而不仅仅是feature_importance_ method的平均值。</p><p id="861b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">参考文献</strong></p><div class="mv mw ez fb mx my"><a href="https://towardsdatascience.com/explainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608" rel="noopener follow" target="_blank"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hj fi z dy nd ea eb ne ed ef hh bi translated">动手机器学习模型解释</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">解释机器学习模型的综合指南</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm jh my"/></div></div></a></div><div class="mv mw ez fb mx my"><a href="https://eli5.readthedocs.io/en/latest/overview.html" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hj fi z dy nd ea eb ne ed ef hh bi translated">概述- ELI5 0.9.0文档</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">ELI5在Python 2.7和Python 3.4+中工作。目前它需要scikit-learn 0.18+。您可以使用pip:或…安装ELI5</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">eli5.readthedocs.io</p></div></div></div></a></div><div class="mv mw ez fb mx my"><a href="http://blog.datadive.net/interpreting-random-forests/" rel="noopener  ugc nofollow" target="_blank"><div class="mz ab dw"><div class="na ab nb cl cj nc"><h2 class="bd hj fi z dy nd ea eb ne ed ef hh bi translated">解读随机森林</h2><div class="nf l"><h3 class="bd b fi z dy nd ea eb ne ed ef dx translated">想象一下，一家信用卡公司使用随机森林构建了一个欺诈检测模型。该模型可以…</h3></div><div class="ng l"><p class="bd b fp z dy nd ea eb ne ed ef dx translated">blog.datadive.net</p></div></div><div class="nh l"><div class="nn l nj nk nl nh nm jh my"/></div></div></a></div><p id="2403" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><a class="ae jn" href="https://archive.ics.uci.edu/ml/index.php" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/index.php</a></p><p id="07c5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><a class="ae jn" href="https://yourstory.com/journal/the-wisdom-of-the-crowds-can-it-beat-or-match-expe-zl424v9h4k" rel="noopener ugc nofollow" target="_blank">https://your story . com/journal/the-wisdom-of-the-crowds-can-it-beat-or-match-expe-ZL 424 v9h4k</a></p></div></div>    
</body>
</html>