<html>
<head>
<title>Topic Modelling using Word Embeddings and Latent Dirichlet Allocation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用单词嵌入和潜在狄利克雷分配的主题建模</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/topic-modelling-using-word-embeddings-and-latent-dirichlet-allocation-3494778307bc?source=collection_archive---------0-----------------------#2020-01-18">https://medium.com/analytics-vidhya/topic-modelling-using-word-embeddings-and-latent-dirichlet-allocation-3494778307bc?source=collection_archive---------0-----------------------#2020-01-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="81dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">使用聚类(关于嵌入)和</em> <strong class="ih hj"> <em class="jd"> LDA </em> </strong> <em class="jd">技术从一百万个标题中提取</em> <strong class="ih hj"> <em class="jd">主题</em> </strong> <em class="jd"/></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/134ddd4327d473ca289a717c6e7d0341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c12Wf83aChMtNDH63sVfzg.png"/></div></div></figure><p id="0233" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">世界各地的媒体、期刊和报纸每天都必须将他们拥有的所有数据聚集到特定的主题中，以结构化的方式展示特定主题下的文章或新闻。所有的社交网络公司，如<strong class="ih hj">脸书</strong>、<strong class="ih hj">推特</strong>等，在使用推荐引擎向用户推荐东西之前，都会对帖子和广告进行某种主题建模。甚至谷歌也在他们的搜索中运行主题建模来识别与用户搜索相关的文档。想象一下，有一个数字图书馆，里面的书被随机放置，与主题无关。搜索它们或搜索属于我们感兴趣的特定主题的书籍会有多困难。幸运的是，我们有深度学习和分析工具来拯救我们脱离这些情况。</p><p id="1358" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个项目中，我将从著名的澳大利亚新闻来源ABC(澳大利亚广播公司)的一百万条新闻标题中提取主题。该数据集在Kaggle中可用。</p><p id="da30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jq" href="https://www.kaggle.com/therohk/million-headlines" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/therohk/million-headlines</a></p><p id="162b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据集内容</strong></p><p id="3522" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">publish_date:以yyyyMMdd格式发布文章的日期</p><p id="97b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">headline_text:标题文本，Ascii，英文，小写</p><p id="a5d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">开始日期:2003年2月19日；结束日期:2019年12月31日</p><p id="199e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将从两个方面探讨这一点:</p><p id="1629" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1)在第一种情况下，我们将使用'<strong class="ih hj">Google News ' wortovec ' embeddings</strong>'为每个标题创建嵌入，它负责语义和含义，并将标题聚类成8个聚类，并查看不同聚类中最频繁出现的单词</p><p id="49f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2)在第二种情况下，我们将使用<strong class="ih hj"> LDA </strong>(潜在狄利克雷分配)方法对这些标题中的主题进行建模。LDA假设每个标题来自几个主题，每个主题由几个单词组成。</p><p id="0b97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们从导入一些库开始。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="ec80" class="jw jx hi js b fi jy jz l ka kb"><em class="jd">#importing libraries</em></span><span id="60f2" class="jw jx hi js b fi kc jz l ka kb"><strong class="js hj">import</strong> <strong class="js hj">numpy</strong> <strong class="js hj">as</strong> <strong class="js hj">np</strong><br/><strong class="js hj">import</strong> <strong class="js hj">pandas</strong> <strong class="js hj">as</strong> <strong class="js hj">pd</strong><br/><strong class="js hj">import</strong> <strong class="js hj">numpy</strong> <strong class="js hj">as</strong> <strong class="js hj">np</strong><br/><strong class="js hj">import</strong> <strong class="js hj">pandas</strong> <strong class="js hj">as</strong> <strong class="js hj">pd</strong><br/><strong class="js hj">import</strong> <strong class="js hj">seaborn</strong> <strong class="js hj">as</strong> <strong class="js hj">sns</strong><br/><strong class="js hj">import</strong> <strong class="js hj">matplotlib.pyplot</strong> <strong class="js hj">as</strong> <strong class="js hj">plt</strong><br/><strong class="js hj">from</strong> <strong class="js hj">sklearn.feature_extraction.text</strong> <strong class="js hj">import</strong> CountVectorizer<br/><strong class="js hj">from</strong> <strong class="js hj">sklearn.feature_extraction.text</strong> <strong class="js hj">import</strong> TfidfVectorizer</span><span id="2f79" class="jw jx hi js b fi kc jz l ka kb">headlines = pd.read_csv('abcnews-date-text.csv',parse_dates=[0], infer_datetime_format=<strong class="js hj">True</strong>)</span><span id="5bda" class="jw jx hi js b fi kc jz l ka kb">headlines.head()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kd"><img src="../Images/f2e99f59669e2dba82247374293d0b8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*Mcfa8oN85pK4Fl9LAQjyiA.png"/></div></figure><p id="d34a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然我们已经导入了数据，我们将开始探索性的数据分析，以便我们对数据包含的内容有一个直观的了解。让我们首先创建一个包含每个标题长度的列，以直观地了解标题中使用的平均字数。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="18f9" class="jw jx hi js b fi jy jz l ka kb">headlines['NumWords'] = headlines['headline_text'].apply(<strong class="js hj">lambda</strong> x: len(x.split()))<br/>headlines[['NumWords']].hist(figsize=(12, 6), bins=10, xlabelsize=8, ylabelsize=8);<br/>plt.title("Distributon of number of words in the headlines")</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ke"><img src="../Images/7ac9fda67cd22d4daef04296c6df3c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*TRtoOMC4137EbRUP6uP7CQ.png"/></div></figure><p id="c2b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，大多数标题大约有4-6个单词，现在我们也制作一个年、月和日字段，以查看标题在这些属性中的分布。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="2fd1" class="jw jx hi js b fi jy jz l ka kb">headlines['year'] = pd.DatetimeIndex(headlines['publish_date']).year<br/>headlines['month'] = pd.DatetimeIndex(headlines['publish_date']).month<br/>headlines['day'] = pd.DatetimeIndex(headlines['publish_date']).day</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kf"><img src="../Images/9f0a33c6a6c3f1ae64e45b0b06344250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W1pkbu0SwLsHUk9JFy54Sw.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kg"><img src="../Images/1b94f0d04437dc6ec4746464498e926c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ISd42iHv0RV3DsFnc8YHRw.png"/></div></div></figure><p id="c190" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">似乎这些天的分布是均匀的。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kh"><img src="../Images/e5bea3da240cc3b8656f7c3c763abc70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XChlKQptQHKv02IvsEwGxw.png"/></div></div></figure><p id="63cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据集中的大部分标题都是由年初和年末决定的。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ki"><img src="../Images/5f08db4d43e2bf8d20f29662d65c0f7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ag9t_tyPz1KQqdvQ5FtRaQ.png"/></div></div></figure><p id="bb86" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，我们可以看到头条新闻的数量是如何跨年度分布的。</p><p id="f186" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们以时间序列模式来观察标题数量的日常变化。这将会给我们一个更好的直觉，并且有趣的是去思考所有可能在某些年份导致峰值的原因。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="6962" class="jw jx hi js b fi jy jz l ka kb">monthly_counts = headlines.resample('M').count()<br/>yearly_counts = headlines.resample('A').count()<br/>daily_counts = headlines.resample('D').count()</span><span id="f062" class="jw jx hi js b fi kc jz l ka kb">fig, ax = plt.subplots(3, figsize=(18,16))<br/>ax[0].plot(daily_counts);<br/>ax[0].set_title('Daily Counts');<br/>ax[1].plot(monthly_counts);<br/>ax[1].set_title('Monthly Counts');<br/>ax[2].plot(yearly_counts);<br/>ax[2].set_title('Yearly Counts');<br/>plt.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kj"><img src="../Images/b0e7f813dbe5c493a509d5a377370935.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z6C8j0maS6VviriUWi8HaQ.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kk"><img src="../Images/320c7dbd36ddc8e82af7672fce15dd2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-PzXAVBy8g0oQtdtBXfjw.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kl"><img src="../Images/6a7910f1710e0e711e4f35ea3a0434cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3x7JvTPXqsyP5Yzbvk4ipw.png"/></div></div></figure><p id="f6ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们构建一个词云，来看看这些年来在标题中使用频率最高的词。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="c3c8" class="jw jx hi js b fi jy jz l ka kb"><strong class="js hj">from</strong> <strong class="js hj">wordcloud</strong> <strong class="js hj">import</strong> WordCloud<br/>all_words = ''.join([word <strong class="js hj">for</strong> word <strong class="js hj">in</strong> headlines['headline_text'][0:100000]])<br/>all_words<br/>wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)<br/>plt.figure(figsize=(15, 8))<br/>plt.imshow(wordcloud, interpolation="bilinear")<br/>plt.axis('off')<br/>plt.title("Some frequent words used in the headlines", weight='bold', fontsize=14)<br/>plt.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es km"><img src="../Images/346b05c11a07f70044e0f63143bab264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l3R1hHCk05BrV4lVBrpRQQ.png"/></div></div></figure><p id="3c30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">云这个词似乎很有趣。尽管新闻频道属于澳大利亚，但我们可以看到一些常见的词，如“伊拉克”和其他一些词，如“警察”，“计划”，“健康”，“委员会”等。</p><p id="4fa3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们继续执行一些清理操作，比如将每个单词转换为小写字体，从这些标题中删除与建模主题无关的标点符号和非ASCII字符。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="b91e" class="jw jx hi js b fi jy jz l ka kb"><strong class="js hj">import</strong> <strong class="js hj">re</strong><br/>NON_ALPHANUM = re.compile(r'[\W]')<br/>NON_ASCII = re.compile(r'[^a-z0-1\s]')<br/><strong class="js hj">def</strong> normalize_texts(texts):<br/>  normalized_texts = ''<br/>  lower = texts.lower()<br/>  no_punctuation = NON_ALPHANUM.sub(r' ', lower)<br/>  no_non_ascii = NON_ASCII.sub(r'', no_punctuation)<br/>  <strong class="js hj">return</strong> no_non_ascii<br/>  <br/>headlines['headline_text'] = headlines['headline_text'].apply(normalize_texts)<br/>headlines.head()<br/>headlines['headline_text'] = headlines['headline_text'].apply(<strong class="js hj">lambda</strong> x: ' '.join([w <strong class="js hj">for</strong> w <strong class="js hj">in</strong> x.split() <strong class="js hj">if</strong> len(w)&gt;2]))</span></pre><p id="53a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们为使用频率最高的15个单词画最后一张图。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="2213" class="jw jx hi js b fi jy jz l ka kb"><strong class="js hj">def</strong> get_top_n_words(corpus, n=10):<br/>  vec = CountVectorizer(stop_words='english').fit(corpus)<br/>  bag_of_words = vec.transform(corpus)<br/>  sum_words = bag_of_words.sum(axis=0) <br/>  words_freq = [(word, sum_words[0, idx]) <strong class="js hj">for</strong> word, idx <strong class="js hj">in</strong>   vec.vocabulary_.items()]<br/>  words_freq =sorted(words_freq, key = <strong class="js hj">lambda</strong> x: x[1], reverse=<strong class="js hj">True</strong>)<br/>  <strong class="js hj">return</strong> words_freq[:n]</span><span id="adc2" class="jw jx hi js b fi kc jz l ka kb">words = []<br/>word_values = []<br/><strong class="js hj">for</strong> i,j <strong class="js hj">in</strong> get_top_n_words(headlines['headline_text'],15):<br/>  words.append(i)<br/>  word_values.append(j)<br/>fig, ax = plt.subplots(figsize=(16,8))<br/>ax.bar(range(len(words)), word_values);<br/>ax.set_xticks(range(len(words)));<br/>ax.set_xticklabels(words, rotation='vertical');<br/>ax.set_title('Top 15 words in the headlines dataset');<br/>ax.set_xlabel('Word');<br/>ax.set_ylabel('Number of occurences');<br/>plt.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es kn"><img src="../Images/d199e8e5ba3dbd58be3c89b57173f78d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kA-EvACIJ1eYD1LS3ufFeA.png"/></div></div></figure><h1 id="d389" class="ko jx hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">方法1:使用“wordtovec”嵌入进行聚类</h1><p id="ad43" class="pw-post-body-paragraph if ig hi ih b ii ll ik il im lm io ip iq ln is it iu lo iw ix iy lp ja jb jc hb bi translated">现在，让我们从第一个方法开始。我们将从谷歌新闻上预先训练的深度神经网络中导入单词嵌入，然后用该标题中每个单词的单词嵌入的平均值来表示每个标题。如果看起来很复杂，坚持住。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="9e52" class="jw jx hi js b fi jy jz l ka kb">pip install --upgrade gensim<br/><em class="jd">#importing wordtovec embeddings </em><br/><strong class="js hj">from</strong> <strong class="js hj">gensim.models</strong> <strong class="js hj">import</strong> KeyedVectors<br/>pretrained_embeddings_path = "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"</span><span id="53ec" class="jw jx hi js b fi kc jz l ka kb">word2vec = KeyedVectors.load_word2vec_format(pretrained_embeddings_path, binary=<strong class="js hj">True</strong>)</span></pre><p id="f766" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看一个单词是如何以其嵌入格式表示的。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="9396" class="jw jx hi js b fi jy jz l ka kb">word = 'iraq'<br/>print('Word: <strong class="js hj">{}</strong>'.format(word))<br/>print('First 20 values of embedding:<strong class="js hj">\n{}</strong>'.format(word2vec[word][:20]))</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lq"><img src="../Images/d862dcc4b94275d62ecb6d86aa06cb7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qfsbrasrc0ekPV5fIApMyw.png"/></div></div></figure><p id="c5be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我先向你展示嵌入这个词的美妙之处。它捕捉了同义词、反义词和所有人类能够理解的逻辑类比。如果有人问你“什么是女人+国王-男人”,我们首先想到的会是女王。现在让我们看看“wordtovec”嵌入给出了这个问题最相似的答案。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="0d84" class="jw jx hi js b fi jy jz l ka kb">print(word2vec.most_similar(positive=['woman', 'king'], negative=['man'], topn=3))</span><span id="21cb" class="jw jx hi js b fi kc jz l ka kb">print(word2vec.most_similar(positive=['Tennis', 'Ronaldo'], negative=['Soccer'], topn=3))</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lr"><img src="../Images/70324c0482c2b16ebc01f07823b0349d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1P3nZjzrt6f9s2p5_EEy5Q.png"/></div></div></figure><p id="769c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗯，这是相当聪明的给皇后。</p><p id="c661" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，由于内存限制，我们将随机抽取20%的数据，然后使用我们刚刚导入的单词嵌入来构建聚类模型。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="8ca4" class="jw jx hi js b fi jy jz l ka kb">X_train = pd.DataFrame(X)<br/>headlines_smaller = X_train.sample(frac = 0.2, random_state= 423)<br/>headlines_smaller.columns = ['head_line']</span><span id="293d" class="jw jx hi js b fi kc jz l ka kb"><strong class="js hj">class</strong> <strong class="js hj">WordVecVectorizer</strong>(object):<br/>    <strong class="js hj">def</strong> __init__(self, word2vec):<br/>        self.word2vec = word2vec<br/>        self.dim = 300</span><span id="2daf" class="jw jx hi js b fi kc jz l ka kb">    <strong class="js hj">def</strong> fit(self, X, y):<br/>        <strong class="js hj">return</strong> self</span><span id="69be" class="jw jx hi js b fi kc jz l ka kb">    <strong class="js hj">def</strong> transform(self, X):<br/>        <strong class="js hj">return</strong> np.array([<br/>            np.mean([self.word2vec[w] <strong class="js hj">for</strong> w <strong class="js hj">in</strong> texts.split() <strong class="js hj">if</strong> w <strong class="js hj">in</strong> self.word2vec]<br/>                    <strong class="js hj">or</strong> [np.zeros(self.dim)], axis=0)<br/>            <strong class="js hj">for</strong> texts <strong class="js hj">in</strong> X<br/>        ])</span><span id="0d2c" class="jw jx hi js b fi kc jz l ka kb"><em class="jd">#representing each headline by the mean of word embeddings for the words used in the headlines.</em></span><span id="3587" class="jw jx hi js b fi kc jz l ka kb">wtv_vect = WordVecVectorizer(word2vec)<br/>X_train_wtv = wtv_vect.transform(headlines_smaller.head_line)<br/>print(X_train_wtv.shape)</span></pre><p id="ed67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们有220733个标题，每个标题有300个特征。让我们使用KMeans集群将它们分成8个集群。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="cf3a" class="jw jx hi js b fi jy jz l ka kb"><strong class="js hj">from</strong> <strong class="js hj">sklearn.cluster</strong> <strong class="js hj">import</strong> KMeans</span><span id="86cb" class="jw jx hi js b fi kc jz l ka kb">km = KMeans(<br/>    n_clusters=8, init='random',<br/>    n_init=10, max_iter=300, <br/>    tol=1e-04, random_state=0<br/>)<br/>y_km = km.fit_predict(X_train_wtv)</span><span id="6329" class="jw jx hi js b fi kc jz l ka kb">df = pd.DataFrame({'headlines' :headlines_smaller.head_line, 'topic_cluster' :y_km })</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ls"><img src="../Images/f2d3c737e924c02566c0cf6b3275b31a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*S7k_kpZCf8QsvimlWY7Q2Q.png"/></div></figure><p id="b44a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们想象每组中的前15个单词，并思考它们可能代表的主题。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lt"><img src="../Images/5a18314fff93eb66f8fde59249ca81db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0lHC-GmeXmNhMl8GfBHRRg.png"/></div></div></figure><p id="cc8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">聚类0代表像“计划”、“健康”、“预算”等这样的词，看起来这属于新闻的经济或商业部分。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lu"><img src="../Images/088c882c1401ce763b4304a5aa4e088e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aGXtzYtZqij6ioeSlx7VKQ.png"/></div></div></figure><p id="8fb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">聚类1代表像“警察”、“谋杀”、“死亡”等这样的词，似乎这属于新闻的犯罪部分。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lv"><img src="../Images/70399f8ac9c28655b9666bd8bc005946.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jwQSZMpxTSAPJx8oi33Z3g.png"/></div></div></figure><p id="7760" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">集群2代表像“选举”、“部长”、“法院”等词，似乎这属于新闻的政治部分。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lw"><img src="../Images/a48adf4eeec5e5b3d562bb5ea58b8ae8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8FXyQ-puJgs8JICLeM1oFw.png"/></div></div></figure><p id="c828" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">聚类3代表像“澳大利亚”、“英格兰”、“印度”等这样的词，并且似乎这属于新闻的全球部分。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lx"><img src="../Images/d3f8324f28366cc25622abe465cb379d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eLZDXVt4ZnYpmwyM6u-8Pw.png"/></div></div></figure><p id="dced" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">聚类4代表像“农村”、“水”、“洪水”等词，看起来这属于经济学部分，但更倾向于微观问题。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ly"><img src="../Images/d443465d81c18fa07714fc82398a8128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XGwxZ4q1bLe8-p1k8lWKNA.png"/></div></div></figure><p id="96d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">聚类5代表像“迈克尔”、“酷跑”、“史密斯”等这样的词，并且看起来这属于新闻的采访或人物部分。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lz"><img src="../Images/0f898cdb0122048f60df860955841419.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ePfxlx1e5-cw1MS8C74AqQ.png"/></div></div></figure><p id="808a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">聚类6代表像“撞车”、“死亡”、“失踪”等这样的词，并且似乎这属于新闻的事故或当前事件部分。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es ma"><img src="../Images/749d69290617eecc2f41f3c5fdf9732e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FgsJr-7WEslZ1XxSmEq8Cg.png"/></div></div></figure><p id="c632" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">聚类7代表像“公开赛”、“冠军”、“决赛”等这样的词，并且看起来这属于新闻的体育部分。</p><p id="01b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到，word2vec嵌入以一种非常智能的方式将我们带到了一些随机新闻到属于特定主题的新闻。现在，让我们转到方法2。</p><h1 id="bb28" class="ko jx hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">方法2:使用LDA(潜在狄利克雷分析)的聚类</h1><p id="64e7" class="pw-post-body-paragraph if ig hi ih b ii ll ik il im lm io ip iq ln is it iu lo iw ix iy lp ja jb jc hb bi translated">LDA是一种从文档中提取主题的概率方法。它假设每个文档由几个具有不同概率分布的主题组成，每个主题由几个具有不同分布的单词组成。因此，它的工作方式是为每个文档中的每个单词初始化随机主题，并以相反的方式发现会在文档中生成这些单词的主题。要获得高层次的直觉，请阅读本博客—<a class="ae jq" rel="noopener" href="/@pratikbarhate/latent-dirichlet-allocation-for-beginners-a-high-level-intuition-23f8a5cbad71">https://medium . com/@ pratikbarhate/latent-Dirichlet-allocation-for-初学者-a-high-level-intuition-23 F8 a5 cbad 71</a></p><p id="477e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于LDA有大量的计算，我们将对2%的数据进行采样并执行分析，这可能不会导致非常智能的主题，但它会让我们对LDA的工作有一个高层次的理解。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="d5c0" class="jw jx hi js b fi jy jz l ka kb">news = headlines.sample(frac = 0.02, random_state= 423)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mb"><img src="../Images/647b6713e3333bc1deff1ef173582d24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mp_cLnBxIRwK1hmwg_KuFg.png"/></div></div></figure><p id="97a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用TfIdf矢量器构建我们的特征，该矢量器类似于单词袋模型，唯一的区别是“tfidf”对出现在几个文档中的单词进行惩罚。现在，让我们拟合LDA模型，看看LDA使用每个主题的前15个词提取了哪些主题。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="9531" class="jw jx hi js b fi jy jz l ka kb">tf_vectorizer = TfidfVectorizer(stop_words='english', max_features=50000)<br/>news_matrix = tf_vectorizer.fit_transform(news['headline_text'])<br/><em class="jd">#importing LDA</em></span><span id="4bae" class="jw jx hi js b fi kc jz l ka kb"><strong class="js hj">from</strong> <strong class="js hj">gensim</strong> <strong class="js hj">import</strong> corpora, models<br/><strong class="js hj">from</strong> <strong class="js hj">sklearn.decomposition</strong> <strong class="js hj">import</strong> LatentDirichletAllocation</span><span id="af95" class="jw jx hi js b fi kc jz l ka kb"><em class="jd">#Fitting LDA</em></span><span id="8aaf" class="jw jx hi js b fi kc jz l ka kb">lda = LatentDirichletAllocation(n_components=8, learning_method='online', <br/>                                          random_state=0, verbose=0, n_jobs = -1)<br/>lda_model = lda.fit(news_matrix)</span><span id="301f" class="jw jx hi js b fi kc jz l ka kb">lda_matrix = lda_model.transform(news_matrix)</span><span id="83f5" class="jw jx hi js b fi kc jz l ka kb">lda_matrix</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mc"><img src="../Images/3914695381c9d3ad094ac03cb9cf07f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VQqWMiJk2dcE5Zvr_pq-GA.png"/></div></div></figure><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="e729" class="jw jx hi js b fi jy jz l ka kb"><strong class="js hj">def</strong> print_topics(model, count_vectorizer, n_top_words):<br/>    words = tf_vectorizer.get_feature_names()<br/>    <strong class="js hj">for</strong> topic_idx, topic <strong class="js hj">in</strong> enumerate(model.components_):<br/>      <br/>        print("<strong class="js hj">\n</strong>Topic #<strong class="js hj">%d</strong>:" % topic_idx )<br/>        print(" ".join([words[i]<br/>                        <strong class="js hj">for</strong> i <strong class="js hj">in</strong> topic.argsort()[:-n_top_words - 1:-1]]))</span><span id="0743" class="jw jx hi js b fi kc jz l ka kb"><em class="jd"># Print the topics found by the LDA model</em><br/>print("Topics found via LDA:")<br/>print_topics(lda_model, news_matrix, 15)</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es md"><img src="../Images/8d8b5376fb2a360c093767da8ee77ccb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G4Ejh_iE3MvnPs2pStvT0w.png"/></div></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es me"><img src="../Images/2e08dad25b0b58539f8f4092b6971333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CCEjER7J75xtiLiO6cN_8A.png"/></div></div></figure><p id="d50c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以试着去理解以上每一个代表了什么主题。一旦开始增加训练样本，主题将变得更加具体，模型将更加智能地提取。现在，让我们使用TSNE来绘制所有的文档，并根据它们所代表的主题给它们上色。尽管我们知道每个文档都来自几个主题(LDA的一个假设)，但我们会认为对于每个文档，概率最高的主题就是该文档所代表的主题。</p><pre class="jf jg jh ji fd jr js jt ju aw jv bi"><span id="8493" class="jw jx hi js b fi jy jz l ka kb"><strong class="js hj">from</strong> <strong class="js hj">sklearn.manifold</strong> <strong class="js hj">import</strong> TSNE<br/>model = TSNE(n_components=2, perplexity=50, learning_rate=100, <br/>                        n_iter=1000, verbose=1, random_state=0, angle=0.75)</span><span id="dc6d" class="jw jx hi js b fi kc jz l ka kb">tsne_features = model.fit_transform(lda_matrix)</span><span id="3de8" class="jw jx hi js b fi kc jz l ka kb">df = pd.DataFrame(tsne_features)<br/>df['topic'] = lda_matrix.argmax(axis=1)<br/>df.columns = ['TSNE1', 'TSNE2', 'topic']</span><span id="5a2a" class="jw jx hi js b fi kc jz l ka kb"><strong class="js hj">import</strong> <strong class="js hj">seaborn</strong> <strong class="js hj">as</strong> <strong class="js hj">sns</strong><br/>plt.figure(figsize=(15, 10))<br/>plt.title('T-SNE plot of different headlines ( headlines are clustered among their topics)')<br/>ax = sns.scatterplot(x = 'TSNE1', y = 'TSNE2', hue = 'topic', data = df, legend = 'full')<br/>plt.show()</span></pre><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mf"><img src="../Images/563752d46511b799eca114754e9e9602.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4QvIkfaOE3x2y5ACow0Ycw.png"/></div></div></figure><p id="2bb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从TSNE图中，我们可以看到不同的文档属于它们的集群。</p><p id="9b3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望您至少对主题建模的工作原理有所了解。现在试着从twitter上删除数据，并从这些数据中获取话题。</p><p id="64c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">谢谢你。</p><p id="8587" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">图片参考:</em><a class="ae jq" href="https://appliedmachinelearning.blog/2017/09/28/topic-modelling-part-2-discovering-topics-from-articles-with-latent-dirichlet-allocation/" rel="noopener ugc nofollow" target="_blank">https://appliedmachinehlearning . blog/2017/09/28/topic-modeling-part-2-discovering-topics-from-articles-with-latent-Dirichlet-allocation/</a></p></div></div>    
</body>
</html>