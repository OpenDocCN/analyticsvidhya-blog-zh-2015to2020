<html>
<head>
<title>From feature engineering to feature learning in machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从特征工程到机器学习中的特征学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-from-feature-engineering-to-feature-learning-1d81fbf2dc23?source=collection_archive---------6-----------------------#2019-10-28">https://medium.com/analytics-vidhya/machine-learning-from-feature-engineering-to-feature-learning-1d81fbf2dc23?source=collection_archive---------6-----------------------#2019-10-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1063" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">机器学习包括从数据获取到可视化的许多方面。在本文中，我们将通过一个基于Galton数据集的简单例子来解释其中的两个例子，即<strong class="ih hj">特征学习</strong>和<strong class="ih hj">特征工程</strong>。</p><p id="61f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">高尔顿是线性回归的创始人之一。他在1886年发表了一篇题为“<a class="ae jd" href="http://galton.org/essays/1880-1889/galton-1886-jaigi-regression-stature.pdf" rel="noopener ugc nofollow" target="_blank">回归世袭身份的平庸</a>”的论文。他接着质疑为什么父母身高和孩子身高之间似乎没有什么联系。用他自己的话说:</p><blockquote class="je jf jg"><p id="ec33" class="if ig jh ih b ii ij ik il im in io ip ji ir is it jj iv iw ix jk iz ja jb jc hb bi translated">“从这些实验中似乎可以看出，后代在大小上并不倾向于与他们的父母种子相似，而是总是比他们更平庸——如果父母很大，那么比父母更小；比父母大，如果父母很小的话。”</p></blockquote><p id="e95e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">高尔顿遗产的一部分是人类身高与父母身高的<a class="ae jd" href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/T0HSJ1" rel="noopener ugc nofollow" target="_blank">数据集，我们将在本文中使用该数据集使用Python通过线性回归来预测儿童身高。</a></p><p id="9237" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将进行三个实验:</p><ul class=""><li id="058d" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">使用<a class="ae jd" href="https://scikit-learn.org" rel="noopener ugc nofollow" target="_blank"> Scikit Learn </a>对父母身高进行初始线性回归</li><li id="0990" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">使用一些简单的特征引擎来说明孩子的性别</li><li id="1502" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">使用<a class="ae jd" href="https://keras.io" rel="noopener ugc nofollow" target="_blank"> Keras </a>中的两层系统让神经网络学习该特性</li></ul><p id="ed69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你会在这个笔记本里找到这篇文章的<strong class="ih hj">完整代码</strong><strong class="ih hj">:</strong><a class="ae jd" href="https://tonio73.github.io/data-science/linear/LinearRegressionFeatureEngineering-Keras.html" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">HTML</strong></a><strong class="ih hj">/</strong><a class="ae jd" href="https://nbviewer.jupyter.org/urls/tonio73.github.io/data-science/linear/LinearRegressionFeatureEngineering-Keras.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">Jupyter</strong></a></p></div><div class="ab cl jz ka gp kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hb hc hd he hf"><h1 id="34f3" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">双亲的线性回归</h1><p id="1d6b" class="pw-post-body-paragraph if ig hi ih b ii le ik il im lf io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">第一步，让我们执行线性回归“开箱即用”,将父母的身高作为输入，将孩子的身高作为预测变量。</p><p id="7686" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">预期之后与神经网络的比较，数据在回归之前被标准化:对于每个特征，均值被移除并且方差被缩放为等于1。</p><p id="40df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用<a class="ae jd" href="https://scikit-learn.org" rel="noopener ugc nofollow" target="_blank"> SKLearn的</a>回归的Python代码摘录:</p><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="bd00" class="ls kh hi lo b fi lt lu l lv lw">trainX_scaled = scalerX.transform(df_train[['Mother', 'Father']])<br/>trainY_scaled = scalerY.transform(df_train[['Height']])</span><span id="dbfe" class="ls kh hi lo b fi lx lu l lv lw">model1 = linear_model.LinearRegression()</span><span id="0208" class="ls kh hi lo b fi lx lu l lv lw">model1.fit(trainX_scaled, trainY_scaled)<br/>b1 = model1.intercept_<br/>w1 = model1.coef_.reshape(-1)</span></pre><p id="ad99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结果:截距b1 = 0.0，权重w1 = (0.184，0.217)</p><figure class="lj lk ll lm fd lz er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es ly"><img src="../Images/185352d50bb93ff3c5232bfe18aeff97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UkNUB2KFyRtmSN_RyK5YoQ.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">测试集的初始回归预测</figcaption></figure><p id="ebb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">双变量回归绘制了一个最佳计划，该计划似乎不太符合要预测的标签:蓝色的参考点分布在橙色的预测周围。</p><p id="fccd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于线性回归的更多解释，请参见二元函数逼近笔记本:<a class="ae jd" href="https://tonio73.github.io/data-science/linear/LinearRegressionBivariate.html" rel="noopener ugc nofollow" target="_blank"> HTML </a> / <a class="ae jd" href="https://nbviewer.jupyter.org/urls/tonio73.github.io/data-science/linear/LinearRegressionBivariate.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter </a></p></div><div class="ab cl jz ka gp kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hb hc hd he hf"><h1 id="8d07" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">考虑性别因素的线性回归模型</h1><p id="12d2" class="pw-post-body-paragraph if ig hi ih b ii le ik il im lf io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">我们可能有这样的直觉，模型没有考虑到一个主要信息:性别分布依赖于孩子的性别。</p><p id="82b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，让我们做一些<strong class="ih hj">功能工程</strong>并创建两个模型:</p><ul class=""><li id="f903" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">基于训练数据集的女孩的模型，将预测女孩的身高</li><li id="6c1d" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">男孩的相似模型</li></ul><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="1d28" class="ls kh hi lo b fi lt lu l lv lw">trainX_girls_scaled = scalerX.transform(df_train[['Mother', 'Father']][girls_train])<br/>trainY_girls_scaled = scalerY.transform(df_train[['Height']][girls_train])</span><span id="afad" class="ls kh hi lo b fi lx lu l lv lw">model2_girl = linear_model.LinearRegression()<br/>model2_girl.fit(trainX_girls_scaled, trainY_girls_scaled)<br/></span><span id="fb8a" class="ls kh hi lo b fi lx lu l lv lw">trainX_boys_scaled = scalerX.transform(df_train[['Mother', 'Father']][boys_train])<br/>trainY_boys_scaled = scalerY.transform(df_train[['Height']][boys_train])</span><span id="f3a7" class="ls kh hi lo b fi lx lu l lv lw">model2_boy   = linear_model.LinearRegression()<br/>model2_boy.fit(trainX_boys_scaled, trainY_boys_scaled)</span></pre><p id="0bac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，根据要预测身高的儿童的性别，将这些模型应用于测试数据集。</p><p id="540d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结果:</p><ul class=""><li id="b398" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">试衣女生，截距= -0.773，权重= 0.207，0.283</li><li id="f507" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">试衣男孩，截距= 0.710，体重= 0.209，0.268</li><li id="f7d6" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">测试数据集的均方误差= 5.123</li></ul><p id="7bb9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">截距不再为0，因为归一化是在混合女孩和男孩的完整数据集上执行的。</p><p id="8c62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">均方差减少了60%，证实了我们对模型组合的直觉。</p></div><div class="ab cl jz ka gp kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hb hc hd he hf"><h1 id="3168" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">使用神经网络的特征学习</h1><p id="a00a" class="pw-post-body-paragraph if ig hi ih b ii le ik il im lf io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">机器有足够的智能来猜测这个结果吗？那就是学习结合父母身高和孩子性别的特征？</p><p id="df73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在切换到神经网络，因此切换到基于梯度的优化或“机器学习”。</p><p id="8bd6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用<a class="ae jd" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>。它为<a class="ae jd" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>和其他机器学习框架提供了一个标准且简化的编程接口。</p><p id="4c1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的网络中，使用了两层:</p><ul class=""><li id="83b6" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">初始层由两个神经元组成，每个神经元将四个特征作为输入:母亲和父亲的身高，使用“一键编码”进行性别编码</li><li id="541c" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">第二层是结合前面神经元的两个输出，以提供最终的预测</li></ul><pre class="lj lk ll lm fd ln lo lp lq aw lr bi"><span id="d469" class="ls kh hi lo b fi lt lu l lv lw">model = keras.models.Sequential([<br/>    keras.layers.Dense(2, activation=’linear’, <br/>        input_shape=[4],<br/>        kernel_regularizer=keras.regularizers.l2(0.0001)),<br/>    keras.layers.Dense(1, activation=’linear’, <br/>        input_shape=[4],<br/>        kernel_regularizer=keras.regularizers.l2(0.0001))<br/>])<br/>model.compile(optimizer=’adam’,<br/>    loss=keras.losses.mean_squared_error,<br/>    metrics=[‘mse’])</span></pre><p id="7f62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一位热码方案用于对两位(两个值)的性别进行编码，如下表所示。在性别和预测之间获得线性关系需要一些特征工程。</p><figure class="lj lk ll lm fd lz er es paragraph-image"><div class="er es mk"><img src="../Images/c3e04677801ce8cee025e6b040e9e4e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*aZ0dbyWp00OmJ5uUQxNArw.png"/></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">一次性性别编码</figcaption></figure><p id="32f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有神经元都表现为线性回归的预测函数:它们对输入进行加权求和，并添加一个截距。</p><p id="2d3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，系数的数量为:</p><ul class=""><li id="2cf7" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">第一层为2 * 4 +2</li><li id="c5a1" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">第二层的2 + 1</li></ul><p id="de5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与先前实验的另一个不同之处是，在最小化成本函数(均方误差)时迭代地识别系数，最小化是基于该函数的梯度的计算。</p><figure class="lj lk ll lm fd lz er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es ml"><img src="../Images/7f2c48fd8465d5caa1223ea08a79574e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MNrXzm0aJNngx21oLEdapQ.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">梯度下降损失函数和均方误差</figcaption></figure><p id="7dda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有关Keras梯度下降的更多解释，请参见Keras二元函数近似笔记本:<a class="ae jd" href="https://tonio73.github.io/data-science/linear/LinearRegressionBivariate-Keras.html" rel="noopener ugc nofollow" target="_blank"> HTML </a> / <a class="ae jd" href="https://nbviewer.jupyter.org/urls/tonio73.github.io/data-science/linear/LinearRegressionBivariate-Keras.ipynb" rel="noopener ugc nofollow" target="_blank"> Jupyter </a></p><p id="d047" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该实验的均方误差为5.167，与基于独立回归的模型对女孩和男孩的均方误差非常接近。</p><figure class="lj lk ll lm fd lz er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es mm"><img src="../Images/ce62a166619289f4c2ff7e8e83f0a3e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gwDa8493zKo6uPmW74Rwsw.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">使用2层神经网络测量和预测的身高作为每个父母身高的函数</figcaption></figure><p id="baf8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">机器已经能够学习这个特性了！</strong></p><p id="cfcc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是不是意味着机器已经有意检测到了性别和身高的关系？</p><p id="a2c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与我们预期的不完全一样，我们将在下一节展示。</p></div><div class="ab cl jz ka gp kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hb hc hd he hf"><h1 id="8922" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">两种“性别化”模式的比较</h1><p id="5a54" class="pw-post-body-paragraph if ig hi ih b ii le ik il im lf io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">使用两个替代线性回归模型的预测也可以实现为具有两层的神经网络:</p><ul class=""><li id="2214" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">第一层有两个神经元，每个线性回归模型一个。只有一个模型将“激活”，即基于性别产生非空输出(一个热编码)</li><li id="bc49" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">第二层是层1的两个输出的简单加法器</li></ul><p id="476c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用这种拓扑，我们现在可以使用测试样本的第一个子样本作为示例(以及一个示例优化运行)来比较这两个解决方案。</p><figure class="lj lk ll lm fd lz er es paragraph-image"><div class="er es mn"><img src="../Images/87d3982b1ecfe4ec4759ec41205d5d03.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*cGcbcaOnqpEAJqWtxl35lA.png"/></div></figure><figure class="lj lk ll lm fd lz er es paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="er es mo"><img src="../Images/29c2b7c4a4d96ccdea34e6f254039462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ti6Gr8RuxybiZFFmXwU7gA.png"/></div></div><figcaption class="mg mh et er es mi mj bd b be z dx translated">组合回归模型与两层神经网络的系数比较</figcaption></figure><p id="0dfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上表可以看出，两个网络的系数在大小和符号上有很大不同。对于自动训练的网络，神经元2的系数非常低，它作为对神经元1的校正。</p><p id="6883" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理论上说，对于线性回归，在这种情况下，回归最优是唯一的。显然，在神经网络的情况下，有几个最佳值。</p></div><div class="ab cl jz ka gp kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hb hc hd he hf"><h1 id="c3f1" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">结论</h1><p id="9cdf" class="pw-post-body-paragraph if ig hi ih b ii le ik il im lf io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">从一个简单的实验开始，我们采取了几个工程步骤来改进它:</p><ul class=""><li id="0e14" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">特征工程对一次性编码和两个线性回归模型的组合</li><li id="3f58" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">使用神经网络和梯度下降优化的特征学习来学习性别和身高之间的关系</li></ul><p id="b165" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你喜欢这篇文章，请访问我的Github知识库，那里有更多关于“<a class="ae jd" href="https://tonio73.github.io/data-science/" rel="noopener ugc nofollow" target="_blank">逐步学习数据科学</a>”的笔记本。</p><p id="9db1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">并在项目上加一颗星，提高知名度。</p></div></div>    
</body>
</html>