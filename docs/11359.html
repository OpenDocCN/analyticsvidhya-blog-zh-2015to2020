<html>
<head>
<title>Deep RL: a Model-Based approach (part 3)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度RL:基于模型的方法(第3部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-rl-a-model-based-approach-part-3-planet-6427b76a21?source=collection_archive---------24-----------------------#2020-11-30">https://medium.com/analytics-vidhya/deep-rl-a-model-based-approach-part-3-planet-6427b76a21?source=collection_archive---------24-----------------------#2020-11-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="eb1c" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">不要猜测你的下一步行动。计划一下！</h2><div class=""/><p id="9563" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">深度计划网络(星球)</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/535ebe2ebccbfe7d5d12466e256f89d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*76pmUpoB05CoNwSDjDXljQ.jpeg"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">图片来自:<a class="ae kc" href="https://unsplash.com/@ninjason" rel="noopener ugc nofollow" target="_blank">梁朝伟</a>——<a class="ae kc" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="2dc2" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">在之前的文章<a class="ae kc" rel="noopener" href="/@enrico-busto/deep-rl-a-model-based-approach-part-2-drl-explained-837591ffadaa"> <strong class="iq hs">深度RL:基于模型的方法(第二部分)</strong> </a>中，我们看到了深度强化学习(DRL)是如何工作的，以及基于模型的方法如何提高样本效率。在本文中，我们提出了一种基于特定算法模型的算法，称为<strong class="iq hs"> PlaNet </strong>。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kd"><img src="../Images/f537be6755448b87cbc56df26ad5ea44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mnNMbhEsWi7Tqb3wwz4lrA@2x.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">学习模型可以让机器人在行动前进行规划。来源:<a class="ae kc" href="https://bair.berkeley.edu/blog/2019/12/12/mbpo/" rel="noopener ugc nofollow" target="_blank"> BAIR </a></figcaption></figure><h1 id="e743" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">从观察中学习</h1><p id="70b0" class="pw-post-body-paragraph io ip hi iq b ir lc it iu iv ld ix iy iz le jb jc jd lf jf jg jh lg jj jk jl hb bi translated">在绝大多数情况下，我们使用模拟器来创建用于训练具有强化学习的代理的环境。在每个时间步，模拟器收集所有必要的信息来产生一个新的状态，并将其发送给代理。</p><p id="9a27" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">在现实世界中，所有这些信息并不总是可用的。在更现实的场景中，机器人没有它们。例如，移动茶杯的机器人没有相对于桌子的精确坐标。通常，它只能使用相机来捕捉图像。出于这个原因，我们必须开发一种算法，允许代理解决问题，并教会它建立其内部表示。换句话说，代理将自动识别、收集和维护所有必要的信息。这使得训练问题变得更加复杂。</p><h1 id="b27b" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">星球:一个深度规划网络</h1><p id="0ffa" class="pw-post-body-paragraph io ip hi iq b ir lc it iu iv ld ix iy iz le jb jc jd lf jf jg jh lg jj jk jl hb bi translated">2019年，Danijar等人发布了<strong class="iq hs">深度规划网络(PlaNet) </strong>，这是一种基于模型的算法，能够仅从图像输入中直接学习环境模型，并将其用于规划。让我们简单解释一下它是如何工作的。</p><p id="d61d" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated"><em class="lh">注意:为了更好地理解这种方法，你应该知道自动编码器和门控循环单元(GRU)网络是如何工作的。</em></p><p id="87cd" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">自动编码器将输入限制在一个潜在向量中，然后重建它。它被训练以最小化原始输入和重构输入之间的均方误差(MSE)。这样，我们确保潜在向量将保持重建所需的所有主要输入信息。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es li"><img src="../Images/ce8d7e803bcaea78f79b668e24794bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7dgDximA_8tjaNO2bLkloA@2x.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">Autoencoder将输入压缩成一个潜在向量，这是Lux博士的实验室提供的。</figcaption></figure><p id="3b49" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">相反，GRU网络检查潜在向量的序列和执行动作的列表。通过这种方式，它可以提取物体的速度和方向等时间信息。</p><p id="9c0f" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">该代理将所有视觉和时间信息编码在一个紧凑的潜在向量序列中，并构建自己的抽象表示。它不是直接预测下一幅图像，而是学习预测下一个潜在向量并重建相应的观察。PlaNet也可以学习奖励功能。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es li"><img src="../Images/2a69c4ac2e4fcd99ba89891d34adfd02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SJp40xL9XoLIPrdOgQ1ftw@2x.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">行星建筑概述。来源:<a class="ae kc" href="https://drlux.github.io/planpix.html" rel="noopener ugc nofollow" target="_blank">勒克斯博士实验室</a>。</figcaption></figure><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es li"><img src="../Images/f433941a7301f393597052fdca60cfe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TM46i8mrUvvfn1n4u7f5JQ@2x.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">报酬与观察模型。最后一个从状态重构观察。来源:<a class="ae kc" href="https://drlux.github.io/planpix.html" rel="noopener ugc nofollow" target="_blank">勒克斯博士实验室</a>。</figcaption></figure><p id="d083" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">使用潜在向量也可以使规划过程更快。该规划器基于一种称为交叉熵方法(CEM)的遗传算法。对于每一代，我们使用多元高斯分布来生成一个动作序列群体。</p><p id="092a" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">然后，我们使用所学的奖励模型来计算，对于每个行动列表，我们通过执行它们可以获得的奖励金额。评估完所有的动作序列后，我们会从中挑选一个分组(最佳候选)。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es lj"><img src="../Images/fdfc0e1c118ddb26ddefe23ce0c785d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*geNiAV2Weig9a17IAlhMDg@2x.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">CEM算法的可视化表示。来源:<a class="ae kc" href="https://drlux.github.io/planpix.html" rel="noopener ugc nofollow" target="_blank">勒克斯博士实验室</a>。</figcaption></figure><p id="7e90" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">我们使用这个子组来更新高斯参数并生成下一个种群。</p><p id="6e25" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">在下一篇文章中，我们将介绍一些使用PlaNet进行的实验，在这些实验中，我们分析了有效的预测能力，并将获得的结果与无模型基线进行了比较。</p><p id="1215" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">* *本文是与<a class="lk ll ge" href="https://medium.com/u/4bd9c016c60?source=post_page-----6427b76a21--------------------------------" rel="noopener" target="_blank">卢卡·索伦蒂诺</a>合作撰写的</p></div></div>    
</body>
</html>