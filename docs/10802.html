<html>
<head>
<title>Concept of AlexNet:- Convolutional Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AlexNet的概念:-卷积神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/concept-of-alexnet-convolutional-neural-network-6e73b4f9ee30?source=collection_archive---------4-----------------------#2020-11-03">https://medium.com/analytics-vidhya/concept-of-alexnet-convolutional-neural-network-6e73b4f9ee30?source=collection_archive---------4-----------------------#2020-11-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/392bff97cf95f45f328b79908b1130b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pJ3o_2zTTNnixhKH.png"/></div></div></figure><p id="7256" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本文包括九个部分:</p><ol class=""><li id="a24a" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">AlexNet是什么？</li><li id="7c9b" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">AlexNet的架构。</li><li id="7f94" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">数据集。</li><li id="de9d" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">雷鲁。</li><li id="a3c6" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">辍学。</li><li id="77eb" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">AlexNet的优点。</li><li id="17cb" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">AlexNet的缺点。</li><li id="f3c5" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">使用Python的AlexNet。</li><li id="a5c1" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">结论。</li></ol><h1 id="ca58" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">AlexNet是什么？</h1><p id="4eff" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi lf translated"><span class="l lg lh li bm lj lk ll lm ln di">一个</span> <a class="ae lo" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank"> lexNet </a>是一个<strong class="is hj">卷积神经网络架构</strong>的名字，它在<strong class="is hj"> 2012 </strong>赢得了LSVRC竞赛。</p><p id="66db" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">LSVRC <strong class="is hj"> ( </strong>大规模视觉识别挑战)是一项竞赛，研究团队在一个巨大的标记图像数据集(<strong class="is hj"> ImageNet </strong>)上评估他们的算法，并竞争在几项视觉识别任务上实现更高的准确性。这对团队之后如何完成工作产生了巨大的影响。</p><h1 id="a546" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">AlexNet的体系结构</h1><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lp"><img src="../Images/2bdbe66bd0a2bd97693155b650e01104.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ecOq26xunjQ5jWTF.png"/></div></div></figure><p id="9ad8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> AlexNet包含8层</strong>带重量；</p><p id="7d53" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 5个卷积层</strong></p><p id="ecfd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 3个全连接层</strong>。</p><p id="2b28" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在每一层的末端，执行ReLu激活，除了最后一层，其输出具有分布在1000个类标签上的softmax。前两个完全连接的层中应用了下降。如上图所示，在第一、第二和第五卷积层之后也应用最大池。第二层、第四层和第五层卷积层的内核仅连接到前一层的内核映射，这些内核映射驻留在同一个GPU上。第三卷积层的核连接到第二层中的所有核映射。完全连接层中的神经元连接到前一层中的所有神经元。</p><h1 id="420d" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">资料组</h1><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/3a347312bbad9d9d8ed31a8136765b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*km4DulG98uH6g5kT.jpeg"/></div></div></figure><p id="1de3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi lf translated">mageNet 是一个图像数据库，包含超过1500万张标记为22000个类别的高分辨率图像。该比赛使用ImageNet的图像子集，并要求研究人员实现最低的前1名和前5名错误率。AlexNet的输入是大小为256×256的RGB图像。这意味着训练集中的所有图像和所有测试图像的大小都需要为256×256。也就是说，在使用它来训练网络之前，需要将其转换为256×256。</p><h1 id="7898" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">热卢</h1><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="ab fe cl lv"><img src="../Images/96f78dc7dc195a026f600fd590ed0eef.png" data-original-src="https://miro.medium.com/v2/format:webp/0*8sSgOQN8a-XPOMdd.png"/></div></figure><p id="0723" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">AlexNet的一个重要特征是使用ReLU(校正线性单元)非线性。</p><p id="bff4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Tanh或sigmoid激活函数曾经是训练神经网络模型的常用方法。</p><p id="c558" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">AlexNet表明，使用ReLU非线性，深度CNN可以比使用饱和激活函数(如tanh或sigmoid)更快地训练。</p><p id="ae48" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在CIFAR-10数据集上测试。</p><p id="c37b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们看看为什么它用ReLUs训练得更快。ReLU函数由下式给出</p><p id="5d2a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">f(x) = max(0，x)</p><p id="5476" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">两个函数的曲线图—</p><p id="93ff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 1。tanh </strong></p><p id="6eda" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 2。雷鲁。</strong></p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/589fb264fa15d08d316a44f411cf6361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4oyP8c5Uqp-TL4C3.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">www.learnopencv.com<a class="ae lo" href="http://www.learnopencv.com" rel="noopener ugc nofollow" target="_blank">图片来源</a></figcaption></figure><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/d0b06a562c5c789a7decd2bdf50e32a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*2cfozND0q0Auj2g9.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">图片来源<a class="ae lo" href="http://www.learnopencv.com" rel="noopener ugc nofollow" target="_blank">www.learnopencv.com</a></figcaption></figure><p id="e2ee" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当z值非常高或非常低时，双曲正切函数会饱和，在这些区域，函数的斜率非常接近于零。这可以减缓梯度下降。</p><p id="a96e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于z的较高正值，ReLU函数的斜率不接近零。这有助于优化更快地收敛。对于z的负值，斜率仍然为零，但是神经网络中的大多数神经元通常以正值结束。</p><p id="a0fd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">出于同样的原因，ReLU也优于sigmoid函数。</strong></p><blockquote class="mc md me"><p id="3cd5" class="iq ir mf is b it iu iv iw ix iy iz ja mg jc jd je mh jg jh ji mi jk jl jm jn hb bi translated"><strong class="is hj">过拟合问题。</strong> AlexNet有6000万个参数，这是过度拟合方面的一个主要问题。</p></blockquote><p id="7ae4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">减少过度拟合的两种方法:</p><ol class=""><li id="31fc" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated"><strong class="is hj">数据扩充</strong></li><li id="e1cd" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated"><strong class="is hj">辍学。</strong></li></ol><h1 id="28c4" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated"><strong class="ak">数据扩充。</strong></h1><p id="6cea" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">作者生成了图像翻译和水平反射，到2048年增加了训练集。他们还对RGB像素值执行了主成分分析(PCA ),以改变RGB通道的强度，从而将top-1错误率降低了1%以上。</p><h1 id="9615" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">拒绝传统社会的人</h1><p id="e4c6" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">AlexNet用来避免过度拟合的第二个技巧是辍学。它包括以0.5的概率将每个隐藏神经元的输出设置为零。以这种方式“退出”的神经元对正向传递没有贡献，并且不参与<strong class="is hj">反向传播</strong>。因此，每次输入出现时，神经网络都会采样不同的架构。这项技术包括<strong class="is hj">以预定的概率关闭</strong>神经元。这意味着每次迭代，“关闭”的神经元对正向传递没有贡献，也不参与反向传播。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/871455a6c6cfcb2da4d57679043424a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/0*GVpofJTybmYuMYMN.gif"/></div></figure><h1 id="31cc" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">AlexNet的优点</h1><ol class=""><li id="de32" class="jo jp hi is b it la ix lb jb mk jf ml jj mm jn jt ju jv jw bi translated">AlexNet是consideredasthemistoneofcnnforimageclassification。</li><li id="5ed6" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">许多方法，如conv+池设计、dropout、GPU、并行计算、ReLU，仍然是计算机视觉的工业标准。</li><li id="f72c" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">TheuniqueadvantageofAlexNet是分类模型的直接图像输入。</li><li id="2dcb" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">进化层可以自动提取边缘图像和学习这些特征的完全连接的层</li><li id="105f" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">理论上，visual patterns的复杂性可以有效地提取dbyaddingmoreconvlayer</li></ol><h1 id="b056" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">AlexNet的缺点</h1><ol class=""><li id="8ccd" class="jo jp hi is b it la ix lb jb mk jf ml jj mm jn jt ju jv jw bi translated">AlexNet相对于后来的模型，比如VGGNet、GoogLENet、ResNet，深度不够。</li><li id="7e75" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">此后不久，不鼓励使用大型卷积滤波器(5*5)。</li><li id="7b8f" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">使用正态分布来初始化神经网络中的权重，不能有效地解决梯度消失的问题，后来被Xavier方法所取代。</li><li id="2be3" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">谷歌网络(6.7%)和ResNet (3.6%)等更复杂的模型超越了这一表现</li></ol><h1 id="60cf" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">使用Python的AlexNet</h1><div class="mn mo ez fb mp mq"><a href="https://github.com/abhijeetpujara/AlexNet" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hj fi z dy mv ea eb mw ed ef hh bi translated">abhijeetpujara/AlexNet</h2><div class="mx l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">github.com</p></div></div><div class="my l"><div class="mz l na nb nc my nd io mq"/></div></div></a></div><h1 id="936b" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">结论</h1><p id="7bb8" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">AlexNet是一个监督学习的作品，取得了很好的效果。</p><p id="5d48" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">选择有助于提高网络性能的方法(如删除和数据扩充)也很重要。</p><p id="bd13" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">AlexNet在延续至今的ConvNets上做了革命性的实现，比如ReLU和dropout。</p><p id="2e54" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在没有过拟合的情况下，不容易有低分类误差。</p><p id="3dcf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">为了更清楚地了解，请访问此视频</strong></p><figure class="lq lr ls lt fd ij"><div class="bz dy l di"><div class="ne nf l"/></div></figure></div><div class="ab cl ng nh gp ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="hb hc hd he hf"><p id="2630" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p><h1 id="842c" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">我关于机器学习算法的其他帖子</h1><div class="mn mo ez fb mp mq"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/random-forest-algorithm-with-python-7ccfbe9bcb47"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hj fi z dy mv ea eb mw ed ef hh bi translated">用Python实现随机森林算法</h2><div class="nn l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">在本文中，我们将探索著名的监督机器学习算法“随机…</h3></div><div class="mx l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">medium.com。</p></div></div><div class="my l"><div class="no l na nb nc my nd io mq"/></div></div></a></div><div class="mn mo ez fb mp mq"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/what-is-the-support-vector-machine-svm-dc89207c011"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hj fi z dy mv ea eb mw ed ef hh bi translated">什么是支持向量机(SVM)</h2><div class="nn l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">本文包括三个部分:</h3></div><div class="mx l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">medium.com</p></div></div><div class="my l"><div class="np l na nb nc my nd io mq"/></div></div></a></div><div class="mn mo ez fb mp mq"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/k-nearest-neighbors-algorithm-knn-with-python-e570f6bb8aed"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hj fi z dy mv ea eb mw ed ef hh bi translated">基于Python的k近邻算法(KNN)</h2><div class="nn l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">本文包括六个部分:</h3></div><div class="mx l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">medium.com</p></div></div><div class="my l"><div class="nq l na nb nc my nd io mq"/></div></div></a></div><div class="mn mo ez fb mp mq"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/naïve-bayes-algorithm-with-python-7b3aef57fb59"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hj fi z dy mv ea eb mw ed ef hh bi translated">用Python实现朴素贝叶斯算法</h2><div class="nn l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">本文包括五个部分:</h3></div><div class="mx l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">medium.com</p></div></div><div class="my l"><div class="nr l na nb nc my nd io mq"/></div></div></a></div><div class="mn mo ez fb mp mq"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/image-classification-with-mobilenet-cc6fbb2cd470"><div class="mr ab dw"><div class="ms ab mt cl cj mu"><h2 class="bd hj fi z dy mv ea eb mw ed ef hh bi translated">利用MobileNet进行图像分类</h2><div class="nn l"><h3 class="bd b fi z dy mv ea eb mw ed ef dx translated">本文包括五个部分:</h3></div><div class="mx l"><p class="bd b fp z dy mv ea eb mw ed ef dx translated">medium.com</p></div></div><div class="my l"><div class="ns l na nb nc my nd io mq"/></div></div></a></div></div><div class="ab cl ng nh gp ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="hb hc hd he hf"><p id="ab7b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">快乐学习！！！</strong></p><p id="880d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">快乐编码:)</strong></p><p id="c04b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">别忘了拍手拍手拍手…</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es nt"><img src="../Images/072de3aad6da33986b21858816626a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*lDdMocjqj7zXCOnk.gif"/></div></figure></div></div>    
</body>
</html>