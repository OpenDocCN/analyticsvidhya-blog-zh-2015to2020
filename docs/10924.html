<html>
<head>
<title>Implementing Gradient Descent for multilinear regression from scratch.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始实现多线性回归的梯度下降。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementing-gradient-descent-for-multi-linear-regression-from-scratch-3e31c114ae12?source=collection_archive---------2-----------------------#2020-11-09">https://medium.com/analytics-vidhya/implementing-gradient-descent-for-multi-linear-regression-from-scratch-3e31c114ae12?source=collection_archive---------2-----------------------#2020-11-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b38ac4cdcd89d48748f3ea43b2e6464d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*31b0zgfcDVIRmw5zL6yQsA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://unsplash.com/photos/wrfj-SRaB1Q?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditShareLink" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/wrfj-SRaB1Q?utm_source=unsplash&amp;UTM _ medium = referral&amp;UTM _ content = creditShareLink</a></figcaption></figure></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="21d0" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><strong class="je hj">梯度下降</strong>是最流行的优化算法之一，每个数据科学爱好者都应该对这个话题有深刻的理解。</p><p id="1e8b" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">在这里，在这个博客中，我的目标是让即使是机器学习领域的新手也能容易地理解。唯一的先决条件只是基本的 python。在这篇博客中，我将使用波士顿房价数据集，这是一个由 sklearn 图书馆提供的玩具数据集。</p><p id="5e34" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">关于数据集:它有 506 条记录、13 个特征和 1 个目标变量。</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ka"><img src="../Images/68d0b0e375fdc4a5bab7368eedc687af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cyJgFIRzwGYySuX7r-tRYg.png"/></div></div></figure><p id="f661" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">梯度下降法是一种迭代算法，用于损失函数中寻找全局极小值。损失可以是任何微分损失函数。不同类型的损失函数是线性损失、逻辑损失、铰链损失等。对于我们的数据集，我们将使用线性损失，因为目标是一个连续变量。</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kf"><img src="../Images/e6c653dfc3909578bfb3a8df3cd9e353.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wc4t5QAh8PP92zEAh4CiVQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://datascience.stackexchange.com/questions/24534/does-gradient-descent-always-converge-to-an-optimum/24537" rel="noopener ugc nofollow" target="_blank">https://data science . stack exchange . com/questions/24534/does-gradient-descent-always-converge-to-a-optimum/24537</a></figcaption></figure><p id="6522" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">在上图中，将损失函数视为凸函数，在 x 轴上，我们有参数权重，在 y 轴上，我们有损失值，蓝色曲线是损失函数。首先，我们随机初始化权重“w ”,并使用梯度下降算法获得给出最小损失的最终权重“w”。</p><p id="09ff" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">当我们说我们正在建立一个线性回归模型时，它只不过是我们试图找到一条最适合数据的直线(一个特征)或超平面(多个特征)。并且直线的方程由“mx+b=0”表示，其中“m”是斜率，“b”是偏差。我们也可以把“m”称为重量，“b”称为截距。</p><p id="3e3d" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">例如，如果我们有 5 个特征，那么超平面的方程表示为:</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kg"><img src="../Images/1444a9004ee485c05762d0206e294eed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CDcFW4cGGSpksG2CuL6oYQ.png"/></div></div></figure><h2 id="448d" class="kh ki hi bd kj kk kl km kn ko kp kq kr jn ks kt ku jr kv kw kx jv ky kz la lb bi translated">现在让我们看看如何找到最佳拟合线使用梯度下降。</h2><p id="bc55" class="pw-post-body-paragraph jc jd hi je b jf lc jh ji jj ld jl jm jn le jp jq jr lf jt ju jv lg jx jy jz hb bi translated">在上面的线方程中，“m”和“b”是我们需要使用梯度下降来更新以找到最佳拟合线(当我说最佳拟合线时，它无非是在损失函数中找到最小值)的参数，“x”是给定的输入数据。</p><p id="b71d" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">更新权重和偏差的等式:</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/060d274d37c0b1a79835f27301ffddbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ASJrg7FBDmMaxfwdZtFDRw.png"/></div></div></figure><p id="b181" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">其中“dl/dw”是损耗 w.r.t .重量的导数，“dl/db”是损耗 w.r.t .偏差的导数，“n”是记录总数。这里，权重是一个大小=13 的向量(我们有 13 个特征)。</p><p id="b21e" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><strong class="je hj">梯度下降的步骤:</strong></p><ol class=""><li id="a4f8" class="li lj hi je b jf jg jj jk jn lk jr ll jv lm jz ln lo lp lq bi translated">标准化数据:</li></ol><figure class="kb kc kd ke fd ij"><div class="bz dy l di"><div class="lr ls l"/></div></figure><p id="cf2a" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">2.初始化参数和超级参数</p><figure class="kb kc kd ke fd ij"><div class="bz dy l di"><div class="lr ls l"/></div></figure><p id="7b0f" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">3.求重量和偏差损失的导数。</p><ul class=""><li id="925c" class="li lj hi je b jf jg jj jk jn lk jr ll jv lm jz lt lo lp lq bi translated">损耗方程</li></ul><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/a3189b55aa464922a1a20bacf8c8ea92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dD7p3j4J-oIiI2h-ZgnCLA.png"/></div></div></figure><figure class="kb kc kd ke fd ij"><div class="bz dy l di"><div class="lr ls l"/></div></figure><figure class="kb kc kd ke fd ij"><div class="bz dy l di"><div class="lr ls l"/></div></figure><ul class=""><li id="f870" class="li lj hi je b jf jg jj jk jn lk jr ll jv lm jz lt lo lp lq bi translated">“重量”损失的衍生产品</li></ul><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lv"><img src="../Images/f1d2ce0ecd06275d273fcb711fe915f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*24qg_EizfunJ9rE1UTq3wg.png"/></div></div></figure><figure class="kb kc kd ke fd ij"><div class="bz dy l di"><div class="lr ls l"/></div></figure><ul class=""><li id="4e8b" class="li lj hi je b jf jg jj jk jn lk jr ll jv lm jz lt lo lp lq bi translated">损失的导数与“偏差”或“截距”有关</li></ul><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/8b2b765e3da48ee07fa51ec854515ecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nNSWovmz5Yh4CMqZ5SEq2g.png"/></div></div></figure><figure class="kb kc kd ke fd ij"><div class="bz dy l di"><div class="lr ls l"/></div></figure><p id="1d55" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">4.更新权重和偏差，直到我们得到全局最小值。</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lh"><img src="../Images/060d274d37c0b1a79835f27301ffddbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ASJrg7FBDmMaxfwdZtFDRw.png"/></div></div></figure><figure class="kb kc kd ke fd ij"><div class="bz dy l di"><div class="lr ls l"/></div></figure><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/8dd1c75c4bee509eb83223c82abad8e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ceOQS4q2NYl5i_HC2X78Q.png"/></div></div></figure><ul class=""><li id="1ca7" class="li lj hi je b jf jg jj jk jn lk jr ll jv lm jz lt lo lp lq bi translated">上图是损失和纪元编号之间的关系图。</li><li id="4f82" class="li lj hi je b jf ly jj lz jn ma jr mb jv mc jz lt lo lp lq bi translated">在每个时期之后，损失减少</li><li id="e592" class="li lj hi je b jf ly jj lz jn ma jr mb jv mc jz lt lo lp lq bi translated">最初，损失急剧下降，直到 1000 纪元</li><li id="12f4" class="li lj hi je b jf ly jj lz jn ma jr mb jv mc jz lt lo lp lq bi translated">在 1000 个时期之后，损失有最小的减少。</li><li id="d15b" class="li lj hi je b jf ly jj lz jn ma jr mb jv mc jz lt lo lp lq bi translated">这表明我们已经达到了全球最低水平。</li></ul><p id="43dd" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">5.使用最终的权重向量和偏差，我们可以预测输出</p><figure class="kb kc kd ke fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/0c553256391a7b424fe8d9f7ea0ab97f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7CU9Gw93XuZl7QK_4Jcq0A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">这是 2000 年后的最终权重和偏差</figcaption></figure><figure class="kb kc kd ke fd ij er es paragraph-image"><div class="er es me"><img src="../Images/cfe4dd625b7a0e8afdf98961467a78ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*PJ09JMQL7kdIYsRdnhe4cg.png"/></div></figure><ul class=""><li id="05eb" class="li lj hi je b jf jg jj jk jn lk jr ll jv lm jz lt lo lp lq bi translated">上表是实际目标和模型预测目标之间的比较。</li></ul><p id="a934" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><em class="mf">最后，如果你想看到完整的代码，可以点击这个链接</em><a class="ae iu" href="https://github.com/GUNAND12/multi_linear-Gradient-descent/blob/main/gradient_descent.ipynb" rel="noopener ugc nofollow" target="_blank"><em class="mf">【Github】</em></a><em class="mf">。</em></p><p id="2a1a" class="pw-post-body-paragraph jc jd hi je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">这里是在同一数据集上实现多线性回归的随机梯度下降的链接:<a class="ae iu" href="https://github.com/GUNAND12/multi_linear-Gradient-descent/blob/main/Stocastic_gradient_descent.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a></p><h1 id="02b8" class="mg ki hi bd kj mh mi mj kn mk ml mm kr mn mo mp ku mq mr ms kx mt mu mv la mw bi translated">如果你喜欢这篇文章:</h1><ul class=""><li id="27bb" class="li lj hi je b jf lc jj ld jn mx jr my jv mz jz lt lo lp lq bi translated">你可以在<a class="ae iu" href="https://www.linkedin.com/in/gunand-mayanglambam-98727b141/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我</li></ul></div></div>    
</body>
</html>