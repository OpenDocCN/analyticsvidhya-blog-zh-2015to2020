<html>
<head>
<title>Hierarchical Attention Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分层注意网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/hierarchical-attention-networks-d220318cf87e?source=collection_archive---------0-----------------------#2018-08-24">https://medium.com/analytics-vidhya/hierarchical-attention-networks-d220318cf87e?source=collection_archive---------0-----------------------#2018-08-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="8242" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">最人性化的文本分类方式</h2></div><h2 id="c9c2" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">这些关于文本分类的宣传是怎么回事？</h2><p id="e342" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">自从人工智能兴起以来，文本分类已经成为最令人震惊的任务之一。通俗地说，我们可以说人工智能是试图实现类似人类的智能模型来减轻我们所有人的工作的领域。我们在文本分类方面有着惊人的熟练程度，但即使是许多复杂的NLP模型也无法达到甚至接近它的熟练程度。所以问题来了，我们人类做的有什么不同？我们如何对文本进行分类？</p><p id="8b67" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">首先，我们理解单词不是每个单词，而是许多单词，我们甚至可以通过句子的结构来猜测未知的单词。然后我们就理解了那一系列单词(句子)所传达的信息。然后从那一系列的句子中，我们理解一段话或者一篇文章的意思。类似的方法也用于分层注意力模型。</p><h2 id="d236" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">那么这个分等级的东西有什么特别的呢？</h2><p id="dd2f" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">用一种“太复杂以至于技术人员都无法理解”的方式来说，它在单词级别上使用堆叠的递归神经网络，然后使用注意力模型来提取对句子意义重要的单词，并聚集这些信息性单词的表示来形成句子向量。然后将相同的过程应用于导出的句子向量，然后生成一个向量，该向量理解给定文档的含义，并且该向量可以被进一步传递用于文本分类。</p><h2 id="9db3" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">等等…什么？</h2><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/79bf0429dc9eb9840093a63062516d9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*28XVtq2lOjOmZhcSgu1NmQ.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">汉族结构</figcaption></figure><p id="45a7" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">论文背后的思想是“词造句，句造文”。目的是从单词中推导出句子的意思，然后从这些句子中推导出文档的意思。但并不是所有的单词都同样重要。它们中的一些比其他的更能描述一个句子。因此，我们使用注意模型，以便句子向量能够更多地注意“重要”的单词。注意模型由两部分组成:双向RNN和注意网络。双向RNN学习这些单词序列背后的含义，并返回对应于每个单词的向量，而注意力网络使用其自己的浅层神经网络来获得对应于每个单词向量的权重。然后，它聚集这些单词的表示来形成句子向量，即它计算每个向量的加权和。这个加权和体现了整个句子。同样的过程也适用于句子向量，因此最终的向量体现了整个文档的主旨。由于它有两个层次的注意模型，因此，它被称为分层注意网络。</p><h2 id="ff8c" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">说够了…给我看看代码</h2><p id="4798" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">我们使用<a class="ae lj" href="https://www.kaggle.com/rmisra/news-category-dataset" rel="noopener ugc nofollow" target="_blank">新闻类别数据集</a>对新闻类别进行分类。你可以在这里看到整个实现<a class="ae lj" href="https://www.kaggle.com/hsankesara/news-classification-using-han/notebook" rel="noopener ugc nofollow" target="_blank">。现在第一个出现在脑海中的问题是注意力到底是什么？</a></p><h2 id="571f" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">注意力模型</h2><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lk"><img src="../Images/00f2946f808bb707f886fa3cd4e77ab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABkaR2glZNP6oh08oY4l-Q.png"/></div></div></figure><p id="49cf" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">来自双向RNN的向量通过浅层神经网络来确定每个向量对应的权重。每个向量的加权和体现了这些向量组合的含义。要更简单地理解它，只需进入<a class="ae lj" href="https://github.com/Hsankesara/DeepResearch/blob/master/Hierarchical_Attention_Network/attention_with_context.py" rel="noopener ugc nofollow" target="_blank">代码</a>。</p><p id="3a71" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated"><strong class="jx hj">数据预处理</strong></p><p id="8a04" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">为了处理数据，我们需要把它转换成合适的形式。</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="ed25" class="ix iy hi lm b fi lq lr l ls lt">tokenizer = Tokenizer(num_words=max_features, oov_token=True)<br/>tokenizer.fit_on_texts(texts)<br/>data = np.zeros((len(texts), max_senten_num, max_senten_len), dtype='int32')<br/>for i, sentences in enumerate(paras):<br/>    for j, sent in enumerate(sentences):<br/>        if j&lt; max_senten_num:<br/>            wordTokens = text_to_word_sequence(sent)<br/>            k=0<br/>            for _, word in enumerate(wordTokens):<br/>                try:<br/>                    if k&lt;max_senten_len and tokenizer.word_index[word]&lt;max_features:<br/>                        data[i,j,k] = tokenizer.word_index[word]<br/>                        k=k+1<br/>                except:<br/>                    print(word)<br/>                    pass</span></pre><p id="aacb" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">我们使用上面的代码将训练数据集转换为三维数组:第一维表示文档总数，第二维表示文档中的每个句子，最后一维表示句子中的每个单词。然而，我们必须设置一些上限，以便创建一个静态图，在这种情况下是<code class="du lu lv lw lm b">max_senten_len</code>(一个段落中句子的最大数量)<code class="du lu lv lw lm b">max_senten_num</code>(一个句子中的最大字数)和<code class="du lu lv lw lm b">max_features</code>(分词器可以拥有的最大字数)。</p><p id="5329" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">如果我们随机初始化所有的单词，是不是对模型不公平？因此，我们使用<a class="ae lj" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">训练过的嵌入向量</a>，这在性能方面给予模型额外的优势，并产生更好的结果。</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="cb11" class="ix iy hi lm b fi lq lr l ls lt">GLOVE_DIR = "../input/glove6b/glove.6B.100d.txt"<br/>embeddings_index = {}<br/>f = open(GLOVE_DIR)<br/>for line in f:<br/>    try:<br/>        values = line.split()<br/>        word = values[0]<br/>        coefs = np.asarray(values[1:], dtype='float32')<br/>        embeddings_index[word] = coefs<br/>    except:<br/>        print(word)<br/>        pass<br/>f.close()<br/>embedding_matrix = np.zeros((len(word_index) + 1, embed_size))<br/>absent_words = 0<br/>for word, i in word_index.items():<br/>    embedding_vector = embeddings_index.get(word)<br/>    if embedding_vector is not None:<br/>        # words not found in embedding index will be all-zeros.<br/>        embedding_matrix[i] = embedding_vector<br/>    else:<br/>        absent_words += 1<br/>print('Total absent words are', absent_words, 'which is', "%0.2f" % (absent_words * 100 / len(word_index)), '% of total words')</span></pre><p id="cd07" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">我们在<code class="du lu lv lw lm b">embedding_matrix</code>中将已知单词替换为其对应的向量。的确，有些词会被遗漏，但我们的模型必须学会应对这种情况。</p><h2 id="8218" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">现在是韩模型的时候了</h2><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="42c2" class="ix iy hi lm b fi lq lr l ls lt">embedding_layer = Embedding(len(word_index) + 1,embed_size,weights=[embedding_matrix], input_length=max_senten_len, trainable=False)</span><span id="0838" class="ix iy hi lm b fi lx lr l ls lt"># Words level attention model<br/>word_input = Input(shape=(max_senten_len,), dtype='float32')<br/>word_sequences = embedding_layer(word_input)<br/>word_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(word_sequences)<br/>word_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(word_lstm)<br/>word_att = AttentionWithContext()(word_dense)<br/>wordEncoder = Model(word_input, word_att)</span><span id="8ae4" class="ix iy hi lm b fi lx lr l ls lt"># Sentence level attention model<br/>sent_input = Input(shape=(max_senten_num, max_senten_len), dtype='float32')<br/>sent_encoder = TimeDistributed(wordEncoder)(sent_input)<br/>sent_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(sent_encoder)<br/>sent_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(sent_lstm)<br/>sent_att = Dropout(0.5)(AttentionWithContext()(sent_dense))<br/>preds = Dense(30, activation='softmax')(sent_att)<br/>model = Model(sent_input, preds)<br/>model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])</span></pre><p id="0564" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">对于word2vec，我们使用了Keras <a class="ae lj" href="https://keras.io/layers/embeddings/" rel="noopener ugc nofollow" target="_blank">嵌入</a>层。<a class="ae lj" href="https://keras.io/layers/wrappers/" rel="noopener ugc nofollow" target="_blank">时间分布</a>方法用于将<code class="du lu lv lw lm b">Dense</code>层独立应用于每个时间步。我们使用<code class="du lu lv lw lm b">Dropout</code>和<code class="du lu lv lw lm b">l2_reg</code>正则化来减少过度拟合。</p><h2 id="dd48" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">结论</h2><p id="f033" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">你现在一定很惊讶或者很困惑。有时这些事情可能会过头，但文本分类是一个趋势领域，尽管有许多新的和多产的研究，改进的范围是如此之大。所以不要只在现在绝望，因为你以后会有更多的失望😅。开玩笑，如果你有任何疑问，请在下面评论或参考资源页面。</p><h2 id="6949" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">资源</h2><ol class=""><li id="d466" class="ly lz hi jx b jy jz kb kc ji ma jm mb jq mc kn md me mf mg bi translated">前往<a class="ae lj" href="https://github.com/Hsankesara/DeepResearch/tree/master/Hierarchical_Attention_Network" rel="noopener ugc nofollow" target="_blank">此处</a>查看代码。</li><li id="0745" class="ly lz hi jx b jy mh kb mi ji mj jm mk jq ml kn md me mf mg bi translated">去<a class="ae lj" href="https://www.kaggle.com/hsankesara/news-classification-using-han/notebook" rel="noopener ugc nofollow" target="_blank">这里</a>结账实现。</li><li id="f0a0" class="ly lz hi jx b jy mh kb mi ji mj jm mk jq ml kn md me mf mg bi translated"><a class="ae lj" href="https://www.ncbi.nlm.nih.gov/pubmed/29155996" rel="noopener ugc nofollow" target="_blank">用于从癌症病理学报告中提取信息的分级注意网络。</a></li><li id="bc16" class="ly lz hi jx b jy mh kb mi ji mj jm mk jq ml kn md me mf mg bi translated"><a class="ae lj" href="http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" rel="noopener ugc nofollow" target="_blank">用于文档分类的分层注意力网络</a></li></ol><h1 id="bc4d" class="mm iy hi bd iz mn mo mp jd mq mr ms jh io mt ip jl ir mu is jp iu mv iv jt mw bi translated">作者说明</h1><p id="bcb3" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">本教程是我的系列文章<a class="ae lj" href="https://github.com/Hsankesara/DeepResearch" rel="noopener ugc nofollow" target="_blank"> DeepResearch </a>的第一篇。如果你喜欢这个教程，请在评论中告诉我，如果你不喜欢，请在评论中简单告诉我。如果你有任何疑问或批评，请在评论中大量发表。我会尽快回复你的。如果你喜欢这个教程，请与你的同伴分享。</p></div></div>    
</body>
</html>