<html>
<head>
<title>Linear Regression — Part II — Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归第二部分梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-part-ii-gradient-descent-4ddd205da2ca?source=collection_archive---------21-----------------------#2020-06-11">https://medium.com/analytics-vidhya/linear-regression-part-ii-gradient-descent-4ddd205da2ca?source=collection_archive---------21-----------------------#2020-06-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/bef6790ce77dc946b17b2f3949e99008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZHQmzXr3Sqzwrbs3Xgvz-A.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">班格班格，西澳大利亚。来源:https://www.kimberleyspirit.com/east-kimberley</figcaption></figure><div class=""/><blockquote class="iv iw ix"><p id="0412" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jb hz">梯度下降</strong>是一种优化算法，用于<strong class="jb hz">最小化由模型参数化的成本函数</strong>(即误差)。</p></blockquote><p id="93be" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">我们知道梯度意味着一个面或一条线的斜率。这种算法涉及斜率计算。</p><p id="84da" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">为了理解梯度下降，我们必须知道什么是成本函数。</p><blockquote class="iv iw ix"><p id="fc2a" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jb hz">线性回归<strong class="jb hz">的代价函数</strong>(J)</strong>是预测y值(predicted)与真实y值(y)之间的均方根误差(RMSE)。</p></blockquote><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es ka"><img src="../Images/2827f0ff04a8dbe3204d3baedf86e369.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*-pgdFnT8cn5u4uKeVLgAaA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">来源:<a class="ae hv" href="https://www.geeksforgeeks.org/ml-linear-regression/" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/ml-linear-regression/</a></figcaption></figure><p id="4ae1" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">对于线性回归模型，我们的最终目标是得到一个代价函数的最小值。</p><p id="3933" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">要熟悉线性回归，请阅读本文。</p><p id="6f47" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">首先让我们想象一下梯度下降是什么样子，以便更好地理解。</p><p id="d0bd" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">由于梯度下降是一种迭代算法，我们将拟合各种线，以找到最佳拟合线迭代。</p><p id="d035" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">每次我们得到一个错误值(SSE)。</p><p id="1d3d" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">如果我们把所有的误差值拟合在一个图形里，它就会变成一条抛物线。</p><p id="e4fc" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi kf translated"><span class="l kg kh ki bm kj kk kl km kn di"> W </span> <strong class="jb hz">斜率、截距和SSE是什么关系？为什么梯度下降是抛物线？</strong></p><p id="3156" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">要回答这两个问题，让我们看看下面的例子。</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es ko"><img src="../Images/d634f4e7b41015254f362761583195cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*lq35fwR-oCRXIK0whOLCqQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">Excel女士制作。</figcaption></figure><p id="be5a" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">你可以注意到我取了随机值m = 0.45 &amp; c = 0.75。</p><p id="dc03" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">对于这个斜率和截距，我们为回归线提出了一个新的Y预测值。</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es kp"><img src="../Images/732e8d7da33b3684de5318014e2e5049.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*9cg5-BtcgEs-mz9hTqRMMw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">在MS. Excel中制作</figcaption></figure><p id="1256" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">在上图中，蓝色点是原始Y值，橙色点是我们刚刚发现的m = 0.45 &amp; c = 0.75的Y预测值。我们可以明确地说，这条线不是该数据的最佳拟合线。</p><p id="5c84" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">这个迭代的总SSE大约是6.66。</p><p id="360e" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">然后，让我们通过随机减小m &amp; c值来进行迭代，结果如下所示:</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kq"><img src="../Images/1acb692b58d2c829c14ee9f02f604369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b68xzx-tYG0qFmx___svwQ.png"/></div></div></figure><p id="16fb" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">在上表中，我们可以看到，对于不同的m &amp; c值，误差值逐渐减小，并从某一点开始再次增大。</p><p id="836e" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">如果我们单独为所有的误差值绘制一个图表，那么它将看起来像一条曲线，如右侧所示。</p><p id="65b7" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">现在，我们将绘制原始图表中由上述m &amp; c值形成的所有线条(列:线条方程):</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kr"><img src="../Images/465058fc372f5a4bc79cd6900a4c6991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bnKPp8cKIQo4L0JHelf6YA.png"/></div></div></figure><p id="fd3f" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">您可以看到，粉色标记线是由m = 1.7和c = 1组成的，这产生了m，c &amp; Error表中所有其他m &amp; c值的最小误差。蓝色线是excel本身产生的最佳拟合线。我们的线<strong class="jb hz"> YP = 1.7x +1 </strong>最接近最佳拟合线。</p><p id="af32" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">如果我们在m/c值变化最小的情况下认真地进行迭代，我们将达到最佳拟合线。</p><p id="7d3e" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">在抛物线图中，我们可以确定误差0.28是最小值，它是由直线方程<strong class="jb hz"> YP = 1.7x +1 </strong>产生的。</p><p id="317e" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi kf translated"><span class="l kg kh ki bm kj kk kl km kn di"> M </span> <strong class="jb hz">矿比1自变量:</strong></p><p id="e972" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">在只有一个独立变量X的情况下，线性回归图将是二维的，最佳拟合将是直线。</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es ks"><img src="../Images/6087b4bc9a476a2efbafd331ef5a29e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*jhhM6KUU9Bi9LC_SrKvuVQ.png"/></div></figure><p id="7fbd" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">当有两个独立变量时，二维区域变成三维的表面/空间，最佳拟合变成平面。</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es kt"><img src="../Images/b14589a3c08b21fbdd8317fd5297295d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*9EwZL9v9OIrQeSh5PIhfUA.png"/></div></figure><p id="5a32" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">当存在n个变量(n&gt;2)时，则n维区域变成n维空间，最佳拟合变成超平面。实际上绘制n维图要复杂得多。</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es ku"><img src="../Images/2d6052d358911fd25be31e723ca3850a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*3cCPmyaQ30gzLuYDyVA9Ew.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">来源:<a class="ae hv" href="https://en.wikipedia.org/wiki/Arrangement_of_hyperplanes" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Arrangement_of_hyperplanes</a></figcaption></figure><p id="10b1" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">当线性回归为二维时，梯度下降误差图将形成抛物线。</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es kv"><img src="../Images/355a91df1379e9f949d1cd24e27d74d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*5GJHv1a3DkP_hD8rJqJXSA.png"/></div></figure><p id="79f4" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">当线性回归是三维时，梯度下降将变成椭圆抛物面。</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es kw"><img src="../Images/7719882492bfd8e39a7f5bc72c76fd76.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*ZCr7YBOylBntgAya680MCA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">来源:<a class="ae hv" href="https://en.wikipedia.org/wiki/Paraboloid" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Paraboloid</a></figcaption></figure><p id="c7df" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi kf translated">如何求最小成本？</p><p id="cc5d" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">我们发现所有可能的误差值形成一个抛物线(或三维抛物面),最大的最小值位于曲线的最深处。</p><p id="095c" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">步骤#1:为m &amp; c取一个随机值。对于m &amp; c的任何随机值，误差值不一定是最小的。假设它在抛物线的某处。</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es kv"><img src="../Images/61305aaf3ef98889bf2af11437863bd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*OJ5h1P33EogseqP6r4kzfA.png"/></div></figure><p id="e8e4" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">第二步:现在我们需要找到，斜率在哪个方向减少，这样我们就可以达到抛物线的深度。</p><p id="7ed9" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">这就是偏导数发挥作用的时候了。</p><p id="540e" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">我们可以比较从抛物线/抛物面的顶部下降，我们必须降低曲线的斜率才能到达底部。</p><p id="76d1" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">让我解释清楚。如果你在u取为随机值的点上画一条切线，它就是曲线在该点上的斜率。减小直线的斜率会使曲线下移。</p><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es ku"><img src="../Images/511fc7f91008229a91c3eb154c3c9c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*0FqOprtTSXPuByHNQcgBJQ.png"/></div></figure><p id="1cdb" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">我们减小的速率值就是减小直线的斜率。</p><p id="4631" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">基本上，为了降低斜率，我们可以使用一阶偏导数来获得降低的斜率。下面是使用偏导数的m &amp; c的更新规则。</p><p id="1fc6" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hz">更新规则:</strong></p><figure class="kb kc kd ke fd hk er es paragraph-image"><div class="er es kx"><img src="../Images/368915e8d0b0928f7b649fc59c5f2425.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*Kva_pd_ALi4plPLoQo5hTg.png"/></div></figure><p id="b6d5" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">步骤#3:我们需要使用更新规则改变m &amp; c值，并知道新的SSE值。重复这个过程，直到我们达到全局最小值。</p><p id="1a22" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi kf translated"><span class="l kg kh ki bm kj kk kl km kn di"> T </span> <strong class="jb hz">要记住的事情:</strong></p><p id="173f" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">1.m &amp; c应该同步更新。我们不应该先更新m，然后应用新m的值来更新c。</p><p id="9f0a" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">2.学习率通常为0.01，但不一定对所有模型都是0.01。最高的学习率将迅速降低m &amp; c值，并且它可能跳过全局最小值(一个有趣的词来提及抛物线中的最深点)。</p><p id="40ae" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">3.即使对于固定值的学习率，一旦你接近全局最小值，你的步长会自动变小。你不需要经常降低学习速度。</p><p id="e499" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi kf translated"><span class="l kg kh ki bm kj kk kl km kn di"> C </span> <strong class="jb hz">结束语:</strong></p><ol class=""><li id="8652" class="ky kz hy jb b jc jd jg jh jx la jy lb jz lc jw ld le lf lg bi translated">梯度下降是一种迭代方法，与最小二乘法不同，我们将为最佳拟合线的斜率m和截距c取一个随机值。</li><li id="5a5f" class="ky kz hy jb b jc lh jg li jx lj jy lk jz ll jw ld le lf lg bi translated">最终的SSE值(J或成本)可能不是最小值。</li><li id="398c" class="ky kz hy jb b jc lh jg li jx lj jy lk jz ll jw ld le lf lg bi translated">确定最负斜率的方向，并朝着那个方向前进。这就是偏导数发挥作用的地方。</li><li id="3757" class="ky kz hy jb b jc lh jg li jx lj jy lk jz ll jw ld le lf lg bi translated">然后，我们会以一个学习率迭代地这样做，直到我们得到所有误差的最小值。</li><li id="910b" class="ky kz hy jb b jc lh jg li jx lj jy lk jz ll jw ld le lf lg bi translated">对于最终最小SSE，使用的m &amp; c值是最终结果。具有最终m &amp; c的线集合是最佳拟合线。</li></ol><p id="452e" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">好吧！我们到达了文章的结尾。</p><p id="7fa1" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">是啊！我能听到你说“嘿！没有程序，你怎么能到达终点”。</p><p id="4a3e" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">在本文的下一部分，我们将研究误差指标(<a class="ae hv" rel="noopener" href="/@aasha01/linear-regression-part-iii-r-squared-45be92750ffb">线性回归—第三部分— R的平方</a>)，然后我们将使用python进行编程。</p><p id="d462" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">我将很快发布那篇文章并链接到这个页面。</p><p id="7474" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">到那时再见&amp;编程愉快！</p><p id="2397" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hz">感谢:</strong></p><p id="c5cc" class="pw-post-body-paragraph iy iz hy jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">coursera:<a class="ae hv" href="https://www.coursera.org/learn/machine-learning/home/welcome" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/learn/machine-learning/home/welcome</a></p></div></div>    
</body>
</html>