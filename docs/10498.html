<html>
<head>
<title>Mathematics Behind Logistic Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归背后的数学</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mathematics-behind-logistic-regression-bba91062fa78?source=collection_archive---------7-----------------------#2020-10-21">https://medium.com/analytics-vidhya/mathematics-behind-logistic-regression-bba91062fa78?source=collection_archive---------7-----------------------#2020-10-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e6119434022f523a162fffe547a69d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*idWgDYw0s1Rodg_i"/></div></div></figure><h1 id="fd0b" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">什么是逻辑回归？</h1><p id="2ff5" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">逻辑回归是一种简单的分类算法，其中输出或因变量为<a class="ae km" href="https://en.wikipedia.org/wiki/Categorical_variable" rel="noopener ugc nofollow" target="_blank">分类</a>。例如:</p><ul class=""><li id="c39c" class="kn ko hi jq b jr kp jv kq jz kr kd ks kh kt kl ku kv kw kx bi translated">将电子邮件分类为垃圾邮件或非垃圾邮件</li><li id="01f9" class="kn ko hi jq b jr ky jv kz jz la kd lb kh lc kl ku kv kw kx bi translated">来预测病人是否患有癌症</li></ul><p id="0cc3" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">逻辑回归为此使用逻辑函数，因此得名。这个算法可以被认为是一个回归问题，即使它做了分类。因为逻辑回归不只是给出类别，而是可以告诉我们一个数据点属于每个类别的概率。我们将在接下来的会议中了解细节。</p><h1 id="2944" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">几何直觉</h1><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/754b0c86068dbb631f1e19fc46edafd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/0*Pzp-mEB4F2uP0Ind"/></div></figure><p id="518a" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">我们都知道如何训练逻辑回归，以及如何使用训练好的模型进行预测。但是这个算法背后的核心数学概念是什么？是怎么训练出来的？让我们从几何角度来理解这个算法背后的数学。</p><p id="63e2" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">考虑一些数据在二维空间中的分布。数据有两个独立的属性X1和X2。这意味着每个数据点在这个二维空间中的位置取决于X1和X2的值。现在，我们可以看到有两种类型的数据点——红色和绿色。有一条线把这些点分开。所以我们可以说，这条线是一个模型，可以将这些数据点分类为红色或绿色。逻辑回归就是寻找最佳的直线或平面，将数据点分成不同的类别。因此，数据应该几乎是线性可分的，以便使用直线或平面进行分类。</p><p id="9155" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><em class="ll">给定一组点，我们如何找到这条线？给定这一行，我们如何获得对应于每个数据点的类，而不用可视化它？</em></p><p id="a4ea" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj">模型预测</strong></p><p id="ddc9" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">目前，让我们假设我们得到了如上图所示的分隔或分类一组数据点的完美线条。现在我们如何得到每个数据点对应的类呢？</p><p id="0eba" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">我们稍微修改一下上图。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/ae79e6e98d1286fd74e586032588d110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/0*odAgpLQ9Dl8zb3zo"/></div></figure><p id="95a8" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">我们把这条线的方程取为y=ax1+bx2+c其中c是截距。现在我们忽略c，等式变成y=ax1+bx2。</p><p id="77e5" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">在这个方程中(a，b)可以看作是y线的垂直向量。在上图中，我们认为正类点位于法线指向的线的一侧。现在给定(x1，x2)上的任意点x，该点到直线的距离为</p><p id="0fb6" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">d=(a，b) <strong class="jq hj">。</strong> (x1，x2) / (||(a，b)||)</p><p id="13fc" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj">其中(a，b)。</strong> (x1，x2)是数据点和法向量的点积，||(a，b)||是这个向量的第二范数或者我们可以说向量离原点的欧氏距离。因此，如果(x1，x2)在(a，b)的方向上，那么点积将是正的，因此d将是正的，因此(x1，x2)将是类1或正类。</p><p id="a9b0" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">考虑点x (x1，x2)。距离直线的距离d1在(a，b)方向上测量。因此，点积和d1将为正，输出为正类。类似地，对于点x’，到直线的距离是d2，这是负号。因此x '属于负类。</p><p id="66d6" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">现在，我们可以给出输出的概率解释，而不是仅仅给出类标签作为输出。为此，我们在这个带符号的距离上应用一个sigmoid函数。这个函数是一个“S”形函数，也称为逻辑函数。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ln"><img src="../Images/de40071df5af43129d95d502cb7a1c1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ek77Wm9vcqNDV026"/></div></div></figure><p id="1afd" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">为了得到概率输出，我们只取输出的sigmoid。sigmoid函数的输出可以被认为是点在类1或正类中的概率。从图中可以看出，对于x，如果d为正，sigmoid(d)大于0.5。或者输出属于正类或类1的概率大于0.5。类似地，如果d为负，则sigmoid(d)小于0.5，因此x属于类别1的概率较低。我们得到的概率是数据点在正类中的概率。</p><p id="602a" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">总之，我们使用逻辑函数并预测输出属于正类的概率。这就是为什么这种算法被称为回归，即使它是一种分类算法。</p><p id="be22" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><strong class="jq hj">寻找最佳分离器</strong></p><p id="dc3a" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><em class="ll">如何找到分隔类别的最佳直线或平面？</em></p><p id="9d73" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">让我们假设分隔类的线是y=ax1+bx2。这里的数据是二维的。如果数据是n维的，那么分类的平面就是y= a1x1 + a2x2 + a3x3 + …。+ anxn。为简单起见，我们考虑二维数据本身。</p><p id="6ec5" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">为了得到最佳线，我们需要找到最佳系数。或者换句话说，我们需要找到最佳直线的垂直向量。我们如何找到它？</p><p id="76ae" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">我们的目标是正确分类最多的数据点。考虑一个数据点x，如果x对应的实际输出y_true是1，那么我们的模型输出应该是1。如果我们使用概率分数(p)作为输出，那么输出必须接近1。如果y_true为0，那么模型预测的概率(p)应该接近0。</p><p id="ad38" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">换句话说，我们需要最大化类1的ytrue*p和类0的(1-ytrue)(1-p)。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/0bf7783ad60615171a5fb78d5c76d137.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*XsW_VLtN7_VWUTOGWuYFig.png"/></div></figure><p id="decd" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">使用sigmoid函数替换概率值，</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/4f56c72dddb5648a25252e304583a604.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*fpBghUuHa2peZ5Jk_aYWKw.png"/></div></figure><p id="8cac" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">其中d=(a，b)。(x1，x2)/(||a，b||)并且符号sigma是sigmoid运算。因此，我们可以获得最佳系数:</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/5765a72bf020a739c7b3401be0098cc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*dSSszABFu8jnUjJSFaXxPg.png"/></div></figure><p id="5317" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">最佳值是给出分数最大值的a和b的值。这就是为什么我们在这里使用argmax。我们可以将上面的等式写成:</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/0469e5e4c8cb52b54184d5c3b092c986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*mH6kNNs7qlGGkRo7F1A_cg.png"/></div></figure><p id="628b" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">所以我们的目标是找到可以最小化这个分数的系数。这是通过求解<a class="ae km" href="https://en.wikipedia.org/wiki/Gradient_descent#:~:text=Gradient%20descent%20is%20a%20first,function%20at%20the%20current%20point." rel="noopener ugc nofollow" target="_blank">梯度下降算法</a>完成的。</p><p id="fe0b" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">梯度下降算法的一个主要问题是优化更容易陷入<a class="ae km" href="http://mathonline.wikidot.com/local-maxima-and-minima-and-absolute-maxima-and-minima" rel="noopener ugc nofollow" target="_blank">局部最小值</a>。为了避免这个问题，我们引入一个单调函数到我们的得分函数中。这里使用的单调函数是对数函数。由于其<a class="ae km" href="https://en.wikipedia.org/wiki/Monotonic_function" rel="noopener ugc nofollow" target="_blank">单调</a>行为，该对数函数不会影响分数。修改后的等式变成</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/9143f42ed59158b97e92e2b1cc9c6985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1194/format:webp/1*V4iN4kkmLURsOB3WWq68bw.png"/></div></figure><p id="ac41" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">我们可以这样写，</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/8f62e143734d7e610c3eb18d68bd9786.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*2droJdDfuUbVtSfKqGmR1A.png"/></div></figure><p id="4905" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">其中p是属于1类或正类的概率。这是两类分类问题中的对数损失方程。</p><p id="c15e" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">现在，在获得最佳系数后，我们可以根据概率找到数据点的输出，如下所示:</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/b1c1cfc522ddf60cdf03d0bd9603f1d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*gESWF7ItrLzHdZgMMogpAQ.png"/></div></figure><p id="c852" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">其中(a*，b*) <strong class="jq hj">。</strong> (x1，x2)是从直线测量的x的有符号距离。现在，如果p的值大于0.5，我们可以将其分类为1类，否则为0类。</p><p id="f127" class="pw-post-body-paragraph jo jp hi jq b jr kp jt ju jv kq jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">我希望这篇文章能帮助你对逻辑回归背后的数学有一个基本的了解。如有任何疑问或建议，您可以通过<a class="ae km" href="https://www.linkedin.com/in/vinitha-v-n-5a0560179/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我。</p></div></div>    
</body>
</html>