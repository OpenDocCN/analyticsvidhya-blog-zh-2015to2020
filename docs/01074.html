<html>
<head>
<title>Gradient Descent — Intro and Implementation in python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降python中的介绍和实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-intro-and-implementation-in-python-8b6ab0557b7c?source=collection_archive---------3-----------------------#2019-09-29">https://medium.com/analytics-vidhya/gradient-descent-intro-and-implementation-in-python-8b6ab0557b7c?source=collection_archive---------3-----------------------#2019-09-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/956764d9170f4be9bd8c4e31471a30a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*FVjzT69K4YSXHDZBdjVZxg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">梯度下降在行动</figcaption></figure><h1 id="230d" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">介绍</h1><p id="47df" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">梯度下降是机器学习中的一种优化算法，用于通过迭代地向函数的最小值移动来最小化函数。</p><p id="dc05" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">当我们必须找到满足给定成本函数的最小可能值时，我们基本上使用这种算法。在机器学习中，更多的时候我们不是试图最小化<strong class="jq hj">损失函数</strong>(像<a class="ae kr" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj">均方误差</strong> </a>)。通过最小化损失函数，我们可以改进我们的模型，梯度下降是用于此目的的最流行的算法之一。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es ks"><img src="../Images/a983e9ce74a1b1efe4f5b39b34fbb790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y8nYa2Ij4ic_lPdD1c_dtw.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">成本函数的梯度下降</figcaption></figure><p id="9ecb" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">上图显示了梯度下降算法的工作原理。</p><p id="6c0a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们首先在成本函数中取一个点，并开始逐步向最小点移动。步长的大小，或者说我们必须多快收敛到最小点，是由<strong class="jq hj">学习速率定义的。我们可以以更高的学习率覆盖更多的区域，但有超过最小值的风险。另一方面，小步/更小的学习率会消耗大量时间到达最低点。</strong></p><p id="d2b8" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在，算法必须前进的方向(向最小值)也很重要。我们通过使用导数来计算。你必须熟悉微积分的导数。导数基本上是根据图形在任何特定点的斜率来计算的。我们通过在该点找到图形的切线来得到它。切线越陡，将意味着需要更多的步骤来达到最小点，越陡将意味着需要更少的步骤来达到最小点。</p><p id="7652" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们举个例子。</p><p id="8c7b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">成本函数<strong class="jq hj"> f(x) = x -4x +6 </strong></p><p id="6f78" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">f(x)的导数[x_derivative]，<strong class="jq hj"> f'(x) = 3x -8x </strong>(这将给出任意点x沿f(x)的斜率)</p><p id="e9ff" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">从x的任意值开始。假设0.5，learning_rate = 0.05</p><p id="9d75" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">迭代多次，继续计算x的值。</p><p id="495f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">x = x-(x _导数*学习率)</strong></p><p id="26b8" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">x = 0.5 - (-3.25*0.05) = 0.6625</p><p id="28fb" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在第二次迭代中使用x = 0.6625</p><p id="5a26" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">x = 0.6625+(3.983*0.05) = 0.86165</p><p id="fa82" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">依此类推，直到我们看不到x值的任何变化。</p><p id="d4ef" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">迭代次数可以是固定的，由用户给定。或者，如果你心中有一个精度(~0.001)。一旦达到这个精度值，就可以停止计算。</p><p id="581a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们将探索这两种方法。</p></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><h1 id="3b44" class="iq ir hi bd is it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn bi translated">Python实现</h1><p id="f498" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">我们将使用python实现一种简单形式的梯度下降。让我们取上一节中的多项式函数，将其视为成本函数，并尝试找到该函数的局部最小值。</p><p id="3885" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">成本函数<strong class="jq hj"> f(x) = x - 4x +6 </strong></p><p id="d40b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">让我们首先导入所需的库并创建f(x)。同样生成从-1到4的1000个值作为<strong class="jq hj"> x </strong>并绘制出<strong class="jq hj"> f(x) </strong>的曲线。</p><pre class="kt ku kv kw fd ln lo lp lq aw lr bi"><span id="4f38" class="ls ir hi lo b fi lt lu l lv lw"># Importing required libraries<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="182a" class="ls ir hi lo b fi lx lu l lv lw">f_x = lambda x: (x**3)-4*(x**2)+6<br/>x = np.linspace(-1,4,1000)</span><span id="24c2" class="ls ir hi lo b fi lx lu l lv lw">#Plot the curve<br/>plt.plot(x, f_x(x))<br/>plt.show()</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/ea12349e7f67d5fa58207e527b18a36f.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*VLNUZZEtnbzvwQ4BhNCEFQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">f(x) = x - 4x + 6</figcaption></figure><p id="e0a3" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">让我们找出f(x)的导数。</p><p id="5390" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">d f(x)/dx = 3x - 8x。让我们用python为导数创建一个lambda函数。</p><pre class="kt ku kv kw fd ln lo lp lq aw lr bi"><span id="7b5b" class="ls ir hi lo b fi lt lu l lv lw">f_x_derivative = lambda x: 3*(x**2)-8*x</span></pre><p id="b23a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">让我们创建一个绘制梯度下降的函数和一个计算梯度下降的函数，方法是将固定次数的迭代作为输入之一。</p><pre class="kt ku kv kw fd ln lo lp lq aw lr bi"><span id="2a86" class="ls ir hi lo b fi lt lu l lv lw">def plot_gradient(x, y, x_vis, y_vis):<br/>    plt.subplot(1,2,2)<br/>    plt.scatter(x_vis, y_vis, c = "b")<br/>    plt.plot(x, f_x(x), c = "r")<br/>    plt.title("Gradient Descent")<br/>    plt.show()</span><span id="60db" class="ls ir hi lo b fi lx lu l lv lw">plt.subplot(1,2,1)<br/>    plt.scatter(x_vis, y_vis, c = "b")<br/>    plt.plot(x,f_x(x), c = "r")<br/>    plt.xlim([2.0,3.0])<br/>    plt.title("Zoomed in Figure")<br/>    plt.show()<br/>    <br/>def gradient_iterations(x_start, iterations, learning_rate):<br/>    <br/>    # These x and y value lists will be used later for visualization.<br/>    x_grad = [x_start]<br/>    y_grad = [f_x(x_start)]<br/>    # Keep looping until number of iterations<br/>    for i in range(iterations):<br/>        <br/>        # Get the Slope value from the derivative function for x_start<br/>        # Since we need negative descent (towards minimum), we use '-' of derivative<br/>        x_start_derivative = - f_x_derivative(x_start)<br/>        <br/>        # calculate x_start by adding the previous value to <br/>        # the product of the derivative and the learning rate calculated above.<br/>        x_start += (learning_rate * x_start_derivative)        <br/>        <br/>        x_grad.append(x_start)<br/>        y_grad.append(f_x(x_start))</span><span id="f945" class="ls ir hi lo b fi lx lu l lv lw">print ("Local minimum occurs at: {:.2f}".format(x_start))<br/>    print ("Number of steps: ",len(x_grad)-1)<br/>    plot_gradient(x, f_x(x) ,x_grad, y_grad)</span></pre><p id="adf3" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在我们已经定义了这些函数，让我们通过传递<strong class="jq hj"> x_start = 0.5，迭代次数= 1000，学习速率= 0.05 </strong>来调用gradient_iterations函数</p><pre class="kt ku kv kw fd ln lo lp lq aw lr bi"><span id="0b96" class="ls ir hi lo b fi lt lu l lv lw">gradient_iteration(0.5, 1000, 0.05)</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/462e6c19d373c6e278cd8da6fecf0f87.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*4CmuYy8SosPFnOoMhh_acQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">渐变迭代(0.5，1000，0.05)</figcaption></figure><p id="cbab" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们能够在<strong class="jq hj"> 2.67 </strong>找到局部最小值，因为我们已经给定迭代次数为1000，算法已经进行了1000步。它可能在更早的迭代中达到值2.67。但是由于我们不知道在给定的学习速率下，我们的算法将在什么点达到局部最小值，所以我们给出一个高的迭代值，以确保我们找到我们的局部最小值。</p><p id="f9ea" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这听起来不是很理想，因为即使在找到局部最小值之后，循环迭代的次数也是不必要的。</p><p id="a64b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">让我们采用另一种方法，通过使用precision来固定迭代次数。</p><p id="f1e2" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在这种方法中，由于我们知道数据集，我们可以定义我们想要的精度级别，并在达到该精度级别时停止算法。</p><p id="298f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">对于这个例子，让我们写一个新的函数，它采用精度而不是迭代次数。</p><pre class="kt ku kv kw fd ln lo lp lq aw lr bi"><span id="e936" class="ls ir hi lo b fi lt lu l lv lw">def gradient_precision(x_start, precision, learning_rate):<br/>    <br/>    # These x and y value lists will be used later for visualisation.<br/>    x_grad = [x_start]<br/>    y_grad = [f_x(x_start)]</span><span id="7fcf" class="ls ir hi lo b fi lx lu l lv lw">while True:<br/>        <br/>        # Get the Slope value from the derivative function for x_start<br/>        # Since we need negative descent (towards minimum), we use '-' of derivative<br/>        x_start_derivative = - f_x_derivative(x_start)<br/>        <br/>        # calculate x_start by adding the previous value to <br/>        # the product of the derivative and the learning rate calculated above.<br/>        x_start += (learning_rate * x_start_derivative)<br/>        <br/>        <br/>        x_grad.append(x_start)        <br/>        y_grad.append(f_x(x_start))<br/>        # Break out of the loop as soon as we meet precision.<br/>        if abs(x_grad[len(x_grad)-1] - x_grad[len(x_grad)-2]) &lt;= precision:<br/>            break</span><span id="02ad" class="ls ir hi lo b fi lx lu l lv lw">print ("Local minimum occurs at: {:.2f}".format(x_start))<br/>    print ("Number of steps taken: ",len(x_grad)-1)<br/>    plot_gradient(x, f_x(x) ,x_grad, y_grad)</span></pre><p id="61ab" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在我们调用这个函数，参数<strong class="jq hj"> x_start = 0.5，精度= 0.001，学习率= 0.05 </strong></p><pre class="kt ku kv kw fd ln lo lp lq aw lr bi"><span id="a810" class="ls ir hi lo b fi lt lu l lv lw">gradient_precision(0.5, 0.001, 0.05)</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/39befd09e666c3d744554ec439e7e96c.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*h3ru7gHzgwMk10X8ZPI0pQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">渐变精度(0.5，0.001，0.05)</figcaption></figure><p id="1877" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">本地最小值= 2.67</p><p id="fd84" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">步骤数= 20</p><p id="a806" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们的梯度下降算法能够在仅仅20步内找到局部最小值！因此，在前面的方法中，我们不必要地运行了980次迭代！</p><p id="ed45" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">既然我们能够成功地使<strong class="jq hj"> f(x) </strong>最小化，即找到x的最小值，其中<strong class="jq hj"> f(x) </strong>最小，那么让我们研究学习率值，看看它如何影响算法输出。</p><h2 id="610f" class="ls ir hi bd is mb mc md iw me mf mg ja jz mh mi je kd mj mk ji kh ml mm jm mn bi translated">学习率= 0.01</h2><pre class="kt ku kv kw fd ln lo lp lq aw lr bi"><span id="b1a1" class="ls ir hi lo b fi lt lu l lv lw">gradient_precision(0.5, 0.001, 0.01)</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/87bf66c7d2aa1e30d2c3e8c4cade3950.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*MstUaWDAyixWY6y_d4SQdA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">x_start = 0.5，精度= 0.001，学习率= 0.01</figcaption></figure><p id="c350" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">因为学习率较低，这意味着达到局部最小值所需的步数较高(85)。正如我们在图中看到的，85 x值用蓝色标出，这意味着我们的算法在寻找局部最小值时速度较慢。</p><h2 id="7ebc" class="ls ir hi bd is mb mc md iw me mf mg ja jz mh mi je kd mj mk ji kh ml mm jm mn bi translated">学习率= 0.05</h2><pre class="kt ku kv kw fd ln lo lp lq aw lr bi"><span id="2c73" class="ls ir hi lo b fi lt lu l lv lw">gradient_precision(0.5, 0.001, 0.05)</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/b08eea9ec1eea05a4b29a8f4e7e6ba30.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*B_nlFYwtGbH8hSCvj2O7Dg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">x_start = 0.5，精度= 0.001，学习率= 0.05</figcaption></figure><p id="5dcb" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">对于相同的精度值和x_start值，但是学习率= 0.05，我们看到我们的算法能够在仅仅20个步骤中找到局部最小值。这表明通过增加学习速率，算法更快地达到局部最小值。</p><h2 id="096d" class="ls ir hi bd is mb mc md iw me mf mg ja jz mh mi je kd mj mk ji kh ml mm jm mn bi translated">学习率= 0.14</h2><pre class="kt ku kv kw fd ln lo lp lq aw lr bi"><span id="8d93" class="ls ir hi lo b fi lt lu l lv lw">gradient_precision(0.5, 0.001, 0.14)</span></pre><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/c9cbfd0e4414af38a087baba275d1309.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*YbCvaeIxHxrOpVpmDF5ncA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">x_start = 0.5，精度= 0.001，学习率= 0.14</figcaption></figure><p id="d66a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">通过将学习率增加到0.14，该算法能够在仅仅6个步骤中找到局部最小值。</p><p id="8201" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">停下来。不要陷入这样的陷阱:增加学习率总是会减少算法寻找局部最小值的迭代次数。我们就把学习率提高0.01看看结果吧。</p><figure class="kt ku kv kw fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/f4358a76a130d7b6a532ccc9e564f226.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/format:webp/1*NxmBA1k2QicsjceV46PlVA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">x_start = 0.5，精度= 0.001，学习率= 0.15</figcaption></figure><p id="07e4" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">哎呦！这次走的步数增加了！。看起来学习率= 0.14是精度= 0.001的最佳点。</p><p id="13ee" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">需要注意的一点是，这种实现将适用于成本函数只有一个变量<strong class="jq hj"> x </strong>的情况。如果有多个变量(x，y，z…)实现会改变，可能会在另一篇文章中发布。也有不同的<a class="ae kr" href="https://www.geeksforgeeks.org/gradient-descent-algorithm-and-its-variants/" rel="noopener ugc nofollow" target="_blank">类型的梯度下降</a></p><p id="f31c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">批量梯度下降<br/>随机梯度下降<br/>迷你批量梯度下降</strong></p><p id="7e31" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们将在以后的文章中深入了解这些不同类型的梯度下降。</p></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><p id="720e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这个帖子到此为止！。这个的jupyter笔记本在我的<a class="ae kr" href="https://gist.github.com/fahadanwar10/e8636d73c6ee9150cd108576f57e7c73" rel="noopener ugc nofollow" target="_blank"> github </a>里。</p><p id="30de" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">请查看我在<a class="ae kr" rel="noopener" href="/@fahadanwar10/introduction-to-linear-regression-e-commerce-dataset-cfa65b2c1213">线性回归介绍(电商数据集</a>上的帖子，秀一把爱。</p><div class="ms mt ez fb mu mv"><a rel="noopener follow" target="_blank" href="/@fahadanwar10/introduction-to-linear-regression-e-commerce-dataset-cfa65b2c1213"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hj fi z dy na ea eb nb ed ef hh bi translated">线性回归简介—电子商务数据集</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">在这篇文章中，我们将了解什么是线性回归，它背后的一点数学知识，并试图拟合一个…</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">medium.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj ik mv"/></div></div></a></div><p id="d79b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在你看到我的博客之前，不要忘记看看我的博客并订阅它来获取内容。<a class="ae kr" href="https://machinelearningmind.com/" rel="noopener ugc nofollow" target="_blank">https://machinelearningmind.com/</a></p></div></div>    
</body>
</html>