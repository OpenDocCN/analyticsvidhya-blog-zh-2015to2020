<html>
<head>
<title>FaceNet Architecture</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">FaceNet架构</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/facenet-architecture-part-1-a062d5d918a1?source=collection_archive---------1-----------------------#2019-04-04">https://medium.com/analytics-vidhya/facenet-architecture-part-1-a062d5d918a1?source=collection_archive---------1-----------------------#2019-04-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="69a5" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">第1部分:架构和在Google Colab上运行一个基本示例</h2></div></div><div class="ab cl ix iy gp iz" role="separator"><span class="ja bw bk jb jc jd"/><span class="ja bw bk jb jc jd"/><span class="ja bw bk jb jc"/></div><div class="hb hc hd he hf"><blockquote class="je jf jg"><p id="4ccd" class="jh ji jj jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">帮助弱者！打击网络犯罪<a class="ae ke" href="https://forms.gle/JWAPHzf2gd7jGq2YA" rel="noopener ugc nofollow" target="_blank">了解如何</a>。</p></blockquote></div><div class="ab cl ix iy gp iz" role="separator"><span class="ja bw bk jb jc jd"/><span class="ja bw bk jb jc jd"/><span class="ja bw bk jb jc"/></div><div class="hb hc hd he hf"><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es kf"><img src="../Images/7dcda6e2280653cbd18b8217dcdab6bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DNOChML8i4r-C2Bgfxa85g.png"/></div></div></figure><p id="03ef" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated"><em class="jj">本文理解来自</em> <a class="ae ke" href="https://arxiv.org/pdf/1503.03832.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="jj"> FaceNet </em> </a> <em class="jj">和</em><a class="ae ke" href="https://arxiv.org/pdf/1409.4842.pdf" rel="noopener ugc nofollow" target="_blank"><em class="jj">GoogleNet</em></a><em class="jj">论文。这是一个由两部分组成的系列，第一部分我们将介绍FaceNet架构以及在Google Colab上运行的示例，后面的部分将介绍移动版本。</em></p><p id="90ae" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">FaceNet是一个先进的人脸识别、验证和聚类神经网络。是22层深度神经网络，直接训练其输出为128维嵌入。在最后一层使用的损失函数被称为三重损失。</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es ku"><img src="../Images/3e23ff2790cd2b6b41ded4f5b98ec1f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZD-mw2aUQfFwCLS3cV2rGA.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图1:高级模态结构</figcaption></figure><p id="0dd4" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">FaceNet由上述构建模块组成，因此我们将依次介绍每一个模块。</p><p id="ba5f" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">图1中显示的深度网络来自GoogleNet架构(<em class="jj">它有很多版本，但是‘Inception-Resenet-v1’是我们将在我们的编码示例中使用的那个</em>)。FaceNet论文并没有太多地涉及GoogleNet架构的内部工作方式，它将深度神经网络视为一个黑盒，但我们将触及一些重要的概念，以了解它是如何使用的以及用于什么目的。</p><h1 id="c45b" class="kz la hi bd lb lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated">深度网络——谷歌网</h1><p id="e36a" class="pw-post-body-paragraph jh ji hi jk b jl lr ij jn jo ls im jq kr lt jt ju ks lu jx jy kt lv kb kc kd hb bi translated">GoogleNet是ImageNet 2014挑战赛的获胜者，该网络给出了一些突破性的结果，并对传统的卷积神经网络(CNN)进行了改进。下面列出了其中的一些功能:</p><ul class=""><li id="bea6" class="lw lx hi jk b jl jm jo jp kr ly ks lz kt ma kd mb mc md me bi translated">22层深度网络与8层AlexNet相比。</li><li id="5732" class="lw lx hi jk b jl mf jo mg kr mh ks mi kt mj kd mb mc md me bi translated">高效、更快计算能力。计算成本:比AlexNet少2倍。</li><li id="f466" class="lw lx hi jk b jl mf jo mg kr mh ks mi kt mj kd mb mc md me bi translated">与AlexNet相比要精确得多。</li><li id="b9c7" class="lw lx hi jk b jl mf jo mg kr mh ks mi kt mj kd mb mc md me bi translated">低内存使用和低功耗。</li><li id="c923" class="lw lx hi jk b jl mf jo mg kr mh ks mi kt mj kd mb mc md me bi translated">与AlexNet相比，网络更大，但参数数量更少。参数比AlexNet少12倍。</li><li id="ac42" class="lw lx hi jk b jl mf jo mg kr mh ks mi kt mj kd mb mc md me bi translated">MUL-ADD Ops预算被限制在15亿美元(在推断期间)，这样该架构可以用于现实世界的应用，特别是移动电话等便携式设备。</li></ul><blockquote class="je jf jg"><p id="7154" class="jh ji jj jk b jl jm ij jn jo jp im jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在传统的CNN中，<a class="ae ke" rel="noopener" href="/@tomdeore/deep-learning-in-gradient-descent-style-part-2-e159e2cf8a99">卷积</a>是用给定的滤波器在图像上进行的，以构建相关统计，逐层然后将这些高度相关的神经元聚类作为输出。需要注意的重要一点是，相关性是图像块的局部相关性，最高相关性存在于网络的早期层，因此大的滤波器尺寸和早期汇集将减少隐藏在图像块中的重要信息。</p></blockquote><p id="9fb5" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">这是谷歌网络架构背后的主要灵感，并被转化为所谓的网络中的网络，命名为“<strong class="jk hj"> <em class="jj">盗梦空间模块</em> </strong>”。</p><p id="3c79" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">传统的美国有线电视新闻网(CNN)很少有其他挑战被谷歌网很好地解决了，它们是:</p><h2 id="9784" class="mk la hi bd lb ml mm mn lf mo mp mq lj kr mr ms ll ks mt mu ln kt mv mw lp mx bi translated">关注:</h2><ol class=""><li id="cf05" class="lw lx hi jk b jl lr jo ls kr my ks mz kt na kd nb mc md me bi translated">网络中的层数越多越好，但缺点是，它也增加了参数的数量，并可能导致过拟合。</li><li id="1f54" class="lw lx hi jk b jl mf jo mg kr mh ks mi kt mj kd nb mc md me bi translated">深层网络也存在梯度消失问题，因为在反向传播过程中，梯度直到初始层才能到达网络，导致权重不变，这是不希望的。</li><li id="ab1a" class="lw lx hi jk b jl mf jo mg kr mh ks mi kt mj kd nb mc md me bi translated">滤波器的线性增加导致运算的二次增加，由此需要更多的计算能力。</li><li id="5204" class="lw lx hi jk b jl mf jo mg kr mh ks mi kt mj kd nb mc md me bi translated">更多参数，将需要更多的数据集和更长的训练时间，即使数据扩充也没有多大帮助。通常，表面数据生成并不是一个合适的解决方案。</li><li id="e6e4" class="lw lx hi jk b jl mf jo mg kr mh ks mi kt mj kd nb mc md me bi translated"><strong class="jk hj">减少代表权瓶颈</strong>。这可以理解为<em class="jj">降维与信息提取</em>之间的权衡。在卷积中，当我们深入网络时，输入的维数减少，信息衰减发生，因此信息提取应该对每一层都有效，特别是关于局部区域。</li></ol><h2 id="5b95" class="mk la hi bd lb ml mm mn lf mo mp mq lj kr mr ms ll ks mt mu ln kt mv mw lp mx bi translated">解决方案:</h2><ol class=""><li id="39a3" class="lw lx hi jk b jl lr jo ls kr my ks mz kt na kd nb mc md me bi translated">GoogleNet使用1x1过滤器进行降维。1x1卷积背后的想法是保持输入大小(高度和宽度)不变，但收缩通道。示例:将256x256x3 RGB图像转换为256x256x1图像。</li><li id="17d4" class="lw lx hi jk b jl mf jo mg kr mh ks mi kt mj kd nb mc md me bi translated">除了1x1之外，还使用其他更小但在空间上分散的滤波器，如3x3、5x5和7x7。由于max-polling成功地对图像进行了下采样，因此滤波器被并行应用，并且最终所有中间输出被连接用于下一阶段。这使得inception模块在中间更宽，但背靠背连接许多这样的模块，使它更深。从视觉上看，基本构建模块“初始模块”如下所示:</li></ol><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es nc"><img src="../Images/99b0a12cd893fe8b694ed7dfcfb5f915.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZXKsEUIwNYY-nWDiQZxoVQ.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图2:降维后的初始模块</figcaption></figure><p id="9f50" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">3.考虑到网络的深度，在反向传播期间必然会出现消失梯度问题，因此在中间层分接两个辅助输出，并在将其添加到总损耗之前进行加权平均，即:</p><pre class="kg kh ki kj fd nd ne nf ng aw nh bi"><span id="03af" class="mk la hi ne b fi ni nj l nk nl">total_loss = final_loss + (1/3 * aux1_loss) + (1/3 * aux2_loss)</span></pre><p id="3fce" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">自Inception-v1以来，模块已经经历了各种改进，如下所述，简而言之:</p><p id="f189" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated"><strong class="jk hj">盗梦空间-v2和盗梦空间-v3 ( </strong> <a class="ae ke" href="https://arxiv.org/pdf/1512.00567v3.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jk hj">纸</strong> </a> <strong class="jk hj"> ) </strong></p><p id="4949" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">这个版本有<strong class="jk hj">因子分解</strong>，它减少了参数并减少了过拟合问题，引入了<strong class="jk hj">批处理规范化</strong>，以及<strong class="jk hj">标签平滑</strong>，它防止特定的logit与其他logit相比变得太大，因此在分类器层应用了正则化。</p><p id="f13c" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated"><strong class="jk hj"> Inception-v4和Inception-ResNet-v1(</strong><a class="ae ke" href="https://arxiv.org/pdf/1602.07261.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jk hj">论文</strong> </a> <strong class="jk hj"> ) </strong></p><p id="b8e3" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">这个版本简化了网络的主干(这是连接到第一个初始模块的网络的前导)。初始块和以前一样，只是它们被命名为A、B、c。对于ResNet版本，引入了剩余连接，取代了初始模块中的池。</p><p id="9128" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">在<a class="ae ke" href="https://github.com/davidsandberg" rel="noopener ugc nofollow" target="_blank">大卫·桑德伯格</a>的FaceNet <a class="ae ke" href="https://github.com/davidsandberg/facenet" rel="noopener ugc nofollow" target="_blank">实现</a>中，使用的是’<a class="ae ke" href="https://github.com/davidsandberg/facenet/blob/master/src/models/inception_resnet_v1.py" rel="noopener ugc nofollow" target="_blank">Inception-ResNet-v1</a>版本。</p><p id="e074" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">在FaceNet训练期间，深度网络提取并学习各种面部特征，然后将这些特征直接转换为128D嵌入，其中相同的面部应该彼此靠近，不同的面部应该在嵌入空间中远离(<em class="jj">嵌入空间</em>无非是<em class="jj">特征空间</em>)。这只是给你一个直觉，但在实现上，这是通过一个称为三重损失的损失函数来实现的。</p><h1 id="239f" class="kz la hi bd lb lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated">价值函数</h1><p id="51eb" class="pw-post-body-paragraph jh ji hi jk b jl lr ij jn jo ls im jq kr lt jt ju ks lu jx jy kt lv kb kc kd hb bi translated">FaceNet的非常具体的特性是它的损失函数。三元组损失是用于面部验证的函数的名称，但是David的FaceNet实现具有两个损失函数'<strong class="jk hj">三元组损失</strong>以及'<strong class="jk hj"> Softmax激活与交叉熵损失</strong>'。三元成本函数看起来像:</p><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es nm"><img src="../Images/7f20adfc1db6b0482c6fd3c59c3e28df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iW_U25w1EPsPpLZFrpF4pA.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图4:成本函数</figcaption></figure><p id="7778" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated"><strong class="jk hj">三重损失:</strong>假设，<em class="jj"> f(x) </em>在<em class="jj"> d </em>维空间中为图像<em class="jj"> x </em>创建嵌入。示例图像有:</p><ul class=""><li id="4c83" class="lw lx hi jk b jl jm jo jp kr ly ks lz kt ma kd mb mc md me bi translated"><strong class="jk hj">主播</strong>:我们要对比的埃隆面具的图像，</li><li id="110f" class="lw lx hi jk b jl mf jo mg kr mh ks mi kt mj kd mb mc md me bi translated"><strong class="jk hj">正</strong>:埃隆面具的另一个图像，正例，</li><li id="5fa7" class="lw lx hi jk b jl mf jo mg kr mh ks mi kt mj kd mb mc md me bi translated">反面人物:约翰·特拉沃尔塔的形象，反面例子。</li></ul><figure class="kg kh ki kj fd kk er es paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="er es nn"><img src="../Images/4256f7e21dd43bbd332c529f5496cf15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ciqZG8KPcqyvFtQwINkew.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图3:分组的三幅图像。</figcaption></figure><p id="9257" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">理论上，在欧几里德空间中，锚图像应该更靠近正图像而远离负图像。这可以计算为:</p><pre class="kg kh ki kj fd nd ne nf ng aw nh bi"><span id="dee5" class="mk la hi ne b fi ni nj l nk nl"><strong class="ne hj">   dist(A,P)</strong>               <strong class="ne hj">dist(A,N)</strong></span><span id="96c3" class="mk la hi ne b fi no nj l nk nl">||f(A) - f(P)||² + α &lt;= ||f(A) - f(N)||²</span><span id="eca9" class="mk la hi ne b fi no nj l nk nl">similarly, </span><span id="5b4d" class="mk la hi ne b fi no nj l nk nl">||f(A) - f(P)||² + α - ||f(A) - f(N)||² &lt;= 0                ... (1)</span></pre><p id="eda7" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">这里，</p><p id="69a6" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated"><code class="du np nq nr ne b">||f(A) — f(P)||</code>是锚和正之间的距离，</p><p id="bc09" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated"><code class="du np nq nr ne b">||f(A) — f(N)||</code>是锚和负之间的距离。</p><p id="3e55" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">为了让积极的一组远离消极的一组，一个边缘<strong class="jk hj"> α </strong>被加到积极的一组，这样我们把积极的一组推得更远。</p><p id="0118" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">损失函数(1)可以是<strong class="jk hj">零</strong>，在这种情况下，等式看起来如下(因为我们不需要低于零的值):</p><pre class="kg kh ki kj fd nd ne nf ng aw nh bi"><span id="1667" class="mk la hi ne b fi ni nj l nk nl">L(A,P,N) = max(||f(A) - f(P)||² + α - ||f(A) - f(N)||², 0)   ... (2)</span></pre><p id="5062" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated"><strong class="jk hj">三元组选择:</strong>想到的明显问题是我们如何选择<code class="du np nq nr ne b">f(A,P)</code>和<code class="du np nq nr ne b">f(A,N)</code>对，因为如果我们随机选择它们，上面的等式(2)将很容易满足，但是我们的网络不会从中学习很多，此外，寻找局部最小值也是不正确的，并且梯度下降可能收敛到错误的权重。</p><p id="2959" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">论文指出，使用非常硬的<strong class="jk hj">示例会导致收敛在一开始就发生，并可能导致模型崩溃。<strong class="jk hj">半硬</strong>示例是首选选项。这可以通过使用合理的小批量来实现，本文作者在小批量中使用了40个面。</strong></p><p id="3ae2" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">因此，我们必须将'<strong class="jk hj">半硬</strong>'示例配对，并将其呈现给网络。使得:</p><pre class="kg kh ki kj fd nd ne nf ng aw nh bi"><span id="62e2" class="mk la hi ne b fi ni nj l nk nl">                           d(A,P) ≈ d(A,N)</span></pre><p id="9639" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">因为<strong class="jk hj"> α </strong>缘会一直把他们拒之门外，即使他们彼此很近。</p><p id="0642" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated">FaceNet论文提出了两种方法:</p><ol class=""><li id="0e56" class="lw lx hi jk b jl jm jo jp kr ly ks lz kt ma kd nb mc md me bi translated">在每一个<code class="du np nq nr ne b">n</code>训练步骤中离线:计算最新检查点的<code class="du np nq nr ne b">argmin</code>和<code class="du np nq nr ne b">argmax</code>，并将其应用于数据子集。</li><li id="2482" class="lw lx hi jk b jl mf jo mg kr mh ks mi kt mj kd nb mc md me bi translated">在线:选择一个大的小批量，并在该批量中计算<code class="du np nq nr ne b">argmin</code>和<code class="du np nq nr ne b">argmax</code>。</li></ol><p id="602a" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated"><strong class="jk hj">注意</strong> :-带有三重丢失的训练可能会很麻烦，因此David的FaceNet实现建议使用“带有交叉熵丢失的Softmax”，该理论来自<a class="ae ke" href="http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><h1 id="017d" class="kz la hi bd lb lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated">SVM训练—推理</h1><p id="dd06" class="pw-post-body-paragraph jh ji hi jk b jl lr ij jn jo ls im jq kr lt jt ju ks lu jx jy kt lv kb kc kd hb bi translated">然后，这些嵌入被用来寻找欧几里德距离以匹配或验证照片。SVM是最适合用于分类的机器学习算法，该算法根据这些生成的嵌入进行训练，并且稍后可以针对测试数据进行推断。</p><h1 id="aeeb" class="kz la hi bd lb lc ld le lf lg lh li lj io lk ip ll ir lm is ln iu lo iv lp lq bi translated">示例代码</h1><p id="e04e" class="pw-post-body-paragraph jh ji hi jk b jl lr ij jn jo ls im jq kr lt jt ju ks lu jx jy kt lv kb kc kd hb bi translated">代码可以在这里找到<a class="ae ke" href="https://gist.github.com/milinddeore/0e36ad7dda227f708bef0beb4fe595da" rel="noopener ugc nofollow" target="_blank">。最好是你能在Google Colab上打开并运行它。</a></p><p id="18b3" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated"><em class="jj">在接下来的</em> <a class="ae ke" rel="noopener" href="/@tomdeore/facenet-on-mobile-cb6aebe38505"> <em class="jj">第二部分</em> </a> <em class="jj">中，我们将介绍在移动设备上使用FaceNet的实际例子，我们还将了解什么是</em> <code class="du np nq nr ne b"><em class="jj">.tflite</em></code> <em class="jj">模型，以及为什么它需要在移动设备上使用。</em></p><p id="d2ac" class="pw-post-body-paragraph jh ji hi jk b jl jm ij jn jo jp im jq kr js jt ju ks jw jx jy kt ka kb kc kd hb bi translated"><strong class="jk hj">你可以在|</strong><a class="ae ke" href="https://www.linkedin.com/in/mdeore/" rel="noopener ugc nofollow" target="_blank"><strong class="jk hj">LinkedIn</strong></a><strong class="jk hj">|</strong><a class="ae ke" href="https://tomdeore.wixsite.com/epoch" rel="noopener ugc nofollow" target="_blank"><strong class="jk hj">网站</strong></a><strong class="jk hj">|</strong><a class="ae ke" href="https://github.com/milinddeore" rel="noopener ugc nofollow" target="_blank"><strong class="jk hj">Github</strong></a><strong class="jk hj">|</strong></p></div></div>    
</body>
</html>