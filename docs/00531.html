<html>
<head>
<title>Understanding Sentence Embeddings using Facebook’s Infersent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用脸书推理理解句子嵌入</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sentence-embeddings-facebooks-infersent-6ac4a9fc2001?source=collection_archive---------2-----------------------#2019-07-23">https://medium.com/analytics-vidhya/sentence-embeddings-facebooks-infersent-6ac4a9fc2001?source=collection_archive---------2-----------------------#2019-07-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/0c79f0d322888fd43e6916a735a054b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FMENbu8BcCALAA9Oz8J9LA.png"/></div></div></figure><div class=""/><p id="8ee5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">自从Word2Vec(以及其他单词向量模型)及其丰富的单词表示出现以来，自然语言处理模型在一些任务上表现出色，如情感分析、语言模型、机器翻译等。这篇文章谈到了一个叫做<a class="ae jo" href="https://en.wikipedia.org/wiki/Sentence_embedding" rel="noopener ugc nofollow" target="_blank"> <strong class="is hu">句子嵌入</strong> </a> <strong class="is hu"> </strong>的类似概念，它在几个自然语言处理任务中导致了显著的性能提升，它归结为使用整个句子而不仅仅是单词来编码。首先，让我们快速粗略地看一下单词嵌入。</p><h1 id="8b2a" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">单词嵌入</h1><p id="2130" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">一个集合术语，指学习将词汇表中的一组单词映射到数值向量的模型。单词嵌入的核心概念是，语言中使用的每个单词都可以用一组实数(一个向量)来表示。单词嵌入是N维向量，试图在其值中捕捉词义和上下文。任何一组数字都是有效的单词向量，但是为了有用，词汇表的一组单词向量应该捕获单词的含义、单词之间的关系以及不同单词在自然使用时的上下文。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ks"><img src="../Images/caa3a4b3b191cdbd8a0e947561b1319f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gtIEU7Z6XqUbRToz.png"/></div></div></figure><p id="2f26" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这样做的副产品非常有趣。一旦词汇表中的所有单词都被映射到向量，对于每个单词，嵌入就捕获了该单词的“含义”。相似的单词最终具有相似的嵌入值，因此，它们最终在潜在空间中彼此接近。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/503dee3e4cb11fe7013ce141e2d4cb8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uHE4REieW_kHuKPJ.png"/></div></div></figure><h1 id="ceae" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">句子嵌入</h1><p id="3ed7" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">句子嵌入是一个类似的概念。它将一个完整的句子嵌入到一个向量空间中。这些句子嵌入保留了一些好的特性，因为它们继承了潜在单词嵌入的特性。</p><p id="f274" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最近有一个巨大的趋势是寻求<a class="ae jo" href="https://research.google.com/pubs/archive/46808.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="is hu">通用嵌入</strong> </a> <strong class="is hu"> : </strong>嵌入，这些嵌入在大型语料库上预先训练，可以插入各种下游任务模型(情感分析、分类、翻译……)中，通过合并在更大数据集上学习的一些通用单词/句子表示来自动提高它们的性能。</p><p id="2111" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然无监督的句子表示学习已经成为常态有一段时间了，但去年已经出现了向监督和多任务学习计划的转变，在2017年底/2018年初提出了许多非常有趣的建议。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/58a0726e1ada86921a4893f4fe957d0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JFoxPQZ_NIsjoND9.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><strong class="bd jr">通用单词/句子嵌入的最新趋势。</strong>在本帖中，我们描述了用黑色表示的型号。所有标明型号的参考文件都列在文章末尾。</figcaption></figure><p id="6c77" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然有许多学习句子嵌入的竞争方法，但在这篇文章中，我们主要关注Infersent，这是一种由脸书设计的提供语义句子表示的句子嵌入方法。</p></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><h1 id="3ac5" class="jp jq ht bd jr js li ju jv jw lj jy jz ka lk kc kd ke ll kg kh ki lm kk kl km bi translated"><strong class="ak">推断</strong></h1><p id="93cb" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated"><a class="ae jo" href="https://research.fb.com/downloads/infersent/" rel="noopener ugc nofollow" target="_blank">脸书研究院</a>的人在2018年7月发布了一篇论文，谈到了“<a class="ae jo" href="https://arxiv.org/abs/1705.02364v5" rel="noopener ugc nofollow" target="_blank">从自然语言推理数据中监督学习通用句子表示</a>”，它采用了一种监督方法来学习句子嵌入。</p><p id="3a62" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">他们讨论了在NLI任务中学习的模型比在无监督条件下或在其他监督任务中训练的模型表现更好。通过探索各种架构，他们表明，具有max-pooling的BiLSTM网络是当前最好的通用句子编码方法，优于现有的方法，如<a class="ae jo" href="https://arxiv.org/abs/1506.06726" rel="noopener ugc nofollow" target="_blank"> SkipThought </a> vectors。下面详细讨论一下Infersent是如何工作的。</p><h1 id="1a0b" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">挖苦NLI的任务</strong></h1><p id="62d0" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated"><strong class="is hu">自然语言推理</strong>，也称为<strong class="is hu">识别文本蕴涵</strong>的目的是找到文本片段之间的方向关系。在这个框架中，包含和<strong class="is hu">的<strong class="is hu">和包含</strong>的<strong class="is hu">文本(t) </strong>和<strong class="is hu">假设(h) </strong>分别被称为。并且，<strong class="is hu">“t需要h”(t =&gt;h)</strong>如果，典型地，一个阅读<strong class="is hu"> <em class="ln"> t </em> </strong>的人会推断<strong class="is hu"> <em class="ln"> h </em> </strong>最有可能是真的。</strong></p><figure class="kt ku kv kw fd hk er es paragraph-image"><div class="er es lo"><img src="../Images/0b4f09f5055a64d91aa1ac401f6f08a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/0*gRQZgVA85lBoMDcX"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><a class="ae jo" href="https://nlp.stanford.edu/projects/snli/" rel="noopener ugc nofollow" target="_blank"><strong class="bd jr"><em class="lp">SNLI</em></strong></a></figcaption></figure><p id="d73f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于这个任务，使用的数据集是<a class="ae jo" href="https://nlp.stanford.edu/projects/snli/" rel="noopener ugc nofollow" target="_blank"> <strong class="is hu"> SNLI(斯坦福自然语言推理)数据集</strong> </a>。它由570，000个人工生成的英语句子对组成，人工标记有三个类别之一— <strong class="is hu">蕴涵、矛盾</strong>和<strong class="is hu">中性</strong>。</p><h1 id="3aa4" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">Infersent的工作原理:</h1><p id="b558" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">该架构由两部分组成:</p><ol class=""><li id="f803" class="lq lr ht is b it iu ix iy jb ls jf lt jj lu jn lv lw lx ly bi translated">一个是句子编码器，它获取单词向量并将句子编码成向量</li><li id="df11" class="lq lr ht is b it lz ix ma jb mb jf mc jj md jn lv lw lx ly bi translated">二是NLI分类器，其接收编码向量并输出<strong class="is hu">蕴涵、矛盾</strong>和<strong class="is hu">中性</strong>中的类别。</li></ol><figure class="kt ku kv kw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es me"><img src="../Images/6036b1e50a1778b68b31a7e101f38264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wbuFlMRo_NTqg8w52M8THw.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">逆流</figcaption></figure><h1 id="c114" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">句子编码器</h1><p id="ca3a" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">本文讨论了句子编码的多种体系结构。让我们从论文中陈述的最好的一个开始看它们。</p><p id="b14c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">具有最大/平均池的BiLSTM</strong></p><p id="ea76" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是一个双向LSTM网络，它为n个单词计算n个向量，每个向量是一个正向LSTM和一个反向LSTM的串联输出，它们以相反的方向阅读句子。然后，将最大/平均池应用于每个连接的向量，以形成固定长度的最终向量。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div class="er es mf"><img src="../Images/11cd33e3b353efe9c20d8b7214bcd3ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/0*Vbqvd9eCXpDz8yWO"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">具有最大池的BiLSTM</figcaption></figure><p id="2978" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> LSTM和GRU </strong></p><p id="96e6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些是普通版本，其中网络输出n个单词序列的n个隐藏向量，最后一个隐藏向量被认为是最终的固定长度向量。</p><p id="29c6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它的另一个变体是BiGRU-last，其中使用两个GRU，一个用于前向，另一个用于后向，并且两个GRU的最后隐藏状态被连接用于最终向量。</p><p id="6402" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">自我关注网</strong></p><p id="073e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这使用对BiLSTM的隐藏状态的注意机制来生成输入句子的表示u。</p><p id="c987" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意机制的本质是它们是对人类视觉机制的模仿。当人类视觉机制检测到一个项目时，它通常不会端到端地扫描整个场景；更确切地说，它总是根据人的需要，专注于特定的部分。当一个人注意到他们想要关注的物体通常出现在场景的特定部分时，他们将会知道在未来，该物体将会看向该部分，并且倾向于将他们的注意力集中在该区域。</p><p id="0a81" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在论文中，他们使用了一个具有输入句子多视图的自我注意网络，以便该模型可以学习句子的哪个部分对于给定的任务来说是重要的。具体地，它们具有4个上下文向量u1、u2、u3、u4，这些向量生成4个表示，然后这些表示被连接以获得单句表示。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div class="er es mg"><img src="../Images/8ecb994a75ab70ee64262719b0067809.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/0*fst9ZWGPIz4FDR_k"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">自我注意网络</figcaption></figure><p id="989c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">分级通信网</strong></p><p id="60bc" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">目前在分类任务上表现最好的模型之一是称为<em class="ln"> AdaSent、</em>的卷积架构，它在不同的抽象级别上连接句子的不同表示。本文介绍的分层卷积网络受此启发，由4个卷积层组成。在每一层，最大化特征地图池以获得表示。最终的句子嵌入由4个最大汇集表示的串联来表示。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div class="er es mh"><img src="../Images/68020a649d2e6514f79b9d25259f3d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/0*J_4Tz3jh5AaU8Jp-"/></div></figure><h1 id="023f" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">自然语言推理分类器</h1><p id="872e" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">本节讨论推理分类器网络，它采用这些句子嵌入并预测输出标签。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/edc0650cac2a12633b0c1dff34d9508c.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/0*oih3Zo16glkwPLm4"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated">NLI分类器</figcaption></figure><p id="6d70" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在将句子向量作为该模型的输入之后，应用3种匹配方法来提取文本、<strong class="is hu"> u </strong>和假设、<strong class="is hu"> v </strong>之间的关系</p><ul class=""><li id="9cba" class="lq lr ht is b it iu ix iy jb ls jf lt jj lu jn mj lw lx ly bi translated">两个表示的串联<strong class="is hu"> (u，v) </strong></li><li id="68ad" class="lq lr ht is b it lz ix ma jb mb jf mc jj md jn mj lw lx ly bi translated">基于元素的乘积<strong class="is hu"> u * v </strong></li><li id="ffa5" class="lq lr ht is b it lz ix ma jb mb jf mc jj md jn mj lw lx ly bi translated">并且，绝对元素差异<strong class="is hu">| u-v |</strong></li></ul><p id="e372" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">产生的向量从文本u和假设v中捕获信息，并被馈送到3类分类器(<strong class="is hu">蕴涵、矛盾</strong>和<strong class="is hu">中性</strong>)中，该分类器由多个完全连接的层和随后的softmax层组成。</p><p id="1d58" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">希望这能让你清楚地了解Infersent是如何工作的。我策划了一个单独的<a class="ae jo" href="https://github.com/gotorehanahmad/Natural-Language-Processing/tree/master/NLI" rel="noopener ugc nofollow" target="_blank">jupyter notebook</a>infer sent的实现以及如何使用它。</p><h1 id="e61f" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">先决条件</h1><ol class=""><li id="4b4e" class="lq lr ht is b it kn ix ko jb mk jf ml jj mm jn lv lw lx ly bi translated">首先，下载最先进的快速文本嵌入</li></ol><pre class="kt ku kv kw fd mn mo mp mq aw mr bi"><span id="3834" class="ms jq ht mo b fi mt mu l mv mw">#### to download Glove<br/>curl -Lo glove.840B.300d.zip <a class="ae jo" href="http://nlp.stanford.edu/data/glove.840B.300d.zip" rel="noopener ugc nofollow" target="_blank">http://nlp.stanford.edu/data/glove.840B.300d.zip</a></span><span id="2b82" class="ms jq ht mo b fi mx mu l mv mw"><br/>#### To download Fasttext <br/>curl -Lo crawl-300d-2M.vec.zip https://s3-us-west-1.amazonaws.com/fasttext-vectors/crawl-300d-2M.vec.zip</span></pre><p id="fcd1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.要获得推断模型并重现我们的结果，请下载预训练模型。</p><pre class="kt ku kv kw fd mn mo mp mq aw mr bi"><span id="dd6d" class="ms jq ht mo b fi mt mu l mv mw">#### Glove based model<br/>curl -Lo examples/infersent1.pkl https://dl.fbaipublicfiles.com/senteval/infersent/infersent1.pkl</span><span id="335a" class="ms jq ht mo b fi mx mu l mv mw">#### Fasttext based model<br/>curl -Lo examples/infersent2.pkl https://dl.fbaipublicfiles.com/senteval/infersent/infersent2.pkl</span></pre><p id="c295" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3.确保您有NLTK和Punkt tokenizer。</p><pre class="kt ku kv kw fd mn mo mp mq aw mr bi"><span id="5d66" class="ms jq ht mo b fi mt mu l mv mw">import nltk<br/>nltk.download('punkt')</span></pre><p id="a7aa" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.您可以使用sample.txt文件输入句子，也可以用自己的句子替换。</p><p id="3d9b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5.推断还可以显示单词在句子中的重要性，这可以用来构建意图提取器。从笔记本中找到如何使用它。</p><figure class="kt ku kv kw fd hk er es paragraph-image"><div class="er es my"><img src="../Images/29cb4038ee47a04853939ff48161b54b.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*SpyYiUlcta08ZFbd-aS_DQ.png"/></div></figure></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><blockquote class="mz na nb"><p id="8e7a" class="iq ir ln is b it iu iv iw ix iy iz ja nc jc jd je nd jg jh ji ne jk jl jm jn hb bi translated"><em class="ht">关于我</em></p></blockquote><p id="1206" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我是<a class="ae jo" href="https://wavelabs.ai/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Wavelabs.ai </a>的资深AI专家。我们Wavelabs帮助您利用人工智能(AI)来彻底改变用户体验并降低成本。我们使用人工智能独特地增强您的产品，以达到您的全部市场潜力。我们试图将尖端研究引入您的应用中。</p><p id="457a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">欢迎访问<a class="ae jo" href="https://wavelabs.ai/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Wavelabs.ai </a>了解更多信息。</p></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><p id="2896" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">嗯，这些都在这个博客里。感谢阅读:)</p><p id="c051" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">保持好奇！</p><p id="a8ef" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以在<a class="ae jo" href="https://www.linkedin.com/in/rehan-a-18675296?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我。</p></div></div>    
</body>
</html>