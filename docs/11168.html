<html>
<head>
<title>From Spiders to R and back…</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从蜘蛛到R再回来…</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/from-spiders-to-r-and-back-8b2e90a94473?source=collection_archive---------24-----------------------#2020-11-20">https://medium.com/analytics-vidhya/from-spiders-to-r-and-back-8b2e90a94473?source=collection_archive---------24-----------------------#2020-11-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="ac67" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">一个面向社会科学家(喜欢计算机，但不知道如何编码)的文本挖掘项目</h2></div><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="ab fe cl jc"><img src="../Images/3f08ea72263f8b0eff5b4501fdd73e50.png" data-original-src="https://miro.medium.com/v2/format:webp/1*zinv5pvKpNnz5syoXAbHBA.jpeg"/></div></figure><p id="aba1" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">对于那些对这些新的计算方法和机器学习为这门学科提供了什么感到好奇的人，这里有一个品尝者！在这篇文章的整个过程中，我将带你经历如何开始(并希望完成)一个计算社会科学项目(以及另外2个)。</p><p id="0d36" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">当然，我将带你进入一个非常特殊的研究领域:英国的极左团体，他们是委内瑞拉马杜罗政府的支持者。的确是小众。为了了解委内瑞拉目前的情况(也是为了让你自己得出结论)，我建议你看看BBC iPlayer上的纪录片。我不会对这件事发表自己的看法——只能说它离家太近，讨论可能需要几个小时(不过，如果你好奇，你肯定可以问我，因为我已经写了4年了！)</p><p id="c700" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在这个分析中，我将关注英国这些群体的在线输出:具体来说，我将关注他们更广泛的<em class="kc">书面</em>输出——因此为什么文本挖掘和文本分析方法是合适的。在本教程中，我将只关注英国的一个特殊团体，即“不干涉委内瑞拉”运动(HOV)。(如果你想知道的话，实际上在英国有几个<em class="kc">马杜罗支持者团体。很奇怪，我知道。)我也将只关注他们的<em class="kc">博客</em>输出，而不是他们的脸书，甚至是他们的Twitter输出，我将把它们留给另一个项目。</em></p><p id="b9c4" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因此，我们的首要任务是<em class="kc">将</em>所有HOV写的博客输出到一个可爱的表格中。收集这些数据的一种方法是复制并粘贴HOV网站上的所有文章。你知道，用鼠标和网络浏览器。但是，我强烈认为，这是一个可怕的想法。它不仅耗时、枯燥、容易出错，而且不可复制。作为一名有抱负的数据科学家，我必须告诉你，不可再现性是绝对不允许的(同样，如果你友好地提出来，我很乐意讨论为什么)。</p><p id="db1c" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">所以，首先我们必须学会如何从HOV的博客中“搜集”这些信息。换句话说，我们已经学会了如何通过一套指令让我们的<em class="kc">计算机</em>为我们复制和粘贴。你怎么问？进入Scrapy:一个用Python编写的漂亮的小应用程序，我们可以用它来完成这个任务。(参见Scrapy的文档<a class="ae kb" href="https://docs.scrapy.org/en/latest/intro/overview.html" rel="noopener ugc nofollow" target="_blank">此处</a>。)</p><p id="1ba0" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我将尝试Scrapy可以做什么，如果你不太害怕你的计算机控制台(或终端),你可以通过输入如下所示的命令来跟随。用来抓取HOV网站的真实蜘蛛就在我在GitHub上的仓库里。<a class="ae kb" href="https://github.com/psubbiah/solidarity_data/tree/master/data" rel="noopener ugc nofollow" target="_blank">在这里</a>你也可以下载我用来进行文本分析的文件，并遵循更高级的说明(你真幸运！你可以完全跳过这个教程！)</p><p id="2b57" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">一些需要注意的法律问题:网站的条款和条件会注明他们的抓取政策。一些网站非常明确地禁止抓取(例如IMDB ),而另一些则不那么明确。我们试图捕捉的数据增加了服务器的负载，因此公司有理由希望限制抓取。一个网站的文档应该清楚地说明这一点。您可以通过在网站名称中添加斜杠并键入/robots.txt来访问这些信息。</p><p id="5136" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在，让我们建立一个简单的蜘蛛，它会教我们基本的刮擦，嗯…合法的。对于Scrapy更全面的介绍，我强烈推荐这些文章:</p><ol class=""><li id="e92f" class="kh ki hi jh b ji jj jl jm jo kj js kk jw kl ka km kn ko kp bi translated">李彦宏终极<a class="ae kb" href="https://www.smashingmagazine.com/2019/07/ultimate-guide-scalable-web-scrapers-scrapy/" rel="noopener ugc nofollow" target="_blank">简介</a></li><li id="5777" class="kh ki hi jh b ji kq jl kr jo ks js kt jw ku ka km kn ko kp bi translated">pyimagesearch.com<a class="ae kb" href="https://www.pyimagesearch.com/2015/10/12/scraping-images-with-python-and-scrapy/" rel="noopener ugc nofollow" target="_blank">教程</a>用于抓取图片和理解如何在不同的页面上爬行。</li></ol><p id="542d" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><em class="kc">如果您想学习如何处理已经拥有的文本数据，请跳过这一节Python webscraping，转到下一节，在那里我将讨论如何在r中进行文本分析。</em></p><p id="8223" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">要安装Scrapy，请到您的控制台。随意复制粘贴。我注意到散列后的所有内容都被“注释掉”，控制台并不解释它，尽管我们可以复制和粘贴，但它只对人类有用。</p><h2 id="10ca" class="kv kw hi bd kx ky kz la lb lc ld le lf jo lg lh li js lj lk ll jw lm ln lo lp bi translated">Windows操作系统</h2><p id="309e" class="pw-post-body-paragraph jf jg hi jh b ji lq ij jk jl lr im jn jo ls jq jr js lt ju jv jw lu jy jz ka hb bi translated">在这里，我们确保1)我们的pip是最新的，2)我们安装了依赖项(在本例中为“pypiwin32”)，3)然后才安装Scrapy:</p><pre class="ix iy iz ja fd lv kg lw lx aw ly bi"><span id="555f" class="kv kw hi kg b fi lz ma l mb mc"># Install pip<br/>python -m pip install --upgrade pip<br/><br/># Install dependencies<br/>pip install pypiwin32<br/><br/># Install scrapy<br/>pip install scrapy</span></pre><h2 id="99ce" class="kv kw hi bd kx ky kz la lb lc ld le lf jo lg lh li js lj lk ll jw lm ln lo lp bi translated">Linux操作系统</h2><p id="42c1" class="pw-post-body-paragraph jf jg hi jh b ji lq ij jk jl lr im jn jo ls jq jr js lt ju jv jw lu jy jz ka hb bi translated">在这里，我们安装依赖项，然后确保pip是最新的，然后才安装Scrapy:</p><pre class="ix iy iz ja fd lv kg lw lx aw ly bi"><span id="2511" class="kv kw hi kg b fi lz ma l mb mc"># Install dependencies<br/>sudo apt-get install python3 python3-dev python-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev<br/><br/># Upgrade pip<br/>pip install --upgrade pip<br/><br/># Install scrapy<br/>pip install scrapy</span></pre><h2 id="d31f" class="kv kw hi bd kx ky kz la lb lc ld le lf jo lg lh li js lj lk ll jw lm ln lo lp bi translated">苹果个人计算机</h2><p id="c467" class="pw-post-body-paragraph jf jg hi jh b ji lq ij jk jl lr im jn jo ls jq jr js lt ju jv jw lu jy jz ka hb bi translated">在这里，我们更新Xcode，然后确保home-brew和python是最新的，然后才安装Scrapy(我注意到根据Python (2，3)的版本设置，你需要使用<code class="du kd ke kf kg b">pip3</code>而不是下面的<code class="du kd ke kf kg b">pip</code>)。</p><pre class="ix iy iz ja fd lv kg lw lx aw ly bi"><span id="c176" class="kv kw hi kg b fi lz ma l mb mc"># Xcode update<br/>xcode-select --install<br/><br/># Install homebrew<br/>/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)"<br/><br/># Update path variable so that homebrew starts before system packages<br/>echo "export PATH=/usr/local/bin:/usr/local/sbin:$PATH" &gt;&gt; ~/.bashrc<br/><br/>source ~/.bashrc<br/><br/># Make sure brew and python are up to date<br/>brew update; brew upgrade python<br/><br/># Install scrapy<br/>pip install scrapy</span></pre><h2 id="1263" class="kv kw hi bd kx ky kz la lb lc ld le lf jo lg lh li js lj lk ll jw lm ln lo lp bi translated">编码我们的蜘蛛！</h2><p id="49ba" class="pw-post-body-paragraph jf jg hi jh b ji lq ij jk jl lr im jn jo ls jq jr js lt ju jv jw lu jy jz ka hb bi translated">首先，为我们的蜘蛛选择一个容易记忆的名字，然后运行下面的命令——你猜对了！— <code class="du kd ke kf kg b">startproject</code>:</p><pre class="ix iy iz ja fd lv kg lw lx aw ly bi"><span id="5f16" class="kv kw hi kg b fi lz ma l mb mc">scrapy startproject wikiNobel</span></pre><p id="58e6" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这将生成一组文件，我们将需要运行我们的蜘蛛。我特别害怕真正的蜘蛛，所以即使拼写和阅读这个词也感觉不好(我是一种视觉动物)。事实上，我写了这么多关于蜘蛛的东西，尽管我对真正的蜘蛛是怎么想的，这也说明了这些小软件版本的效用。他们太棒了！</p><p id="30b7" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在，您应该有一组如下所示的文件夹:</p><pre class="ix iy iz ja fd lv kg lw lx aw ly bi"><span id="378e" class="kv kw hi kg b fi lz ma l mb mc">wikiNobel<br/>├── scrapy.cfg          --&gt; deploys configuration of scrapy project.<br/>└── wikiNobel           --&gt; this is your scrapy project module.<br/>    ├── __init__.py     --&gt; this initialises the module initializer<br/>    ├── items.py        --&gt; this project item definition py file<br/>    ├── middlewares.py  --&gt; project middleware .py file<br/>    ├── pipelines.py    --&gt; project pipeline .py file<br/>    ├── settings.py     --&gt; project settings .py file<br/>    └── spiders         --&gt; save flaurates.py here!<br/>        ├── __init__.py         <br/>        ├── flaurates.py</span></pre><p id="9b55" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们现在将在<code class="du kd ke kf kg b">/wikiNobel/spiders</code>文件夹中创建一个新的Python脚本，并将其命名为<code class="du kd ke kf kg b">flaureates.py</code>。如果您从未创建过Python脚本，不要担心，您不需要流利地掌握这种语言，这里的介绍应该足以编写您的蜘蛛。你应该看看Al Sweigart著名的<a class="ae kb" href="https://automatetheboringstuff.com/" rel="noopener ugc nofollow" target="_blank">指南</a>作为这门语言的介绍！这很有趣——我保证。</p><p id="e117" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我确实建议你在开始单飞之前获得一些Python编程经验——也就是编写你自己的蜘蛛，因为它们会变得相当复杂，正如你将看到的。</p><p id="8150" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">为了创建这个Python脚本，我们需要一个文本编辑器。我们可以在任何文本编辑器中写这个——甚至你信任的文本编辑器也可以。但是还有其他一些可爱的代码编辑器，它们突出显示代码并给出条目建议(就像你在智能手机上打字时的建议一样)。我个人对<a class="ae kb" href="https://atom.io/" rel="noopener ugc nofollow" target="_blank"> Atom </a>情有独钟。</p><p id="2fa9" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">进入编辑器后，键入:</p><pre class="ix iy iz ja fd lv kg lw lx aw ly bi"><span id="0fbb" class="kv kw hi kg b fi lz ma l mb mc">import scrapy<br/><br/>class NobelSpider(scrapy.Spider):<br/>	name = "wikiNobel"<br/>	allowed_domains = ["en.wikipedia.org"]<br/>	start_urls = ["https://en.wikipedia.org/wiki/List_of_female_Nobel_laureates"]</span></pre><h2 id="bb10" class="kv kw hi bd kx ky kz la lb lc ld le lf jo lg lh li js lj lk ll jw lm ln lo lp bi translated">了解蜘蛛文件</h2><p id="4bc8" class="pw-post-body-paragraph jf jg hi jh b ji lq ij jk jl lr im jn jo ls jq jr js lt ju jv jw lu jy jz ka hb bi translated">好的。让我们认真对待这一切…</p><p id="76a2" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">首先，<code class="du kd ke kf kg b">import scrapy</code>告诉Python访问Scrapy脚本，以及它的函数和类。</p><p id="98dd" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">第二，在<code class="du kd ke kf kg b">class</code>中，我们正在创建蜘蛛刺儿头已经提供的<code class="du kd ke kf kg b">(scrapy.Spider)</code>的<em class="kc">子集</em>，我们将其命名为<code class="du kd ke kf kg b">wikiNobel</code>。这个名称很重要，因为我们将使用这个名称从命令行调用这个特定的蜘蛛。</p><p id="2682" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">然后，我们设置蜘蛛可以访问的网页的限制(在这种情况下，只有维基):</p><p id="d24c" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><code class="du kd ke kf kg b">allowed_domains</code>列出了蜘蛛可以抓取的域</p><p id="60cd" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><code class="du kd ke kf kg b">start_urls</code>列出允许蜘蛛开始“爬行”的url，即它是蜘蛛将“读取”的第一个URL。</p><p id="9cb2" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们练习提取页面的标题(现在)。缩进第一行(从上一行开始)，键入:</p><pre class="ix iy iz ja fd lv kg lw lx aw ly bi"><span id="bd1e" class="kv kw hi kg b fi lz ma l mb mc">def parse(self, response): <br/>	data = {}<br/>	data['title'] = response.xpath('//h1[@class="firstHeading"]/text()').extract()<br/>	yield data</span></pre><p id="e3d9" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><code class="du kd ke kf kg b">data = {}</code>定义了一个空字典，Scrapy将保存我们提取的标题。这需要连同它下面的所有内容一起缩进！<code class="du kd ke kf kg b">parse</code>是蜘蛛的主要功能。注意不要改变这个函数的名字</p><p id="719a" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们用非常(非常)直白的术语来剖析下一行代码的含义:</p><p id="d1cb" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们已经告诉Scrapy去<code class="du kd ke kf kg b">extract()</code>，从保存在它的<code class="du kd ke kf kg b">response</code>中的整个HTML代码，包含属性<code class="du kd ke kf kg b">class="firstHeading"</code>的<code class="du kd ke kf kg b">&lt;h1&gt;</code>标题的<code class="du kd ke kf kg b">text()</code>——所有这些都使用xpath符号。</p><p id="2f47" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">注意:确保你将你的Python脚本保存在<code class="du kd ke kf kg b">/wikinobel/spiders/</code>目录下，并且作为<code class="du kd ke kf kg b">flaureates.py</code>——否则意味着Scrapy将无法找到它！</p><h2 id="af70" class="kv kw hi bd kx ky kz la lb lc ld le lf jo lg lh li js lj lk ll jw lm ln lo lp bi translated">运行我们的蜘蛛</h2><p id="9d9f" class="pw-post-body-paragraph jf jg hi jh b ji lq ij jk jl lr im jn jo ls jq jr js lt ju jv jw lu jy jz ka hb bi translated">完整的小蜘蛛保存后应该是这样的:</p><pre class="ix iy iz ja fd lv kg lw lx aw ly bi"><span id="be81" class="kv kw hi kg b fi lz ma l mb mc">import scrapy<br/><br/>class NobelSpider(scrapy.Spider):<br/>	name = "wikiNobel"<br/>	allowed_domains = ["en.wikipedia.org"]<br/>	start_urls = ["https://en.wikipedia.org/wiki/List_of_female_Nobel_laureates"]<br/>	<br/>	def parse(self, response): <br/>		data = {}<br/>		data['title'] = response.xpath('//h1[@class="firstHeading"]/text()').extract()<br/>		yield data</span></pre><p id="7dc4" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们去追蜘蛛吧！前往您的控制台(确保您仍然在<code class="du kd ke kf kg b">wikiNobel/wikiNobel</code>目录中),并键入:</p><pre class="ix iy iz ja fd lv kg lw lx aw ly bi"><span id="cee9" class="kv kw hi kg b fi lz ma l mb mc">scrapy crawl wikiNobel</span></pre><p id="bad7" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在你会在控制台里看到一堆打印出来的文字(吓人的东西吧？？)</p><p id="00d0" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在Scrapy已提交的信息中，如果一切顺利(日期显然会不同)，您还应该看到类似这样的内容:</p><pre class="ix iy iz ja fd lv kg lw lx aw ly bi"><span id="496d" class="kv kw hi kg b fi lz ma l mb mc">2020-11-19 11:18:52 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://en.wikipedia.org/wiki/List_of_female_Nobel_laureates&gt;<br/>{'title': ['List of female Nobel laureates - Wikipedia']}</span></pre><p id="79c7" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">答对了。您已经构建了第一个蜘蛛，它为女性诺贝尔奖获得者提取了维基百科页面的标题！我承认，还不是很有用。我们会处理的。</p><h2 id="0503" class="kv kw hi bd kx ky kz la lb lc ld le lf jo lg lh li js lj lk ll jw lm ln lo lp bi translated">编写稍微复杂一点的蜘蛛程序</h2><p id="e1ac" class="pw-post-body-paragraph jf jg hi jh b ji lq ij jk jl lr im jn jo ls jq jr js lt ju jv jw lu jy jz ka hb bi translated">现在……制造一个蜘蛛来提取所有获奖者的名字，然后到他们的维基条目中提取他们的生日，以及他们的诺贝尔奖的类别，这不是很棒吗？</p><p id="edb9" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这样的蜘蛛需要:</p><ol class=""><li id="063a" class="kh ki hi jh b ji jj jl jm jo kj js kk jw kl ka km kn ko kp bi translated">为每位获奖者获取一个维基条目；</li><li id="5c93" class="kh ki hi jh b ji kq jl kr jo ks js kt jw ku ka km kn ko kp bi translated">开始一个爬行之旅，访问每个条目并提取出生日期和奖项名称。</li></ol><h2 id="eb59" class="kv kw hi bd kx ky kz la lb lc ld le lf jo lg lh li js lj lk ll jw lm ln lo lp bi translated">第一步:获取所有的维基条目</h2><p id="3409" class="pw-post-body-paragraph jf jg hi jh b ji lq ij jk jl lr im jn jo ls jq jr js lt ju jv jw lu jy jz ka hb bi translated">编辑您的<code class="du kd ke kf kg b">def parse(self, response):</code>部分，用显示的新代码行保持所有的缩进。</p><p id="4a63" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">注意:<em class="kc">不要使用任何空格来创建缩进，否则Python将不知道如何阅读它！当你复制/粘贴时，文本编辑器有时会将缩进转换成空格，你永远也不会知道发生了什么。代码看起来完全一样。最佳实践是自己键入代码。相信我。我去过那里，一点也不好玩。</em></p><p id="5d3d" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如您所见，xpaths会变得非常复杂！！</p><pre class="ix iy iz ja fd lv kg lw lx aw ly bi"><span id="9c99" class="kv kw hi kg b fi lz ma l mb mc">def parse(self, response):<br/>        for href in response.xpath('//span[@class="vcard"]//a/@href').extract():<br/>            url = response.urljoin(href)<br/>            print(url)<br/>            req = scrapy.Request(url, callback=self.parse_bday)<br/>            yield req</span></pre><p id="089f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们来看看我们的第一个<code class="du kd ke kf kg b">parse</code>函数，它比上一个函数有了很大的提高。在这里，<code class="du kd ke kf kg b">response.xpath('//span[@class="vcard"]//a/@href).extract()</code>查找html中所有的<em class="kc"/><code class="du kd ke kf kg b">//</code><code class="du kd ke kf kg b">&lt;span&gt;</code>元素，这些元素也将<code class="du kd ke kf kg b">vcard</code>作为<code class="du kd ke kf kg b">class</code>。</p><p id="24f3" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">xpath指向也包含一个<code class="du kd ke kf kg b">href</code>的<code class="du kd ke kf kg b">&lt;a&gt;</code>元素(在<code class="du kd ke kf kg b">&lt;span&gt;</code>的那个部分内)。<code class="du kd ke kf kg b">href</code>HTML中的s其他URL。在我们的HTML中有许多<code class="du kd ke kf kg b">href</code>，但是在这种情况下，我们寻找的是获奖者的维基条目<em class="kc">的链接。这些恰好出现在<code class="du kd ke kf kg b">vcard</code>类下。</em></p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="ab fe cl jc"><img src="../Images/7f33827cbd279da16d6fcd9129ded04a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*3byAxduWnJhFMzE0ndXoWw.png"/></div></figure><p id="8585" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因为<code class="du kd ke kf kg b">href</code>是以缩写形式给出的(即作为<code class="du kd ke kf kg b">/wiki/Bertha_von_Suttner</code>而不是作为https://en . Wikipedia . org/wiki/Bertha _ von _ Suttner)，我们必须让Scrapy检查它提取的<code class="du kd ke kf kg b">href</code>和<code class="du kd ke kf kg b">urljoin</code>的列表。这个方便的join函数将为我们提供完整或绝对的地址。</p><p id="8657" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们告诉Scrapy打印URL，这样一旦我们运行蜘蛛，我们就可以在屏幕上看到它，然后我们要求它转到它的第二个任务<code class="du kd ke kf kg b">parse_bday</code>，我们在下面定义它。</p><h2 id="985c" class="kv kw hi bd kx ky kz la lb lc ld le lf jo lg lh li js lj lk ll jw lm ln lo lp bi translated">步骤2:从<em class="md">每个</em>维基条目中获取信息</h2><p id="23e3" class="pw-post-body-paragraph jf jg hi jh b ji lq ij jk jl lr im jn jo ls jq jr js lt ju jv jw lu jy jz ka hb bi translated">现在我们需要让我们的蜘蛛遍历它找到的每一个<code class="du kd ke kf kg b">href</code>，提取获奖者的名字、她的生日以及她获得的诺贝尔奖的类别。</p><p id="fd3b" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><em class="kc">还是那句话，这里注意缩进！</em></p><p id="5661" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这是我们的代码:</p><pre class="ix iy iz ja fd lv kg lw lx aw ly bi"><span id="606a" class="kv kw hi kg b fi lz ma l mb mc">def parse_bday(self, response):<br/>        for sel in response.css('html').extract():<br/>            data = {}<br/>            data['title'] = response.xpath('//h1/text()').extract()<br/>            data['bday'] = response.xpath('//span[@class="bday"]/text()').extract()<br/>            data['award'] = response.xpath('//a[contains(text(), "Nobel")]/@title').extract_first()<br/>            <br/>        yield data</span></pre><p id="9914" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">呀！好吧，让我们来分解一下…</p><p id="5617" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们的新函数是<code class="du kd ke kf kg b">parse_bday</code>,但是我们也会给蜘蛛指令来提取其他的东西…</p><p id="263e" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">对于蜘蛛从主要获奖者摘要中提取的每个链接，它应该:</p><ol class=""><li id="14e5" class="kh ki hi jh b ji jj jl jm jo kj js kk jw kl ka km kn ko kp bi translated">创建一个名为<code class="du kd ke kf kg b">data</code>的空字典(把它想象成我们的‘表’)</li><li id="42d2" class="kh ki hi jh b ji kq jl kr jo ks js kt jw ku ka km kn ko kp bi translated">提取“标题”列下的标题(当前条目的<code class="du kd ke kf kg b">&lt;h1&gt;</code>标签)</li><li id="5d3b" class="kh ki hi jh b ji kq jl kr jo ks js kt jw ku ka km kn ko kp bi translated">在“bday”列下提取生日(在<code class="du kd ke kf kg b">class="bday"</code>下的所有span元素的<code class="du kd ke kf kg b">text</code>)。</li><li id="8039" class="kh ki hi jh b ji kq jl kr jo ks js kt jw ku ka km kn ko kp bi translated">提取包含<code class="du kd ke kf kg b">text()</code>“Nobel”的<code class="du kd ke kf kg b">&lt;a&gt;</code>元素的<em class="kc">第一个</em>(即使用<code class="du kd ke kf kg b">extract_first</code>函数代替Extract)实例。保存在“奖励”栏下。(我们提取第一个给定的每个条目多次提到该奖项。我们从<code class="du kd ke kf kg b">&lt;a&gt;</code>元素中提取它(而不是从整个文本中)，因为它包含一个到特定类别的wiki条目的链接。从理论上来说，这应该能确保我们不会只提取Nobel这个词本身——除非它与阿尔弗雷德·诺贝尔本人有联系，否则我们就需要核实一下。</li></ol><p id="c5ce" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在我们回到控制台，再次运行蜘蛛。不过这次我们将输出保存为. csv文件。</p><pre class="ix iy iz ja fd lv kg lw lx aw ly bi"><span id="f22e" class="kv kw hi kg b fi lz ma l mb mc">scrapy crawl wikiNobel -o nobels.csv</span></pre><h2 id="377b" class="kv kw hi bd kx ky kz la lb lc ld le lf jo lg lh li js lj lk ll jw lm ln lo lp bi translated">测试xpaths的工具</h2><p id="6f42" class="pw-post-body-paragraph jf jg hi jh b ji lq ij jk jl lr im jn jo ls jq jr js lt ju jv jw lu jy jz ka hb bi translated">Xpath选择器可能会变得非常复杂。我们仅仅触及了表面。一般来说，我们需要详细研究HTML代码，以了解我们希望我们的蜘蛛提取什么，所以很好地理解HTML是如何构建的是有用的。现在，你最好考虑一下标签的结构:<code class="du kd ke kf kg b">&lt;h1&gt;</code>、<code class="du kd ke kf kg b">&lt;h2&gt;</code>等等，信号标题。<code class="du kd ke kf kg b">href</code>通常在<code class="du kd ke kf kg b">&lt;a&gt;</code>标签下，文字通常在<code class="du kd ke kf kg b">&lt;p&gt;</code>下，等等。关于HTML和它是如何构建的更多信息，请查看这个关于基础的<a class="ae kb" href="https://www.w3schools.com/html/html_basic.asp" rel="noopener ugc nofollow" target="_blank">课程</a>。</p><p id="2942" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们可以通过打开Chrome的DevTools来测试xpaths。我们选择想要提取的网页部分，右键单击它，然后从选项中选择“Inspect”。这将打开整个HTML(你会在旁边看到它)。一旦我们在想要抓取的页面旁边有了整个HTML，按下<code class="du kd ke kf kg b">⌘F</code>(Mac上的command + F，windows上的control + F)。这将打开一个搜索工具，我们可以在其中键入您的xpathsChrome将突出显示它在HTML文档中找到的内容(见下图)。</p><figure class="ix iy iz ja fd jb er es paragraph-image"><div class="ab fe cl jc"><img src="../Images/4dc75ea8da309b8bce4f95424178ec46.png" data-original-src="https://miro.medium.com/v2/format:webp/1*JacErto6atSauK6jBXbaFw.jpeg"/></div><figcaption class="me mf et er es mg mh bd b be z dx translated">使用Chrome开发工具中的搜索工具</figcaption></figure><p id="384a" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在这个例子中，你可以看到Chrome用我们给它的Xpath找到了58个名字</p><p id="8234" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><code class="du kd ke kf kg b">//span[@class="vcard"]//a/text()</code>。您可以按下向下箭头，沿着找到的所有实例移动。在所提供的例子中，Chrome突出显示了它找到的所有名字的第二个实例，在这种情况下，就是老好人Bertha von Suttner(58个名字中的第二个)。这意味着总共有57位获奖者——玛丽·居里两次获得诺贝尔奖，是唯一一个在两个不同学科获得诺贝尔奖的人！(说说做一个超水平的人吧！！多传奇啊！！)检查xpath返回的实例数量是一个很好的做法，只是为了确保它得到的是您需要的，而不是更多(例如，您可能需要使用<code class="du kd ke kf kg b">extract_first</code>)。</p><p id="31c5" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如果您发现自己在Xpath方面有困难，不要绝望！学习曲线很陡。</p><p id="fa98" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">Scrapy包括自己的“外壳”,也就是说，它有自己的定制终端，你可以直接输入scrapy命令。shell帮助我们在编写选择器代码时检查<code class="du kd ke kf kg b">response</code><em class="kc">；换句话说，我们可以输入<code class="du kd ke kf kg b">response.xpath('//h1/text()').extract()</code>，它将返回找到的内容(如果它找到了任何内容的话)。这样，我们就能确保Scrapy选择了我们想要的信息，<em class="kc">然后</em>在我们的文本编辑器中将其“固定下来”。</em></p><p id="f680" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">要获得更全面的xpath介绍，请查看这篇文章、这篇文章和这篇非常简洁但写得很棒的<a class="ae kb" href="http://plasmasturm.org/log/xpath101/" rel="noopener ugc nofollow" target="_blank">文章</a>。</p><h2 id="9d77" class="kv kw hi bd kx ky kz la lb lc ld le lf jo lg lh li js lj lk ll jw lm ln lo lp bi translated">一种更复杂的蜘蛛…</h2><p id="9d6b" class="pw-post-body-paragraph jf jg hi jh b ji lq ij jk jl lr im jn jo ls jq jr js lt ju jv jw lu jy jz ka hb bi translated">你可以在这里找到我用来提取HOV <a class="ae kb" href="https://github.com/psubbiah/solidarity_data/blob/master/solidarity/solidarity/spiders/solidarity.py" rel="noopener ugc nofollow" target="_blank">信息的蜘蛛。</a></p><p id="d1aa" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如您所见，路径非常复杂，并且需要大量的试验和错误！</p><p id="ba3f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在我们的下一个教程中，我们将进入项目的分析部分…我将假设你有你的数据，这将是一个挖掘的问题！</p><p id="aa44" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如果你是一个编程新手，你会意识到有很多信息可以帮助你处理错误、故障排除和一般的“新手”——把你自己当成一个好伙伴。编程和数据科学社区<em class="kc">对他们的时间非常</em>慷慨，你可以在<a class="ae kb" href="https://stackoverflow.com/" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/</a>上找到你大部分问题的答案——如果你从未试图在那里找到答案，你就上当了！真的很神奇。</p><p id="d498" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">当我们开始学习一门完全不同的语言时，再见，我最喜欢的语言(至少在分析方面)R！</p></div></div>    
</body>
</html>