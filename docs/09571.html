<html>
<head>
<title>PyTorch For Deep Learning — Feed Forward Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于深度学习的PyTorch前馈神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pytorch-for-deep-learning-feed-forward-neural-network-d24f5870c18?source=collection_archive---------2-----------------------#2020-09-11">https://medium.com/analytics-vidhya/pytorch-for-deep-learning-feed-forward-neural-network-d24f5870c18?source=collection_archive---------2-----------------------#2020-09-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e50239dd2c020f58852f22c53a387678.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jHLsllWe2sU8kUsia4QZRw.jpeg"/></div></div></figure><p id="6c5f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">注意:神经网络理论不会在这篇博文中讨论。这纯粹是为了PyTorch的实现，你需要知道他们如何工作背后的理论。</em></p><h1 id="5747" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">用PyTorch实现人工神经网络</h1><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kn"><img src="../Images/5c2decd51c21885c54ef5a2b5ba081ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tWvofX1zZZnIHyHs-nKAfA.png"/></div></div></figure><p id="9e69" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们直接看代码。对于这段代码，我们将使用来自sklearn的著名糖尿病数据集。</p><p id="cc19" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将要遵循的流水线:<br/> →导入数据<br/> →创建数据加载器<br/> →创建神经网络<br/> →训练模型</p><ol class=""><li id="a0a3" class="ks kt hi is b it iu ix iy jb ku jf kv jj kw jn kx ky kz la bi translated"><strong class="is hj">导入所需的库</strong></li></ol><pre class="ko kp kq kr fd lb lc ld le aw lf bi"><span id="0c9d" class="lg jq hi lc b fi lh li l lj lk">#importing the libraries<br/>import torch<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span></pre><p id="3f2b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 2。导入数据集</strong></p><pre class="ko kp kq kr fd lb lc ld le aw lf bi"><span id="cd12" class="lg jq hi lc b fi lh li l lj lk">#importing the dataset</span><span id="7c25" class="lg jq hi lc b fi ll li l lj lk">from sklearn.datasets import load_diabetes<br/>data = load_diabetes()<br/>x = data['data']<br/>y = data['target']</span><span id="c8e8" class="lg jq hi lc b fi ll li l lj lk">#shape<br/>print('shape of x is : ',x.shape)<br/>print('shape of y is : ',y.shape)</span><span id="2254" class="lg jq hi lc b fi ll li l lj lk"><strong class="lc hj">Output:<br/></strong>shape of x is :  (442, 10)<br/>shape of y is :  (442,)</span></pre><p id="1cd8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 3。数据集和数据加载器</strong></p><p id="4dba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">pytorch中的Dataset类基本上覆盖了一个元组中的数据，并使我们能够访问每个数据的索引。这对于创建可用于混洗、应用小批量梯度下降等的dataloader类是必要的。</p><pre class="ko kp kq kr fd lb lc ld le aw lf bi"><span id="b774" class="lg jq hi lc b fi lh li l lj lk">#dataset<br/>from torch.utils.data import Dataset, DataLoader<br/>class diabetesdataset(Dataset):<br/>  def __init__(self,x,y):<br/>    self.x = torch.tensor(x,dtype=torch.float32)<br/>    self.y = torch.tensor(y,dtype=torch.float32)<br/>    self.length = self.x.shape[0]</span><span id="6f4b" class="lg jq hi lc b fi ll li l lj lk">  def __getitem__(self,idx):<br/>    return self.x[idx],self.y[idx]</span><span id="92a1" class="lg jq hi lc b fi ll li l lj lk">  def __len__(self):<br/>    return self.length</span><span id="9f69" class="lg jq hi lc b fi ll li l lj lk"><br/>dataset = diabetesdataset(x,y)</span></pre><ul class=""><li id="bf26" class="ks kt hi is b it iu ix iy jb ku jf kv jj kw jn lm ky kz la bi translated">init函数用于初始化数据集的x和y，并在需要时将它们转换为张量。</li><li id="8a9c" class="ks kt hi is b it ln ix lo jb lp jf lq jj lr jn lm ky kz la bi translated">getitem函数用于返回数据集中的特定索引。它返回x和y值。</li><li id="4fb6" class="ks kt hi is b it ln ix lo jb lp jf lq jj lr jn lm ky kz la bi translated">len函数返回数据集的大小。</li></ul><p id="2613" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面将展示如何使用这个类。如果你还不明白，不要担心。</p><p id="3127" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，转到数据加载器</p><pre class="ko kp kq kr fd lb lc ld le aw lf bi"><span id="d4bd" class="lg jq hi lc b fi lh li l lj lk">#dataloader<br/>dataloader = DataLoader(dataset=dataset,shuffle=True,batch_size=100)</span></pre><p id="5c95" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">DataLoader用作迭代函数，用于执行微型批处理或随机梯度下降。有关数据集和数据加载器的更多详细信息，请查看pytorch文档。</p><p id="97b3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 4。神经网络</strong></p><pre class="ko kp kq kr fd lb lc ld le aw lf bi"><span id="4a48" class="lg jq hi lc b fi lh li l lj lk">#creeating the network</span><span id="b703" class="lg jq hi lc b fi ll li l lj lk">from torch import nn</span><span id="5096" class="lg jq hi lc b fi ll li l lj lk">class net(nn.Module):<br/>  def __init__(self,input_size,output_size):<br/>  super(net,self).__init__()<br/>    self.l1 = nn.Linear(input_size,5)<br/>    self.relu = nn.ReLU()<br/>    self.l2 = nn.Linear(5,output_size)</span><span id="084e" class="lg jq hi lc b fi ll li l lj lk">  def forward(self,x):<br/>    output = self.l1(x) <br/>    output = self.relu(output)<br/>    output = self.l2(output)<br/>    return output</span></pre><p id="c6b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在PyTorch中，神经网络是使用面向对象编程创建的。这些层是在init函数中定义的，向前传递是在forward函数中定义的，当调用该类时会自动调用该函数。<br/>由于有了<strong class="is hj"> nn类，这些功能是可能的。从torch继承的模块</strong>。如图所示，我使用了2 L线性T21隐藏层和一个激活函数</p><p id="f55f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性图层接受输入形状和输出形状，并为指定形状生成权重和偏差项。</p><p id="9dc6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 5。一些参数</strong></p><pre class="ko kp kq kr fd lb lc ld le aw lf bi"><span id="0671" class="lg jq hi lc b fi lh li l lj lk">model = net(x.shape[1],1)<br/>criterion = nn.MSELoss()<br/>optimizer = torch.optim.SGD(model.parameters(),lr=0.001)<br/>epochs = 1500</span></pre><p id="2afb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们通过指定输入和输出大小创建了一个net类的对象。<br/>只要这个对象在数据上被调用，网络中的转发功能就会被调用。</p><p id="0678" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">损失函数是均方损失，因为这是一个回归问题。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/0f79e681b8bfb787a2905d3819222388.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*TCE9Kui4fbyZl5u3ARRBJw.png"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">均方误差损失</figcaption></figure><p id="b0b3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du lx ly lz lc b">torch.optim.SGD</code>接受网络的权重和偏差以及学习率。在为此类的对象调用函数时，权重会相应地更新。</p><p id="406d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du lx ly lz lc b">epochs</code>是训练的迭代次数</p><p id="788a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">6。培训</p><pre class="ko kp kq kr fd lb lc ld le aw lf bi"><span id="a3ef" class="lg jq hi lc b fi lh li l lj lk">costval = []</span><span id="05d5" class="lg jq hi lc b fi ll li l lj lk">for j in range(epochs):<br/>  for i,(x_train,y_train) in enumerate(dataloader):</span><span id="f7bf" class="lg jq hi lc b fi ll li l lj lk">    #prediction<br/>    y_pred = model(x_train)<br/>    <br/>    #calculating loss<br/>    cost = criterion(y_pred,y_train.reshape(-1,1))<br/>  <br/>    #backprop<br/>    optimizer.zero_grad()<br/>    cost.backward()<br/>    optimizer.step()<br/>  if j%50 == 0:<br/>    print(cost)<br/>    costval.append(cost)</span><span id="9814" class="lg jq hi lc b fi ll li l lj lk"><strong class="lc hj">Output:<br/></strong>tensor(26336.3301, grad_fn=&lt;MseLossBackward&gt;) <br/>tensor(3607.3894, grad_fn=&lt;MseLossBackward&gt;) <br/>tensor(2773.9294, grad_fn=&lt;MseLossBackward&gt;) <br/>tensor(2302.8511, grad_fn=&lt;MseLossBackward&gt;) <br/>tensor(2928.7632, grad_fn=&lt;MseLossBackward&gt;) <br/>tensor(2525.0527, grad_fn=&lt;MseLossBackward&gt;) <br/>tensor(3494.0715, grad_fn=&lt;MseLossBackward&gt;) <br/>tensor(3227.6035, grad_fn=&lt;MseLossBackward&gt;) <br/>tensor(2275.8088, grad_fn=&lt;MseLossBackward&gt;) <br/>tensor(3081.6348, grad_fn=&lt;MseLossBackward&gt;)</span></pre><p id="7e14" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">训练好的模型用于预测相同输入值(<em class="jo">通常，这是测试设定值</em>)，预测值与实际值相对照。从这个图中，我们可以看到预测值和实际值几乎是相似的，尽管有一些误差。</p><p id="984e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们可以说该模型运行良好。</p><p id="26db" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 7。模型分析</strong></p><p id="20dc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">训练好的模型用于预测相同输入的值(<em class="jo">，通常，这是测试设定值</em>)，预测值与实际值相对照。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/25c722e63bbd3a98b45ee6997d5099f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*Rbdw0yG4XoAF2MgeOYqxUg.png"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">损失与时代</figcaption></figure><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/1701e74640f9d25a26f524a076a0cbe3.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*ijYsXvY8AbrmrHVMFSqtQg.png"/></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">y_pred对y_test</figcaption></figure><p id="486b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从这张图中，我们可以看出预测值和实际值几乎是一样的，尽管有一些误差。因此，我们可以说该模型运行良好。</p><h1 id="452b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">结论</h1><p id="8d9e" class="pw-post-body-paragraph iq ir hi is b it mc iv iw ix md iz ja jb me jd je jf mf jh ji jj mg jl jm jn hb bi translated">这是PyTorch中一个简单神经网络的实现。<br/> <strong class="is hj">谢谢。</strong></p></div></div>    
</body>
</html>