<html>
<head>
<title>Activation Functions in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的激活函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-functions-in-neural-networks-69197497bd1d?source=collection_archive---------20-----------------------#2020-12-06">https://medium.com/analytics-vidhya/activation-functions-in-neural-networks-69197497bd1d?source=collection_archive---------20-----------------------#2020-12-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7582" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">激活函数决定了神经网络的输出。它们负责神经网络的准确性和训练网络所需的计算能力。这些激活功能被连接到网络中的每个神经元，并作为一个门，从而在接收到正确的输入集时“激发”神经元。</p><p id="0b23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例 —你把手浸入冷水中，你会感觉到冷，把手浸入热水中，你会感觉到热。这是由于大脑中负责检测冷热的某些神经元的激活。</p><p id="cc2d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">神经元中心房颤动的表现</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/762ff9367001889d2910822b5c23700c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*oFag9RctZbgcyAUfpyDmig.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">激活函数的表示</figcaption></figure><p id="7d58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">激活功能的类型- </strong></p><p id="2194" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.二进制步骤</p><p id="ca64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.线性的</p><p id="bee7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.非线性的</p><ol class=""><li id="9583" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated"><strong class="ih hj">二进制步骤</strong></li></ol><p id="ced0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个基于<em class="jy">阈值</em>的 AF。你决定一个阈值，如果输出值大于阈值，那么神经元被激活，如果输出值小于阈值，则不被激活。这个 AF 的问题是，如果你有多个神经元输出激活的<em class="jy">信号，那么应该考虑哪一个？。结果，采用二进制阶跃函数的网络不能将输出分类为许多期望输出类别中的一个。</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/cc968fdcc371a023132402a90ebb0652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*9OK6ZN9UOg25t7CsFwV49w.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">二元步进 AF</figcaption></figure><p id="9b09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">二进制阶跃函数不可微，因此没有反向传播的余地。</p><p id="d222" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。</strong> <strong class="ih hj">线性函数</strong> —将输入乘以权重并添加偏差。之后，应用线性 AF，产生对应于输入信号的输出。因此，它适用于多种类别，我们可以以最大的概率获取神经元的输出。</p><p id="9cae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">直线的斜率是一个常数，所以在反向传播中，我们不知道应该调整多少权重以及针对哪个神经元。因此，网络效率不高，无法正确理解输入，导致输出层的精度非常低。</p><p id="4889" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。非线性函数</strong>——适用于复杂网络，允许构建具有多个层的更深层次网络。他们可以处理复杂的数据集，包括图像、音频和视频。此外，它们可以有效地调整反向传播中的权重。</p><h1 id="985f" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak"> 3.1 乙状结肠房颤</strong></h1><p id="0855" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">无论它接收什么输入，输出值都将在<strong class="ih hj"> 0 和 1 之间。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/90b4a2baf877047311970b11f71d8999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Z9x4Geq25q7BnpMAupjisA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">乙状结肠房颤</figcaption></figure><p id="41fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺点</strong>—</p><p id="a638" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">●曲线不是正态分布的，因此网络的计算成本很高，达到全局最小值需要很长时间。</p><p id="05ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">●在反向传播中会出现消失梯度问题，由于该问题，必须添加的新权重将等于旧权重，网络的学习会减少。</p><h1 id="7e68" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak"> 3.2 </strong> <strong class="ak">塔那夫</strong></h1><p id="1e97" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">无论它接收到什么输入，输出值都会在<strong class="ih hj"> -1 和 1 之间。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/f212b4039b18da4f0706103ca055feea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*11WQq-vCPdf4SgkyxFq5Lw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">tanh AF</figcaption></figure><p id="e620" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优势</strong> —数据呈正态分布或以零为中心，因此计算成本更低。</p><p id="325a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺点</strong>——反向传播中会出现消失梯度问题，因此需要增加的新权值将等于旧权值，网络的学习能力会下降。</p><h1 id="520a" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak"> 3.3 ReLu(整流线性单元)</strong> <strong class="ak"> AF </strong></h1><p id="3208" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">●如果 x 的值&gt; 0，则输出为 x。</p><p id="47a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">●如果 x 的值为&lt; 0 then output is 0.</p><p id="eae4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">则优势</strong>-解决消失梯度问题。</p><p id="35f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">劣势</strong> —死亡<strong class="ih hj">神经元</strong>状态，x 相对于 x 的导数为 1，但相对于 x 的负权重的导数为 0，因此在反向传播中没有新的权重可添加，从而导致<strong class="ih hj">死于</strong>或<strong class="ih hj">死亡神经元</strong>状态。</p><h1 id="7541" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak"> 3.4 泄漏的 ReLU AF </strong></h1><p id="1ee7" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">为了避免死神经元状态，我们将一个小正数乘以 x，但这再次引入了消失梯度问题，正如我们在下面的示例中看到的-</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/f2e56523f1b647e709dbd5267f8d0a2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*FdAL5TJr2ICbfZhUUU5sZQ.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">泄漏 ReLU AF</figcaption></figure><h1 id="4942" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak"> 3.5 ELU(指数线性单位)AF </strong></h1><p id="5b57" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">由于存在指数项，计算非常昂贵。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/d65d70f2d0ecc68836b33f7a5b930540.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*1V7mb-9xlU8Doet12fUKNw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">ELU</figcaption></figure><h1 id="7409" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak"> 3.6 PReLU(参数 ReLU) </strong></h1><p id="2d2b" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">●如果 x &gt; 0，则输出为 x 值。</p><p id="dc85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">●如果 x = 0，则输出为 0。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/0596d5c6c781fcef09ded4b18bb6e9e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*SmufqpjpLvmhkl-ih0Nk2Q.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">预聚焦自动对焦</figcaption></figure><p id="bc40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上两个条件使激活功能像 ReLU 一样工作。</p><p id="32fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们引入了∞，这只是一个学习率。</p><p id="46b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">●如果∝ = 0.01，则变为泄漏 ReLU。</p><p id="ab2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">●如果∞= 0，则变为 ReLU。</p><p id="c7b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">●任何其他值∝则变为参数 ReLU。</p><p id="220c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:∞是一个超参数。</p><h1 id="96ef" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak"> 3.7 嗖嗖</strong></h1><p id="23c7" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">当网络由 LSTM 小区组成时最常用。因为当隐藏层的数量大于 40 时，使用 Swish 会在计算上比较昂贵。</p><p id="2338" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们在上面的图像中看到的，x 只不过是(权重*输入)+偏差，因此我们得到了用蓝色绘制的曲线。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ld le di lf bf lg"><div class="er es lc"><img src="../Images/d3f766d824b0e409496aa265dee780ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R_n5GUHWFz48mzmOqQ5bXQ.jpeg"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">Swish AF</figcaption></figure><p id="a0bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">优势-</p><p id="2f41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">●解决消失梯度问题</p><p id="747c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">●防止神经元死亡状态</p><p id="36d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">●正态分布</p><h1 id="a467" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak"> 3.8 Softplus </strong></h1><p id="8fc3" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">Softplus 房颤的衍生物与乙状结肠房颤相同。这是 2001 年开发的一种较新的房颤。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/cfa163cd07ed961e66e4175267c870af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*ToUiyvu_hsmeEwqRFlblVA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">SoftPlus AF</figcaption></figure><h1 id="0e6c" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated"><strong class="ak"> 3.9 SoftMax </strong></h1><p id="1804" class="pw-post-body-paragraph if ig hi ih b ii kx ik il im ky io ip iq kz is it iu la iw ix iy lb ja jb jc hb bi translated">当输出层有超过 2 个神经元时，则使用 SoftMax。</p><p id="2882" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">soft max 的工作- </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/77d6fe1285647d66678fe147705e9c37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*1LWlb9A639Amf5iV3sP-Tg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">SoftMax 工作</figcaption></figure><p id="3b34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">致谢</strong> -</p><ol class=""><li id="7fb2" class="jp jq hi ih b ii ij im in iq jr iu js iy jt jc ju jv jw jx bi translated">克里斯·纳伊克—<a class="ae lh" href="https://www.youtube.com/user/krishnaik06/featured" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/user/krishnaik06/featured</a></li><li id="7157" class="jp jq hi ih b ii li im lj iq lk iu ll iy lm jc ju jv jw jx bi translated"><a class="ae lh" href="https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/#:~:text=Activation%20functions%20are%20mathematical%20equations,relevant%20for%20the%20model's%20prediction" rel="noopener ugc nofollow" target="_blank">https://missing link . ai/guides/neural-network-concepts/7-types-neural-network-Activation-functions-right/#:~:text = Activation % 20 functions % 20 are % 20 mathematic % 20 equations，relevant % 20 for % 20 model % 20 prediction</a>。</li><li id="a29a" class="jp jq hi ih b ii li im lj iq lk iu ll iy lm jc ju jv jw jx bi translated">【https://en.wikipedia.org/wiki/Activation_function T2】号</li></ol><p id="d1c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过以下方式联系我—</p><p id="fe41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">电子邮件—tejasta@gmail.com</p><p id="b5a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">领英—【https://www.linkedin.com/in/tejasta/ T4】</p><p id="4973" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读！</p></div></div>    
</body>
</html>