<html>
<head>
<title>Removing Multicollinearity for Linear and Logistic Regression.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">消除线性和逻辑回归的多重共线性。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/removing-multi-collinearity-for-linear-and-logistic-regression-f1fa744f3666?source=collection_archive---------4-----------------------#2020-06-03">https://medium.com/analytics-vidhya/removing-multi-collinearity-for-linear-and-logistic-regression-f1fa744f3666?source=collection_archive---------4-----------------------#2020-06-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="1b6b" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">多重共线性简介</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/42ff206ce3b86136c3622adfc6978a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aAn9mrVteweaHupdfl_8pw.jpeg"/></div></div></figure><p id="cc22" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> <em class="kf">什么是多重共线性？</em>T3】</strong></p><p id="4c60" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">多重共线性</strong>是指多元回归模型中两个或两个以上解释变量高度线性相关的情况。<em class="kf">【这是直接来自维基百科】</em>。<em class="kf">多重共线性</em>发生在您的模型包括多个不仅与您的目标变量相关，而且彼此相关的因素时。</p><p id="7040" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在让我们用简单的话来解释一下……</p><p id="7249" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">当数据集中的一列A增加时，它也会影响另一列B，它可能会增加或减少，但它们有一个非常相似的行为。</p><p id="0d60" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">假设我们有一个包含4个要素和1个连续目标值的数据集。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kg"><img src="../Images/e9845bac886f6396ac804037a8066e0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*WxUxZIPU_cLcMugzx--KRw.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">以Y为目标列的虚拟数据框架</figcaption></figure><p id="7260" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">现在，如果我们在这里观察到，随着X1列的值增加，X2的值也增加。这表明X1和X2彼此有些关联。用统计学的话来说，X1和X2的相关系数是相似的。</p><p id="e85d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">换句话说，X1和X2高度相关，因此这种情况简单地称为多重共线性。</p><h2 id="5ab8" class="kl km hi bd kn ko kp kq kr ks kt ku kv js kw kx ky jw kz la lb ka lc ld le lf bi translated">现在你可能会想，它会如何影响我正在构建的模型？</h2><p id="28a7" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">线性回归和逻辑回归的假设之一是特征列相互独立。因此，多重共线性明显违反了线性和逻辑回归的假设，因为它表明独立的要素(即要素列)相互依赖。</p><p id="e480" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">此外，虽然您的模型有时会在不消除多重共线性的情况下提供较高的准确性，但它不能依赖于真实世界的数据。此外，系数对模型中的微小变化非常敏感。简单地说，模型将不能一般化，如果您的模型在生产环境中，这会导致巨大的失败。从数据集中移除多重共线性的另一个重要原因是降低模型的开发和计算成本，这将使您离'<em class="kf">完美'</em>模型更近一步。<strong class="jl hj"> <em class="kf">所以一定要谨慎，不要跳过这一步！！</em>T15】</strong></p><h2 id="33c5" class="kl km hi bd kn ko kp kq kr ks kt ku kv js kw kx ky jw kz la lb ka lc ld le lf bi translated">那么现在我们如何检测这种多重共线性呢？</h2><blockquote class="ll lm ln"><p id="f340" class="jj jk kf jl b jm jn ij jo jp jq im jr lo jt ju jv lp jx jy jz lq kb kc kd ke hb bi translated"><strong class="jl hj">使用相关系数热图</strong></p></blockquote><p id="fb08" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">一个简单的步骤是我们观察相关系数矩阵并排除那些具有高相关系数的列。使用pandas可以很容易地找到数据框架的相关系数，为了更好地理解<a class="ae lr" href="https://seaborn.pydata.org/introduction.html" rel="noopener ugc nofollow" target="_blank"><strong class="jl hj"><em class="kf">seaborn</em></strong></a>软件包有助于构建热图。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ls"><img src="../Images/ee8e63572817039117cb5da4d1bfc6ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*oKtZ2epsnbUHEnx9-WrNwQ.png"/></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">使用seaborn的热图</figcaption></figure><p id="aaa7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">但是等等，当我们有很多特征的时候，这个方法不会变得复杂吗？</p><p id="dd7c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">是啊！！这将适用于较小的数据集，但对于较大的数据集，分析这将是困难的。哎呀…我们被困住了吗？别担心，我们还有其他方法。</p><blockquote class="ll lm ln"><p id="7086" class="jj jk kf jl b jm jn ij jo jp jq im jr lo jt ju jv lp jx jy jz lq kb kc kd ke hb bi translated"><strong class="jl hj">利用</strong><a class="ae lr" href="https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html" rel="noopener ugc nofollow" target="_blank"><strong class="jl hj"/></a><strong class="jl hj">(方差影响因子)</strong></p></blockquote><h2 id="6625" class="kl km hi bd kn ko kp kq kr ks kt ku kv js kw kx ky jw kz la lb ka lc ld le lf bi translated">VIF的理念是什么？</h2><p id="f8b6" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">它一次将一列作为目标，将其他列作为特征，并符合线性回归模型。之后，它计算r平方值，对于VIF值，我们取1平方的倒数，即1/(1平方)。</p><p id="3a0f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">因此，在每次迭代之后，我们得到数据集中每一列<em class="kf">(上面作为目标)</em>的VIF值。VIF值越高，在建立实际回归模型时删除列的可能性就越大。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lt"><img src="../Images/80fb2897bd83d7f50ec8eab1f06bc96e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZOCSFVs2MSMACLm9NpuDQg.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">计算每列的VIF的代码段</figcaption></figure><p id="269e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">但是，如果你不想删除这些列，也许他们有一些重要的信息。如果是这样，你可以使用下面提到的这个小而有用的技巧:</p><p id="0b99" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们可以使用<strong class="jl hj"> <em class="kf">岭回归或</em> </strong>套索回归，因为在这些类型的回归技术中，我们添加了一个额外的λ值，该值对特定列的一些系数不利，从而降低了多重共线性的影响。</p><h2 id="df75" class="kl km hi bd kn ko kp kq kr ks kt ku kv js kw kx ky jw kz la lb ka lc ld le lf bi translated">这就是检测和移除数据集中多重共线性的方法。</h2><p id="d09f" class="pw-post-body-paragraph jj jk hi jl b jm lg ij jo jp lh im jr js li ju jv jw lj jy jz ka lk kc kd ke hb bi translated">很简单，对吧？去尝试一下吧，如果你通过这篇文章学到了新的东西，别忘了鼓掌！！</p><p id="70fb" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">检查我的GitHub存储库中的基本Python代码:<a class="ae lr" href="https://github.com/princebaretto99/removing_multiCollinearity" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/prince baretto 99/removaling _ multico linearity</a></p></div></div>    
</body>
</html>