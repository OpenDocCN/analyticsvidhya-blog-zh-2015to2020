<html>
<head>
<title>Fine-tuning XLNet language model to get better results on text classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微调XLNet语言模型以获得更好的文本分类结果</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fine-tuning-xlnet-language-model-to-get-better-results-on-text-classification-8dfb96eb49ab?source=collection_archive---------11-----------------------#2020-03-30">https://medium.com/analytics-vidhya/fine-tuning-xlnet-language-model-to-get-better-results-on-text-classification-8dfb96eb49ab?source=collection_archive---------11-----------------------#2020-03-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="410f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果你在这里，你可能听说过XLNet。在我们继续之前，让我简单介绍一下XLNet</p></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="7935" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> XLNet </strong> <em class="jk">是一种使用广义自回归预训练方法学习语言表示的</em>方法。它的目标是学习语言模型。已经使用置换语言建模目标在大型语料库上对其进行了训练。</p><p id="c2e4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在各种NLP任务上已经超过了BERT。在我的<a class="ae jl" rel="noopener" href="/analytics-vidhya/fine-tuning-bert-language-model-to-get-better-results-on-text-classification-3dac5e3c348e">上一篇文章</a>中，我展示了如何微调BERT模型以获得更好的准确性。下面我将列出BERT和XLNet的一些基本区别，然后我们将深入XLNet的实现。</p><p id="a4b8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">XLNet的架构与BERT非常相似。主要区别在于他们的培养目标。BERT屏蔽数据，并尝试使用双向上下文预测屏蔽的数据，而XLNet使用置换目标。顾名思义，在其最简单的形式中，它生成一个句子中所有单词的排列，并试图最大化序列的可能性。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/2252cc611a7c826a38811513c8a2eaa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*AUgc5eQDROr6cvHl.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">置换语言模型(来自XLNet论文)</figcaption></figure><p id="3577" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">它已经在各种NLP任务上取得了最先进的结果。我们可以使用XLNet学习的语言表示来完成我们的任务，例如文本分类等等，以获得关于我们问题的最先进的结果。</p><p id="e3ad" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="jk">注意:我们不会进入技术细节，但如果有人有兴趣阅读关于变形金刚和XLNet，</em> <a class="ae jl" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> <em class="jk">这个</em> </a> <em class="jk">博客会非常有帮助。</em></p><p id="d6bc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将看到如何使用XLNet的语言模型来完成文本分类任务。拥抱脸的变形金刚让这变得非常容易。通过transformers，我们可以使用XLNet预训练语言模型进行序列分类。我们还可以微调/重新训练XLNet的预训练语言模型，以适应我们的任务，然后使用该模型获得一些改进。</p><p id="dbb0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在本教程中，我将展示如何从检查点重新训练/微调XLNet的语言模型，然后如何使用微调后的语言模型进行序列分类。我们将使用Tensorflow对模型进行微调。然后，我们将使用我最喜欢的名为Transformers的库之一，将微调后的TensorFlow模型转换为Pytorch模型。然后我们会用变形金刚库做序列分类。我们还将把结果与直接使用预先训练的XLNet模型进行比较。这是一个非常实用的强化教程。我用两个Jupyter笔记本在Google Colab上做了这个。首先，我展示了如何从检查点重新训练您的模型，然后使用Transformers-cli将重新训练的模型转换为Pytorch模型。在第二个笔记本中，我已经展示了如何使用预训练模型进行序列分类。我们还将比较微调和预训练模型之间的结果。所有代码都可以在下面的共享Github资源库中找到。</p><h1 id="526e" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">入门指南</h1><p id="d044" class="pw-post-body-paragraph im in hi io b ip la ir is it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj hb bi translated">整个代码的实现和解释可以在<a class="ae jl" href="https://github.com/Shivampanwar/Bert-text-classification" rel="noopener ugc nofollow" target="_blank">这个</a> repo中找到。</p><p id="d474" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们经常有大量未标记的数据集，只有少量标记的数据集。如果我们需要获得准确的分类，我们可以使用在大语料库上训练的预训练模型来获得不错的结果。通常，我们使用在大型语料库上训练的预训练语言模型来获得嵌入，然后主要在其上添加一层或两层神经网络来适应我们手头的任务。这种方法非常有效，直到训练语言模型的数据与我们的数据相似。</p><p id="9fe9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果我们的数据不同于用于预训练的数据，结果将不会令人满意。例如，如果我们有印地语和英语语言的混合数据，并且我们使用在维基百科上训练的预训练模型，这将导致不好的结果。在这种情况下，我们也需要微调我们的语言模型。</p><p id="ecb9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以从给定的检查点重新训练我们的语言模型，或者可以再次训练它。两者都在这个库中展示过。</p><h1 id="fd7f" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">电影评论数据集</h1><p id="2a98" class="pw-post-body-paragraph im in hi io b ip la ir is it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj hb bi translated">让我们收集数据。我们将使用https://www.kaggle.com/c/word2vec-nlp-tutorial/data<a class="ae jl" href="https://www.kaggle.com/c/word2vec-nlp-tutorial/data" rel="noopener ugc nofollow" target="_blank">的</a>数据来达到我们的目的。</p><p id="89ff" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">该数据集包含25000个具有带标签的电影评论的训练示例和25000个不带标签的测试示例。我们将在总共有50000条评论的组合训练和测试数据上微调我们的语言模型。</p><p id="5c4a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">本教程将分三步进行:</p><p id="7b02" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">1 —第一步是在组合的训练和测试数据集上微调我们的语言模型。然后，我们将对训练数据集进行80:20分割。这个已经在<a class="ae jl" href="https://github.com/Shivampanwar/Bert-text-classification/blob/master/XLNet/Xlnet-finetuning.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="io hj">Xlnet-fine tuning . ipynb</strong></a>中做了。</p><p id="832d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2-第二步将使用直接预训练的XLNet语言模型，并在80%的数据上训练该模型，然后在20%的数据上测试。</p><p id="7a5e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">3 —第三步与第二步相同，唯一的区别是我们这次将使用微调的语言模型。</p><p id="6c33" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">第二步和第三步已经在<a class="ae jl" href="https://github.com/Shivampanwar/Bert-text-classification/blob/master/XLNet/xlnet_experimentation.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="io hj">xlnet _ experiments . ipynb</strong></a><strong class="io hj">中做了。</strong></p><h1 id="9f6d" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">从检查点重新训练语言模型</h1><p id="c8b5" class="pw-post-body-paragraph im in hi io b ip la ir is it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj hb bi translated">我们将使用<a class="ae jl" href="https://github.com/zihangdai/xlnet" rel="noopener ugc nofollow" target="_blank">这个</a>库来重新训练我们的语言模型。我们将首先结合我们的训练和测试数据。我们的组合数据需要写在一个。' txt '文件。该文件需要遵循以下条件:</p><ul class=""><li id="f610" class="lf lg hi io b ip iq it iu ix lh jb li jf lj jj lk ll lm ln bi translated">每行都是一个句子。</li><li id="e09a" class="lf lg hi io b ip lo it lp ix lq jb lr jf ls jj lk ll lm ln bi translated">空行表示文档结束。</li></ul><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es lt"><img src="../Images/b462ca557e02c29d98fea1d74c22fcf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*D1B5yzv4qFeUjzN0zqqiEg.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">txt文件所需的格式</figcaption></figure><p id="073b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在此之后，我们将需要谷歌的SentencePieceModel来标记数据。我们将在上面创建的txt文件上训练这个句子片断模型。它修饰了我们的句子。</p><p id="ff27" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在训练完我们的句子片段模型后，我们将使用<a class="ae jl" href="https://github.com/zihangdai/xlnet/blob/master/data_utils.py" rel="noopener ugc nofollow" target="_blank"> data_utils.py </a>文件将我们的数据转换成TFrecords格式。然后我们可以将这些数据输入到我们的模型中。我们将使用<a class="ae jl" href="https://github.com/zihangdai/xlnet/blob/master/train_gpu.py" rel="noopener ugc nofollow" target="_blank"> train_gpu.py </a>来训练我们的最终模型。我从检查点模型重新训练了这个模型。检查点是不必要的，如果检查点不存在，那么我们将从头开始预训练我们的新模型。检查点模型是基于XLNet的。</p><p id="c62e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在对我们的模型进行微调之后，我使用transformer-CLI将这个模型转换成Pytorch模型。</p><p id="330e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">完成所有这些需要很多天，并且解决了许多Github问题。</p><h1 id="c6af" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">使用XLNet模型进行分类</h1><p id="299d" class="pw-post-body-paragraph im in hi io b ip la ir is it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj hb bi translated">所有的预处理和代码已经显示在<a class="ae jl" href="https://github.com/Shivampanwar/Bert-text-classification/blob/master/XLNet/xlnet_experimentation.ipynb" rel="noopener ugc nofollow" target="_blank">XLNet-experimental . ipynb</a>中。我已经试着让这变得尽可能的合理可行。我使用了预训练和微调的语言模型，并试图比较它们的结果。</p><p id="1fd5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用预训练的XLNet给出大约88%的准确度，而使用微调的XLNet模型给出大约87.4%的准确度。使用微调的语言模型有一个可以忽略的下降。其中一个原因可能是我们的数据集与XLNet被训练的数据集非常相似。如果我们可以在不同的数据集上尝试，比如印地语-英语混合语言数据集，那么它可以获得显著的改进。</p><h1 id="ab9d" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">包扎</h1><p id="c33e" class="pw-post-body-paragraph im in hi io b ip la ir is it lb iv iw ix lc iz ja jb ld jd je jf le jh ji jj hb bi translated">我们看到了如何从给定的检查点重新训练我们的语言模型。我们还看到了如何使用PyTorch-transformers来使用xlnet进行序列分类。希望你会喜欢它。</p><h1 id="9241" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">Github代码链接</h1><div class="lu lv ez fb lw lx"><a href="https://github.com/Shivampanwar/Bert-text-classification" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hj fi z dy mc ea eb md ed ef hh bi translated">shivampanwar/Bert-文本-分类</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">这展示了如何微调Bert语言模型并使用PyTorch-transformers进行文本分类。这包含…</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">github.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml jw lx"/></div></div></a></div><h1 id="abe1" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">参考</h1><div class="lu lv ez fb lw lx"><a href="https://mccormickml.com/2019/09/19/XLNet-fine-tuning/" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hj fi z dy mc ea eb md ed ef hh bi translated">使用PyTorch的XLNet微调教程</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">在本教程中，我将向您展示如何使用…</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">mccormickml.com</p></div></div></div></a></div><div class="lu lv ez fb lw lx"><a href="https://github.com/zihangdai/xlnet" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hj fi z dy mc ea eb md ed ef hh bi translated">紫行代/xlnet</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">XLNet是一种新的无监督语言表示学习方法，它基于一种新的广义置换语言…</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">github.com</p></div></div><div class="mg l"><div class="mm l mi mj mk mg ml jw lx"/></div></div></a></div><div class="lu lv ez fb lw lx"><a href="https://github.com/huggingface/transformers" rel="noopener  ugc nofollow" target="_blank"><div class="ly ab dw"><div class="lz ab ma cl cj mb"><h2 class="bd hj fi z dy mc ea eb md ed ef hh bi translated">拥抱脸/变形金刚</h2><div class="me l"><h3 class="bd b fi z dy mc ea eb md ed ef dx translated">用于TensorFlow 2.0和PyTorch的最先进的自然语言处理🤗变形金刚(以前称为…</h3></div><div class="mf l"><p class="bd b fp z dy mc ea eb md ed ef dx translated">github.com</p></div></div><div class="mg l"><div class="mn l mi mj mk mg ml jw lx"/></div></div></a></div></div></div>    
</body>
</html>