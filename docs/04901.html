<html>
<head>
<title>Author(Multi-class text) Classification using Bidirectional LSTM &amp; Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用双向LSTM和Keras的作者(多类文本)分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/author-multi-class-text-classification-using-bidirectional-lstm-keras-c9a533a1cc4a?source=collection_archive---------2-----------------------#2020-04-05">https://medium.com/analytics-vidhya/author-multi-class-text-classification-using-bidirectional-lstm-keras-c9a533a1cc4a?source=collection_archive---------2-----------------------#2020-04-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="12c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用双向递归神经网络的多类文本分类，长短期记忆，Keras &amp; Tensorflow 2.0。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/6a50bfe0318634f4c12326c98720e7bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s7He49rEONuDkS0qxw_yvA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><a class="ae jt" href="https://nlpforhackers.io/wp-content/uploads/2016/08/text-classification.png" rel="noopener ugc nofollow" target="_blank">https://nlpforhackers . io/WP-content/uploads/2016/08/text-classification . png</a></figcaption></figure><h1 id="2153" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">目标:</h1><p id="f715" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">在本文中，我们将了解LSTM网络的基本架构。然后我们将学习如何使用Tensorflow 2.0和Keras构建LSTM机器学习模型。对于这个应用程序，我们将使用来自Kaggle的竞争数据集。</p><h1 id="0e58" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">让我们开始:</h1><p id="632c" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated"><strong class="ih hj">T3【递归神经网络:T5】</strong></p><p id="6fda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">递归神经网络(RNN)是一种神经网络，其中前一步的输出作为输入馈送到当前步骤。这是一种处理连续数据(语音数据、音乐、文本中的单词序列)的短期记忆。这是一个示例架构图。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ky"><img src="../Images/aab76473e55c580a1972c363f4b707b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5maxJSGQaYoQusC5y1JhKw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:<a class="ae jt" href="https://doi.org/10.1371/journal.pone.0180944.g004" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1371/journal.pone.0180944.g004</a></figcaption></figure><p id="da45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">优点:</em> </strong></p><p id="1699" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">i) RNN有一种记忆，能捕捉到迄今为止已经计算过的东西。</p><p id="69b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ii)rnn是文本和语音数据分析的理想选择。</p><p id="f831" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">缺点:</em> </strong></p><p id="4626" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">i) RNN遭受爆炸和消失梯度，这使得RNN模型通过向后传播更少量的误差而学习得更慢。</p><p id="68ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ii)这对于短句很有效，当我们处理一篇长文章时，会有一个长期依赖问题</p><p id="9706" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">【长短期记忆】(LSTM) : </em> </strong></p><p id="fccb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">长短期记忆网络通常简称为“LSTM”——是一种特殊的RNN。它能够学习长期依赖性。它是由<a class="ae jt" href="http://www.bioinf.jku.at/publications/older/2604.pdf" rel="noopener ugc nofollow" target="_blank">hoch Reiter&amp;schmid Huber(1997)</a>介绍的。LSTM的设计明确避免了长期依赖问题。</p><p id="d474" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">建筑:</em> </strong></p><p id="3dd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LSTM有重复LSTM街区的链条。它被称为LSTM细胞。每个LSTM细胞有四个神经网络层相互作用。</p><ol class=""><li id="7c2b" class="kz la hi ih b ii ij im in iq lb iu lc iy ld jc le lf lg lh bi translated">细胞状态</li><li id="ef05" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated">忘记大门</li><li id="3133" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated">输入门</li><li id="fd6b" class="kz la hi ih b ii li im lj iq lk iu ll iy lm jc le lf lg lh bi translated">输出门</li></ol><p id="46f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个LSTM单元接收来自输入序列的输入、先前的单元状态和来自先前的LSTM单元的输出。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ln"><img src="../Images/a4fe7ed04993a37aa83c3eea4018d64f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q9pxGy5Y-JPfkkwP4xVbdw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">来源:<a class="ae jt" href="https://www.researchgate.net/publication/334360853/figure/fig1/AS:778955447599106@1562728859405/The-LSTM-cell-internals.png" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/publication/334360853/figure/fig 1/AS:778955447599106 @ 1562728859405/The-LSTM-cell-internals . png</a></figcaption></figure><p id="c0a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">双向LSTM </em> </strong></p><p id="ce3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">双向LSTM在输入序列上训练两层。输入序列上的一个LSTM层和输入序列的反向副本上的第二个LSTM层为学习序列提供了更多的上下文:</p><h1 id="8fac" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">让我们开始编码:</h1><p id="6979" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">现在我们将解决一个基于文本文档的作者分类问题。</p><p id="143b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">数据描述:</em> </strong></p><p id="c49c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个Kaggle竞赛数据集，包含由公共领域的幽灵作者写的小说作品的文本:埃德加·爱伦·坡，惠普·洛夫克拉夫特和玛丽·雪莱。这些数据是通过使用CoreNLP的MaxEnt句子标记器将较大的文本分成句子来准备的，所以我们可能会注意到这里和那里奇怪的非句子。我们的目标是准确识别测试集中句子的作者。</p><p id="8eab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据来源:【https://www.kaggle.com/c/spooky-author-identification/data T4】</p><p id="b420" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">导入所有需要的库</em> </strong></p><p id="06ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在开始编写代码之前，让我们导入所有需要的库。</p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="d708" class="lt jv hi lp b fi lu lv l lw lx">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from nltk.tokenize import word_tokenize<br/>from nltk.stem import WordNetLemmatizer<br/>from nltk.stem import PorterStemmer<br/>from tqdm import tqdm<br/>from keras.models import Sequential<br/>from keras.layers.recurrent import LSTM, GRU<br/>from keras.layers.core import Dense, Activation, Dropout<br/>from keras.layers.embeddings import Embedding<br/>from keras.layers.normalization import BatchNormalization<br/>from keras.utils import np_utils<br/>from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D<br/>from sklearn.model_selection import train_test_split<br/>from keras.preprocessing import sequence<br/>from keras.preprocessing.text import Tokenizer<br/>from keras.callbacks import EarlyStopping<br/>import matplotlib.pyplot as plt<br/>from nltk.corpus import stopwords</span><span id="fb74" class="lt jv hi lp b fi ly lv l lw lx">stop_words = stopwords.words('english')</span><span id="9327" class="lt jv hi lp b fi ly lv l lw lx">import tensorflow as tf<br/>print(tf.__version__)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lz"><img src="../Images/d453db1a607a0e307e75187cab821eba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MXzZFywd9GCTMP4eA7OcuQ.png"/></div></div></figure><p id="6ae7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们正在导入用于数组操作的NumPy和用于处理数据的pandas。还从nltk库中导入PorterStemmer和WordNetLemmatizer进行数据预处理。还导入了用于开发我们的Keras模型的基本库。</p><p id="aa38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">加载数据:</em> </strong></p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="1ee3" class="lt jv hi lp b fi lu lv l lw lx">df = pd.read_csv(‘/kaggle/input/author-classify/train.csv’)</span></pre><p id="c0fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将看到一些示例数据行。</p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="dfbb" class="lt jv hi lp b fi lu lv l lw lx">df.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ma"><img src="../Images/fbb2647234c1bc93e445700413070610.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*Ngav799gJhVLN3FLyKZ2wQ.png"/></div></figure><p id="0060" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将检查我们的数据中有多少独特的作者。</p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="92c6" class="lt jv hi lp b fi lu lv l lw lx">df.author.unique()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mb"><img src="../Images/a5be6192a07669a5ddd8b58b1df07513.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*DccfDKkuCuRmlJoZQbfQDw.png"/></div></figure><p id="54d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以我们的标签数据集中只有三个作者。</p><p id="bdcf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们检查数据集的其他基本细节。</p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="4bee" class="lt jv hi lp b fi lu lv l lw lx">df.info()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mc"><img src="../Images/cb71cd6e971a124f2db346b8fb30a217.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*PtqyahGpnh1BVwUjJGVmqw.png"/></div></figure><p id="5999" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们总共有19579个条目没有空值。太棒了。</p><p id="bde1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">数据预处理:</em> </strong></p><p id="9f65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在继续之前，我们将做一些数据清理和预处理。有几种数据预处理技术，</p><p id="59f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">I)删除停用词、标点符号、URL等。</p><p id="9bc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ii)俚语查找</p><p id="96b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">iii)转义html条目。</p><p id="32e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">iv)拼写纠正</p><p id="52b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">v)引理满足</p><p id="a12c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">六)词干</p><p id="0679" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们只应用词汇化和词干化。其余的事情你可以自己尝试。</p><p id="0cfe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将所有文本转换成小写。</p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="0265" class="lt jv hi lp b fi lu lv l lw lx">df[‘text’]=df[‘text’].str.lower()</span></pre><p id="53ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">词汇化:</em> </strong></p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="165d" class="lt jv hi lp b fi lu lv l lw lx">def getLemmText(text):<br/> tokens=word_tokenize(text)<br/> lemmatizer = WordNetLemmatizer()<br/> tokens=[lemmatizer.lemmatize(word) for word in tokens]<br/> return ‘ ‘.join(tokens)</span><span id="4ee2" class="lt jv hi lp b fi ly lv l lw lx">df[‘text’] = list(map(getLemmText,df[‘text’]))</span></pre><p id="8eca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">炮泥:</em> </strong></p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="6e26" class="lt jv hi lp b fi lu lv l lw lx">def getStemmText(text):<br/>    tokens=word_tokenize(text)<br/>    ps = PorterStemmer()<br/>    tokens=[ps.stem(word) for word in tokens]<br/>    return ' '.join(tokens)</span><span id="7994" class="lt jv hi lp b fi ly lv l lw lx">df['text'] = list(map(getStemmText,df['text']))</span></pre><p id="be89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将数据分为训练集和测试集。我们将使用scikit-learn的model_selection模块中的train_test_split来完成。在这里，我们将以这样的方式分割数据，2/3的数据行将用作训练数据，1/3的数据行将用于验证模型。</p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="9ba7" class="lt jv hi lp b fi lu lv l lw lx">xtrain, xtest, ytrain, ytest = train_test_split(<br/> df[‘text’], df[‘author’], <br/> test_size=0.33, <br/> random_state=53)</span><span id="0e0b" class="lt jv hi lp b fi ly lv l lw lx">print(xtrain.shape)<br/>print(xtest.shape)<br/>print(ytrain)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es md"><img src="../Images/facaa86a233ba5dcd986f5a1eba6e7a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P_NQLTbw_1cP-qwqruizpQ.png"/></div></div></figure><p id="eb25" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">定义超参数:</em> </strong></p><p id="bbd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将定义我们的超参数。我们有六个重要的超参数。</p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="4354" class="lt jv hi lp b fi lu lv l lw lx">EMBEDDING_DIMENSION = 64<br/>VOCABULARY_SIZE = 2000<br/>MAX_LENGTH = 100<br/>OOV_TOK = '&lt;OOV&gt;'<br/>TRUNCATE_TYPE = 'post'<br/>PADDING_TYPE = 'post'</span></pre><p id="07fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“EMBEDDING_DIMENSION:它定义了我们的向量的嵌入维数。</p><p id="8b2f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“VOCABULARY_SIZE”:它定义了tokenizer中的最大字数。</p><p id="5d55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“MAX_LENGTH”:定义每个句子的最大<em class="kx">长度，包括填充。</em></p><p id="446d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“OOV _托克”:这是在遇到一个看不见的单词时放入一个特殊的值</p><p id="9377" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">标记化:</em> </strong></p><p id="350f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步是对我们的数据进行标记化，并从中构建word_index。我们将使用Keras Tokenizer。在我们的例子中，需要2000个最常见的单词。我们将为那些不在<code class="du me mf mg lp b">word_index</code>中的单词加上&lt;00V&gt;。<code class="du me mf mg lp b">fit_on_text.</code></p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="9a77" class="lt jv hi lp b fi lu lv l lw lx">tokenizer = Tokenizer(num_words=VOCABULARY_SIZE, oov_token=OOV_TOK)<br/>tokenizer.fit_on_texts(list(xtrain) + list(xtest))</span></pre><p id="6aa2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步是将这些标记转换成序列列表。我们将使用<em class="kx"> texts_to_sequences() </em>方法来做到这一点。所以，我们的单词字典会是这样的。</p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="7254" class="lt jv hi lp b fi lu lv l lw lx">xtrain_sequences = tokenizer.texts_to_sequences(xtrain)<br/>xtest_sequences = tokenizer.texts_to_sequences(xtest)<br/>word_index = tokenizer.word_index<br/>print('Vocabulary size:', len(word_index))<br/>dict(list(word_index.items())[0:10])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es et"><img src="../Images/2c9431f1220557b20b22f340f802c187.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sjhBS0pfaZxFpCq_Ba-IgA.png"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mh"><img src="../Images/0b0cb0d533e35ef30833f36a8e7ce402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H7d-17CPviAmFVOUtHNoyw.png"/></div></div></figure><p id="5028" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是已经变成序列的训练数据中的第101篇。</p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="686f" class="lt jv hi lp b fi lu lv l lw lx">print(xtrain_sequences[100])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mi"><img src="../Images/dc50d5edb53a23beae9c18c7c466ff46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dwMGrjU2jp15m0QQgGV4bQ.png"/></div></div></figure><p id="1e07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">填充:</em> </strong></p><p id="620a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将添加填充到我们的数据，使其统一。Keras通过使用pad_sequences函数使填充数据变得很容易。我们将在应用填充后打印第101个文档。</p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="ca26" class="lt jv hi lp b fi lu lv l lw lx">xtrain_pad = sequence.pad_sequences(xtrain_sequences, maxlen=MAX_LENGTH, padding=PADDING_TYPE, truncating=TRUNCATE_TYPE)<br/>xtest_pad = sequence.pad_sequences(xtest_sequences, maxlen=MAX_LENGTH, padding=PADDING_TYPE, truncating=TRUNCATE_TYPE)</span><span id="6a5e" class="lt jv hi lp b fi ly lv l lw lx">print(len(xtrain_sequences[0]))<br/>print(len(xtrain_pad[0]))<br/>print(xtrain_pad[100])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mj"><img src="../Images/c67c0789303ed1e6c112dcf172f98475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gtR6WkeBkvMurT4O8SkFVg.png"/></div></div></figure><p id="be84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我们对验证序列做同样的事情。</p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="42d5" class="lt jv hi lp b fi lu lv l lw lx">label_tokenizer = Tokenizer()<br/>label_tokenizer.fit_on_texts(list(ytrain))</span><span id="18c5" class="lt jv hi lp b fi ly lv l lw lx">training_label_seq = np.array(label_tokenizer.texts_to_sequences(ytrain))<br/>test_label_seq = np.array(label_tokenizer.texts_to_sequences(ytest))<br/>print(training_label_seq[0])<br/>print(training_label_seq[1])<br/>print(training_label_seq[2])<br/>print(training_label_seq.shape)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mk"><img src="../Images/82c9038c5ea7b017046685c1868e8b3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*kq3G7Yb1rACTBAKMSxM7pA.png"/></div></figure><p id="cce2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在训练一个深度神经网络之前，我们应该探索一下我们的原始文本和填充后的文本是什么样子的。</p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="24bb" class="lt jv hi lp b fi lu lv l lw lx">reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])<br/>def decode_article(text):<br/> return ‘ ‘.join([reverse_word_index.get(i, ‘?’) for i in text])<br/>print(decode_article(xtrain_pad[11]))</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ml"><img src="../Images/a512ce898e0e9bfbb49ec31cbcc8964f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SENj7pJx1Gi1DTmiy2_CBQ.png"/></div></div></figure><p id="bbf0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">长短期记忆的实现(LSTM): </em> </strong></p><p id="74ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们完成了数据预处理和单词嵌入。现在我们将创建一个序列模型，嵌入一个LSTM层。</p><p id="a020" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">模型初始化:</em> </strong></p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="2a81" class="lt jv hi lp b fi lu lv l lw lx">model = Sequential()<br/>model.add(Embedding(len(word_index) + 1,<br/>                     EMBEDDING_DIMENSION))<br/>model.add(SpatialDropout1D(0.3))<br/>model.add(Bidirectional(LSTM(EMBEDDING_DIMENSION, dropout=0.3, recurrent_dropout=0.3)))<br/>model.add(Dense(EMBEDDING_DIMENSION, activation='relu'))<br/>model.add(Dropout(0.8))</span><span id="d309" class="lt jv hi lp b fi ly lv l lw lx">model.add(Dense(EMBEDDING_DIMENSION, activation='relu'))<br/>model.add(Dropout(0.8))</span><span id="68ad" class="lt jv hi lp b fi ly lv l lw lx">model.add(Dense(3))<br/>model.add(Activation('softmax'))<br/>model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])</span></pre><p id="68ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">模特培训:</em> </strong></p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="cc19" class="lt jv hi lp b fi lu lv l lw lx">num_epochs = 10<br/>history = model.fit(xtrain_pad, training_label_seq, epochs=num_epochs, validation_data=(xtest_pad, test_label_seq), verbose=2)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mm"><img src="../Images/dad9f4634b05a7e9fd51b7df0c67ffa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*za3_EAPxMIbpaw1KZZ4bLA.png"/></div></div></figure><p id="6382" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kx">模型评估:</em> </strong></p><pre class="je jf jg jh fd lo lp lq lr aw ls bi"><span id="12df" class="lt jv hi lp b fi lu lv l lw lx">def graph_plots(history, string):<br/>  plt.plot(history.history[string])<br/>  plt.plot(history.history['val_'+string])<br/>  plt.xlabel("Epochs")<br/>  plt.ylabel(string)<br/>  plt.legend([string, 'val_'+string])<br/>  plt.show()<br/>  <br/>graph_plots(history, "accuracy")<br/>graph_plots(history, "loss")</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mn"><img src="../Images/0fdec441cc76d37942e3bdcbeba7efd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ASV8ZNpn1lzkMr3lp2MsDw.png"/></div></div></figure><p id="4038" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下一篇文章中，我们将看到如何使用手套单词嵌入技术来构建一个LSTM模型。</p><p id="0168" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><p id="34df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我)<a class="ae jt" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Recurrent_neural_network</a></p><p id="cd36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">二)<a class="ae jt" href="http://www.bioinf.jku.at/publications/older/2604.pdf" rel="noopener ugc nofollow" target="_blank">http://www.bioinf.jku.at/publications/older/2604.pdf</a></p><p id="64aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">三)<a class="ae jt" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></p><p id="8c79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">iv)<a class="ae jt" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Long_short-term_memory</a></p><p id="a801" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jt" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Recurrent_neural_network</a></p></div></div>    
</body>
</html>