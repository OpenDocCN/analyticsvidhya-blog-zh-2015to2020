<html>
<head>
<title>Web Scraping Wikipedia Tables using BeautifulSoup and Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用BeautifulSoup和Python对维基百科表格进行网络抓取</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/web-scraping-wiki-tables-using-beautifulsoup-and-python-6b9ea26d8722?source=collection_archive---------0-----------------------#2018-05-01">https://medium.com/analytics-vidhya/web-scraping-wiki-tables-using-beautifulsoup-and-python-6b9ea26d8722?source=collection_archive---------0-----------------------#2018-05-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/62d263790d43525ce6d31923677b8a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/0*RYfebqkzWZ3RToOh.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">资料来源:SixFeetUp</figcaption></figure><h1 id="fd15" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><em class="jo">‘数据是新油’</em></h1><p id="c383" class="pw-post-body-paragraph jp jq hi jr b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km hb bi translated">作为一名有抱负的数据科学家，我做了很多涉及从各种网站收集数据的项目。一些像Twitter这样的公司确实提供了API来以一种更有组织的方式获取他们的信息，而我们不得不从其他网站获取结构化格式的数据。</p><p id="863a" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">web抓取背后的一般思想是检索网站上存在的数据，并将其转换成可用于分析的格式。在本教程中，我将详细而简单地解释如何使用<a class="ae ks" href="https://www.crummy.com/software/BeautifulSoup/" rel="noopener ugc nofollow" target="_blank"> BeautifulSoup </a>在Python中抓取数据。我将搜索<a class="ae ks" href="https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area" rel="noopener ugc nofollow" target="_blank">维基百科</a>来找出亚洲所有的国家。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es kt"><img src="../Images/87eea23cc8031915e9d07827cc52be92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Uls6YZNTUwapvAv4uCeLw.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">维基上的亚洲国家名称表</figcaption></figure><p id="a4e7" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">首先，我们将<strong class="jr hj">导入请求</strong>库。Requests允许你发送<em class="lc"> organic，grass-feed</em>HTTP/1.1请求，不需要手工劳动。</p><p id="3704" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">导入请求</p><p id="c122" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">现在我们分配我们将要抓取数据的网站的链接，并将其分配给名为<strong class="jr hj"> website_url </strong>的变量。</p><p id="807c" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated"><strong class="jr hj"> requests.get(网址)。text </strong>将ping一个网站并返回该网站的HTML。</p><blockquote class="ld le lf"><p id="7c6b" class="jp jq lc jr b js kn ju jv jw ko jy jz lg kp kc kd lh kq kg kh li kr kk kl km hb bi translated">website _ URL = requests . get('<a class="ae ks" href="https://en.wikipedia.org/wiki/List_of_Asian_countries_by_area" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/List _ of _ Asian _ countries _ by _ area</a><a class="ae ks" href="https://en.wikipedia.org/wiki/Premier_League).text" rel="noopener ugc nofollow" target="_blank">')。正文</a></p></blockquote><p id="91d6" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">我们首先读取给定网页的源代码，并使用<a class="ae ks" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank"> BeautifulSoup </a>函数创建一个BeautifulSoup (soup)对象。Beautiful Soup是一个用于解析HTML和XML文档的Python包。它为解析过的页面创建了一个解析树，可以用来从HTML中提取数据，这对web抓取很有用。BeautifulSoup中的Prettify()函数将使我们能够查看标签是如何嵌套在文档中的。</p><blockquote class="ld le lf"><p id="0fde" class="jp jq lc jr b js kn ju jv jw ko jy jz lg kp kc kd lh kq kg kh li kr kk kl km hb bi translated">从bs4导入beautiful soup<br/>soup = beautiful soup(website _ URL，' lxml ')<br/>print(soup . pretify())</p></blockquote><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es lj"><img src="../Images/b5f1d7f69edad3f740637f9084f99934.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hsZ0QrwflHUjYkYJxhEjxQ.png"/></div></div></figure><p id="83e5" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">如果你仔细检查HTML脚本，所有的表格内容，也就是我们想要提取的国家名称，都在Wikitable类下。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es lk"><img src="../Images/71ce65e9b615494068564dd0c7bd3420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NyaaGqqHnemKSWu8DQqUHQ.png"/></div></div></figure><p id="b901" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">所以我们的第一个任务是在HTML脚本中找到类‘wiki table sortable’。</p><blockquote class="ld le lf"><p id="c70e" class="jp jq lc jr b js kn ju jv jw ko jy jz lg kp kc kd lh kq kg kh li kr kk kl km hb bi translated">My_table = soup.find('table '，{'class':'wikitable sortable'})</p></blockquote><p id="c681" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">在表格类“wikitable sortable”下，我们有以国家名称作为标题的链接。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ll"><img src="../Images/378f82e58c29d72fa8a163aaf8f55a4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s6f-dPOmoNQVMe0yOqLV-w.png"/></div></div></figure><p id="2e3c" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">现在提取<a>中的所有链接，我们将使用<strong class="jr hj"> find_all()。</strong></a></p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/708a1d7e316d39d3c89d063cdb1488e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*nVwRAyrF5LLqwqOrxdwtmw.png"/></div></figure><p id="9126" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">从链接中，我们必须提取标题，即国家的名称。</p><p id="fdf4" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">为此，我们创建了一个列表<strong class="jr hj"> Countries </strong>，这样我们就可以从链接中提取国家的名称，并将其附加到列表Countries中。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="er es ln"><img src="../Images/35955e20ef5e34e3b17bee79b9b949e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qNhJCThsfkxZWHKiPq1RkQ.png"/></div></div></figure><p id="2690" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">将列表国家转换为熊猫数据框架，以便在python中工作。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/22131907133d8386a82fafba21a6bfd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*Sn71ZZa_pal70zK3Neki5w.png"/></div></figure><p id="09c1" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">感谢您阅读我的第一篇关于媒体的文章。我会坚持定期写下我的数据科学之旅。再次感谢您选择在这里度过您的时间——这意味着整个世界。</p><p id="fefe" class="pw-post-body-paragraph jp jq hi jr b js kn ju jv jw ko jy jz ka kp kc kd ke kq kg kh ki kr kk kl km hb bi translated">你可以在<a class="ae ks" href="https://github.com/stewync/Web-Scraping-Wiki-tables-using-BeautifulSoup-and-Python/blob/master/Scraping%2BWiki%2Btable%2Busing%2BPython%2Band%2BBeautifulSoup.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到我的代码。</p></div></div>    
</body>
</html>