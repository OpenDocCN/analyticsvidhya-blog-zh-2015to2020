<html>
<head>
<title>A-Z of Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树的A-Z</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/part-2-a-z-decision-trees-f88d704968d1?source=collection_archive---------13-----------------------#2019-09-21">https://medium.com/analytics-vidhya/part-2-a-z-decision-trees-f88d704968d1?source=collection_archive---------13-----------------------#2019-09-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/75c1b610f510dfcbde3eccdcee2a4f30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*50Gm-CpnauA2WlIJAMS35w.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="0f78" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh dx translated">第二部分:信息增益、基尼系数、过度拟合和不足拟合</h2></div><p id="ce12" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">现在，既然我们已经学习了一些关于决策树的知识，让我们更深入地研究其他类似的概念。</p><h1 id="a30e" class="ke kf ht bd kg kh ki kj kk kl km kn ko iz kp ja kq jc kr jd ks jf kt jg ku kv bi translated">信息增益:这个名字具有误导性</h1><p id="de33" class="pw-post-body-paragraph ji jj ht jk b jl kw iu jn jo kx ix jq jr ky jt ju jv kz jx jy jz la kb kc kd hb bi translated">信息增益实际上是由于将数据集划分到决策树中而导致的熵的预期减少。然而，我们称之为增益，因为我们测量的是相对于样本集合的增益。</p><p id="8975" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">样本S的属性A的IG定义如下。</p><figure class="lc ld le lf fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lb"><img src="../Images/a5cd258f080cd2a1354bf59e49249207.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KJI-gW3QR90oD7YAOfljKQ.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图1:信息增益G(S，A)</figcaption></figure><p id="a791" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">Values(A)是属性A的一组正值。'<em class="lk">Sv【T1]'是属性A的值为v的S的子集。</em></p><figure class="lc ld le lf fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ll"><img src="../Images/297a37899bc96beb764046b34bd49229.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uz3fk-lm2arJCisy03kdqw.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图2:计算ig</figcaption></figure><p id="08a9" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">让我们根据'<em class="lk"> Outlook </em>'属性将我们的样本数据集分成3个部分。如果我们计算这些数据集的单个熵，我们得到E = {0.97，0，0.97}。</p><p id="7fe5" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">图1中的第二个分量被称为“<strong class="jk hu"> <em class="lk">加权熵</em> </strong>”，我们将如下计算它。</p><p id="8071" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">(5/14 * 0.97)+(4/14 * 0)+(5/14 * 0.97)= 0.6928，其中第1和第3个数据集中有5个点，第2个数据集中有4个点。</p><p id="15c4" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">现在，我们可以很容易地发现信息增益为(<strong class="jk hu">0.94–0.69 = 0.25</strong>)。参考本文第一部分的图6，检查我们如何得到0.94。</p><h2 id="1a61" class="lm kf ht bd kg ln lo lp kk lq lr ls ko jr lt lu kq jv lv lw ks jz lx ly ku lz bi translated">信息增益的重要性</h2><p id="ab2a" class="pw-post-body-paragraph ji jj ht jk b jl kw iu jn jo kx ix jq jr ky jt ju jv kz jx jy jz la kb kc kd hb bi translated">现在，如果我们用不同的属性而不是outlook重复这个练习，我们会得到不同的IG值。下面的例子解释了同样的问题。</p><figure class="lc ld le lf fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ma"><img src="../Images/655fd76b42b35b931fac68fb18ee0561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GJibBQF_ZOQX8JNGm-L9Fw.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图2:具有其他属性的ig</figcaption></figure><p id="245a" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">很明显，如果我们使用'<em class="lk"> Outlook </em>'作为我们的根节点，并从那里开始生成树，那么将会产生很大的IG。</p><p id="cf79" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">因此IG告诉我们一个特征向量有多重要，以及在一组特征向量中哪个属性对区分类别最有用。</p><h1 id="84ff" class="ke kf ht bd kg kh ki kj kk kl km kn ko iz kp ja kq jc kr jd ks jf kt jg ku kv bi translated">基尼杂质</h1><p id="92ee" class="pw-post-body-paragraph ji jj ht jk b jl kw iu jn jo kx ix jq jr ky jt ju jv kz jx jy jz la kb kc kd hb bi translated"><strong class="jk hu"> Gini杂质</strong>是根据数据集中的类别分布对数据集中随机选择的元素进行<em class="lk">错误分类</em>的概率。计算方法如下</p><figure class="lc ld le lf fd hk er es paragraph-image"><div class="er es mb"><img src="../Images/15d92ad383ac512af042f5965ff5c51e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*imTbiOM2eT2SRAbhCdI3Iw.png"/></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图3:基尼系数杂质</figcaption></figure><p id="a6a6" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">其中C <em class="lk"> C </em>是类的数量，<em class="lk"> p </em> ( <em class="lk"> i </em>)是随机选取类<em class="lk"> i </em>的一个元素的概率。</p><p id="f7c9" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在训练决策树时，通过<strong class="jk hu">最大化基尼增益</strong>来选择最佳分裂，基尼增益是通过从原始杂质中减去分支的加权杂质来计算的。</p><p id="25cb" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">完美分裂数据集的基尼杂质为'<strong class="jk hu"> <em class="lk"> 0.5' </em> </strong>。</p><h1 id="f817" class="ke kf ht bd kg kh ki kj kk kl km kn ko iz kp ja kq jc kr jd ks jf kt jg ku kv bi translated"><strong class="ak">构建决策树</strong></h1><figure class="lc ld le lf fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mc"><img src="../Images/7aa896ee4489e84f729d2445dc7d2fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V85gVelRqPTbKbSyCuhrMQ.png"/></div></div><figcaption class="lg lh et er es li lj bd b be z dx translated">图4:打网球数据集的DT</figcaption></figure><p id="ae17" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">正如前面几节所解释的，<strong class="jk hu"> <em class="lk">信息增益</em> </strong>，在决策树中起着非常重要的作用。</p><p id="52b8" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">✒︎由于图2表明“<em class="lk"> Outlook </em>”属性具有最高的IG，我们将以此作为“根节点”来拆分数据集。</p><p id="8e03" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">︎✒︎在下一步中，我们为所有剩余的特征重新计算IG，不包括用作根节点的特征。</p><p id="33b2" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">✒︎我们将像这样分割数据，直到我们达到我们明确设置的点阈值(<strong class="jk hu"> <em class="lk"> max_depth </em> </strong>)或者我们只到达叶节点。</p><h2 id="9bc3" class="lm kf ht bd kg ln lo lp kk lq lr ls ko jr lt lu kq jv lv lw ks jz lx ly ku lz bi translated">过度拟合和<strong class="ak"> <em class="md">最大_深度</em> </strong></h2><p id="21a9" class="pw-post-body-paragraph ji jj ht jk b jl kw iu jn jo kx ix jq jr ky jt ju jv kz jx jy jz la kb kc kd hb bi translated">为了避免使用决策树过度拟合我们的数据，有时必须选择一个最大水平，超过这个水平我们将停止分割我们的数据。这个叫做<strong class="jk hu"> <em class="lk"> max_depth </em> </strong>。</p><p id="4709" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">当DT达到max_depth时，我们将对类标签进行多数投票并得出结论。</p><p id="f590" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">如果模型的深度非常高，模型的可解释性会受到严重影响，因为在到达叶节点之前，我们将有许多<em class="lk"> if-then-else </em>条件要遍历。</p><h2 id="0be3" class="lm kf ht bd kg ln lo lp kk lq lr ls ko jr lt lu kq jv lv lw ks jz lx ly ku lz bi translated">欠拟合和决策残肢</h2><p id="66c1" class="pw-post-body-paragraph ji jj ht jk b jl kw iu jn jo kx ix jq jr ky jt ju jv kz jx jy jz la kb kc kd hb bi translated">也可能发生这样的情况，我们将max_depth设置得非常低，并且在非常低的级别，我们必须对主节点进行多数投票。</p><p id="1b33" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">这样的叶节点不会是'<em class="lk">纯节点</em>'(既有正类标签又有负类标签)，因此我们可能必须采取多数投票来预测类标签。这可能会导致模型不匹配。</p><p id="beb2" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">一棵<strong class="jk hu"> <em class="lk"> max_depth = 1 </em> </strong>的决策树称为<em class="lk">决策树桩</em>。</p><p id="1f22" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">max_depth的默认值通常是30，但是必须使用交叉验证技术来决定。</p><h2 id="fcfd" class="lm kf ht bd kg ln lo lp kk lq lr ls ko jr lt lu kq jv lv lw ks jz lx ly ku lz bi translated">决策树的用例</h2><p id="1357" class="pw-post-body-paragraph ji jj ht jk b jl kw iu jn jo kx ix jq jr ky jt ju jv kz jx jy jz la kb kc kd hb bi translated">决策树背后的基本直觉被用于像随机森林这样的实际算法中，但是在应用决策树之前，我们应该知道一些关于数据准备的事情。</p><p id="e352" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">不平衡数据集</strong>:建议在应用DT算法之前，将不平衡数据集转换为平衡数据集。这可以通过使用上采样技术来实现，否则它会影响熵计算。</p><p id="bc16" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">维数灾难</strong>:当维数较大时，在每次迭代中计算每个特征的IG会增加算法的复杂度。对于分类特征，建议避免一个热编码，因为特征集将被扩展。分类特征应该转换成数字特征。</p><p id="8065" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">多类分类:</strong>多类分类不像其他分类算法那样改变决策树的性质，我们必须使用'<em class="lk">一对多'</em>技术。在决策树中，我们仍然可以得到超过2个类别标签的纯节点。如果我们在一个纯节点之前到达最大深度，我们简单地对类标签进行多数投票来确定随机变量的类。</p><p id="2b34" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">参考资料:</p><div class="hh hi ez fb hj me"><a href="https://slideplayer.com/slide/12489855/" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hu fi z dy mj ea eb mk ed ef hs bi translated">决策树:另一个例子- ppt下载</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">演示文稿正在加载。请稍等。为了让这个网站工作，我们记录用户数据并与处理器共享。要使用…</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">slideplayer.com</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms hp me"/></div></div></a></div><div class="hh hi ez fb hj me"><a href="https://knowhowspot.com/technology/ai-and-machine-learning/machine-learning-is-fun-part-14-bias-variance/" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hu fi z dy mj ea eb mk ed ef hs bi translated">机器学习很有趣——第14部分——偏差/差异</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">让我们试着解释一下今天的话题，而不去钻研统计数据。考虑到我们正在分析男性的身高…</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">knowhowspot.com</p></div></div><div class="mn l"><div class="mt l mp mq mr mn ms hp me"/></div></div></a></div><div class="hh hi ez fb hj me"><a href="https://victorzhou.com/blog/gini-impurity/" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hu fi z dy mj ea eb mk ed ef hs bi translated">基尼系数的简单解释——victorzhou.com</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">如果您在scikit-learn中查看DecisionTreeClassifier类的文档，您会看到类似这样的内容…</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">victorzhou.com</p></div></div><div class="mn l"><div class="mu l mp mq mr mn ms hp me"/></div></div></a></div><div class="hh hi ez fb hj me"><a href="https://www.appliedaicourse.com/course/11/Applied-Machine-learning-course" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab dw"><div class="mg ab mh cl cj mi"><h2 class="bd hu fi z dy mj ea eb mk ed ef hs bi translated">应用课程</h2><div class="ml l"><h3 class="bd b fi z dy mj ea eb mk ed ef dx translated">家庭课程应用机器学习在线课程</h3></div><div class="mm l"><p class="bd b fp z dy mj ea eb mk ed ef dx translated">www.appliedaicourse.com</p></div></div><div class="mn l"><div class="mv l mp mq mr mn ms hp me"/></div></div></a></div></div></div>    
</body>
</html>