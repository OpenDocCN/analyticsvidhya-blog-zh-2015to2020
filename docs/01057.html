<html>
<head>
<title>Unsupervised learning: Clustering Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无监督学习:聚类算法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/unsupervised-learning-clustering-algorithms-k-means-hierarchical-and-dbscan-clustering-3981c23efb93?source=collection_archive---------7-----------------------#2019-09-28">https://medium.com/analytics-vidhya/unsupervised-learning-clustering-algorithms-k-means-hierarchical-and-dbscan-clustering-3981c23efb93?source=collection_archive---------7-----------------------#2019-09-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="a030" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">k均值、分层和DBSCAN聚类</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/52b90f7f74edeeb997c209e2a303a57d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*du_JjNC1rjvF5Wgn3W4NZg.jpeg"/></div></div></figure><h1 id="5161" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">聚类:</h1><p id="ea2a" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">聚类是对相似数据点进行分组的过程，它是一种无监督的机器学习技术，无监督ML技术的主要目标是找到数据点之间的相似性并将它们分组在一起，无监督意味着我们有没有类别标签的数据。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es kx"><img src="../Images/51721677a55f7d1a0845dedfbf5a1036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*Dl3TvY58YFsEKbDFnWNbUg.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">聚类相似的数据点</figcaption></figure><p id="35c2" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated">有3种基本的聚类算法:</p><ol class=""><li id="41d4" class="lh li hi kd b ke lc kh ld kk lj ko lk ks ll kw lm ln lo lp bi translated">k均值</li><li id="6a68" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw lm ln lo lp bi translated">等级体系的</li><li id="050e" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw lm ln lo lp bi translated">DBSCAN(含噪声应用的基于密度的空间聚类)聚类技术。</li></ol><h1 id="2105" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated"><strong class="ak"> 1。K-Means聚类:</strong></h1><p id="d33a" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">它是最流行和最简单的聚类算法，该算法的主要目的是为给定的数据找到聚类，我们可以说K-Means聚类技术是一种基于质心的聚类技术，这里“K”意味着我们需要多少个聚类，为了确定正确的“K”，我们应该有领域知识，否则我们可以使用“肘”膝方法找到正确的K。</p><p id="dd96" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated"><strong class="kd hj">K均值的数学目标函数:</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lv"><img src="../Images/d58f0057d957f053efac4260a1b7a0be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*riInbzp5CiuMOOq8rldQ7w.png"/></div></figure><p id="20e1" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated">*目标函数的主要目标是最小化聚类内距离(对于每个点集中的每个聚类，最小化点与质心之间的距离)。</p><h2 id="2a10" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">劳埃德算法:</h2><ol class=""><li id="823c" class="lh li hi kd b ke kf kh ki kk mk ko ml ks mm kw lm ln lo lp bi translated">初始化:从数据集中随机选取“k”个点，假设它们是C1、C2、C3……Ck(质心)。</li><li id="9d7a" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw lm ln lo lp bi translated">任务:</li></ol><ul class=""><li id="89d0" class="lh li hi kd b ke lc kh ld kk lj ko lk ks ll kw mn ln lo lp bi translated">对于数据中的每个数据点<em class="mo"> (xi)，选择最近的Ci(质心)并将它们添加到set(sj)中。</em></li></ul><p id="954c" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated">3.重新计算/更新质心:</p><ul class=""><li id="3c21" class="lh li hi kd b ke lc kh ld kk lj ko lk ks ll kw mn ln lo lp bi translated">如下重新计算质心(平均值计算方法):</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mp"><img src="../Images/382a745b475847c70cbb43d7a0cd0479.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*_xRk3AL0aMvEE_fHSTDl-Q.jpeg"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">每组的平均值计算</figcaption></figure><p id="97c5" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated">4.重复步骤2和步骤3，直到质心变化不大(旧质心和新质心之间的微小差异)。</p><h2 id="a329" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">注意:</h2><p id="56f4" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">在劳埃德算法中有一个小问题，最终的聚类依赖于初始随机选择的点，为了克服这个问题，我们有另一个算法K-Means++。</p><ul class=""><li id="b967" class="lh li hi kd b ke lc kh ld kk lj ko lk ks ll kw mn ln lo lp bi translated">K-Means++算法是一种初始化质心的概率方法。</li></ul><h2 id="1310" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">K均值的限制:</h2><ul class=""><li id="8a7e" class="lh li hi kd b ke kf kh ki kk mk ko ml ks mm kw mn ln lo lp bi translated">集群将形成不同的大小和不同的形状。</li></ul><h1 id="c007" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated">2.分层聚类:</h1><p id="7108" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">有两种类型的分层聚类</p><ol class=""><li id="d233" class="lh li hi kd b ke lc kh ld kk lj ko lk ks ll kw lm ln lo lp bi translated">凝聚集群。</li><li id="1bc9" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw lm ln lo lp bi translated">可分聚类。</li></ol><h2 id="2018" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">1.凝聚聚类:</h2><ul class=""><li id="2a2c" class="lh li hi kd b ke kf kh ki kk mk ko ml ks mm kw mn ln lo lp bi translated">凝聚聚类技术是最流行的技术和迭代算法，在初始阶段，每个数据点是一个簇(簇的数量=数据点的数量)。</li><li id="f9ae" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">在每次迭代中，相似的聚类彼此合并，直到形成单个聚类。</li><li id="f2e0" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">它可以借助于基于树的(分层)树状图来可视化。</li></ul><p id="0c75" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated">树状图:(记录合并的顺序)。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mq"><img src="../Images/4e28dffed18e09bc0dc1bc41841998c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*540clwNpJWoDhZKEI0w3WA.png"/></div></figure><h2 id="3cf0" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">基本算法工作如下:</h2><ul class=""><li id="520f" class="lh li hi kd b ke kf kh ki kk mk ko ml ks mm kw mn ln lo lp bi translated">计算每个数据点的邻近矩阵(从我们的数据集中)。</li><li id="f68b" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">在计算邻近度矩阵之后，合并两个最近的聚类，然后更新邻近度。</li><li id="5c40" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">重复步骤2，直到形成单个簇。</li></ul><h2 id="3eb4" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">计算邻近方法:</h2><p id="1af7" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">在算法的第一阶段，我们计算邻近度矩阵，邻近度就是类间的相似度。根据4种方法计算簇间相似性。</p><ol class=""><li id="7b09" class="lh li hi kd b ke lc kh ld kk lj ko lk ks ll kw lm ln lo lp bi translated">最小值:</li></ol><ul class=""><li id="86dc" class="lh li hi kd b ke lc kh ld kk lj ko lk ks ll kw mn ln lo lp bi translated">两个聚类中两个较近点之间的最小距离，相似性(c1，c2) =最小相似性(pi，pj)其中pi ∈ c1和pj∈ c2聚类。</li></ul><p id="e5a2" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated">2.最大值:</p><ul class=""><li id="4a8f" class="lh li hi kd b ke lc kh ld kk lj ko lk ks ll kw mn ln lo lp bi translated">相似性(c1，c2) =最大相似性(pi，pj)其中pi ∈ c1和pj∈ c2聚类。</li></ul><p id="cb39" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated">3.组平均值:</p><ul class=""><li id="1f99" class="lh li hi kd b ke lc kh ld kk lj ko lk ks ll kw mn ln lo lp bi translated">相似度(c1，C2)=σsim(pi，pj)/|c1|x|c2|</li><li id="5e9d" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">其中pi ∈ c1和pj∈ c2群集。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mr"><img src="../Images/7109fc85b14fb99a7d99d7e53026ea50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5hjMUWHEePIleJUxa1F8dA.jpeg"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">邻近矩阵</figcaption></figure><h2 id="b509" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">分层聚类的局限性:</h2><ul class=""><li id="b1ae" class="lh li hi kd b ke kf kh ki kk mk ko ml ks mm kw mn ln lo lp bi translated">在层次聚类中没有我们直接求解的数学目标函数。</li><li id="79cd" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">空间和时间复杂度更多。</li></ul><h1 id="3682" class="jj jk hi bd jl jm jn jo jp jq jr js jt io ju ip jv ir jw is jx iu jy iv jz ka bi translated"><strong class="ak"> 3。DBSCAN(带噪声的应用程序的基于密度的空间聚类)</strong></h1><p id="081d" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">这是另一种流行的无监督学习技术。这里我们要知道如何测量密度。</p><p id="0c5f" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated">对于这个问题，答案是首先我们应该知道一些术语:</p><ul class=""><li id="91d4" class="lh li hi kd b ke lc kh ld kk lj ko lk ks ll kw mn ln lo lp bi translated">最小点数</li><li id="f52b" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">埃普西隆</li><li id="5f02" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">核心要点</li><li id="f387" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">边境点</li><li id="961f" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">和噪声点</li></ul><p id="930f" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated"><strong class="kd hj">一个点上的密度(p) </strong>:超球面内的点数，以p(点)为中心半径ε。</p><p id="477b" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated"><strong class="kd hj">稠密区:</strong>半径为ε的超球面，至少包含Min个点，这里MIN个点和ε值都是超参数。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ms"><img src="../Images/7b84e69c7c0a0a2f9d04e3fa75b09db7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-7bHQemQZiWIzgp6Kp9t7Q.png"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">核心、边界和噪声点</figcaption></figure><p id="7906" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated"><strong class="kd hj">核心点:</strong>如果p在其周围的ε半径上有&gt;个最小点，则p是核心点。</p><p id="e4bf" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated"><strong class="kd hj">边界点:</strong>一个点是边界点如果p不是核心点，那就意味着(p在ε半径上有&lt;个最小点)并且p属于邻域(q) q是核心点。</p><p id="ff50" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated"><strong class="kd hj">噪声点:</strong>如果一个点既不是核心点也不是边界点，则称该点为噪声点。</p><p id="e9b4" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated"><strong class="kd hj">密度边(连接):</strong>如果p和q是核心点，那么它们之间的距离dist(p，q) &lt; ε</p><h2 id="7d2b" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">基本算法工作如下:</h2><h2 id="bc51" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">第一步:</h2><ul class=""><li id="2d45" class="lh li hi kd b ke kf kh ki kk mk ko ml ks mm kw mn ln lo lp bi translated">我们必须记住，这里我们有两个超参数，我们必须使用肘(膝)方法来调整(最小点和ε)。</li><li id="a85f" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">让我们假设我们的数据集D包含n个数据点，D = (x1，x2，x3…xn)，对于我们的数据集中的每个点，将其标记为核心点、边界点和噪声点。</li></ul><h2 id="292a" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">第二步:</h2><ul class=""><li id="19af" class="lh li hi kd b ke kf kh ki kk mk ko ml ks mm kw mn ln lo lp bi translated">如果发现任何点是任何聚类的一部分，则该点必须在ε范围内，并返回ε距离内的所有主题。</li><li id="7067" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">对所有数据点进行聚类后，从数据集中移除噪声点(属于稀疏区域的噪声点)。</li></ul><h2 id="88d9" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">第三步:</h2><p id="4f5b" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">不与群集关联的每个核心点(p)。</p><ul class=""><li id="3c01" class="lh li hi kd b ke lc kh ld kk lj ko lk ks ll kw mn ln lo lp bi translated">用“p”创建一个新的集群。</li><li id="ef8f" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">将密度连接到“p”的所有点添加到一个新群集中。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mt"><img src="../Images/f74f5fad6c2bf9ff38cfea9954d2e2b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lyDd9Al4CIURtgkwWVGyow.jpeg"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">密度连接点</figcaption></figure><h2 id="f0da" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">第四步:</h2><ul class=""><li id="b220" class="lh li hi kd b ke kf kh ki kk mk ko ml ks mm kw mn ln lo lp bi translated">对于每个边界，点被分配给聚类中最近的核心点。</li></ul><h2 id="45e0" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">超参数调谐:</h2><p id="121c" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">我们有两个超参数，即最小点和ε，总是保持最小点大于我们数据集的维数，并且最小点应该更大，在某些情况下，我们必须请领域专家选择正确的ε值。</p><ul class=""><li id="9c28" class="lh li hi kd b ke lc kh ld kk lj ko lk ks ll kw mn ln lo lp bi translated">我们必须使用肘(膝)法找到正确的参数。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mu"><img src="../Images/bdd9bd4ea7c101b2db60cd944d9f4ba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OlC-FSDIaKZzWrc1LmNyBA.png"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">选择正确的ε值</figcaption></figure><p id="1af9" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated">x轴代表ε值，y轴代表误差度量。</p><h2 id="3b53" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">使用DBSCAN的优势:</h2><ul class=""><li id="7ef7" class="lh li hi kd b ke kf kh ki kk mk ko ml ks mm kw mn ln lo lp bi translated">它主要用于抵抗噪声。</li><li id="cebe" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">它甚至可以处理不同形状的簇。</li></ul><h2 id="fe97" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">缺点:</h2><ul class=""><li id="a00d" class="lh li hi kd b ke kf kh ki kk mk ko ml ks mm kw mn ln lo lp bi translated">在某些情况下，我们可以观察到密度的细微变化。</li><li id="adf3" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">如果ε值发生小的变化，那么它对群集密度的影响将会改变。</li><li id="3540" class="lh li hi kd b ke lq kh lr kk ls ko lt ks lu kw mn ln lo lp bi translated">它对超参数非常敏感。</li></ul><h2 id="5d50" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">聚类的应用</h2><p id="3ac3" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">1.电子商务:在像(Amazon.com，e-bay，Flipkart…如果我们的任务是根据相似客户的购买行为对他们进行分组，在这种情况下，聚类技术对于解决问题非常有用。</p><p id="fbe5" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated">2.图像分割:聚类技术对于给定图片中相似像素的分组/聚类是有用的，它在计算机视觉和对象检测算法中被大量使用。</p><p id="0dfd" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated">3.如果我们有文本数据，我们可以使用聚类技术轻松地对相似的文本内容进行分组。</p><h2 id="50be" class="lw jk hi bd jl lx ly lz jp ma mb mc jt kk md me jv ko mf mg jx ks mh mi jz mj bi translated">结论:</h2><p id="bdc6" class="pw-post-body-paragraph kb kc hi kd b ke kf ij kg kh ki im kj kk kl km kn ko kp kq kr ks kt ku kv kw hb bi translated">我们已经看到了聚类(无监督学习)的基本介绍和不同类型的聚类技术。</p><p id="6872" class="pw-post-body-paragraph kb kc hi kd b ke lc ij kg kh ld im kj kk le km kn ko lf kq kr ks lg ku kv kw hb bi translated">参考:我要非常感谢<a class="ae mv" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a>这门课程清晰地展示了机器学习算法。</p></div></div>    
</body>
</html>