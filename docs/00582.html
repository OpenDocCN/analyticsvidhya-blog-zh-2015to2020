<html>
<head>
<title>Web Scraping a Site with Pagination using BeautifulSoup</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用BeautifulSoup对带有分页的站点进行网络抓取</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/webscraping-a-site-with-pagination-using-beautifulsoup-fa0a09804445?source=collection_archive---------0-----------------------#2019-08-07">https://medium.com/analytics-vidhya/webscraping-a-site-with-pagination-using-beautifulsoup-fa0a09804445?source=collection_archive---------0-----------------------#2019-08-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="543c" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">本文描述了如何使用Python中的BeautifulSoup包抓取使用了分页的网站</h2></div><h2 id="7a10" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">什么是网络搜集？</h2><p id="8295" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">从网站提取数据的方法被称为网络抓取。它也被称为网络数据提取或网络收获。这项技术的历史不超过30年。</p><h2 id="2a43" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated"><strong class="ak"> 2)为什么要进行网络搜集？</strong></h2><p id="5b7d" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">web抓取的目的是从任何网站获取数据，从而节省大量收集数据/信息的人工劳动。例如，您可以从IMDB网站收集一部电影的所有评论。此后，您可以执行文本分析，从收集的大量评论中获得关于电影的见解。</p><h2 id="1377" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated"><strong class="ak"> 3)抓取网页的方法</strong></h2><p id="b44c" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">目前市场上有几种收费/免费的网络抓取工具。我们还可以编写自己的代码，使用python执行带有请求和漂亮汤的抓取。另一个名为Scrapy的包也可以用于相同的目的。在这篇文章中，我们将学习如何使用美丽的汤网页抓取。</p><p id="6340" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">链接到BeautifulSoup文档:<a class="ae kt" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank">https://www.crummy.com/software/BeautifulSoup/bs4/doc/</a></p><p id="c0f8" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">请求文档的链接:<a class="ae kt" href="http://docs.python-requests.org/en/master/" rel="noopener ugc nofollow" target="_blank">http://docs.python-requests.org/en/master/</a></p><h2 id="94aa" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated"><strong class="ak"> 4)所需工具和知识</strong></h2><ul class=""><li id="5e3a" class="ku kv hi jx b jy jz kb kc ji kw jm kx jq ky kn kz la lb lc bi translated">计算机编程语言</li><li id="be6a" class="ku kv hi jx b jy ld kb le ji lf jm lg jq lh kn kz la lb lc bi translated">超文本标记语言</li><li id="de49" class="ku kv hi jx b jy ld kb le ji lf jm lg jq lh kn kz la lb lc bi translated">Jupyter笔记本</li><li id="456b" class="ku kv hi jx b jy ld kb le ji lf jm lg jq lh kn kz la lb lc bi translated">要求</li><li id="9560" class="ku kv hi jx b jy ld kb le ji lf jm lg jq lh kn kz la lb lc bi translated">BeautifulSoup4</li><li id="f807" class="ku kv hi jx b jy ld kb le ji lf jm lg jq lh kn kz la lb lc bi translated">熊猫</li></ul><h2 id="0624" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated"><strong class="ak"> 5)用代码</strong>找到打击前的刮目的</h2><p id="5267" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">有数不清的网站提供了大量的数字或文本信息。在开始抓取代码之前，我们需要确定我们要从网站上抓取什么数据。这将帮助我们在编码时瞄准网页的那些部分。例如，opencodez.com为我们提供了几个关于各种技术的帖子。</p><p id="3f32" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">我想创建一个excel文档，包含所有文章的标题，短文，作者，日期和这些文章的网络链接。下面的屏幕截图显示了我需要在代码中定位的部分。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/d43cc55d6289eae2a7f9de8035abd758.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*D3pWbBYNzWYgvgMacZBnyQ.png"/></div></figure><h2 id="2e67" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated"><strong class="ak"> 6)了解网站结构(CSS/HTML) </strong></h2><p id="ee50" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">每个网站都有自己的结构，并且是使用HTML、CSS和JavaScript创建的。HTML由我们需要理解的标签组成。我们可以利用w3schools来获取一些关于HTML/CSS的基础知识。如果我们了解我们的目标网站的结构，这是有帮助的。在<a class="ae kt" href="https://www.elated.com/first-10-html-tags/" rel="noopener ugc nofollow" target="_blank">链接</a>中提供了与几个HTML标签相关的非常清晰的信息。</p><p id="9037" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">当我们在网站的任何部分点击右键后点击Inspect元素，我们可以看到它的结构。为了便于理解，请参见下面提供的与上面的快照相同的文章部分的截屏。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/da8733e11485f8bbcc8cf813571d3941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*QMpg98B_BONzb_wkDzdkDw.png"/></div></figure><p id="0afb" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">点击Inspect元素后，详细信息如下所示:</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/2e6726a791ba3d0809f4ffb92c3935c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*py5spKaAmPgbVnFMEo8-mw.png"/></div></figure><p id="7fe2" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">在快照中观察名为“Pavan”的部分及其元素标记span。</p><h2 id="47c6" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated"><strong class="ak"> 7)理解分页</strong></h2><p id="8d6d" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">opencodez网站由分页组成，即我们有几个页面来生成所有文章的集合。下面主页的第一个屏幕截图显示了网址和底部的页码。如果我们点击“最后”按钮，我们可以看到地址发生了变化，如第二个屏幕截图中突出显示的那样，并指向第15页。你可以观察到顶部的网页链接由“/page/15”组成，用来标记网页地址。我们将应用一些逻辑把这个网站刮到第15页。</p><p id="6a02" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">下面显示了主页的快照。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es lq"><img src="../Images/44e06bd6a3b63f4af2448599806e21aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*LZisFNe4gNPAgUuQ2Lm_kA.png"/></div></figure><p id="1cd3" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">一旦我们点击了最后一个按钮，网址就会改变，如下图所示，显示了网页链接的页码。在我们的情况下是15。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es lr"><img src="../Images/db7963f80f1cc5489c8a8f0e24e43b03.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*XwrZhw4eEG7gcxDaM87now.png"/></div></figure><h2 id="c378" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated"><strong class="ak"> 8)刮第一页开始</strong></h2><p id="9584" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">如果我们改变地址空间上的页码，您将能够看到从0到15的各种页面。我们将开始刮第一页，是https://www.opencodez.com/page/0的<a class="ae kt" href="https://www.opencodez.com/page/0" rel="noopener ugc nofollow" target="_blank"/>。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/2aee47e16caaa52dd3032cbb9f5d2e5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*KTXlgB_pWhzKCA1LCN_QYg.png"/></div></figure><p id="91d0" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">作为第一步，我们将向URL发送一个请求，并将其响应存储在一个名为response的变量中。这将发送所有的web代码作为响应。</p><pre class="lj lk ll lm fd ls lt lu lv aw lw bi"><span id="297f" class="ix iy hi lt b fi lx ly l lz ma">url = <a class="ae kt" href="https://www.opencodez.com/page/0" rel="noopener ugc nofollow" target="_blank">https://www.opencodez.com/page/0</a><br/>response= requests.get(url)</span></pre><p id="420b" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">然后我们要用html.parser解析HTML响应内容，姑且命名为soup。</p><pre class="lj lk ll lm fd ls lt lu lv aw lw bi"><span id="a700" class="ix iy hi lt b fi lx ly l lz ma">soup = BeautifulSoup(response.content,"html.parser")</span></pre><p id="5edc" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">现在让我们看看响应是什么样子的。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/ace93829343d9f9bbd074af381ed8b73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*msw7zsw7PyYukuGY4FtQmQ.png"/></div></figure><p id="822d" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">我们将使用美化功能来组织它。使用此命令后，查看输出是如何组织的。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/ba1bb78472b990a22a318e3445f1e414.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*0l5VNxfGPjmvhnEqZTRM-w.png"/></div></figure><p id="19a0" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">让我们观察页面部分，从那里我们必须提取细节。如果我们用我前面说过的右键单击方法检查它的元素，我们会看到any article的href和title的详细信息位于标签h2中，其中有一个名为title的类。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/31bd6ad599cc757bd6f07f51cd73a84a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*dIcICyx9NqZ5QRyXoTlckQ.png"/></div></figure><p id="3267" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">文章标题及其链接的HTML代码在上面用蓝色突出显示的截图中。</p><p id="3ac6" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">我们将通过下面的命令来拉它。</p><pre class="lj lk ll lm fd ls lt lu lv aw lw bi"><span id="2e72" class="ix iy hi lt b fi lx ly l lz ma">soup_title= soup.findAll("h2",{"class":"title"})</span><span id="c61c" class="ix iy hi lt b fi mb ly l lz ma">len(soup_title)</span></pre><p id="3d99" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">将拉出12个值的列表。通过使用如下命令，我们将从中提取所有文章的标题和hrefs。</p><pre class="lj lk ll lm fd ls lt lu lv aw lw bi"><span id="e331" class="ix iy hi lt b fi lx ly l lz ma">for x in range(12):<br/>   print(soup_title[x].a['href'])</span><span id="6aec" class="ix iy hi lt b fi mb ly l lz ma">for x in range(12):<br/>   print(soup_title[x].a['title'])</span></pre><p id="14d5" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">为了收集帖子的简短描述、作者和日期，我们需要瞄准包含名为“post-content image-caption-format-1”的类的div标签。</p><p id="1cb6" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">这些命令如下所示:</p><pre class="lj lk ll lm fd ls lt lu lv aw lw bi"><span id="4589" class="ix iy hi lt b fi lx ly l lz ma">soup_para= soup.findAll("div",{"class":"post-content image-caption-format-1"})</span><span id="07a2" class="ix iy hi lt b fi mb ly l lz ma">for x in range(12):<br/>    print((soup_para[x]).p.text.strip())</span><span id="aa57" class="ix iy hi lt b fi mb ly l lz ma">for x in range(12):<br/>    print(soup_para[x].a.text)</span><span id="0178" class="ix iy hi lt b fi mb ly l lz ma">soup_date= soup.findAll("span",{"class","thetime"})</span><span id="ef3d" class="ix iy hi lt b fi mb ly l lz ma">for x in range(12):<br/>    print(soup_date[x].text)</span></pre><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/77560208dcd1bc4712019388b5a64dd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*g7wZYSdVukKCHqY3Ei3IYA.png"/></div></figure><h2 id="4ec0" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">7)对代码的进一步解释</h2><p id="ca83" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">一旦为第一页收集了这些信息，我们需要应用loop从分页的其他页面中提取这些细节。我们将使用for循环并一个接一个地追加变量值。使用变量page_number并递增，以创建下一个网页地址，该地址将作为函数中的参数提供。在成功获得每个页面的所有数据后，我们创建一个包含所有变量的数据框架，并使用pandas包将其存储在csv中。</p><h2 id="664c" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">8)完整的代码</h2><pre class="lj lk ll lm fd ls lt lu lv aw lw bi"><span id="cffa" class="ix iy hi lt b fi lx ly l lz ma">#importing requests, BeautifulSoup, pandas, csv</span><span id="7396" class="ix iy hi lt b fi mb ly l lz ma">import bs4<br/>from bs4 import BeautifulSoup<br/>import requests<br/>import pandas<br/>from pandas import DataFrame<br/>import csv</span><span id="95c4" class="ix iy hi lt b fi mb ly l lz ma">#command to create a structure of csv file in which we will populate our scraped data</span><span id="f25f" class="ix iy hi lt b fi mb ly l lz ma">with open('Opencodez_Articles.csv', mode='w') as csv_file:<br/>   fieldnames = ['Link', 'Title', 'Para', 'Author', 'Date']<br/>   writer = csv.DictWriter(csv_file, fieldnames=fieldnames)<br/>   writer.writeheader()</span><span id="e781" class="ix iy hi lt b fi mb ly l lz ma">#Creating an empty lists of variables</span><span id="55f1" class="ix iy hi lt b fi mb ly l lz ma">article_link = []<br/>article_title = []<br/>article_para = []<br/>article_author = []<br/>article_date = []</span><span id="407f" class="ix iy hi lt b fi mb ly l lz ma">#Defining the opencodezscraping function</span><span id="6cad" class="ix iy hi lt b fi mb ly l lz ma">def opencodezscraping(webpage, page_number):<br/>   next_page = webpage + str(page_number)<br/>   response= requests.get(str(next_page)<br/>   soup = BeautifulSoup(response.content,"html.parser")<br/>   soup_title= soup.findAll("h2",{"class":"title"})<br/>   soup_para= soup.findAll("div",{"class":"post-content image-caption-format-1"})<br/>   soup_date= soup.findAll("span",{"class":"thetime"})</span><span id="506e" class="ix iy hi lt b fi mb ly l lz ma">   for x in range(len(soup_title)):<br/>      article_author.append(soup_para[x].a.text.strip())<br/>      article_date.append(soup_date[x].text.strip())<br/>      article_link.append(soup_title[x].a['href'])<br/>      article_title.append(soup_title[x].a['title'])<br/>      article_para.append(soup_para[x].p.text.strip())</span><span id="2dde" class="ix iy hi lt b fi mb ly l lz ma">   #Generating the next page url<br/>   if page_number &lt; 16:<br/>      page_number = page_number + 1<br/>      opencodezscraping(webpage, page_number)</span><span id="a3c5" class="ix iy hi lt b fi mb ly l lz ma">   #calling the function with relevant parameters<br/>   opencodezscraping(<a class="ae kt" href="https://www.opencodez.com/page/'" rel="noopener ugc nofollow" target="_blank">'https://www.opencodez.com/page/'</a>, 0)<br/>   <br/>   #creating the data frame and populating its data into the csv file</span><span id="650a" class="ix iy hi lt b fi mb ly l lz ma">data = { 'Article_Link': article_link,<br/>'Article_Title':article_title, 'Article_Para':article_para, 'Article_Author':article_author, 'Article_Date':article_date}</span><span id="cb66" class="ix iy hi lt b fi mb ly l lz ma">df = DataFrame(data, columns = ['Article_Link','Article_Title','Article_Para','Article_Author','Article_Date'])</span><span id="4c8d" class="ix iy hi lt b fi mb ly l lz ma">df.to_csv(r'C:\Users\**\**\OpenCodez_Articles.csv')</span></pre><h2 id="b15f" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">9)输出文件</h2><p id="1bdc" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">提供了一个csv文件快照。</p><figure class="lj lk ll lm fd ln er es paragraph-image"><div class="er es li"><img src="../Images/5fc0ca3c2af2684a54463578846dbd44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*KVbQj_bPvs7Cjr8HBwS9JA.png"/></div></figure><h2 id="aadf" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">10)以各种方式使用输出</h2><p id="5cd2" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">现在我们有了我们想要的csv。我们可以对这些数据进行一些探索性的数据分析，例如查看每位作者撰写的文章数量，或者对文章数量进行年度分析。我们还可以从简短描述专栏的语料库中创建一个单词云，以查看帖子中使用最多的单词。</p><h2 id="212d" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">11)网页抓取的注意事项</h2><p id="c5b9" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">然而，这种做法的合法性没有明确界定。网站通常会在使用条款和robots.txt文件中说明是否允许使用刮刀。因此，请注意不要涉足禁区，也不要在短时间内点击大量请求的URL，导致网站本身出现问题:)</p><p id="c6b4" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">页（page的缩写）S —这篇文章最初发表在<a class="ae kt" href="https://www.opencodez.com/web-development/web-scraping-using-beautiful-soup-part-1.htm" rel="noopener ugc nofollow" target="_blank">https://www.opencodez.com/</a></p></div></div>    
</body>
</html>