<html>
<head>
<title>Multiclass Text Categorization | 97 perc. accuracy | Bert Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多类文本分类。精确度| Bert模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multiclass-text-categorization-97-perc-accuracy-bert-model-2b97d8118903?source=collection_archive---------0-----------------------#2019-06-08">https://medium.com/analytics-vidhya/multiclass-text-categorization-97-perc-accuracy-bert-model-2b97d8118903?source=collection_archive---------0-----------------------#2019-06-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="6c80" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><em class="hi">这是基于</em> <a class="ae jh" href="https://www.kaggle.com/thebrownviking20/bert-multiclass-classification" rel="noopener ugc nofollow" target="_blank"> <em class="hi">的作品https://www . ka ggle . com/the brownviking 20/Bert-multi class-classification</em></a></p></blockquote><div class="ji jj ez fb jk jl"><a href="https://www.kaggle.com/thebrownviking20/bert-multiclass-classification" rel="noopener  ugc nofollow" target="_blank"><div class="jm ab dw"><div class="jn ab jo cl cj jp"><h2 class="bd hj fi z dy jq ea eb jr ed ef hh bi translated">伯特多类分类法</h2><div class="js l"><h3 class="bd b fi z dy jq ea eb jr ed ef dx translated">使用来自科幻小说文本语料库的数据</h3></div><div class="jt l"><p class="bd b fp z dy jq ea eb jr ed ef dx translated">www.kaggle.com</p></div></div><div class="ju l"><div class="jv l jw jx jy ju jz ka jl"/></div></div></a></div><h1 id="7b1a" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">1.项目概述</h1><h2 id="5523" class="kz kc hi bd kd la lb lc kh ld le lf kl lg lh li kp lj lk ll kt lm ln lo kx lp bi translated">1.1定义:</h2><p id="8440" class="pw-post-body-paragraph ii ij hi il b im lq io ip iq lr is it lg ls iw ix lj lt ja jb lm lu je jf jg hb bi translated">在当今世界<strong class="il hj">文本分类/分段/分类</strong>(例如呼叫中心的票据分类、电子邮件分类、日志类别检测等。)是一个常见的任务。面对如此庞大的数据，手动操作几乎是不可能的。让我们尝试使用机器学习和自然语言处理工具来自动解决这个问题。</p><h2 id="fc45" class="kz kc hi bd kd la lb lc kh ld le lf kl lg lh li kp lj lk ll kt lm ln lo kx lp bi translated">1.2问题陈述</h2><p id="d400" class="pw-post-body-paragraph ii ij hi il b im lq io ip iq lr is it lg ls iw ix lj lt ja jb lm lu je jf jg hb bi translated">BBC文章数据集(2126条记录)由两个特征文本和相关类别组成，即</p><ol class=""><li id="03a9" class="lv lw hi il b im in iq ir lg lx lj ly lm lz jg ma mb mc md bi translated">运动</li><li id="c125" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg ma mb mc md bi translated">商业</li><li id="b9ac" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg ma mb mc md bi translated">政治</li><li id="8c40" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg ma mb mc md bi translated">技术</li><li id="33a5" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg ma mb mc md bi translated">其他人</li></ol><p id="ba3c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated"><strong class="il hj">我们的任务是在上述数据集上训练一个多类分类模型。</strong></p><h2 id="f246" class="kz kc hi bd kd la lb lc kh ld le lf kl lg lh li kp lj lk ll kt lm ln lo kx lp bi translated">1.3指标</h2><p id="ef84" class="pw-post-body-paragraph ii ij hi il b im lq io ip iq lr is it lg ls iw ix lj lt ja jb lm lu je jf jg hb bi translated"><strong class="il hj">准确度</strong> —分类准确度是正确预测的数量占所有预测的比例</p><p id="15e9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated"><strong class="il hj">精度</strong> —精度(也称为正预测值)是检索到的实例中相关实例的分数</p><p id="7318" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated"><strong class="il hj"> F1_score </strong> —考虑测试的精确度和召回率来计算分数</p><p id="7334" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated"><strong class="il hj">召回</strong> —召回(也称为敏感度)是相关实例在相关实例总数中所占的比例</p><p id="fb99" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated"><strong class="il hj">为什么要采用这些指标？</strong> —我们将准确度、精确度、F1分数和召回率作为评估我们模型的指标，因为准确度将给出正确预测的估计。精度将为我们提供一个关于正类别预测值的估计，即我们的模型给出了多少相关结果。F1分数给出了精确度和召回率的综合评估。回忆将为我们提供与假阴性和真阳性类别识别结果相关的阳性类别预测。</p><h2 id="b7e1" class="kz kc hi bd kd la lb lc kh ld le lf kl lg lh li kp lj lk ll kt lm ln lo kx lp bi translated">1.4算法和技术:</h2><p id="909e" class="pw-post-body-paragraph ii ij hi il b im lq io ip iq lr is it lg ls iw ix lj lt ja jb lm lu je jf jg hb bi translated">对于这个用例，我们将使用<strong class="il hj"> BERT Base无案例模型</strong>。</p><div class="ji jj ez fb jk jl"><a href="https://github.com/google-research/bert" rel="noopener  ugc nofollow" target="_blank"><div class="jm ab dw"><div class="jn ab jo cl cj jp"><h2 class="bd hj fi z dy jq ea eb jr ed ef hh bi translated">谷歌研究/bert</h2><div class="js l"><h3 class="bd b fi z dy jq ea eb jr ed ef dx translated">用于BERT的TensorFlow代码和预训练模型。通过创建帐户为google-research/bert开发做出贡献…</h3></div><div class="jt l"><p class="bd b fp z dy jq ea eb jr ed ef dx translated">github.com</p></div></div><div class="ju l"><div class="mj l jw jx jy ju jz ka jl"/></div></div></a></div><figure class="mk ml mm mn fd mo er es paragraph-image"><div class="ab fe cl mp"><img src="../Images/350bc6aff57bc9ccd02d3af80f86c7bd.png" data-original-src="https://miro.medium.com/v2/0*6nWY5kMHxGOZzGdK.JPG"/></div><figcaption class="mr ms et er es mt mu bd b be z dx translated">伯特从ELMO那里获得了洞察力和计划，他怎样才能做得更好？[3]</figcaption></figure><p id="ce73" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated"><strong class="il hj">简介:</strong> BERT(来自变压器的双向编码器表示)是谷歌AI语言的研究人员最近发表的一篇论文。</p><div class="ji jj ez fb jk jl"><a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/" rel="noopener  ugc nofollow" target="_blank"><div class="jm ab dw"><div class="jn ab jo cl cj jp"><h2 class="bd hj fi z dy jq ea eb jr ed ef hh bi translated">自然语言处理的近期历史回顾</h2><div class="js l"><h3 class="bd b fi z dy jq ea eb jr ed ef dx translated">这篇文章讨论了NLP的主要最新进展，重点是基于神经网络的方法。这个帖子原本…</h3></div><div class="jt l"><p class="bd b fp z dy jq ea eb jr ed ef dx translated">ruder.io</p></div></div><div class="ju l"><div class="mv l jw jx jy ju jz ka jl"/></div></div></a></div><p id="19cc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated">塞巴斯蒂安·鲁德的博客中的一些美丽的类比。推荐！！</p><blockquote class="if ig ih"><p id="efc1" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">在伯特、ELMO、乌尔姆菲特和OpenAI transformer取得最新进展之前，我们曾经使用过Glove、Word2Vec等单词嵌入技术。</p><p id="d7cd" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">预训练的单词嵌入，如Glove、Word2Vec等。它们有巨大的影响力，但有一个主要的局限性:它们只在模型的第一层整合了以前的知识——网络的其余部分仍然需要从头开始训练。使用单词嵌入就像用只对边缘进行编码的预训练表示来初始化计算机视觉模型:它们对许多任务都有帮助，但它们无法捕捉可能更有用的高级信息。ULMFiT、ELMo和OpenAI transformer的最新进展的核心是一个关键的范式转变:从仅仅初始化我们模型的第一层到用分层表示来预训练整个模型。如果学习单词向量就像只学习边缘，这些方法就像学习特征的完整层次，从边缘到形状到高级语义概念。</p></blockquote><blockquote class="mw"><p id="ba76" class="mx my hi bd mz na nb nc nd ne nf jg dx translated"><strong class="ak">伯特获得了</strong>转移学习的力量</p></blockquote><h1 id="bc64" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km ng ko kp kq nh ks kt ku ni kw kx ky bi translated">2.数据探索:</h1><p id="deee" class="pw-post-body-paragraph ii ij hi il b im lq io ip iq lr is it lg ls iw ix lj lt ja jb lm lu je jf jg hb bi translated">数据集来源—<a class="ae jh" href="https://apc01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fstorage.googleapis.com%2Fdataset-uploader%2Fbbc%2Fbbc-text.csv&amp;data=02%7C01%7Csarthak.dargan%40infosys.com%7Cf792e3f3ca574e04d1b208d6fa02b4f3%7C63ce7d592f3e42cda8ccbe764cff5eb6%7C1%7C0%7C636971286858840027&amp;sdata=V%2BLol4CYKz8%2FjJjqaT%2FiwOuOxw9ywDKlwlHqxO0nwsU%3D&amp;reserved=0" rel="noopener ugc nofollow" target="_blank">https://storage . Google APIs . com/dataset-uploader/BBC/BBC-text . CSV</a></p><div class="ji jj ez fb jk jl"><a href="https://www.kaggle.com/yufengdev/bbc-fulltext-and-category" rel="noopener  ugc nofollow" target="_blank"><div class="jm ab dw"><div class="jn ab jo cl cj jp"><h2 class="bd hj fi z dy jq ea eb jr ed ef hh bi translated">BBC文章全文和分类</h2><div class="js l"><h3 class="bd b fi z dy jq ea eb jr ed ef dx translated">超过2000篇BBC全文文章的标题、正文和类别。</h3></div><div class="jt l"><p class="bd b fp z dy jq ea eb jr ed ef dx translated">www.kaggle.com</p></div></div><div class="ju l"><div class="nj l jw jx jy ju jz ka jl"/></div></div></a></div><p id="b7fa" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated">2.1加载数据集</p><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="0cbf" class="kz kc hi nl b fi np nq l nr ns">import pandas as pd<br/>datasetpath=r"../input/bbc-text.csv"<br/>data=pd.read_csv(datasetpath)</span></pre><p id="cfe1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated">2.2可视化数据分布</p><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="cadd" class="kz kc hi nl b fi np nq l nr ns">data['category'].value_counts()</span></pre><p id="9f66" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated">正如我们所看到的，数据均匀地分布在各个类中。没有必要追求统一的数据分布。</p><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="d3a9" class="kz kc hi nl b fi np nq l nr ns">sport            511<br/>business         510<br/>politics         417<br/>tech             401<br/>entertainment    386</span></pre><figure class="mk ml mm mn fd mo er es paragraph-image"><div class="er es nt"><img src="../Images/0a53b35fc3cf011462a9511f0af256d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*MYgVGsAVykAJRYECgo5fpA.png"/></div><figcaption class="mr ms et er es mt mu bd b be z dx translated">跨类的数据集分布</figcaption></figure><h1 id="36f0" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">3.实施:</h1><h2 id="127e" class="kz kc hi bd kd la lb lc kh ld le lf kl lg lh li kp lj lk ll kt lm ln lo kx lp bi translated">3.1使用标签编码器将文本标签映射到数字标签:</h2><p id="3ebc" class="pw-post-body-paragraph ii ij hi il b im lq io ip iq lr is it lg ls iw ix lj lt ja jb lm lu je jf jg hb bi translated">作为NLP模型，不能将文本作为例如“体育”、“商业”等。我们需要将它们转换成伯特模型能够理解的数字格式。</p><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="f9f1" class="kz kc hi nl b fi np nq l nr ns">from sklearn.preprocessing import LabelEncoder<br/>df2 = pd.DataFrame()<br/>df2["text"] = data["text"]<br/>df2["label"] = LabelEncoder().fit_transform(data["category"])</span></pre><h2 id="819f" class="kz kc hi bd kd la lb lc kh ld le lf kl lg lh li kp lj lk ll kt lm ln lo kx lp bi translated">3.2划分数据集以测试和训练数据集:</h2><p id="e335" class="pw-post-body-paragraph ii ij hi il b im lq io ip iq lr is it lg ls iw ix lj lt ja jb lm lu je jf jg hb bi translated">将数据集分为测试数据集和训练数据集。我们用80(训练)20(测试)的比例来划分。</p><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="25c7" class="kz kc hi nl b fi np nq l nr ns">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(df2["text"].values, df2["label"].values, test_size=0.2, random_state=42)</span></pre><h2 id="f55c" class="kz kc hi bd kd la lb lc kh ld le lf kl lg lh li kp lj lk ll kt lm ln lo kx lp bi translated">3.3下载Google的BERT BASE无案例模型:</h2><p id="7709" class="pw-post-body-paragraph ii ij hi il b im lq io ip iq lr is it lg ls iw ix lj lt ja jb lm lu je jf jg hb bi translated"><code class="du nu nv nw nl b"><a class="ae jh" href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="noopener ugc nofollow" target="_blank">BERT-Base, Uncased</a></code> : 12层，768隐，12头，110M参数</p><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="b850" class="kz kc hi nl b fi np nq l nr ns">!wget <a class="ae jh" href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="noopener ugc nofollow" target="_blank">https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip</a></span></pre><p id="b817" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated">正在解压缩的文件<code class="du nu nv nw nl b"><a class="ae jh" href="https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip" rel="noopener ugc nofollow" target="_blank">BERT-Base, Uncased</a></code>:</p><ul class=""><li id="2d3e" class="lv lw hi il b im in iq ir lg lx lj ly lm lz jg nx mb mc md bi translated">vocab.txt</li><li id="fdab" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg nx mb mc md bi translated">伯特配置文件</li><li id="edc4" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg nx mb mc md bi translated">伯特_模型. ckpt</li></ul><h2 id="8993" class="kz kc hi bd kd la lb lc kh ld le lf kl lg lh li kp lj lk ll kt lm ln lo kx lp bi translated">3.4从Google Github资源库下载BERT helper文件:</h2><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="81aa" class="kz kc hi nl b fi np nq l nr ns">!wget <a class="ae jh" href="https://raw.githubusercontent.com/google-research/bert/master/modeling.py" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/google-research/bert/master/modeling.py</a> <br/>!wget <a class="ae jh" href="https://raw.githubusercontent.com/google-research/bert/master/optimization.py" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/google-research/bert/master/optimization.py</a> <br/>!wget <a class="ae jh" href="https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/google-research/bert/master/run_classifier.py</a> <br/>!wget <a class="ae jh" href="https://raw.githubusercontent.com/google-research/bert/master/tokenization.py" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/google-research/bert/master/tokenization.py</a></span></pre><h2 id="cdfe" class="kz kc hi bd kd la lb lc kh ld le lf kl lg lh li kp lj lk ll kt lm ln lo kx lp bi translated">3.5设置BERT配置</h2><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="f804" class="kz kc hi nl b fi np nq l nr ns">import modeling<br/>import optimization<br/>import run_classifier<br/>import tokenization</span></pre><p id="9037" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated">将下载的BERT基本模型解压到MODEL文件夹中。配置输出文件夹和预训练模型目录</p><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="edab" class="kz kc hi nl b fi np nq l nr ns">import zipfile</span><span id="8c11" class="kz kc hi nl b fi ny nq l nr ns">folder = 'model_folder'<br/>with zipfile.ZipFile("uncased_L-12_H-768_A-12.zip","r") as zip_ref:<br/>    zip_ref.extractall(folder)</span><span id="d808" class="kz kc hi nl b fi ny nq l nr ns">BERT_MODEL = 'uncased_L-12_H-768_A-12'<br/>BERT_PRETRAINED_DIR = f'<strong class="nl hj">{folder}</strong>/uncased_L-12_H-768_A-12'<br/>OUTPUT_DIR = f'<strong class="nl hj">{folder}</strong>/outputs'<br/>print(f'&gt;&gt; Model output directory: <strong class="nl hj">{OUTPUT_DIR}</strong>')<br/>print(f'&gt;&gt;  BERT pretrained directory: <strong class="nl hj">{BERT_PRETRAINED_DIR}</strong>')</span></pre><h2 id="3365" class="kz kc hi bd kd la lb lc kh ld le lf kl lg lh li kp lj lk ll kt lm ln lo kx lp bi translated">3.6为训练和测试数据集创建示例</h2><p id="00db" class="pw-post-body-paragraph ii ij hi il b im lq io ip iq lr is it lg ls iw ix lj lt ja jb lm lu je jf jg hb bi translated">create_examples函数获取数据集(训练/测试)、set_type("train"/ "test ")和标签(用于训练数据集)，该函数从InputExample类(来自<a class="ae jh" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">https://github.com/google-research/bert</a>)调用run_classifier函数。它采用guid(例如唯一的id，在我们的例子中，我们传递“train”或“test”)、text_a(序列的未标记文本)、label(用于训练数据集)和text_b(可选)。对于我们的例子，text_b将是None。run_classifier函数返回追加到示例列表的InputExample对象。</p><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="a362" class="kz kc hi nl b fi np nq l nr ns">import os<br/>import tensorflow as tf</span><span id="2fb0" class="kz kc hi nl b fi ny nq l nr ns">def create_examples(lines, set_type, labels=None):<br/><em class="ik">#Generate data for the BERT model</em><br/>    guid = f'<strong class="nl hj">{set_type}</strong>'<br/>    examples = []<br/>    if guid == 'train':<br/>        for line, label <strong class="nl hj">in</strong> zip(lines, labels):<br/>            text_a = line<br/>            label = str(label)<br/>            examples.append(<br/>              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))<br/>    else:<br/>        for line <strong class="nl hj">in</strong> lines:<br/>            text_a = line<br/>            label = '0'<br/>            examples.append(<br/>              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))<br/>    return examples</span></pre><h2 id="aeba" class="kz kc hi bd kd la lb lc kh ld le lf kl lg lh li kp lj lk ll kt lm ln lo kx lp bi translated">3.7配置BERT超参数</h2><ul class=""><li id="3741" class="lv lw hi il b im lq iq lr lg nz lj oa lm ob jg nx mb mc md bi translated"><strong class="il hj"> TRAIN_BATCH_SIZE </strong> -用于训练的总批量。</li><li id="2644" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg nx mb mc md bi translated"><strong class="il hj">EVAL _批量_大小</strong> -评估的总批量。</li><li id="df75" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg nx mb mc md bi translated"><strong class="il hj">LEARNING _ RATE</strong>-Adam的初始学习速率。</li><li id="ebcc" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg nx mb mc md bi translated"><strong class="il hj">训练次数</strong> -要执行的训练次数。</li><li id="b7eb" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg nx mb mc md bi translated"><strong class="il hj">热身_比例</strong>-执行线性学习率热身的训练比例。0.1表示10%</li><li id="e997" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg nx mb mc md bi translated"><strong class="il hj">MAX _ SEQ _ LENGTH</strong>——分词后的最大总输入序列长度。长于此长度的序列将被截断，短于此长度的序列将被填充。</li></ul><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="2441" class="kz kc hi nl b fi np nq l nr ns"><em class="ik"># Model Hyper Parameters</em><br/>TRAIN_BATCH_SIZE = 32<br/>EVAL_BATCH_SIZE = 8<br/>LEARNING_RATE = 1e-5<br/>NUM_TRAIN_EPOCHS = 3.0<br/>WARMUP_PROPORTION = 0.1<br/>MAX_SEQ_LENGTH = 50</span></pre><h2 id="aa3b" class="kz kc hi bd kd la lb lc kh ld le lf kl lg lh li kp lj lk ll kt lm ln lo kx lp bi translated">3.8配置BERT配置</h2><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="8973" class="kz kc hi nl b fi np nq l nr ns"><em class="ik"># Model configs</em><br/>SAVE_CHECKPOINTS_STEPS = 100000 <em class="ik">#if you wish to finetune a model on a larger dataset, use larger interval</em><br/><em class="ik"># each checpoint weights about 1,5gb</em><br/>ITERATIONS_PER_LOOP = 100000<br/>NUM_TPU_CORES = 8<br/>VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')<br/>CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')<br/>INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')<br/>DO_LOWER_CASE = BERT_MODEL.startswith('uncased')</span></pre><h2 id="4385" class="kz kc hi bd kd la lb lc kh ld le lf kl lg lh li kp lj lk ll kt lm ln lo kx lp bi translated">3.9使用TPU估计器API训练模型:</h2><div class="ji jj ez fb jk jl"><a rel="noopener follow" target="_blank" href="/tensorflow/how-to-write-a-custom-estimator-model-for-the-cloud-tpu-7d8bd9068c26"><div class="jm ab dw"><div class="jn ab jo cl cj jp"><h2 class="bd hj fi z dy jq ea eb jr ed ef hh bi translated">如何为云TPU编写自定义估算器模型</h2><div class="js l"><h3 class="bd b fi z dy jq ea eb jr ed ef dx translated">作者Lak Lakshmanan (@lak_gcp)，谷歌云平台技术负责人</h3></div><div class="jt l"><p class="bd b fp z dy jq ea eb jr ed ef dx translated">medium.com</p></div></div><div class="ju l"><div class="oc l jw jx jy ju jz ka jl"/></div></div></a></div><div class="ji jj ez fb jk jl"><a href="https://cloud.google.com/tpu/docs/using-estimator-api" rel="noopener  ugc nofollow" target="_blank"><div class="jm ab dw"><div class="jn ab jo cl cj jp"><h2 class="bd hj fi z dy jq ea eb jr ed ef hh bi translated">在云TPU |云TPU |谷歌云上使用TPU估算器API</h2><div class="js l"><h3 class="bd b fi z dy jq ea eb jr ed ef dx translated">无论您的企业是刚刚踏上数字化转型之旅，还是已经走上数字化转型之路，谷歌云的解决方案…</h3></div><div class="jt l"><p class="bd b fp z dy jq ea eb jr ed ef dx translated">cloud.google.com</p></div></div></div></a></div><p id="0121" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated"><strong class="il hj"> TPUEstimator通过处理大量低层次、硬件特定的细节，简化了在云TPU上运行的模型</strong>。使用TPUEstimator编写的模型可以跨CPU、GPU、单个TPU设备和整个TPU pods工作，<strong class="il hj">通常不需要修改代码</strong>。TPUEstimator还通过自动为您执行一些优化，使您更容易获得最佳性能。</p></div><div class="ab cl od oe gp of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="hb hc hd he hf"><ul class=""><li id="7cae" class="lv lw hi il b im in iq ir lg lx lj ly lm lz jg nx mb mc md bi translated">初始化BERT tokenizer，提供从BERT预训练模型压缩文件夹中获得的vocab文件，这里我们将“do_lower_case”标记设为True，因为我们使用的是未封装的模型。</li><li id="6f1c" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg nx mb mc md bi translated">train_examples是上面提到的InputExample类的对象列表。</li></ul><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="89ab" class="kz kc hi nl b fi np nq l nr ns">label_list = [str(num) for num <strong class="nl hj">in</strong> range(8)]<br/>tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)<br/>train_examples = create_examples(X_train, 'train', labels=y_train)</span></pre><ul class=""><li id="e683" class="lv lw hi il b im in iq ir lg lx lj ly lm lz jg nx mb mc md bi translated">TPUEstimatorAPI需要run_config。它获取使用的集群、保存检查点的输出目录、经过多少检查点以及每个循环的TPU共享内核和迭代次数。</li></ul><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="b979" class="kz kc hi nl b fi np nq l nr ns">tpu_cluster_resolver = None </span><span id="a606" class="kz kc hi nl b fi ny nq l nr ns">run_config = tf.contrib.tpu.RunConfig(<br/>    cluster=tpu_cluster_resolver,<br/>    model_dir=OUTPUT_DIR,<br/>    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,<br/>    tpu_config=tf.contrib.tpu.TPUConfig(<br/>        iterations_per_loop=ITERATIONS_PER_LOOP,<br/>        num_shards=NUM_TPU_CORES,<br/>        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))</span><span id="95bd" class="kz kc hi nl b fi ny nq l nr ns">num_train_steps = int(<br/>    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)</span><span id="a30a" class="kz kc hi nl b fi ny nq l nr ns">num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)</span></pre><p id="6947" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated">这是伯特模型的核心。模型函数生成器接受特征、标签和模式，并为TPUEstimator返回“model_fn”闭包</p><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="b4cd" class="kz kc hi nl b fi np nq l nr ns">model_fn = run_classifier.model_fn_builder(<br/>    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),<br/>    num_labels=len(label_list),<br/>    init_checkpoint=INIT_CHECKPOINT,<br/>    learning_rate=LEARNING_RATE,<br/>    num_train_steps=num_train_steps,<br/>    num_warmup_steps=num_warmup_steps,<br/>    use_tpu=False, <em class="ik">#If False training will fall on CPU or GPU, depending on what is available  </em><br/>    use_one_hot_embeddings=True)</span></pre><ul class=""><li id="bd60" class="lv lw hi il b im in iq ir lg lx lj ly lm lz jg nx mb mc md bi translated"><code class="du nu nv nw nl b"><a class="ae jh" href="https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimator" rel="noopener ugc nofollow" target="_blank">TPUEstimator</a></code>包装计算(<code class="du nu nv nw nl b">model_fn</code>)并将其分配给所有可用的云TPU核心。</li><li id="ee9c" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg nx mb mc md bi translated">传递从上一步获得的run_config。</li></ul><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="4dc9" class="kz kc hi nl b fi np nq l nr ns">estimator = tf.contrib.tpu.TPUEstimator(<br/>    use_tpu=False, <em class="ik">#If False training will fall on CPU or GPU, depending on what is available </em><br/>    model_fn=model_fn,<br/>    config=run_config,<br/>    train_batch_size=TRAIN_BATCH_SIZE,<br/>    eval_batch_size=EVAL_BATCH_SIZE)</span></pre><ul class=""><li id="5339" class="lv lw hi il b im in iq ir lg lx lj ly lm lz jg nx mb mc md bi translated">将上述步骤中获得的InputExample对象转换为BERT输入特征。</li></ul><figure class="mk ml mm mn fd mo er es paragraph-image"><div class="er es ok"><img src="../Images/a440cd6d95fa1d5f37a7ccb114a4ff66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*lE21QJTaCOhdV6jXNNKMww.png"/></div><figcaption class="mr ms et er es mt mu bd b be z dx translated">伯特输入表示</figcaption></figure><ul class=""><li id="8bfe" class="lv lw hi il b im in iq ir lg lx lj ly lm lz jg nx mb mc md bi translated">输入功能列表—</li></ul><ol class=""><li id="14a5" class="lv lw hi il b im in iq ir lg lx lj ly lm lz jg ma mb mc md bi translated"><strong class="il hj">输入标识</strong> —输入标识是根据BERT vocab与每个令牌相关联的标识</li><li id="13fd" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg ma mb mc md bi translated"><strong class="il hj"> input_mask- </strong> input_mask区分填充和实令牌。</li><li id="1d22" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg ma mb mc md bi translated"><strong class="il hj">Segment _ Id</strong>-Segment Id将是0的列表，因为分类任务中只有一个文本。</li><li id="eb60" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg ma mb mc md bi translated"><strong class="il hj">标签标识</strong> -标签标识对应于标签编码器类别</li></ol><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="cea4" class="kz kc hi nl b fi np nq l nr ns">import datetime</span><span id="e7b6" class="kz kc hi nl b fi ny nq l nr ns">print('Please wait...')<br/>train_features = run_classifier.convert_examples_to_features(<br/>    train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)</span></pre><p id="8559" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated">创建一个要传递给TPUEstimator的“input_fn”闭包。<code class="du nu nv nw nl b">input_fn</code>函数对输入管道进行建模。</p><p id="aa70" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated">将<code class="du nu nv nw nl b">input_fn</code>和列车步数传递给estimator.train</p><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="cd39" class="kz kc hi nl b fi np nq l nr ns">train_input_fn = run_classifier.input_fn_builder(<br/>    features=train_features,<br/>    seq_length=MAX_SEQ_LENGTH,<br/>    is_training=True,<br/>    drop_remainder=True)</span><span id="e915" class="kz kc hi nl b fi ny nq l nr ns">estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)<br/>print('&gt;&gt; Finished training at <strong class="nl hj">{}</strong>'.format(datetime.datetime.now()))</span></pre><h2 id="5a83" class="kz kc hi bd kd la lb lc kh ld le lf kl lg lh li kp lj lk ll kt lm ln lo kx lp bi translated">步骤3.10使用TPU估计器API对测试数据集进行预测:</h2><p id="5f97" class="pw-post-body-paragraph ii ij hi il b im lq io ip iq lr is it lg ls iw ix lj lt ja jb lm lu je jf jg hb bi translated">如前所述，首先测试数据集将被转换为InputExample类对象，这些对象将被进一步传递，以从中获取特性。输入函数生成器将为TPU估计器创建输入管道，并最终作为输入提供给estimator.predict</p><p id="3ef3" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi translated">结果是应用argmax的每个类的概率，即找到具有最大概率的类。</p><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="b148" class="kz kc hi nl b fi np nq l nr ns">predict_examples = create_examples(X_test, 'test')</span><span id="b0d6" class="kz kc hi nl b fi ny nq l nr ns">predict_features = run_classifier.convert_examples_to_features(<br/>    predict_examples, label_list, MAX_SEQ_LENGTH, tokenizer)</span><span id="0abe" class="kz kc hi nl b fi ny nq l nr ns">predict_input_fn = input_fn_builder(<br/>    features=predict_features,<br/>    seq_length=MAX_SEQ_LENGTH,<br/>    is_training=False,<br/>    drop_remainder=False)</span><span id="0d04" class="kz kc hi nl b fi ny nq l nr ns">result = estimator.predict(input_fn=predict_input_fn)</span><span id="cc2b" class="kz kc hi nl b fi ny nq l nr ns">import numpy as np<br/>preds = []<br/>for prediction <strong class="nl hj">in</strong> result:<br/>      preds.append(np.argmax(prediction['probabilities']))<br/>print(len(preds))</span></pre><h1 id="4d9a" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">4.结果</h1><ul class=""><li id="8996" class="lv lw hi il b im lq iq lr lg nz lj oa lm ob jg nx mb mc md bi translated">准确性报告</li></ul><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="355f" class="kz kc hi nl b fi np nq l nr ns">from sklearn.metrics import accuracy_score</span><span id="312f" class="kz kc hi nl b fi ny nq l nr ns">print("Accuracy of BERT is:",accuracy_score(y_test,preds))</span><span id="1754" class="kz kc hi nl b fi ny nq l nr ns">Accuracy of BERT is: 0.9752808988764045</span></pre><ul class=""><li id="eb74" class="lv lw hi il b im in iq ir lg lx lj ly lm lz jg nx mb mc md bi translated">分类报告</li></ul><pre class="mk ml mm mn fd nk nl nm nn aw no bi"><span id="95c7" class="kz kc hi nl b fi np nq l nr ns">from sklearn.metrics import classification_report</span><span id="b624" class="kz kc hi nl b fi ny nq l nr ns">print(classification_report(y_test,preds))</span><span id="dd5d" class="kz kc hi nl b fi ny nq l nr ns">precision    recall  f1-score   support</span><span id="d9f7" class="kz kc hi nl b fi ny nq l nr ns">           0       0.97      0.96      0.97       101<br/>           1       1.00      0.99      0.99        81<br/>           2       0.94      0.98      0.96        83<br/>           3       0.98      0.99      0.98        98<br/>           4       0.99      0.96      0.98        82</span><span id="43c9" class="kz kc hi nl b fi ny nq l nr ns">   micro avg       0.98      0.98      0.98       445<br/>   macro avg       0.98      0.98      0.98       445<br/>weighted avg       0.98      0.98      0.98       445</span></pre><blockquote class="if ig ih"><p id="3f98" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><strong class="il hj"> <em class="hi">在这个数据集上提到的过去的工作最多达到95.22的准确度。BERT基本模型相同，没有任何预处理，达到了97.75的准确率。</em>T15】</strong></p></blockquote><h1 id="c8e4" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">5.未来工作:</h1><ul class=""><li id="a04b" class="lv lw hi il b im lq iq lr lg nz lj oa lm ob jg nx mb mc md bi translated">探索数据的预处理步骤。</li><li id="53de" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg nx mb mc md bi translated">探索其他模型作为基线。</li><li id="d6ae" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg nx mb mc md bi translated">让这个笔记本更具知识性和说明性。</li><li id="a3a6" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg nx mb mc md bi translated">在数据探索和特征工程方面做出更大的努力。</li><li id="8435" class="lv lw hi il b im me iq mf lg mg lj mh lm mi jg nx mb mc md bi translated">对TPUEstimatorAPI的更好解释。</li></ul><h1 id="96c0" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">6.参考</h1><p id="ab6a" class="pw-post-body-paragraph ii ij hi il b im lq io ip iq lr is it lg ls iw ix lj lt ja jb lm lu je jf jg hb bi">1.</p><div class="ji jj ez fb jk jl"><a href="https://www.kaggle.com/thebrownviking20/bert-multiclass-classification" rel="noopener  ugc nofollow" target="_blank"><div class="jm ab dw"><div class="jn ab jo cl cj jp"><h2 class="bd hj fi z dy jq ea eb jr ed ef hh bi translated">伯特多类分类法</h2><div class="js l"><h3 class="bd b fi z dy jq ea eb jr ed ef dx translated">使用来自科幻小说文本语料库的数据</h3></div><div class="jt l"><p class="bd b fp z dy jq ea eb jr ed ef dx translated">www.kaggle.com</p></div></div><div class="ju l"><div class="jv l jw jx jy ju jz ka jl"/></div></div></a></div><p id="1437" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi">2.</p><div class="ji jj ez fb jk jl"><a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/" rel="noopener  ugc nofollow" target="_blank"><div class="jm ab dw"><div class="jn ab jo cl cj jp"><h2 class="bd hj fi z dy jq ea eb jr ed ef hh bi translated">自然语言处理的近期历史回顾</h2><div class="js l"><h3 class="bd b fi z dy jq ea eb jr ed ef dx translated">这篇文章讨论了NLP的主要最新进展，重点是基于神经网络的方法。这个帖子原本…</h3></div><div class="jt l"><p class="bd b fp z dy jq ea eb jr ed ef dx translated">ruder.io</p></div></div><div class="ju l"><div class="mv l jw jx jy ju jz ka jl"/></div></div></a></div><p id="1029" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi">3.</p><div class="ji jj ez fb jk jl"><a href="https://www.google.com/imgres?imgurl=https%3A%2F%2Fupload.wikimedia.org%2Fwikipedia%2Fen%2Ff%2Ff1%2FBert_and_Ernie.JPG&amp;imgrefurl=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FBert_%28Sesame_Street%29&amp;docid=CuWLZM9iu4n0aM&amp;tbnid=aVTOsvRPXC0OhM%3A&amp;vet=10ahUKEwjWi-6K84PjAhUQ7XMBHVMmDq8QMwh9KBQwFA..i&amp;w=365&amp;h=273&amp;bih=524&amp;biw=1242&amp;q=bert%20elmo&amp;ved=0ahUKEwjWi-6K84PjAhUQ7XMBHVMmDq8QMwh9KBQwFA&amp;iact=mrc&amp;uact=8" rel="noopener  ugc nofollow" target="_blank"><div class="jm ab dw"><div class="jn ab jo cl cj jp"><h2 class="bd hj fi z dy jq ea eb jr ed ef hh bi translated">https://upload . wikimedia . org/Wikipedia/en/f/f1/Bert _ and _ Ernie的Google图片结果。使用jpeg文件交换格式存储的编码图像文件扩展名</h2><div class="js l"><h3 class="bd b fi z dy jq ea eb jr ed ef dx translated">编辑描述</h3></div><div class="jt l"><p class="bd b fp z dy jq ea eb jr ed ef dx translated">www.google.com</p></div></div><div class="ju l"><div class="ol l jw jx jy ju jz ka jl"/></div></div></a></div><p id="dec7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi">4.</p><div class="ji jj ez fb jk jl"><a href="https://cloud.google.com/tpu/docs/using-estimator-api" rel="noopener  ugc nofollow" target="_blank"><div class="jm ab dw"><div class="jn ab jo cl cj jp"><h2 class="bd hj fi z dy jq ea eb jr ed ef hh bi translated">在云TPU |云TPU |谷歌云上使用TPU估算器API</h2><div class="js l"><h3 class="bd b fi z dy jq ea eb jr ed ef dx translated">无论您的企业是刚刚踏上数字化转型之旅，还是已经走上数字化转型之路，谷歌云的解决方案…</h3></div><div class="jt l"><p class="bd b fp z dy jq ea eb jr ed ef dx translated">cloud.google.com</p></div></div></div></a></div><p id="5f52" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi">5.</p><div class="ji jj ez fb jk jl"><a rel="noopener follow" target="_blank" href="/tensorflow/how-to-write-a-custom-estimator-model-for-the-cloud-tpu-7d8bd9068c26"><div class="jm ab dw"><div class="jn ab jo cl cj jp"><h2 class="bd hj fi z dy jq ea eb jr ed ef hh bi translated">如何为云TPU编写自定义估算器模型</h2><div class="js l"><h3 class="bd b fi z dy jq ea eb jr ed ef dx translated">作者Lak Lakshmanan (@lak_gcp)，谷歌云平台技术负责人</h3></div><div class="jt l"><p class="bd b fp z dy jq ea eb jr ed ef dx translated">medium.com</p></div></div><div class="ju l"><div class="oc l jw jx jy ju jz ka jl"/></div></div></a></div><p id="7b78" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it lg iv iw ix lj iz ja jb lm jd je jf jg hb bi">6.</p><div class="ji jj ez fb jk jl"><a href="https://github.com/google-research/bert" rel="noopener  ugc nofollow" target="_blank"><div class="jm ab dw"><div class="jn ab jo cl cj jp"><h2 class="bd hj fi z dy jq ea eb jr ed ef hh bi translated">谷歌研究/bert</h2><div class="js l"><h3 class="bd b fi z dy jq ea eb jr ed ef dx translated">用于BERT的TensorFlow代码和预训练模型。通过创建帐户为google-research/bert开发做出贡献…</h3></div><div class="jt l"><p class="bd b fp z dy jq ea eb jr ed ef dx translated">github.com</p></div></div><div class="ju l"><div class="mj l jw jx jy ju jz ka jl"/></div></div></a></div></div></div>    
</body>
</html>