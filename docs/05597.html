<html>
<head>
<title>Classifying Twitter Disaster Response Messages Trough NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过自然语言处理对Twitter灾难响应消息进行分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/classifying-twitter-disaster-response-messages-f551e9e60c71?source=collection_archive---------25-----------------------#2020-04-26">https://medium.com/analytics-vidhya/classifying-twitter-disaster-response-messages-f551e9e60c71?source=collection_archive---------25-----------------------#2020-04-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/861b2487059da3bd76ba97d8d7744455.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rt-87PjMn2tWrPsrwccPuA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">女人坐在棕色混凝土地面旁边的棕色柳条篮子[1]</figcaption></figure><p id="95e9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">由于新冠肺炎病毒，我们正面临着一场全球健康危机——这场危机正在杀人，传播人类痛苦，颠覆人们的生活。但这不仅仅是一场健康危机。这是一场人类、经济和社会危机。</p><p id="30b4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在疫情或自然灾害期间，快速响应人们的需求至关重要，这些需求通过各种渠道发送的信息来表达。<strong class="iw hj">使用NLP的机器学习算法可以帮助对消息进行分类，以便它们可以被发送到适当的救灾机构</strong>，这些机构负责医疗援助、水、住所、食物、物流等。本文使用由<a class="ae js" href="https://appen.com/" rel="noopener ugc nofollow" target="_blank">图八</a>提供的包含在灾难事件期间发送的真实Twitter消息的数据集，来为对灾难消息进行分类的API建立模型。</p><p id="3533" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">该代码分为三个部分:</p><ul class=""><li id="ac64" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated"><strong class="iw hj"> ETL管道</strong>:管道读入数据，清理并存储在SQL数据库中。该脚本合并消息和类别数据集，将类别列拆分为单独的列，将值转换为二进制，并删除重复的列。</li><li id="ac92" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr jy jz ka kb bi translated"><strong class="iw hj">机器学习管道</strong>:使用自然语言处理转换数据，使用GridSearchCV、RandomForest训练机器学习模型，将推文背后的信息分为36类。</li><li id="1b62" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr jy jz ka kb bi translated"><strong class="iw hj"> Web开发:</strong> Flask app和用于预测结果并显示结果的用户界面。</li></ul><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kh"><img src="../Images/e736a1d14d3a5cc92d8b71201123e29b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ht1yULJ3t0jyytkb4_3p5A.png"/></div></div></figure><h1 id="eee2" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">ETL管道</h1><h2 id="ac1e" class="lk kn hi bd ko ll lm ln ks lo lp lq kw jf lr ls la jj lt lu le jn lv lw li lx bi translated">加载和合并数据</h2><pre class="ki kj kk kl fd ly lz ma mb aw mc bi"><span id="8194" class="lk kn hi lz b fi md me l mf mg"><strong class="lz hj"># import libraries</strong><br/>import pandas as pd<br/>from sqlalchemy import create_engine</span><span id="779f" class="lk kn hi lz b fi mh me l mf mg"><strong class="lz hj"># load messages dataset</strong><br/>messages = pd.read_csv('messages.csv')<br/># load categories dataset<br/>categories = pd.read_csv('categories.csv')<br/># merge datasets<br/>df = messages.merge(categories, on='id')</span><span id="0393" class="lk kn hi lz b fi mh me l mf mg"><strong class="lz hj"># Split categories into separate category columns.</strong><br/># create a dataframe of the 36 individual category columns<br/>categories_split = df ['categories'].str.split (pat = ';', expand = True)<br/>categories_split.head()</span></pre><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/6e34c9375a508a97af8b8f91d25a3872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZBtNSZQbygISrfJAAhjXcw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">在将类别拆分到单独的列之前合并数据集</figcaption></figure><h2 id="4f56" class="lk kn hi bd ko ll lm ln ks lo lp lq kw jf lr ls la jj lt lu le jn lv lw li lx bi translated">数据清理</h2><pre class="ki kj kk kl fd ly lz ma mb aw mc bi"><span id="4cad" class="lk kn hi lz b fi md me l mf mg"><strong class="lz hj"># rename the columns of 'categories'<br/></strong><em class="mj"># select the first row of the categories dataframe</em><br/>row = categories_split.iloc [0]<br/><em class="mj"># use this row to extract a list of new column names for categories.</em><br/>category_colnames = row.apply (<strong class="lz hj">lambda</strong> x: x.rstrip ('- 0 1'))<br/>categories_split.columns = category_colnames</span><span id="a8dd" class="lk kn hi lz b fi mh me l mf mg"><strong class="lz hj"># convert category values to 0 and 1 </strong><br/><strong class="lz hj">for</strong> column <strong class="lz hj">in</strong> categories_split:<br/>    <em class="mj"># set each value to be the last character of the string</em><br/>    categories_split[column] = categories_split[column].str [-1]<br/>    <em class="mj"># convert column from string to numeric</em><br/>    categories_split[column] = pd.to_numeric(categories_split[column], errors = 'coerce')</span><span id="44a7" class="lk kn hi lz b fi mh me l mf mg"><strong class="lz hj"><em class="mj"># drop the original categories column from `df`</em></strong><br/>df.drop (['categories'], axis = 1, inplace = <strong class="lz hj">True</strong>)</span><span id="8aaa" class="lk kn hi lz b fi mh me l mf mg"><strong class="lz hj"><em class="mj"># concatenate the original dataframe with the new `categories` dataframe</em></strong><br/>df = pd.concat([df,categories_split], axis = 1, sort = <strong class="lz hj">False</strong>)</span><span id="70e1" class="lk kn hi lz b fi mh me l mf mg"><strong class="lz hj"><em class="mj"># drop duplicates</em></strong><br/>df.drop_duplicates(inplace=<strong class="lz hj">True</strong>)</span></pre><figure class="ki kj kk kl fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/d204fbf4e05eb56c4661c8c2cdae774f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ClLODjRrL1tuMwobsb1PA.png"/></div></div></figure><h2 id="3b64" class="lk kn hi bd ko ll lm ln ks lo lp lq kw jf lr ls la jj lt lu le jn lv lw li lx bi translated">将数据帧保存在SQL数据库中</h2><pre class="ki kj kk kl fd ly lz ma mb aw mc bi"><span id="8e99" class="lk kn hi lz b fi md me l mf mg">engine = create_engine(‘sqlite:///InsertDatabaseName.db’) df.to_sql(‘InsertTableName’, engine, index=<strong class="lz hj">False</strong>)</span></pre><h1 id="fb98" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated"><strong class="ak">机器学习管道</strong></h1><h2 id="ad47" class="lk kn hi bd ko ll lm ln ks lo lp lq kw jf lr ls la jj lt lu le jn lv lw li lx bi translated">导入库并从数据库加载数据。</h2><pre class="ki kj kk kl fd ly lz ma mb aw mc bi"><span id="e1b4" class="lk kn hi lz b fi md me l mf mg"><em class="mj"># import libraries</em><br/><strong class="lz hj">import</strong> <strong class="lz hj">nltk</strong><br/>nltk.download('punkt')<br/>nltk.download('wordnet')<br/>nltk.download('stopwords')<br/><strong class="lz hj">import</strong> <strong class="lz hj">pandas</strong> <strong class="lz hj">as</strong> <strong class="lz hj">pd</strong><br/><strong class="lz hj">from</strong> <strong class="lz hj">sqlalchemy</strong> <strong class="lz hj">import</strong> create_engine<br/><strong class="lz hj">from</strong> <strong class="lz hj">sklearn.pipeline</strong> <strong class="lz hj">import</strong> Pipeline, FeatureUnion<br/><strong class="lz hj">from</strong> <strong class="lz hj">sklearn.feature_extraction.text</strong> <strong class="lz hj">import</strong> CountVectorizer, TfidfTransformer<br/><strong class="lz hj">from</strong> <strong class="lz hj">sklearn</strong> <strong class="lz hj">import</strong> multioutput<br/><strong class="lz hj">from</strong> <strong class="lz hj">nltk.tokenize</strong> <strong class="lz hj">import</strong> word_tokenize<br/><strong class="lz hj">from</strong> <strong class="lz hj">nltk.stem.porter</strong> <strong class="lz hj">import</strong> PorterStemmer<br/><strong class="lz hj">from</strong> <strong class="lz hj">nltk.stem.wordnet</strong> <strong class="lz hj">import</strong> WordNetLemmatizer<br/><strong class="lz hj">from</strong> <strong class="lz hj">sklearn.feature_extraction.text</strong> <strong class="lz hj">import</strong> TfidfVectorizer<br/><strong class="lz hj">from</strong> <strong class="lz hj">nltk.corpus</strong> <strong class="lz hj">import</strong> stopwords<br/><strong class="lz hj">from</strong> <strong class="lz hj">sklearn.ensemble</strong> <strong class="lz hj">import</strong> RandomForestClassifier,AdaBoostClassifier<br/><strong class="lz hj">from</strong> <strong class="lz hj">sklearn.model_selection</strong> <strong class="lz hj">import</strong> train_test_split<br/><strong class="lz hj">from</strong> <strong class="lz hj">sklearn.metrics</strong> <strong class="lz hj">import</strong> precision_recall_fscore_support<br/><strong class="lz hj">import</strong> <strong class="lz hj">re</strong><br/><strong class="lz hj">from</strong> <strong class="lz hj">sklearn.metrics</strong> <strong class="lz hj">import</strong> fbeta_score, make_scorer<br/><strong class="lz hj">from</strong> <strong class="lz hj">sklearn.model_selection</strong> <strong class="lz hj">import</strong> GridSearchCV<br/><strong class="lz hj">import</strong> <strong class="lz hj">pickle</strong></span></pre><h2 id="130e" class="lk kn hi bd ko ll lm ln ks lo lp lq kw jf lr ls la jj lt lu le jn lv lw li lx bi translated">处理文本数据的符号化函数</h2><pre class="ki kj kk kl fd ly lz ma mb aw mc bi"><span id="c8a7" class="lk kn hi lz b fi md me l mf mg"><strong class="lz hj">def</strong> tokenize(text):<br/>    <em class="mj">#normalize text</em><br/>    text = re.sub(r"[^a-zA-Z0-9]", " ", text.lower())<br/>    <br/>    <em class="mj"># stopword list </em><br/>    stop_words = stopwords.words("english")<br/>    <br/>    <em class="mj">#tokenize</em><br/>    words = word_tokenize(text)<br/>    <br/>    <em class="mj">#stemming</em><br/>    stemmed = [PorterStemmer().stem(w) <strong class="lz hj">for</strong> w <strong class="lz hj">in</strong> words]<br/>    <br/>    <em class="mj">#lemmatizing</em><br/>    words_lemmed = [WordNetLemmatizer().lemmatize(w) <strong class="lz hj">for</strong> w <strong class="lz hj">in</strong> stemmed <strong class="lz hj">if</strong> w <strong class="lz hj">not</strong> <strong class="lz hj">in</strong> stop_words]<br/>   <br/>    <strong class="lz hj">return</strong> words_lemmed</span></pre><p id="1e6b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">建立机器学习管道</strong></p><p id="3ee6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这个ML管道应该将消息作为36个类别的输入和输出分类结果。</p><pre class="ki kj kk kl fd ly lz ma mb aw mc bi"><span id="4a81" class="lk kn hi lz b fi md me l mf mg">pipeline = Pipeline([<br/>        ('vect', CountVectorizer(tokenizer=tokenize)),<br/>        ('tfidf', TfidfTransformer()),<br/>        ('clf', multioutput.MultiOutputClassifier (RandomForestClassifier()))<br/>        ])</span></pre><h1 id="7f02" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">火车管道</h1><p id="177a" class="pw-post-body-paragraph iu iv hi iw b ix ml iz ja jb mm jd je jf mn jh ji jj mo jl jm jn mp jp jq jr hb bi translated">将数据分成训练和测试集以及训练管道</p><pre class="ki kj kk kl fd ly lz ma mb aw mc bi"><span id="d7d4" class="lk kn hi lz b fi md me l mf mg">X_train, X_test, y_train, y_test = train_test_split(X, y, X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 22)<br/><em class="mj"># train classifier</em><br/>new_pipeline.fit(X_train, y_train)</span></pre><h1 id="56a4" class="km kn hi bd ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj bi translated">测试您的模型</h1><p id="4ecb" class="pw-post-body-paragraph iu iv hi iw b ix ml iz ja jb mm jd je jf mn jh ji jj mo jl jm jn mp jp jq jr hb bi translated">报告数据集每个输出类别的f1分数、精确度和召回率。您可以通过遍历列并在每个列上调用sklearn的<code class="du mq mr ms lz b">classification_report</code>来实现这一点。</p><pre class="ki kj kk kl fd ly lz ma mb aw mc bi"><span id="3aaf" class="lk kn hi lz b fi md me l mf mg">y_pred = new_pipeline.predict(X_test)<br/><em class="mj"># Get results and add them to a dataframe.</em><br/><strong class="lz hj">def</strong> get_results(y_test, y_pred):<br/>    results = pd.DataFrame(columns=['Category', 'f_score', 'precision', 'recall'])<br/>    num = 0<br/>    <strong class="lz hj">for</strong> cat <strong class="lz hj">in</strong> y_test.columns:<br/>        precision, recall, f_score, support = precision_recall_fscore_support(y_test[cat], y_pred[:,num], average='weighted')<br/>        results.set_value(num+1, 'Category', cat)<br/>        results.set_value(num+1, 'f_score', f_score)<br/>        results.set_value(num+1, 'precision', precision)<br/>        results.set_value(num+1, 'recall', recall)<br/>        num += 1<br/>    print('Aggregated f_score:', results['f_score'].mean())<br/>    print('Aggregated precision:', results['precision'].mean())<br/>    print('Aggregated recall:', results['recall'].mean())<br/>    <strong class="lz hj">return</strong> results<br/>#---------------------------------------------------------</span><span id="e852" class="lk kn hi lz b fi mh me l mf mg">y_pred_tuned_ada = new_pipeline.predict(X_test)<br/>results_tuned = get_results(y_test, y_pred_tuned_ada)<br/>results_tuned</span></pre><p id="c6c7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">聚合f_score: 0.940272096531 <br/>聚合精度:0.94022097111 <br/>聚合召回率:0.94022097</p><p id="42a3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">代码可以在这个<a class="ae js" href="https://github.com/isakkabir/Disaster-Response-ML-Pipeline" rel="noopener ugc nofollow" target="_blank">库</a>中找到</p><p id="4444" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">参考资料:</p><ol class=""><li id="d2a0" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr mt jz ka kb bi translated"><a class="ae js" href="https://unsplash.com/photos/iZ2v4FwtMLc" rel="noopener ugc nofollow" target="_blank">https://unsplash.com/photos/iZ2v4FwtMLc</a></li><li id="c1fa" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr mt jz ka kb bi translated"><a class="ae js" href="https://www.un.org/development/desa/dspd/2020/04/social-impact-of-covid-19/" rel="noopener ugc nofollow" target="_blank">https://www . un . org/development/DESA/dspd/2020/04/新冠肺炎的社会影响/ </a></li></ol></div></div>    
</body>
</html>