# 调试神经网络

> 原文：<https://medium.com/analytics-vidhya/debugging-neural-networks-96d3421e9646?source=collection_archive---------2----------------------->

神经网络已经成为解决我的学科中许多问题的标准规范。尽管这些网络看起来很酷，但是当某些东西的表现没有达到我们的预期时，调试会带来巨大的痛苦。如果你正在挠头，为什么神经网络的输出看起来不太好，但在弄清楚发生了什么方面遇到了障碍，这篇文章是给你的。。这篇文章是假设 Pytorch 框架而写的，但是其中一些想法是跨框架通用的。这篇文章中将要讨论的调试神经网络的工具有

1.  张量板
2.  梯度流动可视化
3.  动态图形可视化
4.  模型可视化
5.  可视化数据

这篇文章假设你对深度学习有一些了解，能够理解这里讨论的内容。但是即使事实并非如此，你也可以继续阅读。如果你觉得这篇文章中使用的语言和词汇对你来说是陌生的，你应该考虑浏览一下这些材料

## **模式 1:我没有监控培训进度的基础设施。测试精度低。**

不管你正在解决的问题是什么，拥有一个图形化的可视化来监控训练进度是高效解决深度学习问题的一个强烈要求。Tensorboard 是一个非常好的工具。您可以使用 pip 安装 tensorboard。安装后，您可以使用以下代码片段作为参考，开始您的 Tensorboard 日志记录之旅

```
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter(path_to_write)
writer.add_scalar(GraphName,  Y value, X value)
```

X 值通常是迭代次数，而 Y 值是训练精度/验证精度/测试精度。一旦您为每个迭代记录了这些准确性值，您就有了一个在线显示来监控您的训练进度

![](img/3c257c9c913661b008493bf2012aeb4b.png)

使用 Tensorbaord 可以在线观看模型火车的可视化示例

要查看这一点，请从命令行运行以下命令

```
tensorboard --logdir directory_name
```

这将自动告诉您可以看到可视化的本地主机端口地址。例如，我的系统指示我在 localhost:6006 上查看该文件。只需在您的 web 浏览器中输入这些内容，就可以看到实际效果。如果您的 GPU 是远程机器，您可以使用以下内容来启用远程查看

```
tensorboard --logdir directory_name --bind_all
```

## 模式二:我已经写好了我的关系网。损失没有减少，精度接近随机—梯度可视化和小数据集过拟合

如果一个网络没有学习到任何有用的东西，可能会有很多地方出错，但最有用的工具之一是可视化网络中的梯度流。pytorch 论坛的这个帖子给出了一个绘制网络中梯度流的函数。需要传递到本文中讨论的 plot_gradient_flow 函数中的只是模型的命名参数(这可以通过 model.named_parameters()获得)。当你试图想象时，会发生三件事之一

1.  显示“无类型对象没有属性”的错误
2.  不良梯度流
3.  良好的梯度流动

**问题 1:没有渐变**:在 pytorch 中，每个模型层都有“grad”成员。必须注意的是，在 pytorch 中，反向传播是自动计算的(或者像某些论坛中描述的那样自动计算)。当您第一次使用 pytorch 构建网络时，您会遇到这种错误，很有可能您使用了不支持反向传播的 pytorch 操作。我遇到的一个这样的操作是 argmax 的使用。当我现在逻辑地思考这个问题时，我觉得自己太傻了，以至于我一开始就不应该想到 argmax 的反向传播。但是很有可能你会像我一样，在没有一杯咖啡的情况下写深夜代码。当没有为图层定义反向传播时，plot_gradient_flow 会发出错误，因为它尝试从 None 类型对象访问值，这是不期望的。此错误的另一个不太常见的原因可能是您有一个图层。当你试图在某些层固定(这些层不变)的情况下进行迁移学习时，通常就是这种情况。如果这是我们代码的意图，你可以修改论坛上发布的绘图渐变函数来忽略这些层。

**问题 2:糟糕的渐变效果:**当你绘制渐变效果时，图形会自己讲述一个故事。例如:当我试图训练一个网络时，我得到了这个图

![](img/0faaa07041d4dfb2747ec9bc9bf91b2d.png)

通过网络的差梯度流将永远不会被训练

正如我们所见，梯度甚至在到达第一个卷积层之前就消失了。这可能是由多种原因造成的，但是有一件无害的事情总是值得一试，那就是使用批处理规范化。同样的网络训练在我使用批量归一化的时候差别很大。

![](img/09210c56d43ab652387fcf43e2c9fca2.png)

良好梯度流动的一个例子

但这并不是不良梯度流发生的唯一原因。其他一些可能有帮助的因素有

1.  增加您的批量
2.  提高学习速度
3.  你可能只是运气不好——尝试不同的初始化。

第三点值得强调。深度学习网络有很大的随机性。因此，总是通过在相同的 hyper parameter 设置下运行几个实验来验证关于调试的结论，但是使用不同的初始化。我总是在我的总管道中播种尽可能可重复的结果。

**问题三:梯度流好但网络不训练。这是一个棘手的情况。你可能正在尝试一个不可能的问题，或者你正在喂垃圾。因为作为工程师，我们必须对解决问题保持尽可能乐观的态度，所以让我们看看在得出一个大胆的结论——一个问题对于神经网络来说是不可能的——之前我们能做些什么。**

1.  可视化数据集，以确保输入的数据不是垃圾。举个例子，有一次当我的网络没有训练时，我对数据集的可视化给了我一个直接的结论，我在喂垃圾

![](img/b8803f5ce619ce27babae5bb45fa3ab9.png)

在我试图解决的问题中，黄色箭头表示机器人必须采取的行动，以便绳子的状态可以从顶部显示的状态变为底部显示的状态。显然，在我的数据处理代码中有一个错误，一旦我修复了这个错误，网络就可以顺利地训练了。根据你的创造力水平和你想要可视化的数据的复杂程度，你可以在这上面花费你想要的时间。但是任何认为自己聪明到可以跳过这一步的人，都会在某个阶段被迫这样做，作为调试工具。当您从第 0 步开始工作时，最好这样做。

2.但是，如果您的梯度流是好的，并且您确信输入的数据也是好的，那么我们唯一需要改进的就是超参数。尝试不同的超参数值，甚至尝试不同的网络架构。

3.另一个经常被忽视的方面是，网络的好坏取决于你的损失函数。如果你不明白你在使用什么损失函数，你可能需要退后一步，做一些功课，然后回来。因此，为您的问题选择错误的损失函数也可能是这个问题的一个常见来源。

4.一个好的调试步骤总是让你的模型适合一个非常小的数据。我发现在 100 个数据点的数据集上获得 100%的准确率很方便。即使在分析了所有这些之后，如果你甚至在小数据集上也无法获得良好的准确性，那么你应该重新考虑你最初使用深度学习的决定，或者你可以考虑稍微重新表述一下问题，以便学习可以更容易地用于网络。

## **模式三:我发现某处网络建设有问题，但我不知道问题出在哪里**

至少有两种工具可供您使用

1.  可视化模型
2.  可视化动态图形

在进入每一个的细节之前，有必要讨论一下两者之间的区别。模型可视化是您在模型中构建的层的可视化。pytorch 使用动态图来查找层之间的反向传播(Tensorflow 使用静态图，这是使 pytorch 更加 pythonic 化和更加强大的一个特性。但这是另一个帖子的故事)

1.  为了可视化模型，我们可以使用 summary writer 并在 tensorboard 中可视化。假设您已经声明了一个摘要编写器，如第 1 节所示，您可以使用下面的代码将模型添加为一个图。

```
writer.add_graph(net, (arguments_to_forward_function_of_net))
```

通常，网络的前向函数的自变量是单个张量，但也有网络可以接受多个自变量，在这种情况下，前向函数将接受多个自变量。您可以打开 tensorboard 来可视化模型，如下所示

![](img/d38270949bb1e69606fb98cd6c72003f.png)

模型可视化

乍一看可能很吓人，尤其是当你的模型有多个层并且很复杂的时候。但是，如果您花一些时间使用它，您将学会将这里的结构映射到您在纸上的模型，从而向您提供一个来验证模型构造是否是您想要的。

2.如果梯度在某些图层被截断，并且您确信这是因为您使用了不支持 pytorch 反向传播的无效操作，那么可以尝试使用动态图形可视化工具。查看这个 [pytorch 帖子](https://discuss.pytorch.org/t/print-autograd-graph/692/5)来可视化动态图形。这帮助我在使用不存在反向传播的操作时调试我的问题

![](img/8650a844899bd272e31229730c10fc66.png)

动态图形可视化

我觉得从底部看这个图很方便。根据损失函数，考虑每一层依赖的所有运算，如加法、乘法等..如果你在网络中间发现弹出的蓝盒子，你应该对它们有所怀疑，尽管有结构并不完全奇怪。如果您以这种方式回溯，您将或多或少地找到一个点，在这个点上，图没有超出某个操作，这就是您正在寻找的罪魁祸首。这不是进行这种搜索的唯一策略。如果您的网络很小，您可以为每个操作遍历网络，并在每个图层上检查 layer.requires_grad。根据问题的复杂性，这两种方法都可以使用，但我通常从遍历网络开始，并求助于动态图形可视化作为后备调试计划。

## 模式 4:我的网络训练，但它超负荷。我的训练精度很好，但测试精度很差，或者在某一点停滞不前。

1.  最容易尝试的就是辍学。有激烈的讨论认为使用批处理规范化使得使用辍学变得多余。但是一些程序员确实报告了辍学后性能的提高。所以试试这个也无妨。但是当你退学的时候，在计算你的验证精度或者测试精度或者做推断的时候，一定要用 model.eval()。这样做是为了在使用所有连接时衡量你的反应，而不是在训练中关闭某一部分连接。如果你不确定为什么需要这样做，你必须考虑阅读什么是辍学。
2.  构建数据集分布直方图以检查类别不平衡。如果你的数据集由猫和狗分类中的 90%的狗组成，网络将学习捷径，预测狗总是给它一个非常高的准确性。为了获得良好的结果，数据集应尽可能均匀分布。如果您无法收集问题中特定数据类的数据，您应该考虑使用像焦点丢失这样的工具，或者您可以在数据加载中使用试探法，或者您可以为具有非常数据计数的类增加数据，或者在最坏的情况下，您可以只为具有最低计数的类重复数据。虽然最后一个想法本质上是一个黑客，但它有时是避免数据不平衡问题的快捷方式

总之，深度学习网络的构建和训练不同于其他算法的构建，在其他算法的构建中，我们可以单步调试代码并找到 bug 的来源。虽然深度学习世界越来越多，但人们只是下载开源代码并运行它们，声称自己是“深度学习专家”。如果你真的需要做深度学习，坚持做门徒，把尽可能多的像这些工具集成到你的工作流程中，在调试深度学习问题上变得更高效。我知道我已经遗漏了相当多的调试工具，如可视化激活和更高级的工具，如降维(如 TSNE)，最大化活动区域可视化。我只是在这里展示了简单的技术和工具。如果您正在使用任何其他有用的技术或工具，请在下面评论。如果你想在我的代码中看到这些工具的运行，请在我的 github 页面中查看代码