<html>
<head>
<title>Principal Components and How to Find them in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主要组件以及如何在Python中找到它们</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/principal-components-and-how-to-find-them-in-python-a810363c0489?source=collection_archive---------20-----------------------#2020-02-06">https://medium.com/analytics-vidhya/principal-components-and-how-to-find-them-in-python-a810363c0489?source=collection_archive---------20-----------------------#2020-02-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1f61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从我的上一篇博文继续，我将讨论用Python计算主成分，并在各种分类模型中使用它们。</p><p id="ab88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将讨论非概率主成分分析以及如何使用主成分分析绘制特征重要性图。我在随后的文章中介绍了其他类型的PCA，即核PCA、概率PCA和稀疏PCA。</p><p id="d204" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">出于本文的目的，我使用了乳腺癌数据集，您可以通过以下链接轻松下载:</p><p id="0e77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data" rel="noopener ugc nofollow" target="_blank">https://archive . ics . UCI . edu/ml/machine-learning-databases/breast-cancer-Wisconsin/wdbc . data</a></p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><h2 id="4f00" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">步骤1:导入数据</h2><p id="a7cd" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">数据集由569行和32个特征列组成。虽然PCA是用来加速机器学习过程的，但是我还是会在这里使用它。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es kl"><img src="../Images/3322444d5759e7f9809a2ffca37b1214.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wMvmRJpbRU5BPYRUYX7zEQ.png"/></div></div></figure><h2 id="5e3e" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">步骤2:转换数据</h2><p id="e5ba" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">标准化在PCA中很重要，因为它是一个方差最大化的过程。它将原始数据投射到最大化方差的方向上。新轴基于变量的标准偏差，因此通过标准化数据，所有变量将具有相同的标准偏差和相同的权重。这样就计算出了相关的轴。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es kx"><img src="../Images/66ed057911ff111766b59decd7d47245.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*xLGrHxqanNKD9XD1qOa54A.png"/></div></figure><h2 id="dd68" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">步骤3:训练-测试分割</h2><p id="e8ef" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">我使用<em class="ky"> sklearn </em> train-test split函数分割数据，然后一次性编码目标变量，因为我们正在处理分类变量。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es kz"><img src="../Images/99a56eaf64ed5c077f6ab552743891be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GIwHDb_6g5R-UxhZ0kJ71g.png"/></div></div></figure><h2 id="463c" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">步骤4:应用PCA</h2><p id="4d27" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">因此，现在我们已经分割了数据，并为模型准备做好了准备，我们继续应用PCA。首先，我们将从<em class="ky"> sklearn </em>库中导入函数，然后只适合使用它的训练集。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es la"><img src="../Images/cb7480de79c7ba54b4bb388b2ddca586.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*-z62JZp0-iReM8tdAebm5Q.png"/></div></figure><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es lb"><img src="../Images/9374d032c0b420f29f33a98604bac282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*fTDBIA4khnqnG79G1qtU_A.png"/></div></figure><p id="f5c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ky"> explained_var </em>按降序取所有分量的累积和，直到我们得到一个。我们可以利用这一点来了解我们需要多少主成分来解释数据中的差异，从而提高我们模型的准确性。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es lc"><img src="../Images/2d53f55280a2d8d1e3b0e593dcd69056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QlCbuI8ppzGHFfIsqd-39A.png"/></div></div></figure><p id="3301" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由此我们可以看出，最好是取16个分量。但是<em class="ky">解释了_var[16] = 0.99177181 </em>并且从那以后继续增加。所以，让我们画一个图来看看发生了什么。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es ld"><img src="../Images/9f3b35663437f9b8f948ae830837fdc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*AAZItvAoD-PnworOhvfs_Q.png"/></div></figure><p id="4ea6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">被解释为成分的函数的变化量是一个递增量，尽管它增加得很少。因此，现在我们只取模型中的16–17个组件。</p><p id="ffd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了找出前16个主成分，让我们通过给它分配权重来绘制一个特征重要性图。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es le"><img src="../Images/79dbcbd76ce651a8893780bddba983e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*djHyS040HXICTNEWlg-nSQ.png"/></div></div></figure><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es lf"><img src="../Images/adb233fc1d902f264e8a746e72d777ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MYc2FdX9TCgU9yx5mN44KQ.png"/></div></div></figure><p id="887d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在可以在我们的线性回归算法中输入前17个特征(主成分)。</p><h1 id="a50e" class="lg jm hi bd jn lh li lj jr lk ll lm jv ln lo lp jy lq lr ls kb lt lu lv ke lw bi translated">结果</h1><h2 id="3216" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">线性回归:无PCA</h2><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es lx"><img src="../Images/9c4a0daec665c7416fb3a767951abeb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w9w_3idBmbwZWSE6vmtATA.png"/></div></div></figure><h2 id="8ed8" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">使用PCA</h2><p id="baa3" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">我将前17个主成分存储在变量<em class="ky"> X_train_pca </em>中，我们可以看到准确性的巨大飞跃。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es ly"><img src="../Images/475356ed316ce1eebc0d3f99bef3b35a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-RmIsw6jFY5pIH5mGCTwYw.png"/></div></div></figure><h2 id="9f81" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">逻辑回归:无主成分分析</h2><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es lz"><img src="../Images/0dbbfc8b39c68190ffba018d2982d482.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*vPBpz_wnv5n9ABRrdv9i-A.png"/></div></figure><h2 id="1b4c" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">使用PCA</h2><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es ma"><img src="../Images/0f42c67def8420d19772e19985a29de3.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*IHO7nYzE37MleDlvpXlv4Q.png"/></div></figure><h1 id="8f37" class="lg jm hi bd jn lh li lj jr lk ll lm jv ln lo lp jy lq lr ls kb lt lu lv ke lw bi translated">结论</h1><p id="0e61" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">在线性回归中使用主成分分析时，我们看到了准确性的大幅提升。虽然，我们在逻辑回归中获得了更好的准确性，但并不显著。附上，我想说，主成分分析可以有助于提高效率的算法使用，但决定最好是在数据预处理后。</p><p id="52bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我的其他帖子中，我已经介绍了各种PCA。请务必看一看，感谢您的阅读！</p></div></div>    
</body>
</html>