<html>
<head>
<title>Machine Learning &amp; Deep Learning Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习和深度学习指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-deep-learning-guide-da303a71b8e0?source=collection_archive---------22-----------------------#2019-11-26">https://medium.com/analytics-vidhya/machine-learning-deep-learning-guide-da303a71b8e0?source=collection_archive---------22-----------------------#2019-11-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9fdd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated"><span class="l je jf jg bm jh ji jj jk jl di"> W </span>欢迎来到机器学习&amp;深度学习指南的第4部分，在这里我们学习和实践机器学习和深度学习，而不会被概念和数学规则所淹没。</p><blockquote class="jm jn jo"><p id="085f" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><a class="ae jt" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-part-1-4ba7ce8cf7eb"> <em class="hi">第1部分:关键术语、定义和从监督学习(线性回归)开始。</em>T5】</a></p><p id="e9fd" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><a class="ae jt" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-db520c4797da"> <em class="hi">第二部分:监督学习:回归(SGD)和分类(SVM、朴素贝叶斯、KNN和决策树)。</em>T9】</a></p><p id="ee06" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><a class="ae jt" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-11ad26e0854c"> <em class="hi">第三部分:无监督学习(KMeans，PCA)，欠拟合vs过拟合和交叉验证</em> </a> <em class="hi">。</em></p><p id="af44" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><em class="hi">第4部分:深度学习:定义、层次、度量和损失、优化器和正则化</em></p></blockquote></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="cada" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">学习目标</strong></h1><p id="5434" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">在教程的第4部分，我们将讨论<strong class="ih hj">深度学习。</strong>首先，我们会有深度学习和神经网络的定义。然后我们讨论两种主要的神经网络结构。之后，我们将列出与优化函数一起使用的主要误差和度量函数。最后，我们会写一个深度学习的例子。</p><h1 id="b668" class="kb kc hi bd kd ke le kg kh ki lf kk kl km lg ko kp kq lh ks kt ku li kw kx ky bi translated">定义</h1><p id="4c62" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated"><strong class="ih hj">深度学习</strong></p><p id="cc88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本指南的<a class="ae jt" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-part-1-4ba7ce8cf7eb">第1部分</a>中，我们将<a class="ae jt" rel="noopener" href="/datadriveninvestor/difference-between-ml-ai-dl-23db64f7aa2">深度学习</a>定义为机器学习的子集，它使用网状网络，工作方式类似于我们的大脑，技术上称为<em class="jp">深度神经网络</em>。<br/>就像我们的大脑识别模式对事物进行分类，并从错误中学习一样——<em class="jp">深度学习也是如此</em>。它将未知数据与已知数据进行比较，从而对其进行相应的分类。</p><p id="9177" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第3部分<a class="ae jt" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-11ad26e0854c">中，我们看到了一个无监督学习的例子，我们做了一些特征提取。深度学习具有学习多层表示的能力，是少数几种帮助我们自动提取特征的方法之一。可以假设较低层正在执行自动特征提取，很少或不需要程序员的指导。</a></p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es lj"><img src="../Images/4798ddeed20bb75a7248a9c8a41c88fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*xtlapcPtuJN3SNUZMXrqSg.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated"><a class="ae jt" href="https://www.slideshare.net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-andrew-ng" rel="noopener ugc nofollow" target="_blank">神经网络示意图</a></figcaption></figure><p id="5c31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">神经网络</strong> <br/>神经网络是一类用层构建的模型。常用的神经网络类型包括卷积神经网络和递归神经网络。</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es lv"><img src="../Images/2fc7327b5a636aafaba78aa2b6b7b5ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QfDeUIptdulcodai6TCbTg.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">神经网络</figcaption></figure><ul class=""><li id="8dcd" class="ma mb hi ih b ii ij im in iq mc iu md iy me jc mf mg mh mi bi translated"><strong class="ih hj">输入层:</strong>代表我们输入神经网络的数据。</li><li id="663f" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc mf mg mh mi bi translated"><strong class="ih hj">输出层:</strong>它生成我们的神经网络的输出。它能出去</li><li id="77b9" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc mf mg mh mi bi translated"><strong class="ih hj">隐藏层:</strong>是我们神经网络的核心。它可以输出二进制值(二进制分类)、属于一类的概率(多类分类)或连续值(回归)。如果我们有一层(或几层)，它被称为浅网络。否则它被称为深度神经网络(DNN)</li></ul><p id="d1ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">单个神经元可能如下所示</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mo"><img src="../Images/e21a30f99bf9a45c2a587bc22bcc3e43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MXMSWQSc-6shCJ4Iz87RmA.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated"><a class="ae jt" href="https://pythonprogramming.net/introduction-deep-learning-python-tensorflow-keras/" rel="noopener ugc nofollow" target="_blank">单个神经元</a></figcaption></figure><p id="389a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在每一层中，你都有输入数据，你<strong class="ih hj"> <em class="jp">加权</em> </strong>它，并把它通过神经元中被称为激活函数的函数。它是所有值与某个值比较后的总和。如果你发射一个信号，那么结果是(1) out，或者没有发射任何东西，那么是(0)。然后加权并传递给下一个神经元，运行相同的功能。一旦我们到达输出层，我们生成我们的输出和<strong class="ih hj"> <em class="jp">比较</em> </strong>以匹配您想要的输出并计算<strong class="ih hj"> <em class="jp">损失/成本</em> </strong>。</p><p id="cc65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将数据从一个神经元传播到下一个神经元(按顺序)的过程称为<strong class="ih hj">前馈。<br/> </strong>基于损失，我们回溯并开始更新<strong class="ih hj"> <em class="jp">权重</em> </strong>和<strong class="ih hj"> <em class="jp">偏差</em> </strong>以最小化损失/成本。这个过程叫做<strong class="ih hj"> <em class="jp">反向传播。</em> </strong></p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mp"><img src="../Images/674d10eb3415ef27090416ecdb0c9b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P1J0iJTf86EQr1inA9eIBQ.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">学习过程</figcaption></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mq"><img src="../Images/bb8d05cdc7c6cb42ddce763efffd64b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ThYvY-56eV3rv2e7dn7a4Q.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">深度学习可以用于有监督的、无监督的或RL。来源:弗里德曼等人| <a class="ae jt" href="https://deeplearning.mit.edu/" rel="noopener ugc nofollow" target="_blank">麻省理工深度学习</a></figcaption></figure><p id="ac3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然我们已经看到了神经网络的结构。让我们来看一些神经网络的例子。</p><h1 id="6acf" class="kb kc hi bd kd ke le kg kh ki lf kk kl km lg ko kp kq lh ks kt ku li kw kx ky bi translated"><strong class="ak">卷积神经网络</strong></h1><p id="431c" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">卷积神经网络背后的思想是通过图像的移动滤波器(卷积)的思想。然后我们应用下采样(池化),选择一个区域并应用该区域中值的平均值或最大值。最后一层(输出)是生成输出的全连接层。</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mr"><img src="../Images/edd030656c871123676261fe61f4c00d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yICU9GS2nAxNNpdAL6eWEA.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">卷积神经网络(CNN)</figcaption></figure><p id="f0ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">CNN主要用于<strong class="ih hj">机器视觉</strong>项目。但是，它仍然可以用于其他应用程序。</p><h1 id="485c" class="kb kc hi bd kd ke le kg kh ki lf kk kl km lg ko kp kq lh ks kt ku li kw kx ky bi translated">递归神经网络</h1><blockquote class="jm jn jo"><p id="f8b8" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated">递归网络是一种人工神经网络，旨在识别数据序列中的模式，如文本、基因组、手写、口语或来自传感器、股票市场和政府机构的数字时间序列数据。这些算法考虑了时间和顺序，它们有一个时间维度。</p></blockquote><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es ms"><img src="../Images/4f85ed9ec951b48beee726d6aab9ab82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*AIdoprxuMAjbDS8Kd6sEQQ.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated"><strong class="bd kd">长期依赖问题，每个节点代表一个rnn小区。</strong>来源:Github</figcaption></figure><h1 id="d841" class="kb kc hi bd kd ke le kg kh ki lf kk kl km lg ko kp kq lh ks kt ku li kw kx ky bi translated"><strong class="ak">图层类型及功能:</strong></h1><p id="e450" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">让我们考虑最重要和最常用的层:</p><ul class=""><li id="8a99" class="ma mb hi ih b ii ij im in iq mc iu md iy me jc mf mg mh mi bi translated"><strong class="ih hj">输入层</strong>——按原样获取原始数据。</li><li id="1e9f" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc mf mg mh mi bi translated"><strong class="ih hj">卷积层</strong>——这一层是卷积神经网络(<strong class="ih hj"> CNN) </strong>的核心构建模块，完成大部分计算。这一层计算输入中神经元和各种小块之间的卷积。</li><li id="fc69" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc mf mg mh mi bi translated"><strong class="ih hj">池化层</strong>—池化有助于我们在网络中前进时仅保留重要部分。池层在输入的每个深度切片上独立操作，并在空间上调整其大小。它使用最大值函数。</li><li id="6a64" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc mf mg mh mi bi translated"><strong class="ih hj">全连接层(密集)</strong>—该层计算最后一层的输出分数。结果输出的大小为<strong class="ih hj"> 𝟏×𝟏×𝑳 </strong>，其中l是训练数据集类的数量。</li><li id="7c7e" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc mf mg mh mi bi translated"><strong class="ih hj">LSTM</strong><strong class="ih hj"/>长短期记忆网络——通常简称为“lstm”——是一种特殊的RNN，能够学习长期依赖关系。</li></ul><p id="e20b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如前所述，隐藏层之间有一个激活函数，应用于前一层的输出。它增加了网络的非线性，因此它可以很好地推广到任何类型的函数。以下是最常见的几种:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lw lx di ly bf lz"><div class="er es mt"><img src="../Images/2c0d5e5b12186f169ad888106b443cb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2FT8cMGC_5wm05uUeFx8Fw.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">a.Sigmoid b.Tanh c .整流线性单元(ReLU) d .泄漏ReLU</figcaption></figure><h1 id="0f55" class="kb kc hi bd kd ke le kg kh ki lf kk kl km lg ko kp kq lh ks kt ku li kw kx ky bi translated">指标和损失:</h1><p id="c3c0" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">与<strong class="ih hj">机器学习一样，</strong>在深度学习中，我们使用损失函数来评估我们模型的错误，我们使用度量来评估性能。</p><p id="1a4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是深度学习中使用的主要损失函数:</p><p id="fe21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">用于分类:</strong></p><ul class=""><li id="dd91" class="ma mb hi ih b ii ij im in iq mc iu md iy me jc mf mg mh mi bi translated">二元分类:二元交叉熵</li><li id="d889" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc mf mg mh mi bi translated">多类分类:类别交叉熵和稀疏类别交叉熵</li></ul><p id="fa38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">用于回归:</strong></p><ul class=""><li id="06c1" class="ma mb hi ih b ii ij im in iq mc iu md iy me jc mf mg mh mi bi translated">均方误差</li><li id="0c4a" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc mf mg mh mi bi translated">平均绝对误差</li></ul><p id="17b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是深度学习的主要指标:</p><ul class=""><li id="1295" class="ma mb hi ih b ii ij im in iq mc iu md iy me jc mf mg mh mi bi translated">准确(性)</li><li id="0a49" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc mf mg mh mi bi translated">平均绝对误差</li></ul><h1 id="b9da" class="kb kc hi bd kd ke le kg kh ki lf kk kl km lg ko kp kq lh ks kt ku li kw kx ky bi translated">优化器:</h1><p id="726a" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">它是使用反向传播来更新权重的函数。我们主要使用以下优化器。</p><ul class=""><li id="32f9" class="ma mb hi ih b ii ij im in iq mc iu md iy me jc mf mg mh mi bi translated"><strong class="ih hj">亚当:</strong>自适应动量</li><li id="d637" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc mf mg mh mi bi translated">均方根传播</li><li id="2032" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc mf mg mh mi bi translated"><strong class="ih hj"> SGD </strong>:随机梯度下降优化器</li></ul><h1 id="530f" class="kb kc hi bd kd ke le kg kh ki lf kk kl km lg ko kp kq lh ks kt ku li kw kx ky bi translated">正规化:</h1><p id="be78" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">帮助网络归纳出它没有见过的数据。它用于解决过拟合问题:</p><ul class=""><li id="d9f1" class="ma mb hi ih b ii ij im in iq mc iu md iy me jc mf mg mh mi bi translated"><strong class="ih hj">丢弃:</strong>随机移除网络中的一些节点(以及输入和输出边)</li><li id="888c" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc mf mg mh mi bi translated"><strong class="ih hj">提前停止:</strong>当验证集的性能下降时，停止训练(或者至少保存一个检查点)</li><li id="0a18" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc mf mg mh mi bi translated"><strong class="ih hj">可用惩罚:<br/> L1惩罚:</strong>惩罚绝对重量。<br/> <strong class="ih hj"> L2罚:</strong>罚平方重量</li></ul></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="ca97" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">足够的定义…开始编码</strong></h1><p id="b59f" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">我们将实现<strong class="ih hj">卷积神经网络(CNN) </strong>模型<a class="ae jt" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> Keras </strong> </a>。</p><blockquote class="jm jn jo"><p id="13fd" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><em class="hi">你可以从</em> <a class="ae jt" href="https://www.kaggle.com/mohammadhatoum/deep-learning-cnn" rel="noopener ugc nofollow" target="_blank"> <em class="hi">这里</em> </a>下载完整的Kaggle笔记本</p></blockquote><p id="bdde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.<strong class="ih hj">数据定义</strong>:我们将使用<a class="ae jt" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj">MNIST</strong></a><strong class="ih hj"/>数据集。我们将定义一些参数如下:</p><pre class="lk ll lm ln fd mu mv mw mx aw my bi"><span id="6017" class="mz kc hi mv b fi na nb l nc nd">from __future__ import print_function<br/>import keras<br/>from keras.datasets import mnist<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, Flatten<br/>from keras.layers import Conv2D, MaxPooling2D<br/>from keras import backend as K</span><span id="0e35" class="mz kc hi mv b fi ne nb l nc nd"># Set few parameters to be used<br/>batch_size = 128<br/>num_classes = 10<br/>epochs = 12<br/># input image dimensions<br/>img_rows, img_cols = 28, 28</span><span id="93f6" class="mz kc hi mv b fi ne nb l nc nd">#Load MNIST dataset the split it between train and test sets<br/>(x_train, y_train), (x_test, y_test) = mnist.load_data()</span><span id="ef9c" class="mz kc hi mv b fi ne nb l nc nd">if K.image_data_format() == 'channels_first':<br/>    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)<br/>    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)<br/>    input_shape = (1, img_rows, img_cols)<br/>else:<br/>    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)<br/>    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)<br/>    input_shape = (img_rows, img_cols, 1)</span></pre><p id="1a85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.执行预处理:</p><p id="c765" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a.规范培训和测试输入</p><pre class="lk ll lm ln fd mu mv mw mx aw my bi"><span id="3916" class="mz kc hi mv b fi na nb l nc nd">x_train = x_train.astype('float32')<br/>x_test = x_test.astype('float32')<br/>x_train /= 255<br/>x_test /= 255<br/>print('x_train shape:', x_train.shape)<br/>print(x_train.shape[0], 'train samples')<br/>print(x_test.shape[0], 'test samples')</span></pre><blockquote class="nf"><p id="bb1d" class="ng nh hi bd ni nj nk nl nm nn no jc dx translated">结果:</p><p id="5dbb" class="ng nh hi bd ni nj np nq nr ns nt jc dx translated">x _训练形状:(60000，28，28，1) <br/> 60000个训练样本<br/> 10000个测试样本</p></blockquote><p id="bca6" class="pw-post-body-paragraph if ig hi ih b ii nu ik il im nv io ip iq nw is it iu nx iw ix iy ny ja jb jc hb bi translated">b.将类别向量转换为二进制类别矩阵</p><pre class="lk ll lm ln fd mu mv mw mx aw my bi"><span id="9191" class="mz kc hi mv b fi na nb l nc nd">y_train = keras.utils.to_categorical(y_train, num_classes)<br/>y_test = keras.utils.to_categorical(y_test, num_classes)</span></pre><p id="99a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.建立模型</p><pre class="lk ll lm ln fd mu mv mw mx aw my bi"><span id="d778" class="mz kc hi mv b fi na nb l nc nd">model = Sequential()<br/>model.add(Conv2D(32, kernel_size=(3, 3),<br/>                 activation='relu',<br/>                 input_shape=input_shape))<br/>model.add(Conv2D(64, (3, 3), activation='relu'))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Dropout(0.25))<br/>model.add(Flatten())<br/>model.add(Dense(128, activation='relu'))<br/>model.add(Dropout(0.5))<br/>model.add(Dense(num_classes, activation='softmax'))</span></pre><p id="0e4e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.绘制模型</p><pre class="lk ll lm ln fd mu mv mw mx aw my bi"><span id="2aa6" class="mz kc hi mv b fi na nb l nc nd">from keras.utils import plot_model<br/>plot_model(model)</span></pre><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es nz"><img src="../Images/82754ca55e19b20f64a9a6cfa4362529.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*svLJSVZaRT8_qHyTOnoUPQ.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">CNN模型</figcaption></figure><p id="0f7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">5.为培训配置模型</p><pre class="lk ll lm ln fd mu mv mw mx aw my bi"><span id="3199" class="mz kc hi mv b fi na nb l nc nd">model.compile(loss=keras.losses.categorical_crossentropy,<br/>              optimizer=keras.optimizers.Adam(),<br/>              metrics=['accuracy'])</span></pre><p id="ac92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">6.为模型定型固定数量的历元(数据集上的迭代)</p><pre class="lk ll lm ln fd mu mv mw mx aw my bi"><span id="7c97" class="mz kc hi mv b fi na nb l nc nd">history = model.fit(x_train, y_train,<br/>          batch_size=batch_size,<br/>          epochs=epochs,<br/>          verbose=1,<br/>          validation_data=(x_test, y_test))</span></pre><p id="13f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">7.通过在测试模式下获取损失值和度量值来评估模型</p><pre class="lk ll lm ln fd mu mv mw mx aw my bi"><span id="07c6" class="mz kc hi mv b fi na nb l nc nd">score = model.evaluate(x_test, y_test, verbose=0)<br/>print('Test loss:', score[0])<br/>print('Test accuracy:', score[1])</span></pre><blockquote class="nf"><p id="db90" class="ng nh hi bd ni nj nk nl nm nn no jc dx translated">结果:</p><p id="8844" class="ng nh hi bd ni nj np nq nr ns nt jc dx translated">测试损耗:0.029843983797999968<br/>测试精度:0.99000001</p></blockquote><p id="0c5e" class="pw-post-body-paragraph if ig hi ih b ii nu ik il im nv io ip iq nw is it iu nx iw ix iy ny ja jb jc hb bi translated">令人印象深刻的是，我们得到了99.19%的准确率</p><p id="9795" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">8.绘制一个图像和我们的模型提供的标签</p><pre class="lk ll lm ln fd mu mv mw mx aw my bi"><span id="a5b2" class="mz kc hi mv b fi na nb l nc nd">import matplotlib.pyplot as plt<br/>image_index = 8855<br/>plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')<br/>pred = model.predict(x_test[image_index].reshape(1, img_rows, img_cols, 1))<br/>print(f"Label predicated by model: {pred.argmax()}")</span></pre><blockquote class="nf"><p id="2587" class="ng nh hi bd ni nj nk nl nm nn no jc dx translated">结果:</p><p id="c36c" class="ng nh hi bd ni nj np nq nr ns nt jc dx translated">由模型预测的标签:5</p></blockquote><figure class="ob oc od oe of lo er es paragraph-image"><div class="er es oa"><img src="../Images/d3626919c1da7872e67e68d5ce28556f.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*zT70_Y5tBMk8jX4kPfc7jw.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">预测结果</figcaption></figure><p id="23f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">9.为训练和测试绘制准确度和损失值</p><pre class="lk ll lm ln fd mu mv mw mx aw my bi"><span id="076c" class="mz kc hi mv b fi na nb l nc nd"># Plot training &amp; validation accuracy values<br/>plt.plot(history.history['accuracy'])<br/>plt.plot(history.history['val_accuracy'])<br/>plt.title('Model accuracy')<br/>plt.ylabel('Accuracy')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Train', 'Test'], loc='upper left')<br/>plt.show()</span><span id="677f" class="mz kc hi mv b fi ne nb l nc nd"># Plot training &amp; validation loss values<br/>plt.plot(history.history['loss'])<br/>plt.plot(history.history['val_loss'])<br/>plt.title('Model loss')<br/>plt.ylabel('Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Train', 'Test'], loc='upper left')<br/>plt.show()</span></pre><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es og"><img src="../Images/8c1ade3238265e69cfe8dbe80dd8ec8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*h75KO4r6mj133ddIbl7eGw.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">模型精度</figcaption></figure><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es og"><img src="../Images/96fca655afa852e99cdcf4b25ef59e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*L1Qe4KRcfDpQkyAyLq7saQ.png"/></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">模型损失</figcaption></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="8fae" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">概述</h1><p id="7bb9" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">我们已经到了机器学习和深度学习指南系列的第四部分也是最后一部分的末尾。在这一部分，我们讨论了深度学习:<strong class="ih hj">定义</strong>，<strong class="ih hj">层</strong>，<strong class="ih hj">度量</strong>和<strong class="ih hj">损失</strong>，<strong class="ih hj">优化器</strong>和<strong class="ih hj">正则化</strong>。然后我们有了一个完整的<strong class="ih hj">卷积神经网络(CNN) </strong>的例子。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="23cd" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">下一步该怎么做，</strong></h1><p id="8cd3" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">我们整个指南的主要目标是帮助程序员和软件工程师开始使用<strong class="ih hj"> ML/DL。</strong>这只是一个切入点，如果你想深入这个领域，我建议你查看一下每个教程中提到的参考资料——尤其是<a class="ae jt" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a>和<a class="ae jt" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> Keras </a>。而最重要的是要靠自己<strong class="ih hj">练习。</strong></p><p id="c5ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读！</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="f91d" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">参考链接:</h1><ol class=""><li id="c1db" class="ma mb hi ih b ii kz im la iq oh iu oi iy oj jc ok mg mh mi bi translated"><a class="ae jt" href="https://www.tutorialspoint.com/python_deep_learning/python_deep_basic_machine_learning.htm" rel="noopener ugc nofollow" target="_blank">https://www . tutorialspoint . com/python _ deep _ learning/python _ deep _ basic _ machine _ learning . htm</a></li><li id="7a60" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc ok mg mh mi bi translated"><a class="ae jt" href="https://stanford.edu/~shervine/" rel="noopener ugc nofollow" target="_blank">https://stanford.edu/~shervine/</a></li><li id="5d17" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc ok mg mh mi bi translated"><a class="ae jt" href="https://pythonprogramming.net/introduction-deep-learning-python-tensorflow-keras/" rel="noopener ugc nofollow" target="_blank">https://python programming . net/introduction-deep-learning-python-tensor flow-keras/</a></li><li id="4aa1" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc ok mg mh mi bi translated"><a class="ae jt" href="https://www.tutorialspoint.com/python_deep_learning/python_deep_learning_artificial_neural_networks.htm" rel="noopener ugc nofollow" target="_blank">https://www . tutorialspoint . com/python _ deep _ learning/python _ deep _ learning _ artificial _ neural _ networks . htm</a></li><li id="cc86" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc ok mg mh mi bi translated"><a class="ae jt" href="https://www.slideshare.net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-andrew-ng" rel="noopener ugc nofollow" target="_blank">https://www . slide share . net/TessFerrandez/notes-from-coursera-deep-learning-courses-by-Andrew-ng</a></li><li id="642a" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc ok mg mh mi bi translated"><a class="ae jt" href="https://skymind.ai/wiki/lstm" rel="noopener ugc nofollow" target="_blank">https://skymind.ai/wiki/lstm</a></li><li id="4294" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc ok mg mh mi bi translated"><a class="ae jt" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></li><li id="15cc" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc ok mg mh mi bi translated"><a class="ae jt" href="https://deeplearning.mit.edu/" rel="noopener ugc nofollow" target="_blank">https://deeplearning.mit.edu/</a></li><li id="ac47" class="ma mb hi ih b ii mj im mk iq ml iu mm iy mn jc ok mg mh mi bi translated"><a class="ae jt" href="https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py" rel="noopener ugc nofollow" target="_blank">https://github . com/keras-team/keras/blob/master/examples/Mn ist _ CNN . py</a></li></ol></div></div>    
</body>
</html>