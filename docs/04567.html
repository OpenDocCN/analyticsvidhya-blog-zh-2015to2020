<html>
<head>
<title>K-Nearest Neighbors From a Theoretical Perspective</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理论视角下的k近邻</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-nearest-neighbors-from-a-theoretical-perspective-5d0da1ded4e0?source=collection_archive---------20-----------------------#2020-03-24">https://medium.com/analytics-vidhya/k-nearest-neighbors-from-a-theoretical-perspective-5d0da1ded4e0?source=collection_archive---------20-----------------------#2020-03-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7b7e50c94ac74c8d11180b7340948f2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BJBIPhENxYzu8yw6"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">李·杰夫斯在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h2 id="54dc" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">k-NN背后的思想</h2><p id="b9fc" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated"><em class="ko"> k </em>最近邻(<em class="ko"> k </em> -NN)是最古老的监督机器学习算法之一。它是由盖和哈特在1967年创建的。</p><p id="9bd0" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">为了理解<em class="ko"> k </em> -NN是如何工作的，我们先来分析下图。</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es ku"><img src="../Images/47eac9dbf6169df565792f56209219af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*C-eVCJz2MKTMlDX6xiv4Yg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">鲁本斯·巴博萨制图</figcaption></figure><p id="a83d" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">我们注意到一个数据集，其中的点分散在图上，我们有两个类别:紫色和橙色。我们又观察到一个新的点，一个小三角形。出现问题是:这个新的点是紫色还是橙色？</p><figure class="kv kw kx ky fd ij"><div class="bz dy l di"><div class="kz la l"/></div></figure><p id="0799" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">你可能认为新点的类别是紫色的。现在想想你是怎么做到的！你怎么知道它是紫色的？你用了什么衍生规则吗？你用概率了吗？你用线性代数了吗？不要！</p><p id="841e" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">你可能会看到这些点的接近程度。小三角形离紫色点比离橙色点更近。然后，您决定根据紫色点的接近程度将其归类为紫色。这和<em class="ko"> k </em> -NN算法背后的思路是一样的！</p><h2 id="ed81" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated"><strong class="ak">邻里</strong></h2><p id="7f06" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">在<em class="ko"> k </em> -NN中，邻域<em class="ko">Vk(</em><strong class="jv hj"><em class="ko">x</em></strong><em class="ko">)</em>定义为数据集<em class="ko"> D </em>中输入值最接近<strong class="jv hj"> <em class="ko"> x </em> </strong>的<em class="ko"> k </em>有序对</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/f0ec3485182acec94b57e05c379f8db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*AXOJj-Q-l72umBMBxwbOrQ.png"/></div></figure><p id="8f11" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">根据距离函数<em class="ko"> d </em>给出了接近度的概念</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/3f333faadae4a4d4f6999d495d81b77d.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*3rrZZZfY079Gk4YKpmgthQ.png"/></div></figure><p id="ad18" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">形式上，邻域<em class="ko">Vk(</em><strong class="jv hj"><em class="ko">x</em></strong><em class="ko">)</em>定义为<em class="ko"> D </em>的子集，基数<em class="ko"> k </em>，满足:</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/c298183d05ca13ec53e4c618550516d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NvIDKJFmFrEJospZ_SihjA.png"/></div></div></figure><p id="fd9b" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">上式表示，对于不属于<strong class="jv hj"> <em class="ko"> x </em> </strong>邻域的所有训练点，如果我们计算它们到<strong class="jv hj"> <em class="ko"> x </em> </strong>的距离，这样的距离不会小于<strong class="jv hj"> <em class="ko"> x. </em> </strong>的任何一个邻域的最大距离</p><p id="1208" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">更简单的说，如果一个点不属于<strong class="jv hj"> <em class="ko"> x </em> </strong>的邻域，那么它不可能比它的邻域更接近<strong class="jv hj"> <em class="ko"> x </em> </strong>。</p><h2 id="6703" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">预言</h2><p id="664d" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">您可以使用<em class="ko"> k- </em> NN进行分类以及回归，但是在本文中，我们将只使用它进行分类。在这种情况下，对任何新输入<strong class="jv hj"> <em class="ko"> x </em> </strong>的预测由<em class="ko">Vk(</em><strong class="jv hj"><em class="ko">x</em></strong><em class="ko">)</em>中最常见的标签给出，如下所示:</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es le"><img src="../Images/b669e06b1e2ec2ec6265361faa1e9a16.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*H9R1cHsDEW6hy6qGkhBSlg.png"/></div></figure><p id="5257" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">mode函数返回最常见的情况。有些人把这种分类规则称为多数表决。上面的等式说明了<em class="ko"> k- </em> NN算法的预测，其中h( <strong class="jv hj"> <em class="ko"> x </em> </strong>)被称为假设函数。</p><h2 id="caaa" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">k-NN算法</h2><p id="8a3d" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">给定数据集<em class="ko"> D = {(x1，y1)，。。。，(xN，yN)} </em>，任何向量<strong class="jv hj"> <em class="ko"> x </em> </strong>的标号都可以通过以下步骤用<em class="ko"> k </em> -NN来估计:</p><ol class=""><li id="7d59" class="lf lg hi jv b jw kp ka kq jg lh jk li jo lj kn lk ll lm ln bi translated">计算距离<em class="ko">d:= d(</em><strong class="jv hj"><em class="ko">x</em></strong><em class="ko">，</em><strong class="jv hj"><em class="ko">x</em></strong><em class="ko">I)</em>对于<em class="ko"> i = 1，…，N</em>；要每分<strong class="jv hj"> <em class="ko"> x </em> </strong></li><li id="6b34" class="lf lg hi jv b jw lo ka lp jg lq jk lr jo ls kn lk ll lm ln bi translated">从<em class="ko"> d </em>中找出k个最小元素的索引及其标签；</li><li id="f867" class="lf lg hi jv b jw lo ka lp jg lq jk lr jo ls kn lk ll lm ln bi translated">return<em class="ko">h(</em><strong class="jv hj"><em class="ko">x</em></strong><em class="ko">)= mode(y1，…，yk)。</em></li></ol><h2 id="5099" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">距离</h2><p id="9d7b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">到目前为止，我们一直使用符号<em class="ko"> d </em>来表示距离。最常见的选择包括闵可夫斯基距离的特殊情况:</p><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/b7010ebcf44fae76d95370c77f21f8cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*Pf3fp9icANcadAEDxvOiqw.png"/></div></figure><ul class=""><li id="c909" class="lf lg hi jv b jw kp ka kq jg lh jk li jo lj kn lu ll lm ln bi translated">对于p = 1，它叫做曼哈顿距离；</li><li id="282f" class="lf lg hi jv b jw lo ka lp jg lq jk lr jo ls kn lu ll lm ln bi translated">对于p = 2，它被称为欧几里德距离；</li><li id="4b87" class="lf lg hi jv b jw lo ka lp jg lq jk lr jo ls kn lu ll lm ln bi translated">对于<em class="ko"> p </em> → ∞返回向量分量之间的最大差值。如果你想证明，你可以做到。</li></ul><p id="cea3" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated"><strong class="jv hj"> <em class="ko">一个提示</em> </strong>:当<em class="ko"> p </em> → ∞时，可以用极限三明治定理证明上面的结果。🙂</p><h2 id="74a1" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">对你的挑战</h2><p id="744d" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">文件<a class="ae iu" href="https://www.dropbox.com/s/vzsg86s39z1cncx/data-qs1-1.csv?dl=0" rel="noopener ugc nofollow" target="_blank">https://www.dropbox.com/s/vzsg86s39z1cncx/data-qs1-1.csv?dl=0 </a>有一个二元分类问题的训练样本，X<strong class="jv hj"><em class="ko"/></strong><em class="ko">= R</em><em class="ko">Y = { 0，1}。</em>数据是以<strong class="jv hj">的形式出现的。包含三列的csv </strong>文件:</p><p id="b6ad" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">1.<strong class="jv hj">列1 </strong>:数值属性1(<strong class="jv hj"><em class="ko">x1</em></strong>)；</p><p id="5d09" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">2.<strong class="jv hj">第2列</strong>:数值属性2(<strong class="jv hj"><em class="ko">x2</em></strong>)；</p><p id="f3fd" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">3.<strong class="jv hj">第3列</strong> : class (y)。</p><p id="acc1" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">考虑到<em class="ko"> k </em> -NN分类器，对于<em class="ko"> k </em> = 1，3，9，27，在区间【1，1】<em class="ko"/>中画出决策边界。对于问题中的<strong class="jv hj"> <em class="ko"> k </em> </strong>值的影响，你的结论是什么？</p><p id="6990" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">推荐你用python编程语言。<strong class="jv hj"> <em class="ko">拜托:</em> </strong>不要用sklearn的<em class="ko"> k </em> -NN的库。创建自己的<em class="ko"> k </em> -NN算法，使用欧氏距离。</p><figure class="kv kw kx ky fd ij"><div class="bz dy l di"><div class="lv la l"/></div></figure><h2 id="0372" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">给你更多建议</h2><p id="3e8c" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">当您完成自己的<em class="ko"> k </em> -NN分类器算法时，您的图形应该如下图所示。😊</p><h2 id="925d" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">k = 1时[-1，1]之间的判定边界</h2><pre class="kv kw kx ky fd lw lx ly lz aw ma bi"><span id="4a2e" class="iv iw hi lx b fi mb mc l md me">k = 1<br/>hypothesis = knn(k)<br/>plot_decision_boundary(k, hypothesis)</span></pre><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/6de3b9beb69be9004e692a3ddc42ed6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*BNRTIhFFktH5WWWFPESE0g.png"/></div></figure><h2 id="e9a4" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">k = 3时[-1，1]之间的判定边界</h2><pre class="kv kw kx ky fd lw lx ly lz aw ma bi"><span id="42e4" class="iv iw hi lx b fi mb mc l md me">k = 3<br/>hypothesis = knn(k)<br/>plot_decision_boundary(k, hypothesis)</span></pre><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/f2b2dc78cb62e331ae6bb0b9b7217774.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*KnjgihWHqhKb9JbTT0r-Gg.png"/></div></figure><h2 id="d267" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">k = 9时[-1，1]之间的判定边界</h2><pre class="kv kw kx ky fd lw lx ly lz aw ma bi"><span id="aa01" class="iv iw hi lx b fi mb mc l md me">k = 9<br/>hypothesis = knn(k)<br/>plot_decision_boundary(k, hypothesis)</span></pre><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/50f9e3a4b5b80b12303081f6bfdaf367.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*YRm65K6ZcusTTxsrA3sZTg.png"/></div></figure><h2 id="fed4" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">k = 27时[-1，1]之间的判定边界</h2><pre class="kv kw kx ky fd lw lx ly lz aw ma bi"><span id="d2c9" class="iv iw hi lx b fi mb mc l md me">k = 27<br/>hypothesis = knn(k)<br/>plot_decision_boundary(k, hypothesis)</span></pre><figure class="kv kw kx ky fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/dfa2e36192f73f255588e30ea3f583fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*EmMeay8atzwP6ScKsvd5gA.png"/></div></figure><h2 id="736b" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">结论</h2><p id="2c75" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn hb bi translated">在本文中，我们看到了如何使用人工数据集实现k  -NN分类器算法。我们还看到结果很有趣，当我们增加(1，3，9，27奇数)中的<em class="ko"> k </em>值时，决策边界变得更加可分。决策边界保持更加平滑，k等于27，因为它几乎可以被一条直线分割。</p><p id="c9c1" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">你可能想知道的一些问题是:我如何选择k的大小？为什么k等于1不是一个好的选择？为什么k等于我的数据集的大小不是一个好的选择？我的数据集的大小对我选择<em class="ko"> k </em>有影响吗？思考这些问题，找到答案。另外，我希望你搜索一下关于<em class="ko"> k </em> -NN的利弊。</p><p id="d92d" class="pw-post-body-paragraph jt ju hi jv b jw kp jy jz ka kq kc kd jg kr kf kg jk ks ki kj jo kt kl km kn hb bi translated">很神奇吧？我希望你喜欢阅读这篇文章。谢谢大家！</p></div></div>    
</body>
</html>