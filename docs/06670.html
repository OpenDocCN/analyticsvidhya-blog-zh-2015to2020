<html>
<head>
<title>Sequence Learning with RNN’s</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RNN氏症的序列学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/sequence-learning-with-rnns-e672386a195e?source=collection_archive---------24-----------------------#2020-05-29">https://medium.com/analytics-vidhya/sequence-learning-with-rnns-e672386a195e?source=collection_archive---------24-----------------------#2020-05-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="1cec" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">你有没有想过谷歌翻译是如何在一瞬间将句子从一种语言翻译成另一种语言的？你有没有想过一些拼写检查应用程序是如何理解你的句子的语义并提出纠正建议的？你来对地方了。</h2></div><p id="54c4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇博文中，我们将详细讨论递归神经网络的力量，以及为什么它是下一个大事件？(已经是了！)</p><h2 id="52ad" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">RNN氏症的需要？</h2><p id="82b5" class="pw-post-body-paragraph ix iy hi iz b ja ko ij jc jd kp im jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">深度神经网络(DNN)是非常强大的ML模型。他们被称为<strong class="iz hj">通用函数逼近器</strong>。让我们把注意力集中到“<strong class="iz hj">逼近者”</strong>这个词上。理解数据的一种简单方法如下。我们今天拥有的每一份数据都是某种行为的结果。</p><p id="b926" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="kt">例句:你按下音量键，手机的音量就会增大。音量增加的速度取决于您按住该按钮的时间或您点按该按钮的次数。</em></p><p id="e6a3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，按下按钮的动作就是你如何创建数据，而你按下按钮的方式决定了音量如何增加。这就是你对数据应用的函数。我可以通过<strong class="iz hj">逼近数据中的函数</strong>来理解/想象你按下按钮的方式。</p><p id="5438" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是DNN的问题是他们需要一个固定的输入维度。在句子翻译任务中，用户可以输入任意长度的字符。您不能将其长度限制为一个常数。同样，在语音识别中，用户可以说出任何长度的句子。这是RNN的闪光点。</p><h2 id="0e96" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">体系结构</h2><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ku"><img src="../Images/a6d07a5cb670971a57a8a457b108a1be.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*_2GPZm4R7olFvzsMFMKY3Q.png"/></div><figcaption class="lc ld et er es le lf bd b be z dx translated">一个RNN细胞</figcaption></figure><p id="4d5c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设我们的输入是一段文本。例句<em class="kt"> : </em>“橙色是新的黑色”。如果在时间步长= 1时首先将单词“Orange”传入网络。RNN接受该输入，对该输入应用一个函数，并在时间步长= 2时将转换后的输入传递给自身，此时传递下一个单词“is”。所以在这个整个过程中有一个输入，前一个状态输出和一个函数。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lg"><img src="../Images/49a97c6589252d1be4b2f20b3934fcd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*OrgmhpLqHo-WqeD1EnVONA.png"/></div></figure><p id="80b7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这里，<strong class="iz hj"> <em class="kt"> xt </em> </strong>是我们输入的“是”，<strong class="iz hj"> <em class="kt"> ht-1 </em> </strong>是我们输入的“橙”的变换版本，<strong class="iz hj"> <em class="kt"> fw </em> </strong>是我们应用在“橙”上的变换来变换它。很简单。下一步是实际定义我们的转换，定义我们的计算，这样我们得到我们的<strong class="iz hj"> <em class="kt"> ht-1 </em> </strong>等等。如下所示</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lh"><img src="../Images/115e1d5d61c19a7e491b2baae4e1d6f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*VceL4AhfIK2QUHcEmrN21g.png"/></div></figure><p id="4a8e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里<strong class="iz hj"> <em class="kt"> W_hh </em> </strong>是我们的隐藏状态权重矩阵，而<strong class="iz hj"> <em class="kt"> W_xh </em> </strong>将是我们的输入状态权重矩阵。下图显示了这些重量的确切位置。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es li"><img src="../Images/38b0ff0646a9c65872a8b9439a646287.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*0s32N2qA7sCb45KGYzl6CA.png"/></div></figure><p id="ffc1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"><em class="kt"/></strong>是一种类似乙状结肠激活的激活。它给传入的输入增加了一些非线性。它将输入值压缩在1和-1之间。因此，对于我们输入的每个单词，同样的步骤被反复执行，直到我们用完所有的单词。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lj"><img src="../Images/bca26e75650684d8011a35a4c9c2678e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8nRb34LYi4n08Tn4GllaNg.png"/></div></div></figure><h2 id="92e2" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak">别搞混了</strong></h2><p id="8439" class="pw-post-body-paragraph ix iy hi iz b ja ko ij jc jd kp im jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">在上面的图像中，我们展示了多个RNN细胞来全面描述正向传播过程。这只是为了说明的目的。但实际上，它只是一个单RNN单元，在随后的时间步接受输入。把x轴当作我们的时间轴。我们只是在时间中放松。</p><h2 id="cd6e" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated"><strong class="ak">正向传播</strong></h2><p id="789c" class="pw-post-body-paragraph ix iy hi iz b ja ko ij jc jd kp im jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">如果你读到这里。恭喜你。您已经完成了正向传播。正向传播是你在每个时间步将例句“橙色是新的黑色”中的每个单词发送到我们的RNN单元。一旦我们不再输入单词，我们就计算出<strong class="iz hj">损失或成本。我假设你熟悉损失函数，并且以前已经使用过。</strong></p><blockquote class="lo lp lq"><p id="e14d" class="ix iy kt iz b ja jb ij jc jd je im jf lr jh ji jj ls jl jm jn lt jp jq jr js hb bi translated">损失函数是一个数学函数，它告诉我们的预测与实际输出相比有多差。</p></blockquote><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lu"><img src="../Images/79ad517eff0b50a448d7df396ccb6665.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NyIyVPb1iDi4ZFhH7vxv7g.png"/></div></div></figure><p id="3b13" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里<strong class="iz hj"> <em class="kt"> y_i </em> </strong>是我们的正确输出<strong class="iz hj"> <em class="kt"> y^i </em> </strong>是模型输出。我们将两者都传递给损失函数，损失函数会返回一个分数，表示我们的模型对数据的逼近程度。</p><h2 id="c100" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">反向传播(通过时间)</h2><p id="8b4c" class="pw-post-body-paragraph ix iy hi iz b ja ko ij jc jd kp im jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">在RNN氏症的情况下，标准反向传播变得有趣。因为正向传播在时间步长上是独立发生的。甚至我们调整权重的反向传播也发生在每个时间步长。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lv"><img src="../Images/1f071cb9bad4893bd8a9acbb000418f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VtTGC74HJLHLRPsmQaigpQ.png"/></div></div></figure><p id="18d3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们进入偏微分方程之前，如果你注意到上图在输出端有一个权重矩阵<strong class="iz hj"><em class="kt"/></strong><em class="kt">。</em>下面是我们推导出的偏导数方程来更新权数</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lw"><img src="../Images/cf2b8c47ab771d1d7d2946f8f2913964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*Lq7_6ntBVHfoo_j9kwhRkA.png"/></div></figure><h2 id="8a54" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">缺点</h2><p id="f842" class="pw-post-body-paragraph ix iy hi iz b ja ko ij jc jd kp im jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">瓦尼拉·RNN的问题在于，当我们对一长串输入进行这种反向传播时。多重导数相乘的效果导致导数变得非常小，使得第一时间步上的权重几乎不更新。这就是著名的<strong class="iz hj">消失渐变问题。下面是一个小例子，告诉你如何可视化消失的渐变</strong></p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lx"><img src="../Images/e1820a6b6839bbdd5d18c92e13e1bd5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*g9fxxQUoushN4LoAsT1cgQ.png"/></div></figure><p id="fbe8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我用5个小于1的数相乘，得到0.0001。这就是RNN的情况。因此，梯度变得非常小，当权重更新时，它们几乎不变。另一个有趣的问题是<strong class="iz hj">爆炸梯度问题</strong>梯度是如此之大，以至于我们乘以多个大数，结果是一个非常大的数。</p><h2 id="deef" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">那么下一步是什么？</h2><p id="3ba3" class="pw-post-body-paragraph ix iy hi iz b ja ko ij jc jd kp im jf jg kq ji jj jk kr jm jn jo ks jq jr js hb bi translated">为了解决这些问题，研究人员想出了一种新的改良RNN，叫做<strong class="iz hj"> LSTM。LSTM的比香草RNN的更有力量。我会把LSTM的问题称为顺序问题的结果。比如skip连接如何帮助Resnet控制信息流。LSTM的“遗忘之门”让他们能够忘记过去的不相关信息，从而加强各层之间的信息流动。</strong></p><h2 id="53c4" class="jt ju hi bd jv jw jx jy jz ka kb kc kd jg ke kf kg jk kh ki kj jo kk kl km kn bi translated">信用</h2><ol class=""><li id="545d" class="ly lz hi iz b ja ko jd kp jg ma jk mb jo mc js md me mf mg bi translated">幻灯片(<a class="ae mh" href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf" rel="noopener ugc nofollow" target="_blank">http://cs 231n . Stanford . edu/slides/2017/cs 231n _ 2017 _ lecture 10 . pdf</a>)</li></ol></div></div>    
</body>
</html>