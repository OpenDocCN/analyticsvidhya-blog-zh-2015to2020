# Kaggle 竞赛的特征工程

> 原文：<https://medium.com/analytics-vidhya/feature-engineering-for-kaggle-competition-5616196bf274?source=collection_archive---------9----------------------->

# 0.介绍

根据维基百科:

*特征工程是使用数据的领域知识来创建使机器学习算法工作的特征的过程。特征工程是机器学习应用的基础，既困难又昂贵。*

特征工程是大多数 Kaggle 竞赛的重要步骤。本文将讨论四种不同特征类型的常用特征工程技术:数值型、分类型、时间型和空间型数据。这些技巧帮助我获得了 Kaggle 竞赛专家的头衔。

# 1.特征类型

在开始特性工程之前，我们应该先了解不同的特性类型。在常见的 Kaggle 竞赛或数据科学项目中，我们可能会处理以下类型的功能:

-数字(区间或比率)

-分类(序数或名词)

-时间(日期和时间)

-空间(坐标和位置)

数字特征衡量一个量的大小，如银行存款余额、温度、年龄、身高、体重、人口…

有两种不同的数字特征:间隔和比率。如果一个数值有一个有意义的(非任意的)零点，那么它就是比值，否则就是区间。例如，摄氏温度的零点是任意的，没有意义，因此它是一个区间值。我们不能说 20 摄氏度是 10 摄氏度的两倍，因为它们不成比例。然而，具有有意义的零点的银行余额是一个比值。这意味着我们可以说 200 美元是 100 美元的两倍。

**分类**值也叫离散变量。它们描述一个主题的组或类别，如性别、产品类型、城市名称。所有这些也被称为**名义**变量。相比之下，另一种分类变量叫做顺序变量。序数变量是排序的范畴变量，如偏好得分、顾客评分。

不同类型的变量需要不同的特征工程技术，这将在下面介绍。

更多阅读:[https://en.m.wikipedia.org/wiki/Statistical_data_type](https://en.m.wikipedia.org/wiki/Statistical_data_type)

# 2.数字特征

使用的特征工程方法取决于我们使用的机器学习模型。而对于大多数基于树的模型(如回归树、随机森林、xgboost、lightGBM)，我们不需要做任何特征工程。因为基于树的模型基于不同值的比较进行预测，而不是基于数值的大小。

非基于树的模型，如线性回归、支持向量机和神经网络模型，基于数值的大小进行预测。这意味着这些模型对输入值的大小很敏感。

为了解决这个问题，可以使用缩放器将数值变量缩放到一个标准化的范围内，例如 Python scikit-learn 包中的**minmax scaler/standard scaler**。

此外，数值中的异常值会给非基于树的模型带来问题。解决异常值的一个简单方法是从数据集中删除它们。其他解决方案还有 **Winsorization** 和**排位**。Winsorization 使用下限和上限裁剪数值。排名方法用数值在数据集中的相对排名替换数值。

此外，具有偏斜分布的数字特征也会给非基于树的模型带来问题。例如，一些数值变量是对数正态分布的，如货币值。在这些情况下，我们可以应用对数变换来生成正态分布变量。根据变量分布，可以使用其他变换技术，例如 0.5 次幂。

此外，许多特征生成方法可以与数字变量一起使用，主要基于**先验知识/数据**分析。例如，使用描述超市中商品价格的数据集，我们可以从价格值中提取小数部分(例如从 4.99 中提取 0.99)，这可能会影响人们对价格的感知。

更多阅读:[https://towards data science . com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e 47099 a7b](https://towardsdatascience.com/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b)

# 3.分类特征

分类特征是数据科学项目中最重要的特征之一。因为分类变量不是数字，不能被大多数机器学习模型处理，所以我们必须对它们进行编码，并用数字的方式表示它们。

分类特征有许多编码方法。最广为人知的是一键编码。它为特定列 N 个类别创建 N 个新列。这些新创建的列中的 1 或 0 值表示该类别在该行中是否存在。然而，在实践中，一次性编码并不能很好地工作，因为它会导致共线性问题。此外，当分类列的基数非常高时，一个热编码将生成非常稀疏的数据集，这对于机器学习模型来说是不好的。

共线性问题:[https://www . algo some . com/articles/dummy-variable-trap-regression . html](https://www.algosome.com/articles/dummy-variable-trap-regression.html)

分类编码的另一种方法是**数字编码**，它只是用正整数对一列中的每个类别进行编号。例如，我们可以将三个类别 A/B/C 编码成 1/2/3。它不会增加数据大小，并且非常适合基于树的模型。并且它是 lightGBM 模型的推荐编码方法。

[https://light GBM . readthedocs . io/en/latest/Advanced-topics . html](https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html)

然而，数字编码不能很好地处理非基于树的模型。解决方案之一是二进制编码。二进制编码将正整数从数字编码转换为二进制形式，即 0 和 1 的序列。然后创建几个新列来存储这些二进制序列。它看起来像一个热编码的压缩版本，创建了一个密集的数据集，可以避免共线性问题。

几种特征生成方法也适用于分类特征。一种是频率编码，它计算分类变量中每个标签的频率，并使用相应的计算频率创建一个新列。例如，如果在一个分类变量中包含 5 个标签 A，3 个标签 B，2 个标签 C，那么 A/B/C 可以分别用 0.5/0.3/0.2 来编码。这背后的逻辑是，在某些情况下，重要的是一个标签的受欢迎程度，而不是标签本身。

另一种类似的编码方法是均值编码。它计算分类变量中每个标签的预测目标的平均值。那么它可以作为一个新功能。除了平均值之外，诸如中值或众数之类的其他聚合统计也可以用于目标编码。对于目标编码要注意的一个重要的事情是，平均值应该仅在训练/验证分割之后从训练数据计算。否则模型可以预见验证数据，导致数据泄露。

分类变量的最后一件事是特征交叉。对于许多非基于树的模型，没有考虑输入变量之间的交互，这意味着我们需要进行自己的特征工程，并创建能够表示这些交互的特征。通常我们可以简单地将两个或更多的分类变量连接成一个。然而，选择交叉的特征基本上取决于您对数据集和问题的理解。

更多阅读:[https://medium . com/data-design/visiting-category-features-and-encoding-in-decision-trees-53400 fa 65931](/data-design/visiting-categorical-features-and-encoding-in-decision-trees-53400fa65931)

# 4.时间特征

通常，时间特征是指任何随时间变化的特征，如股票价格和温度。移动平均是产生时间特征的基本和流行的方法之一。这个研究领域被称为时间序列分析，这是一个很大的话题，在这篇短文中不会涉及。

狭义上，时态特征是指数据集中的时间戳，如日期时间、日期或时间。处理这些时间戳的一种常见方法是从时间戳中提取时间属性，如年、月、日、小时、分钟、秒。此外，诸如星期几、一年中的某一天、季节之类的属性在某些情况下也会有所帮助。如果我们知道数据集来自哪个国家或地区，我们可以将每一天标记为假日或非假日，这对一些商业案例研究特别有帮助。

除了时间属性之外，日期之间的差异也可能是要生成的重要特征。例如，从当前日期到下一个假期的天数可以是客户行为预测的重要特征。并且自上次重要事件以来的持续时间可以与其对我们的研究对象的影响相关。

# 5.空间坐标数据

空间数据是指包含空间信息的任何数据，如地图、GPS 位置或轨迹，甚至是卫星图像。同样，这是另一个名为空间信息科学的大主题，无法在这篇短文中涵盖。但是，在处理空间数据时，仍有一些要点需要了解。

空间数据的一个重要属性是它描述了球体上的位置。我们不应该使用平面几何中的任何简单方程。例如，在计算由经度和纬度描述的 to 位置之间的距离时，sqrt((lat1-lat2) +(lon1-lon2))完全不正确。最好的公式是大圆公式。

[https://en.m.wikipedia.org/wiki/Great-circle_distance](https://en.m.wikipedia.org/wiki/Great-circle_distance)

另一个要考虑的属性是空间相关性，这也被称为托布勒地理第一定律:“一切都与其他一切相关，但近的东西比远的东西更相关。”当我们预测一个具有空间信息的主题时，我们通常可以使用其附近的信息作为参考或模型训练样本。例如，房屋的价格可能与附近房屋的价格密切相关。房子和最近的火车站之间的距离也是一个重要因素。

[https://en . m . Wikipedia . org/wiki/to bler % 27s _ first _ law _ of _ geography](https://en.m.wikipedia.org/wiki/Tobler%27s_first_law_of_geography)

# 6.缺少值

最后要注意的是缺少值。通常，基于树的模型可以很好地处理缺失值，只要我们将“nan”或“null”值放入数据集中。但是，这不适用于非基于树的模型。一种解决方案是在数据集中创建另一列来指示特定列中的数据是否缺失，这可能会增加数据量并带来其他问题。

另一种方法是基于相应列的中间值的平均值来填充空值。此外，我们可以建立另一个机器学习模型来预测那些缺失的值。但是，这些重建值可能不准确，可能会降低模型性能。当具有缺失值的数据记录不是很多时，丢弃那些有噪声的记录可能是更好的解决方案。

# 7.包裹

总之，本文讨论了四种不同特征类型的特征工程:数字、分类、时间和空间。

理想情况下，对于非基于树的模型，数字变量应换算成标准正态分布，对于基于树的模型，不需要任何处理。

对于基于树的模型，分类变量最好是数字编码值，对于非基于树的模型，最好是二进制编码值。

时间和空间变量是非常特殊的数据类型，需要特定领域的知识来进行要素处理。

正如我们已经讨论过的，许多特征工程方法取决于您正在使用的模型。并且基于树的模型通常需要较少的特征工程，这也是基于树的模型(如 XGBoost 和 lightGBM)在 Kaggle 比赛中如此受欢迎的原因。

最后但同样重要的是，上述方法只是常用的特征工程方法。在真正的机器学习项目中，特征工程很大程度上取决于研究人员对问题本身的知识。