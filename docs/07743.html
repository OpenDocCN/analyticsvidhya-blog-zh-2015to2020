<html>
<head>
<title>Seq2Seq Models : French to English translation using encoder-decoder model with attention.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Seq2Seq模型:注意使用编码器-解码器模型进行法语到英语的翻译。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/seq2seq-models-french-to-english-translation-using-encoder-decoder-model-with-attention-9c05b2c09af8?source=collection_archive---------6-----------------------#2020-07-06">https://medium.com/analytics-vidhya/seq2seq-models-french-to-english-translation-using-encoder-decoder-model-with-attention-9c05b2c09af8?source=collection_archive---------6-----------------------#2020-07-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="0824" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">具有注意机制的机器翻译的张量流实现。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/50fd7d8f08ad63f4f9356310501fe629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dyaQNNLIW8wbZpooQ8dzRQ.png"/></div></div></figure><h1 id="a2a4" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">目录:</strong></h1><ol class=""><li id="d4f4" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc ku kv kw kx bi translated">序列对序列模型介绍。</li><li id="983e" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">编码器。</li><li id="4e37" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">解码器。</li><li id="c1ed" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">注意。</li><li id="7065" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">代码演练。</li><li id="0c56" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">评估测试数据的性能。</li><li id="1ba8" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">结论。</li><li id="8c0e" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">参考文献。</li></ol><h1 id="2847" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak"> 1-序列对序列模型介绍:</strong></h1><p id="ae9b" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">序列对序列学习就是训练模型，将序列从一个域转换到另一个域。这里需要注意的一点是，两个序列可能长度相同。一个典型的Seq2Seq模型由一个编码器和一个解码器组成，它们本身是两个独立的神经网络，合并成一个巨大的网络。编码器和解码器都是典型的LSTM或GRU模型。</p><p id="b126" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Seq2Seq模型的一些应用是神经机器翻译、图像字幕、语音识别、聊天机器人、时间序列预测等。</p><p id="3c55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">编码器网络的工作是理解输入序列，并创建一个更小的维度表示，然后转发给产生输出的解码器网络。</p><p id="dd4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">编码器的输入可以是编码的句子(在神经机器翻译的情况下)或者图像特征(在图像字幕的情况下)或者甚至是声波(在语音识别的情况下)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lg"><img src="../Images/91c710e808c89c04779e3c77251993f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9p8Z0Pd6yL8w2qfwGK0dcg.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><strong class="bd jr">神经机器翻译</strong></figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ll"><img src="../Images/c94cee3d0b68539e531c2b339a68bd7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4GRzd5ClyGYLPdNVdRLJuA.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><strong class="bd jr">语音识别</strong></figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lm"><img src="../Images/9edd8acb86296050d4e7759af25253d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D3JoyOelmtyowMuoSIkadg.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><strong class="bd jr">图像字幕</strong></figcaption></figure><p id="f268" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇博客中，我将讨论神经机器翻译的编码器-解码器模型。我将训练模型将法语句子翻译成英语句子。</p><h1 id="ae8f" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">2-编码器:</h1><p id="d034" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">让我们了解编码器架构。假设我们的输入句子有“n”个单词。为了简单起见，我们假设语料库中只有一个句子。</p><ul class=""><li id="98cb" class="kn ko hi ih b ii ij im in iq ln iu lo iy lp jc lq kv kw kx bi translated"><strong class="ih hj"> <em class="lr">法语:“我不会说英语”</em></strong></li><li id="5185" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated"><strong class="ih hj"> <em class="lr">翻译:“我不会说英语”</em> </strong></li><li id="a507" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">输入词汇:{Je，ne，parle，pas，anglais} (5个唯一单词)。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ls"><img src="../Images/095c95cfe40142ffb695d5f176a36808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aOE0gPyM9rKcFjL1RfjazA.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><strong class="bd jr">编码器架构</strong></figcaption></figure><ul class=""><li id="104e" class="kn ko hi ih b ii ij im in iq ln iu lo iy lp jc lq kv kw kx bi translated">Xi →由于我们要使用单词级编码，在每个时间戳输入的将是句子中的每个单词。这意味着X1 = '日本'，X2 = '东北'，等等..高达X5 = '英国人'。如果我们使用字符级编码器-解码器模型，那么每个时间戳的输入将是单个字符:X1 : 'J '，X2 : 'e '，X3 : 'n '，等等。每个单词都以向量的形式表示。为此，每个单词都被语料库中该单词的word_index替换。最频繁使用的单词的单词索引小于最不频繁使用的单词。</li><li id="9ad3" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">‘h0’和‘c0’→初始隐藏状态和上下文向量，它们都是零(通常)并且在第0个时间戳被馈送到编码器。在这之后，我们开始输入单词。</li><li id="c95e" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">“hi”和“ci”→时间戳I之后的隐藏状态和上下文向量。这些向量简单地表示编码器在该时间戳之前已经看到的内容。例如，h3和c3会记得网络到目前为止已经看到了“Je ne parle”。这些向量中的每一个的大小等于LSTM/格鲁的单位数。在最后一个时间戳之后获得的状态作为解码器初始状态被馈送到解码器中。</li><li id="9350" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">‘yi’→时间戳I处的输出。这是通过使用Softmax激活函数生成的整个词汇的概率分布。我们不需要来自编码器网络的输出，因此我们将丢弃它们。我们关心的唯一编码器输出是隐藏/上下文向量。</li></ul><p id="054e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="lr">注:上图是我们在时间轴上展开LSTM/GRU电池时的样子。即，它是单个LSTM/GRU单元，在每个时间戳取单个字。我们的编码器/解码器网络中可以有多个这样的单元。</em>T3】</strong></p><h1 id="3e3a" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">3解码器:</h1><p id="71e4" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">在学习了编码器之后，现在让我们进入另一部分:解码器网络。与编码器不同，解码器在训练和推理阶段表现不同。此外，我们需要向输出句子添加两个特殊的标记，原因如下所述。这些记号是“<start>”(在字符串的开头)和“<end>”(在字符串的结尾)。</end></start></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lt"><img src="../Images/68ff9c45c9eefa8f3c828cd0a05eb424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i2gD3Xb3f3Lv6-Iq7gfG3w.png"/></div></div></figure><h2 id="e565" class="lu jq hi bd jr lv lw lx jv ly lz ma jz iq mb mc kd iu md me kh iy mf mg kl mh bi translated">a)。训练阶段的解码器:</h2><p id="0f06" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">注意，编码器的最终状态被设置为解码器的初始状态。在初始状态，我们提供'<start>'作为输入，以便解码器开始生成下一个标记(英语句子的第一个单词)。我们使用一种称为<strong class="ih hj"> <em class="lr">【教师强制】</em> </strong>的技术，其中将来自先前时间戳的实际输出(而不是预测输出)作为输入馈送到当前时间戳。在输入实际翻译的最后一个单词后，我们让我们的解码器学习预测表示翻译结束的'&lt; end &gt;'。这个'&lt; end &gt;'标记在推断阶段充当停止条件。</start></p><ul class=""><li id="f941" class="kn ko hi ih b ii ij im in iq ln iu lo iy lp jc lq kv kw kx bi translated">(编码器+解码器)网络的整个训练可以总结如下图所示:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mi"><img src="../Images/6227d5249d636c313fd5a4422bcdf298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iPlKq8vwt8Vv-kz4TM41Rw.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><strong class="bd jr">整个编解码训练网络</strong></figcaption></figure><h2 id="f774" class="lu jq hi bd jr lv lw lx jv ly lz ma jz iq mb mc kd iu md me kh iy mf mg kl mh bi translated">a)。推断阶段的解码器:</h2><ul class=""><li id="c3be" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc lq kv kw kx bi translated">解码器在训练和推断阶段的唯一区别在于，在推断阶段，来自先前时间戳的预测输出(而不是与训练阶段不同的实际输出)被作为输入馈送到当前时间戳。休息和训练阶段一样。</li><li id="089c" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">下图总结了整个推理过程:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mi"><img src="../Images/c52d95c1ca366d2bdff2e01cc7a17a9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dSk_CKlpwpIEu_wL31BtcQ.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><strong class="bd jr">推理网络</strong></figcaption></figure><h1 id="b18c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">立正！</h1><ul class=""><li id="4cb8" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc lq kv kw kx bi translated">到目前为止所讨论的都是没有注意机制的简单的编码器-解码器模型。</li><li id="b7e1" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">这种模式的一个主要缺点是，一旦他们进一步处理，他们往往会忘记序列的早期部分。对于较长的句子是不好的。请看下图:</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mj"><img src="../Images/f6d7c9d3c856e32ee7ef081137ba461d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_VDrVBZR67P7owGX5z3nzQ.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><strong class="bd jr">句子长度vs BLEU分数</strong></figcaption></figure><ul class=""><li id="1279" class="kn ko hi ih b ii ij im in iq ln iu lo iy lp jc lq kv kw kx bi translated">上图是句子长度与BLEU分数的关系图。第一个模型(RNNSearch-50)使用注意机制，而其余3个模型不使用。</li><li id="ddc3" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">我们可以清楚地看到，随着句子长度的增加，后3个模型的BLEU得分下降，而RNNSearch-50的BLEU得分保持稳定。因此，在处理较长的序列时，注意力是一个非常重要的方面。我们就在这一节讨论吧。</li></ul><h1 id="656b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">注意力是如何工作的？</h1><p id="9745" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq ld is it iu le iw ix iy lf ja jb jc hb bi translated">让我们以前面的例子为例，一步一步地理解它:</p><p id="cb78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="lr">法语:“我不会说英语”</em> </strong></p><p id="3bba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="lr">翻译:“我不会说英语”</em> </strong></p><p id="19c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下图显示了它们对应的隐藏编码器状态:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mk"><img src="../Images/eca8119e3a86896d3a15b521a0d52166.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CeV8nGMYBUCvvlMbpG-W6A.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><strong class="bd jr">编码器GRU </strong></figcaption></figure><ul class=""><li id="d7c0" class="kn ko hi ih b ii ij im in iq ln iu lo iy lp jc lq kv kw kx bi translated"><strong class="ih hj">获取编码器隐藏状态:</strong>首先从编码器获取每个时间戳后的隐藏状态，如上图所示。</li><li id="1cc2" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated"><strong class="ih hj">将编码器最终状态设置为解码器初始状态:</strong>由于我们试图预测输出序列的第一个字，解码器将不会有任何当前内部状态。为此，我们将使用最终编码器状态(h5)作为初始解码器状态。</li><li id="7e6f" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated"><strong class="ih hj">计算分数:</strong>现在，使用所有编码器状态和当前解码器状态，我们训练一个简单的前馈神经网络，该网络将通过为相关编码器状态生成高分并为不相关状态生成低分来学习识别相关编码器状态。例如，为了预测单词“speak”，相关信息可以处于状态h1、h2、h3，而剩余的状态h4和h5可能是不相关的。我们的前馈神经网络将学习给前3个状态高分，给后2个状态低分。设这些分数分别为[s1，s2，s3，s4，s5 ]。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ml"><img src="../Images/677f846d1d42893258ed9a5cf018ccd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0xDyW4QAz322kQ5NYpavdg.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><strong class="bd jr">计算分数</strong></figcaption></figure><ul class=""><li id="8033" class="kn ko hi ih b ii ij im in iq ln iu lo iy lp jc lq kv kw kx bi translated"><strong class="ih hj">获取关注权重:</strong>将前一阶段得到的分数送入<strong class="ih hj"><em class="lr">【soft max】</em></strong>函数，获取关注权重。设这些权重为e = [e1，e2，e3，e4，e5]。所有这些权重的总和等于1。因此，他们给出了一个很好的概率解释。</li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mm"><img src="../Images/f521ef1ba063be1ae9e3d29d9ef3b0e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nIUcA-lkQwBXpM3C8fRBmg.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated"><strong class="bd jr">从softmax获取注意力权重</strong></figcaption></figure><ul class=""><li id="5c22" class="kn ko hi ih b ii ij im in iq ln iu lo iy lp jc lq kv kw kx bi translated"><strong class="ih hj">计算上下文向量:</strong>得到关注权重后，上下文向量计算如下。它将被解码器用来预测下一个字。</li></ul><blockquote class="mn mo mp"><p id="b4ea" class="if ig lr ih b ii ij ik il im in io ip mq ir is it mr iv iw ix ms iz ja jb jc hb bi translated"><strong class="ih hj">context-vec(cv)= E1 * h1+e2h 2+E3 * H3+E4 * H4+E5 H5</strong></p></blockquote><ul class=""><li id="47ad" class="kn ko hi ih b ii ij im in iq ln iu lo iy lp jc lq kv kw kx bi translated"><strong class="ih hj">将上下文向量与先前的输出连接:</strong>对于第一个时间戳，我们没有任何先前的输出，因此我们将把<strong class="ih hj"> <em class="lr"> ' &lt;开始&gt; ' </em> </strong>与上下文向量连接起来。然后，我们将合并后的向量提供给解码器，解码器将使用它来预测下一个单词。</li></ul><h1 id="11bb" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">代码演练:</h1><ul class=""><li id="931e" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc lq kv kw kx bi translated">首先，像任何其他NLP任务一样，我们加载文本数据并执行预处理，还进行训练测试分割。</li><li id="09f2" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">作为清理的一部分，我们从文本数据中删除html标签、数字和不需要的符号。</li><li id="709f" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">从这个<a class="ae mt" href="http://www.manythings.org/anki/" rel="noopener ugc nofollow" target="_blank">链接</a>下载法语到英语的数据集。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mu mv l"/></div></figure><ul class=""><li id="0cd5" class="kn ko hi ih b ii ij im in iq ln iu lo iy lp jc lq kv kw kx bi translated">标记化和填充。</li><li id="d62a" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">执行填充以使所有句子的长度相同(等于max_len)。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mu mv l"/></div></figure><ul class=""><li id="55aa" class="kn ko hi ih b ii ij im in iq ln iu lo iy lp jc lq kv kw kx bi translated">使用tensorflow.data创建数据输入管道。请参考此<a class="ae mt" href="https://www.tensorflow.org/guide/data" rel="noopener ugc nofollow" target="_blank">链接</a>了解有关tensorflow数据的更多信息。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mu mv l"/></div></figure><ul class=""><li id="5f47" class="kn ko hi ih b ii ij im in iq ln iu lo iy lp jc lq kv kw kx bi translated">定义编码器和解码器架构。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mu mv l"/></div></figure><ul class=""><li id="d985" class="kn ko hi ih b ii ij im in iq ln iu lo iy lp jc lq kv kw kx bi translated">定义优化器、损失函数和设置检查点目录路径以在训练时保存进度。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mu mv l"/></div></figure><ul class=""><li id="66ac" class="kn ko hi ih b ii ij im in iq ln iu lo iy lp jc lq kv kw kx bi translated">最后，是训练循环。它使用上面讨论的教师强制概念进行训练。</li><li id="6458" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">它也在每一秒保存进度。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mu mv l"/></div></figure><ul class=""><li id="f70e" class="kn ko hi ih b ii ij im in iq ln iu lo iy lp jc lq kv kw kx bi translated">用于评估测试数据和绘制注意力权重的函数:</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mu mv l"/></div></figure><h1 id="379c" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">评估测试数据:</h1><ul class=""><li id="39dd" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc lq kv kw kx bi translated">我们的翻译将根据BLEU(双语评估替角)评分进行评估。它是一个介于0和1之间的值。BLEU分数越接近1越好。要了解更多信息，请参考<a class="ae mt" href="https://en.wikipedia.org/wiki/BLEU" rel="noopener ugc nofollow" target="_blank">本</a>和<a class="ae mt" href="https://towardsdatascience.com/evaluating-text-output-in-nlp-bleu-at-your-own-risk-e8609665a213" rel="noopener" target="_blank">本</a>链接。</li><li id="4fbd" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">我已经训练了6个时期的模型，损失在0.05左右。你可以看到有些翻译很完美，有些没那么好。</li></ul><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mu mv l"/></div></figure><h1 id="5022" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">结论:</h1><ul class=""><li id="a728" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc lq kv kw kx bi translated">测试数据的翻译相当准确。他们中的一些人没有达到标准</li><li id="8d6a" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">我们可以添加更多的数据，并运行大量的时期，以获得更好的翻译。</li><li id="2e85" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">我们也可以使用不同的注意力得分函数(点和一般得分函数)。</li><li id="18bf" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc lq kv kw kx bi translated">使用双向包装器的LSTM代替GRU也可以大大提高翻译。</li></ul><h1 id="75bb" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">参考资料:</h1><ol class=""><li id="922d" class="kn ko hi ih b ii kp im kq iq kr iu ks iy kt jc ku kv kw kx bi translated"><a class="ae mt" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1409.0473.pdf</a></li><li id="497c" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><a class="ae mt" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1409.3215</a></li><li id="f622" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><a class="ae mt" rel="noopener" href="/@martin.monperrus/sequence-to-sequence-learning-program-repair-e39dc5c0119b">https://medium . com/@ Martin . monperrus/sequence-to-sequence-learning-program-repair-e 39 DC 5c 0119 b</a>(使用seq-to-seq学习进行程序修复)。</li><li id="e589" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><a class="ae mt" href="http://www.manythings.org/anki/" rel="noopener ugc nofollow" target="_blank">http://www.manythings.org/anki/</a>(各种数据集链接)</li><li id="0637" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">Keras序列对序列学习教程。</li><li id="8ed6" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><a class="ae mt" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention" rel="noopener ugc nofollow" target="_blank">https://www . tensor flow . org/API _ docs/python/TF/keras/layers/Attention</a>(tensor flow关注)</li><li id="b44b" class="kn ko hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><a class="ae mt" href="https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39" rel="noopener" target="_blank">https://towards data science . com/light-on-math-ml-attention-with-keras-DC 8 db C1 fad 39</a></li></ol></div></div>    
</body>
</html>