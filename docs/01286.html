<html>
<head>
<title>An Intuitive Explanation of DeepWalk</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对DeepWalk的直观解释</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/an-intuitive-explanation-of-deepwalk-84177f7f2b72?source=collection_archive---------0-----------------------#2019-10-13">https://medium.com/analytics-vidhya/an-intuitive-explanation-of-deepwalk-84177f7f2b72?source=collection_archive---------0-----------------------#2019-10-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="5d39" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">网络上的机器学习有明显的优势。在这个故事中，我们讨论DeepWalk来学习节点嵌入。</h2></div><h1 id="7b22" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">介绍</h1><p id="bfbd" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">这个故事是几何深度学习系列<em class="kl">不同方法的第二个。</em>在第一篇文章中，我们简要介绍了网络和几何深度学习(GDL ),展示了其优于传统ML方法的优势。这是链接，如果你感兴趣的话。</p><div class="km kn ez fb ko kp"><a rel="noopener follow" target="_blank" href="/@rizaozcelik96/different-methods-in-geometric-deep-learning-part-1-fa64f0deb3b5"><div class="kq ab dw"><div class="kr ab ks cl cj kt"><h2 class="bd hj fi z dy ku ea eb kv ed ef hh bi translated">几何深度学习的不同方法—第一部分</h2><div class="kw l"><h3 class="bd b fi z dy ku ea eb kv ed ef dx translated">这里我们开始一个系列，讨论不同网络类型的各种几何深度学习算法。</h3></div><div class="kx l"><p class="bd b fp z dy ku ea eb kv ed ef dx translated">medium.com</p></div></div><div class="ky l"><div class="kz l la lb lc ky ld le kp"/></div></div></a></div><p id="6413" class="pw-post-body-paragraph jp jq hi jr b js lf ij ju jv lg im jx jy lh ka kb kc li ke kf kg lj ki kj kk hb bi translated">在这个故事中，我们将讨论一种称为DeepWalk[1]的开创性方法，该方法使用语言建模方法，通过利用网络中的本地结构来学习节点嵌入。有了节点嵌入，我们可以使用适当的最大似然方法来完成GDL任务，例如节点分类、链路预测和社区检测任务。</p><p id="6f9b" class="pw-post-body-paragraph jp jq hi jr b js lf ij ju jv lg im jx jy lh ka kb kc li ke kf kg lj ki kj kk hb bi translated">DeepWalk是一个两阶段的方法。在第一阶段，它通过随机行走遍历网络，根据邻域关系推断局部结构。在第二阶段，它使用一种称为SkipGram的算法来学习嵌入，这些嵌入通过推断的结构来丰富。在接下来的部分中，我们首先描述这两个阶段，然后强调DeepWalk的优势。</p><h1 id="8ab1" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">阶段1 —局部结构发现</h1><p id="1cce" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">在这个阶段，我们的目标是发现网络中的邻居。为此，我们从每个节点开始生成固定数量的<em class="kl"> (k) </em>随机行走。每次步行的长度<em class="kl"> (l) </em>是预先确定的。因此，当这个阶段结束时，我们获得长度为<em class="kl"> l </em>的<em class="kl"> k </em>节点序列。随机游走生成的要点是以下假设:</p><blockquote class="lk"><p id="4daa" class="ll lm hi bd ln lo lp lq lr ls lt kk dx translated">相邻节点是相似的，应该有相似的嵌入。</p></blockquote><p id="9c18" class="pw-post-body-paragraph jp jq hi jr b js lu ij ju jv lv im jx jy lw ka kb kc lx ke kf kg ly ki kj kk hb bi translated">基于这一假设，我们将接受在路径中同时出现的节点作为相似节点。这意味着随机行走中的共现频率是节点相似性的指标。请注意，这是一个合理的假设，因为根据设计，边通常表示网络中相似或相互作用的节点。</p><figure class="ma mb mc md fd me er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es lz"><img src="../Images/a685f5c5a35d91caa53e7c7875f2e253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_j6NxkVNEzwR-_cSrQM2Fw.png"/></div></div><figcaption class="mk ml et er es mm mn bd b be z dx translated">网络上长度为5的随机漫步示例。</figcaption></figure><p id="72be" class="pw-post-body-paragraph jp jq hi jr b js lf ij ju jv lg im jx jy lh ka kb kc li ke kf kg lj ki kj kk hb bi translated">这里<em class="kl"> k </em>和<em class="kl"> l </em>的作用很重要。k<em class="kl">增加得越多，网络被探索得越多，因为产生了更多的随机行走。另一方面，当<em class="kl"> l </em>增加时，路径变得更长，并且更远的节点被接受为相似节点。这对应于放松相似性约束，并且会引入噪声和误导性的共现。</em></p><h1 id="3019" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">第二阶段—skip program</h1><p id="df90" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">SkipGram算法是一种用于学习单词嵌入的流行技术。是米科洛夫等人介绍的。艾尔。在他们关于word2vec的著名论文中[2]。给定语料库和窗口大小，SkipGram旨在最大化出现在同一窗口中的单词的单词嵌入的相似性。这些窗口在NLP中也被称为<em class="kl">上下文</em>。这个想法是基于这样的假设:</p><blockquote class="lk"><p id="d77c" class="ll lm hi bd ln lo lp lq lr ls lt kk dx translated">出现在相同上下文中的单词往往有相近的意思。因此，它们的嵌入也应该彼此接近。</p></blockquote><p id="757b" class="pw-post-body-paragraph jp jq hi jr b js lu ij ju jv lv im jx jy lw ka kb kc lx ke kf kg ly ki kj kk hb bi translated">如果你愿意学习更多关于word2vec或SkipGram的知识，我强烈推荐<a class="ae mo" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>。</p><figure class="ma mb mc md fd me er es paragraph-image"><div class="er es mp"><img src="../Images/0552063885498a6ec06f731499d553e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/1*jeblg1fXxREXSUKvfB23XQ.png"/></div><figcaption class="mk ml et er es mm mn bd b be z dx translated">在SkipGram中，目标是在给定单词本身的情况下预测单词的上下文。<a class="ae mo" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a></figcaption></figure><p id="2885" class="pw-post-body-paragraph jp jq hi jr b js lf ij ju jv lg im jx jy lh ka kb kc li ke kf kg lj ki kj kk hb bi translated">为了使SkipGram适应网络，我们需要确定<em class="kl">上下文</em>在网络世界中对应于什么。这就是随机漫步发挥作用的地方。我们可以把上一步中产生的每一步看作文本中的上下文或单词窗口。因此，我们可以最大化发生在相同遍历上的节点嵌入的相似性。</p><blockquote class="mq mr ms"><p id="f64a" class="jp jq kl jr b js lf ij ju jv lg im jx mt lh ka kb mu li ke kf mv lj ki kj kk hb bi translated">在这个意义上，网络中的节点序列对应于文本中的单词序列。</p></blockquote><p id="f843" class="pw-post-body-paragraph jp jq hi jr b js lf ij ju jv lg im jx jy lh ka kb kc li ke kf kg lj ki kj kk hb bi translated">为了通过SkipGram学习嵌入，我们首先为每个节点生成维度为<em class="kl"> d </em>的随机向量。其次，在给定节点本身的情况下，我们对随机游走集进行迭代，并通过梯度下降来更新节点嵌入，以最大化节点邻居的概率。这可以通过softmax函数来实现。当所有的遍历都被处理后，我们可以在相同的遍历集上继续优化其他的遍历，或者在网络上生成新的遍历。</p><h1 id="26d9" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">讨论</h1><p id="b827" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">假设网络具有节点特征和节点分类任务。使用传统的ML技术，我们可以通过假设每个实例相互独立来学习特征到标签的映射。显然，这对于网络结构化数据是一个无效的假设。</p><p id="bdba" class="pw-post-body-paragraph jp jq hi jr b js lf ij ju jv lg im jx jy lh ka kb kc li ke kf kg lj ki kj kk hb bi translated">由于DeepWalk，我们可以将网络中的邻居关系和局部结构反映到节点表示中。因为SkipGram试图最大化相邻节点的相似性，所以节点在彼此之间共享信息。这意味着我们用以自我监督的方式学习的节点嵌入来丰富我们现有的特征集。</p><p id="19c7" class="pw-post-body-paragraph jp jq hi jr b js lf ij ju jv lg im jx jy lh ka kb kc li ke kf kg lj ki kj kk hb bi translated">已经学习了新的节点表示，我们现在可以使用分类模型来完成我们的任务。然而，现在我们已经摆脱了一个无效的假设，并为我们的分类器引入了更多的信息。在论文中，作者进行了大量的实验来证明深度行走嵌入提高了几个任务的分类性能。他们还进行了参数敏感性实验，以观察每个参数对嵌入质量的影响。</p><h1 id="af44" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">结论</h1><p id="eb32" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">在这个故事中，我们给出了DeepWalk的直观解释，这是一种学习节点嵌入的方法。DeepWalk借鉴了语言建模的思想，并将其与网络概念相结合。它的主要命题是，链接的节点往往是相似的，它们也应该有相似的嵌入。尽管现在存在更成功和有效的节点嵌入算法，但DeepWalk因其提出的直观思想和其先锋角色而突出。</p><h2 id="2f43" class="mw iy hi bd iz mx my mz jd na nb nc jh jy nd ne jj kc nf ng jl kg nh ni jn nj bi translated">参考</h2><p id="9e2d" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">[1] <a class="ae mo" href="http://www.perozzi.net/publications/14_kdd_deepwalk.pdf" rel="noopener ugc nofollow" target="_blank">深走</a></p><p id="dbe7" class="pw-post-body-paragraph jp jq hi jr b js lf ij ju jv lg im jx jy lh ka kb kc li ke kf kg lj ki kj kk hb bi translated">[2]<a class="ae mo" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank">skip program</a></p></div></div>    
</body>
</html>