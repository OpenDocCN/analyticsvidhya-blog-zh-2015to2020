<html>
<head>
<title>Tensorflow 2.0 Tutorial on Categorical Features Embedding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Tensorflow 2.0分类特征嵌入教程</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/tensorflow-2-tutorial-on-categorical-features-embedding-93dd81027ea9?source=collection_archive---------2-----------------------#2019-08-13">https://medium.com/analytics-vidhya/tensorflow-2-tutorial-on-categorical-features-embedding-93dd81027ea9?source=collection_archive---------2-----------------------#2019-08-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3673" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">分类特征嵌入综合指南</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/c9c5f02ebd85cad883b6ec6e4046e6d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rocfs8aJ59I3DKZ__PxkJA.png"/></div></div></figure><h2 id="f7c8" class="jj jk hi bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">简介:</h2><p id="e905" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp ju kq kr ks jy kt ku kv kc kw kx ky kz hb bi translated">众所周知，数据准备可能占交付真实ML产品所需时间的80%。此外，处理分类特征是一件有点棘手且耗时的事情，尤其是在基数较高的情况下。当您拥有超过1000个类别的要素，并且需要在此基础上构建模型时，每个数据科学家都需要选择最佳方法来将这些分类要素呈现给模型。举几个常见的例子:</p><ul class=""><li id="71c2" class="la lb hi kj b kk lc kn ld ju le jy lf kc lg kz lh li lj lk bi translated">因子分解，其中每个唯一的类别被分配一个唯一的标签。</li><li id="eb56" class="la lb hi kj b kk ll kn lm ju ln jy lo kc lp kz lh li lj lk bi translated">一个热编码，这个方法产生一个长度等于数据集中类别数的向量。如果一个数据点属于第<em class="lq"> i </em>类，那么你会在第<em class="lq"> i </em>类中找到1，而在其他地方找到0(这会让你处于高维数据的情况)</li><li id="85b1" class="la lb hi kj b kk ll kn lm ju ln jy lo kc lp kz lh li lj lk bi translated">目标编码(有点棘手，因为它可能会导致过度拟合)，包括用目标变量的平均值对每个值进行编码(必须在交叉验证方案中完成)</li></ul><p id="87ac" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">在本教程中，我们将学习另一种非常有效的处理分类特征的方法(特别是在高基数的情况下),为此我们将使用Tensorflow 2.0所以一定要升级才能跟进。</p><p id="631f" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">数据集:我们将致力于一个真实世界的人口普查收入数据集，也称为<em class="lq">成人数据集</em>，可在<em class="lq"> UCI ML知识库</em>中获得，在那里我们将预测人们的潜在收入是否超过5万美元/年。</p><h2 id="f3a1" class="jj jk hi bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">分类特征嵌入:</h2><p id="75bb" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp ju kq kr ks jy kt ku kv kc kw kx ky kz hb bi translated">如果你以前从事过NLP项目，那么你很可能会熟悉单词Embedding。如果不是，我来解释一下:</p><blockquote class="lu lv lw"><p id="54ae" class="kh ki lq kj b kk lc ij km kn ld im kp lx lr kr ks ly ls ku kv lz lt kx ky kz hb bi translated">嵌入意味着用一个向量，一个投影来表示一些东西。</p></blockquote><p id="5077" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">就这么简单，但问题是，我们如何得到这个向量？这就是深度神经网络派上用场的地方。</p><p id="b009" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">说够了，现在让我们说代码:</p><p id="6bd3" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">1 —首先，我们将加载数据(我们不必下载数据，只需安装<a class="ae ma" href="https://pypi.org/project/shap/" rel="noopener ugc nofollow" target="_blank"> shap </a>包，即可从中访问数据) :</p><pre class="iy iz ja jb fd mb mc md me aw mf bi"><span id="d393" class="jj jk hi mc b fi mg mh l mi mj">import shap<br/><br/>data,labels = shap.datasets.adult(display=True)</span></pre><p id="b2ff" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2 —接下来，让我们通过运行这行代码来检查我们的分类特征:</p><pre class="iy iz ja jb fd mb mc md me aw mf bi"><span id="941e" class="jj jk hi mc b fi mg mh l mi mj">data.select_dtypes('category').columns</span></pre><p id="adf9" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">所以，我们的分类特征是:“工作阶级”、“婚姻状况”、“职业”、“关系”、“种族”、“性别”、“国家”。</p><p id="24e5" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2.1 —出于好奇，让我们也通过运行以下代码来检查我们的数字特征:</p><pre class="iy iz ja jb fd mb mc md me aw mf bi"><span id="2278" class="jj jk hi mc b fi mg mh l mi mj">data.select_dtypes('number').columns</span></pre><p id="3cfc" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">输出是:“年龄”、“教育人数”、“资本收益”、“资本损失”、“每周小时数”。</p><p id="4503" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">3-现在，让我们检查分类特征的基数(每个特征包含多少个唯一值) :</p><pre class="iy iz ja jb fd mb mc md me aw mf bi"><span id="a4bb" class="jj jk hi mc b fi mg mh l mi mj">data[data.select_dtypes('category').columns].nunique().reset_index(n<br/>ame='cardinality')</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mk"><img src="../Images/88e9fef30d1d987f8aa079d04eaa0963.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*1v2kw7W2xMi-a0pQJZFADw.png"/></div><figcaption class="ml mm et er es mn mo bd b be z dx translated">功能基数</figcaption></figure><p id="1d28" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">因此，我们似乎没有一个非常高的基本特征(超过100个类别)，但我们有42个类别的“国家”，还有15个类别的“职业”。</p><p id="2f82" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">然而，本教程适用于任何数量。</p><p id="d461" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">4-现在让我们开始构建我们的模型:正如之前强调的，该项目是关于根据一系列特征预测一个人将获得多于还是少于5万美元，为此我们将在Tensorflow 2.0中构建一个神经网络，使用分类和数字特征。</p><h2 id="94b2" class="jj jk hi bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">我们将做什么:</h2><p id="67f2" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp ju kq kr ks jy kt ku kv kc kw kx ky kz hb bi translated">我们要做的是建立一个多输入的神经网络，每个分类特征有一个输入，至于数字特征，所有这些都来自一个单一的输入。让我进一步解释一下:</p><p id="3092" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">我上面说的意思是这样的:</p><ul class=""><li id="2208" class="la lb hi kj b kk lc kn ld ju le jy lf kc lg kz lh li lj lk bi translated">首先我们需要嵌入分类特征(用向量表示分类特征的每个唯一值)，</li><li id="06f4" class="la lb hi kj b kk ll kn lm ju ln jy lo kc lp kz lh li lj lk bi translated">为此，我们将为每个分类特征定义一个嵌入模型(它是一个输入层加上一个嵌入层)，</li><li id="6c2e" class="la lb hi kj b kk ll kn lm ju ln jy lo kc lp kz lh li lj lk bi translated">至于其他数字特征，我们将把它们输入到我们的模型中，就像我们通常从最后一个输入层对任何常规深度学习网络所做的那样。</li></ul><p id="84bc" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">所以总共将有数量_分类_特征+1个模型(数量_分类_特征嵌入模型+一个身份模型)。</p><p id="bfd3" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">一旦我们定义了这些模型，由于我们最后需要一个模型，我们将把它们连接成一个层。</p><p id="7ab8" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">再说一遍，说够了，让我们说代码:</p><p id="40d6" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">1 —首先，我们将构建这个小函数，它将分解我们的分类特征，因为deepnet需要数字而不是字符串:</p><pre class="iy iz ja jb fd mb mc md me aw mf bi"><span id="9bf4" class="jj jk hi mc b fi mg mh l mi mj">def prepar_data_set(data_df):<br/>    categoy_features = data_df.select_dtypes('category').columns<br/>    numerique_features = data_df.select_dtypes('number').columns<br/>    for col in categoy_features:<br/>        encoder = LabelEncoder()<br/>        data_df[col] = encoder.fit_transform(data_df[col])<br/>    return data_df,categoy_features,numerique_features</span></pre><p id="cb12" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">该函数将获取分类特征，并将它们逐个编码为整数，并将返回3个内容:(I)编码的数据，(ii)分类特征列表，以及(iii)数字特征列表。</p><p id="1345" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2 —现在我们已经准备好了训练数据和标签，让我们构建模型的架构:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="f58a" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">让我们打破上面代码平静:</p><p id="056b" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2.1 —如您所见，对于我们分类特征中的每个类别，我们定义了一个接受形状1输入的输入层(因为我们的输入将是类别的值，它只是一个数字)。</p><p id="df3e" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2.2 —我们给它一个名字，这样我们就可以正确地向它发送正确的数据(非常实用的做法，我推荐)。</p><p id="006d" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2.3 —然后我们定义我们的嵌入层，它基本上是一个具有许多行和列的矩阵。</p><p id="b860" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2 . 3 . 1-行数将是分类特征的基数(有多少唯一值)，</p><p id="bc2e" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2 . 3 . 2-列数将是代表这些唯一值(即要调整的参数)的向量的借项。对于本教程，我们选择200(一个非常常见的数字开始)。</p><p id="c86c" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2.3.2 —注意我们将层设置为可训练。因为我们已经用对我们没有价值的随机数初始化了它，我们需要它在训练期间保持更新(反向传播)。</p><p id="2ad6" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2.3.3 —最后，我们必须将输出整形为一个一维数组，它基本上具有嵌入向量的lent的形状。</p><p id="0fe9" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">因此，这就是我们将如何定义我们的number _ of _ categorical _ feature嵌入模型(在这种情况下，7个分类特征意味着7个嵌入模型)。</p><p id="3de8" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2.4 —至于我们的数字特征，将像我们通常做的那样，从它们自己的输入层提供给它们，就像这样:</p><pre class="iy iz ja jb fd mb mc md me aw mf bi"><span id="91e1" class="jj jk hi mc b fi mg mh l mi mj">num_input = tf.keras.layers.Input(shape=(len(num_features)),\<br/>                                  name='input_number_features')</span><span id="cd93" class="jj jk hi mc b fi mr mh l mi mj"># append this model to the list of models<br/>models.append(num_input)<br/># keep track of the input, we are going to feed them later to the #final model<br/>inputs.append(num_input)</span></pre><p id="a5c0" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2.5 —现在我们有8个模型(7个嵌入模型和1个身份模型)，并且我们都将它们附加到一个名为模型的列表中，让我们将它们连接到一个层中:</p><pre class="iy iz ja jb fd mb mc md me aw mf bi"><span id="c3e1" class="jj jk hi mc b fi mg mh l mi mj">merge_models= tf.keras.layers.concatenate(models)</span></pre><p id="0d9b" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2.6 —现在我们有了一个层，我们可以在其上堆叠一个完全连接的层列表:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mp mq l"/></div></figure><p id="f217" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">如您所见，我们在合并的模型上构建了两个完全连接的层，每个层有1000个单元。之后，我们添加了预测层，这些预测层将返回一个人拥有多于或少于50K美元的概率，最后，我们编译了我们的模型，以使用<strong class="kj hj"> adam optimizer </strong>最小化交叉熵，并将准确性作为评估函数。</p><p id="52fd" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2.7 —现在，最后一件事是将数据输入到我们的模型中。</p><p id="7f97" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">由于我们使用了多输入神经网络，因此最好将训练数据作为字典输入，其中键是输入层的名称，值是每个层的预期值。所以让我们这样做，这样你就能明白我的意思:</p><pre class="iy iz ja jb fd mb mc md me aw mf bi"><span id="3a1f" class="jj jk hi mc b fi mg mh l mi mj">input_dict= {<br/>    'input_Workclass':train["Workclass"],<br/>    "input_Marital_Status":train["Marital Status"],<br/>    "input_Occupation":train["Occupation"],<br/>    "input_Relationship":train["Relationship"],<br/>    "input_Race":train["Race"],<br/>    "input_Sex":train["Sex"],<br/>    "input_Country":train["Country"],<br/>    "input_number_features": train[num_featture]<br/>}</span></pre><p id="79cb" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">如您所见，像这样，我们100 %确定我们正在向正确的模型发送正确的数据。</p><p id="d5e8" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">2.8 —就这样，让我们来拟合我们的模型:</p><pre class="iy iz ja jb fd mb mc md me aw mf bi"><span id="50e7" class="jj jk hi mc b fi mg mh l mi mj">model.fit(input_dict,labels,epochs=50,batch_size=64)</span></pre><p id="f81d" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">瞧，我们到了本教程的结尾，我希望它足够清晰和实用。</p><p id="a6ac" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">PS:关于训练的最后一点意见:神经网络对具有大数字的特征高度敏感，就像在这个数据集的情况下，如果保持原样，它将不会学习任何东西，您将必须使用以下代码在-1和1之间缩放您的数字特征:</p><pre class="iy iz ja jb fd mb mc md me aw mf bi"><span id="cbff" class="jj jk hi mc b fi mg mh l mi mj">from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()<br/>train[num_featture] = scaler.fit_transform(train[num_featture])</span></pre><p id="396e" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">我希望你喜欢这个教程，更多的即将到来，敬请期待。</p><p id="fad8" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">完整代码回购链接:<a class="ae ma" href="https://github.com/oussamaErra/tf-2-tutorial-categorical-features-embedding" rel="noopener ugc nofollow" target="_blank">https://github . com/oussamaErra/TF-2-教程-分类-特征-嵌入</a></p><p id="0d68" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">如果你觉得这个教程很好很实用，一定要去<a class="ae ma" rel="noopener" href="/@errabia.oussama"> <strong class="kj hj">关注我</strong> </a>关于Medium的更多实用数据科学和人工智能的好东西。</p><p id="b87e" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">如果你对我有任何问题，你可以直接通过Linkedin或gmail联系我</p><h1 id="061c" class="ms jk hi bd jl mt mu mv jp mw mx my jt io mz ip jx ir na is kb iu nb iv kf nc bi translated">关于我</h1><p id="d9be" class="pw-post-body-paragraph kh ki hi kj b kk kl ij km kn ko im kp ju kq kr ks jy kt ku kv kc kw kx ky kz hb bi translated">我是首席数据科学家@ Clever Ecommerce Inc，我们利用基于人工智能的强大技术，帮助企业创建和管理谷歌广告活动。</p><p id="7180" class="pw-post-body-paragraph kh ki hi kj b kk lc ij km kn ld im kp ju lr kr ks jy ls ku kv kc lt kx ky kz hb bi translated">你可以通过Linkedin或Gmail:errabia.oussama@gmail.com联系我。</p></div></div>    
</body>
</html>