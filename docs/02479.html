<html>
<head>
<title>Data Handling in Pandas</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">熊猫的数据处理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/data-handling-in-pandas-69905bd10f3f?source=collection_archive---------13-----------------------#2019-12-18">https://medium.com/analytics-vidhya/data-handling-in-pandas-69905bd10f3f?source=collection_archive---------13-----------------------#2019-12-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/fe95492e06f7ce29962a81a13f6c11e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r4ars-VG0ZmrmhDNqhsfzg.jpeg"/></div></div></figure><p id="193e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Pandas是Python中最强大的高级数据操作库。我在我的<a class="ae jo" rel="noopener" href="/@listontellis/pandas-the-best-in-the-business-fd96d85113d7">上一篇文章</a>中简要解释了熊猫在数据科学中的重要性。Pandas在处理较小的数据集方面表现出色。但是，即使它通过其数据帧结构对处理大型数据集进行了高度优化，也可能存在内存问题。制约因素是计算机上的内存容量。如果有足够的内存来处理数据，那么我们可以使用熊猫。如果没有，要么我们需要将我们的数据分区并分块处理，要么我们需要升级我们系统中的RAM，这是非常昂贵的。</p><h2 id="1c4c" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated">基于大小的数据分类</h2><ul class=""><li id="2c42" class="kk kl hi is b it km ix kn jb ko jf kp jj kq jn kr ks kt ku bi translated">小数据:小于1 GB</li><li id="dbf5" class="kk kl hi is b it kv ix kw jb kx jf ky jj kz jn kr ks kt ku bi translated">大数据:1 GB到100 GB</li><li id="91c4" class="kk kl hi is b it kv ix kw jb kx jf ky jj kz jn kr ks kt ku bi translated">大数据:100 GB以上</li></ul><p id="a142" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Pandas可以轻松处理1 GB以下的数据集，但随着数据大小的增加，它也增加了机器的负载。如果RAM不足以处理，机器可能会遇到内存问题。为了克服这个困难，我们需要以更小的块来处理数据，而不是一次加载整个数据集。</p><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/6cb244f0d1156ea1fc2f85bfaab1e4ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fq1YIcUQHnl3_jjywzjgTw.png"/></div></div></figure><h2 id="dc3c" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated">组块</h2><p id="3e3c" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb lf jd je jf lg jh ji jj lh jl jm jn hb bi translated">Pandas有一个名为<strong class="is hj"> read_csv() </strong>的内置函数，用于加载csv数据。但是如果文件太大，那么系统可能会耗尽内存。即使我们有一个拥有大量RAM的系统，读取这样的大文件也会降低系统的速度。为了解决这个问题，Pandas允许我们分段读取数据，而不是一次加载所有数据。这些片段被称为<strong class="is hj">组块</strong>。根据RAM大小，用户可以根据自己的方便设置<strong class="is hj"> chunksize </strong>。读取、处理和保存来自各个区块的数据。这在所有的块上重复，然后组合在一起。</p><h2 id="e3f8" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated"><strong class="ak">用途</strong></h2><p id="ea02" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb lf jd je jf lg jh ji jj lh jl jm jn hb bi translated">大多数现实生活中的数据集是复杂的，有大量的行和列。在大多数情况下，许多像name、id这样的列在分析中没有用。在读取数据之前忽略这些列可以节省大量内存。这可以通过添加<strong class="is hj"> usecols </strong>参数来实现，以便在读取数据时只读取所需的列。这导致节省大量内存。</p><h2 id="242a" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated"><strong class="ak">掉线</strong></h2><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es li"><img src="../Images/3b25d8472b69443c656a6583e2163233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*NmwHj7tiQNq45PiXjipTIA.png"/></div></figure><p id="cac8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">减少内存使用的另一种方法是删除所有缺失值或NA值的行或列。这可以通过使用熊猫内置函数<strong class="is hj"> dropna() </strong>来实现。默认情况下，如果没有任何参数，dropna()将删除所有完全为空的行。或者，可以通过定义以下参数来进一步完善该函数:</p><ul class=""><li id="1ccb" class="kk kl hi is b it iu ix iy jb lj jf lk jj ll jn kr ks kt ku bi translated">轴:按行或列放置。0 =行，1 =列。</li><li id="0e84" class="kk kl hi is b it kv ix kw jb kx jf ky jj kz jn kr ks kt ku bi translated">how:接受两个可能值之一:any或all。这将删除一个完全空的轴(all)，或者一个只有一个空单元格的轴(any)。</li><li id="9e9b" class="kk kl hi is b it kv ix kw jb kx jf ky jj kz jn kr ks kt ku bi translated">thresh:它接受一个整数，只有在超出空单元格数量阈值时才删除一个轴。</li></ul><h2 id="3dc0" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated">数据类型说明</h2><figure class="lb lc ld le fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/84b77255147e06e5a2cec2ab3a037a38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gsr6pGH7dsIfSBWGN1WSHA.png"/></div></div></figure><p id="5947" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当从文件中加载数据时，Pandas会自动推断数据类型。但是，这些数据类型通常不是最佳的，并且会占用比所需更多的内存。当处理非常大的数据集时，处理数据类型变得非常重要。通常，从文件中加载数据后，列的数据类型会根据需要进行转换。但是对于更大的数据集，我们必须注意内存空间。默认情况下，一旦推断出列的数据类型，就为每一列分配标准内存大小。在大多数情况下，这种分配的内存是太多和多余的。因此，可以通过使用<strong class="is hj"> read_csv() </strong>函数中的<strong class="is hj"> dtype </strong>参数来优化内存使用，而不是使用原始数据类型读取数据。dtype参数是一个字典，其中每个键是数据集中的一列，每个值是其优化的数据类型。</p><h2 id="01c5" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated">达斯克</h2><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/94472f8ee2bef922b8a4bdf5b5e50c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*Sjy6G18SAMFEKbJfBYEU8g.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">图片来源:Dask.org</figcaption></figure><p id="3549" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Dask允许Pandas在分布式集群上对大数据集进行操作。这是一个用于并行和分布式计算的Python库。它在大于内存限制的大型数据集上提供多核执行。Dask通过提供模拟列表、熊猫和Numpy的高级包、数据帧和数组集合，在不适合内存的大型数据集上并行操作。Dask也基于<strong class="is hj">延迟加载</strong>的概念，当数据集非常大时，数据集被分配到不同的内核，而不是单个内核。</p><h2 id="c907" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated">结论</h2><p id="0a27" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb lf jd je jf lg jh ji jj lh jl jm jn hb bi translated">本文涵盖了轻松处理大型数据集而不会遇到内存问题的技巧和提示。Pandas在处理高达100 GB的小型和大型数据集方面表现出色。对于文件大小超过100 GB的大数据集，最好使用可扩展的数据分析工具，如<strong class="is hj"> Apache Spark </strong>。</p><p id="6d3c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="ls">如果你喜欢这篇文章，请鼓掌分享！快乐阅读！</em></p></div></div>    
</body>
</html>