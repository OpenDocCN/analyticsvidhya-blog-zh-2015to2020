<html>
<head>
<title>Information Entropy in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的信息熵</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/information-entropy-in-machine-learning-199c0f12ff4?source=collection_archive---------13-----------------------#2020-01-11">https://medium.com/analytics-vidhya/information-entropy-in-machine-learning-199c0f12ff4?source=collection_archive---------13-----------------------#2020-01-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="925a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">在机器学习领域，交叉熵和KL散度是分类算法中广泛使用的代价函数。在本文中，我们将通过一些有趣的例子来讨论它的来源。</em></p><p id="2257" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">信息科学诞生于20世纪50年代，源于一位杰出人士的工作:克劳德·e·香农。1948年，在贝尔实验室写的一篇里程碑式的论文《通信的数学理论》中，香农用数学术语定义了什么是信息，以及如何在有噪声的情况下有效地传输信息。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/9cdc34e965f544b68e102cccb3ef377d.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*e5mCLlbFrySbgiKXy3haVg.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">克劳德·香农</figcaption></figure><p id="824a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是“信息理论”科学的开端，这是一套允许我们建立互联网、数字计算机和电信系统的思想。<br/> <em class="jd">熵是这种信息理论的量度。</em></p><h1 id="ad48" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">信息和不确定性</h1><p id="1d00" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">就交流而言，信息可以通过信息的内容或通过直接或间接的观察来表达。在我们的数字世界中，这些信息由“比特”组成。<br/>你们很多人都知道，二进制数包含0或1。</p><blockquote class="kt ku kv"><p id="bda9" class="if ig jd ih b ii ij ik il im in io ip kw ir is it kx iv iw ix ky iz ja jb jc hb bi translated">信息是不确定性的解决方案。</p></blockquote><p id="3eb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们用一个例子来理解:</p><p id="69f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在公平的硬币中，我们有50–50%的机会得到正面或反面。抛完硬币后，我们要么得到正面要么得到反面，这样我们的不确定性减少了2。为什么？因为在投掷之前，我们有50%的把握会收到一条尾巴，但是在收到一条尾巴之后，我们现在100%的把握结果是尾巴。</p><blockquote class="kt ku kv"><p id="7b00" class="if ig jd ih b ii ij ik il im in io ip kw ir is it kx iv iw ix ky iz ja jb jc hb bi translated">因为在公平硬币的情况下，有两种可能的结果以相等的概率发生，所以了解实际结果包含一位信息。</p></blockquote><p id="976e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么我们如何计算收到的这1比特信息呢？但是首先，让我们看另一个例子，这次是一枚有偏见的硬币。让我们假设头部出现的概率是75%，尾部出现的概率是25%。如果我们抛硬币，再次收到尾巴，那么我们的不确定性减少了4倍。为什么这次是4？我想我们已经知道了。以前只有25%的机会得到尾巴，但现在我们有了100%的结果，所以我们现在有4倍的把握。</p><p id="4b3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个实验清楚地表明“不确定性的减少是事件概率的倒数”。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es kz"><img src="../Images/da26bf5dd6d486ed023a472bb18bba4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*tX2xkNm4p4pKWJwMa5T3KQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">其中p是事件在分布中发生的概率</figcaption></figure><blockquote class="kt ku kv"><p id="7fe8" class="if ig jd ih b ii ij ik il im in io ip kw ir is it kx iv iw ix ky iz ja jb jc hb bi translated">事件越不确定，解决该事件的不确定性所需的信息就越多。</p></blockquote><p id="4f58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">发送/接收的信息比特数与不确定度降低成正比，表示为不确定度降低的二进制对数。</strong></p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es la"><img src="../Images/b23219ae869afabacc8521e02f497772.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*PpplVeHTAac4hb7rZYp0rA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">发送/接收的信息</figcaption></figure><p id="b525" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">请注意，一位是典型的信息单位，但也可以使用其他单位，如nat或ban，其中我们分别采用e或10的自然对数，而不是二进制对数。</em></p><p id="70f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，在公平硬币的情况下，我们有1/2的概率得到一条尾巴，所以根据公式，我们得到1比特的信息。</p><p id="cd0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样的，一枚75%正面，25%反面的有偏硬币，我们接收到多少比特的信息？</p><p id="73e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们收到一个尾部，收到的信息= -log₂(0.25) = 2比特</p><p id="2fd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们接收一个报头，那么接收的信息= -log₂(0.75) = 0.41比特</p><p id="7c9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是，我们平均收到多少信息呢？所以，有</p><p id="7985" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">= 0.41位的75%几率+ 2位的25%几率</p><p id="3625" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">=0.75*0.41+0.25*2</p><p id="30e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">=0.81位</p><blockquote class="kt ku kv"><p id="23dc" class="if ig jd ih b ii ij ik il im in io ip kw ir is it kx iv iw ix ky iz ja jb jc hb bi translated">我们刚刚收到的这个平均信息实际上就是我们所知的熵。</p></blockquote><h1 id="ffb2" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">熵</h1><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lb"><img src="../Images/9992eadd17715b74a40c1a0fec07cbb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*BQUDparzLklbcgJbwoKDOQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">艺术噪音GIF由Adambanaszek</figcaption></figure><p id="d723" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望到目前为止你对此已经有所了解。</p><p id="cf72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在一个更正式的定义中，熵是你从一个给定的概率分布p中抽取的样本中获得的平均信息量。</p><p id="0d19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">具有可能值{x₁、x₂、x₃…xₙ}以及{p(x₁、p(x₂、p(x₃),… p(xₙ)}的相应概率分布的离散随机变量x的熵η(p)可以用比特表示为:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lc"><img src="../Images/f503739b6e59b09279f5aff9869c083c.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*a3BCLxD7Tp31ti-4VQ7vEA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">熵</figcaption></figure><p id="7359" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它也可以用来自每个结果(I)的信息received(Iᵢ表示为:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ld"><img src="../Images/9685e124f953ddbbf27c8bf6b2c8d471.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*eJTJSaYDNV0X2kI9ui5jaw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">Iᵢ是从结果I发送/接收的信息</figcaption></figure><h1 id="e4ac" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">交叉熵</h1><p id="19f0" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">交叉熵是平均消息长度。让我们用一个例子来了解更多。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es le"><img src="../Images/22ccf0930a5cb56fae8d78e7154f9267.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/1*wKtjgL9XD4wM4OYyrbCixw.gif"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">嘻哈舞蹈GIF由克里斯蒂蒙斯</figcaption></figure><p id="64ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">考虑一个有偏差的骰子，其结果的概率分布如下:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lf"><img src="../Images/ca1ef787e14baee0d16d2173f7675449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*Wu-tDNFcleqAsmAaa6681w.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">每个结果都有概率分布的有偏骰子</figcaption></figure><p id="6d6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个骰子的熵=</p><p id="714f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">= -[0.35*log₂(0.35)+0.35*log₂(0.35)+0.10*log₂(0.10)+0.10*log₂(0.10)+ 0.05*log₂(0.05)+0.05*log₂(0.05)]</p><p id="5038" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">= 2.16位</p><p id="710f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们为每个结果定义3位编码，如下所示:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lf"><img src="../Images/cdd9c173261d973bcc0e9f527a1454f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*Epm1VhzTNPyQJf3hdj_0Kw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">真实百分比分布，每个结果的假定消息长度以位为单位</figcaption></figure><blockquote class="kt ku kv"><p id="a661" class="if ig jd ih b ii ij ik il im in io ip kw ir is it kx iv iw ix ky iz ja jb jc hb bi translated"><em class="hi">通过假设每个结果的</em> k <em class="hi">比特编码，我们隐含地声明在得到这个结果之后接收的信息是</em> k <em class="hi">。</em></p></blockquote><p id="6286" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此估计分布的熵= [35%的(3位)+35%的(3位)+10%的(3位)+10%的(3位)+5%的(3位)+5%的(3位)]</p><p id="565a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">= 3位</p><p id="7e67" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，根据估计的分布，平均消息长度或接收的信息是3位，但只有2.16位是有用的信息(根据实际分布)，其余的0.84(3–2.16)位是噪声。</p><p id="cd8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们需要更好的编码。</p><p id="a3a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不是假设每个结果3位，让我们这样编码:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lg"><img src="../Images/87ae433a297566b7591adcdaf941b523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*JXpzGzf6IK5D1MszCMsVyg.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">真实百分比分布，每个结果的假定消息长度以位为单位</figcaption></figure><p id="f8f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，平均来说，收到的信息是</p><p id="5cde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">= [0.35*(2)+0.35*(2)+0.10*(3)+ 0.10*(3)+ 0.05*(4)+0.05*(4)]</p><p id="9088" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">=2.40位，比3位好得多</p><p id="206d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们接收到的噪声仅为0.24，即(2.40–2.16)位。</p><p id="916e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是为什么会这样呢？</p><p id="12a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们考虑一个2比特的信息时，我们暗示这个结果有25%的可能性发生。</p><p id="968a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这可以用公式来表示:</p><p id="6662" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">收到的信息= 2位</p><p id="db09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">=&gt; -log₂(p) = 2</p><p id="b7c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">=&gt; p =或25%</p><p id="7e15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，对于其他结果，</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lh"><img src="../Images/6e1c6be637cc21cc5f0d282fc2ff3ab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xEYWwVPjgjUq2XfX5VnkIA.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">有偏骰子的实际和预测分布</figcaption></figure><p id="8d3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">这个预测分布的熵就是交叉熵。</em></p><p id="8bcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">交叉熵是一种测量两个概率分布之间“距离”的方法，这里是p和q。</p><p id="4a5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">交叉熵可以表示为:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lm"><img src="../Images/564639aaeb4b4c13029f1883aaa112df.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*rnbpFNkzrhrL_m7uicmUqw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">交叉熵，其中p为真，q为预测分布</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lf"><img src="../Images/b3e15509afa494399f71858d42b3fbbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*ddrUpgTkNBK0-ELp0pbm7Q.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">p和q分别是真实分布和预测分布</figcaption></figure><h1 id="8a26" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">每次都是同样的编码？</h1><p id="04ba" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">但是，我们可以对每个骰子使用相同的编码吗？</p><p id="e1ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看，如果我们用新的概率分布(p1)和相同的预测分布q来取新的有偏骰子，会发生什么。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lf"><img src="../Images/f9e6c875d8765345349f1ebbbbbac586.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*hVeldM8PK4W0VZ_dhWP0CA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">真实百分比分布p1，假设每个结果的消息长度为比特</figcaption></figure><p id="2fc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">估计分布接收的平均信息=[0.05 *(2)+0.05 *(2)+0.10 *(3)+0.10 *(3)+0.35 *(4)+0.35 *(4)]</p><p id="6302" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">= 3.60位</p><p id="2e3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们平均接收3.60比特的信息，其中只有2.16比特是有用的，其余的1.44比特是噪声。这是非常低效的。</p><p id="6ed9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样的，</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ln"><img src="../Images/1e00e312b76c8d19e636174b3b8be07d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nw3FRZTZNRZ5njHHNrMFWg.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">有偏骰子的实际和预测分布</figcaption></figure><p id="1c1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，你能看出区别吗，我们预测结果“1”有25%的机会，而实际上只有5%的机会。</p><p id="e1ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">所以对于不同的分布，我们需要对结果进行相应的编码，否则就要以交叉熵的形式承担高昂的代价。</em></p><blockquote class="lo"><p id="baa0" class="lp lq hi bd lr ls lt lu lv lw lx jc dx translated">如果预测结果与真实结果完全匹配，交叉熵本身就是熵。</p></blockquote><h1 id="8bb0" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb ly kd ke kf lz kh ki kj ma kl km kn bi translated">库尔贝克-莱布勒散度</h1><p id="19fc" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">当交叉熵不等于熵时，它们的差称为相对熵或kull back–lei bler散度。</p><p id="b8fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更正式地说，KL散度是一个概率分布与第二个参考概率分布如何不同的度量。</p><p id="4624" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">KL散度表示为:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mb"><img src="../Images/c879e35d8d7b9ee27fb72b448258fe33.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*l10mtS5aEX6NFH03lO0Frw.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">H(p，q)和H(p)分别是交叉熵和熵</figcaption></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mc"><img src="../Images/4626f9987fe7a6b034b011e83e1785d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*xBbGjQrdZtXYh0QXJLFhHQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">从q -&gt; p的kull back-lei bler散度</figcaption></figure><p id="f46c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在机器学习的背景下，如果使用q而不是p，KL散度通常被称为获得的<a class="ae md" href="https://en.wikipedia.org/wiki/Information_gain_in_decision_trees" rel="noopener ugc nofollow" target="_blank">信息增益</a></p><p id="37cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，总的来说，</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es me"><img src="../Images/51d5fd022fb13b1555938a2fed2c4468.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*igaXyKPs7dxrnxi8O24pew.png"/></div></figure><h1 id="bcee" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">分类中的交叉熵</h1><p id="3f37" class="pw-post-body-paragraph if ig hi ih b ii ko ik il im kp io ip iq kq is it iu kr iw ix iy ks ja jb jc hb bi translated">考虑这个图像。通过观察，我们知道这是一只可爱的狗的形象。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mf"><img src="../Images/a1c6dd6dafa7b40e7559e22d807886ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*bsKH2UmpaYF2lA1QF6v-QQ.jpeg"/></div></figure><p id="8204" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在考虑一个图像分类器，它将一幅图像作为输入，输出10个不同类别的预测概率。我们知道这幅图像中“狗”的真实概率是1，其余的是0。</p><p id="7c1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们将此图像输入分类器时，它将为每个类别产生以下概率。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mg"><img src="../Images/1512c4fe499f2741f3d0fb34ed291386.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JopH_15-RSa-uPaRbTQtQQ.png"/></div></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">p作为一个热编码向量，正确的类别为dog，q作为来自分类器的每个类别的预测概率</figcaption></figure><p id="8564" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以使用交叉熵作为损失函数来测量两个分布p和q之间的距离，这也被称为<strong class="ih hj">交叉熵损失</strong>或<strong class="ih hj">对数损失</strong>。</p><p id="99b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这可以类似于交叉熵来计算，但是通常使用自然对数而不是二进制对数。</p><p id="afc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">交叉熵损失= -(1)*ln(0.35) = 1.05</p><p id="d592" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">在真分布中，除了正确的类，所有类的概率都是0。因此，我们只考虑正确类别的预测概率。</em></p><blockquote class="kt ku kv"><p id="3c58" class="if ig jd ih b ii ij ik il im in io ip kw ir is it kx iv iw ix ky iz ja jb jc hb bi translated"><em class="hi">如果真实类(此处为dog)的预测概率接近于0，则意味着高成本，而如果接近于1，则成本将变得等于真实分布的熵，在这种情况下，真实分布的熵为零，因为真实分布是独热向量。</em></p></blockquote><p id="f3a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于分类，KL散度也可以用作损失函数来代替交叉熵损失。如果通过梯度优化方法训练，假定真实分布是恒定的，两者给出相同的优化结果。</p><p id="6258" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢您的阅读:)</p><p id="d08d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">欢迎任何建议、纠正或批评。</strong></p><p id="8ddd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请给我发电子邮件到nikitagupta585@gmail.com。</p><p id="3241" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">祝您愉快！！</p><p id="c6e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考资料:</p><ol class=""><li id="7663" class="mh mi hi ih b ii ij im in iq mj iu mk iy ml jc mm mn mo mp bi translated"><a class="ae md" href="https://en.wikipedia.org/wiki/Claude_Shannon" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Claude_Shannon</a></li><li id="f48c" class="mh mi hi ih b ii mq im mr iq ms iu mt iy mu jc mm mn mo mp bi translated"><a class="ae md" href="https://www.nyu.edu/pages/linguistics/courses/v610003/shan.html" rel="noopener ugc nofollow" target="_blank">https://www . NYU . edu/pages/linguistic/courses/v 610003/shan . html</a></li><li id="d67b" class="mh mi hi ih b ii mq im mr iq ms iu mt iy mu jc mm mn mo mp bi translated"><a class="ae md" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Information</a></li><li id="35b6" class="mh mi hi ih b ii mq im mr iq ms iu mt iy mu jc mm mn mo mp bi translated"><a class="ae md" href="https://en.wikipedia.org/wiki/Information_theory" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Information_theory</a></li><li id="ce83" class="mh mi hi ih b ii mq im mr iq ms iu mt iy mu jc mm mn mo mp bi translated"><a class="ae md" href="https://www.youtube.com/watch?v=ErfnhcEV1O8&amp;t=1s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=ErfnhcEV1O8&amp;t = 1s</a></li><li id="2dd3" class="mh mi hi ih b ii mq im mr iq ms iu mt iy mu jc mm mn mo mp bi translated"><a class="ae md" href="https://www.youtube.com/watch?v=R4OlXb9aTvQ" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=R4OlXb9aTvQ</a></li></ol></div></div>    
</body>
</html>