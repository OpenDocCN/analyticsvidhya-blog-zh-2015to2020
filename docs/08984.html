<html>
<head>
<title>U2-Net: Going Deeper with Nested U-Structure for Salient Object Detection Research Paper Summary</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">U2-Net:使用嵌套 U-结构深入显著对象检测研究论文摘要</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/u2-net-going-deeper-with-nested-u-structure-for-salient-object-detection-research-paper-summary-e36911353b38?source=collection_archive---------4-----------------------#2020-08-21">https://medium.com/analytics-vidhya/u2-net-going-deeper-with-nested-u-structure-for-salient-object-detection-research-paper-summary-e36911353b38?source=collection_archive---------4-----------------------#2020-08-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="0f83" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">我们要前景！！！🔥 🔥</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/2ab10f7e8a80509dbca6865e26b3c321.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aMiHU1VV0NirmacqwpgsSA.jpeg"/></div></div></figure><h2 id="a6a9" class="jj jk hi bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">概述:-📑</h2><ol class=""><li id="f738" class="kh ki hi kj b kk kl km kn ju ko jy kp kc kq kr ks kt ku kv bi translated">什么是显著物体检测？</li><li id="3088" class="kh ki hi kj b kk kw km kx ju ky jy kz kc la kr ks kt ku kv bi translated">介绍</li><li id="3122" class="kh ki hi kj b kk kw km kx ju ky jy kz kc la kr ks kt ku kv bi translated">使用的主要技术</li><li id="86d5" class="kh ki hi kj b kk kw km kx ju ky jy kz kc la kr ks kt ku kv bi translated">造纸技术</li><li id="532b" class="kh ki hi kj b kk kw km kx ju ky jy kz kc la kr ks kt ku kv bi translated">体系结构</li><li id="5dcc" class="kh ki hi kj b kk kw km kx ju ky jy kz kc la kr ks kt ku kv bi translated">关于培训</li><li id="2743" class="kh ki hi kj b kk kw km kx ju ky jy kz kc la kr ks kt ku kv bi translated">结论</li></ol><h2 id="0c69" class="jj jk hi bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">什么是显著物体检测？🎃</h2><p id="63b5" class="pw-post-body-paragraph lb lc hi kj b kk kl ij ld km kn im le ju lf lg lh jy li lj lk kc ll lm ln kr hb bi translated"><strong class="kj hj">显著物体检测(SOD) </strong>是一种基于任务的视觉注意机制，算法旨在探索场景或图像上比周围区域更注意的<strong class="kj hj">物体</strong>或区域。😃</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lo"><img src="../Images/3858441e4c691f42020d17031598bd12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2uKSZAqJ7UgFccvpn4wMPA.jpeg"/></div></div></figure><h2 id="bfc5" class="jj jk hi bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">简介:-📗</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/0950911e181ae068a897bd32dcad3937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zu8vqtrYnLU3rYF3Pd5sHA.png"/></div></div></figure><p id="acfe" class="pw-post-body-paragraph lb lc hi kj b kk lq ij ld km lr im le ju ls lg lh jy lt lj lk kc lu lm ln kr hb bi translated"><strong class="kj hj"> SOD </strong>不是一个有趣的😕与<strong class="kj hj">图像识别或图像分类</strong>相比，受 DL 研究人员的影响，因此开发的神经网络更专注于图像识别和图像分类，而不是 SOD。迄今为止，神经网络是通过考虑图像分类或图像识别的问题而发展起来的。因此，他们面临的一个问题是，SOD 领域的研究人员因为使用图像分类或图像识别模型，如<strong class="kj hj"> VGG、AlexNet、ResNet 等</strong>而面临的问题是，这些网络过去常常采用额外的特征，并且<strong class="kj hj">深度</strong>，因此，与上下文信息相比，他们提取了许多精细的细节。</p><p id="788d" class="pw-post-body-paragraph lb lc hi kj b kk lq ij ld km lr im le ju ls lg lh jy lt lj lk kc lu lm ln kr hb bi translated"><strong class="kj hj">使用的主要技术:-🔆</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/b8e8e08943d3b34ed6df155eea4e79db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KQtkzbMlTN-OoLc4XThqHg.png"/></div></div></figure><p id="2a32" class="pw-post-body-paragraph lb lc hi kj b kk lq ij ld km lr im le ju ls lg lh jy lt lj lk kc lu lm ln kr hb bi translated">在<strong class="kj hj">多层次深度特征集成技术中，</strong>我们获取图像的碎片并尝试提取上下文特征，另一种技术是<strong class="kj hj">多尺度特征提取</strong>，如下图所示</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/64c968edfb9666521f11573e53ed5bd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TLmPp5VH8NWFoj_tzV4V1g.png"/></div></div></figure><p id="1c62" class="pw-post-body-paragraph lb lc hi kj b kk lq ij ld km lr im le ju ls lg lh jy lt lj lk kc lu lm ln kr hb bi translated">它放弃了使用小块来提取上下文特征的方法，而是使用金字塔方案，即😻</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lv"><img src="../Images/bbd13dbf1e5b4b646581317c8ca4632e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gf_U4Rs3iuuG7CF3z8Dctg.jpeg"/></div></div></figure><p id="d51d" class="pw-post-body-paragraph lb lc hi kj b kk lq ij ld km lr im le ju ls lg lh jy lt lj lk kc lu lm ln kr hb bi translated">这也是用于俱乐部的两个对象，但这些技术更复杂一点，这将在文章中进一步解释。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/4a35757e8a6042eca0aaf67be0829cef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0XDImDp28Eb3flE4ZrDYkg.png"/></div></div></figure><h2 id="10a0" class="jj jk hi bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">纸的技术:-</h2><p id="ff9c" class="pw-post-body-paragraph lb lc hi kj b kk kl ij ld km kn im le ju lf lg lh jy li lj lk kc ll lm ln kr hb bi translated">本文中使用的技术是<strong class="kj hj">多尺度特征提取</strong>，它是从零开始实现的，不像以前使用的那样使用预训练的主干模型，如 VGG、AlexNet、ResNet 等。😲它是受 U-Net 的启发而建立的，一种新颖的网络剩余 U-block，RSU 捕捉级内多尺度特征。🔥</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/d9c0d05bf8f78350891eedbfc87564d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X2Joacl7qn130HftVitLwg.png"/></div></div></figure><p id="1b3c" class="pw-post-body-paragraph lb lc hi kj b kk lq ij ld km lr im le ju ls lg lh jy lt lj lk kc lu lm ln kr hb bi translated">为了保留上下文信息而不是更多地关注细节，使用了上述架构，其中 U-block 提取了上下文特征。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/1e324b7e5aa4e854807aab5880006a0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fjxOjZ4rAI-Y5Zd8dz76EA.png"/></div></div></figure><p id="3a8d" class="pw-post-body-paragraph lb lc hi kj b kk lq ij ld km lr im le ju ls lg lh jy lt lj lk kc lu lm ln kr hb bi translated">所获得的特征被下采样，从而使得<strong class="kj hj">上下文特征</strong>被保留，并且最终，我们的结果不会受到所提取的图片细节(即颜色、纹理等)的影响。在特征被提取之后，它们在<strong class="kj hj">上采样</strong>之后被编码和解码🌝。该模型的计算也非常少(可以在图中观察到)，因为它关注于<strong class="kj hj">下采样</strong>特征。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/63d3c719ceff211a3283bfd16af4f527.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_-BptZ8GRmNEfSCnJaCLEw.png"/></div></div></figure><h2 id="c629" class="jj jk hi bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">建筑:-🌉</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/1242fa30aec8c232323337aaaa2ab079.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aUr6saHqTWvMcZL5m8FSqQ.png"/></div></div></figure><p id="439d" class="pw-post-body-paragraph lb lc hi kj b kk lq ij ld km lr im le ju ls lg lh jy lt lj lk kc lu lm ln kr hb bi translated">该模型的完整架构包含 6 个编码器、5 个解码器和 1 个包含 sigmoid 函数和 1 个 3*3 维卷积层的显著图融合模块。</p><h2 id="4f4a" class="jj jk hi bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">关于培训:-🏃</h2><p id="112e" class="pw-post-body-paragraph lb lc hi kj b kk kl ij ld km kn im le ju lf lg lh jy li lj lk kc ll lm ln kr hb bi translated">用于训练的数据集是最大和最常用的训练数据集，即 DUTS-TR💯，为了进行评估，他们使用了 6 个常用数据集 DUTS 欧姆龙，DUTS-TE，HKU-IS，ECSSD，PASCAL-S，SOD。</p><p id="c372" class="pw-post-body-paragraph lb lc hi kj b kk lq ij ld km lr im le ju ls lg lh jy lt lj lk kc lu lm ln kr hb bi translated">使用以下方法测量精度:- <br/> 1。精确召回曲线<br/> 2。最大 F 值<br/> 3。平均绝对误差<br/> 4。加权 F 测量值<br/> 5。结构尺寸<br/> 6。边界的松弛 F-测度</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/87a59b190ac4f69377afe43b3dd8cac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5_sqd58CBJGfXf-GYPtXLQ.png"/></div></div></figure><h2 id="c242" class="jj jk hi bd jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg bi translated">结论:-💻</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lp"><img src="../Images/1be51ffe0744ebf71fb7edf6ee59a5b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k3z1eg-8Xmt_wdULBdUBIg.png"/></div></div></figure><p id="978c" class="pw-post-body-paragraph lb lc hi kj b kk lq ij ld km lr im le ju ls lg lh jy lt lj lk kc lu lm ln kr hb bi translated">结果似乎非常令人满意，并且比图中所示的其他显著对象检测模型更加清晰，但是我注意到这个模型的一个不令人满意的地方是它失败了😿当画面移动时。我要感谢这篇研究论文的作者。这是<a class="ae lw" href="https://arxiv.org/pdf/2005.09007v2.pdf" rel="noopener ugc nofollow" target="_blank">研究论文</a>的链接。点击<a class="ae lw" href="https://github.com/NathanUA/U-2-Net" rel="noopener ugc nofollow" target="_blank">链接</a>可以找到代码。😍和往常一样，你可以在 LinkedIn 上找到我😻还有在<a class="ae lw" href="https://github.com/MaheepChaudhary" rel="noopener ugc nofollow" target="_blank"> Github </a>上。❤️</p></div></div>    
</body>
</html>