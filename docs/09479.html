<html>
<head>
<title>How to Treat Overfitting in Convolutional Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何看待卷积神经网络中的过拟合</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-treat-overfitting-in-convolutional-neural-networks-d6e74a0a479?source=collection_archive---------22-----------------------#2020-09-07">https://medium.com/analytics-vidhya/how-to-treat-overfitting-in-convolutional-neural-networks-d6e74a0a479?source=collection_archive---------22-----------------------#2020-09-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="a70f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="5fc5" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">当您的训练数据集(用于“教授”模型的数据集)的准确性大于您的测试准确性时，机器学习模型中会出现过度拟合或高方差。就“损失”而言，当您的模型在训练集中的误差较低而在测试集中的误差较高时，过度拟合就会显现出来。您可以通过绘制损失和准确性指标，并查看两个数据集的性能指标的汇合点来直观地识别这一点。</p><p id="af59" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">过度拟合表明您的模型对于它正在解决的问题来说太复杂，即您的模型在回归模型和集成学习的情况下有太多的特征，在卷积神经网络的情况下有太多的过滤器，在整体深度学习模型的情况下有太多的层。这导致您的模型很好地了解示例数据，但是对于任何新数据表现不佳。</p><p id="469b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这很烦人，但是可以通过调整您的超参数来解决，但是首先，让我们确保我们的数据被划分成均匀的集合。</p><h1 id="39f7" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">拆分数据</h1><p id="c4fc" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">对于深度学习模型，我建议有 3 个数据集:训练、验证和测试。验证集应该用于微调您的模型，直到您对其性能感到满意，然后切换到测试数据来训练您的模型的最佳版本。首先，我们将导入必要的库:</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="7776" class="kp ig hi kl b fi kq kr l ks kt">from sklearn.model_selection import train_test_split</span></pre><p id="3f69" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在我们来谈谈比例。我的理想比例是 70/10/20，意思是训练集应该由~70%的数据组成，然后将 10%投入验证集，20%投入测试集，就像这样，</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="2436" class="kp ig hi kl b fi kq kr l ks kt"># Create the Validation Dataset <br/>Xtrain, Xval, ytrain, yval = train_test_split(train_images, train_labels_final, train_size=0.9, test_size=0.1, random_state=42) </span><span id="c0e3" class="kp ig hi kl b fi ku kr l ks kt"># Create the Test and Final Training Datasets Xtrain, Xtest, ytrain, ytest = train_test_split(Xtrain, ytrain, train_size=0.78, random_state=42)</span></pre><p id="9163" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">您将需要执行<em class="kv">两个 train_test_split() </em>函数调用。第一次调用是在图像和标签的初始训练集上完成的，以形成验证集。我们将调用参数 random_state 以在运行函数时保持结果的一致性，并调用 test_size 以注意我们希望验证集的大小为训练数据的 10%,并调用 train_size 以将其设置为等于剩余数据百分比的 90%。</p><p id="394d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这在默认情况下可以省略，因为 python 足够智能来进行计算。变量 Xval 和 yval 指的是我们的验证图像和标签。在第二次调用时，我们将根据新形成的训练数据 Xtrain 和 ytrain 生成测试数据集。我们将重复上述操作，但这次我们将把最新的训练集设置为以前的 78%,并将最新的数据集赋给与以前相同的变量，以保持一致性。同时，我们将把测试图像的测试数据分配给 Xtest，并对标签数据进行测试。</p><p id="c1fd" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在我们准备开始建模。参考我之前的博客来深入了解 CNN 的初始设置。我们将从第二个模型开始，假设我们的第一个结果如上图所示。我们将使用以下技术:</p><ol class=""><li id="4312" class="kx ky hi jf b jg kb jk kc jo kz js la jw lb ka lc ld le lf bi translated"><strong class="jf hj">正规化</strong></li><li id="bb20" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><strong class="jf hj">重量初始化</strong></li><li id="ca8c" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><strong class="jf hj">退学正规化</strong></li><li id="a701" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><strong class="jf hj">重量限制</strong></li><li id="dc98" class="kx ky hi jf b jg lg jk lh jo li js lj jw lk ka lc ld le lf bi translated"><strong class="jf hj">其他</strong></li></ol><h1 id="2a6d" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">正规化</h1><p id="c2e8" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae kw" href="https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/" rel="noopener ugc nofollow" target="_blank">正则化</a>通过惩罚复杂模型来优化模型，从而最小化损失和复杂性。因此，这迫使我们的神经网络更简单。这里我们将使用一个<em class="kv"> L2 正则化子</em>，因为它是最常见的，并且比<em class="kv"> L1 正则化子</em>更稳定。这里，我们将在网络的第二层和第三层添加一个正则项，学习率(lr)为 0.01。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="a936" class="kp ig hi kl b fi kq kr l ks kt"># Hidden Layer 1 <br/>model2.add(layers.Conv2D(64, (4, 4), activation='relu', kernel_regularizer=regularizers.l2(l=0.01))) <br/>model2.add(layers.MaxPooling2D((2, 2))) </span><span id="7570" class="kp ig hi kl b fi ku kr l ks kt"># Hidden Layer 2 <br/>model2.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(l=0.01))) model2.add(layers.MaxPooling2D((2,2)))</span></pre><h1 id="ee44" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">重量初始化</h1><p id="ae49" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">权重初始化在训练过程开始之前第一次为所有神经元设置权重向量。选择正确的权重至关重要，因为我们希望在足够长的时间内尽可能接近成本函数的全局最小值。在我们模型的这个迭代中，我们将使用一个初始化:</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="84e4" class="kp ig hi kl b fi kq kr l ks kt"># Input Layer of the 3rd Model <br/>model3.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', input_shape=(96, 96, 3)))</span></pre><h1 id="f632" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">辍学正规化</h1><p id="508e" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在训练阶段将权重设置为零时，丢弃正则化会忽略层中单元的随机子集。</p><figure class="kg kh ki kj fd lm er es paragraph-image"><div class="er es ll"><img src="../Images/bc99da540b6c641ad22abe8d815b2446.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*MlYbJzwH43i0qLv4.png"/></div></figure><p id="c130" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">输入层和隐藏层的理想比率是 0.4，输出层的理想比率是 0.2。见下文:</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="7f56" class="kp ig hi kl b fi kq kr l ks kt">random.seed(123) # Establish Consistency in results <br/>model4 = Sequential() # Instantiate the 4th Model model4.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(96, 96, 3))) <br/>model4.add(layers.MaxPooling2D((2, 2))) model4.add(Dropout(0.4)) model4.add(layers.Conv2D(64, (4, 4), activation='relu')) model4.add(layers.MaxPooling2D((2, 2))) model4.add(Dropout(0.4)) </span><span id="ccef" class="kp ig hi kl b fi ku kr l ks kt"># Flattening- Convert 2D matrix to a 1D vector model4.add(layers.Flatten()) <br/>model4.add(layers.Dense(512, activation = 'relu')) model4.add(Dropout(0.2)) <br/>model4.add(layers.Dense(1, activation='sigmoid'))</span></pre><h1 id="cde8" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">重量限制</h1><p id="893d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">权重约束检查网络权重的大小，并在大小超过预定义的限制时对其进行重新缩放。重量约束根据需要工作。下面我们使用约束<em class="kv">单位范数</em>，强制权重为 1.0。</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="387c" class="kp ig hi kl b fi kq kr l ks kt">model5.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_constraint=unit_norm(), input_shape=(96, 96, 3)))</span></pre><h1 id="9217" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">其他的</h1><p id="38e6" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如果所有这些都失败了，你可以通过生成更多的数据来增加你的<strong class="jf hj">训练集</strong>的大小。以下是如何在不改变图像的情况下做到这一点:</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="ea78" class="kp ig hi kl b fi kq kr l ks kt">datagen = ImageDataGenerator(rotation_range = 0, width_shift_range = 0, height_shift_range = 0, rescale = None, shear_range = 0, zoom_range = 0, horizontal_flip = False, fill_mode = 'nearest')</span></pre><p id="4ea9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">另一种方法是通过增加尺寸来提高所有照片的分辨率。为此，您可以为训练、验证和测试数据集调用新的图像数据生成器。请看下面我是如何将照片的尺寸从(96 x 96)增加到(128 x 128)的:</p><pre class="kg kh ki kj fd kk kl km kn aw ko bi"><span id="80c9" class="kp ig hi kl b fi kq kr l ks kt"># Import the Original Training Dataset <br/>train_gen2 = ImageDataGenerator(rescale=1./255).flow_from_directory(train_dir, target_size=(128,128), batch_size=15200) </span><span id="74f7" class="kp ig hi kl b fi ku kr l ks kt"># Import the Original Validation Dataset <br/>val_gen2 = ImageDataGenerator(rescale=1./255).flow_from_directory(val_dir, target_size=(128,128), batch_size=16)</span><span id="5f0b" class="kp ig hi kl b fi ku kr l ks kt"># Import the Original Testing Dataset <br/>test_gen2 = ImageDataGenerator(rescale=1./255).flow_from_directory(test_dir, target_size=(128,128), batch_size=624)</span></pre><p id="7396" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">艾丽卡·加布里埃尔</p><p id="1a13" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">您也可以在我们的移动应用程序上阅读这篇文章</p></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><p id="175a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><em class="kv">原载于 2020 年 9 月 7 日</em><a class="ae kw" href="https://www.analyticsvidhya.com/blog/2020/09/overfitting-in-cnn-show-to-treat-overfitting-in-convolutional-neural-networks/" rel="noopener ugc nofollow" target="_blank"><em class="kv">【https://www.analyticsvidhya.com】</em></a><em class="kv">。</em></p></div></div>    
</body>
</html>