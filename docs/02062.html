<html>
<head>
<title>Solving the FrozenLake environment from OpenAI gym using Value Iteration</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用值迭代求解OpenAI gym的FrozenLake环境</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/solving-the-frozenlake-environment-from-openai-gym-using-value-iteration-5a078dffe438?source=collection_archive---------1-----------------------#2019-11-28">https://medium.com/analytics-vidhya/solving-the-frozenlake-environment-from-openai-gym-using-value-iteration-5a078dffe438?source=collection_archive---------1-----------------------#2019-11-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7e5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以我在尝试学习强化学习，然后我碰到了这个东西叫做'<strong class="ih hj">值迭代</strong>。我真的无法理解价值迭代。我很难理解它是如何工作的，以及它如何帮助一个代理人找到最优策略。然后我想到了一个主意。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/c938e1339445308721af8927c77e1149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*KmheoOxlTCBv3RvyUqlm0Q.jpeg"/></div></figure><p id="439b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理解“价值迭代”还有什么比用它来解决某个游戏或环境更好的方法呢？就这样，我开始了寻找一些足够简单的游戏问题来解决的旅程。然后我偶然发现了<a class="ae jl" href="https://gym.openai.com/envs/FrozenLake8x8-v0/" rel="noopener ugc nofollow" target="_blank">这个来自OpenAI的仙女</a>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jm"><img src="../Images/78dd5b55a07bb74f6866b7dfe0fb535b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IdDJqLkkdwTw5UyD4vcOOA.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">一个简单的玩具文本游戏</figcaption></figure><h2 id="911c" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">了解FrozenLake8x8</h2><p id="202a" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">我先解释一下游戏/环境。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kv"><img src="../Images/cbb7be0fee4cd4c3b98609a37fa15ce0.png" data-original-src="https://miro.medium.com/v2/resize:fit:178/format:webp/1*sAAlNzWL5RX3ziwruibOog.png"/></div><figcaption class="jr js et er es jt ju bd b be z dx translated">FrozenLake8x8</figcaption></figure><p id="57b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">游戏中有64个州。代理从S (S代表开始)开始，我们的目标是到达G (G代表目标)。所以快走吧。没有。这是一个光滑的表面。中间的F和H是很奇怪的东西。所以F的意思是冰冻的表面。你可以在上面行走。但是H的意思是洞。如果你掉进了一个H，BOoom，<strong class="ih hj">为你结束了</strong>的游戏，重新从S开始。所以只要通过所有的F避开H就可以到达G了。没有。还有更多。既然这是一个“<em class="kw">冰冻的</em>湖，那么如果你往某个方向走，那么代理人真的往那个方向走的几率只有0.333%。我的意思是，代理的运动是不确定的，只是部分取决于所选择的方向。所以你不会一直朝着你想要的方向前进。关于FrozenLake8x8的更详细解释，<a class="ae jl" href="https://gym.openai.com/envs/FrozenLake8x8-v0/" rel="noopener ugc nofollow" target="_blank">点击这里</a>。</p><h2 id="b913" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">了解开放式健身房</h2><p id="d62d" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">好了，我们明白了如何从OpenAI加载环境。为此，我们将使用OpenAI的python库'<a class="ae jl" href="https://gym.openai.com/docs/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> gym </strong> </a>'。</p><p id="6e56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以使用<em class="kw"> env.render() </em>查看环境，其中红色突出显示了代理的当前状态。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kx"><img src="../Images/f144d881718d2df04c8b75fc499ed91f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*jRn4-NypP7L6Rd101JeZZw.png"/></div></figure><p id="d162" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kw">env . action _ space . sample()</em>从所有可能的动作中随机选择一个动作。而<em class="kw"> env.step(action) </em>根据给定的动作走一步。在这里，我们可以看到动作是“向右”<strong class="ih hj"/>，所以代理从S向右到F ( <em class="kw">这可能不总是这样，因为代理的移动是不确定的，所以有时当动作是“向右”时，代理也可能向下或向上。</em>)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ky"><img src="../Images/a4649b6ada9c0d4a299adec02a3a15c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*q91j9LbWKZD4v6UPt6JXUg.png"/></div></figure><h1 id="9f82" class="kz jw hi bd jx la lb lc kb ld le lf kf lg lh li ki lj lk ll kl lm ln lo ko lp bi translated">有趣的部分</h1><p id="d74d" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">好吧，这是最简单的部分。现在是困难的部分，或者我应该说是有趣的部分。代理人如何在这个湿滑的湖中航行并到达目标而不掉进洞里？</p><p id="1dda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们一步一步来。首先让我们编写“<em class="kw">值迭代</em>函数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lq"><img src="../Images/990c674237217fe0bbc2010ceba65e9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*397HTBFExRuJ2glvsKLPCw.png"/></div></div><figcaption class="jr js et er es jt ju bd b be z dx translated">值迭代函数的伪代码(I)</figcaption></figure><p id="81c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在价值迭代中，故事是这样的。对于一个特定的状态，首先我们计算来自该状态的所有可能动作的状态-动作值，然后用最大的状态-动作值更新该状态的值函数。这不同于“<em class="kw">政策迭代</em>”，在这里我们计算预期/平均国家行为值。当所有新状态值和旧状态值之间的差是一个可忽略的小值时，值迭代终止。</p><h2 id="7340" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">代码代码代码</h2><p id="e164" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">下面是我用于值迭代函数的代码。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lr ls l"/></div></figure><p id="0323" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大部分代码都很容易理解。我来解释一下非直觉部分。</p><p id="33e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="kw"> env.nS </em>和<em class="kw"> env.nA </em>分别给出状态和动作的总数。不过最有趣的还是<em class="kw"> env。p</em>；<em class="kw"> env。P[0] </em>输出这样一个字典。这里0在<em class="kw"> env。P[0] </em>是环境的第一状态。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/fd34bc03915c1b02d2b1734bcb1a1f56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*_6vIsfUdQWeM6bK_viqQow.png"/></div></figure><p id="eb0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里你可以猜到，字典的键0，1，2，3是我们可以从状态0开始陈述的动作。此外，每个动作包含一个列表，其中列表的每个元素是一个元组，显示转换到状态、下一个状态、奖励的概率，并且如果完成=真，完成=假。(<em class="kw">如果下一个状态是球洞或球门</em>，则done=True)。所以<em class="kw"> env。P </em>是包含所有状态的列表，其中每个状态包含一个字典，该字典将所有可能的动作从该状态映射到下一个状态，如果我们采取该动作，进入下一个状态的概率，奖励以及游戏是否在那里终止。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/80aa671ef79bbccd519fffa99ec4c627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*89UWxj0esJBnSgyAdZVb6Q.png"/></div></figure><p id="623e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里你可以看到54是一个如此完成的洞=真。同样63是这样做的目标=真。</p><p id="4c16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，根据<strong class="ih hj">值迭代公式</strong>，我们迭代所有这些动作，并使用公式计算动作状态值:</p><p id="87a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="kw"> Prob *(奖励+折扣_因子*状态_下一状态值)</em> </strong></p><p id="60b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些都在<em class="kw"> env中提供。P </em>。然后，我们更新具有最高状态-动作值的状态的值函数。我们迭代环境的所有64个状态，直到每次迭代后新状态值和旧状态值之间的差异小到可以忽略不计，或者如果我们已经超过了最大迭代次数。</p><h2 id="5259" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">从价值函数中提取策略</h2><p id="0186" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">现在我们有了所有状态的值函数，我们的下一步是从值函数中提取策略。</p><p id="6093" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用类似的技术来做这件事。对于特定的状态，我们从该状态计算所有可能动作的状态-动作值，并选择具有最高状态-动作值的动作。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lr ls l"/></div></figure><h1 id="7dcd" class="kz jw hi bd jx la lb lc kb ld le lf kf lg lh li ki lj lk ll kl lm ln lo ko lp bi translated">那么我们是达到了目标还是掉进了坑里？</h1><p id="828a" class="pw-post-body-paragraph if ig hi ih b ii kq ik il im kr io ip iq ks is it iu kt iw ix iy ku ja jb jc hb bi translated">终于！现在我们有了策略，我们可以遵循该策略，看看我们的代理是否达到了目标或掉进了洞里。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lr ls l"/></div></figure><p id="7abb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们运行代理1000集，并计算达到目标平均需要多少步。我们还计算它有多少次达不到目标，掉进了一个洞里。运行上述函数后，我们最终得到了这个答案。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es lv"><img src="../Images/5a268a2d556f6107fe29260248efbda1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nqmnvTIs_7wNqGJgSYfGBw.png"/></div></div></figure><p id="b60e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我觉得代理做的挺好的:)</p><p id="0a68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你也可以检查一下<a class="ae jl" href="https://gym.openai.com/envs/FrozenLake-v0/" rel="noopener ugc nofollow" target="_blank"> FrozenLake-v0 </a>，它是一个较小的版本，只有16个状态，并检查代理到达目标平均需要多少步。关于我解决FrozenLake8x8环境的完整代码，请访问我的GitHub repo，这里:<a class="ae jl" href="https://github.com/realdiganta/solving_openai/tree/master/FrozenLake8x8" rel="noopener ugc nofollow" target="_blank">https://GitHub . com/realdiganta/solving _ open ai/tree/master/frozen lake 8 x 8</a></p><p id="9b12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同时，随着我继续进入强化学习的令人兴奋的领域，我将在不久的将来解决更多的开放AI环境。请继续关注更多内容。</p><h2 id="ca60" class="jv jw hi bd jx jy jz ka kb kc kd ke kf iq kg kh ki iu kj kk kl iy km kn ko kp bi translated">参考资料:</h2><ol class=""><li id="52fc" class="lw lx hi ih b ii kq im kr iq ly iu lz iy ma jc mb mc md me bi translated"><a class="ae jl" href="https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249/ref=pd_sbs_14_t_0/147-7865363-3852556?_encoding=UTF8&amp;pd_rd_i=0262039249&amp;pd_rd_r=96f1b22e-1339-4f4f-93ff-26c787125b93&amp;pd_rd_w=vDJJ6&amp;pd_rd_wg=y1LXN&amp;pf_rd_p=5cfcfe89-300f-47d2-b1ad-a4e27203a02a&amp;pf_rd_r=ACR43W905JY9PYNWDRRS&amp;psc=1&amp;refRID=ACR43W905JY9PYNWDRRS" rel="noopener ugc nofollow" target="_blank">强化学习:导论|第二版，作者理查德·萨顿&amp;安德鲁·巴尔托</a></li></ol></div></div>    
</body>
</html>