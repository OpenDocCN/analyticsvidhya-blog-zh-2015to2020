<html>
<head>
<title>Feature Selection Using Wrapper Methods in R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">R中使用包装器方法的特征选择</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-selection-in-r-9bfae551d22b?source=collection_archive---------11-----------------------#2020-04-13">https://medium.com/analytics-vidhya/feature-selection-in-r-9bfae551d22b?source=collection_archive---------11-----------------------#2020-04-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f218ae54dbbea310d1ac8630ad519623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ztmORK6f8knhVG6X"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@edgr?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">edu·格兰德</a>在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="137e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">什么是特征选择？</strong>从数据集中过滤不相关或冗余特征的过程。通过这样做，我们可以降低模型的复杂性，使其更容易解释，并且如果选择了正确的子集，还可以提高准确性。</p><p id="0f10" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我将首先通过使用r来演示使用包装器方法的特性选择。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="4210" class="kc kd hi jy b fi ke kf l kg kh">cardData = read.csv(“Discover_step.csv”, head=TRUE)<br/>dim(cardData)<br/>head(cardData)</span></pre><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ki"><img src="../Images/c694a7fc105b7dd52512bea75ea34882.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SqOETigBxu4w195MZ0FUlQ.png"/></div></div></figure><p id="b410" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这个244 x 15的数据集中，第二列“q4”是我们的因变量，表示总体满意度，其他是从调查中挑选出来的问题。(您可以在下面找到选择的调查问题)</p><p id="d1bb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后我们检查是否有共线性。有共线性是指多元回归的多个自变量高度相关时。这给解释可能有偏差的系数估计带来了困难。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="e586" class="kc kd hi jy b fi ke kf l kg kh">library(psych)<br/>IVs &lt;- as.matrix(cardData[,3:15])<br/>corr.test(IVs)</span></pre><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kj"><img src="../Images/d79427302dcc7f2bef39c624ae3dfe13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hibMuYKW2jUJLqWYax6hZQ.png"/></div></div></figure><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kk"><img src="../Images/ebb746add00720410a92042656ba7353.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0xGXoHN1ZKzpUg6Vo7Jntw.jpeg"/></div></div></figure><p id="e202" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有阴影的单元格是大于0.6的值，这意味着它们有很强的相关性。我们仍然不确定这是否真的会影响结果，所以让我们运行线性回归来检查一下。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="650e" class="kc kd hi jy b fi ke kf l kg kh">model &lt;- lm(q4~., data=cardData[,-1]) #remove 1st column “id”<br/>summary(model)</span></pre><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kl"><img src="../Images/0ab8c6073564be7108e5bc35ab0fcfe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nJd_1nEN75E2lVjgpYeF-A.png"/></div></div></figure><p id="8126" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">事实上，我们可以从结果中看到，只有少数自变量是显著的(p &lt;0.05). Proves that the collinearity and overfitting problem is in this model. Hence, we do the variable selection to pick the key factors. There are three ways to use:</p><h1 id="d8c9" class="km kd hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated"><strong class="ak">前向选择</strong></h1><p id="a10b" class="pw-post-body-paragraph iv iw hi ix b iy lj ja jb jc lk je jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated">从方程中没有独立变量开始，成功地一次增加一个变量，直到没有剩余变量做出显著贡献。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="4e8b" class="kc kd hi jy b fi ke kf l kg kh">library(MASS)<br/>step_for &lt;- stepAIC(model, direction=”forward”)<br/>summary(step_for)</span></pre><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lo"><img src="../Images/f0f4cdb2c11cbbb97b21e57983af4fef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E43hNGwlP5M4jbBaDXLhmg.png"/></div></div></figure><p id="8b34" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">前向选择似乎不足以适用于这种情况。我们可以看到结果与原来的没有什么不同，存在的问题仍然存在。</p><h1 id="8e08" class="km kd hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">反向选择</h1><p id="e763" class="pw-post-body-paragraph iv iw hi ix b iy lj ja jb jc lk je jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated">从模型中所有潜在的独立变量开始，在每次迭代中删除最不重要的变量，直到进一步的决策弊大于利。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="76b5" class="kc kd hi jy b fi ke kf l kg kh">step_back &lt;- stepAIC(model, direction="backward")<br/>summary(step_back)</span></pre><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lp"><img src="../Images/ce4e7244e0d05bb4b442a6fd16557c34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kn8NJCoQwws7Fk8kcfccKg.png"/></div></div></figure><p id="b76d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">经过选择，我们得到了四个关键问题，它们是“q5f”、“q5g”、“q5h”和“q5m”。虽然R平方较低，但它在BIC (BIC = 667.36)和调整后的R平方(调整后的R平方= 0.2)上给我们提供了更好的性能。</p><h1 id="f2ee" class="km kd hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">逐步选择</h1><p id="8c98" class="pw-post-body-paragraph iv iw hi ix b iy lj ja jb jc lk je jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated">与正向选择非常相似，只是它还考虑了可能的删除(删除模型中已经变得无关紧要的变量，并用其他变量替换)。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="10ff" class="kc kd hi jy b fi ke kf l kg kh">step_both &lt;- stepAIC(model, direction=”both”)<br/>summary(step_both)</span></pre><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lp"><img src="../Images/ce4e7244e0d05bb4b442a6fd16557c34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kn8NJCoQwws7Fk8kcfccKg.png"/></div></div></figure><p id="e769" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">逐步回归是一种贪婪算法，在每一轮添加最佳特征或删除最差特征。如此往复通常有助于我们选择更合适的变量，因此，它成为传统回归分析中最流行的特征选择形式。</p><p id="74ca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在接下来的几篇文章中，我将展示其他有用的方法:特征提取中的因子分析和主成分分析。</p><blockquote class="lq lr ls"><p id="5c91" class="iv iw lt ix b iy iz ja jb jc jd je jf lu jh ji jj lv jl jm jn lw jp jq jr js hb bi translated"><strong class="ix hj">选择的调查问题</strong></p></blockquote><figure class="jt ju jv jw fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/49eabed4da9a5e516ee4bcc61d685844.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*trcgNUkEzCitx5MFtqBrSg.jpeg"/></div></div></figure></div><div class="ab cl ly lz gp ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="hb hc hd he hf"><h1 id="295b" class="km kd hi bd kn ko mf kq kr ks mg ku kv kw mh ky kz la mi lc ld le mj lg lh li bi translated">关于我</h1><p id="b909" class="pw-post-body-paragraph iv iw hi ix b iy lj ja jb jc lk je jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated">嗨，我是Kelly，一名具有新闻和传播背景的商业分析研究生，喜欢分享探索数据和有趣发现的生活。如果您有任何问题，请随时联系我，电话:<a class="ae iu" href="mailto:kelly.szutu@gmail.com" rel="noopener ugc nofollow" target="_blank">k<em class="lt">elly.szutu@gmail.com</em>T5】</a></p></div></div>    
</body>
</html>