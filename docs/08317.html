<html>
<head>
<title>Guessing Sentiment in 100 Languages…</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用100种语言猜度感悟…</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/guessing-sentiment-in-100-languages-4574ceed3b67?source=collection_archive---------6-----------------------#2020-07-25">https://medium.com/analytics-vidhya/guessing-sentiment-in-100-languages-4574ceed3b67?source=collection_archive---------6-----------------------#2020-07-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="1ade" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">使用mBERT和XLM-罗伯塔的零射击多语言情感分类。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/79e0ff7925c55895f81496d75163c79c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ayPD6k6543XjaTKl"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">马克·拉斯姆森在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="58d6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果我们使用非英语语言，使用NLP会是一件非常痛苦的事情。其他语言的困难是由于资源和预训练模型的稀缺。随着迁移学习和预训练模型的出现，NLP生态系统发生了巨大的变化，那么什么是迁移学习呢:</p><blockquote class="kk kl km"><p id="facb" class="jo jp kn jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated">迁移学习是一种机器学习方法，其中为一项任务开发的模型被重新用作第二项任务模型的起点。</p></blockquote><p id="8045" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">各种预先训练好的模型，比如:<strong class="jq hj">谷歌的BERT，XLNET，facebook的RoBERTa，Open AI的GPT，FastAi的ULMFiT </strong>等等。提供了很好的结果，但这些大多局限于英语。我们也可以从头创建一个语言模型，我们可以检查如何从头构建一个印地语语言模型，在这个<a class="ae jn" rel="noopener" href="/analytics-vidhya/predicting-the-next-hindi-words-de58541fbbcf?source=friends_link&amp;sk=1663d0980a834c4581fed0ec0dc08b0b">以前的文章</a>中，但是从头为每种语言创建一个语言模型是非常困难的，对于低资源语言来说更困难。</p><h1 id="7e53" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">零射击学习:</h1><p id="2a3f" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">解决上述问题的方法是零镜头学习，我们需要用一种特定语言的数据来填充我们的模型，它可以在各种其他语言上工作！！</p><blockquote class="kk kl km"><p id="5b86" class="jo jp kn jq b jr js ij jt ju jv im jw ko jy jz ka kp kc kd ke kq kg kh ki kj hb bi translated">我们将在特定语言(此处为<em class="hi">英语</em>)的任务(此处为<em class="hi">情感分析</em>)上训练语言模型，我们的模型将能够在任何其他语言上执行该任务，而无需对该语言进行任何显式训练！！！</p></blockquote><h1 id="c8c0" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">竞争者:</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es lo"><img src="../Images/db3efba6ba400317b6231ea522de1a48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MN-2n1hvnP2vf9bz"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">照片由<a class="ae jn" href="https://unsplash.com/@darthxuan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">宣阮</a>在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="a5ed" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在让我们来看两个当今最突出的最先进的多语言模型。</p><ol class=""><li id="5a05" class="lp lq hi jq b jr js ju jv jx lr kb ls kf lt kj lu lv lw lx bi translated"><strong class="jq hj"> mBERT: </strong> <a class="ae jn" href="https://github.com/google-research/bert/blob/master/multilingual.md" rel="noopener ugc nofollow" target="_blank">多语言BERT </a> (mBERT)与BERT一同发布，支持<a class="ae jn" href="https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages" rel="noopener ugc nofollow" target="_blank"> 104种语言</a>。这种方法非常简单:本质上就是对来自多种语言的文本进行BERT训练。特别是，它在维基百科的内容上进行了训练，并拥有跨所有语言的共享词汇。例如，为了解决维基百科内容不平衡的问题，英文维基百科的文章比冰岛语维基百科多120倍，小语种被过采样，而大语种被欠采样。</li><li id="e230" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj lu lv lw lx bi translated"><strong class="jq hj">XLM-罗伯塔</strong>:脸书人工智能团队在2019年11月发布了<a class="ae jn" href="https://arxiv.org/pdf/1911.02116.pdf" rel="noopener ugc nofollow" target="_blank">XLM-罗伯塔</a>，作为他们最初<a class="ae jn" href="https://arxiv.org/pdf/1901.07291.pdf" rel="noopener ugc nofollow" target="_blank"> XLM-100 </a>模型的更新。两者都是基于transformer的语言模型，两者都依赖于屏蔽语言模型目标，两者都能够处理来自100种不同语言的文本。XLM-罗伯塔对原版的最大更新是训练数据量的显著增加。我们可以从下面的图表中了解到训练数据的增长情况。</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es md"><img src="../Images/07dba90c3cade4cf0c7f67fbd1854fa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9cRchmIyxP4LUnONXLM82g.png"/></div></div></figure><p id="c2c0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">对其进行训练的清理过的<strong class="jq hj"> CommonCrawl </strong>数据占用了高达<strong class="jq hj"> 2.5tb </strong>的存储空间！它比用于训练其前身的Wiki-100语料库大几个数量级。“罗伯塔”部分来自于其训练程序与单语罗伯塔模型相同的事实，具体来说，唯一的训练目标是<strong class="jq hj">掩蔽语言模型</strong>。XLM-R使用自我监督的训练技术来实现跨语言理解的最先进的性能，在这项任务中，模型以一种语言进行训练，然后在没有额外训练数据的情况下与其他语言一起使用。该模型通过纳入更多的训练数据和语言(包括所谓的低资源语言，缺乏大量的标记和未标记数据集)，对以前的多语言方法进行了改进。</p><h1 id="7c1d" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">让我们检查一下数据:</h1><p id="48b9" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">我们将使用<a class="ae jn" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank"> imdb电影评论数据集</a>，它由来自电影评论情感的句子组成。任务是预测给定句子的情绪(影评)。我们使用双向(正/负)类分裂，并且仅使用句子级标签。</p><p id="296a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们看一下我们的数据:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es me"><img src="../Images/434ffe059b75a0b83f044d0cd5c91650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*pbR_7WPp85yOZ_pFu2VYBw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">数据快照..</figcaption></figure><h1 id="3a80" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">对峙:</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mf"><img src="../Images/169e0688d8790aef014079d47bdd2d0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ErXGT0qHXw3whOYC"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">赫尔墨斯·里维拉在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="a69b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们将使用上述imdb数据集微调预训练的m-bert和xlm-roberta模型。我们将使用<a class="ae jn" href="https://huggingface.co/transformers/index.html" rel="noopener ugc nofollow" target="_blank">拥抱脸的PyTorch </a>实现，因为它非常简单直观地实现和微调m-bert和xlm-roberta。这是<a class="ae jn" href="https://gist.github.com/sayakmisra/b0cd67f406b4e4d5972f339eb20e64a5" rel="noopener ugc nofollow" target="_blank">笔记本</a>，这些是我们广泛遵循的实施步骤:</p><ol class=""><li id="bae3" class="lp lq hi jq b jr js ju jv jx lr kb ls kf lt kj lu lv lw lx bi translated">加载<a class="ae jn" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank">数据集</a>并解析它。</li><li id="d85d" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj lu lv lw lx bi translated">将句子编码成XLM-罗伯塔/伯特可以理解的格式。</li><li id="e502" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj lu lv lw lx bi translated">训练(微调)，它包括这些步骤<strong class="jq hj"> : </strong></li></ol><ul class=""><li id="c2d8" class="lp lq hi jq b jr js ju jv jx lr kb ls kf lt kj mg lv lw lx bi translated">打开我们的数据输入和标签</li><li id="a585" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj mg lv lw lx bi translated">将数据加载到GPU上进行加速</li><li id="fbf6" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj mg lv lw lx bi translated">清除上一步中计算的渐变。</li><li id="f79b" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj mg lv lw lx bi translated">正向传递(通过网络输入数据)</li><li id="6243" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj mg lv lw lx bi translated">反向传递(反向传播)</li><li id="1f92" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj mg lv lw lx bi translated">告诉网络使用optimizer.step()更新参数</li><li id="2140" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj mg lv lw lx bi translated">跟踪用于监控进度的变量</li><li id="cb58" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj mg lv lw lx bi translated">我们将指定<strong class="jq hj">XLM-罗伯塔/伯特福序列分类</strong>作为最后一层，因为它是一个分类任务<strong class="jq hj">。</strong></li></ul><p id="74bf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">4.将微调后的模型保存到我们的本地磁盘或驱动器。</p><p id="d605" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">5.下载保存的模型，并在我们的本地机器上做一些语法检查。</p><p id="6c42" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这里是包含全部代码的<a class="ae jn" href="https://gist.github.com/sayakmisra/b0cd67f406b4e4d5972f339eb20e64a5" rel="noopener ugc nofollow" target="_blank">笔记本</a>。</p><h1 id="ebb8" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">结果:</h1><p id="aa84" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">我们将检查两个句子:</p><ol class=""><li id="cbc9" class="lp lq hi jq b jr js ju jv jx lr kb ls kf lt kj lu lv lw lx bi translated">这是一次奇妙的经历。(正面情绪)。</li><li id="e4c5" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj lu lv lw lx bi translated">我不会推荐给任何人。(负面情绪)。</li></ol><p id="e0f1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们将这些翻译成六种不同的语言(印地语、法语、汉语、泰米尔语、乌尔都语和孟加拉语)，并用我们微调过的模型对它们进行测试，并检查结果。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mh"><img src="../Images/6d0bc3d18dd905817a4081134e2e0328.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QTHbXJKO51vsw448pfR7dQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">7种语言的结果..</figcaption></figure><p id="beef" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">正如我们所看到的，两个模型都表现得很好，尽管mBERT对像乌尔都语这样的低资源语言进行了几次错误分类。这里我们必须考虑一件事，<strong class="jq hj">我们只使用英语数据对我们的模型进行了微调，并没有将它暴露给任何其他语言数据</strong>。尽管如此，它仍然能够非常成功地检测出所有语言中的情感！</p><h1 id="607b" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">有明显的赢家吗？</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mi"><img src="../Images/474e299c14662aa76cfafebe556c85ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ryh50izX8tRblr_o"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://unsplash.com/@macauphotoagency?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">澳门图片社</a>在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="265a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">从我们执行的几个测试来看，还不太清楚谁是赢家，所以我们需要检查大型多语言数据集的结果，让我们来看看这些结果:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mj"><img src="../Images/d5d5a1a96d8da5dacc86ecaae6268d68.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*5fpOBdLykhdP_irNlI6v7w.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">关于NER的结果</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mk"><img src="../Images/340ced6e2f650aacce5473844bbf7be0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KxM6myaO-47awqTL7r0bBA.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">关于MLQA的结果</figcaption></figure><p id="b14e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在考虑了大型多语言和单语语料库的结果后，我们可以说我们确实有一个赢家，那就是<strong class="jq hj">XLM-罗伯塔。</strong>与最先进的单语模型如:<strong class="jq hj"> BERT、RoBERTa、XLNet等相比，它在所有标准单语任务(如GLUE)中的表现即使不更好，也同样出色。</strong>我们也可以从下表中查看结果。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ml"><img src="../Images/1f6cbc8e281fc72d36ddadf214aee22e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jBhIbK-7EC8icVcJr9mTYQ.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">胶水基准XLM-R</figcaption></figure><h1 id="fb2a" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">结论:</h1><p id="446c" class="pw-post-body-paragraph jo jp hi jq b jr lj ij jt ju lk im jw jx ll jz ka kb lm kd ke kf ln kh ki kj hb bi translated">总结讨论，我们可以有把握地说，XLM-罗伯塔是零射击多语言任务的更好选择，尽管mBERT也紧随其后。多语言模型非常强大。最新的发展，XLM-R，处理100种语言，仍然保持与单语同行的竞争力。</p><h1 id="e060" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">参考资料:</h1><ol class=""><li id="5e53" class="lp lq hi jq b jr lj ju lk jx mm kb mn kf mo kj lu lv lw lx bi translated">拥抱脸变形金刚<a class="ae jn" href="https://huggingface.co/transformers/index.html" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/transformers/index.html</a></li><li id="fd52" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj lu lv lw lx bi translated">Chris mcCormick的博客文章<a class="ae jn" href="http://mccormickml.com/2019/11/11/bert-research-ep-1-key-concepts-and-sources/" rel="noopener ugc nofollow" target="_blank">http://mccormickml . com/2019/11/11/Bert-research-EP-1-key-concepts-and-sources/</a></li><li id="705f" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj lu lv lw lx bi translated">https://arxiv.org/pdf/1911.02116.pdf XLM-罗伯塔论文<a class="ae jn" href="https://arxiv.org/pdf/1911.02116.pdf" rel="noopener ugc nofollow" target="_blank"/></li><li id="f283" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj lu lv lw lx bi translated">伯特论文<a class="ae jn" href="https://arxiv.org/pdf/1906.01502.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1906.01502.pdf</a></li><li id="e833" class="lp lq hi jq b jr ly ju lz jx ma kb mb kf mc kj lu lv lw lx bi translated"><a class="ae jn" href="https://peltarion.com/blog/data-science/a-deep-dive-into-multilingual-nlp-models" rel="noopener ugc nofollow" target="_blank">https://pelt arion . com/blog/data-science/a-deep-dive-into-multilingual-NLP-models</a></li></ol></div></div>    
</body>
</html>