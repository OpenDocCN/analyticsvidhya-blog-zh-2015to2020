<html>
<head>
<title>Gradient Descent- A Race to find the Global Minimum</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降-寻找全局最小值的竞赛</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-a-race-to-find-the-global-minimum-a4d32e61071e?source=collection_archive---------20-----------------------#2020-08-22">https://medium.com/analytics-vidhya/gradient-descent-a-race-to-find-the-global-minimum-a4d32e61071e?source=collection_archive---------20-----------------------#2020-08-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="dd63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated"><span class="l je jf jg bm jh ji jj jk jl di"/>一个基本的神经网络基本上由输入层、隐藏层和带有一些网络参数(权重和偏差)的最终输出层组成。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/b0ecb13f5e5d4ad44f58a59be09a831b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bOO2P7-8Pevweth0KznpFQ.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">一个简单的神经网络</figcaption></figure><p id="d97a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以从神经网络中看到损失函数的存在，即实际值(y)和预测的 value(ŷ之间的差异。<br/>为了减少这种损失，我们使用了优化器。有各种类型的优化器，其中之一是梯度下降。</p><p id="7ae6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降是一种非常通用的优化算法，能够找到各种问题的最优解。<br/>梯度下降的一般思想是反复调整参数，以最小化损失函数。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es kc"><img src="../Images/856d25afced92d7343ebfdf98b7f8a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*-ykiedATGNU1r_tWZeu9pg.gif"/></div></figure><p id="9510" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简而言之，梯度下降所做的是找出最佳权重(或参数),以便通过在网络反向传播的帮助下修改所有权重(或参数)来减少损失函数。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es kd"><img src="../Images/b63c05b8676d8a251d537d8db36c5768.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*w3Jrjc_byb5cwwDCPwmLhQ.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">等式 1:修改旧权重以获得新权重</figcaption></figure><blockquote class="ke kf kg"><p id="648a" class="if ig kh ih b ii ij ik il im in io ip ki ir is it kj iv iw ix kk iz ja jb jc hb bi translated">这里λ被称为学习率。</p></blockquote><h1 id="b866" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">梯度下降对我们有什么帮助？</h1><p id="bcea" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">假设你在浓雾中迷失在群山之中，你只能感觉到脚下地面的坡度。快速到达谷底的一个好策略是朝着坡度最陡的方向下山。这正是梯度下降所做的:它测量误差函数关于参数θ的局部梯度，并且它沿着梯度下降的方向前进。一旦梯度为零，你就达到了最小值。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es lo"><img src="../Images/e2f4fd811ffcafea5e30c0f52260331f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SQNi-hdqsgJJj3eGr76Jcg.png"/></div></div></figure><p id="a6eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们从用随机值填充θ开始(这称为随机初始化)，然后我们逐渐改进它，一次一小步，每一步都试图降低成本(损失)函数(例如，MSE)，直到算法收敛到最小值。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es lp"><img src="../Images/c7499f1a6e1f908e1c62f264292c9f35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*kjhFSp6mwyL-lonG6UM-0w.png"/></div></figure><h1 id="740e" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">坡度对重量计算的影响</h1><p id="97df" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">根据我们计算新的或优化的权重的等式(等式 1)，函数的斜率起着非常重要的作用。</p><p id="45af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设我们在随机点上画一条切线，以确定斜率，如果切线的右侧指向下方(图 1)，那么它确定斜率为负，因此新的权重将大于旧的权重，并向曲线右侧移动。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es lq"><img src="../Images/b5dd732e3ac15881cad5d375dc80c408.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*aaYGm4pLNwP8DKEy1gq3QA.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">图一</figcaption></figure><p id="51b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果该切线的右手侧指向上方(图 2)，则确定斜率为正，因此新的权重将小于旧的权重，因此将向曲线的左侧移动。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es lr"><img src="../Images/b8e01bf87bdf1f3dff1187d7607b562c.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*DKL2lhn5UsrOm0TBx-ctWw.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">图 2</figcaption></figure><h1 id="5c9f" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">学习率参数(λ)的影响</h1><p id="4147" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">梯度下降中的一个重要参数是步长，由学习速率参数决定。如果学习率太小，那么算法将不得不经历多次迭代才能收敛，因此将花费很长时间。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es ls"><img src="../Images/e687b8b2030d948cdfd3aa997be3f3be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*n5q6pLNba0ZNI5fCliex0A.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">学习率太小</figcaption></figure><p id="68a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一方面，如果学习率太高，我们可能会跳过山谷，到达另一边，甚至可能比以前更高。这可能会使算法发散，值越来越大，无法找到好的解决方案。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es lt"><img src="../Images/f45c7ef81f70e9e8cb6fda0706abbced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*vHSl21qOSqet6iESTkPKig.png"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">高学习率</figcaption></figure><h1 id="b672" class="kl km hi bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">摘要</h1><p id="b3c1" class="pw-post-body-paragraph if ig hi ih b ii lj ik il im lk io ip iq ll is it iu lm iw ix iy ln ja jb jc hb bi translated">最后简化上面解释的梯度下降过程:</p><ol class=""><li id="6e8e" class="lu lv hi ih b ii ij im in iq lw iu lx iy ly jc lz ma mb mc bi translated">随机初始化曲线上的点。</li><li id="c2ed" class="lu lv hi ih b ii md im me iq mf iu mg iy mh jc lz ma mb mc bi translated">找到最佳学习率很重要，值可以是 0.1、0.001 或 0.0001。针对问题尝试不同的学习率值，看看哪个效果最好。</li><li id="e479" class="lu lv hi ih b ii md im me iq mf iu mg iy mh jc lz ma mb mc bi translated">计算当前位置的斜率。</li><li id="37c7" class="lu lv hi ih b ii md im me iq mf iu mg iy mh jc lz ma mb mc bi translated">如果斜率为负，则向右移动。</li><li id="7e5b" class="lu lv hi ih b ii md im me iq mf iu mg iy mh jc lz ma mb mc bi translated">如果斜率为正，向左移动。</li><li id="e2ab" class="lu lv hi ih b ii md im me iq mf iu mg iy mh jc lz ma mb mc bi translated">重复这个过程，直到斜率接近等于 0。</li></ol></div></div>    
</body>
</html>