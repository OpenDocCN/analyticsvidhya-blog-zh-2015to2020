<html>
<head>
<title>How to get background blur using Deep Learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何利用深度学习获得背景模糊？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-get-background-blur-using-deep-learning-91c2dfada33e?source=collection_archive---------19-----------------------#2020-11-15">https://medium.com/analytics-vidhya/how-to-get-background-blur-using-deep-learning-91c2dfada33e?source=collection_archive---------19-----------------------#2020-11-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/df92a42c0020415597329f7ba24a87e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rzv55bHOrEHEQ_3WI5_wVQ.png"/></div></div></figure><p id="e680" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">背景模糊效果也称为“散景”，是一种众所周知的效果，我们很多人主要在特写镜头中使用，它给我们的图像增加了深度感，因为我们只专注于图像的特定部分。为了获得这种效果，我们通常使用一些照片编辑应用程序，如Photoshop、Gimp、Picsart、Snapseed等。随着时间的推移，我们利用深度学习在计算机视觉和图像处理方面取得了重大进展。所以就产生了一个问题，我们可以利用深度学习来获得这种散景效应吗？答案是肯定的，我们可以，在下面的博客中，我将带您浏览完整的实现以及代码和一些理论方面，以便更好地理解。</p><h2 id="2e7d" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">内容</h2><ol class=""><li id="b4c3" class="kj kk hi is b it kl ix km jb kn jf ko jj kp jn kq kr ks kt bi translated">是如何实现的？</li><li id="84bc" class="kj kk hi is b it ku ix kv jb kw jf kx jj ky jn kq kr ks kt bi translated">我们将使用的深度学习模型</li><li id="bc0b" class="kj kk hi is b it ku ix kv jb kw jf kx jj ky jn kq kr ks kt bi translated">ReLu6</li><li id="9adf" class="kj kk hi is b it ku ix kv jb kw jf kx jj ky jn kq kr ks kt bi translated">履行</li><li id="b547" class="kj kk hi is b it ku ix kv jb kw jf kx jj ky jn kq kr ks kt bi translated">信用</li><li id="b0e8" class="kj kk hi is b it ku ix kv jb kw jf kx jj ky jn kq kr ks kt bi translated">结论</li></ol></div><div class="ab cl kz la gp lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="hb hc hd he hf"><h2 id="9a09" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">1.是如何实现的？</h2><p id="5fa3" class="pw-post-body-paragraph iq ir hi is b it kl iv iw ix km iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">基本上，整个目标是基于一种称为<strong class="is hj">图像分割</strong>的卷积神经网络的高级实现。<br/>我们都熟悉细胞神经网络，它用于根据输入标签的数量对图像进行分类。但是假设我们必须在给定的图像中识别特定的对象，为此我们必须使用对象检测的概念，然后是图像分割。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lj"><img src="../Images/c0d3af446d8b07936e32a6ae82640dac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VT9k81GPiNjPVJds.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">来源-谷歌</figcaption></figure><p id="41fe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是图像分类和检测的经典示例，其中，如果在单个图像中存在多种类型的对象，则我们进行对象检测，一旦我们找到图像中多个对象的坐标，给定的图像就进入感兴趣区域池，然后对这些对象进行分类，并在每个识别的对象周围绘制边界框。</p><p id="2963" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦所有这些都完成了，我们就进入图像分割的下一步，因为边界框只显示了对象在图像中的位置，而没有给出任何关于对象形状的信息。<br/>简单来说，图像分割就是将图像像素分割成小的部分或片段，并根据相似的信息或属性对它们进行分组，并为它们分配一个标签。这有助于捕捉像素级别的非常小的细节。分割为图像中每一个被识别的物体创建了一个像素级的遮罩，请看下图。主要目的是以这样一种方式训练神经网络，使得它可以作为图像的逐像素掩码给出。要了解更多细节，请点击<a class="ae ls" href="https://towardsdatascience.com/going-deep-into-object-detection-bed442d92b34" rel="noopener" target="_blank">此处</a>。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/35a8910f0f5beaf9b6d9a3d60edea60d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*LWdlB98WdEBYNMYe.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">来源- MissingLink.ai</figcaption></figure><h2 id="6dee" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">2.我们将使用的深度学习模型:</h2><p id="48a3" class="pw-post-body-paragraph iq ir hi is b it kl iv iw ix km iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">一旦我们清楚了图像分割，现在让我们看看我们将使用的模型，这是在coco数据集上训练的mobilenetv2。<br/>mobilenetv2是一款轻量级型号，可用于手机等低功耗设备，这是2017年推出的第二版mobilenetv1型号。<br/>现在让我们简单了解一下模型架构。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/f363665aa01957b4bc864e4534e2995e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0ZAKob-GxgVL7jW6.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">来源-走向数据科学</figcaption></figure><p id="af17" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">v2也基于v1，因此它继承了相同的深度方向可分离卷积，其包括深度方向卷积和点方向卷积，这降低了卷积运算的成本。</p><p id="efdb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">深度方向卷积简单地意味着，假设一个图像包含3个通道，那么每个核将分别在每个通道上迭代。</p><p id="1471" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，您有一个(10 x 10 x 3)的图像和3个(3 x 3 x 1)的过滤器，那么结果输出将是一个这样的过滤器的(8 x 8 x 1)，之后所有其他过滤器的输出堆叠在一起，形成由(8 x 8 x 3)组成的特征图。<br/>在逐点卷积中，我们采用(8 x 8 x 3)的先前特征图，并应用大小为(1 x 1 x 3)的过滤器。如果应用15个这样的过滤器，那么最终结果将被堆叠起来以形成(8×8×15)的特征图。</p><p id="7a3f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">mobilenetv2在v1上有一些改进，如反向残差、线性瓶颈和残差连接的实现。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/38e00935a5c66a47aadf23bbcbb07dd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/0*pRA_DdI8fxuNt9vT.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">来源-机器思考。网</figcaption></figure><p id="35e1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">v2总共有3个卷积层，其中第一个是扩展层，第二个是深度层，第三个是投影层。</p><p id="c300" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">扩展层:该层获取输入数据，并将低维数据扩展到高维，以便保留重要信息，并将其输出到深度层，扩展因子是一个<strong class="is hj">超参数</strong>，可根据试验次数进行调整。</p><p id="9201" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">深度方向层:该层接收来自扩展层的输入，并执行深度方向和点方向卷积，将特征图提供给投影层。</p><p id="cf66" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">投影层:这一层负责缩小数据的维度，以便只有有限数量的数据在网络中进一步传递，此时输入维度与输出维度匹配，也称为“瓶颈层”。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/2ac1e95968dd442d5e2f613502e98514.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/0*oIB8tqAPffWQ60e4.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">来源-机器思考。网</figcaption></figure><p id="e092" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">剩余连接是网络的新增部分，它基于ResNet，有助于控制梯度在网络中的流动。当输入数据的维度与输出数据的维度相同时使用。</p><h2 id="4665" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">3.ReLu6:</h2><figure class="lk ll lm ln fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/e4253eed9bea39494a8e482b10cec570.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/0*SNqlXmPE70WrIqZI"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">来源-谷歌</figcaption></figure><p id="9433" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个网络中的每一层都带有ReLu6，而不是带有批处理规范化的ReLu。ReLu6将值的范围限制在0到6之间，这是一个线性激活函数。通过限制小数点左边的3位信息，也有助于保持小数点右边的精度。</p><p id="bcfe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后一层(即投影层)的输出没有激活函数，因为它的输出是低维数据，根据研究人员的说法，向最后一层添加任何非线性函数都可能导致有用信息的丢失。</p><h2 id="a242" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">4.履行</h2><p id="0257" class="pw-post-body-paragraph iq ir hi is b it kl iv iw ix km iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">现在，我们对图像分割和我们将使用的mobilenetv2有了一个简单的概念，让我们开始实现部分。</p><p id="295c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">先决条件:-代码使用tensorflow版本1.x，所以你需要有版本1.x才能工作，如果你使用2.x，那么它会在执行时出错，所以我建议简单地使用Google Collab来执行它。</p><p id="ad83" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我将快速浏览代码的所有重要方面，完整的实现和逐行解释将在<a class="ae ls" href="https://github.com/patrickn699/Background_Blur.git" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上的笔记本中给出。</p><p id="6a55" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了演示，我们将使用下面的图像大小(596 x 900)</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/6c319ec66b4ef9b57418c6137b448121.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zweqlN8xwd7-os0PVv-yNQ.jpeg"/></div></div></figure><p id="1d5d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第一步</strong>:下载预先训练好的模型。</p><p id="bae3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于模型是预先训练好的，只需要下载它并把我们的图像传给它，<br/>，它就会返回分割后的图像。</p><pre class="lk ll lm ln fd lz ma mb mc aw md bi"><span id="4109" class="jo jp hi ma b fi me mf l mg mh">MODEL_NAME = 'mobilenetv2_coco_voctrainaug'  # <a class="ae ls" href="http://twitter.com/param" rel="noopener ugc nofollow" target="_blank">@param</a> ['mobilenetv2_coco_voctrainaug', 'mobilenetv2_coco_voctrainval', 'xception_coco_voctrainaug', 'xception_coco_voctrainval']</span><span id="43cf" class="jo jp hi ma b fi mi mf l mg mh">_DOWNLOAD_URL_PREFIX = '<a class="ae ls" href="http://download.tensorflow.org/models/'" rel="noopener ugc nofollow" target="_blank">http://download.tensorflow.org/models/'</a><br/>_MODEL_URLS = {<br/>    'mobilenetv2_coco_voctrainaug':<br/>        'deeplabv3_mnv2_pascal_train_aug_2018_01_29.tar.gz',<br/>    'mobilenetv2_coco_voctrainval':<br/>        'deeplabv3_mnv2_pascal_trainval_2018_01_29.tar.gz',<br/>    'xception_coco_voctrainaug':<br/>        'deeplabv3_pascal_train_aug_2018_01_04.tar.gz',<br/>    'xception_coco_voctrainval':<br/>        'deeplabv3_pascal_trainval_2018_01_04.tar.gz',<br/>}<br/>_TARBALL_NAME = 'deeplab_model.tar.gz'</span><span id="4b23" class="jo jp hi ma b fi mi mf l mg mh">model_dir = tempfile.mkdtemp()<br/>tf.gfile.MakeDirs(model_dir)</span><span id="967c" class="jo jp hi ma b fi mi mf l mg mh">download_path = os.path.join(model_dir, _TARBALL_NAME)<br/>print('downloading model, this might take a while...')<br/>urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME],<br/>                   download_path)<br/>print('download completed! loading DeepLab model...')</span><span id="bd89" class="jo jp hi ma b fi mi mf l mg mh">MODEL = DeepLabModel(download_path)<br/>print('model loaded successfully!')</span></pre><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mj"><img src="../Images/317e180d7f786ed51d000622f31bdc8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j8JOn8JLW9H6dLjZmoKz2w.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">输出</figcaption></figure><p id="acdd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">步骤2 </strong>:将输入的分割图像可视化的功能。</p><pre class="lk ll lm ln fd lz ma mb mc aw md bi"><span id="6347" class="jo jp hi ma b fi me mf l mg mh">def run_visualization():<br/>  """Inferences DeepLab model and visualizes result."""<br/>  try:<br/>    original_im = Image.open(IMAGE_NAME)<br/>  except IOError:<br/>    print('Cannot retrieve image. Please check url: ' + url)<br/>    return</span><span id="113e" class="jo jp hi ma b fi mi mf l mg mh">print('running deeplab on image')<br/>  resized_im, seg_map = MODEL.run(original_im)<br/>  vis_segmentation(resized_im, seg_map)<br/>  return resized_im, seg_map</span></pre><p id="34f2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 2.1 </strong>:用上图调用上述函数。</p><pre class="lk ll lm ln fd lz ma mb mc aw md bi"><span id="e765" class="jo jp hi ma b fi me mf l mg mh">IMAGE_NAME = 'download2.jpg'<br/>resized_im, seg_map = run_visualization()</span></pre><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/f8a28d3ee34deb0c8af41a5692671f1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cVSb-NE-Fs80H0drwXAaCA.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">分段后输出。</figcaption></figure><p id="8e36" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 2.2 </strong>:现在我们将读取输入图像并转换成numpy数组。</p><pre class="lk ll lm ln fd lz ma mb mc aw md bi"><span id="0d04" class="jo jp hi ma b fi me mf l mg mh">print(type(resized_im))<br/>numpy_image = np.array(resized_im)</span></pre><p id="e3b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第三步</strong>:背景与前景分离。</p><p id="129c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这一步中，我们将创建一个图像的副本，然后通过在背景中用0替换值并在创建蒙版的地方保留255来从分割的图像中分离背景和前景。这里7表示汽车类别。</p><pre class="lk ll lm ln fd lz ma mb mc aw md bi"><span id="f778" class="jo jp hi ma b fi me mf l mg mh">person_not_person_mapping = deepcopy(numpy_image)<br/>person_not_person_mapping[seg_map != 7] = 0<br/>person_not_person_mapping[seg_map == 7] = 255</span></pre><p id="f72e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 3.1 </strong>:可视化分离的屏蔽图像</p><pre class="lk ll lm ln fd lz ma mb mc aw md bi"><span id="683c" class="jo jp hi ma b fi me mf l mg mh">plt.imshow(person_not_person_mapping)</span></pre><figure class="lk ll lm ln fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/b8280b0e002d211c0555b55461432f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*kJcycir4fjsNGfrqQkd7NA.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">输出</figcaption></figure><p id="20d4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我们可以清楚地看到，背景被替换为黑色，蒙版汽车变成了白色，如前一步所述，我们也没有因为替换值而丢失任何重要信息。</p><p id="22d4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 3.2: </strong>将蒙版图像的大小调整为原始图像的大小。</p><p id="4f10" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在分割过程之后，图像的尺寸减小，在我们的例子中，它减小到(300 x 500)的尺寸，因此我们将调整图像的尺寸到它的原始尺寸，即(900 x 596)。</p><pre class="lk ll lm ln fd lz ma mb mc aw md bi"><span id="b7f8" class="jo jp hi ma b fi me mf l mg mh">orig_imginal = Image.open(IMAGE_NAME)<br/>orig_imginal = np.array(orig_imginal)</span><span id="5e6e" class="jo jp hi ma b fi mi mf l mg mh">mapping_resized = cv2.resize(person_not_person_mapping, <br/>                             (orig_imginal.shape[1],<br/>                              orig_imginal.shape[0]),<br/>                              Image.ANTIALIAS)<br/>mapping_resized.shape</span></pre><figure class="lk ll lm ln fd ij er es paragraph-image"><div class="er es mm"><img src="../Images/2f07ccc68082795405a54c3d28b8bb68.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*25Dvve_p-VWltZOJ6truVw.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">输出</figcaption></figure><p id="eeb6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 3.3 </strong>:二值化。</p><p id="607b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于调整了图像的大小，生成的值范围为0，1，2…255，为了再次将值限制在0–255之间，我们必须使用Otsu的二值化技术对图像进行二值化。<br/>简而言之，Otsu二值化是一种寻找灰度图像阈值<br/>的自适应方法。它遍历0-255范围内所有可能的阈值，并为给定图像找到最佳可能阈值。<br/>内部基于一些统计概念，如方差，找出基于选定阈值的类。<br/>一旦选择了最佳阈值，则大于阈值的像素值将被视为白色像素，小于阈值的像素值将被视为黑色像素。查看此<a class="ae ls" rel="noopener" href="/@hbyacademic/otsu-thresholding-4337710dc519">文章</a>了解详情。</p><pre class="lk ll lm ln fd lz ma mb mc aw md bi"><span id="85bd" class="jo jp hi ma b fi me mf l mg mh">gray = cv2.cvtColor(mapping_resized, cv2.COLOR_BGR2GRAY)<br/>blurred = cv2.GaussianBlur(gray,(15,15),0)<br/>ret3,thresholded_img = cv2.threshold(blurred,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)<br/>plt.imshow(thresholded_img)</span></pre><figure class="lk ll lm ln fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/ec6d8fed4253260e004c6c24434ef11d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*h-VsllvJ7hmM_wpXr14npA.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">输出</figcaption></figure><p id="62be" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">产量将保持不变，与前一次相比不会有太大差别。这是二维的灰度图像。</p><p id="2214" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">第四步</strong>:给阈值图像添加颜色。</p><p id="79a6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们已经完成了二值化，是时候将灰度图像转换成RGB图像了。</p><pre class="lk ll lm ln fd lz ma mb mc aw md bi"><span id="371a" class="jo jp hi ma b fi me mf l mg mh">mapping = cv2.cvtColor(thresholded_img, cv2.COLOR_GRAY2RGB)<br/>np.unique(mapping)</span></pre><figure class="lk ll lm ln fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/47fb29b23f759d50b9868f9d0193a849.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*9hY1jgCYrf8ocrP5QFj72Q.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">输出</figcaption></figure><p id="bdac" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在输出中，将颜色应用到图像后，它包含两个唯一的像素值，即0.255。使用此贴图，我们将在接下来的步骤中应用背景模糊。</p><p id="ae8e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 4.1 </strong>:对原始图像应用模糊。</p><p id="8a39" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，让我们应用模糊效果到我们的原始输入图像。</p><pre class="lk ll lm ln fd lz ma mb mc aw md bi"><span id="9dc8" class="jo jp hi ma b fi me mf l mg mh">blurred_original_image = cv2.GaussianBlur(orig_imginal,<br/>                                          (251,251),0)<br/>plt.imshow(blurred_original_image)</span></pre><figure class="lk ll lm ln fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/5e6f05a5219b55a8e30375a1e781a09a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*DC6Thlt8M0p2XiZUDsapHg.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">输出</figcaption></figure><p id="27b7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 4.2 </strong>:获取背景模糊。</p><p id="a3c3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是我们用一行简单的代码片段对输入图像的背景进行模糊处理的步骤。</p><pre class="lk ll lm ln fd lz ma mb mc aw md bi"><span id="3459" class="jo jp hi ma b fi me mf l mg mh">layered_image = np.where(mapping != (0,0,0), <br/>                         orig_imginal, <br/>                         blurred_original_image)<br/>plt.imshow(layered_image)</span></pre><p id="2570" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上面的片段中，我们所做的只是简单地填充像素强度值为0的模糊图像，即填充所有黑色像素，并填充像素强度值为255的原始图像，这是基于分割图的白色像素。<br/>这产生了如下好看的散景效果。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/a476ce6b47208507c7efa1d1b77dfe09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*HEALyTEyGXPnyGC5W1WT1g.png"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">输出</figcaption></figure><p id="ba59" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 4.3 </strong>:最终保存图像。</p><p id="c7c1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在唯一要做的就是保存散景图像，我们完成了！</p><pre class="lk ll lm ln fd lz ma mb mc aw md bi"><span id="f139" class="jo jp hi ma b fi me mf l mg mh">im_rgb = cv2.cvtColor(layered_image, cv2.COLOR_BGR2RGB)<br/>cv2.imwrite("Potrait_Image.jpg", im_rgb)</span></pre><h2 id="0469" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">5.学分:</h2><p id="5e16" class="pw-post-body-paragraph iq ir hi is b it kl iv iw ix km iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">这篇文章是参考Bhavesh Bhatt在YouTube上的视频写的，所以向他致敬，上面给出的所有代码片段只是重要的，完整的代码和逐行注释可以在我的<a class="ae ls" href="https://github.com/patrickn699/Background_Blur.git" rel="noopener ugc nofollow" target="_blank"> GitHub </a>页面上找到。</p><h2 id="cdb2" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">6.结论:</h2><p id="53e0" class="pw-post-body-paragraph iq ir hi is b it kl iv iw ix km iz ja jb lg jd je jf lh jh ji jj li jl jm jn hb bi translated">总而言之，这只是深度学习可以做的事情之一，使用它可以实现很多这样的事情。随着我们的进步，模型越来越好，从分类到生成深度假货，我们所有人都期待着它！。</p></div></div>    
</body>
</html>