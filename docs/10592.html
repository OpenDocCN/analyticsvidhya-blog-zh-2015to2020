<html>
<head>
<title>Every Data Scientist Needs To Learn This</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">每个数据科学家都需要了解这一点</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/every-data-scientist-needs-to-learn-this-4632e3a2e275?source=collection_archive---------18-----------------------#2020-10-25">https://medium.com/analytics-vidhya/every-data-scientist-needs-to-learn-this-4632e3a2e275?source=collection_archive---------18-----------------------#2020-10-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="5546" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">这项技能将为你打开一个充满新的可能性的世界</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/392a3d6bbe8c557a00342f6c1e0d7b81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4Zw7_m7VVmtjAAhT"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://unsplash.com/@rocknrollmonkey?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">摇滚猴子</a>在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="df36" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">曾经有过这个惊人的数据科学项目的想法，你在网上查找你需要的数据，但遗憾的是无处可寻？不幸的是，并不是所有你需要的数据集都在网上。那么，你应该怎么做呢？放弃你的想法回到kaggle？不要！一个真正的数据科学家应该能够收集自己的数据！</p><h1 id="050e" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">什么是网络抓取，为什么要学习？</h1><p id="fdd7" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">网络是最大的数据来源，至少在过去的20年里，它是人类知识的真实档案。网络抓取是从网络上提取数据的艺术，作为一名数据科学家，它是一个如此方便的工具，并打开了许多冷却项目的大门。</p><p id="357c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">请注意，一些网站禁止抓取，如果你抓取太频繁或恶意，可能会禁止你的IP地址。</strong></p><h1 id="9737" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated"><strong class="ak">我们怎么刮？</strong></h1><p id="7bc8" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">有两种方法来抓取网页。</p><p id="751f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">基于请求的抓取</strong>:通过这种方法，我们将向网站的服务器发送一个请求，服务器将返回页面的HTML，这与你在谷歌浏览器上点击“查看页面源代码”时找到的内容相同，你现在可以通过按下<strong class="jq hj"> ctrl+u </strong>来尝试一下。然后，我们通常会使用一个库来解析HTML并提取我们想要的数据。这种方法简单，轻量级，非常快，但是它并不完美，有一个缺点可能会阻止你使用它，事实上，现在大多数现代网站都使用JavaScript来呈现他们的内容，即:直到JavaScript执行后，你才能看到页面的内容，而请求方法无法处理。</p><p id="627c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><strong class="jq hj">基于浏览器的抓取</strong>:要执行JavaScript，我们需要一个完全成熟的浏览器，这就是这个方法的目的，我们将模拟一个浏览器，导航到我们想要的页面，等待JavaScript执行，我们甚至可以通过点击按钮、填写表格等方式与页面进行交互，然后只需查看HTML状态并提取数据。这种方法非常灵活，你几乎可以抓取任何你想要的网站，但是它比仅仅发送一个请求要慢得多，而且占用大量资源。</p><h1 id="4585" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">用硒刮任何东西:</h1><p id="7540" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">Selenium是广泛用于web自动化的库，但是实际上您也可以使用它来进行抓取！基本上，任何人类可以手动完成的任务，您都可以用selenium来模拟，您可以创建一个在发生某些事情时执行特定操作的机器人，或者您可以让selenium浏览网页并为您收集数据，这就是我们在本文中将要做的。</p><p id="d915" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了解析HTML，我们将使用美丽的汤。</p><p id="f5f0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">为了进一步阅读，这里有关于<a class="ae jn" href="https://selenium-python.readthedocs.io/" rel="noopener ugc nofollow" target="_blank">硒</a>和<a class="ae jn" href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/" rel="noopener ugc nofollow" target="_blank">美汤</a>的文档链接</p><h1 id="5241" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">演示:刮真的工作</h1><p id="0e0f" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">让我们来做一些练习，这个演示的目标是从给定的搜索查询中抓取作业，并将它们保存在csv文件中。</p><p id="7e1e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">更确切地说，我们感兴趣的是:</p><ul class=""><li id="2a62" class="lh li hi jq b jr js ju jv jx lj kb lk kf ll kj lm ln lo lp bi translated">职称</li><li id="7965" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated">工作地点</li><li id="be91" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated">发布报价的公司</li><li id="a16d" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated">工作说明</li><li id="86ef" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated">职务发布的时间</li></ul><p id="fc0a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这里有一个到样本<a class="ae jn" href="https://ma.indeed.com/viewjob?jk=8fb003e7a434a0c5&amp;tk=1elgb9sfbstbv800&amp;from=serp&amp;vjs=3" rel="noopener ugc nofollow" target="_blank">工作页面</a>的链接，这里有<a class="ae jn" href="https://github.com/tariqmassaoudi/IndeedScraping" rel="noopener ugc nofollow" target="_blank">项目代码</a></p><p id="e74c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">首先让我们导入所需的库:</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="4980" class="ma kl hi lw b fi mb mc l md me">from bs4 import BeautifulSoup<br/>from webdriver_manager.chrome import ChromeDriverManager<br/>import pandas as pd<br/>from selenium import webdriver<br/>from selenium.webdriver.chrome.options import Options</span><span id="2e35" class="ma kl hi lw b fi mf mc l md me">chrome_options = Options()<br/>chrome_options.add_argument("--headless")</span></pre><ul class=""><li id="cef0" class="lh li hi jq b jr js ju jv jx lj kb lk kf ll kj lm ln lo lp bi translated">漂亮的汤是用来和HTML交互的</li><li id="0db8" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated">熊猫出口csv</li><li id="07b9" class="lh li hi jq b jr lq ju lr jx ls kb lt kf lu kj lm ln lo lp bi translated">web驱动程序是实际的浏览器，我们将使用chrome，并将其配置为在<strong class="jq hj">无头模式</strong>下运行，这意味着它将在后台运行，我们将无法看到浏览工作页面的浏览器，这是可选的，如果您想查看浏览器，您可以删除它！</li></ul><p id="93ff" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">首先要做的是获得实际的工作页面，lucky的确有搜索功能，你所要做的就是导航到:</p><p id="124f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">"https://ma.indeed.com/jobs？q =数据+科学家&amp;start=10 "</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mg"><img src="../Images/235496dc4e7370fd51e7a8700618d675.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YbDv1RXV1AZGmKX2Xp7m_g.png"/></div></div></figure><p id="e4eb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">您将获得与数据科学相关的作业的第二页，因此您可以指定更改<strong class="jq hj"> q </strong>参数的搜索查询和更改<strong class="jq hj"> start </strong>参数的页码。请注意，我使用的是Indeed的摩洛哥门户网站，但这将适用于任何国家。</p><p id="22ef" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们将实现两个功能，一个是助手功能，导航到提取HTML的URL，并将其转换为我们可以与之交互的漂亮的Soup对象，另一个是提取工作页面的链接:</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="17e2" class="ma kl hi lw b fi mb mc l md me">def toSoup(url):<br/>    driver.get(url)<br/>    html = driver.page_source<br/>    soup = BeautifulSoup(html, 'lxml')<br/>    return soup</span><span id="9905" class="ma kl hi lw b fi mf mc l md me">def getPageUrls(query,number):<br/>    url="<a class="ae jn" href="https://ma.indeed.com/emplois?q=" rel="noopener ugc nofollow" target="_blank">https://ma.indeed.com/emplois?q=</a>"+str(query)+"&amp;start="+str(((number-1)*10))<br/>    soup=toSoup(url)<br/>    maxPages=soup.find("div",{"id":"searchCountPages"}).text.strip().split(" ")[3]<br/>    return maxPages,[appendIndeedUrl(a["href"]) for a in soup.findAll("a",{"class":"jobtitle turnstileLink"})]</span></pre><p id="9dd6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们有了URL，让我们实现一些功能来从工作页面中提取我们想要的内容:</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="cde3" class="ma kl hi lw b fi mb mc l md me">def paragraphArrayToSingleString(paragraphs):<br/>    string=""<br/>    for paragraph in paragraphs:<br/>        string=string+"\n"+paragraph.text.strip()<br/>    return string</span><span id="ed0c" class="ma kl hi lw b fi mf mc l md me">def appendIndeedUrl(url):<br/>    return "<a class="ae jn" href="https://ma.indeed.com" rel="noopener ugc nofollow" target="_blank">https://ma.indeed.com</a>"+str(url)</span><span id="c54d" class="ma kl hi lw b fi mf mc l md me">def processPage(url):<br/>    soup=toSoup(url)<br/>    title=soup.find("h1",{"class":"icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title"}).text.strip()<br/>    CompanyAndLocation=soup.find("div",{"class":"jobsearch-InlineCompanyRating icl-u-xs-mt--xs jobsearch-DesktopStickyContainer-companyrating"})<br/>    length=len(CompanyAndLocation)<br/>    if length==3:<br/>        company=CompanyAndLocation.findAll("div")[0].text.strip()<br/>        location=CompanyAndLocation.findAll("div")[2].text.strip()<br/>    else:<br/>        company="NAN"<br/>        location=CompanyAndLocation.findAll("div")[0].text.strip()<br/>    date=soup.find("div",{"class":"jobsearch-JobMetadataFooter"}).text.split("-")[1].strip()<br/>    description=paragraphArrayToSingleString(soup.find("div",{"id":"jobDescriptionText"}).findAll())<br/>    return {"title":title,"company":company,"location":location,"date":date,"description":description}</span><span id="0ceb" class="ma kl hi lw b fi mf mc l md me">def getMaxPages(query):<br/>    url="<a class="ae jn" href="https://ma.indeed.com/emplois?q=" rel="noopener ugc nofollow" target="_blank">https://ma.indeed.com/emplois?q=</a>"+str(query)</span></pre><p id="55fa" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这里我们使用HTML属性如“class”或“id”来定位我们想要的信息，您可以通过检查页面来决定如何选择您需要的数据</p><p id="f63b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">以下是title属性的示例:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mh"><img src="../Images/1a4099752de8a61d161451c59e8ab857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fK-8qgIao3G_IPZhEhk6Pg.png"/></div></div></figure><p id="eade" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们可以看到标题是一个“h1”，我们可以使用它的类来选择它</p><p id="ca92" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">最后，让我们实现一个函数来运行获取所有的作业，并将它们保存在csv文件中。</p><p id="8687" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">请注意，我们正在获取最大页数，这样当我们到达最后一页时，爬虫就会停止。</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="0285" class="ma kl hi lw b fi mb mc l md me">def getJobsForQuery(query):<br/>    data=[]<br/>    maxPages=999<br/>    for number in range(maxPages):<br/>        maxPages,urls=getPageUrls(query,number+1)<br/>        for url in urls:<br/>            try:<br/>                page=processPage(url)<br/>                data.append(page)<br/>            except:<br/>                pass<br/>        print("finished Page number: "+str(number+1))<br/>    #Save the data to a csv file<br/>    pd.DataFrame(data).to_csv("jobs_"+query+".csv")</span></pre><p id="4b07" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在让我们刮数据科学工作:</p><pre class="iy iz ja jb fd lv lw lx ly aw lz bi"><span id="2bdf" class="ma kl hi lw b fi mb mc l md me">driver = webdriver.Chrome(ChromeDriverManager().install(),options=chrome_options)<br/>getJobsForQuery("data scientist")</span></pre><p id="eedd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">结果如下:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mi"><img src="../Images/bc3a12c69b9dd8ed73127215adf5e728.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VVRsIA6zuTDgtgdYGeuT_g.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">报废工作的样本</figcaption></figure><h1 id="4274" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">结论</h1><p id="9449" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">在这篇文章中，我们了解了网络抓取，为什么它对每个有抱负的数据科学家都很重要，以及这样做的不同方法，我们已经将它应用于抓取工作。</p><p id="0a09" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">如果你设法来到这里，恭喜你。感谢阅读，我希望你喜欢这篇文章。如需个人联系或讨论，请随时通过<a class="ae jn" href="https://www.linkedin.com/in/tariqmassaoudi/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我。</p></div></div>    
</body>
</html>