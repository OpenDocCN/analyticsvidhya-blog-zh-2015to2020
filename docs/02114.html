<html>
<head>
<title>Understanding Neural Style Transfer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解神经类型转移</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-neural-style-transfer-3061cd92648?source=collection_archive---------10-----------------------#2019-12-01">https://medium.com/analytics-vidhya/understanding-neural-style-transfer-3061cd92648?source=collection_archive---------10-----------------------#2019-12-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/155fc712479a391569c8491c83ca0202.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ChB5iqHsq3Nta-4uRdvMcw.png"/></div></div></figure><div class=""/><p id="9306" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">图像到图像的翻译是一个众所周知的问题，在深度学习中得到了广泛的研究。</p><p id="f2f3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于那些不知道图像到图像翻译的人来说，它是一种将图像从一个领域翻译到另一个领域的方法。例如白天夜晚、黑白图像、彩色图像、素描图像等。</p><p id="fde5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">图像到图像转换中的一个特殊问题是<strong class="is hu">样式转移，</strong>其中样式从一个图像(样式-图像)转移到另一个图像(内容-图像)。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es jo"><img src="../Images/a51722c797da13569564a5526618cba5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*wc5d6isZ79DAZxRmchEROQ.png"/></div></figure><p id="e510" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然而，不像其他图像到图像的翻译问题利用每像素的<a class="ae jt" href="https://deepai.org/machine-learning-glossary-and-terms/per-pixel-loss-function" rel="noopener ugc nofollow" target="_blank">损失</a>作为目标函数，很难根据相同的损失来测量风格转换。</p><p id="5c13" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="ju">那么，我们如何确定风格转换问题的损失函数呢？</em></p><p id="c352" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jt" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank">一种艺术风格转移的神经算法</a>提出使用一种在对象识别上训练的卷积神经网络(<strong class="is hu"> VGG-19 </strong>)来计算风格转移的客观损失。</p><p id="8a8a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本文的主要思想是图像的风格和内容可以在卷积神经网络中分别表示。这允许我们将一个图像的样式表示(样式-图像)和另一个图像的内容表示(内容-图像)组合起来，以生成新的<strong class="is hu">样式转换图像</strong>。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es jv"><img src="../Images/789baead0c62b50d62c1930dea063297.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5XQMKX5K8iF7o9Ulll6Ckw.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">使用VGG-19网络的风格和内容重建。(<a class="ae jt" href="https://arxiv.org/abs/1508.06576" rel="noopener ugc nofollow" target="_blank"> arXiv:1508.06576 </a>)</figcaption></figure><p id="c254" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="ju">内容和风格表现的真正含义是什么？</em></p><h1 id="241e" class="ka kb ht bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">内容表示</h1><blockquote class="ky kz la"><p id="550d" class="iq ir ju is b it iu iv iw ix iy iz ja lb jc jd je lc jg jh ji ld jk jl jm jn hb bi translated">网络中的较高层根据对象及其在输入图像中的排列来捕获高级内容，但是不约束重建的精确像素值。相比之下，较低层的重建只是简单地再现原始图像的精确像素值。</p></blockquote><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es le"><img src="../Images/ed088cef20833866cbdcc20a0c629b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*TjfPh1zQJtsWmUwzOuzHAw.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">更高层的内容表示(<a class="ae jt" href="https://arxiv.org/abs/1508.06576v2" rel="noopener ugc nofollow" target="_blank"> arXiv:1508.06576v2 </a></figcaption></figure><p id="530a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们知道，在为物体识别而训练的CNN中，网络的每一层都学习图像的表示，并且随着我们深入各层，这些表示变得更加具体。例如，初始层学习检测边缘和轮廓，而较高层学习检测一些对象。这意味着图像内容在CNN的较高层中被更好地表示，而较低层仅提供相同的像素值。我们使用这种内容表示来计算内容损失。</p><h2 id="eeea" class="lf kb ht bd kc lg lh li kg lj lk ll kk jb lm ln ko jf lo lp ks jj lq lr kw ls bi translated">内容损失</h2><p id="0955" class="pw-post-body-paragraph iq ir ht is b it lt iv iw ix lu iz ja jb lv jd je jf lw jh ji jj lx jl jm jn hb bi translated">在风格转换中，我们需要内容图像和生成图像的内容表示是相同的。我们假设一层CNN的输出由<strong class="is hu"> <em class="ju"> ϕ(x).给出</em> </strong>内容损失简单地说就是内容图像的内容表示和来自特定层的生成图像之间的欧几里德距离，计算如下</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es ly"><img src="../Images/ec257c4d1eda91806ab0af63df72077e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*kiJBt7132leaAep1vF8-Bw.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">VGG网络第j层的内容丢失。</figcaption></figure><p id="2292" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意:<strong class="is hu"><em class="ju"/></strong><em class="ju">，</em><strong class="is hu"><em class="ju">【hⱼ】</em></strong><em class="ju">，</em><strong class="is hu"><em class="ju">【wⱼ】</em></strong>分别代表第j层输出的通道、高度和宽度。</p><h1 id="f124" class="ka kb ht bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">风格表现</h1><blockquote class="ky kz la"><p id="5d95" class="iq ir ju is b it iu iv iw ix iy iz ja lb jc jd je lc jg jh ji ld jk jl jm jn hb bi translated">为了获得输入图像风格的表示，我们使用最初设计的特征空间来捕获纹理信息。</p></blockquote><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es lz"><img src="../Images/eec31673c2638de1cca4d43e5606ae0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:682/format:webp/1*i3GwLpzkSyiIgVJgIQLu6A.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">来自一个层面的星夜风格表现。(<a class="ae jt" href="https://arxiv.org/abs/1508.06576v2" rel="noopener ugc nofollow" target="_blank"> arXiv:1508.06576v2 </a>)</figcaption></figure><p id="260c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">卷积神经网络的每一层都提供一个特征图作为输出。对于在物体识别上训练的CNN，特征图中的每个通道代表图像的某些方面，例如边缘、圆、螺旋等。这些特征图不同通道之间存在相关性。从多个层考虑这些相关性，我们获得捕捉纹理的输入图像的多尺度表示。</p><h2 id="5a4e" class="lf kb ht bd kc lg lh li kg lj lk ll kk jb lm ln ko jf lo lp ks jj lq lr kw ls bi translated">风格丧失</h2><p id="d274" class="pw-post-body-paragraph iq ir ht is b it lt iv iw ix lu iz ja jb lv jd je jf lw jh ji jj lx jl jm jn hb bi translated">使用这种技术，我们可以获得任何图像的风格表示。现在，为了执行正确的样式转换，我们需要输入图像的样式表示和参考样式图像的样式表示相同。因此，这两种风格表现之间的距离可以作为我们需要最小化的损失。</p><p id="f5ce" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是我们如何计算相关性和相关性之间的距离呢？</p><p id="f096" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> Gram矩阵</strong>可用于计算特征图不同通道之间的相关性。我们假设一层CNN的输出由<strong class="is hu"><em class="ju">【ϕ(x】</em></strong>给出。那么特征图的gram矩阵可以被计算为</p><figure class="jp jq jr js fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ma"><img src="../Images/b71ea9e2bf301ccbd06e9b3947610a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t3AuMui-mMz9FoOVCqPMng.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">CNN第j层的Gram矩阵。</figcaption></figure><p id="878d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一旦我们知道如何计算特征之间的相关性，现在我们就可以继续计算风格损失。这种损失不过是样式图像(<em class="ju"> yₛ </em>)和生成图像(<em class="ju"> ŷ </em>)的gram矩阵之间的欧几里德距离。注意，该论文提到了gram矩阵的差之间的Frobenius范数，它只不过是欧几里得距离。因为我们使用了多个层，所以我们对每个层的距离求和。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es mb"><img src="../Images/8f0a281fe9879b48801c9da05bc667fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*sCY3STxEF6sGQXaQUlg4XA.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">VGG网络的第j次风格丧失。</figcaption></figure><p id="4708" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">内容损失和风格损失统称为<strong class="is hu">感知损失</strong>。确切地说，感知损失是内容损失和风格损失的加权和。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es mc"><img src="../Images/ddf39a44f817cd1ea5262953368757ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*7ui5IXcnPeNpVwihTezg6g.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">内容损失和风格损失的加权和</figcaption></figure><h1 id="b834" class="ka kb ht bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">感知优化</h1><p id="bdad" class="pw-post-body-paragraph iq ir ht is b it lt iv iw ix lu iz ja jb lv jd je jf lw jh ji jj lx jl jm jn hb bi translated">使用我们之前讨论的损失，现在我们需要设置算法来生成风格转移图像。我们使用感知优化来完成这项任务。</p><p id="f8cd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">感知优化将白噪声、风格图像和内容图像作为输入。我们的目标是以这样一种方式更新白噪声，使得它匹配样式图像的样式表示和内容图像的内容表示。为此，我们计算感知损失，即白噪声和风格图像之间的风格损失(用于匹配风格表示)以及白噪声和内容图像之间的内容损失(用于匹配内容表示)。一旦我们得到损失，然后我们反向传播计算梯度和更新白噪声。我们重复这个过程，直到感知损失收敛到最小值。<br/> <strong class="is hu">注意:</strong>我们也可以使用内容-图像来代替白噪声。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es md"><img src="../Images/b99c1b7cc74ff515579b275c5aeacc59.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/1*9Ot2GHv3FncdutIw5pdPog.gif"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">感知优化的可视化。</figcaption></figure><p id="d238" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Pytorch有一个非常好的<a class="ae jt" href="https://pytorch.org/tutorials/advanced/neural_style_tutorial.html" rel="noopener ugc nofollow" target="_blank">教程</a>用于神经风格转移的感知优化。一定要去看看！！。</p><p id="fc6f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这种算法的缺点是，每次对新图像，我们都需要从头开始执行感知优化。这样效率不高。</p><p id="a268" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="ju">那么，如何才能让这个过程更快呢？</em></p><h1 id="35ed" class="ka kb ht bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">变压器网络</h1><p id="fdde" class="pw-post-body-paragraph iq ir ht is b it lt iv iw ix lu iz ja jb lv jd je jf lw jh ji jj lx jl jm jn hb bi translated">我们可以根据感知损失训练一个网络，而不是更新白噪声。这个想法是在论文<a class="ae jt" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">实时风格转换和超分辨率的感知损失</a>中给出的，该论文使用了图像变换网络，并使用预先训练好的<strong class="is hu"> VGG-16 </strong>网络对感知损失进行训练。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es me"><img src="../Images/56ec1ecc2af5b9888e0f80c3ae8ceac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UYHpq1xoP1kmFieQg7IYUw.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">使用感知损失训练变压器网络(<a class="ae jt" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank"> arXiv:1603.08155 </a></figcaption></figure><p id="4419" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用这种方法，我们只需要训练网络一次。一旦我们训练了变压器网络，我们现在可以用它来进行风格转换。与前一种方法相比，这花费的时间要少得多。</p><p id="7dfb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是，我们只能针对一种风格——形象来训练网络。我们需要从头开始训练一个不同的风格-形象的新网络。训练一个全新的网络是一项耗时的任务。</p><p id="a459" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="ju">单个网络有可能学会所有风格吗？</em></p><p id="205c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">是的，答案是<strong class="is hu">条件实例规范化</strong>。</p><h1 id="ed90" class="ka kb ht bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">多风格传输网络</h1><p id="1339" class="pw-post-body-paragraph iq ir ht is b it lt iv iw ix lu iz ja jb lv jd je jf lw jh ji jj lx jl jm jn hb bi translated">论文<a class="ae jt" href="https://arxiv.org/abs/1610.07629" rel="noopener ugc nofollow" target="_blank">艺术风格的学习表示</a>提到许多风格可能共享某种程度的计算，并且这种共享被从零开始训练N个网络所丢弃。</p><p id="83c8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了在多种风格上训练单个模型，我们需要有一个条件网络。</p><p id="4538" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是我们应该把我们的条件放在哪里呢？</p><p id="dc44" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">规范化层可以用来整合我们的条件。在我们进入集成条件之前，让我们回顾一下规范化层的作用。</p><h2 id="2100" class="lf kb ht bd kc lg lh li kg lj lk ll kk jb lm ln ko jf lo lp ks jj lq lr kw ls bi translated">正常化</h2><p id="21ba" class="pw-post-body-paragraph iq ir ht is b it lt iv iw ix lu iz ja jb lv jd je jf lw jh ji jj lx jl jm jn hb bi translated">标准化层接收前一卷积层的输出特征，计算平均值(<strong class="is hu"> μ </strong>和标准偏差(<strong class="is hu"> σ </strong>)并标准化这些特征。然后使用可学习的权重<strong class="is hu"> <em class="ju"> γ </em> </strong>和<strong class="is hu"> <em class="ju"> β </em> </strong>对标准化特征进行缩放和转换。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es mf"><img src="../Images/a8d287798cddb9c5dec34e53a4c28f21.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*IHbPKEQpsM1JSGTYLEaPRg.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">正常化</figcaption></figure><h2 id="8d58" class="lf kb ht bd kc lg lh li kg lj lk ll kk jb lm ln ko jf lo lp ks jj lq lr kw ls bi translated">条件实例规范化</h2><p id="70e5" class="pw-post-body-paragraph iq ir ht is b it lt iv iw ix lu iz ja jb lv jd je jf lw jh ji jj lx jl jm jn hb bi translated">现在我们知道了规范化是如何工作的，我们可以继续前进了。发现我们可以为不同的风格使用不同的可学习权重。每种风格有不同的可学习权重，这使我们有可能调节网络。通过对不同的风格使用不同的<strong class="is hu"> <em class="ju"> γ </em> </strong>和<strong class="is hu"> <em class="ju"> β </em> </strong>，我们能够单独学习每种风格。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div class="er es mg"><img src="../Images/4e83899c3309e7bf098246bbd33a4d9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:638/format:webp/1*Kv_Cbd5t3U0vIXo1zXb2pQ.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">条件实例规范化。下标“s”是条件。</figcaption></figure><p id="d209" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于归一化仅缩放和转换特征，所以训练N型转移模型比从头开始训练N个单独的网络需要更少的参数。</p><p id="7ed6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该模型的感知结果类似于单风格转移网络。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/171380192d9e86bea3cafa18242d9abf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8u6OqhxsjmoVKHwhmTaxRQ.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">不同风格的风格转换结果。</figcaption></figure><p id="812a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">除了能够执行多种风格转换之外。该网络在视频输入方面也表现良好，并实时提供结果。</p><figure class="jp jq jr js fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mi"><img src="../Images/1250a8e44933c8b2874da3042bd50bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*nxg94N2VDUCnJQI01FcS5A.gif"/></div></div></figure><p id="a269" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">学习神经类型转移帮助我理解卷积神经网络中每一层的工作。此外，用于计算损失的技术让我对如何在神经网络中观察纹理有了很好的了解。我希望这篇文章能让你对风格转移网络的工作原理有一点直觉，并向你解释使用感知损失函数的原因。</p><p id="d1aa" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你感兴趣，想看看风格转换是如何工作的，那么我在Github上有一个实现。请检查一下！<a class="ae jt" href="https://github.com/ChinmayLad/neural-style-transfer" rel="noopener ugc nofollow" target="_blank">神经式转移</a>。</p><p id="68ba" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">请随意分享你对内容的想法，因为这是我的第一篇帖子，请帮助我改进它。谢谢你。</p><h1 id="3523" class="ka kb ht bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">参考</h1><ol class=""><li id="e625" class="mj mk ht is b it lt ix lu jb ml jf mm jj mn jn mo mp mq mr bi translated"><a class="ae jt" href="https://arxiv.org/abs/1508.06576v2" rel="noopener ugc nofollow" target="_blank">艺术风格转换的神经算法</a></li><li id="f97a" class="mj mk ht is b it ms ix mt jb mu jf mv jj mw jn mo mp mq mr bi translated"><a class="ae jt" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">实时风格转换和超分辨率的感知损失</a></li><li id="5cf7" class="mj mk ht is b it ms ix mt jb mu jf mv jj mw jn mo mp mq mr bi translated"><a class="ae jt" href="https://arxiv.org/abs/1610.07629" rel="noopener ugc nofollow" target="_blank">艺术风格的学术表现</a></li></ol></div></div>    
</body>
</html>