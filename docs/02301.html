<html>
<head>
<title>Human Activity Recognition(HAR) using Multi-Modal Attention</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用多模态注意的人类活动识别(HAR)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/human-activity-recognition-har-using-multi-modal-attention-8c81ceff6745?source=collection_archive---------7-----------------------#2019-12-10">https://medium.com/analytics-vidhya/human-activity-recognition-har-using-multi-modal-attention-8c81ceff6745?source=collection_archive---------7-----------------------#2019-12-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="e09b" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">“机器学习的一个突破抵得上10个微软”</p><p id="5ba7" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">比尔·盖茨</p></blockquote><h1 id="8122" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">什么是HAR，为什么它很重要？</h1><p id="ee80" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated">了解视频、直播流、电影等中发生的事情是一项有趣且有益的任务。这可以帮助我们更好地理解大量的可用内容。2012年，每天消耗的数据量超过7.6。这个数字每天都在增长，所消耗的数据变得更加密集和复杂，对于一个人来说，浏览如此丰富的内容并分享他们的理解几乎是不可能的。</p><p id="cf9d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">这就是我们需要自动化系统帮助的地方。主要在计算机视觉和自然语言处理领域的深度学习的进展使我们能够解决这个问题，并理解丰富的多模态数据。</p><p id="58da" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">人体动作识别的目标是识别日常环境中的人体活动。由于人类行为的多样性和复杂性，活动识别是一个具有挑战性的问题。鉴于我们的认知能力，这项任务对人类来说是微不足道的，但由于这项任务的巨大复杂性，对任何机器来说都是一项困难的任务。尽管由于深度学习的出现，我们在过去几年中看到了图像任务方面令人难以置信的进展，但视频任务的架构进展缓慢。</p><p id="b277" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">活动识别是一个有可能帮助理解视频流的问题，它可以用于从废弃物体检测到CCTV监控、异常检测和攻击行为检测的各种应用中。</p><h1 id="d37e" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">相关作品</h1><h2 id="0502" class="kn ji hi bd jj ko kp kq jn kr ks kt jr kh ku kv jv kj kw kx jz kl ky kz kd la bi translated"><em class="lb">注意:多模态人体活动识别的多级注意机制</em></h2><p id="79c2" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated">马等人提出使用传感器级数据(加速度计、陀螺仪等。)来预测人类的活动识别。多模态由子网级注意处理。这是通过使用CNN上的注意力子网和GRU上的单独注意力来实现的，并且这进一步用于对动作进行分类。</p><h2 id="fa17" class="kn ji hi bd jj ko kp kq jn kr ks kt jr kh ku kv jv kj kw kx jz kl ky kz kd la bi translated"><em class="lb">用于自我中心活动识别的多模态多流深度学习【9】</em></h2><p id="6472" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated">基于光流、单帧等的CNN融合与基于传感器数据的LSTMs融合一起使用。分数融合都被合并，并且使用softmax值。</p><h2 id="ef19" class="kn ji hi bd jj ko kp kq jn kr ks kt jr kh ku kv jv kj kw kx jz kl ky kz kd la bi translated">使用多模态的人体动作识别；</h2><p id="e0a5" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated">Carter等人[2]提出了一种对人体行为可疑行为的多模式数据进行合并分析的方法。这些模态包括RGB视频、深度视频、骨骼位置，以及来自Kinect摄像头和可穿戴惯性传感器的惯性信号，用于一套全面的27种人类动作[1]。</p><h1 id="eaec" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">哑谜数据集</h1><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lc"><img src="../Images/7535b97269c031a207da24dc79614b93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8V87-JV4KdtFM1FQ5HtcRg.jpeg"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">哑谜数据集由数百人表演给他们的动作的视频组成。</figcaption></figure><p id="abd5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">哑谜数据集由9848个室内活动视频组成，这些视频是通过亚马逊机械土耳其人收集的。给用户一个句子，并要求他们录制一段视频来表演这个句子，类似于一种猜字谜游戏。每个视频都使用训练集上的4个工作人员和测试集上的8个工作人员的共识进行了注释。</p><p id="5be8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">该数据集包含157个动作类的66，500个时间注释、46个对象类的41，404个标签和视频的27，847个文本描述。该数据集包含使用ffmpeg以H.264 / MPEG-4编码的视频。视频保持其原始分辨率和帧速率。它还包含从24 fps的视频中提取的jpeg帧。</p><p id="7f92" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">训练数据包括附加特征，例如:</p><ol class=""><li id="801e" class="ls lt hi il b im in iq ir kh lu kj lv kl lw jg lx ly lz ma bi translated">质量:- 7分制，7表示最高，由注释者评判。</li><li id="50e7" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg lx ly lz ma bi translated">相关性:- 7分制，7表示最高，由注释者判断。</li><li id="296b" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg lx ly lz ma bi translated">脚本:-生成视频所基于的句子。</li><li id="74f1" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg lx ly lz ma bi translated">已验证:-注释器是否成功验证了视频与脚本匹配。</li><li id="2b2a" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg lx ly lz ma bi translated">描述—观看视频的注释者的描述列表。</li></ol><h1 id="57a5" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">我们的方法和新颖性</h1><p id="bd8f" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated">使用多模态多级模型来实现该任务。多模态指的是使用一种以上的模态，例如:图像+音频或音频+文本等。如上所述，我们的模态是文本(对应于动作的脚本)和动作的视频。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mg"><img src="../Images/f39ed1b18106c711db246d5b56f8e095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5-Ixw2WkV3tr-Qqc"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">多模态架构</figcaption></figure><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="mh mi l"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated"><a class="ae mj" href="https://gist.github.com/rsk97/e2f2f35f069ad2046506c01dc8e3fe9e" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/rsk 97/e2f 35 f 069 ad 2046506 c 01 DC 8 e 3 Fe 9 e</a>(网络架构)</figcaption></figure><figure class="ld le lf lg fd lh"><div class="bz dy l di"><div class="mh mi l"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">句子的文本嵌入生成</figcaption></figure><h2 id="9dee" class="kn ji hi bd jj ko kp kq jn kr ks kt jr kh ku kv jv kj kw kx jz kl ky kz kd la bi translated">程序概述</h2><ol class=""><li id="78b8" class="ls lt hi il b im kf iq kg kh mk kj ml kl mm jg lx ly lz ma bi translated">对提取的视频帧(在数据集中可用)进行处理，以确保没有冗余。</li><li id="ea66" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg lx ly lz ma bi translated">然后将这些帧作为输入提供给MobileNet_V2和Inception模型来计算嵌入。</li><li id="2d61" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg lx ly lz ma bi translated">还对可用的脚本文件进行处理，并从中提取多级特征。</li></ol><ul class=""><li id="b13d" class="ls lt hi il b im in iq ir kh lu kj lv kl lw jg mn ly lz ma bi translated">脚本被分成句子，相应的嵌入使用USE(通用句子编码器)来计算</li><li id="2fff" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg mn ly lz ma bi translated">在GLoVe的帮助下，句子被进一步分成单词，并且提取相应的单词嵌入。</li></ul><p id="9773" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">4.这些提取的嵌入被馈送到多个BiLSTMs中，并且句子嵌入到前馈神经网络中，并且其结果被作为注意层的输入给出。</p><p id="f5ed" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">5.从输入到具有相关性和质量的分类值的下一层注意力建立了剩余连接。</p><p id="b96b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">6.这些特征一起用于输出预测。</p><h1 id="ceb4" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">结果</h1><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mo"><img src="../Images/205d796d076aff5a19e965ecd4d614f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/0*qmTMnAzEhumY5C98"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">交叉熵损失相对于时期数(训练集)作图</figcaption></figure><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mp"><img src="../Images/b4d45d900735ac7c0a11cc4fec31b9ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/0*c5aRlDtjjBEAhvdf"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">交叉熵损失相对于时期数(测试集)作图</figcaption></figure><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mq"><img src="../Images/3ebb5a605da6be1a8ecb5bc8944cdf45.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/0*szd_beBE1UpIOwDS"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">比较测试集和训练集的损失</figcaption></figure><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es mr"><img src="../Images/e812d3c889347418f098b6593a824189.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/0*vfXKWr1GAHtmH-U5"/></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">将训练损失与LSTM和CNN进行比较</figcaption></figure><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ms"><img src="../Images/8b07c341569956cee0fc9614955c7da4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LyIOb8Anmyt13OhA"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">多种形式的注意力得分</figcaption></figure><h1 id="bfc5" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">结论</h1><p id="749d" class="pw-post-body-paragraph ii ij hi il b im kf io ip iq kg is it kh ki iw ix kj kk ja jb kl km je jf jg hb bi translated">我们可以从结果中看到，基于多模态的方法比基于简单CNN的模型给出了更好的预测准确性。更丰富的输入表示(如视频和句子(子单词级别))使模型能够概括更新的环境或看不见的数据。基于注意力的方法有助于有效地简化给予不同模态相当大的权重的过程。</p><h1 id="4f30" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">未来作品</h1><ul class=""><li id="7f87" class="ls lt hi il b im kf iq kg kh mk kj ml kl mm jg mn ly lz ma bi translated">通过在LSTM时间步长之间引入注意，这项工作可以进一步扩展。</li><li id="2a09" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg mn ly lz ma bi translated">所使用的注意块可以用自我注意来改变(被限制只能访问过去，类似于《变形金刚》的解码器层)。</li><li id="622d" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg mn ly lz ma bi translated">元学习技术可以用来识别相似的任务，这有助于解决数据缺乏的问题。</li><li id="c382" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg mn ly lz ma bi translated">基于强化学习的注意力模型可以用来更好地适应新的数据集。</li><li id="f1f5" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg mn ly lz ma bi translated">可以使用来自各种其他网络的图像和文本嵌入。</li><li id="f743" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg mn ly lz ma bi translated">组合多个摄像机视图(以自我为中心，广角等)，这可以结合使用，以更好地理解注释。</li></ul><h1 id="2f42" class="jh ji hi bd jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke bi translated">参考</h1><ol class=""><li id="370c" class="ls lt hi il b im kf iq kg kh mk kj ml kl mm jg lx ly lz ma bi translated">C. Chen、R. Jafari和N. Kehtarnavaz，“UTD-MHAD:利用深度相机和可佩戴惯性传感器进行人体动作识别的多模态数据集”，正在进行中。IEEE Int。糖膏剂图像处理。(ICIP)，2015年9月，第168–172页。</li><li id="fbf0" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg lx ly lz ma bi translated"><em class="ik"> C. Chiu，J. Zhan，F. Zhan，“从部分配对和不完整的多模态数据中发现可疑活动”，IEEE Access，第5卷，第13689-13698页，2017年。</em></li><li id="05b3" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg lx ly lz ma bi translated"><em class="ik">夏若迪，吴廷通，杨庆，王，“多模态多部分学习在深度视频动作识别中的应用”，中国电机工程学会会刊。肛门模式。马赫。智能。，第38卷，第10期，第2123–2129页，2016年10月。</em></li><li id="1386" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg lx ly lz ma bi translated">Fortin，Mathieu Pagé和Brahim Chaib-draa。"使用图像、文本和标签的多模态多任务情感识别."WCRML '19 (2019)。</li><li id="a527" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg lx ly lz ma bi translated">张士卿、、黄铁军和。2016.用于视听情感识别的多模态深度卷积神经网络。2016年ACM多媒体检索国际会议论文集(ICMR '16)。美国纽约州纽约市ACM，281–284。</li><li id="28ad" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg lx ly lz ma bi translated"><em class="ik">陈光诚、裴东光、方振聪、王志军和r .内瓦蒂亚。AMC:用于图像搜索的注意力引导的多模态相关学习。2017年在CVPR。</em></li><li id="d2cb" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg lx ly lz ma bi translated"><em class="ik"> Hori，c .，Hori，t .，Lee，T.Y .，Zhang，z .，Harsham，b .，Hershey，J.R .，Marks，T.K .，Sumi，k .:基于注意力的多模态融合用于视频描述。In: 2017 IEEE计算机视觉国际会议(ICCV)。</em></li><li id="16dc" class="ls lt hi il b im mb iq mc kh md kj me kl mf jg lx ly lz ma bi translated"><em class="ik">马、、、小张、高松成、陆桑璐。"注意力:用于多模态人类活动识别的多级注意力机制."《第28届国际人工智能联合会议论文集》，第3109-3115页。AAAI出版社，2019。</em></li></ol><p id="fdd9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it kh iv iw ix kj iz ja jb kl jd je jf jg hb bi translated">没有我的队友们的贡献，这项工作是不可能完成的:<a class="ae mj" href="https://github.com/rsk97" rel="noopener ugc nofollow" target="_blank">罗汉·苏库马兰</a>、<a class="ae mj" href="https://github.com/arvinddeshraj" rel="noopener ugc nofollow" target="_blank">阿尔温德·德什拉杰</a>、<a class="ae mj" href="https://github.com/sid-kumar-iyer" rel="noopener ugc nofollow" target="_blank">西达尔特·库马尔</a>和<a class="ae mj" href="https://github.com/junaidnz97" rel="noopener ugc nofollow" target="_blank">朱奈德·NZ</a>。他们在这方面的专业知识是这项工作成功的关键。</p></div></div>    
</body>
</html>