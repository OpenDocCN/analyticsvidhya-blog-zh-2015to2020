<html>
<head>
<title>Implementing PCA using Sklearn.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Sklearn实现PCA。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementing-pca-using-sklearn-a88987e5a127?source=collection_archive---------15-----------------------#2020-06-23">https://medium.com/analytics-vidhya/implementing-pca-using-sklearn-a88987e5a127?source=collection_archive---------15-----------------------#2020-06-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/bf1a77f5a4098360275d37ea098f381d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aFNXrWpc_-QmrYIbMx4TKQ.jpeg"/></div></div></figure><p id="98f0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这篇文章中，我们尝试使用sklearn实现PCA(主成分分析),并理解PCA背后的逻辑。</p><p id="4067" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">来自谷歌来源:-</p><p id="2955" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">主成分分析</strong> ( <strong class="is hj"> PCA </strong>)是一种通过抑制变化来显示数据集中强模式的技术。它用于清理数据集，使其易于探索和分析。<strong class="is hj">主成分分析</strong>的<strong class="is hj">算法</strong>基于几个数学思想，即:方差和协方差。</p><p id="a895" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> *** </strong> <em class="jo">第一个问题</em> <strong class="is hj">我们要知道我们应该在什么地方使用PCA，在什么类型的数据集使用PCA算法</strong>。所有这些问题都有一个答案，即我们可以在高维数据集中使用PCA，这意味着我们将在特征或列数非常高的数据集中使用PCA，如(100到1000)列数或更多。</p><p id="12ed" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">* * *第二个问题</em> <strong class="is hj">我们为什么要用PCA？<br/> </strong>我们将在高维数据中使用PCA，因为在高维数据中，由于数据的稀疏性，我们基于距离的算法(如KNN算法)表现不佳。预测需要更多时间，因为我们有1000个列，这需要更多时间和空间<strong class="is hj">。</strong></p><p id="47dd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">* * *第三个问题</em><strong class="is hj">PCA是如何工作的？<br/> </strong>在PCA算法的帮助下我们减少了特征或列的数量。假设在我们的数据集中有1000个列，它看起来是这样的，data_old={ c1，c2，c3，c4，c1000 }在主成分分析的帮助下，我们在以前数据的线性组合的基础上创建了一组新的列(即data_old)。他们选择了能够解释我之前数据的最大方差的列数。 <em class="jo">(这意味着假设有一个名为“年龄”的列，我们看到，仅由于这个“年龄”列，我们的整个数据变化高达30%到35%。类似地，由于数据变化15 %，我们有一些其他列，所以这里我们选择数据中出现最大变化的列)</em></p><p id="846b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们将开始该项目，我们将处理一个图像数据集，其中有60，000个图像的数据，每个图像的分辨率为28*28，即784个像素，每个像素位于该特定图像的一列中，因此这里我们的数据形状为(60000，785)，其中有60000个图像，每个图像有784个像素列+1个标签列。</p><p id="1f24" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我正在写卡格尔笔记本。首先，我们将加载数据:-</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="7cbd" class="jy jz hi ju b fi ka kb l kc kd">data=pd.read_csv('/kaggle/input/fashionmnist/fashion-mnist_train.csv')</span></pre><p id="bbc7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你想看数据。<br/> <strong class="is hj"> data.head(5) </strong></p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ke"><img src="../Images/94896eda2788f2e36fe481c676194be2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H5UblgLEy_1C4jdrWrhxcA.png"/></div></div></figure><p id="af27" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里，我们看到第一列标签中有一个类似(2，9，6，0，3……)的数字，这些是我们在数据集中看到的不同布料的标签。这里，我们将该数据训练为y。这些列pixel1到pixel784是显示特定图像的784个像素的列，其中每列是一个像素。在这个模型中，我们将这个数据训练为x。</p><p id="084b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在是时候从数据集中获取X和y来训练和测试我们的模型了。其中在X中，我们有从索引1到784的图像像素值。在y轴上，我们有一个图像，不管这个图像是衬衫还是裤子，或者是索引为0的不同布料的图像。</p><p id="8f4e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">X=data.iloc[:，1:]。价值观念</p><p id="5147" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">y=data.iloc[:，0]。价值观念</p><p id="71c2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里，我们将看到我们的数据是怎样的:- <br/>首先，我们导入matplotlib.pyplot作为plt</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="5e1e" class="jy jz hi ju b fi ka kb l kc kd">import matplotlib.pyplot as plt<br/>a=X[0].reshape(28,28).   <br/>plt.imshow(a)</span><span id="5206" class="jy jz hi ju b fi kf kb l kc kd"># Here we grab 1st image from X and reshape into (28,28) because our data is in 28 *28 resolution then we put in plt.imshow(a)</span></pre><p id="022b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">结果是这样的</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kg"><img src="../Images/0de3f751a5d6fad6a6b3acba61ed2925.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DrYYtqKq5RVlyMa8z2Z86A.png"/></div></div></figure><p id="8a23" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们有了X和y，是时候训练测试分割了，因为我们从sklearn.model_selection导入了<strong class="is hj"> train_test_split </strong></p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="a740" class="jy jz hi ju b fi ka kb l kc kd">from sklearn.model_selection import train_test_split<br/>X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)</span></pre><p id="4802" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在train_test_split之后，现在我们将为此训练我们的模型，我们从sklearn.neighbors导入<strong class="is hj"> KNeighborsClassifier </strong>，并创建KNeighborsClassifier的对象clf。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="4547" class="jy jz hi ju b fi ka kb l kc kd">from sklearn.neighbors import KNeighborsClassifier<br/>clf=KNeighborsClassifier(n_neighbors=5)</span></pre><p id="e53d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们将我们的<strong class="is hj"> X_train </strong>和<strong class="is hj"> y_train </strong>放入fit来训练模型</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="bec6" class="jy jz hi ju b fi ka kb l kc kd">clf.fit(X_train,y_train)</span></pre><p id="5ea3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在这个时候，我的模型已经训练好了，可以预测了。所以使用clf对象的预测函数，我们在其中通过X_test，结果存储在y_pred中。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="2aab" class="jy jz hi ju b fi ka kb l kc kd">y_pred=clf.predict(X_test)</span></pre><p id="7613" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在是时候测量我们模型的准确性了，因为我们从sklearn.metrics导入了accuracy_score，其中我们通过了y_test和y_pred，其中我们得到了我们模型的预测。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="141b" class="jy jz hi ju b fi ka kb l kc kd">from sklearn.metrics import accuracy_score<br/>accuracy_score(y_test,y_pred)</span></pre><p id="f267" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们有了大约85%准确度的准确度分数，但是预测结果需要将近20分钟。<strong class="is hj">现在我们对模型应用PCA，然后我们看到模型预测模型的和<em class="jo">准确度分数所用的<em class="jo">时间。</em> </em></strong></p><p id="c9a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，在使用PCA之前，我们应该做一件事情，即<strong class="is hj">特征缩放</strong>，其中我们将把所有训练数据的值转换为0到1之间的值，之前这些值在0到255之间。</p><p id="42eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为此，我们将从sklearn.preprocessing包中导入StandardScaler类，并创建standardScaler类的对象sc，然后在该对象中传递我们的X_train和X_test。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="4843" class="jy jz hi ju b fi ka kb l kc kd">from sklearn.preprocessing import StandardScaler<br/>sc = StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test = sc.transform(X_test)</span></pre><p id="88c5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里我们有新的X_train和X_test，其中所有的特征值或列都在0和1之间。</p><p id="8ac1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">现在是应用PCA的时候了:- </strong></p><p id="2ab4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了在我们的模型中应用pca，我们首先要从sklearn.decomposition包中导入PCA类，然后创建一个PCA类的对象PCA，在该对象中我们传递n_components的值，该值为我们提供了关于您想要使用多少最佳特征或列的信息。</p><p id="f7eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">之后，您将创建一个X_train_new和X_test_new，其中我们通过了旧的X_train和X_test。然后，如果我们看到X_train_new或X_test _new，其行数与之前相同，但X_train_new和X_test_new中的列数等于n_components(假设n_components=50，这意味着PCA自动选取最佳的50列，这<strong class="is hj">解释了我之前数据的最大方差。(最大方差意味着</strong> <em class="jo">(这意味着假设有一个名为“pixel40”的列，我们看到仅由于这个“pixel-40”列，我们的整个数据变化高达22%到23%。类似地，我们有一些其他列，如“pixel60 ”,由于该数据变化14 %,所以这里我们看到只有这两列可以解释全部数据的36%。)</em></p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="c07d" class="jy jz hi ju b fi ka kb l kc kd">from sklearn.decomposition import PCA<br/>pca=PCA(n_components=50) <br/>X_train_new=pca.fit_transform(X_train)<br/>X_test_new=pca.transform(X_test)</span></pre><p id="65aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们打印出X_train_new和X_train的形状。你可以理解这里发生了什么</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="dddb" class="jy jz hi ju b fi ka kb l kc kd">print(X_train.shape)<br/>print(X_train_new.shape)</span><span id="523d" class="jy jz hi ju b fi kf kb l kc kd">(48000, 784)<br/>(48000, 50)</span></pre><p id="fe31" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你认为PCA内部发生了什么？那么借助于explained _ variance _ ratio _ method就可以看到了。我们在PCA对象中使用这个方法</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="6664" class="jy jz hi ju b fi ka kb l kc kd">pca.explained_variance_ratio_</span></pre><p id="633e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">此处输出:-</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="895e" class="jy jz hi ju b fi ka kb l kc kd">array([0.22056076, 0.14386729, 0.05453896, 0.05120558, 0.04079104,<br/>       0.03015539, 0.02755343, 0.02319117, 0.01699604, 0.01319041,<br/>       0.01164759, 0.00968628, 0.00893679, 0.00855538, 0.00746062,<br/>       0.00730633, 0.00654816, 0.00632012, 0.0062467 , 0.00581158,<br/>       0.00513186, 0.00507826, 0.00472421, 0.00452921, 0.004372  ,<br/>       0.00415789, 0.00397649, 0.00395591, 0.00379696, 0.00374935,<br/>       0.00369472, 0.003553  , 0.00334677, 0.00330491, 0.0032868 ,<br/>       0.00320113, 0.00305559, 0.00293335, 0.00289486, 0.00282557,<br/>       0.00273297, 0.00263301, 0.00255788, 0.00252376, 0.0024603 ,<br/>       0.0024129 , 0.00235778, 0.00226992, 0.0022254 , 0.00216175])</span></pre><p id="4f77" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这个数组中，我们看到第一个元素是0.22056076，这意味着我们的数据集中的任何一列，我们不知道是哪一列，但这个特定的列可以解释整个数据的22.05 %的方差。如果你同意，那就是0.14386729，另一栏解释了整个数据的14.39 %的方差。类似地，这就是为什么我们取n_components=50，其中我们取前50列来解释整个数据的最大方差。</p><p id="cd76" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们有了X_train_new和X_test_new，因此为此我们将在X_train_new和y_train的帮助下训练我们的模型。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="d246" class="jy jz hi ju b fi ka kb l kc kd">clf.fit(X_train_new,y_train)</span></pre><p id="32d8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们要预测。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="7756" class="jy jz hi ju b fi ka kb l kc kd">y_pred_new=clf.predict(X_test_new)</span></pre><p id="9d4e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在是时候在应用PCA之后测量准确度分数了</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="4d53" class="jy jz hi ju b fi ka kb l kc kd">accuracy_score(y_test,y_pred_new)</span></pre><p id="bdf3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">相信我在我们的模型中应用主成分分析后的预测结果。预测大约需要45秒，准确率接近85%。</p><p id="c478" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以你意识到在我们的高维数据集中应用主成分分析的重要性，它花费更少的时间，并且准确性几乎与不应用主成分分析相同..</p><h2 id="c156" class="jy jz hi bd kh ki kj kk kl km kn ko kp jb kq kr ks jf kt ku kv jj kw kx ky kz bi translated">不使用五氯苯甲醚:-</h2><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es la"><img src="../Images/e5cbc501d8caa316ed6391c859bc668c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UsZPc4MB6k42zIvoF7NMAw.png"/></div></div></figure><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lb"><img src="../Images/c92cbfc2c974e0d00c50649d8d55547b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OqVkB_NAOc8DiuwFiJggPQ.png"/></div></div></figure><h2 id="73d9" class="jy jz hi bd kh ki kj kk kl km kn ko kp jb kq kr ks jf kt ku kv jj kw kx ky kz bi translated">应用五氯苯甲醚后:-</h2><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/d1b807ac371111e7a3d5c7c0c339acc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*_jHldUHK5-SMJ9w7PKrwQw.png"/></div></figure><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ld"><img src="../Images/6a899b5bc2e54411dcb77e108d5c024d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O6ysy_83-8bfofjc94d4aw.png"/></div></div></figure><p id="e119" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在未应用PCA的两种情况下，我们看到准确率为85.75%，预测需要20分钟<strong class="is hj">和</strong>，在应用PCA后，准确率为85.56%，预测需要不到1分钟。所以我们在高维数据集中使用主成分分析。</p></div></div>    
</body>
</html>