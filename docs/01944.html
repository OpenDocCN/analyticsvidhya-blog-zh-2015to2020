<html>
<head>
<title>Explainability Of BERT Through Attention</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">伯特通过注意的可解释性</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/explainability-of-bert-through-attention-7dbbab8a7062?source=collection_archive---------3-----------------------#2019-11-23">https://medium.com/analytics-vidhya/explainability-of-bert-through-attention-7dbbab8a7062?source=collection_archive---------3-----------------------#2019-11-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="e359" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我们将向伯特的可解释性迈进一步，通过分析最近的一篇论文《伯特在看什么？伯特注意力分析<a class="ae jd" href="https://arxiv.org/pdf/1906.04341.pdf" rel="noopener ugc nofollow" target="_blank"> (Clark et al .，2019) </a>。</p><h1 id="e735" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak">简介</strong></h1><p id="7362" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">BERT(来自Transformer的双向编码器表示)是由谷歌人工智能语言在2018年推出的。它通过出色地完成机器翻译、句子分类、问题回答等广泛的任务，呈现出最先进的结果。</p><p id="3093" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BERT使用Transformer的双向训练(一个纯粹基于注意力的模型来捕捉长期依赖)。本文还介绍了Masked-LM算法，使双向训练成为可能。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es kh"><img src="../Images/eed1ae491363428f307650e15b8bd352.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*SOvSKfuipjXuwq94ukEYHw.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">伯特模型体系结构</figcaption></figure><p id="8753" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">调查伯特正在学习语言的哪个方面有助于验证我们模型的稳健性。因为BERT也是基于注意力机制，所以最简单的方法是可视化不同输入句子的注意力权重。更好地理解语言模型的一些其他方法是检查精心手工制作的句子的输出，或者使用调查模型的内部向量表示的探测分类器。作者关于伯特的一些发现是-</p><ul class=""><li id="454d" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc ky kz la lb bi translated">伯特的注意力呈现出一些模式，比如注意定界符、特定的位置偏移或者广泛地注意整个句子。</li><li id="04f5" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">同一层的注意力头经常表现出相似的行为。</li><li id="6390" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">一些中心词很好地符合语言学的句法和共指概念。</li></ul><p id="73e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从最近的工作中，我们可以说预训练的BERT嵌入向模型教授语言结构，但是我们并不确切地知道我们的模型正在学习哪种语言特征。这里有几个图，显示了模型的不同头部在注意力权重的帮助下学习到的不同语言特征。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lh"><img src="../Images/0b44950b873f8f564bee0dfae640d4cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wkI797LxIyy2fzHXphcQfg.png"/></div></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">不同的头脑关注句子的不同部分的例子。</figcaption></figure><h1 id="6037" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">注意分隔符标记</h1><p id="0c49" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">作者观察到的一个重要现象是，BERT的大部分注意力都集中在少数几个标记上。比如6-10层的BERT超过50%的注意力都集中在[SEP] token上。[SEP]和[CLS]令牌保证存在，并且它们从不被屏蔽。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lm"><img src="../Images/ce0902cf554b7d22feef587f4339d2f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*admvA3QvAtNirEKFdhd2TA.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">每个点对应于一个特定的BERT注意力头对一个标记的平均注意力</figcaption></figure><p id="dcd7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个可能的原因是，当注意头不适用时，这些特殊标记充当“无操作”。为了验证这一假设，作者计算了每个注意力权重的基于梯度的损失。结果显示，从第5层开始-----------------------------------------------------------------------------------《体感诱发电位》的关注权重变得很高，对《体感诱发电位》的关注梯度变得很小，表明《体感诱发电位》基本上不影响BERT的输出。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ln"><img src="../Images/371b21468c11b89f106e105d6b74887e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*_jCRoiWxX9Ru1GaToIQ4nA.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">基于梯度的特征重要性估计，用于对[SEP]、句号/逗号和其他标记的关注</figcaption></figure><h1 id="9150" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">注意整个句子还是注意单个标记</h1><p id="3e9d" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">为了分析注意力有多广，我们计算了每个头部注意力分布的平均熵。作者发现，下层的一些注意头具有非常广泛的注意。这些大脑在任何一个单词上最多花费10%的注意力。这些头的输出几乎是一个单词表示的包。而中间层的关注范围有所缩小。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lo"><img src="../Images/bd472ae43fa87af43b72478b86417f27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*No5k1IPkgA1fpqs05lbwbg.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">注意力分布的熵</figcaption></figure><p id="c01e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们还测量了所有注意力头部的[CLS]表征的熵。对于起始层，熵很高，然后随着中间层(6-10)注意力权重的增加而降低，但是随后再次增加到大约3.89，表明非常广泛的注意力。当我们进入最后一层时，注意力的焦点从多余的单词如[CLS]，“，”等转移到更重要的单词上。</p><h1 id="15ed" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">学习语法</h1><p id="2e7e" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">现在，我们将调查我们的注意力头学习语言的哪一方面。BERT使用字节对记号化，这个记号化器有时会把单词分成多个记号。因此，为了评估单词级注意头，将标记-标记注意图转换为单词-单词注意图。我们通过以下方式发现注意力:</p><ul class=""><li id="ac3d" class="kt ku hi ih b ii ij im in iq kv iu kw iy kx jc ky kz la lb bi translated">将单词拆分成单个单词:取其标记上注意力权重的平均值。</li><li id="359e" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">单个单词到拆分单词:对其标记上的所有注意力权重求和。</li></ul><p id="1810" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">他们还提出，一些注意力头专门捕捉特定的依赖关系。这里显示了对应于标题8-11(名词修饰语)和标题9-6(介词)的两个句子的注意力图的可视化，这些图指的是句子中所有词对之间的注意力权重。深色线条表示更多的注意力权重。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lp"><img src="../Images/bae1227b7046e4740176d536976f7c7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ADZlfwx7muEdhm-nbXUrZg.png"/></div></figure><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lq"><img src="../Images/cacb2498e89dd3ce12d6791a439a5f05.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*BgxopRX6J7BFQhgTPvyVpw.png"/></div></figure><h1 id="bfc6" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">聚集注意力</h1><p id="9027" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">本文中提出的另一个重要结果是，同一层中的头部通常彼此非常接近，这意味着同一层中的头部具有相似的注意力分布。为了找到这种Jensen-Shanon发散，我们使用了所有可能的注意力头对。为了使这些相似性可视化，每个头部都使用多维缩放投影到二维空间。原来是一层对应的注意力头相互靠近，它们形成一个集群。</p><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es lr"><img src="../Images/98de8fb740b0326a266ce585eee3b08d.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/0*78vE4mTe-xs4eIeQ"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">两个头之间的距离，其中JS是Jensen-Shanon散度</figcaption></figure><figure class="ki kj kk kl fd km er es paragraph-image"><div class="er es ls"><img src="../Images/d6c6f8ae816681730950ef626b172694.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*Fn5n_zOvccM3_u2peTgBUw.png"/></div><figcaption class="kp kq et er es kr ks bd b be z dx translated">伯特的注意力集中在嵌入二维平面的头部。</figcaption></figure><p id="0833" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上图可以明显看出，同一层的注意头聚集在一起，具有相似的注意权重。</p><blockquote class="lt lu lv"><p id="32f8" class="if ig lw ih b ii ij ik il im in io ip lx ir is it ly iv iw ix lz iz ja jb jc hb bi translated">注- <a class="ae jd" href="https://arxiv.org/abs/1902.10186" rel="noopener ugc nofollow" target="_blank"> Jain和Wallace(2019) </a>认为，注意力通常不能解释模型预测，并且这些注意力权重通常与特征重要性的其他度量不相关。我认为，注意力只是决定结果的几个重要因素之一。</p></blockquote><h1 id="6d33" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">参考</h1><ul class=""><li id="9642" class="kt ku hi ih b ii kc im kd iq ma iu mb iy mc jc ky kz la lb bi translated">凯文·克拉克、乌尔瓦希·汉德尔瓦尔、奥默·利维和克里斯托弗·曼宁。2019.伯特在看什么？伯特注意力分析。<a class="ae jd" href="https://arxiv.org/pdf/1906.04341.pdf" rel="noopener ugc nofollow" target="_blank"> abs/1906.04341 </a>。</li><li id="6e52" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">雅各布·德夫林、张明蔚、肯顿·李和克里斯蒂娜·图塔诺娃。2019.伯特:<a class="ae jd" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">深度双向转换语言理解预训练</a>。在NAACL-HLT</li><li id="f7ae" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan Gomez、Lukasz Kaiser和Illia Polosukhin。2017.你所需要的只是注意力。神经信息处理系统进展，6000-6010页。</li><li id="5a53" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated"><a class="ae jd" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ethayarajh%2C+K" rel="noopener ugc nofollow" target="_blank"> Kawin Ethayarajh </a>，<a class="ae jd" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Duvenaud%2C+D" rel="noopener ugc nofollow" target="_blank"> David Duvenaud </a>，<a class="ae jd" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hirst%2C+G" rel="noopener ugc nofollow" target="_blank"> Graeme Hirst </a>。理解线性单词类比，ACL 2019。</li><li id="0def" class="kt ku hi ih b ii lc im ld iq le iu lf iy lg jc ky kz la lb bi translated">贾恩和华莱士(2019) <a class="ae jd" href="https://www.arxiv-vanity.com/papers/1902.10186/" rel="noopener ugc nofollow" target="_blank">萨尔萨克·贾恩和拜伦·c·华莱士。2019.注意不是解释。<em class="lw"> CoRR </em>，abs/1902.10186。</a></li></ul></div></div>    
</body>
</html>