<html>
<head>
<title>Unsupervised Machine Translation Using Monolingual Corpora (Paper summary)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用单语语料库的无监督机器翻译(论文摘要)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/unsupervised-machine-translation-using-monolingual-corpora-paper-summary-c387de4ed6e3?source=collection_archive---------10-----------------------#2020-07-18">https://medium.com/analytics-vidhya/unsupervised-machine-translation-using-monolingual-corpora-paper-summary-c387de4ed6e3?source=collection_archive---------10-----------------------#2020-07-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><ul class=""><li id="f8cb" class="if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw bi translated">声明:这篇文章是我的学习日志。</li></ul><p id="9b88" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">原文链接:<a class="ae jm" href="https://arxiv.org/pdf/1711.00043.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1711.00043.pdf</a></p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es jn"><img src="../Images/aa900b0a13be2f92cc5e83b3f0bceb4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a_V_TVVB1Hmvwswbzmj5lQ.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">图片来源:<a class="ae jm" href="https://ehlion.com/ehlion-magazine/translation-software/2019/" rel="noopener ugc nofollow" target="_blank"> ehlion </a></figcaption></figure><p id="f54c" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">神经机器翻译(NMT)在当今世界非常重要。它允许说不同语言的人有效地相互交流。一个好的NMT模型能够高效准确地将一个句子从一种语言翻译成另一种语言。然而，NMT模型很难训练。这篇文章介绍了脸书开发的无监督机器翻译模型。</p><h1 id="36bb" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">为什么要无监督的机器翻译？</h1><ul class=""><li id="c7dc" class="if ig hi ih b ii lb ik lc im ld io le iq lf is it iu iv iw bi translated">平行语料库数据集的构建成本很高。这需要大量的人力和专业知识。</li><li id="13a0" class="if ig hi ih b ii lg ik lh im li io lj iq lk is it iu iv iw bi translated">并行语料库数据集不可用于低资源语言。</li></ul><h1 id="c1af" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">本文的目标</h1><p id="38fd" class="pw-post-body-paragraph ix iy hi ih b ii lb iz ja ik lc jb jc im ll je jf io lm jh ji iq ln jk jl is hb bi translated">仅使用每种语言的单语语料库来训练没有监督的通用机器翻译系统。</p><h1 id="2fa0" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">关键想法</strong></h1><ul class=""><li id="7cf9" class="if ig hi ih b ii lb ik lc im ld io le iq lf is it iu iv iw bi translated">在两种语言/领域(例如英语和法语)之间建立一个<strong class="ih hj">共同的潜在空间</strong>，通过在两个领域中的重构来学习翻译。</li><li id="7bb4" class="if ig hi ih b ii lg ik lh im li io lj iq lk is it iu iv iw bi translated">该模型能够处理嘈杂的翻译(从源语言到目标语言，反之亦然)。</li><li id="9561" class="if ig hi ih b ii lg ik lh im li io lj iq lk is it iu iv iw bi translated">源和目标句子潜在表示被<strong class="ih hj">约束</strong>为具有<strong class="ih hj">相同的分布</strong>使用对抗性正则化项(模型试图欺骗鉴别器，鉴别器同时被训练以识别给定潜在表示的语言。这非常类似于GAN的工作机制)。</li></ul><h1 id="7295" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">体系结构</h1><p id="38e9" class="pw-post-body-paragraph ix iy hi ih b ii lb iz ja ik lc jb jc im ll je jf io lm jh ji iq ln jk jl is hb bi translated">由编码器和解码器组成。</p><p id="7c18" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">编码器-&gt;将源语句和目标语句编码到潜在空间。(两种语言只有一个编码器)</p><p id="b48a" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">解码器-&gt;从潜在空间解码出源语句和目标语句。(两种语言只有一个解码器)。解码器与语言无关。</p><p id="6bea" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">让我们声明:</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es lo"><img src="../Images/ac1c72b7680e1b9504e3da19a29507ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2kyHFZ8zfBrDu_vbSV5T3w.png"/></div></div></figure><p id="01ec" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">编码器接收W并生成Z。解码器接收Z和语言<em class="lp"> l </em>以生成语言<em class="lp"> l. </em>中的单词</p><p id="b424" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated"><strong class="ih hj">模型设计:</strong></p><p id="16d9" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">无输入反馈的注意序列对序列模型。</p><p id="ca81" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">输入馈送是一种将注意力向量"<em class="lp">作为输入馈送到下一个时间步骤的方法，以告知模型过去的对齐决策</em> " ( <a class="ae jm" href="https://opennmt.net/OpenNMT/training/models/" rel="noopener ugc nofollow" target="_blank">来源</a>)</p><p id="28cf" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">编码器是一个双向LSTM，它返回一系列隐藏状态。作为LSTM的解码器接收先前的隐藏状态、当前单词和由编码器状态的加权和给出的上下文向量。编码器和解码器都有三层。如前所述，源语言和目标语言共享同一个编码器，解码器也是如此。注意力权重也在编码器和解码器之间共享。嵌入和LSTM隐藏状态维度被设置为300。使用贪婪解码生成句子。</p><h2 id="1405" class="lq ke hi bd kf lr ls lt kj lu lv lw kn im lx ly kr io lz ma kv iq mb mc kz md bi translated"><strong class="ak">方法概述:</strong></h2><p id="9c04" class="pw-post-body-paragraph ix iy hi ih b ii lb iz ja ik lc jb jc im ll je jf io lm jh ji iq ln jk jl is hb bi translated">培训包括几个部分:</p><ol class=""><li id="12e4" class="if ig hi ih b ii ij ik il im in io ip iq ir is me iu iv iw bi translated"><strong class="ih hj">去噪部分:</strong></li></ol><p id="ddf8" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">通过在特定领域中重构句子来训练编码器和解码器。<strong class="ih hj">输入的句子可以来自相同或不同的领域</strong>。输入的句子有噪音/损坏。随机丢弃或交换单词会破坏输入的句子。</p><p id="4170" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">编码器和解码器通过最小化目标函数来训练，该目标函数测量它们从噪声输入中重建句子的能力。基本上，它的训练方式与典型的去噪自动编码器相同。</p><p id="fa9d" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">该零件的损失函数:</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es mf"><img src="../Images/9b89786c822bfc6ffdaec8e81852e730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8sdSA47Uj2x0YZ0iffC9hw.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">截图摘自原<a class="ae jm" href="https://arxiv.org/pdf/1711.00043.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</figcaption></figure><p id="4403" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">2.<strong class="ih hj">跨领域培训部分:</strong></p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es mg"><img src="../Images/10206eed39593d3c8683ceb6413cc6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y4iTdcGaYhukO8tuwOrBJw.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">截图摘自原<a class="ae jm" href="https://arxiv.org/pdf/1711.00043.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</figcaption></figure><ul class=""><li id="a9ef" class="if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw bi translated">m是由编码器和解码器组成的完整翻译模型。从等式2中，我们可以看到，loss函数计算x和x_hat之间的令牌级交叉熵损失之和。x从原始数据集中采样。x_hat是将讹误的y馈入M (x_hat ~ d(e(C(M(x))，l2)，l1))而产生的。注意等式2中l1和l2的差异。)</li></ul><p id="79ba" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">解释等式2的简单例子:</p><p id="a4c3" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">采样一个适当的英语句子(X)，对其进行编码(l1编码)并馈入模型M以生成西班牙语。取生成的西班牙语，将其破坏然后编码(l2编码)并再次馈入M以生成X_hat。损失函数将试图减少X和X_hat之间的差异。</p><p id="2685" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">3.<strong class="ih hj">对抗性训练</strong></p><p id="463a" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">神经机器翻译系统的<strong class="ih hj">解码器</strong>只有当它的输入是用训练的编码器产生的<strong class="ih hj">时，或者至少，当输入<strong class="ih hj">来自非常接近于由它的编码器产生的分布</strong>时，才能很好地工作。</strong></p><p id="8515" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">为了确保输入is来自非常接近其编码器诱导的分布，我们需要一个鉴别器(如GAN中的一个)进行训练，并确保编码器可以产生能够欺骗鉴别器的输出嵌入。鉴别器将被训练来区分由编码器产生的编码是源还是目标的编码。最终，目标是训练编码器，使其能够成功欺骗鉴别器，使鉴别器无法区分源和目标的编码。当这种情况发生时，我们将非常确信解码器的输入来自一个非常接近于它所训练的编码器所诱导的分布。如果没有这种对抗性训练，我们就无法对输入来自的分布有信心，因为在训练的前两个部分期间，编码器的输入可以是任何语言(源语言或目标语言)，但现在我们希望确保神经机器翻译模型能够很好地用于源-&gt;目标翻译，所以我们添加了这种对抗性训练，以确保它能够相应地工作。</p><p id="e0c5" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">该零件的损失函数:</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es mh"><img src="../Images/84421b01ccb994816afeb1d351b59a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CK9L9sxv3StxXXpddKoClA.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">截图摘自原<a class="ae jm" href="https://arxiv.org/pdf/1711.00043.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</figcaption></figure><p id="8439" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated"><strong class="ih hj">最终损失函数:</strong></p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es mi"><img src="../Images/586a7f3bf7f7cbae452dbeb071479c5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VwRvzQgfobV4zKjxhRBG_Q.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">截图摘自原<a class="ae jm" href="https://arxiv.org/pdf/1711.00043.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</figcaption></figure><p id="0129" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated"><strong class="ih hj">整体型号:</strong></p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es mj"><img src="../Images/f71b3565292a0246d6b3a42224a3e662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8dTbN_xSB7ITye1_re-Mhw.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">截图摘自原<a class="ae jm" href="https://arxiv.org/pdf/1711.00043.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</figcaption></figure><h1 id="a1d0" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">结果:</h1><p id="3423" class="pw-post-body-paragraph ix iy hi ih b ii lb iz ja ik lc jb jc im ll je jf io lm jh ji iq ln jk jl is hb bi translated">论文发表的结果很有希望。只需3次迭代，该模型就能生成非常好的翻译。</p><figure class="jo jp jq jr fd js er es paragraph-image"><div role="button" tabindex="0" class="jt ju di jv bf jw"><div class="er es mk"><img src="../Images/69ab94c562d0c284f6ddc43d16863eae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MOSOQKIt1ij45oqaDuQvaw.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">截图摘自原<a class="ae jm" href="https://arxiv.org/pdf/1711.00043.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>。</figcaption></figure><h1 id="a7e3" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">个人观点:</h1><p id="8813" class="pw-post-body-paragraph ix iy hi ih b ii lb iz ja ik lc jb jc im ll je jf io lm jh ji iq ln jk jl is hb bi translated">这篇论文非常容易阅读，即使对像我这样的NMT新手来说也是如此。它不需要任何NMT的背景知识。我强烈建议任何对这个主题感兴趣的人阅读论文<a class="ae jm" href="https://arxiv.org/pdf/1711.00043.pdf" rel="noopener ugc nofollow" target="_blank">以获得更详细的解释和实现。这种无监督的NMT训练非常重要，我相信在未来会更加重要，因为标记数据总是很难找到，而未标记数据更容易获得。</a></p><p id="6d04" class="pw-post-body-paragraph ix iy hi ih b ii ij iz ja ik il jb jc im jd je jf io jg jh ji iq jj jk jl is hb bi translated">如果你喜欢我的论文总结，请跟我来，给我鼓掌。谢谢你。这是我第一次写论文总结，不吝赐教，如有错误信息请告知。谢谢你。</p></div></div>    
</body>
</html>