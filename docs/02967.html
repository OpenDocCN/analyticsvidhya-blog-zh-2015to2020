<html>
<head>
<title>Matrix Factorization made easy (Recommender Systems)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简化矩阵分解(推荐系统)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/matrix-factorization-made-easy-recommender-systems-7e4f50504477?source=collection_archive---------2-----------------------#2020-01-10">https://medium.com/analytics-vidhya/matrix-factorization-made-easy-recommender-systems-7e4f50504477?source=collection_archive---------2-----------------------#2020-01-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/24dcd28da71f6e415bc15a2e9d5c3b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lu2FNONJWeXcP6lRn-u-5Q.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">克里斯多夫·伯恩斯在<a class="ae iu" href="https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><blockquote class="jc"><p id="011a" class="jd je hi bd jf jg jh ji jj jk jl jm dx translated">推荐系统被用于各种领域，最常见的是作为视频和音乐服务的播放列表生成器，如<a class="ae iu" href="https://www.netflix.com/in/" rel="noopener ugc nofollow" target="_blank">网飞</a>、<a class="ae iu" href="https://www.youtube.com/" rel="noopener ugc nofollow" target="_blank"> YouTube </a>和<a class="ae iu" href="https://www.spotify.com/in/free/?utm_source=in-en_brand_contextual_text&amp;utm_medium=paidsearch&amp;utm_campaign=alwayson_asia_in_premiumbusiness_core_brand+contextual-desktop+text+exact+in-en+google&amp;ds_rl=1270915&amp;gclid=CjwKCAiA3uDwBRBFEiwA1VsajJxGFa5Lvr-pakcYcy-Wv30tqAwvOqZeqKdLsYC7gzymdlFvYUJo5xoCw9kQAvD_BwE&amp;gclsrc=aw.ds" rel="noopener ugc nofollow" target="_blank"> Spotify </a>，服务的产品推荐器，如<a class="ae iu" href="https://www.amazon.in/" rel="noopener ugc nofollow" target="_blank">亚马逊</a>，或社交媒体平台的内容推荐器，如<a class="ae iu" href="https://en.wikipedia.org/wiki/Facebook" rel="noopener ugc nofollow" target="_blank">脸书</a>和<a class="ae iu" href="https://en.wikipedia.org/wiki/Twitter" rel="noopener ugc nofollow" target="_blank">推特</a>。</p></blockquote></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><h2 id="66c3" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">推荐系统类型-</h2><blockquote class="kl km kn"><p id="32f8" class="ko kp kq kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll jm hb bi translated"><strong class="kr hj">协同过滤</strong> →协同过滤方法根据用户过去的行为(之前购买或选择的项目和/或对这些项目的数字评级)以及其他用户做出的类似决定建立模型。该模型然后被用于预测用户可能感兴趣的项目(或项目的评级)。<a class="ae iu" href="https://towardsdatascience.com/various-implementations-of-collaborative-filtering-100385c6dfe0" rel="noopener" target="_blank">深入</a></p><p id="736b" class="ko kp kq kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll jm hb bi translated"><strong class="kr hj">基于内容的过滤→ </strong>基于内容的过滤方法利用一个项目的一系列离散的、预先标记的特征来推荐具有相似属性的附加项目。<a class="ae iu" href="https://towardsdatascience.com/how-we-built-a-content-based-filtering-recommender-system-for-music-with-python-c6c3b1020332" rel="noopener" target="_blank">深入</a></p></blockquote><h2 id="ef85" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">现在，让我们从矩阵分解开始</h2><p id="cf6f" class="pw-post-body-paragraph ko kp hi kr b ks lm ku kv kw ln ky kz jy lo lc ld kc lp lg lh kg lq lk ll jm hb bi translated">矩阵分解的简单直觉可以表述为将一个矩阵分解成两个或三个矩阵的乘积。</p><p id="6d32" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">这也被称为乘法分解，也称为矩阵分解</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/1b4f9ceac6525f90483863f18220033a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o7A3tcHNxy2e011WMHxA7Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">矩阵分解/分解为三个矩阵(SVD)(图1)</figcaption></figure><p id="a687" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">上图是一种简单且使用最广泛的矩阵分解，称为<strong class="kr hj"> SVD →奇异值分解</strong></p><p id="7779" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">SVD的直觉→设存在维数为(m，n)的矩阵<strong class="kr hj"> X </strong>这个矩阵可以看作两个或三个矩阵之间的点积，每个矩阵的维数分别为(m，r)和(r，n)。</p><p id="e133" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">SVD在推荐系统中被广泛使用，稍后我会解释为什么以及如何使用。</p><p id="e210" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">首先，让我们理解从X矩阵(矩形)中得到的三个矩阵。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/81c10ef22ff2758b6375d6af85e610a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*O-JrSN1-mJg3cb9IfF_iKw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">奇异值分解(图2)</figcaption></figure><p id="5ac0" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated"><strong class="kr hj"> <em class="kq"> U </em> </strong> →左奇异矩阵，<strong class="kr hj"><em class="kq">V _转置</em>→右奇异矩阵，<strong class="kr hj"><em class="kq">S/D</em></strong><em class="kq">→S</em>奇异值矩阵<em class="kq">。</em>注意每个矩阵的形状，我们可以看到<strong class="kr hj"> <em class="kq"> S </em> </strong>是对角矩阵而<strong class="kr hj"> <em class="kq"> U，V_Transpose </em> </strong>是矩形矩阵，它们有各自的形状。</strong></p><blockquote class="kl km kn"><p id="9506" class="ko kp kq kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll jm hb bi translated"><strong class="kr hj"> U </strong>矩阵中的行称为左奇异向量，<strong class="kr hj"> V </strong>矩阵中的列称为“<strong class="kr hj"> X </strong>的右奇异向量</p></blockquote><p id="e321" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">你可以把这三个矩阵想象成<strong class="kr hj"> <em class="kq"> X </em> </strong>矩阵的<strong class="kr hj">因子</strong>。所以，如果你把这三个矩阵相乘，你会得到预期的<strong class="kr hj"> <em class="kq"> X </em> </strong>。</p><blockquote class="kl km kn"><p id="5538" class="ko kp kq kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll jm hb bi translated">SVD的一个缺点是，当原始矩阵稀疏(不完整)时，左右奇异向量是未定义的。</p></blockquote><h2 id="e32f" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">矩阵分解作为推荐系统中的特征工程</h2><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/58c059c26aead4756921aa5dc089e814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EKYGZohO9XTJMo68OPyImA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">分解成用户和项目矩阵的用户项目数据集(图3)</figcaption></figure><p id="c228" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">假设我们有一个数据集，其中包含不同用户给出的项目评级。这是一个典型的推荐系统问题，你的工作是根据i^th用户之前的评价向i^th用户推荐新的商品。</p><p id="7e1b" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">让我们取n →用户数，m →项目数，那么我们的评级矩阵将是(nxm)的数量级。</p><p id="994b" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">在应用矩阵分解之后，我们得到两个矩阵，形状的用户矩阵(nxd)和形状的项目矩阵(dxm)。你可以把形状和图1比较一下，看看左右奇异矩阵的形状。</p><blockquote class="kl km kn"><p id="dac0" class="ko kp kq kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll jm hb bi translated">用户矩阵中的i^th行是i^th用户向量，项目矩阵中的i^th列是i^th项目向量。所有向量都是维数为“<strong class="kr hj"> d </strong>的实数。</p></blockquote><p id="4c00" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">你为你的用户和项目得到的d-dim表示是通过矩阵分解得到的。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/f4833678b2f39431812f980f697e55f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*_5HAVjqRS4JMv3uUMpdPsw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">用户-用户和项目-项目相似性(图4)</figcaption></figure><blockquote class="kl km kn"><p id="39e3" class="ko kp kq kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll jm hb bi translated">因此，我们不要忘记，现在您可以使用这些向量进行协作过滤，找到基于<strong class="kr hj">用户-用户</strong>和<strong class="kr hj">项目-项目</strong>的相似性。</p></blockquote></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><blockquote class="jc"><p id="b6e5" class="jd je hi bd jf jg jh ji jj jk jl jm dx translated">矩阵分解&amp;数学上的目标函数。</p></blockquote><figure class="ma mb mc md me ij er es paragraph-image"><div class="er es lz"><img src="../Images/3702d39adec4733474b164f106dd05e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/1*Abyoh7Qy-z0F6Ug8xKBRww.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">(图— 5)</figcaption></figure><p id="8d97" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">这里，<strong class="kr hj"> p </strong>是用户矩阵，<strong class="kr hj"> q </strong>是项目矩阵。</p><p id="a0f2" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">现在，我们来看一下我们的<strong class="kr hj">目标函数</strong>，我们希望根据<strong class="kr hj"> q (dxn) </strong>和<strong class="kr hj"> p(nxd) </strong>来最小化该目标函数，与上面我针对项目和用户矩阵解释的形状相同。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/b536f3fdadf278f3c7d096bc5d1a586a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/1*6PVLUV8pD_KFpEE0PdeqGA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">目标函数(图6)</figcaption></figure><p id="9de1" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">这里，<strong class="kr hj"> r_ui </strong> →用户u对物品I的评分，<strong class="kr hj"> q_i </strong> →物品向量，<strong class="kr hj"> p_u → </strong>用户向量。</p><blockquote class="kl km kn"><p id="84f5" class="ko kp kq kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll jm hb bi translated">如果我们仔细观察，方程的前半部分是损失的平方，后半部分是L2正则化。</p></blockquote><h1 id="ff3b" class="mg jo hi bd jp mh mi mj jt mk ml mm jx mn mo mp kb mq mr ms kf mt mu mv kj mw bi translated">矩阵分解的代码解释</h1><p id="0446" class="pw-post-body-paragraph ko kp hi kr b ks lm ku kv kw ln ky kz jy lo lc ld kc lp lg lh kg lq lk ll jm hb bi translated">首先让我们看看我们的数据</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es mx"><img src="../Images/f781e45c175542af8d9b58dd8ec8668b.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*j4TqZ9ZbLSWbxCf3sCcyYw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">用户-项目数据(图7)</figcaption></figure><p id="e544" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">我们将预测(用户标识，电影标识)对的收视率。</p><p id="1259" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">这里，我们的数据集中的列是user_id、item_id和rating。让我们把电影作为我们的项目。</p><p id="983d" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">我们将把这个数据集转换成我们的<strong class="kr hj">X/等级/邻接</strong>矩阵。</p><blockquote class="kl km kn"><p id="c179" class="ko kp kq kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll jm hb bi translated"><strong class="kr hj"> X[i][j] = rij </strong>，这里<strong class="kr hj"> i </strong>是用户id，<strong class="kr hj"> j </strong>是电影id，<strong class="kr hj"> rij </strong>是用户给出的评分</p><p id="1548" class="ko kp kq kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll jm hb bi translated">去看电影。我们将在这个矩阵上应用SVD分解。</p></blockquote><p id="9098" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">我们需要我们的用户、项目和评分数据来创建稀疏矩阵<strong class="kr hj"> X </strong>。</p><pre class="ls lt lu lv fd my mz na nb aw nc bi"><span id="74ee" class="jn jo hi mz b fi nd ne l nf ng">#getting user id and appending to list<br/>#getting item value and appending to list<br/>#getting rating value and appending to list</span><span id="4887" class="jn jo hi mz b fi nh ne l nf ng">user_data=[]<br/>item_data=[]<br/>ratings=[]<br/>for i in range(data.shape[0]):<br/>    user=(data['user_id'].iloc[i])                <br/>    item=(data['item_id'].iloc[i])               <br/>    rat=(data['rating'].iloc[i])                  <br/>    user_data.append(user)<br/>    item_data.append(item)<br/>    ratings.append(rat)</span></pre><blockquote class="kl km kn"><p id="6132" class="ko kp kq kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll jm hb bi translated">它很稀疏，因为数据集中有许多电影，但i^th用户不会对所有电影进行评级。因此，它是一个稀疏矩阵。</p></blockquote><p id="e0dd" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">现在，我们将创建一个稀疏矩阵<strong class="kr hj"> X </strong>(邻接矩阵)</p><pre class="ls lt lu lv fd my mz na nb aw nc bi"><span id="2ab7" class="jn jo hi mz b fi nd ne l nf ng">from scipy.sparse import csr_matrix</span><span id="f05a" class="jn jo hi mz b fi nh ne l nf ng">adj_matrix = csr_matrix((ratings, (user_data, item_data)))<br/>adj_matrix.shape</span></pre><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es ni"><img src="../Images/88804221ca500616a1dc85e4937dfee2.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*DKLC00aiI3dmrE_F69q29w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">矩阵<strong class="bd jp"> X/adj_matrix </strong>(图— 8)</figcaption></figure><blockquote class="kl km kn"><p id="d092" class="ko kp kq kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll jm hb bi translated">这就是adj_matrix的样子。</p></blockquote></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="2cec" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">现在，因为我们有了矩阵<strong class="kr hj"> X </strong>，我们现在需要做的就是执行SVD矩阵分解，如下所示。</p><pre class="ls lt lu lv fd my mz na nb aw nc bi"><span id="9903" class="jn jo hi mz b fi nd ne l nf ng">from sklearn.utils.extmath import randomized_svd<br/>import numpy as np <br/><br/>U, S, VT = randomized_svd(adj_matrix, n_components=5,n_iter=5, random_state=None)<br/>print(U.shape)<br/>print(Sigma.shape)<br/>print(VT.T.shape)</span></pre><p id="eb16" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">U →左奇异矩阵(用户)，VT →右奇异矩阵(项)，S →奇异值矩阵</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/8f3e1e5822469dc4eaeeba68162dfb2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:470/format:webp/1*pHtdRFM6UrR0opMDFKSv_w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">上述代码的输出(图— 9)</figcaption></figure><p id="63a9" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">这是形状的样子。</p></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="4e43" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">现在，我们已经分别得到了用户矩阵和项目矩阵。类似于图3</p><blockquote class="kl km kn"><p id="2f49" class="ko kp kq kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll jm hb bi translated">所以矩阵<strong class="kr hj"> U </strong>可以表示为用户的矩阵表示，其中每一行<strong class="kr hj"> u_i </strong>表示一个用户的d维向量，矩阵<strong class="kr hj"> VT </strong>可以表示为电影的矩阵表示，其中每一行<strong class="kr hj"> v_j </strong>表示一个电影的d维向量。</p></blockquote><h2 id="c0a5" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">学习方法</h2><p id="3ff0" class="pw-post-body-paragraph ko kp hi kr b ks lm ku kv kw ln ky kz jy lo lc ld kc lp lg lh kg lq lk ll jm hb bi translated">求矩阵q和p的一个明显的方法是梯度下降法。因为我们已经定义了损失函数，所以对q和p取偏导数来优化这些值。给定如下，</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es nk"><img src="../Images/7b1f75a6a32ee4a134d62de06e9d80e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*J2D_INjtbjzQARjVfrZ5ww.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">更新公式(图— 10)</figcaption></figure><p id="0770" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated"><strong class="kr hj">伪代码</strong></p><pre class="ls lt lu lv fd my mz na nb aw nc bi"><span id="54f9" class="jn jo hi mz b fi nd ne l nf ng">for each epoch:<br/>    for each pair of (user, movie):<br/>        b_i =  b_i - learning_rate * dL/db_i<br/>        m_j =  m_j - learning_rate * dL/dm_j<br/>    predict the ratings with formula y_pred_ij = mu + b_i + m_j + dot_product(u_i , v_j)</span></pre><p id="5083" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated"><strong class="kr hj"> mu → </strong>是所有评分的平均值，<strong class="kr hj"> u_i → </strong>用户向量，<strong class="kr hj"> v_j → </strong>项目向量，<strong class="kr hj"> b_i </strong>和<strong class="kr hj"> m_j </strong>将通过更新公式和偏导数得到。</p><p id="b653" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">通过应用偏导数，更新规则将看起来像下面这样，它被应用在伪代码中</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es nl"><img src="../Images/a23c7cf9e5ccccbd0bbd92d78e864a76.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*f8hdyvaA_UNHAPGfQb0-Fw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">用户和项目的偏导数(图11)</figcaption></figure><p id="74f9" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated"><strong class="kr hj"> gamma → </strong>梯度下降的学习率</p><h2 id="8940" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">添加偏差后</h2><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es nm"><img src="../Images/be5d4c981afb1d5c0d7203ab4c494a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*AFv33o3O5Bkx2sE7_-aOTg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">更新有偏差的公式(图— 12)</figcaption></figure><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es nn"><img src="../Images/d6b74b1f2ec6aa6c3d36505466276b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*JLQij_D-YPXC7FkgLGho6A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">带有偏差和正则化的目标函数(图13)</figcaption></figure></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="425b" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated"><strong class="kr hj"> <em class="kq">矩阵分解是一种前沿技术，隐藏在其他方法中，如PCA(维数约减)、聚类等。</em>T15】</strong></p><p id="ff43" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated"><strong class="kr hj"> <em class="kq">它可以用在许多解决重要问题的技术中，这一事实本身就令人震惊。</em>T19】</strong></p></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><blockquote class="kl km kn"><p id="3679" class="ko kp kq kr b ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll jm hb bi translated">最后，如果没有写这篇<a class="ae iu" href="https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf" rel="noopener ugc nofollow" target="_blank">研究论文</a>的聪明人，矩阵分解的使用在推荐系统中是不可能的。</p></blockquote><h2 id="7d02" class="jn jo hi bd jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk bi translated">参考</h2><ol class=""><li id="0413" class="no np hi kr b ks lm kw ln jy nq kc nr kg ns jm nt nu nv nw bi translated"><a class="ae iu" href="https://towardsdatascience.com/paper-summary-matrix-factorization-techniques-for-recommender-systems-82d1a7ace74" rel="noopener" target="_blank">https://towards data science . com/paper-summary-matrix-factorization-techniques-for-recommender-systems-82d 1a 7 ace 74</a></li><li id="42e2" class="no np hi kr b ks nx kw ny jy nz kc oa kg ob jm nt nu nv nw bi translated"><a class="ae iu" href="https://www.google.com/search?q=collaborative+filtering+example&amp;rlz=1C1CHBF_enIN850IN850&amp;oq=collab&amp;aqs=chrome.0.69i59j69i57j69i59j69i60l3.9854j0j7&amp;sourceid=chrome&amp;ie=UTF-8" rel="noopener ugc nofollow" target="_blank">https://www.google.com/search?q =协同+过滤+示例&amp;rlz = 1 C1 chbf _ enin 850 in 850&amp;OQ = collab&amp;aqs = chrome . 0.69 I 59j 69 I 57j 69 I 59j 69 I 60 l 3.9854j 0j 7&amp;sourceid = chrome&amp;ie = UTF-8</a></li><li id="02fc" class="no np hi kr b ks nx kw ny jy nz kc oa kg ob jm nt nu nv nw bi translated">应用人工智能课程</li></ol><blockquote class="jc"><p id="acb2" class="jd je hi bd jf jg oc od oe of og jm dx translated">如有任何疑问，欢迎您在推特上与我联系。</p></blockquote></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><figure class="ls lt lu lv fd ij er es paragraph-image"><a href="https://usejournal.com/?utm_source=medium.com&amp;utm_medium=noteworthy_blog&amp;utm_campaign=tech&amp;utm_content=guest_post_image"><div class="er es oh"><img src="../Images/f955592b7910434e19d20f04ba21b5ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iwthGEeUMHMu9QK60Q2mTA.png"/></div></a></figure><p id="bf2e" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">📝稍后在<a class="ae iu" href="https://usejournal.com/?utm_source=medium.com&amp;utm_medium=noteworthy_blog&amp;utm_campaign=tech&amp;utm_content=guest_post_read_later_text" rel="noopener ugc nofollow" target="_blank">杂志</a>上阅读这个故事。</p><p id="0847" class="pw-post-body-paragraph ko kp hi kr b ks kt ku kv kw kx ky kz jy lb lc ld kc lf lg lh kg lj lk ll jm hb bi translated">👩‍💻每周日早上醒来，你的收件箱里会有本周最值得关注的科技新闻。<a class="ae iu" href="https://usejournal.com/newsletter/noteworthy-in-tech/?utm_source=medium.com&amp;utm_medium=noteworthy_blog&amp;utm_campaign=tech&amp;utm_content=guest_post_text" rel="noopener ugc nofollow" target="_blank">阅读科技简讯</a>中值得注意的内容。</p></div></div>    
</body>
</html>