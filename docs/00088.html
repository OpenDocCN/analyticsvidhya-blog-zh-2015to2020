<html>
<head>
<title>Python Implementation of Andrew Ng’s Machine Learning Course (Part 2.2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">吴恩达机器学习课程的Python实现(第2.2部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-2-2-dceff1a12a12?source=collection_archive---------0-----------------------#2018-09-08">https://medium.com/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-2-2-dceff1a12a12?source=collection_archive---------0-----------------------#2018-09-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/94ebbf40d69a4802596d583eed4026ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FGGge_GilZ_KJYaoryaxkA.png"/></div></div></figure><p id="6f96" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">到目前为止，这是一个非常受欢迎的系列，我非常感谢你们所有人阅读它。查看下面以前的文章(以防你还没有涉及到它们):</p><ul class=""><li id="8e09" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-1-6b8dd1c73d80">一元和多元线性回归</a>(上)</li><li id="6f23" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-2-1-1a666f049ad6">逻辑回归或分类</a>(第2.1部分)</li></ul><p id="e7f2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">继续我们的吴恩达课程的Pythonic版本的旅程，在这篇博文中，我们将学习正则化逻辑回归。</p><blockquote class="kd ke kf"><p id="0bc9" class="iq ir kg is b it iu iv iw ix iy iz ja kh jc jd je ki jg jh ji kj jk jl jm jn hb bi translated"><strong class="is hj">先决条件</strong></p><p id="cc7d" class="iq ir kg is b it iu iv iw ix iy iz ja kh jc jd je ki jg jh ji kj jk jl jm jn hb bi translated">强烈建议您首先观看<a class="ae jx" href="https://www.coursera.org/learn/machine-learning/home/week/3" rel="noopener ugc nofollow" target="_blank">第3周</a>视频讲座，并完成视频测验。</p><p id="a6bf" class="iq ir kg is b it iu iv iw ix iy iz ja kh jc jd je ki jg jh ji kj jk jl jm jn hb bi translated">应该对Python生态系统有基本的了解。</p></blockquote><p id="7e34" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">正则化逻辑回归</strong></p><blockquote class="kd ke kf"><p id="7d57" class="iq ir kg is b it iu iv iw ix iy iz ja kh jc jd je ki jg jh ji kj jk jl jm jn hb bi translated">问题背景</p><p id="652a" class="iq ir kg is b it iu iv iw ix iy iz ja kh jc jd je ki jg jh ji kj jk jl jm jn hb bi translated">您将实现正则化逻辑回归来预测来自制造厂的微芯片是否通过质量保证(QA)。在质量保证过程中，每个微芯片都要经过各种测试，以确保其功能正常。</p><p id="5a8f" class="iq ir kg is b it iu iv iw ix iy iz ja kh jc jd je ki jg jh ji kj jk jl jm jn hb bi translated">假设你是工厂的产品经理，你有一些微芯片在两次不同测试中的测试结果。从这两个测试中，你想要决定微芯片是应该被接受还是被拒绝。为了帮助你做决定，你有一个过去微芯片测试结果的数据集，从中你可以建立一个逻辑回归模型。</p></blockquote><p id="df6a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先让我们加载必要的库。</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="f8c7" class="kt ku hi kp b fi kv kw l kx ky">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import scipy.optimize as opt    # more on this later</span></pre><p id="7a46" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来，我们读取数据(必要的数据可在<a class="ae jx" href="https://www.coursera.org/learn/machine-learning/home/week/3" rel="noopener ugc nofollow" target="_blank"> week-3 </a> content下获得)</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="6764" class="kt ku hi kp b fi kv kw l kx ky">data = pd.read_csv('ex2data2.txt', header = None)<br/>X = data.iloc[:,:-1]<br/>y = data.iloc[:,2]<br/>data.head()</span></pre><figure class="kk kl km kn fd ij er es paragraph-image"><div class="er es kz"><img src="../Images/c150d74461e235d27e09f1b535f6b2d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*odNzovKkDMEzoalmErapSg.png"/></div></figure><p id="5215" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以我们有两个独立特征和一个因变量。这里<code class="du la lb lc kp b">0</code>表示芯片被拒绝，<code class="du la lb lc kp b">1</code>表示被接受。</p><p id="a41b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">可视化数据</strong></p><p id="3921" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在开始实现任何学习算法之前，如果可能的话，可视化数据总是好的。</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="89c1" class="kt ku hi kp b fi kv kw l kx ky">mask = y == 1<br/>passed = plt.scatter(X[mask][0].values, X[mask][1].values)<br/>failed = plt.scatter(X[~mask][0].values, X[~mask][1].values)<br/>plt.xlabel('Microchip Test1')<br/>plt.ylabel('Microchip Test2')<br/>plt.legend((passed, failed), ('Passed', 'Failed'))<br/>plt.show()</span></pre><figure class="kk kl km kn fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/e0b98d32727c0c0bec78553735aad83e.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*iEINkAxhdyJrCTjMyjwKrw.png"/></div></figure><p id="7f51" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上图显示，我们的数据集不能被穿过图的直线分成正例与反例。因此，逻辑回归的直接应用将不会在该数据集上很好地执行，因为逻辑回归将只能找到线性决策边界。</p><p id="870c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">特征映射</strong></p><p id="e7d7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">更好地拟合数据的一种方法是从每个数据点创建更多的要素。因此，我们将把这些特征映射到所有的多项式项<code class="du la lb lc kp b">x1</code>和<code class="du la lb lc kp b"> x2</code>的六次方。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div class="er es le"><img src="../Images/4d40d983d26803b52b899d95b7e195b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*-n0H6dB-gVYUXg3nGvTAgw.png"/></div></figure><p id="c07e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">作为这种映射的结果，我们的两个特征的向量(两个QA测试的分数)已经被转换成28维向量。在这个更高维特征向量上训练的逻辑回归分类器将具有更复杂的决策边界，并且当在我们的二维图中绘制时将呈现非线性。</p><p id="e5b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">虽然特征映射允许我们构建更具表现力的分类器，但它也更容易过度拟合。在练习的下一部分，您将实现正则化逻辑回归来拟合数据，并亲自了解正则化如何帮助解决过度拟合问题。</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="a2b1" class="kt ku hi kp b fi kv kw l kx ky">def mapFeature(X1, X2):<br/>    degree = 6<br/>    out = np.ones(X.shape[0])[:,np.newaxis]<br/>    for i in range(1, degree+1):<br/>        for j in range(i+1):<br/>            out = np.hstack((out, np.multiply(np.power(X1, i-j),                                     np.power(X2, j))[:,np.newaxis]))<br/>    return out</span><span id="7a6e" class="kt ku hi kp b fi lf kw l kx ky">X = mapFeature(X.iloc[:,0], X.iloc[:,1])</span></pre><p id="c4f9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">实施</strong></p><p id="4230" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在开始实际成本函数之前，回想一下逻辑回归假设使用了sigmoid函数。让我们定义我们的sigmoid函数。</p><p id="d4ad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">乙状结肠功能</strong></p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="db11" class="kt ku hi kp b fi kv kw l kx ky">def sigmoid(x):<br/>  return 1/(1+np.exp(-x))</span></pre><p id="b706" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">成本函数</strong></p><p id="49b3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">像往常一样，让我们编码成本函数和梯度函数。</p><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/d63384484ab4ecaf10c45be4ae131ae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CQpFY41vGbjxIM0-rYqeSw.png"/></div></div></figure><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="4ec8" class="kt ku hi kp b fi kv kw l kx ky">def lrCostFunction(theta_t, X_t, y_t, lambda_t):<br/>    m = len(y_t)<br/>    J = (-1/m) * (y_t.T @ np.log(sigmoid(X_t @ theta_t)) + (1 - y_t.T) @ np.log(1 - sigmoid(X_t @ theta_t)))<br/>    reg = (lambda_t/(2*m)) * (theta_t[1:].T @ theta_t[1:])<br/>    J = J + reg<br/>    return J</span></pre><blockquote class="kd ke kf"><p id="54f1" class="iq ir kg is b it iu iv iw ix iy iz ja kh jc jd je ki jg jh ji kj jk jl jm jn hb bi translated">编码成本函数有多种方法。更重要的是潜在的数学思想和我们将它们转化为代码的能力。</p></blockquote><p id="525f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">渐变功能</strong></p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="66f5" class="kt ku hi kp b fi kv kw l kx ky">def lrGradientDescent(theta, X, y, lambda_t):<br/>    m = len(y)<br/>    grad = np.zeros([m,1])<br/>    grad = (1/m) * X.T @ (sigmoid(X @ theta) - y)<br/>    grad[1:] = grad[1:] + (lambda_t / m) * theta[1:]<br/>    return grad</span></pre><p id="c324" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们用初始参数调用这些函数。</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="0b1a" class="kt ku hi kp b fi kv kw l kx ky">(m, n) = X.shape<br/>y = y[:, np.newaxis]<br/>theta = np.zeros((n,1))<br/>lmbda = 1</span><span id="7b45" class="kt ku hi kp b fi lf kw l kx ky">J = lrCostFunction(theta, X, y, lmbda)<br/>print(J)</span></pre><p id="b7f6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这给了我们一个值<code class="du la lb lc kp b">0.69314718</code>。</p><p id="b632" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">使用fmin_tnc学习参数</strong></p><p id="cccb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">与之前的帖子类似，我们将使用<code class="du la lb lc kp b">fmin_tnc</code></p><p id="4b0f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><code class="du la lb lc kp b">fmin_tnc</code>是一个优化求解器，可以找到一个无约束函数的最小值。对于逻辑回归，您希望使用参数<code class="du la lb lc kp b">theta</code>优化成本函数。</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="9bfc" class="kt ku hi kp b fi kv kw l kx ky">output = opt.fmin_tnc(func = lrCostFunction, x0 = theta.flatten(), fprime = lrGradientDescent, \<br/>                         args = (X, y.flatten(), lmbda))<br/>theta = output[0]<br/>print(theta) # theta contains the optimized values</span></pre><blockquote class="kd ke kf"><p id="f100" class="iq ir kg is b it iu iv iw ix iy iz ja kh jc jd je ki jg jh ji kj jk jl jm jn hb bi translated">关于<code class="du la lb lc kp b"><a class="ae jx" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html" rel="noopener ugc nofollow" target="_blank">flatten()</a></code>函数的说明:不幸的是<code class="du la lb lc kp b">scipy’s fmin_tnc </code>不能很好地处理列或行向量。它要求参数采用数组格式。<code class="du la lb lc kp b">flatten()</code>函数将一个列或行向量简化为数组格式。</p></blockquote><p id="0699" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">模型精度</strong></p><p id="f0b0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们通过从我们学习的参数预测结果，然后与原始结果进行比较，来尝试找到模型的准确性。</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="21db" class="kt ku hi kp b fi kv kw l kx ky">pred = [sigmoid(np.dot(X, theta)) &gt;= 0.5]<br/>np.mean(pred == y.flatten()) * 100</span></pre><p id="3555" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这使得我们的模型精度为<code class="du la lb lc kp b">83.05%</code>。</p><p id="241c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">标绘决定边界(可选)</strong></p><p id="ed8f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了帮助您可视化该分类器所学习的模型，我们将绘制(非线性)决策边界来分隔正例和负例。我们通过在均匀间隔的网格上计算分类器的预测来绘制非线性决策边界，然后绘制预测从y = 0变化到y = 1的等高线图。</p><pre class="kk kl km kn fd ko kp kq kr aw ks bi"><span id="ef19" class="kt ku hi kp b fi kv kw l kx ky">u = np.linspace(-1, 1.5, 50)<br/>v = np.linspace(-1, 1.5, 50)<br/>z = np.zeros((len(u), len(v)))</span><span id="54d7" class="kt ku hi kp b fi lf kw l kx ky">def mapFeatureForPlotting(X1, X2):<br/>    degree = 6<br/>    out = np.ones(1)<br/>    for i in range(1, degree+1):<br/>        for j in range(i+1):<br/>            out = np.hstack((out, np.multiply(np.power(X1, i-j), np.power(X2, j))))<br/>    return out</span><span id="f286" class="kt ku hi kp b fi lf kw l kx ky">for i in range(len(u)):<br/>    for j in range(len(v)):<br/>        z[i,j] = np.dot(mapFeatureForPlotting(u[i], v[j]), theta)</span><span id="1aad" class="kt ku hi kp b fi lf kw l kx ky">mask = y.flatten() == 1<br/>X = data.iloc[:,:-1]<br/>passed = plt.scatter(X[mask][0], X[mask][1])<br/>failed = plt.scatter(X[~mask][0], X[~mask][1])<br/>plt.contour(u,v,z,0)<br/>plt.xlabel('Microchip Test1')<br/>plt.ylabel('Microchip Test2')<br/>plt.legend((passed, failed), ('Passed', 'Failed'))<br/>plt.show()</span></pre><figure class="kk kl km kn fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/05441764d927141148a6a21a8d270aba.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*ig_NaOU6jiRlXEfUzIRxRQ.png"/></div></figure><p id="3f8d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们的模型在对各种数据点进行分类方面做得非常好。</p><p id="17c2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">还可以尝试改变<code class="du la lb lc kp b">lambda</code>的值，亲自看看决策边界是如何变化的。</p><p id="e087" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">谢谢你坚持到现在。如果你喜欢我的作品，给我一个(或几个)掌声。</p><p id="568d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本系列的下一篇文章将会非常有趣，因为我们将<a class="ae jx" rel="noopener" href="/analytics-vidhya/a-guide-to-using-logistic-regression-for-digit-recognition-with-python-codes-86aae6da10fe">构建一个识别手写数字的模型</a>。</p></div></div>    
</body>
</html>