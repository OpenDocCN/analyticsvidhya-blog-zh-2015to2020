<html>
<head>
<title>Optimized ways to Read Large CSVs in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 Python 中读取大型 CSV 的优化方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/optimized-ways-to-read-large-csvs-in-python-ab2b36a7914e?source=collection_archive---------0-----------------------#2020-07-29">https://medium.com/analytics-vidhya/optimized-ways-to-read-large-csvs-in-python-ab2b36a7914e?source=collection_archive---------0-----------------------#2020-07-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7e68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你好。🙋</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/e1b4fcd74f153adf84446250e81cdf78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S8zanoXmvTjCJZy1sW_YOA.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><a class="ae jt" href="https://leaderonomics.com/business/love-innovation-or-die" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="866d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi ju translated"><span class="l jv jw jx bm jy jz ka kb kc di">在</span>当前的时代，数据在分析和建立 ML/AI 模型中起着非常重要的作用。数据可以在 CSV、平面文件、JSON 等各种格式中找到，当数据很大时，很难读入内存。这个博客围绕着处理 CSV 格式的表格数据，这些数据是逗号分隔的文件。</p><p id="003a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">问题:</strong>导入(读取)大型 CSV 文件导致内存不足错误。没有足够的内存来一次读取整个 CSV 文件，导致计算机崩溃。</p><p id="96ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里有一些在 Python 中导入 CSV 的有效方法。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kd"><img src="../Images/b282c8dbaeb8843eff23baebe7d9854f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sMx-Rq5DC7akWxo44CoeYA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图 1</figcaption></figure><p id="f34a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在怎么办？好吧，让我们准备一个应该很大的数据集，然后比较实现图 1 所示选项的性能(时间)。让我们开始吧..🏃</p><p id="daf4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用随机数和字符串创建一个 15 列 1000 万行的数据框架。将其导出为大约 1 GB 大小的 CSV 格式。</p><pre class="je jf jg jh fd ke kf kg kh aw ki bi"><span id="f683" class="kj kk hi kf b fi kl km l kn ko">df = pd.DataFrame(data=np.random.randint(99999, 99999999, size=(10000000,14)),columns=['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12','C13','C14'])</span><span id="acf9" class="kj kk hi kf b fi kp km l kn ko">df['C15'] = pd.util.testing.rands_array(5,10000000)<br/>df.to_csv("huge_data.csv")</span></pre><p id="3622" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们看一下导入选项，比较一下将 CSV 读入内存所用的时间。</p><h1 id="1f0f" class="kq kk hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">熊猫</h1><p id="9a5e" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated">pandas python 库提供 read_csv()函数，将 csv 作为数据帧结构导入，以便于计算或分析。这个函数提供了一个参数(将在后面的部分中描述)来更快地导入您的巨大文件。</p><h2 id="fd66" class="kj kk hi bd kr ls lt lu kv lv lw lx kz iq ly lz ld iu ma mb lh iy mc md ll me bi translated">1.pandas.read_csv()</h2><p id="fa28" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated"><strong class="ih hj"> <em class="mf">输入</em> </strong>:读取 CSV 文件<br/> <strong class="ih hj"> <em class="mf">输出</em> </strong>:熊猫数据帧</p><p id="4f95" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">pandas.read_csv()将整个 csv 文件一次性加载到内存中的单个数据帧中。</p><pre class="je jf jg jh fd ke kf kg kh aw ki bi"><span id="b794" class="kj kk hi kf b fi kl km l kn ko">start = time.time()<br/>df = pd.read_csv('huge_data.csv')<br/>end = time.time()<br/>print("Read csv without chunks: ",(end-start),"sec")</span><span id="2b4c" class="kj kk hi kf b fi kp km l kn ko">Read csv without chunks:  26.88872528076172 sec</span></pre><p id="0eca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果 CSV 大小超过您的内存大小(RAM ),这有时可能会由于 OOM(内存不足)错误而使您的系统崩溃。通过下一个导入方法改进了解决方案。</p><h2 id="301a" class="kj kk hi bd kr ls lt lu kv lv lw lx kz iq ly lz ld iu ma mb lh iy mc md ll me bi translated">2.pandas.read_csv(chunksize)</h2><p id="9c03" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated"><strong class="ih hj"> <em class="mf">输入</em> </strong>:读取 CSV 文件<br/> <strong class="ih hj"> <em class="mf">输出</em> </strong>:熊猫数据帧</p><p id="52d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不是一次读取整个 CSV，<strong class="ih hj">CSV 的大块被读入内存</strong>。使用 chunksize 参数指定块的大小，该参数指的是行数。这个函数返回一个迭代器来遍历这些块，然后随心所欲地处理它们。因为一次只能读取一个大文件的一部分，所以低内存足以容纳数据。稍后，这些块可以连接成一个数据帧。</p><pre class="je jf jg jh fd ke kf kg kh aw ki bi"><span id="87a5" class="kj kk hi kf b fi kl km l kn ko">start = time.time()<br/>#read data in chunks of 1 million rows at a time<br/>chunk = pd.read_csv('huge_data.csv',chunksize=1000000)<br/>end = time.time()<br/>print("Read csv with chunks: ",(end-start),"sec")<br/>pd_df = pd.concat(chunk)</span><span id="fa80" class="kj kk hi kf b fi kp km l kn ko">Read csv with chunks:  0.013001203536987305 sec</span></pre><p id="8494" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此选项速度更快，最适合在 RAM 有限的情况下使用。或者，也可以使用一个新的 python 库 DASK，如下所述。</p><h1 id="ebb8" class="kq kk hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">达斯克</h1><p id="e6d2" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated"><strong class="ih hj"> <em class="mf">输入</em> </strong>:读取 CSV 文件<br/> <strong class="ih hj"> <em class="mf">输出</em> </strong> : Dask dataframe</p><p id="de11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在读取大的 CSV 文件时，如果它不适合你的 RAM，你可能会遇到内存不足的错误，因此 DASK 出现了。</p><ul class=""><li id="54ef" class="mg mh hi ih b ii ij im in iq mi iu mj iy mk jc ml mm mn mo bi translated">Dask 是一个<strong class="ih hj">开源</strong> python 库，具有 python 中的并行性和可伸缩性特性，默认包含在 Anaconda 发行版中。</li><li id="c110" class="mg mh hi ih b ii mp im mq iq mr iu ms iy mt jc ml mm mn mo bi translated">它通过重用<strong class="ih hj">现有的 Python 库</strong>，如 pandas、numpy 或 sklearn，扩展了其可伸缩性和并行性的特性。这对那些已经熟悉这些 Python 库的人来说很方便。</li><li id="419c" class="mg mh hi ih b ii mp im mq iq mr iu ms iy mt jc ml mm mn mo bi translated">怎么入手呢？您可以通过 pip 或 conda 安装。我会推荐 conda，因为通过 pip 安装可能会产生一些问题。</li></ul><pre class="je jf jg jh fd ke kf kg kh aw ki bi"><span id="6b8c" class="kj kk hi kf b fi kl km l kn ko">pip install dask</span></pre><p id="c708" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗯，当我尝试上面的方法时，它产生了一些问题，使用一些 GitHub 链接从外部添加 dask path 作为环境变量解决了这些问题。但是当有更简单的选择时，为什么要大惊小怪呢？</p><pre class="je jf jg jh fd ke kf kg kh aw ki bi"><span id="bce2" class="kj kk hi kf b fi kl km l kn ko">conda install dask</span></pre><ul class=""><li id="a783" class="mg mh hi ih b ii ij im in iq mi iu mj iy mk jc ml mm mn mo bi translated">代码实现:</li></ul><pre class="je jf jg jh fd ke kf kg kh aw ki bi"><span id="9071" class="kj kk hi kf b fi kl km l kn ko">from dask import dataframe as dd</span><span id="e0e7" class="kj kk hi kf b fi kp km l kn ko">start = time.time()<br/>dask_df = dd.read_csv('huge_data.csv')<br/>end = time.time()<br/>print("Read csv with dask: ",(end-start),"sec")</span><span id="63c0" class="kj kk hi kf b fi kp km l kn ko">Read csv with dask:  0.07900428771972656 sec</span></pre><p id="d54b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Dask 在读取这个大型 CSV 时似乎是最快的，不会使计算机崩溃或变慢。哇！那有多好？！！一个新的 Python 库，修改了现有的库以引入可伸缩性。</p><h2 id="9e00" class="kj kk hi bd kr ls lt lu kv lv lw lx kz iq ly lz ld iu ma mb lh iy mc md ll me bi translated">为什么达斯克比熊猫强？</h2><ul class=""><li id="db05" class="mg mh hi ih b ii ln im lo iq mu iu mv iy mw jc ml mm mn mo bi translated">Pandas 使用单个 CPU 内核，而<strong class="ih hj"> Dask 通过内部分块数据帧和并行处理来使用多个 CPU 内核</strong>。简而言之，一个大数据帧的多个小数据帧同时被处理，而在 pandas 下，操作一个大数据帧需要很长时间。</li><li id="5bef" class="mg mh hi ih b ii mp im mq iq mr iu ms iy mt jc ml mm mn mo bi translated">DASK 可以在单个 CPU 上处理大型数据集，利用它的多个核心或机器集群，称为分布式计算。它提供了一种<strong class="ih hj">规模的 pandas 和 numpy 库</strong>。</li><li id="5f39" class="mg mh hi ih b ii mp im mq iq mr iu ms iy mt jc ml mm mn mo bi translated">不仅仅是 dataframe，dask 还提供了数组和 scikit-learn 库来利用并行性。</li></ul><p id="0a2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一些 DASK 提供的库如下所示。</p><ul class=""><li id="a498" class="mg mh hi ih b ii ij im in iq mi iu mj iy mk jc ml mm mn mo bi translated"><strong class="ih hj"> Dask 阵列</strong>:并行数字阵列</li><li id="3f90" class="mg mh hi ih b ii mp im mq iq mr iu ms iy mt jc ml mm mn mo bi translated"><strong class="ih hj"> Dask 数据帧</strong>:平行熊猫</li><li id="c3ab" class="mg mh hi ih b ii mp im mq iq mr iu ms iy mt jc ml mm mn mo bi translated"><strong class="ih hj"> Dask ML </strong>:并行 Scikit-Learn</li></ul><p id="e205" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我们将只关注数据帧，因为其他两个不在讨论范围内。但是，要想弄脏你的手，最好考虑一下</strong> <a class="ae jt" href="https://www.analyticsvidhya.com/blog/2018/08/dask-big-datasets-machine_learning-python/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">这个博客</strong> </a> <strong class="ih hj">。</strong></p><h2 id="48f9" class="kj kk hi bd kr ls lt lu kv lv lw lx kz iq ly lz ld iu ma mb lh iy mc md ll me bi translated">Dask 如何存储大于内存(RAM)的数据？</h2><p id="cb91" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated">当我们导入数据时，它被读入我们的 RAM，这突出了内存限制。<br/>比方说，你想在你的 4 GB RAM 中导入 6 GB 数据。这不能通过熊猫来实现，因为一个镜头中的全部数据都不适合内存，但 Dask 可以。怎么会？<br/> <em class="mf"> Dask 首先代替计算，</em> <strong class="ih hj"> <em class="mf">创建一个任务图</em> </strong> <em class="mf">其中说的是关于如何执行那个任务。它相信懒惰计算，这意味着 dask 的任务调度程序首先创建一个图，然后在请求时</em> <strong class="ih hj"> <em class="mf">计算该图</em> </strong> <em class="mf">。为了执行任何计算，compute()被显式调用，它调用任务调度器来利用所有核处理数据，最后将结果组合成一个。</em></p><p id="5d7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于那些已经熟悉熊猫的人来说，这并不难理解。</p><blockquote class="mx"><p id="eb5c" class="my mz hi bd na nb nc nd ne nf ng jc dx translated"><a class="ae jt" rel="noopener" href="/@shachikaul35/dask-for-python-and-machine-learning-dbe1356b5d7a">T19】</a></p></blockquote><h1 id="8f94" class="kq kk hi bd kr ks kt ku kv kw kx ky kz la nh lc ld le ni lg lh li nj lk ll lm bi translated">结论</h1><p id="3520" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated">使用各种导入选项在内存中读取约 1 GB CSV 可以通过加载到内存中所用的时间来评估。</p><p id="3c65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="mf"> pandas.read_csv </em>在读取比 RAM 大的 csv 时<strong class="ih hj">最差</strong>。<br/><em class="mf">pandas . read _ CSV(chunksize)</em>比上面的<strong class="ih hj"/>执行得更好，并且可以通过调整 chunk size 来进一步改进。<br/> <em class="mf"> dask.dataframe </em>被证明是<strong class="ih hj">最快的</strong>，因为它处理并行处理。</p><p id="f61a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我会建议你走出使用熊猫的舒适区，尝试 dask。但是仅供参考，我只测试了 DASK 读取大 CSV 的能力，而没有测试我们在熊猫身上做的计算。</p><blockquote class="mx"><p id="277a" class="my mz hi bd na nb nc nd ne nf ng jc dx translated"><strong class="ak">你可以查看我的</strong> <a class="ae jt" href="https://github.com/shachi01/dask_in_python_ml/blob/master/efficient_read_csv.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> github 代码</strong> </a> <strong class="ak">来访问覆盖本博客编码部分的笔记本。</strong></p></blockquote><h1 id="ff7d" class="kq kk hi bd kr ks kt ku kv kw kx ky kz la nh lc ld le ni lg lh li nj lk ll lm bi translated">参考</h1><ul class=""><li id="96e8" class="mg mh hi ih b ii ln im lo iq mu iu mv iy mw jc ml mm mn mo bi translated"><a class="ae jt" href="https://docs.dask.org/en/latest/" rel="noopener ugc nofollow" target="_blank"> Dask 最新文档</a></li><li id="68f4" class="mg mh hi ih b ii mp im mq iq mr iu ms iy mt jc ml mm mn mo bi translated"><a class="ae jt" href="https://www.amazon.in/Data-Science-Scale-Python-Dask/dp/1617295604" rel="noopener ugc nofollow" target="_blank">值得一读的书</a></li><li id="1d6b" class="mg mh hi ih b ii mp im mq iq mr iu ms iy mt jc ml mm mn mo bi translated">这篇博客中没有包括的其他 CSV 读写选项。</li></ul><div class="nk nl ez fb nm nn"><a href="https://realpython.com/python-csv/" rel="noopener  ugc nofollow" target="_blank"><div class="no ab dw"><div class="np ab nq cl cj nr"><h2 class="bd hj fi z dy ns ea eb nt ed ef hh bi translated">用 Python 读写 CSV 文件-真正的 Python</h2><div class="nu l"><h3 class="bd b fi z dy ns ea eb nt ed ef dx translated">立即观看本教程有一个由真正的 Python 团队创建的相关视频课程。和书面的一起看…</h3></div><div class="nv l"><p class="bd b fp z dy ns ea eb nt ed ef dx translated">realpython.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob jn nn"/></div></div></a></div><p id="b3f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.想在 DASK 里弄脏你的手，应该浏览一下下面的链接。</p><div class="nk nl ez fb nm nn"><a href="https://pythonspeed.com/articles/faster-pandas-dask/" rel="noopener  ugc nofollow" target="_blank"><div class="no ab dw"><div class="np ab nq cl cj nr"><h2 class="bd hj fi z dy ns ea eb nt ed ef hh bi translated">从分块到并行:更快的熊猫与 Dask</h2><div class="nu l"><h3 class="bd b fi z dy ns ea eb nt ed ef dx translated">当数据不适合内存时，您可以使用分块:加载数据，然后分块处理，这样只有一个子集…</h3></div><div class="nv l"><p class="bd b fp z dy ns ea eb nt ed ef dx translated">pythonspeed.com</p></div></div><div class="nw l"><div class="oc l ny nz oa nw ob jn nn"/></div></div></a></div></div><div class="ab cl od oe gp of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="hb hc hd he hf"><p id="8787" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你喜欢这位作者的博客，请随意关注，因为这位作者保证会带来更多有趣的人工智能相关内容。<br/> 感谢，<br/>学习愉快！😄</p><p id="8e49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="mf">可以通过</em></strong><a class="ae jt" href="https://www.linkedin.com/in/kaul-shachi" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"><em class="mf">LinkedIn</em></strong></a><strong class="ih hj"><em class="mf">取得联系。</em> </strong></p></div></div>    
</body>
</html>