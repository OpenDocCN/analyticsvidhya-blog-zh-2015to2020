<html>
<head>
<title>RNN vs GRU vs LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RNN对GRU对LSTM</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/rnn-vs-gru-vs-lstm-863b0b7b1573?source=collection_archive---------0-----------------------#2020-11-14">https://medium.com/analytics-vidhya/rnn-vs-gru-vs-lstm-863b0b7b1573?source=collection_archive---------0-----------------------#2020-11-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f699" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将让你先了解RNN、GRU和LSTM的理论，然后我将向你展示如何用代码实现和使用它们。</p><p id="fe8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">已经有很多关于这些话题的帖子了。但是在这篇文章中，我想借助代码提供更好的理解和比较。</p><p id="44af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们从RNN开始吧！</p><h2 id="18c6" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">递归神经网络</strong></h2><p id="7ed0" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">递归神经网络(RNN)设计用于处理序列数据。顺序数据(可以是时间序列)可以是文本、音频、视频等形式。</p><p id="fc8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RNN使用序列中先前的信息来产生当前的输出。为了更好地理解这一点，我举了一个例句。</p><blockquote class="kd"><p id="b4e9" class="ke kf hi bd kg kh ki kj kk kl km jc dx translated">“我的班是最好的班。”</p></blockquote><p id="279c" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated"><strong class="ih hj">当时(<em class="ks"> T0 ) </em> </strong> <em class="ks">，</em>第一步就是把<em class="ks">我的</em>这个词馈入网络。RNN产生一个输出。</p><p id="3d6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">当时(<em class="ks"> T1 ) </em> </strong> <em class="ks">，</em>然后在下一步我们输入单词<em class="ks">“class”</em>和上一步的激活值。现在RNN有了“我的T21”和“阶级”这两个词的信息。</p><p id="4ab1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个过程一直持续到句子中的所有单词都被输入。你可以看下面的动画来形象化理解。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es kt"><img src="../Images/e459911825166c117446cf4a0f7b6b30.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*gEA0-LTj05xtESA5XoBxPw.gif"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">RNN的工作流程。</figcaption></figure><p id="7385" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<strong class="ih hj">最后一步</strong>，RNN具有关于所有先前单词的信息。</p><blockquote class="lf lg lh"><p id="f30d" class="if ig ks ih b ii ij ik il im in io ip li ir is it lj iv iw ix lk iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="hi">注意:</em> </strong> <em class="hi">在RNN </em>层中所有节点的权重和偏差都是相同的。</p></blockquote><p id="b6f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看RNN单元的架构。它接受上一步的输入和当前输入。这里<em class="ks"> tanh </em>是激活功能，代替<em class="ks"> tanh </em>你也可以使用其他激活功能。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es ll"><img src="../Images/e06915a18d57cca600a0328bb4a97a2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/0*eRJCRsikdGGu8ffA.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">RNN基础建筑</figcaption></figure><p id="919a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">💡</strong> RNN面临短期记忆问题。这是由于消失梯度问题造成的。随着RNN处理的步骤越来越多，它比其他神经网络架构更容易遭受消失梯度的影响。</p><p id="d7b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">问:什么是消失渐变问题？</strong></p><p id="b997" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">答:</strong>在RNN训练网络你通过时间反向传播，在每一步计算梯度。梯度用于更新网络中的权重。如果前一层对当前层的影响很小，那么梯度值也很小，反之亦然。如果前一层的梯度较小，则当前层的梯度将更小。当我们反向传播时，这使得梯度指数地缩小。较小的梯度意味着它不会影响重量的增加。因此，网络不会了解早期输入的影响。从而导致短期记忆问题。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es lm"><img src="../Images/018181521a6c35459804cd73e864eca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*Wn7g1eFkJDVzM2mBjHfyFw.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">RNN的隐藏状态</figcaption></figure><blockquote class="lf lg lh"><p id="e8cb" class="if ig ks ih b ii ij ik il im in io ip li ir is it lj iv iw ix lk iz ja jb jc hb bi translated">主要的问题是，对RNN来说，要学会在多个时间步长内保存信息太难了。在普通的RNN，隐藏的州正不断地被<strong class="ih hj">改写。</strong></p></blockquote><p id="b0cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有独立内存的RNN怎么样？</p><p id="0489" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">消失渐变解决方案</strong></p><p id="276e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解决这个问题，两个专门版本的RNN被创造出来。它们是1) GRU(门控循环单元)2) LSTM(长短期记忆)。假设有两个句子。第一句是“我的<strong class="ih hj"> <em class="ks">猫</em> </strong>是……她<strong class="ih hj"> <em class="ks">是</em> </strong>病了。”，第二个是“猫的<strong class="ih hj"><em class="ks"/></strong>…..他们<strong class="ih hj"> <em class="ks">都</em> </strong>病了。”在句子的结尾，如果我们需要预测单词"<strong class="ih hj"> <em class="ks">"被"/"被"</em>" T25 "，网络就得记住起始单词"<strong class="ih hj"> <em class="ks">"猫"/"猫"</em> </strong>。因此，LSTM和GRU利用存储单元来存储长序列中前几个词的激活值。现在<strong class="ih hj"> <em class="ks">盖茨</em> </strong>的概念进入画面。网关用于控制网络中的信息流。门能够学习序列中的哪些输入是重要的，并将它们的信息存储在存储单元中。他们可以传递长序列的信息，并利用它们进行预测。</strong></p><h2 id="49cb" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">门控循环单位</h2><p id="a076" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">GRU的工作流程与RNN相同，但不同之处在于GRU分部的内部运作。让我们看看它的架构。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ln"><img src="../Images/d5e516fd4ba44edd35320d94a9462702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RiOzdOVaaeKrUotY7-1a2A.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx translated">GRU基础建筑</figcaption></figure><p id="9cc6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在GRU内部，它有两个门1)重置门2)更新门</p><p id="54ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">门只不过是神经网络，每个门都有自己的权重和偏差(但不要忘记一层中所有节点的权重和偏差都是相同的)。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es ls"><img src="../Images/63de82fb91d912826a0a2a190bc927e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rtSzebDodKO9NfQj"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx translated">GRU的门和细胞态公式</figcaption></figure><p id="5cec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">更新门</strong></p><p id="c418" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">更新门决定是否应该用候选状态(当前激活值)更新单元状态。</p><p id="0b97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">复位门</strong></p><p id="3768" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">复位门用于决定先前的单元状态是否重要。有时在简单的GRU中不使用复位门。</p><p id="6177" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">候选单元格</strong></p><p id="bc0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它只是简单地与RNN的隐藏状态(激活)相同。</p><p id="4708" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">最终单元状态</strong></p><p id="5ffc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最终单元状态取决于更新门。它可以用候选状态更新，也可以不用。从上一个单元格状态中删除一些内容，并写入一些新的单元格内容。</p><blockquote class="lf lg lh"><p id="40ea" class="if ig ks ih b ii ij ik il im in io ip li ir is it lj iv iw ix lk iz ja jb jc hb bi translated">在GRU中，最终单元状态作为激活直接传递到下一个单元。</p></blockquote><p id="9f5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在GRU，</p><ul class=""><li id="a70b" class="lt lu hi ih b ii ij im in iq lv iu lw iy lx jc ly lz ma mb bi translated">如果重置为接近0，则忽略先前的隐藏状态(允许模型丢弃将来不相关的信息)。</li><li id="cf8a" class="lt lu hi ih b ii mc im md iq me iu mf iy mg jc ly lz ma mb bi translated">如果gamma(更新门)接近1，那么我们可以通过许多步骤在该单元中复制信息！</li><li id="655b" class="lt lu hi ih b ii mc im md iq me iu mf iy mg jc ly lz ma mb bi translated">伽玛控制着过去的状态现在应该有多重要。</li></ul><h2 id="a9d5" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated">长短期记忆</h2><p id="581b" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">现在你知道了RNN和GRU，所以让我们简单地快速了解一下LSTM是如何运作的。LSTMs与GRU的非常相似，它们也是为了解决梯度消失的问题。除了GRU，这里还有2个门1)忘记门2)输出门。</p><p id="518e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，看看它的架构。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es mh"><img src="../Images/311f654732f45ffa9edb7902c7e099ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lSDKRennQMpJFL4xxJHloQ.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx translated">LSTM基础建筑</figcaption></figure><p id="e7b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，看看它内部的操作。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es mi"><img src="../Images/990dccd6ed85876cf13146d68b922342.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ri87goy5AAVVEK7s"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx translated">LSTM的门和细胞态公式</figcaption></figure><p id="c0a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从GRU，你已经知道了除了遗忘门和输出门之外的所有其他操作。</p><blockquote class="lf lg lh"><p id="4559" class="if ig ks ih b ii ij ik il im in io ip li ir is it lj iv iw ix lk iz ja jb jc hb bi translated">所有3个门(输入门、输出门、遗忘门)都使用sigmoid作为激活函数，因此所有门的值都在0和1之间。</p></blockquote><p id="ccbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">忘记大门</strong></p><p id="550f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它控制着从以前的细胞状态中哪些被保留，哪些被遗忘。用外行人的话来说，它将决定应该保留多少来自先前状态的信息，并忘记剩余的信息。</p><p id="d431" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输出门</strong></p><p id="1a79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它控制单元格的哪些部分输出到隐藏状态。它将决定下一个隐藏状态是什么。</p><p id="0239" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">唷！理论到此为止，现在让我们开始编码。</p><p id="cec5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我采用航空乘客数据集，并在数据集上提供所有3个(RNN、GRU、LSTM)模型的性能。</p><blockquote class="lf lg lh"><p id="21f0" class="if ig ks ih b ii ij ik il im in io ip li ir is it lj iv iw ix lk iz ja jb jc hb bi translated">我的动机是让您理解并知道如何在任何数据集上实现这些模型。简单来说，我并不关注隐藏层的神经元数量或网络的层数(您可以玩玩这些来获得更好的准确性)。</p></blockquote><p id="9d65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">关于数据集</strong>:</p><p id="caa0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该数据集提供了特定月份乘坐美国航空公司航班的人数记录。它有142个月的记录。它有两列“月”和“乘客数量”。但在这种情况下，我想使用单变量数据集。仅使用“乘客数量”。</p><p id="9f87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">导入所有必需的库和数据集。</p><pre class="ku kv kw kx fd mj mk ml mm aw mn bi"><span id="d33a" class="jd je hi mk b fi mo mp l mq mr">import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense, LSTM,GRU,SimpleRNN<br/>from sklearn.preprocessing import MinMaxScaler</span><span id="7129" class="jd je hi mk b fi ms mp l mq mr">df = pd.read_csv('airline-passengers.csv')<br/>df.head()<br/>df.drop(['Month'],axis=1,inplace=True)<br/>dataset = np.array(df)<br/>dataset.reshape(-1,1)</span></pre><p id="df9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了可视化数据集，<code class="du mt mu mv mk b">plt.plot(dataset)</code></p><figure class="ku kv kw kx fd ky er es paragraph-image"><div class="er es mw"><img src="../Images/1d3f1be313858981106fbf6410b2b9b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*Mo5aZKRtJLekZDGojKhIDQ.png"/></div><figcaption class="lb lc et er es ld le bd b be z dx translated">乘客数量(Y轴)与月数(X轴)</figcaption></figure><p id="0041" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它显示乘客数量在几个月内呈线性增长。</p><p id="4baa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果所有数据都经过缩放，机器学习模型/神经网络的效果会更好。</p><pre class="ku kv kw kx fd mj mk ml mm aw mn bi"><span id="5c57" class="jd je hi mk b fi mo mp l mq mr">scaler = MinMaxScaler()<br/>dataset = scaler.fit_transform(dataset)</span></pre><p id="bad9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将数据划分为训练和测试。我将数据集分成(75%的训练和25%的测试)。在数据集中，我们可以根据第<strong class="ih hj">‘I-1’</strong>个值来估计第<strong class="ih hj">‘I’</strong>个值。您也可以通过取i-1，i-2，i-3…来增加输入序列的长度，以预测第<strong class="ih hj">‘I’</strong>个值。</p><pre class="ku kv kw kx fd mj mk ml mm aw mn bi"><span id="252f" class="jd je hi mk b fi mo mp l mq mr">train_size = int(len(dataset) * 0.75)<br/>test_size = len(dataset) - train_size<br/>train=dataset[:train_size,:]<br/>test=dataset[train_size:142,:]<br/>def getdata(data,lookback):<br/>    X,Y=[],[]<br/>    for i in range(len(data)-lookback-1):<br/>        X.append(data[i:i+lookback,0])<br/>        Y.append(data[i+lookback,0])<br/>    return np.array(X),np.array(Y).reshape(-1,1)<br/>lookback=1<br/>X_train,y_train=getdata(train,lookback)<br/>X_test,y_test=getdata(test,lookback)<br/>X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)<br/>X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)</span></pre><p id="29d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我制作了只有两层的序列模型。图层:</p><ol class=""><li id="467d" class="lt lu hi ih b ii ij im in iq lv iu lw iy lx jc mx lz ma mb bi translated">简单的RNN/GRU/LSTM</li><li id="df39" class="lt lu hi ih b ii mc im md iq me iu mf iy mg jc mx lz ma mb bi translated">致密层</li></ol><p id="dd51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这段代码中，我用的是LSTM。您也可以使用其他两个，只需将下面代码(第2行)中的“LSTM”替换为“SimpleRNN”/“GRU”。</p><pre class="ku kv kw kx fd mj mk ml mm aw mn bi"><span id="9a6e" class="jd je hi mk b fi mo mp l mq mr">model=Sequential()<br/>model.add(<strong class="mk hj">LSTM</strong>(5,input_shape=(1,lookback)))<br/>model.add(Dense(1))<br/>model.compile(loss='mean_squared_error',optimizer='adam')</span></pre><p id="0acb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在LSTM层，我使用了5个神经元，这是神经网络的第一层(隐藏层)，因此input_shape是我们将传递的输入的形状。</p><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es my"><img src="../Images/c14c787fb1ad41661fd34591dc75514d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gu4Ms4udSn_NxSKFWZSdTg.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx translated">神经网络概述</figcaption></figure><p id="1b2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，模型已经准备好了。所以开始训练模型吧。</p><pre class="ku kv kw kx fd mj mk ml mm aw mn bi"><span id="e762" class="jd je hi mk b fi mo mp l mq mr">model.fit(X_train, y_train, epochs=50, batch_size=1)<br/>y_pred=model.predict(X_test)<br/>y_test=scaler.inverse_transform(y_test)<br/>y_pred=scaler.inverse_transform(y_pred)</span></pre><p id="eb9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，可视化真实值和预测结果。</p><pre class="ku kv kw kx fd mj mk ml mm aw mn bi"><span id="ade1" class="jd je hi mk b fi mo mp l mq mr">plt.figure(figsize=(14,5))<br/>plt.plot(y_test, label = 'Real number of passengers')<br/>plt.plot(y_pred, label = 'Predicted number of passengers')<br/>plt.ylabel('# passengers')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="er es mz"><img src="../Images/80d42d9603e66ef86db1efc5e9c2d2c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*noa7fjnChy86sfgM1P--Vg.png"/></div></div><figcaption class="lb lc et er es ld le bd b be z dx translated">乘客(Y轴)与样本数量(X轴)</figcaption></figure><p id="28d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于此数据集和使用50个历元的简单网络，我得到了以下均方误差值。</p><pre class="ku kv kw kx fd mj mk ml mm aw mn bi"><span id="b553" class="jd je hi mk b fi mo mp l mq mr">from sklearn.metrics import mean_squared_error<br/>mean_squared_error(y_test,y_pred)</span></pre><p id="3a7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单的RNN:3000<br/>GRU:2584<br/>LSTM:2657</p><h2 id="e234" class="jd je hi bd jf jg jh ji jj jk jl jm jn iq jo jp jq iu jr js jt iy ju jv jw jx bi translated"><strong class="ak">总结</strong></h2><p id="54bd" class="pw-post-body-paragraph if ig hi ih b ii jy ik il im jz io ip iq ka is it iu kb iw ix iy kc ja jb jc hb bi translated">在了解了这3个模型后，我们可以说RNN对序列数据表现良好，但有短期记忆问题(对于长序列)。这并不意味着总是使用GRU/LSTM。简单RNN有其自身的优势(训练速度更快，计算成本更低)。</p></div></div>    
</body>
</html>