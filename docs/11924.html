<html>
<head>
<title>Basics of Deep Learning explained in 3 minutes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">3 分钟解释深度学习的基础知识</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understand-basics-of-deep-learning-in-5-minutes-484c6ada47cf?source=collection_archive---------18-----------------------#2020-12-23">https://medium.com/analytics-vidhya/understand-basics-of-deep-learning-in-5-minutes-484c6ada47cf?source=collection_archive---------18-----------------------#2020-12-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3792" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">一步一步的实用解释</h2></div><p id="a5b9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated">eep 学习代表了机器学习的一个子领域，它依赖于一种在结构和功能上都类似于人脑的算法，称为人工神经网络。它模拟人类神经元传输和处理数据的功能，将数据转化为有用的信息。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es kc"><img src="../Images/547e04f86cf81868d8583f61f6de88cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DZF4g7BvmICvGCVQ"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">由<a class="ae ks" href="https://unsplash.com/@averey?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Robina Weermeijer </a>在<a class="ae ks" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="ece3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">神经网络的基本形式是一个<strong class="iz hj">感知器</strong>，它是一个单层神经网络模型，设计用于监督二进制分类，由 Frank Rosenblatt 于 1957 年发明[1]。</p><h1 id="f504" class="kt ku hi bd kv kw kx ky kz la lb lc ld io le ip lf ir lg is lh iu li iv lj lk bi translated">感知器:</h1><p id="16cf" class="pw-post-body-paragraph ix iy hi iz b ja ll ij jc jd lm im jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">该模型的隐含层由<strong class="iz hj">一个神经元</strong> ( <strong class="iz hj">计算单元</strong>)组成，因此其简单性将有助于我们理解神经网络功能的基础。<strong class="iz hj">正向传播</strong>中感知器的结构如下所示:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lq"><img src="../Images/3b4f5c16dbf5a0f2aa0ec4049c8f55d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pal8nN1nf4_lDADB_vX01w.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">作者创建的图像</figcaption></figure><p id="b80a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输入向量的预测输出为:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es lr"><img src="../Images/e47211dc2eb0cc051ed2327fcca6dfbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P46ewIkAKeXtstDje7Tr-g.png"/></div></div></figure><blockquote class="ls"><p id="1841" class="lt lu hi bd lv lw lx ly lz ma mb js dx translated">感知器的算法经历两个主要阶段<strong class="ak">，首先是正向传播</strong> <strong class="ak">，然后是反向传播。</strong></p></blockquote></div><div class="ab cl mc md gp me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="hb hc hd he hf"><h2 id="6e67" class="mj ku hi bd kv mk ml mm kz mn mo mp ld jg mq mr lf jk ms mt lh jo mu mv lj mw bi translated">1/正向传播<strong class="ak">的步骤</strong>:</h2><ul class=""><li id="f6a6" class="mx my hi iz b ja ll jd lm jg mz jk na jo nb js nc nd ne nf bi translated">初始化权重(大部分时间是随机的)和<strong class="iz hj">输入层</strong>，它用数字表示数据。</li><li id="d6f8" class="mx my hi iz b ja ng jd nh jg ni jk nj jo nk js nc nd ne nf bi translated">计算输入的<strong class="iz hj">加权和。</strong></li><li id="54ac" class="mx my hi iz b ja ng jd nh jg ni jk nj jo nk js nc nd ne nf bi translated">将激活函数<strong class="iz hj"> </strong>(可以是整流线性单元、<strong class="iz hj"> </strong> Sigmoid、tanh 或 Softmax…) <strong class="iz hj"> </strong>应用于无法计算的和与偏差，这给了我们<strong class="iz hj">预测输出</strong>。</li></ul><h2 id="9472" class="mj ku hi bd kv mk ml mm kz mn mo mp ld jg mq mr lf jk ms mt lh jo mu mv lj mw bi translated">2/反向传播的步骤:</h2><ul class=""><li id="fbc6" class="mx my hi iz b ja ll jd lm jg mz jk na jo nb js nc nd ne nf bi translated">计算并优化预测输出和期望输出之间的<strong class="iz hj">损失函数</strong>(可以是均方误差损失、平均绝对误差损失、二元交叉熵……)</li><li id="faf0" class="mx my hi iz b ja ng jd nh jg ni jk nj jo nk js nc nd ne nf bi translated">更新<strong class="iz hj">权重</strong>。</li></ul><p id="aed7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">重复这两个步骤，直到通过在每次迭代中更新权重来最小化误差。</p><p id="31c4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个简图总结了感知器的学习过程:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es nl"><img src="../Images/15b94909750e88ef764863028c1e0cd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0D-jMDJ86WeBvvo0WPX0MA.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">作者创建的图像</figcaption></figure></div><div class="ab cl mc md gp me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="hb hc hd he hf"><p id="e266" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">作为结论，人工神经网络是由<strong class="iz hj">多个感知器</strong>连接而成，并对<strong class="iz hj">不同的</strong>激活函数进行操作。</p><p id="6093" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">隐藏层数</strong>表示网络的深度。因此，一个深度神经网络由<strong class="iz hj">多于一个隐含层</strong>组成如下:</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div role="button" tabindex="0" class="ki kj di kk bf kl"><div class="er es nm"><img src="../Images/6d4af7ca3381e46c1bda5341cd43e335.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1j_bYsFLl9f7k9B8TG4P7A.png"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">作者创建的图像</figcaption></figure><p id="2444" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">参考文献</strong>:</p><p id="fa7b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">[1] Marvin Papert Minsky 和 Seymour。感知器，第一卷。麻省理工出版社，1969 年。</p></div></div>    
</body>
</html>