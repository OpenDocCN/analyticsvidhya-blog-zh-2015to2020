<html>
<head>
<title>Linear Regression With One Variable</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一元线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-with-one-variable-c6a7d07c9b88?source=collection_archive---------25-----------------------#2020-11-01">https://medium.com/analytics-vidhya/linear-regression-with-one-variable-c6a7d07c9b88?source=collection_archive---------25-----------------------#2020-11-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="23b0" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><strong class="ak">用梯度下降法和正规方程预测</strong></h2></div><p id="5306" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">线性回归是一种<strong class="iz hj">线性模型</strong>，例如，假设输入变量(x)和单个输出变量(y)之间存在线性关系的模型。更具体地说，y可以从输入变量(x)的线性组合中计算出来。</p><p id="280c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们将使用梯度下降来拟合数据集的线性回归参数θ。</p><p id="338c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据集取自<a class="ae jt" href="https://storage.googleapis.com/kaggle-data-sets/8834/12327/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&amp;X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20201031%2Fauto%2Fstorage%2Fgoog4_request&amp;X-Goog-Date=20201031T062320Z&amp;X-Goog-Expires=259199&amp;X-Goog-SignedHeaders=host&amp;X-Goog-Signature=2bdaab426616f3ca61ffe408b382f5bcabf0fd6ff4b9de913750c1087d7a98d5cdf6422361f0556c63fbe85ed27c02cd1efd00f9f411dfef9521ae992a3c8cda7c3e8033905484ad17bf346c6cffd074409dc832d957397190ce755d6556f9d239bed6687fbad708d81be9620e9c57af176e0a3395b108cf21dccef8516ffa3bc962f2b63e55e6eb24d58c3732474d5ed68d42a8ae1f45bf496c1d4175ee6d52e4b2c12b68684a2f23fb9fe96307d8b15acc1a6ee8f365f2ae290bf68a9d72f3ab26af5be627b8a7b4a6f9b93ba35133d0c2faa48857dc83ad50e2d58a48a8bc58f1ee35d2b1c5106f4a8bb47ab4c27d4d1706ea2e67bdbcd2b2ff08c92a7e72" rel="noopener ugc nofollow" target="_blank">这里</a></p><p id="45f2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，让我们导入我们需要的库</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es ju"><img src="../Images/e80e017e824aaa7696a93390b2fe334e.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*fLgRlRoGV-E2o4EFfDeW9g.png"/></div></figure><p id="2da4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们来看看数据</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es kc"><img src="../Images/dc9f7743847beef7104b7d649c12c628.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*htYLfuRhKpVsGgxjlEjL8A.png"/></div></figure><p id="4991" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们定义输入变量(X)和输出变量(y)</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es kd"><img src="../Images/8f6553247f34841b85e53ae935ed90e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:576/format:webp/1*oN5eraVe1yzuAb3_i-bI2Q.png"/></div></figure><p id="d33e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在开始任何任务之前，通过可视化来理解数据通常是有用的。对于此数据集，您可以使用散点图来可视化数据，因为它只有两个属性可以绘制(利润和人口)。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es ke"><img src="../Images/1ff02da568ea0153dfd132cba8d77f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*9-fjhn9M-9E_kJEHN3unUw.png"/></div></figure><h1 id="8220" class="kf kg hi bd kh ki kj kk kl km kn ko kp io kq ip kr ir ks is kt iu ku iv kv kw bi translated"><strong class="ak">梯度下降</strong></h1><p id="b81e" class="pw-post-body-paragraph ix iy hi iz b ja kx ij jc jd ky im jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">在某种程度上，我们将使用梯度下降将线性回归参数θ拟合到我们的数据集。</p><p id="c94b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">更新方程式</strong></p><p id="66a3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">线性回归的目标是最小化成本函数</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lc"><img src="../Images/6cd479e90a2c73514d804f41b5dde976.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*qY9r3iQmFhLn6kXpGe5RJQ.png"/></div></figure><p id="9320" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中假设hθ(x)由线性模型给出</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es ld"><img src="../Images/1bca538bc4c88e0a15413b7b8d1b40a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*2Ngthamm9lovD96it9saSA.png"/></div></figure><p id="7df5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">模型的参数是θj值。这些是您将调整以最小化成本J(θ)的值。一种方法是使用批量梯度下降算法。在批量梯度下降中，每次迭代执行更新。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es le"><img src="../Images/0c13ee926f76b58c04da1bc982a73e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gwr19HrzjQNQYjZ1G-SRhg.png"/></div></div></figure><p id="aedd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">实施</strong></p><p id="3833" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们已经为线性回归建立了数据。在下面的单元格中，我们为数据增加了另一个维度，以适应θ0截距项。不要多次执行此单元格。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lj"><img src="../Images/db1d02ffcdfee7c6e63cc03fc7175643.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*7IanoJn70BCgt2S-fOfazQ.png"/></div></figure><p id="38be" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">计算成本J(θ) </strong></p><p id="683e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当您执行梯度下降来学习最小化成本函数J(θ)时，通过计算成本来监控收敛是有帮助的。在本节中，您将实现一个计算J(θ)的函数，以便检查梯度下降实现的收敛性。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lk"><img src="../Images/f1f6af5575a4d3aa478120e9e85b14ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*cOndN5QkrqfnnxC-KSPpIQ.png"/></div></figure><p id="ea13" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">梯度下降</strong></p><p id="e4ea" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，您将实现梯度下降</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es ll"><img src="../Images/b18d1faa7420d8bafb526a02133d1c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nNvS_mhmtVE6ISOY57Tf9Q.png"/></div></div></figure><p id="24f4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在您完成调用实现后，我们将θ参数初始化为0，迭代初始化为2000，学习速率α初始化为0.5。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lm"><img src="../Images/02642ecfc3d274c1944de5e9da3a9b43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*dLyz0z7WG4JEg6TekbMMeQ.png"/></div></figure><p id="ce16" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将使用最终值θ来预测身高1.52的体重</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es ln"><img src="../Images/bd64d89617e1f02a18ba60fcd3e558e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yMwSDRNtmdvXbHTKXfnBjA.png"/></div></div></figure><h1 id="1fcf" class="kf kg hi bd kh ki kj kk kl km kn ko kp io kq ip kr ir ks is kt iu ku iv kv kw bi translated">正态方程</h1><p id="b2f8" class="pw-post-body-paragraph ix iy hi iz b ja kx ij jc jd ky im jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">线性回归的封闭解是</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lo"><img src="../Images/d553958bc95792c8b14e9afc2e33a0b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*0uU7A4JqEXdjCEBbVNIZTA.png"/></div></figure><p id="9398" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用这个公式，你将在一次计算中得到精确解:不存在像梯度下降中那样的“循环直到收敛”。</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lp"><img src="../Images/ca3a6052b0b7a2f685d5d6ff57e68b45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w803HOAGJdQvJ5E_3tOwXQ.png"/></div></div></figure><p id="d8e7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们计算θ</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lq"><img src="../Images/47431dd8d3a05e9e997460be0f13a0e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*25v1J3PnhVUOBNNS5jr5aQ.png"/></div></figure><p id="e535" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将使用最终值θ来预测身高1.52的体重</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lr"><img src="../Images/34da52b09ddf59830146ba1a7bbca33f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*u0RZAd4SDjOTOy5mip2Hbg.png"/></div></figure><h1 id="852e" class="kf kg hi bd kh ki kj kk kl km kn ko kp io kq ip kr ir ks is kt iu ku iv kv kw bi translated">与Sklearn线性回归比较</h1><p id="b957" class="pw-post-body-paragraph ix iy hi iz b ja kx ij jc jd ky im jf jg kz ji jj jk la jm jn jo lb jq jr js hb bi translated">现在我们使用sklearn将其与线性回归进行比较</p><p id="abe9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们首先确定特征和目标</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es ls"><img src="../Images/f5c30185960efff8ff3c441e630ccd6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*0wYEEhzdQmDI6Q38wGhtwA.png"/></div></figure><p id="fa93" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后我们使用sklearn进行线性回归</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div class="er es lt"><img src="../Images/f3af8d3b87387321e4b95b5978a33808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*Ux8_z8gK0XagDSs-E3YAWA.png"/></div></figure><p id="8527" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的截距和系数，我们将用它来预测身高1.52的体重</p><figure class="jv jw jx jy fd jz er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lu"><img src="../Images/af4f2f1094797669223f155f55ce1ab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5q1sRy08GHwnSfkS2CftjQ.png"/></div></div></figure><p id="026d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从上面的结果可以看出，对于截距和系数，正规方程和线性回归sklearn之间的计算结果是相同的。然而，使用梯度下降的计算给出不同的结果。</p><p id="21f3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了克服这一点，有必要将迭代和学习速率α设置为不同的值，以便可以获得与使用正规方程和sklearn线性回归的计算结果接近的θ值。</p><p id="b453" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">谢谢你</p></div></div>    
</body>
</html>