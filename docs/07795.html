<html>
<head>
<title>Machine Learning 101 — Linear Regression using the OLS Method</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习101 —使用OLS方法的线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-101-linear-regression-using-the-ols-method-299808eab233?source=collection_archive---------3-----------------------#2020-07-08">https://medium.com/analytics-vidhya/machine-learning-101-linear-regression-using-the-ols-method-299808eab233?source=collection_archive---------3-----------------------#2020-07-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9020" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归是最基本的机器学习算法之一，用于预测真实值。它涉及到使用一个或多个<em class="jd">自变量</em>来预测一个<em class="jd">因变量</em>。虽然它是我们将遇到的最简单的算法之一，但它在本质上非常强大和健壮，使它成为有抱负的数据专业人员的必要工具。</p><p id="967e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇博文中，我们将介绍线性回归的类型，使用普通最小二乘法(OLS)的实现，以及线性回归模型做出的某些基本假设。在这篇文章中，我们还将引用一个例子，根据员工的<em class="jd">经验</em>来预测他们的<em class="jd">薪水</em>。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/b359cd89bc200c9db861d928f39f2e43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H1WcfW1kbQpfJMGEf7ECJQ.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated"><a class="ae ju" href="https://unsplash.com/@sharonmccutcheon?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Sharon McCutcheon </a>在<a class="ae ju" href="https://unsplash.com/s/photos/money?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h1 id="6294" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">线性回归的类型</h1><h2 id="6460" class="kt jw hi bd jx ku kv kw kb kx ky kz kf iq la lb kj iu lc ld kn iy le lf kr lg bi translated">简单线性回归</h2><p id="43b2" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">在简单的线性回归模型中，只有一个决定因变量的自变量。因此，对于我们的工资-经验的例子，自变量是员工的经验，而工资是因变量。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es lm"><img src="../Images/6524b7ae5e4cd7e85a882f660014413c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NwYBLTCxMInuc3pu.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">图片来源:<a class="ae ju" rel="noopener" href="/@manjabogicevic/multiple-linear-regression-using-python-b99754591ac0">https://medium . com/@ manjabogicevic/multiple-linear-regression-using-python-b 99754591 AC 0</a></figcaption></figure><p id="3348" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们简单的线性回归方程中:</p><ul class=""><li id="9b05" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated"><strong class="ih hj"> y </strong>是因变量</li><li id="c52b" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated"><strong class="ih hj"> b₀ </strong>是我们的偏向项，而</li><li id="e892" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated"><strong class="ih hj"> x₁ </strong>为自变量，其权重为<strong class="ih hj"> b₁ </strong></li></ul><h2 id="074a" class="kt jw hi bd jx ku kv kw kb kx ky kz kf iq la lb kj iu lc ld kn iy le lf kr lg bi translated">多元线性回归</h2><p id="9cca" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">这种类型的回归只是简单线性回归的扩展。这里，我们的因变量<strong class="ih hj"> y </strong>是使用两个或更多自变量作为输入特征集的一部分来预测的。简单来说，如果我们添加更多的输入特征，如每日工作时间、年龄、职位等。对于我们的工资-经验的例子，然后我们得到一个多元线性回归模型。</p></div><div class="ab cl mb mc gp md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="hb hc hd he hf"><h1 id="742a" class="jv jw hi bd jx jy mi ka kb kc mj ke kf kg mk ki kj kk ml km kn ko mm kq kr ks bi translated">复习基础知识</h1><p id="05fc" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">因此，让我们假设我们最终收集了一些数据，其中包含了许多雇员的经验和工资。为了更好地理解我们的数据，我们将其绘制成下图:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mn"><img src="../Images/eef1adb67e946a23131f26eca9441e5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*3mWjaaN-HkIJSi9aF3ITSw.png"/></div></figure><p id="53db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归背后的基本思想是用一条直线来拟合我们的数据。我们可以通过使用<strong class="ih hj"/><strong class="ih hj">普通最小二乘法(OLS </strong>)来实现。在这个方法中，我们通过数据画一条线，测量每个点到这条线的距离，平方每个距离，然后将它们全部加起来。经过多次反复试验，我们能够找到最合适的线路。本质上，最佳拟合线覆盖了我们所有的数据点，使得每个数据点到该线的距离最小化。这反过来最小化了所获得的误差。我们可以在下图中看到数据的最佳拟合线:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mn"><img src="../Images/92b5e9a4192134169cf31573f8004763.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*XV9Fe7uwnB-mvigRQPHg3g.png"/></div></figure><p id="e668" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们过程的下一步是通过计算它的<strong class="ih hj"> R </strong>值来确定我们的回归模型实际上有多好或者有用。要做到这一点，我们首先要找到经验的平均值，计算每个数据点的平均值与值之间的差异，将其平方，然后将所有这些值相加。我们称此为<strong class="ih hj"> SS(均值)</strong>，即均值周围的平方和。在数学上，我们可以将这种计算表示如下(其中n是我们的样本大小):</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mo"><img src="../Images/4354ffbe884f1c70b473b241f9a40caa.png" data-original-src="https://miro.medium.com/v2/resize:fit:704/format:webp/0*yBbhkViNHbl8PxM6.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es mp"><img src="../Images/8968ad037aeb2b38ef7fbe4146f6b228.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mByWLHMRnX9fNBM-.png"/></div></div></figure></div><div class="ab cl mb mc gp md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="hb hc hd he hf"><p id="892d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们回到最初的<em class="jd">工资与经验</em>图，该图描绘了我们数据的最佳拟合线。正如我们之前所做的，我们计算<strong class="ih hj"> SS(fit) </strong>，即围绕最佳拟合线的平方和:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mq"><img src="../Images/0e3d4224a552a89de08ba9168d54fd64.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/0*AnsBMJd1Dbl9Vjsa.png"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mr"><img src="../Images/11110cddc5eff0b9461078ce96856650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1058/format:webp/0*X7-B4FORsXdFduk8.png"/></div></figure><p id="d02e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一般来说，我们可以以更抽象的形式查看一些数据的方差，如下所示:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ms"><img src="../Images/994e6ead5f5e5666de232a6272bd6ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/0*z4kPbxszEQytdVVh.png"/></div></figure><p id="a014" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在所有这些公式中，事实上有一个<em class="jd">模式</em>值得注意。如果我们仔细观察，就会发现SS(均值)的值总是大于SS(拟合)的值。考虑到SS(拟合)描绘了最佳拟合线的事实，即它使平方和最小化，这并不奇怪。因此，可以恰当地说，R值可以告诉我们，考虑到员工的经历，工资的变化有多大。在数学上，我们得到以下结果:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mt"><img src="../Images/cefe787e009fc71c68d4395f9570d262.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/0*Nbtjew7bzkeG2uRl.png"/></div></figure><p id="3fdc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，通过去除<em class="jd"> n </em>(样本大小)，该公式也可以写成如下形式:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mu"><img src="../Images/6f58889e00c1a91e136c8df87068b149.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/0*ihuxiqXB-p2qcuOo.png"/></div></figure><p id="34e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模型的R值介于负无穷大和1之间。我们可以说，我们的值越接近1，我们的自变量在解释因变量的方差方面就越好。</p><p id="f6c3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，假设我们的数据的R值为0.75。这意味着当我们考虑员工的经历时，差异减少了75%。或者，我们可以说员工的经历可以解释75%的工资差异。</p><blockquote class="mv mw mx"><p id="6cdc" class="if ig jd ih b ii ij ik il im in io ip my ir is it mz iv iw ix na iz ja jb jc hb bi translated"><strong class="ih hj">注意:</strong>在分析回归模型的R值时，需要考虑的一个重要事实是，如果您继续向模型中添加更多的特征，R值会一直增加。因此，如果你发现自己在没有任何修补的情况下获得了0.95或更高的R值，那么你应该对你的结果持保留态度。</p></blockquote><p id="f564" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">还有许多其他指标来评估我们的回归模型，如<em class="jd">平均绝对误差</em>、<em class="jd">均方误差</em>，以及<em class="jd">调整后的R值</em>。我们将在另一篇博文中详细讨论这些。</p></div><div class="ab cl mb mc gp md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="hb hc hd he hf"><p id="c2b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们得到了一个对我们来说似乎很大的R值，但是我们怎么知道这个值是否正确呢？为了确定我们的R值是否具有统计显著性，我们需要计算p值。使用称为F的东西来计算<em class="jd"> p </em>值，如下所示:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es nb"><img src="../Images/2d65bbe67d98c5fbd460818973d610c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/0*HBFaJvQ7qP2JVdTq.png"/></div></figure><p id="8983" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然这个等式乍一看似乎令人困惑，但分子仅仅表示我们考虑经验后方差的减少，分母表示下图中残差的变化(用虚线表示):</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mn"><img src="../Images/6ea7f64eecb3d3c7479b8d4727f9eb17.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*_AckcDg_g8Auk84rekLa7Q.png"/></div></figure><p id="89e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数学上，F通过以下公式计算:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es nc"><img src="../Images/4b6c7a796a34bfd3e3eaec542434c1d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/0*oG5Z0eq10BDxWchP.png"/></div></figure><p id="bf3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在哪里</p><ul class=""><li id="f114" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated">分母<strong class="ih hj"> (p_fit-p_mean) </strong>和<strong class="ih hj"> (n-p_fit) </strong>代表自由度</li><li id="097c" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated"><strong class="ih hj"> p_fit </strong>是拟合线中的参数个数</li><li id="e92b" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated"><strong class="ih hj"> p_mean </strong>是平均线中的参数个数</li></ul><p id="411e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本质上，分子变成了由任何额外参数解释的方差，分母是找到最佳拟合线后残差的平方和。因此，如果拟合度很好，那么F就是一个很大的数字。现在，为了将F的值转换成p的值，我们采取以下步骤:</p><ol class=""><li id="86e0" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc nd lt lu lv bi translated">生成一组随机数据</li><li id="fb93" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc nd lt lu lv bi translated">计算平均值和SS(平均值)</li><li id="67ca" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc nd lt lu lv bi translated">计算拟合和SS(拟合)</li><li id="00be" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc nd lt lu lv bi translated">将所有这些值代入方程，求出F</li><li id="f3a1" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc nd lt lu lv bi translated">将该值绘制在直方图中</li><li id="d2a7" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc nd lt lu lv bi translated">重复很多很多次</li></ol><p id="02f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们重复这个过程数千次(甚至数百万次)，我们就可以计算出最佳拟合线的F值。然后，通过更多极值的计数除以值的总数，获得<em class="jd"> p </em>值。</p><p id="3050" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，如果我们的最佳拟合线的F值是5，并且我们在100个实例中有6个实例大于或等于5，那么我们的<em class="jd">p</em>-值将是6/100 = 0.06</p><p id="e27c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实际上，我们并不经常遵循这个过程来生成p值，因为这非常耗时。相反，我们通过使用f分布用一条线来近似直方图。</p></div><div class="ab cl mb mc gp md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="hb hc hd he hf"><h1 id="e9bd" class="jv jw hi bd jx jy mi ka kb kc mj ke kf kg mk ki kj kk ml km kn ko mm kq kr ks bi translated">OLS方法的假设</h1><p id="4bf7" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">既然我们已经知道了OLS方法是如何工作的，我们也应该知道在这个方法中有哪些潜在的假设。线性回归有七个经典的OLS假设。在这些假设中，前六个是产生一个好的模型所必需的，而最后一个假设主要用于分析。</p><ol class=""><li id="38c5" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc nd lt lu lv bi translated"><strong class="ih hj">回归模型是线性的— </strong>这意味着模型中的项要么是常数，要么是一个参数乘以一个独立变量，我们的模型仅限于我们之前讨论过的通用方程。</li><li id="d10a" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc nd lt lu lv bi translated"><strong class="ih hj">误差项的总体为零— </strong>误差项描述了自变量无法解释的因变量的变化。我们只想把随机误差留给我们的误差项，即误差项应该是不可预测的。</li><li id="4a72" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc nd lt lu lv bi translated"><strong class="ih hj">所有自变量都与误差项不相关— </strong>如果自变量与误差项相关，那么我们可以用自变量来预测误差项。这对于我们的回归模型来说不应该是真的，因为它违背了误差项在本质上是不可预测的这一概念。这种假设通常被称为<strong class="ih hj"><em class="jd"/></strong>。</li><li id="7a34" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc nd lt lu lv bi translated"><strong class="ih hj">误差项的观测值是不相关的— </strong>我们的误差项应该具有随机性，因此误差项的一个观测值不能预测下一个观测值。</li><li id="1c56" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc nd lt lu lv bi translated"><strong class="ih hj">误差项具有恒定的方差— </strong>误差的方差对于所有观测值应该是一致的。如果方差对于每个观察值或一系列观察值不变，则称为<strong class="ih hj"> <em class="jd">同方差</em> </strong>，这是我们的回归模型所需要的。另一方面，<em class="jd">异方差</em>降低了我们在OLS线性回归中估计的精度。</li><li id="178e" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc nd lt lu lv bi translated"><strong class="ih hj">没有一个自变量是另一个自变量的完美线性函数——</strong>当两个变量的<em class="jd">皮尔逊相关系数</em>为+1或-1时，完美相关存在。这意味着，如果我们增加一个变量，那么另一个变量也会增加(当相关性为+1时)，如果我们增加一个变量，那么另一个变量会减少(当相关性为-1时)。如果两个变量完全相关，普通的最小二乘法不能区分它们，这将在我们的模型中引起误差。这种假设被称为<strong class="ih hj"> <em class="jd">多重共线性</em> </strong>。</li><li id="71a2" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc nd lt lu lv bi translated"><strong class="ih hj">误差项呈正态分布— </strong>虽然这不是必要条件，但如果满足，这可以帮助我们生成可靠的置信区间和预测区间。如果我们需要计算系数估计的<em class="jd"> p </em>值，这个假设也非常有用。</li></ol></div><div class="ab cl mb mc gp md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="hb hc hd he hf"><h1 id="a013" class="jv jw hi bd jx jy mi ka kb kc mj ke kf kg mk ki kj kk ml km kn ko mm kq kr ks bi translated">正在总结…</h1><p id="e17e" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">在这篇博文中，我们学习了不同类型的线性回归，以及如何使用普通最小二乘法(OLS)来实现。除此之外，我们还讨论了在实现OLS线性回归时回归模型所做的假设。</p><p id="2029" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">感谢阅读，敬请期待更多！</em> </strong></p></div><div class="ab cl mb mc gp md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="hb hc hd he hf"><p id="9b12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">资源:</em> </strong></p><ul class=""><li id="6798" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated"><a class="ae ju" href="https://www.youtube.com/watch?v=nk2CQITm_eo&amp;t=268s" rel="noopener ugc nofollow" target="_blank"> <em class="jd">与Josh Starmer的stat quest</em></a></li><li id="ead9" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated"><a class="ae ju" href="https://www.udemy.com/course/machinelearning/" rel="noopener ugc nofollow" target="_blank"> <em class="jd">机器学习A-Z:数据科学中的动手Python&amp;R</em></a></li><li id="b4b6" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated"><a class="ae ju" href="https://statisticsbyjim.com/regression/ols-linear-regression-assumptions/" rel="noopener ugc nofollow" target="_blank"> <em class="jd">吉姆统计</em> </a></li></ul></div></div>    
</body>
</html>