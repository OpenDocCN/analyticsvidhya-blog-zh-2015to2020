<html>
<head>
<title>Hierarchical Attention Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">åˆ†å±‚æ³¨æ„ç½‘ç»œ</h1>
<blockquote>åŸæ–‡ï¼š<a href="https://medium.com/analytics-vidhya/hierarchical-attention-networks-d220318cf87e?source=collection_archive---------0-----------------------#2018-08-24">https://medium.com/analytics-vidhya/hierarchical-attention-networks-d220318cf87e?source=collection_archive---------0-----------------------#2018-08-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="8242" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">æœ€äººæ€§åŒ–çš„æ–‡æœ¬åˆ†ç±»æ–¹å¼</h2></div><h2 id="c9c2" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">è¿™äº›å…³äºæ–‡æœ¬åˆ†ç±»çš„å®£ä¼ æ˜¯æ€ä¹ˆå›äº‹ï¼Ÿ</h2><p id="e342" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">è‡ªä»äººå·¥æ™ºèƒ½å…´èµ·ä»¥æ¥ï¼Œæ–‡æœ¬åˆ†ç±»å·²ç»æˆä¸ºæœ€ä»¤äººéœ‡æƒŠçš„ä»»åŠ¡ä¹‹ä¸€ã€‚é€šä¿—åœ°è¯´ï¼Œæˆ‘ä»¬å¯ä»¥è¯´äººå·¥æ™ºèƒ½æ˜¯è¯•å›¾å®ç°ç±»ä¼¼äººç±»çš„æ™ºèƒ½æ¨¡å‹æ¥å‡è½»æˆ‘ä»¬æ‰€æœ‰äººçš„å·¥ä½œçš„é¢†åŸŸã€‚æˆ‘ä»¬åœ¨æ–‡æœ¬åˆ†ç±»æ–¹é¢æœ‰ç€æƒŠäººçš„ç†Ÿç»ƒç¨‹åº¦ï¼Œä½†å³ä½¿æ˜¯è®¸å¤šå¤æ‚çš„NLPæ¨¡å‹ä¹Ÿæ— æ³•è¾¾åˆ°ç”šè‡³æ¥è¿‘å®ƒçš„ç†Ÿç»ƒç¨‹åº¦ã€‚æ‰€ä»¥é—®é¢˜æ¥äº†ï¼Œæˆ‘ä»¬äººç±»åšçš„æœ‰ä»€ä¹ˆä¸åŒï¼Ÿæˆ‘ä»¬å¦‚ä½•å¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Ÿ</p><p id="8b67" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">é¦–å…ˆï¼Œæˆ‘ä»¬ç†è§£å•è¯ä¸æ˜¯æ¯ä¸ªå•è¯ï¼Œè€Œæ˜¯è®¸å¤šå•è¯ï¼Œæˆ‘ä»¬ç”šè‡³å¯ä»¥é€šè¿‡å¥å­çš„ç»“æ„æ¥çŒœæµ‹æœªçŸ¥çš„å•è¯ã€‚ç„¶åæˆ‘ä»¬å°±ç†è§£äº†é‚£ä¸€ç³»åˆ—å•è¯(å¥å­)æ‰€ä¼ è¾¾çš„ä¿¡æ¯ã€‚ç„¶åä»é‚£ä¸€ç³»åˆ—çš„å¥å­ä¸­ï¼Œæˆ‘ä»¬ç†è§£ä¸€æ®µè¯æˆ–è€…ä¸€ç¯‡æ–‡ç« çš„æ„æ€ã€‚ç±»ä¼¼çš„æ–¹æ³•ä¹Ÿç”¨äºåˆ†å±‚æ³¨æ„åŠ›æ¨¡å‹ã€‚</p><h2 id="d236" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">é‚£ä¹ˆè¿™ä¸ªåˆ†ç­‰çº§çš„ä¸œè¥¿æœ‰ä»€ä¹ˆç‰¹åˆ«çš„å‘¢ï¼Ÿ</h2><p id="dd2f" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">ç”¨ä¸€ç§â€œå¤ªå¤æ‚ä»¥è‡³äºæŠ€æœ¯äººå‘˜éƒ½æ— æ³•ç†è§£â€çš„æ–¹å¼æ¥è¯´ï¼Œå®ƒåœ¨å•è¯çº§åˆ«ä¸Šä½¿ç”¨å †å çš„é€’å½’ç¥ç»ç½‘ç»œï¼Œç„¶åä½¿ç”¨æ³¨æ„åŠ›æ¨¡å‹æ¥æå–å¯¹å¥å­æ„ä¹‰é‡è¦çš„å•è¯ï¼Œå¹¶èšé›†è¿™äº›ä¿¡æ¯æ€§å•è¯çš„è¡¨ç¤ºæ¥å½¢æˆå¥å­å‘é‡ã€‚ç„¶åå°†ç›¸åŒçš„è¿‡ç¨‹åº”ç”¨äºå¯¼å‡ºçš„å¥å­å‘é‡ï¼Œç„¶åç”Ÿæˆä¸€ä¸ªå‘é‡ï¼Œè¯¥å‘é‡ç†è§£ç»™å®šæ–‡æ¡£çš„å«ä¹‰ï¼Œå¹¶ä¸”è¯¥å‘é‡å¯ä»¥è¢«è¿›ä¸€æ­¥ä¼ é€’ç”¨äºæ–‡æœ¬åˆ†ç±»ã€‚</p><h2 id="9db3" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">ç­‰ç­‰â€¦ä»€ä¹ˆï¼Ÿ</h2><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es kt"><img src="../Images/79bf0429dc9eb9840093a63062516d9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*28XVtq2lOjOmZhcSgu1NmQ.png"/></div></div><figcaption class="lf lg et er es lh li bd b be z dx translated">æ±‰æ—ç»“æ„</figcaption></figure><p id="45a7" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">è®ºæ–‡èƒŒåçš„æ€æƒ³æ˜¯â€œè¯é€ å¥ï¼Œå¥é€ æ–‡â€ã€‚ç›®çš„æ˜¯ä»å•è¯ä¸­æ¨å¯¼å‡ºå¥å­çš„æ„æ€ï¼Œç„¶åä»è¿™äº›å¥å­ä¸­æ¨å¯¼å‡ºæ–‡æ¡£çš„æ„æ€ã€‚ä½†å¹¶ä¸æ˜¯æ‰€æœ‰çš„å•è¯éƒ½åŒæ ·é‡è¦ã€‚å®ƒä»¬ä¸­çš„ä¸€äº›æ¯”å…¶ä»–çš„æ›´èƒ½æè¿°ä¸€ä¸ªå¥å­ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨æ³¨æ„æ¨¡å‹ï¼Œä»¥ä¾¿å¥å­å‘é‡èƒ½å¤Ÿæ›´å¤šåœ°æ³¨æ„â€œé‡è¦â€çš„å•è¯ã€‚æ³¨æ„æ¨¡å‹ç”±ä¸¤éƒ¨åˆ†ç»„æˆ:åŒå‘RNNå’Œæ³¨æ„ç½‘ç»œã€‚åŒå‘RNNå­¦ä¹ è¿™äº›å•è¯åºåˆ—èƒŒåçš„å«ä¹‰ï¼Œå¹¶è¿”å›å¯¹åº”äºæ¯ä¸ªå•è¯çš„å‘é‡ï¼Œè€Œæ³¨æ„åŠ›ç½‘ç»œä½¿ç”¨å…¶è‡ªå·±çš„æµ…å±‚ç¥ç»ç½‘ç»œæ¥è·å¾—å¯¹åº”äºæ¯ä¸ªå•è¯å‘é‡çš„æƒé‡ã€‚ç„¶åï¼Œå®ƒèšé›†è¿™äº›å•è¯çš„è¡¨ç¤ºæ¥å½¢æˆå¥å­å‘é‡ï¼Œå³å®ƒè®¡ç®—æ¯ä¸ªå‘é‡çš„åŠ æƒå’Œã€‚è¿™ä¸ªåŠ æƒå’Œä½“ç°äº†æ•´ä¸ªå¥å­ã€‚åŒæ ·çš„è¿‡ç¨‹ä¹Ÿé€‚ç”¨äºå¥å­å‘é‡ï¼Œå› æ­¤æœ€ç»ˆçš„å‘é‡ä½“ç°äº†æ•´ä¸ªæ–‡æ¡£çš„ä¸»æ—¨ã€‚ç”±äºå®ƒæœ‰ä¸¤ä¸ªå±‚æ¬¡çš„æ³¨æ„æ¨¡å‹ï¼Œå› æ­¤ï¼Œå®ƒè¢«ç§°ä¸ºåˆ†å±‚æ³¨æ„ç½‘ç»œã€‚</p><h2 id="ff8c" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">è¯´å¤Ÿäº†â€¦ç»™æˆ‘çœ‹çœ‹ä»£ç </h2><p id="4798" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">æˆ‘ä»¬ä½¿ç”¨<a class="ae lj" href="https://www.kaggle.com/rmisra/news-category-dataset" rel="noopener ugc nofollow" target="_blank">æ–°é—»ç±»åˆ«æ•°æ®é›†</a>å¯¹æ–°é—»ç±»åˆ«è¿›è¡Œåˆ†ç±»ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°æ•´ä¸ªå®ç°<a class="ae lj" href="https://www.kaggle.com/hsankesara/news-classification-using-han/notebook" rel="noopener ugc nofollow" target="_blank">ã€‚ç°åœ¨ç¬¬ä¸€ä¸ªå‡ºç°åœ¨è„‘æµ·ä¸­çš„é—®é¢˜æ˜¯æ³¨æ„åŠ›åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ</a></p><h2 id="571f" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">æ³¨æ„åŠ›æ¨¡å‹</h2><figure class="ku kv kw kx fd ky er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es lk"><img src="../Images/00f2946f808bb707f886fa3cd4e77ab0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABkaR2glZNP6oh08oY4l-Q.png"/></div></div></figure><p id="49cf" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">æ¥è‡ªåŒå‘RNNçš„å‘é‡é€šè¿‡æµ…å±‚ç¥ç»ç½‘ç»œæ¥ç¡®å®šæ¯ä¸ªå‘é‡å¯¹åº”çš„æƒé‡ã€‚æ¯ä¸ªå‘é‡çš„åŠ æƒå’Œä½“ç°äº†è¿™äº›å‘é‡ç»„åˆçš„å«ä¹‰ã€‚è¦æ›´ç®€å•åœ°ç†è§£å®ƒï¼Œåªéœ€è¿›å…¥<a class="ae lj" href="https://github.com/Hsankesara/DeepResearch/blob/master/Hierarchical_Attention_Network/attention_with_context.py" rel="noopener ugc nofollow" target="_blank">ä»£ç </a>ã€‚</p><p id="3a71" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated"><strong class="jx hj">æ•°æ®é¢„å¤„ç†</strong></p><p id="8a04" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">ä¸ºäº†å¤„ç†æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦æŠŠå®ƒè½¬æ¢æˆåˆé€‚çš„å½¢å¼ã€‚</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="ed25" class="ix iy hi lm b fi lq lr l ls lt">tokenizer = Tokenizer(num_words=max_features, oov_token=True)<br/>tokenizer.fit_on_texts(texts)<br/>data = np.zeros((len(texts), max_senten_num, max_senten_len), dtype='int32')<br/>for i, sentences in enumerate(paras):<br/>    for j, sent in enumerate(sentences):<br/>        if j&lt; max_senten_num:<br/>            wordTokens = text_to_word_sequence(sent)<br/>            k=0<br/>            for _, word in enumerate(wordTokens):<br/>                try:<br/>                    if k&lt;max_senten_len and tokenizer.word_index[word]&lt;max_features:<br/>                        data[i,j,k] = tokenizer.word_index[word]<br/>                        k=k+1<br/>                except:<br/>                    print(word)<br/>                    pass</span></pre><p id="aacb" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">æˆ‘ä»¬ä½¿ç”¨ä¸Šé¢çš„ä»£ç å°†è®­ç»ƒæ•°æ®é›†è½¬æ¢ä¸ºä¸‰ç»´æ•°ç»„:ç¬¬ä¸€ç»´è¡¨ç¤ºæ–‡æ¡£æ€»æ•°ï¼Œç¬¬äºŒç»´è¡¨ç¤ºæ–‡æ¡£ä¸­çš„æ¯ä¸ªå¥å­ï¼Œæœ€åä¸€ç»´è¡¨ç¤ºå¥å­ä¸­çš„æ¯ä¸ªå•è¯ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¿…é¡»è®¾ç½®ä¸€äº›ä¸Šé™ï¼Œä»¥ä¾¿åˆ›å»ºä¸€ä¸ªé™æ€å›¾ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯<code class="du lu lv lw lm b">max_senten_len</code>(ä¸€ä¸ªæ®µè½ä¸­å¥å­çš„æœ€å¤§æ•°é‡)<code class="du lu lv lw lm b">max_senten_num</code>(ä¸€ä¸ªå¥å­ä¸­çš„æœ€å¤§å­—æ•°)å’Œ<code class="du lu lv lw lm b">max_features</code>(åˆ†è¯å™¨å¯ä»¥æ‹¥æœ‰çš„æœ€å¤§å­—æ•°)ã€‚</p><p id="5329" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">å¦‚æœæˆ‘ä»¬éšæœºåˆå§‹åŒ–æ‰€æœ‰çš„å•è¯ï¼Œæ˜¯ä¸æ˜¯å¯¹æ¨¡å‹ä¸å…¬å¹³ï¼Ÿå› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨<a class="ae lj" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">è®­ç»ƒè¿‡çš„åµŒå…¥å‘é‡</a>ï¼Œè¿™åœ¨æ€§èƒ½æ–¹é¢ç»™äºˆæ¨¡å‹é¢å¤–çš„ä¼˜åŠ¿ï¼Œå¹¶äº§ç”Ÿæ›´å¥½çš„ç»“æœã€‚</p><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="cb11" class="ix iy hi lm b fi lq lr l ls lt">GLOVE_DIR = "../input/glove6b/glove.6B.100d.txt"<br/>embeddings_index = {}<br/>f = open(GLOVE_DIR)<br/>for line in f:<br/>    try:<br/>        values = line.split()<br/>        word = values[0]<br/>        coefs = np.asarray(values[1:], dtype='float32')<br/>        embeddings_index[word] = coefs<br/>    except:<br/>        print(word)<br/>        pass<br/>f.close()<br/>embedding_matrix = np.zeros((len(word_index) + 1, embed_size))<br/>absent_words = 0<br/>for word, i in word_index.items():<br/>    embedding_vector = embeddings_index.get(word)<br/>    if embedding_vector is not None:<br/>        # words not found in embedding index will be all-zeros.<br/>        embedding_matrix[i] = embedding_vector<br/>    else:<br/>        absent_words += 1<br/>print('Total absent words are', absent_words, 'which is', "%0.2f" % (absent_words * 100 / len(word_index)), '% of total words')</span></pre><p id="cd07" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">æˆ‘ä»¬åœ¨<code class="du lu lv lw lm b">embedding_matrix</code>ä¸­å°†å·²çŸ¥å•è¯æ›¿æ¢ä¸ºå…¶å¯¹åº”çš„å‘é‡ã€‚çš„ç¡®ï¼Œæœ‰äº›è¯ä¼šè¢«é—æ¼ï¼Œä½†æˆ‘ä»¬çš„æ¨¡å‹å¿…é¡»å­¦ä¼šåº”å¯¹è¿™ç§æƒ…å†µã€‚</p><h2 id="8218" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">ç°åœ¨æ˜¯éŸ©æ¨¡å‹çš„æ—¶å€™äº†</h2><pre class="ku kv kw kx fd ll lm ln lo aw lp bi"><span id="42c2" class="ix iy hi lm b fi lq lr l ls lt">embedding_layer = Embedding(len(word_index) + 1,embed_size,weights=[embedding_matrix], input_length=max_senten_len, trainable=False)</span><span id="0838" class="ix iy hi lm b fi lx lr l ls lt"># Words level attention model<br/>word_input = Input(shape=(max_senten_len,), dtype='float32')<br/>word_sequences = embedding_layer(word_input)<br/>word_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(word_sequences)<br/>word_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(word_lstm)<br/>word_att = AttentionWithContext()(word_dense)<br/>wordEncoder = Model(word_input, word_att)</span><span id="8ae4" class="ix iy hi lm b fi lx lr l ls lt"># Sentence level attention model<br/>sent_input = Input(shape=(max_senten_num, max_senten_len), dtype='float32')<br/>sent_encoder = TimeDistributed(wordEncoder)(sent_input)<br/>sent_lstm = Bidirectional(LSTM(150, return_sequences=True, kernel_regularizer=l2_reg))(sent_encoder)<br/>sent_dense = TimeDistributed(Dense(200, kernel_regularizer=l2_reg))(sent_lstm)<br/>sent_att = Dropout(0.5)(AttentionWithContext()(sent_dense))<br/>preds = Dense(30, activation='softmax')(sent_att)<br/>model = Model(sent_input, preds)<br/>model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['acc'])</span></pre><p id="0564" class="pw-post-body-paragraph jv jw hi jx b jy ko ij ka kb kp im kd ji kq kf kg jm kr ki kj jq ks kl km kn hb bi translated">å¯¹äºword2vecï¼Œæˆ‘ä»¬ä½¿ç”¨äº†Keras <a class="ae lj" href="https://keras.io/layers/embeddings/" rel="noopener ugc nofollow" target="_blank">åµŒå…¥</a>å±‚ã€‚<a class="ae lj" href="https://keras.io/layers/wrappers/" rel="noopener ugc nofollow" target="_blank">æ—¶é—´åˆ†å¸ƒ</a>æ–¹æ³•ç”¨äºå°†<code class="du lu lv lw lm b">Dense</code>å±‚ç‹¬ç«‹åº”ç”¨äºæ¯ä¸ªæ—¶é—´æ­¥ã€‚æˆ‘ä»¬ä½¿ç”¨<code class="du lu lv lw lm b">Dropout</code>å’Œ<code class="du lu lv lw lm b">l2_reg</code>æ­£åˆ™åŒ–æ¥å‡å°‘è¿‡åº¦æ‹Ÿåˆã€‚</p><h2 id="dd48" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">ç»“è®º</h2><p id="f033" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">ä½ ç°åœ¨ä¸€å®šå¾ˆæƒŠè®¶æˆ–è€…å¾ˆå›°æƒ‘ã€‚æœ‰æ—¶è¿™äº›äº‹æƒ…å¯èƒ½ä¼šè¿‡å¤´ï¼Œä½†æ–‡æœ¬åˆ†ç±»æ˜¯ä¸€ä¸ªè¶‹åŠ¿é¢†åŸŸï¼Œå°½ç®¡æœ‰è®¸å¤šæ–°çš„å’Œå¤šäº§çš„ç ”ç©¶ï¼Œæ”¹è¿›çš„èŒƒå›´æ˜¯å¦‚æ­¤ä¹‹å¤§ã€‚æ‰€ä»¥ä¸è¦åªåœ¨ç°åœ¨ç»æœ›ï¼Œå› ä¸ºä½ ä»¥åä¼šæœ‰æ›´å¤šçš„å¤±æœ›ğŸ˜…ã€‚å¼€ç©ç¬‘ï¼Œå¦‚æœä½ æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·åœ¨ä¸‹é¢è¯„è®ºæˆ–å‚è€ƒèµ„æºé¡µé¢ã€‚</p><h2 id="6949" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">èµ„æº</h2><ol class=""><li id="d466" class="ly lz hi jx b jy jz kb kc ji ma jm mb jq mc kn md me mf mg bi translated">å‰å¾€<a class="ae lj" href="https://github.com/Hsankesara/DeepResearch/tree/master/Hierarchical_Attention_Network" rel="noopener ugc nofollow" target="_blank">æ­¤å¤„</a>æŸ¥çœ‹ä»£ç ã€‚</li><li id="0745" class="ly lz hi jx b jy mh kb mi ji mj jm mk jq ml kn md me mf mg bi translated">å»<a class="ae lj" href="https://www.kaggle.com/hsankesara/news-classification-using-han/notebook" rel="noopener ugc nofollow" target="_blank">è¿™é‡Œ</a>ç»“è´¦å®ç°ã€‚</li><li id="f0a0" class="ly lz hi jx b jy mh kb mi ji mj jm mk jq ml kn md me mf mg bi translated"><a class="ae lj" href="https://www.ncbi.nlm.nih.gov/pubmed/29155996" rel="noopener ugc nofollow" target="_blank">ç”¨äºä»ç™Œç—‡ç—…ç†å­¦æŠ¥å‘Šä¸­æå–ä¿¡æ¯çš„åˆ†çº§æ³¨æ„ç½‘ç»œã€‚</a></li><li id="bc16" class="ly lz hi jx b jy mh kb mi ji mj jm mk jq ml kn md me mf mg bi translated"><a class="ae lj" href="http://www.cs.cmu.edu/~./hovy/papers/16HLT-hierarchical-attention-networks.pdf" rel="noopener ugc nofollow" target="_blank">ç”¨äºæ–‡æ¡£åˆ†ç±»çš„åˆ†å±‚æ³¨æ„åŠ›ç½‘ç»œ</a></li></ol><h1 id="bc4d" class="mm iy hi bd iz mn mo mp jd mq mr ms jh io mt ip jl ir mu is jp iu mv iv jt mw bi translated">ä½œè€…è¯´æ˜</h1><p id="bcb3" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ji ke kf kg jm kh ki kj jq kk kl km kn hb bi translated">æœ¬æ•™ç¨‹æ˜¯æˆ‘çš„ç³»åˆ—æ–‡ç« <a class="ae lj" href="https://github.com/Hsankesara/DeepResearch" rel="noopener ugc nofollow" target="_blank"> DeepResearch </a>çš„ç¬¬ä¸€ç¯‡ã€‚å¦‚æœä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ï¼Œè¯·åœ¨è¯„è®ºä¸­å‘Šè¯‰æˆ‘ï¼Œå¦‚æœä½ ä¸å–œæ¬¢ï¼Œè¯·åœ¨è¯„è®ºä¸­ç®€å•å‘Šè¯‰æˆ‘ã€‚å¦‚æœä½ æœ‰ä»»ä½•ç–‘é—®æˆ–æ‰¹è¯„ï¼Œè¯·åœ¨è¯„è®ºä¸­å¤§é‡å‘è¡¨ã€‚æˆ‘ä¼šå°½å¿«å›å¤ä½ çš„ã€‚å¦‚æœä½ å–œæ¬¢è¿™ä¸ªæ•™ç¨‹ï¼Œè¯·ä¸ä½ çš„åŒä¼´åˆ†äº«ã€‚</p></div></div>    
</body>
</html>