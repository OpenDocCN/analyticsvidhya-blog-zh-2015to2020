# è‰¾å†™è¯—:ç”¨ç«ç‚¬æ„ç­‘æ¨¡å¼

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/ai-writing-poems-building-lstm-model-using-pytorch-d1c58a24bb64?source=collection_archive---------8----------------------->

![](img/948265820c2fc9979798748bebc50d6a.png)

å¤§å®¶å¥½ï¼ï¼åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ PyTorch æ„å»ºä¸€ä¸ªæ¨¡å‹æ¥é¢„æµ‹æ®µè½ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å°†äº†è§£ RNN å’Œ LSTMï¼Œä»¥åŠä»–ä»¬æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚ç„¶åæˆ‘ä»¬å°†åˆ›å»ºæˆ‘ä»¬çš„æ¨¡å‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åŠ è½½æ•°æ®å¹¶å¯¹å…¶è¿›è¡Œé¢„å¤„ç†ã€‚ç„¶åæˆ‘ä»¬å°†ä½¿ç”¨ PyTorch æ¥è®­ç»ƒæ¨¡å‹å¹¶ä¿å­˜å®ƒã€‚åœ¨é‚£ä¹‹åï¼Œæˆ‘ä»¬å°†é€šè¿‡ç»™å®ƒä¸€ä¸ªèµ·å§‹æ–‡æœ¬æ¥ä»è¯¥æ¨¡å‹ä¸­è¿›è¡Œé¢„æµ‹ï¼Œå¹¶ä½¿ç”¨å®ƒæ¥ç”Ÿæˆå®Œæ•´çš„æ®µè½ã€‚

# ä»€ä¹ˆæ˜¯ RNNï¼Ÿ

åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œåƒå¯¹ç‹—æˆ–çŒ«çš„å›¾åƒè¿›è¡Œåˆ†ç±»è¿™æ ·çš„ç®€å•é—®é¢˜å¯ä»¥é€šè¿‡ç®€å•åœ°å¯¹æ•°æ®é›†è¿›è¡Œè®­ç»ƒå¹¶ä½¿ç”¨åˆ†ç±»å™¨æ¥è§£å†³ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬çš„é—®é¢˜æ›´å¤æ‚ï¼Œæ¯”å¦‚æˆ‘ä»¬å¿…é¡»é¢„æµ‹æ®µè½ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬ä»”ç»†ç ”ç©¶è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä¼šå‘ç°ï¼Œæˆ‘ä»¬äººç±»ä¸èƒ½ä»…ä»…é€šè¿‡ä½¿ç”¨æˆ‘ä»¬ç°æœ‰çš„è¯­è¨€å’Œè¯­æ³•çŸ¥è¯†æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚åœ¨è¿™ç§ç±»å‹çš„é—®é¢˜ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ®µè½çš„å‰ä¸€ä¸ªå•è¯å’Œæ®µè½çš„ä¸Šä¸‹æ–‡æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ã€‚

ä¼ ç»Ÿçš„ç¥ç»ç½‘ç»œæ— æ³•åšåˆ°è¿™ä¸€ç‚¹ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åœ¨å›ºå®šçš„æ•°æ®é›†ä¸Šè®­ç»ƒçš„ï¼Œç„¶åç”¨æ¥è¿›è¡Œé¢„æµ‹ã€‚RNN è¢«ç”¨æ¥è§£å†³è¿™ç±»é—®é¢˜ã€‚RNN ä»£è¡¨**é€’å½’ç¥ç»ç½‘ç»œ**ã€‚æˆ‘ä»¬å¯ä»¥æŠŠ RNN æƒ³è±¡æˆä¸€ä¸ªæœ‰å›è·¯çš„ç¥ç»ç½‘ç»œã€‚å®ƒå°†ä¸€ä¸ªçŠ¶æ€çš„ä¿¡æ¯ä¼ é€’ç»™ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚å› æ­¤ï¼Œä¿¡æ¯åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­æŒç»­å­˜åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨å®ƒæ¥ç†è§£ä¹‹å‰çš„èƒŒæ™¯ï¼Œå¹¶åšå‡ºå‡†ç¡®çš„é¢„æµ‹ã€‚

# ä»€ä¹ˆæ˜¯ LSTMï¼Ÿ

å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬é€šè¿‡ä½¿ç”¨ RNN æ¥è§£å†³æ•°æ®åºåˆ—çš„é—®é¢˜ï¼Œè€Œä»¥å‰çš„ä¸Šä¸‹æ–‡æ˜¯è¢«ä½¿ç”¨çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¸ºä»€ä¹ˆéœ€è¦ LSTM å‘¢ï¼Ÿè¦å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¿…é¡»çœ‹çœ‹è¿™ä¸¤ä¸ªä¾‹å­ã€‚

**ä¾‹ä¸€**:â€œé¸Ÿä½åœ¨*å·¢*ã€‚â€è¿™é‡Œå¾ˆå®¹æ˜“é¢„æµ‹å•è¯â€œnest â€,å› ä¸ºæˆ‘ä»¬å·²ç»æœ‰äº† bird çš„ä¸Šä¸‹æ–‡ï¼ŒRNN åœ¨è¿™ç§æƒ…å†µä¸‹ä¼šå·¥ä½œå¾—å¾ˆå¥½ã€‚

**ä¾‹ 2:**

â€œæˆ‘åœ¨å°åº¦é•¿å¤§â€¦â€¦..æ‰€ä»¥æˆ‘ä¼šè¯´ H *indi* â€æ‰€ä»¥è¿™é‡Œé¢„æµ‹å•è¯â€œHindiâ€çš„ä»»åŠ¡å¯¹äºä¸€ä¸ª RNN äººæ¥è¯´æ˜¯å›°éš¾çš„ï¼Œå› ä¸ºè¿™é‡Œè¯­å¢ƒä¹‹é—´çš„å·®è·å¾ˆå¤§ã€‚é€šè¿‡çœ‹â€œæˆ‘èƒ½è¯´â€¦â€¦â€è¿™ä¸€è¡Œï¼Œæˆ‘ä»¬æ— æ³•é¢„æµ‹è¯­è¨€ï¼Œæˆ‘ä»¬å°†éœ€è¦é¢å¤–çš„å°åº¦èƒŒæ™¯ã€‚æ‰€ä»¥åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬éœ€è¦å¯¹æˆ‘ä»¬çš„æ®µè½æœ‰ä¸€äº›é•¿æœŸçš„ä¾èµ–ï¼Œè¿™æ ·æˆ‘ä»¬æ‰èƒ½ç†è§£ä¸Šä¸‹æ–‡ã€‚

ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ LSTM( **é•¿çŸ­æœŸè®°å¿†**)ã€‚é¡¾åæ€ä¹‰ï¼Œå®ƒä»¬æœ‰é•¿æ—¶è®°å¿†å’ŒçŸ­æ—¶è®°å¿†(gate ),ä¸¤è€…éƒ½ç”¨åœ¨è¿è¯ä¸­æ¥åšé¢„æµ‹ã€‚å¦‚æœæˆ‘ä»¬è°ˆè®º LSTM çš„å»ºç­‘ï¼Œå®ƒä»¬åŒ…å« 4 ä¸ªé—¨ï¼Œå³å­¦ä¹ é—¨ã€å¿˜è®°é—¨ã€è®°å¿†é—¨ã€ä½¿ç”¨é—¨ã€‚ä¸ºäº†è®©è¿™ç¯‡æ–‡ç« ç®€å•æ˜“æ‡‚ï¼Œæˆ‘ä¸æ‰“ç®—æ·±å…¥ LSTM çš„å»ºç­‘ç†è®ºã€‚ä½†æ˜¯ä¹Ÿè®¸æˆ‘ä»¬ä¼šåœ¨æ¥ä¸‹æ¥çš„æ–‡ç« ä¸­è®¨è®ºå®ƒã€‚(ä¹Ÿè®¸åœ¨ä¸‹ä¸€éƒ¨ğŸ˜‰).

# è®©æˆ‘ä»¬å»ºç«‹æˆ‘ä»¬çš„æ¨¡å‹ã€‚

ç°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†ç†è®ºï¼Œè®©æˆ‘ä»¬å¼€å§‹æœ‰è¶£çš„éƒ¨åˆ†â€”â€”å»ºç«‹æˆ‘ä»¬çš„æ¨¡å‹ã€‚

# åŠ è½½å’Œé¢„å¤„ç†æ•°æ®

æˆ‘å°†ä½¿ç”¨æ¥è‡ª Kaggle çš„è¯—æ­Œæ•°æ®é›†ã€‚å®ƒæ€»å…±æœ‰ 15ï¼Œ000 é¦–è¯—ï¼Œå› æ­¤å¯¹äºæˆ‘ä»¬æ¨¡å‹å­¦ä¹ å’Œåˆ›å»ºæ¨¡å¼æ¥è¯´è¶³å¤Ÿäº†ã€‚ç°åœ¨è®©æˆ‘ä»¬å¼€å§‹æŠŠå®ƒè½½å…¥æˆ‘ä»¬çš„ç¬”è®°æœ¬ã€‚

1.é¦–å…ˆå¯¼å…¥åº“ã€‚

```
import numpy as np
import torch
from torch import nn
import torch.nn.functional as F
```

2.ç°åœ¨ä»æ–‡æœ¬æ–‡ä»¶åŠ è½½æ•°æ®ã€‚

```
# open text file and read in data as `text`
with open('/data/poems_data.txt', 'r') as f:
    text = f.read()
```

3.æˆ‘ä»¬å¯ä»¥é€šè¿‡æ‰“å°å‰ 100 ä¸ªå­—ç¬¦æ¥éªŒè¯æˆ‘ä»¬çš„æ•°æ®ã€‚

```
text[:100]
```

4.æ­£å¦‚æˆ‘ä»¬æ‰€çŸ¥ï¼Œæˆ‘ä»¬çš„ç¥ç»ç½‘ç»œä¸ç†è§£æ–‡æœ¬ï¼Œæ‰€ä»¥æˆ‘ä»¬å¿…é¡»å°†æˆ‘ä»¬çš„ txt æ•°æ®è½¬æ¢ä¸ºæ•´æ•°ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºä»¤ç‰Œå­—å…¸ï¼Œå¹¶å°†å­—ç¬¦æ˜ å°„åˆ°æ•´æ•°ï¼Œåä¹‹äº¦ç„¶ã€‚

```
# encode the text and map each character to an integer and vice versa# we create two dictionaries:
# 1\. int2char, which maps integers to characters
# 2\. char2int, which maps characters to unique integers
chars = tuple(set(text))
int2char = dict(enumerate(chars))
char2int = {ch: ii for ii, ch in int2char.items()}# encode the text
encoded = np.array([char2int[ch] for ch in text])
```

5.ä¸€ç§çƒ­ç¼–ç ç”¨äºè¡¨ç¤ºå­—ç¬¦ã€‚æ¯”å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸‰ä¸ªå­—ç¬¦ aï¼Œbï¼Œcï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥è¿™æ ·æ¥è¡¨ç¤ºå®ƒä»¬[1ï¼Œ0ï¼Œ0]ï¼Œ[0ï¼Œ1ï¼Œ0]ï¼Œ[0ï¼Œ0ï¼Œ1]è¿™é‡Œæˆ‘ä»¬ç”¨ 1 æ¥è¡¨ç¤ºè¿™ä¸ªå­—ç¬¦ï¼Œå…¶ä»–çš„éƒ½æ˜¯ 0ã€‚å¯¹äºæˆ‘ä»¬çš„ç”¨ä¾‹ï¼Œæˆ‘ä»¬æœ‰è®¸å¤šå­—ç¬¦å’Œç¬¦å·ï¼Œæ‰€ä»¥æˆ‘ä»¬çš„ä¸€ä¸ªçƒ­ç‚¹å‘é‡ä¼šå¾ˆé•¿ã€‚ä½†æ˜¯æ²¡å…³ç³»ã€‚

```
def one_hot_encode(arr, n_labels):

    # Initialize the the encoded array
    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)

    # Fill the appropriate elements with ones
    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.

    # Finally reshape it to get back to the original array
    one_hot = one_hot.reshape((*arr.shape, n_labels))

    return one_hot
```

ç°åœ¨ç”¨è¿™ç§æ–¹æ³•æµ‹è¯•å®ƒã€‚

```
# check that the function works as expected
test_seq = np.array([[0, 5, 1]])
one_hot = one_hot_encode(test_seq, 8)print(one_hot)
```

6.ç°åœ¨æˆ‘ä»¬å¿…é¡»ä¸ºæˆ‘ä»¬çš„æ¨¡å‹åˆ›å»ºæ‰¹æ¬¡ï¼Œè¿™æ˜¯éå¸¸å…³é”®çš„éƒ¨åˆ†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†é€‰æ‹©ä¸€ä¸ªæ‰¹é‡å¤§å°ï¼Œå³è¡Œæ•°ï¼Œç„¶ååºåˆ—é•¿åº¦æ˜¯ä¸€ä¸ªæ‰¹é‡ä¸­è¦ä½¿ç”¨çš„åˆ—æ•°ã€‚

```
def get_batches(arr, batch_size, seq_length):
    '''Create a generator that returns batches of size
       batch_size x seq_length from arr.

       Arguments
       ---------
       arr: Array you want to make batches from
       batch_size: Batch size, the number of sequences per batch
       seq_length: Number of encoded chars in a sequence
    '''

    batch_size_total = batch_size * seq_length
    # total number of batches we can make
    n_batches = len(arr)//batch_size_total

    # Keep only enough characters to make full batches
    arr = arr[:n_batches * batch_size_total]
    # Reshape into batch_size rows
    arr = arr.reshape((batch_size, -1))

    # iterate through the array, one sequence at a time
    for n in range(0, arr.shape[1], seq_length):
        # The features
        x = arr[:, n:n+seq_length]
        # The targets, shifted by one
        y = np.zeros_like(x)
        try:
            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]
        except IndexError:
            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]
        yield x, y
```

7 .ç°åœ¨æˆ‘ä»¬å¯ä»¥æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨ã€‚(å¦‚æœ GPU ä¸å¯ç”¨ï¼Œè¯·ä¿æŒè¾ƒä½çš„ epochs æ•°)

```
# check if GPU is available
train_on_gpu = torch.cuda.is_available()
if(train_on_gpu):
    print('Training on GPU!')
else: 
    print('No GPU available, training on CPU; consider making n_epochs very small.')
```

8.è¿™é‡Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåä¸º CharRNN çš„ç±»ã€‚è¿™æ˜¯æˆ‘ä»¬çš„æ¨¡ç‰¹ç­ã€‚åœ¨ init æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å¿…é¡»ä¸ºæˆ‘ä»¬çš„æ¨¡å‹å®šä¹‰å±‚ã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ª LSTM å±‚ã€‚æˆ‘ä»¬è¿˜ä½¿ç”¨äº† dropout(è¿™æœ‰åŠ©äºé¿å…è¿‡åº¦æ‹Ÿåˆ)ã€‚å¯¹äºè¾“å‡ºï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€å•çš„çº¿æ€§å±‚ã€‚

```
class CharRNN(nn.Module):

    def __init__(self, tokens, n_hidden=256, n_layers=2,
                               drop_prob=0.5, lr=0.001):
        super().__init__()
        self.drop_prob = drop_prob
        self.n_layers = n_layers
        self.n_hidden = n_hidden
        self.lr = lr

        # creating character dictionaries
        self.chars = tokens
        self.int2char = dict(enumerate(self.chars))
        self.char2int = {ch: ii for ii, ch in self.int2char.items()}

        #lstm layer
        self.lstm=nn.LSTM(len(self.chars),n_hidden,n_layers,
                          dropout=drop_prob,batch_first=True)

        #dropout layer
        self.dropout=nn.Dropout(drop_prob)

        #output layer
        self.fc=nn.Linear(n_hidden,len(self.chars)) def forward(self, x, hidden):
        ''' Forward pass through the network. 
            These inputs are x, and the hidden/cell state `hidden`. '''
        ## Get the outputs and the new hidden state from the lstm
        r_output, hidden = self.lstm(x, hidden)

        ## pass through a dropout layer
        out = self.dropout(r_output)

        # Stack up LSTM outputs using view
        # you may need to use contiguous to reshape the output
        out = out.contiguous().view(-1, self.n_hidden)

        ## put x through the fully-connected layer
        out = self.fc(out)
        return out, hidden

    def init_hidden(self, batch_size):
        ''' Initializes hidden state '''
        # Create two new tensors with sizes n_layers x batch_size x n_hidden,
        # initialized to zero, for hidden state and cell state of LSTM
        weight = next(self.parameters()).data

        if (train_on_gpu):
            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),
                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())
        else:
            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),
                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())

        return hidden
```

9.ç°åœ¨æˆ‘ä»¬æœ‰äº†æ¨¡å‹ï¼Œæ˜¯æ—¶å€™è®­ç»ƒæ¨¡å‹äº†ã€‚å¯¹äºè®­ç»ƒï¼Œæˆ‘ä»¬å¿…é¡»ä½¿ç”¨ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ã€‚æˆ‘ä»¬ç®€å•åœ°è®¡ç®—æ¯ä¸€æ­¥åçš„æŸå¤±ï¼Œç„¶åä¼˜åŒ–å™¨é˜¶è·ƒå‡½æ•°å°†å…¶åå‘ä¼ æ’­ï¼Œå¹¶é€‚å½“åœ°æ”¹å˜æƒé‡ã€‚æŸå¤±ä¼šæ…¢æ…¢å‡å°‘ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬çš„æ¨¡å¼æ­£åœ¨å˜å¾—æ›´å¥½ã€‚

æˆ‘ä»¬è¿˜ä½¿ç”¨ä¸¤ä¸ªæ—¶æœŸä¹‹é—´çš„éªŒè¯æ¥è·å¾—éªŒè¯æŸå¤±ï¼Œå› ä¸ºè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å†³å®šæˆ‘ä»¬çš„æ¨¡å‹æ˜¯æ¬ æ‹Ÿåˆè¿˜æ˜¯è¿‡æ‹Ÿåˆã€‚

```
def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):
    ''' Training a network 

        Arguments
        ---------

        net: CharRNN network
        data: text data to train the network
        epochs: Number of epochs to train
        batch_size: Number of mini-sequences per mini-batch, aka batch size
        seq_length: Number of character steps per mini-batch
        lr: learning rate
        clip: gradient clipping
        val_frac: Fraction of data to hold out for validation
        print_every: Number of steps for printing training and validation loss

    '''
    net.train()

    opt = torch.optim.Adam(net.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    # create training and validation data
    val_idx = int(len(data)*(1-val_frac))
    data, val_data = data[:val_idx], data[val_idx:]

    if(train_on_gpu):
        net.cuda()

    counter = 0
    n_chars = len(net.chars)
    for e in range(epochs):
        # initialize hidden state
        h = net.init_hidden(batch_size)

        for x, y in get_batches(data, batch_size, seq_length):
            counter += 1

            # One-hot encode our data and make them Torch tensors
            x = one_hot_encode(x, n_chars)
            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)

            if(train_on_gpu):
                inputs, targets = inputs.cuda(), targets.cuda() # Creating new variables for the hidden state, otherwise
            # we'd backprop through the entire training history
            h = tuple([each.data for each in h]) # zero accumulated gradients
            net.zero_grad()

            # get the output from the model
            output, h = net(inputs, h)

            # calculate the loss and perform backprop
            loss = criterion(output, targets.view(batch_size*seq_length).long())
            loss.backward()
            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
            nn.utils.clip_grad_norm_(net.parameters(), clip)
            opt.step()

            # loss stats
            if counter % print_every == 0:
                # Get validation loss
                val_h = net.init_hidden(batch_size)
                val_losses = []
                net.eval()
                for x, y in get_batches(val_data, batch_size, seq_length):
                    # One-hot encode our data and make them Torch tensors
                    x = one_hot_encode(x, n_chars)
                    x, y = torch.from_numpy(x), torch.from_numpy(y)

                    # Creating new variables for the hidden state, otherwise
                    # we'd backprop through the entire training history
                    val_h = tuple([each.data for each in val_h])

                    inputs, targets = x, y
                    if(train_on_gpu):
                        inputs, targets = inputs.cuda(), targets.cuda() output, val_h = net(inputs, val_h)
                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())

                    val_losses.append(val_loss.item())

                net.train() # reset to train mode after iterationg through validation data

                print("Epoch: {}/{}...".format(e+1, epochs),
                      "Step: {}...".format(counter),
                      "Loss: {:.4f}...".format(loss.item()),
                      "Val Loss: {:.4f}".format(np.mean(val_losses)))
```

ç°åœ¨ç”¨ä¸‹é¢çš„æ–¹æ³•è®­ç»ƒå®ƒã€‚

```
# define and print the net
n_hidden = 512
n_layers = 2net = CharRNN(chars, n_hidden, n_layers)
print(net)batch_size = 128
seq_length = 100
n_epochs =  10# start small if you are just testing initial behavior# train the model
train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)
```

10.æˆ‘ä»¬å¯ä»¥ç”¨ä¸‹é¢çš„æ–¹æ³•ä¿å­˜æ¨¡å‹ã€‚

```
# change the name, for saving multiple files
model_name = 'poem_4_epoch.net'checkpoint = {'n_hidden': net.n_hidden,
              'n_layers': net.n_layers,
              'state_dict': net.state_dict(),
              'tokens': net.chars}with open(model_name, 'wb') as f:
    torch.save(checkpoint, f)
```

11.æ—¢ç„¶æ¨¡å‹å·²ç»è®­ç»ƒå¥½äº†ï¼Œæˆ‘ä»¬å°±è¦ä»ä¸­å–æ ·å¹¶é¢„æµ‹ä¸‹ä¸€ä¸ªè§’è‰²ï¼ä¸ºäº†é‡‡æ ·ï¼Œæˆ‘ä»¬ä¼ å…¥ä¸€ä¸ªå­—ç¬¦ï¼Œè®©ç½‘ç»œé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚ç„¶åæˆ‘ä»¬æŠŠé‚£ä¸ªå­—ç¬¦ä¼ å›æ¥ï¼Œå¾—åˆ°å¦ä¸€ä¸ªé¢„æµ‹çš„å­—ç¬¦ã€‚åªè¦ç»§ç»­è¿™æ ·åšï¼Œä½ å°±ä¼šç”Ÿæˆä¸€å †æ–‡æœ¬ï¼è¿™é‡Œï¼Œå‰ k ä¸ªæ ·æœ¬åªæ˜¯æˆ‘ä»¬çš„æ¨¡å‹å°†é¢„æµ‹å¹¶ä»ä¸­ä½¿ç”¨æœ€ç›¸å…³çš„ä¸€ä¸ªå­—æ¯çš„æ•°é‡ã€‚

```
def predict(net, char, h=None, top_k=None):
        ''' Given a character, predict the next character.
            Returns the predicted character and the hidden state.
        '''

        # tensor inputs
        x = np.array([[net.char2int[char]]])
        x = one_hot_encode(x, len(net.chars))
        inputs = torch.from_numpy(x)

        if(train_on_gpu):
            inputs = inputs.cuda()

        # detach hidden state from history
        h = tuple([each.data for each in h])
        # get the output of the model
        out, h = net(inputs, h) # get the character probabilities
        p = F.softmax(out, dim=1).data
        if(train_on_gpu):
            p = p.cpu() # move to cpu

        # get top characters
        if top_k is None:
            top_ch = np.arange(len(net.chars))
        else:
            p, top_ch = p.topk(top_k)
            top_ch = top_ch.numpy().squeeze()

        # select the likely next character with some element of randomness
        p = p.numpy().squeeze()
        char = np.random.choice(top_ch, p=p/p.sum())

        # return the encoded value of the predicted char and the hidden state
        return net.int2char[char], hdef sample(net, size, prime='The', top_k=None):

    if(train_on_gpu):
        net.cuda()
    else:
        net.cpu()

    net.eval() # eval mode

    # First off, run through the prime characters
    chars = [ch for ch in prime]
    h = net.init_hidden(1)
    for ch in prime:
        char, h = predict(net, ch, h, top_k=top_k) chars.append(char)

    # Now pass in the previous character and get a new one
    for ii in range(size):
        char, h = predict(net, chars[-1], h, top_k=top_k)
        chars.append(char) return ''.join(chars)
```

12.ç°åœ¨æˆ‘ä»¬å¯ä»¥ç”¨è¿™ä¸ªæ ·æœ¬æ–¹æ³•æ¥åšé¢„æµ‹ã€‚

```
print(sample(net, 500, prime='christmas', top_k=2))
```

è¾“å‡ºçœ‹èµ·æ¥ä¼šåƒè¿™æ ·ã€‚

```
christmas a son of thisthe sun wants the street of the stars, and the way the way
they went and too man and the star of the words
of a body of a street and the strange shoulder of the skyand the sun, an end on the sun and the sun and so to the stars are stars
and the words of the water and the streets of the world
to see them to start a posture of the streets
on the street of the streets, and the sun and soul of the station
and so too too the world of a sound and stranger and to the world
to the sun a
```

æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œæˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆä¸€äº›å¥½çš„çº¿æ¡ã€‚å†…å®¹æ²¡æœ‰å¤ªå¤šæ„ä¹‰ï¼Œä½†å®ƒèƒ½å¤Ÿç”Ÿæˆä¸€äº›è¯­æ³•æ­£ç¡®çš„è¡Œã€‚å¦‚æœæˆ‘ä»¬èƒ½å¤šè®­ç»ƒå®ƒä¸€æ®µæ—¶é—´ï¼Œå®ƒä¼šè¡¨ç°å¾—æ›´å¥½ã€‚

# ç»“è®º

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬äº†è§£äº† RNN å’Œ LSTMã€‚æˆ‘ä»¬è¿˜ç”¨ PyTorch å»ºç«‹äº†æˆ‘ä»¬çš„è¯—æ­Œæ¨¡å‹ã€‚æˆ‘å¸Œæœ›è¿™ç¯‡æ–‡ç« å¯¹ä½ æœ‰æ‰€å¸®åŠ©ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•ç–‘é—®æˆ–å»ºè®®ï¼Œæ¬¢è¿åœ¨ä¸‹é¢çš„è¯„è®ºåŒºå‘è¡¨ï¼Œæˆ–è€…é€šè¿‡[yash.yn59@gmail.com](mailto:yash.yn59@gmail.com)è”ç³»æˆ‘ï¼Œæˆ‘å°†éå¸¸ä¹æ„ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚