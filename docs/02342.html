<html>
<head>
<title>Initializing weights in deep neural networks improves results</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">初始化深度神经网络中的权重可以改善结果</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/initializing-weights-in-deep-neural-networks-improves-results-1e1669aa87bb?source=collection_archive---------27-----------------------#2019-12-11">https://medium.com/analytics-vidhya/initializing-weights-in-deep-neural-networks-improves-results-1e1669aa87bb?source=collection_archive---------27-----------------------#2019-12-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/9783855e31a84d7a09f255c14ef155c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*u7hdCxNZkIpa04gI"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">维克多·弗雷塔斯在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="fbbd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">深层神经网络是核心，许多层乘法(和其他运算)层层叠加。网络越深，我们就有越多的机会进行更多的计算，从而改进预测。那么，我们应该仅仅创造可以计算任何东西的超级深度网络吗？理论上..也许吧。实际上，它并不真正起作用。</p><p id="a2ee" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">也就是说，在某种程度上，将网做得更深是有好处的，但要充分利用它，我们需要小心一些。事实证明，图层输入的平均值为0且标准差为1非常重要。在第一个输入中，这很容易做到，只需归一化(减去平均值并除以标准差)，但随着数据经过一些运算，可能是与权重相乘，然后是激活函数(例如ReLU)，平均值和标准差(std)都会发生变化。让我们看看这个:</p><p id="e951" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">*我将使用奇妙的fastai库，因为这篇文章的灵感来自于<a class="ae iu" href="https://www.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fastai的</a>伟大作品</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="f462" class="kc kd hi jy b fi ke kf l kg kh"># import what we need<br/>from fastai import datasets<br/>import gzip, pickle<br/>import torch<br/>from torch import tensor</span><span id="899e" class="kc kd hi jy b fi ki kf l kg kh"># we will use the MNIST dataset for this demonstration</span><span id="c8c0" class="kc kd hi jy b fi ki kf l kg kh">MNIST_URL='http://deeplearning.net/data/mnist/mnist.pkl'</span></pre><p id="8f0a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们创建一个获取数据并将其映射到张量的函数。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="dba3" class="kc kd hi jy b fi ke kf l kg kh">def get_data():<br/>    path = datasets.download_data(MNIST_URL, ext='.gz')<br/>    with gzip.open(path, 'rb') as f:<br/>        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')<br/>    return map(tensor, (x_train, y_train, x_valid, y_valid))</span><span id="2e85" class="kc kd hi jy b fi ki kf l kg kh">x_train, y_train, x_valid, y_valid = get_data()</span><span id="64a3" class="kc kd hi jy b fi ki kf l kg kh"># Lets see what we've got:<br/>x_train.shape, y_train.shape, x_valid.shape, y_valid.shape</span><span id="3091" class="kc kd hi jy b fi ki kf l kg kh">---&gt; (torch.Size([50000, 784]),  torch.Size([50000]),  torch.Size([10000, 784]),  torch.Size([10000]))</span></pre><p id="c751" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们有了一个训练集和一个验证集，训练集是50k x 784矩阵，验证集是10k。我没有详细说明，因为我想你知道MNIST数据集，但更多细节请参考<a class="ae iu" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank">这个</a>。</p><p id="a858" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">查看训练集的均值和标准差，我们看到它没有均值0和标准差1(为什么会有？)</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="ded4" class="kc kd hi jy b fi ke kf l kg kh">x_train.mean(), x_train.std()</span><span id="6bab" class="kc kd hi jy b fi ki kf l kg kh">---&gt; (tensor(0.1304), tensor(0.3073))</span></pre><p id="5f62" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们得到的平均值为0.13，标准差为0.3。这不是我们想要的，但是我们可以用一个简单的规范化函数轻松地解决这个问题。让我们创建一个，然后在数据上激活它。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="c793" class="kc kd hi jy b fi ke kf l kg kh">def normalize(data, mean, std):<br/>    return (data - mean)/std</span><span id="3063" class="kc kd hi jy b fi ki kf l kg kh">x_train = normalize(x_train, x_train.mean(), x_train.std())<br/>x_valid = normalize(x_valid, x_train.mean(), x_train.std())</span><span id="d707" class="kc kd hi jy b fi ki kf l kg kh"># check the new mean and std<br/>x_train.mean(), x_train.std()</span><span id="15ea" class="kc kd hi jy b fi ki kf l kg kh">---&gt; (tensor(0.0001), tensor(1.))</span></pre><p id="5cbd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">太好了，我们得到了平均值0和标准差1，这正是我们想要的。<em class="kj">*注意我们使用了与</em> <strong class="ix hj"> <em class="kj">相同的</em> </strong> <em class="kj">列车数据的均值和标准差作为验证数据。如果数据没有被相同的统计数据标准化，训练数据的训练将不会预测验证数据。</em></p><p id="1234" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们说完了吗？不完全是。由于我们有更多的层，我们对数据做更多的操作。甚至在第一次简单的矩阵乘法之后，我们可以看到均值和标准差，再次不是我们所需要的。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="1156" class="kc kd hi jy b fi ke kf l kg kh"># A few convenience variables<br/>n, m = x_train.shape<br/>c = y_train.max() + 1<br/>n, m, c</span><span id="3920" class="kc kd hi jy b fi ki kf l kg kh">---&gt; 50000, 784, tensor(10)</span><span id="8efb" class="kc kd hi jy b fi ki kf l kg kh"># number of hidden <br/>nh = 50</span><span id="1a44" class="kc kd hi jy b fi ki kf l kg kh"># random weights matrix<br/>w1 = torch.randn(m, nh)</span><span id="064e" class="kc kd hi jy b fi ki kf l kg kh"># biases<br/>b1 = torch.zeros(nh)</span><span id="a5ed" class="kc kd hi jy b fi ki kf l kg kh">w1.shape, b1.shape<br/>---&gt; (torch.Size([784, 50]), torch.Size([50]))</span></pre><p id="d29d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们有了随机权重矩阵和偏差向量，让我们创建并运行线性乘法运算。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="394c" class="kc kd hi jy b fi ke kf l kg kh"># linear operation<br/>def linear(x, w, b):<br/>    return x@w + b</span><span id="bcd1" class="kc kd hi jy b fi ki kf l kg kh"># x_valid is normalized<br/>r1 = lin(x_valid, w1, b1)</span><span id="6038" class="kc kd hi jy b fi ki kf l kg kh">r1.mean(), r1.std()<br/>---&gt; (tensor(1.7274), tensor(26.3374))</span></pre><p id="70e2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们所看到的，我们对数据进行了规范化，这很好，但在一次操作后，它又偏离了轨道。我们的平均值为1.7，标准差为26.3。不行，这样学习效率不高。我们要做的是使用一种技术来解决这个问题，这种技术深受一位名叫何的研究员的启发。简单而有效(在某种程度上，很快会有更多关于这方面的内容)。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="a6e0" class="kc kd hi jy b fi ke kf l kg kh">import math<br/>w1 = torch.randn(m, nh)/math.sqrt(m)<br/>b1 = torch.zeros(nh)</span></pre><p id="4b14" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们在这里所做的，就是将权重除以行数的平方根。我们的体重除以784。仅仅这个简单的技巧，就能带来巨大的不同。让我们来看看实际情况:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="436f" class="kc kd hi jy b fi ke kf l kg kh"># calculate a linear operation with the new w1, b1<br/>r2 = lin(x_valid, w1, b1)</span><span id="2f96" class="kc kd hi jy b fi ki kf l kg kh">r2.mean(), r2.std<br/>---&gt; (tensor(-0.0807), tensor(1.0222))</span></pre><p id="88f1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">和..成功了！这非常接近0和1。还不错。</p><p id="b156" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">不过……这个线性操作层印象不是很深刻吧？它甚至没有激活功能，这意味着它无法学习非线性数据，而这些数据可能会学习。让我们添加一个激活函数，然后，我们将使用ReLU。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="7110" class="kc kd hi jy b fi ke kf l kg kh">def relu(x):<br/>    return x.clamp_min(0.)</span><span id="f140" class="kc kd hi jy b fi ki kf l kg kh">t = relu(linear(x_valid, w1, b1))</span><span id="11b3" class="kc kd hi jy b fi ki kf l kg kh">t.mean(), t.std()</span><span id="45ee" class="kc kd hi jy b fi ki kf l kg kh">---&gt; (tensor(0.3671), tensor(0.5580))</span></pre><p id="b25a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们所见..问题又回来了。执行relu操作后，mean不为0，std不为1。我们如何解决这个问题？基于<a class="ae iu" href="https://arxiv.org/abs/1502.01852" rel="noopener ugc nofollow" target="_blank">这篇论文</a>，对我们的初始化做了一个微小但重要的改变。发生的情况是，当ReLU用0替换任何负数时，我们不仅丢失了所有负的绝对数字(它们应该保持平均值不上升)，我们还每次都将方差减半。如果我们有8层，方差变成(1/2)⁸，这使得梯度，从而学习，是不可能的。</p><p id="2106" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">**关于ReLU定义的注释。这可以用很多方法来实现，但是，如果有一种方法使用Pytorch方法来实现它，这通常意味着它会快得多，因为它很有可能不是用Python实现的。</p><p id="41fc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，要解决这个问题，我们要做的就是用(2/m)替换(/m)。在上面放一个2。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="e016" class="kc kd hi jy b fi ke kf l kg kh"># note the 2/m<br/>w1 = torch.randn(m, nh)*math.sqrt(2/m)</span></pre><p id="e2d7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这一点至关重要:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="d268" class="kc kd hi jy b fi ke kf l kg kh">t2 = relu(linear(x_valid, w1, b1))</span><span id="378e" class="kc kd hi jy b fi ki kf l kg kh">t.mean(), t.std()</span><span id="75a0" class="kc kd hi jy b fi ki kf l kg kh">---&gt; (tensor(0.5414), tensor(0.8305))</span></pre><p id="42da" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然还不完美，但已经好多了。我们可以看到std越来越接近1。我们仍然有一个问题。如前所述，我们用0代替了所有的负数，所以这有点过于简单(虽然在我的测试中效果很好),只是从平均值中减去了0.5。让我们稍微修改一下ReLU:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="4a43" class="kc kd hi jy b fi ke kf l kg kh">def relu(x):<br/>    return x.clamp_min(0.) - 0.5</span><span id="3fe9" class="kc kd hi jy b fi ki kf l kg kh">t = relu(lin(x_valid, w1, b1))</span><span id="75f1" class="kc kd hi jy b fi ki kf l kg kh">t.mean(), t.std()</span><span id="000d" class="kc kd hi jy b fi ki kf l kg kh">---&gt; (tensor(-0.0025), tensor(0.7597))</span></pre><p id="4bcc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在好多了！这个非常简单但非常重要的技巧让学习变得更有效率，没有它(或任何足够好的替代方法)很难得到STOA结果。希望你学到了重要的东西，如果有任何问题/评论，请随时联系我们！</p><p id="d01d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Bakugan@gmail.com或在<a class="ae iu" href="https://www.linkedin.com/in/chen-margalit-4b1a67117/" rel="noopener ugc nofollow" target="_blank">领英</a></p></div></div>    
</body>
</html>