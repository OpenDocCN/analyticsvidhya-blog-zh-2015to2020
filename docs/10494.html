<html>
<head>
<title>2 ways to train a Linear Regression Model-Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">训练线性回归模型的两种方法-第1部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/2-ways-to-train-a-linear-regression-model-part-1-e643dbef3df1?source=collection_archive---------3-----------------------#2020-10-21">https://medium.com/analytics-vidhya/2-ways-to-train-a-linear-regression-model-part-1-e643dbef3df1?source=collection_archive---------3-----------------------#2020-10-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/4c858b2347308d3521868bba8fad7bb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3sw9mZqkgC8lEklFK8lkg.png"/></div></div></figure><p id="b796" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最简单的监督机器学习模型之一是<em class="jo">线性回归</em>。在本帖中，我们将讨论训练线性回归模型的两种不同方法。</p><p id="9a5d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在详细解释每一个之前，让我快速地列出它们:</p><p id="41de" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.列车使用<em class="jo">闭式方程</em></p><p id="3225" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.列车采用<em class="jo">梯度下降</em></p><p id="dd83" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一种方式<em class="jo">直接</em>计算最适合模型的模型参数，第二种方式<em class="jo">迭代计算</em>。我将在本帖中讨论封闭型方程，并将创建第二部分，讨论梯度下降及其变体。</p><p id="395c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">但是首先，线性模型是如何做出预测的？</strong></p><p id="e4c3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性回归包括通过数据找到最合适的直线。这条最佳拟合线就是我们所说的回归线。线性模型通过计算输入要素的加权和以及一个称为<em class="jo">偏差</em>项的常数来进行预测</p><p id="9302" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性回归模型预测的方程可以由下式给出:</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/276a4fdc2029bb722f037b790d41b194.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_cnXEFVfe563Cz2sQruAow.png"/></div></div></figure><p id="20b9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">我们如何训练这个模型？</strong></p><p id="86ff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你看到等式中不同的θ了吗？这些被称为<em class="jo">模型参数。为了训练一个模型，我们需要找到使成本函数</em>最小化的θ(模型参数)的值。有两种方法可以做到这一点！但首先，让我们定义我们的成本函数。</p><p id="33a7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">回归模型最常见的性能度量是RMSE。因此，我们需要找到使RMSE最小的θ值。但实际上，最小化MSE要容易得多。假设我们试图最小化的成本函数是<em class="jo">均方误差。</em>使用下面的等式计算MSE</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/636b29c55386cfe6ac094ce7ae6b3dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NZV-4OMfQWUHOFgTfe3A_A.png"/></div></div></figure><p id="d78f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们已经定义了成本函数，让我们继续前进，进入寻找最佳模型参数值(θ)的第一种方法。</p><ol class=""><li id="22d0" class="jt ju hi is b it iu ix iy jb jv jf jw jj jx jn jy jz ka kb bi translated"><strong class="is hj">“封闭形式”方程</strong></li></ol><p id="a2ff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有一个数学方程可以直接计算模型参数。这个方程叫做<em class="jo">正规方程。</em>你可以在这里阅读更多关于正规方程<a class="ae kc" href="https://www.geeksforgeeks.org/ml-normal-equation-in-linear-regression/" rel="noopener ugc nofollow" target="_blank">。法线方程由下式给出:</a></p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/639bc4d1bf9efef40b8f320da3263fdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2qG0qr6vMIOjg2MQ0X2BGg.png"/></div></div></figure><p id="4817" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是把我们所有的值代入这个方程。我们将能够计算出使我们的成本函数最小化的最佳模型参数。简单对吗？让我们在线性生成的数据上测试该等式。</p><pre class="jp jq jr js fd kd ke kf kg aw kh bi"><span id="0b7c" class="ki kj hi ke b fi kk kl l km kn">X = 2*np.random.rand(100,1)<br/>#add X0 = 1 to each instance(bias term)<br/>X_b = np.c_[np.ones((100,1)), X]<br/>y = 2+4*X +np.random.randn(100,1)</span><span id="fb33" class="ki kj hi ke b fi ko kl l km kn">fig = plt.figure()<br/>plt.plot(X,y,"b.")<br/>plt.axis([0,2,0,10])<br/>plt.title("Randomly generated Linear data")<br/>plt.show()<br/>fig.savefig('result1.png')</span></pre><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/9904d6320801115312623e29c22bedda.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*8fjq1NCpxwIYFpbHcm6Hvw.png"/></div></figure><pre class="jp jq jr js fd kd ke kf kg aw kh bi"><span id="eeff" class="ki kj hi ke b fi kk kl l km kn"># Normal equation<br/>theta_cap = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span><span id="6090" class="ki kj hi ke b fi ko kl l km kn">print(theta_cap)</span></pre><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/a7c39af1643b218661127e2e1ae935de.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*2_guNAWe0qk2V-Ujx6VrCg.png"/></div></figure><p id="f3fe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们用来生成数据的函数是2+4x+噪声。法线方程已经计算出足够接近的θ0和θ1值！让我们写一些代码来做预测</p><pre class="jp jq jr js fd kd ke kf kg aw kh bi"><span id="a463" class="ki kj hi ke b fi kk kl l km kn">X_new = np.array([[0],[4]])<br/>X_new_b = np.c_[np.ones((2,1)), X_new]</span><span id="3bc3" class="ki kj hi ke b fi ko kl l km kn">y_predict = X_new_b.dot(theta_cap)<br/>print(y_predict)</span><span id="36a9" class="ki kj hi ke b fi ko kl l km kn">plt.plot(X_new, y_predict,"r-")<br/>plt.plot(X,y,"b.")<br/>plt.title("Linear Regression model prediction")<br/>plt.axis([0,2,0,15])<br/>plt.show()</span></pre><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/41dac84da5ea56aefd49584c75279274.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*gIMP5gCvYiU1fN9nUFwh8Q.png"/></div></figure><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es kp"><img src="../Images/c43ae6793907cca7cdc30e1279f575db.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*dTOmicpEyv1f0hXzzh4_hQ.png"/></div></figure><p id="4f5f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">而正规方程计算X^T X的逆，计算复杂度几乎是O(n^3)，如果你还没搞清楚这是不好的！在O(n^2时间有另一种方法可以解决这个问题。这种方法使用称为<strong class="is hj"> <em class="jo">奇异值分解的矩阵分解技术来计算X的<em class="jo">伪逆</em>。</em> </strong>简单来说，SVD将矩阵<strong class="is hj"> X </strong>分解为3个矩阵的矩阵乘法。更多关于SVD的信息可以在<a class="ae kc" rel="noopener" href="/@abdullatif.h">这里</a>找到。使用Numpy的线性代数模型中的<em class="jo"> pinv() </em>函数可以实现这一点。</p><pre class="jp jq jr js fd kd ke kf kg aw kh bi"><span id="90a1" class="ki kj hi ke b fi kk kl l km kn">np.linalg.pinv(X_b).dot(y)</span></pre><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/d4423afa4d274ccb17f1de88ea80ee39.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*CerRJJBKXilRcf1SkCmLgw.png"/></div></figure><p id="3e01" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">为什么奇异值分解比正规方程更有效？</strong></p><p id="8cee" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">a.正规方程只有在X^T X可逆或奇异时才有效。另一方面，伪逆总是被定义的。</p><p id="1c5d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">b.如上所述，SVD比正规方程具有更少的计算复杂度。</p><p id="fbb8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是当<em class="jo">特征和实例的数量很大</em>时，这两种方法都非常慢。这就是为什么下一种训练模型的方法可能更适合这种情况。我将在我的第二部分文章中讨论这个问题。</p><p id="bdf8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们通过对生成的数据集进行简单的Scikit-Learn线性回归实现来结束这篇文章，并看看我们是否可以获得类似于由Normal Equation计算的模型参数。</p><pre class="jp jq jr js fd kd ke kf kg aw kh bi"><span id="da1d" class="ki kj hi ke b fi kk kl l km kn">from sklearn.linear_model import LinearRegression<br/>lin_reg = LinearRegression()<br/>lin_reg.fit(X,y)<br/>print(lin_reg.intercept_)<br/>print(lin_reg.coef_)</span></pre><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es kt"><img src="../Images/dccfdc9dd96a22818bc719d951695e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*QYjC72IbRhN5_RatFT8rQg.png"/></div></figure><p id="18e0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你瞧，确实如此！注意sklearn如何给出与计算<em class="jo"> pinv()相同的结果。</em>那是因为sklearn计算x的伪逆。</p><p id="1d42" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">结论</strong></p><p id="55f7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这篇博客中，我们了解到:</p><ol class=""><li id="9e47" class="jt ju hi is b it iu ix iy jb jv jf jw jj jx jn jy jz ka kb bi translated">线性模型如何进行预测—线性回归的一般方程</li><li id="c5aa" class="jt ju hi is b it ku ix kv jb kw jf kx jj ky jn jy jz ka kb bi translated">如何训练一个模型并使代价函数最小化——<strong class="is hj">均方误差</strong>的方程</li><li id="fe6b" class="jt ju hi is b it ku ix kv jb kw jf kx jj ky jn jy jz ka kb bi translated">使用封闭形式方程训练线性回归模型的第一种方法——<strong class="is hj">正规方程</strong></li><li id="7843" class="jt ju hi is b it ku ix kv jb kw jf kx jj ky jn jy jz ka kb bi translated">法线方程的简单实现</li><li id="0140" class="jt ju hi is b it ku ix kv jb kw jf kx jj ky jn jy jz ka kb bi translated">使用<strong class="is hj">奇异值分解</strong>计算<em class="jo">伪逆</em></li><li id="11dc" class="jt ju hi is b it ku ix kv jb kw jf kx jj ky jn jy jz ka kb bi translated">线性回归的Scikit-learn实现</li></ol><p id="4dfe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你喜欢这个博客，请留下掌声或评论。它让我知道我的博客是有帮助的:)祝大家学习愉快！</p><p id="90de" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">参考文献</strong></p><p id="fcb3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">1.用Scikit-Learn和TensorFlow进行机器学习:构建智能系统的概念、工具和技术。奥雷连·杰龙的书。</p></div></div>    
</body>
</html>