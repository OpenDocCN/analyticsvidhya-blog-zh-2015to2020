<html>
<head>
<title>Artificial Neural Networks- An intuitive approach Part 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工神经网络——直观的方法第四部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/artificial-neural-networks-an-intuitive-approach-part-4-2e3e2a71994d?source=collection_archive---------21-----------------------#2020-07-25">https://medium.com/analytics-vidhya/artificial-neural-networks-an-intuitive-approach-part-4-2e3e2a71994d?source=collection_archive---------21-----------------------#2020-07-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9eb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">前一篇文章的续篇</p><p id="8592" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请找到早期文章的链接</p><p id="5bc5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" rel="noopener" href="/@nikethnarasimhan/artificial-neural-networks-an-intuitive-approach-part-3-a5888af9ac0">https://medium . com/@ nikethnarasimhan/artificial-neural-networks-an-intuitive-approach-part-3-a 5888 a F9 AC 0</a></p><h2 id="04f6" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">内容</h2><ol class=""><li id="fc73" class="jz ka hi ih b ii kb im kc iq kd iu ke iy kf jc kg kh ki kj bi translated">权重初始化</li><li id="9bde" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">Xavier初始化</li><li id="79bf" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">He初始化</li><li id="56f2" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">标准化方法</li></ol><h1 id="7ec9" class="kp jf hi bd jg kq kr ks jk kt ku kv jo kw kx ky jr kz la lb ju lc ld le jx lf bi translated">权重初始化</h1><h2 id="8bb4" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">有效初始化的重要性</h2><p id="f1c2" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">为了建立一个机器学习算法，通常你会定义一个架构(如逻辑回归，支持向量机，神经网络)并训练它学习参数。以下是神经网络的常见训练过程:</p><ol class=""><li id="8b00" class="jz ka hi ih b ii ij im in iq lj iu lk iy ll jc kg kh ki kj bi translated">初始化参数</li><li id="b137" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">选择一个<em class="lm">优化算法</em></li><li id="2623" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">重复这些步骤:</li><li id="980b" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">向前传播输入</li><li id="739c" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">计算成本函数</li><li id="ec05" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">使用反向传播计算成本相对于参数的梯度</li><li id="136f" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">根据优化算法，使用梯度更新每个参数</li></ol><p id="5c5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，给定一个新的数据点，您可以使用该模型来预测其类别。</p><p id="fc3c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">初始化步骤对模型的最终性能至关重要，它需要正确的方法。为了说明这一点，考虑下面的三层神经网络。你可以尝试用不同的方法初始化这个网络，观察对学习的影响。</p><p id="d8d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">情况1:过大的初始化导致爆炸梯度</strong></p><p id="de3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果用非常高的值初始化权重，则项<code class="du ln lo lp lq b">np.dot(W,X)+b</code>变得显著更高，并且如果应用类似sigmoid()的激活函数，则该函数将其值映射到1附近，其中梯度的斜率变化缓慢，并且学习花费大量时间。当这些激活被用于反向传播时，这导致爆炸梯度问题。也就是说，成本相对于参数的梯度太大。这导致成本在其最小值附近波动。</p><p id="9fcc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">情况2:太小的初始化导致渐变消失</strong></p><p id="fd0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果权重初始化为低值，它将被映射为0。</p><p id="097e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当这些激活用于反向传播时，这导致了消失梯度问题。成本相对于参数的梯度太小，导致成本在达到最小值之前就已经收敛。</p><p id="8b34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">或者<strong class="ih hj">直观地</strong>我们可以用下面的推理来理解上面提到的几点:</p><ul class=""><li id="9270" class="jz ka hi ih b ii ij im in iq lj iu lk iy ll jc lr kh ki kj bi translated">当你的权重和你的渐变接近零时，你的上游层<strong class="ih hj">的渐变消失</strong>，因为你在乘以小值，例如0.1 x 0.1 x 0.1 x 0.1 = 0.0001。因此，很难找到一个最优值，因为你的上游层学习得很慢。</li><li id="c30b" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc lr kh ki kj bi translated">相反的情况也可能发生。当你的权重和梯度大于1时，乘法变得非常强大。10 x 10 x 10 x 10 = 1000。因此，梯度也可能<strong class="ih hj">爆炸</strong>，导致你的上游层中的数字溢出，使它们“无法达到”(甚至使那些层中的神经元死亡)。</li></ul><p id="774c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">因此，我们可以得出结论，我们必须在所有层上保持初始化的权重的方差近似等于1。</strong></p><h1 id="ba62" class="kp jf hi bd jg kq kr ks jk kt ku kv jo kw kx ky jr kz la lb ju lc ld le jx lf bi translated">Xavier初始化</h1><p id="5842" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">我们需要从均值为零、方差为1/ <strong class="ih hj"> N </strong>的高斯分布中选取权重，其中<strong class="ih hj"> N </strong>指定输入神经元的数量。</p><p id="8cee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种策略基本上假设从例如标准正态分布进行随机初始化，但是具有产生输出方差为1的特定方差。<strong class="ih hj">这是针对双曲正切函数</strong></p><figure class="lt lu lv lw fd lx er es paragraph-image"><div class="er es ls"><img src="../Images/cbfc97bf47c1c21b5ecd2ce30282ddc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/format:webp/1*jR6VHs-okxkEiShuwxUigg.png"/></div></figure><h1 id="b9d3" class="kp jf hi bd jg kq kr ks jk kt ku kv jo kw kx ky jr kz la lb ju lc ld le jx lf bi translated">He初始化</h1><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="er es ma"><img src="../Images/236442deab4c9540a2370723b4200e77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m4sxXlbVtDdowrRPV1VznQ.png"/></div></div></figure><p id="335e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当你的神经网络<strong class="ih hj">被重新激活时，初始化是选择的方法之一，数学上它试图做同样的事情</strong></p><p id="c209" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这种差异与ReLU激活函数的非线性有关，这使得它在x=0时不可微。然而，在其他值，它要么是0，要么是1，如上图所示。最佳权重初始化策略是随机初始化权重，但具有以下方差:</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div class="er es mf"><img src="../Images/81d8b3e6d65362c99eff02b6bd40c949.png" data-original-src="https://miro.medium.com/v2/resize:fit:202/format:webp/1*X5-EzbJ6r-MQnY1-4KUWlg.png"/></div></figure><h1 id="76ca" class="kp jf hi bd jg kq kr ks jk kt ku kv jo kw kx ky jr kz la lb ju lc ld le jx lf bi translated">标准化方法:</h1><p id="4cf8" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">让我们回忆一下最基本形式的规范化和标准化的含义。</p><p id="e901" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">典型的标准化过程包括将数字数据缩小到从0到1的范围内，典型的标准化过程包括从每个数据点减去数据集的平均值，然后将差值除以数据集的标准偏差。</p><p id="bd02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这迫使标准化数据的平均值为0，标准差为1。在实践中，这个标准化过程也经常被称为规范化。</p><p id="12b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一般来说，这都归结为把我们的数据放在某种已知的或标准的尺度上。我们为什么要这样做？</p><p id="0f58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们没有以某种方式对数据进行标准化，我们可以想象我们的数据集中可能有一些数值很高的数据点，而其他数据点可能很低。</p><p id="0231" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目前，请理解这种不平衡、非标准化的数据可能会导致我们的网络出现问题，从而使训练变得更加困难。此外，非标准化数据会显著降低我们的训练速度。</p><p id="703e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">非标准化数据中较大的数据点会导致神经网络不稳定，因为:</p><ol class=""><li id="b807" class="jz ka hi ih b ii ij im in iq lj iu lk iy ll jc kg kh ki kj bi translated">大的输入可以通过网络中的层向下级联，这可能导致不平衡的梯度，这可能导致“爆炸梯度问题”。</li><li id="5e2e" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">非标准化数据显著降低了训练速度。</li><li id="1c7d" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">如果数据没有标准化，权重的微小变化可能会严重歪曲决策界限。</li></ol><p id="1970" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们介绍一个新的重要概念</p><p id="0bf6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">内部协变量移位:</strong></p><p id="0c8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设你有一个要达到的目标，一个固定的目标和一个不断前进的目标哪个更容易？很明显，静态目标比动态目标更容易实现。</p><p id="7760" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">神经网络中的每一层都有一个简单的目标，即对其下一层的输入进行建模，因此每一层都试图适应其输入，但对于隐藏层来说，事情就有点复杂了。输入的统计分布在几次迭代后会发生变化，因此，如果输入的统计分布不断变化，称为<strong class="ih hj">内部协变量移位</strong>，隐藏层将不断尝试适应新的分布，从而减慢收敛速度。它就像一个隐藏层不断变化的目标。</p><p id="5947" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">非常深的模型包括几个功能或层的组合。在其他层不变的假设下，梯度告诉如何更新每个参数。实际上，我们同时更新所有的层。</p><p id="cfdf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为在更新过程中所有的层都被改变了，所以更新过程永远在追踪一个移动的目标。</p><p id="dbe1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，假设期望前一层输出具有给定分布的值，则更新该层的权重。在前一层的权重被更新后，这种分布可能会改变。</p><p id="aca0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，<strong class="ih hj">批量归一化</strong> (BN)算法(前面已经深入介绍过)试图归一化每个隐藏层的输入，以便随着训练的进行，它们的分布相当恒定。这提高了神经网络的收敛性。</p><p id="532d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们快速总结一下为什么规范化是必不可少的:</p><ol class=""><li id="ef6e" class="jz ka hi ih b ii ij im in iq lj iu lk iy ll jc kg kh ki kj bi translated">每个特征被归一化，因此它们被变换到相同的尺度，因此它们的贡献保持无偏，而不管非归一化特征的值是高还是低。</li><li id="16ed" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">它减少了<strong class="ih hj">内部协变量移位</strong>。它是由于训练期间网络参数的变化而引起的网络激活分布的变化。为了改善训练，我们寻求减少内部协变量的变化。</li><li id="1848" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">众所周知，批范数可以使损失曲面更平滑，并得到定义明确的决策边界。</li><li id="672c" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">它使优化更快，因为归一化处理了爆炸梯度问题，并确保权重以均匀的方式分布。</li></ol><p id="c17b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们深入研究各种标准化技术:</p><h1 id="fc62" class="kp jf hi bd jg kq kr ks jk kt ku kv jo kw kx ky jr kz la lb ju lc ld le jx lf bi translated">批量标准化:</h1><p id="0710" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">批量标准化是一种标准化方法，它通过小批量标准化网络中的激活。对于每个要素，批次归一化会计算小批次中该要素的均值和方差。然后减去平均值，并将该特征除以其最小批量标准偏差。</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div class="er es mg"><img src="../Images/d836a6bf7270a8c691812fbdbeb35eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*ABy4gQtuLjVwTFusxSuKtg.png"/></div></figure><p id="6d47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意:在上述情况下，Gamma和Beta是可学习的参数。它们可用于根据预期将砝码放大或缩小至原始值。</strong></p><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="er es mh"><img src="../Images/b37de4697fc2b0bf52c8c970f3b20f91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZnJukf8xPhn2dRk7_FOOEA.png"/></div></div><figcaption class="mi mj et er es mk ml bd b be z dx translated">批量标准化步骤</figcaption></figure><p id="cd29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">批量归一化优势:</strong></p><ol class=""><li id="3606" class="jz ka hi ih b ii ij im in iq lj iu lk iy ll jc kg kh ki kj bi translated">批量标准化解决了渐变爆炸的问题</li></ol><p id="4b01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.批量标准化使损失表面“更容易导航”，使优化更容易，能够使用更高的学习率，并提高跨多个任务的模型性能。</p><p id="b474" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">批量标准化的缺点:</p><ol class=""><li id="cc08" class="jz ka hi ih b ii ij im in iq lj iu lk iy ll jc kg kh ki kj bi translated"><strong class="ih hj">对批量大小的依赖性→ </strong>如果我们的批量大小为1，则随后方差变为0，因此批量定额不起作用。当小批量的规模变得太小，那么它变得太嘈杂，训练受到影响。</li><li id="d881" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated"><strong class="ih hj">递归神经网络</strong> →不适用于RNN氏症——在RNN中，每个时间步的递归激活将具有不同的统计数据。这意味着我们必须为每个时间步安装一个单独的批量标准化层。这使得模型更加复杂，更重要的是，它迫使我们在训练期间存储每个时间步长的统计数据。</li></ol><p id="d6a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">权重归一化:</strong></p><p id="142b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重量标准化是一种标准化重量而不是最小批次的方法</p><p id="96e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">权重归一化将权重向量参数化为</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div class="er es mm"><img src="../Images/64dda5e3f1b4512c8c85b029ea7e8f69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*Fu5rcKLlrkXkhLIW46GiMQ.png"/></div></figure><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="er es mn"><img src="../Images/484ff15209b3e8e7f218dfdc015a85e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BLCQnlTbBKDm5MH4G2uq7Q.png"/></div></div></figure><p id="2fee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类似于批量归一化，权重归一化并不会降低网络的表达能力。它所做的是<strong class="ih hj">将权重向量的范数与方向</strong>分开。然后优化两者</p><p id="db9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">一般而言，仅均值批次归一化</strong>和<strong class="ih hj">权重归一化</strong>用于获得所需的输出，即使在小批量中也是如此。这意味着它们减去了小批量的平均值，但没有除以方差。最后，他们使用权重归一化，而不是除以方差。</p><p id="37e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优点</strong></p><ol class=""><li id="376a" class="jz ka hi ih b ii ij im in iq lj iu lk iy ll jc kg kh ki kj bi translated">除了平均值和方差独立于批次之外，</li><li id="5107" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc kg kh ki kj bi translated">重量标准化通常比批量标准化快得多。在卷积神经网络中，权重的数量往往远小于输入的数量，这意味着权重归一化在计算上比批量归一化更便宜。批量标准化需要通过输入的所有元素，这可能是非常昂贵的，尤其是当输入的维数很高时，例如在图像的情况下。卷积在多个位置使用相同的过滤器，因此通过权重的过程要快得多。</li></ol><p id="7be8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">“仅平均值批量标准化”和重量标准化。</strong></p><p id="76ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此方法与批量归一化相同，只是它不会将输入除以标准差或对其进行重缩放。虽然这种方法抵消了重量归一化的一些计算速度，但它比批量归一化更便宜，因为它不需要计算标准偏差。这种方法具有以下优点:</p><p id="92b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.它使得激活的平均值独立于</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div class="er es mo"><img src="../Images/791ca8d28614fc365241b5be74ae171e.png" data-original-src="https://miro.medium.com/v2/resize:fit:18/0*EjbgmCfKcb0-imvA"/></div></figure><p id="6d91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">权重归一化不能独立地从层的权重中分离出激活的平均值，导致每层的平均值之间的高度依赖性。仅均值批量归一化可以解决这个问题。</p><p id="0b77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.它给激活增加了“柔和的噪音”</p><p id="d062" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">批量标准化的副作用之一是，由于使用了对小批量计算的噪声估计，它给激活增加了一些随机噪声。这在一些应用中具有正则化效果，但在一些噪声敏感的领域(如强化学习)中可能是有害的。然而，由于大数定律确保激活的平均值近似正态分布，所以由平均值估计引起的噪声“更温和”。</p><p id="4b27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">图层归一化</strong></p><p id="d991" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">小批量由具有相同数量特征的多个实例组成。小批量是矩阵，如果每个输入是多维的，则是张量，其中一个轴对应于批量，另一个轴对应于特征尺寸。</p><p id="402f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">批量归一化将输入要素跨批量维度进行归一化。图层标准化的关键特征是<strong class="ih hj">通过特征</strong>标准化输入。</p><p id="52d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这与批量标准化非常相似，但是区别可以在下图中看到:</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div class="er es mp"><img src="../Images/767c34a2435934674fa842449e67234c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*GdTAI1KMyxtjIzC5NCvp-Q.png"/></div></figure><p id="b5a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在批量标准化中，统计数据在批量中计算<em class="lm">，并且对于批量中的每个示例都是相同的。相比之下，在层标准化中，统计是跨每个特征</em>计算的<em class="lm">，并且<strong class="ih hj">独立于其他示例</strong>。</em></p><p id="ff58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优点:</strong></p><p id="760a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于RNN来说，分层定额比批量定额更有效.</p><p id="c39b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">&gt;-输入之间的独立性意味着每个输入都有不同的归一化操作，允许使用任意的小批量。</p><h2 id="e12b" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">实例(或对比)标准化</h2><p id="65a9" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated"><strong class="ih hj">实例归一化</strong>与层<strong class="ih hj">归一化</strong>相似，但更进一步:它计算每个训练示例中每个通道的平均值/标准偏差和<strong class="ih hj">归一化</strong></p><p id="e535" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面提到了实例规范化的优点</p><ul class=""><li id="fe83" class="jz ka hi ih b ii ij im in iq lj iu lk iy ll jc lr kh ki kj bi translated">这种标准化简化了模型的学习过程。</li><li id="82fe" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc lr kh ki kj bi translated">实例规范化可以在测试时应用。</li><li id="595e" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc lr kh ki kj bi translated">因此，它是特定于图像的，并且不能简单地扩展到RNNs。</li><li id="c719" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc lr kh ki kj bi translated">实验结果表明，实例规范化代替批量规范化在风格转换方面表现良好。</li><li id="2ce6" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc lr kh ki kj bi translated">最近，在GANs中，实例规范化也被用来代替批处理规范化。</li></ul><h2 id="a86e" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">群体规范化</h2><p id="09f2" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">顾名思义，组标准化-为每个训练示例计算通道的<strong class="ih hj">组的平均值和标准偏差。在某种程度上，组规范化是层规范化和实例规范化的结合。事实上，当我们将所有通道放入单个组中时，组规范化就变成了层规范化，而当我们将每个通道放入不同的组中时，它就变成了实例规范化。</strong></p><p id="d537" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">组规范化可以说是批量规范化的替代方法。这种方法的工作原理是将通道分成组，并在每组内计算归一化的均值和方差，即归一化每组内的特征。与批标准化不同，组标准化与批大小无关，并且其准确性在很大范围的批大小内都是稳定的。</p><h2 id="11ae" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">优势</h2><p id="48dc" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated">下面提到了组规范化的优点:</p><ul class=""><li id="778e" class="jz ka hi ih b ii ij im in iq lj iu lk iy ll jc lr kh ki kj bi translated">它有能力在许多深度学习任务中取代批量标准化</li><li id="ec29" class="jz ka hi ih b ii kk im kl iq km iu kn iy ko jc lr kh ki kj bi translated">只用几行代码就可以在现代图书馆中轻松实现</li></ul><p id="38b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过下图，我们可以直观地理解不同的图层规范:</p><figure class="lt lu lv lw fd lx er es paragraph-image"><div role="button" tabindex="0" class="mb mc di md bf me"><div class="er es mq"><img src="../Images/3f6486d92ee0495c245f26a28a693c5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YQr9RBmo3w6rSHsg9NKdeA.png"/></div></div></figure></div></div>    
</body>
</html>