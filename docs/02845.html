<html>
<head>
<title>Panorama Formation using Image Stitching using OpenCV</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用OpenCV的图像拼接形成全景图</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/panorama-formation-using-image-stitching-using-opencv-1068a0e8e47b?source=collection_archive---------0-----------------------#2020-01-05">https://medium.com/analytics-vidhya/panorama-formation-using-image-stitching-using-opencv-1068a0e8e47b?source=collection_archive---------0-----------------------#2020-01-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f3cb469b2d2e866a3a8e7b009e868942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wuuaaIWN-1UbwaJBY3peJQ.png"/></div></div></figure><h1 id="a029" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">背景</h1><p id="6604" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">你好，朋友们，</p><p id="c6a7" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">你们中的一些人可能知道，我正在写一系列的文章来解释今天的移动相机的各种常见功能，如全景、HDR、慢镜头、重影等。</p><p id="95b5" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">同样，本文是我关于全景图/图像拼接的两部分文章系列的第一部分。本文将关注使用两幅图像形成全景的基础，这将在下一篇文章中使用，我们将看到如何将多幅图像拼接在一起。</p><h1 id="c1c8" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">我们开始吧</h1><p id="6d3b" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">为了构建我们的图像全景，我们将利用计算机视觉和图像处理技术，例如:关键点检测和局部不变描述符；关键点匹配；RANSAC和透视扭曲。</p><p id="21e6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">既然有<a class="ae kr" href="https://www.pyimagesearch.com/2015/07/16/where-did-sift-and-surf-go-in-opencv-3/" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj"> <em class="ks">主要区别</em> </strong>如何OpenCV 2.4.X和OpenCV 3。x处理关键点检测和局部不变描述符</a>(比如SIFT和SURF)，我已经特别注意提供与<strong class="jq hj"> <em class="ks">和</em> </strong>两个版本兼容的代码(当然前提是你编译的OpenCV 3支持opencv_contrib)。</p><h1 id="c37a" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">OpenCV全景拼接</h1><figure class="ku kv kw kx fd ij er es paragraph-image"><div class="er es kt"><img src="../Images/8c10d7fe804fab41b31a3baaf69d5c87.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*pkpnw6-U5ggkGOytz-gMRQ.jpeg"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">关键点和局部不变量提取示例</figcaption></figure><p id="04c3" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们的全景拼接算法包括四个步骤:</p><ul class=""><li id="919e" class="lc ld hi jq b jr km jv kn jz le kd lf kh lg kl lh li lj lk bi translated"><strong class="jq hj">步骤#1: </strong>检测关键点(狗、哈里斯等。)并提取局部不变描述符(SIFT、SURF等)。)来自两个输入图像。</li><li id="27cd" class="lc ld hi jq b jr ll jv lm jz ln kd lo kh lp kl lh li lj lk bi translated"><strong class="jq hj">步骤#2: </strong>匹配两幅图像之间的描述符。</li><li id="4071" class="lc ld hi jq b jr ll jv lm jz ln kd lo kh lp kl lh li lj lk bi translated"><strong class="jq hj">步骤#3: </strong>使用<a class="ae kr" href="https://en.wikipedia.org/wiki/RANSAC" rel="noopener ugc nofollow" target="_blank"> RANSAC算法</a>使用我们匹配的特征向量来估计<a class="ae kr" href="https://en.wikipedia.org/wiki/Homography_(computer_vision)" rel="noopener ugc nofollow" target="_blank">单应矩阵</a>。</li><li id="622a" class="lc ld hi jq b jr ll jv lm jz ln kd lo kh lp kl lh li lj lk bi translated"><strong class="jq hj">步骤#4: </strong>使用从<strong class="jq hj">步骤#3 </strong>获得的单应矩阵应用扭曲变换。</li></ul><p id="0352" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们将把所有这四个步骤封装在panorama.py中，其中我们将定义一个用于构建全景图的<strong class="jq hj"> <em class="ks"> Stitcher </em> </strong>类。</p><p id="4d7f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">Stitcher类将依赖于<a class="ae kr" href="https://github.com/jrosebr1/imutils" rel="noopener ugc nofollow" target="_blank"> imutils </a> Python包，所以如果您的系统上还没有安装它，那么您将希望现在就安装它:</p><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="f00a" class="lv ir hi lr b fi lw lx l ly lz"># import the necessary packages<br/>import numpy as np<br/>import imutils<br/>import cv2<br/>class Stitcher:<br/>def __init__(self):</span><span id="1647" class="lv ir hi lr b fi ma lx l ly lz"># determine if we are using OpenCV v3.X<br/>self.isv3 = imutils.is_cv3(or_better=True)<br/>def stitch(self, images, ratio=0.75, reprojThresh=4.0,showMatches=False):</span><span id="cf3c" class="lv ir hi lr b fi ma lx l ly lz"> # unpack the images, then detect keypoints and extract<br/> # local invariant descriptors from them<br/> (imageB, imageA) = images<br/> (kpsA, featuresA) = self.detectAndDescribe(imageA)<br/> (kpsB, featuresB) = self.detectAndDescribe(imageB)</span><span id="4807" class="lv ir hi lr b fi ma lx l ly lz"> # match features between the two images<br/> M = self.matchKeypoints(kpsA, kpsB, featuresA, featuresB, ratio, reprojThresh)</span><span id="def5" class="lv ir hi lr b fi ma lx l ly lz"> # if the match is None, then there aren’t enough matched<br/> # keypoints to create a panorama<br/> if M is None:<br/> return None</span></pre><p id="12b8" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">缝合方法只需要一个参数images，这是我们将要缝合在一起形成全景图的(两个)图像的列表。</p><p id="a647" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们还可以有选择地提供ratio，用于匹配特征时的David Lowe比率测试(在本教程的后面会详细介绍这个比率测试)，<strong class="jq hj"><em class="ks">repro thresh</em></strong>，这是RANSAC算法允许的最大像素“回旋空间”，最后是<strong class="jq hj"> <em class="ks"> showMatches </em> </strong>，这是一个布尔值，用于指示关键点匹配是否应该可视化。</p><p id="fe0c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">第4行</strong>解包图像列表(同样，我们假设它只包含两个图像)。图像列表的排序很重要:<strong class="jq hj">我们希望图像按照<em class="ks">从左到右</em>的顺序提供。</strong>如果图像是<em class="ks">而不是</em>按照这个顺序提供的，那么我们的代码仍然会运行——但是我们的输出全景图将只包含一个图像，而不是两个。</p><p id="bc74" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">一旦我们打开了图像列表，我们调用第5行<strong class="jq hj">和第6行</strong>上的<strong class="jq hj"> <em class="ks">检测和描述</em> </strong>方法。该方法简单地检测<strong class="jq hj"> <em class="ks">关键点</em> </strong>，并从两幅图像中提取局部不变描述符(即SIFT)。</p><p id="ea56" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">给定关键点和特征，我们使用<strong class="jq hj"> <em class="ks">匹配关键点</em> </strong> ( <strong class="jq hj">第9行和第10行</strong>)来匹配两幅图像中的特征。我们将在本课稍后定义此方法。</p><p id="5f97" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">如果返回的匹配M是None，那么没有足够的关键点被匹配来创建全景，所以我们简单地返回到调用函数。</p><p id="ac92" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">否则，我们现在准备应用透视变换:</p><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="f64e" class="lv ir hi lr b fi lw lx l ly lz"># otherwise, apply a perspective warp to stitch the images<br/># together</span><span id="ac66" class="lv ir hi lr b fi ma lx l ly lz">(matches, H, status) = M<br/>result = cv2.warpPerspective(imageA, H,<br/>(imageA.shape[1] + imageB.shape[1], imageA.shape[0]))<br/>result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB</span><span id="446c" class="lv ir hi lr b fi ma lx l ly lz"># check to see if the keypoint matches should be visualized<br/>if showMatches:<br/>vis = self.drawMatches(imageA, imageB, kpsA, kpsB, matches,status)</span><span id="f4b6" class="lv ir hi lr b fi ma lx l ly lz"># return a tuple of the stitched image and the<br/># visualization<br/>return (result, vis)</span><span id="6e14" class="lv ir hi lr b fi ma lx l ly lz"># return the stitched image<br/>return result</span></pre><p id="707a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">假设M不为None，我们在第30 行<strong class="jq hj">解包元组，给出关键点匹配的列表，从RANSAC算法导出的单应矩阵H，以及最后的状态，指示使用RANSAC成功地空间验证了匹配中的哪些关键点的索引列表。</strong></p><p id="f92b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">给定我们的单应矩阵H，我们现在准备将两幅图像缝合在一起。首先，我们调用<strong class="jq hj"><em class="ks">cv2 . warp perspective</em></strong>，它需要三个参数:我们要扭曲的图像(在本例中是<em class="ks">右</em>图像)、<em class="ks"> 3 x 3 </em>变换矩阵(H)，最后是输出图像的形状。我们通过对两幅图像的宽度求和，然后使用第二幅图像的高度，从输出图像中导出形状。</p><p id="d86c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">第2行</strong>检查我们是否应该可视化关键点匹配，如果是，我们调用<strong class="jq hj"> <em class="ks"> drawMatches </em> </strong>并将全景和可视化的元组返回给调用方法(<strong class="jq hj">第9–14行</strong>)。</p><p id="162c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">否则，我们只需返回拼接后的图像(<strong class="jq hj">第17行</strong>)。</p><p id="9747" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在已经定义了stitch方法，让我们看看它调用的一些辅助方法。我们将从<strong class="jq hj"> <em class="ks">检测开始，描述</em> </strong>:</p><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="f4c4" class="lv ir hi lr b fi lw lx l ly lz">def detectAndDescribe(self, image):</span><span id="22a4" class="lv ir hi lr b fi ma lx l ly lz"># convert the image to grayscale<br/>gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><span id="4a47" class="lv ir hi lr b fi ma lx l ly lz"># check to see if we are using OpenCV 3.X<br/>if self.isv3:</span><span id="50ac" class="lv ir hi lr b fi ma lx l ly lz"># detect and extract features from the image<br/>descriptor = cv2.xfeatures2d.SIFT_create()<br/>(kps, features) = descriptor.detectAndCompute(image, None)</span><span id="2d4e" class="lv ir hi lr b fi ma lx l ly lz"># otherwise, we are using OpenCV 2.4.X<br/>else:</span><span id="6bd1" class="lv ir hi lr b fi ma lx l ly lz"># detect keypoints in the image<br/>detector = cv2.FeatureDetector_create(“SIFT”)<br/>kps = detector.detect(gray)</span><span id="b527" class="lv ir hi lr b fi ma lx l ly lz"># extract features from the image<br/>extractor = cv2.DescriptorExtractor_create(“SIFT”)<br/>(kps, features) = extractor.compute(gray, kps)</span><span id="e841" class="lv ir hi lr b fi ma lx l ly lz"># convert the keypoints from KeyPoint objects to NumPy<br/># arrays<br/>kps = np.float32([kp.pt for kp in kps])</span><span id="7162" class="lv ir hi lr b fi ma lx l ly lz"># return a tuple of keypoints and features<br/>return (kps, features)</span></pre><p id="b41f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">顾名思义，detectAndDescribe方法接受图像，然后检测关键点并提取局部不变描述符。在我们的实现中，我们使用了<a class="ae kr" href="http://www.cs.utexas.edu/~grauman/courses/fall2009/papers/local_features_synthesis_draft.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj">高斯差分</strong> </a>(狗)关键点检测器和<a class="ae kr" href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj"> SIFT特征提取器</strong> </a> <strong class="jq hj">。</strong></p><p id="4695" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在<strong class="jq hj">第5行</strong>上，我们检查是否正在使用OpenCV 3.X。如果是，那么我们使用<strong class="jq hj"><em class="ks">cv2 . xfeatures 2d . SIFT _ create函数</em> </strong>来实例化我们的狗关键点检测器和SIFT特征提取器。对<strong class="jq hj"> <em class="ks">检测器和计算机</em> </strong>的调用处理提取关键点和特征(<strong class="jq hj">行54和55 </strong>)。</p><p id="d193" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">需要注意的是，你<strong class="jq hj"> <em class="ks">必须</em> </strong>已经编译了OpenCV 3。x与<a class="ae kr" href="https://github.com/itseez/opencv_contrib" rel="noopener ugc nofollow" target="_blank"><strong class="jq hj"><em class="ks">opencv _ contrib</em></strong></a>支持使能。否则，您将得到一个错误，如AttributeError:“模块”对象没有属性<strong class="jq hj"><em class="ks">“xfeatures 2d”</em></strong>。如果是这样的话，请前往我的<a class="ae kr" href="https://www.pyimagesearch.com/opencv-tutorials-resources-guides/" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj"> <em class="ks"> OpenCV 3教程页面</em> </strong> </a>，在那里我将详细介绍如何安装OpenCV 3，并为各种操作系统和Python版本启用<strong class="jq hj"><em class="ks">OpenCV _ contrib</em></strong>支持。</p><p id="9009" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">如果我们使用OpenCV 2.4，第11–18行处理。<strong class="jq hj">cv2<em class="ks">。FeatureDetector_create函数</em> </strong>实例化我们的关键点检测器(DoG)。调用detect返回我们的关键点集。</p><p id="f22c" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">从那里，我们需要初始化<strong class="jq hj"> <em class="ks"> cv2。使用SIFT关键字设置我们的SIFT特征提取器。调用提取器的compute方法会返回一组特征向量，这些向量量化了图像中每个检测到的关键点周围的区域。</em></strong></p><p id="a5c8" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">最后，我们的关键点从关键点对象转换成一个NumPy数组(<strong class="jq hj">行69 </strong>)并返回给调用方法(<strong class="jq hj">行72 </strong>)。</p><p id="6c1f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">接下来，我们来看看<strong class="jq hj"> <em class="ks">匹配关键点</em> </strong>的方法:</p><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="8db7" class="lv ir hi lr b fi lw lx l ly lz">def matchKeypoints(self, kpsA, kpsB, featuresA, featuresB,<br/>ratio, reprojThresh):</span><span id="2348" class="lv ir hi lr b fi ma lx l ly lz">     # compute the raw matches and initialize the list of actual<br/>     # matches<br/>     matcher = cv2.DescriptorMatcher_create(“BruteForce”)<br/>     rawMatches = matcher.knnMatch(featuresA, featuresB, 2)<br/>     matches = []</span><span id="c003" class="lv ir hi lr b fi ma lx l ly lz">     # loop over the raw matches<br/>     for m in rawMatches:<br/>        # ensure the distance is within a certain ratio of each<br/>        # other (i.e. Lowe’s ratio test)<br/>        if len(m) == 2 and m[0].distance &lt; m[1].distance * ratio:<br/>           matches.append((m[0].trainIdx, m[0].queryIdx))</span><span id="fea1" class="lv ir hi lr b fi ma lx l ly lz">     # computing a homography requires at least 4 matches<br/>     if len(matches) &gt; 4:<br/>        # construct the two sets of points<br/>        ptsA = np.float32([kpsA[i] for (_, i) in matches])<br/>        ptsB = np.float32([kpsB[i] for (i, _) in matches])</span><span id="c345" class="lv ir hi lr b fi ma lx l ly lz">     # compute the homography between the two sets of points<br/>     (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,<br/> reprojThresh)</span><span id="2cee" class="lv ir hi lr b fi ma lx l ly lz">     # return the matches along with the homograpy matrix<br/>     # and status of each matched point<br/>     return (matches, H, status)</span><span id="de6f" class="lv ir hi lr b fi ma lx l ly lz">     # otherwise, no homograpy could be computed<br/>     return None</span></pre><p id="37c4" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> <em class="ks">匹配关键点</em> </strong>函数需要四个自变量:与第一图像相关联的关键点和特征向量，随后是与第二图像相关联的关键点和特征向量。还提供了David Lowe的比率测试变量和RANSAC重投影阈值。</p><p id="8d45" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">将特征匹配在一起实际上是一个相当简单的过程。我们简单地循环两个图像的描述符，计算距离，并找到每对描述符的最小距离。由于这是计算机视觉中非常常见的做法，OpenCV有一个内置函数叫做<strong class="jq hj"> <em class="ks"> cv2。descriptor matcher _ create</em></strong>为我们构造了特征匹配器。BruteForce值表示我们将<em class="ks">彻底地</em>计算来自两幅图像的<em class="ks">所有特征向量</em>之间的欧几里德距离，并找到具有最小距离的描述符对。</p><p id="067a" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">对第5 行<strong class="jq hj">上的knnMatch的调用使用<em class="ks"> k=2 </em>在两个特征向量集之间执行<a class="ae kr" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj"> <em class="ks"> k-NN匹配</em> </strong> </a>(指示返回每个特征向量的前两个匹配)。</strong></p><p id="fa02" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们想要前两个匹配而不仅仅是前一个匹配的原因是因为我们需要应用David Lowe的比率测试来进行假阳性匹配修剪。</p><p id="1364" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">再次，<strong class="jq hj">第5行</strong>计算每对描述符的原始匹配——但是有可能这些对是假阳性的，这意味着图像补片实际上不是真正的匹配。在尝试修剪这些假阳性匹配时，我们可以单独循环每个原始匹配(<strong class="jq hj">第9行</strong>)，并应用Lowe比率测试，该测试用于确定高质量的特征匹配。劳氏比的典型值通常在<em class="ks">【0.7，0.8】</em>范围内。</p><p id="5855" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">计算两组点之间的单应性最少需要<em class="ks">四个匹配的初始组</em>。对于更可靠的单应性估计，我们应该具有实质上不止四个匹配点。</p><p id="62b6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">最后，我们的Stitcher方法中的最后一个方法drawMatches用于可视化两幅图像之间的关键点对应关系:</p><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="596b" class="lv ir hi lr b fi lw lx l ly lz">def drawMatches(self, imageA, imageB, kpsA, kpsB, matches, status):<br/>    # initialize the output visualization image<br/>    (hA, wA) = imageA.shape[:2]<br/>    (hB, wB) = imageB.shape[:2]<br/>    vis = np.zeros((max(hA, hB), wA + wB, 3), dtype=”uint8")<br/>    vis[0:hA, 0:wA] = imageA<br/>    vis[0:hB, wA:] = imageB</span><span id="bf87" class="lv ir hi lr b fi ma lx l ly lz">    # loop over the matches<br/>    for ((trainIdx, queryIdx), s) in zip(matches, status):<br/>       # only process the match if the keypoint was successfully<br/>       # matched<br/>       if s == 1:<br/>         # draw the match<br/>         ptA = (int(kpsA[queryIdx][0]), int(kpsA[queryIdx][1]))<br/>         ptB = (int(kpsB[trainIdx][0]) + wA, int(kpsB[trainIdx][1]))<br/>         cv2.line(vis, ptA, ptB, (0, 255, 0), 1)</span><span id="4d44" class="lv ir hi lr b fi ma lx l ly lz">    # return the visualization<br/>    return vis</span></pre><p id="4e01" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">该方法要求我们传入两幅原始图像、与每幅图像相关联的一组关键点、应用劳氏比率测试后的初始匹配，以及最后由单应性计算提供的状态列表。使用这些变量，我们可以通过从第一幅图像中的关键点<em class="ks"> N </em>到第二幅图像中的关键点<em class="ks"> M </em>画一条直线来可视化“内侧”关键点。</p><p id="1132" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在我们已经定义了Stitcher类，让我们继续创建stitch.py驱动程序脚本:</p><pre class="ku kv kw kx fd lq lr ls lt aw lu bi"><span id="f7df" class="lv ir hi lr b fi lw lx l ly lz"># import the necessary packages<br/>from pyimagesearch.panorama import Stitcher<br/>import argparse<br/>import imutils<br/>import cv2</span><span id="1139" class="lv ir hi lr b fi ma lx l ly lz"># construct the argument parse and parse the arguments<br/>ap = argparse.ArgumentParser()<br/>ap.add_argument(“-f”, “ — first”, required=True,<br/> help=”path to the first image”)<br/>ap.add_argument(“-s”, “ — second”, required=True,<br/> help=”path to the second image”)<br/>args = vars(ap.parse_args())</span><span id="96bd" class="lv ir hi lr b fi ma lx l ly lz"># load the two images and resize them to have a width of 400 pixels<br/># (for faster processing)<br/>imageA = cv2.imread(args[“first”])<br/>imageB = cv2.imread(args[“second”])<br/>imageA = imutils.resize(imageA, width=400)<br/>imageB = imutils.resize(imageB, width=400)</span><span id="99ed" class="lv ir hi lr b fi ma lx l ly lz"># stitch the images together to create a panorama<br/>stitcher = Stitcher()<br/>(result, vis) = stitcher.stitch([imageA, imageB], showMatches=True)</span><span id="62dc" class="lv ir hi lr b fi ma lx l ly lz"># show the images<br/>cv2.imshow(“Image A”, imageA)<br/>cv2.imshow(“Image B”, imageB)<br/>cv2.imshow(“Keypoint Matches”, vis)<br/>cv2.imshow(“Result”, result)<br/>cv2.waitKey(0)</span></pre><p id="fcbb" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们从在<strong class="jq hj">2–5号线</strong>进口我们需要的包装开始。请注意，我们如何将panorama.py和Stitcher类放入pyimagesearch模块中，只是为了保持代码整洁。</p><p id="3284" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> <em class="ks">注意:</em> </strong> <em class="ks">如果你在阅读这篇文章时遇到了组织代码的困难，请务必使用这篇文章底部的表格下载源代码。的。代码下载的zip文件将立即运行，不会出现任何错误。</em></p><p id="8b58" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">从那里，<strong class="jq hj">第8–14行</strong>解析我们的命令行参数:—首先，这是全景图中第一幅图像的路径(最左边的<em class="ks">图像)，其次，是全景图中第二幅图像的路径(最右边的</em>图像)。</p><p id="5efd" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj">记住，这些图像路径需要按照<em class="ks">从左到右</em>的顺序提供！</strong></p><h1 id="6119" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">全景拼接结果:</strong></h1><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/e0ff70e8183f9a2f991659d5616a0310.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u4Wr5anQjYqXP2BPMNCpyg.png"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">两幅图像之间匹配的关键点对应关系。</figcaption></figure><p id="37fc" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">上图显示了两幅图像之间的关键点关系。您可以观察到，代码很容易识别两幅图像之间的所有关键点，并且使用这些图像，它可以很容易地缝合图像。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/57904346dbcf2025f8bf0224564aa7be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0mW1pORoweYyt87zxv6EA.jpeg"/></div></div></figure><p id="148e" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在上面的输入图像中，我们可以看到两幅输入图像之间有很大的重叠。全景图的主要增加部分是在拼接图像的右侧，我们可以看到更多的“突出部分”被添加到输出中。</p><figure class="ku kv kw kx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/32648828769847ba81fd5059f3626da9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TnnHCnbk8H5yEECYiE4ffw.jpeg"/></div></div></figure><p id="d0c1" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">现在你有了，使用Python和OpenCV的图像拼接和全景图构建！</p><h1 id="2b03" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">总结:</h1><p id="e7ae" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在这篇博文中，我们学习了如何使用OpenCV执行图像拼接和全景构建。为<em class="ks">OpenCV 2.4和OpenCV 3提供了图像拼接的源代码。</em></p><p id="b944" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们的图像拼接算法需要四个步骤:(1)检测关键点和提取局部不变描述符；(2)图像间的描述符匹配；(3)应用RANSAC估计单应矩阵；以及(4)使用单应矩阵应用扭曲变换。</p><p id="cf69" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">虽然简单，但在为两幅图像构建全景图时，该算法在实践中效果很好。在未来的博客文章中，我们将回顾如何构建全景图，以及如何为两张以上的图像执行图像拼接。</p><p id="1800" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">无论如何，我希望你喜欢这篇文章！</p></div></div>    
</body>
</html>