<html>
<head>
<title>Training a double-jointed arm with DDPG — Deep Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用DDPG训练双关节手臂——深度强化学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/training-a-double-jointed-arm-with-ddpg-deep-reinforcement-learning-e25529a5915b?source=collection_archive---------18-----------------------#2020-07-18">https://medium.com/analytics-vidhya/training-a-double-jointed-arm-with-ddpg-deep-reinforcement-learning-e25529a5915b?source=collection_archive---------18-----------------------#2020-07-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="20e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">今天我们将看到另一种强大的深度强化学习算法。</p><p id="ffbf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重要的是你能知道关于深度强化学习的基础知识，你可以看我上一篇文章<a class="ae jd" rel="noopener" href="/analytics-vidhya/collecting-bananas-with-drl-agent-bdd8bf216d11?source=friends_link&amp;sk=c1c829667c90e9a535af585864f083f2">这里</a>(用深度强化学习收集香蕉)。</p><p id="eef3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们开始之前，我将与您分享一些关于深度强化学习的更多信息。很快，我保证！</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/2a39487ffeebf36adeb65e48fa5cd154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*-Jp_qGo-BBmDs_jU.gif"/></div></figure><h1 id="358f" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">理论第一</h1><p id="6cef" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">强化学习是来自人工智能的一个子类，这个子类有不同类型的算法。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es kp"><img src="../Images/697ba71b01a22bec329fe9ac2d6185c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zLaKarYx3unc21nQO1XPRw.jpeg"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">强化学习的类型</figcaption></figure><p id="dee5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">了解这些类型很重要，因为每种类型都有自己的学习方式。</p><h2 id="b7f3" class="ky jn hi bd jo kz la lb js lc ld le jw iq lf lg ka iu lh li ke iy lj lk ki ll bi translated">基于价值的</h2><p id="8c23" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">估计最优值函数<em class="lm"> Q* (s，a)。</em></p><p id="795c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是在任何策略下都能达到的最大值。</p><h2 id="1448" class="ky jn hi bd jo kz la lb js lc ld le jw iq lf lg ka iu lh li ke iy lj lk ki ll bi translated">基于政策</h2><p id="131a" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">直接搜索最优策略<em class="lm"> π* </em>。</p><p id="4cd0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是实现未来最大回报的政策。</p><h2 id="48e6" class="ky jn hi bd jo kz la lb js lc ld le jw iq lf lg ka iu lh li ke iy lj lk ki ll bi translated">演员兼评论家</h2><p id="e1c2" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">我们结合了基于价值和基于政策的方法。</p><p id="1b65" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">行动者基于政策，批评者基于价值。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="er es ln"><img src="../Images/83c04c2c738cf86cfc1d5a6574e27fc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Gnx9eyqoZdyi7G5I"/></div></div><figcaption class="ku kv et er es kw kx bd b be z dx translated"><a class="ae jd" rel="noopener" href="/free-code-camp/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d">优势演员评论方法简介:我们来玩刺猬索尼克吧！</a></figcaption></figure><p id="2054" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个项目中，我使用了演员-评论家的方法。选择的算法是DDPG(深度确定性策略梯度)。</p><p id="824d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与强化学习的基本方法相比，DDPG的工作方式略有不同。我们将在代理内部有两个神经网络。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lo"><img src="../Images/0c6452bc630a1e0ae875a3cca91c0155.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/0*Mmhgtkak888G6bW7"/></div></figure><p id="e31e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们进一步了解环境和双关节臂！</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lp"><img src="../Images/e3d7009216304d76973b4f3d48980999.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/0*Kiqh2B0BiH382Pmf.gif"/></div></figure><p id="4f36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗯，我们这里的环境比上面的GIF要好。我用了Unity的Reacher环境。</p><h1 id="63d3" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">环境</h1><p id="f796" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">在这种环境下，双关节手臂可以移动到目标位置。</p><p id="2862" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一个版本包含单个代理。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lq"><img src="../Images/c2f284c435dc58074efb8d75861bd810.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*A5z6DtO3DCcnW10h"/></div></figure><p id="c4aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二个版本包含20个相同的代理，每个代理都有自己的环境副本。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/dbeb7606a3b0841ff0a5ebe8e14f3258.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*egCoqwOVPGwdHKN6"/></div></figure><p id="4fd1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这种环境，DDPG是一个很好的方法，因为我们可以有不止一个代理。我们可以向所有代理学习，也可以与所有代理分享知识。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lr"><img src="../Images/c3d2fb822c8b40e22f248fcef6b9ce04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1MbQlf3VyTiOH4UF.png"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated"><a class="ae jd" href="https://www.groundai.com/project/deep-reinforcement-learning-for-multi-agent-systems-a-review-of-challenges-solutions-and-applications/1" rel="noopener ugc nofollow" target="_blank">多智能体系统的深度强化学习</a></figcaption></figure><h2 id="e3a7" class="ky jn hi bd jo kz la lb js lc ld le jw iq lf lg ka iu lh li ke iy lj lk ki ll bi translated">状态矢量空间</h2><p id="139b" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">观察空间由33个变量组成，对应于手臂的位置、旋转、速度和角速度。</p><h2 id="8818" class="ky jn hi bd jo kz la lb js lc ld le jw iq lf lg ka iu lh li ke iy lj lk ki ll bi translated">行动</h2><p id="5da7" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">每个动作都是一个有四个数字的向量，对应着适用于两个关节的扭矩。动作向量中的每个条目都应该是介于-1和1之间的数字。</p><h2 id="053b" class="ky jn hi bd jo kz la lb js lc ld le jw iq lf lg ka iu lh li ke iy lj lk ki ll bi translated">报酬</h2><p id="4cc3" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">代理人的手在目标位置的每一步提供+0.1的奖励。因此，代理的目标是在尽可能多的时间步长内保持其在目标位置的位置。</p><p id="c9be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解决这个环境，我们需要在最后100集有30+的分数。</p></div><div class="ab cl ls lt gp lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="hb hc hd he hf"><h1 id="9be1" class="jm jn hi bd jo jp lz jr js jt ma jv jw jx mb jz ka kb mc kd ke kf md kh ki kj bi translated">代理体系结构</h1><p id="d863" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">在这个项目上创建的代理由一个带有<strong class="ih hj">演员模型</strong>的<strong class="ih hj">代理</strong>，一个<strong class="ih hj">评论家模型</strong>，一个<strong class="ih hj">探索策略</strong>和一个<strong class="ih hj">存储单元</strong>组成。</p><h2 id="236a" class="ky jn hi bd jo kz la lb js lc ld le jw iq lf lg ka iu lh li ke iy lj lk ki ll bi translated"><strong class="ak">代理人</strong></h2><p id="1d01" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">代理具有与环境交互的方法:<code class="du me mf mg mh b">step()</code>、<code class="du me mf mg mh b">act()</code>、<code class="du me mf mg mh b">learn()</code>等。</p><p id="d9b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">行动者模型、批评家模型、探索策略和记忆单元将是代理的一部分(作为属性)。</p><h2 id="120d" class="ky jn hi bd jo kz la lb js lc ld le jw iq lf lg ka iu lh li ke iy lj lk ki ll bi translated"><strong class="ak">演员模型</strong></h2><p id="9048" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">这个神经网络有三层。</p><p id="e867" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输入层</strong>将接收状态空间中的所有33个特征。</p><p id="eb7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输出层</strong>将根据动作空间的要求提供一个4元素的数组。</p><pre class="jf jg jh ji fd mi mh mj mk aw ml bi"><span id="907e" class="ky jn hi mh b fi mm mn l mo mp">Actor(<br/>(fc1): Linear(in_features=33, out_features=256, bias=True)     (fc2): Linear(in_features=256, out_features=128, bias=True)     (fc3): Linear(in_features=128, out_features=4, bias=True)<br/>)</span></pre><h2 id="0bc0" class="ky jn hi bd jo kz la lb js lc ld le jw iq lf lg ka iu lh li ke iy lj lk ki ll bi translated">批评家模型</h2><p id="99c2" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">对于Critic神经网络，我们也有3层。</p><p id="3a71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输入层</strong>将接收状态空间中的所有33个特征。</p><p id="919e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">输出层</strong>将只提供一个值来指导演员模型的学习。</p><pre class="jf jg jh ji fd mi mh mj mk aw ml bi"><span id="fd93" class="ky jn hi mh b fi mm mn l mo mp">Critic(<br/>(fc1): Linear(in_features=33, out_features=256, bias=True)<br/>(fc2): Linear(in_features=260, out_features=128, bias=True)<br/>(fc3): Linear(in_features=128, out_features=1, bias=True)<br/>)</span></pre><p id="242b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些神经网络是https://pytorch.org/用<a class="ae jd" href="https://pytorch.org/" rel="noopener ugc nofollow" target="_blank">开发的</a></p><h2 id="cb54" class="ky jn hi bd jo kz la lb js lc ld le jw iq lf lg ka iu lh li ke iy lj lk ki ll bi translated"><strong class="ak">存储单元</strong></h2><p id="467b" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">通过两种简单的方法<code class="du me mf mg mh b">add()</code>和<code class="du me mf mg mh b">sample()</code>，该存储器可以存储经验，也可以返回一些随机的经验用于代理培训。</p><h2 id="a3f9" class="ky jn hi bd jo kz la lb js lc ld le jw iq lf lg ka iu lh li ke iy lj lk ki ll bi translated">勘探政策</h2><p id="445c" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">探索策略对我们的代理很重要，因为它将帮助代理尝试几种行动结果，并避免在局部最小值上停止。</p><p id="afa2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我的探索政策基于<a class="ae jd" href="https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process" rel="noopener ugc nofollow" target="_blank">奥恩斯坦-乌伦贝克过程</a>。</p><p id="0931" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们看看我们的多智能体DDPG算法如何解决这个环境。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/40da4123b3dd20441b6072ca48ebfe75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*VYgHUJQ33J7xpc4T.gif"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">我的手臂训练得很狡猾！</figcaption></figure><p id="8aa9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">多智能体方法每集都要花很长时间，你知道，我们有几个计算要做，两个神经网络要学习，但我们用56集完成了这个环境！</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es mq"><img src="../Images/8881142fb3fd7faeab5eb54e5a061902.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/0*VEZK1N9ddFNdjRSP"/></div></figure><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/88ce87730e61d8c2a47aeed19662405a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/0*FeRpEjQChGa2hzii.gif"/></div><figcaption class="ku kv et er es kw kx bd b be z dx translated">我的代理人当我们完成了环境</figcaption></figure><p id="95a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在我的GitHub的仓库里找到代码。</p><div class="mr ms ez fb mt mu"><a href="https://github.com/DougTrajano/ds_drl_continuous_control" rel="noopener  ugc nofollow" target="_blank"><div class="mv ab dw"><div class="mw ab mx cl cj my"><h2 class="bd hj fi z dy mz ea eb na ed ef hh bi translated">DougTrajano/ds _ drl _ continuous _ control</h2><div class="nb l"><h3 class="bd b fi z dy mz ea eb na ed ef dx translated">这个项目包含一个基于深度强化学习的代理，它可以从零(无标记数据)学习到…</h3></div><div class="nc l"><p class="bd b fp z dy mz ea eb na ed ef dx translated">github.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni jk mu"/></div></div></a></div><h1 id="6c1d" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">对未来工作的想法</h1><h2 id="3b1b" class="ky jn hi bd jo kz la lb js lc ld le jw iq lf lg ka iu lh li ke iy lj lk ki ll bi translated">超参数的新组合</h2><p id="451a" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">这种算法需要太多的时间来学习，测试几个超参数的组合非常复杂，但是我们可以继续训练过程来找到最优的超参数。</p><h2 id="658c" class="ky jn hi bd jo kz la lb js lc ld le jw iq lf lg ka iu lh li ke iy lj lk ki ll bi translated">实施优先体验重放</h2><p id="d813" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">如果你读了我的上一篇文章，你就会知道我在那里使用了优先体验重放。这种记忆非常好，可以为代理提供我们在培训过程中受益更多的经验，但这会增加代理的培训时间。</p><p id="93b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个优先化的存储器在这里被描述为<a class="ae jd" href="https://arxiv.org/pdf/1511.05952.pdf" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="fb80" class="ky jn hi bd jo kz la lb js lc ld le jw iq lf lg ka iu lh li ke iy lj lk ki ll bi translated">改变勘探政策</h2><p id="6727" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">我使用了DDPG最初的作者推荐的奥恩斯坦-乌伦贝克过程，但最近的论文说，我们使用奥恩斯坦-乌伦贝克过程而不是高斯噪声没有好处。大概是可以提高代理性能吧。</p></div><div class="ab cl ls lt gp lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="hb hc hd he hf"><p id="9a7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那都是乡亲们！非常感谢您阅读它，不要忘记在下面发送您的评论！</p></div></div>    
</body>
</html>