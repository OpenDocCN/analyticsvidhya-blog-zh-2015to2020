<html>
<head>
<title>Hello World of Computer Vision: MNIST handwritten digit classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你好，计算机视觉世界:MNIST手写数字分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/hello-world-of-computer-vision-mnist-handwritten-digit-classification-a5c28f6276e1?source=collection_archive---------17-----------------------#2020-09-19">https://medium.com/analytics-vidhya/hello-world-of-computer-vision-mnist-handwritten-digit-classification-a5c28f6276e1?source=collection_archive---------17-----------------------#2020-09-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/835f6213201ffe8805fd736b6e2107d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9tLBAgfIMQ6WWvux_8ov-w.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由<a class="ae iu" href="https://unsplash.com/@popnzebra?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">波普&amp;斑马</a>在<a class="ae iu" href="https://unsplash.com/s/photos/handwritten?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">挡泥板</a>上拍摄</figcaption></figure><p id="e7b0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated"><span class="l ju jv jw bm jx jy jz ka kb di">在</span>这个故事中，我们将处理传说中的MNIST数据集。这个数据集在机器学习初学者中相当受欢迎。事实上，这个数据集如此受欢迎，以至于它经常被称为机器学习或计算机视觉的<strong class="ix hj">“Hello World】</strong>。</p><p id="f554" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个故事也将给出一个关于机器学习中分类问题的思路。最后，你将能够回答一些关于如何处理多类分类的常见问题？应该使用什么性能指标？如何提高性能？。</p><p id="9312" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">目录</strong></p><ol class=""><li id="cf47" class="kc kd hi ix b iy iz jc jd jg ke jk kf jo kg js kh ki kj kk bi translated">引言。</li><li id="e674" class="kc kd hi ix b iy kl jc km jg kn jk ko jo kp js kh ki kj kk bi translated">导入库。</li><li id="9220" class="kc kd hi ix b iy kl jc km jg kn jk ko jo kp js kh ki kj kk bi translated">正在加载数据集。</li><li id="6ba1" class="kc kd hi ix b iy kl jc km jg kn jk ko jo kp js kh ki kj kk bi translated">探索数据集。</li><li id="5cfb" class="kc kd hi ix b iy kl jc km jg kn jk ko jo kp js kh ki kj kk bi translated">将数据集分成训练集和测试集。</li><li id="5c12" class="kc kd hi ix b iy kl jc km jg kn jk ko jo kp js kh ki kj kk bi translated">二元分类。</li><li id="a41a" class="kc kd hi ix b iy kl jc km jg kn jk ko jo kp js kh ki kj kk bi translated">多类分类。</li><li id="9ba1" class="kc kd hi ix b iy kl jc km jg kn jk ko jo kp js kh ki kj kk bi translated">分析。</li></ol><h1 id="268a" class="kq kr hi bd ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln bi translated"><strong class="ak"> 1。简介</strong></h1><p id="2ec5" class="pw-post-body-paragraph iv iw hi ix b iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated">修改后的国家标准与技术研究所(MNIST)数据集是一个由70，000幅手写数字图像组成的大型数据集。该数据集是通过修改原始NIST数据集创建的。最初的NIST数据集包含美国人口普查局员工用于训练的手写数字，而测试集包含高中生写的数字，这使得数据集不适合机器学习。因此，数据集被修改，以便训练和测试集包含人口普查局员工和高中生写的数字。</p><p id="41c2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的工作是建立一个能够识别这些手写数字的分类器。所以让我们开始吧。</p></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><h1 id="47a7" class="kq kr hi bd ks kt ma kv kw kx mb kz la lb mc ld le lf md lh li lj me ll lm ln bi translated">2.导入库</h1><p id="ef14" class="pw-post-body-paragraph iv iw hi ix b iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated">首先，我们需要导入所需的库，对于我们的任务，我们将主要使用<em class="mf"> NumPy </em>、<em class="mf"> pandas、scikit-learn </em>和<em class="mf"> matplotlib </em>库。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="2a88" class="mp kr hi ml b fi mq mr l ms mt"><em class="mf"># for handling arrays and dataframe</em><br/><strong class="ml hj">import</strong> <strong class="ml hj">numpy</strong> <strong class="ml hj">as</strong> <strong class="ml hj">np </strong><br/><strong class="ml hj">import</strong> <strong class="ml hj">pandas</strong> <strong class="ml hj">as</strong> <strong class="ml hj">pd</strong></span><span id="fd27" class="mp kr hi ml b fi mu mr l ms mt"><em class="mf"># for ploting</em><strong class="ml hj"><em class="mf"> <br/></em>import</strong> <strong class="ml hj">matplotlib</strong> <strong class="ml hj">as</strong> <strong class="ml hj">mpl</strong><br/><strong class="ml hj">import</strong> <strong class="ml hj">matplotlib.pyplot</strong> <strong class="ml hj">as</strong> <strong class="ml hj">plt</strong></span><span id="b939" class="mp kr hi ml b fi mu mr l ms mt"><em class="mf"># to load data</em><br/><strong class="ml hj">from</strong> <strong class="ml hj">sklearn.datasets</strong> <strong class="ml hj">import</strong> fetch_openml</span><span id="9e25" class="mp kr hi ml b fi mu mr l ms mt"><em class="mf"># to standardize data</em><br/><strong class="ml hj">from</strong> <strong class="ml hj">sklearn.preprocessing</strong> <strong class="ml hj">import</strong> StandardScaler</span><span id="8de1" class="mp kr hi ml b fi mu mr l ms mt"><em class="mf"># for Cross validation and Grid Searching<br/></em><strong class="ml hj">from</strong> <strong class="ml hj">sklearn.model_selection</strong> <strong class="ml hj">import</strong> cross_val_score, cross_val_predict, GridSearchCV</span><span id="40c4" class="mp kr hi ml b fi mu mr l ms mt"><em class="mf"># classifiers<br/></em><strong class="ml hj">from</strong> <strong class="ml hj">sklearn.linear_model</strong> <strong class="ml hj">import</strong> LogisticRegression<br/><strong class="ml hj">from</strong> <strong class="ml hj">sklearn.multiclass</strong> <strong class="ml hj">import</strong> OneVsOneClassifier<br/><strong class="ml hj">from</strong> <strong class="ml hj">sklearn.ensemble</strong> <strong class="ml hj">import</strong> RandomForestClassifier<br/><strong class="ml hj">import</strong> <strong class="ml hj">xgboost</strong><br/><strong class="ml hj">from</strong> <strong class="ml hj">xgboost</strong> <strong class="ml hj">import</strong> XGBClassifier<br/><strong class="ml hj">from sklearn.neighbors import </strong>KNeighborsClassifier</span><span id="95dd" class="mp kr hi ml b fi mu mr l ms mt"><em class="mf"># metrics to evaluate models<br/></em><strong class="ml hj">from</strong> <strong class="ml hj">sklearn.metrics</strong> <strong class="ml hj">import</strong> precision_score, recall_score, f1_score, confusion_matrix</span></pre></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><h1 id="5f22" class="kq kr hi bd ks kt ma kv kw kx mb kz la lb mc ld le lf md lh li lj me ll lm ln bi translated">3.正在加载数据集</h1><p id="a555" class="pw-post-body-paragraph iv iw hi ix b iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated">我们可以从多个来源下载数据，也可以使用Scikit-Learn库。现在，我们将使用后一个选项，因为它非常简单。人们可以使用Scikit-Learn提供的帮助函数下载许多流行的数据集。代码如下:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="632f" class="mp kr hi ml b fi mq mr l ms mt"><strong class="ml hj"><em class="mf">#from sklearn.datasets import fetch_openml</em></strong><em class="mf"><br/></em>mnist = fetch_openml('mnist_784')<br/>mnist.keys()<strong class="ml hj"> <em class="mf">#to get keys</em></strong></span></pre><p id="bfc0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[1]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="c6f8" class="mp kr hi ml b fi mq mr l ms mt">dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'DESCR', 'details', 'categories', 'url'])</span></pre></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><h1 id="561d" class="kq kr hi bd ks kt ma kv kw kx mb kz la lb mc ld le lf md lh li lj me ll lm ln bi translated"><strong class="ak"> 4。探索数据。</strong></h1><p id="3778" class="pw-post-body-paragraph iv iw hi ix b iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated">让我们将数据分解成独立和相关(目标)变量。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="87dd" class="mp kr hi ml b fi mq mr l ms mt">X,y= mnist["data"],mnist["target"]<br/>print(X.shape,y.shape)</span></pre><p id="6e9b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[2]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="37e0" class="mp kr hi ml b fi mq mr l ms mt">(70000, 784) (70000,)</span></pre><p id="2da5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，我们的数据有784个特征，这意味着每张图像有784 (28x28)个像素。这意味着图像非常小。并且由于图像是黑白的，每个特征将具有0到255之间的值。</p><p id="dc94" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们看看这些图像。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="6228" class="mp kr hi ml b fi mq mr l ms mt"><strong class="ml hj">def</strong> showDigits(instances, images_per_row=20):<br/>    images_per_row = min(len(instances), images_per_row)<br/>    images = [instance.reshape(28,28) <strong class="ml hj">for</strong> instance <strong class="ml hj">in</strong> instances]<br/>    n_rows = (len(instances) - 1) // images_per_row + 1<br/>    row_images = []<br/>    n_empty = n_rows * images_per_row - len(instances)<br/>    images.append(np.zeros((28, 28 * n_empty)))<br/>    <strong class="ml hj">for</strong> row <strong class="ml hj">in</strong> range(n_rows):<br/>        rimages = images[row * images_per_row : (row + 1) * images_per_row]<br/>        row_images.append(np.concatenate(rimages, axis=1))<br/>    image = np.concatenate(row_images, axis=0)<br/>    plt.imshow(image, cmap = mpl.cm.binary)<br/>    plt.axis("off")<br/><br/>plt.figure(figsize=(20,10))<br/>example_images = X[:100]<br/>showDigits(example_images)<br/>plt.show()</span></pre><p id="529e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[3]:</p><figure class="mg mh mi mj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mv"><img src="../Images/903586977ac1d594dba68a95d307155d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TAaOMK8Vc39ULw3Wp06zeg.png"/></div></div></figure><p id="00f8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在继续之前，让我们仔细看看。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="02fb" class="mp kr hi ml b fi mq mr l ms mt">randDigit = X[27]<br/>randDigitImage = randDigit.reshape(28,28)<br/>plt.imshow(randDigitImage, cmap=mpl.cm.binary, interpolation = "nearest")<br/>plt.axis("off")<br/>plt.show()</span></pre><figure class="mg mh mi mj fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/389683f35941f9614fa873b7e67125bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*wh6n5K7PCPGyyN8CJy8-ZA.png"/></div></figure><p id="a2f4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个很清楚，我们可以说它是一个“3”。但并非每个数字都是如此。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="173b" class="mp kr hi ml b fi mq mr l ms mt">y[27]</span></pre><p id="0bfe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[4 ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="e2b4" class="mp kr hi ml b fi mq mr l ms mt">'3'</span></pre><p id="fbeb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们所料，它被标记为3。让我们看另一个图像。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="fba0" class="mp kr hi ml b fi mq mr l ms mt">randDigit = X[27]<br/>randDigits=np.array([randDigit,randDigit2])"""<strong class="ml hj"><em class="mf">lookout for this in later code"""</em></strong></span><span id="14ea" class="mp kr hi ml b fi mu mr l ms mt">randDigitImage = randDigit.reshape(28,28)<br/>plt.imshow(randDigitImage, cmap=mpl.cm.binary, interpolation = "nearest")<br/>plt.axis("off")<br/>plt.show()</span></pre><figure class="mg mh mi mj fd ij er es paragraph-image"><div class="er es mw"><img src="../Images/05ff1c4d75c40a34e60b465d48b8c4f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:462/format:webp/1*aM-cJI0iJn8KTLdgOAR9Bg.png"/></div></figure><p id="33ae" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，这个看起来有点混乱，不管它是1还是7。嗯，看起来更像是7。我们去看看。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="0428" class="mp kr hi ml b fi mq mr l ms mt">y[42]</span></pre><p id="1bad" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ 5]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="e312" class="mp kr hi ml b fi mq mr l ms mt">'7'</span></pre><p id="adca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">嗯，我们猜对了，但电脑可能不会。</p><p id="ff2b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还有，这里有一点要注意，我们的标签是字符串值，让我们把它们转换成整数值。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="d0d0" class="mp kr hi ml b fi mq mr l ms mt">y=y.astype(np.uint8)</span></pre></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><h1 id="17d8" class="kq kr hi bd ks kt ma kv kw kx mb kz la lb mc ld le lf md lh li lj me ll lm ln bi translated">5.将数据集分成训练集和测试集。</h1><p id="c1cc" class="pw-post-body-paragraph iv iw hi ix b iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated">MNIST数据集已经分为训练集和测试集。前60，000个值用于训练，其余10，000个值用于测试。集合已经为我们洗牌，这意味着所有交叉验证折叠将包含每个数字的实例。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="138a" class="mp kr hi ml b fi mq mr l ms mt">XTrain, XTest, yTrain, yTest = X[:60000],   X[60000:],y[0:60000],y[60000:]</span></pre></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><h1 id="1c8e" class="kq kr hi bd ks kt ma kv kw kx mb kz la lb mc ld le lf md lh li lj me ll lm ln bi translated">6.二元分类。</h1><p id="b064" class="pw-post-body-paragraph iv iw hi ix b iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated">在构建多类分类器之前，让我们尝试为数字3构建一个二元分类器。这个分类器将预测一个给定的数字是否是“3”。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="9533" class="mp kr hi ml b fi mq mr l ms mt">yTrain_3, tTest_3 = (yTrain==3), (yTest==3)</span></pre><p id="49b8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这将创建该任务所需的目标向量。</p><p id="5ed1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们挑选一个分类器并训练它。我们可以用逻辑回归。因此，让我们创建一个逻辑回归器，并对整个训练数据进行训练。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="1922" class="mp kr hi ml b fi mq mr l ms mt">lr_clf = LogisticRegression(random_state=42)<br/>lr_clf.fit(XTrain, yTrain_3)</span><span id="f326" class="mp kr hi ml b fi mu mr l ms mt">lr_clf.predict(randDigits)</span></pre><p id="e2b8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="fd19" class="mp kr hi ml b fi mq mr l ms mt">array([ True, False])</span></pre><p id="7ecc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的模型预测3为真，7为假。很好，不是吗！让我们用交叉验证分数来了解更多。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="068f" class="mp kr hi ml b fi mq mr l ms mt">cross_val_score(lr_clf, XTrain, yTrain_3, cv=3, scoring="accuracy")</span></pre><p id="3212" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="89d4" class="mp kr hi ml b fi mq mr l ms mt">array([0.9714 , 0.97265, 0.9716 ])</span></pre><p id="1d2a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">看起来好得难以置信。准确率达到97%以上是相当惊人的。等等，让我们看看混乱矩阵。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="b8b0" class="mp kr hi ml b fi mq mr l ms mt">yPred_3 = cross_val_predict(lr_clf, XTrain, yTrain_3, cv=3)<br/>confusion_matrix(yTrain_3, yPred_3)</span></pre><p id="0bca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="da7f" class="mp kr hi ml b fi mq mr l ms mt">array([[53201,   668],<br/>       [ 1019,  5112]])</span></pre><p id="e5c3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在不太好了。让我们也检查一下f1的分数。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="5df3" class="mp kr hi ml b fi mq mr l ms mt">f1_score(yTrain_3, yPred_3)</span></pre><p id="af1a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="5430" class="mp kr hi ml b fi mq mr l ms mt">0.8583662161027621</span></pre><p id="842c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">事实是，我们的模型有超过95%的准确率，因为只有大约10%的图像是数字“3”。这就是为什么我们不喜欢将准确度作为分类器的性能度量。</p><p id="75ee" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们总是可以通过使用更复杂的算法或调整一些超参数来增加我们的分数。现在让我们转到多类分类器。</p></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><h1 id="f680" class="kq kr hi bd ks kt ma kv kw kx mb kz la lb mc ld le lf md lh li lj me ll lm ln bi translated">7.多类分类。</h1><p id="2a31" class="pw-post-body-paragraph iv iw hi ix b iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated">我们的工作是对一个给定的数字进行分类，也就是说我们需要区分10个类别。我们可以通过使用能够直接处理多个类的分类器来实现这一点，或者我们可以使用带有二元分类器的<em class="mf">一对一(OvA) </em>策略。</p><p id="1a01" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们先试试OvA。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="ee69" class="mp kr hi ml b fi mq mr l ms mt">ovr_clf= OneVsRestClassifier(LogisticRegression(random_state=42))<br/>ovr_clf.fit(XTrain,yTrain)<br/>ovr_clf.predict(randDigits)</span></pre><p id="6d27" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="b9dd" class="mp kr hi ml b fi mq mr l ms mt">array([3, 7], dtype=uint8)</span></pre><p id="c294" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的新模型正确预测了数字“3”和“7”。让我们检查它的交叉验证分数。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="1746" class="mp kr hi ml b fi mq mr l ms mt">cross_val_score(ovr_clf,XTrain, yTrain,cv=3, scoring="accuracy")</span></pre><p id="2388" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="4660" class="mp kr hi ml b fi mq mr l ms mt">array([0.90935, 0.9073 , 0.91305])</span></pre><p id="c93f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它在所有折叠上都达到了90%以上的准确率，还不错。但是我们可以通过缩放数据来做得更好。</p><p id="cdbe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们试试<em class="mf"> KNN分类器</em>。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="a3cd" class="mp kr hi ml b fi mq mr l ms mt">knn = KNeighborsClassifier()<br/>knn.fit(XTrain, yTrain)<br/>cross_val_score(knn, XTrain, yTrain, cv=3, scoring="accuracy")</span></pre><p id="755e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="3b84" class="mp kr hi ml b fi mq mr l ms mt">array([0.9676 , 0.9671 , 0.96755])</span></pre><p id="4c69" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">不错！我们的分类器表现良好。我们得到了96%以上的平均准确率。</p><p id="2aab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们还将尝试<em class="mf">随机森林分类器</em>。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="8b91" class="mp kr hi ml b fi mq mr l ms mt">forest_clf = RandomForestClassifier(random_state=42)<br/>forest_clf.fit(XTrain,yTrain)</span></pre><p id="fd62" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="fbc1" class="mp kr hi ml b fi mq mr l ms mt">array([0.9646 , 0.96255, 0.9666 ])</span></pre><p id="3fae" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">太好了，我们所有折叠的准确率都超过了96%。</p><p id="77e3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们也来看看<em class="mf"> xgboost </em>:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="8725" class="mp kr hi ml b fi mq mr l ms mt">xgboost = XGBClassifier(random_state=42)<br/>cross_val_score(xgboost, XTrain, yTrain, cv=3, scoring="accuracy")</span></pre><p id="8b86" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="0d3d" class="mp kr hi ml b fi mq mr l ms mt">array([0.9315, 0.9301, 0.9335])</span></pre><p id="b4a2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们得到了93%以上的准确率，这比随机森林分类器要低，但仍然是不错的。</p><p id="1e27" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以通过<em class="mf">标准化</em>我们的数据并使用<em class="mf"> GridSearchCV </em>方法调整超参数来提高我们模型的性能。</p><p id="b9ec" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="mf">标准化:</em> </strong></p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="7ff5" class="mp kr hi ml b fi mq mr l ms mt">scaler = StandardScaler()<br/>XTrainScaled = scaler.fit_transform(XTrain.astype(np.float64))<br/>cross_val_score(ovr_clf, XTrainScaled, yTrain, cv=3, scoring="accuracy")</span></pre><p id="f684" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="a98d" class="mp kr hi ml b fi mq mr l ms mt">array([0.90795, 0.9101 , 0.9126 ])</span></pre><p id="2bb2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您可以注意到<em class="mf"> ovr_clf </em>在标准化数据方面表现稍好。这可能不是一个非常大的改进，但仍然值得一试。</p><p id="22af" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="mf"> GridSearchCV : </em> </strong></p><p id="fc3f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们在<em class="mf"> KNN分类器</em>上尝试<em class="mf"> GridSearchCV </em>方法。</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="fdc2" class="mp kr hi ml b fi mq mr l ms mt">kRange = list(range(3,8))<br/>weights = ["uniform", "distance"]<br/>param_grid = dict(n_neighbors = kRange, weights = weights)<br/>grid = GridSearchCV(knn, param_grid, cv = 5, verbose=3,scoring = 'accuracy')<br/>grid.fit(XTrainScaled,yTrain)<br/>print(grid.best_params_)</span></pre><p id="82fd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="91a7" class="mp kr hi ml b fi mq mr l ms mt">{'n_neighbors': 4, 'weights': 'distance'}</span></pre><p id="4ef6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看best_score_</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="1485" class="mp kr hi ml b fi mq mr l ms mt">grid.best_score_</span></pre><p id="3979" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="844e" class="mp kr hi ml b fi mq mr l ms mt">0.9782833526566011</span></pre><p id="f7db" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">几乎98%都很好。</p><p id="1ba7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您还应该为其他算法尝试GridSearchCV方法。</p></div><div class="ab cl lt lu gp lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="hb hc hd he hf"><h1 id="f3fa" class="kq kr hi bd ks kt ma kv kw kx mb kz la lb mc ld le lf md lh li lj me ll lm ln bi translated">8.分析</h1><p id="55c5" class="pw-post-body-paragraph iv iw hi ix b iy lo ja jb jc lp je jf jg lq ji jj jk lr jm jn jo ls jq jr js hb bi translated">我们主要使用了四种分类算法，其中KNN表现最好。让我们来分析我们最终的KNN模型。</p><p id="83ab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们来看看<em class="mf">混淆矩阵:</em></p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="9899" class="mp kr hi ml b fi mq mr l ms mt">knn_final = KNeighborsClassifier(weights=’distance’, n_neighbors=4)<br/>knn_final.fit(XTrain,yTrain)<br/>yPred = knn_final.predict(XTest)<br/>conf_mx = confusion_matrix(yTest, yPred)<br/>conf_mx</span></pre><p id="5926" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="183e" class="mp kr hi ml b fi mq mr l ms mt">array([[ 973,    1,    1,    0,    0,    1,    3,    1,    0,    0],        [   0, 1132,    2,    0,    0,    0,    1,    0,    0,    0],        [  10,    5,  995,    2,    1,    0,    0,   16,    3,    0],        [   0,    1,    3,  974,    1,   14,    1,    7,    4,    5],        [   1,    5,    0,    0,  950,    0,    4,    3,    0,   19],        [   4,    0,    0,    9,    2,  862,    7,    1,    3,    4],        [   4,    2,    0,    0,    3,    3,  946,    0,    0,    0],        [   0,   17,    4,    0,    3,    0,    0,  994,    0,   10],        [   5,    2,    4,   14,    5,   11,    4,    4,  920,    5],        [   3,    4,    2,    7,    9,    4,    1,   10,    1,  968]])</span></pre><p id="7e8b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这看起来不错，因为大多数非对角线元素的值为零或很小。此外，这里需要注意的一点是，大多数错误都与以下一对数字有关:</p><blockquote class="mx my mz"><p id="306d" class="iv iw mf ix b iy iz ja jb jc jd je jf na jh ji jj nb jl jm jn nc jp jq jr js hb bi translated">(1，7)、(2，7)和(4，9)</p></blockquote><p id="d7f5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看<em class="mf">精度</em>值</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="ab10" class="mp kr hi ml b fi mq mr l ms mt">precision_score(yTest, yPred, average=None)</span></pre><p id="529a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="cd1a" class="mp kr hi ml b fi mq mr l ms mt">array([0.973, 0.96834902, 0.98417409, 0.96819085, 0.97535934,        0.96312849, 0.97828335, 0.95945946, 0.98818475, 0.95746785])</span></pre><p id="1fd3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以看到，我们的模型在预测时大约有97%是正确的。</p><p id="3b99" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">和<em class="mf">召回值</em></p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="846b" class="mp kr hi ml b fi mq mr l ms mt">recall_score(yTest, yPred, average=None)</span></pre><p id="7c0c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Out[ ]:</p><pre class="mg mh mi mj fd mk ml mm mn aw mo bi"><span id="41db" class="mp kr hi ml b fi mq mr l ms mt">array([0.99285714, 0.99735683, 0.96414729, 0.96435644, 0.96741344,        0.96636771, 0.9874739 , 0.96692607, 0.94455852, 0.95936571])</span></pre><p id="4ba5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，我们的模型能够在大约97%的时间内检测到这些数字。</p><p id="2fa5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">那都是我这边的。玩数据永远是一个好习惯。我建议你尝试不同的算法，调整我们在这里使用的模型的参数。这里有很大的改进空间，比如管道的使用、特性选择等等。此外，2017年发布了一个类似于MNIST的扩展数据集，名为EMNIST。所以你自己试试这个吧。</p><p id="7caa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另外，给你一个专业建议</p><blockquote class="nd"><p id="2d39" class="ne nf hi bd ng nh ni nj nk nl nm js dx translated">在分类器之前应用<em class="nn"> PCA </em>将减少内存消耗和执行时间。</p></blockquote><p id="6607" class="pw-post-body-paragraph iv iw hi ix b iy no ja jb jc np je jf jg nq ji jj jk nr jm jn jo ns jq jr js hb bi translated">此外，我会喜欢任何建议！这个故事到此为止。</p><figure class="mg mh mi mj fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nt"><img src="../Images/e91468e7c55de5ab7b32a57586bb7af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rfJ7INQ39Yd-BGISvSg-jw.png"/></div></div></figure></div></div>    
</body>
</html>