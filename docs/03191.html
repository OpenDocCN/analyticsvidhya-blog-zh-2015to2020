<html>
<head>
<title>Complete Guide about CNNs(Part-2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">CNN全指南(下)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/complete-guide-about-cnns-part-2-12c71b5d06bd?source=collection_archive---------26-----------------------#2020-01-19">https://medium.com/analytics-vidhya/complete-guide-about-cnns-part-2-12c71b5d06bd?source=collection_archive---------26-----------------------#2020-01-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="d49e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">各位学习者好！</em> <br/>在通过各种渠道对一项非常有用的计算机视觉技术CNN进行深入研究后，发表了一篇详细的博客。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/6eebf82832fcc158ddc4adc795940333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rMEBdOpQaq9N63si_i8Odw.jpeg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated"><a class="ae ju" href="https://www.thewowstyle.com/35-cool-collection-of-desktop-wallpaper/" rel="noopener ugc nofollow" target="_blank">https://www . the wow style . com/35-cool-collection-of-desktop-wallpaper/</a></figcaption></figure><h1 id="7f50" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">路标</h1><p id="892b" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">1.简介<br/> 2。CNN怎么看一个图像<br/> 3。通用CNN架构<br/> 4。CNN层概念详解<br/> 5。CNN <br/> 6摘要。训练CNN型号<br/> 7。CNN <br/> 8中的超参数。推荐CNN架构规则<br/> 9。容易引起好奇心的“为什么”问题</p><p id="e016" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第一部分:<a class="ae ju" rel="noopener" href="/@shachikaul35/complete-guide-about-cnns-part-1-65bdfee3cae3">介绍CNN摘要</a> <br/>第二部分:从训练CNN模型到最后的为什么问题</p></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><h1 id="b799" class="jv jw hi bd jx jy lf ka kb kc lg ke kf kg lh ki kj kk li km kn ko lj kq kr ks bi translated">介绍</h1><p id="8824" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">本博客纯属CNN执行相关。因此，如果你对这些概念的理解不是很好，请随意阅读这篇名为<a class="ae ju" rel="noopener" href="/@shachikaul35/complete-guide-about-cnns-part-1-65bdfee3cae3">CNN完全指南(第一部分)</a>的博客。</p><h1 id="67cd" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">训练CNN模型</h1><p id="711c" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">总的来说，NN训练就像向前传递、计算损失、向后传递、权重更新和在每个时期重复一样。在CNN中，网络开始于从图像中的低级特征到高级对象的学习。但是它是如何识别这些特征的呢？<br/>内核如何知道要寻找哪种低级特征，如边缘或曲线？后面层中的内核如何知道要查看哪些高级特性？内核如何知道哪些值有利于声音分类？</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lk"><img src="../Images/a7701c7f9f16d42cf2028c98804b1aa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*nFFMQvnmdEwjBRF0dflnoA.png"/></div></figure><ol class=""><li id="60d5" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated"><strong class="ih hj">设置一些参数</strong>，如输入图像大小、内核大小、时期、批处理大小、优化器、步幅、填充、学习率等。</li><li id="6f41" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">在<strong class="ih hj">中，一个训练迭代</strong>为一批，<br/>在第一层，做<strong class="ih hj">正向传递</strong>，其中传递一个形状为(32*32*3)带标签的图像数组到网络中。最初，权重(内核)是随机初始化的，这意味着过滤器不知道该看哪个特征。输出可以像[.1 .1 .1 .1]一样，其中任何给定的特性都没有权重。因此，<strong class="ih hj">没有检测到低级特征</strong>。<br/> 2.1 <strong class="ih hj">内核用步长在填充图像周围卷积</strong>产生激活图。传递到<strong class="ih hj"> ReLU层</strong>上，然后传递到<strong class="ih hj">池层</strong>上，在那里进行缩减采样。<br/> 2.2前一个conv层(激活图)的输出传递到下一个conv层，在该层中，卷积运算增加了内核大小，随后是ReLU和池化。这是为了<strong class="ih hj">检测中级或高级特征</strong>。</li><li id="fb4a" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">最后，到达<strong class="ih hj">损失函数</strong>(可能是MSE)。</li><li id="2ab0" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><strong class="ih hj">反向传递</strong>损失函数和具有学习率的优化器将<strong class="ih hj">根据哪个权重对损失贡献最大来更新权重</strong>。</li></ol><p id="45a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于特定批次的固定训练迭代，重复上述步骤。在最后一个训练批次的权重更新时，网络学习良好并最小化损失以进行精确预测，并移动到下一个时期。</p></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><p id="ca56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">你还在混淆epoch、iteration或step、batch或batch_size吗？</strong></p><p id="cc25" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好吧，至少我花时间消化了。所以让我再经历一次…</p><p id="9912" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设<strong class="ih hj"> 1000个图像</strong>，给定<strong class="ih hj"> 500个批量</strong>和<strong class="ih hj"> 3个时期</strong>。<br/> 1历元是<strong class="ih hj">整个数据集的训练</strong>包括一个向前和向后的行程。因此，为了训练1000个图像，其中批次大小为500，因此，2个<br/>批次。完成一个历元需要2次迭代(批)。总迭代次数将是6(3*2个时期)。在整个CNN培训期间，模型将经历3次，即6个批次。</p></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><h1 id="0a79" class="jv jw hi bd jx jy lf ka kb kc lg ke kf kg lh ki kj kk li km kn ko lj kq kr ks bi translated">CNN中的超参数</h1><p id="2c7c" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">参数根据数据决定，但一般要求如下:</p><ol class=""><li id="df55" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated"><strong class="ih hj">通道</strong>:颜色通道越多，内核越多，学到的特性越多，但可能会导致过度拟合。因此，在CNN中，辍学被用来避免过度适应。</li><li id="114d" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><strong class="ih hj">仁大小:</strong>应为<strong class="ih hj">奇数</strong>。核越大，感受野越大，一个图像块中捕获的信息越多。<br/>如果图像具有一些不同的局部特征，建议使用较小的内核(3*3，5*5)。如果要识别更大的物体，使用大的内核(9*9)。</li><li id="65be" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><strong class="ih hj">跨步</strong>:垂直/水平跨步时跳过像素。<br/>步幅值越大，输入图像下降越快。</li><li id="e196" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><strong class="ih hj">填充</strong>:指沿边框添加0的列和行，以节省空间维度。更多关于原因的信息，请访问这个博客。</li><li id="c0bf" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><strong class="ih hj">池化</strong>:根据最大或平均池化缩小图像尺寸。此外，使模型在任何类型的图像变换中都能稳健地进行分类。</li></ol><h1 id="92a6" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">推荐的CNN架构规则</h1><p id="8c2d" class="pw-post-body-paragraph if ig hi ih b ii kt ik il im ku io ip iq kv is it iu kw iw ix iy kx ja jb jc hb bi translated">有一些经验法则，如果遵循的话，将会建立一个好的健壮的网络。<br/> 1。<strong class="ih hj">从最小尺寸的</strong>开始内核，以便最初最好地识别局部(粒度)细节，并且<strong class="ih hj">逐渐增加</strong>的尺寸，以便完美地识别高级别。<br/> 2。为了更深入地了解图像尺寸通过卷积减少的地方，最好通过使用<strong class="ih hj">填充</strong>来保持图像尺寸不变。<br/> 3。<strong class="ih hj">更多的层，更多的学习</strong>因此继续添加更多的层，直到模型完全学习，记住不要过度拟合。<br/> 4。如果观察到某种过度拟合，可以使用<strong class="ih hj">脱落层</strong>或l1/l2正则化。<br/> 5。一般<strong class="ih hj">内核大小</strong> in: <br/> 5.1 <strong class="ih hj">卷积层:</strong> (3*3，5*5)用于较小的图像，而(7*7，9*9)用于较大的图像。<br/> 5.2 <strong class="ih hj">合并:</strong> (2*2，3*3)，步幅为2</p><h1 id="6bee" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">容易引起好奇心的“为什么”问题</h1><blockquote class="lz ma mb"><p id="365e" class="if ig jd ih b ii ij ik il im in io ip mc ir is it md iv iw ix me iz ja jb jc hb bi translated">为什么是卷积网络而不是神经网络？</p></blockquote><ol class=""><li id="8a9d" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated"><strong class="ih hj">计算时间和成本更少:</strong>在NN中，形状为(1000，1000，3)的彩色百万像素图像是3层的堆叠，其中每层具有范围为(0–255)值的值数组。具有(1000*1000*3)个特征的图像将难以传递到神经元并更新这三百万个特征的权重。因此，卷积出现在图像中。<strong class="ih hj">多对一映射</strong>如在池<strong class="ih hj">中。</strong></li><li id="5037" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated"><strong class="ih hj">仅共享权重:</strong>根据CNN架构，仅某些权重跨层前馈，因此网络必须仅关注内核权重以捕捉重要特征。在NN中，每个输入有更多的权重(/参数)。因此，CNN需要学习的参数<strong class="ih hj">比输入图像参数<strong class="ih hj">少得多</strong>。</strong></li></ol><blockquote class="lz ma mb"><p id="d9a8" class="if ig jd ih b ii ij ik il im in io ip mc ir is it md iv iw ix me iz ja jb jc hb bi translated">为什么我们需要stride？为什么我们不能不受跳过像素的限制直接进行卷积呢？</p></blockquote><p id="f263" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在图像具有相似的相近像素的情况下，较小的图像足以给出信息。因此，步距为2减小了图像尺寸，这对计算时间和空间有最好的贡献。</p><blockquote class="lz ma mb"><p id="dfd1" class="if ig jd ih b ii ij ik il im in io ip mc ir is it md iv iw ix me iz ja jb jc hb bi translated">为什么ReLU在CNN被证明更好？</p></blockquote><ol class=""><li id="a896" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">减少<strong class="ih hj">消失梯度问题</strong>在较低层，梯度呈指数下降，使得斜率值变小，几乎消失梯度(可能不训练)，NN根本不收敛。ReLU使之成为可能，因为它<strong class="ih hj">在一个方向</strong>饱和。</li><li id="06ce" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">增加模型的非线性而不影响感受野，其中早期的conv层执行conv运算和矩阵计算的线性计算。</li></ol><blockquote class="lz ma mb"><p id="d25b" class="if ig jd ih b ii ij ik il im in io ip mc ir is it md iv iw ix me iz ja jb jc hb bi translated">为什么会出现消失梯度问题？</p></blockquote><p id="4c1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当您的超参数(批次、learn_rate、时期等)和参数(权重、偏差)设置不正确时，通常会在基于神经网络梯度的方法(如反向传播等)中发生。</p><p id="2d76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">模型要么花很长时间训练，要么根本不训练。这取决于坡度或坡度。因此，你的神经网络不会收敛，导致性能不佳。</p><blockquote class="lz ma mb"><p id="daee" class="if ig jd ih b ii ij ik il im in io ip mc ir is it md iv iw ix me iz ja jb jc hb bi translated">为什么在网络的末端使用全连接层，而不是在网络的起点？</p></blockquote><p id="be45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简而言之，参数少，时间少，避免过拟合。全连接的层将有更多的权重来训练，因此卷积是参数较少的救星。</p><blockquote class="lz ma mb"><p id="4d61" class="if ig jd ih b ii ij ik il im in io ip mc ir is it md iv iw ix me iz ja jb jc hb bi translated">CNN为什么强制实行联营？</p></blockquote><p id="4988" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">池层的目标是从根本上减少图像的空间维度。我们不要混淆深度。</p><blockquote class="lz ma mb"><p id="e392" class="if ig jd ih b ii ij ik il im in io ip mc ir is it md iv iw ix me iz ja jb jc hb bi translated">为什么建议通过汇集来减少空间维度？</p></blockquote><ol class=""><li id="b850" class="ll lm hi ih b ii ij im in iq ln iu lo iy lp jc lq lr ls lt bi translated">更少的参数导致更快的<strong class="ih hj">计算</strong></li><li id="8a18" class="ll lm hi ih b ii lu im lv iq lw iu lx iy ly jc lq lr ls lt bi translated">信息越少越好<strong class="ih hj">过度拟合</strong></li></ol></div><div class="ab cl ky kz gp la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="hb hc hd he hf"><p id="3cfb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">点击此链接了解更多信息:</p><div class="mf mg ez fb mh mi"><a href="https://www.deeplearningwizard.com/deep_learning/practical_pytorch/pytorch_convolutional_neuralnetwork/" rel="noopener  ugc nofollow" target="_blank"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hj fi z dy mn ea eb mo ed ef hh bi translated">卷积神经网络(CNN) -深度学习向导</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">学习深度学习和深度强化轻松快速地学习数学和代码。被成千上万的学生和…</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">www.deeplearningwizard.com</p></div></div></div></a></div><div class="mf mg ez fb mh mi"><a href="https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1" rel="noopener follow" target="_blank"><div class="mj ab dw"><div class="mk ab ml cl cj mm"><h2 class="bd hj fi z dy mn ea eb mo ed ef hh bi translated">直观理解用于深度学习的卷积</h2><div class="mp l"><h3 class="bd b fi z dy mn ea eb mo ed ef dx translated">探索让它们工作的强大的视觉层次</h3></div><div class="mq l"><p class="bd b fp z dy mn ea eb mo ed ef dx translated">towardsdatascience.com</p></div></div><div class="mr l"><div class="ms l mt mu mv mr mw jo mi"/></div></div></a></div><p id="68df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">快乐阅读！</p><p id="8b3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">可以通过</em></strong><a class="ae ju" href="https://www.linkedin.com/in/kaul-shachi" rel="noopener ugc nofollow" target="_blank"><strong class="ih hj"><em class="jd">LinkedIn</em></strong></a><strong class="ih hj"><em class="jd">与我联系。</em> </strong></p><p id="3a17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">欢迎在评论区分享你的观点或任何误导性的信息。:)</p></div></div>    
</body>
</html>