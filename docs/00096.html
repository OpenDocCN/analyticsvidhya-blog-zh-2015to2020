<html>
<head>
<title>Deep Learning Tutorial to Calculate the Screen Time of Actors in any Video (with Python codes)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习教程计算任意视频中演员的屏幕时间(附Python代码)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-learning-tutorial-to-calculate-the-screen-time-of-actors-in-any-video-with-python-codes-a26e12daba0c?source=collection_archive---------2-----------------------#2018-09-11">https://medium.com/analytics-vidhya/deep-learning-tutorial-to-calculate-the-screen-time-of-actors-in-any-video-with-python-codes-a26e12daba0c?source=collection_archive---------2-----------------------#2018-09-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/0ad8bd7731d8b05a0b4caa829dea5cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MO7Y4zgqvoD05gju.jpg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">资料来源:sporcle.com</figcaption></figure><div class=""/><p id="cd1d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">当我开始我的深度学习之旅时，我学到的第一件事就是图像分类。这是计算机视觉界如此迷人的一部分，我完全沉浸其中！但我有一个好奇的想法，一旦我掌握了图像分类，我想知道我是否可以将这种学习转移到视频中。</p><p id="a043" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">有没有办法建立一个模型，在特定的时间间隔自动识别特定视频中的特定人物？事实证明，是有的，我很高兴能与你分享我的方法！</p><figure class="jt ju jv jw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es js"><img src="../Images/9b2a794ef5aa551a912121c05c11b572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7R3FNWcmeI4I0dCH.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">来源:海岸线自动化</figcaption></figure><p id="8e6a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在给你一些关于我们将要解决的问题的背景，记住屏幕时间对一个演员来说是非常重要的。和他/她拿到的钱直接相关。为了让你对这个佣金有个概念，你知道吗，小罗伯特·唐尼·唐尼在《蜘蛛侠归来》中仅仅15分钟的出镜时间就赚了1000万美元？难以置信。</p><p id="fcca" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果我们可以拍摄任何视频，并计算其中任何演员的屏幕时间，这将有多酷？</p><p id="3366" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在本文中，我将帮助您了解如何在视频数据上使用深度学习。为了做到这一点，我们将使用广受欢迎的汤姆和杰瑞卡通系列的视频。目的是计算任何给定视频中汤姆和杰里的屏幕时间。</p><p id="9d6e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">听起来很有趣？接着读下去！</p><p id="f9c6" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="jx">注意:本文假设你对使用深度学习的图像分类有先验知识。如果没有，我推荐你浏览一下</em> <a class="ae jy" href="https://www.analyticsvidhya.com/blog/2016/10/tutorial-optimizing-neural-networks-using-keras-with-image-recognition-case-study/" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hy"> <em class="jx">这篇文章</em> </strong> </a> <em class="jx">，它将帮助你掌握深度学习和图像分类的基础知识。</em></p><h1 id="07d1" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">目录</h1><ol class=""><li id="e8da" class="kx ky hx iw b ix kz jb la jf lb jj lc jn ld jr le lf lg lh bi translated">读取视频并提取帧</li><li id="0aae" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr le lf lg lh bi translated">如何在Python中处理视频文件</li><li id="c487" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr le lf lg lh bi translated">计算屏幕时间—一个简单的解决方案</li><li id="0119" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr le lf lg lh bi translated">我的经验——哪些有效，哪些无效</li></ol><h1 id="c0fb" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">读取视频并提取帧</h1><p id="9242" class="pw-post-body-paragraph iu iv hx iw b ix kz iz ja jb la jd je jf ln jh ji jj lo jl jm jn lp jp jq jr hb bi translated">听说过翻页书吗？如果你没有，你就错过了！看看下面这个:</p><figure class="jt ju jv jw fd hk er es paragraph-image"><div class="er es lq"><img src="../Images/30c517cfbed324d01b2a52105751b233.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*J_cc5ZcRGMq3M_D1.gif"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><em class="lr">资料来源:giphy.com</em></figcaption></figure><p id="146d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们在书的每一页上都有不同的图像，当我们翻动这些页面时，我们得到一个鲨鱼跳舞的动画。你甚至可以称之为一种视频。我们翻页越快，视觉效果就越好。换句话说，这种视觉是以特定顺序排列的不同图像的集合。</p><p id="3120" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">同样，视频也不过是一组图像的集合。这些图像被称为帧，可以组合起来获得原始视频。因此，与视频数据相关的问题与图像分类或对象检测问题没有太大不同。从视频中提取帧只需要一个额外的步骤。</p><p id="5eb9" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">记住，我们这里的挑战是从给定的视频中计算汤姆和杰瑞的屏幕时间。让我首先总结一下我们在本文中将遵循的解决这个问题的步骤:</p><ol class=""><li id="92f1" class="kx ky hx iw b ix iy jb jc jf ls jj lt jn lu jr le lf lg lh bi translated">导入和阅读视频，从中提取帧，并将其存储为图像</li><li id="0b35" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr le lf lg lh bi translated">标注几张图片用于训练模型(别担心，我已经帮你做好了)</li><li id="dfc3" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr le lf lg lh bi translated">根据训练数据建立我们的模型</li><li id="e823" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr le lf lg lh bi translated">对剩余的图像进行预测</li><li id="d599" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr le lf lg lh bi translated">计算汤姆和杰瑞的屏幕时间</li></ol><p id="6057" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">相信我，只要遵循这些步骤，就能帮助你解决深度学习中的许多此类视频相关问题。现在是时候戴上我们的Python帽子，开始这项挑战了。</p><h1 id="f3a2" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">如何在Python中处理视频文件</h1><p id="939c" class="pw-post-body-paragraph iu iv hx iw b ix kz iz ja jb la jd je jf ln jh ji jj lo jl jm jn lp jp jq jr hb bi translated">让我们从导入所有必需的库开始。如果您还没有安装下面的库，请继续安装:</p><ul class=""><li id="f414" class="kx ky hx iw b ix iy jb jc jf ls jj lt jn lu jr lv lf lg lh bi translated"><a class="ae jy" href="https://scipy.org/install.html" rel="noopener ugc nofollow" target="_blank"> Numpy </a></li><li id="b99e" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr lv lf lg lh bi translated"><a class="ae jy" href="https://scipy.org/install.html" rel="noopener ugc nofollow" target="_blank">熊猫</a></li><li id="4eb7" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr lv lf lg lh bi translated"><a class="ae jy" href="https://scipy.org/install.html" rel="noopener ugc nofollow" target="_blank"> Matplotlib </a></li><li id="c509" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr lv lf lg lh bi translated"><a class="ae jy" href="https://keras.io/#installation" rel="noopener ugc nofollow" target="_blank"> Keras </a></li><li id="815a" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr lv lf lg lh bi translated"><a class="ae jy" href="http://scikit-image.org/docs/dev/install.html" rel="noopener ugc nofollow" target="_blank">克扣</a></li><li id="41e2" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr lv lf lg lh bi translated"><a class="ae jy" href="https://pypi.org/project/opencv-python/" rel="noopener ugc nofollow" target="_blank"> OpenCV </a></li></ul><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="3b6a" class="mb ka hx lx b fi mc md l me mf">import cv2     # for capturing videos<br/>import math   # for mathematical operations<br/>import matplotlib.pyplot as plt    # for plotting the images<br/>%matplotlib inline<br/>import pandas as pd<br/>from keras.preprocessing import image   # for preprocessing the images<br/>import numpy as np    # for mathematical operations<br/>from keras.utils import np_utils<br/>from skimage.transform import resize   # for resizing images</span></pre><h1 id="63b3" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">第一步:阅读视频，从中提取帧并保存为图像</h1><p id="a272" class="pw-post-body-paragraph iu iv hx iw b ix kz iz ja jb la jd je jf ln jh ji jj lo jl jm jn lp jp jq jr hb bi translated">现在我们将加载视频并将其转换为帧。<strong class="iw hy">你可以从</strong> <a class="ae jy" href="https://drive.google.com/file/d/1_DcwBhYo15j7AU-v2gN61qGGd1ZablGK/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hy">这个链接</strong> </a>下载用于这个例子的视频。我们将首先使用<em class="jx"> VideoCapture() </em>函数从给定的目录中捕获视频，然后我们将使用<em class="jx"> imwrite() </em>函数从视频中提取帧并将它们保存为图像。我们来编码一下:</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="f5b1" class="mb ka hx lx b fi mc md l me mf">count = 0<br/>videoFile = "Tom and jerry.mp4"<br/>cap = cv2.VideoCapture(videoFile)   # capturing the video from the given path<br/>frameRate = cap.get(5) #frame rate<br/>x=1<br/>while(cap.isOpened()):<br/>    frameId = cap.get(1) #current frame number<br/>    ret, frame = cap.read()<br/>    if (ret != True):<br/>        break<br/>    if (frameId % math.floor(frameRate) == 0):<br/>        filename ="frame%d.jpg" % count;count+=1<br/>        cv2.imwrite(filename, frame)<br/>cap.release()<br/>print ("Done!")</span></pre><p id="6c4e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">搞定了。</p><p id="c8c7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">一旦这个过程完成，“完成！”将打印在屏幕上，作为已创建框架的确认。</p><p id="2694" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们试着想象一幅图像(画面)。我们将首先使用<em class="jx"> matplotlib，</em>的<em class="jx"> imread() </em>函数读取图像，然后使用<em class="jx"> imshow() </em>函数绘制图像。</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="c883" class="mb ka hx lx b fi mc md l me mf">img = plt.imread('frame0.jpg')   # reading image using its name<br/>plt.imshow(img)</span></pre><figure class="jt ju jv jw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es js"><img src="../Images/b03f2e8a1aaaee5c17e147388d402c9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6W9Ne4VnFCR8Z2pH.jpg"/></div></div></figure><p id="b406" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">开始兴奋了吗？</p><p id="b29f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这是视频的第一帧。我们已经从视频的整个持续时间中提取了每秒一帧。由于视频的持续时间是4分58秒(298秒)，我们现在总共有298张图像。</p><p id="92a7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们的任务是识别哪个图像有汤姆，哪个图像有杰里。如果我们提取的图像与流行的Imagenet数据集中存在的图像相似，这个挑战可能是轻而易举的。怎么会？我们可以简单地使用根据Imagenet数据预先训练的模型，并获得高准确度分数！但是那有什么意思呢？</p><p id="b4f2" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们有卡通图像，所以任何预先训练的模型都很难(如果不是不可能的话)在给定的视频中识别出汤姆和杰里。</p><h1 id="3c37" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">步骤2:标记一些图像来训练模型</h1><p id="d3ca" class="pw-post-body-paragraph iu iv hx iw b ix kz iz ja jb la jd je jf ln jh ji jj lo jl jm jn lp jp jq jr hb bi translated">那么，我们如何着手处理这件事呢？一种可能的解决方案是手动给一些图像添加标签，并在这些图像上训练模型。一旦模型学习了这些模式，我们就可以用它来对一组以前看不到的图像进行预测。</p><p id="89fe" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">请记住，可能会出现汤姆和杰瑞都不在场的情况。所以，我们将把它作为一个多类分类问题来处理。我定义的类有:</p><ul class=""><li id="e82b" class="kx ky hx iw b ix iy jb jc jf ls jj lt jn lu jr lv lf lg lh bi translated">杰瑞和汤姆都不是</li><li id="da23" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr lv lf lg lh bi translated">1:给杰瑞</li><li id="e228" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr lv lf lg lh bi translated">2:给汤姆的</li></ul><p id="ddce" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">不要担心，我已经标记了所有的图片，所以你不必！继续下载<a class="ae jy" href="https://drive.google.com/file/d/1NbU8Sdj_YNF5Dl_zbdeBcnqEyU3Xw9TU/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hy"> mapping.csv </strong> </a>文件，其中包含每个图像名称及其对应的类(0或1或2)。</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="b51d" class="mb ka hx lx b fi mc md l me mf">data = pd.read_csv('mapping.csv')     # reading the csv file<br/>data.head()      # printing first five rows of the file</span></pre><figure class="jt ju jv jw fd hk er es paragraph-image"><div class="er es mg"><img src="../Images/8c088d0d8efdd70d972995ae192ec946.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/format:webp/0*ywm-r_hnOUSbAri-.png"/></div></figure><p id="4c5d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">映射文件包含两列:</p><ul class=""><li id="5ab2" class="kx ky hx iw b ix iy jb jc jf ls jj lt jn lu jr lv lf lg lh bi translated"><strong class="iw hy"><em class="jx">Image _ ID:</em></strong><em class="jx">C</em>包含每个图像的名称</li><li id="4aa7" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr lv lf lg lh bi translated"><strong class="iw hy">类<em class="jx">类</em>。<em class="jx">Image _ ID:</em></strong><em class="jx">C</em>包含每个图像对应的类</li></ul><p id="183c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们的下一步是读取图像，我们将基于它们的名称，也就是<em class="jx"> Image_ID </em>列。</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="2352" class="mb ka hx lx b fi mc md l me mf">X = [ ]     # creating an empty array<br/>for img_name in data.Image_ID:<br/>    img = plt.imread('' + img_name)<br/>    X.append(img)  # storing each image in array X<br/>X = np.array(X)    # converting list to array</span></pre><p id="e607" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Tada！我们现在有图像了。记住，我们需要两样东西来训练我们的模型:</p><ul class=""><li id="c4c8" class="kx ky hx iw b ix iy jb jc jf ls jj lt jn lu jr lv lf lg lh bi translated">训练图像，以及</li><li id="ebad" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr lv lf lg lh bi translated">他们对应的类</li></ul><p id="56fe" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因为有三个类，我们将使用<em class="jx"> keras.utils </em>的<em class="jx">to _ categorial()</em>函数对它们进行编码。</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="644a" class="mb ka hx lx b fi mc md l me mf">y = data.Class<br/>dummy_y = np_utils.to_categorical(y)    # one hot encoding Classes</span></pre><p id="c78f" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将使用VGG16预训练模型，该模型采用形状(224 X 224 X 3)的输入图像。由于我们的图像大小不同，我们需要对它们进行整形。我们将使用<em class="jx"> skimage.transform </em>的<em class="jx"> resize() </em>函数来实现这一点。</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="60ac" class="mb ka hx lx b fi mc md l me mf">image = []<br/>for i in range(0,X.shape[0]):<br/>    a = resize(X[i], preserve_range=True, output_shape=(224,224)).astype(int)      # reshaping to 224*224*3<br/>    image.append(a)<br/>X = np.array(image)</span></pre><p id="d27e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">所有的图像都被调整为224 X 224 X 3。但是在向模型传递任何输入之前，我们必须按照模型的要求对其进行预处理。否则，模型的性能将不够好。使用<em class="jx">keras . applications . vgg 16</em>的<em class="jx"> preprocess_input() </em>函数来执行此步骤。</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="aca3" class="mb ka hx lx b fi mc md l me mf">from keras.applications.vgg16 import preprocess_input<br/>X = preprocess_input(X, mode='tf')      # preprocessing the input data</span></pre><p id="348d" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们还需要一个验证集来检查模型在看不见的图像上的性能。我们将利用<em class="jx">sk learn . model _ selection</em>模块的<em class="jx"> train_test_split() </em>函数将图像随机分为训练集和验证集。</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="884c" class="mb ka hx lx b fi mc md l me mf">from sklearn.model_selection import train_test_split<br/>X_train, X_valid, y_train, y_valid = train_test_split(X, dummy_y, test_size=0.3, random_state=42)    # preparing the validation set</span></pre><h1 id="32e0" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">步骤3:构建模型</h1><p id="19f3" class="pw-post-body-paragraph iu iv hx iw b ix kz iz ja jb la jd je jf ln jh ji jj lo jl jm jn lp jp jq jr hb bi translated">下一步是构建我们的模型。如上所述，我们将使用VGG16预训练模型来完成这项任务。让我们首先导入构建模型所需的库:</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="c903" class="mb ka hx lx b fi mc md l me mf">from keras.models import Sequential<br/>from keras.applications.vgg16 import VGG16<br/>from keras.layers import Dense, InputLayer, Dropout</span></pre><p id="1853" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们现在将加载VGG16预训练模型，并将其存储为<em class="jx"> base_model </em>:</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="20b2" class="mb ka hx lx b fi mc md l me mf">base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))    # include_top=False to remove the top layer</span></pre><p id="d332" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们将使用该模型对<em class="jx"> X_train </em>和<em class="jx"> X_valid </em>进行预测，获得特征，然后使用这些特征重新训练该模型。</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="3a2e" class="mb ka hx lx b fi mc md l me mf">X_train = base_model.predict(X_train)<br/>X_valid = base_model.predict(X_valid)<br/>X_train.shape, X_valid.shape</span></pre><p id="1b27" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="jx"> X_train </em>和<em class="jx"> X_valid </em>的形状分别为(208，7，7，512)，(90，7，7，512)。为了将它传递给我们的神经网络，我们必须将其重塑为一维。</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="c3ab" class="mb ka hx lx b fi mc md l me mf">X_train = X_train.reshape(208, 7*7*512)      # converting to 1-D<br/>X_valid = X_valid.reshape(90, 7*7*512)</span></pre><p id="ee0b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们现在将预处理图像，并使它们以零为中心，这有助于模型更快地收敛。</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="bf14" class="mb ka hx lx b fi mc md l me mf">train = X_train/X_train.max()      # centering the data<br/>X_valid = X_valid/X_train.max()</span></pre><p id="fd59" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">最后，我们将构建我们的模型。该步骤可分为3个子步骤:</p><ol class=""><li id="382d" class="kx ky hx iw b ix iy jb jc jf ls jj lt jn lu jr le lf lg lh bi translated">构建模型</li><li id="38f4" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr le lf lg lh bi translated">编译模型</li><li id="1d3e" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr le lf lg lh bi translated">训练模型</li></ol><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="5ffe" class="mb ka hx lx b fi mc md l me mf"># i. Building the model<br/>model = Sequential()<br/>model.add(InputLayer((7*7*512,)))    # input layer<br/>model.add(Dense(units=1024, activation='sigmoid')) # hidden layer<br/>model.add(Dense(3, activation='sigmoid'))    # output layer</span></pre><p id="4de8" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们使用<em class="jx"> summary() </em>函数来检查模型的摘要:</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="df74" class="mb ka hx lx b fi mc md l me mf">model.summary()</span></pre><figure class="jt ju jv jw fd hk er es paragraph-image"><div class="er es mh"><img src="../Images/decab55ca2ecd0a86ac49b851626fc0f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/0*bn5vnIJdH4VxRBjL.png"/></div></figure><p id="b15c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们有一个具有1，024个神经元的隐藏层和一个具有3个神经元的输出层(因为我们有3个类要预测)。现在我们将编译我们的模型:</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="796b" class="mb ka hx lx b fi mc md l me mf"># ii. Compiling the model<br/>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</span></pre><p id="8608" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在最后一步中，我们将拟合模型，同时还将检查它在看不见的图像(即验证图像)上的性能:</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="583c" class="mb ka hx lx b fi mc md l me mf"># iii. Training the model<br/>model.fit(train, y_train, epochs=100, validation_data=(X_valid, y_valid))</span></pre><figure class="jt ju jv jw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mi"><img src="../Images/797b092eacd0f9e55b3e4a4e68161712.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5I3k_ELz33AAoIyH.png"/></div></div></figure><p id="7f6b" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们可以看到它在训练和验证图像上表现得非常好。<strong class="iw hy">我们对看不见的图像有大约85%的准确率</strong>。这就是我们如何在视频数据上训练一个模型来得到每一帧的预测。</p><p id="82fb" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在下一节中，我们将尝试计算汤姆和杰瑞在新视频中的屏幕时间。</p><h1 id="15e8" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">计算屏幕时间—一个简单的解决方案</h1><p id="52d8" class="pw-post-body-paragraph iu iv hx iw b ix kz iz ja jb la jd je jf ln jh ji jj lo jl jm jn lp jp jq jr hb bi translated">首先，从这里 下载我们将在本节<a class="ae jy" href="https://drive.google.com/file/d/1MQHRosZmeYpK2onCWr_A9p5SI93pEDw0/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="iw hy">中使用的视频。一旦完成，继续加载视频并从中提取帧。我们将遵循与上面相同的步骤:</strong></a></p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="fc5b" class="mb ka hx lx b fi mc md l me mf">count = 0<br/>videoFile = "Tom and Jerry 3.mp4"<br/>cap = cv2.VideoCapture(videoFile)<br/>frameRate = cap.get(5) #frame rate<br/>x=1<br/>while(cap.isOpened()):<br/>    frameId = cap.get(1) #current frame number<br/>    ret, frame = cap.read()<br/>    if (ret != True):<br/>        break<br/>    if (frameId % math.floor(frameRate) == 0):<br/>        filename ="test%d.jpg" % count;count+=1<br/>        cv2.imwrite(filename, frame)<br/>cap.release()<br/>print ("Done!")</span></pre><p id="1586" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">搞定了。</p><p id="ab51" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从新视频中提取帧后，我们现在将加载test.csv文件，该文件包含每个提取帧的名称。<strong class="iw hy">下载</strong><a class="ae jy" href="https://drive.google.com/open?id=1uIAXp_2WHwb_SLZF3fwpW9lbo9eRaTtp" rel="noopener ugc nofollow" target="_blank"><strong class="iw hy">test . CSV</strong></a><strong class="iw hy">文件</strong>并加载:</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="bc1b" class="mb ka hx lx b fi mc md l me mf">test = pd.read_csv('test.csv')</span></pre><p id="db9e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">接下来，我们将导入图像进行测试，然后根据前面提到的预训练模型的要求对它们进行整形:</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="389e" class="mb ka hx lx b fi mc md l me mf">test_image = []<br/>for img_name in test.Image_ID:<br/>    img = plt.imread('' + img_name)<br/>    test_image.append(img)<br/>test_img = np.array(test_image)</span><span id="7486" class="mb ka hx lx b fi mj md l me mf">test_image = []<br/>for i in range(0,test_img.shape[0]):<br/>    a = resize(test_img[i], preserve_range=True, output_shape=(224,224)).astype(int)<br/>    test_image.append(a)<br/>test_image = np.array(test_image)</span></pre><p id="ac09" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们需要对这些图像进行类似于我们对训练图像所做的更改。我们将对图像进行预处理，使用<em class="jx"> base_model.predict() </em>函数使用VGG16预训练模型从这些图像中提取特征，将这些图像整形为一维形式，并使它们以零为中心:</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="7ce6" class="mb ka hx lx b fi mc md l me mf"># preprocessing the images<br/>test_image = preprocess_input(test_image, mode='tf')<br/><br/># extracting features from the images using pretrained model<br/>test_image = base_model.predict(test_image)<br/><br/># converting the images to 1-D form<br/>test_image = test_image.reshape(186, 7*7*512)<br/><br/># zero centered images<br/>test_image = test_image/test_image.max()</span></pre><p id="3013" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">由于我们之前已经训练了该模型，所以我们将利用该模型来对这些图像进行预测。</p><h1 id="9823" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">步骤4:对剩余的图像进行预测</h1><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="a332" class="mb ka hx lx b fi mc md l me mf">predictions = model.predict_classes(test_image)</span></pre><h1 id="5168" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">第五步:计算汤姆和杰瑞的屏幕时间</h1><p id="7bc3" class="pw-post-body-paragraph iu iv hx iw b ix kz iz ja jb la jd je jf ln jh ji jj lo jl jm jn lp jp jq jr hb bi translated">回想一下，类“1”表示JERRY的存在，而类“2”表示TOM的存在。我们将利用上述预测来计算这两个传奇人物的出场时间:</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="bf6f" class="mb ka hx lx b fi mc md l me mf">print("The screen time of JERRY is", predictions[predictions==1].shape[0], "seconds")<br/>print("The screen time of TOM is", predictions[predictions==2].shape[0], "seconds")</span></pre><figure class="jt ju jv jw fd hk er es paragraph-image"><div class="er es mk"><img src="../Images/a2732e4ec03af3d05c91652b3a09c420.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/0*65xvsMvww1RNzr84.png"/></div></figure><p id="ffa6" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这就对了。我们有给定视频中汤姆和杰里的总屏幕时间。</p><h1 id="583a" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">我的经验——哪些有效，哪些无效</h1><p id="8f53" class="pw-post-body-paragraph iu iv hx iw b ix kz iz ja jb la jd je jf ln jh ji jj lo jl jm jn lp jp jq jr hb bi translated">为了这个挑战，我尝试和测试了许多东西——有些非常有效，而有些则以失败告终。在这一部分，我将详细阐述我所面临的一些困难，以及我是如何解决这些困难的。之后，我提供了最终模型的全部代码，这给了我最好的准确性。</p><p id="dc66" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">首先，我尝试在不移除顶层的情况下使用预训练模型。结果并不令人满意。可能的原因是这些是卡通图像，我们的预训练模型是在实际图像上训练的，因此它不能对这些卡通图像进行分类。为了解决这个问题，我使用少量标记图像重新训练预训练模型，结果比以前的结果更好。</p><p id="92c4" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">即使在对标记图像进行训练之后，准确度也不令人满意。该模型不能在训练图像本身上很好地执行。所以，我试着增加层数。增加层数被证明是提高训练精度的好方法，但是在训练和验证精度之间没有同步。该模型过拟合，其对未知数据的表现不令人满意。因此，我在每个密集层后添加了一个下降层，这样训练和验证准确性之间就有了很好的同步。</p><p id="587c" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我注意到班级不平衡。汤姆有更多的屏幕时间，所以预测受其支配，大多数帧被预测为汤姆。为了克服这个问题并使类平衡，我使用了sklearn.utils.class_weight模块的compute_class_weight()函数。与具有较高值计数的类相比，它将较高的权重分配给具有较低值计数的类。</p><p id="a423" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我还使用模型检查点来保存最佳模型，即产生最低验证损失的模型，然后使用该模型进行最终预测。我将总结上述所有步骤，并给出最终代码。测试图像的实际类可以在<a class="ae jy" href="https://drive.google.com/file/d/1blewkgF0M6SlJp4x47MVqQEbu4NZmGuF/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"> testing.csv </a>文件中找到。</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="7c94" class="mb ka hx lx b fi mc md l me mf">import cv2<br/>import math<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>%matplotlib inline<br/>from keras.preprocessing import image<br/>import numpy as np<br/>from skimage.transform import resize</span><span id="b4b0" class="mb ka hx lx b fi mj md l me mf">count = 0<br/>videoFile = "Tom and jerry.mp4"<br/>cap = cv2.VideoCapture(videoFile)<br/>frameRate = cap.get(5) #frame rate<br/>x=1<br/>while(cap.isOpened()):<br/>    frameId = cap.get(1) #current frame number<br/>    ret, frame = cap.read()<br/>    if (ret != True):<br/>        break<br/>    if (frameId % math.floor(frameRate) == 0):<br/>        filename ="frame%d.jpg" % count;count+=1<br/>        cv2.imwrite(filename, frame)<br/>cap.release()<br/>print ("Done!")</span></pre><p id="8356" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">搞定了。</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="7d1b" class="mb ka hx lx b fi mc md l me mf">count = 0<br/>videoFile = "Tom and Jerry 3.mp4"<br/>cap = cv2.VideoCapture(videoFile)<br/>frameRate = cap.get(5) #frame rate<br/>x=1<br/>while(cap.isOpened()):<br/>    frameId = cap.get(1) #current frame number<br/>    ret, frame = cap.read()<br/>    if (ret != True):<br/>        break<br/>    if (frameId % math.floor(frameRate) == 0):<br/>        filename ="test%d.jpg" % count;count+=1<br/>        cv2.imwrite(filename, frame)<br/>cap.release()<br/>print ("Done!")</span></pre><p id="d581" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">搞定了。</p><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="2670" class="mb ka hx lx b fi mc md l me mf">data = pd.read_csv('mapping.csv')<br/>test = pd.read_csv('testing.csv')</span><span id="b312" class="mb ka hx lx b fi mj md l me mf">X = []<br/>for img_name in data.Image_ID:<br/>    img = plt.imread('' + img_name)<br/>    X.append(img)<br/>X = np.array(X)</span><span id="3ff6" class="mb ka hx lx b fi mj md l me mf">test_image = []<br/>for img_name in test.Image_ID:<br/>    img = plt.imread('' + img_name)<br/>    test_image.append(img)<br/>test_img = np.array(test_image)</span><span id="72f8" class="mb ka hx lx b fi mj md l me mf">from keras.utils import np_utils<br/>train_y = np_utils.to_categorical(data.Class)<br/>test_y = np_utils.to_categorical(test.Class)</span><span id="93df" class="mb ka hx lx b fi mj md l me mf">image = []<br/>for i in range(0,X.shape[0]):<br/>    a = resize(X[i], preserve_range=True, output_shape=(224,224,3)).astype(int)<br/>    image.append(a)<br/>X = np.array(image)</span><span id="fae1" class="mb ka hx lx b fi mj md l me mf">test_image = []<br/>for i in range(0,test_img.shape[0]):<br/>    a = resize(test_img[i], preserve_range=True, output_shape=(224,224)).astype(int)<br/>    test_image.append(a)<br/>test_image = np.array(test_image)</span><span id="09b6" class="mb ka hx lx b fi mj md l me mf">from keras.applications.vgg16 import preprocess_input<br/>X = preprocess_input(X, mode='tf')<br/>test_image = preprocess_input(test_image, mode='tf')</span><span id="2d5b" class="mb ka hx lx b fi mj md l me mf">from sklearn.model_selection import train_test_split<br/>X_train, X_valid, y_train, y_valid = train_test_split(X, train_y, test_size=0.3, random_state=42)</span><span id="df38" class="mb ka hx lx b fi mj md l me mf">from keras.models import Sequential<br/>from keras.applications.vgg16 import VGG16<br/>from keras.layers import Dense, InputLayer, Dropout</span><span id="2fd4" class="mb ka hx lx b fi mj md l me mf">base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))</span><span id="1346" class="mb ka hx lx b fi mj md l me mf">X_train = base_model.predict(X_train)<br/>X_valid = base_model.predict(X_valid)<br/>test_image = base_model.predict(test_image)</span><span id="0e67" class="mb ka hx lx b fi mj md l me mf">X_train = X_train.reshape(208, 7*7*512)<br/>X_valid = X_valid.reshape(90, 7*7*512)<br/>test_image = test_image.reshape(186, 7*7*512)</span><span id="daf9" class="mb ka hx lx b fi mj md l me mf">train = X_train/X_train.max()<br/>X_valid = X_valid/X_train.max()<br/>test_image = test_image/test_image.max()</span><span id="2653" class="mb ka hx lx b fi mj md l me mf">model = Sequential()<br/>model.add(InputLayer((7*7*512,)))    # input layer<br/>model.add(Dense(units=1024, activation='sigmoid'))   # hidden layer<br/>model.add(Dropout(0.5))      # adding dropout<br/>model.add(Dense(units=512, activation='sigmoid'))    # hidden layer<br/>model.add(Dropout(0.5))      # adding dropout<br/>model.add(Dense(units=256, activation='sigmoid'))    # hidden layer<br/>model.add(Dropout(0.5))      # adding dropout<br/>model.add(Dense(3, activation='sigmoid'))            # output layer</span><span id="3828" class="mb ka hx lx b fi mj md l me mf">model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</span><span id="cea6" class="mb ka hx lx b fi mj md l me mf">from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight<br/>class_weights = compute_class_weight('balanced',np.unique(data.Class), data.Class)  # computing weights of different classes</span><span id="a84b" class="mb ka hx lx b fi mj md l me mf">from keras.callbacks import ModelCheckpoint<br/>filepath="weights.best.hdf5"<br/>checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')<br/>callbacks_list = [checkpoint]      # model check pointing based on validation loss</span><span id="3f75" class="mb ka hx lx b fi mj md l me mf">model.fit(train, y_train, epochs=100, validation_data=(X_valid, y_valid), class_weight=class_weights, callbacks=callbacks_list)</span></pre><figure class="jt ju jv jw fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mi"><img src="../Images/8578f096f31decce82e4cd834a4fef70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*RoPVQRa-3xG7CPoG.png"/></div></div></figure><pre class="jt ju jv jw fd lw lx ly lz aw ma bi"><span id="48c1" class="mb ka hx lx b fi mc md l me mf">model.load_weights("weights.best.hdf5")</span><span id="a49f" class="mb ka hx lx b fi mj md l me mf">model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</span><span id="9bbf" class="mb ka hx lx b fi mj md l me mf">scores = model.evaluate(test_image, test_y)<br/>print("%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))</span></pre><figure class="jt ju jv jw fd hk er es paragraph-image"><div class="er es ml"><img src="../Images/98798b3d84091470920ca6f75c27fa60.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/0*Zq1fifoROj4JP8Yb.png"/></div></figure><h1 id="1585" class="jz ka hx bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">结论</h1><p id="f400" class="pw-post-body-paragraph iu iv hx iw b ix kz iz ja jb la jd je jf ln jh ji jj lo jl jm jn lp jp jq jr hb bi translated">使用该模型，我们在验证数据和测试数据上分别获得了大约88%和64%的准确率。</p><p id="2552" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">测试数据准确性低的一个可能原因是缺少训练数据。由于模型没有很多像汤姆和杰里的卡通形象的知识，我们必须在训练过程中给它更多的图像。我的建议是从不同的《猫和老鼠》视频中提取更多的帧，相应地标记它们，并用它们来训练模型。一旦模型看到了这两个角色的大量图像，它很有可能会导致更好的分类结果。</p><p id="850a" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这样的模型可以在各个领域帮助我们:</p><ul class=""><li id="fb23" class="kx ky hx iw b ix iy jb jc jf ls jj lt jn lu jr lv lf lg lh bi translated">我们可以计算电影中特定演员的屏幕时间</li><li id="54f9" class="kx ky hx iw b ix li jb lj jf lk jj ll jn lm jr lv lf lg lh bi translated">计算你喜欢的超级英雄的屏幕时间等。</li></ul><p id="4104" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这些只是可以使用这种技术的几个例子。你可以自己想出更多这样的应用程序！</p></div><div class="ab cl mm mn gp mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hb hc hd he hf"><p id="7aef" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="jx">原载于2018年9月11日</em><a class="ae jy" href="https://www.analyticsvidhya.com/blog/2018/09/deep-learning-video-classification-python/" rel="noopener ugc nofollow" target="_blank"><em class="jx">【www.analyticsvidhya.com】</em></a><em class="jx">。</em></p></div></div>    
</body>
</html>