# 微调用于 tensorflow 中情感提取的 XLNet

> 原文：<https://medium.com/analytics-vidhya/finetuning-a-xlnet-for-sentiment-extraction-9e75ded91f1c?source=collection_archive---------15----------------------->

![](img/2b57b707f6113558fceb6b77e9f51333.png)

来源:KDnuggets

# 内容:

1.  XLNet 简介
2.  什么是情感提取任务？
3.  标记化
4.  建筑模型

# XLNet 简介:

XLNet 是自然语言处理领域最先进的模型之一。它通过在 20 个任务上超越 BERT 模型接管了帝国，这些任务包括问题回答、自然语言推理、情感分析和文档排序。与 BERT 不同，XLNet 利用了自回归(AR)语言建模和自动编码的预训练优势。

好吧，等等！这些是什么？别担心，我们会弄清楚上面的话在下面是什么意思。

1.  自回归语言建模；

这里，我们将要求模型逐步预测单词。让我举一个例子来说明这一点:

让我们考虑“无监督表示学习在自然语言处理领域非常成功”这句话

现在，在训练期间，模型将被给予“无监督表示学习已经”，并被要求预测下一个单词，即这里的单词“已经”。它还没有完成，现在模型将再次被给予“无监督的表示学习已经”,并被要求预测下一个单词“高度”!这个过程还在继续。

> 输入 1 →“无监督表示学习有”
> 
> 输出 1→‘去过’
> 
> 输入 2 →“无监督表示学习已经”
> 
> 输出 2 →“高度”
> 
> .
> 
> .
> 
> .

2.自动编码:

自动编码与我们之前讨论的有点不同。这里，句子中的一些单词将被屏蔽，现在模型应该预测这些被屏蔽的单词。让我用上面同样的例子来说明这一点。

该模型将提供上述序列，但带有一些屏蔽的标记(如下所示)，并被要求预测这些单词:

> 无监督表示<mask>在自然语言处理领域已经高度<mask>化。</mask></mask>

BERT 的预训练目标只是屏蔽语言建模(一种特殊的自动编码)，而不是自回归建模，而 XLNet 两者都用。

自回归建模的优点是，当模型试图预测一个单词时，它将可以访问所有单词，包括它之前预测的单词，这与自动编码不同，在自动编码中，它只能访问未屏蔽的标记。

我试图给出一个 XLNet 与 BERT 有什么不同的粗略概念。

要了解更多信息，我强烈推荐这篇写得非常棒的 [medium](https://towardsdatascience.com/xlnet-explained-in-simple-terms-255b9fb2c97c) 文章。

## 什么是情感抽取？

情感提取是一项自然语言处理任务，其中将给出一个句子及其相应的情感(无论是肯定句、否定句还是中性句),模型应该提取强烈支持给定情感的短语或词组。

我们如何做到这一点？

我们只是试图预测与给定情感非常一致的文本的开始和结束索引。

现在，让我们通过学习代码来弄脏我们的手。

注意:在这里，我没有特别使用任何数据来完成这项任务，我将提供基本的逻辑和代码，以便人们可以将此逻辑嵌入到他们自己的定制数据集的模型中。

从头开始编写像 XLNet 这样的模型并对它们进行预训练确实是一项艰巨的任务。非常感谢 [**拥抱脸变形金刚**](https://huggingface.co/transformers/) ，这个库让我们的任务变得更简单。它提供了预训练的转换器模型和标记器……它还为问答任务提供了 XLNet 模型(非常类似于情感提取任务),但我们将在这里微调我们自己的模型。

# 符号化:

这是所有自然语言处理任务的第一步。我们需要对句子进行标记。

那么为什么要符号化呢？

因为不幸的是，计算机不擅长处理文字但幸运的是，它们擅长数字。

XLNet 使用[句子片段](https://arxiv.org/abs/1808.06226)分词器对其文本进行分词，然而，在拥抱面部变形器中提供了一个用于 XLNet 的预训练分词器。

```
from transformers import *tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased'
 ,do_lower_case = True)
```

上面这段代码用于导入 transformers 库和预训练的 XLNet 标记器。我们将要讨论的代码要求数据框中的训练集具有“文本”、“情感”和“选定文本”列。

现在，让我们看看如何标记一个句子并获得一个数据点，以便我们可以将相同的逻辑放入一个循环中来标记整个数据集。

XLNet 需要三种类型的输入:

1.  input_ids:标记化的输入，必要时用<cls>和<sep>标记填充到最大长度。</sep></cls>
2.  attention_mask:这有助于我们的 XLNet 模型只关注非填充令牌。
3.  token_type_ids:如果我们的输入有两种不同类型的序列，例如在我们的任务中，我们有两种不同的序列，一种是从中提取文本的句子，另一种是相应的情感。所以这两个可以通过在 token_type_ids 中给不同的值来区分。

为了简单起见，在这篇文章中我将忽略令牌类型 id，如果我们使用令牌类型 id，性能可能会略有不同。

好吧！现在，让我们标记并获得句子的输入 id 和注意力屏蔽。

> 输入句子:“巴黎是个美妙的地方！”
> 
> 情绪:“积极”

大量代码！别担心，让我们一部分一部分地过一遍！

1.  第 3 行:我创建了一个字典，将特殊标记映射到它们的编号。这样我们就可以避免每次都调用 tokenizer.encode()。
2.  第 7，8 行:默认情况下，tokenizer.encode()在末尾添加 sep 和 cls 标记，但是这里我们需要做更多的工作(比如查找偏移量),所以我们将它们排除在外。

> *注意 1 : tokenizer.encode()有时可能会将一个单词拆分成多个片段，例如它将 paris 拆分成 pari & s.*
> 
> 注意 2 : Offsets 帮助我们回到原来的句子，它存储了每个标记的开始和结束索引。

3.第 10、11、12 行:我们创建了一个数组 *chars* ，将 1 赋给对应于所选文本的位置，将 0 赋给剩余的位置。

4.第 15，16，17 行:我们找到偏移量来知道每个令牌的位置，并把它存储在一个列表中。

5.第 20、21、22 行:这段代码帮助我们找出句子编码中的哪个标记也在我们选择的文本中(这样我们就可以据此计算开始和结束索引)。

仅此而已！将上述逻辑放在 for 循环中，对整个数据集进行标记化。现在，让我们在 XLNet 上构建一个 CNN 头。

# 建筑模型:

上面的代码在 XLNet 上构建了一个简单的 CNN 头，它将 input_ids 和 attention mask 作为输入，将开始和结束索引作为输出。

就是这样！我们完了。

现在可以通过选择优化器(可能是 adam 优化器)和损失函数(通常是分类交叉熵)来训练该模型。

# 结论:

我希望你喜欢阅读这篇文章。如果您有任何疑问，请在此发布。要查看完整代码，请随意访问这个 [GitHub repo](https://github.com/mano3-1/Sentiment-Extraction-from-tweets) 。