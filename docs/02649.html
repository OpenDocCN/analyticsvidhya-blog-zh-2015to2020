<html>
<head>
<title>House rental — the Data Science way Part 1: scrape it all with python and BeautifulSoup — UPDATED</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">房屋租赁—数据科学方法第1部分:使用python和BeautifulSoup收集所有信息—更新</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/house-rental-the-data-science-way-part-1-scrape-it-all-with-python-and-beautifulsoup-94d9d1222e32?source=collection_archive---------10-----------------------#2019-12-27">https://medium.com/analytics-vidhya/house-rental-the-data-science-way-part-1-scrape-it-all-with-python-and-beautifulsoup-94d9d1222e32?source=collection_archive---------10-----------------------#2019-12-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/da891c87a60b5de971dd2c1537410ef7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WqvtqgF2o7e2I1BliQhL2g.png"/></div></div></figure><p id="49f3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">去年我从原来的房子搬到了一个新的城市，换了工作。一切都进展得如此之快，在开始新工作之前，我只有几周的时间找到住处。在这种匆忙中，我没有足够的时间去了解这个城市的房地产市场，最终选择了在工作距离和服务之间取得更好平衡的住宿。但是……嗯……房间很小，我想我为那栋房子付了太多钱。但我只是猜测！</p><p id="574b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">那…如果我能找出来呢？<br/>那么……让我们回到我所学的机器学习。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><h1 id="3c98" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated">构建您自己的数据集</h1><p id="72cf" class="pw-post-body-paragraph iq ir hi is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">这是机器学习应用的一个非常常见和标准的例子:对房价进行回归，以找出房价的真实成本。你可以在网上任何你想要的地方找到它的例子。除了他们通常使用来自…嗯…任何地方的数据集。我需要来自<strong class="is hj">的<strong class="is hj">新鲜价格</strong>，我想要的城市</strong>，我希望它可以在几个月内更新。只有一个办法:刮！</p><p id="e173" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我住在意大利，确切地说是都灵。在意大利，收集所有租房或买房信息的最大网站是www.immobiliare.it  </p><p id="5065" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Immobiliare .它收集了意大利每个机构可以用来展示他们所管理的建筑的公告，所以这可能是了解特定城市房地产市场的最佳方式。现在是我们动手的时候了。</p><h1 id="db74" class="jv jw hi bd jx jy la ka kb kc lb ke kf kg lc ki kj kk ld km kn ko le kq kr ks bi translated">做汤—更新</h1><p id="70ff" class="pw-post-body-paragraph iq ir hi is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated"><em class="kz">更新:在夏季immobiliare.it的开发者用新的布局和更难浏览的html更新了网站。我将更新发布的代码以反映变化。</em></p><p id="95a2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们要做的是上网站，导航到我们城市的主页，收集该城市所有地区的列表，并收集该地区发布的每个公告。<br/>工具集:</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="351e" class="lo jw hi lk b fi lp lq l lr ls"><strong class="lk hj">import</strong> <strong class="lk hj">requests</strong><br/><strong class="lk hj">from</strong> <strong class="lk hj">bs4</strong> <strong class="lk hj">import</strong> BeautifulSoup<br/><strong class="lk hj">import</strong> <strong class="lk hj">pandas</strong> <strong class="lk hj">as</strong> <strong class="lk hj">pd</strong><br/><strong class="lk hj">from</strong> <strong class="lk hj">tqdm</strong> <strong class="lk hj">import</strong> tqdm_notebook <strong class="lk hj">as</strong> tqdm<br/><strong class="lk hj">import</strong> <strong class="lk hj">csv</strong></span></pre><p id="c3ed" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们准备好出发了！</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="35f5" class="lo jw hi lk b fi lp lq l lr ls">def get_pages(main):<br/>    try:<br/>        soup = connect(main)<br/>        n_pages = [_.get_text(strip=True) for _ in soup.find('ul', {'class': 'pagination pagination__number'}).find_all('li')]<br/>        last_page = int(n_pages[-1])<br/>        pages = [main]<br/>        <br/>        for n in range(2,last_page+1):    <br/>            page_num = "/?pag={}".format(n)<br/>            pages.append(main + page_num)<br/>    except:<br/>        pages = [main]<br/>        <br/>    return pages</span><span id="14fe" class="lo jw hi lk b fi lt lq l lr ls">def connect(web_addr):<br/>    resp = requests.get(web_addr)<br/>    return BeautifulSoup(resp.content, "html.parser")</span><span id="9eda" class="lo jw hi lk b fi lt lq l lr ls">def get_areas(website):<br/>    data = connect(website)<br/>    areas = []<br/>    for ultag in data.find_all('ul', {'class': 'breadcrumb-list breadcrumb-list_list breadcrumb-list__related'}):<br/>        for litag in ultag.find_all('li'):<br/>            for i in range(len(litag.text.split(','))):<br/>                areas.append(litag.text.split(',')[i])<br/>    areas = [x.strip() for x in areas]<br/>    urls = []<br/>    <br/>    for area in areas:<br/>        url = website + '/' + area.replace(' ','-').lower()<br/>        urls.append(url)<br/>    <br/>    return areas, urls</span><span id="f72b" class="lo jw hi lk b fi lt lq l lr ls">def get_apartment_links(website):<br/>    data = connect(website)<br/>    links = []<br/>    for link in data.find_all('ul', {'class': 'annunci-list'}):<br/>        for litag in link.find_all('li'):<br/>            try:<br/>                links.append(litag.a.get('href'))<br/>            except:<br/>                continue<br/>    return links</span><span id="4eb8" class="lo jw hi lk b fi lt lq l lr ls">def scrape_link(website):<br/>    data = connect(website)<br/>    info = data.find_all('dl', {'class': 'im-features__list'})<br/>    comp_info = pd.DataFrame()<br/>    cleaned_id_text = []<br/>    cleaned_id__attrb_text = []<br/>    for n in range(len(info)):<br/>        for i in info[n].find_all('dt'):<br/>            cleaned_id_text.append(i.text)<br/>        for i in info[n].find_all('dd'):<br/>            cleaned_id__attrb_text.append(i.text)</span><span id="aa2f" class="lo jw hi lk b fi lt lq l lr ls">comp_info['Id'] = cleaned_id_text<br/>    comp_info['Attribute'] = cleaned_id__attrb_text<br/>    comp_info<br/>    feature = []<br/>    for item in comp_info['Attribute']:<br/>        try:<br/>            feature.append(clear_df(item))<br/>        except:<br/>            feature.append(ultra_clear_df(item))</span><span id="a153" class="lo jw hi lk b fi lt lq l lr ls">comp_info['Attribute'] = feature<br/>    return comp_info['Id'].values, comp_info['Attribute'].values</span><span id="cd42" class="lo jw hi lk b fi lt lq l lr ls">def remove_duplicates(x):<br/>    return list(dict.fromkeys(x))</span><span id="38d6" class="lo jw hi lk b fi lt lq l lr ls">def clear_df(the_list):<br/>    the_list = (the_list.split('\n')[1].split('  '))<br/>    the_list = [value for value in the_list if value != ''][0]<br/>    return the_list</span><span id="40f3" class="lo jw hi lk b fi lt lq l lr ls">def ultra_clear_df(the_list):<br/>    the_list = (the_list.split('\n\n')[1].split('  '))<br/>    the_list = [value for value in the_list if value != ''][0]<br/>    the_list = (the_list.split('\n')[0])<br/>    return the_list</span></pre><p id="94a2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">啊，我们走吧！<br/>我们刚刚定义了5个函数:</p><ul class=""><li id="34ac" class="lu lv hi is b it iu ix iy jb lw jf lx jj ly jn lz ma mb mc bi translated">connect():用于连接到网站，并从网站下载原始html代码；</li><li id="ae5b" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">get_areas():它抓取原始html来查找地区。每个地区都有一个唯一的链接，过滤与该地区相关的公告；</li><li id="7fe5" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">get_pages():对于每个地区的“主页”，它查找有多少页的公告可用，并为每一页创建一个链接；</li><li id="00de" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">get_apartment_links():对于找到的每个页面，它查找每个公告并收集每个链接</li><li id="7b19" class="lu lv hi is b it md ix me jb mf jf mg jj mh jn lz ma mb mc bi translated">scrape_link():这个函数是announces的正确抓取过程</li></ul><p id="2b1e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在执行的最后，我们将会有每一个公告的链接，并注明来源地区。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="22c5" class="lo jw hi lk b fi lp lq l lr ls"><em class="kz">## Get areas inside the city (districts)</em><br/><br/>website = "https://www.immobiliare.it/affitto-case/torino"<br/>districts = get_areas(website)<br/>print("Those are district's links <strong class="lk hj">\n</strong>")<br/>print(districts)</span><span id="6a44" class="lo jw hi lk b fi lt lq l lr ls">## Now we need to find all announces' links, in order to scrape informations inside them one by one</span><span id="c09a" class="lo jw hi lk b fi lt lq l lr ls">address = []<br/>location = []</span><span id="9649" class="lo jw hi lk b fi lt lq l lr ls">try:<br/>    for url in tqdm(districts):<br/>        pages = get_pages(url)<br/>        for page in pages:<br/>            add = get_apartment_links(page)<br/>            address.append(add)<br/>            for num in range(0,len(add)):<br/>                location.append(url.rsplit('/', 1)[-1])<br/>except Exception as e:<br/>    print(e)<br/>    continue<br/>        <br/>announces_links = [item for value in address for item in value]</span><span id="b9c1" class="lo jw hi lk b fi lt lq l lr ls">## Just check it has some sense and save it</span><span id="4513" class="lo jw hi lk b fi lt lq l lr ls">print("The numerosity of announces:<strong class="lk hj">\n</strong>")<br/>print(len(announces_links))<br/><strong class="lk hj">with</strong> open('announces_list.csv', 'w') <strong class="lk hj">as</strong> myfile:<br/>    wr = csv.writer(myfile)<br/>    wr.writerow(announces_links)</span></pre><p id="d665" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们有了那个特定城市的每个公告的链接，所以让我们寻找宝藏。</p><h1 id="acfc" class="jv jw hi bd jx jy la ka kb kc lb ke kf kg lc ki kj kk ld km kn ko le kq kr ks bi translated">工作中的厨师！</h1><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="b5c9" class="lo jw hi lk b fi lp lq l lr ls">## Now we pass all announces' links do the scrape_link function to obtain apartments' informations</span><span id="4baf" class="lo jw hi lk b fi lt lq l lr ls">df_scrape = pd.DataFrame()<br/>to_be_dropped = []<br/>counter = 0<br/>for link in tqdm(list(announces_links)):<br/>    counter=counter+1<br/>    try:<br/>        names, values = scrape_link(link)<br/>        temp_df = pd.DataFrame(columns=names)<br/>        temp_df.loc[len(temp_df), :] = values[0:len(names)]<br/>        df_scrape = df_scrape.append(temp_df, sort=False)<br/>    except Exception as e:<br/>        print(e)<br/>        to_be_dropped.append(counter)<br/>        print(to_be_dropped)<br/>        continue</span><span id="6cf1" class="lo jw hi lk b fi lt lq l lr ls">## Eventually save useful informations odtained during the scrape process</span><span id="210c" class="lo jw hi lk b fi lt lq l lr ls">pd.DataFrame(location).to_csv('location.csv', sep=';')<br/>pd.DataFrame(to_be_dropped).to_csv('to_be_dropped.csv', sep=';')</span></pre><p id="0825" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这段代码遍历每个公告并从中提取信息，收集两个列表中的所有信息:<em class="kz"> nomi </em>和<em class="kz"> valori。</em>第一个包含特征的名称，第二个包含值。<br/>在最后的刮削过程中，我们终于有了一只熊猫。一种数据帧，其中每一行都存储着一个不同的通告及其特征和所属的地区。<br/>只要检查出精细的数据帧就有意义。</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="cb0e" class="lo jw hi lk b fi lp lq l lr ls">print(df_scrape.shape)<br/>df_scrape[‘district’] = location<br/>df_scrape[‘links’] = announces_links<br/>df_scrape.columns = map(str.lower, df_scrape.columns)<br/>df_scrape.to_csv(‘dataset.csv’, sep=”;”)</span></pre><p id="9415" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们有了一个包含24列(24个特征)的数据框架，我们可以用它来训练我们的回归算法。</p><p id="dae5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，在把所有的东西放进锅里之前，我们必须清理干净并把配料切片…</p><h1 id="ae8c" class="jv jw hi bd jx jy la ka kb kc lb ke kf kg lc ki kj kk ld km kn ko le kq kr ks bi translated">切片、切割和清理</h1><p id="fc96" class="pw-post-body-paragraph iq ir hi is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">不幸的是，数据集并不完全…嗯…准备好了。我们收集的东西通常很脏，我们无法处理。仅举几个例子:价格以“600€/月”的形式存储为<em class="kz">字符串</em>，5间以上的房子列为<em class="kz">“6+”</em>，等等。</p><p id="ee4b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以在这里我们有工具来“清洗他们所有人”(“咕鲁，咕鲁！”)</p><pre class="lf lg lh li fd lj lk ll lm aw ln bi"><span id="86fa" class="lo jw hi lk b fi lp lq l lr ls">df_scrape = df_scrape[['contratto', 'zona', 'tipologia', 'superficie', 'locali', 'piano', 'tipo proprietà', 'prezzo', 'spese condominio', 'spese aggiuntive', 'anno di costruzione', 'stato', 'riscaldamento', 'climatizzazione', 'posti auto', 'links']]</span><span id="ad4e" class="lo jw hi lk b fi lt lq l lr ls">def cleanup(df):<br/>    price = []<br/>    rooms = []<br/>    surface = []<br/>    bathrooms = []<br/>    floor = []<br/>    contract = []<br/>    tipo = []<br/>    condominio = []<br/>    heating = []<br/>    built_in = []<br/>    state = []<br/>    riscaldamento = []<br/>    cooling = []<br/>    energy_class = []<br/>    tipologia = []<br/>    pr_type = []<br/>    arredato = []<br/>    <br/>    for tipo in df['tipologia']:<br/>        try:<br/>            tipologia.append(tipo)<br/>        except:<br/>            tipologia.append(None)<br/>    <br/>    for superficie in df['superficie']:<br/>        try:<br/>            if "m" in superficie:<br/>                #z = superficie.split('|')[0]<br/>                s = superficie.replace(" m²", "")<br/>                surface.append(s)<br/>        except:<br/>            surface.append(None)<br/>    <br/>    for locali in df['locali']:<br/>        try:<br/>            rooms.append(locali[0:1])<br/>        except:<br/>            rooms.append(None)<br/>    <br/>    for prezzo in df['prezzo']:<br/>        try:<br/>            price.append(prezzo.replace("Affitto ", "").replace("€ ", "").replace("/mese", "").replace(".",""))<br/>        except:<br/>            price.append(None)<br/>            <br/>    for contratto in df['contratto']:<br/>        try:<br/>            contract.append(contratto.replace("\n ",""))<br/>        except:<br/>            contract.append(None)<br/>    <br/>    for piano in df['piano']:<br/>        try:<br/>            floor.append(piano.split(' ')[0])<br/>        except:<br/>            floor.append(None)<br/>    <br/>    for tipologia in df['tipo proprietà']:<br/>        try:<br/>            pr_type.append(tipologia.split(',')[0])<br/>        except:<br/>            pr_type.append(None)<br/>            <br/>    for condo in df['spese condominio']:<br/>        try:<br/>            if "mese" in condo:<br/>                condominio.append(condo.replace("€ ","").replace("/mese",""))<br/>            else:<br/>                condominio.append(None)<br/>        except:<br/>            condominio.append(None)<br/>        <br/>    for ii in df['spese aggiuntive']:<br/>        try:<br/>            if "anno" in ii:<br/>                mese = int(int(ii.replace("€ ","").replace("/anno","").replace(".",""))/12)<br/>                heating.append(mese)<br/>            else:<br/>                heating.append(None)<br/>        except:<br/>            heating.append(None)<br/>   <br/>    for anno_costruzione in df['anno di costruzione']:<br/>        try:<br/>            built_in.append(anno_costruzione)<br/>        except:<br/>            built_in.append(None)<br/>    <br/>    for stato in df['stato']:<br/>        try:<br/>            stat = stato.replace(" ","").lower()<br/>            state.append(stat)<br/>        except:<br/>            state.append(None)<br/>    <br/>    for tipo_riscaldamento in df['riscaldamento']:<br/>        try:<br/>            if 'Centralizzato' in tipo_riscaldamento:<br/>                riscaldamento.append('centralizzato')<br/>            elif 'Autonomo' in tipo_riscaldamento:<br/>                riscaldamento.append('autonomo')<br/>        except:<br/>            riscaldamento.append(None)<br/>    <br/>    for clima in df['climatizzazione']:<br/>        try:<br/>            cooling.append(clima.lower().split(',')[0])<br/>        except:<br/>            cooling.append('None')<br/>    <br/>    final_df = pd.DataFrame(columns=['contract', 'district', 'renting_type', 'surface', 'locals', 'floor', 'property_type', 'price', 'spese condominio', 'other_expences', 'building_year', 'status', 'heating', 'air_conditioning', 'energy_certificate', 'parking_slots'])#, 'Arredato S/N'])<br/>    final_df['contract'] = contract<br/>    final_df['renting_type'] = tipologia<br/>    final_df['surface'] = surface<br/>    final_df['locals'] = rooms<br/>    final_df['floor'] = floor<br/>    final_df['property_type'] = pr_type<br/>    final_df['price'] = price<br/>    final_df['spese condominio'] = condominio<br/>    final_df['heating_expences'] = heating<br/>    final_df['building_year'] = built_in<br/>    final_df['status'] = state<br/>    final_df['heating_system'] = riscaldamento<br/>    final_df['air_conditioning'] = cooling<br/>    #final_df['classe energetica'] = energy_class<br/>    final_df['district'] = df['zona'].values<br/>    #inal_df['Arredato S/N'] = arredato<br/>    final_df['announce_link'] = announces_links<br/>    <br/>    return final_df</span><span id="8c59" class="lo jw hi lk b fi lt lq l lr ls">final = cleanup(df_scrape)<br/>final.to_csv('regression_dataset.csv', sep=";")</span></pre><p id="aa03" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该函数根据脏数据的类型，以不同的方式处理脏数据。他们中的大多数已经用Regex<strong class="is hj">和string相关的工具清理过了。看看这个脚本，看看它是如何工作的。<br/> <em class="kz"> PS:数据集上可能仍然存在少量错误。像你喜欢的那样对待他们。</em></strong></p><h1 id="077a" class="jv jw hi bd jx jy la ka kb kc lb ke kf kg lc ki kj kk ld km kn ko le kq kr ks bi translated">准备锅！</h1><p id="64f9" class="pw-post-body-paragraph iq ir hi is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">我们到了！现在我们有了一个手工制作的数据集，我们可以在其上处理ML和回归。在下一篇文章中，我将解释我如何处理所有的成分。</p><p id="13b6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">敬请期待！</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="c195" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="kz">链接GitHub</em><a class="ae ky" href="https://github.com/wonka929/house_scraping_and_regression" rel="noopener ugc nofollow" target="_blank"><em class="kz">https://github.com/wonka929/house_scraping_and_regression</em></a></p><p id="396c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这篇文章是教程的第一部分。你可以在这个链接找到第二篇:<br/><a class="ae ky" rel="noopener" href="/@wonka929/house-rental-the-data-science-way-part-2-train-a-regression-model-tpot-and-auto-ml-9cdb5cb4b1b4">https://medium . com/@ wonka 929/house-rent-the-data-science-way-part-2-train-a-regression-model-tpot-and-auto-ml-9 CDB 5 CB 4 B1 b 4</a></p><p id="3710" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="kz">更新</em> </strong> <em class="kz">:在处理immobiliare.it的新网站时，我决定也更新一下回归方法论。<br/>这是您可以在网上找到的新的更新文章:<br/></em><a class="ae ky" href="https://wonka929.medium.com/house-rental-the-data-science-way-part-2-1-train-and-regression-model-using-pycaret-72d054e22a78" rel="noopener">https://wonka 929 . medium . com/house-rent-the-data-science-way-part-2-1-train-and-regression-model-using-py caret-72d 054 e22a 78</a></p></div></div>    
</body>
</html>