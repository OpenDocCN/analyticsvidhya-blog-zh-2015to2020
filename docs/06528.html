<html>
<head>
<title>How to scrape a news website using Python, BeautyfulSoup, and Selenium to build the Word Cloud</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用Python、BeautyfulSoup和Selenium构建单词云来构建新闻网站</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-scrape-a-news-website-using-python-beautyfulsoup-and-selenium-to-build-the-word-cloud-3355066b72dc?source=collection_archive---------13-----------------------#2020-05-25">https://medium.com/analytics-vidhya/how-to-scrape-a-news-website-using-python-beautyfulsoup-and-selenium-to-build-the-word-cloud-3355066b72dc?source=collection_archive---------13-----------------------#2020-05-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="8d5f" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">通过使用词云实现更好的可视化</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/cbf0f04ba3d902e324244702922cf234.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oFjaReilo0xBbXt766hKwA.png"/></div></div></figure><p id="a663" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">从2020年初到现在，香港报纸上最常出现的新闻是什么？香港社会运动，2019冠状病毒还是中美贸易战？我在香港刮了一份知名报纸，希望用一个好的可视化方法~ <strong class="jl hj">字云</strong>得到我想要的答案。</p><p id="8cf3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">什么是网页抓取？</strong></p><p id="ddb6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">网络抓取工具专门用于从网站上提取信息。它们也被称为网络收集工具或网络数据提取工具。</p><p id="1478" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">为什么要做网页抓取？</strong></p><p id="c379" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">网络抓取工具可以在各种情况下用于各种目的。例如:</p><ol class=""><li id="3dc4" class="kf kg hi jl b jm jn jp jq js kh jw ki ka kj ke kk kl km kn bi translated">收集市场研究数据</li><li id="96d1" class="kf kg hi jl b jm ko jp kp js kq jw kr ka ks ke kk kl km kn bi translated">收集股票市场信息</li><li id="f097" class="kf kg hi jl b jm ko jp kp js kq jw kr ka ks ke kk kl km kn bi translated">收集联系信息</li><li id="9a38" class="kf kg hi jl b jm ko jp kp js kq jw kr ka ks ke kk kl km kn bi translated">收集数据以下载供离线阅读或存储</li><li id="659c" class="kf kg hi jl b jm ko jp kp js kq jw kr ka ks ke kk kl km kn bi translated">跟踪多个市场的价格等。</li></ol><p id="7891" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">如何在Python中刮？</strong></p><p id="b1d9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">用Python抓取一个网站非常容易，尤其是在BeautifulSoup和Selenium库的帮助下。Beautiful Soup是一个Python库模块，允许开发人员通过编写少量代码，快速解析网页HTML代码并从中提取有用的数据，减少开发时间，加快网页抓取的编程速度。Selenium是一个自动化测试网页的工具，可以通过它提供的一些方法自动操作浏览器，可以完全模拟真人的操作。</p><p id="9a6f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在抓取一个网站之前，必须做一些准备工作，在Jupiter笔记本中输入命令，它将安装以下库。</p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="d687" class="ky kz hi ku b fi la lb l lc ld">!pip install BeautifulSoup4<br/>!pip install selenium</span></pre><p id="1c47" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">什么是词云，我们为什么需要它？</strong></p><p id="99a4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">Word Cloud是Python中非常好的第三方word cloud可视化库。单词云是在文本中更频繁出现的关键词的可视化显示。WordCloud会过滤掉大量低频低质量的文字信息，让受众一眼就能明白文字的主旨。</p><p id="2c4f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">什么是分词？</strong></p><p id="044e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">分词是将连续的词序列按照一定的规范重新组合成词序列的过程。Jieba是一个比较好的中文分词库，因为中文通常包含整个句子，所以我们需要使用Jieba来辅助分词的工作。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es le"><img src="../Images/2218b1e03ecc10a326d8ae50b5cb7f1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*26qQCmYtmhLhPLjmy2kY4g.png"/></div></div></figure><p id="dae2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在生成word云映像之前，必须做一些准备工作，在Jupiter笔记本中输入命令，它将安装以下库。</p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="e100" class="ky kz hi ku b fi la lb l lc ld">!pip install wordcloud<br/>!pip install jieba</span></pre></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><p id="df97" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> #加载库</strong></p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="ffb2" class="ky kz hi ku b fi la lb l lc ld">#! /usr/bin/env python<br/># -*- encoding UTF-8 -*-</span><span id="8028" class="ky kz hi ku b fi lm lb l lc ld">import os<br/>import sys<br/>from importlib import reload<br/>reload(sys)</span><span id="1016" class="ky kz hi ku b fi lm lb l lc ld">if sys.version[0] == '2':<br/>    sys.setdefaultencoding("utf-8")<br/>    <br/>    import parse<br/>    import urllib2<br/>else:<br/>    import urllib.parse<br/>    from urllib.request import urlopen</span><span id="0fc4" class="ky kz hi ku b fi lm lb l lc ld">import re<br/>import jieba</span><span id="5e17" class="ky kz hi ku b fi lm lb l lc ld">import pandas as pd<br/>import numpy as np</span><span id="ae3d" class="ky kz hi ku b fi lm lb l lc ld">#from bs4 import BeautifulSoup<br/>from selenium import webdriver<br/>from selenium.webdriver import ChromeOptions</span><span id="0552" class="ky kz hi ku b fi lm lb l lc ld">from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator</span><span id="de24" class="ky kz hi ku b fi lm lb l lc ld">from PIL import Image</span><span id="7c7b" class="ky kz hi ku b fi lm lb l lc ld">import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="da5f" class="ky kz hi ku b fi lm lb l lc ld">jieba.enable_paddle()</span></pre><p id="5062" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> #清理CSS、JavaScript和HTML标签</strong></p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="e46a" class="ky kz hi ku b fi la lb l lc ld">def cleanHTML(html):<br/>    for script in html(["script", "style"]): # remove all javascript and stylesheet code<br/>        script.extract()<br/>    # get text<br/>    text = html.get_text()<br/>    # break into lines and remove leading and trailing space on each<br/>    lines = (line.strip() for line in text.splitlines())<br/>    # break multi-headlines into a line each<br/>    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))<br/>    # drop blank lines<br/>    text = '\n'.join(chunk for chunk in chunks if chunk)<br/>    <br/>    return text</span></pre><p id="11dd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> #查找当日主要焦点新闻链接</strong></p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="5ca6" class="ky kz hi ku b fi la lb l lc ld">def getNewsLink(news_date):<br/>    try:<br/>        options = ChromeOptions()<br/>        options.add_argument('headless')<br/>        driver = webdriver.Chrome(options=options)</span><span id="0fc5" class="ky kz hi ku b fi lm lb l lc ld">        url = '<a class="ae ln" href="https://orientaldaily.on.cc/cnt/news/'" rel="noopener ugc nofollow" target="_blank">https://orientaldaily.on.cc/cnt/news/'</a> + str(news_date) + '/mobile/index.html'</span><span id="62dc" class="ky kz hi ku b fi lm lb l lc ld">        driver.get(url)<br/>        driver.implicitly_wait(30)</span><span id="0585" class="ky kz hi ku b fi lm lb l lc ld">        html_source = (driver.page_source.encode('utf-8'))</span><span id="cd6e" class="ky kz hi ku b fi lm lb l lc ld">        driver.quit()</span><span id="86e4" class="ky kz hi ku b fi lm lb l lc ld">        soup = BeautifulSoup(html_source, 'html.parser')<br/>        news = soup.find('div', attrs={'id':'swipe'})<br/>        main_focus = soup.find('div', attrs={'class':'main-focus-container'})<br/>        main_focus_link = '<a class="ae ln" href="https://orientaldaily.on.cc'" rel="noopener ugc nofollow" target="_blank">https://orientaldaily.on.cc'</a> + main_focus.find('a', href = re.compile(r'[/]([a-z]|[A-Z])\w+')).attrs['href']</span><span id="c5aa" class="ky kz hi ku b fi lm lb l lc ld">        return (main_focus_link)<br/>    except:<br/>        return 0</span></pre><p id="0e5e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> #数据收集</strong></p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="d81f" class="ky kz hi ku b fi la lb l lc ld">def getNews(news_url):<br/>    options = ChromeOptions()<br/>    options.add_argument('headless')<br/>    driver = webdriver.Chrome(options=options)<br/>    <br/>    driver.get(news_url)<br/>    driver.implicitly_wait(30)<br/>        <br/>    html_source = (driver.page_source.encode('utf-8'))<br/>        <br/>    driver.quit()<br/>    <br/>    soup = BeautifulSoup(html_source, 'html.parser')<br/>    paragraph = soup.find_all('div', attrs={'class':'paragraph'})<br/>    paragraph_list = []<br/>    <br/>    for sub_paragraph in paragraph:<br/>        clean_sub_paragraph = cleanHTML(sub_paragraph)<br/>        paragraph_list.append(clean_sub_paragraph)<br/>        <br/>    full_paragraph_list = [e for e in paragraph_list if e]<br/>    <br/>    if (len(full_paragraph_list) &gt; 0):<br/>        f=open("news.txt", "a+")<br/>        <br/>        for i in range(len(full_paragraph_list)):<br/>            <br/>            <br/>            for line in full_paragraph_list[i].splitlines():<br/>                clean_paragraph = cleanText(line)<br/>                <br/>                f.write(clean_paragraph)<br/>                <br/>        f.write('\n\n')<br/>        f.close()</span></pre><p id="ff79" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> #画字云</strong></p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="4a99" class="ky kz hi ku b fi la lb l lc ld">def draw_word_cloud(text, images_name, plt_title):<br/>    images_path = images_name<br/>    images = Image.open(images_path)<br/>    <br/>    #create a write mask <br/>    images_mask = Image.new("RGB", images.size, (255,255,255))<br/>    images_mask.paste(images, images)<br/>    images_mask = np.array(images_mask)<br/>    <br/>    color = ImageColorGenerator(images_mask)<br/>    <br/>    #Chinese need to use another font, download it from <a class="ae ln" href="https://www.freechinesefont.com/" rel="noopener ugc nofollow" target="_blank">https://www.freechinesefont.com/</a><br/>    font_path = 'HanyiSentyCandy.ttf'<br/>    <br/>    #create wordcloud ~ <br/>    wc = WordCloud(font_path=font_path, max_font_size=250, max_words=1000, mask=images_mask, \<br/>                   margin=5, background_color="black").generate_from_text(text)<br/>    wc.recolor(color_func = color, random_state = 7)<br/>    <br/>    #Save the image<br/>    wc.to_file("news.png")<br/>    <br/>    plt.rcParams["figure.figsize"] = (16, 12)<br/>    <br/>    plt.title(plt_title)<br/>    plt.imshow(wc, interpolation="bilinear")<br/>    plt.axis("off")<br/>    plt.show()</span></pre><p id="4da8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> #主逻辑:</strong></p><pre class="iy iz ja jb fd kt ku kv kw aw kx bi"><span id="566e" class="ky kz hi ku b fi la lb l lc ld">if __name__ == "__main__":<br/>    start_date = input("Enter the start date (yyyymmdd): ")<br/>    end_date = input("Enter the end date (yyyymmdd): ")<br/>    <br/>    if ((start_date != "") and (end_date != "")):<br/>        daterange = pd.date_range(start_date, end_date)<br/>        <br/>        #Download the main focus news<br/>        for news_date in daterange:<br/>            print ('Downloading ' + str(news_date) + ' news...')<br/>            single_news_date = dateConvert(news_date)<br/>            getNews(getNewsLink(single_news_date))<br/>            <br/>        source_text = open('news.txt', 'r',encoding= 'UTF-8').read()<br/>        tokens = ' '.join(jieba.cut_for_search(source_text))<br/>        <br/>        title = str(start_date) + ' - ' + str(end_date) + ' Main Focus News on on.cc'<br/>        <br/>        draw_word_cloud(tokens, 'hongkongpng.png', title)</span></pre><p id="e197" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> #未来改善</strong></p><ol class=""><li id="a0a7" class="kf kg hi jl b jm jn jp jq js kh jw ki ka kj ke kk kl km kn bi translated">添加停用词的处理</li><li id="4226" class="kf kg hi jl b jm ko jp kp js kq jw kr ka ks ke kk kl km kn bi translated">使用不同的分词库，如thulac、FoolNLTK、HanLP、nlpir和ltp。</li></ol></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><p id="1311" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">感谢阅读！如果你喜欢这篇文章，请通过鼓掌来感谢你的支持(👏🏼)按钮，或者通过共享这篇文章让其他人可以找到它。</p><p id="7052" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">最后希望你能学会刮痧的技巧。你也可以在<a class="ae ln" href="https://github.com/kindersham/100DaysDS/tree/master/NewsWordCloud" rel="noopener ugc nofollow" target="_blank"> GitHub </a>库上找到完整的项目。</p><p id="687e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">参考文献</strong></p><div class="lo lp ez fb lq lr"><a rel="noopener follow" target="_blank" href="/the-artificial-impostor/nlp-four-ways-to-tokenize-chinese-documents-f349eb6ba3c3"><div class="ls ab dw"><div class="lt ab lu cl cj lv"><h2 class="bd hj fi z dy lw ea eb lx ed ef hh bi translated">[NLP]中文文档的四种分词方法</h2><div class="ly l"><h3 class="bd b fi z dy lw ea eb lx ed ef dx translated">有了一些支持子词分割技术的经验证据</h3></div><div class="lz l"><p class="bd b fp z dy lw ea eb lx ed ef dx translated">medium.com</p></div></div><div class="ma l"><div class="mb l mc md me ma mf jh lr"/></div></div></a></div><div class="lo lp ez fb lq lr"><a href="https://amueller.github.io/word_cloud/index.html" rel="noopener  ugc nofollow" target="_blank"><div class="ls ab dw"><div class="lt ab lu cl cj lv"><h2 class="bd hj fi z dy lw ea eb lx ed ef hh bi translated">WordCloud for Python文档-word cloud 1 . 6 . 0 . post 54+GB 870 feb文档</h2><div class="ly l"><h3 class="bd b fi z dy lw ea eb lx ed ef dx translated">在这里你可以找到如何用我的Python wordcloud项目创建wordcloud的说明。与其他词云相比…</h3></div><div class="lz l"><p class="bd b fp z dy lw ea eb lx ed ef dx translated">amueller.github.io</p></div></div><div class="ma l"><div class="mg l mc md me ma mf jh lr"/></div></div></a></div></div></div>    
</body>
</html>