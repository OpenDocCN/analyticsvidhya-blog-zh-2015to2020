<html>
<head>
<title>PyTorch for Deep Learning — Tensor Broadcasting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习的PyTorch张量广播</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pytorch-for-deep-learning-part-2-bc0cfa12e74?source=collection_archive---------16-----------------------#2020-09-09">https://medium.com/analytics-vidhya/pytorch-for-deep-learning-part-2-bc0cfa12e74?source=collection_archive---------16-----------------------#2020-09-09</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/db2981e8083e386c4ee3f03820e11b3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H7dzTvdlMy_06hcDUIxDMA.jpeg"/></div></div></figure><h1 id="3b2c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">PyTorch中的更多张量运算</h1><p id="42cf" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在继续之前，让我们导入所需的库</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="1727" class="kv ir hi kr b fi kw kx l ky kz">#importing the libraries</span><span id="2ca5" class="kv ir hi kr b fi la kx l ky kz">import numpy as np<br/>import torch</span></pre><p id="fe3d" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">让我们创建两个张量，以便对它们执行一些操作</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="ee07" class="kv ir hi kr b fi kw kx l ky kz">t1 = torch.tensor([[1,2],[3,4]])<br/>t2 = torch.tensor([[5,6],[7,8]])</span></pre><h1 id="cb67" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">元素式操作</strong></h1><p id="f079" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">抱歉格式混乱</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="5d6d" class="kv ir hi kr b fi kw kx l ky kz">print("Tensor addition : {}\nTensor Subtraction : {}\nTensor Multiplication : {}\nTensor Division : {}".format(t1+t2,<br/>t1-t2,t1*t2,t1//t2))</span><span id="22cf" class="kv ir hi kr b fi la kx l ky kz"><strong class="kr hj">Output:<br/></strong>Tensor addition : tensor([[ 6,  8],         <br/>                          [10, 12]])<br/> <br/>Tensor Subtraction : tensor([[-4, -4],         <br/>                             [-4, -4]])<br/> <br/>Tensor Multiplication : tensor([[ 5, 12],         <br/>                                [21, 32]])<br/> <br/>Tensor Division : tensor([[0, 0],         <br/>                          [0, 0]])</span></pre><h1 id="6291" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">广播</h1><p id="7e34" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">广播是一种允许我们对不同大小的张量执行算术运算的功能<br/>在下面的例子中，标量“2”被转换为t1形状的张量，然后执行加法和减法。</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="5337" class="kv ir hi kr b fi kw kx l ky kz">#broadcasting with scalars</span><span id="868e" class="kv ir hi kr b fi la kx l ky kz">print("{}\n{}".format(t1 + 2,t1 - 2))</span><span id="0cfe" class="kv ir hi kr b fi la kx l ky kz"><strong class="kr hj">output:</strong></span><span id="00b7" class="kv ir hi kr b fi la kx l ky kz">tensor([[3, 4],         <br/>        [5, 6]]) </span><span id="bb0d" class="kv ir hi kr b fi la kx l ky kz">tensor([[-1,  0],         <br/>        [ 1,  2]])</span></pre><p id="a45d" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">再来看看广播除了2个张量<br/>广播也可以在多个张量上进行。<br/>在下面的示例中，将一行附加到t5，其值与第一行的值相同。然后，执行加法</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="8f7c" class="kv ir hi kr b fi kw kx l ky kz">#broadcasting tensors</span><span id="6492" class="kv ir hi kr b fi la kx l ky kz">t3 = torch.tensor([[1,2],[3,4]])<br/>t5 = torch.tensor([5,5])<br/>print(t3+t5)</span><span id="ce74" class="kv ir hi kr b fi la kx l ky kz"><strong class="kr hj">output:</strong></span><span id="53ba" class="kv ir hi kr b fi la kx l ky kz">tensor([[6, 7],         <br/>        [8, 9]])</span></pre><figure class="km kn ko kp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/1ab2ee69db73bac9c4dd89dbe7aaead3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ppFeOG4nHEQ5EULYvzzl5A.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated">广播如何工作</figcaption></figure><p id="8440" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">广播一开始可能会令人困惑。但是，经过足够的练习，它会变得容易</p><h1 id="7f89" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">归约操作</h1><p id="798b" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">归约运算是在执行时减小张量大小的运算</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="4ced" class="kv ir hi kr b fi kw kx l ky kz">#reduction operations</span><span id="3f2f" class="kv ir hi kr b fi la kx l ky kz">print(t1)<br/>print("Some of the reduction operations are:\nadd:{}\nmax:{}\nproduct:{}".format(t1.sum(),t1.max(),t1.prod()))</span><span id="b66e" class="kv ir hi kr b fi la kx l ky kz"><strong class="kr hj">output:</strong></span><span id="a913" class="kv ir hi kr b fi la kx l ky kz">tensor([[1, 2],<br/>        [3, 4]])</span><span id="f10a" class="kv ir hi kr b fi la kx l ky kz">Some of the reduction operations are: <br/>add:10 <br/>max:4 <br/>product:24</span></pre><p id="b333" class="pw-post-body-paragraph jo jp hi jq b jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">Argmax函数:返回张量最大元素的索引</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="2231" class="kv ir hi kr b fi kw kx l ky kz">#argmax function</span><span id="0a01" class="kv ir hi kr b fi la kx l ky kz">print(t1,"\nindex of the largest element is : ",t1.argmax())</span><span id="b939" class="kv ir hi kr b fi la kx l ky kz"><strong class="kr hj">output:</strong></span><span id="3125" class="kv ir hi kr b fi la kx l ky kz">tensor([[1, 2],         <br/>        [3, 4]])<br/>  <br/>index of the largest element is :  tensor(3)</span></pre><h1 id="aee3" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">张量到Python数据类型</h1><p id="b427" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">就像numpy数组可以转换成torch张量一样，张量也可以转换成numpy数组或常规python数据类型。<br/>如果是标量，。item()会将张量转换成python整数<br/>如果是向量，。tolist()将张量转换成python列表</p><pre class="km kn ko kp fd kq kr ks kt aw ku bi"><span id="c808" class="kv ir hi kr b fi kw kx l ky kz">#tensor to python datatypes</span><span id="7768" class="kv ir hi kr b fi la kx l ky kz">print(t1.max().item(),"\n{}".format(t1.flatten().tolist()))</span><span id="a85c" class="kv ir hi kr b fi la kx l ky kz"><strong class="kr hj">output:</strong></span><span id="fa2f" class="kv ir hi kr b fi la kx l ky kz">4  <br/>[1, 2, 3, 4]</span></pre><h1 id="2701" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">结论</h1><p id="7afd" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">一些更多的张量运算和广播显示在这个职位。</p><h2 id="4cf9" class="kv ir hi bd is ll lm ln iw lo lp lq ja jz lr ls je kd lt lu ji kh lv lw jm lx bi translated">谢谢你</h2></div></div>    
</body>
</html>