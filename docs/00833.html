<html>
<head>
<title>Fine-tuning Bert language model to get better results on text classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">微调Bert语言模型以获得更好的文本分类结果</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fine-tuning-bert-language-model-to-get-better-results-on-text-classification-3dac5e3c348e?source=collection_archive---------0-----------------------#2019-09-12">https://medium.com/analytics-vidhya/fine-tuning-bert-language-model-to-get-better-results-on-text-classification-3dac5e3c348e?source=collection_archive---------0-----------------------#2019-09-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="7af6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你在这里，你可能听说过伯特。在我们继续之前，让我简单介绍一下伯特。</p><blockquote class="jd je jf"><p id="fd9e" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">BERT(来自转换器的双向编码器表示)是一种基于转换器的学习语言表示的方法。它是一个双向转换器预训练模型，使用两个任务的组合开发，即:<strong class="ih hj">掩蔽语言建模目标</strong>和<strong class="ih hj">下一句预测</strong>在大型语料库上。</p></blockquote><p id="5730" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它已经在各种NLP任务上取得了最先进的结果。我们可以使用BERT学习的语言表示来完成我们的任务，例如文本分类等等，以获得我们问题的最新结果。</p><p id="6bb5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jg">注意:我们不会深入技术细节，但如果有人对阅读关于变形金刚的文章感兴趣，</em> <a class="ae jk" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> <em class="jg">这篇</em> </a> <em class="jg">博客会非常有帮助。</em></p><p id="42e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将看到如何使用Bert的语言模型来完成文本分类任务。HuggingFace的Pytorch变形金刚让这变得非常容易。通过Pytorch-transformers，我们可以使用Bert的预训练语言模型进行序列分类。我们还可以微调Bert的预训练语言模型以适应我们的任务，然后使用该模型获得一些改进。</p><p id="af12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本教程中，我将展示如何微调Bert的语言模型，然后如何使用微调的语言模型进行序列分类。我们还将把结果与直接使用预先训练的Bert模型进行比较。我们将使用PyTorch-transformers和电影评论的数据在Google Colab上完成所有这些工作。所有代码都可以在下面的共享Github资源库中找到。</p><h1 id="a91f" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">入门指南</h1><p id="4784" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">完整代码的实现和解释可在<a class="ae jk" href="https://github.com/Shivampanwar/Bert-text-classification" rel="noopener ugc nofollow" target="_blank">本</a>回购中找到。</p><p id="ea2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们经常有大量的未标记数据集，只有少量的已标记数据集。如果我们需要获得准确的分类，我们可以使用在大语料库上训练的预训练模型来获得不错的结果。通常，我们使用在大型语料库上训练的预训练语言模型来获得嵌入，然后主要在其上添加一层或两层神经网络来适应我们手头的任务。这种方法非常有效，直到训练语言模型的数据与我们的数据相似。</p><p id="b1f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们的数据不同于用于预训练的数据，结果将不会令人满意。例如，如果我们有印地语和英语语言的混合数据，并且我们使用在维基百科上训练的预训练模型，这将导致不好的结果。在这种情况下，我们也需要微调我们的语言模型。</p><p id="d3d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如杰瑞米·霍华德和塞巴斯蒂安·鲁德在<a class="ae jk" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">文本分类通用语言模型微调</a>中所示，微调语言模型可以提高性能。我们通常修改语言模型的最后几层来适应我们的数据。这已经由Fast.ai在<a class="ae jk" href="https://docs.fast.ai/text.html#Fine-tuning-a-language-model" rel="noopener ugc nofollow" target="_blank">微调FastAI语言模型</a>中完成并解释。他们已经为ULMFit做了大量的工作。我们可以对伯特采用同样的方法。</p><h2 id="7db5" class="ko jm hi bd jn kp kq kr jr ks kt ku jv iq kv kw jz iu kx ky kd iy kz la kh lb bi translated">电影评论数据集</h2><p id="00a9" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">让我们收集数据。为了我们的目的，我们将使用https://www.kaggle.com/c/word2vec-nlp-tutorial/data的T4数据。</p><p id="3c07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该数据集包含25000个具有带标签的电影评论的训练示例和25000个不带标签的测试示例。我们将在总共有50000条评论的组合训练和测试数据上微调我们的语言模型。</p><p id="2729" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本教程将分三步进行:</p><p id="9676" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1 —第一步是在训练和测试数据集上微调我们的语言模型。然后，我们将对训练数据集进行80:20分割。</p><p id="2a36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2 —第二步是直接使用预先训练的Bert语言模型，在80%的数据上训练该模型，然后在20%的数据上进行测试。</p><p id="2d01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3 —第三步与第二步相同，唯一的区别是我们这次将使用微调的语言模型。</p><h1 id="e72e" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">微调语言模型</h1><p id="4e0d" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">我们将使用PyTorch-transformers来微调预训练的Bert语言模型。它写得很好，有据可查。我们将使用<a class="ae jk" href="https://github.com/huggingface/pytorch-transformers/tree/master/examples/lm_finetuning" rel="noopener ugc nofollow" target="_blank">这个</a>进行微调。</p><p id="9185" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上所述，它使用屏蔽语言建模和下一句预测来获得正确的嵌入。在掩蔽语言建模中，它在训练期间掩蔽或隐藏某些单词，并试图预测它们，同时它还试图预测两个句子是否相邻。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es lc"><img src="../Images/81e5ce71a1a3fcb0e0049c1997cb8070.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*_HXzcDcM5yy84tZ6TgbHbA.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated"><strong class="bd jn">遮罩建模任务</strong></figcaption></figure><figure class="ld le lf lg fd lh er es paragraph-image"><div class="er es lo"><img src="../Images/1755516ea9fad156c8c709dd3f8eae99.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*AC9b8oAaSZg4g6Qzzt0xvw.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated"><strong class="bd jn">下一句预测任务</strong></figcaption></figure><p id="3559" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于相同的任务，即掩码建模和下一句预测，Bert要求训练数据采用特定的格式。这种格式是使用<a class="ae jk" href="https://github.com/huggingface/pytorch-transformers/blob/master/examples/lm_finetuning/pregenerate_training_data.py" rel="noopener ugc nofollow" target="_blank">pregenerate _ training _ data . py</a>制作的</p><p id="328f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个脚本需要一个单一的文件作为输入，由未标记的文本组成，每行一句话，文档之间有一个空行。分割句子的原因是BERT训练的一部分涉及下一个句子目标，其中模型必须预测两个文本序列是否是来自同一文档的连续文本。</p><p id="761b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">运行这个脚本将创建一个名为training的新目录，其中包含所需格式的训练数据。如果你想在训练前了解Bert期望的数据是如何产生的，一定要看看这个目录中的数据格式。训练数据如下所示。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lp"><img src="../Images/f2a10a322639b6f33c050ed4496e083f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wzUllrO-PW3rXBWrz8aBdA.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">【Bert所需格式的数据</figcaption></figure><p id="c7b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在将运行finetune_on_pregenerated.p来获取微调后的语言模型。至此，我们的<strong class="ih hj">语言模型已经创建完毕。</strong></p><p id="22bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">微调模型包含五个文件。<strong class="ih hj"> vocab.txt </strong>文件的全部词汇。<strong class="ih hj"> config.json </strong>'包含你的模型的配置。我们的模型已保存到。“bin”文件。下面是相同的截图。</p><figure class="ld le lf lg fd lh er es paragraph-image"><div role="button" tabindex="0" class="lq lr di ls bf lt"><div class="er es lu"><img src="../Images/9e49a5c2174be1e519129de28237258a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KaLkuM3sVf1y1fQTxhYmJA.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated"><strong class="bd jn">微调语言模型</strong></figcaption></figure><p id="1b1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们现在将使用这种微调的语言模型和预训练的语言模型，上面有简单的神经网络层，并将比较结果。</p><h1 id="2c75" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">使用Bert模型进行分类</h1><p id="6c1e" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">我们的任务是序列分类，我们将使用<a class="ae jk" href="https://huggingface.co/pytorch-transformers/model_doc/bert.html#bertforsequenceclassification" rel="noopener ugc nofollow" target="_blank">这个</a>来达到我们的目的。这要求我们的句子分别以'<strong class="ih hj"> cls' </strong>和'<strong class="ih hj"> sep' </strong>开始和结束。</p><p id="c2c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将首先以这种格式重新格式化我们的评论或文本栏。这样做是因为Bert在下一句预测任务中使用了<strong class="ih hj">‘sep’</strong>。然后，我们将使用<strong class="ih hj">bertokenizer</strong>来标记Bert格式的文本数据。对于给定的标记或单词，如果在Bert的词汇表中找到该标记，tokenizer将保持该单词不变，否则它将在Bert的词汇表中找到小的子单词。我们现在将根据Bert的词汇表将令牌转换为单词id。我们还需要填充序列，使它们具有固定的长度。对于给定的标记化文本，我们还需要区分它是标记还是填充的一部分。这是使用<strong class="ih hj">警示面具完成的。</strong></p><p id="76ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将把我们的训练数据分成80:20。我们将在80%的数据上训练我们的模型，并在20%的数据上进行测试。Rest只是简单地按照一个常规的Pytorch方法制作数据加载器并训练模型。所有这些都已经在知识库中做了大量的解释。通过使用预先训练的模型，我们在我们的测试数据上获得了接近90.7的准确度。对于本教程的最后一步，我们使用了微调的语言模型，并保持所有其他可控参数不变，如学习速率、时期等。在这种情况下，在测试数据上观察到的准确度是90.9。</p><p id="80a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这在精确度上是一个微不足道的改进。我们可以看到，有了语言模型，分数提高了0.15%。看，我们的数据是用英语写的评论，这与训练Bert的任务非常相似。如果您的数据与Bert预先训练的数据相对不同，您会看到一个显著的结果。例如，如果客户和客户之间有大量使用他们母语的数据，那么这种方法可能会更好。</p><h1 id="b545" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">包扎</h1><p id="9d3b" class="pw-post-body-paragraph if ig hi ih b ii kj ik il im kk io ip iq kl is it iu km iw ix iy kn ja jb jc hb bi translated">我们看到了如何微调我们的语言模型。我们还看到了如何使用PyTorch-transformers来使用Bert进行序列分类。我们还看到了如何使用微调语言模型进行序列分类。</p><h2 id="ee8c" class="ko jm hi bd jn kp kq kr jr ks kt ku jv iq kv kw jz iu kx ky kd iy kz la kh lb bi translated">Github代码链接</h2><div class="lv lw ez fb lx ly"><a href="https://github.com/Shivampanwar/Bert-text-classification/blob/master/bert_language_model_with_sequence_classification.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hj fi z dy md ea eb me ed ef hh bi translated">shivampanwar/Bert-文本-分类</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">github.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm li ly"/></div></div></a></div><h1 id="17da" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">参考</h1><div class="lv lw ez fb lx ly"><a href="https://mccormickml.com/2019/07/22/BERT-fine-tuning/" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hj fi z dy md ea eb me ed ef hh bi translated">使用PyTorch的BERT微调教程</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">在本教程中，我将向你展示如何使用BERT和huggingface PyTorch库来快速有效地…</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">mccormickml.com</p></div></div></div></a></div><div class="lv lw ez fb lx ly"><a href="https://github.com/google-research/bert" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hj fi z dy md ea eb me ed ef hh bi translated">谷歌研究/bert</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">这是几个新模型的发布，是预处理代码改进的结果。在…</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">github.com</p></div></div><div class="mh l"><div class="mn l mj mk ml mh mm li ly"/></div></div></a></div></div></div>    
</body>
</html>