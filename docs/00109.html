<html>
<head>
<title>A Guide to using Logistic Regression for Digit Recognition (with Python codes)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用逻辑回归进行数字识别的指南(带Python代码)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-guide-to-using-logistic-regression-for-digit-recognition-with-python-codes-86aae6da10fe?source=collection_archive---------0-----------------------#2018-09-20">https://medium.com/analytics-vidhya/a-guide-to-using-logistic-regression-for-digit-recognition-with-python-codes-86aae6da10fe?source=collection_archive---------0-----------------------#2018-09-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div class="ab fe cl ij"><img src="../Images/1283a049fb86c4fb4ec9b43ec1109f09.png" data-original-src="https://miro.medium.com/v2/format:webp/1*FGGge_GilZ_KJYaoryaxkA.png"/></div></figure><p id="73cf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">任何有抱负的数据科学家遇到的第一种分类技术通常是逻辑回归。事实上，尽管出现了像随机森林这样更强大的方法，许多银行服务仍然在使用它。然而，考虑到逻辑回归是如此简单易懂，这并不奇怪。</p><p id="420f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">但是你能想象用它来完成计算机视觉任务吗？</p><p id="b15a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在本帖中，我们将学习如何使用一种称为一对一分类的技术，使用像逻辑回归这样的简单算法来识别手写数字(0-9)。在此过程中，我们还将了解矢量化及其优势。</p><p id="eebf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="jk">这篇博文的灵感来源于吴恩达的机器学习课程习题集3(数据集可以在这里</em><a class="ae jl" href="https://s3.amazonaws.com/spark-public/ml/exercises/on-demand/machine-learning-ex3.zip" rel="noopener ugc nofollow" target="_blank"><em class="jk"/></a><em class="jk">)获得。</em></p><p id="dc2a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果你是这个领域的新手，一定要看看我下面的帖子:</p><ul class=""><li id="be68" class="jm jn hi io b ip iq it iu ix jo jb jp jf jq jj jr js jt ju bi translated"><a class="ae jl" rel="noopener" href="/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-1-6b8dd1c73d80">一元和多元线性回归</a>(第一部分)</li><li id="2844" class="jm jn hi io b ip jv it jw ix jx jb jy jf jz jj jr js jt ju bi translated"><a class="ae jl" rel="noopener" href="/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-2-1-1a666f049ad6">逻辑回归或分类</a>(第2.1部分)</li><li id="9e89" class="jm jn hi io b ip jv it jw ix jx jb jy jf jz jj jr js jt ju bi translated"><a class="ae jl" rel="noopener" href="/analytics-vidhya/python-implementation-of-andrew-ngs-machine-learning-course-part-2-2-dceff1a12a12">正则化逻辑回归</a>(第2.2部分)</li></ul><h1 id="a68b" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak">了解一对多分类</strong></h1><p id="5ca0" class="pw-post-body-paragraph im in hi io b ip ky ir is it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj hb bi translated">如果在一个数据集中有K个不同的类，我们将首先建立一个模型，其中我们将属于一个类的数据视为正的，而将所有其他类视为负的。接下来，我们将构建另一个模型，假设属于某个其他类的数据为正，其余的为负。我们将不断重复这个过程，直到我们建立K个不同的模型。</p><p id="8fe3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们借助一个例子来更好地理解这一点。在下图中，我们有属于3个不同类别的数据。因此，我们将构建3个不同的模型，将一类特定数据视为正值，将其余数据视为负值。</p><p id="844d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一般来说，如果数据集中有K个类，我们需要建立K个不同的模型。</p><figure class="le lf lg lh fd ii er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ld"><img src="../Images/e3527ed16cf05472afc2c4483738c968.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U4A7gakHXGqaWwbGxP-Dlw.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">借用吴恩达机器学习课程(Coursera)</figcaption></figure><h1 id="e6ac" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak">使用逻辑回归的一对一对比</strong></h1><p id="d4b1" class="pw-post-body-paragraph im in hi io b ip ky ir is it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj hb bi translated">数据集由从0到9的数字组成，所以这里有10个不同的类。如上所述，我们将通过训练10个不同的逻辑回归分类器来利用一对一分类技术。</p><p id="0665" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">首先，让我们加载必要的库。</p><pre class="le lf lg lh fd lq lr ls lt aw lu bi"><span id="6985" class="lv kb hi lr b fi lw lx l ly lz">from scipy.io import loadmat<br/>import numpy as np<br/>import scipy.optimize as opt<br/>import matplotlib.pyplot as plt</span></pre><p id="0e71" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">读取数据</strong></p><pre class="le lf lg lh fd lq lr ls lt aw lu bi"><span id="9546" class="lv kb hi lr b fi lw lx l ly lz">data = loadmat('ex3data1.mat')<br/>X = data['X']<br/>y = data['y']</span></pre><p id="1f71" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">数据集有5000个训练样本，每个样本是一个20×20像素的灰度图像，展开成一个400维的向量，从而形成一个5000×400的矩阵<code class="du ma mb mc lr b">X</code>。还要注意，在标记向量<code class="du ma mb mc lr b">y</code>中，数字<code class="du ma mb mc lr b">0</code>被标记为<code class="du ma mb mc lr b">10</code>，而数字1-9被标记为1-9。</p><p id="22ba" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">可视化数据</strong></p><pre class="le lf lg lh fd lq lr ls lt aw lu bi"><span id="fdd5" class="lv kb hi lr b fi lw lx l ly lz">_, axarr = plt.subplots(10,10,figsize=(10,10))<br/>for i in range(10):<br/>    for j in range(10):<br/>       axarr[i,j].imshow(X[np.random.randint(X.shape[0])].\<br/>reshape((20,20), order = 'F'))          <br/>       axarr[i,j].axis('off')     <br/></span></pre><figure class="le lf lg lh fd ii er es paragraph-image"><div class="er es md"><img src="../Images/2bb0b31d1045f9f258912f9718057d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*WJdmUan0j7Hb1dbvaN6ixA.png"/></div></figure><p id="6521" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">增加截距项</strong></p><pre class="le lf lg lh fd lq lr ls lt aw lu bi"><span id="4795" class="lv kb hi lr b fi lw lx l ly lz">m = len(y)<br/>ones = np.ones((m,1))<br/>X = np.hstack((ones, X)) #add the intercept<br/>(m,n) = X.shape</span></pre><p id="8220" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">矢量化</strong></p><p id="edcd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">根据吴恩达的说法，“向量化是一种消除代码中显式for循环的艺术”。作为数据科学家，我们处理大量数据。在处理如此庞大的数据时使用for循环是非常低效的。因此，我们利用了矢量化技术，避免了for循环的使用，并且提高了计算的效率和速度。</p><p id="1c7d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">例如，让我们考虑两个一维数组- <code class="du ma mb mc lr b">a</code>和<code class="du ma mb mc lr b">b</code>，每个都有一百万个元素。为了演示矢量化与for循环的运行速度，我们对两个数组执行元素乘法，并对结果数组中的元素求和，然后比较时间差。</p><pre class="le lf lg lh fd lq lr ls lt aw lu bi"><span id="7c39" class="lv kb hi lr b fi lw lx l ly lz">import numpy as np<br/>import time</span><span id="d132" class="lv kb hi lr b fi me lx l ly lz">a = np.random.rand(1000000)<br/>b = np.random.rand(1000000)</span><span id="11ca" class="lv kb hi lr b fi me lx l ly lz">c = 0<br/>tic = time.time()<br/>for i in range(1000000):<br/>  c += a[i] * b[i]<br/>toc = time.time()<br/>print("value of c {0:.5f}".format(c))<br/>print("time taken using for-loop " + str(1000*(toc-tic)) + " ms")</span><span id="9d81" class="lv kb hi lr b fi me lx l ly lz">c = 0<br/>tic = time.time()<br/>c = np.dot(a,b) # no for-loops in vectorized version<br/>toc = time.time()<br/>print("value of c {0:.5f}".format(c))<br/>print("time taken using vectorized operation " + str(1000*(toc-tic)) + " ms")</span></pre><ul class=""><li id="c7aa" class="jm jn hi io b ip iq it iu ix jo jb jp jf jq jj jr js jt ju bi translated">c的值<code class="du ma mb mc lr b">249740.84172</code></li><li id="193d" class="jm jn hi io b ip jv it jw ix jx jb jy jf jz jj jr js jt ju bi translated">使用for循环花费的时间<code class="du ma mb mc lr b">431.77247047424316</code>毫秒</li><li id="b8cf" class="jm jn hi io b ip jv it jw ix jx jb jy jf jz jj jr js jt ju bi translated">c的值<code class="du ma mb mc lr b">249740.84172</code></li><li id="d95f" class="jm jn hi io b ip jv it jw ix jx jb jy jf jz jj jr js jt ju bi translated">使用矢量化操作花费的时间<code class="du ma mb mc lr b">1.9989013671875</code>毫秒</li></ul><p id="a832" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从上面的输出可以看出，在这种情况下，矢量化版本比for循环快200倍。</p><h1 id="6423" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak">矢量化逻辑回归</strong></h1><p id="58c6" class="pw-post-body-paragraph im in hi io b ip ky ir is it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj hb bi translated">使用逻辑回归的矢量化版本比使用for-loops更有效，尤其是当数据量很大时。在本练习中，我们将通过实现矢量化逻辑回归来避免使用for循环。</p><p id="f32a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因为我们知道逻辑回归使用sigmoid函数，我们将首先实现它:</p><pre class="le lf lg lh fd lq lr ls lt aw lu bi"><span id="8bc1" class="lv kb hi lr b fi lw lx l ly lz">def sigmoid(z):<br/>    return 1/(1+np.exp(-z))</span></pre><p id="9f05" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">矢量化成本函数；</p><pre class="le lf lg lh fd lq lr ls lt aw lu bi"><span id="ec9f" class="lv kb hi lr b fi lw lx l ly lz">def costFunctionReg(theta, X, y, lmbda):<br/>    m = len(y)<br/>    temp1 = np.multiply(y, np.log(sigmoid(np.dot(X, theta))))<br/>    temp2 = np.multiply(1-y, np.log(1-sigmoid(np.dot(X, theta))))<br/>    return np.sum(temp1 + temp2) / (-m) + np.sum(theta[1:]**2) * lmbda / (2*m)</span></pre><p id="2479" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">矢量化渐变:</p><pre class="le lf lg lh fd lq lr ls lt aw lu bi"><span id="ba30" class="lv kb hi lr b fi lw lx l ly lz">def gradRegularization(theta, X, y, lmbda):<br/>    m = len(y)<br/>    temp = sigmoid(np.dot(X, theta)) - y<br/>    temp = np.dot(temp.T, X).T / m + theta * lmbda / m<br/>    temp[0] = temp[0] - theta[0] * lmbda / m<br/>    return temp</span></pre><p id="3a57" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从上面可以看出，我们避免了使用for循环，还增加了正则项来处理过度拟合。</p><p id="0180" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">优化参数</strong></p><p id="4d10" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里，我们将利用来自<code class="du ma mb mc lr b">scipy</code>库的一个名为<code class="du ma mb mc lr b">fmin_cg</code>的高级数值优化库函数来找到我们的参数的最佳值。</p><pre class="le lf lg lh fd lq lr ls lt aw lu bi"><span id="a2c5" class="lv kb hi lr b fi lw lx l ly lz">lmbda = 0.1<br/>k = 10<br/>theta = np.zeros((k,n)) #inital parameters</span><span id="5c0d" class="lv kb hi lr b fi me lx l ly lz">for i in range(k):<br/>    digit_class = i if i else 10<br/>    theta[i] = opt.fmin_cg(f = costFunctionReg, x0 = theta[i],  fprime = gradRegularization, args = (X, (y == digit_class).flatten(), lmbda), maxiter = 50)</span></pre><p id="24f1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">由于我们有10个不同的模型，我们需要通过使用for循环找到每个模型的最佳参数。</p><h1 id="1851" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak">使用一对一技术进行预测</strong></h1><p id="b2ec" class="pw-post-body-paragraph im in hi io b ip ky ir is it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj hb bi translated">在训练了一对一分类器之后，我们现在可以用它来预测给定图像中包含的数字。对于每个输入，您应该使用经过训练的逻辑回归分类器来计算它属于每个类的“概率”。我们将选择相应的逻辑回归分类器输出最高概率的类，并返回类标签(1、2、…、或K)作为输入示例的预测。然后，我们使用返回的预测向量来确定模型的准确性。</p><pre class="le lf lg lh fd lq lr ls lt aw lu bi"><span id="6789" class="lv kb hi lr b fi lw lx l ly lz">pred = np.argmax(X @ theta.T, axis = 1)<br/>pred = [e if e else 10 for e in pred]<br/>np.mean(pred == y.flatten()) * 100</span></pre><p id="7401" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这应该给我们一个<code class="du ma mb mc lr b">95.08%</code>的精度。印象深刻！我们的模型在预测数字方面做得非常好。</p><h1 id="c9e3" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">结束注释</h1><p id="3622" class="pw-post-body-paragraph im in hi io b ip ky ir is it kz iv iw ix la iz ja jb lb jd je jf lc jh ji jj hb bi translated">我们已经了解了如何使用逻辑回归等简单算法来执行数字识别等复杂任务，并在此过程中了解了一对一技术和矢量化。</p><p id="ff50" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">谢谢你能走到这一步。如果你喜欢我的作品，给我一个(或几个)掌声。在下一篇文章中，我们将学习神经网络。敬请期待！</p></div></div>    
</body>
</html>