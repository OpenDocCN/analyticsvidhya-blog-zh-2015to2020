# 我应该用什么来验证分类模型？下面介绍几种方法！

> 原文：<https://medium.com/analytics-vidhya/quick-model-validation-metrics-classification-e6e63fbf9aec?source=collection_archive---------44----------------------->

![](img/2a8b0d814f7fa741e6a707b63bec79ab.png)

Daria Nepriakhina 在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

**精度** —被归类为真的数据点的比例实际上是真的。也可以理解为“预测项有多少是相关的”。

**TP/(TP+FP)。**

**特异性** —实际为假的预测假值的比例。这也叫做**【TNR】****【TN/(TN+FP)**

**准确性** —这是预测为正确值的值的比例。这不一定是确定模型预测能力的最佳方法，因为它取决于目标变量中真值和假值的平衡。如果正面数量多于负面数量，模型将预测更高的正面数量，反之亦然。反过来，这将增加模型精度，而这不适合捕捉模型的能力。

**回忆/TPR/敏感度** —模型在分类中找到正确数据点的能力。也可以理解为“选择多少个相关项”。

**TP/(TP+FN)**

模型找出感兴趣的数据点的能力。如果我们提高精确度，我们就会降低回忆。需要在它们之间进行权衡。如果你的模型有一个 1 的回忆，这是没有必要的，因为在某些情况下，它会识别实际上是假的真实值，这对于用例是不好的。

**F1——得分** —这是精确度和召回率的混合，计算为两个值的调和平均值。以下公式适用— **F1 = 2*(精度*召回)/(精度+召回)**

在这种情况下，数值中的任何异常都将导致 F1 值降低，这将表明模型存在问题。一个平衡的模型会有更高的 F1 值。

**混淆矩阵** —它显示矩阵表示中的真阳性、假阳性、真阴性和假阴性的数量，实际值与预测的标签相对比。这用于计算多个指标，如准确度、精确度、召回率(TPR)和特异性(TNR)。

**ROC —接收器工作特性**这个想法是为了显示当我们改变阈值时精度关系的变化&。阈值是一个值，用于判断预测是真还是假。ROC 曲线可用于找出精确度和召回率之间的适当平衡。ROC 曲线的轴线是 y 轴上的**TPR**和 x 轴上的**FPR**。随着我们提高阈值，我们倾向于降低找到真正阳性的可能性。这里，

**TPR(召回/灵敏度)= TP / (TP+FN)**

**FPR = 1-特异性或 1-TNR = FP/(FP+TN)**

随着我们降低阈值，我们开始找到真正的阳性，在某个点上，我们可以确定阈值的正确值，该值在精确度和召回率之间提供了正确的平衡。当我们提高阈值时，我们会提高召回率，因为我们会识别更多的真值，但是，这意味着我们会降低精确度，因为我们可能会将假值识别为真值。为了量化，我们计算 **AUC，曲线下面积，**，其基本上给出了关于模型在识别真/假方面有多好的信息。**较高的值通常意味着好的模型。**然而，一个真正高的值~ 0.9–1 也可能意味着存在潜在的过度拟合，并且需要重新检查该过程以发现任何问题。