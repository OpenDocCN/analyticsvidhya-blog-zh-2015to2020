<html>
<head>
<title>K-Nearest Neighbors Algorithm (KNN) for beginners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">适用于初学者的k-最近邻算法(KNN)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-nearest-neighbors-algorithm-knn-for-beginners-d289a28f0d5?source=collection_archive---------13-----------------------#2020-10-06">https://medium.com/analytics-vidhya/k-nearest-neighbors-algorithm-knn-for-beginners-d289a28f0d5?source=collection_archive---------13-----------------------#2020-10-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="eb0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文的目标读者是对机器学习不熟悉，并且有兴趣学习更多关于ML算法的知识的人。因为K-最近邻是许多其他机器学习算法中最简单的，所以从这里开始是一个相当好的决定:)</p><p id="47dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">KNN是一种<strong class="ih hj"><em class="jd"/></strong><strong class="ih hj"><em class="jd"/></strong>监督式机器学习算法，易于实现，用于解决既有<strong class="ih hj"><em class="jd"/></strong>分类又有<strong class="ih hj"><em class="jd"/></strong>回归的问题。</p><h2 id="067a" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">让我们潜得更深！</h2><p id="d6d9" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">KNN的基本事实简单如下:</p><blockquote class="ke"><p id="cdd1" class="kf kg hi bd kh ki kj kk kl km kn jc dx translated">相似的输入有相似的输出。</p></blockquote><figure class="kp kq kr ks kt ku er es paragraph-image"><div class="er es ko"><img src="../Images/dde6e9ce623ca577e0625e61d142b024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/0*zSfW2u1ng--ZgbqZ.png"/></div></figure><p id="0881" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">KNN将相似输入定义为那些<strong class="ih hj">彼此靠近(具有最小距离)</strong>的物体。因为它们彼此接近，所以它们具有相同的类/标签/输出。上面的图片表示了相同的场景，即彼此靠近的对象具有相同的颜色(标签)。</p><h1 id="f3ee" class="kx jf hi bd jg ky kz la jk lb lc ld jo le lf lg jr lh li lj ju lk ll lm jx ln bi translated"><strong class="ak">测距</strong></h1><figure class="lp lq lr ls fd ku er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es lo"><img src="../Images/e12bf72d94795be0d44201bee2c1de3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*79H-TgJ_7ViQ-gnB"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">托马斯·哈布尔在<a class="ae mb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="bfb8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在问题来了，我们如何找到物体之间的距离？有很多方法可以找到它，你可能对其中一些很熟悉。</p><p id="37ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用于找出任意给定两点<strong class="ih hj"> <em class="jd"> p </em> </strong>和<strong class="ih hj"> <em class="jd"> q </em> </strong>之间的距离的通用形式如下，并且也称为<strong class="ih hj"> <em class="jd">【闵可夫斯基距离】</em> </strong>:</p><figure class="lp lq lr ls fd ku er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mc"><img src="../Images/48893e66d86bf0916db87837a85e4059.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1x9j2HyfG8bumfR7rZz4GA.png"/></div></div></figure><ul class=""><li id="16b6" class="md me hi ih b ii ij im in iq mf iu mg iy mh jc mi mj mk ml bi translated"><strong class="ih hj">欧几里德距离(a = 2 ) </strong></li></ul><p id="5619" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是最基本和最短的距离查找技术，定义如下:</p><figure class="lp lq lr ls fd ku er es paragraph-image"><div class="er es mm"><img src="../Images/da960080e0d2b598417f929d923ca451.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*9SgsYFEV7l1s1xF2RA2aBA.png"/></div></figure><p id="0917" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它对大的差异高度敏感。因为它对具有较大差异的距离求平方，并对计算产生巨大影响。</p><ul class=""><li id="a664" class="md me hi ih b ii ij im in iq mf iu mg iy mh jc mi mj mk ml bi translated"><strong class="ih hj">曼哈顿距离(a = 1 ) </strong></li></ul><p id="e59b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它也被称为<em class="jd">城市街区、出租车距离、蛇形距离或L1范数</em>，它计算点坐标之间的绝对差之和。</p><figure class="lp lq lr ls fd ku er es paragraph-image"><div class="er es mn"><img src="../Images/ad6fee7ec723a0ff50fe2c078b5dc4ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*5w37ddEeM2pVD54g-L-ocA.png"/></div></figure><ul class=""><li id="63d8" class="md me hi ih b ii ij im in iq mf iu mg iy mh jc mi mj mk ml bi translated"><strong class="ih hj">切比雪夫距离(a =无穷大)</strong></li></ul><p id="7d56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于切比雪夫距离，两点之间的距离是它们沿任意坐标维度的最大差值。</p><figure class="lp lq lr ls fd ku er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es mo"><img src="../Images/0d13a46c8c5dfdb2f7481991ffcbe045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*GCxST7T8jVEhE9vYoNFz8w.png"/></div></div></figure><p id="2217" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由此得出的一个重要结论是，随着<strong class="ih hj">‘a’</strong>值的增加，坐标间的最大距离开始变大，小距离特征被忽略。对于切比雪夫，具有最大距离的坐标将被幂至无穷大，并且将主要用于距离计算。</p><h1 id="09f1" class="kx jf hi bd jg ky kz la jk lb lc ld jo le lf lg jr lh li lj ju lk ll lm jx ln bi translated"><strong class="ak"> KNN算法和复杂性分析</strong></h1><p id="9ead" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">KNN算法遵循以下步骤:</p><ul class=""><li id="b01d" class="md me hi ih b ii ij im in iq mf iu mg iy mh jc mi mj mk ml bi translated">获取训练样本数据集D= {(x1，y1)，(x2，y2)，(x3，y3)，…，(xn，yn)}，以及要预测其标签的测试数据样本(x，y)。假设<em class="jd"> x </em>为<em class="jd"> m </em>维，数据集中的点的总数为<em class="jd"> n. </em></li><li id="6276" class="md me hi ih b ii mp im mq iq mr iu ms iy mt jc mi mj mk ml bi translated">为<strong class="ih hj"> k </strong>选择一个合适的值(将进一步讨论)</li><li id="9da4" class="md me hi ih b ii mp im mq iq mr iu ms iy mt jc mi mj mk ml bi translated">对于给定的测试数据，计算数据集的每个样本点的距离，并将距离和点的标签一起存储在集合中。</li></ul><blockquote class="ke"><p id="535e" class="kf kg hi bd kh ki mu mv mw mx my jc dx translated">c<!-- -->complexity:O(Mn)，因为我们遍历所有数据点，并且对于每个点将花费m时间(在计算距离时)。</p></blockquote><ul class=""><li id="7180" class="md me hi ih b ii mz im na iq nb iu nc iy nd jc mi mj mk ml bi translated">根据距离度量对集合进行升序排序，并从集合中选取前K个值。</li></ul><blockquote class="ke"><p id="81da" class="kf kg hi bd kh ki mu mv mw mx my jc dx translated">使用快速选择算法找到第k个最小距离，然后返回不大于第k个最小距离的所有距离。O(n)</p></blockquote><ul class=""><li id="8525" class="md me hi ih b ii mz im na iq nb iu nc iy nd jc mi mj mk ml bi translated">从K值中选择最常出现的标签，并将其分配给测试数据。这将是我们的输出标签。<em class="jd"> O(k) </em></li></ul><p id="06a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:在回归的情况下，最后一步取标签的平均值(它们可能是一些连续的值)。</p><blockquote class="ke"><p id="9d58" class="kf kg hi bd kh ki kj kk kl km kn jc dx translated">时间复杂度:O(mn + n + k) = O(mn)</p><p id="6892" class="kf kg hi bd kh ki kj kk kl km kn jc dx translated">空间复杂度:O(mn)用于存储m维的n个数据点。</p></blockquote><h1 id="c236" class="kx jf hi bd jg ky kz la jk lb lc ld jo le ne lg jr lh nf lj ju lk ng lm jx ln bi translated"><strong class="ak">如何选择合适的K值？</strong></h1><figure class="lp lq lr ls fd ku er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nh"><img src="../Images/97b3d82f2995d487a2e6ecca3cdd9655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EyxyB5KNcf6Ij2SN"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">由<a class="ae mb" href="https://unsplash.com/@kobuagency?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> KOBU机构</a>在<a class="ae mb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h1 id="6de8" class="kx jf hi bd jg ky kz la jk lb lc ld jo le lf lg jr lh li lj ju lk ll lm jx ln bi translated"><strong class="ak"> K=1(非常小的值)</strong></h1><p id="c750" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">假设我们从1开始取k的值。这通常不是一个好的选择。因为它会使数据<strong class="ih hj"> <em class="jd">对噪声</em> </strong>高度敏感，会导致<strong class="ih hj"><em class="jd"/></strong>过拟合，分类器会<strong class="ih hj"> <em class="jd">过于具体而不一般化。</em> </strong>通过使用K=1，将训练集作为测试集输入到1-NN分类器，每个点将最接近自己，并将输出自己的标签。因此，在训练数据上，1-NN将以最大的准确性表现出色，但在测试数据(看不见的数据)上，它不会做出正确的预测。最终，在看不见的数据上，我们的错误率将是最高的，分类器被称为过度拟合。</p><h1 id="d9d7" class="kx jf hi bd jg ky kz la jk lb lc ld jo le lf lg jr lh li lj ju lk ll lm jx ln bi translated"><strong class="ak"> K = 100(非常大的值)</strong></h1><p id="b052" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">这会使模型<strong class="ih hj"> <em class="jd">过于泛化</em> </strong>，以<strong class="ih hj"> <em class="jd">偏高</em> </strong>和<strong class="ih hj"> <em class="jd">欠拟合。</em> </strong>测试和训练数据的性能都不好。</p><h1 id="1efc" class="kx jf hi bd jg ky kz la jk lb lc ld jo le lf lg jr lh li lj ju lk ll lm jx ln bi translated"><strong class="ak"> K = n(等于训练数据的大小)</strong></h1><p id="7697" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">在选择k的值时，这也不是一个好的选择。因为这种类型的分类器将总是输出我们数据集中最大类的标签。或者我们可以说最大类总是从我们的数据集中胜出！</p><p id="a931" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">那么该怎么办呢？？</strong></p><h1 id="edc6" class="kx jf hi bd jg ky kz la jk lb lc ld jo le lf lg jr lh li lj ju lk ll lm jx ln bi translated"><strong class="ak">调整超参数K </strong></h1><p id="70ab" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">使用这种技术，我们应该比较k的不同值，并选择误差最小的k。对于这种方法，对于不同的k值，考虑以下步骤:</p><ol class=""><li id="4a05" class="md me hi ih b ii ij im in iq mf iu mg iy mh jc ni mj mk ml bi translated">对于m倍迭代，运行以下算法。如下图m = 10，运行这些步骤十次。</li></ol><p id="088e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.1使用适当的比率将数据集分为训练数据和验证数据。</p><p id="dd74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.2在验证数据上测试分类器(test fold)，计算误差并保存。对剩余的折叠迭代重复步骤1.1。</p><p id="2043" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.用k值和所有记录分数的平均值绘制误差图。对下一个k值重复步骤1。</p><p id="c690" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选择误差最小的<strong class="ih hj"> <em class="jd"> k </em> </strong>，如下图所示。</p><figure class="lp lq lr ls fd ku er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nj"><img src="../Images/b784847c5dd129a3aba01fe01c7adda5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o34VZmOcBhNxP-pwVp3AbQ.png"/></div></div></figure><figure class="lp lq lr ls fd ku er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es nj"><img src="../Images/341d28e635abf74ff2849f987d6808db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RDga4E10FAW35y_OsHF_8Q.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">x轴显示k值，y轴显示误差</figcaption></figure><p id="c38b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">误差最小的‘k’就是我们想要的k值，随着k值的增加，误差率并没有急剧下降。在k = 3之后，我们没有发现我们的分类器有任何显著的改进。因此，这将是k的最优值。</p><p id="27e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不建议选择K 的偶数值，因为在从K个例子中挑选频繁标签时会产生平局。例如，如果我们选择<strong class="ih hj"> <em class="jd"> k=4 </em> </strong>，并假设有两个示例带有“+”标签，其余两个带有“—”标签。我们将如何获得最频繁的标签？看，这导致了平局！一些专家建议，在这种情况下，建议将k值减少1(使k为奇数)，并重新运行实验以获得结果。</p></div><div class="ab cl nk nl gp nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="hb hc hd he hf"><h1 id="b54e" class="kx jf hi bd jg ky nr la jk lb ns ld jo le nt lg jr lh nu lj ju lk nv lm jx ln bi translated">优势</h1><ol class=""><li id="e383" class="md me hi ih b ii jz im ka iq nw iu nx iy ny jc ni mj mk ml bi translated">KNN-是一种<strong class="ih hj"><em class="jd"/></strong><strong class="ih hj"><em class="jd"/></strong>非参数算法。</li></ol><ul class=""><li id="7e22" class="md me hi ih b ii ij im in iq mf iu mg iy mh jc mi mj mk ml bi translated">超参数“k”的调整由我们手动完成，它有助于学习或预测过程。与线性回归等其他算法不同，它不需要学习参数，也不需要在学习参数的基础上进行预测。</li><li id="48d6" class="md me hi ih b ii mp im mq iq mr iu ms iy mt jc mi mj mk ml bi translated">它不对基础分布做任何假设，也不试图估计它。</li><li id="598b" class="md me hi ih b ii mp im mq iq mr iu ms iy mt jc mi mj mk ml bi translated">KNN可用于监督机器学习算法类别下的分类和回归问题。</li></ul><p id="fadb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.K-NN是一种基于<strong class="ih hj"> <em class="jd">实例的</em> </strong>学习算法。</p><ul class=""><li id="d08c" class="md me hi ih b ii ij im in iq mf iu mg iy mh jc mi mj mk ml bi translated">K-NN不是执行显式概括，而是通过将新问题实例与训练实例进行比较来形成假设。</li><li id="e82b" class="md me hi ih b ii mp im mq iq mr iu ms iy mt jc mi mj mk ml bi translated">可以轻松适应看不见的数据。</li></ul><h1 id="e3c6" class="kx jf hi bd jg ky kz la jk lb lc ld jo le lf lg jr lh li lj ju lk ll lm jx ln bi translated">不足之处</h1><p id="4f28" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">除了一些优点之外，这种易于实现的算法也有一些缺点。一些定义如下:</p><ol class=""><li id="711d" class="md me hi ih b ii ij im in iq mf iu mg iy mh jc ni mj mk ml bi translated">K-NN是一种<strong class="ih hj"> <em class="jd">懒学习</em> </strong>算法。</li></ol><ul class=""><li id="5089" class="md me hi ih b ii ij im in iq mf iu mg iy mh jc mi mj mk ml bi translated">因为K-NN不需要对训练数据进行任何显式训练。当对它进行查询时，它开始执行计算。在大数据的情况下，我们的分类器将花费大量的计算时间。</li></ul><p id="6c3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.对于<strong class="ih hj"> <em class="jd">高维问题</em> </strong>表现不佳。所以维度的诅咒也在这里。</p><p id="1e9b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.这个算法<strong class="ih hj"> <em class="jd">较慢</em> </strong>求值，需要存储整个训练数据。因此，也可能是<strong class="ih hj"> <em class="jd">计算量大的</em> </strong>。</p><h1 id="b3aa" class="kx jf hi bd jg ky kz la jk lb lc ld jo le lf lg jr lh li lj ju lk ll lm jx ln bi translated"><strong class="ak">参考文献</strong></h1><ul class=""><li id="9715" class="md me hi ih b ii jz im ka iq nw iu nx iy ny jc mi mj mk ml bi translated"><a class="ae mb" href="https://www.cs.cornell.edu/courses/cs4780/2018fa/lectures/lecturenote02_kNN.html" rel="noopener ugc nofollow" target="_blank">https://www . cs . Cornell . edu/courses/cs 4780/2018 fa/lectures/lecture note 02 _ KNN . html</a></li><li id="a947" class="md me hi ih b ii mp im mq iq mr iu ms iy mt jc mi mj mk ml bi translated"><a class="ae mb" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm</a></li><li id="7584" class="md me hi ih b ii mp im mq iq mr iu ms iy mt jc mi mj mk ml bi translated"><a class="ae mb" href="https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/difference-a-parameter-and-a-hyperparameter/</a></li></ul><p id="36ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这都是关于K-NN的，如果你发现任何改进或者想提出任何建议，请随时联系我，在emanijaz583@gmail.com。一个更好的理解和K-NN的实际例子将很快上传。希望这篇阅读会有所帮助，谢谢！:)</p></div></div>    
</body>
</html>