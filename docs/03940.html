<html>
<head>
<title>Computer vision Kaggle Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">计算机视觉难题</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/kinship-classification-using-faces-7cc81610079b?source=collection_archive---------13-----------------------#2020-02-26">https://medium.com/analytics-vidhya/kinship-classification-using-faces-7cc81610079b?source=collection_archive---------13-----------------------#2020-02-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="182f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">基于人脸的亲属分类</h1><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/310c9f4a00042f64cef78b5b942e8262.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/1*zuz1GGIfZ_TnH8-tAWgTMQ.gif"/></div></figure><p id="29a9" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">不幸的是，人类甚至很难仅凭他们的脸来预测两个人是否是血亲，但机器不再是这种情况。</p><p id="83cf" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">人脸图像的亲属分类是模式识别和计算机视觉中的一个新问题，它在现实世界中有许多潜在的应用，包括社交媒体分析和儿童收养。</p><h1 id="9b0e" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">1.关于这个问题</h1><p id="d684" class="pw-post-body-paragraph jl jm hi jn b jo kj jq jr js kk ju jv jw kl jy jz ka km kc kd ke kn kg kh ki hb bi translated">这是一个由东北大学微笑实验室通过Kaggle主办的比赛。数据由Families In the Wild (FIW)提供，这是最大、最全面的自动亲属识别图像数据库。<a class="ae ko" href="https://www.kaggle.com/c/recognizing-faces-in-the-wild/overview" rel="noopener ugc nofollow" target="_blank">点击此处</a>了解详情</p><blockquote class="kp kq kr"><p id="8192" class="jl jm ks jn b jo jp jq jr js jt ju jv kt jx jy jz ku kb kc kd kv kf kg kh ki hb bi translated"><strong class="jn hj">问题陈述</strong></p></blockquote><p id="11d1" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">给定图像对，更具体地说是人脸，任务是确定两个图像是否有任何血缘关系。所以这是一个分类问题。关系可以是父亲-女儿/儿子、母亲-女儿/儿子、祖父-孙子以及兄弟姐妹之间的关系。</p><p id="bd6b" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">注意，夫妻之间即使同属一个家庭，也没有血缘关系。</p><p id="dafe" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">标签:相关对为1或0</p><blockquote class="kp kq kr"><p id="c16e" class="jl jm ks jn b jo jp jq jr js jt ju jv kt jx jy jz ku kb kc kd kv kf kg kh ki hb bi translated"><strong class="jn hj">评估指标</strong></p></blockquote><p id="d634" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">选择的度量是预测和观察标记之间的AUC-ROC。</p><p id="6add" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">它告诉我们模型在多大程度上能够区分不同的类。AUC越高，模型预测0为0和1为1的能力越强。以此类推，AUC越高，该模型在区分患病和未患病患者方面就越好。</p><blockquote class="kp kq kr"><p id="8e6a" class="jl jm ks jn b jo jp jq jr js jt ju jv kt jx jy jz ku kb kc kd kv kf kg kh ki hb bi translated"><strong class="jn hj">利用深度学习解决这个问题:</strong></p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es kw"><img src="../Images/97d95933453d55c0f336d31f9be983f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uSXUN3dn1xKV8Q40MvfQEg.png"/></div></div></figure><p id="7956" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">因为我们需要分类图像对是否是血缘亲属，这涉及通过输入两个图像并学习预测输出的函数f来训练模型。在深度学习的帮助下，模型可以学习函数f，并将输入映射到期望的输出。</p><h1 id="44de" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">让我们开始吧…</h1><blockquote class="kp kq kr"><p id="e018" class="jl jm ks jn b jo jp jq jr js jt ju jv kt jx jy jz ku kb kc kd kv kf kg kh ki hb bi translated">E <strong class="jn hj">现有方法</strong></p></blockquote><p id="a351" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这个问题可以用很多方法解决，但事实是，数据越多，结果越好。这个特定问题的现有方法是使用数据生成器来解决的，该数据生成器可能不能确保使用所有可用的图像，即数据生成器简单地使用样本和选择来获取图像。这引起了数据重复的问题，并且可能无法确认使用所有可用的图像。这种方法的主要问题是，由于我们使用预先训练的权重，模型往往会过度拟合训练数据，因此无法在看不见的数据上表现得更好。</p><blockquote class="kp kq kr"><p id="17b5" class="jl jm ks jn b jo jp jq jr js jt ju jv kt jx jy jz ku kb kc kd kv kf kg kh ki hb bi translated">我的<strong class="jn hj">方法</strong></p></blockquote><p id="4799" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我的主要意图是使用每一个图像，使模型能够在看不见的地方表现得更好。生成带有标签(0或1)的列形式的影像路径数据框，并使用数据生成器将模型拟合到影像上。</p><h1 id="ff8c" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">2.电子设计自动化(Electronic Design Automation)</h1><p id="44c8" class="pw-post-body-paragraph jl jm hi jn b jo kj jq jr js kk ju jv jw kl jy jz ka km kc kd ke kn kg kh ki hb bi translated">首先，让我们阅读所有必要的文件</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="27cf" class="lg ig hi lc b fi lh li l lj lk"># Reading all neccessary files required</span><span id="86de" class="lg ig hi lc b fi ll li l lj lk">train_file_path='recognizing-faces-in-the-wild/train_relationships.csv'</span><span id="885d" class="lg ig hi lc b fi ll li l lj lk">train_folders_path='train/'</span><span id="caee" class="lg ig hi lc b fi ll li l lj lk">train_csv=pd.read_csv(train_file_path)<br/>train_csv.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lm"><img src="../Images/1d599d7759c4d2d5eee5a12082669274.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*huuYqlnxBiAa4esoeA1RwQ.png"/></div></figure><p id="2238" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里提供的数据只包含相关对，但我们的主要任务是生成非相关对。在此之前，我们先来看一下这个问题提供的数据。图像存储在'<em class="ks"> train_folders_path' </em>文件夹中，并在csv文件中给出一对一的关系。下图是所给数据中Id为F0002的一个系列的示例。</p><p id="3a51" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">F0002是主系列文件夹，具有ID为MID1、MID2和MID3的系列中的3个成员，后跟他们各自的多个图像。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es ln"><img src="../Images/41be0e400c3e37b121f91c18d8cb84a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y6GcLj9cpicLvmo_HfaCug.png"/></div></div></figure><p id="490d" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">总共有470个独特的家庭作为训练数据给出，它们也遵循如上所示的相同格式。我们需要根据“train_csv”文件中给出的关系获取图像。</p><p id="69a8" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们可以看到列P1和P2的格式是FamilyID/MemberID，我们需要根据给定的关系获取相应的图像。</p><p id="81a4" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">让我们试着将csv中成对给出的相应图像可视化</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lo"><img src="../Images/6f2c475ff800ff969148ff4c0f1ae307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*4Qk5orZ_FK9FGhRYjjaEJA.png"/></div></figure><h1 id="062f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">3.数据准备</h1><p id="ce75" class="pw-post-body-paragraph jl jm hi jn b jo kj jq jr js kk ju jv jw kl jy jz ka km kc kd ke kn kg kh ki hb bi translated"><strong class="jn hj">程序</strong></p><ol class=""><li id="8285" class="lp lq hi jn b jo jp js jt jw lr ka ls ke lt ki lu lv lw lx bi translated">生成不相关的对。</li><li id="429b" class="lp lq hi jn b jo ly js lz jw ma ka mb ke mc ki lu lv lw lx bi translated">对生成的对执行数据清理</li><li id="cfd7" class="lp lq hi jn b jo ly js lz jw ma ka mb ke mc ki lu lv lw lx bi translated">获取每个图像对路径，并将其存储在数据框中。</li></ol><p id="afc0" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">正如我们从上图中看到的，只有相关对，我们还需要生成不相关对。我们可以通过从csv中生成所有可能的配对组合并从中删除相关配对来生成不相关的配对。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="c647" class="lg ig hi lc b fi lh li l lj lk">non_relation=list(itertools.product(relationships.p1.values, relationships.p2.values))</span><span id="277d" class="lg ig hi lc b fi ll li l lj lk"><strong class="lc hj">O/p <br/></strong>(’NUMBER OF TUPLES CONTAINING ALL POSSIBLE COMBINATION OF PAIRS except validation pairs:’,  2138402)</span></pre><p id="d37e" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">上面的代码生成所有可能的对组合作为元组列表，这不是我们想要的，但是从中删除所有相关的对将导致不相关的图像对。但是上面的方法有一个问题，会导致巨大的数据不平衡，因为我们生成了每个可能的组合，不相关的对(标签0)比相关的对(标签1)多得多。我们需要以某种方式解决这个问题，使我们的预测更加一般化。</p><p id="81de" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">上面的代码以csv中给出的格式生成对，即FamilyID/MemberID，但是我们需要生成以图像路径为列的数据帧，以便将其作为模型的输入。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="4152" class="lg ig hi lc b fi lh li l lj lk">#storing each image path using following code.</span><span id="80fd" class="lg ig hi lc b fi ll li l lj lk">train_person_to_images_map=defaultdict(list) #  for train</span><span id="94b4" class="lg ig hi lc b fi ll li l lj lk">for x in train_images:<br/>    train_person_to_images_map[x.split('/')[-3]+'/'+x.split("/")               [-2]].append(x)</span></pre><p id="1c5e" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">根据给定的关系获取图像路径可以使用下面的代码</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="27b4" class="lg ig hi lc b fi lh li l lj lk"># FOR RELATION PAIRS<br/>a1=[]<br/>for i in train:<br/>   a=list(train_person_to_images_map[i[0]])<br/>   b=list(train_person_to_images_map[i[1]])<br/>   a1.append(list(itertools.product(a,b)))</span><span id="c504" class="lg ig hi lc b fi ll li l lj lk"># FOR NON-RELATION PAIRS<br/>a2=[]<br/>for i in non_relation:<br/>   a=list(train_person_to_images_map[i[0]])  <br/>   b=list(train_person_to_images_map[i[1]])<br/>   a2.append(list(zip(a,b)))</span></pre><blockquote class="kp kq kr"><p id="5168" class="jl jm ks jn b jo jp jq jr js jt ju jv kt jx jy jz ku kb kc kd kv kf kg kh ki hb bi translated"><strong class="jn hj">生成数据帧</strong></p></blockquote><p id="a7d7" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">下面的代码片段使列表变平，并生成适合输入我们的模型的数据框架。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="331d" class="lg ig hi lc b fi lh li l lj lk"># CREATING FLATTEN LIST OUT OF LISTS OF LISTS<br/># for label 1<br/>merged = list(itertools.chain(*a1))</span><span id="95fc" class="lg ig hi lc b fi ll li l lj lk"># for label 0<br/>merged1 = list(itertools.chain(*a2))</span><span id="87a4" class="lg ig hi lc b fi ll li l lj lk"># Generating dataframe<br/>frame=pd.DataFrame(merged)# Fore class 1 <br/>frame['class']=1</span><span id="0fce" class="lg ig hi lc b fi ll li l lj lk">frame1=pd.DataFrame(merged1)# Fore class 0<br/>frame['class']=0</span></pre><p id="8143" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们总共生成了4个数据帧，2个用于训练，2个用于两个类的交叉验证。</p><p id="30c1" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如上所述，标签0对比标签1对多得多，这可以从下面的图片中看到。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es md"><img src="../Images/4f74ddacedb42aff88ebeaaa9e0df232.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*-7ujlQE0ZvttOZJ_DhQYSw.png"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es me"><img src="../Images/d19e916032e70677115aff49568cff7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p1Ubou4cS3yrSlkOF7Doyw.png"/></div></div></figure><p id="4dbc" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">为了避免数据不平衡，我们对标签0对进行下采样。</p><blockquote class="kp kq kr"><p id="6277" class="jl jm ks jn b jo jp jq jr js jt ju jv kt jx jy jz ku kb kc kd kv kf kg kh ki hb bi translated"><strong class="jn hj">数据发生器</strong></p></blockquote><p id="67cd" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">因为我们需要根据数据帧中列出的路径提取图像，所以我们需要一个数据生成器来读取路径并处理图像，同时生成两个图像以及各自的类标签。</p><p id="9250" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">该生成器根据指定的批处理大小顺序获取图像。</p><p id="4d94" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">数据生成器的代码</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es mf"><img src="../Images/1c9dad5b4586b21e8c1fc756394a6f98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IloHVHB93zg-tsNL6FhaQg.png"/></div></div></figure><p id="71a6" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">在所有的数据准备之后，让我们使用深度学习来深入研究模型的实际训练。</p><h1 id="c426" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">4.模型</h1><p id="9e9a" class="pw-post-body-paragraph jl jm hi jn b jo kj jq jr js kk ju jv jw kl jy jz ka km kc kd ke kn kg kh ki hb bi translated">在这里，我在每个模型中广泛使用了<strong class="jn hj"> VggFace </strong>来使用预训练权重的力量。</p><p id="cf6a" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jn hj">什么是Vggface </strong></p><p id="d1c6" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">VGGFace是指一系列为人脸识别开发的模型，由牛津大学视觉几何小组(VGG)的成员在基准计算机视觉数据集上演示。</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="c84a" class="lg ig hi lc b fi lh li l lj lk">model = VGGFace(model=’…’)</span></pre><p id="6d9d" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><em class="ks"> keras-vggface </em>库提供了三个预训练的VGGModels，一个VGGFace1模型通过<em class="ks">model = ' vgg 16 '</em>(默认)，两个VGGFace2模型'<em class="ks"> resnet50 </em>'和'<em class="ks"> senet50 </em>'。</p><p id="fce4" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">ResNet-50是一个经过预训练的深度学习模型，用于<em class="ks">卷积神经网络(CNN，或conv net)</em>的图像分类，卷积神经网络是一种深度神经网络，最常用于分析视觉图像。ResNet-50有50层深，并在来自ImageNet数据库的1000个类别的一百万幅图像上进行训练。此外，该模型有超过2300万个可训练参数，这表明了一个更好的图像数据的深层架构。使用预训练模型是一种非常有效的方法，相比之下，如果您需要从头开始构建它，您需要收集大量的数据并自己训练它。当然，还有其他预先训练的深度模型可以使用，如AlexNet，GoogleNet或VGG19，但ResNet-50以出色的泛化性能和更少的识别任务错误率而闻名，因此是一个有用的工具。</p><p id="a75a" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jn hj">为什么要预训练权重(Vggface) </strong></p><p id="d14c" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">预训练模型是我们学习现有框架的一个很好的帮助来源。由于时间限制或计算限制，不可能总是从零开始构建模型，这就是预训练模型存在的原因！我们可以使用一个预训练的模型作为基准来改进现有的模型或者测试我们自己的模型。</p><p id="3ed4" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jn hj">Vgg face resnet 50的架构:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mg"><img src="../Images/feecc23c664738a92ca9bc9460bd547b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*pvAWLQXBaNzAPbKRfCaWcA.png"/></div></figure><p id="6af6" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jn hj">现在让我们看看实际的深度学习模型</strong></p><blockquote class="kp kq kr"><p id="04c6" class="jl jm ks jn b jo jp jq jr js jt ju jv kt jx jy jz ku kb kc kd kv kf kg kh ki hb bi translated"><strong class="jn hj">型号1 </strong></p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mg"><img src="../Images/d96af7526dc13622cafbfed12a6ff845.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*bjjxhYtfToL7OEx2w46nOw.png"/></div></figure><p id="c97b" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">该模型使用了Vggface的Resnet50 </p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="ec36" class="lg ig hi lc b fi lh li l lj lk">input1=Input(shape=(224,224,3))</span><span id="e4dd" class="lg ig hi lc b fi ll li l lj lk">input2=Input(shape=(224,224,3))</span><span id="1259" class="lg ig hi lc b fi ll li l lj lk">base_model=VGGFace(model='resnet50',include_top=False)  # Pretrained model  Vggface<br/>for x in base_model.layers[:-3]:<br/>   x.trainable = false</span><span id="c081" class="lg ig hi lc b fi ll li l lj lk">x1=base_model(input1)<br/>x2=base_model(input2)<br/>x1=Dropout(0.2)(x1)<br/>x2=Dropout(0.2)(x2)<br/>x1=Concatenate(axis=-1)([GlobalMaxPool2D()(x1),GlobalAvgPool2D()(x1)])<br/>x2=Concatenate(axis=-1)([GlobalMaxPool2D()(x2),GlobalAvgPool2D()(x2)])<br/>x3=Subtract()([x1,x2])<br/>x3=Multiply()([x3,x3])<br/>x1_=Multiply()([x1,x1])<br/>x2_=Multiply()([x2,x2])<br/>x4=Subtract()([x1_,x2_])<br/>x5=Multiply()([x1,x2])<br/>x=Concatenate(axis=-1)([x3,x4,x5])<br/>x=(BatchNormalization())(x)<br/>x=Dropout(0.2)(x)<br/>x=Dense(256,activation='relu')(x)<br/>x=(BatchNormalization())(x)<br/>x=Dropout(0.2)(x)<br/>out=Dense(1,activation='sigmoid')(x)</span><span id="20e7" class="lg ig hi lc b fi ll li l lj lk">model=Model([input1,input2],out)<br/>model.compile(loss='binary_crossentropy',metrics=['acc',auroc],optimizer=Adam(0.00001))</span></pre><p id="794a" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">模型1架构背后的基本思想是，我们利用函数式API来构建模型，图像作为输入提供给vggface，幸运的是，vgg face将224X224分辨率的彩色图像作为输入，然后生成大小为2048的<a class="ae ko" href="https://www.quora.com/What-does-the-word-embedding-mean-in-the-context-of-Machine-Learning" rel="noopener ugc nofollow" target="_blank">嵌入</a>。</p><p id="cba2" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我使用了不同的数学计算层组合，</p><p id="aa5b" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里x1和x2是面部嵌入。</p><p id="e193" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">第一个是x3=(x1-x2)，其直观地执行两个图像的两点之间的欧几里德距离，即执行两个图像之间的相似性，其指示随着两点之间的距离增加，相似性降低，反之亦然。</p><p id="a319" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">第二个是x4=(x -x ),这也是计算图像两点之间的距离，但直观上，我们可以将其概括为曼哈顿距离，但每个点的平方。</p><p id="5e26" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">第三个是x5 =(x1*x2 ),增加它是为了给我们的模型增加某种正则化。添加了一些密集层的组合，以便我们的模型可以变得更加通用。</p><p id="94d9" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">模型1的输出结果非常好，获得了良好的AUC分数，下图显示了训练和交叉验证指标。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mh"><img src="../Images/125499c90e2f626ac58ac6bc236e23e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*o_2tYkd4Ibwi1mgzVqmO4A.png"/></div></figure><p id="f39e" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jn hj">模型1输出:</strong></p><p id="2004" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jn hj">列车AUC </strong> : 0.87 <strong class="jn hj">，C.V AUC </strong> :0.85</p><p id="d770" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">model1架构的优点是该架构更加一般化，即模型不会过度拟合甚至测试数据。</p><blockquote class="kp kq kr"><p id="221f" class="jl jm ks jn b jo jp jq jr js jt ju jv kt jx jy jz ku kb kc kd kv kf kg kh ki hb bi translated"><strong class="jn hj">型号2 </strong></p></blockquote><p id="e344" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我们没有像在模型1中那样计算欧几里得距离，而是计算两点之间的余弦距离。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mg"><img src="../Images/71ee9c7f16dc4b4b49ee922dbac9fe08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*RKrTaDlnatUOhyfVFLxfTw.png"/></div></figure><p id="662f" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">下面的代码执行两点之间的余弦距离</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="e5e9" class="lg ig hi lc b fi lh li l lj lk">def cosine_distance(vests):<br/>   x, y = vests<br/>   x = K.l2_normalize(x, axis=-1)<br/>   y = K.l2_normalize(y, axis=-1)<br/>   return -K.mean(x * y, axis=-1, keepdims=True)<br/>def cos_dist_output_shape(shapes):<br/>  shape1, shape2 = shapes<br/>  return (shape1[0],1)</span></pre><p id="1cb7" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">下图显示了模型2的架构</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="de5d" class="lg ig hi lc b fi lh li l lj lk">return (shape1[0],1)</span><span id="b638" class="lg ig hi lc b fi ll li l lj lk">input_1 = Input(shape=(224, 224, 3))<br/>input_2 = Input(shape=(224, 224, 3))</span><span id="b329" class="lg ig hi lc b fi ll li l lj lk">base_model = VGGFace(model='resnet50', include_top=False)</span><span id="3bc8" class="lg ig hi lc b fi ll li l lj lk">for x in base_model.layers[:-3]:<br/> x.trainable = False<br/>x1 = base_model(input_1)<br/>x2 = base_model(input_2)<br/>x1=Dropout(0.2)(x1)<br/>x2=Dropout(0.2)(x2)<br/>x1 = GlobalMaxPool2D()(x1)<br/>x2 = GlobalMaxPool2D()(x2)<br/>x3 = Subtract()([x1, x2])<br/>x4 = Multiply()([x1, x2])<br/>x5 = Lambda(cosine_distance, output_shape=cos_dist_output_shape)([x1, x2])<br/>x = Concatenate(axis=-1)([x5, x3,x4])<br/>x = Dropout(0.1)(x)<br/>out = Dense(1, activation="sigmoid")(x)</span><span id="2c62" class="lg ig hi lc b fi ll li l lj lk">model = Model([input_1, input_2], out)<br/>model.compile(loss=['binary_crossentropy'], metrics=['acc',auroc], optimizer=Adam(0.00001))</span></pre><p id="af15" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">模型2的输出没有获得好的结果，即余弦相似性没有给最终输出增加任何显著的差异。从下图中可以看出，这种架构在12个时期后也是过度适合的</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mi"><img src="../Images/a4d5f3488a41e274182b05f9054a82af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*CQl9MWHIgctShdr0dR1b2w.png"/></div></figure><p id="9daa" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jn hj">模式2输出:</strong></p><p id="ec6d" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jn hj">列车AUC </strong> : 0.91 <strong class="jn hj">，C.V AUC </strong> :0.82</p><blockquote class="kp kq kr"><p id="31a9" class="jl jm ks jn b jo jp jq jr js jt ju jv kt jx jy jz ku kb kc kd kv kf kg kh ki hb bi translated"><strong class="jn hj">模型3 </strong></p></blockquote><p id="c3f2" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这个模型是模型1的轻微修改。这里我用Senet 50代替resnet50，其余的都保持不变。</p><p id="fd6f" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">SENet是指<strong class="jn hj">挤压和激励(SE)。</strong>SE块尝试使用全局信息来有选择地强调信息性特征并抑制一次不太有用的特征。从字面上看，它会尝试向图层中的每个要素地图添加权重。</p><p id="5a61" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">但是这个数据上的SeNet执行得不是很好，因为模型3在某些时期后过度拟合，而具有相同架构的模型1 resenet执行得更好。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mj"><img src="../Images/9ab3e99c50c318dfcb8e562c20970d5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*D6bIF6bCRaSHXGv609Gddw.png"/></div></figure><p id="6aa3" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jn hj">模型3输出:</strong></p><p id="4e6b" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jn hj">训练AUC </strong> : 0.91 <strong class="jn hj">，C.V AUC </strong> :0.82</p><p id="c4af" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">正如我们所看到的，采用SEnet架构的模型3未能很好地推广。</p><blockquote class="kp kq kr"><p id="3ff4" class="jl jm ks jn b jo jp jq jr js jt ju jv kt jx jy jz ku kb kc kd kv kf kg kh ki hb bi translated"><strong class="jn hj">模型4 </strong></p></blockquote><p id="6d88" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">架构与模型1相同，但是这里我使用另一个数据生成器来执行数据扩充。数据扩充是增加数据数量和多样性的过程。在我们处理图像之前，最常用的数据增强操作是-</p><ol class=""><li id="ff74" class="lp lq hi jn b jo jp js jt jw lr ka ls ke lt ki lu lv lw lx bi translated">旋转</li><li id="a579" class="lp lq hi jn b jo ly js lz jw ma ka mb ke mc ki lu lv lw lx bi translated">剪羊毛</li><li id="4bba" class="lp lq hi jn b jo ly js lz jw ma ka mb ke mc ki lu lv lw lx bi translated">变焦</li><li id="a757" class="lp lq hi jn b jo ly js lz jw ma ka mb ke mc ki lu lv lw lx bi translated">种植</li><li id="8206" class="lp lq hi jn b jo ly js lz jw ma ka mb ke mc ki lu lv lw lx bi translated">轻弹</li><li id="93ba" class="lp lq hi jn b jo ly js lz jw ma ka mb ke mc ki lu lv lw lx bi translated">更改亮度级别</li></ol><p id="9fd6" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">我用来执行增强的操作如下所示</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="3a1f" class="lg ig hi lc b fi lh li l lj lk">datagen =ImageDataGenerator(rotation_range=25,width_shift_range=0.2, height_shift_range=0.2,horizontal_flip=True, vertical_flip=True)</span></pre><p id="746e" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">数据生成器如下所示</p><pre class="je jf jg jh fd lb lc ld le aw lf bi"><span id="14d8" class="lg ig hi lc b fi lh li l lj lk">def get_flow_from_dataframe(a,b):<br/> train_generator1=a<br/> train_generator2=b<br/> while True:<br/>   x_1 = train_generator1.next()<br/>   x_2 = train_generator2.next()<br/>   yield [x_1[0], x_2[0]], x_1[1]</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es mk"><img src="../Images/af347b6ecb682b414bd9bc3690c6c3af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*Rb0MMyj7dm5oKPQGH_xBQA.png"/></div></div></figure><p id="1a8a" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jn hj">模式4输出:</strong></p><p id="b368" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jn hj">列车精度</strong> : 0.601 <strong class="jn hj">，C.V精度</strong> :0.49</p><p id="4d23" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">这里的输出非常令人惊讶，数据的增加使事情变得更糟。模型4还没有达到50%的准确率。我们可以说这个模型是哑模型。这背后的可能原因是数据扩充正在创建有噪声和不相关的图像对，这导致我们的模型基于创建的数据而不是基于实际数据来移动超平面。</p><blockquote class="kp kq kr"><p id="a22d" class="jl jm ks jn b jo jp jq jr js jt ju jv kt jx jy jz ku kb kc kd kv kf kg kh ki hb bi translated"><strong class="jn hj">车型对比</strong></p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/aaf4621373cb9ce4c4473187b9187445.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*-DKKGz7WXzQ7AI5kZ91Ltw.png"/></div></figure><p id="2af0" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">模型1表现得非常好，正如我们从图中可以看到的，在训练数据上获得的AUC是0.87，接近交叉验证数据上的0.85。数学运算(即欧几里德距离)显著提高了性能。</p><p id="5906" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">另一方面，模型2和模型3在训练数据上表现良好，但是模型过度拟合，不能在交叉验证数据上表现良好。从上图可以看出，这些模型的表现并不一致。</p><p id="e8ed" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如上所述，模型4是哑模型，尽管其架构与模型1相同。在这里，数据扩充使事情变得更糟，导致巨大的过度拟合和训练损失只是停留在局部最小值。</p><p id="cb7d" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">下图显示了模型1在测试数据上的AUC分数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es mm"><img src="../Images/a57340c809e9428c5c373c3ac09709a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7etRzwEYnIM2iTstSgY89w.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated">测试数据的模型1 Auc得分</figcaption></figure><h1 id="d913" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">5.未来的工作</h1><ol class=""><li id="5d4a" class="lp lq hi jn b jo kj js kk jw mr ka ms ke mt ki lu lv lw lx bi translated">为了改善结果，我们可以尝试集合所有模型并平均输出。</li><li id="06bc" class="lp lq hi jn b jo ly js lz jw ma ka mb ke mc ki lu lv lw lx bi translated">在未来，如果我们收集更多的数据，即相关对，我们可以尝试实现我们自己的架构，而不是依赖于VGG面，因为正如我们所观察到的那样，在较少的数据上应用预训练权重，我们的模型往往会过度拟合，我们可能无法保证我们的模型有多一般化。</li></ol><h1 id="af3e" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">6.参考</h1><ol class=""><li id="e445" class="lp lq hi jn b jo kj js kk jw mr ka ms ke mt ki lu lv lw lx bi translated"><a class="ae ko" href="https://github.com/rcmalli/keras-vggface" rel="noopener ugc nofollow" target="_blank">https://github.com/rcmalli/keras-vggface</a></li><li id="0790" class="lp lq hi jn b jo ly js lz jw ma ka mb ke mc ki lu lv lw lx bi translated"><a class="ae ko" rel="noopener" href="/@14prakash/image-classification-architectures-review-d8b95075998f">https://medium . com/@ 14 Prakash/image-class ification-architectures-review-d8b 95075998 f</a></li><li id="6375" class="lp lq hi jn b jo ly js lz jw ma ka mb ke mc ki lu lv lw lx bi translated"><a class="ae ko" href="https://www.kaggle.com/c/recognizing-faces-in-the-wild/notebooks" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/recogniting-faces-in-the-wild/notebooks</a></li><li id="4df4" class="lp lq hi jn b jo ly js lz jw ma ka mb ke mc ki lu lv lw lx bi translated"><a class="ae ko" href="https://nanonets.com/blog/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2/" rel="noopener ugc nofollow" target="_blank">https://nano nets . com/blog/data-augmentation-how-to-use-deep-learning-when-you-have-limited-data-part-2/</a></li><li id="eb2f" class="lp lq hi jn b jo ly js lz jw ma ka mb ke mc ki lu lv lw lx bi translated">https://nnabla.org/paper/imagenet_in_224sec.pdf<a class="ae ko" href="https://nnabla.org/paper/imagenet_in_224sec.pdf" rel="noopener ugc nofollow" target="_blank"/></li><li id="b0ac" class="lp lq hi jn b jo ly js lz jw ma ka mb ke mc ki lu lv lw lx bi translated">【https://www.appliedaicourse.com/ T4】</li></ol><p id="497f" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated"><strong class="jn hj">点击此处查看完整代码</strong>:<a class="ae ko" href="https://github.com/santoshketa/Kinship-Classification-using-faces-recognizing_faces_in_the-wild" rel="noopener ugc nofollow" target="_blank">https://github . com/santoshketa/亲属关系分类使用面孔识别野外面孔</a></p><p id="72a9" class="pw-post-body-paragraph jl jm hi jn b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki hb bi translated">如果您有任何疑问，请联系我:<a class="ae ko" href="https://www.linkedin.com/in/sri-santosh-bhargav-354974192/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/sri-santosh-bhargav-354974192/</a></p></div></div>    
</body>
</html>