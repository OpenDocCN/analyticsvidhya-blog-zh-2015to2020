<html>
<head>
<title>Drone Aerial View Segmentation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">无人机航拍图分割</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/drone-aerial-view-segmentation-44046ff003b5?source=collection_archive---------13-----------------------#2020-07-28">https://medium.com/analytics-vidhya/drone-aerial-view-segmentation-44046ff003b5?source=collection_archive---------13-----------------------#2020-07-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c18c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">如何教会无人机看清下面是什么并以高分辨率分割物体</em></p><h2 id="99be" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">介绍</h2><p id="f951" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">无人机的使用在过去几年中已经得到普及，它提供<em class="jd">高分辨率</em>图像，与卫星图像<em class="jd">相比，具有更低的成本</em>，灵活性和低空飞行，从而导致对该领域越来越多的兴趣，甚至它可以<em class="jd">携带各种传感器</em>，如磁传感器。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es ke"><img src="../Images/2abde7949194f01f653d36c2f2197200.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*kZkbcDErSUJd4wKcXbzrHg.jpeg"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">无人机(<a class="ae kq" href="https://unsplash.com/photos/ZlkRrzJl20Q" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>)</figcaption></figure><p id="83df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于鸟瞰视图，教无人机看东西相当具有挑战性，并且大多数预训练模型都是在我们日常看到的正常图像(视点)中训练的(ImageNet、PASCAL VOC、COCO)。在这个项目中，我想尝试如何训练无人机数据集，目标是:</p><ul class=""><li id="942d" class="kr ks hi ih b ii ij im in iq kt iu ku iy kv jc kw kx ky kz bi translated">模型重量轻(参数少)</li><li id="1ed1" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">高分(希望如此)</li><li id="469d" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">快速推理延迟。</li></ul></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h2 id="a161" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">数据集</h2><blockquote class="lm ln lo"><p id="9b2c" class="if ig jd ih b ii ij ik il im in io ip lp ir is it lq iv iw ix lr iz ja jb jc hb bi translated">[2] <a class="ae kq" href="https://www.kaggle.com/bulentsiyah/semantic-drone-dataset" rel="noopener ugc nofollow" target="_blank">语义无人机数据集</a>专注于对城市场景的语义理解，以提高自主无人机飞行和着陆程序的安全性。这些图像描绘了在离地面5到30米的<strong class="ih hj">高度上从天底(鸟瞰)获得的20多所房屋。高分辨率摄像机用于采集尺寸为<strong class="ih hj"> 6000x4000px (24Mpx </strong>)的图像。训练集包含<strong class="ih hj"> 400个公开可用的图像</strong>，测试集由200个私有图像组成。</strong></p></blockquote><div class="kf kg kh ki fd ab cb"><figure class="ls kj lt lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/e8191e90402cf239c0ee7ddb5d26d2dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ZzdYo1Xy0d-K7qzbRNlODw.jpeg"/></div></figure><figure class="ls kj lt lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/30d70a7718e71e87b73789cb25c9ceeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ff467Z3PK7NiJ231b-8Xhg.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx mc di md me translated">数据集的样本图像</figcaption></figure></div><p id="f31b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据集的复杂性限于下面列出的<strong class="ih hj"> 20个类别</strong>(但实际上它的掩码中有23个类别:<em class="jd">树、草、其他植被、泥土、砾石、岩石、水、铺砌区域、水池、人、狗、汽车、自行车、屋顶、墙、栅栏、栅栏柱、窗户、门、障碍物。</em></p></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h2 id="1343" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">方法</h2><p id="b379" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated"><strong class="ih hj">预处理</strong></p><p id="233a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我<strong class="ih hj"> <em class="jd">用<em class="jd">相同的比例</em>调整图像</em> </strong>的大小就像原来输入到<strong class="ih hj"> 704 x 1056 </strong>一样，我<em class="jd">不把图像裁剪成面片</em>因为几个原因，对象不算太小，不占用太多内存，节省我的训练时间。我将数据集分为三部分训练(306)、验证(54)和测试(40)集，并应用了水平翻转、垂直翻转、网格变形、随机亮度对比度，并将高斯噪声添加到训练数据中，小批量大小为3。</p><p id="6a5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">模型架构</strong></p><p id="662f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我使用两种模型架构，我特意使用轻量级模型作为主干，如MobileNet和EfficientNet，以提高计算效率。</p><ul class=""><li id="0e30" class="kr ks hi ih b ii ij im in iq kt iu ku iy kv jc kw kx ky kz bi translated"><strong class="ih hj"> U-Net </strong>以<em class="jd"> MobileNet_V2 </em>和<em class="jd"> EfficientNet-B3 </em>为骨干</li><li id="3243" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated"><strong class="ih hj"> FPN </strong>(特色金字塔网络)与<em class="jd">高效网-B3 </em>骨干网</li></ul><p id="ed28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我看到<em class="jd"> </em> <a class="ae kq" href="https://arxiv.org/abs/2007.02839" rel="noopener ugc nofollow" target="_blank"> <em class="jd"> Parmar的论文</em></a><em class="jd">【3】</em>对于模型的选择(我之前已经训练过不同的模型，这些选择看起来很管用)</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mf"><img src="../Images/5f4149727f622753a1ae4194b4e97b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zATsN0wYM8UKcgGL3WEOIQ.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">U-Net Arch ( <a class="ae kq" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank">来源</a>)</figcaption></figure><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es mg"><img src="../Images/ca49a1d73e071960850945e5fbaf981b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*w2oYCjNbY9jedmU38TCRkw.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">特色金字塔网络(FPN) ( <a class="ae kq" href="https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610" rel="noopener" target="_blank">来源</a>)</figcaption></figure><p id="7612" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">培训策略</strong></p><p id="0ea0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在培训过程中我采用了<em class="jd">两阶段</em>培训，</p><ul class=""><li id="a36f" class="kr ks hi ih b ii ij im in iq kt iu ku iy kv jc kw kx ky kz bi translated"><strong class="ih hj"> <em class="jd">第一次训练的模型为第30个</em> </strong> <em class="jd"> </em>历元患有斜视</li><li id="5eff" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated"><strong class="ih hj"> <em class="jd">其次，使用加权CrossEntropyLoss和Lovasz Softmax损失继续20个历元</em> </strong>以最大化IoU得分[1][4]，对两个阶段使用一个周期学习率策略。损失看起来是这样的。</li></ul><figure class="kf kg kh ki fd kj er es paragraph-image"><div class="er es mh"><img src="../Images/cced2588b9f83bd0280c8fc2c4fb88ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*klfpQvoF3a-i267zNrcXFQ.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx translated">加权CrossEntropyLoss和Lovasz Softmax</figcaption></figure><p id="7989" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用<strong class="ih hj"> <em class="jd"> α </em> </strong>表示重量，用<strong class="ih hj"> <em class="jd">输出</em> </strong>表示模型。在此过程中，给定的权重为0。<strong class="ih hj"> 7 </strong>。</p><ul class=""><li id="2a2a" class="kr ks hi ih b ii ij im in iq kt iu ku iy kv jc kw kx ky kz bi translated">如果验证损失7次没有改善，使用早期停止</li></ul></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h2 id="2185" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">结果</h2><p id="9ba3" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">在这个实验中，FPN模型比U-Net给出了最好的结果，FPN损耗也比U-Net基模型下降得更快，由于早期停止，训练时间更少。</p><p id="f909" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">损失</strong></p><p id="e95b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与其他模型相比，FPN基本模型收敛很快，且达到大约0.3最小损耗，而U-Net B3和MobileNet_v2大约为0.4到0.5。第二阶段训练中的损失增加，因为它<em class="jd">对交叉熵和Lovasz之间的乘积求和。</em></p><div class="kf kg kh ki fd ab cb"><figure class="ls kj mi lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/4a25dfda8ff5e8499e07cb308ae1b472.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*M3pMcjrKjuSsgaMcGs45yg.png"/></div></figure><figure class="ls kj mj lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/ea9964ba0c4c8f701b16b11b709f30d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*GRwYBRosv7Maeeb3-boU5g.png"/></div></figure><figure class="ls kj mi lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/ac8b1e4f2e3613412a442153f15faf94.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*5mpwiSHA5yeb6REKz3PelQ.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx mk di ml me translated">培训损失</figcaption></figure></div><p id="04b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">评估指标</strong></p><p id="b6fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用两个指标来评估模型性能，交集/并集和像素精度，使用Lovasz loss的训练提高了IoU得分，如果我们看到下图，交叉熵将IoU固定在50%左右，这在U-Net MobileNet_v2中可以清楚地看到</p><div class="kf kg kh ki fd ab cb"><figure class="ls kj mi lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/66ceeb090da17a457dd66f0735f3884f.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*yiL3SQx_omYXNA6C0oQgmg.png"/></div></figure><figure class="ls kj mj lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/b24f5db84b9d918ee9a8ed34d49ba4d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*Nsh6FyEEQP4JFdOkHcLHuA.png"/></div></figure><figure class="ls kj mi lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/43c6add702797d92b25dd1a9833a9d2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*9VArjkO9f7lRBdh8iG9YiQ.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx mk di ml me translated">培训mIoU</figcaption></figure></div><p id="f566" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们看到下面的像素精度，使用Lovasz的训练并没有真正提高像素精度，它只是停留在阶段1的精度，而训练阶段1的交叉熵在提高精度方面更好，这就是为什么我首先使用交叉熵并继续使用Lovasz。</p><div class="kf kg kh ki fd ab cb"><figure class="ls kj mi lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/2fad754f06ec4ad7254e3151ffc7a67c.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*_RDjO8pytoTujeLmjd2b5g.png"/></div></figure><figure class="ls kj mj lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/1a5ccff0bc0368f506702a425d1884db.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*Y5uBSqCYvYvMKBMHPGxMtQ.png"/></div></figure><figure class="ls kj mi lu lv lw lx paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><img src="../Images/51443fb591ad3a591473126380764396.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*dGlgNZIeteDDS-pGG002UQ.png"/></div><figcaption class="km kn et er es ko kp bd b be z dx mk di ml me translated">训练像素精度</figcaption></figure></div><p id="d8fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">推论</strong></p><p id="6bb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在模型被教会看物体之后，是时候用测试集来看看模型的表现了，在我把它放在掩体里之前，模型从来没有见过这个测试集。测试集由40幅图像组成，我将测试集的大小调整为768 x 1152(原来是4000 x 6000)以保存我的免费colab GPU，结果相当令人满意，下面是来自3个不同模型的推理示例。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mm"><img src="../Images/b31d0979511864ea3267b413cce54150.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2N6-OqAKGHzVLWB6x7bq5Q.png"/></div></div></figure><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mm"><img src="../Images/84ffe78a12a1aab1483417758480c64c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CtdAgQ_okx0U-jQZYa6Suw.png"/></div></div></figure><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mm"><img src="../Images/f08a85fe0f7a51d4bdf291762c4a827f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZnROICZ96P6krnmgPv2mQ.png"/></div></div></figure><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mm"><img src="../Images/7c2c1dd4b4259e3d5860173430b1803d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jkmZ60gGZUccvibPwqYmOw.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">模型推理</figcaption></figure><p id="5584" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了总结结果，我制作了这个表格来比较每个型号性能的结果，以记住推断延迟实际上取决于您的机器和图像分辨率(我的是768 x 1152)，在这个总结中，我使用随时间变化的Google Colab GPU，通常包括<a class="ae kq" href="https://research.google.com/colaboratory/faq.html#resource-limits" rel="noopener ugc nofollow" target="_blank"> <em class="jd"> Nvidia K80s、T4s、P4s和P100s </em> </a>来进行推断，并计算所有测试集花费的平均时间<em class="jd">。</em></p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="ly lz di ma bf mb"><div class="er es mn"><img src="../Images/12a4863e1b4ca6cb60c9fdafcc869235.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cOsf087p2rzREYP1P6tO-A.png"/></div></div><figcaption class="km kn et er es ko kp bd b be z dx translated">模型比较</figcaption></figure></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h2 id="0524" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">结论</h2><p id="261b" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">让我们总结一下到目前为止我们已经完成的工作</p><ul class=""><li id="79a4" class="kr ks hi ih b ii ij im in iq kt iu ku iy kv jc kw kx ky kz bi translated"><strong class="ih hj"> <em class="jd"> FPN架构在该数据集中运行良好</em> </strong>，甚至在相同主干的情况下也优于U-Net基础模型</li><li id="a588" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated"><strong class="ih hj"> Lovasz Softmax </strong> loss对<strong class="ih hj"> <em class="jd">提升IoU得分</em> </strong>效果很好，并没有真正提高精度，反而交叉熵提升了像素精度。</li><li id="e0af" class="kr ks hi ih b ii la im lb iq lc iu ld iy le jc kw kx ky kz bi translated">用MobileNet_v2建模轻量级(较少参数)且还具有快速推理延迟是U-Net，即使牺牲了模型分数。</li></ul><p id="2cf4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分数和推理时间之间的权衡实际上取决于目的和计算资源。</p><p id="a012" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我真的很兴奋将这个模型嵌入到真正的无人机应用中。</p><p id="7922" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有关我的项目的更多信息，请访问<a class="ae kq" href="https://github.com/said-rasidin" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> <em class="jd">我的github </em> </strong> </a></p><h2 id="c337" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">参考</h2><p id="905b" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">[1]伯曼，马克西姆&amp;兰嫩，阿马尔&amp;布拉施科，马修。(2018).Lovasz-Softmax损失:神经网络中交集过合并度量优化的易处理替代。4413–4421.2018.00464。</p><p id="9840" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]格拉茨工业大学。(2019).2020年7月13日访问。<a class="ae kq" href="http://dronedataset.icg.tugraz.at/" rel="noopener ugc nofollow" target="_blank">http://dronedataset . ICG . tugraz . at</a>。</p><p id="efdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3]帕马尔、维韦克&amp;巴蒂亚、纳拉亚尼&amp;内吉、舒巴姆&amp;苏里、马南博士。(2020).面向无人机边缘部署的优化语义分割架构探索。</p><p id="84d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[4]拉赫林，亚历山大&amp;达维多夫，亚历克斯&amp;尼科连科，谢尔盖。(2018).基于U-Net和Lovász-Softmax Loss的卫星影像土地覆盖分类。257–2574.10.1109/CVPRW.2018.00048</p></div></div>    
</body>
</html>