<html>
<head>
<title>Intel OpenVINO: Inference Engine</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">英特尔OpenVINO:推理引擎</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/intel-openvino-inference-engine-7ba5076dc6e0?source=collection_archive---------9-----------------------#2020-08-04">https://medium.com/analytics-vidhya/intel-openvino-inference-engine-7ba5076dc6e0?source=collection_archive---------9-----------------------#2020-08-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9f4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我之前的文章中，我已经讨论了OpenVINO工具包的<a class="ae jd" rel="noopener" href="/swlh/introduction-to-intel-openvino-toolkit-5f98dbb30ffb">基础和OpenVINO的</a><a class="ae jd" rel="noopener" href="/analytics-vidhya/intel-openvino-model-optimizer-e381affa458c">模型优化器</a>。在这篇文章中，我们将探索:-</p><ul class=""><li id="7b12" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">什么是推理机？</li><li id="3f45" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">支持的设备</li><li id="68c3" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">向推理引擎提供中间表示(IR)</li><li id="c86c" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">推理请求</li><li id="7efd" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">处理输出</li></ul><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es js"><img src="../Images/48edc9a7968773f116c63b6d69d82c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*1CAlUjDyQRGf0ZUIRrgGoQ.png"/></div></figure><h1 id="3ef1" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">什么是推理机？</h1><p id="c11b" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">推理引擎，顾名思义，在模型上运行实际的推理。它仅适用于来自模型优化器的中间表示(IR)或已经以IR格式呈现的英特尔预训练模型。</p><p id="91b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与模型优化器一样，它根据模型的大小和复杂性提供改进，以提高内存和计算时间，推理引擎提供基于硬件的优化，以进一步改进模型。</p><p id="d405" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">推理机本身实际上是在C++中构建的，导致整体更快的操作；然而，在Python代码中利用内置的Python包装器与之交互是非常常见的。</p><h1 id="8bc1" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">支持的设备</h1><p id="fe0a" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">推理引擎支持的设备都是英特尔硬件:-</p><ul class=""><li id="9eda" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">中央处理器</li><li id="eb32" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">图形处理单元</li><li id="0817" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">神经计算棒</li><li id="63b1" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">现场可编程门阵列</li></ul><p id="80a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大多数情况下，一个设备上的操作将与其他支持的设备相同，但有时，当使用推理引擎时，某些硬件不支持某些层(不支持的层)，在这种情况下，有一些可用的扩展可以添加支持。我们将在本文后面讨论它们。</p><h1 id="fa7f" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">向推理引擎提供一个IR</h1><p id="843d" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">推理引擎有两个类:-</p><ul class=""><li id="13b2" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">网络</li><li id="e6af" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">IECore</li></ul><p id="e3b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">网络</strong></p><p id="0d7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个类读取中间表示(。xml &amp;。bin文件)并加载模型。</p><p id="4172" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">类属性</strong></p><ul class=""><li id="2234" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">名称→加载网络的名称。</li><li id="2627" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">输入→包含网络模型所需输入的字典。</li><li id="e657" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">输出→包含网络模型输出的字典。</li><li id="3ec0" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">batch_size →网络的批量大小。</li><li id="6845" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">精度→网络的精度(INT8、FP16、FP32)</li><li id="4f6d" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">layers →返回字典，该字典按拓扑顺序将网络图层名称映射到包含图层属性的IENetLayer对象。</li><li id="fce0" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">stats →返回LayersStatsMap对象，该对象包含将网络层名称映射到由LayerStats对象表示的校准统计数据的字典。</li></ul><p id="0c58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> __init__() </strong></p><p id="d579" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它是类构造函数。它需要两个参数:-</p><ul class=""><li id="21c7" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">模型→通往。xml文件。</li><li id="7b88" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">权重→通往。bin文件。</li></ul><p id="a7a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">返回IENetwork类的实例。</p><p id="5cff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">成员功能</strong></p><p id="0b14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> from_ir() </strong></p><p id="3f23" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从读取模型。xml和。IR的bin文件。它需要两个参数:-</p><ul class=""><li id="dd77" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">模型:-的路径。IR的xml文件</li><li id="e16c" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">权重:-路径到。IR的bin文件</li></ul><p id="c4c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">返回IENetwork类的实例。</p><p id="e0a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:您可以使用IENetwork类构造函数来代替from_ir()</p><p id="3149" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">重塑()</strong></p><p id="5254" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重塑网络以更改空间维度、批量大小或任何维度。它使用一个参数:-</p><ul class=""><li id="a728" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">input_shapes →将输入图层名称映射到具有目标形状的元组的字典</li></ul><p id="e866" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:使用此方法之前，请确保目标形状适用于网络。将网络形状更改为任意值可能会导致不可预测的行为。</p><p id="d182" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">连载()</strong></p><p id="5ea6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">序列化网络并将其存储在文件中。它需要两个参数:-</p><ul class=""><li id="255d" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">path_to_xml →存储序列化模型的文件路径。</li><li id="8393" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">path_to_bin →存储序列化重量的文件路径</li></ul><p id="a0b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> IECore </strong></p><p id="6aff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个Python包装类，用于推理引擎。</p><p id="d13d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">类属性</strong></p><ul class=""><li id="0895" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">available_devices <strong class="ih hj"> → </strong>设备返回为[CPU，FPGA.0，FPGA.1，MYRIAD]。如果某个特定类型的设备不止一个，则会列出所有设备，后面跟一个点和一个数字。</li></ul><p id="7f43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">成员功能</strong></p><p id="586b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它有许多成员函数，但是我将集中讨论三个主要函数</p><p id="df2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> load_network() </strong></p><p id="c800" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将从中间表示(IR)读取的网络加载到具有指定设备名称的插件，并创建I network类的ExecutableNetwork对象。</p><p id="b830" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它需要四个参数:-</p><ul class=""><li id="211b" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">网络→有效的IENetwork实例。</li><li id="7ca0" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">设备名称→目标插件的设备名称。</li><li id="faae" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">config →插件配置键及其值的字典(可选)。</li><li id="f0b7" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">num_requests →要创建的推断请求的正整数值。推断请求的数量受到设备能力的限制。</li></ul><p id="aea9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">返回ExecutableNetwork对象。</p><p id="47d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> add_extension() </strong></p><p id="9a69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将扩展库加载到具有指定设备名称的插件中。</p><p id="2f08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它需要两个参数:-</p><ul class=""><li id="5fb5" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">extension_path →要加载到插件的扩展库文件的路径。</li><li id="9b5e" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">设备名称→要加载扩展的插件的设备名称。</li></ul><p id="69ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">查询_网络()</strong></p><p id="0257" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用指定的设备名称查询插件，当前配置支持哪些网络层。</p><p id="4923" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它需要三个参数:-</p><ul class=""><li id="32da" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">网络→有效的IENetwork实例。</li><li id="e551" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">设备名称→目标插件的设备名称。</li><li id="eb05" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">config →插件配置键及其值的字典(可选)。</li></ul><p id="7a3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">返回支持它们的字典映射层和设备名称</p><p id="5507" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">将模型(在IR中)加载到IE </strong></p><p id="b971" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将首先从导入所需的库开始(我将使用Python)</p><pre class="jt ju jv jw fd ld le lf lg aw lh bi"><span id="6537" class="li kb hi le b fi lj lk l ll lm"><strong class="le hj">from</strong> openvino.inference_engine <strong class="le hj">import </strong>IENetwork <br/><strong class="le hj">from</strong> openvino.inference_engine <strong class="le hj">import </strong>IECore</span></pre><p id="f965" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们定义一个函数来加载模型。</p><pre class="jt ju jv jw fd ld le lf lg aw lh bi"><span id="af76" class="li kb hi le b fi lj lk l ll lm"><strong class="le hj">def </strong>load_IR_to_IE(model_xml):<br/> <em class="ln">   </em>### Load the Inference Engine API<br/>    plugin = IECore()</span><span id="38a3" class="li kb hi le b fi lo lk l ll lm">    ### Loading the IR files to IENetwork class<br/>    model_bin = model_xml[:-3]+"bin" <br/>    network = IENetwork(model=model_xml, weights=model_bin)</span></pre><p id="feb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">检查无支撑层</strong></p><p id="320d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上所述，即使在成功转换到IR后，仍有一些硬件设备不支持某些层，我们有一些扩展可以添加支持。</p><p id="8f7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当在CPU上使用推理引擎时，可能会有CPU不支持的某些层，在这种情况下，我们可以添加CPU扩展来支持其他层。</p><p id="59ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将使用IECore类的“query_network()”来获取推理引擎支持的层列表。然后，您可以遍历您创建的IENetwork中的层，并检查它们是否在支持的层列表中。如果不支持某个层，CPU扩展可能会有所帮助。</p><p id="2300" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“device_name”参数只是一个字符串，用于表示设备“CPU”、“GPU”、“FPGA”或“MYRIAD”(适用于Neural Compute Stick)。</p><p id="f128" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们添加CPU扩展并检查不支持的层</p><pre class="jt ju jv jw fd ld le lf lg aw lh bi"><span id="9545" class="li kb hi le b fi lj lk l ll lm">    ### Defining CPU Extension path<br/>    CPU_EXT_PATH=      "/opt/intel/openvino/deployment_tools/inference_engine/lib/intel64/ libcpu_extension_sse4.so"</span><span id="4124" class="li kb hi le b fi lo lk l ll lm">    ### Adding CPU Extension<br/>    plugin.add_extension(CPU_EXT_PATH,"CPU")</span></pre><p id="c033" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然它们应该仍然在同一个位置，但是它们在操作系统上还是有一些不同(我在Linux上工作)。如果您导航到您的OpenVINO安装目录，然后是deployment_tools、inference_engine、lib、intel64。</p><pre class="jt ju jv jw fd ld le lf lg aw lh bi"><span id="6188" class="li kb hi le b fi lj lk l ll lm">    ### Get the supported layers of the network<br/>    supported_layers = plugin.query_network(network=network, device_name="CPU")</span><span id="71e4" class="li kb hi le b fi lo lk l ll lm">    ### Finding unsupported layers<br/>    unsupported_layers = [l <strong class="le hj">for</strong> l <strong class="le hj">in</strong> network.layers.keys() <strong class="le hj">if</strong> l <strong class="le hj">not</strong> <strong class="le hj">in</strong> supported_layers]</span><span id="f323" class="li kb hi le b fi lo lk l ll lm">    ### Checking for unsupported layers<br/>    <strong class="le hj">if</strong> len(unsupported_layers) != 0:<br/>        print("Unsupported layers found")<br/>        print(unsupported_layers)<br/>        exit(1)</span></pre><p id="cf8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的代码检查不支持的层的存在。为了简单起见，我们来分解一下上面的代码。</p><ul class=""><li id="b38f" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">我已经使用了query_network()，它是IECore类(上面提到的)的成员函数，用来获取支持的层的列表。</li><li id="d3af" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">“network”是I network类的一个对象，它有一个名为“layers”的属性(如上所述)，它返回一个字典，其中包含每个支持的层的名称(作为键)及其属性(作为值)。利用这一点，我们找出不支持的层(如果存在)。</li><li id="3141" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">最后，如果存在任何不支持的层。我们显示消息和不支持的层，然后退出。</li></ul><p id="5b32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们加载网络</p><pre class="jt ju jv jw fd ld le lf lg aw lh bi"><span id="da83" class="li kb hi le b fi lj lk l ll lm">    ### Loading the network<br/>    executable_net = plugin.load_network(network,"CPU")</span><span id="3dee" class="li kb hi le b fi lo lk l ll lm">    print("Network succesfully loaded into the Inference Engine")</span><span id="79a8" class="li kb hi le b fi lo lk l ll lm">    return executable_net</span></pre><p id="db18" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意:-只有到2019R3版本的OpenVINO工具包才需要CPU扩展。在2020R1(以及未来可能的更新)中，CPU扩展不再需要添加到推理引擎中</p><h1 id="76de" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated"><strong class="ak">推理请求</strong></h1><p id="42cd" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">在将IENetwork加载到IECore中之后，您将获得一个ExecutableNetwok，这是您将向其发送推理请求的对象。</p><p id="bcb6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有两种类型的推理请求</p><ul class=""><li id="4462" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">同步的</li><li id="28a1" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">异步的</li></ul><p id="652a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">同步</strong></p><p id="8f6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在同步推理的情况下，系统将等待并保持空闲，直到返回推理响应(阻塞主线程)。在这种情况下，一次只处理一个帧，并且在当前帧的推断完成之前不能收集下一个帧。</p><p id="e772" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于同步推理，我们使用“infer()”</p><p id="6139" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">推断()</strong></p><p id="1c4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它需要一个参数:-</p><ul class=""><li id="7299" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">输入→将输入图层名称映射到numpy.ndarray对象的字典，这些对象的形状与图层的输入数据一致</li></ul><p id="0aa8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">返回一个字典，该字典将输出图层名称映射到带有图层输出数据的numpy.ndarray对象</p><p id="8c87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">异步</strong></p><p id="ac66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如您可能已经猜到的那样，在异步推理的情况下，如果对特定请求的响应花费了很长时间，那么您不会暂停，而是在当前流程执行的同时继续下一个流程。与同步推理相比，异步推理确保了更快的推理。</p><p id="8486" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当主线程在同步模式下被阻塞时，异步模式不会阻塞主线程。因此，您可以发送一个帧进行推断，同时仍然收集和预处理下一个帧。您可以利用“等待”过程来等待推理结果可用。</p><p id="e306" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于异步推理，我们使用“start_async()”</p><p id="8970" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> start_async() </strong></p><p id="fcbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接受两个参数:-</p><ul class=""><li id="f404" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">request_id →开始推断的推断请求的索引。</li><li id="2aee" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">输入→将输入图层名称映射到numpy.ndarray对象的字典，这些对象的形状与图层的输入数据一致</li></ul><p id="dd94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们实现同步和异步推理</p><pre class="jt ju jv jw fd ld le lf lg aw lh bi"><span id="307b" class="li kb hi le b fi lj lk l ll lm"><strong class="le hj">def </strong>synchronous_inference(executable_net, image):</span><span id="be4d" class="li kb hi le b fi lo lk l ll lm">    ### Get the input blob for the inference request<br/>    input_blob = next(iter(executable_net.inputs))</span><span id="7f48" class="li kb hi le b fi lo lk l ll lm">    ### Perform Synchronous Inference<br/>    result = executable_net.infer(inputs = {input_blob: image})<br/>    return result</span></pre><p id="d6f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的代码显示了同步推理，input_blob用作字典的键，字典作为参数提供给infer()。infer()返回结果，该结果由函数返回。</p><pre class="jt ju jv jw fd ld le lf lg aw lh bi"><span id="286d" class="li kb hi le b fi lj lk l ll lm"><strong class="le hj">def </strong>asynchronous_inference(executable_net, request_id=0, image):</span><span id="e43d" class="li kb hi le b fi lo lk l ll lm">    ### Get the input blob for the inference request<br/>    input_blob = next(iter(executable_net.inputs))</span><span id="5a06" class="li kb hi le b fi lo lk l ll lm">    ### Perform asynchronous inference<br/>    <!-- -->executable_net<!-- -->.start_async(request_id=request_id, inputs={input_blob: image})</span><span id="e443" class="li kb hi le b fi lo lk l ll lm"><strong class="le hj">    while True</strong>:<br/>        status = <!-- -->executable_net<!-- -->.requests[request_id].wait(-1)<br/>        <strong class="le hj">if</strong> status == 0:<br/>            <strong class="le hj">break</strong><br/>        <strong class="le hj">else</strong>:<br/>            time.sleep(1)<br/>    <strong class="le hj">return</strong> <!-- -->executable_net</span></pre><p id="f8f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的代码展示了异步推理，这里我们提供了推理请求的request_id(在多个推理的情况下)。如上所述，异步推理使用wait()等待推理结果可用，如果结果可用(status=0)，则它退出循环，否则它等待1秒。如果我们调用wait(0)，它将立即返回状态，即使处理没有完成。但是如果我们调用wait(-1)，它将等待进程完成。</p><p id="d506" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，异步推理不会像同步推理那样阻塞主线程。</p><h1 id="a841" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">处理输出</h1><p id="4edb" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">让我们看看如何从异步推理请求中提取结果。</p><pre class="jt ju jv jw fd ld le lf lg aw lh bi"><span id="c510" class="li kb hi le b fi lj lk l ll lm"><strong class="le hj">def </strong>asynchronous_inference(executable_net, request_id=0, image):</span><span id="4f09" class="li kb hi le b fi lo lk l ll lm">    ### Get the output_blob<br/>    output_blob = next(iter(executable_net.outputs))</span><span id="72de" class="li kb hi le b fi lo lk l ll lm">    ### Get the status<br/>    status = executable_net.requests[request_id].wait(-1)<br/>    <br/>    <strong class="le hj">if</strong> status == 0:<br/>        result = executable_net.<!-- -->requests[request_id].outputs[output_blob]<br/>        return result</span></pre><p id="e8ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上所述,“输出”是保存网络模型输出的字典。“output_blob”充当访问特定输出的键。</p></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><p id="0491" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">非常感谢您阅读这篇文章。我希望到现在为止，您已经对推理引擎有了正确的理解。</p></div></div>    
</body>
</html>