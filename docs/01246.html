<html>
<head>
<title>Selfie Filter using Face Land Mark Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用面部地标检测的自拍过滤器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/selfie-filter-using-face-land-mark-detection-54dc43499c61?source=collection_archive---------12-----------------------#2019-10-10">https://medium.com/analytics-vidhya/selfie-filter-using-face-land-mark-detection-54dc43499c61?source=collection_archive---------12-----------------------#2019-10-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a558b93190eb9ce3c2ed7a2e88d3f805.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zVnZk71xvirIgtwpthuxpQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">面部标志</figcaption></figure><p id="44b1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">本文的目的是通过检测人的面部痕迹，在网络摄像头馈送中显示一副太阳镜作为自拍滤镜。</p><p id="4fac" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于面部陆地标记检测，我们将使用来自<em class="js"> Kaggle </em>站点的标记的面部陆地标记数据集(<a class="ae jt" href="https://www.kaggle.com/drgilermo/face-images-with-marked-landmark-points" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/drgilermo/face-images-with-marked-landmark-points</a>)。然后会训练一个卷积神经网络(CNN)来识别人脸陆标。当网络摄像机输入开始时，OpenCV的Haar分类器将用于检测人脸。然后，对于所有的脸，我们将运行我们的CNN来检测这些脸的痕迹。检测后，我们将调整我们的过滤器，以适应我们的脸的大小。然后将人脸图像像素值替换为滤波图像像素值，得到最终结果。</p><p id="964f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们导入库。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="332c" class="kd ke hi jz b fi kf kg l kh ki">%matplotlib inline<br/>import numpy as np <br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span></pre><p id="c164" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">数据都在<em class="js">里。npz </em>文件格式。NPZ文件格式是一个类似字典的压缩存档对象。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="0468" class="kd ke hi jz b fi kf kg l kh ki">data = np.load('face_images.npz')['face_images']<br/>print(data.shape)</span><span id="d571" class="kd ke hi jz b fi kj kg l kh ki">## (96, 96, 7049)</span></pre><p id="8561" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">应该改变数据的形状，以便可以用于训练神经网络。为此，我们移动轴。在<em class="js"> np.moveaxis，</em>第一个参数是矩阵，第二个参数是源，最终轴(-1)和目的，第一个轴(0)。其他轴保持原来的顺序。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="07f5" class="kd ke hi jz b fi kf kg l kh ki">x = np.moveaxis(data, -1, 0)<br/>print(x.shape)</span><span id="ab45" class="kd ke hi jz b fi kj kg l kh ki">##(7049, 96, 96)</span></pre><h1 id="ee27" class="kk ke hi bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">组织X和Y数据</h1><p id="797f" class="pw-post-body-paragraph iu iv hi iw b ix lh iz ja jb li jd je jf lj jh ji jj lk jl jm jn ll jp jq jr hb bi translated">照片的面部标志在'<em class="js">face _ key points . CSV</em>'文件中。熊猫图书馆是用来打开csv文件。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="6c00" class="kd ke hi jz b fi kf kg l kh ki">ys = pd.read_csv('facial_keypoints.csv')<br/>ys_cols = ys.columns.tolist()<br/>print(ys_cols)</span><span id="ada6" class="kd ke hi jz b fi kj kg l kh ki">##['left_eye_center_x', 'left_eye_center_y', 'right_eye_center_x', 'right_eye_center_y', 'left_eye_inner_corner_x', 'left_eye_inner_corner_y', 'left_eye_outer_corner_x', 'left_eye_outer_corner_y', 'right_eye_inner_corner_x', 'right_eye_inner_corner_y', 'right_eye_outer_corner_x', 'right_eye_outer_corner_y', 'left_eyebrow_inner_end_x', 'left_eyebrow_inner_end_y', 'left_eyebrow_outer_end_x', 'left_eyebrow_outer_end_y', 'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y', 'right_eyebrow_outer_end_x', 'right_eyebrow_outer_end_y', 'nose_tip_x', 'nose_tip_y', 'mouth_left_corner_x', 'mouth_left_corner_y', 'mouth_right_corner_x', 'mouth_right_corner_y', 'mouth_center_top_lip_x', 'mouth_center_top_lip_y', 'mouth_center_bottom_lip_x', 'mouth_center_bottom_lip_y']</span></pre><p id="787a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">总共给出了15个标志点的<em class="js"> x，y </em>值。但是没有包括所有的点，这意味着存在一些<em class="js"> NaN </em>值。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="9ef4" class="kd ke hi jz b fi kf kg l kh ki">print(ys.isnull().sum())</span><span id="bd27" class="kd ke hi jz b fi kj kg l kh ki">##left_eye_center_x              10<br/>left_eye_center_y              10<br/>right_eye_center_x             13<br/>right_eye_center_y             13<br/>left_eye_inner_corner_x      4778<br/>left_eye_inner_corner_y      4778<br/>left_eye_outer_corner_x      4782<br/>left_eye_outer_corner_y      4782<br/>right_eye_inner_corner_x     4781<br/>right_eye_inner_corner_y     4781<br/>right_eye_outer_corner_x     4781<br/>right_eye_outer_corner_y     4781<br/>left_eyebrow_inner_end_x     4779<br/>left_eyebrow_inner_end_y     4779<br/>left_eyebrow_outer_end_x     4824<br/>left_eyebrow_outer_end_y     4824<br/>right_eyebrow_inner_end_x    4779<br/>right_eyebrow_inner_end_y    4779<br/>right_eyebrow_outer_end_x    4813<br/>right_eyebrow_outer_end_y    4813<br/>nose_tip_x                      0<br/>nose_tip_y                      0<br/>mouth_left_corner_x          4780<br/>mouth_left_corner_y          4780<br/>mouth_right_corner_x         4779<br/>mouth_right_corner_y         4779<br/>mouth_center_top_lip_x       4774<br/>mouth_center_top_lip_y       4774<br/>mouth_center_bottom_lip_x      33<br/>mouth_center_bottom_lip_y      33<br/>dtype: int64</span></pre><p id="9dbb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">但是对于我们的应用程序，我们只使用<em class="js">左眼中心、右眼中心、鼻尖和嘴中心下唇</em>。因此，我们只从csv文件中提取那些没有这些特性的任何<em class="js"> NaN </em>值的列。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="2f05" class="kd ke hi jz b fi kf kg l kh ki">y_pro = np.nonzero(ys.left_eye_center_x.notna() &amp;     ys.right_eye_center_x.notna() &amp; ys.nose_tip_x.notna() &amp; ys.mouth_center_bottom_lip_x.notna())[0]</span><span id="0fa7" class="kd ke hi jz b fi kj kg l kh ki">print(y_pro.shape)</span><span id="49e0" class="kd ke hi jz b fi kj kg l kh ki">##(7000,)</span><span id="f2e4" class="kd ke hi jz b fi kj kg l kh ki">m = y_pro.shape[0]<br/>size = x.shape[1]<br/>print(m, size)</span><span id="0887" class="kd ke hi jz b fi kj kg l kh ki">##7000 96</span></pre><p id="90b7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="js"> m </em>表示图像的数量，<em class="js">尺寸</em>表示图像一边的像素。</p><h2 id="376c" class="kd ke hi bd kl lm ln lo kp lp lq lr kt jf ls lt kx jj lu lv lb jn lw lx lf ly bi translated">x和Y</h2><p id="8e45" class="pw-post-body-paragraph iu iv hi iw b ix lh iz ja jb li jd je jf lj jh ji jj lk jl jm jn ll jp jq jr hb bi translated"><em class="js"> X </em>的形状应该是<em class="js">(照片数量，照片大小，照片大小，1) </em>，因为所有照片都是1通道图像。<em class="js"> Y </em>的形状应为<em class="js">(照片数，特征数)</em>。因为我们选择了4个地标，所以我们有8个特征来描述图像的<em class="js"> (x，y) </em>位置。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="4fef" class="kd ke hi jz b fi kf kg l kh ki">X = np.zeros((m, size, size, 1))<br/>Y = np.zeros((m, 8))</span></pre><p id="1b51" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们只是用<em class="js"> ys </em>中非零值的照片填充<em class="js"> X </em>。为了标准化，所有值都除以255.0。<em class="js"> Y </em> is值除以图像的大小，这样每个值都将在0和1之间。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="9ceb" class="kd ke hi jz b fi kf kg l kh ki">X[:,:,:,0] = x[y_pro, :, :] / 255.0</span><span id="3d12" class="kd ke hi jz b fi kj kg l kh ki">Y[:, 0] = ys.left_eye_center_x[y_pro] / size<br/>Y[:, 1] = ys.left_eye_center_y[y_pro] / size<br/>Y[:, 2] = ys.right_eye_center_x[y_pro] / size<br/>Y[:, 3] = ys.right_eye_center_y[y_pro] / size<br/>Y[:, 4] = ys.nose_tip_x[y_pro] / size<br/>Y[:, 5] = ys.nose_tip_y[y_pro] / size<br/>Y[:, 6] = ys.mouth_center_bottom_lip_x[y_pro] / size<br/>Y[:, 7] = ys.mouth_center_bottom_lip_y[y_pro] / size</span></pre><h2 id="cc43" class="kd ke hi bd kl lm ln lo kp lp lq lr kt jf ls lt kx jj lu lv lb jn lw lx lf ly bi translated">分割数据集</h2><p id="e025" class="pw-post-body-paragraph iu iv hi iw b ix lh iz ja jb li jd je jf lj jh ji jj lk jl jm jn ll jp jq jr hb bi translated">为了模型验证的目的，x和Y被分开以训练和测试数据集。使用训练集和测试集的精度值，我们可以确定欠拟合或过拟合问题。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="9f47" class="kd ke hi jz b fi kf kg l kh ki">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)<br/>print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)</span><span id="3006" class="kd ke hi jz b fi kj kg l kh ki">##(5600, 96, 96, 1) (1400, 96, 96, 1) (5600, 8) (1400, 8)</span></pre><h1 id="cfac" class="kk ke hi bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">创建CNN模型和培训</h1><p id="13f9" class="pw-post-body-paragraph iu iv hi iw b ix lh iz ja jb li jd je jf lj jh ji jj lk jl jm jn ll jp jq jr hb bi translated">对于模型构建和训练，我们可以使用Keras。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="8c4e" class="kd ke hi jz b fi kf kg l kh ki">from keras import layers</span><span id="a5c9" class="kd ke hi jz b fi kj kg l kh ki">from keras.layers import Input, Dense, Activation, ZeroPadding2D,  BatchNormalization, Flatten, Conv2D</span><span id="fb38" class="kd ke hi jz b fi kj kg l kh ki">from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D</span><span id="a780" class="kd ke hi jz b fi kj kg l kh ki">from keras.models import Model</span></pre><p id="d8e4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">请参考以下<a class="ae jt" href="https://keras.io/layers/core/" rel="noopener ugc nofollow" target="_blank"> <em class="js">链接</em> </a>了解更多关于keras图层的信息</p><p id="4fdb" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这是我们的CNN模型，下面列出了一些重要的事实。</p><ul class=""><li id="b41a" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">使用Keras functional API建立模型，使用输入和输出张量。</li><li id="5e1d" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">脱落层被用来减少过度拟合的机会。</li><li id="0aec" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">展平图层用于将矩阵转换为展平的特征集。</li><li id="cdd0" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">由于所有的<em class="js"> (x，y) </em>值都在0和1之间，所以使用sigmoid作为最后一层的激活函数。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="1ea2" class="kd ke hi jz b fi kf kg l kh ki">def facemodel(input_shape):</span><span id="ea41" class="kd ke hi jz b fi kj kg l kh ki">    X_input = Input(input_shape)<br/>    <br/>    X = Conv2D(32, (3, 3), strides = (1, 1), name = 'conv0')(X_input)<br/>    X = BatchNormalization(axis = 3, name = 'bn0')(X)<br/>    X = Activation('relu')(X)<br/>    <br/>    X = MaxPooling2D((2, 2), name = 'max_pool')(X)<br/>    X = Dropout(0.25)(X)<br/>    <br/>    X = Conv2D(32, (3, 3), strides = (1, 1), name = 'conv1')(X)<br/>    X = BatchNormalization(axis = 3, name = 'bn1')(X)<br/>    X = Activation('relu')(X)<br/>    <br/>    X = MaxPooling2D((2, 2), name = 'max_pool1')(X)<br/>    X = Dropout(0.25)(X)<br/>    <br/>    X = Flatten()(X)<br/>    X = Dense(256, activation = 'tanh', name = 'fc')(X)<br/>    X = Dropout(0.5)(X)<br/>    <br/>    X = Dense(8, activation = 'sigmoid', name = 'fc1')(X)<br/>    <br/>    model = Model(inputs = X_input, outputs = X, name = 'facemodel')<br/>    <br/>    return model</span></pre><p id="c554" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于模型，应提供合适的输入张量。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="645d" class="kd ke hi jz b fi kf kg l kh ki">faceModel = facemodel((96, 96, 1))</span></pre><p id="e679" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">使用<a class="ae jt" href="https://keras.io/optimizers/" rel="noopener ugc nofollow" target="_blank"> <em class="js"> Adam </em> </a>优化器编译模型。<a class="ae jt" href="https://keras.io/losses/" rel="noopener ugc nofollow" target="_blank"> <em class="js">使用均方误差</em> </a> <em class="js"> </em>是因为我们在处理<em class="js"> (x，y) </em>的连续值。我们试图在每次迭代中减少预测值<em class="js"> (x，y) </em>和实际值<em class="js"> (x，y) </em>之间的差异。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="cff5" class="kd ke hi jz b fi kf kg l kh ki">faceModel.compile(optimizer = “adam”, loss = ‘mean_squared_error’, metrics = [‘accuracy’])</span></pre><p id="a488" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">然后使用<em class="js"> X_train </em>和<em class="js"> y_train </em>对模型进行训练。Epochs表示模型在整个数据集中训练的次数。批次大小表示在更新模型参数之前一次发送的图像数量。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="f0b5" class="kd ke hi jz b fi kf kg l kh ki">faceModel.fit(x = X_train, y = y_train, epochs = 3, batch_size = 32)</span><span id="ed63" class="kd ke hi jz b fi kj kg l kh ki">##WARNING:tensorflow:From D:\anaconda\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.<br/>Instructions for updating:<br/>Use tf.cast instead.<br/>Epoch 1/3<br/>5600/5600 [==============================] - 101s 18ms/step - loss: 0.0581 - acc: 0.0805<br/>Epoch 2/3<br/>5600/5600 [==============================] - 97s 17ms/step - loss: 0.0254 - acc: 0.6721<br/>Epoch 3/3<br/>5600/5600 [==============================] - 93s 17ms/step - loss: 0.0086 - acc: 0.8377</span><span id="4644" class="kd ke hi jz b fi kj kg l kh ki">&lt;keras.callbacks.History at 0x24c50562eb8&gt;</span></pre><p id="3788" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">由于精度不够，让我们再次训练模型。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="311b" class="kd ke hi jz b fi kf kg l kh ki">faceModel.fit(x = X_train, y = y_train, epochs = 3, batch_size = 32)</span><span id="b99c" class="kd ke hi jz b fi kj kg l kh ki">##Epoch 1/3<br/>5600/5600 [==============================] - 89s 16ms/step - loss: 0.0029 - acc: 0.9746<br/>Epoch 2/3<br/>5600/5600 [==============================] - 92s 16ms/step - loss: 0.0021 - acc: 0.9921<br/>Epoch 3/3<br/>5600/5600 [==============================] - 90s 16ms/step - loss: 0.0020 - acc: 0.9923</span><span id="a445" class="kd ke hi jz b fi kj kg l kh ki">&lt;keras.callbacks.History at 0x24c525cc358&gt;</span></pre><p id="2afe" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们的训练集精度是0.9923。但是实际的准确性是使用测试集图像获得的。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="fa7c" class="kd ke hi jz b fi kf kg l kh ki">preds = faceModel.evaluate(x = X_test, y = y_test)</span><span id="0024" class="kd ke hi jz b fi kj kg l kh ki">print()<br/>print("Loss = " + str(preds[0]))<br/>print("Test Accuracy = " + str(preds[1]))</span><span id="2fe2" class="kd ke hi jz b fi kj kg l kh ki">##1400/1400 [==============================] - 14s 10ms/step<br/><br/>Loss = 0.0018785889100815568<br/>Test Accuracy = 0.9928571425165449</span></pre><p id="8eb8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们的测试集精度是0.9928。因为两者的精确度都很高，所以这种模式是合适的。由于测试集精度和训练集精度之间的差异非常小，因此该模型也不会过度拟合。</p><p id="0273" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在我们必须保存模型以备将来使用。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="ec4d" class="kd ke hi jz b fi kf kg l kh ki">import os</span><span id="3582" class="kd ke hi jz b fi kj kg l kh ki">faceModel.save_weights('weights.h5', overwrite = True)<br/>open('architecture.json', 'w').write(faceModel.to_json())</span><span id="46cf" class="kd ke hi jz b fi kj kg l kh ki">##5453</span></pre><p id="134a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">程序的第一部分已经完成。</p><h1 id="d5dc" class="kk ke hi bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">添加自拍滤镜</h1><p id="9207" class="pw-post-body-paragraph iu iv hi iw b ix lh iz ja jb li jd je jf lj jh ji jj lk jl jm jn ll jp jq jr hb bi translated">让我们从加载保存的模型开始。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="1bd7" class="kd ke hi jz b fi kf kg l kh ki">import os<br/>%matplotlib inline<br/>import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from keras.models import model_from_json</span><span id="1c7f" class="kd ke hi jz b fi kj kg l kh ki">model = model_from_json(open('architecture.json').read())<br/>model.load_weights('weights.h5')</span></pre><p id="b015" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，我们必须检测网络摄像头馈送中的人脸。为此，我们可以使用Haar Classifer。</p><blockquote class="mn"><p id="edcc" class="mo mp hi bd mq mr ms mt mu mv mw jr dx translated">它们就像我们的卷积核。每个特征是通过从黑色矩形下的像素总和中减去白色矩形下的像素总和而获得的单个值。</p></blockquote><p id="2205" class="pw-post-body-paragraph iu iv hi iw b ix mx iz ja jb my jd je jf mz jh ji jj na jl jm jn nb jp jq jr hb bi translated">更多信息请访问链接。</p><p id="4495" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这里，我们使用<em class="js">Haar scade _ frontal face _ default . XML</em>来检测人脸。<a class="ae jt" href="https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_default.xml" rel="noopener ugc nofollow" target="_blank"> <em class="js">下载</em> </a>该文件并保存在<em class="js">【Haar】</em>文件夹中。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="43d7" class="kd ke hi jz b fi kf kg l kh ki">face_cascade = cv2.CascadeClassifier('haar/haarcascade_frontalface_default.xml')</span></pre><p id="435e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">然后应该读取用于过滤器的<a class="ae jt" href="https://github.com/bimsarapathiraja/Selfie-filter-using-face-landmark-detection/tree/master/images" rel="noopener ugc nofollow" target="_blank"> <em class="js"> png </em> </a>图像。cv2。IMREAD_UNCHANGED用于读取png文件，因为<em class="js"> png </em>文件有四个通道，其中第四个通道用于透明。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="5407" class="kd ke hi jz b fi kf kg l kh ki">sunglasses = cv2.imread(‘images/sunglasses.png’, cv2.IMREAD_UNCHANGED)</span></pre><p id="47ba" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">视频捕捉是通过创建<em class="js">视频捕捉</em>对象来完成的。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="f214" class="kd ke hi jz b fi kf kg l kh ki">camera = cv2.VideoCapture(0)</span></pre><p id="fdd3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面的代码块完成了添加过滤器的全部工作。</p><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="20e1" class="kd ke hi jz b fi kf kg l kh ki">while True:<br/>   <br/>    (grabbed, frame) = camera.read()<br/>    frame = cv2.flip(frame, 1)<br/>    <!-- -->frame2 = np.copy(frame)</span><span id="edf0" class="kd ke hi jz b fi kj kg l kh ki">    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</span><span id="b51e" class="kd ke hi jz b fi kj kg l kh ki">    faces = face_cascade.detectMultiScale(gray, 1.25, 6)</span><span id="10e5" class="kd ke hi jz b fi kj kg l kh ki">    for (x, y, w, h) in faces:</span><span id="cf3d" class="kd ke hi jz b fi kj kg l kh ki">        gray_face = gray[y : y+h, x : x+w]<br/>        color_face = frame[y : y+h, x : x+w]</span><span id="aeb1" class="kd ke hi jz b fi kj kg l kh ki">        gray_norm = gray_face / 255</span><span id="ed89" class="kd ke hi jz b fi kj kg l kh ki">        original_shape = gray_face.shape<br/>        face_resized = cv2.resize(gray_norm, (96, 96), interpolation =               cv2.INTER_AREA)<br/>        face_resized_copy = face_resized.copy()<br/>        face_resized = face_resized.reshape(1, 96, 96, 1)</span><span id="dc13" class="kd ke hi jz b fi kj kg l kh ki">        keypoints = model.predict(face_resized)<br/>        #print(keypoints)</span><span id="f281" class="kd ke hi jz b fi kj kg l kh ki">        keypoints = keypoints * 96</span><span id="284e" class="kd ke hi jz b fi kj kg l kh ki">        face_resized_color = cv2.resize(color_face, (96, 96), interpolation = cv2.INTER_AREA)<br/>        face_resized_color2 = np.copy(face_resized_color)</span><span id="3e61" class="kd ke hi jz b fi kj kg l kh ki">        points = keypoints[0]</span><span id="a406" class="kd ke hi jz b fi kj kg l kh ki">        sunglass_width = int((points[0] - points[2]) * 1.9)<br/>        sunglass_height = int((points[5] - points[1]) * 1.5)</span><span id="0839" class="kd ke hi jz b fi kj kg l kh ki">        a = int(points[2]) - int(sunglass_height / 2)<br/>        b = int(points[2]) + sunglass_height - int(sunglass_height / 2)<br/>        c = int(points[2]) #+ int(sunglass_width / 5)<br/>        d = int(points[2]) + sunglass_width #+ int(sunglass_width / 5)</span><span id="f3b5" class="kd ke hi jz b fi kj kg l kh ki">        sunglass_resized = cv2.resize(sunglasses, (sunglass_width,     sunglass_height), interpolation = cv2.INTER_CUBIC)<br/>        transparent_region = sunglass_resized[:,:,:3] != 0</span><span id="2dd0" class="kd ke hi jz b fi kj kg l kh ki">        face_resized_color[a : b, c : d, :][transparent_region] =  sunglass_resized[:,:,:3][transparent_region]</span><span id="fc91" class="kd ke hi jz b fi kj kg l kh ki">        frame[y:y+h, x:x+w] = cv2.resize(face_resized_color, original_shape, interpolation = cv2.INTER_CUBIC)</span><span id="ecfb" class="kd ke hi jz b fi kj kg l kh ki">        for i in range(0, len(points), 2):<br/>            cv2.circle(face_resized_color2, (points[i], points[i+1]), 1, (0, 255, 0), 1)</span><span id="1e94" class="kd ke hi jz b fi kj kg l kh ki">        frame2[y:y+h, x:x+w] = cv2.resize(face_resized_color2, original_shape, interpolation = cv2.INTER_CUBIC)<br/>        <br/>        cv2.imshow("Selfie filters", frame)<br/>        cv2.imshow("keypoints", frame2)<br/>        <br/>    if cv2.waitKey(1) &amp; 0xFF == ord('q'):<br/>        break<br/>        <br/>camera.release()<br/>cv2.destroyAllWindows()</span></pre><p id="cd94" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我们深入研究代码。</p><ul class=""><li id="bff1" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">为了连续运行代码，使用<code class="du nc nd ne jz b">while True</code></li><li id="9019" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">使用<code class="du nc nd ne jz b">camera.read()</code>线存储图像或帧。</li><li id="da7b" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">网络摄像机图像不是镜像图像。为了让它像镜子一样，我们必须翻转框架</li></ul><p id="2b33" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><code class="du nc nd ne jz b">frame = cv2.flip(frame, 1)</code></p><ul class=""><li id="dbc3" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">由于我们所有的训练图像都是单通道图像，我们必须将帧转换为灰度图像。</li></ul><p id="7956" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><code class="du nc nd ne jz b">gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</code></p><ul class=""><li id="f9b5" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">现在，我们可以使用之前加载的haar分类器进行人脸识别。detectMultiScale用于检测不同大小的不同人脸。它以矩形坐标的形式返回结果。</li></ul><p id="9e32" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><code class="du nc nd ne jz b">faces = face_cascade.detectMultiScale(gray, 1.25, 6)</code></p><ul class=""><li id="4022" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">faces的一个元素是显示包含面<em class="js"> (x，y，w，h) </em>的矩形的坐标。<em class="js"> x </em>和<em class="js"> y </em>是矩形左上角的<em class="js"> (x，y) </em>坐标。<em class="js"> w </em>是矩形的相对宽度。<em class="js"> h </em>是矩形的高度。现在我们可以从帧中只提取面部矩形。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="8a1d" class="kd ke hi jz b fi kf kg l kh ki">gray_face = gray[y : y+h, x : x+w]<br/>color_face = frame[y : y+h, x : x+w]</span></pre><ul class=""><li id="d07a" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">像在模型训练步骤中一样，图像像素值被归一化。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="f2d4" class="kd ke hi jz b fi kf kg l kh ki">gray_norm = gray_face / 255</span></pre><ul class=""><li id="c595" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">接下来，灰度图像被调整到训练图像的大小。之后，图像被转换为首选输入类型。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="30b5" class="kd ke hi jz b fi kf kg l kh ki">face_resized = cv2.resize(gray_norm, (96, 96), interpolation = cv2.INTER_AREA)<br/>face_resized = face_resized.reshape(1, 96, 96, 1)</span></pre><ul class=""><li id="e006" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">现在<em class="js"> face_resized </em>已经准备好输入到模型中。预测将被保存为<em class="js">关键点</em>。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="7268" class="kd ke hi jz b fi kf kg l kh ki">keypoints = model.predict(face_resized)</span></pre><ul class=""><li id="7791" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">由于面部标志是在除以图像大小后给出的，现在我们必须乘以它才能得到图像的真实坐标值。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="24da" class="kd ke hi jz b fi kf kg l kh ki">keypoints = keypoints * 96</span></pre><ul class=""><li id="ae30" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">对于应用程序，我们必须将滤镜添加到彩色图像中。因此，我们必须调整彩色图像的大小来处理给定的关键点。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="c53e" class="kd ke hi jz b fi kf kg l kh ki">face_resized_color = cv2.resize(color_face, (96, 96), interpolation = cv2.INTER_AREA)</span></pre><ul class=""><li id="f89c" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">因为关键点是一个列表中的一个列表，所以点被定义来获得面部标志的列表。<code class="du nc nd ne jz b">points = keypoints[0]</code></li><li id="939f" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">最精彩的部分来了。现在，我们必须根据面部标志的大小来调整过滤器的大小。宽度为<em class="js">(左眼中心到右眼中心的x距离)* 1.9 </em>，高度为<em class="js">(左眼中心到鼻尖的y距离)* 1.5 </em>。无论如何，如果这些值没有超出帧边界，这些值和距离分配方法可以被改变。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="6754" class="kd ke hi jz b fi kf kg l kh ki">sunglass_width = int((points[0] - points[2]) * 1.9)<br/>sunglass_height = int((points[5] - points[1]) * 1.5)</span><span id="0d9b" class="kd ke hi jz b fi kj kg l kh ki">sunglass_resized = cv2.resize(sunglasses, (sunglass_width, sunglass_height), interpolation = cv2.INTER_CUBIC)</span></pre><ul class=""><li id="f46e" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">在调整大小的太阳镜图像中，我们必须忽略透明像素区域。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="e9e9" class="kd ke hi jz b fi kf kg l kh ki">transparent_region = sunglass_resized[:,:,:3] != 0</span></pre><ul class=""><li id="8463" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">现在，我们必须定义一个与调整后的太阳镜图像大小相同的区域来替换不透明区域的像素值。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="1c13" class="kd ke hi jz b fi kf kg l kh ki">a = int(points[2]) - int(sunglass_height / 2)<br/>b = int(points[2]) + sunglass_height - int(sunglass_height / 2)<br/>c = int(points[2]) <br/>d = int(points[2]) + sunglass_width</span><span id="66f1" class="kd ke hi jz b fi kj kg l kh ki">face_resized_color[a : b, c : d, :][transparent_region] =  sunglass_resized[:,:,:3][transparent_region]</span></pre><ul class=""><li id="d342" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">现在我们可以调整框架的大小到它的原始形状</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="dc12" class="kd ke hi jz b fi kf kg l kh ki">frame[y:y+h, x:x+w] = cv2.resize(face_resized_color, original_shape, interpolation = cv2.INTER_CUBIC)</span></pre><ul class=""><li id="4910" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">下面的块用于在另一个窗口中圈出面部标志。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="ec86" class="kd ke hi jz b fi kf kg l kh ki">for i in range(0, len(points), 2):<br/>            cv2.circle(face_resized_color2, (points[i], points[i+1]), 1,   (0, 255, 0), 1)</span></pre><ul class=""><li id="aded" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">现在，我们可以使用过滤器添加的框架来替换原始网络摄像机框架的面部矩形。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="684b" class="kd ke hi jz b fi kf kg l kh ki">frame2[y:y+h, x:x+w] = cv2.resize(face_resized_color2, original_shape, interpolation = cv2.INTER_CUBIC)</span></pre><ul class=""><li id="ed0d" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">使用cv2.imshow函数显示窗口。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="e21a" class="kd ke hi jz b fi kf kg l kh ki">cv2.imshow("Selfie filters", frame)<br/>cv2.imshow("keypoints", frame2)</span></pre><ul class=""><li id="c25e" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">要中断while循环，可按下<em class="js"> q </em>。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="5ad1" class="kd ke hi jz b fi kf kg l kh ki">if cv2.waitKey(1) &amp; 0xFF == ord('q'):<br/>        break</span></pre><ul class=""><li id="e9a8" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">最后，我们不得不释放摄像机并关闭所有窗口。释放相机是重新运行代码的必要条件。</li></ul><pre class="ju jv jw jx fd jy jz ka kb aw kc bi"><span id="d059" class="kd ke hi jz b fi kf kg l kh ki">camera.release()<br/>cv2.destroyAllWindows()</span></pre><p id="2b5b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">您可以使用更多的界标，并使用这些界标来显示更多的过滤器。</p><p id="3508" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">致谢:</p><ul class=""><li id="5dc3" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated"><a class="ae jt" href="https://www.kaggle.com/richardarendsen/face-landmarks-with-cnn" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/richardarendsen/face-landmarks-with-CNN</a></li><li id="ca9c" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated"><a class="ae jt" href="https://towardsdatascience.com/facial-keypoints-detection-deep-learning-737547f73515" rel="noopener" target="_blank">https://towards data science . com/face-key points-detection-deep-learning-737547 f 73515</a></li></ul></div></div>    
</body>
</html>