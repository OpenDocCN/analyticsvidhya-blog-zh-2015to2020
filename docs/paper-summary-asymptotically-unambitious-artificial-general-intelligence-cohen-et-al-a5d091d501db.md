# 论文摘要:渐近无野心的人工通用智能(科恩等人。艾尔。)

> 原文：<https://medium.com/analytics-vidhya/paper-summary-asymptotically-unambitious-artificial-general-intelligence-cohen-et-al-a5d091d501db?source=collection_archive---------26----------------------->

![](img/3c3222f6f50fef2ee8863e98350b4acc.png)

伊万·c·法哈多在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的照片

[科恩、维尔兰比和哈特的一篇优秀论文](https://arxiv.org/abs/1905.12186)于 12 月发表。这篇论文说的很正式，所以我花了一点时间来理解它，但是即使你没有时间去理解这些定理，了解它所说的大概内容也是有用的。这里是一些不太正式的观点。

我将使用一些手写的语言，这不可避免地注入一些我的主观解释。如果你认为你比我更了解这项工作，并且你不同意我说的话，联系我们，我们可以打一架。或者，如果你是(但愿不会如此)作者之一，并且你觉得我说的有些东西是误导性的，让我知道，我可以修正它。

**阅读时间:**~ 10–20 分钟

**假设知识:** [贝叶斯推理](https://towardsdatascience.com/probability-concepts-explained-bayesian-inference-for-parameter-estimation-90e8930e5348)，[强化学习](https://www.freecodecamp.org/news/a-brief-introduction-to-reinforcement-learning-7799af5840db/)

# 介绍

很多人担心如果人工智能成为现实会发生什么。这甚至可能是人类的生存风险。关于这一点已经写了很多东西( [Bostrom](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies) ， [Amodei](https://arxiv.org/abs/1606.06565) ， [80k](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/) ，…)，但是考虑到这个主题的重要性，我们可能需要更多的东西。

不管怎么说，激励这项工作的最好方法是关注 [*的工具性衔接论题*](https://en.wikipedia.org/wiki/Instrumental_convergence) *。一个聪明的代理人会追求获得尽可能多的权力和影响力的工具性目标，而不管它的最终目标是什么。尽可能多做回形针？接管世界，将所有建筑转换成回形针工厂。*

本文提出了一个强化学习(RL)算法(以及一个物理设置),回避了工具收敛论题。他们称之为**麦博**，盒装近视 AI。 **Bo** 部分说它被设计成(经过适当的训练后)只关心它的近邻(∴不会试图影响远处的事物)。 **M** 部分说它(经过适当的训练后)只关心眼前的未来(∴不会计划太远)。然而，它学会了至少像人类一样聪明地做决定。

作者明确表示

> 像现有的 AGI 算法一样，麦博一点也不容易处理。

麦博并不意味着明天就可以实现，它是一个正式的构建，旨在为安全的未来人工通用智能(AGI)系统的设计提供信息。这篇论文中关于麦博安全性的正式结果暗示，将来自麦博的特征结合到未来的代理中可能意味着它们更可能是安全的。

说完这些，让我们看看它是如何工作的。

# 物理设置

麦博住在一个对外界不透明的房间里的电脑上。这个房间应该设计成没有信息可以穿过它的墙壁。有一扇门，操作员可以用它进出。

像在任何 RL 环境中一样，算法会经历几集。当人进入房间时，一个情节开始，当门被打开让人离开时，一个情节结束。门的机械装置连接到电脑上，以确保这一集在门打开的那一刻结束。参见论文中的图 1。

该算法的设计使得人工智能在当前一集中采取的行动只能最大化其收到的奖励*。因此，理论上它没有动机做长期规划。理论上，它也没有影响外部世界的动机，因为它影响外部世界的唯一方式是通过人类离开房间，但是当人类离开房间时，这一集已经结束，奖励已经确定。*

# 该算法

麦博是贝叶斯强化学习者。它经历了许多集，每一集包含固定数量的时间步长。在每集 *i* 的每个时间步*j*；

*   它从可能的动作𝓐的某个有限集合中选择一个动作 *a* 。
*   它从一组可能的观察值中获取对其环境的一些观察值𝓞.
*   它从一组可能的奖励𝓡(基本上只是实数)中获得奖励。

我们将*交互历史*定义为直到当前时间步的所有行为观察和奖励的集合。它属于所有可能的交互历史𝓗*.的集合

**学习**

“环境” *ν* 可以被视为一个随机函数，它接受一个交互历史和一个动作，并返回一个关于观察和回报的概率分布: *ν* : 𝓗*×𝓐 ⇝ 𝓞×𝓡.那么每一个时间步都是这个函数 *ν的一个评价。*⇝表示随机函数，它不是将𝓗*×𝓐的元素映射到𝓞×𝓡的元素，而是将𝓗*×𝓐的元素映射到𝓞×𝓡.的概率分布将环境视为随机函数是理解麦博的一个重要部分。

类似地，代理π的“策略”可以定义为π : 𝓗* ⇝𝓐.给定交互历史，它定义了代理将采取的动作的概率分布。

麦博是贝叶斯学习者。这意味着它跟踪所有可能的世界模型上的概率分布(对什么是 *ν* 的信念)，并根据贝叶斯法则在每个时间步长从𝓞×𝓡𝓗*×𝓐的每次观察后更新该分布。

它还在所有可能的 *πʰ* ∈ 𝓟 *，*上保持概率分布，其中 *πʰ* 是人类操作员的策略。对这一点进行建模的需要将变得很明显。

要做如上所述的任何贝叶斯推断，需要插入一组可能的世界模型𝓜和可能的人类政策𝓟.对于 AGI 来说，这些必须是巨大的。我将在下面的*先验和世界模型*部分说明如何定义这些。

**政策**

我们如何为代理建立策略？在 RL 中，政策必须实现两件事，探索(进入许多不同的情况中学习)和开发(T21)。每一集要么是探索集，要么是剥削集。

在**探索剧集**中，所有的动作都由人类操作员来完成(所以每个动作都来自 *πʰ* )。人工智能是被动的，只在每个时间步观察𝓗*×𝓐和𝓞×𝓡，并以此学习它在𝓜和𝓟.的概率分布

在一集**剥削**中，AI 处于控制中，每个动作都来自它自己的策略 *π*。*它选择 *π** 来最大化这一集(仅&这一集*)的总回报的期望值，给定它在世界模型上的概率分布。*

*它通过以下方式选择是进行探索还是开发。如果有可能获得大量新信息，它选择进入探索阶段。更确切地说；它使用其在𝓜 ×𝓟的当前分布来预测潜在勘探事件，并使用它来预测其在𝓜 ×𝓟的分布将会变成什么。然后，它计算𝓜 ×𝓟上空初始和最终概率分布之间的[*kull back–lei bler 散度*](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=2ahUKEwjfnJGnuuXoAhUuVxUIHTGDBkAQFjABegQIARAB&url=https%3A%2F%2Fen.wikipedia.org%2Fwiki%2FKullback%25E2%2580%2593Leibler_divergence&usg=AOvVaw1WDI6yw_y7O7jNH5aAWXZU) ，这是概率分布之间差异的一个常用度量。然后，它生成 0 和 1 之间的随机数 *ε* ，如果 *ε* 小于 Kullback-Leibler 散度乘以可调参数 *η，则进入探索阶段。**

***属性***

*给定这种算法，作者证明了 4 个定理，关于麦博将如何在无限次数的事件后表现。*

*   **定理 1:有限探索*。在期望中，在无限数量的剧集中进入探索剧集的概率的总和是有限的。换句话说，麦博最终必须完全停止勘探，只做开发。直观的情况是，随着人工智能学习越来越多的环境和人类政策，它可以学习越来越少的新东西，最终零东西。*
*   **定理 2:关于人类政策的最优预测。在∞集之后，根据𝓜上空的麦博分布，具有最大概率的结果总是勘探集的真实结果。这是定理 1 的结果，完美预测意味着探索中没有新的东西要学，所以总是对的。**
*   **定理 3:星上策略最优预测*。与应用于剥削情节的定理 2 相同。这是从定理 1 & 2 推导出来的。*
*   **定理 4:人类水平的智力。在∞集之后，麦博的政策将总是导致每集的总报酬至少与人类操作员的政策一样高。这是根据定理 1-3 得出的。**

*因此，最终不再需要人类操作员，人工智能可以在给定的任务中做得和人类一样好。*

*这篇论文提供了一些直觉，解释了为什么麦博会以相当快的速度接近这个极限。想象一下，例如，任务是开发一种新的癌症治疗方法。在探索剧集中，人类做了一系列的研究，调用了关于这个主题的教科书和研究文章。人类尽最大努力整合所有信息，并试图利用这些信息提出新的治疗方法。麦博接受人类的所有信息，但与人类不同的是，麦博是一个具有照相记忆的贝叶斯代理。它保持了一套数学上严格的关于这个主题的信念。根据荷兰书的论点，麦博采取的行动应该比人类有更好的结果。*

***一个潜在的问题***

*这个设置是这样设计的:第*集第*集的**奖励与第*集第*集第**集麦博的行为所导致的盒子外事件没有因果关系。这是为了确保麦博没有影响外部世界的动机。看看报纸上的图 2。这是真实世界的真实情况，但不幸的是，这种设置不能保证麦博*相信*这是事实，即使它能完美地预测结果，如定理 1-4 所详述。*

*报纸上说*

> *我们刚才提到，当房间的门打开时，第一集的奖励就已经确定了。事实上，它们是由硅制成的。*

*麦博可以发展出下面的世界模型，这与它所观察到的完全一致。它可以相信，有可能影响人在他们离开盒子之后采取某种行动，这导致当前情节的记录的奖励被修改。这将使麦博有理由试图影响外部世界。*

*更一般地说，我们希望防止麦博认为它可以通过外部世界影响当前事件的回报的方式来影响外部世界。这可以通过适当选择𝓜和𝓜.来解决*

# *模型类和先验*

*所以，让我们看看𝓜实际上是什么样子。受索洛莫洛夫归纳推理理论的启发，本文对𝓜进行了界定。回忆𝓜是一个家庭的随机函数𝓗*×𝓐 ⇝ 𝓞×𝓡.将𝓜定义为所有可能的程序家族，这些程序接受𝓗*×𝓐元素，输出𝓞×𝓡.元素现在你明白为什么这是一个难以处理的算法了。*

*先验被设计成有利于世界模型，在这种模型中**外部世界被冻结，直到那一集的回报停止变化**。在奖励不再变化和下一集之间的时间里，允许外界变化。这解决了上面的问题，人工智能不能相信它能以一种有用的方式影响外部世界，因为它不能影响外部世界，直到为时已晚。*

*为了实现这一点，prior 倾向于使用𝓜的程序，这些程序依赖于有限的内存。它依赖的内存越小，它在先验中的权重就越高。假设程序需要相对大量的内存来模拟变化的外部世界，而不是仅仅模拟盒子内部的变化。在学习过程中幸存下来的内存较小的程序必须使用它们宝贵的内存来模拟盒子的内部，以便它们可以准确地预测盒子内部的结果。*

*一般假设，称为*空间需求假设，*是模型所需的内存随着环境的大小而增长。在我看来很合理。*

*为了更精确地定义先验:需要长度为 *l* 的内存的世界模型程序的先验概率与 *βˡ* 成比例，其中*β*∈【0，1】是可调参数。*

*这篇论文继续证明了最后一个定理:*

*   **定理 5:最终的良性。*经过适当调整的 *β* ，有一段插曲之后，麦博给非良性的世界模型分配零概率。*

*良性世界模型被定义为这样一种世界模型，其中 AI 不可能以影响当前情节的奖励的方式影响外部世界。*

# *结论*

*麦博是一个正式的算法+物理设置，最终被证明是没有野心的，最终至少是人类水平的智能。将来当创造 AGI 的实际方法为人所知时，它可能会变得有用。很不舒服。*