<html>
<head>
<title>Journey From Principle Component Analysis To Autoencoders</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从主成分分析到自动编码器的旅程</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/journey-from-principle-component-analysis-to-autoencoders-e60d066f191a?source=collection_archive---------0-----------------------#2018-09-10">https://medium.com/analytics-vidhya/journey-from-principle-component-analysis-to-autoencoders-e60d066f191a?source=collection_archive---------0-----------------------#2018-09-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="96f6" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="a045" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们知道PCA的力量，以及图像压缩、数据分析和监督机器学习等it领域取得的奇迹。PCA用较少数量的线性不相关变量解释相关的多元数据，这些变量是原始变量的线性组合。简而言之，它通过首先捕捉最重要的特征(即具有最高方差的特征)来移除冗余信息。这里有一些有趣的例子，是由<a class="ae kb" href="https://www.youtube.com/watch?v=IbE0tbjy6JQ&amp;list=PLBv09BD7ez_5_yapAg86Od6JeeypkS4YM" rel="noopener ugc nofollow" target="_blank"> Victor </a>提供的。要开始学习PCA的重要性，你可以阅读他的PCA背后的数学系列。首先看两张图片，回答问题，然后看解释。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kc"><img src="../Images/3ec11f6daffa477a209fa1250537185a.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*xaY0TT655d8CUijwFY7Wuw.png"/></div><figcaption class="kk kl et er es km kn bd b be z dx translated">对于某些群体，测量两个属性。数据的维度是什么？</figcaption></figure><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ko"><img src="../Images/de027df860910b695878894b66f34027.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/format:webp/1*f5Z1x4ewJmvkqzaat9Ghhg.png"/></div><figcaption class="kk kl et er es km kn bd b be z dx translated">比方说20x20图像数字数据集，导致400个网格框。数据的维度是什么？</figcaption></figure><p id="107a" class="pw-post-body-paragraph jd je hi jf b jg kp ji jj jk kq jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated"><em class="ku">解释:</em>比方说第一个例子，我们知道<em class="ku"> urefu </em>在斯瓦希里语中是身高的意思。显然不需要两个独立的维度，一个特征就可以了。同样，对于包含400个维度的图像的第二种情况。对于2⁴⁰⁰的位图图像来说，只有很少的可能性是捕捉类似于数字的图像。显然，在这种情况下，维度必须小于400(仅限于少数几个维度)才能捕获有关数字的所有信息。像，只有特定的曲线，一个数字的线存在或不存在。</p><p id="0a57" class="pw-post-body-paragraph jd je hi jf b jg kp ji jj jk kq jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">PCA及其基于方差和协方差计算的统计线性变换在检测这种冗余方面非常成功。从上面的例子中，我们可以清楚地看到需要<strong class="jf hj">一般化机制</strong>，以便在数据集中以特征的形式表达信息。让我们看看PCA在这种归纳中有多成功，导致最重要的特征被首先捕获，以及是否需要更好的机制。</p><h1 id="ab57" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">解释了PCA的威力</h1><p id="3733" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">假设您有一个包含大量要处理的要素的数据集。图像、视频或音频数据大多如此。你不能将我们的常规机器学习模型直接应用于它，你肯定会寻找一些有意义的预处理步骤，这将减少你的训练时间，但仍不会降低数据的表示能力，即准确性/误差权衡必须更少。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kv"><img src="../Images/026f5ea9fe9eda4b789f62f7e8493b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*NCeFDFaFtIc4sejSfnfzvg.png"/></div><figcaption class="kk kl et er es km kn bd b be z dx translated">100 x 100图像[10K尺寸]。数据将具有m x 10k维矩阵</figcaption></figure><p id="202f" class="pw-post-body-paragraph jd je hi jf b jg kp ji jj jk kq jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">与图像相关的数据的特殊之处在于，每个像素都与其相邻像素相关，我们应该利用这一事实。图像、音频和视频数据集通常就是这种情况。这种关系迫使我们得出结论，更高维度没有太多的信息存储在其中，我们可能不需要它们。如果某些维度是稀疏的，我们可以丢弃它们而不会丢失太多信息。现在的问题是选择哪个维度？输入PCA。它选取维度，使得数据在这些维度上呈现出<strong class="jf hj">高方差</strong>，从而获得更多的表示能力。它确保新维度之间的协方差最小化。这将确保可以使用更少的维度来表示数据。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kw"><img src="../Images/ddbd2d147591ecd197b23092914027cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*D-LO8jSgCDBLaOKUZx9d3A.png"/></div><figcaption class="kk kl et er es km kn bd b be z dx translated">来自前16个特征向量结果。</figcaption></figure><p id="279e" class="pw-post-body-paragraph jd je hi jf b jg kp ji jj jk kq jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">以上结果令人难以置信。在没有任何明显损失的情况下，以原始维数的<strong class="jf hj"> 0.16% </strong>重建图像。上面的图像是用构成基础的特征向量的线性组合重建的。想象一下，使用这种简化形式的数据，我们可以在存储和训练模型时分别节省多少空间和时间。但是这和自动编码器有什么关系呢？</p><h1 id="ad08" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">自动编码器如何链接到PCA？</h1><p id="c585" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">嗯，自动编码器是一种特殊类型的前馈神经网络，它将输入<em class="ku"> x </em>编码到隐藏层<em class="ku"> h </em>中，然后从其隐藏表示中解码回来。该模型被训练以最小化输入和输出层的损耗。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kx"><img src="../Images/461c59d6426254b06deffd23231ed931.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*t3n6L6ZWx7pALlnziz_Tyg.png"/></div><figcaption class="kk kl et er es km kn bd b be z dx translated">一个自动编码器，其中dim(h) &lt; dim(xi) is called an under complete autoencoder</figcaption></figure><p id="07b8" class="pw-post-body-paragraph jd je hi jf b jg kp ji jj jk kq jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">Now, let’s say with the help of hidden layer <em class="ku"> h，</em>你能够完美地重构<em class="ku"> xhat </em>，<em class="ku"> h </em>是<em class="ku"> xi的无损编码。它囊括了xi的所有重要特征。</em>与PCA的相似性很明显，h的行为类似于PCA的降维矩阵，从该降维矩阵重构输出，但在值上有一些损失。因此，编码器部分与PCA相似。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es ky"><img src="../Images/5fbb4e69ea0eb5a22fa1118ad3ada189.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*WVskPlog47WFJrWiew3rjw.png"/></div><figcaption class="kk kl et er es km kn bd b be z dx translated">如何实现这种对等？这种等价能有任何用处吗？</figcaption></figure><h1 id="9295" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">哪些条件使autoencoder成为PCA？</strong></h1><p id="5fe2" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">如果使用线性编码器、线性解码器、具有归一化输入的平方误差损失函数，编码器部分将等同于PCA。这意味着PCA仅限于线性映射，而自动编码器则不然。</p><figure class="kd ke kf kg fd kh er es paragraph-image"><div class="er es kz"><img src="../Images/45afa43b08a5e6912ec27082325d391b.png" data-original-src="https://miro.medium.com/v2/resize:fit:540/format:webp/1*yb-cECFzTTRenX9_iBlUoA.png"/></div></figure><p id="6829" class="pw-post-body-paragraph jd je hi jf b jg kp ji jj jk kq jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">由于这些线性约束，我们转向了具有类sigmoid非线性函数的编码器，这在数据重建中给出了更高的精度。参见与之相关的插图。</p><h1 id="ea16" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">对比说明</h1><p id="80ce" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这里是PCA和自动编码器的比较代码，用于以<em class="ku">均方误差</em>作为损失函数来训练MNIST数据集。下图中使用了自动编码器的以下架构784→512→128→2→128→512→784。<em class="ku">在Python 3.x中，您需要安装TensorFlow和Keras来实现这个代码演示。</em></p><p id="588d" class="pw-post-body-paragraph jd je hi jf b jg kp ji jj jk kq jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated"><strong class="jf hj">设置</strong></p><pre class="kd ke kf kg fd la lb lc ld aw le bi"><span id="215f" class="lf ig hi lb b fi lg lh l li lj">import numpy as np<br/>import keras<br/>from keras.datasets import mnist<br/>from keras.models import Sequential, Model<br/>from keras.layers import Dense<br/>from keras.optimizers import Adam<br/><br/>(x_train, y_train), (x_test, y_test) = mnist.load_data()<br/>x_train = x_train.reshape(60000, 784) / 255<br/>x_test = x_test.reshape(10000, 784) / 255</span></pre><p id="fc94" class="pw-post-body-paragraph jd je hi jf b jg kp ji jj jk kq jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated"><strong class="jf hj"> PCA </strong></p><pre class="kd ke kf kg fd la lb lc ld aw le bi"><span id="0974" class="lf ig hi lb b fi lg lh l li lj">mu = x_train.mean(axis=0)<br/>U,s,V = np.linalg.svd(x_train - mu, full_matrices=False)<br/>Zpca = np.dot(x_train - mu, V.transpose())<br/><br/>Rpca = np.dot(Zpca[:,:2], V[:2,:]) + mu    # reconstruction<br/>err = np.sum((x_train-Rpca)**2)/Rpca.shape[0]/Rpca.shape[1]<br/>print('PCA reconstruction error with 2 PCs: ' + str(round(err,3)));</span></pre><p id="6b43" class="pw-post-body-paragraph jd je hi jf b jg kp ji jj jk kq jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated"><strong class="jf hj">自动编码器</strong></p><pre class="kd ke kf kg fd la lb lc ld aw le bi"><span id="c58a" class="lf ig hi lb b fi lg lh l li lj">m = Sequential()<br/>m.add(Dense(512,  activation='elu', input_shape=(784,)))<br/>m.add(Dense(128,  activation='elu'))<br/>m.add(Dense(2,    activation='linear', name="bottleneck"))<br/>m.add(Dense(128,  activation='elu'))<br/>m.add(Dense(512,  activation='elu'))<br/>m.add(Dense(784,  activation='sigmoid'))<br/>m.compile(loss='mean_squared_error', optimizer = Adam())<br/>history = m.fit(x_train, x_train, batch_size=128, epochs=5, verbose=1, validation_data=(x_test, x_test))</span><span id="5604" class="lf ig hi lb b fi lk lh l li lj">encoder = Model(m.input, m.get_layer('bottleneck').output)<br/>Zenc = encoder.predict(x_train)  # bottleneck representation<br/>Renc = m.predict(x_train)        # reconstruction</span></pre><p id="51bb" class="pw-post-body-paragraph jd je hi jf b jg kp ji jj jk kq jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">将所有激活函数改为<code class="du ll lm ln lb b">activation='linear'</code>，观察损耗如何精确收敛到PCA损耗。这是因为如上所述，线性自动编码器等效于PCA。</p><h1 id="90d1" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结论</h1><p id="dcd6" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">上面的理论和例子给出了为什么需要从PCA转移到自动编码器的想法。提出了与PCA中的线性变换相比的非线性函数的情况。就其功能而言，普通编码器毫无疑问是原始的，即使在MNIST这样的数据集上，稀疏或去噪的自动编码器也会在过度完整的自动编码器的情况下占上风。但是，非线性激活函数的基础仍然为深度学习模型超越其他常规ML模型铺平了道路。谢谢！！</p><h1 id="4dce" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">承认</h1><p id="c438" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我只是触及了表面&amp;试图解释这个概念的基础，要深入了解理论，请参考CS7015(DL，IITM)，要了解实现的细节，请参考<a class="ae kb" href="https://stats.stackexchange.com/questions/190148/building-an-autoencoder-in-tensorflow-to-surpass-pca" rel="noopener ugc nofollow" target="_blank">下面由</a><a class="ae kb" href="https://stats.stackexchange.com/users/28666/amoeba" rel="noopener ugc nofollow" target="_blank"> amoeba </a>编写的线程。</p></div></div>    
</body>
</html>