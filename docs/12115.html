<html>
<head>
<title>Explained Deep Sequence Modeling with RNN and LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">与RNN和LSTM一起解释深度层序建模</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/explained-deep-sequence-modeling-with-rnn-and-lstm-e71521f86036?source=collection_archive---------5-----------------------#2020-12-31">https://medium.com/analytics-vidhya/explained-deep-sequence-modeling-with-rnn-and-lstm-e71521f86036?source=collection_archive---------5-----------------------#2020-12-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/cfcba83f62ec8a2c6e35cf59cb6f7c86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_yY3Fdrn68ooXww9NE1JuQ.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">照片来自<a class="ae it" href="https://unsplash.com/photos/505eectW54k" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><p id="2361" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">嘿，你好！😃</p><p id="dfca" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">你难道不好奇知道……<em class="js"><br/>谷歌是怎么把德语翻译成法语的吗？【Alexa和Siri是如何如此恰当地互动的？<br/>或者💭<br/>ML怎样才能谱出一首清新的新歌？</em></p><p id="92ae" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">拿着你的咖啡。疏导你的思想，让我们克服你的好奇心。😏</p><h1 id="48c5" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">路标</h1><ul class=""><li id="fcc9" class="kr ks hh iw b ix kt jb ku jf kv jj kw jn kx jr ky kz la lb bi translated">序列输入</li><li id="d102" class="kr ks hh iw b ix lc jb ld jf le jj lf jn lg jr ky kz la lb bi translated">什么是序列建模<br/>为什么不是前馈或者CNN？</li><li id="fba3" class="kr ks hh iw b ix lc jb ld jf le jj lf jn lg jr ky kz la lb bi translated">数列问题的算法<br/> RNN <br/> LSTM</li><li id="6164" class="kr ks hh iw b ix lc jb ld jf le jj lf jn lg jr ky kz la lb bi translated">具有序列的架构变体</li><li id="5d79" class="kr ks hh iw b ix lc jb ld jf le jj lf jn lg jr ky kz la lb bi translated">履行</li><li id="12c4" class="kr ks hh iw b ix lc jb ld jf le jj lf jn lg jr ky kz la lb bi translated">现实生活的应用</li></ul></div><div class="ab cl lh li go lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ha hb hc hd he"><h1 id="ee31" class="jt ju hh bd jv jw lo jy jz ka lp kc kd ke lq kg kh ki lr kk kl km ls ko kp kq bi translated">序列输入</h1><p id="4fbc" class="pw-post-body-paragraph iu iv hh iw b ix kt iz ja jb ku jd je jf lt jh ji jj lu jl jm jn lv jp jq jr ha bi translated">在ML中，我们通常处理用于预测的训练/测试数据，其中样本的顺序或位置不重要。当需要排序时，相同的数据就成为一个序列。序列作为输入在现实世界中是无处不在的。医院里的音乐、人类语言甚至心电图都是有顺序的。基于序列的预测称为序列建模预测。</p><h1 id="2dad" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">什么是序列建模</h1><p id="35e9" class="pw-post-body-paragraph iu iv hh iw b ix kt iz ja jb ku jd je jf lt jh ji jj lu jl jm jn lv jp jq jr ha bi translated">序列建模是<strong class="iw hi">根据前面的实体预测输入序列(单词/字母)中的下一个实体</strong>的任务。深度学习中的普通ANN或CNN不适合这样的输入，这引发了RNN、LSTM、变形金刚等算法的出现。</p><h2 id="1330" class="lw ju hh bd jv lx ly lz jz ma mb mc kd jf md me kh jj mf mg kl jn mh mi kp mj bi translated">为什么不是前馈或者CNN？</h2><p id="3cf2" class="pw-post-body-paragraph iu iv hh iw b ix kt iz ja jb ku jd je jf lt jh ji jj lu jl jm jn lv jp jq jr ha bi translated">无法根据前馈或CNN架构中的前一个实体预测下一个实体，如下所示:</p><ul class=""><li id="86a9" class="kr ks hh iw b ix iy jb jc jf mk jj ml jn mm jr ky kz la lb bi translated"><strong class="iw hi">独立输出:</strong>输出独立于之前的输出。<em class="js">说</em>，预测狗不依赖于更早的预测输出，猫。</li><li id="ac1d" class="kr ks hh iw b ix lc jb ld jf le jj lf jn lg jr ky kz la lb bi translated"><strong class="iw hi">固定长度输入:</strong>输入的长度是相同的，其中我们固定输入层中神经元的数量以进行结转。</li><li id="b129" class="kr ks hh iw b ix lc jb ld jf le jj lf jn lg jr ky kz la lb bi translated"><strong class="iw hi">不可共享的参数:</strong>不共享不同职位之间学习到的特性。</li></ul><p id="f8e0" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果还是不清楚也不用担心。要了解更多还有很长的路要走...</p><h1 id="9bb2" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">序列问题的算法</h1><p id="c0f2" class="pw-post-body-paragraph iu iv hh iw b ix kt iz ja jb ku jd je jf lt jh ji jj lu jl jm jn lv jp jq jr ha bi translated">基本上，可以处理输入序列的算法有RNN、LSTM、变形金刚。考虑到博客的范围，我们将研究RNN和LSTM。</p><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es mn"><img src="../Images/6ef75830c3939037be1778a468fd7fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*O-EHly1kZA6aEnnV50CBQQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图1</figcaption></figure><h1 id="ab1f" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">RNN(递归神经网络)</h1><p id="f5d2" class="pw-post-body-paragraph iu iv hh iw b ix kt iz ja jb ku jd je jf lt jh ji jj lu jl jm jn lv jp jq jr ha bi translated">与普通的人工神经网络不同，RNN将多个人工神经网络链接起来，以便跟踪以前的输出。当前时间步长的输出充当下一个时间步长的输入。在需要记忆先前输入的情况下，必须基于过去的输入进行预测。因此，RNN有“<strong class="iw hi">隐藏状态</strong>”，作为所有信息计算的<strong class="iw hi">存储器</strong>。</p><h2 id="463b" class="lw ju hh bd jv lx ly lz jz ma mb mc kd jf md me kh jj mf mg kl jn mh mi kp mj bi translated">体系结构</h2><p id="82be" class="pw-post-body-paragraph iu iv hh iw b ix kt iz ja jb ku jd je jf lt jh ji jj lu jl jm jn lv jp jq jr ha bi translated">RNN类似于安，只是链式人工神经网络有所不同。左图显示了展开时的神经网络，其中每个神经网络都有输入层、隐藏状态层和输出层。<strong class="iw hi">相同的权重(W)和偏差(b)在每个时间步</strong>中共享。由于绿色箭头所示的重复模块只有一个tanh层，因此称为循环模块。</p><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es ms"><img src="../Images/a78ded13f95677423b393336959613a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*XjR9M3inWIRVJFvKD9iOdg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图2</figcaption></figure><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es mt"><img src="../Images/fa823d4ad7d847d8dce87dd9537a1106.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*j2zasSH45yrI3UQDslmJPw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图3</figcaption></figure><h2 id="7907" class="lw ju hh bd jv lx ly lz jz ma mb mc kd jf md me kh jj mf mg kl jn mh mi kp mj bi translated">RNN背后的数学</h2><p id="f079" class="pw-post-body-paragraph iu iv hh iw b ix kt iz ja jb ku jd je jf lt jh ji jj lu jl jm jn lv jp jq jr ha bi translated"><strong class="iw hi">前馈传播</strong></p><p id="3425" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在MLP(图4)，输出是通过将权重与当前输入相乘并加上偏差来表示的。RNN也是如此，如图6所示。</p><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es mu"><img src="../Images/aad6a39a3ee14c386abdaf16aa5f554c.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*UiJr_zpChhf0LibjZm6oHw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图4</figcaption></figure><p id="6623" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><em class="js">(参考等式1) </em>考虑时间步长=2，当传递给激活函数时，使用权重、当前输入和偏置来计算隐藏状态。它的输出、权重和传递到激活的偏差导致了我们的目标预测。</p><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es mv"><img src="../Images/97455ac8aa9560430d6fc3f7e806fc3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*s1jG6GlOpSOxBmfrTvVuxg.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图5</figcaption></figure><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es mw"><img src="../Images/9abd8ba728f781fd2378e4167e249988.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*Jm05DR1aHIRdjr40SxPYlA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Eq1</figcaption></figure><p id="9f73" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi">反向传播</strong></p><p id="7d85" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在到达输出时，在每个时间步长计算损失，因此称为通过时间的<strong class="iw hi">反向传播，然后传播以更新权重，从而最小化误差。</strong></p><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es mx"><img src="../Images/84f1c952d517bf1f600b27adab1f9bcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*NcbfJl6V0ayM6Apu8JTHuA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图6</figcaption></figure><p id="ba78" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">损耗当在每个输出的每个时间步计算时，将这些相加得到总损耗。</p><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es my"><img src="../Images/55d1bffc2d0a767041860cfafb985a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:218/format:webp/1*DcgSoTO1Rfj360EotamhWw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Eq2</figcaption></figure><p id="be2c" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在对于一个长序列，权重将从最后一个时间步反向传播到初始时间步，导致可忽略的权重变化，导致<strong class="iw hi">消失梯度问题</strong>缺点:</p><ul class=""><li id="cdb6" class="kr ks hh iw b ix iy jb jc jf mk jj ml jn mm jr ky kz la lb bi translated">消失梯度问题比爆炸梯度问题更受关注</li><li id="c7da" class="kr ks hh iw b ix lc jb ld jf le jj lf jn lg jr ky kz la lb bi translated">长期依赖</li></ul><p id="0d17" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">在RNN的堆栈上，其他算法解决了它的缺点，接下来我们将了解LSTM。因此，跳过RNN的实现，让我们检查一下LSTM的实现。</p><h1 id="f4ef" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">LSTM(长短期记忆)</h1><p id="2fee" class="pw-post-body-paragraph iu iv hh iw b ix kt iz ja jb ku jd je jf lt jh ji jj lu jl jm jn lv jp jq jr ha bi translated">LSTM是一种RNN，是为了克服长期依赖问题而出现的。它通过细胞状态和门实现了对信息的长时间记忆。让我们来了解两者的架构差异。</p><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es mz"><img src="../Images/4addefc7eae59d22db35c23a1504af1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*13U9b5CvtwNvka8vb_oxbw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图7</figcaption></figure><p id="0b2f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">RNN单元接收隐藏状态(h)和输入(x)的输入，而LSTM单元具有称为单元状态(c)的附加输入。单元状态是存储信息的内部存储器，隐藏状态是进行计算的地方。</p><h2 id="6408" class="lw ju hh bd jv lx ly lz jz ma mb mc kd jf md me kh jj mf mg kl jn mh mi kp mj bi translated">LSTM电池的基本元件</h2><p id="4057" class="pw-post-body-paragraph iu iv hh iw b ix kt iz ja jb ku jd je jf lt jh ji jj lu jl jm jn lv jp jq jr ha bi translated">遗忘门(f):具有Sigmoid <br/>输入门的神经网络(I):具有Sigmoid <br/>候选门的神经网络(g):具有tanh <br/>输出门的神经网络(o):具有Sigmoid <br/>单元状态的神经网络(c):向量<br/>隐藏状态(h):向量</p><h2 id="de2b" class="lw ju hh bd jv lx ly lz jz ma mb mc kd jf md me kh jj mf mg kl jn mh mi kp mj bi translated"><strong class="ak">详细流程</strong></h2><p id="21f0" class="pw-post-body-paragraph iu iv hh iw b ix kt iz ja jb ku jd je jf lt jh ji jj lu jl jm jn lv jp jq jr ha bi translated">如图8所示，输入数据与前一个单元格状态和隐藏状态一起传递。让我们一步一步来理解这个过程。</p><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es na"><img src="../Images/11c9cd4a630e076b5fa30ebd329d63b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*XzdBrNpaDWJClvIpbomHPA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图8</figcaption></figure><ol class=""><li id="036e" class="kr ks hh iw b ix iy jb jc jf mk jj ml jn mm jr nb kz la lb bi translated"><strong class="iw hi">忘记不相关的</strong> : <br/>这里，来自前一时间步(ht-1)的<strong class="iw hi">不相关信息</strong>被移除。该决定由具有范围(0，1)的<strong class="iw hi"> Sigmoid </strong>函数的<strong class="iw hi">遗忘门</strong>做出。然后，将其输出与先前的单元状态相乘，得到必要的信息(cft)。<br/>如果遗忘门的值为0，那么来自单元状态的信息应该被移除。<br/> <em class="js">例如，</em>沙池是印度人。她会多种语言。雪莉是一名国际瑜伽老师。<br/>这里，主语从沙池到雪莱的变化应该被识别，因此“沙池”主语应该被忘记。</li></ol><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es na"><img src="../Images/6dcdd16f2fdeb8a1ff9b0f3d0c570ad8.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*3JlOqKV22Gr9d2dVgYQ63w.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图9</figcaption></figure><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es nc"><img src="../Images/9472a639390dd09b26c7870cbecac8a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*gApZ6sW0kpE7gcVJFlCkEw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Eq3</figcaption></figure><p id="0421" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 2。存储/更新当前单元格状态:</strong> <br/> <strong class="iw hi">决定单元格状态中应该存储什么信息。</strong>先前的单元格信息被传递给<strong class="iw hi">输入门</strong> (it)的<strong class="iw hi">s形函数</strong>。值0表示不需要存储信息。先前的时间步长信息传递到<strong class="iw hi"> tanh层</strong>以创建要添加到当前单元格状态中的候选值。<br/>以上两者相乘的结果加到单元状态时，<strong class="iw hi">当前单元状态用新的必要信息(ct) </strong>更新。</p><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es na"><img src="../Images/ff7393ea6c9d90da74d489cc85acc23d.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*K6M3wdZOCZFFqGda6YmnbA.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图10</figcaption></figure><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es nd"><img src="../Images/da33341b6c445c94413911cca94b333a.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*hgLjQn85j_TSw9s2LDbEYw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Eq4</figcaption></figure><p id="df41" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 3。输出单元状态的某些部分:</strong><br/><strong class="iw hi">输出门</strong>具有<strong class="iw hi"> Sigmoid </strong>功能，决定从单元输出什么。对于值0，隐藏状态(ht-1)不会传递到输出。更新的单元信息(ct)被传递到<strong class="iw hi"> tanh </strong>并乘以输出门的输出，以给出当前时间步长信息(ht)。</p><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es na"><img src="../Images/ee68a8918e987a5e32243a4702825bd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*0ipUX8QqtkImTFDt6_OifQ.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图11</figcaption></figure><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es nc"><img src="../Images/796308f7c845fb71b8eb7f1d61ad279d.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*9Tl6nuCWkAapY4NTeYg5ag.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">Eq5</figcaption></figure><p id="d76f" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">现在，当所需的信息被传递时，如果它是最后一个LSTM单元，则隐藏状态(ht)被传递给<strong class="iw hi"> softmax </strong>函数以给出输出y</p><figure class="mo mp mq mr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ne"><img src="../Images/72f2770147743f84fb80da48f61b89d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J-dBwb_0Cy55IcM2Cz6wzA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图12</figcaption></figure><h2 id="42e9" class="lw ju hh bd jv lx ly lz jz ma mb mc kd jf md me kh jj mf mg kl jn mh mi kp mj bi translated">缺点:</h2><ul class=""><li id="3807" class="kr ks hh iw b ix kt jb ku jf kv jj kw jn kx jr ky kz la lb bi translated">长期依赖问题依然存在</li></ul><p id="c26e" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">根除了这个问题，变形金刚出现了，这将在我的下一篇博客中讨论，<a class="ae it" href="https://medium.com/p/58d952fc476c/edit?source=your_stories_page-------------------------------------" rel="noopener">NLP</a>中深度序列建模的变形金刚时代。</p><h1 id="2f27" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">具有序列的架构变体</h1><p id="3068" class="pw-post-body-paragraph iu iv hh iw b ix kt iz ja jb ku jd je jf lt jh ji jj lu jl jm jn lv jp jq jr ha bi translated">这是基于安德鲁的<a class="ae it" href="https://www.coursera.org/learn/nlp-sequence-models/lecture/fyXnn/bidirectional-rnn" rel="noopener ugc nofollow" target="_blank">课程的学习。</a></p><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es nf"><img src="../Images/a5ffdaf4e6dfc119bd1e0741d5dcec2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*P2xnwPI_CA-_GqA2XsaZuw.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">图13</figcaption></figure><ol class=""><li id="d40c" class="kr ks hh iw b ix iy jb jc jf mk jj ml jn mm jr nb kz la lb bi translated"><strong class="iw hi">一对一</strong> <br/>用于常规ML问题的标准神经网络。</li></ol><figure class="mo mp mq mr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ng"><img src="../Images/dda2efc7e54ae0769bf3bff9cb5b5ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:82/format:webp/1*Mqm0Kajkim92BCImoLtcTw.png"/></div></div></figure><p id="6c2d" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 2。一对多</strong> <br/>一个单一的输入(比如说整数)被给定输出一个序列。<br/>例如，<br/>音乐生成(通过给定流派名称来生成音乐歌词)，<br/>文本生成&amp;图像字幕(将图像放入有序字幕中)</p><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es nh"><img src="../Images/79911b3453fb72661f8cb2d30e765c1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*GJrNEYIpHk5WNB3-FTBOeA.png"/></div></figure><p id="17bb" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 3。多对一</strong> <br/>根据输入序列预测类别标签。<br/>例如，DNA序列分类、异常检测、天气预报、股市预测</p><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es nh"><img src="../Images/c831d3e29b1fe62abcfc1bfa15eb14b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*gvdyxJEv1Md0j4BU0a_DVw.png"/></div></figure><p id="6865" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> 4。多对多</strong> <br/>预测与输入序列长度相同/不同的序列长度。<br/>例如，问答、机器翻译、语音识别</p><figure class="mo mp mq mr fd ii er es paragraph-image"><div class="er es ni"><img src="../Images/192178980486832e59673f174e1291a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/format:webp/1*Jy1MVqOJggaXL-z588qMxg.png"/></div></figure><p id="057b" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">这是基于从<a class="ae it" href="https://machinelearningmastery.com/sequence-prediction/" rel="noopener ugc nofollow" target="_blank">这篇博客</a>中学到的。</p><h1 id="9a17" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">履行</h1><p id="046e" class="pw-post-body-paragraph iu iv hh iw b ix kt iz ja jb ku jd je jf lt jh ji jj lu jl jm jn lv jp jq jr ha bi translated">要进行实际操作，最好从预测股票价格一个非常基本的模型开始。找到在股票价格数据集上实现的LSTM模型，链接如下所示。</p><blockquote class="nj"><p id="d63c" class="nk nl hh bd nm nn no np nq nr ns jr dx translated">GitHub知识库中的LSTM模型</p></blockquote><h1 id="f81c" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke nt kg kh ki nu kk kl km nv ko kp kq bi translated">现实生活的应用</h1><p id="c550" class="pw-post-body-paragraph iu iv hh iw b ix kt iz ja jb ku jd je jf lt jh ji jj lu jl jm jn lv jp jq jr ha bi translated">让我们来看看一些实现序列建模的非常酷的用例。</p><figure class="mo mp mq mr fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es nw"><img src="../Images/8f5a67b0d80ed87a3182e5ef85ad8733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*8k-BMJT4PYHQ3_NrAUuF7Q.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated"><a class="ae it" href="https://www.analyticsvidhya.com/blog/2018/04/sequence-modelling-an-introduction-with-practical-use-cases/" rel="noopener ugc nofollow" target="_blank">表</a></figcaption></figure><h1 id="9291" class="jt ju hh bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">参考</h1><figure class="mo mp mq mr fd ii"><div class="bz dy l di"><div class="nx ny l"/></div></figure><figure class="mo mp mq mr fd ii"><div class="bz dy l di"><div class="nx ny l"/></div></figure></div><div class="ab cl lh li go lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ha hb hc hd he"><p id="d957" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated">如果你喜欢这位作者的博客，请随意关注，因为这位作者保证会带来更多有趣的人工智能相关内容。<br/> 感谢，<br/>学习愉快！😄</p><p id="25a4" class="pw-post-body-paragraph iu iv hh iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr ha bi translated"><strong class="iw hi"> <em class="js">可以通过</em></strong><a class="ae it" href="https://www.linkedin.com/in/kaul-shachi" rel="noopener ugc nofollow" target="_blank"><strong class="iw hi"><em class="js">LinkedIn</em></strong></a><strong class="iw hi"><em class="js">取得联系。</em> </strong></p></div></div>    
</body>
</html>