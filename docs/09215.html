<html>
<head>
<title>Gradient Descent Optimization Techniques.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降优化技术。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-optimization-techniques-4316419c5b74?source=collection_archive---------9-----------------------#2020-08-29">https://medium.com/analytics-vidhya/gradient-descent-optimization-techniques-4316419c5b74?source=collection_archive---------9-----------------------#2020-08-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="885e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降是执行优化的最流行算法之一，也是迄今为止优化神经网络的最常见方法。同时，每个最先进的深度学习库都包含各种算法的实现，以优化梯度下降。这篇博文旨在为你提供关于优化梯度下降的不同算法行为的直觉，这将有助于你使用它们。</p><p id="84f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降是通过在目标函数<strong class="ih hj"> <em class="jd"> ∇.的梯度的相反方向上更新参数来最小化由模型参数参数化的目标函数<strong class="ih hj"><em class="jd">【j(θ)</em></strong>的方法</em>j(θ)</strong>w . r . t .为参数。学习率<strong class="ih hj"> <em class="jd"> η </em> </strong>决定了我们达到(局部)最小值所采取的步长。换句话说，我们沿着由目标函数创建的表面坡度的方向下坡，直到我们到达一个山谷。</p><p id="3444" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度下降有三种变体，不同之处在于我们使用多少数据来计算目标函数的梯度。</p><h1 id="67f4" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">批量梯度下降</h1><p id="b156" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">普通梯度下降，也称为批量梯度下降，计算整个训练数据集的成本函数相对于参数θθ的梯度:</p><blockquote class="kh ki kj"><p id="622f" class="if ig jd ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">θ= θ−η⋅∇J(θ)</p></blockquote><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kn"><img src="../Images/28a9fd3d0eb759f907acc366360f4a3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*lhEF_VbpXHW76p6KI5cycQ.gif"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">批量梯度下降的收敛性</figcaption></figure><p id="2e33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我们需要计算整个数据集的梯度来执行一次更新，所以批量梯度下降可能非常慢，并且对于不适合内存的数据集来说是难以处理的。批量梯度下降也不允许我们在线更新我们的模型，即BGD不会在持续更新的数据集上执行</p><p id="c650" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在代码中，批量梯度下降看起来像这样:</p><pre class="ko kp kq kr fd ld le lf lg aw lh bi"><span id="02c3" class="li jf hi le b fi lj lk l ll lm">for i in range(nb_epochs):<br/>  params_grad = evaluate_gradient(loss_function, data, params)<br/>  params = params - learning_rate * params_grad</span></pre><h1 id="ec2a" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">随机梯度下降</h1><p id="fbab" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">相反，随机梯度下降(SGD)对每个训练示例x(i)x(i)和标签y(i)y(i)执行参数更新:</p><blockquote class="kh ki kj"><p id="3d33" class="if ig jd ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated"><strong class="ih hj"><em class="hi">θ=θ−η⋅∇j(θ；x(一)；y(i)) </em> </strong></p></blockquote><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es ln"><img src="../Images/95345e94736e35fd1f40f05a24d6294b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*B8Yu3Ap__nyOQFbIG2kEGQ.gif"/></div></figure><p id="8cdf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">批量梯度下降对大型数据集执行冗余计算，SGD通过一次执行一次更新来避免这种冗余。因此，它通常要快得多，也可以用于在线学习。<br/> SGD频繁执行高方差更新，导致目标函数大幅波动。</p><pre class="ko kp kq kr fd ld le lf lg aw lh bi"><span id="ac23" class="li jf hi le b fi lj lk l ll lm">for i in range(nb_epochs):<br/>  np.random.shuffle(data)<br/>  for example in data:<br/>    params_grad = evaluate_gradient(loss_function, example, params)<br/>    params = params - learning_rate * params_grad</span></pre><p id="9dcf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">而SGD的波动，一方面，使它能够跳到新的和可能更好的局部最小值。另一方面，这最终会使收敛复杂化到精确的最小值，因为SGD会持续超调。然而，已经表明，当我们缓慢降低学习速率时，SGD显示出与批量梯度下降相同的收敛行为。</p><h1 id="db58" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">小批量梯度下降</h1><p id="86da" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">小批量梯度下降最终取两者之长，并对n个训练示例的每个小批量执行更新:</p><blockquote class="kh ki kj"><p id="9bc7" class="if ig jd ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated"><strong class="ih hj"><em class="hi">θ=θ−η⋅∇j(θ；x(I:I+n)；y(I:I+n))</em>T3】</strong></p></blockquote><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lo"><img src="../Images/119339c4ea79c768aa6db6d5e656f713.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*m4taSeNUZzTiEr-aSv1c3w.gif"/></div></div></figure><p id="3c5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这样，<em class="jd"> a) </em>减小了参数更新的方差，这可以导致更稳定的收敛；和<em class="jd"> b) </em>可以利用高度优化的矩阵优化，这是最先进的深度学习库中常见的，使计算梯度w.r.t .成为非常高效的小批量。常见的小批量大小范围在50到256之间，但可以根据不同的应用而变化。当训练神经网络时，小批量梯度下降通常是选择的算法，并且当使用小批量时，通常也使用术语SGD。注意:在这篇文章后面对SGD的修改中，我们省略了参数x(I:I+n)；y(i:i+n)为简单起见。</p><p id="30ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在代码中，我们现在迭代大小为50的小批量，而不是迭代示例:</p><pre class="ko kp kq kr fd ld le lf lg aw lh bi"><span id="7a4a" class="li jf hi le b fi lj lk l ll lm">for i in range(nb_epochs):<br/>  np.random.shuffle(data)<br/>  for batch in get_batches(data, batch_size=50):<br/>    params_grad = evaluate_gradient(loss_function, batch, params)<br/>    params = params - learning_rate * params_grad</span></pre><h1 id="aca5" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">挑战</h1><p id="183a" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">然而，普通的小批量梯度下降并不能保证良好的收敛性，但也带来了一些需要解决的挑战:</p><ul class=""><li id="10a4" class="lp lq hi ih b ii ij im in iq lr iu ls iy lt jc lu lv lw lx bi translated">选择一个合适的学习速度可能很困难。学习率太小会导致收敛速度非常慢，而学习率太大会阻碍收敛，并导致损失函数在最小值附近波动，甚至发散。</li><li id="d854" class="lp lq hi ih b ii ly im lz iq ma iu mb iy mc jc lu lv lw lx bi translated">此外，相同的学习率适用于所有参数更新。如果我们的数据很稀疏，并且我们的要素具有非常不同的频率，我们可能不希望将所有要素更新到相同的程度，而是对很少出现的要素执行更大的更新。</li></ul><h1 id="9c50" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">梯度下降优化算法</h1><p id="a286" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在下文中，我们将概述一些被深度学习社区广泛使用的算法。以下是最常用的和基础成型优化</p><h1 id="0cb0" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">动力</h1><p id="90bd" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">SGD在通过峡谷时有困难，峡谷是指表面在一个维度上比在另一个维度上弯曲得更陡峭的区域，这在局部最优值附近很常见。在这些场景中，SGD在峡谷的斜坡上振荡，同时沿着底部朝着局部最优值缓慢前进。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es md"><img src="../Images/126b48040ec688f50a47947c4c77f4c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*v-AV_I7Fsn0KucX6ukhwIA.gif"/></div></div></figure><p id="fd11" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">动量是一种有助于在相关方向加速SGD并抑制振荡的方法。这是通过将过去时间步长的更新向量的一部分γ加到当前更新向量来实现的:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es me"><img src="../Images/2bd87e1aa26d9aaa242c4d60c1070104.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zSYBCOk-kLnlXWo52Ca0rw.jpeg"/></div></div></figure><p id="1d29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本质上，当使用动量时，我们把球推下山。球在下坡滚动时积累动量，在途中变得越来越快(如果有空气阻力，即γ&lt;1，则直到达到其极限速度。同样的事情也发生在我们的参数更新上:对于梯度指向相同方向的维度，动量项增加，而对于梯度改变方向的维度，动量项减少。因此，我们获得了更快的收敛和减少振荡。</p><h1 id="78d0" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">内斯特罗夫加速梯度</h1><p id="c436" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">然而，一个滚下山坡的球，盲目地顺着斜坡，是极不令人满意的。我们希望有一个更聪明的球，一个知道自己要去哪里的球，这样它就知道在山坡再次向上倾斜之前减速。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es mf"><img src="../Images/d22f49851d0c9a12588cf9df67fbe867.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/1*ykEJxOIuS2m7DFptyBNIrQ.gif"/></div></figure><p id="69b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">内斯特罗夫加速梯度(NAG)就是一种赋予我们动量项这种先见之明的方法。我们知道，我们将使用动量项γvt1来移动参数θ。因此，计算(θγv 1)可以给出参数下一个位置的近似值，以及参数的大致位置。现在，我们可以通过计算相对于当前参数θθ的梯度，而不是相对于当前参数θ的梯度，来有效地预测未来参数的大致位置:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mg"><img src="../Images/cb35429de96cbeaa8afcd1b314d4928d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wmHwjPwTkQV3trE0ar0T_A.jpeg"/></div></div></figure><p id="a2f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">再次，我们将动量项γγ设置为大约0.9的值。现在，我们能够使我们的更新适应我们的误差函数的斜率，并依次加速SGD，我们还想使我们的更新适应每个单独的参数，以根据它们的重要性执行更大或更小的更新。</p><h1 id="f91d" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">阿达格拉德</h1><p id="f480" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">Adagrad是一种基于梯度的优化算法，它可以做到这一点:它使学习率适应参数，对与频繁出现的特征相关联的参数执行较小的更新<br/>(即低学习率)，对与不频繁出现的特征相关联的参数执行较大的更新(即高学习率)。因此，它非常适合处理稀疏数据。研究人员发现，Adagrad极大地提高了SGD的鲁棒性，并将其用于训练谷歌的大规模神经网络，其中包括学习识别Youtube视频中的<a class="ae mh" href="https://www.wired.com/2012/06/google-x-neural-network/" rel="noopener ugc nofollow" target="_blank">猫。此外，Pennington使用Adagrad来训练手套单词嵌入，因为不常用的单词比常用的单词需要更多的更新。</a></p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mi"><img src="../Images/963d7ae05bd557204622de7ec34b371f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*qzpsPwaJo0OX96o670ITVQ.gif"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">Adagrad优化技术</figcaption></figure><p id="e024" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">之前，我们一次对所有参数θ进行更新，因为每个参数θi使用相同的学习速率η。由于Adagrad在每个时间步长t对每个参数θi使用不同的学习速率，</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mj"><img src="../Images/754a3911bae35f5c00466377f997568d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X3mFdMlmMliEKsg32X5ygg.jpeg"/></div></div></figure><p id="731d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">优点</strong>-Adagrad的主要优点之一是它消除了手动调整学习速度的需要。大多数实现使用默认值0.01，并保持不变。</p><p id="3828" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Adagrad的主要弱点是它在分母中平方梯度的累积:因为每个增加的项都是正的，累积和在训练期间保持增长。这进而导致学习速率收缩，并最终变得无穷小，此时算法不再能够获得额外的知识。以下算法旨在解决这一缺陷。</p><h1 id="2a4b" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">阿达德尔塔</h1><p id="4096" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">Adadelta是Adagrad的扩展，它试图降低其激进的、单调递减的学习速率。不是累积所有过去的平方梯度，而是将累积的过去梯度的窗口限制到某个固定大小w</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es mk"><img src="../Images/3b3989922d068ecf9238f7f823fa213e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*k-uQ-Gb4FZse1Fr0jBP-sw.png"/></div><figcaption class="kz la et er es lb lc bd b be z dx translated">阿达三角洲上升</figcaption></figure><p id="3589" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度的和被递归地定义为所有过去的平方梯度的衰减平均值，而不是低效地存储w个先前的平方梯度。时间步长t处的移动平均值E[g2](t)然后仅取决于(作为类似于动量项的分数γ)先前平均值和当前梯度:</p><p id="72da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Adadelta更新规则:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mj"><img src="../Images/1061e2c27a3d2e8267ea6103e9c01ea0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ngZj4Sm280CvVZohfrLVug.jpeg"/></div></div></figure><p id="8272" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用Adadelta，我们甚至不需要设置默认的学习速率，因为它已经从更新规则中删除了。</p><h1 id="ef81" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">RMSprop</h1><p id="ea7b" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">RMSprop是Geoff Hinton(也被称为深度科学之父)提出的一种未发表的自适应学习速率方法。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ml"><img src="../Images/4a1a042f394978de64e45265b318a2f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*qoNZTsyaQppaZ41LRG70zg.gif"/></div></div></figure><p id="7dc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RMSprop和Adadelta都是在同一时间独立开发的，因为需要解决Adagrad的学习率急剧下降的问题。RMSprop实际上与我们上面导出的Adadelta的第一个更新向量相同:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mm"><img src="../Images/af730d93a52240aac6e9a4b206e424db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MgMry6-A-vXvlIQjcHQMSg.jpeg"/></div></div></figure><p id="f072" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RMSprop也将学习率除以梯度平方的指数衰减平均值。Hinton建议将γ设置为0.9，而学习率η的一个好的默认值是0.001。</p><h1 id="f753" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</h1><p id="da31" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">自适应矩估计(Adam)是另一种计算每个参数的自适应学习率的方法。除了存储过去平方梯度v(t)的指数衰减平均值(如Adadelta和rms prop ), Adam还保存过去梯度mtmt的指数衰减平均值，类似于动量。动量可以被视为一个沿斜坡向下运行的球，而亚当的行为就像一个有摩擦力的重球，因此更喜欢误差面上平坦的极小值。Adam在差异较小的数据集上表现最佳。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mn"><img src="../Images/5c32554838e04c8d3a574725ec61ebfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tDV5KTcf11yv4gWm43yj6Q.jpeg"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">稀疏数据集上的adam</figcaption></figure><p id="d5c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">m(t)和v(t)分别是梯度的一阶矩(均值)和二阶矩(无中心方差)的估计值，因此该方法得名。由于m(t)和v(t)被初始化为0的向量，Adam的作者观察到它们偏向于0，尤其是在初始时间步长期间，尤其是当衰减率较小时(即β1和β2接近1)。</p><p id="2fa8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，它们使用这些来更新参数，就像我们在Adadelta和RMSprop中看到的那样，这产生了Adam更新规则:</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es mj"><img src="../Images/32063a93f2adc6b8bd6c9c27bab421d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xfMvulOdx-1ffqQHVqD6QQ.jpeg"/></div></div></figure><p id="6271" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作者提出了β1=0.9、β2=0.999和ϵ=10^-8.的默认值</p><blockquote class="kh ki kj"><p id="a01e" class="if ig jd ih b ii ij ik il im in io ip kk ir is it kl iv iw ix km iz ja jb jc hb bi translated">他们展示了Adam在实践中的表现，并与其他自适应学习算法相比较。</p></blockquote><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es ln"><img src="../Images/ae19dc98940b0207972587d4ae8ea7a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*kqPSj6ylpvMf26rAiX7m6g.jpeg"/></div></figure><h1 id="155a" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak">鞍点不同优化器对比</strong></h1><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es mo"><img src="../Images/0ee0f2805dbe37b54e6558438b34ea39.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*okxb3iEckjbqI9dTSztOSg.gif"/></div></figure><p id="e880" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望每个人都了解基础知识，并且必须知道优化器功能的直觉。</p><p id="d3c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于任何建议联系-【ayushpradhan181998@gmail.com】<strong class="ih hj"><em class="jd"/></strong></p></div></div>    
</body>
</html>