<html>
<head>
<title>Deep RL: a Model-Based approach (part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度 RL:基于模型的方法(第 1 部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-rl-a-model-based-approach-part-1-drl-doesnt-really-work-yet-9971c5ccb4d6?source=collection_archive---------21-----------------------#2020-11-30">https://medium.com/analytics-vidhya/deep-rl-a-model-based-approach-part-1-drl-doesnt-really-work-yet-9971c5ccb4d6?source=collection_archive---------21-----------------------#2020-11-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="ddaa" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">不要猜测你的下一步行动。计划一下！</h2><div class=""/><p id="da82" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">深度强化学习并没有真正起作用…目前还没有</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es jm"><img src="../Images/2b994f446ab2bb4bf6470badc5148a21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZUPJd6hUB-Jf71dIRd-NTg.jpeg"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">图片来自:<a class="ae kc" href="https://unsplash.com/@ninjason" rel="noopener ugc nofollow" target="_blank">梁朝伟</a>——<a class="ae kc" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><p id="5a8b" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">使用<strong class="iq hs">深度强化学习(RL) </strong>，我们可以训练一个代理来解决一个任务，而无需显式编程。这种方法非常普遍，原则上，我们可以将其应用于任何顺序决策。例如，在 2015 年，一个研究团队开发了一种叫做<strong class="iq hs"> DQN </strong>的<strong class="iq hs"> DRL </strong>算法来玩雅达利游戏。他们在 57 个不同的游戏中使用相同的方法，每个游戏都有特定的目标要实现，有特殊的敌人，和不同的特工行动。他们的代理学习解决许多游戏。在某些情况下，它甚至达到了更好的人类水平的性能。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es kd"><img src="../Images/b364afad5979fa4d3f0f313949bd8661.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*kslDDPBLG_JMZm1vwA-wNA.gif"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">一些 DQN 代理商玩的雅达利游戏的例子。来源:<a class="ae kc" href="https://syncedreview.com/" rel="noopener ugc nofollow" target="_blank">同步</a> — <a class="ae kc" href="https://syncedreview.com/2020/01/08/slm-lab-new-rl-research-benchmark-software-framework/" rel="noopener ugc nofollow" target="_blank"> SLM 实验室:新 RL 研究基准&amp;软件框架</a></figcaption></figure><p id="4c1c" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">DRL 社区最近取得了令人难以置信的成绩。比如 2016 年，<strong class="iq hs"> Deepmind </strong>成功训练一名 DRL 特工击败围棋世界冠军。</p><p id="553c" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">2019 年<strong class="iq hs"> Open AI </strong>发布<strong class="iq hs"> OpenAI Five </strong>:第一个能够在名为 Dota 2 的电子竞技游戏中击败世界冠军的 AI。同年，Open AI 还训练了一只真实世界的机器人手来解决一个魔方。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es ke"><img src="../Images/03e401f343378e62b6a490f69ed8212c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*600S-Nb_tG0MLhvcFv96Ag.gif"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">来源:Youtube — <a class="ae kc" href="https://youtu.be/kVmp0uGtShk" rel="noopener ugc nofollow" target="_blank">用机械手解魔方:未切割</a></figcaption></figure><p id="99cc" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">然而，DRL 也显示出严重的局限性。其中之一是，在学习一个好的策略之前，算法需要太多的交互。这个问题叫做<strong class="iq hs">样本低效</strong>。为了更好地理解这个问题，我们来看一个实际的例子。</p><blockquote class="kf kg kh"><p id="56a7" class="io ip ki iq b ir is it iu iv iw ix iy kj ja jb jc kk je jf jg kl ji jj jk jl hb bi translated">但是首先，<strong class="iq hs">试着猜一猜:</strong>一个 DQN 特工需要多少经验才能达到雅达利游戏中人类的表现？(对于“体验”这个词，我们指的是游戏中执行的每一个动作。由于代理为每一帧选择一个新的动作，我们将其经验计算为接收帧的数量。)</p></blockquote><p id="abd8" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">让我们看下面的情节来得到一些提示:</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="er es km"><img src="../Images/e79ccadee832231f3281ceefe810c047.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kXCetoJrgvgD_fPGNDIVkg.png"/></div></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">来源:Rainbow: <a class="ae kc" href="https://arxiv.org/pdf/1710.02298.pdf" rel="noopener ugc nofollow" target="_blank">结合深度强化学习的改进</a>，arXiv</figcaption></figure><p id="4d01" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">在上面的图中，我们可以看到原始 DQN 架构获得的结果，用灰色线表示(暂时忽略所有其他算法)。在 x 轴上，我们有所需的帧(注意，它们是数亿帧)。在 y 轴上，我们有“人类标准化得分中值”，这是从相应的 Atari 游戏中获得的所有 57 个得分的平均值，以 100%匹配人类得分的方式标准化。Deepmind 为每个呈现的算法重复这个过程。</p><blockquote class="kf kg kh"><p id="ba80" class="io ip ki iq b ir is it iu iv iw ix iy kj ja jb jc kk je jf jg kl ji jj jk jl hb bi translated"><strong class="iq hs">答案:</strong>从这个图我们可以看出，原来的 DQN 算法需要上亿帧。这也表明，即使它在一些游戏中超越了人类的表现，但在一般情况下并不如此。</p></blockquote><p id="b363" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">自 2015 年以来，研究人员提出了许多改进措施。它们中的一些在前面的图中是可见的，每一个都有不同的颜色。2017 年，来自 Deepmind 的一些研究人员向<em class="ki">展示了如何将它们结合起来以达到最佳效果。DQN 的这个新版本叫做</em> <a class="ae kc" href="https://arxiv.org/pdf/1710.02298.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="iq hs"> <em class="ki">彩虹</em> </strong> </a> : <em class="ki">它在 700 万帧后就克服了原版，</em> <strong class="iq hs"> <em class="ki">在 1800 万帧后就达到了一般人类的性能</em> </strong> <em class="ki">，在 4400 万帧后克服了其他所有基线。</em></p><p id="78de" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">所有游戏每秒提供 60 帧，因此 1800 万帧对应于大约 83 小时的游戏体验。<em class="ki">注意，83 小时只是玩的时间的近似值；完整的训练需要更多的东西！</em></p><blockquote class="kf kg kh"><p id="72a9" class="io ip ki iq b ir is it iu iv iw ix iy kj ja jb jc kk je jf jg kl ji jj jk jl hb bi translated">考虑到大多数人只需要几分钟就能获得雅达利游戏的自信，83 个小时是一段很长的时间。</p></blockquote><p id="fd8d" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">有了更复杂的游戏，情况就更糟了:<a class="ae kc" href="https://openai.com/blog/openai-five/" rel="noopener ugc nofollow" target="_blank"><em class="ki">“open ai Five 每天和自己对抗 180 年的游戏，通过自我游戏学习。”</em> </a></p><p id="9082" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">出于这个原因，DRL 最近的成功故事大多与视频游戏有关:研究人员使用虚拟环境来加快甚至瘫痪训练时间。</p><blockquote class="kf kg kh"><p id="7b3f" class="io ip ki iq b ir is it iu iv iw ix iy kj ja jb jc kk je jf jg kl ji jj jk jl hb bi translated">在合理的时间内，在没有硬件磨损的情况下，用真实世界的机器人进行数百万次实验是不现实的。</p></blockquote><p id="1ed0" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">研究人员甚至为解决魔方的真实世界机器人使用模拟器。在这种情况下，他们使用模拟器预先训练代理。这种方法可能是一种可能的解决方案，但理解如何将学到的策略可靠地转移到现实世界仍然是一个开放的研究问题。此外，每次我们需要为一项任务训练一个智能体时都构建一个模拟器是不可行的。</p><blockquote class="kf kg kh"><p id="a2a3" class="io ip ki iq b ir is it iu iv iw ix iy kj ja jb jc kk je jf jg kl ji jj jk jl hb bi translated">这就是为什么没有办法训练一个真实世界的智能体在合理的时间内解决重要的问题。</p></blockquote><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es kn"><img src="../Images/03a62b3abb31b3284b2d18a491f45b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*fx1jB9_4ytJNQGi_BOzALg.gif"/></div><figcaption class="jy jz et er es ka kb bd b be z dx translated">来源:YouTube — <a class="ae kc" href="https://youtu.be/b7oJSxujWIM" rel="noopener ugc nofollow" target="_blank">通过无监督的非策略强化学习，涌现出真实世界的机器人技能</a></figcaption></figure><p id="eab5" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">在下一篇文章中，<a class="ae kc" rel="noopener" href="/@enrico-busto/deep-rl-a-model-based-approach-part-2-drl-explained-837591ffadaa"> <strong class="iq hs">深度 RL:基于模型的方法(第二部分)</strong> </a>，我们将看到强化学习是如何工作的，我们将介绍基于模型的方法，以展示它如何提高 DRL 样本效率。</p><p id="8d33" class="pw-post-body-paragraph io ip hi iq b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl hb bi translated">* *本文是与<a class="ko kp ge" href="https://medium.com/u/4bd9c016c60?source=post_page-----9971c5ccb4d6--------------------------------" rel="noopener" target="_blank">卢卡·索伦蒂诺</a>合作撰写的</p></div></div>    
</body>
</html>