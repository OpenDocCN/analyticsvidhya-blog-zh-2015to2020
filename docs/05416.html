<html>
<head>
<title>Gaussian Mixture Models(GMM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高斯混合模型(GMM)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gaussian-mixture-models-gmm-ca9911b72b4e?source=collection_archive---------9-----------------------#2020-04-20">https://medium.com/analytics-vidhya/gaussian-mixture-models-gmm-ca9911b72b4e?source=collection_archive---------9-----------------------#2020-04-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="db5f" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">理解GMM:思想、数学、EM算法和python实现</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/b75c50ac2c0389af49b8de05658e15eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5tFTLI-Yc9SEf9Q2FAGrGg.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">来源:<a class="ae jn" href="https://unsplash.com/@franckinjapan" rel="noopener ugc nofollow" target="_blank">弗兰克诉</a>通过<a class="ae jn" href="https://unsplash.com/photos/JjGXjESMxOY" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><p id="6b9a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi kk translated"><span class="l kl km kn bm ko kp kq kr ks di"> B </span> <strong class="jq hj"> <em class="kt"> rief </em>:高斯混合模型是一种流行的无监督学习算法。GMM方法类似于K-Means聚类算法，但是更健壮，因此由于其复杂性而更有用。在这篇文章中，我将给出一个鸟瞰图，数学(<em class="kt"> ba </em> ye <em class="kt"> s </em> ic maths，nothing ab <em class="kt"> normal </em>)，python从头实现以及使用sklearn库。</strong></p><h1 id="544f" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">介绍</h1><p id="3ec6" class="pw-post-body-paragraph jo jp hi jq b jr lm ij jt ju ln im jw jx lo jz ka kb lp kd ke kf lq kh ki kj hb bi translated">查看我关于K-means聚类的博客是一个好主意(3分钟阅读)，以获得聚类、无监督学习和K-Means技术的基本概念。在聚类中，给定一个未标记的数据集<strong class="jq hj"> <em class="kt"> X </em> </strong>，我们希望将样本分组到<strong class="jq hj"> <em class="kt"> K </em> </strong>个聚类中。在GMMs中，假设<strong class="jq hj"/>的不同子群体(<strong class="jq hj"> <em class="kt"> K </em> </strong>共)遵循一个<a class="ae jn" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">正态分布</a>，虽然我们只有总体<strong class="jq hj"> <em class="kt"> X( </em> </strong>因此得名高斯混合模型)的概率分布信息。我们的任务是能够找到<strong class="jq hj"> <em class="kt"> K </em> </strong> <em class="kt">高斯的</em>的参数以便将数据<strong class="jq hj"><em class="kt"/></strong>X<a class="ae jn" href="https://en.wikipedia.org/wiki/Exploratory_data_analysis" rel="noopener ugc nofollow" target="_blank"><em class="kt">进行探索性的数据分析</em></a><strong class="jq hj"/>或者对新的数据做出预测。</p><h1 id="4170" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">对K-均值聚类的改进</h1><p id="bbc6" class="pw-post-body-paragraph jo jp hi jq b jr lm ij jt ju ln im jw jx lo jz ka kb lp kd ke kf lq kh ki kj hb bi translated">K-means使用欧几里德距离函数来发现数据中的聚类。只要数据相对于质心遵循圆形分布，这种方法就能很好地工作。但是如果数据是非线性的，椭圆形的呢？还是数据有非零协方差？如果聚类有不同的均值和协方差呢？</p><p id="6941" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">这就是高斯混合模型拯救世界的地方！</p><blockquote class="lr"><p id="43e8" class="ls lt hi bd lu lv lw lx ly lz ma kj dx translated">GMM假设产生数据的是高斯分布的混合物。它使用数据点到聚类的软分配(即，概率性的，因此更好),与数据点到聚类的硬分配的K-means方法形成对比，假设数据围绕质心呈圆形分布。</p></blockquote><p id="a1d7" class="pw-post-body-paragraph jo jp hi jq b jr mb ij jt ju mc im jw jx md jz ka kb me kd ke kf mf kh ki kj hb bi translated">简而言之，GMM捕获工作得更好，因为<strong class="jq hj"> (A) </strong>它通过使用软分配来捕获属于不同聚类的数据点的不确定性，并且<strong class="jq hj"> (B) </strong>它对圆形聚类没有偏见。因此，即使对于非线性数据分布，它也能很好地工作。</p><h1 id="7edf" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">戈-梅-莫三氏:男性假两性畸形综合征</h1><p id="f2d4" class="pw-post-body-paragraph jo jp hi jq b jr lm ij jt ju ln im jw jx lo jz ka kb lp kd ke kf lq kh ki kj hb bi translated"><em class="kt"> GMM </em>的目标函数是最大化数据X、<strong class="jq hj"> <em class="kt"> p(X) </em> </strong>的<em class="kt">似然值或对数似然值<strong class="jq hj"> <em class="kt"> L </em> </strong>(因为log是单调递增函数)。通过假设混合了<strong class="jq hj"> <em class="kt"> K </em> </strong>高斯分布来生成数据，我们可以将<strong class="jq hj"><em class="kt">【p(X)</em></strong>写成边缘化概率，对所有数据点的所有<strong class="jq hj"> <em class="kt"> K </em> </strong>聚类求和。</em></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mg"><img src="../Images/e793cfd77b73bded1061721b5f8be753.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*iEmFaU2kGTkx0D_wNTNUgw.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mh"><img src="../Images/889f0f8eb7e4fdbdce4467fb5f510601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Udq9JkCv3z9s_WXYtAT2iw.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">似然值</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mi"><img src="../Images/0937274604f8d56835227e41f72deb7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*sZlcLTgOugx_iX-ndqwtPg.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">对数似然值</figcaption></figure><p id="798f" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">利用上面对数函数内的求和，我们无法获得解析解。虽然看起来很龌龊，但这个问题有一个优雅的解决方案:<a class="ae jn" href="https://en.wikipedia.org/wiki/Expectation–maximization_algorithm" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj"> <em class="kt">期望最大化(em)算法</em> </strong> </a>。</p><h1 id="d6ab" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">数学</h1><p id="5c26" class="pw-post-body-paragraph jo jp hi jq b jr lm ij jt ju ln im jw jx lo jz ka kb lp kd ke kf lq kh ki kj hb bi translated"><strong class="jq hj"> <em class="kt"> EM算法</em> </strong>是一种<em class="kt">迭代</em>算法，用于寻找<em class="kt">模型的最大似然估计(MLE) </em>，其中参数无法直接找到，就像我们这里的情况。它包括两个步骤:e <em class="kt">预期</em>步骤和<em class="kt">最大化</em>步骤。</p><ol class=""><li id="faa0" class="mj mk hi jq b jr js ju jv jx ml kb mm kf mn kj mo mp mq mr bi translated"><strong class="jq hj"> <em class="kt">期望步骤</em> </strong>:计算隶属值<em class="kt"> r </em> _ <em class="kt"> ic。</em>这是数据点<em class="kt"> x_i </em>属于聚类<em class="kt"> c </em>的概率。</li></ol><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ms"><img src="../Images/591b6f5e22a859240577a970c6a05d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qwgw-6MIfjaeqczqg7HuQw.png"/></div></div></figure><p id="10da" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">2.<strong class="jq hj"> <em class="kt">最大化步骤</em> </strong>:计算一个新的参数<em class="kt"> mc </em>，该参数决定了属于不同聚类的点的分数。通过计算每个聚类c的<a class="ae jn" href="https://stats.stackexchange.com/questions/351549/maximum-likelihood-estimators-multivariate-gaussian" rel="noopener ugc nofollow" target="_blank"> MLE的</a>来更新参数μ、π、σ</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mt"><img src="../Images/6931c47e9c0362f4e2a1775a33e95393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*0L_HhYtzU1PmntPrOpG9gQ.png"/></div></figure><p id="a7d9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">重复E-M步骤，直到对数似然值<em class="kt"> L </em>收敛。</p><h1 id="d998" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">密码</h1><p id="b3cc" class="pw-post-body-paragraph jo jp hi jq b jr lm ij jt ju ln im jw jx lo jz ka kb lp kd ke kf lq kh ki kj hb bi translated">让我们从头开始用python写一个GMM的基本实现。</p><p id="82d9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">生成一维数据。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="2765" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">初始化GMM的参数:μ，π，σ。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="69c9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">运行EM算法的第一次迭代。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">EM算法的单次迭代</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mw"><img src="../Images/3f7ea392138b10a570fdb5bb002af923.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TLqrqBWlCPwdE3eERvpxtA.png"/></div></div></figure><p id="d674" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">将这段代码放在for循环中，并将其放入一个class对象中。现在我们正在谈话！</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mu mv l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">GMM-1D级</figcaption></figure><div class="iy iz ja jb fd ab cb"><figure class="mx jc my mz na nb nc paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/9dc514400cdb0ba77b581aefff5c131a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*dOIr3nPuZGWv9hiawlDGoA.png"/></div></figure><figure class="mx jc my mz na nb nc paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/8785578293bbd36be11257202843e09c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*XSy3sZKTBGVYWOhFXXvs-Q.png"/></div></figure></div><p id="6538" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们已经有了一个模型，可以运行一维数据。同样的原理也适用于更高维度(≥ 2D)。唯一不同的是，我们将在这种情况下使用多元高斯分布。让我们为2D模型编写代码。</p><p id="0e8b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们生成一些数据并编写我们的模型。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mu mv l"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nd"><img src="../Images/db7e80c9b312abf86e9a6896e9eb91c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*nu0tAapGQsl4nWhaQjFm7g.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">2D斑点</figcaption></figure><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mu mv l"/></div></figure><p id="fb8b" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们对这个模型做一些预测。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mu mv l"/></div></figure><div class="iy iz ja jb fd ab cb"><figure class="mx jc my mz na nb nc paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/d881e589668e7c96ee62ea9c9488f1d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ZSpDCoip56QPLt2MDuxZ6g.png"/></div></figure><figure class="mx jc my mz na nb nc paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/6ca5e6ddeaaaa9ea01cf5872168763ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*7jJIlDnyPTvpzxtMqqXmpg.png"/></div></figure></div><div class="ab cb"><figure class="mx jc my mz na nb nc paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/021c3da9059a6901ec3562d9be8d33b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*rnumu6d8_8kFFdCiZ_s8uw.png"/></div></figure><figure class="mx jc my mz na nb nc paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/d99baf14d33321d320e839186e7f46bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*NYxM_vuTSt-fUf1fZJjRhg.png"/></div></figure></div><div class="ab cb"><figure class="mx jc my mz na nb nc paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/07405385995ea302d065c384a25650ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*tbo99j-fL1hzzZpG8BKPHg.png"/></div></figure><figure class="mx jc my mz na nb nc paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/9d62570036f4e575559abcfee4eb6504.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Wnvlfy1BuauVSak7UaFcJQ.png"/></div></figure></div><p id="4a3a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">使用<a class="ae jn" href="https://scikit-learn.org/stable/tutorial/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj"> <em class="kt"> sklearn </em> </strong> </a>，同样的任务可以在几行代码内完成。很圆滑，是吧？</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mu mv l"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mw"><img src="../Images/90ac5d54c1fe742165ae553a0fcbc52a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nKs7niOt2YAZrwPpOH5uvw.png"/></div></div></figure><p id="9c8e" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，GMM将样本分类为属于第二类。有用！</p><h1 id="5839" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">结论</h1><p id="e110" class="pw-post-body-paragraph jo jp hi jq b jr lm ij jt ju ln im jw jx lo jz ka kb lp kd ke kf lq kh ki kj hb bi translated">实现高斯混合模型并不困难。一旦你对数学有了清晰的认识，就可以找到模型的最大似然估计，无论是1D还是更高维的数据。这种方法在执行聚类任务时是健壮且有用的。既然您已经熟悉了GMMs的python实现，那么您就可以使用数据集执行一些很酷的事情了。假设给你一个病人的数据集，包含两个参数:红细胞体积和红细胞血红蛋白浓度，没有病人和健康病人的标签。插入上面的模型来聚集数据将会给你两个不同的(<em class="kt">几乎是</em>)质量，你可以使用它们来进行进一步的分析和预测。</p><h1 id="0211" class="ku kv hi bd kw kx ky kz la lb lc ld le io lf ip lg ir lh is li iu lj iv lk ll bi translated">来源</h1><ol class=""><li id="60fc" class="mj mk hi jq b jr lm ju ln jx ne kb nf kf ng kj mo mp mq mr bi translated"><a class="ae jn" href="https://www.youtube.com/watch?v=qMTuMa86NzU" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=qMTuMa86NzU</a></li><li id="77de" class="mj mk hi jq b jr nh ju ni jx nj kb nk kf nl kj mo mp mq mr bi translated"><a class="ae jn" href="https://www.python-course.eu/expectation_maximization_and_gaussian_mixture_models.php" rel="noopener ugc nofollow" target="_blank">https://www . python-course . eu/expectation _ maximization _ and _ Gaussian _ mixture _ models . PHP</a></li></ol></div></div>    
</body>
</html>