<html>
<head>
<title>Accuracy Paradox in Classification Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类模型中的准确性悖论</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/accuracy-paradox-in-classification-models-d55a2884410?source=collection_archive---------9-----------------------#2020-11-27">https://medium.com/analytics-vidhya/accuracy-paradox-in-classification-models-d55a2884410?source=collection_archive---------9-----------------------#2020-11-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div class="er es hg"><img src="../Images/6a115e467130ba60ce1c86ed367b64af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*CQt10UPu2ou6vFkHSSG-tA.jpeg"/></div><figcaption class="hn ho et er es hp hq bd b be z dx translated">悖论</figcaption></figure><div class=""/><p id="7eb3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在某些情况下，机器学习行话中的二元分类器的准确性可能是欺骗性的。准确度将正确识别的元素与元素总数进行比较。</p><p id="f96d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在深入研究之前，让我们先了解几个术语:</p><h2 id="ade6" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">真阳性:</h2><p id="9d70" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">这意味着我们预测的东西是相关的，而且实际上是相关的。</p><h2 id="0848" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">真阴性:</h2><p id="4eb7" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">这意味着我们预测的东西是不相关的，实际上是不相关的。</p><h2 id="f7ec" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">假阳性(I 型错误):</h2><p id="9f5d" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">这意味着我们预测的东西是相关的，但实际上是不相关的。</p><h2 id="76a1" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">假阴性(第二类错误):</h2><p id="99f1" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">这意味着我们预测的东西是不相关的，但实际上是相关的。我认为这很危险。我们用一个案例来了解一下。</p><h2 id="a859" class="jo jp ht bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">个案研究</h2><p id="f108" class="pw-post-body-paragraph iq ir ht is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">ABC 医院有一个分类模型来预测患有脑部疼痛的患者是否会在晚期发展成脑瘤。</p><p id="3e8e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">他们的模型有 95%的准确性，也就是说，在 1000 个案例中，模型预测了 950 个正确的预测和 50 个不正确的预测。他们开始将患者的报告放入分类器中，并告诉患者他们正在经历轻微的头痛，并给他西红花苷或其他药片以减轻疼痛。几年后，几乎 30%被医院证明没有肿瘤的病人开始死亡，报告显示所有的死亡都是由于大脑中存在肿瘤。</p><p id="e780" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在医院陷入了严重的困境，上层管理人员成立了数据科学家团队来处理这种情况。数据科学家开始寻找为什么分类器预测错误的结果，但仍然有 95%的准确率。</p><p id="8115" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后有趣的事情发生了。他们发现这个模型是有偏见的。</p><p id="7bcd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设真阴性是患者没有肿瘤的正确预测，真阳性是患者有肿瘤的正确预测，假阳性是患者有肿瘤的不正确预测，假阴性是患者没有肿瘤的不正确预测。</p><figure class="kp kq kr ks fd hk er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es ko"><img src="../Images/f048a2de51d4554a71c8d27d3ee9b877.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LPA6Wtkz0uqYwykni5Kwtg.jpeg"/></div></div></figure><p id="f902" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">根据给定的方案，50 由于患者没有肿瘤而做出不正确的预测，即假阴性= 50，950 由于患者有肿瘤而做出正确的预测，即真阳性= 950。</p><p id="53d6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">精确度的公式是:</p><p id="fcd1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">准确度= (TP + TN) / (TP + FP + FN + TN) </strong></p><p id="bc91" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">根据这个公式，</p><p id="462c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">精度</strong>=(950+0)/(950+0+50+0)= 950/1000 = 0.95 = 95%</p><blockquote class="kx ky kz"><p id="ca0c" class="iq ir la is b it iu iv iw ix iy iz ja lb jc jd je lc jg jh ji ld jk jl jm jn hb bi translated">但是这个模型显然是有偏见的，因为在任何情况下都是说<strong class="is hu"> <em class="ht">患者没有肿瘤。</em> </strong></p></blockquote><p id="e9c7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，准确性不是确定模型性能的可靠指标。</p><blockquote class="kx ky kz"><p id="d3bd" class="iq ir la is b it iu iv iw ix iy iz ja lb jc jd je lc jg jh ji ld jk jl jm jn hb bi translated"><strong class="is hu">这就是它被称为悖论的原因，因为凭直觉，你会认为精确度更高的模型是最好的模型，但精确度悖论告诉我们，有时情况并非如此。</strong></p></blockquote><p id="3803" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">感谢您的阅读。希望有帮助！！</p><p id="82e8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> <em class="la">参考文献:</em> </strong> <em class="la"> <br/> 1。</em><a class="ae le" href="https://towardsdatascience.com/accuracy-paradox-897a69e2dd9b" rel="noopener" target="_blank">https://towardsdatascience . com/accuracy-paradox-897 a 69 e 2 DD 9 b</a><br/>2 .<a class="ae le" href="https://en.wikipedia.org/wiki/Accuracy_paradox" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Accuracy_paradox</a>T12】3。<a class="ae le" href="https://www.utwente.nl/en/eemcs/trese/graduation_projects/2009/Abma.pdf" rel="noopener ugc nofollow" target="_blank">https://www . ut wente . nl/en/EEM cs/trese/graduation _ projects/2009/abma . pdf</a><br/>4 .<a class="ae le" href="https://www.youtube.com/watch?v=mP4gaO4IC5A" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=mP4gaO4IC5A</a><br/>5。<a class="ae le" href="https://www.udemy.com/course/machinelearning/learn/lecture/5773388#notes" rel="noopener ugc nofollow" target="_blank">https://www . udemy . com/course/machine learning/learn/lecture/5773388 #备注</a></p></div></div>    
</body>
</html>