<html>
<head>
<title>Exploring DenseNets and a comparison with other Deep Architectures</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索DenseNets并与其他深层架构进行比较</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/exploring-densenets-and-a-comparison-with-other-deep-architectures-85f02597400a?source=collection_archive---------11-----------------------#2020-05-08">https://medium.com/analytics-vidhya/exploring-densenets-and-a-comparison-with-other-deep-architectures-85f02597400a?source=collection_archive---------11-----------------------#2020-05-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="d01f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们都知道卷积神经网络及其在学习非线性和复杂表示方面的效率。但是，学习这些表示的问题是，随着我们的问题变得越来越复杂，我们添加更多的层，使模型更深，然后我们看到多个问题浮出水面。这使得处理深度神经网络变得棘手。</p><p id="0a85" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在过去十年中，我们已经看到许多深度网络架构成功地解决了这些问题，但所有这些架构的一个共同点是，它们使用一些新奇的东西来抑制梯度消失、过拟合和参数爆炸的问题。我们将进一步探讨如何构建这些架构来解决这些问题，以及DenseNet的方法有多新颖。所有内容均基于黄高、刘庄、劳伦斯·范德马腾和基利安·q·温伯格发表的题为“<a class="ae jk" href="https://arxiv.org/pdf/1608.06993.pdf" rel="noopener ugc nofollow" target="_blank"><em class="jl"/></a>”的大量研究工作</p><h2 id="e0de" class="jm jn hi bd jo jp jq jr js jt ju jv jw ix jx jy jz jb ka kb kc jf kd ke kf kg bi translated">当前最先进的深度神经网络架构</h2><p id="97dd" class="pw-post-body-paragraph im in hi io b ip kh ir is it ki iv iw ix kj iz ja jb kk jd je jf kl jh ji jj hb bi translated">ResNets和高速公路网通过身份连接将信号从一层绕过到下一层，也就是说，它们将来自上一层的输入原样传递到下一层，这保持了将由每一层处理的输入要素的神圣性。随机深度通过在训练期间随机丢弃层来缩短结果，以允许更好的信息和梯度流动。</p><p id="93f0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">FractalNets将几个平行层序列与不同数量的卷积块重复组合，以获得较大的标称深度，同时保持网络中的许多短路径。谷歌的Inception也使用了多种滤镜尺寸，并结合了这些特征图。尽管这些不同的方法在网络拓扑和训练过程上有所不同，但它们都有一个共同的关键特征:</p><blockquote class="km kn ko"><p id="386f" class="im in jl io b ip iq ir is it iu iv iw kp iy iz ja kq jc jd je kr jg jh ji jj hb bi translated">创建从早期层到后期层的短路径。</p></blockquote><h2 id="a191" class="jm jn hi bd jo jp jq jr js jt ju jv jw ix jx jy jz jb ka kb kc jf kd ke kf kg bi translated">这些致密层有多致密？</h2><p id="0821" class="pw-post-body-paragraph im in hi io b ip kh ir is it ki iv iw ix kj iz ja jb kk jd je jf kl jh ji jj hb bi translated">DenseNets基于一种简单的连接模式:网络中的每一层都与其他层直接相连。是啊！你读对了，直接。为了保持前馈性质，每一层从所有前面的层获得输入，并将它自己的特征映射传递给所有后面的层。</p><blockquote class="ks"><p id="3ed2" class="kt ku hi bd kv kw kx ky kz la lb jj dx translated">重申一下这个想法，在一个正常的“L”层的神经网络中，会有“L”个连接，每一层与其下一层之间有一个连接。但是，在DenseNets中将有“L(L+1)/2”个连接。</p></blockquote><figure class="ld le lf lg lh li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es lc"><img src="../Images/618929c7e0d08dd6d53cacd30c578ca1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vSOxRJLQf7m1Gy-tdNc2yg.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">典型的密集区块:每一层都与其他层相连。</figcaption></figure></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><h2 id="4761" class="jm jn hi bd jo jp jq jr js jt ju jv jw ix jx jy jz jb ka kb kc jf kd ke kf kg bi translated">让我们看看这个紧密相连的网络内部！</h2><p id="55b1" class="pw-post-body-paragraph im in hi io b ip kh ir is it ki iv iw ix kj iz ja jb kk jd je jf kl jh ji jj hb bi translated">那么与其他架构相比，这些连接的用途是什么呢？</p><p id="aa14" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="jl">较小参数— </em>耶！即使有额外的连接，这些网络的参数相对较少，因为有更多的输入连接，DenseNet层非常窄(每层12个过滤器)，仅向网络添加一小部分特征映射，并保持其余特征映射不变，最终分类器基于网络中的所有特征映射做出决定。</p><p id="edc6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="jl">更好的信息流— </em>由于过滤器更小，输入得以保留，梯度和信息流可在网络中自由流动，最后一层也可访问输入和所有特征图，因此存在隐含的“深度监管”。</p><p id="64ef" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="jl">过度配合——这些连接也防止了过度配合，也使训练更加容易。</em></p><p id="6878" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">dense net的新颖之处是什么让它脱颖而出？</strong></p><p id="afb1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="jl">特征重用</em>—densenet没有从极深或极宽的架构中提取表示能力，而是通过特征重用来挖掘网络的潜力，将不同层学习到的特征映射连接起来，增加了后续层输入的变化，提高了效率。这是DenseNets和ResNets的主要区别。此外，与初始网络(也连接不同层的功能)相比，DenseNets更简单、更高效。</p><h1 id="da7f" class="lt jn hi bd jo lu lv lw js lx ly lz jw ma mb mc jz md me mf kc mg mh mi kf mj bi translated">建筑</h1><figure class="ml mm mn mo fd li er es paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="er es mk"><img src="../Images/9329bc426ee62d8971118d3c276281a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cLroR3U3PFUxpS-2-fzmFA.png"/></div></div><figcaption class="lp lq et er es lr ls bd b be z dx translated">有三个致密块体的深致密网。两个相邻块之间的层被称为过渡层，并通过卷积和池化来改变特征地图的大小。</figcaption></figure><p id="98f2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">上图描述了典型DenseNet模型的架构。我不会详细讨论这个问题，因为这篇文章对此有更好的解释。在高层次上，我们有由这些互连层组成的密集块。每一层本身都是批标准化，然后是ReLU和卷积层。所有这些密集块通过一组过渡卷积和汇集层相互连接。<br/>现在我们知道了层的这些基本元素，我们可以通过添加任意数量的密集块来构建自己的架构。我们还可以根据我们的问题，通过选择过滤器大小和特征地图的数量来定制每个层。</p><h2 id="146b" class="jm jn hi bd jo jp jq jr js jt ju jv jw ix jx jy jz jb ka kb kc jf kd ke kf kg bi translated">DenseNets的增长率</h2><p id="456b" class="pw-post-body-paragraph im in hi io b ip kh ir is it ki iv iw ix kj iz ja jb kk jd je jf kl jh ji jj hb bi translated">特征映射的初始数目决定了模型的参数数目和复杂性。这个超参数被称为“增长率”。</p><p id="1810" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果每个函数H(L)产生k个特征图，那么第L层具有</p><blockquote class="km kn ko"><p id="df8a" class="im in jl io b ip iq ir is it iu iv iw kp iy iz ja kq jc jd je kr jg jh ji jj hb bi translated">k0+k×(L1)输入特征映射，</p></blockquote><p id="abb2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">其中，k0是输入层的通道数，k是增长率。这意味着每增加一层，我们就向特征地图的总数增加“k”个特征地图，从而增加参数的总数。所以这个数字“k”决定了滤波器的数量。即使在这里，k值很小，为12，这导致了相对较少的参数数量，这个网络产生了最先进的结果。增长率规定了每层对全局状态贡献多少新信息，因此决定了模型的信息流。</p><h1 id="758b" class="lt jn hi bd jo lu lv lw js lx ly lz jw ma mb mc jz md me mf kc mg mh mi kf mj bi translated">绩效和结果</h1><p id="5ff8" class="pw-post-body-paragraph im in hi io b ip kh ir is it ki iv iw ix kj iz ja jb kk jd je jf kl jh ji jj hb bi translated">现在让我们来看看这种新方法的效率有多高！</p><p id="3b17" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="jl">准确性— </em>该模型在CIFAR、SVHN和Imagenets等基准数据集上进行了训练。该模型优于所有当前最先进的模型，如在CIFAR和SVHN数据集上的ResNets、FractalNets。令人印象深刻！</p><p id="1cb6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="jl">容量和参数效率— </em> DenseNet模型是电容性的，也就是说，随着我们增加模型的层数和增长率，模型的效率和精度也会相应增加。这证明了该模型没有屈服于过度拟合，并且具有学习更复杂问题的能力。该模型也是参数高效的，一个250层模型只有15.3M个参数，但它始终优于其他模型，如FractalNet和Wide ResNets，它们有超过30M个参数。</p><h2 id="8539" class="jm jn hi bd jo jp jq jr js jt ju jv jw ix jx jy jz jb ka kb kc jf kd ke kf kg bi translated">最终总结</h2><ul class=""><li id="42f4" class="mp mq hi io b ip kh it ki ix mr jb ms jf mt jj mu mv mw mx bi translated">该模型在具有相同特征地图大小的任意两个层之间引入了直接连接。</li><li id="b2e6" class="mp mq hi io b ip my it mz ix na jb nb jf nc jj mu mv mw mx bi translated">DenseNets可以自然扩展到数百层，而不会出现优化困难。</li><li id="0c48" class="mp mq hi io b ip my it mz ix na jb nb jf nc jj mu mv mw mx bi translated">随着参数数量的增加，DenseNets往往会不断提高精度，而不会出现任何性能下降或过度拟合的迹象。在多种设置下，它在几个竞争激烈的数据集上取得了最先进的结果。</li><li id="83e3" class="mp mq hi io b ip my it mz ix na jb nb jf nc jj mu mv mw mx bi translated">此外，DenseNets需要更少的参数和更少的计算来实现最先进的性能。</li></ul><p id="68c1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">总的来说，由于作者正在训练与当前架构一致的模型以进行比较，这些DenseNets可以通过更详细地调整超参数和学习率计划来进一步优化，并且在深度学习中具有很高的未来前景！</p></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="7107" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所有的功劳归于黄高、刘庄、劳伦斯·范·德·马腾和基利安·q·温伯格。<br/>原创论文—<a class="ae jk" href="https://arxiv.org/pdf/1608.06993.pdf" rel="noopener ugc nofollow" target="_blank"><em class="jl">密集连接的卷积网络</em> </a></p></div></div>    
</body>
</html>