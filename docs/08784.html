<html>
<head>
<title>Easy Web Scraping using Python and BeautifulSoup4 and saving files as well HTML pages as PDF</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python和BeautifulSoup4轻松抓取网页，并将文件和HTML页面保存为PDF格式</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/easy-web-scraping-using-python-and-beautifulsoup4-and-saving-files-as-well-html-pages-as-pdf-d4693d77150b?source=collection_archive---------3-----------------------#2020-08-13">https://medium.com/analytics-vidhya/easy-web-scraping-using-python-and-beautifulsoup4-and-saving-files-as-well-html-pages-as-pdf-d4693d77150b?source=collection_archive---------3-----------------------#2020-08-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="7159" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">下载和保存文件的简单自动化</h1></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><p id="394d" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">随着世界进入21世纪的第二个十年，最近著名的谚语“数据是新的石油”变得越来越贴切。网络抓取是一种非常有用的技术，可以从一个正常工作的网站上获取大量数据。它还可以用来下载文件、图像、文本，甚至从一个或多个网站获取实时更新。</p><p id="3c50" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">像Python这样简单的高级编程语言使它变得更简单，同时也更有趣。</p><p id="cfff" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">今天我将解释两个话题。<strong class="jm hj">下载文件</strong>和<strong class="jm hj">下载PDF格式的网页。</strong></p><p id="9e5d" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><strong class="jm hj">为了简单起见，我选择了一个带有上述链接的网页:</strong><a class="ae ki" href="https://www.bankexamstoday.com/2015/08/banking-awareness-questions-pdf.html" rel="noopener ugc nofollow" target="_blank">https://www . bankexamstoday . com/2015/08/banking-awareness-questions-pdf . html</a>。它既包括PDF文件，也包括针对HTML网页的链接，可以很容易地转换成PDF格式。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es kj"><img src="../Images/5959a80e9183088c3c56c4888f9ab803.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4LiU1OQNCyfNsKl2c3cPOQ.png"/></div></div></figure><p id="508d" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><strong class="jm hj">先决条件:- </strong></p><p id="c675" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">您必须安装Python。它内置在Ubuntu中，Windows用户可以很容易地从官方网站安装它。</p><p id="b13f" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">我正在分享<strong class="jm hj">在各种平台上安装Python的链接:</strong></p><p id="ea54" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">MAC:<a class="ae ki" rel="noopener" href="/@blessedmarcel1/how-to-install-jupyter-notebook-on-mac-using-homebrew-528c39fd530f">https://medium . com/faun/the-right-way-to-set-up-python-on-your-MAC-e 923 FFE 8 cf 8 e</a></p><p id="62f2" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">windows:<a class="ae ki" rel="noopener" href="/@kswalawage/install-python-and-jupyter-notebook-to-windows-10-64-bit-66db782e1d02">https://medium . com/@ kswalawage/install-python-and-jupyter-notebook-to-windows-10-64-bit-66db 782 E1 d 02</a></p><p id="137f" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">对于Linux和Ubuntu它已经安装，你只需要安装Jupyter笔记本或任何IDE(为简单起见安装Jupyter)。只需打开终端并键入</p><pre class="kk kl km kn fd kv kw kx ky aw kz bi"><span id="30c3" class="la ig hi kw b fi lb lc l ld le">$ sudo apt install jupyter-notebook</span></pre><p id="1683" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">本教程不需要更多的库。我在Python3上工作，所以我将使用pip3，对于较低版本使用pip。</p><pre class="kk kl km kn fd kv kw kx ky aw kz bi"><span id="85ea" class="la ig hi kw b fi lb lc l ld le"><em class="lf">$ pip3 install beautifulsoup4</em></span><span id="90db" class="la ig hi kw b fi lg lc l ld le">$ sudo apt-get install wkhtmltopdf</span><span id="b874" class="la ig hi kw b fi lg lc l ld le">$ pip3 install urllib</span></pre><p id="e167" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">库<strong class="jm hj"> wkhtmltopdf </strong>不工作<strong class="jm hj"> pip </strong>所以我用<strong class="jm hj"> apt-get </strong>来安装它。</p><p id="9119" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">在<strong class="jm hj">终端</strong>或<strong class="jm hj">命令提示符</strong>上输入<strong class="jm hj"> jupyter notebook </strong>打开Jupyter notebook，在右上角进入“新建”，在“笔记本”下选择“Python”</p><h1 id="9e30" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">进入编码部分</h1><ol class=""><li id="c241" class="lh li hi jm b jn lj jr lk jv ll jz lm kd ln kh lo lp lq lr bi translated">导入所需的库。</li></ol><pre class="kk kl km kn fd kv kw kx ky aw kz bi"><span id="b546" class="la ig hi kw b fi lb lc l ld le">from bs4 import BeautifulSoup as bs<br/>import requests<br/>import urllib<br/>import pdfkit</span></pre><p id="c3ae" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">2.刮削部件:</p><p id="930f" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">向页面发出get请求，并分配use Soup。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ls"><img src="../Images/44b085e68f1b60fa1cbb10839aea0386.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vDd8c-anXhiu5GXVZUlQgA.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">使用Inspect元素搜索文件链接，对于pdf文件，存在带有class ='pdf '的定位标记</figcaption></figure><pre class="kk kl km kn fd kv kw kx ky aw kz bi"><span id="5731" class="la ig hi kw b fi lb lc l ld le"># findAll and find function of BeautifulSoup search for HTML tags<br/># findAll will return a list with n elements</span><span id="3489" class="la ig hi kw b fi lg lc l ld le">url = ‘<a class="ae ki" href="https://www.bankexamstoday.com/2015/08/banking-awareness-questions-pdf.html'" rel="noopener ugc nofollow" target="_blank">https://www.bankexamstoday.com/2015/08/banking-awareness-questions-pdf.html'</a><br/>response = requests.get(url)<br/>soup = bs(response.text,’html.parser’)<br/>a=soup.findAll(‘a’,{‘class’:’pdf’})<br/>for tag in a:<br/>    print(a)</span></pre><p id="1f6f" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">3.从HTML元素中获取文件名、文件链接并保存到本地PDF文件</p><pre class="kk kl km kn fd kv kw kx ky aw kz bi"><span id="299f" class="la ig hi kw b fi lb lc l ld le">for element in a:<br/> print(l)<br/> name = element[‘href’].split(‘/’)[4]<br/> link = element[‘href’]<br/> directory = ‘address/to/directory’<br/> <br/> print(‘saving : ‘,name)<br/> pdfFile = urllib.request.urlopen(link)<br/> file = open(directory+name, ‘wb’)<br/> file.write(pdfFile.read())<br/> file.close()</span></pre><p id="c6b2" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">4.保存PDF文件后，现在我们将把网络链接保存为PDF。在检查时，我们看到所有的块都以一个div的形式出现，只剩下第一个div，因为我们已经废弃了PDF文件。我们将在一个循环中刮擦每一个其他的</p><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es lx"><img src="../Images/5db07b34d6037d45398157b484e15cf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qnNK2XIlj0Yg1JhsASpABg.png"/></div></div></figure><p id="0cb0" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">a.查找所有栅格盒div</p><pre class="kk kl km kn fd kv kw kx ky aw kz bi"><span id="e1f1" class="la ig hi kw b fi lb lc l ld le">grid_box = soup.findAll(‘div’,{‘class’:’grid-box’})</span></pre><p id="5ab1" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">b.在所有框中循环——创建目录并将HTML文件保存为这些文件夹中的PDF。</p><pre class="kk kl km kn fd kv kw kx ky aw kz bi"><span id="6956" class="la ig hi kw b fi lb lc l ld le">for i in range(1,len(grid_box)):<br/>    # stripping topic heading name from the grid-box   <br/>    dirname = grid_box[i].h2.text.strip()<br/>    print('creating folder for : ',dirname)<br/>    <br/>    # Creating a directory with same name as topic heading     <br/>    # replacing spaces with underscore as spaces can create problem <br/>    # <br/>    <br/>    dirname = 'address/to/directory'+dirname.replace(' ','_')<br/>    if not os.path.isdir(dirname):<br/>        os.mkdir(dirname)</span><span id="4305" class="la ig hi kw b fi lg lc l ld le">    links = (grid_box[i].findAll('a'))<br/>    for f in links:<br/>        html_link = (f['href'])<br/>        html_name = f.text.replace(' ','_').strip()<br/>        html_res = requests.get(html_link)<br/>        <br/>    # creating files with same name as name in html link <br/>        filename =  dirname+'/'+html_name+'.pdf'<br/>        <br/>        if not os.path.isfile(filename):<br/>            pdf = pdfkit.from_url(html_link,filename)<br/>            print('created_file : '+dirname+'/'+html_name+'.pdf')</span></pre><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ly"><img src="../Images/2bd7175848a568070d02399e79cea767.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-9XBQgq3bIchEM-5YkrKQ.png"/></div></div><figcaption class="lt lu et er es lv lw bd b be z dx translated">……….从网站创建和下载的文件/文件夹……..</figcaption></figure><h1 id="ac6f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">下面的完整代码</h1><pre class="kk kl km kn fd kv kw kx ky aw kz bi"><span id="53cc" class="la ig hi kw b fi lb lc l ld le">from bs4 import BeautifulSoup as bs<br/>import requests<br/>import urllib</span><span id="eefa" class="la ig hi kw b fi lg lc l ld le">url = ‘<a class="ae ki" href="https://www.bankexamstoday.com/2015/08/banking-awareness-questions-pdf.html'" rel="noopener ugc nofollow" target="_blank">https://www.bankexamstoday.com/2015/08/banking-awareness-questions-pdf.html'</a><br/>response = requests.get(url)<br/>soup = bs(response.text,’html.parser’)<br/>a=soup.findAll(‘a’,{‘class’:’pdf’})</span><span id="e088" class="la ig hi kw b fi lg lc l ld le">for element in a:<br/> print(l)<br/> name = element[‘href’].split(‘/’)[4]<br/> link = element[‘href’]<br/> directory = ‘address/to/directory’<br/> <br/> print(‘saving : ‘,name)<br/> pdfFile = urllib.request.urlopen(link)<br/> file = open(directory+name, ‘wb’)<br/> file.write(pdfFile.read())<br/> file.close()</span><span id="1480" class="la ig hi kw b fi lg lc l ld le">grid_box = soup.findAll('div',{'class':'grid-box'})</span><span id="f280" class="la ig hi kw b fi lg lc l ld le">for i in range(1,len(grid_box)):<br/>    # stripping topic heading name from the grid-box   <br/>    dirname = grid_box[i].h2.text.strip()<br/>    print('creating folder for : ',dirname)<br/>    <br/>    # Creating a directory with same name as topic heading (replacing<br/>    #spaces with underscore as spaces can create problem in creating folder)   <br/>    <br/>    dirname = 'address/to/directory'+dirname.replace(' ','_')<br/>    if not os.path.isdir(dirname):<br/>        os.mkdir(dirname)</span><span id="d9e4" class="la ig hi kw b fi lg lc l ld le">links = (grid_box[i].findAll('a'))<br/>    for f in links:<br/>        html_link = (f['href'])<br/>        html_name = f.text.replace(' ','_').strip()<br/>        html_res = requests.get(html_link)<br/>        <br/>    # creating files with same name as name in html link <br/>        filename =  dirname+'/'+html_name+'.pdf'<br/>        <br/>        if not os.path.isfile(filename):<br/>            pdf = pdfkit.from_url(html_link,filename)<br/>            print('created_file : '+dirname+'/'+html_name+'.pdf'+' successfully')</span></pre><p id="c78e" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><strong class="jm hj">就这些</strong> …..编码，练习，调试，谷歌，再编码。编码时舒适地坐着，保持水分。</p><p id="87ee" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated"><strong class="jm hj"> <em class="lf">快乐编码！！！</em> </strong></p></div></div>    
</body>
</html>