<html>
<head>
<title>[Paper Summary] Playing Atari with Deep Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">【论文摘要】用深度强化学习玩雅达利</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/paper-summary-playing-atari-with-deep-reinforcement-learning-5e48bc080c9d?source=collection_archive---------4-----------------------#2019-08-20">https://medium.com/analytics-vidhya/paper-summary-playing-atari-with-deep-reinforcement-learning-5e48bc080c9d?source=collection_archive---------4-----------------------#2019-08-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="ddc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我能通读DeepMind已经完成的所有工作，这真是让我大吃一惊。这是他们论文的第一条。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/f622a049aae1b7c09b4582f816fa660c.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/0*jA-Bri5vgGaDktlL"/></div></figure><h2 id="2ce0" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">1.介绍</h2><p id="3895" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated"><strong class="ih hj"> 1.1 —强化学习(RL)比监督学习更难训练的原因是:</strong></p><ol class=""><li id="e7f1" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated">通常稀疏、嘈杂、延迟的标量奖励信号</li><li id="0ef1" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">数据样本不是独立的、高度相关的状态序列。</li><li id="9ca2" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">数据分布会发生变化，因此它不应该采用固定的底层分布。</li></ol><p id="b674" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 1.2 —他们如何克服RL问题:</strong></p><ol class=""><li id="68ba" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated">对于复杂RL环境中的原始视频数据</li></ol><p id="fb35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-&gt;使用具有Q学习的CNN，使用随机梯度下降来更新权重</p><p id="58d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.对于相关数据和非平稳分布</p><p id="48e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">-&gt;一种经验回复机制，随机采样先前的转换</p><p id="4003" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">1.3——他们建造的网络超越了之前所有的RL和人类玩家</strong></p><ol class=""><li id="7176" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated">没有游戏的具体信息或手工设计的视觉功能</li><li id="4a60" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">无法访问仿真器的内部状态</li><li id="f78c" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">刚刚从视频输入中得知，奖励，终端信号，以及可能的行动集合。</li></ol><h2 id="1fe1" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">2.背景</h2><p id="3959" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated"><strong class="ih hj"> 2.1 —什么代理可以观察:</strong></p><ol class=""><li id="8e95" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated">来自游戏模拟器的图像(无法访问内部状态)</li><li id="d6c0" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">奖励依赖于先前的一系列行动和观察。</li></ol><p id="5e88" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.2 —仅从当前屏幕无法全面了解当前情况。</strong></p><ol class=""><li id="560a" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated">运用一系列的行动、观察和学习策略。</li><li id="09a4" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">有限马尔可夫决策过程，其中每个序列是一个不同的状态。</li></ol><p id="44e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.3 —目标是最大化未来回报</strong></p><ol class=""><li id="37b3" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated">未来的奖励会打折扣</li><li id="ef62" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">最佳行动值</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kz"><img src="../Images/d582be3bc4ab0699bf65971640167d57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*Qt7HbR1SL88npDfSqMDH8g.png"/></div></figure><p id="101c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">2.4—RL培训工作如何进行？</strong></p><ol class=""><li id="2b3c" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated">许多RL背后的基本思想是<strong class="ih hj">估计动作值函数，通过使用Bellman方程作为迭代更新，</strong>这样的值迭代收敛到最优动作值函数。</li><li id="3d8a" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">使用带权的非线性函数逼近器作为Q网络</li><li id="2b59" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">最小化Q-网络和目标之间的误差的损失函数序列，通过如下的贝尔曼方程的作用值函数</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es la"><img src="../Images/995404584fd8b658a273f07dd0d36387.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*K6hlbr4cKw4E9KSa1F4j0A.png"/></div></figure><p id="5134" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2.5 —作品特点</strong></p><ol class=""><li id="a91c" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated">无模型—使用来自仿真器的样本，而不构建仿真器的估计值</li><li id="a7e0" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">政策外——贪婪策略，对状态空间进行充分探索。</li></ol><h2 id="39de" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">3.深度强化学习</h2><p id="47b9" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">目标是将RL算法连接到深度神经网络，该网络直接对RGB图像进行操作，并使用随机梯度更新。</p><p id="a87b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">利用经验重放，将Q-learning更新应用于从存储样本池中随机抽取的经验样本。</p><p id="fb75" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3.1 —优于标准在线Q-learning的优势</strong></p><ol class=""><li id="3f08" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated">经验的每一步都可能用于许多称重更新</li><li id="99e8" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">由于样本之间的强相关性；对样本进行随机化打破了这些相关性，并减少了更新的方差</li><li id="9854" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">重播..</li></ol><h2 id="b68e" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">4.预处理和模型架构</h2><p id="dde2" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated"><strong class="ih hj"> 4.1 —预处理</strong></p><ol class=""><li id="3a3a" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated">原始帧(210 × 160)通过将RGB转换为灰度进行预处理</li><li id="ea70" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">向下采样(110×84)</li><li id="3d08" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">最终裁剪为(84 x 84)的粗糙区域，以获取游戏区域。</li></ol><p id="61e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4.2 —模型架构</strong></p><ol class=""><li id="7259" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated">神经网络的输入:84 × 84 × 4图像(使用历史的最后4帧)</li><li id="36c4" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">第一个隐藏层:步幅为4的16个8 × 8过滤器</li><li id="0c41" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">第二个隐藏层:步长为2的32个4 × 4滤波器</li><li id="0e5a" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">最后一个隐藏层:全连接，由256个整流单元组成</li><li id="237a" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">输出层:完全连接的线性层，每个有效动作都有一个输出</li></ol><h2 id="749c" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">5.实验</h2><ol class=""><li id="eca6" class="kl km hi ih b ii kg im kh iq lb iu lc iy ld jc kq kr ks kt bi translated">RMSProp带32号迷你包</li><li id="8fef" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">0.1电子贪婪</li><li id="97b1" class="kl km hi ih b ii ku im kv iq kw iu kx iy ky jc kq kr ks kt bi translated">代理看到并选择每k帧(k=4 -&gt; k=3)上的动作</li></ol><h2 id="f4af" class="jl jm hi bd jn jo jp jq jr js jt ju jv iq jw jx jy iu jz ka kb iy kc kd ke kf bi translated">6.结论</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es le"><img src="../Images/d89241b2a35d89e1c3471a68dadc88ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mHoOfeXjbgTBdRskdgAF-Q.png"/></div></div></figure><ol class=""><li id="8ae0" class="kl km hi ih b ii ij im in iq kn iu ko iy kp jc kq kr ks kt bi translated">仅使用原始像素作为输入</li></ol><p id="1bdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.结合随机小批量更新和经验回放记忆的q学习</p><p id="4a0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.在没有调整架构或超参数的情况下，它在七个游戏中的六个游戏中表现出色。</p><p id="217d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">【https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf T4】</p></div></div>    
</body>
</html>