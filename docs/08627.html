<html>
<head>
<title>Introduction to the Basic concepts of Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习的基本概念介绍</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-the-basic-concepts-of-machine-learning-30c983523b5f?source=collection_archive---------18-----------------------#2020-08-05">https://medium.com/analytics-vidhya/introduction-to-the-basic-concepts-of-machine-learning-30c983523b5f?source=collection_archive---------18-----------------------#2020-08-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7995b12ca143f7896b1d684bae497b1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ILt62-aUj6zCNgJVIBoNug.jpeg"/></div></div></figure><p id="d85a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">机器学习是一个研究领域，它赋予计算机在没有明确编程的情况下学习的能力。我们需要机器学习是因为:</p><ol class=""><li id="a7d8" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">数据生成增加。</li><li id="06b2" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">从数据中发现趋势和数据。</li><li id="2a7c" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">解决复杂问题。</li><li id="d553" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated">提高决策学习。</li></ol><p id="da93" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有几种不同类型的学习算法。</p><p id="fc4d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">主要的两种类型是我们所说的监督学习和非监督学习。其他:强化学习，推荐系统。</p><p id="8961" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">监督学习算法</strong></p><p id="ffb3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">术语监督学习指的是这样一个事实，即我们给算法一个数据集，其中给出了所谓的“正确答案”。也就是说，我们给出过去的数据，让机器学习预测的模式。例如，我们希望基于同类房屋的过去房屋数据集来预测房屋的价格。这里价格是我们要预测的连续值。这叫回归问题。在监督学习中，我们得到一个数据集，并且已经知道我们的正确输出应该是什么样子，知道输入和输出之间有关系。</p><p id="22b3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">监督学习问题分为“回归”和“分类”问题。在回归问题中，我们试图预测连续输出中的结果，这意味着我们试图将输入变量映射到某个连续函数。在分类问题中，我们试图预测离散输出的结果。换句话说，我们试图将输入变量映射到离散的类别中。<strong class="is hj">举例:</strong></p><p id="8fde" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(a)回归——给定一个人的照片，我们必须根据给定的照片预测他们的年龄</p><p id="2204" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">(b)分类——对于患有肿瘤的患者，我们必须预测肿瘤是恶性的还是良性的。</p><p id="1b3e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">无监督学习</strong></p><p id="00aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">无监督学习允许我们在很少或根本不知道我们的结果应该是什么样的情况下处理问题。我们可以从数据中推导出结构，而不一定知道变量的影响。我们可以通过基于数据中变量之间的关系对数据进行聚类来得到这种结构。</p><p id="af97" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">强化学习</strong></p><p id="8074" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">强化学习是机器学习的一部分，其中一个代理被放置在环境中，他通过执行某些动作并观察从这些动作中获得的回报来学习在该环境中的行为。</p><p id="8e01" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">监督学习是最成熟的，研究最多的，也是大多数机器学习算法使用的学习类型。有监督的学习比没有监督的学习容易得多。</p><h1 id="ff97" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">模型表示</h1><p id="0f7f" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">为了便于将来使用，我们将使用x^{(i)} <em class="lf"> x </em> ( <em class="lf"> i </em>)来表示“输入”变量(在本例中为生活区)，也称为输入特征，使用y^{(i)} <em class="lf"> y </em> ( <em class="lf"> i </em>)来表示我们试图预测的“输出”或目标变量(价格)。一对(x^{(i)}，y^{(i)} )( <em class="lf"> x </em> ( <em class="lf"> i </em>)，<em class="lf"> y </em> ( <em class="lf"> i </em>))被称为训练样本，我们将用来学习的数据集——y^{(i)} {(x^{(i)}的m个训练样本列表)；i = 1，.。。，m}( <em class="lf"> x </em> ( <em class="lf"> i </em>)，<em class="lf">y</em>(<em class="lf">I</em>))；<em class="lf"> i </em> =1，…，<em class="lf"> m </em> —称为训练集。请注意，符号中的上标“(I)”只是训练集的一个索引，与取幂无关。我们还将使用X来表示输入值的空间，使用Y来表示输出值的空间。在这个例子中，X = Y = ℝ.</p><p id="aefa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了稍微更正式地描述监督学习问题，我们的目标是，给定一个训练集，学习一个函数h : X → Y，使得h(x)是Y的相应值的“良好”预测器。出于历史原因，这个函数h被称为假设。形象地看，这个过程是这样的:</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/8d89e54a86bdd2f113531b16b1c72569.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*_-79oKkys-f5QnP4hexE5g.png"/></div></figure><p id="6ac8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我们试图预测的目标变量是连续的，比如在我们的住房例子中，我们称学习问题为回归问题。当y只能取少量的离散值时(例如，如果给定居住面积，我们想预测一个住所是房子还是公寓)，我们称之为分类问题。</p><h1 id="929a" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">价值函数</h1><p id="84be" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">我们可以通过使用一个<strong class="is hj">成本函数</strong>来衡量我们假设函数的准确性。这是假设的所有结果与x的输入和y的实际输出的平均差(实际上是一个更好的平均版本)。</p><p id="b63f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们使用称为梯度下降算法来最小化成本函数。事实证明，梯度下降是一种更通用的算法，不仅用于线性回归。它实际上在机器学习中被广泛使用。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/ec1b7910918228e2f16dad0789c50cb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*r5lo3MQUZczfVuFJ1sA97Q.png"/></div></figure><p id="2f46" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设我们基于假设函数的域\theta_0 <em class="lf"> θ </em> 0和\theta_1 <em class="lf"> θ </em> 1来绘制假设函数的图形(实际上我们是将成本函数绘制成参数估计的函数)。我们并没有画出x和y本身，而是画出我们假设函数的参数范围，以及选择一组特定参数所产生的成本。</p><p id="a9ed" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们把\theta_0 <em class="lf"> θ </em> 0放在x轴上，把\theta_1 <em class="lf"> θ </em> 1放在y轴上，代价函数放在垂直的z轴上。我们图表上的点将是使用我们的假设和那些特定θ参数的成本函数的结果。下图描述了这样的设置。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/29863459d819cd8a4d5711dd01b29662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*UtR-eiEaEM5zts65l0h1-A.png"/></div></figure><p id="0b89" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我们的成本函数位于图表的最底部时，也就是当它的值最小时，我们就知道我们成功了。红色箭头显示图表中的最小点。</p><p id="e085" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们的方法是对成本函数求导(函数的切线)。切线的斜率是该点的导数，它会给我们一个前进的方向。我们沿着下降速度最快的方向逐步降低成本函数。每一步的大小由参数α决定，该参数称为学习速率。</p><p id="b3aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，上图中每个“星”之间的距离代表一个由我们的参数α决定的步长。较小的α会导致较小的步长，而较大的α会导致较大的步长。迈步的方向由J(\theta_0，\theta_1) <em class="lf"> J </em> ( <em class="lf"> θ </em> 0，<em class="lf"> θ </em> 1)的偏导数决定。根据一个人在图上的起点，他可能会在不同的点结束。上图向我们展示了两个不同的起点，最终到达两个不同的地方。</p><p id="dbd4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">梯度下降算法是:</p><p id="9227" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">重复直到收敛:</p><p id="1b83" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">\ theta _ j:= \ theta _ j—\ alpha \ frac { \ partial } { \ partial \ theta _ j } j(\ theta _ 0，\ theta _ 1)<em class="lf">θj</em>:=<em class="lf">θj</em>—<em class="lf">α</em>∂<em class="lf">θj</em>∂<em class="lf">j</em>(<em class="lf">θ</em>0，<em class="lf"> θ </em> 1)</p><p id="26f6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在哪里</p><p id="1e14" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">j=0，1表示特征索引号。</p><p id="04e4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在每次迭代j时，应该同时更新参数\theta_1、\theta_2、…、\theta_n <em class="lf"> θ </em> 1、<em class="lf"> θ </em> 2、…、<em class="lf"> θn </em>。在计算j^{(th)}<em class="lf">j</em>(<em class="lf">th</em>)迭代中的另一个参数之前更新一个特定参数会导致错误的实现。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ln"><img src="../Images/7734a5ab611e401213268ad81c9a8275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*DyE3PjG24NXL82009t0_JA.png"/></div></div></figure><h1 id="8051" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">学习率</h1><p id="db49" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">达到最小值或底部的步长称为<strong class="is hj">学习速率</strong>。我们可以用更大的步长/更高的学习率覆盖更多的区域，但是有超过最小值的风险。另一方面，小步/更小的学习率会消耗大量时间到达最低点。另外，我们应该调整我们的参数\alpha <em class="lf"> α </em>以确保梯度下降算法在合理的时间内收敛。未能收敛或获得最小值的时间太长意味着我们的步长是错误的。</p><p id="91c3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">将梯度下降算法应用于线性回归模型(只有一个目标变量)</strong></p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/a88466b364f5338d27e49fd24ec41acd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*SAgRpICi0wjHVvkFzlQ0vA.png"/></div></figure><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/8c16a70d540f58438c0d9a61e598e2c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*fm4_3aHDnl0d6K-4FyuuxQ.png"/></div></figure><p id="d76d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们看看梯度下降是如何工作的。的成本函数</p><p id="4660" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性回归总是像这样的弓形函数。用专业术语来说，这叫做凸函数。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/fa6fabc220e920079d1ea99d9a05cad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*Qpk_2NtVdgEoaDhrY1XQrA.png"/></div></figure><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ln"><img src="../Images/d63d3047d073e7525b4470cc39b2003c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*B43seEpHMRRqmIoYWNx1TA.png"/></div></div></figure><p id="626a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在θ0和θ1的不同值下，我们得到h(x)函数图中的直线。这条线代表最小误差线。</p><p id="bbe7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">机器学习—多元线性回归</strong></p><p id="cb89" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">多变量梯度下降</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/eef76dda4b40426d625a1af0a5fbbac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*UHZa_BW4X3cvUypgh9XLtQ.png"/></div></figure><p id="6ce5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是梯度下降的样子。我们要重复更新每个参数θj，根据θj减去α乘以这个导数项。我们再一次把它写成θ的J，所以θJ更新为θJ减去学习率α乘以导数，一个偏导数</p><p id="504e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">成本函数关于参数θj的导数</p><h2 id="6605" class="lq kd hi bd ke lr ls lt ki lu lv lw km jb lx ly kq jf lz ma ku jj mb mc ky md bi translated">实践中的梯度下降I —特征缩放</h2><p id="06ae" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">如果将梯度下降应用于未缩放的要素，梯度可能会花费很长时间，并且可能会来回振荡，并需要很长时间才能最终找到全局最小值，如下所示。</p><p id="1234" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">更一般地说，当我们执行特征缩放时，我们通常想要做的是将每个特征放入大约-1到+1的范围内，具体地说，你的特征x0总是等于1。所以，这已经在这个范围内了，但是你可能会用不同的数字来划分其他的特性，从而得到这个范围。数字-1和+1并不太重要。所以，如果你有一个特征，x1，在0和3之间，这不是问题。如果你最终有一个不同的特征，风在-2和+ 0.5之间，这足够接近-1和+1，你知道，这很好，这很好。</p><p id="4179" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">只有当你有一个不同的特征，比如介于-100 tp +100之间的X 3，那么，这是一个非常不同于-1和+1的值。因此，这可能是一个不太熟练的特征，同样，如果您的特征取值范围非常非常小，那么如果X 4取值范围在-0.0001和正0.0001之间，那么它取值范围也比-1到+1的范围小得多。同样，我认为这个特性的扩展性很差。</p><p id="a967" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面解释的东西显示在下面的幻灯片中</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/bb83d0346859e27a30afc7c4ab625bc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*mEp3nj8dZrtA1fMh_Qfn-A.png"/></div></figure><p id="7efa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有两种技术可以帮助做到这一点，即<strong class="is hj">特征缩放</strong>和<strong class="is hj">均值归一化</strong>。特征缩放包括将输入值除以输入变量的范围(即最大值减去最小值)，从而得到一个仅为1的新范围。均值归一化包括从某个输入变量的值中减去该输入变量的平均值，从而得出该输入变量的新平均值正好为零。要实施这两种技术，请按以下公式所示调整输入值:</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es me"><img src="../Images/32f5163b1994dcb7e2ea522a9f1b0c07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*OGCC0yJhh1poWcdCKuSa-Q.png"/></div></figure><h1 id="b863" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">正态方程</h1><p id="4825" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">梯度下降给出了最小化j的一种方法，让我们讨论第二种方法，这一次明确地执行最小化，而不求助于迭代算法。在“正规方程”方法中，我们将通过显式计算J相对于θj的导数，并将它们设置为零，从而使J最小。这使得我们无需迭代就能找到最佳θ。法线方程公式如下所示:</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/b9c5033a26dd9c4c532a531995c85b40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*ymnMlraAeRUzq3mYDb5QCg.png"/></div></figure><p id="2fe0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面是梯度下降和正常方程的比较:</p><p id="fb5d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">梯度下降:</strong></p><p id="86ea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">需要选择α</p><p id="58cf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">需要多次迭代</p><p id="3f44" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当n较大时工作良好</p><p id="9500" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">正规方程</strong></p><p id="8575" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">没必要选α</p><p id="94b4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">不需要迭代</p><p id="6c8e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果n很大，速度很慢</p><h1 id="57fb" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">欠装配:</h1><p id="73d1" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">当模型具有较少的特征，因此不能很好地从数据中学习时。这个模型有很高的偏差。</p><h1 id="31df" class="kc kd hi bd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz bi translated">过度装配:</h1><p id="9359" class="pw-post-body-paragraph iq ir hi is b it la iv iw ix lb iz ja jb lc jd je jf ld jh ji jj le jl jm jn hb bi translated">当模型具有复杂的函数，因此能够很好地拟合数据，但不能进行归纳以预测新数据时。这个模型有很高的方差。</p><p id="7f09" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">有三个主要选项来解决过度拟合问题:</p><ol class=""><li id="5f66" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated"><strong class="is hj">减少特征数量:</strong>手动选择保留哪些特征。这样做，我们可能会错过一些重要的信息，如果我们扔掉一些功能。</li><li id="83d9" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated"><strong class="is hj">正则化:</strong>保留所有的特征，但减少权重w的大小。当我们有许多稍微有用的特征时，正则化工作得很好。</li><li id="7b6b" class="jo jp hi is b it jx ix jy jb jz jf ka jj kb jn jt ju jv jw bi translated"><strong class="is hj">提前停止:</strong>当我们迭代地训练一个学习算法时，比如使用梯度下降，我们可以衡量模型的每次迭代执行得有多好。达到一定的迭代次数后，每次迭代都会改进模型。然而，在这一点之后，模型的概括能力会减弱，因为它开始过度拟合训练数据。</li></ol><p id="7173" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">仅此而已。</p><p id="ce44" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">感谢阅读:)</p></div></div>    
</body>
</html>