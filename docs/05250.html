<html>
<head>
<title>Introduction and Installation of PySpark for Mac Users</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向Mac用户的PySpark介绍和安装</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-setup-pyspark-in-your-mac-2e21f0c8f9b5?source=collection_archive---------5-----------------------#2020-04-15">https://medium.com/analytics-vidhya/how-to-setup-pyspark-in-your-mac-2e21f0c8f9b5?source=collection_archive---------5-----------------------#2020-04-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/908d57d1a52a45d757d93621e27cba86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g8xBL4mAaof3CK0TZXcYgQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:https://www.lynda.com/</figcaption></figure><p id="df2d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇博客简要地向你介绍了在大数据和人工智能中不算新但却是应用最广泛的技术之一。阿帕奇火花！</p><p id="616e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">是的，Spark现在已经快10岁了。这篇博客还介绍了PySpark以及在您的机器上安装它所需的步骤。让我们开始吧。</p><h1 id="c24b" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">什么是火花？为什么要火花呢？</h1><p id="3307" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">Spark是一个开源的大数据平台，有助于以快速高效的方式处理大型数据集。它是一个大规模数据处理框架，通过将数据拆分到几个节点(可以把它想象成计算机)中，并在集群的每个节点中并行执行计算任务，从而执行集群计算并快速处理任务，<em class="kw">。</em>当我们不得不处理大到无法放入机器内存的数据时，我们通常会使用大数据技术。你们中的许多人可能想知道为什么我们不直接用Hadoop的MapReduce来完成这样的工作。嗯，那是因为Spark比Hadoop的MapReduce快100倍。以下是如何…</p><blockquote class="kx ky kz"><p id="fcaf" class="iv iw kw ix b iy iz ja jb jc jd je jf la jh ji jj lb jl jm jn lc jp jq jr js hb bi translated">MapReduce在每次map和Reduce操作后将数据写入磁盘，而Spark在每次转换后将大部分数据保存在内存中。</p></blockquote><p id="2289" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">无论是想要快速查询、分析、转换或从“大”数据中获得洞察力的数据分析师还是数据科学家，Spark都是首选解决方案。因为spark有SQL、机器学习、流处理和图形计算的库。它还支持各种编程语言，如Java、Python、r。</p><h1 id="f8b6" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">PySpark</h1><p id="de07" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated"><strong class="ix hj"> PySpark </strong>是Apache Spark发布的Spark框架API，用<strong class="ix hj"> Spark支持<strong class="ix hj"> Python </strong>。使用这个API可以轻松利用Spark的所有功能。让我们看看如何在我们的机器上设置PySpark。</strong></p></div><div class="ab cl ld le gp lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="hb hc hd he hf"><h1 id="309c" class="jt ju hi bd jv jw lk jy jz ka ll kc kd ke lm kg kh ki ln kk kl km lo ko kp kq bi translated">先决条件:</h1><p id="4c8b" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated"><em class="kw">*跳转到</em> <strong class="ix hj"> <em class="kw">安装</em> </strong> <em class="kw">如果你已经安装了所有这些* </em></p><h2 id="9d8e" class="lp ju hi bd jv lq lr ls jz lt lu lv kd jg lw lx kh jk ly lz kl jo ma mb kp mc bi translated">1.公司自产自用</h2><p id="bf76" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">如果未安装，请遵循以下步骤:</p><ul class=""><li id="c26a" class="md me hi ix b iy iz jc jd jg mf jk mg jo mh js mi mj mk ml bi translated">转到终端并运行:</li></ul><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="f45a" class="lp ju hi mr b fi mv mw l mx my">$ /usr/bin/ruby -e “$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"</span></pre><ul class=""><li id="7a19" class="md me hi ix b iy iz jc jd jg mf jk mg jo mh js mi mj mk ml bi translated">安装成功后，运行以下命令确认安装:</li></ul><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="16f5" class="lp ju hi mr b fi mv mw l mx my">brew doctor</span></pre><p id="5d3a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你得到一个消息说“你的系统已经准备好了”，你可以进入下一步。</p><h2 id="97a1" class="lp ju hi bd jv lq lr ls jz lt lu lv kd jg lw lx kh jk ly lz kl jo ma mb kp mc bi translated">2.<strong class="ak"> Python </strong></h2><p id="315b" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">我假设您已经安装了python，否则您可以再次使用Homebrew安装它:</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="aba6" class="lp ju hi mr b fi mv mw l mx my">brew install python3</span></pre><h2 id="349d" class="lp ju hi bd jv lq lr ls jz lt lu lv kd jg lw lx kh jk ly lz kl jo ma mb kp mc bi translated">3.<strong class="ak">jupyter-笔记本(可选)</strong></h2><p id="9664" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">你可以通过Anaconda navigator或者跟随这个<a class="ae iu" href="https://jupyter.org/install" rel="noopener ugc nofollow" target="_blank">链接</a>或者通过Homebrew来安装它。</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="19b6" class="lp ju hi mr b fi mv mw l mx my">brew install jupyter</span></pre><p id="3ca8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你的所有先决条件都完好无损，那么你就“准备好了！”。</p></div><div class="ab cl ld le gp lf" role="separator"><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li lj"/><span class="lg bw bk lh li"/></div><div class="hb hc hd he hf"><h1 id="c4c8" class="jt ju hi bd jv jw lk jy jz ka ll kc kd ke lm kg kh ki ln kk kl km lo ko kp kq bi translated">装置</h1><h2 id="e2a6" class="lp ju hi bd jv lq lr ls jz lt lu lv kd jg lw lx kh jk ly lz kl jo ma mb kp mc bi translated">1.<strong class="ak">安装Java </strong>:</h2><p id="2a31" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">我们需要先安装Java，因为spark是用Scala写的，Scala是一种Java虚拟机语言。</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="6775" class="lp ju hi mr b fi mv mw l mx my">brew cask install java</span></pre><p id="689f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这将安装最新版本的java(安装需要相当长的时间)。</p><h2 id="9ff9" class="lp ju hi bd jv lq lr ls jz lt lu lv kd jg lw lx kh jk ly lz kl jo ma mb kp mc bi translated"><strong class="ak"> 2。安装Scala : </strong></h2><p id="6efb" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">对于安装Apache-spark，Scala是一个依赖项。因此，使用brew安装Scala:</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="52e3" class="lp ju hi mr b fi mv mw l mx my">brew install scala</span></pre><p id="5f09" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您可以通过以下方式确认安装:</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="44a9" class="lp ju hi mr b fi mv mw l mx my">brew info scala</span></pre><p id="5f66" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你会看到这样的东西:</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mz"><img src="../Images/874f9ab7318f6a4fa474a94fd27d3ea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S8k5FjHc8WI2xPGioDneMg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片1</figcaption></figure><h2 id="26ba" class="lp ju hi bd jv lq lr ls jz lt lu lv kd jg lw lx kh jk ly lz kl jo ma mb kp mc bi translated">3.安装Spark</h2><p id="0276" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">Apache Spark是一个处理大数据的分布式框架。我们现在可以马上安装spark。</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="a548" class="lp ju hi mr b fi mv mw l mx my">brew install apache-spark</span></pre><p id="6cdf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">并确认(像我们一直做的那样🤷🏻‍♀️)由:</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="e684" class="lp ju hi mr b fi mv mw l mx my">brew info apache-spark</span></pre><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/22d98307c123cdb0be722a6ea56bb67d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OzPSepJW_XAjoPQ2fQg0xw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片2</figcaption></figure><h2 id="47db" class="lp ju hi bd jv lq lr ls jz lt lu lv kd jg lw lx kh jk ly lz kl jo ma mb kp mc bi translated"><strong class="ak"> 4。安装PySpark </strong></h2><p id="8944" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">使用pip3安装pyspark:</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="2697" class="lp ju hi mr b fi mv mw l mx my">pip3 install pyspark</span></pre><h2 id="58e9" class="lp ju hi bd jv lq lr ls jz lt lu lv kd jg lw lx kh jk ly lz kl jo ma mb kp mc bi translated">5.设置环境变量。</h2><p id="7fdf" class="pw-post-body-paragraph iv iw hi ix b iy kr ja jb jc ks je jf jg kt ji jj jk ku jm jn jo kv jq jr js hb bi translated">我们需要定义某些环境变量和路径，以便通过pyspark访问Spark</p><ul class=""><li id="7d93" class="md me hi ix b iy iz jc jd jg mf jk mg jo mh js mi mj mk ml bi translated">打开您的终端，然后:</li></ul><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="7e11" class="lp ju hi mr b fi mv mw l mx my">cd ~</span><span id="6773" class="lp ju hi mr b fi nb mw l mx my">vim .bashrc</span></pre><p id="33b8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后定义以下变量:</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="a2ae" class="lp ju hi mr b fi mv mw l mx my">export JAVA_HOME=/Library/java/JavaVirtualMachines/adoptopenjdk-8.jdk/contents/Home/</span><span id="9571" class="lp ju hi mr b fi nb mw l mx my">export JRE_HOME=/Library/java/JavaVirtualMachines/openjdk-13.jdk/contents/Home/jre/</span><span id="5803" class="lp ju hi mr b fi nb mw l mx my">export SPARK_HOME=/usr/local/Cellar/apache-spark/2.4.5/libexec</span><span id="c100" class="lp ju hi mr b fi nb mw l mx my">export PATH=/usr/local/Cellar/apache-spark/2.4.5/bin:$PATH</span><span id="a8aa" class="lp ju hi mr b fi nb mw l mx my">export PYSPARK_PYTHON=/usr/local/bin/python3</span><span id="a270" class="lp ju hi mr b fi nb mw l mx my">export PYSPARK_DRIVER_PYTHON=jupyter</span><span id="c5b9" class="lp ju hi mr b fi nb mw l mx my">export PYSPARK_DRIVER_PYTHON_OPTS=’notebook’</span></pre><p id="4dee" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">确保在第三和第四个变量中提供正确的spark版本。</p><p id="3419" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">快好了！现在我们剩下的就是检查pyspark是否工作。</p><p id="a8e9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在键入:</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="0b3d" class="lp ju hi mr b fi mv mw l mx my">pyspark</span></pre><p id="8453" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在您的终端中，如果spark成功安装了所有依赖项，并且环境设置正确，您应该能够获得:</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nc"><img src="../Images/3c9b9936d37a64f8632261ea1737873a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1htc9C5PdGOO_mq1J9AhzQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图3</figcaption></figure><p id="b4d5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">恭喜你！！！👏🏻您已经在机器上成功安装了PySpark。现在，您可以启动jupyter笔记本来进一步体验PySpark功能，也可以开始在您的。py脚本好了，我们已经在学习PySpark的第一步了！</p><p id="2507" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在启动您的jupyter笔记本并执行以下命令。</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="aa9b" class="lp ju hi mr b fi mv mw l mx my">import os<br/>import pyspark</span></pre><p id="ec92" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">默认情况下，PySpark不在sys.path(您设置它的路径)上，不会被用作常规库。因此，需要在运行时将PySpark添加到sys.path中。我们该怎么做？不在话下..“<strong class="ix hj"> <em class="kw"> findspark </em> </strong>”包可以帮你做到。使用以下命令安装<em class="kw"> findspark </em>:</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="8e96" class="lp ju hi mr b fi mv mw l mx my">pip3 install findspark</span></pre><p id="5651" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="kw"> findspark </em>主要是搜索你的spark设置的环境路径，并在你的jupyter笔记本中初始化它。</p><p id="c63c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下代码具有相同的功能:</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="7bdc" class="lp ju hi mr b fi mv mw l mx my">import findspark<br/>findspark.init()</span></pre><p id="28e8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，为了确保jupyter一切正常，请尝试以下命令:</p><pre class="mm mn mo mp fd mq mr ms mt aw mu bi"><span id="a1bd" class="lp ju hi mr b fi mv mw l mx my">#executing the shell.py inside the 'SPARK_HOME' environment<br/>exec(open(os.path.join(os.environ["SPARK_HOME"],'python/pyspark/shell.py')).read())</span></pre><p id="8f27" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">编译完<em class="kw"> exec </em>命令后，您将再次看到:</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nd"><img src="../Images/4f8263fad03edeecb5287cb9526a6363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B-bGdv8I1dMnTe6iM7801Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图4</figcaption></figure><p id="9542" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如您在<em class="kw">图3和图4中看到的，</em>我们已经为自己创建了SparkSession。你在“火花”中看到的是:</p><figure class="mm mn mo mp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ne"><img src="../Images/65525358c74c5f3207993839b7dc2719.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kb_VoDlPMQtGNW4eKHXtzA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图5</figcaption></figure><p id="5a93" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">瞧啊。！一切就绪！！👍🏼现在，您可以将spark功能与python结合使用，非常高效、快速地处理大型数据集。</p><p id="2403" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在接下来的故事中，我将介绍与rdd、数据帧和其他相关主题相关的概念。在那之前，再见。</p></div></div>    
</body>
</html>