<html>
<head>
<title>What is onnx</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是onnx</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/speedup-pytorch-inference-with-onnx-284d6a47936e?source=collection_archive---------4-----------------------#2020-08-10">https://medium.com/analytics-vidhya/speedup-pytorch-inference-with-onnx-284d6a47936e?source=collection_archive---------4-----------------------#2020-08-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="1e12" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">开放神经网络交换(ONNX)是一个开源的人工智能生态系统，允许我们交换深度学习模型。这有助于我们使模型便于携带。</p><p id="1bfa" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">在高层次上，onnx允许我们在不同的深度学习框架中移动我们的模型。目前，ONNX中有对PyTorch、CNTK、MXNet和Caffe2的原生支持，但也有针对TensorFlow和CoreML的转换器。ONNX还使访问硬件优化变得更加容易。</p><p id="37bc" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">在这个博客中，我们将看看如何将pytorch模型转换成onnx格式，并将推理转换成cpu系统。</p></blockquote><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jh"><img src="../Images/7a748a44ca9731823c6eb2a800949630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*o5doPyWdduatUKtX.PNG"/></div></div></figure><h2 id="e0a8" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">知识</h2><p id="1354" class="pw-post-body-paragraph ii ij hi il b im kr io ip iq ks is it ke kt iw ix ki ku ja jb km kv je jf jg hb bi translated">以下是根据您的硬件资源可以使用的提供商列表。我们将在此会话中使用CPUExecutionProvider。</p><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="026d" class="jt ju hi kx b fi lb lc l ld le">providers = [<br/>  "CUDAExecutionProvider",<br/>  "CPUExecutionProvider",            <br/>  "TensorrtExecutionProvider",<br/>  "DnnlExecutionProvider",          <br/>]</span></pre><p id="c985" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated"><strong class="il hj">谨记</strong></p><ul class=""><li id="88eb" class="lf lg hi il b im in iq ir ke lh ki li km lj jg lk ll lm ln bi translated">当您将输入传递给onnx时，您必须使用您在导出时提供的名称来创建输入字典。</li><li id="0dd6" class="lf lg hi il b im lo iq lp ke lq ki lr km ls jg lk ll lm ln bi translated">Onnx采用numpy数组。</li></ul><p id="89e1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">让我们编码…</p><p id="8d91" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">从这里开始，博客是在jupyter_to_medium的帮助下完成的。对于jupyter笔记本和博客创作者来说，这是一个很棒的包。如果你在代码方面遇到任何困难，这是我的<a class="ae lt" href="https://colab.research.google.com/drive/1oQuUp7g1dj5dfjfo0BIcuZUHOVgQsV08?usp=sharing" rel="noopener ugc nofollow" target="_blank"> colab笔记本</a></p><h2 id="0122" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">火车模型</h2><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="c674" class="jt ju hi kx b fi lb lc l ld le">from __future__ import print_function<br/>import argparse<br/>import sys<br/>import torch<br/>import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torch.optim as optim<br/>from torchvision import datasets, transforms<br/>from torch.optim.lr_scheduler import StepLR<br/><br/><br/>class Net(nn.Module):<br/>    def __init__(self):<br/>        super(Net, self).__init__()<br/>        self.conv1 = nn.Conv2d(1, 32, 3, 1)<br/>        self.conv2 = nn.Conv2d(32, 64, 3, 1)<br/>        self.dropout1 = nn.Dropout2d(0.25)<br/>        self.dropout2 = nn.Dropout2d(0.5)<br/>        self.fc1 = nn.Linear(9216, 128)<br/>        self.fc2 = nn.Linear(128, 10)<br/><br/>    def forward(self, x):<br/>        x = self.conv1(x)<br/>        x = F.relu(x)<br/>        x = self.conv2(x)<br/>        x = F.relu(x)<br/>        x = F.max_pool2d(x, 2)<br/>        x = self.dropout1(x)<br/>        x = torch.flatten(x, 1)<br/>        x = self.fc1(x)<br/>        x = F.relu(x)<br/>        x = self.dropout2(x)<br/>        x = self.fc2(x)<br/>        output = F.log_softmax(x, dim=1)<br/>        return output<br/><br/><br/>def train(args, model, device, train_loader, optimizer, epoch):<br/>    model.train()<br/>    for batch_idx, (data, target) in enumerate(train_loader):<br/>        data, target = data.to(device), target.to(device)<br/>        optimizer.zero_grad()<br/>        output = model(data)<br/>        loss = F.nll_loss(output, target)<br/>        loss.backward()<br/>        optimizer.step()<br/>        if batch_idx % args.log_interval == 0:<br/>            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(<br/>                epoch, batch_idx * len(data), len(train_loader.dataset),<br/>                100. * batch_idx / len(train_loader), loss.item()))<br/>            if args.dry_run:<br/>                break<br/><br/><br/>def test(model, device, test_loader):<br/>    model.eval()<br/>    test_loss = 0<br/>    correct = 0<br/>    with torch.no_grad():<br/>        for data, target in test_loader:<br/>            data, target = data.to(device), target.to(device)<br/>            output = model(data)<br/>            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss<br/>            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability<br/>            correct += pred.eq(target.view_as(pred)).sum().item()<br/><br/>    test_loss /= len(test_loader.dataset)<br/><br/>    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(<br/>        test_loss, correct, len(test_loader.dataset),<br/>        100. * correct / len(test_loader.dataset)))<br/><br/><br/>def main():<br/>    # Training settings<br/>    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')<br/>    parser.add_argument('--batch-size', type=int, default=64, metavar='N',<br/>                        help='input batch size for training (default: 64)')<br/>    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',<br/>                        help='input batch size for testing (default: 1000)')<br/>    parser.add_argument('--epochs', type=int, default=14, metavar='N',<br/>                        help='number of epochs to train (default: 14)')<br/>    parser.add_argument('--lr', type=float, default=1.0, metavar='LR',<br/>                        help='learning rate (default: 1.0)')<br/>    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',<br/>                        help='Learning rate step gamma (default: 0.7)')<br/>    parser.add_argument('--no-cuda', action='store_true', default=False,<br/>                        help='disables CUDA training')<br/>    parser.add_argument('--dry-run', action='store_true', default=False,<br/>                        help='quickly check a single pass')<br/>    parser.add_argument('--seed', type=int, default=1, metavar='S',<br/>                        help='random seed (default: 1)')<br/>    parser.add_argument('--log-interval', type=int, default=10, metavar='N',<br/>                        help='how many batches to wait before logging training status')<br/>    parser.add_argument('--save-model', action='store_true', default=True,<br/>                        help='For Saving the current Model')<br/>    args = parser.parse_args()<br/>    use_cuda = not args.no_cuda and torch.cuda.is_available()<br/><br/>    torch.manual_seed(args.seed)<br/><br/>    device = torch.device("cuda" if use_cuda else "cpu")<br/><br/>    kwargs = {'batch_size': args.batch_size}<br/>    if use_cuda:<br/>        kwargs.update({'num_workers': 1,<br/>                       'pin_memory': True,<br/>                       'shuffle': True},<br/>                     )<br/><br/>    transform=transforms.Compose([<br/>        transforms.ToTensor(),<br/>        transforms.Normalize((0.1307,), (0.3081,))<br/>        ])<br/>    dataset1 = datasets.MNIST('../data', train=True, download=True,<br/>                       transform=transform)<br/>    dataset2 = datasets.MNIST('../data', train=False,<br/>                       transform=transform)<br/>    train_loader = torch.utils.data.DataLoader(dataset1,**kwargs)<br/>    test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)<br/><br/>    model = Net().to(device)<br/>    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)<br/><br/>    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)<br/>    for epoch in range(1, args.epochs + 1):<br/>        train(args, model, device, train_loader, optimizer, epoch)<br/>        test(model, device, test_loader)<br/>        scheduler.step()<br/><br/>    if args.save_model:<br/>        torch.save(model.state_dict(), "mnist_cnn.pt")<br/><br/><br/>if __name__ == '__main__':    <br/>    sys.argv = " "<br/>    main()</span><span id="d5c9" class="jt ju hi kx b fi lu lc l ld le">Train Epoch: 14 [57600/60000 (96%)]	Loss: 0.009900<br/>Train Epoch: 14 [58240/60000 (97%)]	Loss: 0.005598<br/>Train Epoch: 14 [58880/60000 (98%)]	Loss: 0.017147<br/>Train Epoch: 14 [59520/60000 (99%)]	Loss: 0.004029<br/><br/>Test set: Average loss: 0.0257, Accuracy: 9916/10000 (99%)</span></pre><p id="011e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">加载训练好的模型，并用它进行预测</p><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="c1ea" class="jt ju hi kx b fi lb lc l ld le">## load the model<br/>model = Net()<br/>state_dict = torch.load("mnist_cnn.pt")<br/>model.load_state_dict(state_dict)</span><span id="32eb" class="jt ju hi kx b fi lu lc l ld le">##&lt;All keys matched successfully&gt;</span><span id="f3a5" class="jt ju hi kx b fi lu lc l ld le">transform=transforms.Compose([<br/>        transforms.ToTensor(),<br/>        transforms.Normalize((0.1307,), (0.3081,))<br/>        ])<br/>dataset2 = datasets.MNIST('../data', train=False,<br/>                       transform=transform)</span><span id="1144" class="jt ju hi kx b fi lu lc l ld le">image, label = dataset2[100]<br/>image = image.unsqueeze(0)<br/>output = model(image)<br/>output =torch.argmax(output)<br/>print(output, label, output == label)</span><span id="2cd3" class="jt ju hi kx b fi lu lc l ld le">##tensor(6) 6 tensor(True)</span></pre><p id="2198" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">让我们借助torch.onnx原生支持，将Mnist训练好的模型转换成onnx</p><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="41a6" class="jt ju hi kx b fi lb lc l ld le">torch.onnx.export(<br/>    model, ## pass model<br/>    (image), ## pass inpout example<br/>    "mnist.onnx", ##output path<br/>    input_names = ['input'], ## Pass names as per model input name<br/>    output_names = ['output'], ## Pass names as per model output name<br/>    opset_version=11, ##  export the model to the  opset version of the onnx submodule.<br/>    dynamic_axes = { ## this will makes export more generalize to take batch for prediction<br/>        'input' : {0: 'batch', 1: 'sequence'},<br/>        'output' : {0: 'batch'},        <br/>    }<br/>)</span></pre><p id="b527" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">为了进行推断，我们将使用Onnxruntime包，它将根据我们的硬件为我们提供提升。</p><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="6c89" class="jt ju hi kx b fi lb lc l ld le">!pip install onnxruntime<br/># !pip install onnxruntime-gpu #for gpu<br/>from onnxruntime import InferenceSession, SessionOptions, get_all_providers</span></pre><p id="7a58" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">为了从onnx模型中进行预测，我们必须使用与您的硬件兼容的提供程序创建推理会话。这里我们使用的是CPUExecutionProvider。您可以通过在选项中提供更多属性来改进它。</p><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="9845" class="jt ju hi kx b fi lb lc l ld le">def create_model_for_provider(model_path: str, provider: str) -&gt; InferenceSession: <br/>  <br/>  assert provider in get_all_providers(), f"provider {provider} not found, {get_all_providers()}"<br/><br/>  # Few properties than might have an impact on performances (provided by MS)<br/>  options = SessionOptions()<br/>  options.intra_op_num_threads = 1<br/><br/>  # Load the model as a graph and prepare the CPU backend <br/>  return InferenceSession(model_path, options, providers=[provider])<br/>cpu_model = create_model_for_provider("mnist.onnx", "CPUExecutionProvider")</span></pre><p id="f795" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">让我们比较pytorch和onnx的预测时间</p><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="5004" class="jt ju hi kx b fi lb lc l ld le">%%time<br/>out = model(image) # Pytorch model</span><span id="87a1" class="jt ju hi kx b fi lu lc l ld le"><strong class="kx hj">CPU times: user 3.5 ms, sys: 36 Âµs, total: 3.54 ms<br/>Wall time: 6.59 ms</strong></span><span id="2ac4" class="jt ju hi kx b fi lu lc l ld le">#onnx model<br/>%%time<br/>inputs_onnx= {'input':image.numpy()} ## same name as passes in onnx.export<br/>output = cpu_model.run(None, inputs_onnx) ## Here first arguments None becuase we want every output sometimes model return more than one output</span><span id="d539" class="jt ju hi kx b fi lu lc l ld le"><strong class="kx hj">CPU times: user 783 Âµs, sys: 982 Âµs, total: 1.76 ms<br/>Wall time: 1.3 ms</strong></span></pre><p id="0a8c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">我们可以在上面看到onnx做出更快的预测。推理会话中的选项越多，预测时间可能会比上面看到的更长。</p><p id="86f3" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">确保我们的模型给出正确的预测。</p><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="3aca" class="jt ju hi kx b fi lb lc l ld le">output =torch.argmax(torch.tensor(output))<br/>print(output, label, output == label)</span><span id="3a13" class="jt ju hi kx b fi lu lc l ld le">tensor(6) 6 tensor(True)</span></pre><h2 id="baa1" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">Alexnet预训练模型</h2><p id="9454" class="pw-post-body-paragraph ii ij hi il b im kr io ip iq ks is it ke kt iw ix ki ku ja jb km kv je jf jg hb bi translated">在这一节中，我们将看到如何将预训练的模型转换成onnx并使用它</p><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="da8b" class="jt ju hi kx b fi lb lc l ld le">!wget <a class="ae lt" href="https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json" rel="noopener ugc nofollow" target="_blank">https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json</a></span><span id="152c" class="jt ju hi kx b fi lu lc l ld le">!wget <a class="ae lt" href="https://www.learnopencv.com/wp-content/uploads/2019/05/dog.jpg" rel="noopener ugc nofollow" target="_blank">https://www.learnopencv.com/wp-content/uploads/2019/05/dog.jpg</a></span><span id="3e34" class="jt ju hi kx b fi lu lc l ld le">import json<br/>idx2label = []<br/>cls2label = {}<br/>with open("imagenet_class_index.json", "r") as read_file:<br/>    class_idx = json.load(read_file)<br/>    idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]<br/>    cls2label = {class_idx[str(k)][0]: class_idx[str(k)][1] for k in range(len(class_idx))}</span><span id="9904" class="jt ju hi kx b fi lu lc l ld le">!pip install torchvision<br/>from torchvision import models<br/>import torch<br/>alexnet = models.alexnet(pretrained=True)<br/>from torchvision import transforms<br/>transform = transforms.Compose([            #[1]<br/> transforms.Resize(256),                    #[2]<br/> transforms.CenterCrop(224),                #[3]<br/> transforms.ToTensor(),                     #[4]<br/> transforms.Normalize(                      #[5]<br/> mean=[0.485, 0.456, 0.406],                #[6]<br/> std=[0.229, 0.224, 0.225]                  #[7]<br/> )])<br/>from PIL import Image<br/>img = Image.open("dog.jpg")<br/>img_t = transform(img)<br/>batch_t = torch.unsqueeze(img_t, 0)<br/>alexnet.eval()<br/>out = alexnet(batch_t)<br/><br/>_, index = torch.max(out, 1)<br/>percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100</span><span id="4780" class="jt ju hi kx b fi lu lc l ld le">print(idx2label[index[0]], percentage[index[0]].item())</span><span id="9988" class="jt ju hi kx b fi lu lc l ld le">Labrador_retriever 41.58518600463867</span><span id="5d92" class="jt ju hi kx b fi lu lc l ld le">torch.onnx.export(<br/>    alexnet, ## pass model<br/>    (batch_t), ## pass inpout example<br/>    "alexnet.onnx", ##output path<br/>    input_names = ['input'], ## Pass names as per model input name<br/>    output_names = ['output'], ## Pass names as per model output name<br/>    opset_version=11, ##  export the model to the  opset version of the onnx submodule.<br/>    dynamic_axes = { ## this will makes export more generalize to take batch for prediction<br/>        'input' : {0: 'batch', 1: 'sequence'},<br/>        'output' : {0: 'batch'},        <br/>    }<br/>)</span><span id="7f65" class="jt ju hi kx b fi lu lc l ld le">!pip install onnxruntime<br/>from onnxruntime import InferenceSession, SessionOptions, get_all_providers</span><span id="e2c5" class="jt ju hi kx b fi lu lc l ld le">def create_model_for_provider(model_path: str, provider: str) -&gt; InferenceSession: <br/>  <br/>  assert provider in get_all_providers(), f"provider {provider} not found, {get_all_providers()}"<br/><br/>  # Few properties than might have an impact on performances (provided by MS)<br/>  options = SessionOptions()<br/>  options.intra_op_num_threads = 1<br/><br/>  # Load the model as a graph and prepare the CPU backend <br/>  return InferenceSession(model_path, options, providers=[provider])<br/>cpu_model = create_model_for_provider("alexnet.onnx", "CPUExecutionProvider")</span></pre><p id="b611" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">Pytorch模型</p><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="821c" class="jt ju hi kx b fi lb lc l ld le">%%time<br/>out = alexnet(batch_t)</span><span id="821f" class="jt ju hi kx b fi lu lc l ld le"><strong class="kx hj">CPU times: user 56.1 ms, sys: 1.72 ms, total: 57.8 ms<br/>Wall time: 61.1 ms</strong></span></pre><p id="d795" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">onnx模型</p><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="4714" class="jt ju hi kx b fi lb lc l ld le">%%time<br/>inputs_onnx= {'input':batch_t.numpy()} ## same name as passes in onnx.export<br/>output = cpu_model.run(None, inputs_onnx) ## Here first arguments None becuase we want every output sometimes model return more than one output</span><span id="192f" class="jt ju hi kx b fi lu lc l ld le"><strong class="kx hj">CPU times: user 42.9 ms, sys: 60 Âµs, total: 42.9 ms<br/>Wall time: 43.3 ms</strong></span><span id="5948" class="jt ju hi kx b fi lu lc l ld le">output = torch.tensor(output[0])<br/>_, index = torch.max(output, 1)<br/>percentage = torch.nn.functional.softmax(output, dim=1)[0] * 100<br/>print(idx2label[index[0]], percentage[index[0]].item())</span><span id="0c07" class="jt ju hi kx b fi lu lc l ld le"><strong class="kx hj">Labrador_retriever 41.58515167236328</strong></span></pre><h2 id="2214" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">伯特拥抱脸QAModel</h2><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="cf07" class="jt ju hi kx b fi lb lc l ld le">!pip install --upgrade git+https://github.com/huggingface/transformers</span><span id="2015" class="jt ju hi kx b fi lu lc l ld le">from transformers import BertTokenizer, BertForQuestionAnswering<br/>import torch<br/><br/>tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')<br/>model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')</span><span id="2a03" class="jt ju hi kx b fi lu lc l ld le">question = "what is google specialization"<br/>text = "Google LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware."<br/>encoding = tokenizer.encode_plus(question, text)<br/>input_ids, attention_mask, token_type_ids = encoding["input_ids"],encoding["attention_mask"], encoding["token_type_ids"]</span><span id="f3c8" class="jt ju hi kx b fi lu lc l ld le">%%time <br/>start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))</span><span id="7399" class="jt ju hi kx b fi lu lc l ld le"><strong class="kx hj">CPU times: user 564 ms, sys: 26.8 ms, total: 591 ms<br/>Wall time: 689 ms</strong></span><span id="f434" class="jt ju hi kx b fi lu lc l ld le">all_tokens = tokenizer.convert_ids_to_tokens(input_ids)<br/>print(tokenizer.convert_tokens_to_string(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))</span><span id="90b0" class="jt ju hi kx b fi lu lc l ld le">an american multinational technology company that specializes in internet - related services and products</span></pre><h2 id="48fe" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">用普通方法转换人脸模型</h2><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="6dec" class="jt ju hi kx b fi lb lc l ld le">input_ids = torch.tensor([input_ids])<br/>attention_mask = torch.tensor([attention_mask])<br/>token_type_ids = torch.tensor([token_type_ids])</span><span id="b3fa" class="jt ju hi kx b fi lu lc l ld le">torch.onnx.export(<br/>    model,<br/>    (input_ids,attention_mask, token_type_ids),<br/>    "qa.onnx",<br/>    input_names = ['input_ids','attention_mask', 'token_type_ids'], ## Be carefule to write this names<br/>    output_names = ['qa_outputs'], ## Be carefule to write this names<br/>    opset_version=11,<br/>    dynamic_axes = {<br/>        'input_ids' : {0: 'batch', 1: 'sequence'},<br/>        'attention_mask' : {0: 'batch', 1: 'sequence'}, <br/>        'token_type_ids' : {0: 'batch', 1: 'sequence'}, <br/>        'qa_outputs': {0: 'batch'}<br/>    }<br/>)</span></pre><p id="6d84" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">让我们做预测</p><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="d045" class="jt ju hi kx b fi lb lc l ld le">!pip install onnxruntime<br/>from onnxruntime import InferenceSession, SessionOptions, get_all_providers</span><span id="872c" class="jt ju hi kx b fi lu lc l ld le">def create_model_for_provider(model_path: str, provider: str) -&gt; InferenceSession: <br/>  <br/>  assert provider in get_all_providers(), f"provider {provider} not found, {get_all_providers()}"<br/><br/>  # Few properties than might have an impact on performances (provided by MS)<br/>  options = SessionOptions()<br/>  options.intra_op_num_threads = 1<br/><br/>  # Load the model as a graph and prepare the CPU backend <br/>  return InferenceSession(model_path, options, providers=[provider])</span><span id="8985" class="jt ju hi kx b fi lu lc l ld le">cpu_model = create_model_for_provider("qa.onnx", "CPUExecutionProvider")</span><span id="e8d8" class="jt ju hi kx b fi lu lc l ld le">inputs_onnx= {<br/>    'input_ids' : input_ids.numpy(),<br/>    'attention_mask' : attention_mask.numpy(), <br/>    'token_type_ids' : token_type_ids.numpy(),<br/>}</span><span id="b754" class="jt ju hi kx b fi lu lc l ld le">%%time<br/>start_scores, end_scores = cpu_model.run(None, inputs_onnx)</span><span id="f876" class="jt ju hi kx b fi lu lc l ld le"><strong class="kx hj">CPU times: user 629 ms, sys: 1.2 ms, total: 631 ms<br/>Wall time: 632 ms</strong></span><span id="8030" class="jt ju hi kx b fi lu lc l ld le">all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])<br/>print(tokenizer.convert_tokens_to_string(all_tokens[torch.argmax(torch.tensor(start_scores)) : torch.argmax(torch.tensor(end_scores))+1]))</span><span id="0b6d" class="jt ju hi kx b fi lu lc l ld le">an american multinational technology company that specializes in internet - related services and products</span></pre><h2 id="7057" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">将huggingface模型转换为具有本机huggingface支持的Onnx</h2><pre class="ji jj jk jl fd kw kx ky kz aw la bi"><span id="4735" class="jt ju hi kx b fi lb lc l ld le">tokenizer.save_pretrained("qa2/")<br/>model.save_pretrained("qa2/")</span><span id="f352" class="jt ju hi kx b fi lu lc l ld le">from pathlib import Path<br/>path = Path("onnx/qa2.onnx")</span><span id="28b1" class="jt ju hi kx b fi lu lc l ld le">!rm -rf onnx/<br/>from transformers.convert_graph_to_onnx import convert<br/><br/># Handles all the above steps for you<br/>convert(framework="pt",  ## pt for pytorch<br/>        model="qa2",     ## model path<br/>        output=path, <br/>        opset=11,<br/>        pipeline_name = "question-answering") ## pipeline_name is most important when you use this function</span><span id="9db0" class="jt ju hi kx b fi lu lc l ld le">cpu_model = create_model_for_provider("onnx/qa2.onnx", "CPUExecutionProvider")</span><span id="1c1a" class="jt ju hi kx b fi lu lc l ld le">%%time<br/>start_scores, end_scores = cpu_model.run(None, inputs_onnx)</span><span id="d445" class="jt ju hi kx b fi lu lc l ld le">CPU times: user 655 ms, sys: 857 Âµs, total: 655 ms<br/>Wall time: 658 ms</span><span id="bf79" class="jt ju hi kx b fi lu lc l ld le">all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])<br/>print(tokenizer.convert_tokens_to_string(all_tokens[torch.argmax(torch.tensor(start_scores)) : torch.argmax(torch.tensor(end_scores))+1]))</span><span id="ee9b" class="jt ju hi kx b fi lu lc l ld le">an american multinational technology company that specializes in internet - related services and products</span></pre><p id="7198" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated"><strong class="il hj">有用链接</strong></p><div class="lv lw ez fb lx ly"><a href="https://github.com/onnx/onnx" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hj fi z dy md ea eb me ed ef hh bi translated">onnx/onnx</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">开放神经网络交换(ONNX)是一个开放的生态系统，使人工智能开发者能够选择正确的工具作为他们的…</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">github.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm jr ly"/></div></div></a></div><div class="lv lw ez fb lx ly"><a rel="noopener follow" target="_blank" href="/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hj fi z dy md ea eb me ed ef hh bi translated">使用拥抱面转换器和ONNX运行时加速你的NLP管道</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">编辑描述</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">medium.com</p></div></div><div class="mh l"><div class="mn l mj mk ml mh mm jr ly"/></div></div></a></div><div class="lv lw ez fb lx ly"><a href="https://blog.paperspace.com/what-every-ml-ai-developer-should-know-about-onnx/" rel="noopener  ugc nofollow" target="_blank"><div class="lz ab dw"><div class="ma ab mb cl cj mc"><h2 class="bd hj fi z dy md ea eb me ed ef hh bi translated">关于ONNX，每个ML/AI开发者都应该知道的</h2><div class="mf l"><h3 class="bd b fi z dy md ea eb me ed ef dx translated">开放神经网络交换格式(ONNYX)是交换深度学习模型的新标准。它承诺…</h3></div><div class="mg l"><p class="bd b fp z dy md ea eb me ed ef dx translated">blog.paperspace.com</p></div></div><div class="mh l"><div class="mo l mj mk ml mh mm jr ly"/></div></div></a></div><p id="8cc0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">如果你喜欢这篇文章，点击给我买杯咖啡！感谢阅读。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><a href="https://www.payumoney.com/paybypayumoney/#/147695053B73CAB82672E715A52F9AA5"><div class="er es mp"><img src="../Images/226d333c001f2bdbc8bc791892ea31ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*-aubdwm03fkW39OOplP4ag.png"/></div></a></figure><p id="924f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it ke iv iw ix ki iz ja jb km jd je jf jg hb bi translated">你的每一个小小的贡献都会鼓励我创造更多这样的内容。</p></div></div>    
</body>
</html>