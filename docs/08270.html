<html>
<head>
<title>A Gentle Introduction to Extreme Learning Machines[ELM]</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">极限学习机温和介绍[ELM]</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-gentle-introduction-to-extreme-learning-machines-elm-91bb793effa8?source=collection_archive---------8-----------------------#2020-07-23">https://medium.com/analytics-vidhya/a-gentle-introduction-to-extreme-learning-machines-elm-91bb793effa8?source=collection_archive---------8-----------------------#2020-07-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="224b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">为什么是榆树？</h1><p id="18e3" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在过去的几十年中，基于反向传播(BP)的算法在前向神经网络的训练中发挥了主导作用。另一方面，它们缺乏更快的神经网络学习算法。传统的学习算法可能需要几个小时、几天甚至更多的时间来训练神经网络。人们提出了许多算法来提高单前馈神经网络的运算速度和精度，如BP算法及其改进算法。由于BP算法的局限性，网络的泛化能力不理想，容易出现过学习。2004年，Huang G.B提出了极限学习机(ELM ),该学习机在训练前馈神经网络和克服BP算法及其变体所面临的局限性方面显示了其有效性。</p><h1 id="e1e7" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">榆树是什么？</h1><p id="307e" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">极端学习机是一种新的学习算法，用于具有单层或多层隐节点的前馈神经网络，其中输入和隐节点之间的权重是随机分配的，并且在训练和预测阶段从不更新。它使用Moore Penrose广义逆来设置其权重。ELMs能够产生可接受的预测性能，并且其学习速度比其他算法快数千倍，并且其计算成本比通过反向传播算法训练的网络低得多。</p><h1 id="4c06" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">算法:</h1><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es kb"><img src="../Images/33e330d971b5afd8242b30ce8095c8e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*GMc2hETZ8Jbeljr2okmhsg.png"/></div><figcaption class="kj kk et er es kl km bd b be z dx translated">图1榆树的结构</figcaption></figure><p id="bf3f" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">ELM的学习阶段通常包括:</p><ul class=""><li id="126e" class="ks kt hi jf b jg kn jk ko jo ku js kv jw kw ka kx ky kz la bi translated">用随机隐含神经元构造隐含层输出矩阵</li><li id="83e5" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated">查找输出连接。</li></ul><p id="1497" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">图1是ELM网络结构，包括n个输入层神经元、l个隐藏层神经元和m个输出层神经元。考虑一下，</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lg"><img src="../Images/56dd7d28420679c338eb7d27cacc5f57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*7XDsm3nFaWvKiheItb5COA.png"/></div></figure><p id="7714" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">其中矩阵X和矩阵Y可以表示为:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lh"><img src="../Images/29ab8373117b8dd585ae13cd7ba5cabb.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*WVi11xKjSXREltZlkyuFCw.png"/></div></figure><p id="8244" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">其中参数n和m是输入和输出矩阵的维数。</p><p id="bf22" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">然后ELM随机设置输入层和隐含层之间以及隐含层和输出层之间的权重，其中图1中的wij表示第<em class="li"> j </em>个输入层和第<em class="li"> i </em>个隐含层神经元之间的权重，<em class="li"> b </em> jk表示第<em class="li"> j </em>个隐含层和第<em class="li"> k </em>个输出层神经元之间的权重。</p><p id="9e2b" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">ELM随机设置隐藏层神经元的偏置:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lj"><img src="../Images/a3e191c5e0fe56aaf218e1c25515e016.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*EJxTctBEuJ-P9gkR91glYg.png"/></div></figure><p id="89dd" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">ELM选择网络激活函数g(x)。根据图1，输出矩阵T可以表示为</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lk"><img src="../Images/ccd80eb648e809e75feccd82d4065014.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*cT0_1KFLx7gvPdU7nKoksQ.png"/></div></figure><p id="1ecc" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">输出矩阵T的每个列向量是:</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es ll"><img src="../Images/c12b54804f1f32fa861bdb954e02d073.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*0gcpw4Ea-RW3CHqR321vDA.png"/></div></figure><p id="4622" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">从输出矩阵和输出矩阵方程的每一列向量，我们可以得到</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lm"><img src="../Images/f799f964b56acb5f332060aaa7d339d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*A0EQM9qv8bb_9PkV4heQmw.png"/></div></figure><p id="a1be" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">其中T '是T的转置，H是隐藏层的输出。为了获得误差最小的唯一解，我们用最小二乘法计算b的权矩阵值。</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es ln"><img src="../Images/696099bb8eb18cf22a491917aeb2a673.png" data-original-src="https://miro.medium.com/v2/resize:fit:490/format:webp/1*Cyl95NXlSKaworCu4BDxGQ.png"/></div></figure><p id="d146" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">为了提高网络的泛化能力，使结果更加稳定，我们加入了一个正则项。当隐藏层神经元的数量小于训练样本的数量时，β可以表示为</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lo"><img src="../Images/7902e719a218e9f74561ad51e5079143.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*BhmFA6MlE5-ESKj4sC2bUQ.png"/></div></figure><p id="0b5f" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">当隐藏层神经元的数量大于训练样本的数量时，β可以表示为</p><figure class="kc kd ke kf fd kg er es paragraph-image"><div class="er es lp"><img src="../Images/7924d49c988ba746a7f733ad617126d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*Z-068VRJ7b79WIGynI51GQ.png"/></div></figure><h1 id="9f91" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">理论:</h1><p id="2875" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">已证明ELM具有通用近似和分类能力。</p><p id="3c8b" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">只要隐藏神经元的输出函数是非线性分段连续的，并且即使它们的形状和建模未知，具有随机隐藏神经元的(生物)神经网络同时获得<strong class="jf hj">通用逼近</strong>和<strong class="jf hj">分类能力</strong>，并且有限数量的隐藏神经元及其相关连接的变化不影响网络的整体性能。从<a class="ae lq" href="https://arxiv.org/pdf/2004.08867.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>你可以更好的了解数学上的普适近似定理。</p><h1 id="147b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">变体:</h1><p id="0a95" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">随着传统ELM算法的发展，许多改进的ELM算法被提出；同时，ELM的实现范围已经从监督学习进一步扩展到半监督和非监督学习。</p><p id="67bf" class="pw-post-body-paragraph jd je hi jf b jg kn ji jj jk ko jm jn jo kp jq jr js kq ju jv jw kr jy jz ka hb bi translated">榆树变种是，</p><ul class=""><li id="182b" class="ks kt hi jf b jg kn jk ko jo ku js kv jw kw ka kx ky kz la bi translated">完全复杂榆树</li><li id="923b" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated">增量ELM</li><li id="0a48" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated">在线顺序ELM</li><li id="c489" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated">全体榆树</li><li id="9a8b" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated">修剪榆树</li></ul><h1 id="e714" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">可靠性:</h1><p id="4c77" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">机器学习方法的可靠应用在具有挑战性的工程领域变得越来越重要。特别是，极限学习机(ELM)的应用似乎很有前景，因为它们明显简单，并且能够非常有效地处理大型和高维数据集。然而，ELM范例是基于具有随机初始化和固定输入权重的单隐层神经网络的概念，因此本质上是不可靠的。一般而言，神经网络的黑盒特性，尤其是极限学习机(ELM)的黑盒特性，是工程师们在不安全的自动化任务中拒绝应用的主要问题之一。这个特殊的问题是通过几种不同的技术来解决的。一种方法是减少对随机输入的依赖。另一种方法集中于将连续约束结合到elm的学习过程中，该学习过程来自于关于特定任务的先验知识。这是合理的，因为机器学习解决方案必须保证许多应用领域中的安全操作。上述研究表明，具有功能分离和线性读出权重的特殊形式的ELMs特别适合于在输入空间的预定义区域中有效合并连续约束。</p><h1 id="3e4b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">问题:</h1><p id="5053" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">虽然在过去的几年里，高维数据应用已经取得了大量的成果，但以下三个问题值得考虑。</p><ul class=""><li id="5485" class="ks kt hi jf b jg kn jk ko jo ku js kv jw kw ka kx ky kz la bi translated">免调优是ELM最重要的贡献之一。然而，各种方法和应用已经将迭代更新处理应用到原始ELM中以产生良好的泛化性能，例如使用遗传算法、增强方法、修剪方法和进化集成。虽然通过引入这些策略，模型回归精度和数据分类性能或多或少得到了提高，但毫无疑问，计算复杂度也增加了。因此，如何平衡性能和处理时间是一个悬而未决的问题，特别是对于高维数据的应用。</li><li id="5544" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated">对于特定的应用，如何选择最佳的隐神经元数目还没有很好地解决。在现有的大部分工作中，很少讨论隐神经元的选择，几乎所有的隐神经元都是以试探性的方式人工选择的。虽然一些研究人员声称，当使用大量隐藏神经元时，ELM及其变体的性能趋于稳定和可接受，但冗余和高计算负担也会出现。</li><li id="bf11" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated">非常需要为具有ELM的应用设计实时处理系统和设备。尽管在过去的10年左右的时间里，已经有大量的研究成果被报道，但是其中的大部分仍然是在实验室中通过计算机模拟进行的。不同应用的真实设备总是面临着各种挑战，这对于大数据应用来说更为明显。</li></ul><h1 id="63f0" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">参考资料:</h1><ul class=""><li id="07f1" class="ks kt hi jf b jg jh jk jl jo lr js ls jw lt ka kx ky kz la bi translated"><a class="ae lq" href="https://en.wikipedia.org/wiki/Extreme_learning_machine#:~:text=Extreme%20learning%20machines%20are%20feedforward,hidden%20nodes)%20need%20not%20be" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Extreme _ learning _ machine #:~:text = Extreme % 20 learning % 20 machines % 20 are % 20前馈，hidden % 20 nodes)% 20 need % 20 not % 20 be</a></li><li id="f9b8" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated"><a class="ae lq" href="https://www.ntu.edu.sg/home/egbhuang/pdf/ELM-Rosenblatt-Neumann.pdf" rel="noopener ugc nofollow" target="_blank">https://www . NTU . edu . SG/home/egbhuang/pdf/ELM-Rosenblatt-neumann . pdf</a></li><li id="7c8c" class="ks kt hi jf b jg lb jk lc jo ld js le jw lf ka kx ky kz la bi translated">【https://arxiv.org/pdf/2004.08867.pdf T4】</li></ul></div></div>    
</body>
</html>