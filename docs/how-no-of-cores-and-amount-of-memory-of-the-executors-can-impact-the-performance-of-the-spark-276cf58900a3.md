# 执行器的核心数量和内存量如何影响 Spark 作业的性能？

> 原文：<https://medium.com/analytics-vidhya/how-no-of-cores-and-amount-of-memory-of-the-executors-can-impact-the-performance-of-the-spark-276cf58900a3?source=collection_archive---------9----------------------->

![](img/f6eaaf3bd0b2e8c258ac68495af3f43c.png)

利亚姆·布里斯在 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上拍摄的照片

当我们处理大量数据时，我们显然会寻找并行性，以便能够快速处理数据，但有时更多的并行性会导致问题，有时会成为瓶颈并影响作业的性能。

让我们通过一个例子来理解这一点。假设我们有一个 8 节点集群，每个集群有 112 GB RAM 和 16 个内核。首先，当我们将工作提交给集群时，我们脑海中很少出现以下问题。

> -我们将如何选择合适的执行人数量来提交工作？
> -每个执行器应该使用多少个内核？
> -每个遗嘱执行人的记忆应该是怎样的？
> -最多可以并行运行多少个任务？

这些是我们在提交作业时需要考虑的几个问题，因为这会对作业的性能产生很大影响。提交带有不正确的群集中执行器数量和执行器配置的作业有时会占用本应在几分钟或几小时内完成的作业的时间。

现在让我们继续并理解为什么这些问题如此重要。第一个问题是——我们如何选择合适的执行者数量来提交作业？现在，这完全取决于我们希望每个执行器有多少个内核。根据经验，研究人员发现，我们可以为每个执行器获取 2 到 5 个最大内核，如果我们使用这个数量，那么将获得最佳性能。我将在这篇博客的后面解释为什么为一个执行器选择正确的内核数量是如此重要。

基本上，核心数量决定了每个执行器可以并行运行的任务数量。例如，如果我为每个执行器分配了 5 个内核，这意味着我可以为执行器分配 5 个并行任务。现在，如果我想要最大的并行性，我将为每个执行器选择 5 个内核，这就回答了我的第二个问题，但是我们可以选择 2 到 5 个内核，为执行器选择 5 个内核总是正确的吗？我们以后会知道的。暂时让我们考虑一下我们采用 5 个内核。

现在，我们知道每个节点有 16 个内核，我们希望每个执行器有 5 个内核，那么每个节点有 3 个执行器(16/3 ~ 5)，每个执行器有 5 个内核。(在这里，我们还需要考虑执行程序对线程和操作系统的要求，但在我们的情况下，考虑到我们有一些额外的内核和内存，我避免了这一点。)所以现在我们已经找到了每个节点的执行器总数和每个执行器所需的内核数。

现在第三个问题，每个遗嘱执行人的记忆应该是怎样的？我们需要为每个执行程序分配内存，可以在三个执行程序之间平均分配 112/3 ~ 36

现在，我们每个执行器都有 36 GB 内存和 5 个内核。

现在让我们来看下一个问题，可以并行运行的最大任务数是多少？这完全取决于执行器中有多少个内核。在我们当前的配置中，我们有 5 个内核，这意味着我们最多可以并行运行 5 个任务，36 GB ram 将平均分配用于执行这 5 个任务。

因此，每个任务将获得 36/5 ~ 7 GB 的内存。

现在我们每个任务都有 7 GB 内存。因为每个节点总共有 3 个执行器，总共有 8 个节点，这意味着我们最多有 5*3*8 => 120 个任务。

现在考虑这样一个场景:任务正在运行，每个任务的数据超过 7 GB，资源管理器无法提供更多的 RAM，因为 executor 总共有 36 GB 的 RAM。这将导致数据溢出到磁盘上。现在，从 spark 的性能来看，将数据从 RAM 溢出到磁盘是一个非常昂贵的场景。虽然我们有 5 个任务来实现并行性，但这种溢出会降低性能。在这种场景调整中，配置可以显著提高作业的性能。

如果我们看到执行器的旧配置，因为我们希望有更多的并行性，我们为每个执行器配备了 5 个内核，因此每个执行器现在获得了 7 GB RAM，因为我们在我们的场景中观察到 RAM 不足，所以我们将数据溢出到磁盘并进行处理。我们应该减少内核的数量，这样 executor 中的任务就可以有更多的内存。

让我们取 4 个核心。现在，如果这里有 4 个内核，35 GB RAM 将在 4 个内核之间分配，因此每个内核将获得大约 9 GB RAM。这意味着我们现在将有 4 个并行任务，每个任务可以处理大约 8GB 的数据。

因此，选择正确的内核数量有时很棘手。当我们设置执行程序的内存和内核数量时，我们应该对数据量有一个估计。此外，经过一段时间后，当数据量增加时，我们需要对执行器进行必要的修改，以适应这些变化。