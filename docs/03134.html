<html>
<head>
<title>LSTM : What’s the fuss about?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTM:有什么大惊小怪的？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/lstm-whats-the-fuss-about-1ae9d4c3e33e?source=collection_archive---------13-----------------------#2020-01-17">https://medium.com/analytics-vidhya/lstm-whats-the-fuss-about-1ae9d4c3e33e?source=collection_archive---------13-----------------------#2020-01-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="9ad6" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">LSTM学会了。LSTM记得。像LSTM一样。</h2></div><p id="30f3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">递归神经网络<strong class="iz hj"> (RNNs) </strong>擅长学习序列，它们跨越时间步骤将信息从先前的状态带到下一个状态。唉，只有当序列不太长时，它们才能提供好的结果。因此患有短期记忆问题。</p><p id="595e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">此外，在反向传播期间，递归神经网络也可能遭受<strong class="iz hj">消失梯度问题</strong>。在反向传播算法中，基于最终计算的损失来更新权重，在权重的变化是较小值的情况下，这将意味着随着时间步长的更新将变得更小(几乎为零)，并且在这种情况下网络中的层将不会学习！</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es jt"><img src="../Images/32a3bcf6cf0d15376d06b7e82f8d3298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1268/format:webp/1*3wj0TYLu7RkvP_22dGyX5Q.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx translated">RNN的消失梯度条件</figcaption></figure></div><div class="ab cl kf kg gp kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="hb hc hd he hf"><p id="b26e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">RNNs中的这个问题由长短期记忆(<strong class="iz hj"> LSTM </strong>)来解决，这是一种特殊类型的RNN，它使用门和增加的交互次数来防止这个问题。LSTM还增加了网络学习更长序列的能力。如果我必须用更简单的话来陈述，它只记住相关的信息，而忘记每个状态下不必要的信息，从而以最小的内存占用增强其知识库。当我们进入LSTM的细胞结构时，我们会看得更远。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es km"><img src="../Images/d6bdc605b77c3bf3bcc78189412b3000.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*hG4zBCCRq18oi8aarj-owA.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx translated">LSTM细胞结构</figcaption></figure><p id="8d66" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从顶部开始的水平线是<strong class="iz hj">单元状态</strong>，你可以把它想象成网络的<strong class="iz hj">“内存”</strong>。所有相关信息在细胞状态中移动，在每一个时间步，信息通过控制LSTM细胞信息流的门被添加或删除。这些门是<strong class="iz hj">遗忘门、输入门、输出门。</strong></p><p id="744d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">忘记门:</strong>这个门决定什么是相关信息。它从先前状态和当前时间步长获取信息，并通过一个<strong class="iz hj"> Sigmoid函数</strong>传递。输出的范围在0到1之间，如果更接近零，则被忽略，否则保留。</p><p id="2e59" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输入门:</strong>该门计算更新单元状态所需的信息。它分两步工作，第一步与在遗忘门执行的操作相同，其中[来自先前状态和当前输入的信息]从<strong class="iz hj"> Sigmoid函数</strong>传递，第二步涉及通过<strong class="iz hj"> tanh函数</strong>传递相同的信息集(取值范围在-1到1之间)。然后将这两个输出相乘。</p><p id="7131" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上述操作的结果与单元状态和遗忘门输出的逐点相乘的输出进行逐点相加，并且该输出是新的单元状态值。</p><p id="4b9a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出门:</strong>输出门也分两步操作，第一步与遗忘门相同(与输入门的步骤1相同)，而在第二步中，从最新更新的单元状态(在输入门中讨论)中获取信息，并通过双曲正切函数传递。</p><p id="65a9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后将这两个输出相乘以获得新的隐藏状态，并将其结转到下一个单元。因此，这就是LSTM将信息从一个细胞传递到另一个细胞的方式。</p><p id="a0b4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们对LSTM细胞如何工作有了基本的直觉。这将使我们在与他们一起工作时稍微舒服一些。我会试着写一篇带有代码示例的帖子。手指交叉。</p></div><div class="ab cl kf kg gp kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="hb hc hd he hf"><p id="6010" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">如果你大老远来到这里，我想花点时间感谢你真的花了心思去读它。我希望它能帮助人们了解lstm:)</strong></p></div></div>    
</body>
</html>