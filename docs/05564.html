<html>
<head>
<title>Improving Voice Separation By Incorporating End-To-End Speech Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过结合端到端语音识别改进语音分离</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/improving-voice-separation-by-incorporating-end-to-end-speech-recognition-c3dd57d80206?source=collection_archive---------16-----------------------#2020-04-25">https://medium.com/analytics-vidhya/improving-voice-separation-by-incorporating-end-to-end-speech-recognition-c3dd57d80206?source=collection_archive---------16-----------------------#2020-04-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b780bca87b8fdc088097f51f9e397dff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j3CD2gR4uIGVjzX4H6rwgQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">分离2个扬声器外壳的单声道音频</figcaption></figure><p id="23c1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这是一篇解释论文<a class="ae js" href="https://ieeexplore.ieee.org/document/9053845" rel="noopener ugc nofollow" target="_blank">通过整合端到端语音识别</a>改进语音分离的文章。</p><p id="a363" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">作者——Naoya Takahashi，Mayank Kumar Singh，Sakya Basak，Parthasaarathy Sudarsanam，Sriram Ganapathy，Yuki Mitsufuji</p><h1 id="c099" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">了解语音分离</h1><p id="0914" class="pw-post-body-paragraph iu iv hi iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr hb bi translated">从其他源中分离语音的问题，例如语音分离(分离多个重叠的语音信号)和歌唱语音分离(从其他乐器声音中分离人声)已经被积极地研究了几十年。</p><p id="6867" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">让我给你一个简短的背景，我们正在努力实现语音/音乐分离。</p><p id="d8b6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">语音/音乐可以表示为压力值的时间序列。它们具有波形属性，当生成两个重叠声部时，波形值会相加。语音分离的任务是将这个单一的时间序列分离成两个或多个具有单独语音源的时间序列数据。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kw"><img src="../Images/91b6f048108a503e11ca106ecc53cff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pizApzfZqcIbzeQbZjBFnw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">单声道(单通道)音频、压力和时间的表示</figcaption></figure><p id="0e4f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">查看时间序列数据的特定索引，问题非常类似于对于两个扬声器的情况，给定<em class="lb"> a + b </em>，找到<em class="lb"> a </em>和<em class="lb"> b </em>。这个问题有无穷多个解，因此乍一看不可能解决。但是，如果我们考虑整个时间序列，并考虑到人类的声音或音乐被限制在一个低维空间，这个问题就变得可以解决。</p></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><h1 id="a2dd" class="jt ju hi bd jv jw lj jy jz ka lk kc kd ke ll kg kh ki lm kk kl km ln ko kp kq bi translated">介绍</h1><p id="1f2f" class="pw-post-body-paragraph iu iv hi iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr hb bi translated">在文献中，通常的做法是通过采用STFT(短时傅立叶变换)来预处理音频样本，然后应用语音分离技术。最近<a class="ae js" href="https://arxiv.org/abs/1809.07454v3" rel="noopener ugc nofollow" target="_blank">conv-塔斯奈特:超越语音分离的理想时间-频率幅度掩蔽</a>取消了这一预处理步骤，声称学习基础表示法优于STFT，并在WSJ数据集上实现了语音分离的最先进结果。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lo"><img src="../Images/1625395b8875081945b63087726fbd9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dIwV_1vfh4F6fKfsfeFaMQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">ConvTasNet的基本工作。编码器是一维卷积滤波器，分离器是时间卷积的堆栈，解码器是一维转置卷积滤波器。</figcaption></figure><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lp"><img src="../Images/9a3b83e5c1cb6be3790d1f748f05d4e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P82YT9cIkRIf-zVNt8pDWA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">ConvTasNet模型的详细架构</figcaption></figure></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><p id="1f1d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">端到端自动语音识别(E2EASR)系统旨在将ASR系统中的所有子模块(如声学模型、发音模型和语言模型)结合起来。因此，它们被设计成模拟更长的相关性，因此，与通常接受几帧作为输入的传统声学模型相比，E2EASR特征被期望包含更长的语言信息。用于端到端语音识别的<a class="ae js" href="https://ieeexplore.ieee.org/document/8068205" rel="noopener ugc nofollow" target="_blank">混合CTC/注意力架构</a>是一个E2EASR模型，它利用CTC(连接主义者时间分类)以及注意力机制来改进语音识别。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lq"><img src="../Images/21d10f98e1dae066657637f7c85f42cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fx-gTMcUBbT6XMFi2X1jiQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">用于端到端语音识别的混合CTC/Attention体系结构</figcaption></figure><p id="8a84" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">感兴趣的论文的<strong class="iw hj">贡献(通过结合端到端语音识别改进语音分离)总结如下:</strong></p><ul class=""><li id="d3b8" class="lr ls hi iw b ix iy jb jc jf lt jj lu jn lv jr lw lx ly lz bi translated">他们提出了一种基于迁移学习的方法来结合语音的语音和语言特性以进行语音分离。为此，他们提出了E2EASR特性。</li><li id="7da3" class="lr ls hi iw b ix ma jb mb jf mc jj md jn me jr lw lx ly lz bi translated">他们使用AVSpeech和Audio-Set数据集在同步语音分离和增强任务上评估了所提出的方法，这些数据集的音频是在非受控环境中记录的，并且表明<br/>所提出的方法与没有使用E2EASR特征训练的模型和使用视觉特征训练的模型相比，显著提高了分离精度。</li><li id="e945" class="lr ls hi iw b ix ma jb mb jf mc jj md jn me jr lw lx ly lz bi translated">他们进一步表明，尽管E2EASR是在标准语音上训练的，但它对于具有有限数据量的歌唱声音分离任务来说是鲁棒地转移的。</li></ul></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><h1 id="ed5e" class="jt ju hi bd jv jw lj jy jz ka lk kc kd ke ll kg kh ki lm kk kl km ln ko kp kq bi translated">提议的方法</h1><p id="01e0" class="pw-post-body-paragraph iu iv hi iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr hb bi translated">该论文提出将E2EASR特征/视觉特征结合到ConvTasNet网络中，以进一步提高分离Si-SNR(尺度不变-信噪比)。在<a class="ae js" href="https://arxiv.org/abs/1804.03619" rel="noopener ugc nofollow" target="_blank"> AVSpeech </a>数据集上训练和评估语音分离模型。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d15297406be29c63c2d01da37fe241df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KISOBo-P5-N2ee-5NfKU9A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Oracle系统:在ConvTasNet框架中整合E2EASR特性。这些特征是从E2EASR模型的编码器输出中提取的，并在分离器模型之前连接</figcaption></figure><p id="8631" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于这两种方法，E2EASR特征和视觉特征，采用迁移学习方法，并且从预训练的特征提取器模型中提取特征。视觉特征提取器模型基本上是一个去掉了解码器部分的自动编码器。在不同的数据集(LibriSpeech)上训练E2EASR特征提取器模型。为每个说话人提取的特征在分离器中连接，并且在冻结特征提取器模型参数的同时训练ConvTasNet模型。使用E2EASR特性的模型称为oracle模型。</p><p id="a1b5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">视觉特征提取器模型像自动编码器一样在AVSpeech数据集中可用的每帧视频的单个嘴唇裁剪上进行训练。添加视觉特征使SI-SNR比纯音频模型提高了1.2dB。</p><h1 id="a7ac" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">Oracle模型的局限性</h1><p id="c1ad" class="pw-post-body-paragraph iu iv hi iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr hb bi translated">尽管在分离Si-SNR值上给出了很大的改进(参见结果表)，但是结合了E2EASR特征的模型没有实际应用，因为它假设分离的语音是预先可用的。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/b91a33c8d5b51e3ea479fb5d56bc4197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BNUpebHd06yNclD1cIAOGg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">为了计算E2EASR特征，需要事先分离语音。</figcaption></figure><h1 id="6aab" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">克服Oracle模型的局限性</h1><p id="6acd" class="pw-post-body-paragraph iu iv hi iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr hb bi translated">代替使用目标分离语音作为E2EASR模型的输入，本文提出使用两级网络，通过使用基本ConvTasNet模型来获得中间分离语音，并使用这些来提取进一步增强分离Si-SNR的特征。</p><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/f0f30e2c09b09fa3bb80a13ad9b6f3a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ZbMyCbZdB493kc3cw_XxA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">估计的E2EASR特征系统:使用基本的ConvTasNet模型，计算中间分离语音。这些被馈送到E2EASR模型，以提取馈送到oracle模型的深层特征。</figcaption></figure><p id="707c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">需要注意的一些要点是</p><ul class=""><li id="8253" class="lr ls hi iw b ix iy jb jc jf lt jj lu jn lv jr lw lx ly lz bi translated">第一阶段(基本ConvTasNet模型)在AVSpeech数据集上进行预训练，其权重保持固定。</li><li id="16ca" class="lr ls hi iw b ix ma jb mb jf mc jj md jn me jr lw lx ly lz bi translated">E2EASR模型在LibriSpeech数据集上进行预训练，其权重保持固定。</li><li id="0ccf" class="lr ls hi iw b ix ma jb mb jf mc jj md jn me jr lw lx ly lz bi translated">假设oracle中间语音分离，对第二阶段模型进行预训练。其权重在估计的E2EASR特征系统中保持固定。</li></ul><h1 id="2980" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结果</h1><figure class="kx ky kz la fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/303f627970916f726488f025b64337a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wrk5yyXF9Fl6AZUd8WI6jw.png"/></div></div></figure><h1 id="0e01" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结论</h1><p id="e080" class="pw-post-body-paragraph iu iv hi iw b ix kr iz ja jb ks jd je jf kt jh ji jj ku jl jm jn kv jp jq jr hb bi translated">他们提出了一种迁移学习方法来利用E2EASR模型进行语音分离。在同步语音分离和增强任务上的实验结果表明，所提出的E2EASR特征相对于包括使用视觉线索的模型的基线提供了显著的改进。他们进一步表明，E2EASR功能提高了歌唱声音分离的性能，证明了其对有限数据可用性和域不匹配的鲁棒性。</p></div></div>    
</body>
</html>