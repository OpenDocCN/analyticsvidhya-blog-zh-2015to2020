<html>
<head>
<title>A Note on various Object Detection Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于各种目标检测算法的一个注记</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-note-on-various-object-detection-algorithms-66ded1152773?source=collection_archive---------12-----------------------#2020-10-08">https://medium.com/analytics-vidhya/a-note-on-various-object-detection-algorithms-66ded1152773?source=collection_archive---------12-----------------------#2020-10-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/864af8afaa704d519cba2b1eb5b2b3b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y_GUkJbWu0B35DnYqmwagw.jpeg"/></div></div></figure><p id="c2f7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">随着世界科技的进步，工作各个方面的自动化受到了广泛关注。从聊天机器人到自动驾驶汽车，这个研究和应用领域正在发生巨大的飞跃。这篇文章是关于深度神经网络架构在物体检测领域的应用。我首先想到的是深入研究各种可用算法的细节，但这将是一个巨大的资源，听起来更像一个无聊的讲座。这一个将包含一些最应用的算法的一般概述，即。<em class="jo">滑动窗口，R-CNN和YOLO </em>，带你入门。这仍然会是一个很长的帖子，我很抱歉。</p><p id="b3eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">给定一幅图像，卷积神经网络可以将其分类为不同的类别。但是真正的问题在于在单个图像中检测多个对象。当图像中有多个对象时，挑战在于识别每个对象并在图像中定位它们。早先，CNN被应用于对象的紧密裁剪的图像或仅包含一个对象的图像。但是当然，为了训练用于物体检测和定位的网络，我们需要在具有一个或多个物体以及标记其在图像中的位置的清晰边界的图像上训练它。让我们来定义网络的输出，即我们希望网络预测什么。我们定义我们的输出向量<strong class="is hj">y</strong>ₑₓₚ<strong class="is hj">T3】，来自被训练来检测n类物体的网络，为:<br/>t5】yₑₓₚ:</strong><br/><strong class="is hj">【pᵢ，</strong> <br/> <strong class="is hj"> L </strong> <strong class="is hj"> ₓ，/ B </strong> ₓ <br/> <strong class="is hj"> L </strong> <strong class="is hj"> ᵧ，/ B </strong> <strong class="is hj"> ᵧ </strong> <br/> <strong class="is hj"> L ₕ，/ B </strong><br/><strong class="is hj">【c₃】，</strong> <br/> <strong class="is hj"> …，</strong> <br/> <strong class="is hj"> …，</strong><br/><strong class="is hj">【cₙ】</strong><br/><strong class="is hj">【pᵢ】</strong>→定义给定图像是否具有属于n类图像中至少一类的任何对象的概率值。 因此，如果图像中有物体，则为1；如果图像只是风景，或者没有感兴趣的特定物体，则为0。<br/> <strong class="is hj"> Lₓ / B </strong> ₓ →表示对象位置周围边界框中心的x坐标。<br/> <strong class="is hj"> Lᵧ </strong> <strong class="is hj"> / Bᵧ </strong> →表示对象位置周围包围盒中心的y坐标。<br/> <strong class="is hj"> Lₕ </strong> <strong class="is hj"> / Bₕ </strong> →表示包围盒的高度。<br/> <strong class="is hj"> Lₗ </strong> <strong class="is hj"> / Bₗ </strong> →表示包围盒的宽度。<br/> <strong class="is hj"> C₁，C₂，…，Cₙ </strong> →它们代表不同类别的物体，网络正在被训练识别。因此，如果图像中有一辆汽车，并且C₂用于表示汽车，那么C₂的值应该是1，而其他参数假定值为0。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es jp"><img src="../Images/8d839a8bfd4ca0bb1fd5e9a16e707874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*KT4aDiLZ1yFWSoB3hveOXA.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">在给定的图像中，输出的值表示什么的例子。注:bₗ在这里用b𝓌表示。</figcaption></figure><p id="3c3d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，在一组标记良好的图像上盲目训练神经网络不会产生令人满意的结果。我们需要更智能的算法实现，对图像进行一些有效的操作，并返回更好的结果。一、<strong class="is hj">滑动窗口检测算法</strong>。</p><h1 id="9f89" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">滑动窗口检测算法:</h1><p id="d93e" class="pw-post-body-paragraph iq ir hi is b it kw iv iw ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn hb bi translated"><strong class="is hj">滑动窗口检测算法</strong>执行如下:<br/> <em class="jo">首先</em>，我们训练一个ConvNet从那些对象的紧密裁剪图像中对对象的类别进行分类。因此，如果网络试图识别人类的图片，我们将在只包含一个人的图像上训练它。每个图像的额外背景被移除。因此，对于给定的数据集，如果是一个人的图像，输出将是<strong class="is hj"> 1 </strong>，否则输出将是<strong class="is hj"> 0 </strong>。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/28c46170ee7df3563b961e6d40b7aef2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*RBCPF29sBPNqXnQcfUD2jQ.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">检测窗口如何滑过输入矩阵。</figcaption></figure><p id="9824" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">接下来是探测物体的工作。为此，我们选择一个特定大小的正方形窗口(与图像的大小相比，尺寸较小)。我们现在将这个窗口放在输入图像上，并通过窗口裁剪出图像中聚焦的部分。我们将这个裁剪后的图像输入到ConvNet中进行分类。现在，这个窗口首先被放置在图像的左上角，然后以某个选定的步幅(或跳跃)滑动到另一边，然后再以相同的步幅向下滑动。因此得名<em class="jo">滑动窗口检测</em>。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/a96bfd5a25128b6f6b061c7dd7058cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/1*iMkjFpC2NKAifrFw-TLazA.gif"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">一个在完整图像上滑动窗口的例子。</figcaption></figure><p id="4b1d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每次，它都将图像的特定部分返回给ConvNet。ConvNet将图像的这一部分分为不同的类别(如果存在对象)。<br/>算法的一个<strong class="is hj">缺点是图像中物体的尺寸经常会大于所选窗口的尺寸。这将返回一个否定的结果，即使对象可能存在。一种补救方法是使用几个不同大小窗口。我们还可以每次按一定比例调整图像大小，以确保滑动窗口至少检测到一次对象。虽然这可能会返回正确的结果，但是它们在计算上是不利的。手头的另一个选择是卷积地实现滑动窗口检测算法<em class="jo"/>。下面是视频的链接，它对这种实现进行了很好的解释。<br/> <a class="ae ld" href="https://www.youtube.com/watch?v=mFunGvD5sVc" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj">利用卷积实现滑动窗口检测</strong> </a></strong></p><h1 id="05e3" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">R-CNN或基于地区的CNN:</h1><p id="55db" class="pw-post-body-paragraph iq ir hi is b it kw iv iw ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn hb bi translated">接下来是R-CNN或基于区域的卷积神经网络。它基于将图像划分成区域的概念。我们已经有了一个训练有素的网络来识别各种类型的物体。为了检测图像中的对象，该算法执行以下步骤。基于选择性搜索，它基本上必须提出近2k个与类别无关的感兴趣区域。这个想法是，这些区域可能包含不同大小的各种对象。这个<em class="jo">区域提议</em>的任务是通过<strong class="is hj">分段</strong>来实现的。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/e547c13174a15aa9de6780c9ae76c7d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UZpreDlUYs0BmaSulHAcXQ.jpeg"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">图像分割的一个例子</figcaption></figure><p id="4296" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，图像的建议区域被转换成先前训练过的ConvNet可接受的格式，并被输入其中。然后，ConvNet对图像的这些聚焦部分执行分类任务。<br/>以下是R-CNN应用的步骤:</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/4e2b81897e5b5690019b134cc0949f2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CP3X0CXeCIUki5NB4cjsTA.png"/></div></div></figure><p id="f34f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是基本概念。除此之外，这个网络还具有一个<em class="jo"> SVM(支持向量机)</em>，它可以根据ConvNet提取的特征执行图像分类。它还应用<em class="jo">线性回归模型</em>来微调区域边界框。很多东西，对吧？如果不需要深入研究它们的实现，就不需要太关注它们。<em class="jo">正如吴恩达所说，“不懂就别担心。”</em></p><p id="e759" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">尽管采取了所有这些步骤，但从视频片段中进行实时对象检测仍然很慢。为了增强它的性能，对这个算法的版本进行了各种更新。从<strong class="is hj">快R-CNN开始到更快R-CNN，再到现在屏蔽R-CNN </strong>。<br/><strong class="is hj">快速R-CNN </strong>基于计算的共享。换句话说，就像我们在滑动窗口的卷积实现中所做的那样，类似地，这里我们将所有信息传递给CNN进行分类和边界框回归。这减少了先前在每个区域上单独训练时产生的总计算成本。一旦理解了滑动窗口检测算法的卷积实现，就可以很容易地确定这里采用的方法。<br/><strong class="is hj">更快的R-CNN </strong>是将区域提议流程集成到ConvNet中的直观改进。而<strong class="is hj">掩模R-CNN </strong>是更快的R-CNN到像素级图像分割的扩展实现。如果你需要了解更多，这里有这些论文的链接。<br/> <a class="ae ld" href="https://arxiv.org/abs/1311.2524" rel="noopener ugc nofollow" target="_blank"> R-CNN </a> <br/> <a class="ae ld" href="https://arxiv.org/abs/1504.08083" rel="noopener ugc nofollow" target="_blank">快R-CNN </a> <br/> <a class="ae ld" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank">快R-CNN </a> <br/> <a class="ae ld" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank">屏蔽R-CNN </a></p><h1 id="9c04" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">YOLO物体检测算法(最后):</h1><p id="17a1" class="pw-post-body-paragraph iq ir hi is b it kw iv iw ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn hb bi translated"><strong class="is hj"> YOLO </strong>或<strong class="is hj">你只看一次</strong>算法采用了一种与其他算法完全不同的方法。而其他算法对图像中的每个建议区域执行分类和定位。YOLO把这两项任务都在一张图像上通过了网络。它所做的是，将图像分成一定数量的区域，并预测每个区域的结果，如果某个对象的中心位于该区域。首先让我们看看它应该输出什么。<br/> <strong class="is hj">包围盒:</strong>我们期望网络预测出图像中物体周围的一个包围盒。我们像以前一样定义输出:</p><p id="050e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">yₑₓₚ:</strong><br/><strong class="is hj">【cₛ，</strong> <br/> <strong class="is hj"> L </strong> <strong class="is hj"> ₓ，/ B </strong> ₓ <br/> <strong class="is hj"> L </strong> <strong class="is hj"> ᵧ，/ B </strong> <strong class="is hj"> ᵧ </strong> <br/> <strong class="is hj"> L ₕ，/b̿</strong><br/><strong class="is hj">l↑，/b↑</strong><br/><strong class="is hj">c 它由<em class="jo">概率</em>(物体存在的Pᵢ)与IoU(结果，预期)的乘积给出。 我将在后面定义借据。<br/> <strong class="is hj"> Pᵢ </strong> →定义给定图像中是否有属于n类图像中至少一类的物体。<br/> <strong class="is hj"> Lₓ / B </strong> ₓ →表示对象位置周围边界框中心的x坐标。<br/> <strong class="is hj"> Lᵧ </strong> <strong class="is hj"> / Bᵧ </strong> →表示对象位置周围包围盒中心的y坐标。<br/> <strong class="is hj"> Lₕ </strong> <strong class="is hj"> / Bₕ </strong> →表示包围盒的高度。<br/> <strong class="is hj"> Lₗ </strong> <strong class="is hj"> / Bₗ </strong> →表示包围盒的宽度。<br/> <strong class="is hj"> C₁，C₂，…，Cₙ </strong> →它们代表不同类别的物体，网络正在被训练识别。这里只是一个例子，说明在给定的图像划分成3×3个网格后，一个经过训练的网络能够返回什么。<em class="jo">这还没有显示置信度得分。</em></strong></p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/af1fdfdc04b591fe689cb66a353edf6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bH3FE3NFLE0-0MhGuxd9nQ.jpeg"/></div></div></figure><p id="c3d9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从图像中可以很容易地理解每个网格单元的输出是如何变化的。边界框的输出值是近似值。这里是另一个例子。假设网络被训练来检测各种街道标志，适用于自动驾驶应用。红色框表示由网络预测的边界框(<strong class="is hj"> Bₒ </strong>)，绿色框表示预期的边界框(<strong class="is hj"> Bₑ </strong>)。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/0bc1ff42d2ccdbb72bcfd45b1aa023ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*e7XbLr7A6HF18dcv9EDqpQ.jpeg"/></div></figure><p id="cfe7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">那么，算法是做什么的呢？<br/> <em class="jo">首先是</em>，像往常一样，我们有一个CNN(卷积神经网络)被训练成将物体分类成不同的类别。<br/>接下来，它将图像分成<strong class="is hj"> S x S网格单元或区域</strong>。如果发现某个对象的中心位于每个网格单元中，它会训练网络返回该网格单元的结果。可以用下面的图像来解释:</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es li"><img src="../Images/c46e709801e3efeef4e9f49f9557262a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e0VY6U1_WMF2KBoKQNZvkQ.png"/></div></div></figure><p id="afb6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里我们有给定的三个物体的图像。图像被分成一定数量的单元，并且网络预测可能包含物体中心的单元的边界框。注意，预测边界框的数量非常大。我们如何选择正确的呢？让我们在接下来的几个话题中理清思路:</p><h1 id="f219" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">IoU(并集上的交集):</h1><p id="5eb8" class="pw-post-body-paragraph iq ir hi is b it kw iv iw ix kx iz ja jb ky jd je jf kz jh ji jj la jl jm jn hb bi translated">因此，在训练期间，我们有两个输出值:第一，真实值或预期输出。第二，我们通过网络传递图像后得到的结果<br/>因此，期望的输出为一个对象定义了一些边界框(<strong class="is hj"> Bₑ </strong>)，而网络为同一对象预测了一些其他的边界框(<strong class="is hj"> Bₒ </strong>)。<br/> <strong class="is hj"> IoU </strong>定义为<strong class="is hj"> Bₑ </strong>和<strong class="is hj"> Bₒ </strong>的交集所覆盖的面积与<strong class="is hj"> Bₑ </strong>和<strong class="is hj"> Bₒ </strong>的并集所覆盖的面积之比。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lj"><img src="../Images/0416dc1c7585151a66a66f39a26da068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mEIEF1xvFAWHJeJWxnGLaw.png"/></div></div></figure><p id="9ae1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，对于一幅图像，我们可能会得到许多由网络预测的边界框。我们只选择<em class="jo">置信度得分</em>超过某个阈值的结果。通常，该阈值取为0.6及以上，但这取决于开发者的判断。<br/>下面是它在汽车检测中的应用实例。请注意预测的和预期的边界框几乎重叠。因此，它有一个高电平的IoU。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/de87850f6edc3f5a825b0cf49041dadd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*pCkoa6OX31sXulVSuAtERQ.jpeg"/></div></figure><p id="0b99" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">非最大值抑制:</strong> <br/>在网络的应用过程中，当对图像中的物体进行检测时，网络可能会对一个置信度得分大于阈值的物体返回多个包围盒。我们应该选择哪一个。<br/>我们考虑具有最大值<strong class="is hj"> IoU </strong>的包围盒。在所有结果中，我们抑制IoU 较少的方框，并考虑IoU<strong class="is hj">最高的方框。因此，它被称为非最大抑制。</strong></p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/526960d5451dd8858412fb0899cdfce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*EJjuzf5go44h2MiQk63ebg.jpeg"/></div></figure><p id="97ff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上面的图像中，假设网络预测了同一图像的三个边界框，其中IoU超过了阈值(例如0.75)。非最大抑制确保选择具有最高IoU(此处为0.9)的边界框。</p><p id="ae8c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">锚盒:</strong> <br/>另一个重要的概念是锚盒。假设我们有两个不同形状的物体，它们的中心位于同一个网格单元中。在这种情况下，网络不会正确预测其中一个对象。这里我们使用锚盒。这只是一个概念，代表各种物体可能的形状或配置。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/b758ea13d9dc333dab221653f79eae84.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*qXQgfI9adv5sFBe1_jVy6w.jpeg"/></div></figure><p id="017c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上图恰当地展示了当两个对象共享同一个单元格作为它们的中心时，定位框是如何有用的。包含两个对象中心的单元格的输出将是:<br/> <strong class="is hj"> Yₑₓₚ: </strong></p><p id="3e65" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">、<br/> 1、</strong> <br/> <strong class="is hj"> Lₓ、/ Bₓ <br/> Lᵧ、/ Bᵧ <br/> Lₕ、/ Bₕ <br/> Lₗ、/ Bₗ </strong> <br/> <strong class="is hj"> 0、</strong> <br/> <strong class="is hj"> 1、</strong><br/><strong class="is hj"/><br/><strong class="is hj">0.75、</strong> <br/> <strong class="is hj"> 1、</strong> <br/> <strong class="is hj"> L行人、汽车和街道标志。让C₁表示行人，C₂表示汽车，C₃表示街道标志。边界框B1(这里是0.82)和B2(这里是0.75)都有一些<strong class="is hj">置信度得分</strong>。汽车和行人同时存在的<strong class="is hj">概率</strong>为<strong class="is hj"> 1 </strong>。它们的边界框的边界由参数<strong class="is hj"> Lₓ </strong> / <strong class="is hj"> Lₓ </strong> <strong class="is hj">，Lᵧ </strong> / <strong class="is hj"> Lᵧ </strong> <strong class="is hj">，Lₕ </strong> / <strong class="is hj"> Lₕ </strong> <strong class="is hj">，Lₗ </strong> / <strong class="is hj"> Lₗ </strong>定义。类似地，对于边界框B1，我们期望它表示一辆汽车，所以<strong class="is hj"> C₂是1 </strong>其他的是0。而对于另一个边界框B2，我们有<strong class="is hj"> C₁是1 </strong>而其他的是0。<br/>接下来，我们基于上述算法训练我们的网络，并获得结果。以下是YOLO的标准网络实现。请注意，输出是一个7 x 7 x 30的向量。因此，图像被分成7×7 = 49个区域，每个网格单元的输出向量包含30个用于预测各种物体的参数。</strong></p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/bfe638aa8c1d4cbb7b6f64f166959947.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ab_OTjlniZG77E9Tnv0xxg.png"/></div></div></figure><p id="edf4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们如何在训练中定义损失函数？这非常困难，并且随着实现的不同而不同。然而，该图像提供了关于该功能的一般概念。(阅读风险自担)</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/fd3645b7cccee0a2c3b7b876f5ef16d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*u4ok_f5bl1zlrkbj9dgwHA.jpeg"/></div></figure><p id="485d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最初，YOLO接受了PASCAL VOC数据集的训练，该数据集可以对20类物体进行分类。已经证明这对于更快的物体检测是有用的。该算法已成功应用于视频中的实时目标检测。随着时间的推移，算法有了很多升级，导致了YOLOv2或YOLO9000以及类似的YOLO v3。如果你感兴趣，这里有相应论文的链接。<br/><a class="ae ld" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank">YOLO</a><br/><a class="ae ld" href="https://arxiv.org/abs/1612.08242" rel="noopener ugc nofollow" target="_blank">YOLO v2</a><br/><a class="ae ld" href="https://arxiv.org/abs/1804.02767" rel="noopener ugc nofollow" target="_blank">YOLO v3</a><br/>下面是一个表格，比较了各种算法在给定数据集上的性能。我决定在另一篇博客中广泛报道YOLO。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lo"><img src="../Images/ac625c73900bd3326439c07238660fc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q6GXy52crw3E4RHxgn1oWg.png"/></div></div></figure><p id="c589" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">希望这些信息对你有用。如有任何建议，欢迎在评论中表达。我仍处于学习这些主题的阶段，欢迎对文章的改进提出任何建议。</p><p id="847c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">长命百岁，编码。</p><figure class="jq jr js jt fd ij er es paragraph-image"><a href="https://www.buymeacoffee.com/crazylazylife"><div class="er es lp"><img src="../Images/1fa2ccb7a9017f3a8d1d9b0d82ccdff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*PNJFpOBwyQ5pS4N0_yvlVA.jpeg"/></div></a><figcaption class="ju jv et er es jw jx bd b be z dx translated">因为在我的国家Medium不付我薪水:'(</figcaption></figure></div></div>    
</body>
</html>