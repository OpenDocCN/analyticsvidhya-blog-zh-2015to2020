<html>
<head>
<title>How to Determine the Optimal K for K-Means?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何确定K-Means的最优K？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb?source=collection_archive---------0-----------------------#2019-06-17">https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb?source=collection_archive---------0-----------------------#2019-06-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/8f0a8c01a34bd8b8d0bac879c885612b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OJ-dzKJdbwVJu_P7INWuyg.png"/></div></div></figure><h1 id="9cd2" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">介绍</h1><p id="d964" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">K-Means算法无需介绍。它很简单，也许是最常用的聚类算法。</p><p id="1b72" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">k-means背后的基本思想包括定义k个聚类，使得总的<strong class="jq hj">类内变化(或误差)最小</strong>。</p><p id="bdc7" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><em class="kr">我鼓励您在继续学习之前查看以下文章，以深入了解不同的聚类方法:</em></p><ul class=""><li id="0077" class="ks kt hi jq b jr km jv kn jz ku kd kv kh kw kl kx ky kz la bi translated"><a class="ae lb" href="https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/" rel="noopener ugc nofollow" target="_blank"> <em class="kr">聚类介绍和不同的聚类方法</em> </a></li><li id="df00" class="ks kt hi jq b jr lc jv ld jz le kd lf kh lg kl kx ky kz la bi translated"><a class="ae lb" href="https://www.analyticsvidhya.com/blog/2019/05/beginners-guide-hierarchical-clustering/" rel="noopener ugc nofollow" target="_blank"> <em class="kr">层次聚类初学者指南以及如何在Python中执行层次聚类</em> </a></li></ul><p id="92ad" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">聚类中心是其所在聚类的代表。每个点与其聚类中心之间的平方距离就是所需的变化量。k-means聚类的目的是找到这k个聚类及其中心，同时减少总误差。</p><p id="c8a4" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">相当优雅的算法。但是有一个问题。你如何决定集群的数量？</p><p id="dead" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在本文中，我将详细解释在k-Means中找到这个神秘的k的两种有用的方法。</p><p id="12f6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这些方法是:</p><ol class=""><li id="6aff" class="ks kt hi jq b jr km jv kn jz ku kd kv kh kw kl lh ky kz la bi translated"><strong class="jq hj">手肘法</strong></li><li id="5889" class="ks kt hi jq b jr lc jv ld jz le kd lf kh lg kl lh ky kz la bi translated"><strong class="jq hj">剪影法</strong></li></ol><p id="0e8b" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我们将使用下面代码生成的数据集来说明这两种方法:</p><figure class="li lj lk ll fd ij"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="4df4" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这是数据的图形外观:</p><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/f76e05f10c6962cd74cf593bf13f9294.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*B1zpDSV9aG-c8jVoJ8ttvg.png"/></div></figure><p id="ee52" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">显然，数据集有3个聚类。我们将在这个数据集上验证我们的两种方法。</p><h1 id="b95c" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">肘法</h1><p id="915e" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">这可能是最广为人知的确定最佳聚类数的方法。<em class="kr">在做法上也有点幼稚。</em></p><blockquote class="lp lq lr"><p id="c234" class="jo jp kr jq b jr km jt ju jv kn jx jy ls ko kb kc lt kp kf kg lu kq kj kk kl hb bi translated">对于<strong class="jq hj">不同的k </strong>值，计算<strong class="jq hj">误差平方和</strong>的组内误差(WSS ),选择WSS最先开始减小的k。在WSS对k的曲线图中，这可以被视为一个<strong class="jq hj">弯头。</strong></p></blockquote><p id="7120" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">类内误差平方和听起来有点复杂。让我们来分解一下:</p><ul class=""><li id="6c88" class="ks kt hi jq b jr km jv kn jz ku kd kv kh kw kl kx ky kz la bi translated">每个点的平方误差是该点与其表示(即其预测聚类中心)的距离的平方。</li><li id="43b7" class="ks kt hi jq b jr lc jv ld jz le kd lf kh lg kl kx ky kz la bi translated">WSS分数是所有点的这些平方误差的总和。</li><li id="0a22" class="ks kt hi jq b jr lc jv ld jz le kd lf kh lg kl kx ky kz la bi translated">可以使用任何距离度量，如欧几里德距离或曼哈顿距离。</li></ul><p id="d821" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">让我们使用Python中的<strong class="jq hj"> <em class="kr"> sklearn </em> </strong>库和我们自己的函数来实现这一点，以计算k的一系列值的WSS</p><figure class="li lj lk ll fd ij"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="b51f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">对于我们的数据集，我们获得了以下WSS对钾的曲线图。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/d03fa4ee07f13802a406b0c9c59a3587.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*O_JmBi6rM6PlrPFx_uNrVQ.png"/></div></figure><p id="7249" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">不出所料，<strong class="jq hj">剧情在k = 3时看起来像一只胳膊有一个清晰的肘关节</strong> <strong class="jq hj">。</strong></p><p id="dec5" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">不幸的是，我们并不总是有如此清晰的聚类数据。这意味着肘部可能不清晰和尖锐。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/b40a4ac03c20f22b7d7a91dfe4ade66e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t7VgOuC_Yp-Y1ND8z-eFtQ.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">资料来源:bl.ocks.org/</figcaption></figure><p id="daf6" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">对于数据集A，肘部在k = 3处是清晰的。然而，对于数据集b，这种选择是不明确的。我们可以选择k为3或4。</p><p id="1208" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在这种模棱两可的情况下，我们可以使用剪影法。</p><h1 id="6b76" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">剪影法</h1><blockquote class="lp lq lr"><p id="e9dc" class="jo jp kr jq b jr km jt ju jv kn jx jy ls ko kb kc lt kp kf kg lu kq kj kk kl hb bi translated">轮廓值衡量一个点与其自己的聚类(内聚)相比与其他聚类(分离)的相似程度。</p><p id="85b2" class="jo jp kr jq b jr km jt ju jv kn jx jy ls ko kb kc lt kp kf kg lu kq kj kk kl hb bi translated">来源:维基百科</p></blockquote><p id="7378" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">轮廓值的范围在+1和-1之间。<strong class="jq hj">高值是期望的</strong>,并且指示该点被放置在正确的聚类中。如果许多点具有负的轮廓值，这可能表明我们创建了太多或太少的簇。</p><p id="4ef4" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">每个数据点<strong class="jq hj"> <em class="kr"> i </em> </strong>的轮廓值<strong class="jq hj"> <em class="kr"> s(i) </em> </strong>定义如下:</p><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/9e65687c08d1012989803f8786860db2.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*vFXfyIjIjI_kbZNWODiuxQ.jpeg"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">来源:维基百科</figcaption></figure><p id="e7d5" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> <em class="kr">注</em> </strong>:如果<strong class="jq hj"> <em class="kr"> i </em> </strong>是簇中唯一的点，则s(i)被定义为等于零。这是为了防止集群的数量随着许多单点集群而显著增加。</p><p id="0076" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这里，<strong class="jq hj"> <em class="kr"> a(i) </em> </strong>是点<strong class="jq hj"> <em class="kr"> i </em> </strong>与其自身聚类的相似性的度量。它被测量为<strong class="jq hj"> <em class="kr"> i </em> </strong>到集群中其他点的平均距离。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/a5e7748bed6c53634a7d2d48fa46c1ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*OFOXVdMIwldl8diJvscw_A.jpeg"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">来源:维基百科</figcaption></figure><p id="590f" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">同样，<strong class="jq hj"> <em class="kr"> b(i) </em> </strong>是<strong class="jq hj"> <em class="kr"> i </em> </strong>与其他聚类中的点的相异度的度量。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es md"><img src="../Images/e91b4f4255082ef42c0323486ad346d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*fGgqT3IHwpEOiDbCtN5dAQ.jpeg"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">来源:维基百科</figcaption></figure><p id="05d8" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated"><strong class="jq hj"> <em class="kr"> d(i，j) </em> </strong>是点<strong class="jq hj"> <em class="kr"> i </em> </strong>和<strong class="jq hj"> <em class="kr"> j </em> </strong>之间的距离。通常，<strong class="jq hj">欧几里德距离</strong>被用作距离度量。</p><p id="5812" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">使用<strong class="jq hj"> <em class="kr"> sklearn </em> </strong>库的度量模块，可以在Python中轻松计算出剪影分数。</p><figure class="li lj lk ll fd ij"><div class="bz dy l di"><div class="lm ln l"/></div></figure><p id="2b6d" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">我之前提到过，高轮廓分数是可取的。轮廓分数在最优k  处达到其<strong class="jq hj"> <em class="kr">全局最大值。理想情况下，这应该在轮廓值对k图中显示为峰值。</em></strong></p><p id="1030" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">这是我们自己的数据集的曲线图:</p><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es me"><img src="../Images/ae76e08bfe8aa2a58635690dd2936f1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*Otg9XRLPqOjb80Png6B9Og.png"/></div></figure><p id="2b60" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">在k = 3处有一个清晰的峰值。因此，它是最佳的。</p><p id="deec" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">最后，数据可以最优地<strong class="jq hj"><em class="kr"/></strong>聚类成3个簇，如下所示。</p><figure class="li lj lk ll fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/1f2743323ff95e65101bb48cad6a5f95.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*EN3KSIqQGQNhqmEcbA1pbw.png"/></div></figure><h1 id="9c8d" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">结束注释</h1><p id="4e61" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">肘方法更多的是一个决策规则，而侧影是一个用于聚类时验证的度量。因此，它可以与肘法结合使用。</p><p id="a1f1" class="pw-post-body-paragraph jo jp hi jq b jr km jt ju jv kn jx jy jz ko kb kc kd kp kf kg kh kq kj kk kl hb bi translated">因此，在寻找最佳k值时，肘法和剪影法并不是相互替代的方法。相反，它们是为了做出更有信心的决策而一起使用的工具。</p></div></div>    
</body>
</html>