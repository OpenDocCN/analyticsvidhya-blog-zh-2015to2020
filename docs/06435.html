<html>
<head>
<title>Reinforcement Learning 101</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习101</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-101-bf42523fd6ad?source=collection_archive---------12-----------------------#2020-05-22">https://medium.com/analytics-vidhya/reinforcement-learning-101-bf42523fd6ad?source=collection_archive---------12-----------------------#2020-05-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="45b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了学习强化学习，我必须努力。主要的问题是我不知道从哪里开始，做什么。所以，这篇文章是在开放人工智能健身房的Taxi-V3环境中实现的Sarsa-Max或Q-learning算法。</p><p id="16bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">理解这个算法主要需要的先决条件只是python和numpy基础知识。所以，加入进来，准备好你的第一个强化学习项目。</p></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="ff60" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">什么是强化学习</h1><p id="d4e1" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">强化学习是一种无监督学习的形式，其中代理通过随机试验学习探索环境，并学习执行一些任务。</p><p id="c9bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这意味着代理探索所有可能的结果，并试图选择最好的结果。定义最佳可能结果的实体称为<strong class="ih hj">策略</strong>。</p><p id="ff26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，为了理解环境，我们将它分为<strong class="ih hj">状态</strong>，并且环境有一组<strong class="ih hj">可能的动作</strong>。</p><p id="2371" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，每个状态都与一个<strong class="ih hj">奖励</strong>相关联，这是代理的主要动机。</p><p id="c16e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">* RL代理总是试图最大化累积回报，这是任何RL算法背后的核心原则*。</strong></p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kn"><img src="../Images/38650561c711907b37002b426f1ae3fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q6tE9BaPeIm5zBeYkQOjBg.png"/></div></div></figure><p id="8f36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图描述了RL的代理环境交互的高级视图。</p><p id="5757" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑一个简单的例子。下图是一个状态图，对于理解环境要素之间的关系非常有用。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es kz"><img src="../Images/038a910b49072a411b5043d3cdbfeff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O2x05I_gKbgJHMUqkWB7fQ.png"/></div></div></figure><p id="c133" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的例子是一个清洁机器人的状态图。这个机器人有三种可能的行动，搜索，等待，充电。每个状态的可能操作通过箭头标记表示。在这些线的顶端是奖励和代理人从当前状态选择一个特定的可能行动的概率。</p><h1 id="8f2c" class="jk jl hi bd jm jn la jp jq jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh bi translated">工作</h1><p id="1c72" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">RL代理随机穿越环境收集奖励，并试图最大化<strong class="ih hj">累积奖励</strong>。环境中的每个循环被称为<strong class="ih hj">事件</strong>。但是，如果我们希望我们的代理人最大限度地达到目的，这将是一个非常缓慢的过程。我们等到一集结束，然后最大化所有集中每一集的回报的RL方法被称为蒙特卡罗方法。</p><p id="6154" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是在我们的Taxi -v3案例中，我们将使用一种更高级的算法，可以在每个时间步长更新参数。</p><p id="169b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是我们要更新的参数到底是什么呢？</p><p id="469f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个参数叫做Q值。当使用有限的MDP时，我们可以在一个称为<strong class="ih hj"> Q表的表中估计对应于一个策略的动作值函数。</strong></p><p id="7ebe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Q表中的值称为Q值。</p><h1 id="7c7e" class="jk jl hi bd jm jn la jp jq jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh bi translated"><strong class="ak"> Q-Learning / Sarsa Max </strong></h1><p id="f279" class="pw-post-body-paragraph if ig hi ih b ii ki ik il im kj io ip iq kk is it iu kl iw ix iy km ja jb jc hb bi translated">蒙特卡罗(MC)预测方法必须等到一集结束时才更新价值函数估计，时间差分(TD)方法在每个时间步长后更新价值函数。</p><p id="f20b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Sarsa Max / Q-Learning是一种TD方法。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="er es lf"><img src="../Images/bedf5ae2943b3fd55bcbe2520346b2dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7KHOMQpWivICrVEopy2CEA.png"/></div></div></figure><p id="9806" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上是Q学习的算法。如果你想尝试自己实现这个算法，你可以或者我在下面附了一个git repo，里面有这个算法的python实现。</p><p id="d9ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我已经在报告中描述了环境、安装程序和运行程序。</p><p id="42ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">自己尝试一下，试着改变alpha和gamma参数，看看奖励会有什么变化。</p><figure class="ko kp kq kr fd ks er es paragraph-image"><div class="er es lg"><img src="../Images/d3e9a44f2e7132ce08114e04239fd410.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*5RnIHumiOL4J4usW4HNmKg.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">出租车v3</figcaption></figure></div><div class="ab cl jd je gp jf" role="separator"><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji jj"/><span class="jg bw bk jh ji"/></div><div class="hb hc hd he hf"><h1 id="1bc6" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">密码</h1><div class="ll lm ez fb ln lo"><a href="https://github.com/srimanthtenneti/Taxi-v3-Solution" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hj fi z dy lt ea eb lu ed ef hh bi translated">srimanthtenneti/Taxi-v3-解决方案</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">这项任务是在[Dietterich2000]中介绍的，以说明分层强化学习中的一些问题。那里…</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">github.com</p></div></div><div class="lx l"><div class="ly l lz ma mb lx mc kx lo"/></div></div></a></div><p id="68a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你需要任何帮助，请随时通过LinkedIn联系我。</p><h1 id="2c0d" class="jk jl hi bd jm jn la jp jq jr lb jt ju jv lc jx jy jz ld kb kc kd le kf kg kh bi translated">接触</h1><div class="ll lm ez fb ln lo"><a href="https://www.linkedin.com/in/srimanth-tenneti-662b7117b/?originalSubdomain=in" rel="noopener  ugc nofollow" target="_blank"><div class="lp ab dw"><div class="lq ab lr cl cj ls"><h2 class="bd hj fi z dy lt ea eb lu ed ef hh bi translated">Srimanth Tenneti -自由职业者-自由职业者| LinkedIn</h2><div class="lv l"><h3 class="bd b fi z dy lt ea eb lu ed ef dx translated">我是Srimanth Tenneti，一名20岁的硬件设计师和软件开发人员。目前就读于北京理工大学二年级…</h3></div><div class="lw l"><p class="bd b fp z dy lt ea eb lu ed ef dx translated">www.linkedin.com</p></div></div><div class="lx l"><div class="md l lz ma mb lx mc kx lo"/></div></div></a></div></div></div>    
</body>
</html>