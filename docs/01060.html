<html>
<head>
<title>[ML from scratch] Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ml-from-scrach-linear-regression-normal-equation-gradient-descent-1af26b542c28?source=collection_archive---------10-----------------------#2019-09-28">https://medium.com/analytics-vidhya/ml-from-scrach-linear-regression-normal-equation-gradient-descent-1af26b542c28?source=collection_archive---------10-----------------------#2019-09-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="e9b9" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">正规方程和梯度下降</h2></div><p id="20fe" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我们开始学习时，第一个机器学习模型是线性回归。</p><p id="8e96" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我将深入解释解决线性回归问题的两种流行方法:<strong class="iz hj">正规方程</strong>和<strong class="iz hj">梯度下降。</strong></p><p id="4022" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了达到最佳效果，线性回归必须遵循以下假设:</p><ul class=""><li id="d653" class="jt ju hi iz b ja jb jd je jg jv jk jw jo jx js jy jz ka kb bi translated"><strong class="iz hj">特征和目标之间的线性关系。</strong></li><li id="7e72" class="jt ju hi iz b ja kc jd kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="iz hj">特征之间很少或没有多重共线性。</strong></li><li id="7ba7" class="jt ju hi iz b ja kc jd kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="iz hj">误差项的正态分布。</strong></li><li id="db0d" class="jt ju hi iz b ja kc jd kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="iz hj">同方差假设(又名恒定方差假设)。</strong></li><li id="9721" class="jt ju hi iz b ja kc jd kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="iz hj">误差项的正态分布。</strong></li><li id="361e" class="jt ju hi iz b ja kc jd kd jg ke jk kf jo kg js jy jz ka kb bi translated"><strong class="iz hj">残差中很少或没有自相关。</strong></li></ul><p id="a4e6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我不会在这里讨论这些假设的细节。更多信息，请查看此<a class="ae kh" href="https://towardsdatascience.com/assumptions-of-linear-regression-algorithm-ed9ea32224e1" rel="noopener" target="_blank">帖子</a>。</p><p id="a63f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">线性回归的思想很简单。我们给定一个具有一组特征的数据，并且输出范围是连续的。我们的任务是找到一条最佳直线(3D中的平面和多维中的超平面),该直线与输出之间的误差尽可能接近。</p><p id="4301" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设我们有一个特征矩阵和一个相应目标的向量:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es ki"><img src="../Images/321d305d53f6a5af433950b559088096.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*srjHYs0GVipFVmWiKz0oag.png"/></div></figure><p id="fb34" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中N是数据点数，D是每个数据点的维数。</p><p id="54c3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">线性回归最简单的形式是通过参数<strong class="iz hj"> w </strong>从<strong class="iz hj"> X </strong>映射到<strong class="iz hj"> y </strong>的线性变换<strong class="iz hj"> h </strong>:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es kq"><img src="../Images/cbc32651398c07ecb1c4b2bfec61aabe.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*Q0ZHccrm3Ds2D9KjZyO1fQ.png"/></div></figure><h1 id="4463" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated"><strong class="ak"> 1。正规方程:</strong></h1><p id="e2d7" class="pw-post-body-paragraph ix iy hi iz b ja lj ij jc jd lk im jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated">使用一点线性代数知识，我们可以找到<strong class="iz hj"> w </strong>的封闭形式:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lo"><img src="../Images/03e995dfdbda97fa4b01345a9a3e4500.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*918C8XrjQQWkTG9d4FCrEQ.png"/></div></figure><p id="74b2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然而，在实践中<strong class="iz hj"> X </strong>很少是方阵，所以我们通常不能对<strong class="iz hj"> X求逆，</strong>概括的方法是将其转置为方阵，然后求逆:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lp"><img src="../Images/36c13743f2c4152bef506c6aa8432df4.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*1PrCkuXUCYFxqakHzTtkeQ.png"/></div></figure><p id="585b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这种方法有两个缺点:</p><ul class=""><li id="4b19" class="jt ju hi iz b ja jb jd je jg jv jk jw jo jx js jy jz ka kb bi translated">首先，X转置X是非常昂贵的计算。假设x有shape = (60000，20)那么x转置x有2⁰ * 60000计算。或者更糟糕的是，如果X维数很高，计算机内存会爆炸。</li><li id="1030" class="jt ju hi iz b ja kc jd kd jg ke jk kf jo kg js jy jz ka kb bi translated">第二，不是每个X转置X都是可逆的。并且可逆计算也很昂贵。</li></ul><p id="6163" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就是<strong class="iz hj">梯度下降</strong>算法的用武之地。</p><h1 id="9a50" class="kr ks hi bd kt ku kv kw kx ky kz la lb io lc ip ld ir le is lf iu lg iv lh li bi translated">2.梯度下降:</h1><p id="0e9d" class="pw-post-body-paragraph ix iy hi iz b ja lj ij jc jd lk im jf jg ll ji jj jk lm jm jn jo ln jq jr js hb bi translated">算法的思想是在任何一点上我们移动一小步作为它的导数的向后方向。该算法始终确保以适当的<strong class="iz hj">学习速率</strong>在有限的步骤中接近全局最小值(这适用于线性回归，但不适用于其他高级模型)。</p><p id="669f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最常见的回归损失是均方误差(MSE ),其形式为:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lp"><img src="../Images/f6340b1e3bb8567cd126d8468c74f449.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*vT0HWMDlL4neyZE5YTktrg.png"/></div></figure><p id="3d87" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们现在的任务是找到使<strong class="iz hj"> y </strong>和<strong class="iz hj"> h(w) </strong>误差最小的参数<strong class="iz hj"> w </strong>。那么我们如何找到最好的<strong class="iz hj"> w </strong>？正如我们从高中学到的，在使函数<strong class="iz hj"> f(x) </strong>有导数<strong class="iz hj">f(x)’</strong>为零的点<strong class="iz hj"> x </strong>处，要么是最小值，要么是最大值，因为我们知道MSE的形状像一个碗(凸函数)，那么我们可以确定该点是最小值。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lq"><img src="../Images/7efe40351837a74505382e8a9c0b2fde.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*C3zklPy47yFs3eD0xMKYRQ.png"/></div></figure><p id="dc37" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> J </strong>相对于<strong class="iz hj"> w </strong>的导数为:</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lr"><img src="../Images/5dd62c4568eafec37149ef5b90f9b47c.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*cpcJ2Sqc-So9HBpppn9BrA.png"/></div></figure><p id="c7e4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于每一步，使用梯度下降算法优化<strong class="iz hj"> w: </strong></p><figure class="kj kk kl km fd kn er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es ls"><img src="../Images/e1842e36c607fb11e0d13a012349fa82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s9vgrPUqWFeqaLS5dVolRA.png"/></div></div></figure><p id="e8a2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果学习率太大，我们对导数的更新就会爆炸，而且永远不会收敛。</p><p id="9ed7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果学习太小，收敛那么慢，或者很可能永远卡在1点。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lx"><img src="../Images/a0c21bf316d654b8496aa704f840c651.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*0yL87-2dAMVTCB1lDzPK-Q.gif"/></div><figcaption class="ly lz et er es ma mb bd b be z dx translated">线性回归梯度下降。</figcaption></figure><p id="abb2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是多项式线性回归，虽然它是一条曲线，但我们的优化仍然是关于线性的<strong class="iz hj"> w </strong>。</p><figure class="kj kk kl km fd kn er es paragraph-image"><div class="er es lx"><img src="../Images/b03724af0c5feeacee2ed84d12f39d72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*bUoZZ1T6KRUNPHAJ3CZtqg.gif"/></div></figure><p id="144a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于线性回归实施，在此处检查<a class="ae kh" href="https://github.com/giangtranml/ml-from-scratch/tree/master/linear_regression" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>