<html>
<head>
<title>Bank Data: PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">银行数据:PCA</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bank-data-pca-eb9e3c5204a3?source=collection_archive---------14-----------------------#2020-08-24">https://medium.com/analytics-vidhya/bank-data-pca-eb9e3c5204a3?source=collection_archive---------14-----------------------#2020-08-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f68a22d1424a6502ee8bca43827d2dab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rTpOejjDSJQ-qyM6YLN8fA.jpeg"/></div></div></figure><p id="e20a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">将用于本示例的数据集位于<a class="ae jo" href="https://www.kaggle.com/zaurbegiev/my-dataset#credit_train.csv" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上。这个讨论将是关于在银行数据上使用主成分分析的过程。</p><h2 id="437a" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated">什么是PCA？</h2><p id="f8d5" class="pw-post-body-paragraph iq ir hi is b it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj ko jl jm jn hb bi translated">PCA，即主成分分析，是一种使用正交变换将一组相关变量转换为一组不相关变量的统计程序。</p><p id="7091" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">PCA是一种主要用于探索性数据分析、EDA和机器学习预测建模的工具。你也可以使用主成分分析进行降维，这也被称为特征提取。当希望通过减少数据集中的要素数量来简化数据集时，这将非常有用。您可能希望这样做来降低模型的计算复杂性，从而使模型运行得更快。</p><h2 id="3ae1" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated">第一步:缩放</h2><p id="58a5" class="pw-post-body-paragraph iq ir hi is b it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj ko jl jm jn hb bi translated">在执行PCA之前，一定要记住缩放数据。缩放数据非常重要，因为变量的值越高，它就越重要。</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kp"><img src="../Images/c5c710e207b79ef1847a350861ee5bbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pMSB24zqNjAOQDMKzqJ5BA.png"/></div></div></figure><p id="cfaf" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上图显示，我们的因变量是贷款状态，正在执行训练测试分割，我们使用MinMaxScaler来缩放数据。</p><p id="a5dc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们使用MinMaxScaler而不是标准Scaler，因为我们的数据不是高斯分布，以便正确使用标准Scaler。我们也不会使用任何假设数据呈高斯分布的机器学习模型。这让我们不用转换数据，少了一个额外的步骤。</p><h2 id="3d9e" class="jp jq hi bd jr js jt ju jv jw jx jy jz jb ka kb kc jf kd ke kf jj kg kh ki kj bi translated">第二步:PCA</h2><p id="9c08" class="pw-post-body-paragraph iq ir hi is b it kk iv iw ix kl iz ja jb km jd je jf kn jh ji jj ko jl jm jn hb bi translated">数据缩放后，我们可以继续执行PCA。我们可以使用sklearn的PCA类，拟合我们的训练数据。</p><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kp"><img src="../Images/16d30c0a450d54fb3e79c74d6d77c72b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FDVmPGpKHEkARmPhwDzrbQ.png"/></div></div></figure><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="1af5" class="jp jq hi kv b fi kz la l lb lc">pca.explained_variance_ratio_</span></pre><p id="d130" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的例子将显示我们的机器学习模型认为对我们的因变量最重要的值。方差越大，该特征越重要。</p><p id="4fe6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们还可以用图表显示我们的结果，以查看我们想要保留的特性。</p><pre class="kq kr ks kt fd ku kv kw kx aw ky bi"><span id="42f2" class="jp jq hi kv b fi kz la l lb lc">fig, ax = plt.subplots()</span><span id="733b" class="jp jq hi kv b fi ld la l lb lc"># Setting width and height<br/>fig.set_figheight(10)<br/>fig.set_figwidth(15)</span><span id="215d" class="jp jq hi kv b fi ld la l lb lc"># x and y values<br/>xi = np.arange(0, 26, step=1)<br/>y = np.cumsum(pca.explained_variance_ratio_)</span><span id="8cfe" class="jp jq hi kv b fi ld la l lb lc">plt.ylim(0.0, 1.1)<br/>plt.plot(xi, y, marker='o', linestyle='--', color='b')</span><span id="0a3f" class="jp jq hi kv b fi ld la l lb lc">plt.xlabel('Number of Components')<br/>plt.xticks(np.arange(0, 26, step=1)) # change from 0-based array index to 1-based human-readable label<br/>plt.ylabel('Cumulative variance (%)')<br/>plt.title('The number of components needed to explain variance')</span><span id="f52e" class="jp jq hi kv b fi ld la l lb lc">plt.axhline(y=0.95, color='r', linestyle='-')<br/>plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)</span><span id="faa4" class="jp jq hi kv b fi ld la l lb lc">ax.grid(axis='x')<br/>plt.show()</span></pre><figure class="kq kr ks kt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es le"><img src="../Images/2cc204b9b40652fbc2c11902425b7963.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*95cQ9sTnCYt5SdtbIJ7vMg.png"/></div></div></figure></div></div>    
</body>
</html>