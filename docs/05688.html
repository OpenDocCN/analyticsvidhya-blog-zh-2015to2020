<html>
<head>
<title>Activation functions in Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的激活函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/activation-functions-in-neural-networks-f36ed3946d91?source=collection_archive---------10-----------------------#2020-04-29">https://medium.com/analytics-vidhya/activation-functions-in-neural-networks-f36ed3946d91?source=collection_archive---------10-----------------------#2020-04-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/25b8060346b66045d5c4f250e316cd91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*2doUN-Ce1kH38NvfHNicxg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">激活功能</figcaption></figure><p id="82ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">激活函数</strong>是一种非线性转换，我们在将输入发送到下一层神经元或最终作为输出之前对其进行转换</p><ul class=""><li id="d3d8" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">我们需要激活函数将非线性真实世界的属性引入人工神经网络。</li></ul><p id="e268" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">那么为什么需要非线性函数呢？</strong></p><p id="8433" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">多阶函数称为非线性函数。</p><ul class=""><li id="a862" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">神经网络使用非线性激活函数，可以帮助网络学习复杂的数据，计算和学习几乎任何代表问题的函数，并提供准确的预测。</li></ul><p id="617c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">没有激活功能的神经网络将只是一个线性回归模型，其能力有限，并且大多数时候表现不佳</strong></p><p id="e85b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">激活功能基本上可以分为两种类型</p><ol class=""><li id="81b5" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jx ju jv jw bi translated">线性激活函数</li><li id="96e0" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jx ju jv jw bi translated">非线性激活函数</li></ol><h1 id="2271" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">线性或身份激活功能</strong></h1><figure class="lc ld le lf fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/0c42567b99ec08c348492bbf6e86d77a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*x2LgVRUoPiS4l0XzRK5TeQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">线性激活函数和导数</figcaption></figure><p id="f034" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如你所见，这个函数是一条直线或线性的。因此，函数的输出将不会被限制在任何范围之间。</p><p id="cad7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">等式:</strong> f(x) = x →输出与输入相同</p><p id="537e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">范围:</strong>(-无穷大到无穷大)</p><p id="b587" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它对输入到神经网络的通常数据的各种参数的复杂性没有帮助。</p><p id="e76e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> y=mx+c(在神经网络中，m是表示W的直线方程，c表示为b，因此该方程可以修改为<em class="lg"> y=Wx+b </em> ) </strong></p><p id="a3a0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它将输入(Xi)乘以每个神经元的权重(Wi ),并产生与输入成比例的输出。简单地说，加权和输入与输出成正比。</p><p id="cf18" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">线性函数的问题，</strong></p><ul class=""><li id="f87a" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">线性函数处理复杂性的能力有限。它可以用于简单的任务，如可解释性。</li><li id="0ca9" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">微分结果是常数。</li><li id="d33b" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">神经网络的所有层都合二为一。</li><li id="4ac2" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">线性函数意味着第一层的输出与第n层的输出相同。</li></ul><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es lh"><img src="../Images/e2ff8c4d6f3f2b7da502f4501d1329d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*347KpZP3z_3lt7hHkiWIvA.png"/></div></div></figure><h1 id="f77c" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">非线性激活功能</strong></h1><p id="ef05" class="pw-post-body-paragraph iq ir hi is b it lm iv iw ix ln iz ja jb lo jd je jf lp jh ji jj lq jl jm jn hb bi translated">大多数现代神经网络使用非线性函数作为激活函数来激活神经元。原因是它们允许模型在网络的输入和输出之间创建复杂的映射，这对于学习和建模复杂的数据是必不可少的，例如图像、视频、音频和非线性或具有高维度的数据集。</p><p id="8a41" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="lg">非线性函数优于线性函数:</em> </strong></p><ul class=""><li id="0d0d" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">微分在所有非线性函数中都是可能的。</li><li id="39c0" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">网络的堆叠是可能的，这有助于我们创建深度神经网络。</li><li id="a2c9" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">这使得模型很容易概括或适应各种数据，并区分输出。</li></ul><p id="2337" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">需要理解非线性函数的主要术语有:</p><p id="f08f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="lg">导数或微分:</em></strong><em class="lg">y轴的变化w . r . t . x轴的变化。它也被称为斜坡。</em></p><p id="9792" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="lg">单调函数:</em> </strong> <em class="lg">要么完全非增，要么完全非减的函数。</em></p><h2 id="7df7" class="lr ke hi bd kf ls lt lu kj lv lw lx kn jb ly lz kr jf ma mb kv jj mc md kz me bi translated"><strong class="ak"> 1。乙状结肠或逻辑激活功能:- </strong></h2><figure class="lc ld le lf fd ij er es paragraph-image"><div class="er es mf"><img src="../Images/69137af12cf4ec3596cbbcbe3971c145.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*JQ17_O6plwj2snALbjE2LA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">Sigmoid激活函数和导数</figcaption></figure><p id="b6f4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Sigmoid函数曲线看起来像S形。</p><p id="871d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们使用sigmoid函数的主要原因是它存在于<strong class="is hj"> (0到1)之间。</strong>因此，它特别适用于我们必须<strong class="is hj">预测概率</strong>作为输出的模型。由于任何事情的概率只存在于<strong class="is hj"> 0到1的范围内，</strong> sigmoid是正确的选择。</p><p id="419b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">函数是<strong class="is hj">可微的。</strong>这意味着，我们可以在任意两点找到s形曲线的斜率。</p><p id="6165" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">函数是<strong class="is hj">单调的</strong>，但函数的导数不是。</p><p id="bac2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">逻辑sigmoid函数会导致神经网络在训练时停滞不前。</p><p id="51c8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="lg"> ***那么乙状结肠功能</em> </strong> <em class="lg">有什么问题吗？</em></p><p id="300e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们仔细观察函数两端的图形，y值对x值的变化反应很小。</p><p id="5961" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">大家想想这是个什么样的问题！这些区域的导数值非常小，收敛到0。这被称为<strong class="is hj">消失梯度</strong>并且学习是最小的。如果为0，则没有任何学习！当缓慢学习发生时，使误差最小化的优化算法可以附加到局部最小值，并且不能从人工神经网络模型获得最大性能。</p><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mg"><img src="../Images/0eae727a717fac3724886c9d67d7c2da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_jC1tFqAaBOLYjCWgxYiEA.png"/></div></div></figure><h2 id="8253" class="lr ke hi bd kf ls lt lu kj lv lw lx kn jb ly lz kr jf ma mb kv jj mc md kz me bi translated"><strong class="ak"> 2。双曲正切或双曲正切激活函数</strong></h2><figure class="lc ld le lf fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/2c6c570e4df8914213fe1b827cfa0459.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*aKqaT1CvdfbE0c5f3sw4zQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">Tanh激活函数及其导数</figcaption></figure><p id="3a11" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Tanh也像逻辑s形但更好。双曲正切函数的范围是从(-1到1)。tanh也是s形的(s形)。</p><p id="55a1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它的结构非常类似于Sigmoid函数。但是，这一次函数被定义为(-1，+ 1)。相对于sigmoid函数的优势在于它的导数更陡，这意味着它可以获得更多的值。这意味着它的效率会更高，因为它的学习和评分范围更广。但是，函数两端的梯度问题仍然存在。</p><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mh"><img src="../Images/a122d2c2563bcdc3306b1366d95fc196.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W26XAQOt1XVf-YJrHlLcfg.png"/></div></div></figure><p id="d274" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 3。ReLU(整流线性单元)激活功能</strong></p><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mi"><img src="../Images/7ed98a0c4965a425a8501058fcd06533.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d1kg_utFKLxGXhHTbQKlpg.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">ReLU激活函数及其导数</figcaption></figure><p id="8381" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">ReLU是目前世界上使用最多的激活函数。因为它用于几乎所有的卷积神经网络或深度学习。</p><p id="7530" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如你所看到的，ReLU是半整流(从底部)。当z小于零时f(z)为零，当z大于或等于零时f(z)等于z。</p><p id="3917" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">范围:</strong>【0到无穷大】</p><p id="6c76" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">函数及其导数<strong class="is hj">都是</strong> <strong class="is hj">单调</strong>。</p><p id="a39b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但问题是，所有的负值立即变为零，这降低了模型根据数据进行适当拟合或训练的能力。这意味着给予ReLU激活函数的任何负输入都会在图形中立即将值变成零，这反过来会通过不适当地映射负值来影响结果图形。</p><p id="0cbe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们想象一个有太多神经元的大型神经网络。Sigmoid和双曲线正切导致几乎所有的神经元都以相同的方式被激活。这意味着激活非常密集。网络中的一些神经元是活跃的，并且激活是不频繁的，所以我们想要有效的计算负载。我们用ReLU得到它。负轴上的值为0意味着网络将运行得更快。<strong class="is hj">计算负荷小于sigmoid和双曲线正切函数的事实导致了对多层网络的更高偏好。</strong></p><p id="c5dd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是即使是热路也不完全是伟大的，为什么？因为这个零值区域给了我们这个过程的速度！所以学习不是发生在那个区域。那你就需要找一个新的激活功能，有窍门的。</p><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mj"><img src="../Images/be5d1b97b56418ca5107a48ccc9e1893.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CJBc4ygB5Ep9eweu8J1kPA.png"/></div></div></figure><p id="f4b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 4。泄漏的ReLU </strong></p><p id="dcc4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">负平面上的泄漏是解决垂死的ReLU问题的一种尝试</p><figure class="lc ld le lf fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/740dd7c19e603b3e1f0d7f82782b0ed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*Q0OhbySqd1XxtTzQRrhTJg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">泄漏ReLU和导数</figcaption></figure><p id="4e59" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">泄漏有助于增加ReLU功能的范围。通常情况下，<strong class="is hj"> a </strong>的值在0.01左右。</p><p id="7b22" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当<strong class="is hj"> a不为0.01 </strong>时，称为<strong class="is hj">随机化ReLU </strong>。</p><p id="9a07" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，泄漏ReLU的<strong class="is hj">范围</strong>是(-无穷大到无穷大)。</p><p id="aab6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">泄漏和随机化ReLU函数本质上都是单调的。同样，它们的导数在性质上也是单调的。</p><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mk"><img src="../Images/8f3177c0092a8e3aef087c31c4124b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zttz83tAhQ8-ljSgNUIPHw.png"/></div></div></figure><p id="7b81" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 5。ELU(指数线性单位)激活函数:</strong></p><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es ml"><img src="../Images/8fbcea77a4c23f8298ddc61da5ddc381.png" data-original-src="https://miro.medium.com/v2/resize:fit:1308/format:webp/1*7t7OQIkiMy7dbgx53hZQcw.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">ELU激活函数及其导数</figcaption></figure><ul class=""><li id="d4f3" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">ELU还提出解决神经元死亡的问题。</li><li id="45db" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">没有死ReLU问题</li><li id="85bb" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">零中心的</li></ul><p id="0966" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">缺点:</strong></p><ul class=""><li id="c29a" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">计算密集型。</li><li id="7cca" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">类似于Leaky ReLU，虽然理论上比ReLU好，但是目前在实践中没有很好的证据证明eLU总是比ReLU好。</li><li id="a2e6" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">只有当alpha大于或等于0时，f(x)才是单调的。</li><li id="8e5f" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">只有当α在0和1之间时，ELU的f′(x)导数才是单调的。</li><li id="86d3" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">指数函数导致收敛缓慢。</li></ul><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mm"><img src="../Images/0bc1c7f6a73e8afc9257f28ea0477d58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6GD4hKeh-5vjqPW08n9RzA.png"/></div></div></figure><p id="b8f8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 6。P ReLu(参数ReLu)激活功能:</strong></p><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mn"><img src="../Images/777f63831da0c49e1a0f0709333d3a13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wDwigaDAxsJwWOSFyKbphw.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated"><strong class="bd kf">参数ReLU </strong></figcaption></figure><ul class=""><li id="ae85" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">漏ReLU的概念可以进一步扩展。</li><li id="ce41" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">我们可以用一个“超参数(可训练参数)”来乘x，而不是用一个常数项来乘x，这似乎对泄漏ReLU更有效。这种对泄漏ReLU的扩展被称为参数ReLU。</li><li id="7a06" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">参数α一般是0到1之间的数，一般比较小。</li><li id="ebc8" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">由于可训练参数，比泄漏Relu稍有优势。</li><li id="9f8b" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">处理神经元死亡的问题。</li></ul><p id="ac44" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">缺点:</strong></p><ul class=""><li id="c5e6" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">与泄漏的Relu相同。</li><li id="72d9" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">当a &gt;或=0时，f(x)是单调的，当a =1时，f'(x)是单调的</li></ul><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mo"><img src="../Images/22d5bcc377a41e20ad0ae846882237a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C7NmlA9l3Aax4qsxVUt1pA.png"/></div></div></figure><p id="281f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> 7。唰(一种自门控)</strong> <strong class="is hj">激活功能:(乙状结肠线性单元)</strong></p><figure class="lc ld le lf fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/b6a9932e5888d5e7475863037b74176f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*M1dMMuKTOamAq7ggWr1-0A.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">Swish激活函数及其导数</figcaption></figure><ul class=""><li id="f876" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">Google Brain团队提出了一个新的激活函数，命名为<strong class="is hj"> Swish </strong>，简单来说就是<strong class="is hj"> f(x) = x sigmoid(x) </strong>。</li><li id="5c9e" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">他们的实验表明，在许多具有挑战性的数据集上，Swish往往比ReLU更好地工作。</li><li id="a3c3" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj"> <em class="lg"> Swish函数的曲线是光滑的</em> </strong>并且函数在所有点上都是可微的<strong class="is hj"><em class="lg"/></strong>。这在模型优化过程中是有帮助的，并且被认为是swish优于ReLU的原因之一。</li><li id="4cc3" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">唰功能是<strong class="is hj"> <em class="lg">不单调</em> </strong>。这意味着即使输入值在增加，函数值也可能减少。</li><li id="f647" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><strong class="is hj">函数上无界下有界。</strong></li></ul><p id="19ca" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"><em class="lg">“Swish倾向于持续匹配或超越ReLu”</em></strong></p><figure class="lc ld le lf fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/cd23398ea9474bbc5074626e234d383b.png" data-original-src="https://miro.medium.com/v2/resize:fit:422/format:webp/1*HwLxldXjEUH9YcK77ZfHdA.png"/></div></figure><p id="516b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="lg">注意，即使输入增加，swish功能的输出也可能下降。这是一个有趣的swish特有的特性。</em> </strong>(由于非单调字符)</p><h2 id="29c3" class="lr ke hi bd kf ls lt lu kj lv lw lx kn jb ly lz kr jf ma mb kv jj mc md kz me bi translated"><strong class="ak"> 8。Softmax功能</strong></h2><ul class=""><li id="fea2" class="jo jp hi is b it lm ix ln jb mq jf mr jj ms jn jt ju jv jw bi translated">“softmax”函数也是一种sigmoid函数，但它对于处理多类分类问题非常有用。</li><li id="40e9" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">" Softmax可以描述为多个s形函数的组合."</li><li id="407e" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">" Softmax函数返回属于每个单独类的数据点的概率."</li><li id="441f" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">"注意所有值的总和是1 . "</li></ul><figure class="lc ld le lf fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/75786b07e526d19637404b2b26e98db5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*xYxbjwe27XDz1bxM3W8j8Q.png"/></div></figure><figure class="lc ld le lf fd ij er es paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="er es mu"><img src="../Images/1fa0c8fddbb655fa421a7c4bc6b16247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EuHJZ2Cr-BVGWSaI6sNqFA.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">激活函数及其方程列表</figcaption></figure></div></div>    
</body>
</html>