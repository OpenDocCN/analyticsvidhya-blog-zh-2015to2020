<html>
<head>
<title>A quick grasp of Convolution Neural Networks (CNN) and its implementation on MNIST dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">快速掌握卷积神经网络及其在MNIST数据集上的实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-quick-grasp-of-convolution-neural-networks-cnn-7a258bc9bfe6?source=collection_archive---------21-----------------------#2020-05-16">https://medium.com/analytics-vidhya/a-quick-grasp-of-convolution-neural-networks-cnn-7a258bc9bfe6?source=collection_archive---------21-----------------------#2020-05-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/8919598e59e94334bf0374bce79d357a.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*TMdVtvyGhvBShHnUVPjHrw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">目录</figcaption></figure><p id="1bc8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">卷积神经网络(CNN)是一种流行的深度人工神经网络。CNN由可学习的权重和偏见组成。CNN与普通的神经网络非常相似，但又不完全相同。</p><p id="1f2f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">CNN主要用于图像识别、图像聚类和分类、目标检测等..</p><h2 id="afa7" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">为什么是CNN的？</h2><p id="59d2" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">CNN的是权重共享，不太复杂，占用内存也少。</p><p id="77dc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们来看一个MNIST数据集图像，它被传递给CNN和NN。</p><p id="ad70" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设在CNN层，10个5x5大小的过滤器，那么我们有5x5x10 +10(偏差)=260个参数。</p><p id="ec7f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设图像尺寸为784，神经网络层有250个神经元，那么在神经网络(NN)中，我们有784×260+1 = 19601个参数</p><p id="79ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，在传统的图像识别任务和许多其他任务上，CNN的表现优于NNs。</p><h2 id="fe6e" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">CNN运作背后的理念</h2><p id="7258" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">计算机视觉中的卷积运算在生物学上受到大脑视觉皮层的启发。CNN的连接模式类似于动物视觉皮层的结构。</p><p id="2d40" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果图像被传递到视觉皮层，那么皮层通过片段/层处理该信息。大脑从每个片段/层提取信息。第一层学习诸如边缘或颜色之类的表示，而中间层学习诸如对象部分之类的中间抽象表示，最后，高级层学习诸如猫的脸之类的完整对象。随着抽象层次的增加，推论变得更加清晰。因此，大脑根据它通过所有层次学习到的信息做出决策。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ko"><img src="../Images/e1f23a6739a645f06df0f8a32a36671f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*IChLRi9E8KSLc9JepgboXQ.png"/></div></figure><h2 id="52b6" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">CNN的几层</h2><p id="7dab" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">Cnn由不同的层组成。它们是输入层和输出层。在这些层之间，有多个隐藏层，如“<em class="kt">卷积层</em>”、“<em class="kt">激活层</em>”、“<em class="kt">最大汇集层”、“全连接层】</em>。对于网络中存在的隐藏层没有限制。输入层接受输入并进行专门训练，然后从输出层给出一个输出。在CNN的帮助下，我们可以更有效、更准确地使用大量数据。</p><p id="868b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在<strong class="is hj">卷积层中，</strong>我们有</p><p id="0bcd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这一层主要有助于边缘检测。详细解释请参考<a class="ae ku" href="https://en.wikipedia.org/wiki/Sobel_operator" rel="noopener ugc nofollow" target="_blank">索贝尔边缘检测器</a>。</p><p id="df8a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">卷积运算</strong></p><p id="3a17" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> </strong>在数学中，卷积是对两个函数的数学运算，产生第三个函数，表示一个函数的形状如何被另一个函数修改。</p><p id="0633" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">卷积层对输入应用卷积运算，以便将结果传递给下一层。每个卷积只处理其各自字段的数据。</p><p id="47b0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里，在卷积中，我们对图像矩阵、核/滤波器矩阵执行逐元素乘法和加法运算，产生输出矩阵。</p><p id="375d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">内核/滤波器矩阵包含随机分配的权重，并在反向传播期间得到更新。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es kv"><img src="../Images/cfb362ca3812f9f82e8a2df550d569df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*exWZPR-OBy-7uUAESFjwLg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">卷积运算</figcaption></figure><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es kw"><img src="../Images/800bae55f5300ae9c2c6a2e8de279f81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ulfFYH5HbWpLTIfuebj5mQ.gif"/></div></div></figure><h2 id="e2b8" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">填充和步幅</h2><p id="965b" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">很明显，在执行卷积之后，我们正在获得一个降维的卷积特征/矩阵。为了保持输入中输出的维数，我们使用填充。</p><p id="d5ab" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">填充过程将0对称地添加到输入矩阵的边界，因此在卷积后，我们会丢失填充的维度，因此我们可以在输出中保留原始维度。</p><p id="a7d7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">填充前的尺寸:</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lb"><img src="../Images/fdb9f23f112352a25ebdd0f82d26fcd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*XEKhJC-ALz_3KROtoqlqxA.png"/></div></figure><p id="86f5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">填充后的尺寸:在图像的两侧添加填充</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/9191a6df4492e4ea771b3e0831f096f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*9u4dEi1S2uIERB9YHq4mjg.png"/></div></figure><h2 id="fcfc" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">进展</h2><p id="e437" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">内核/过滤器在图像矩阵上从左到右、从上到下移动，在水平移动时改变一个像素的列，然后在垂直移动时改变一个像素的行。</p><p id="d36c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">图像上的核矩的量被称为“步幅”。</p><p id="c20b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于步幅S，填充P</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/4710a5cd1406f03df30d79824f23cc13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*4YmJT6garUDlC1xT2SSGZA.png"/></div></figure><p id="7ccc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这些内核值在每次迭代中都会更新。</p><h2 id="b0ce" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">RGB图像上的卷积</h2><p id="6ac8" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated">我们观察到灰度图像的卷积值在0-255之间。</p><p id="5dc3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">RGB图像有额外的3个通道，因此尺寸看起来像(n x n x c)</p><p id="5cef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中c =通道数RGB中的3]</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="er es le"><img src="../Images/40c6047ef1b6a869d0e25b41cc20f2ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*XBTE1s3sI5urwjWRmnYm5Q.png"/></div></div></figure><p id="16a3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">无论如何，通道尺寸在输出中变得中立，因为使用C-dim核在C-dim图像上进行元素方式的乘法和加法操作。</p><p id="fd71" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">与多个内核的卷积:</strong></p><p id="5648" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们考虑一下，我们有m个内核</p><p id="4bfa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设，如果对具有多于一个核的图像执行卷积，那么输出的维度(即卷积特征)也增加了“m”个维度。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lf"><img src="../Images/b6d7f027ede5af8b2e3dfabf3445b49f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*XmuCbyTBHh_7YB4lejN3aA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">多核卷积</figcaption></figure><p id="ece5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">激活层</strong></p><p id="98bb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">激活函数将非线性引入模型，使得复杂度增加，并且模型学习更多。</p><p id="b1ac" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它用于确定神经网络的输出(如是/否)。它也可以连接在两个不同的神经网络之间。在使用内核对元素进行乘法和加法之后，我们对该值应用类似“Relu”、“Sigmoid”、“Tanh”、“Leaky Relu”、“Softmax”的激活。</p><p id="b102" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，在复杂功能中，每个值都是激活层的输出。</p><blockquote class="lg"><p id="d9fd" class="lh li hi bd lj lk ll lm ln lo lp jn dx translated">激活(a*w1 + b*w2 + e*w3 + f*w4)</p></blockquote><p id="5cf3" class="pw-post-body-paragraph iq ir hi is b it lq iv iw ix lr iz ja jb ls jd je jf lt jh ji jj lu jl jm jn hb bi translated">其中，a，b，e，f是图像中元素的聚类。</p><p id="57a3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">w1、w2、w3、w4是核的权重。</p><p id="043b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">汇集层</strong></p><p id="7730" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它负责减小复杂要素的空间大小。池化将一层的元素集群的输出合并到下一层的单个元素中。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/20ede518a5fa59033d86342b709db74d.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*1VAxMqHMX4Ra05kGMa2gVQ.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">池层跨度=2，内核=(2，2)</figcaption></figure><p id="b710" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="kt"> "Max pooling" </em>使用前一层每个元素簇的最大值。</p><p id="4e59" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="kt"> "Mean/Average pooling" </em>使用前一层元素簇的平均值。</p><p id="b60c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">完全连接</strong></p><p id="cfed" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">网络中的最后几层是完全连接的，这意味着前面几层的元素与后面几层中的每个元素都是连接的。这模仿了高级推理，其中考虑了从输入到输出的所有可能路径。</p><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/74262ace28b92f9dec6a4ee18831e809.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*xEUfq5q1SJQ0Q515VCP6rw.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">CNN的所有层</figcaption></figure><h2 id="7f39" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">履行</h2><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/db6d81402301f30e69f52368f684f067.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*oVg5TFJmnfE5DUIaZiG-hQ.png"/></div></figure><figure class="kp kq kr ks fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/21d2d432c4fd86117fed877375b094e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*rVJHOGdN9kBmYem79Khl3w.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">模型架构</figcaption></figure><h2 id="e8dc" class="jo jp hi bd jq jr js jt ju jv jw jx jy jb jz ka kb jf kc kd ke jj kf kg kh ki bi translated">参考资料:</h2><p id="e94d" class="pw-post-body-paragraph iq ir hi is b it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj kn jl jm jn hb bi translated"><a class="ae ku" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Convolutional_neural_network</a></p><p id="4777" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae ku" href="https://www.quora.com/What-is-a-convolutional-neural-network" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/什么是卷积神经网络</a></p><p id="a279" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae ku" href="https://www.quora.com/What-are-the-advantages-of-a-convolutional-neural-network-CNN-compared-to-a-simple-neural-network-from-the-theoretical-and-practical-perspective" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/从理论和实践的角度来看，卷积神经网络与简单神经网络相比有何优势</a></p><p id="20a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae ku" rel="noopener" href="/intuitive-deep-learning/intuitive-deep-learning-part-1a-introduction-to-neural-networks-d7b16ebf6b99">https://medium . com/intuitive-deep-learning/intuitive-deep-learning-part-1a-神经网络导论-d7b16ebf6b99 </a></p><p id="54a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae ku" href="https://www.quora.com/Does-the-visual-cortex-explicitly-perform-segmentation" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/Does-the-visual-cortex-explicitly-perform-segmentation</a></p><h1 id="19bf" class="lz jp hi bd jq ma mb mc ju md me mf jy mg mh mi kb mj mk ml ke mm mn mo kh mp bi translated">快速掌握卷积神经网络</h1></div></div>    
</body>
</html>