<html>
<head>
<title>Machine Learning &amp; Deep Learning Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习和深度学习指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-deep-learning-guide-11ad26e0854c?source=collection_archive---------2-----------------------#2019-11-20">https://medium.com/analytics-vidhya/machine-learning-deep-learning-guide-11ad26e0854c?source=collection_archive---------2-----------------------#2019-11-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4952" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated"><span class="l je jf jg bm jh ji jj jk jl di"> W </span>欢迎来到机器学习&amp;深度学习指南的第3部分，在这里我们学习和实践机器学习和深度学习，而不会被概念和数学规则所淹没。</p><blockquote class="jm jn jo"><p id="f6ee" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><a class="ae jt" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-part-1-4ba7ce8cf7eb"> <em class="hi">第1部分:关键术语、定义和从监督学习(线性回归)开始。</em> </a></p><p id="e9fd" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><a class="ae jt" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-db520c4797da"> <em class="hi">第二部分:监督学习:回归(SGD)和分类(SVM、朴素贝叶斯、KNN和决策树)。</em>T9】</a></p><p id="ee06" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><em class="hi">第3部分:无监督学习(KMeans，PCA)，欠拟合与过拟合以及交叉验证。</em></p><p id="95a1" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><a class="ae jt" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-da303a71b8e0"> <em class="hi">第四部分:深度学习:定义、层次、度量和损失、优化器和正则化</em> </a></p></blockquote></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="7386" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">学习目标</h1><p id="84c9" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">在这一部分，我们将结合实例<strong class="ih hj"> </strong>讨论<strong class="ih hj">无监督学习</strong>以及如何与<strong class="ih hj">有监督学习配合使用。</strong>我们还将学习如何进行交叉验证，以及过度拟合和欠拟合之间的区别。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es le"><img src="../Images/f6582131b9b09cdd1c7a9a8f72e28cb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QQBGYFrckYS5qspJUQ2LpQ.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">来源:<a class="ae jt" href="https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a></figcaption></figure><h1 id="938d" class="kb kc hi bd kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku ly kw kx ky bi translated"><strong class="ak">无监督学习</strong></h1><p id="262d" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">到目前为止，我们考虑监督学习，其中我们有一组特征(X)和标签(y ),我们希望学习从特征到标签的映射。在无监督学习中，我们只有特征(X ),我们希望在数据中找到模式。</p><p id="d612" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如第一部分中提到的，我们将无监督学习分为两个概念:</p><ol class=""><li id="2919" class="lz ma hi ih b ii ij im in iq mb iu mc iy md jc me mf mg mh bi translated"><strong class="ih hj">聚类</strong>:将数据按照相似性进行分组。</li><li id="b61c" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><strong class="ih hj">降维</strong>:降维压缩数据，同时保持其结构和有用性。</li></ol></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="836b" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">聚类</strong></h1><p id="ffd7" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">根据相似性将数据分组。我们将考虑<a class="ae jt" href="https://scikit-learn.org/stable/modules/clustering.html#k-means" rel="noopener ugc nofollow" target="_blank"> k-Means </a>进行聚类。它通过尝试将样本分成n组相等的方差来对数据进行聚类，从而最小化被称为<em class="jp">惯性</em>或类内平方和的标准。该算法要求指定聚类数。</p><blockquote class="jm jn jo"><p id="d11b" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated">你可以从<a class="ae jt" href="https://www.kaggle.com/mohammadhatoum/unsupervised-learning-clustering?scriptVersionId=23776964" rel="noopener ugc nofollow" target="_blank">这里</a>下载完整的Kaggle笔记本</p></blockquote><p id="624e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.<strong class="ih hj">数据定义</strong>:我们将使用<a class="ae jt" href="https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits" rel="noopener ugc nofollow" target="_blank">手写数字</a>数据集。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="6445" class="ms kc hi mo b fi mt mu l mv mw">%matplotlib inline<br/>from sklearn.datasets import load_digits<br/>from sklearn.cluster import KMeans<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns; sns.set()  # for plot styling<br/>from scipy.stats import mode</span><span id="6c1f" class="ms kc hi mo b fi mx mu l mv mw">digits = load_digits()<br/>print(digits.data.shape)</span></pre><blockquote class="my"><p id="6004" class="mz na hi bd nb nc nd ne nf ng nh jc dx translated">结果:(1797，64)</p></blockquote><p id="3f09" class="pw-post-body-paragraph if ig hi ih b ii ni ik il im nj io ip iq nk is it iu nl iw ix iy nm ja jb jc hb bi translated">2.算法选择:我们将使用KMeans</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="7b0c" class="ms kc hi mo b fi mt mu l mv mw">kmeans = KMeans(n_clusters=10, random_state=0)<br/>clusters = kmeans.fit_predict(digits.data)<br/>print(kmeans.cluster_centers_.shape)</span></pre><blockquote class="my"><p id="70d3" class="mz na hi bd nb nc nd ne nf ng nh jc dx translated">结果:(10，64)</p></blockquote><p id="c0c5" class="pw-post-body-paragraph if ig hi ih b ii ni ik il im nj io ip iq nk is it iu nl iw ix iy nm ja jb jc hb bi translated">让我们画出不同的结果</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="3112" class="ms kc hi mo b fi mt mu l mv mw">fig, ax = plt.subplots(2, 5, figsize=(8, 3))<br/>centers = kmeans.cluster_centers_.reshape(10, 8, 8)<br/>for axi, center in zip(ax.flat, centers):<br/>    axi.set(xticks=[], yticks=[])<br/>    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es nn"><img src="../Images/4d2f519318571e02901999dc4d7d2bdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*avwWKimRVbINFC4x_9bZSg.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">数字结果</figcaption></figure><p id="6dc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">酷，它能够正确地识别大多数号码。如你所见，它不能识别1和8。现在，为了能够测试准确性，我们将添加相应的标签。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="cb33" class="ms kc hi mo b fi mt mu l mv mw">labels = np.zeros_like(clusters)<br/>for i in range(10):<br/>    mask = (clusters == i)<br/>    labels[mask] = mode(digits.target[mask])[0]</span></pre><p id="b79f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将测试KMeans模型的准确性。然后我们将绘制混淆矩阵。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="2b80" class="ms kc hi mo b fi mt mu l mv mw">from sklearn.metrics import accuracy_score<br/>print(f"Accuracy for KMeans : {accuracy_score(digits.target, labels)}")</span></pre><blockquote class="my"><p id="ed5d" class="mz na hi bd nb nc nd ne nf ng nh jc dx translated">结果:平均值的精确度为:0。59660 . 68686868661</p></blockquote><pre class="no np nq nr ns mn mo mp mq aw mr bi"><span id="279f" class="ms kc hi mo b fi mt mu l mv mw">from sklearn.metrics import confusion_matrix</span><span id="12e2" class="ms kc hi mo b fi mx mu l mv mw">print(f"Confusion Matrix KMeans")</span><span id="94e1" class="ms kc hi mo b fi mx mu l mv mw">mat = confusion_matrix(digits.target, labels)<br/>sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,<br/>            xticklabels=digits.target_names,<br/>            yticklabels=digits.target_names)<br/>plt.xlabel('true label')<br/>plt.ylabel('predicted label')<br/>plt.show()</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nt"><img src="../Images/45410f8ccf5683d8e7eeb73146b8dc66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*7zkfAtk41nhpImD4_OZUvQ.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">混淆矩阵k均值</figcaption></figure><h1 id="8043" class="kb kc hi bd kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku ly kw kx ky bi translated"><strong class="ak">降维(</strong>特征消除)</h1><p id="718d" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">听起来是这样的:我们通过消除特征来减少特征空间。我们将使用<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">【主成分分析】</strong> </a> <strong class="ih hj">。</strong>这是一种技术，用于强调数据集中的变化和突出强模式。</p><blockquote class="jm jn jo"><p id="67ee" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated">你可以从<a class="ae jt" href="https://www.kaggle.com/mohammadhatoum/dimensionality-reduction-pca?scriptVersionId=23776972" rel="noopener ugc nofollow" target="_blank">这里</a>下载完整的Kaggle笔记本</p></blockquote><ol class=""><li id="2bd9" class="lz ma hi ih b ii ij im in iq mb iu mc iy md jc me mf mg mh bi translated"><strong class="ih hj">数据定义:</strong>我们将使用Olivetti人脸数据集<a class="ae jt" href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html" rel="noopener ugc nofollow" target="_blank">，该数据集包含一组于1992年4月至1994年4月在剑桥大学T实验室拍摄的人脸图像</a>。</li></ol><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="49c7" class="ms kc hi mo b fi mt mu l mv mw">data=np.load("../input/olivetti_faces.npy")<br/>labels=np.load("../input/olivetti_faces_target.npy")</span><span id="dfb8" class="ms kc hi mo b fi mx mu l mv mw">print(f"Shape of inputs: {data.shape}")<br/>print(f"Shape of labels: {labels.shape}")<br/>print(f"Unique values for labels: {np.unique(labels)}")</span></pre><blockquote class="my"><p id="13ab" class="mz na hi bd nb nc nd ne nf ng nh jc dx translated">结果:</p><p id="467e" class="mz na hi bd nb nc nu nv nw nx ny jc dx translated">输入的形状:(400，64，64) <br/>标签的形状:(400，)<br/>标签的唯一值:[0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23<br/>24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39]</p></blockquote><p id="3caf" class="pw-post-body-paragraph if ig hi ih b ii ni ik il im nj io ip iq nk is it iu nl iw ix iy nm ja jb jc hb bi translated">所以我们有40个人的400张照片。每个图像的尺寸为64*64。</p><p id="f3f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看第一张图片。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="f7f4" class="ms kc hi mo b fi mt mu l mv mw">imshow(data[0])</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nz"><img src="../Images/e719482107a4dfb2f85ddf31350333e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*J5AuGB_hW1AENEoakJjvUA.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">奥利维蒂形象</figcaption></figure><p id="c481" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们重塑数据。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="729f" class="ms kc hi mo b fi mt mu l mv mw">X=data.reshape((data.shape[0],data.shape[1]*data.shape[2]))<br/>print("After reshape:",X.shape)</span></pre><blockquote class="my"><p id="715d" class="mz na hi bd nb nc nd ne nf ng nh jc dx translated">结果:</p><p id="23a3" class="mz na hi bd nb nc nu nv nw nx ny jc dx translated">整形后:(400，4096)</p></blockquote><p id="50c4" class="pw-post-body-paragraph if ig hi ih b ii ni ik il im nj io ip iq nk is it iu nl iw ix iy nm ja jb jc hb bi translated"><strong class="ih hj"> 2。训练/测试分割:</strong></p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="d12d" class="ms kc hi mo b fi mt mu l mv mw">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test=train_test_split(X, labels, test_size=0.25, stratify=labels, random_state=0)<br/>print("X_train shape:",X_train.shape)<br/>print("y_train shape:{}".format(y_train.shape))</span></pre><blockquote class="my"><p id="8f9e" class="mz na hi bd nb nc nd ne nf ng nh jc dx translated">结果:</p><p id="3d45" class="mz na hi bd nb nc nu nv nw nx ny jc dx translated">x _火车形状:(300，4096)<br/>y _火车形状:(300，)</p></blockquote><p id="26e9" class="pw-post-body-paragraph if ig hi ih b ii ni ik il im nj io ip iq nk is it iu nl iw ix iy nm ja jb jc hb bi translated"><strong class="ih hj"> 3。算法选择:</strong>我们将使用PCA</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="3a92" class="ms kc hi mo b fi mt mu l mv mw">from sklearn.decomposition import PCA<br/>pca=PCA()<br/>pca.fit(X)</span><span id="d61a" class="ms kc hi mo b fi mx mu l mv mw">plt.figure(1, figsize=(12,8))</span><span id="c99d" class="ms kc hi mo b fi mx mu l mv mw">plt.plot(pca.explained_variance_, linewidth=2)<br/> <br/>plt.xlabel('Components')<br/>plt.ylabel('Explained Variaces')<br/>plt.show()</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es oa"><img src="../Images/9d5be3dde506784f2938a3f7e65efdca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JMaLnG6lQ5LN2r6istowSQ.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">解释了组件的变化</figcaption></figure><p id="3734" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如你所见，60岁以后的数值是一样的。这意味着我们可以依赖60个重要组件。</p><p id="551a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。培训:</strong></p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="b0e1" class="ms kc hi mo b fi mt mu l mv mw">n_components=60<br/>pca=PCA(n_components=n_components, whiten=True)<br/>pca.fit(X)</span></pre><blockquote class="my"><p id="66b0" class="mz na hi bd nb nc nd ne nf ng nh jc dx translated">结果:</p><p id="9a85" class="mz na hi bd nb nc nu nv nw nx ny jc dx translated">PCA(copy=True，iterated_power='auto '，n_components=60，random_state=None，<br/> svd_solver='auto '，tol=0.0，白化=True)</p></blockquote><p id="bba1" class="pw-post-body-paragraph if ig hi ih b ii ni ik il im nj io ip iq nk is it iu nl iw ix iy nm ja jb jc hb bi translated">我们将绘制平均脸。也就是说，我们将基于计算出的PCA来取面部的平均值。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="6131" class="ms kc hi mo b fi mt mu l mv mw">fig,ax=plt.subplots(1,1,figsize=(8,8))<br/>ax.imshow(pca.mean_.reshape((64,64)), cmap="gray")<br/>ax.set_xticks([])<br/>ax.set_yticks([])<br/>ax.set_title('Average Face')</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es ob"><img src="../Images/49b6b72a000709efe89c7478cb0dfea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*Bv5xeMZzxZHL6iH47IKXbA.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">平均脸</figcaption></figure><p id="d682" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">打印特征脸</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="5508" class="ms kc hi mo b fi mt mu l mv mw">number_of_eigenfaces=len(pca.components_)<br/>eigen_faces=pca.components_.reshape((number_of_eigenfaces, data.shape[1], data.shape[2]))</span><span id="3825" class="ms kc hi mo b fi mx mu l mv mw">cols=10<br/>rows=int(number_of_eigenfaces/cols)<br/>fig, axarr=plt.subplots(nrows=rows, ncols=cols, figsize=(15,15))<br/>axarr=axarr.flatten()<br/>for i in range(number_of_eigenfaces):<br/>    axarr[i].imshow(eigen_faces[i],cmap="gray")<br/>    axarr[i].set_xticks([])<br/>    axarr[i].set_yticks([])<br/>    axarr[i].set_title("eigen id:{}".format(i))<br/>plt.suptitle("All Eigen Faces".format(10*"=", 10*"="))</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es oc"><img src="../Images/9d83ef8b5227d059749c29fc8e3e4662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vp-KBkw6MPNj_Y9aHK67LQ.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">特征脸</figcaption></figure><p id="969e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用PCA变换训练集和测试集。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="b428" class="ms kc hi mo b fi mt mu l mv mw">X_train_pca=pca.transform(X_train)<br/>X_test_pca=pca.transform(X_test)<br/>print(f"Shape before {X_train.shape} vs shape after {X_train_pca.shape}")</span></pre><blockquote class="my"><p id="fbb5" class="mz na hi bd nb nc nd ne nf ng nh jc dx translated">结果:</p><p id="7b5a" class="mz na hi bd nb nc nu nv nw nx ny jc dx translated">(300，4096)之前的形状与(300，60)之后的形状</p></blockquote><p id="2f7d" class="pw-post-body-paragraph if ig hi ih b ii ni ik il im nj io ip iq nk is it iu nl iw ix iy nm ja jb jc hb bi translated">所以现在我们能够降低输入的维度。</p><p id="1fc7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。预测和评估:</strong>我们将在应用PCA变换后使用LogisticRegression来研究模型的准确性。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="c44d" class="ms kc hi mo b fi mt mu l mv mw">from sklearn.linear_model import LogisticRegression<br/>from sklearn import metrics</span><span id="1324" class="ms kc hi mo b fi mx mu l mv mw">clf = LogisticRegression()<br/>clf.fit(X_train_pca, y_train)<br/>y_pred = clf.predict(X_test_pca)</span><span id="9888" class="ms kc hi mo b fi mx mu l mv mw">print("Accuracy score:{:.2f}".format(metrics.accuracy_score(y_test, y_pred)))</span></pre><blockquote class="my"><p id="41f4" class="mz na hi bd nb nc nd ne nf ng nh jc dx translated">结果:</p><p id="d7d6" class="mz na hi bd nb nc nu nv nw nx ny jc dx translated">准确度得分:0.95</p></blockquote><p id="fa69" class="pw-post-body-paragraph if ig hi ih b ii ni ik il im nj io ip iq nk is it iu nl iw ix iy nm ja jb jc hb bi translated">令人印象深刻的是我们得到了0.95的精确度。现在，我将让您通过使用PCA进行降维来尝试做同样的事情。这意味着加载数据，将其分成训练和测试，然后使用LogisticRegression。</p><p id="3e1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结果将是相同的精度0.95，但是模型完成的时间将会长得多。</p><p id="baa9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是主要目标，在不影响精度的情况下减少尺寸并保留重要特征。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="4e0e" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated"><strong class="ak">特征提取</strong></h1><p id="7351" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">假设我们有十个独立变量。在特征提取中，我们创建十个“新”独立变量，其中每个“新”独立变量是十个“旧”独立变量的组合。然而，我们以特定的方式创建这些新的自变量，并根据它们预测因变量的程度对这些新变量进行排序。</p><blockquote class="jm jn jo"><p id="9efa" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated">你可以从<a class="ae jt" href="https://www.kaggle.com/mohammadhatoum/unsupervised-learning-feature-extraction?scriptVersionId=23776981" rel="noopener ugc nofollow" target="_blank">这里</a>下载完整的Kaggle笔记本</p></blockquote><p id="ddd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我使用了这本神奇的书中的例子:<a class="ae jt" href="http://shop.oreilly.com/product/0636920034919.do" rel="noopener ugc nofollow" target="_blank"> Python数据科学手册</a>。<br/>实现也是由<a class="ae jt" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.14-image-features.html" rel="noopener ugc nofollow" target="_blank"> Jake VanderPlas </a>完成的。</p><p id="fcae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将执行以下操作:</p><ol class=""><li id="c746" class="lz ma hi ih b ii ij im in iq mb iu mc iy md jc me mf mg mh bi translated">获取一组人脸的图像缩略图，构成“正”训练样本。</li><li id="ba91" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">获取一组非人脸的图像缩略图，构成“负”训练样本。</li><li id="7b6b" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">从这些训练样本中提取[Oriented Gradients(HOG)](<a class="ae jt" href="https://en.wikipedia.org/wiki/Histogram_of_oriented_gradients" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Histogram _ of _ Oriented _ Gradients</a>)特征的直方图。</li><li id="8aff" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">在这些样本上训练线性SVM分类器。</li><li id="8c35" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">对于“未知”的图像，在图像上通过一个滑动窗口，使用该模型来评估该窗口是否包含人脸。</li><li id="207a" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">如果检测重叠，将它们合并到一个窗口中。</li></ol><p id="151a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 1。获取正面图像:</strong>我们将使用野生<a class="ae jt" href="http://vis-www.cs.umass.edu/lfw/" rel="noopener ugc nofollow" target="_blank"> (LFW </a>)数据集中的标记人脸。这是从互联网上收集的名人的JPEG图片集，所有细节可在官方网站上找到:</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="21bb" class="ms kc hi mo b fi mt mu l mv mw">%matplotlib inline<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns; sns.set()<br/>import numpy as np<br/>from sklearn.datasets import fetch_lfw_people<br/>faces = fetch_lfw_people()<br/>positive_patches = faces.images<br/>print(f"Shape of positive data {positive_patches.shape}")</span></pre><blockquote class="my"><p id="194b" class="mz na hi bd nb nc nd ne nf ng nh jc dx translated">结果:</p><p id="dfb0" class="mz na hi bd nb nc nu nv nw nx ny jc dx translated">正数据的形状(13233，62，47)</p></blockquote><p id="fab9" class="pw-post-body-paragraph if ig hi ih b ii ni ik il im nj io ip iq nk is it iu nl iw ix iy nm ja jb jc hb bi translated">所以我们有超过13000张图片。让我们绘制第一幅图像</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="5d1a" class="ms kc hi mo b fi mt mu l mv mw">from skimage.io import imshow<br/>imshow(faces.images[0])</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es od"><img src="../Images/e156682acc5f6e4916b4d9b5545895b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*mIdFMcHz7_0WUyiqFZFfNA.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">阳性图像样本</figcaption></figure><p id="8707" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。获取负面图像:</strong>现在我们将准备一些没有人脸的图像。我们将想象并使用10个类别(“相机”、“文本”、“硬币”、“月亮”、“页面”、“时钟”、“免疫组织化学”、“切尔西”、“咖啡”、“哈勃深度场”)</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="13eb" class="ms kc hi mo b fi mt mu l mv mw">from skimage import data, transform,color, feature</span><span id="6b89" class="ms kc hi mo b fi mx mu l mv mw">imgs_to_use = ['camera', 'text', 'coins', 'moon',<br/>               'page', 'clock', 'immunohistochemistry',<br/>               'chelsea', 'coffee', 'hubble_deep_field']<br/>images = [color.rgb2gray(getattr(data, name)())<br/>          for name in imgs_to_use]</span></pre><p id="17d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们把它们和它们的标签一起标出来</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="3fd9" class="ms kc hi mo b fi mt mu l mv mw">for i,im in enumerate(images):<br/>    print(imgs_to_use[i])<br/>    imshow(im) <br/>    plt.show()</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es oe"><img src="../Images/64cd4d4ad09d5d139b32eae4d58a3cd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*P5J5Daaa0ws4Mla-_uwXTg.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">负像样本</figcaption></figure><p id="ad90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将使用<a class="ae jt" href="https://scikit-learn.org/stable/modules/feature_extraction.html#image-feature-extraction" rel="noopener ugc nofollow" target="_blank"> PatchExtractor </a>，它是图像的特征提取器。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="827b" class="ms kc hi mo b fi mt mu l mv mw">from sklearn.feature_extraction.image import PatchExtractor</span><span id="f6d8" class="ms kc hi mo b fi mx mu l mv mw">def extract_patches(img, N, scale=1.0, patch_size=positive_patches[0].shape):<br/>    extracted_patch_size = tuple((scale * np.array(patch_size)).astype(int))<br/>    extractor = PatchExtractor(patch_size=extracted_patch_size,<br/>                               max_patches=N, random_state=0)<br/>    patches = extractor.transform(img[np.newaxis])<br/>    if scale != 1:<br/>        patches = np.array([transform.resize(patch, patch_size)<br/>                            for patch in patches])<br/>    return patches</span><span id="db6b" class="ms kc hi mo b fi mx mu l mv mw">negative_patches = np.vstack([extract_patches(im, 1000, scale)<br/>                              for im in images for scale in [0.5, 1.0, 2.0]])<br/>print(f"Shape of negative data {negative_patches.shape}")</span></pre><blockquote class="my"><p id="9fb8" class="mz na hi bd nb nc nd ne nf ng nh jc dx translated">结果:</p><p id="c922" class="mz na hi bd nb nc nu nv nw nx ny jc dx translated">负数据的形状(30000，62，47)</p></blockquote><p id="07b9" class="pw-post-body-paragraph if ig hi ih b ii ni ik il im nj io ip iq nk is it iu nl iw ix iy nm ja jb jc hb bi translated">我们有30，000个不包含人脸的图像块。我们将绘制几个:</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="51f6" class="ms kc hi mo b fi mt mu l mv mw">fig, ax = plt.subplots(6, 10)<br/>for i, axi in enumerate(ax.flat):<br/>    axi.imshow(negative_patches[500 * i], cmap='gray')<br/>    axi.axis('off')</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es of"><img src="../Images/7dde88527d3df0bb62996ab475173adb.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*gVbq55nDXGx59CARQrafEg.png"/></div></figure><p id="a33e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。合并图像并应用HOG: </strong>现在将合并正面和负面图像。然后对他们使用猪。我们还将为有脸的图像添加标签1，为没有脸的图像添加标签0。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="8c19" class="ms kc hi mo b fi mt mu l mv mw">from itertools import chain<br/>X_train = np.array([feature.hog(im)<br/>                    for im in chain(positive_patches,<br/>                                    negative_patches)])<br/>y_train = np.zeros(X_train.shape[0])<br/>y_train[:positive_patches.shape[0]] = 1<br/>print(f"Shape after combining the images: {X_train.shape}")</span></pre><blockquote class="my"><p id="908b" class="mz na hi bd nb nc nd ne nf ng nh jc dx translated">结果:组合图像后的形状:(43233，1215)</p></blockquote><p id="bc85" class="pw-post-body-paragraph if ig hi ih b ii ni ik il im nj io ip iq nk is it iu nl iw ix iy nm ja jb jc hb bi translated"><strong class="ih hj"> 4。训练一个支持向量机:</strong>我们将线性SVC。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="1c15" class="ms kc hi mo b fi mt mu l mv mw">from sklearn.svm import LinearSVC</span><span id="0220" class="ms kc hi mo b fi mx mu l mv mw">model = LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,<br/>          intercept_scaling=1, loss='squared_hinge', max_iter=1000,<br/>          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,<br/>          verbose=0)<br/>model.fit(X_train, y_train)</span></pre><p id="e25c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 5。获取新图像:我们将从skimage中获取宇航员图像。</strong></p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="d6d1" class="ms kc hi mo b fi mt mu l mv mw">import skimage.data<br/>test_image = skimage.data.astronaut()<br/>test_image = skimage.color.rgb2gray(test_image)<br/>test_image = skimage.transform.rescale(test_image, 0.5)<br/>test_image = test_image[:160, 40:180]</span><span id="c0dd" class="ms kc hi mo b fi mx mu l mv mw">plt.imshow(test_image, cmap='gray')<br/>plt.axis('off');</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es og"><img src="../Images/b3f093c6e49239cc80fd37b49a178b1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*5_V_D8eTOYwSRTiVDEf0wQ.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">宇航员</figcaption></figure><p id="2ede" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jp">接下来，让我们创建一个窗口，迭代这个图像的小块，并计算每个小块的HOG特征:</em></p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="175f" class="ms kc hi mo b fi mt mu l mv mw">def sliding_window(img, patch_size=positive_patches[0].shape,<br/>                   istep=2, jstep=2, scale=1.0):<br/>    Ni, Nj = (int(scale * s) for s in patch_size)<br/>    for i in range(0, img.shape[0] - Ni, istep):<br/>        for j in range(0, img.shape[1] - Ni, jstep):<br/>            patch = img[i:i + Ni, j:j + Nj]<br/>            if scale != 1:<br/>                patch = transform.resize(patch, patch_size)<br/>            yield (i, j), patch<br/>            <br/>indices, patches = zip(*sliding_window(test_image))<br/>patches_hog = np.array([feature.hog(patch) for patch in patches])<br/>print(f"Patches Hog shape: {patches_hog.shape}")</span></pre><blockquote class="my"><p id="e1b8" class="mz na hi bd nb nc nd ne nf ng nh jc dx translated">结果:补丁猪形状:(1911，1215)</p></blockquote><p id="3043" class="pw-post-body-paragraph if ig hi ih b ii ni ik il im nj io ip iq nk is it iu nl iw ix iy nm ja jb jc hb bi translated"><em class="jp">最后，我们可以使用这些具有猪特征的面片，并使用我们的模型来评估每个面片是否包含人脸:</em></p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="1e72" class="ms kc hi mo b fi mt mu l mv mw">labels = model.predict(patches_hog)<br/>print(f"labels: {labels.sum()}")</span></pre><blockquote class="my"><p id="4307" class="mz na hi bd nb nc nd ne nf ng nh jc dx translated">结果:标签:49.0</p></blockquote><p id="03e9" class="pw-post-body-paragraph if ig hi ih b ii ni ik il im nj io ip iq nk is it iu nl iw ix iy nm ja jb jc hb bi translated"><strong class="ih hj"> 6。人脸检测:</strong> <em class="jp">我们看到在近2000个补丁中，我们找到了49个检测。让我们使用我们所拥有的关于这些小块的信息来显示它们在我们的测试图像上的位置，把它们画成矩形:</em></p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="0589" class="ms kc hi mo b fi mt mu l mv mw">fig, ax = plt.subplots()<br/>ax.imshow(test_image, cmap='gray')<br/>ax.axis('off')</span><span id="a266" class="ms kc hi mo b fi mx mu l mv mw">Ni, Nj = positive_patches[0].shape<br/>indices = np.array(indices)</span><span id="0605" class="ms kc hi mo b fi mx mu l mv mw">for i, j in indices[labels == 1]:<br/>    ax.add_patch(plt.Rectangle((j, i), Nj, Ni, edgecolor='red',<br/>                               alpha=0.3, lw=2, facecolor='none'))</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es oh"><img src="../Images/bd39b41f3ca12c5e68b1b72e3d31bd7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*e0bKHEVbrJEhV6H9dviO3A.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">人脸检测结果</figcaption></figure><p id="0037" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如你所见，使用特征提取和SVC，我们能够创建一个人脸检测模型。</p><h1 id="30d9" class="kb kc hi bd kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku ly kw kx ky bi translated">过度拟合与欠拟合</h1><p id="c8f7" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">考虑我们有两组数据。训练和测试设备。我们定义如下:</p><p id="31ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">拟合不足:</strong>这是指模型未能捕捉到数据中的重要区别和模式，因此即使在训练数据中，它的表现也很差。当模型不够复杂，无法捕捉数据中的潜在趋势时，就会出现这种情况。</p><p id="603b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">过拟合:</strong>是指模型与训练数据几乎完美匹配，但在测试和其他新数据上表现不佳。我们说它不能很好地概括看不见的数据。</p><p id="80d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解决欠拟合:</strong></p><p id="6b73" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可以通过使用更多数据来避免欠拟合，也可以通过特征选择来减少特征，或者简单地继续前进并尝试替代的机器学习算法</p><p id="da5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解决过拟合:</strong></p><p id="67a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">之前我们使用<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split" rel="noopener ugc nofollow" target="_blank"><em class="jp">train _ test _ split</em></a><em class="jp"/>创建<em class="jp">一个验证(维持)</em>集合<em class="jp">时提供了一个解决方案。</em>但这样做的缺点是，我们在模型训练中丢失了一部分数据。例如，如果我们使用验证大小0.3，这意味着我们保留了30%的训练数据。这有些运气，因为我们可能在训练时错过了一些重要的或有效的或新的数据模式。</p><blockquote class="jm jn jo"><p id="7dc0" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><em class="hi">解决这个问题的方法是一个叫做</em> <a class="ae jt" href="https://scikit-learn.org/stable/modules/cross_validation.html" rel="noopener ugc nofollow" target="_blank"> <em class="hi">交叉验证</em> </a> <em class="hi">(简称CV)的过程。测试集仍然应该被保留用于最终评估，但是在做CV时不再需要验证集。<br/>在称为k-fold CV的基本方法中，训练集被分成k个更小的集合(其他方法在下面描述，但通常遵循相同的原则)。</em></p><p id="5a26" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><em class="hi">k个“折叠”中的每一个都遵循以下程序:</em></p><p id="a334" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><em class="hi"> 1)使用k-1个褶皱作为训练数据来训练模型</em></p><p id="218f" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><em class="hi"> 2)生成的模型在数据的剩余部分进行验证(即，它被用作计算性能度量(如准确性)的测试集)。</em></p></blockquote><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es oi"><img src="../Images/c4991f176f33145eec40e550b8f02f79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*QS-iqCLAS4mkdvn9GziQUQ.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/cross_validation.html" rel="noopener ugc nofollow" target="_blank"> <em class="oj"> k </em>【褶皱】</a></figcaption></figure><p id="a592" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了评估交叉验证的结果，我们可以使用<a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" rel="noopener ugc nofollow" target="_blank"> cross_val_score </a>。这很容易使用，我们只需将cv参数设置为折叠次数。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="e7f0" class="ms kc hi mo b fi mt mu l mv mw">from sklearn.cross_validation import cross_val_score<br/>cross_val_score(model, X, y, cv=5)</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es ok"><img src="../Images/141fbe9158d129c085ff457897f84605.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C4dD5w-U2E0zZrmJQs_2bA.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">来源:<a class="ae jt" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a></figcaption></figure><h1 id="9fdc" class="kb kc hi bd kd ke lu kg kh ki lv kk kl km lw ko kp kq lx ks kt ku ly kw kx ky bi translated">偏差-方差权衡</h1><p id="fbac" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated"><strong class="ih hj">偏差:</strong>是用简化模型近似真实世界现象引入的误差量。这是训练集误差和人类水平误差或任何其他最佳误差或基本误差之间的差异。</p><p id="a2ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">方差:</strong>是模型的测试误差根据训练数据的变化而变化的程度。它反映了模型对其接受训练的数据集特性的敏感性。</p><p id="0c4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随着模型复杂性的增加，它变得更加灵活，它的偏差会减少(它在解释训练数据方面做得很好)，但方差会增加(它也不会一般化)。</p><p id="d9a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从技术上讲，<strong class="ih hj">欠拟合</strong>发生在<strong class="ih hj">偏高</strong>的时候。<strong class="ih hj">过度拟合</strong>发生在我们有<strong class="ih hj">高方差的时候。</strong></p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es ol"><img src="../Images/9ed648f67cc313b90259d994a251643d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zoBVkuudD-o2Q2vDlxgXmQ.png"/></div></div><figcaption class="lq lr et er es ls lt bd b be z dx translated"><a class="ae jt" href="https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/05.03-bias-variance.png" rel="noopener ugc nofollow" target="_blank">未拟合(高偏差)与过度拟合(高方差)</a></figcaption></figure><p id="7efe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最终，为了有一个好的模型，你需要一个具有<strong class="ih hj">低偏差</strong>和<strong class="ih hj">低方差</strong>的模型。这意味着我们需要找到最佳的超参数。我们已经在本系列的第二部分中提到过这一点，在那里我们使用了<a class="ae jt" href="https://scikit-learn.org/stable/modules/grid_search.html#grid-search" rel="noopener ugc nofollow" target="_blank">GridSearchCV</a>T19】。</p><pre class="lf lg lh li fd mn mo mp mq aw mr bi"><span id="2f93" class="ms kc hi mo b fi mt mu l mv mw"># Establish a model<br/>model = SGDRegressor(learning_rate='optimal',penalty='l2')<br/>from sklearn.model_selection import GridSearchCV<br/># Grid search - this will take about 1 minute.<br/>param_grid = {<br/>    'alpha': 10.0 ** -np.arange(1, 7),<br/>    'loss': ['squared_loss', 'huber', 'epsilon_insensitive'],<br/>}<br/>clf = GridSearchCV(model, param_grid)<br/>clf.fit(X_train, y_train)<br/>print(f"Best Score: {round(clf.best_score_,3)}" )<br/>print(f"Best Estimator: {clf.best_estimator_}" )<br/>print(f"Best Params: {clf.best_params_}" )</span></pre><blockquote class="jm jn jo"><p id="8eae" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated">但是，有时绘制单个超参数对训练分数和验证分数的影响有助于发现估计量对于某些超参数值是过拟合还是欠拟合。<br/>在这种情况下，函数<code class="du om on oo mo b"><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.validation_curve.html#sklearn.model_selection.validation_curve" rel="noopener ugc nofollow" target="_blank">validation_curve</a></code>会有所帮助。</p></blockquote><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nt"><img src="../Images/6d83752cbe17e32a9fa45e47c0848f20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*CzQyinyBpHN7MDMe-ptvig.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">来源:<a class="ae jt" href="https://scikit-learn.org/stable/modules/learning_curve.html#learning-curve" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a></figcaption></figure><p id="307d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还可以使用<a class="ae jt" href="https://scikit-learn.org/stable/modules/learning_curve.html#learning-curve" rel="noopener ugc nofollow" target="_blank">学习曲线</a>来查看添加更多的训练数据是否会提高我们模型的性能。我们还可以检查它是否存在偏差和方差。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nt"><img src="../Images/b8972b10b08bb9641c363ef2cf60944d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Gv07Sm4CsUPl1QXRtwrq5g.png"/></div><figcaption class="lq lr et er es ls lt bd b be z dx translated">来源:<a class="ae jt" href="https://scikit-learn.org/stable/modules/learning_curve.html#learning-curve" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a></figcaption></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="05fd" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">概述</h1><p id="dc8e" class="pw-post-body-paragraph if ig hi ih b ii kz ik il im la io ip iq lb is it iu lc iw ix iy ld ja jb jc hb bi translated">我们已经到了本系列第3部分的结尾。在这一部分，我们能够了解到:</p><ol class=""><li id="3296" class="lz ma hi ih b ii ij im in iq mb iu mc iy md jc me mf mg mh bi translated">基于KMeans的无监督学习聚类</li><li id="dde9" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">降维利用主成分分析(PCA)进行降维(特征消除)</li><li id="1b53" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">特征提取，并用它来创建一个人脸检测器</li><li id="462a" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">过度拟合与欠拟合</li><li id="73ea" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">验证和交叉验证</li><li id="2770" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">偏差-方差权衡</li><li id="78ca" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated">验证曲线和学习曲线</li></ol><p id="5622" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本教程的前几部分(1、2和3)，我们讨论了有监督和无监督机器学习的不同方面。在我们教程的第4部分，我们将从<strong class="ih hj">深度学习开始。</strong></p><p id="7787" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读！</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="cb30" class="kb kc hi bd kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky bi translated">参考链接:</h1><ol class=""><li id="978e" class="lz ma hi ih b ii kz im la iq op iu oq iy or jc me mf mg mh bi translated"><a class="ae jt" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-part-1-4ba7ce8cf7eb">https://medium . com/analytics-vid hya/machine-learning-deep-learning-guide-part-1-4 ba 7 ce 8 cf 7 EB</a></li><li id="fb8d" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" rel="noopener" href="/analytics-vidhya/machine-learning-deep-learning-guide-db520c4797da">https://medium . com/analytics-vid hya/machine-learning-deep-learning-guide-db 520 c 4797 da</a></li><li id="320d" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://github.com/jakevdp/PythonDataScienceHandbook/tree/master/notebooks" rel="noopener ugc nofollow" target="_blank">https://github . com/jakevdp/python datascience handbook/tree/master/notebooks</a></li><li id="769a" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/clustering.html#k-means" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/clustering . html # k-means</a></li><li id="f0a5" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits" rel="noopener ugc nofollow" target="_blank">https://archive . ics . UCI . edu/ml/datasets/Optical+Recognition+of+手写+数字</a></li><li id="1618" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://www.kaggle.com/imrandude/olivetti" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/imrandude/olivetti</a></li><li id="0052" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://www.kaggle.com/serkanpeldek/face-recognition-on-olivetti-dataset" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/serkanpeldek/face-recognition-on-olivetti-dataset</a></li><li id="e4a6" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html" rel="noopener ugc nofollow" target="_blank">www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html</a></li><li id="8a6d" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://hackernoon.com/supervised-machine-learning-linear-regression-in-python-541a5d8141ce" rel="noopener ugc nofollow" target="_blank">https://hacker noon . com/supervised-machine-learning-linear-regression-in-python-541 a5d 8141 ce</a></li><li id="be4b" class="lz ma hi ih b ii mi im mj iq mk iu ml iy mm jc me mf mg mh bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/cross_validation.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/cross _ validation . html</a></li></ol></div></div>    
</body>
</html>