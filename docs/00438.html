<html>
<head>
<title>Generating meaningful Phrases from unstructured news data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从非结构化新闻数据中生成有意义的短语</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/generating-meaningful-phrases-from-unstructured-news-data-d4e217a7da43?source=collection_archive---------1-----------------------#2019-06-17">https://medium.com/analytics-vidhya/generating-meaningful-phrases-from-unstructured-news-data-d4e217a7da43?source=collection_archive---------1-----------------------#2019-06-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="8e28" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">对语法算法的深入研究</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/0659a8d77c56a9fcd302bc78911c30d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/0*F9vg_DuxM1ItiFiB.jpg"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">有效短语？</figcaption></figure><p id="4088" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">短语是一个多单词表达式或单词n元语法，它表达有效的意思，在索引文档、总结、推荐等方面非常有用。</p><p id="7bfd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在这篇文章中，我将解释一个非常高精度的并行短语模块，它仅从10000篇非结构化新闻文章中获得超过95%的精度。</p><blockquote class="kf kg kh"><p id="0c70" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">将涵盖如何生成有效的二元和三元短语。</p><p id="0e7f" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">你可能想知道有类似gensim等的<a class="ae km" href="https://radimrehurek.com/gensim/models/phrases.html" rel="noopener ugc nofollow" target="_blank">短语</a>模块。</p></blockquote><p id="28f4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">gensim生成的短语不达标，可能需要庞大的语料库来生成基于搭配的短语。(我猜大约100万条新闻)</p><h2 id="599b" class="kn ko hi bd kp kq kr ks kt ku kv kw kx js ky kz la jw lb lc ld ka le lf lg lh bi translated">生成有意义短语的步骤</h2><ol class=""><li id="3c00" class="li lj hi jl b jm lk jp ll js lm jw ln ka lo ke lp lq lr ls bi translated">预处理和文本规范化(有很多关于文本预处理的文章，所以这里就不赘述了。例子<a class="ae km" href="https://stackoverflow.com/questions/45605946/how-to-do-text-pre-processing-using-spacy" rel="noopener ugc nofollow" target="_blank">链接</a>。)</li><li id="23b7" class="li lj hi jl b jm lt jp lu js lv jw lw ka lx ke lp lq lr ls bi translated">从新闻文章中构建n个grams (1 (uni)，2(bi)，3(tri))(CPU并行)(稍后将讨论Spark版本)</li><li id="72ec" class="li lj hi jl b jm lt jp lu js lv jw lw ka lx ke lp lq lr ls bi translated">训练模型</li><li id="7071" class="li lj hi jl b jm lt jp lu js lv jw lw ka lx ke lp lq lr ls bi translated">提取有效短语</li></ol><blockquote class="kf kg kh"><p id="0562" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">核心语法算法与设计</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="er es ly"><img src="../Images/66776e6a3544b25c006e12fdfcb5e969.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k18vqBour2vrOoATWJmweQ.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">语法算法设计(非火花版本)</figcaption></figure><h2 id="615d" class="kn ko hi bd kp kq kr ks kt ku kv kw kx js ky kz la jw lb lc ld ka le lf lg lh bi translated">初始化空间和文本规范化</h2><p id="440d" class="pw-post-body-paragraph jj jk hi jl b jm lk ij jo jp ll im jr js md ju jv jw me jy jz ka mf kc kd ke hb bi translated">首先让我们规范化文本(原始文本字符串)</p><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="ba3e" class="kn ko hi mh b fi ml mm l mn mo"><strong class="mh hj">#initlize spacy in your class init</strong></span><span id="8316" class="kn ko hi mh b fi mp mm l mn mo">self.nlp = spacy.load(lang)</span><span id="71d5" class="kn ko hi mh b fi mp mm l mn mo"><strong class="mh hj">#also need cpu count for parallelism  </strong><br/>partitions = cpu_count()</span><span id="ee33" class="kn ko hi mh b fi mp mm l mn mo">normalized_text = self.normalize_text(doc)<br/>doc = self.nlp(normalized_text)</span></pre><h2 id="a092" class="kn ko hi bd kp kq kr ks kt ku kv kw kx js ky kz la jw lb lc ld ka le lf lg lh bi translated">并行构建n克</h2><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="b51d" class="kn ko hi mh b fi ml mm l mn mo">uni_grams = self.parallel_build_n_grams(doc, 1)<br/>bi_grams = self.parallel_build_n_grams(doc, 2)<br/>tri_grams = self.parallel_build_n_grams(doc, 3)</span></pre><blockquote class="kf kg kh"><p id="37fa" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><em class="hi">生成n克并将</em> <a class="ae km" href="https://spacy.io/api/span" rel="noopener ugc nofollow" target="_blank"> <em class="hi">跨度</em> </a> <em class="hi">拆分成若干个分区(doc是nlp doc) </em></p></blockquote><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="ac88" class="kn ko hi mh b fi ml mm l mn mo">def generate_ngrams(self, doc, n_gram):<br/>    return [doc[i:i + n_gram] for i in range(len(doc) - n_gram + 1)]</span></pre><blockquote class="kf kg kh"><p id="0b16" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">使分离</p></blockquote><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="be73" class="kn ko hi mh b fi ml mm l mn mo">splits = np.array_split(self.generate_ngrams(doc, n_gram), partitions)</span></pre><blockquote class="kf kg kh"><p id="0dc2" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">对于每个拆分构建，n个grams也有附加的pos标签用于以后的分析。看看python <a class="ae km" href="https://docs.python.org/2/library/multiprocessing.html" rel="noopener ugc nofollow" target="_blank">多处理</a>来了解一下<a class="ae km" href="https://docs.python.org/2/library/multiprocessing.html#the-process-class" rel="noopener ugc nofollow" target="_blank">进程</a>。</p></blockquote><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="d719" class="kn ko hi mh b fi ml mm l mn mo">for s in splits:<br/>    q = Queue()<br/>    p = Process(target=self.build_n_grams_from_span_list, args=(s, q))</span></pre><blockquote class="kf kg kh"><p id="2567" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">Ngram从span开始构建，并附加了pos标签，我们将只获取文本的<a class="ae km" href="https://spacy.io/api/lemmatizer" rel="noopener ugc nofollow" target="_blank">引理</a></p></blockquote><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="ef97" class="kn ko hi mh b fi ml mm l mn mo">def build_n_grams_from_span_list(self, sp, q):<br/>    ph_pos = []<br/>    for s in sp:<br/>        ngram = []<br/>        pos_tag = []<br/>        for p in s:<br/>            ngram.append(p.lemma_)<br/>            pos_tag.append(p.pos_)<br/>        phrase = " ".join(ngram)<br/>        pos = "_".join(pos_tag)<br/><br/>        ph_pos.append((phrase, pos))<br/><br/>    q.put(ph_pos)</span></pre><blockquote class="kf kg kh"><p id="899e" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">这是并行构建n元语法的完整代码</p></blockquote><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="9310" class="kn ko hi mh b fi ml mm l mn mo">def parallel_build_n_grams(self, doc, n_gram):<br/>    print('function name: parallel_build_n_grams')<br/>    ngrams = []<br/><br/>    splits = np.array_split(self.generate_ngrams(doc, n_gram), partitions)<br/>    ps = []<br/>    qs = []<br/>    for s in splits:<br/>        q = Queue()<br/>        p = Process(target=self.build_n_grams_from_span_list, args=(s, q))<br/>        p.start()<br/>        ps.append(p)<br/>        qs.append(q)<br/><br/>    rqs = [q.get() for q in qs]<br/><br/>    for q in rqs:<br/>        for ph, po in q:<br/>            ngrams.append((ph, po))<br/><br/>    for p in ps:<br/>        p.join()<br/><br/>    return ngrams</span></pre><blockquote class="kf kg kh"><p id="7733" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">下面是它在<a class="ae km" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" rel="noopener ugc nofollow" target="_blank">数据帧</a>中的样子。应该有三个主<a class="ae km" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" rel="noopener ugc nofollow" target="_blank">数据帧</a>需要与每个新闻文章生成的新数据帧合并。</p></blockquote><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="0c39" class="kn ko hi mh b fi ml mm l mn mo">#for bi grams we need c21, cw2, cw12 (will explain later what these means and why required)</span><span id="25e3" class="kn ko hi mh b fi mp mm l mn mo">#3 master <a class="ae km" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" rel="noopener ugc nofollow" target="_blank">dataframe</a> merge the ngrams reqturned from above with unigram <a class="ae km" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" rel="noopener ugc nofollow" target="_blank">dataframe</a> bigram data trigram data frame</span><span id="b5c1" class="kn ko hi mh b fi mp mm l mn mo">self.uni_gram_df = pd.DataFrame(columns=['pos_tag', 'freq'])<br/>self.bi_gram_df = pd.DataFrame(columns=['pos_tag', 'freq', 'pmi', 'cw1w2', 'cw1', 'cw2'])<br/>self.tri_gram_df = pd.DataFrame(columns=['pos_tag', 'freq', 'pmi', 'cw1w2w3', 'cw1w2', 'cw1', 'cw2', 'cw3'])</span></pre><blockquote class="kf kg kh"><p id="0855" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><a class="ae km" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank"> n-gram </a> <a class="ae km" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" rel="noopener ugc nofollow" target="_blank"> dataframe </a>输出应该是什么样子。请注意，随着越来越多的消息传来，这些<a class="ae km" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" rel="noopener ugc nofollow" target="_blank">数据帧</a>将继续增长。稍后我会分享一些统计数据，这就是为什么我们需要<a class="ae km" href="https://www.coursera.org/lecture/scala-spark-big-data/data-parallel-to-distributed-data-parallel-SWOCr" rel="noopener ugc nofollow" target="_blank"> spark来实现数据并行</a>。</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="er es mq"><img src="../Images/08dcf7a832544bfd6acb966b4ad84ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kCAKsuPJFslZHcHbj4u2eA.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">构建n元语法函数输出</figcaption></figure><h2 id="37f6" class="kn ko hi bd kp kq kr ks kt ku kv kw kx js ky kz la jw lb lc ld ka le lf lg lh bi translated">保留n-gram</h2><p id="22a4" class="pw-post-body-paragraph jj jk hi jl b jm lk ij jo jp ll im jr js md ju jv jw me jy jz ka mf kc kd ke hb bi translated">我们需要为二元模型和三元模型重新训练系统，在构建和合并二元模型时，二元模型的频率会更新。</p><blockquote class="kf kg kh"><p id="5c1e" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">让我们来看看再训练二元模型，人们可以很容易地概括为再训练三元模型。首先，我们需要将主数据帧分割成多个分区，对于每个分区，我们需要调用retain，最后进行合并。(<a class="ae km" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html" rel="noopener ugc nofollow" target="_blank">串联</a>)</p></blockquote><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="e396" class="kn ko hi mh b fi ml mm l mn mo">splits = np.array_split(self.dict_n_gram_df[2], partitions)<br/>ps = []<br/>qs = []<br/>for s in splits:<br/>    q = Queue()<br/>    p = Process(target=self.retrain_2_grams, args=(self.dict_n_gram_df[1], s, q))<br/>    p.start()<br/>    ps.append(p)<br/>    qs.append(q)<br/><br/>dfs = [q.get() for q in qs]<br/>df = pd.concat(dfs)</span><span id="60ca" class="kn ko hi mh b fi mp mm l mn mo">for p in ps:<br/>    p.join()</span></pre><h2 id="9753" class="kn ko hi bd kp kq kr ks kt ku kv kw kx js ky kz la jw lb lc ld ka le lf lg lh bi translated">如何生成高精度短语(<a class="ae km" href="https://en.wikipedia.org/wiki/Bigram" rel="noopener ugc nofollow" target="_blank">二元模型</a>，三元模型)</h2><p id="c261" class="pw-post-body-paragraph jj jk hi jl b jm lk ij jo jp ll im jr js md ju jv jw me jy jz ka mf kc kd ke hb bi translated">让我们了解如何生成高精度二元模型和三元模型，以及这个重训练代码做什么。</p><p id="c270" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">简而言之，我们真正需要知道的是一个二元模型word1(w1) word2(w2)是否是一个有效的二元模型。</p><blockquote class="kf kg kh"><p id="a3b3" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">例如，下面是一些有效的二元模型。</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="er es mr"><img src="../Images/9942c0e5714a050c38832c78209f6dd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YPUo_ENCQZ3E4iGgAWJNDQ.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">有效二元模型</figcaption></figure><blockquote class="kf kg kh"><p id="b01f" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">例如，下面是一些无效的二元模型。注意一些二元模型实际上在P &lt; 0.05 More about this in the Core Idea section below.</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="er es ms"><img src="../Images/9b7cb626c43639f101d210463adf1412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LmuT04nOhNi9wzocCkT7lQ.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">invalid bigrams</figcaption></figure><blockquote class="kf kg kh"><p id="9b95" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">For example following are some of the valid trigrams.</p><p id="ffb7" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">For example following are some of the invalid trigrams.</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="er es mt"><img src="../Images/17e2455c37c432306c3d5eef30aa7df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WVqb26BOS3b4mYajUhqHsQ.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">invalid trigrams</figcaption></figure><h2 id="6a43" class="kn ko hi bd kp kq kr ks kt ku kv kw kx js ky kz la jw lb lc ld ka le lf lg lh bi translated"><strong class="ak">核心思想</strong>的显著性水平上是有效的</h2><p id="e044" class="pw-post-body-paragraph jj jk hi jl b jm lk ij jo jp ll im jr js md ju jv jw me jy jz ka mf kc kd ke hb bi translated">所以我们真的很想知道，在bi grams w1和w2的情况下，是纯粹偶然发生的，还是有效的共存。</p><p id="4dd6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">所以让我们有一个<a class="ae km" href="https://en.wikipedia.org/wiki/Null_hypothesis" rel="noopener ugc nofollow" target="_blank">零假设</a> H0，w1和w2之间没有联系，这纯粹是偶然发生的。</p><p id="d384" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">因此，在某种程度上，我们说w1和w2出现的概率纯粹是偶然的，这是我们的零假设，因此在计算二元模型P(w1w2)或三元模型P(w1w2w3)的概率后，我们可以在α= 0.005(95.95%置信区间)或α= 0.001(99.99%置信区间)的置信水平下拒绝零假设，否则我们保留零假设(即无效短语和同位纯粹是偶然的)。</p><p id="d06a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在高<a class="ae km" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">精度</a>系统的情况下，您可能希望选择0.01的显著性水平。</p><blockquote class="kf kg kh"><p id="df28" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">我们不会使用简单的P(w1w2w3) = P(w1)*P(w2)*P(w3)作为<a class="ae km" href="https://en.wikipedia.org/wiki/Collocation" rel="noopener ugc nofollow" target="_blank">协同定位</a>的概率，因为这不是对<a class="ae km" href="https://en.wikipedia.org/wiki/Collocation" rel="noopener ugc nofollow" target="_blank">协同定位</a>的准确估计，尽管我们将采用独立性作为零假设。</p></blockquote><h2 id="d286" class="kn ko hi bd kp kq kr ks kt ku kv kw kx js ky kz la jw lb lc ld ka le lf lg lh bi translated"><strong class="ak"> t测试</strong></h2><p id="3f35" class="pw-post-body-paragraph jj jk hi jl b jm lk ij jo jp ll im jr js md ju jv jw me jy jz ka mf kc kd ke hb bi translated">在下面链接中可以找到一些关于t统计的解释。</p><blockquote class="kf kg kh"><p id="4cc0" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><a class="ae km" href="https://en.wikipedia.org/wiki/T-statistic" rel="noopener ugc nofollow" target="_blank">维基百科t统计</a></p><p id="661e" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><a class="ae km" href="https://www.khanacademy.org/math/statistics-probability/significance-tests-one-sample/more-significance-testing-videos/v/z-statistics-vs-t-statistics?utm_account=Grant&amp;utm_campaignname=Grant_Math_Dynamic&amp;gclid=CjwKCAjw0ZfoBRB4EiwASUMdYQhfbtqo1HUZFW_INJpiOMcIfKz0NhinrCnYRsxnBMLcI6kyxvhMJxoC_1IQAvD_BwE" rel="noopener ugc nofollow" target="_blank">可汗学院</a></p></blockquote><p id="8590" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">t统计广泛用于<a class="ae km" href="https://en.wikipedia.org/wiki/Collocation" rel="noopener ugc nofollow" target="_blank">同位</a>检验。</p><blockquote class="kf kg kh"><p id="8c16" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">这是一本统计书的摘录</p></blockquote><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="er es mu"><img src="../Images/8d6b3ac8d05e31da9d19caba7e640077.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N9S9XaEElniOmTP1yZ8stQ.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">t统计</figcaption></figure><blockquote class="kf kg kh"><p id="78e1" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">让我们来看一个例子，二元模型<a class="ae km" href="https://en.wikipedia.org/wiki/Rajkumar_Hirani" rel="noopener ugc nofollow" target="_blank">拉库马·希拉尼</a>:</p></blockquote><p id="eadb" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">使用<a class="ae km" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">最大似然估计</a>我们可以如下计算rajkumar和hirani的概率。在我们的语料库rajkumar(cw1)和hirani (cw2)发生如下。<br/> cw1 97 <br/> cw2 209</p><p id="9e88" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">语料库的长度= 689768(这只是在3000篇新闻文章上)</p><blockquote class="kf kg kh"><p id="5e78" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">p(拉杰库马尔)= 97/689768</p><p id="800d" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">p(希拉尼)= 209/689768</p></blockquote><p id="09c8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">其产生P(w1)和P(w2)如下</p><blockquote class="kf kg kh"><p id="8e1d" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">P(w1)0.000140627<br/>P(w2)0.000303</p><p id="ae7b" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated"><strong class="jl hj">无效假设是拉杰库马和希拉尼的出现是独立的。</strong></p></blockquote><p id="c8c6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">H0: P(拉库马·希拉尼)= P(拉杰库马尔)*P(希拉尼)= 4.261e-08</p><p id="57cf" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果零假设为真，则随机生成单词的二元组并将1分配给结果<strong class="jl hj"><em class="ki"/></strong><em class="ki"/>并将0分配给任何其他结果的过程实际上是伯努利试验，对于<strong class="jl hj"> <em class="ki">【拉库马·希拉尼】</em> </strong> <em class="ki"> </em>出现的概率，p= 4.261e-08。该分布的均值为μ = 4.261e-08，方差为σ2 = p(1 p)，约为p，σ2 = p(1 p)的近似值成立，因为对于大多数二元模型来说，p很小。</p><p id="3518" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">拉库马·希拉尼在语料库中共出现89次。</p><blockquote class="kf kg kh"><p id="f715" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">cw1w2 = 89</p><p id="563b" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">因此，我们的样本均值为<em class="hi"> x = 89/ </em> 689768 = 0.000129029</p></blockquote><h2 id="2bd2" class="kn ko hi bd kp kq kr ks kt ku kv kw kx js ky kz la jw lb lc ld ka le lf lg lh bi translated">现在我们有了应用t检验所需的一切:</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mv"><img src="../Images/7d8fef5fbab9141bd364b58a8cb5ba8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*o0MA6ybfRwkWIbeQo6baqQ.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">测试统计</figcaption></figure><p id="fb50" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">t stat = 0.000129029–4.261 e-08/NP . sqrt(0.000129029/689768)</p><p id="4317" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi">= 9.43086968147722</p><p id="7141" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">α= 0.005或置信度为99.95%的临界值为2.576。所以我们可以拒绝零假设，因为9.43比那个大得多，我们可以得出结论，拉库马·希拉尼搭配不是纯粹随机的，是一个有效的二元模型。</p><blockquote class="kf kg kh"><p id="6ab1" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">下面是重新训练二元模型的代码</p></blockquote><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="14f5" class="kn ko hi mh b fi ml mm l mn mo">def update_2_gram_t_stats(self, df):<br/>    df2_len = len(df.index)<br/>    df['pw1'] = df['cw1']/df2_len<br/>    df['pw2'] = df['cw2']/df2_len<br/>    df['p(w1)p(w2)'] = df['pw1'] * df['pw2']<br/>    df['p(w1w2)'] = df['cw1w2']/df2_len<br/>    df['s2/n'] = np.sqrt(df['p(w1w2)']/df2_len)<br/><br/>    df['stat'] = (df['p(w1w2)'] - df['p(w1)p(w2)'])/df['s2/n']<br/><br/>    df['valid_01'] = False<br/>    df['valid_01'] = df['stat'] &gt; 3<br/><br/>    df['valid_05'] = False<br/>    df['valid_05'] = df['stat'] &gt; 2.6<br/>    return df</span></pre><blockquote class="kf kg kh"><p id="697b" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">希望你喜欢阅读关于如何使用搭配生成有效短语的文章，上面的实现有95%的准确率，在下一部分我们将把这个实现带到<a class="ae km" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Spark </a>。</p></blockquote><p id="cae7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这种实现方式比基于<a class="ae km" href="https://en.wikipedia.org/wiki/Pointwise_mutual_information" rel="noopener ugc nofollow" target="_blank"> PMI </a>的措辞方式要好很多，<a class="ae km" href="https://en.wikipedia.org/wiki/Pointwise_mutual_information" rel="noopener ugc nofollow" target="_blank"> PMI </a>没有给出有效短语的足够洞察力。您可以很容易地修改上面的代码，并实现一个pmi评分功能。</p><blockquote class="kf kg kh"><p id="81ba" class="jj jk ki jl b jm jn ij jo jp jq im jr kj jt ju jv kk jx jy jz kl kb kc kd ke hb bi translated">三元组代码的PMI分数</p></blockquote><pre class="iy iz ja jb fd mg mh mi mj aw mk bi"><span id="9647" class="kn ko hi mh b fi ml mm l mn mo">pmi_score = np.log(cw1w2w3 * N / (cw1w2*cw3))</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="lz ma di mb bf mc"><div class="er es mw"><img src="../Images/3b22244f2668e4df712a33f08a865099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rRQxNRc2z95oz8SX0_gCZA.png"/></div></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">这是短语云的一部分</figcaption></figure><p id="0a4e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">也可以看看我关于<a class="ae km" rel="noopener" href="/@iitr.samrat/demystify-wiki-ontology-and-knowledge-graph-part-1-ba919b0d9ce4?source=friends_link&amp;sk=c86df3f4bad504f9ef3f1f69697735e4">构建个人助理和知识图谱</a>的其他系列。</p><p id="141c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">谢谢..</p><p id="796d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">参考文献:</strong></p><p id="fe70" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><a class="ae km" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/N-gram</a></p><p id="2648" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">[1]<a class="ae km" href="https://radimrehurek.com/gensim/models/phrases.html" rel="noopener ugc nofollow" target="_blank">https://radimrehurek.com/gensim/models/phrases.html</a></p><p id="8af4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">[2]<a class="ae km" href="https://nlp.stanford.edu/fsnlp/promo/colloc.pdf" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/fsnlp/promo/colloc.pdf</a></p><p id="98d6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><a class="ae km" href="https://en.wikipedia.org/wiki/N-gram" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/N-gram</a></p><p id="95ee" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><a class="ae km" href="https://wiki.python.org/moin/ParallelProcessing" rel="noopener ugc nofollow" target="_blank">https://wiki.python.org/moin/ParallelProcessing</a></p></div></div>    
</body>
</html>