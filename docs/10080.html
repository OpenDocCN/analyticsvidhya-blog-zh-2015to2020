<html>
<head>
<title>SHAP (SHapley Additive exPlanations) And LIME (Local Interpretable Model-agnostic Explanations) for model explainability.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">SHAP (SHapley 附加解释)和 LIME(局部可解释模型不可知解释)用于模型可解释性。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/shap-shapley-additive-explanations-and-lime-local-interpretable-model-agnostic-explanations-8c0aa33e91f?source=collection_archive---------4-----------------------#2020-10-04">https://medium.com/analytics-vidhya/shap-shapley-additive-explanations-and-lime-local-interpretable-model-agnostic-explanations-8c0aa33e91f?source=collection_archive---------4-----------------------#2020-10-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/5e932ec926e86e2901db176af714300c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*CDaQV4oYPysWIV5VJNNo9w.png"/></div></figure><p id="ede9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为什么模型的可解释性如此重要？</p><p id="73cb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">模拟智能在期望精度、测量生存能力和评估熟练程度方面令人兴奋。无论如何，计算机通常不会解释它们的欲望。这转化为对人工智能模型聚集的限制。如果客户不信任某个型号或需求，他们就不会使用或发送它。因此，问题是帮助客户信任模型的方法。</p><p id="d058" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">虽然较简单类别的模型(如线性模型和决策树)通常容易被人类理解，但对于复杂模型(如集成方法、深度神经网络)来说，情况并非如此。这种复杂的模型实际上是黑盒。理解这种分类器行为的一种方法是建立更简单的解释模型，这些模型是这些黑盒的可解释近似。</p><p id="ccd6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为此，在现有文献中已经提出了几种技术。莱姆和 SHAP 是两种流行的模型不可知的局部解释方法，旨在解释任何给定的黑盒分类器。这些方法通过在每个预测周围局部地学习可解释的模型(例如，线性模型),以可解释的和忠实的方式解释任何分类器的各个预测。具体来说，莱姆和 SHAP 估计个体实例的特征属性，这捕获了每个特征对黑盒预测的贡献。下面，我们提供了这些方法的一些细节，同时也强调了它们之间的关系。</p><h2 id="b193" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated">什么是石灰？</h2><p id="eee3" class="pw-post-body-paragraph im in hi io b ip kf ir is it kg iv iw ix kh iz ja jb ki jd je jf kj jh ji jj hb bi translated">LIME(局部可解释模型不可知解释)是一种新颖的解释技术，它通过学习预测周围的局部可解释模型，以可解释和忠实的方式解释任何分类器的预测。</p><h2 id="1b66" class="jk jl hi bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated">什么是 SHAP？</h2><p id="492b" class="pw-post-body-paragraph im in hi io b ip kf ir is it kg iv iw ix kh iz ja jb ki jd je jf kj jh ji jj hb bi translated">SHapley 代表沙普利附加解释——很可能是机器学习合理性的前沿。Lundberg 和 Lee 在 2017 年首次发布了这种计算方法，这是一种计算任何有先见之明的计算结果的极好方法。</p><p id="14ce" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">无论你有一个令人难以置信的模型(可能是梯度推进、神经网络或任何将一些特征作为输入并产生一些预测作为输出的东西)，你都需要理解模型正在做出什么选择，SHAP 推崇备至。</p><p id="3afd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">利用酒店评论分类数据集，我们将建立一个多类文本分类模型，然后分别应用<em class="kk">石灰</em> &amp; <em class="kk"> SHAP </em>来解释该模型。因为我们以前已经做过多次文本分类，所以我们将快速构建 NLP 模型，并关注模型的可解释性。</p><p id="2cd2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae kl" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank"><em class="kk">LIME</em></a>&amp;<a class="ae kl" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"><em class="kk">SHAP</em></a>帮助我们不仅向最终用户，也向我们自己解释 NLP 模型是如何工作的。</p><h1 id="3147" class="km jl hi bd jm kn ko kp jq kq kr ks ju kt ku kv jx kw kx ky ka kz la lb kd lc bi translated">数据预处理、特征工程和逻辑回归:</h1><p id="60b6" class="pw-post-body-paragraph im in hi io b ip kf ir is it kg iv iw ix kh iz ja jb ki jd je jf kj jh ji jj hb bi translated">任何机器学习模型最重要的部分是从数据集中获得最佳效果</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="e9e6" class="jk jl hi li b fi lm ln l lo lp">import pandas as pd<br/>import numpy as np<br/>import sklearn<br/>import sklearn.ensemble<br/>import sklearn.metrics<br/>from sklearn.utils import shuffle</span><span id="f80e" class="jk jl hi li b fi lq ln l lo lp">import re</span><span id="0c0e" class="jk jl hi li b fi lq ln l lo lp">from nltk.corpus import stopwords<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score<br/>import lime<br/>from lime import lime_text<br/>from lime.lime_text import LimeTextExplainer<br/>from sklearn.pipeline import make_pipeline</span><span id="9356" class="jk jl hi li b fi lq ln l lo lp">df = pd.read_csv('hotel-reviews.csv')<br/>df.head()</span><span id="d9d1" class="jk jl hi li b fi lq ln l lo lp">df = df[pd.notnull(df['Browser_Used'])]<br/>df = df.sample(frac=0.5, random_state=99).reset_index(drop=True)<br/>df = shuffle(df, random_state=22)<br/>df = df.reset_index(drop=True)<br/>df['class_label'] = df['Browser_Used'].factorize()[0]<br/>class_label_df = df[['Browser_Used', 'class_label']].drop_duplicates().sort_values('class_label')<br/>label_to_id = dict(class_label_df.values)<br/>id_to_label = dict(class_label_df[['class_label', 'Browser_Used']].values)</span></pre><p id="4cec" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">文本清洗和预处理。为更好的模型解释生成干净的 CSV</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="115b" class="jk jl hi li b fi lm ln l lo lp">REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')<br/>BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')<br/>STOPWORDS = set(stopwords.words('english'))</span><span id="e5cf" class="jk jl hi li b fi lq ln l lo lp">def clean_text(text):<br/>    """<br/>        text: a string<br/>        <br/>        return: modified initial string<br/>    """<br/>   # text = BeautifulSoup(text, "lxml").text # HTML decoding. BeautifulSoup's text attribute will return a string stripped of any HTML tags and metadata.<br/>    text = text.lower() # lowercase text<br/>    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.<br/>    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. <br/>    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text<br/>    return text<br/>    <br/>df['Description'] = df['Description'].apply(clean_text)<br/>df['class_label'].value_counts()</span></pre><h1 id="4ee7" class="km jl hi bd jm kn ko kp jq kq kr ks ju kt ku kv jx kw kx ky ka kz la lb kd lc bi translated">用 LIME 解释文本预测</h1><p id="b6c0" class="pw-post-body-paragraph im in hi io b ip kf ir is it kg iv iw ix kh iz ja jb ki jd je jf kj jh ji jj hb bi translated">要了解更多信息，请参见<a class="ae kl" href="https://marcotcr.github.io/lime/tutorials/Lime%20-%20multiclass.html" rel="noopener ugc nofollow" target="_blank">石灰教程</a>。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="1f89" class="jk jl hi li b fi lm ln l lo lp">c = make_pipeline(vectorizer, logreg)<br/>class_names=list(df.Browser_Used.unique())<br/>explainer = LimeTextExplainer(class_names=class_names)</span></pre><p id="ffa2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">目标不是产生更高的结果，而是更好的分析。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="9ab5" class="jk jl hi li b fi lm ln l lo lp">print ('Explanation for class %s' % class_names[0])<br/>print ('\n'.join(map(str, exp.as_list(label=1))))</span></pre><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/3d0d755d3e4fbc867aeac44fd1ed5685.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*es5YEy_0Z6fVvOQWiHk33g.png"/></div></figure><p id="5193" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">它给了我们<strong class="io hj"><em class="kk">ie 浏览器</em> </strong>和<strong class="io hj"> <em class="kk"> Firefox。</em> </strong></p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="b3d5" class="jk jl hi li b fi lm ln l lo lp">exp.show_in_notebook(text=False)</span></pre><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="lt lu di lv bf lw"><div class="er es ls"><img src="../Images/ca15ea0e59ed8f7b035230bae0533523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*af54aVDl0Yfvoyksz4fhLw.png"/></div></div></figure><p id="af42" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我试着解释一下这种形象化:</p><ul class=""><li id="0128" class="lx ly hi io b ip iq it iu ix lz jb ma jf mb jj mc md me mf bi translated">对于该文档，单词“组”对于类别<strong class="io hj"><em class="kk">Internet Explorer</em></strong>具有最高的正面得分。</li><li id="540f" class="lx ly hi io b ip mg it mh ix mi jb mj jf mk jj mc md me mf bi translated">我们的模型预测该文档应该以 0.25%的概率被标记为<strong class="io hj"> <em class="kk">组</em> </strong>。</li><li id="8e77" class="lx ly hi io b ip mg it mh ix mi jb mj jf mk jj mc md me mf bi translated">另一方面，easy 对于类 Firefox 来说是负数，我们的模型已经了解到单词“private”对于类<strong class="io hj"> <em class="kk"> firefox 来说有一个小的正数。</em>T29】</strong></li></ul><h1 id="c9b8" class="km jl hi bd jm kn ko kp jq kq kr ks ju kt ku kv jx kw kx ky ka kz la lb kd lc bi translated">用 SHAP 解释文本预测</h1><p id="1469" class="pw-post-body-paragraph im in hi io b ip kf ir is it kg iv iw ix kh iz ja jb ki jd je jf kj jh ji jj hb bi translated">以下过程是从<a class="ae kl" href="https://stackoverflow.blog/2019/05/06/predicting-stack-overflow-tags-with-googles-cloud-ai/" rel="noopener ugc nofollow" target="_blank">本教程</a>中得知的。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="9b48" class="jk jl hi li b fi lm ln l lo lp">from sklearn.preprocessing import MultiLabelBinarizer<br/>import tensorflow as tf<br/>from tensorflow.keras.preprocessing import text<br/>import keras.backend.tensorflow_backend as K<br/>K.set_session<br/>import shap</span><span id="8cda" class="jk jl hi li b fi lq ln l lo lp">tags_split = [tags.split(',') for tags in df['Browser_Used'].values]<br/>print(tags_split[:10])</span><span id="192d" class="jk jl hi li b fi lq ln l lo lp">tag_encoder = MultiLabelBinarizer()<br/>tags_encoded = tag_encoder.fit_transform(tags_split)<br/>num_tags = len(tags_encoded[0])<br/>print(df['Description'].values[0])<br/>print(tag_encoder.classes_)<br/>print(tags_encoded[0])</span><span id="cab4" class="jk jl hi li b fi lq ln l lo lp">train_size = int(len(df) * .8)<br/>print('train size: %d' % train_size)<br/>print('test size: %d' % (len(df) - train_size))</span><span id="1a6a" class="jk jl hi li b fi lq ln l lo lp">y_train = tags_encoded[: train_size]<br/>y_test = tags_encoded[train_size:]</span><span id="dd79" class="jk jl hi li b fi lq ln l lo lp">class TextPreprocessor(object):<br/>    def __init__(self, vocab_size):<br/>        self._vocab_size = vocab_size<br/>        self._tokenizer = None<br/>    def create_tokenizer(self, text_list):<br/>        tokenizer = text.Tokenizer(num_words = self._vocab_size)<br/>        tokenizer.fit_on_texts(text_list)<br/>        self._tokenizer = tokenizer<br/>    def transform_text(self, text_list):<br/>        text_matrix = self._tokenizer.texts_to_matrix(text_list)<br/>        return text_matrix</span><span id="e17a" class="jk jl hi li b fi lq ln l lo lp">model.fit(X_train, y_train, epochs = 2, batch_size=128, validation_split=0.1)<br/>print('Eval loss/accuracy:{}'.format(model.evaluate(X_test, y_test, batch_size = 128)))</span></pre><ul class=""><li id="8b71" class="lx ly hi io b ip iq it iu ix lz jb ma jf mb jj mc md me mf bi translated">在模型准备好之后，我们利用最初的 200 条准备记录作为我们的经验信息集合来整合并制作一个 SHAP 解释器对象。</li><li id="5800" class="lx ly hi io b ip mg it mh ix mi jb mj jf mk jj mc md me mf bi translated">我们得到了对测试集子集的奇异期望的归因尊重。</li><li id="e390" class="lx ly hi io b ip mg it mh ix mi jb mj jf mk jj mc md me mf bi translated">将索引转换为单词。</li><li id="4110" class="lx ly hi io b ip mg it mh ix mi jb mj jf mk jj mc md me mf bi translated">利用 SHAP 的<strong class="io hj"> summary_plot </strong>技巧来展示影响模型预期的主要亮点。</li></ul><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="4f15" class="jk jl hi li b fi lm ln l lo lp">attrib_data = X_train[:200]<br/>explainer = shap.DeepExplainer(model, attrib_data)<br/>num_explanations = 40<br/>shap_vals = explainer.shap_values(X_test[:num_explanations])</span><span id="42ac" class="jk jl hi li b fi lq ln l lo lp">words = processor._tokenizer.word_index<br/>word_lookup = list()<br/>for i in words.keys():<br/>  word_lookup.append(i)</span><span id="430f" class="jk jl hi li b fi lq ln l lo lp">word_lookup = [''] + word_lookup<br/>shap.summary_plot(shap_vals, feature_names=word_lookup, class_names=tag_encoder.classes_)</span></pre><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/bf36b938229d10c13c436df9e972a736.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*X08lhF0jQb64wrHjYxOynw.png"/></div></figure><ul class=""><li id="34a3" class="lx ly hi io b ip iq it iu ix lz jb ma jf mb jj mc md me mf bi translated">单词“hotel”是我们的模型使用的最大信号单词，对类别<strong class="io hj"> <em class="kk"> Edgw </em> </strong>预测贡献最大。</li><li id="29dc" class="lx ly hi io b ip mg it mh ix mi jb mj jf mk jj mc md me mf bi translated">单词“rooms”是我们的模型使用的第四大信号词，当然对类<strong class="io hj"> <em class="kk"> firefox </em> </strong>贡献最大。</li></ul><p id="d4bc" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">就机器学习的可解释性而言，莱姆&amp; SHAP 有很多东西需要学习。</p><p id="05ae" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">希望这有所帮助:)</p><p id="683e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果你喜欢我的帖子，请关注。<em class="kk">如需更多帮助，请查看我的</em> <a class="ae kl" href="https://github.com/Afaf-Athar/NLP_With_Python" rel="noopener ugc nofollow" target="_blank"> <em class="kk"> Github </em> </a></p><p id="51de" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">通过<em class="kk">LinkedIn</em><a class="ae kl" href="https://www.linkedin.com/in/afaf-athar-183621105/" rel="noopener ugc nofollow" target="_blank">T3】https://www.linkedin.com/in/afaf-athar-183621105/T5】连接</a></p><p id="301f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">快乐学习😃</p></div></div>    
</body>
</html>