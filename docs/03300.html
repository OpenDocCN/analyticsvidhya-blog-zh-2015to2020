<html>
<head>
<title>Logistic Regression from Scratch: Multi classification with OneVsAll</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从零开始的逻辑回归:单变量多分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/logistic-regression-from-scratch-multi-classification-with-onevsall-d5c2acf0c37c?source=collection_archive---------1-----------------------#2020-01-25">https://medium.com/analytics-vidhya/logistic-regression-from-scratch-multi-classification-with-onevsall-d5c2acf0c37c?source=collection_archive---------1-----------------------#2020-01-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/0f24d53a6868f6e6fc55a8745085e81d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YbUzYEjl4-asP1bSfXLSfw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.coursera.org/learn/machine-learning" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="be4c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">是或不是！！红的还是黑的！！值得或不值得…哦，上帝！！为什么我们有选择？🤔。我们的世界由许多面向选择的问题组成。不是吗？</p><p id="44f7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此..在我们的现实生活中，当我们不得不做出选择的时候，我们该怎么做呢？在做出决定之前，我们分析了一些方面。就像一个问题“穿什么(红色还是黑色)？”我们考虑这是什么样的场合？活动是什么时候？什么会让我被晒黑😁？所以，是的，有很多因素会影响我们的决定。这正是逻辑回归所做的。在逻辑回归中，我们看到现有的数据，我们称之为因变量，我们画出它们之间的关系，并根据我们拥有的细节预测(因变量)。而我们预测的永远是二分法(二元)。</p><p id="76d6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是，如果我们有多分类问题，即我们有多个因变量要预测，我们可以不使用逻辑回归吗？在本文中，我们将建立一个模型，其中使用逻辑回归，我们将分类2个以上的变量。</p><p id="b7af" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是让我们先来理解逻辑回归背后的思想。“在逻辑回归中，借助于假设函数(也称为<strong class="ix hj"> sigmoid </strong>函数),我们通过给定一些输入数据(已知变量)来计算概率，并在此基础上，模型进行分析以预测所需的分类。”这是什么意思？所以如果我们有一些因变量，比如x1，x2，x3..我们有一个函数f(X ),通过使用因变量，如果我们的函数计算出一个介于0到1之间的数，那么我们可以成功地对问题陈述进行分类。因此，对于分类问题，符号的基本概念可以解释为目标值为0或1。即<strong class="ix hj"> y ∈ {0，1}。</strong>其中y =目标或因变量。</p><p id="c856" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，如果通过某种方式，我们可以找到给定X(特征值)在0到1之间的假设函数的值，那么我们就可以成功地对数据集进行分类。所以，如果<strong class="ix hj"> hθ (x </strong>)是我们的逻辑回归函数或假设，那么<strong class="ix hj"> hθ (x) </strong>需要满足条件为<strong class="ix hj"> 0≤ hθ (x) ≤1。</strong>我们可以通过在我们的逻辑函数中使用<strong class="ix hj"><em class="jt">【θTX(θ转置X)】</em></strong><em class="jt"/>达到同样的目的。逻辑功能可以注意如下:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ju"><img src="../Images/83072b9074d40036ff95d672d9db540f.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*WeB74Mbnc6Q0z7yK_9W44Q.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="http://: https://www.coursera.org/learn/machine-learning/supplement/AqSH6/hypothesis-representation" rel="noopener ugc nofollow" target="_blank">片段来源</a></figcaption></figure><p id="4c65" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下图向我们展示了sigmoid函数(逻辑函数)的样子:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jz"><img src="../Images/4c7216a1e3143593db24068bdd576eb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zi-WA-Eb2k5DeVLbMLhEGA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="http://: https://www.coursera.org/learn/machine-learning/supplement/AqSH6/hypothesis-representation" rel="noopener ugc nofollow" target="_blank">片段来源</a></figcaption></figure><p id="83c5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们在上面看到的，z = θTX的g(z)函数会将所有不同值的特征(因变量)转换为一个介于0和1之间的数字。这肯定有助于我们对目标变量进行分类。</p><p id="fcc3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们前面提到的，逻辑回归对概率起作用，所以这里我们的函数(hθ (x))将给出我们的输出是否为1的概率。例如，如果hθ (x) = 0.8，则表示我们的输出为1的概率为80%。因此，我们可以推断，我们的输出为0的概率是20%。</p><p id="5d27" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">决定边界:</em> </strong></p><p id="fe8e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了得到我们的分类值0或1，我们可以通过继承以下理解来对我们的特征进行分类:</p><p id="f3d9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt"> If，hθ</em>(<em class="jt">x</em>)≥0.5→<em class="jt">y</em>= 1，If<em class="jt">hθ</em>(<em class="jt">x</em>)&lt;0.5→<em class="jt">y</em>= 0</strong></p><p id="bedb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，如果我们对'<strong class="ix hj"> g' </strong>的输入是<strong class="ix hj"> θTX </strong>，那么这意味着，如果<strong class="ix hj"> θTX </strong>变得大于0，那么我们的out概率将接近1，我们可以将输出归类为1，反之亦然。这个想法可以被记为:</p><p id="e73b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"><em class="jt">hθ</em>(<em class="jt">x</em>)=<em class="jt">g</em>(<em class="jt">θTX</em>)≥0.5<em class="jt">当θTX </em> ≥ 0 </strong></p><p id="bdca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"><em class="jt">θTX</em>≥0</strong><strong class="ix hj">T43】y= 1</strong></p><p id="b09d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"><em class="jt">θTX</em>&lt;0</strong>y<em class="jt">y</em>= 0</p><p id="3505" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">判定边界是分隔<strong class="ix hj"> <em class="jt"> y = 0 </em> </strong>和<strong class="ix hj"> <em class="jt"> y = 1 </em> </strong>的区域的线。它是由我们的假设函数创建的。</p><p id="3973" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">成本函数:</em> </strong></p><p id="244f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的模型的主要目标应该是找出权重，即<strong class="ix hj"> <em class="jt"> θ </em> </strong>值。为了找出<strong class="ix hj"> <em class="jt"> θ </em> </strong>的值，我们需要一个函数(姑且称之为代价函数)，通过使用不同的<strong class="ix hj"> <em class="jt"> θ </em> </strong>的值来最小化它的值。最小化成本函数的权重将是我们最适合的权重。<strong class="ix hj"> <em class="jt"> </em> </strong>在逻辑回归中，我们需要选择一个凸函数，以便找到局部最优解。</p><p id="fba0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">逻辑回归的成本函数如下所示:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ka"><img src="../Images/3718bfd8913378c04dc58b56344e2cfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*SQBj1s8D72X4ROVJPY12KA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.coursera.org/learn/machine-learning/supplement/bgEt4/cost-function" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="626b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果y = 1，Cost(hθ(x)，y)=-log(hθ(x))</p><p id="2ee0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果y = 0，Cost(hθ(x)，y)=-log(1hθ(x))</p><p id="40a2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果hθ(x)=y，则Cost(hθ(x)，y) =0</p><p id="c7cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Cost(hθ(x)，y) →∞如果y=0且hθ(x)→1</p><p id="d996" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Cost(hθ(x)，y) →∞如果y=1且hθ(x)→0</p><p id="b7c5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们在解决方案中绘制了成本函数，显示了类似的模式。我们通常从选取随机的θ值开始，随后在梯度下降的帮助下更新这些值，并测量我们的模型结果有多好。该值是使用成本函数计算的，定义为:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es kb"><img src="../Images/01c590f9f5c3d4cef9361bb28030c3ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*PhGWYNV-SB_FqvZQFm77gA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.coursera.org/learn/machine-learning/supplement/bgEt4/cost-function" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="10cf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">渐变下降</em> </strong></p><p id="2699" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的目标是最小化成本函数，通过改变权重，即θ值，我们可以达到同样的目的。我们可以通过对每个权重取成本函数的导数来更新<strong class="ix hj"> <em class="jt"> θ </em> </strong>值。同样，我们可以重复下面的公式:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es kc"><img src="../Images/ec0ce743fcdc39be7d64c867699bf013.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*_V1y7J2MjFEPdFQKOXXQrg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.coursera.org/learn/machine-learning/supplement/0hpMl/simplified-cost-function-and-gradient-descent" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="1b49" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">同样的公式可以写成:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es kd"><img src="../Images/1c03e467b434e0a13e749d1c3ba39154.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*fSXJ65xtVwi4E7T_wvzpuA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.coursera.org/learn/machine-learning/supplement/0hpMl/simplified-cost-function-and-gradient-descent" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="9bc9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是上述公式的矢量化符号:</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ke"><img src="../Images/5227cac60e4c9eef744893d7b8199a8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*VEXLo0acWAT3fwb8YlWTDw.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.coursera.org/learn/machine-learning/supplement/0hpMl/simplified-cost-function-and-gradient-descent" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="a7ca" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用的阿尔法符号被称为学习率。这基本上有助于我们向凸函数的局部最优值发展。我建议阅读下面的文章(<a class="ae iu" href="https://towardsdatascience.com/understanding-the-mathematics-behind-gradient-descent-dde5dc9be06e" rel="noopener" target="_blank"><strong class="ix hj"><em class="jt"/></strong></a><strong class="ix hj"><em class="jt">)</em></strong>，了解更多关于梯度下降和相关术语的知识。</p><p id="b4ad" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">多类分类:一对多:</em> </strong></p><p id="24b9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，当我们有两个以上的类别时，我们将处理数据分类。我们必须扩展我们的描述而不是<strong class="ix hj"> <em class="jt"> y = { 0，1 } </em> </strong>，这样<strong class="ix hj"> <em class="jt"> y = { 0，1 … n} </em> </strong>。</p><p id="6866" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一对一是一种策略，涉及训练N个不同的二元分类器，每个分类器被设计成识别特定的类别。之后，我们共同使用这N个分类器来预测正确的类别。我们如何用代码来实现呢？通过将一个类别视为1，其余类别均视为0，我们训练模型并获得必要的权重(theta值)。我们以字典格式存储每个分类器的权重值。然后借助于<strong class="ix hj"> Sigmoid函数</strong>我们计算概率。最大概率当选总统，我们将数据分类到相应的分类器中。</p><p id="5f7a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="jt">代码:</em> </strong></p><figure class="jv jw jx jy fd ij"><div class="bz dy l di"><div class="kf kg l"/></div></figure><p id="2e09" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">模型的执行:</strong></p><p id="76db" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在<strong class="ix hj"> Fit () </strong>方法中，我们实现了一个vs Rest算法，因为数据集需要多分类模型。我们迭代不同标签时间的代码，然后通过梯度下降法找出<strong class="ix hj"> θ </strong>值。类似地，我们也在计算相对于相应的<strong class="ix hj"> θ </strong>值的成本函数值。在我们获得每个标签类型的最佳<strong class="ix hj"> θ </strong>值后，借助于<strong class="ix hj"> Predict () </strong>方法，我们正在使用我们最初找到的<strong class="ix hj"> θ </strong>值找到给定特征的最大概率。因此，我们通过获得输入的最大概率来对每个特征输入进行分类。</p><p id="2ace" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">绘制成本值:</strong></p><p id="025e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于这个数据集包含一个多分类问题，我们基本上扩展了我们的定义，使y = {0，1…n}。在这种情况下，我们有3个类别，因此我们通过遵循一对一的方法相应地找到了3种不同情况下的成本值。因此，对于相应的成本值，我们有3个不同的图。考虑到与类别相关的成本值，PFB绘制了成本与迭代次数的关系图。我们可以清楚地看到，随着每一次迭代，成本值都在减少，并趋向于零值。</p><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es kh"><img src="../Images/b8fe08cadb77e09d37c269081a0a7297.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*FRLk1tWuTAfgsXacbqEfFg.png"/></div></figure><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es ki"><img src="../Images/cc3c56e06c247b103d1b64a90fe9961e.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*V1MhiXMOr_YogCTQOFS1HA.png"/></div></figure><figure class="jv jw jx jy fd ij er es paragraph-image"><div class="er es kj"><img src="../Images/d3c7137e11b0c272bb15d7d82b940cef.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*yZK-OK_hF6zkoTuxegMo_A.png"/></div></figure><p id="60c6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们现在已经成功地实现了逻辑回归多分类算法，该算法可以用于分类2个以上的目标变量。</p></div></div>    
</body>
</html>