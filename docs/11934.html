<html>
<head>
<title>Introduction To Gradient Boosting Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度推进分类简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-gradient-boosting-classification-da4e81f54d3?source=collection_archive---------3-----------------------#2020-12-24">https://medium.com/analytics-vidhya/introduction-to-gradient-boosting-classification-da4e81f54d3?source=collection_archive---------3-----------------------#2020-12-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/2d1579ccb93ede5d87a35da73b58c09a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kgmImYxdhX01Yfk5qzkX-A.jpeg"/></div></div></figure><div class=""/><h1 id="4a3a" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">增压</strong></h1><p id="6510" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi km translated"><span class="l kn ko kp bm kq kr ks kt ku di"> B </span> <strong class="jq hu"> oosting </strong>是一种集成方法，将几个弱学习器依次组合成一个强学习器。在boosting方法中，我们按顺序训练预测器，每个预测器都试图纠正其前任。</p><h1 id="be52" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">梯度增强</strong></h1><p id="b1cc" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi km translated"><span class="l kn ko kp bm kq kr ks kt ku di"> G </span>梯度升压是<strong class="jq hu">梯度下降和升压</strong>的组合。在梯度推进中，每个新模型使用梯度下降方法最小化来自其前身的损失函数。这个过程一直持续到获得目标变量的更优估计</p><p id="ea3b" class="pw-post-body-paragraph jo jp ht jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">与其他集成技术不同，梯度推进的想法是，他们建立一系列的树，其中每一棵树都试图纠正其前任树的错误。</p><blockquote class="la lb lc"><p id="eddb" class="jo jp ld jq b jr kv jt ju jv kw jx jy le kx kb kc lf ky kf kg lg kz kj kk kl hb bi translated"><strong class="jq hu">为了理解梯度推进，试着想象一名高尔夫球手将高尔夫球击向球洞，每次击球都经过一定的地面距离。他反复击球，通过在每个阶段后重新评估方向和大小，朝着球洞前进。</strong></p></blockquote><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es lh"><img src="../Images/f2a33ab80d5dc601d22b74d88eb186b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*PEkS3fIMguKnOS1TibYmgg.png"/></div></figure><blockquote class="la lb lc"><p id="f1f3" class="jo jp ld jq b jr kv jt ju jv kw jx jy le kx kb kc lf ky kf kg lg kz kj kk kl hb bi translated"><strong class="jq hu">每次击球后，高尔夫球手通过计算实际球洞位置和他所做的近似之间的差异来确定适当的轻推。这就是所谓的剩余向量。这个高尔夫球手的例子类似于我们在梯度推进中所做的，试图尽可能接近目标变量。</strong></p></blockquote><h1 id="d4e2" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">梯度增强的组件</h1><h1 id="c1ba" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">损失函数</h1><p id="9605" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi km translated"><span class="l kn ko kp bm kq kr ks kt ku di"> T </span>梯度推进算法的目标是<strong class="jq hu">最小化损失函数</strong>，即实际类别和预测类别之间的差异。分类算法经常使用<strong class="jq hu">对数损失函数</strong>，而回归算法使用<strong class="jq hu">平方误差</strong>。它们是梯度提升分类器支持的许多标准损失函数，条件是损失函数应该是可微的。</p><h1 id="38d5" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">弱学习者</h1><p id="581f" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">典型地，决策树用于弱学习者。这些人对我们的数据分类很差，这意味着他们有更高的错误率。他们个人不足以做出预测。</p><h1 id="4bec" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">添加剂成分</h1><p id="e08d" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">随着更多的学习者被添加到模型中，树的输出可以被加在一起以最小化预测中的损失。该过程采用类似的程序对计算的损失进行梯度下降，从而迭代地减少损失。</p><blockquote class="lm"><p id="584b" class="ln lo ht bd lp lq lr ls lt lu lv kl dx translated"><strong class="ak">梯度推进分类步骤</strong></p></blockquote><figure class="lx ly lz ma mb hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lw"><img src="../Images/54d7bcd059c14a20d12d991ded0197b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dCnqgqrqwBkR2yYlCXbxhg.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated">梯度推进模型</figcaption></figure><p id="7af4" class="pw-post-body-paragraph jo jp ht jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">第一步:对数据进行简单的线性回归或决策树拟合[𝒙 = 𝒊𝒏𝒑𝒖𝒕，𝒚 = 𝒐𝒖𝒕𝒑𝒖𝒕]</p><p id="038b" class="pw-post-body-paragraph jo jp ht jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">步骤2:通过从实际目标值中减去预测目标值来计算误差残差。[𝒆𝟏 = 𝒚𝒕𝒓𝒖𝒆 −𝒚𝒑𝒓𝒆𝒅𝒊𝒄𝒕𝒆𝒅𝟏]</p><p id="87e3" class="pw-post-body-paragraph jo jp ht jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">步骤3:在保持输入变量不变的情况下，以误差残差为目标变量拟合新模型。[𝒆𝒑𝒓𝒆𝒅𝒊𝒄𝒕𝒆𝒅𝟏]</p><p id="ff24" class="pw-post-body-paragraph jo jp ht jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">第四步:将预测的残差与之前的预测相加[𝒚𝒑𝒓𝒆𝒅𝒊𝒄𝒕𝒆𝒅𝟐=𝒚𝒑𝒓𝒆𝒅𝒊𝒄𝒕𝒆𝒅𝟏+𝒆𝒑𝒓𝒆𝒅𝒊𝒄𝒕𝒆𝒅𝟏]</p><p id="0d2e" class="pw-post-body-paragraph jo jp ht jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">第五步:在剩余的残差上拟合下一个模型。[𝒆𝟐 = 𝒚𝒕𝒓𝒖𝒆 − 𝒚𝒑𝒓𝒆𝒅𝒊𝒄𝒕𝒆𝒅𝟐]</p><p id="eb11" class="pw-post-body-paragraph jo jp ht jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">重复步骤2到5，直到模型开始过度拟合或者残差和没有变化</p><h1 id="9aca" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">例子</h1><p id="1767" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi km translated">让我们通过考虑一个样本数据集来理解梯度推进分类。它有几个输入特征𝑥1，𝑥2，𝑥3，并试图预测目标变量𝑦，这是一个二进制输出。</p><figure class="li lj lk ll fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mg"><img src="../Images/3d1aa25235d395789f68d8a1295ef337.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O8OVHgvkTg8glxW_IIEWWw.png"/></div></div></figure><p id="2621" class="pw-post-body-paragraph jo jp ht jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">第一步:<strong class="jq hu">使用目标变量概率的对数进行初步猜测。</strong></p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mh"><img src="../Images/64d5e98880b97b59dcef7ad77c963950.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*xgAIc0gCY_VEYOgLIN9QtA.png"/></div></figure><blockquote class="la lb lc"><p id="1b25" class="jo jp ld jq b jr kv jt ju jv kw jx jy le kx kb kc lf ky kf kg lg kz kj kk kl hb bi translated">为了进行分类，我们应用softmax变换。</p></blockquote><figure class="li lj lk ll fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mi"><img src="../Images/b1a6e129622d3d41a9a55fd17c7cb9fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AfegzlXjROT8RJD6-kBJ8w.png"/></div></div></figure><p id="b20e" class="pw-post-body-paragraph jo jp ht jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">第二步:<strong class="jq hu">从观测值中减去预测，计算误差残差或伪残差。因此，该表如下所示:</strong></p><figure class="li lj lk ll fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mi"><img src="../Images/04b1c811326cc62b21e5f845e6b4ede4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aZ9oxGtODyExPlxGB9Tb8w.png"/></div></div></figure><p id="5b29" class="pw-post-body-paragraph jo jp ht jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">第三步:<strong class="jq hu">计算分类树。</strong></p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mj"><img src="../Images/4012fb867281ea697f029989dc300338.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*Nn8KFJBaAz0HTTbnKHvZsg.png"/></div></figure><blockquote class="la lb lc"><p id="5cb6" class="jo jp ld jq b jr kv jt ju jv kw jx jy le kx kb kc lf ky kf kg lg kz kj kk kl hb bi translated">这是一个只有两片叶子的分类树的例子。然而，梯度增强通常具有5个以上的叶，并且许多叶可以具有多个值。因此，梯度提升使用变换进行分类。考虑下面的树:</p></blockquote><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mk"><img src="../Images/4290f7bc2e1d86b56583b9667aa63f3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*EZthR13zZP8Mjxrskv-1ww.png"/></div></figure><blockquote class="la lb lc"><p id="8c6a" class="jo jp ld jq b jr kv jt ju jv kw jx jy le kx kb kc lf ky kf kg lg kz kj kk kl hb bi translated">因此，第二片叶子的值由以下变换给出</p></blockquote><figure class="li lj lk ll fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ml"><img src="../Images/a3aa72a318c93786472f3c90ba2c7755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-u2e6XFeI5GJR7hnadI4nQ.png"/></div></div></figure><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mm"><img src="../Images/7457363e80f4703303a8c57c7ad28841.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*3OXJlqCwh8wwJJsz-YxlBQ.png"/></div></figure><p id="f92d" class="pw-post-body-paragraph jo jp ht jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">第四步:<strong class="jq hu">做预测。</strong></p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mm"><img src="../Images/d8ff512fb87b1aebc2ec91fb99b9af14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*OdzBhFAeTKV92mQ5uG_f1w.png"/></div></figure><blockquote class="la lb lc"><p id="3d6d" class="jo jp ld jq b jr kv jt ju jv kw jx jy le kx kb kc lf ky kf kg lg kz kj kk kl hb bi translated">学习率定义了新树的贡献。现在，新的对数赔率预测可以转换为概率使用softmax函数。</p></blockquote><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mn"><img src="../Images/7fc19848800e3f50582e75facda78035.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*RP15S5EUG627vvdoGNMEiw.png"/></div></figure><blockquote class="la lb lc"><p id="dad1" class="jo jp ld jq b jr kv jt ju jv kw jx jy le kx kb kc lf ky kf kg lg kz kj kk kl hb bi translated">如你所见，概率已经从之前的对数优势比下降了。</p></blockquote><p id="a8da" class="pw-post-body-paragraph jo jp ht jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">步骤5: <strong class="jq hu">重复步骤，直到模型开始过拟合或者残差和没有变化。</strong></p><h1 id="f292" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">超参数调谐</h1><p id="743f" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi km translated"><span class="l kn ko kp bm kq kr ks kt ku di"> O </span>梯度增强的主要特征之一是它提供了几个参数的调谐。然而，找到参数的最佳组合可能是一项具有挑战性的任务，并且通常很耗时。最常见的超参数如下:</p><h1 id="6d15" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">学习率</strong></h1><p id="fef1" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">它控制树的贡献以及使用梯度下降算法收敛的速度。较小的值减少了过度拟合的机会，但是算法需要更长的时间来收敛。值越大，算法越快，但可能永远达不到最佳拟合。</p><h1 id="03f3" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">树木数量</strong></h1><p id="b56e" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">这里的目标是使用交叉验证找到最优的树数，以便最小化损失函数。可以使用提前停止来找到最佳的树数。</p><h1 id="0a1e" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">树木的深度</strong></h1><p id="ecd6" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">树中的分裂数称为树的深度(d)。通常算法在d=1时运行良好，然而，它可以大于1，但总是小于10。</p><h1 id="401b" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">二次抽样</strong></h1><p id="b48e" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">它指定了用于训练每棵树的训练实例的比例。如果子样本= 0.20，则随机选择的20%的训练实例用于训练。</p><h1 id="4146" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">优势:</h1><h1 id="29c9" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">灵活性</strong></h1><p id="59df" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">梯度增强可以支持各种损失函数，并提供了许多超参数调整选项，这使得它非常灵活</p><h1 id="f333" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">没有数据预处理</strong></h1><p id="4dcf" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">它可以很好地处理数字和分类特征。</p><h1 id="a3bb" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">处理缺失数据</strong></h1><p id="970a" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">不需要数据插补</p><h1 id="9149" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">缺点:</h1><h2 id="88ec" class="mo ir ht bd is mp mq mr iw ms mt mu ja jz mv mw je kd mx my ji kh mz na jm nb bi translated"><strong class="ak"> 1。过度装配</strong></h2><p id="4372" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">它使所有误差最小化，因此易于过度拟合。人们必须使用交叉验证来中和</p><h2 id="50cd" class="mo ir ht bd is mp mq mr iw ms mt mu ja jz mv mw je kd mx my ji kh mz na jm nb bi translated"><strong class="ak"> 2。计算成本高</strong></h2><p id="57a0" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">这种技术通常需要许多树；因此，它可能会耗尽时间和内存</p><h1 id="987d" class="iq ir ht bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">结论</h1><p id="9c44" class="pw-post-body-paragraph jo jp ht jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">梯度推进是最强大的集成算法之一，最适合回归和分类任务。然而，它们易于过度拟合，但是可以采用上面讨论的各种方法来处理过度拟合。</p><p id="0a48" class="pw-post-body-paragraph jo jp ht jq b jr kv jt ju jv kw jx jy jz kx kb kc kd ky kf kg kh kz kj kk kl hb bi translated">它们比其他机器学习算法的计算要求更高，但一名成功的数据科学家或机器学习工程师基本上应该知道梯度推进算法的工作原理。</p></div></div>    
</body>
</html>