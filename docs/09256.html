<html>
<head>
<title>NEURAL NETWORK - ACTIVATION FUNCTION</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络激活函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-network-activation-function-c53a3f334364?source=collection_archive---------13-----------------------#2020-08-30">https://medium.com/analytics-vidhya/neural-network-activation-function-c53a3f334364?source=collection_archive---------13-----------------------#2020-08-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6d24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在神经网络中，激活函数用于确定神经网络的输出。这种类型的函数被附加到每个神经元，并基于每个神经元的输入是否与模型的预测相关来确定该神经元是否应该激活。在这篇文章中，我们将学习不同类型的激活函数及其优缺点。</p><p id="a98f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在学习激活函数之前，让我们看看激活函数是如何工作的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/26d7d73e6b4f145ebf38c8080dddf711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PEVu_D47bKVvWfGEOj8Y7A.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">图一。激活功能</figcaption></figure><p id="d6d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jt">我们知道每个神经元都含有激活功能。它将输入视为前一层输出与其各自权重的乘积之和。这个总和值被传递给激活函数。</em></p><h2 id="f835" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">常用激活功能</h2><h2 id="f7a1" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">1.Sigmoid 函数:-</h2><p id="9f09" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">Sigmoid 函数是最流行的激活函数之一。sigmoid 函数的方程表示为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ku"><img src="../Images/7e051d9a5322cd7a7b3c3a52ab0e5cd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*JZtFmzt6S6Dh5v3M9QMLkA.png"/></div></figure><p id="1be3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">sigmoid 函数总是给出(0，1)范围内的输出。sigmoid 函数的导数为 f`(x) =f(x)(1-f(x))，其取值范围在(0，0.25)之间。</p><p id="281e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通常，sigmoid 函数用于末端层。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/d05cf500652e4551d8319264632fd04b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qNJyrtuvPi3B2_Ig3ZKk8A.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">sigmoid 激活函数</figcaption></figure><blockquote class="kv kw kx"><p id="3b10" class="if ig jt ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">优势</p></blockquote><p id="22d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jt"> 1 平滑渐变，防止输出值“跳跃”。</em></p><p id="ecfb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jt"> 2 个输出值绑定在 0 和 1 之间，归一化每个神经元的输出。</em></p><blockquote class="kv kw kx"><p id="40f2" class="if ig jt ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">不利之处</p></blockquote><p id="0faf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1 不是以零为中心的函数。</p><p id="2f2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2 遭受梯度消失。</p><p id="cd5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3 远离质心的值的输出接近于零。</p><p id="ae4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4 计算成本高，因为它必须计算函数中的指数值。</p><h2 id="564a" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">2 双曲正切激活函数:-</h2><p id="9111" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">为了克服 sigmoid 函数非零中心函数的缺点，人们引入了 tanh 激活函数。Tanh 激活函数方程和图形表示为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lb"><img src="../Images/d8aa751eb6b55844eae820229aa2c173.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*xAxnqtg6WGdjEvFX-uH4dQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">Tanh 激活函数</figcaption></figure><p id="c24f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">双曲正切函数的输出总是在(-1，1)之间，它的导数在(0，1)之间</p><blockquote class="kv kw kx"><p id="f832" class="if ig jt ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated"><em class="hi">优点</em></p></blockquote><p id="6c44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">双曲正切函数具有 sigmoid 函数的所有优点，并且是零中心函数。</p><blockquote class="kv kw kx"><p id="7442" class="if ig jt ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">不足之处</p></blockquote><p id="cb5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">比 sigmoid 函数多 1 倍的计算成本。</p><p id="8728" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2 遭受梯度消失。</p><p id="bd9b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3 远离质心的值的输出接近于零。</p><h2 id="b53e" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">3 RELU(校正线性单位):-</h2><p id="43e8" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">在上面两个激活中，我们有梯度消失的主要问题，为了克服这个问题，人们引入了 relu 激活函数。</p><p id="9f71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Relu 激活函数很简单 f(x) = max(0，x)。这意味着如果 x(输入值)为正，则输出也为 x。如果 x(输入值)为负，则输出值为零，这意味着特定神经元被去激活。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lc"><img src="../Images/7941450cae1150ec9a17711cacdaa489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*h8RxYp0TcTUw7VKhdcyaYQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">RELU 激活函数</figcaption></figure><blockquote class="kv kw kx"><p id="f2ef" class="if ig jt ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">优势</p></blockquote><p id="217a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1 无渐变消失</p><p id="2eda" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2 导数是常数</p><p id="8d3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3 较少的计算开销</p><blockquote class="kv kw kx"><p id="1552" class="if ig jt ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">不足之处</p></blockquote><p id="7427" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1 无论对于什么负值，神经元都是完全不活动的。</p><p id="5369" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2 非零中心函数。</p><h2 id="ef7f" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">4 漏 RELU</h2><p id="04ca" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">泄漏的 ReLU 是解决“<em class="jt">死亡 ReLU </em>问题的一种尝试。当 x &lt;为 0 时，函数不是零，而是泄漏的 ReLU 具有小的负斜率(大约 0.01)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ld"><img src="../Images/e290380674ba73f658d1db1e852b4030.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*I7HtHyFCTBM0vlNxKy2YBQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">泄漏 relu</figcaption></figure><blockquote class="kv kw kx"><p id="3b8e" class="if ig jt ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated"><em class="hi">理论上，漏 ReLU 具有 ReLU 的所有优点，加上死 ReLU 不会有任何问题，但在实际操作中，并没有完全证明漏 ReLU 总是比 ReLU 好。</em></p></blockquote><h2 id="450d" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">5 ELU(指数线性单位):-</h2><p id="53aa" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">为了克服死中子，还引入了 Elu 活化函数。而不是用 0.01 的倍数当 x &lt;0 ,a ELU do α.( ex — 1).</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es le"><img src="../Images/5b5e03109c28652330f19bc60bd93d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IgyY9WX5dgY69Nnh9OOveA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">ELU activation function</figcaption></figure><blockquote class="kv kw kx"><p id="94f1" class="if ig jt ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">Advantages</p></blockquote><p id="744f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jt"> 1 时没有死 ReLU 问题。</em></p><p id="4b78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jt"> 2 输出的平均值接近 0，以零为中心</em></p><blockquote class="kv kw kx"><p id="ef92" class="if ig jt ih b ii ij ik il im in io ip ky ir is it kz iv iw ix la iz ja jb jc hb bi translated">不足之处</p></blockquote><p id="4ed0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个小问题是它的计算量稍微大一些。类似于 Leaky ReLU，虽然理论上比 ReLU 好，但是目前在实践中没有很好的证据证明 eLU 总是比 ReLU 好。</p><h2 id="93eb" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">6 预逻辑(参数重新逻辑):-</h2><p id="260c" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">PReLU 也是 ReLU 的改进版本。在负区，PReLU 斜率小，也可以避免 ReLU 死的问题。与 ELU 相比，PReLU 是负区域中的线性运算。虽然斜率小，但不趋向于 0，这是一定的优势。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lf"><img src="../Images/6e92bff42aba9ccaab4000abb7283f6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*vvKAXOPCtMgE_kwTh3avqg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">普雷卢</figcaption></figure><p id="76f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们看了 PReLU 的公式。参数α一般是 0 到 1 之间的数，一般比较小，比如几个零。当α = 0.01 时，我们称 PReLU 为 Leaky Relu，它是 PReLU it 的一个特例。</p><p id="2f82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面，yᵢ是第 I 个通道上的任何输入，aᵢ是负斜率，它是一个可学习的参数。</p><ul class=""><li id="2bec" class="lg lh hi ih b ii ij im in iq li iu lj iy lk jc ll lm ln lo bi translated">如果 aᵢ=0，f 变成了 ReLU</li><li id="f9ea" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">如果 aᵢ&gt;0，f 变成泄漏的 ReLU</li><li id="ac18" class="lg lh hi ih b ii lp im lq iq lr iu ls iy lt jc ll lm ln lo bi translated">如果 aᵢ是一个可学习的参数，f 变成 PReLU</li></ul><h2 id="d4b2" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">7 swish(开关门控)功能:-</h2><p id="f190" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">Swish 函数是由 google brains 提出的，它只是将输入与 sigmoid 函数相乘，即 f(x) = x .sigmod(x)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lu"><img src="../Images/c4370cef2c793cb3b4ad5eb929ee4e65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bDbDQaxrXRlZI59PIhTGlQ.png"/></div></div></figure><p id="c0e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">谷歌大脑的实验表明，在一些具有挑战性的数据集上，Swish 比 ReLU 更容易工作。</p><p id="98ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">自门控的优点是它只需要一个简单的标量输入，而正常门控需要多个标量输入。这一特性使得自门控激活函数(如 Swish)能够轻松替换以单个标量作为输入的激活函数(如 ReLU)，而不改变隐藏容量或参数数量。</p><h2 id="3445" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">8 Softmax(归一化指数函数):-</h2><p id="664f" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">当我们想要建立多类分类器时，在神经网络中使用<strong class="ih hj"> softmax 激活函数</strong>。软最大值函数的等式为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/83397cfbece53a86e282bd5ec9aa1f48.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*g2lD2tZpOngG3Vvm_I7Jew.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">softmax 激活功能</figcaption></figure><p id="c0be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Softmax。它总是“返回多类分类问题中目标类的概率分布”。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/6f3913d9774ba261fa8dce9dc8c0020a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Mn0EL0U5JR4L7hulfOq9Q.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">例子</figcaption></figure><p id="8173" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，如果你有<strong class="ih hj"><em class="jt">【A，B，C】</em></strong>三个类，那么在输出层会有三个神经元。假设你得到神经元的输出为[-0.21，0.47，1.72]。对这些值应用 softmax 函数，您将得到以下结果— [0.1，0.2，0.7]。这些表示数据点属于每个类别的概率。从结果中我们可以看出输入属于 c 类。</p><h2 id="1885" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">9 Softplus 激活功能:-</h2><p id="9b65" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated">softplus 函数和 ReLU 函数类似，但是比较平滑。是像 ReLU 一样的单方面打压。接受范围广(0，+ inf)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lx"><img src="../Images/50fc69662077a26bb27726bf53a26b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fIAftt1k79ll_etU3q0ohg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">RELU vs 软加</figcaption></figure><p id="75e8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Softplus 函数:<strong class="ih hj"> f(x) = ln(1+exp x)。</strong><strong class="ih hj">soft plus</strong>的导数为 f′(x)= exp(x)/(1+exp⁡x)= 1/(1+exp(x))也叫逻辑<strong class="ih hj">函数</strong>。</p><h2 id="e1b2" class="ju jv hi bd jw jx jy jz ka kb kc kd ke iq kf kg kh iu ki kj kk iy kl km kn ko bi translated">10 MaxOut 激活功能</h2><p id="650e" class="pw-post-body-paragraph if ig hi ih b ii kp ik il im kq io ip iq kr is it iu ks iw ix iy kt ja jb jc hb bi translated"><strong class="ih hj"> Maxout 激活</strong>是 ReLU 和 leaky ReLU <strong class="ih hj">功能</strong>的概括。这是一个可学习的<strong class="ih hj">激活功能</strong>。这是一个分段线性<strong class="ih hj">函数</strong>，它返回输入的最大值，旨在与丢失正则化技术结合使用。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/1b656c1eb83d800f30b232b601c46cd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*wHwIprXgq_gc971E3UEeBQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">最大输出</figcaption></figure><p id="0fbd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">— — — — — — — — — — — — — — — — — — — — — — — — — — — — — —</p><p id="32ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意</strong> : <em class="jt">一般来说，这些激活功能各有利弊。没有说明哪些不行，哪些激活功能好。所有的好与坏都必须通过实验来获得。</em></p></div></div>    
</body>
</html>