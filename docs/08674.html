<html>
<head>
<title>Hadoop Single Node Cluster on Docker</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Docker上的Hadoop单节点集群</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/hadoop-single-node-cluster-on-docker-e88c3d09a256?source=collection_archive---------10-----------------------#2020-08-08">https://medium.com/analytics-vidhya/hadoop-single-node-cluster-on-docker-e88c3d09a256?source=collection_archive---------10-----------------------#2020-08-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div class="er es hg"><img src="../Images/181da60dc2986ade4b6b64bba4df0449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EudU6fAL7JhzntwF36Bqbg.jpeg"/></div></figure><div class=""/><p id="a6b0" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">本教程将展示如何使用docker获得一个Hadoop单节点集群，我们将从Docker映像构建开始运行一个容器，其环境中的<strong class="io hq"> Hadoop 3.3.0 </strong>配置为单节点集群。</p><h2 id="ffee" class="jk jl hp bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated">创建Hadoop映像</h2><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="41b1" class="jk jl hp kk b fi ko kp l kq kr">$ git clone <a class="ae ks" href="https://github.com/rancavil/hadoop-single-node-cluster.git" rel="noopener ugc nofollow" target="_blank">https://github.com/rancavil/hadoop-single-node-cluster.git</a><br/>$ cd hadoop-single-node-cluster<br/>$ docker build -t hadoop .</span></pre><h2 id="5b18" class="jk jl hp bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated">要运行并创建容器，请执行下一个命令:</h2><p id="fc3d" class="pw-post-body-paragraph im in hp io b ip kt ir is it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj hb bi translated">要运行并创建容器，请执行以下命令:</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="76b6" class="jk jl hp kk b fi ko kp l kq kr">$ docker run -it — name &lt;container-name&gt; -p 9864:9864 -p 9870:9870 -p 8088:8088 — hostname &lt;your-hostname&gt; hadoop</span></pre><p id="241f" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">将<strong class="io hq">的容器名</strong>改为你喜欢的名字，将<strong class="io hq">的主机名</strong>设置为你的IP或机器名。您可以使用本地主机作为您的主机名</p><p id="c863" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当您运行容器时，将执行<strong class="io hq">docker-entry point . sh</strong>shell来创建和启动Hadoop环境。</p><p id="3cd4" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">您应该会看到以下提示:</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="87c1" class="jk jl hp kk b fi ko kp l kq kr">hduser@localhost:~$</span></pre><p id="f8aa" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">您已经准备好开始使用Hadoop了。</p><h2 id="cc75" class="jk jl hp bd jm jn jo jp jq jr js jt ju ix jv jw jx jb jy jz ka jf kb kc kd ke bi translated">检查我们的Hadoop环境</h2><p id="de47" class="pw-post-body-paragraph im in hp io b ip kt ir is it ku iv iw ix kv iz ja jb kw jd je jf kx jh ji jj hb bi translated">要检查Hadoop容器是否正常工作，请访问浏览器中的URL。</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="5f5e" class="jk jl hp kk b fi ko kp l kq kr"><a class="ae ks" href="http://localhost:9870" rel="noopener ugc nofollow" target="_blank">http://localhost:9870</a></span></pre><p id="5d31" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">您将在浏览器上看到以下屏幕。</p><figure class="kf kg kh ki fd hk er es paragraph-image"><div role="button" tabindex="0" class="kz la di lb bf lc"><div class="er es ky"><img src="../Images/8acd8ebb951f2cb20f02b0198bc7f103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B5HP_Qx5spseGjSoKw41bQ.png"/></div></div></figure><p id="39ae" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">注意:</strong>HDFS-site . XML配置具有属性。</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="62e9" class="jk jl hp kk b fi ko kp l kq kr">&lt;property&gt;<br/>      &lt;name&gt;dfs.permissions&lt;/name&gt;<br/>      &lt;value&gt;false&lt;/value&gt;<br/>&lt;/property&gt;</span></pre><p id="7135" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以<strong class="io hq">不要在<strong class="io hq">生产环境</strong>中使用</strong>。</p><p id="60b7" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">第一个例子</strong></p><p id="de8c" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">创建执行MapReduce作业所需的HDFS目录:</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="f36a" class="jk jl hp kk b fi ko kp l kq kr">hduser@localhost:~$ hdfs dfs <strong class="kk hq">-mkdir</strong> /user<br/>hduser@localhost:~$ hdfs dfs <strong class="kk hq">-mkdir</strong> /user/hduser</span></pre><p id="3cbb" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们刚刚在分布式文件系统上创建了目录/user/hduser。</p><p id="2301" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">将输入文件复制到分布式文件系统中:</p><p id="576c" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将在/user/hduser中创建一个名为input的目录。</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="9a8d" class="jk jl hp kk b fi ko kp l kq kr">hduser@localhost:~$ hdfs dfs -mkdir input<br/>hduser@localhost:~$ hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml input</span></pre><p id="5527" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">运行提供的一些示例:</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="0753" class="jk jl hp kk b fi ko kp l kq kr">hduser@localhost:~$ hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar grep input output ‘dfs[a-z.]+’</span><span id="64eb" class="jk jl hp kk b fi ld kp l kq kr">2020–08–08 01:57:02,411 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties<br/> 2020–08–08 01:57:04,754 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).<br/> 2020–08–08 01:57:04,754 INFO impl.MetricsSystemImpl: JobTracker metrics system started<br/> 2020–08–08 01:57:08,843 INFO input.FileInputFormat: Total input files to process : 10<br/> …………..<br/> ………….<br/> …………<br/> File Input Format Counters<br/> Bytes Read=175<br/> File Output Format Counters<br/> Bytes Written=47</span></pre><p id="415d" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">最后，检查输出文件:检查来自分布式文件系统的输出文件，并检查它们:</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="9852" class="jk jl hp kk b fi ko kp l kq kr">hduser@localhost:~$ hdfs dfs <strong class="kk hq">-ls</strong> output/<br/>Found 2 items<br/>-rw-r — r — 1 hduser supergroup 0 2020–08–08 01:58 output/_SUCCESS<br/>-rw-r — r — 1 hduser supergroup 47 2020–08–08 01:58 output/part-r-00000</span></pre><p id="2114" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在分布式文件系统上使用<strong class="io hq"> cat </strong>命令检查结果:</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="826e" class="jk jl hp kk b fi ko kp l kq kr">hduser@localhost:~$ hdfs dfs <strong class="kk hq">-cat</strong> output/*<br/>1 dfsadmin<br/>1 dfs.replication<br/>1 dfs.permissions</span></pre><p id="c3cd" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">停止并重新启动集装箱</strong></p><p id="2316" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">要停止容器，请执行以下命令，以便感激地关闭。</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="19b4" class="jk jl hp kk b fi ko kp l kq kr">hduser@localhost:~$ stop-dfs.sh<br/>hduser@localhost:~$ stop-yarn.sh</span></pre><p id="c1ee" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">之后。</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="2178" class="jk jl hp kk b fi ko kp l kq kr">hduser@localhost:~$ exit</span></pre><p id="8bf2" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">要重新启动容器，并返回到我们的Hadoop环境，请执行:</p><pre class="kf kg kh ki fd kj kk kl km aw kn bi"><span id="8488" class="jk jl hp kk b fi ko kp l kq kr">$ docker start -i &lt;container-name&gt;</span></pre><p id="ce61" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">尽情享受吧！！！</p><p id="05cc" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">更新:</strong>如果你想了解MapReduce是如何工作的，你可以去<a class="ae ks" rel="noopener" href="/@rancavil/mapreduce-example-with-python-b435a9858718"> MapReduce与Python的例子</a>。</p></div></div>    
</body>
</html>