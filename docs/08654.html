<html>
<head>
<title>Detecting Potentially Hazardous Asteroids using Deep Learning (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习探测潜在危险的小行星(第二部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/detecting-potentially-hazardous-asteroids-using-deep-learning-part-2-b3bfd1e6774c?source=collection_archive---------11-----------------------#2020-08-07">https://medium.com/analytics-vidhya/detecting-potentially-hazardous-asteroids-using-deep-learning-part-2-b3bfd1e6774c?source=collection_archive---------11-----------------------#2020-08-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8118" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">见证深度学习在巨大数据集上的威力！</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/94ed693d4fb967e7d1b295c82a546f23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zfUVN0q6mPuQ07nP.jpg"/></div></div></figure><p id="1eb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章的第二部分，我们将使用keras框架制作我们的神经网络。</p><p id="c2cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于第1部分(使用传统分类器)，请访问以下链接:</p><div class="jp jq ez fb jr js"><a rel="noopener follow" target="_blank" href="/@jatin.kataria94/detecting-potentially-hazardous-asteroids-using-deep-learning-part-1-9873a0d97ac8"><div class="jt ab dw"><div class="ju ab jv cl cj jw"><h2 class="bd hj fi z dy jx ea eb jy ed ef hh bi translated">使用深度学习探测潜在危险的小行星(第1部分)</h2><div class="jz l"><h3 class="bd b fi z dy jx ea eb jy ed ef dx translated">见证深度学习在巨大数据集上的威力！</h3></div><div class="ka l"><p class="bd b fp z dy jx ea eb jy ed ef dx translated">medium.com</p></div></div><div class="kb l"><div class="kc l kd ke kf kb kg jn js"/></div></div></a></div><p id="8277" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有关源代码，请访问以下链接:</p><div class="jp jq ez fb jr js"><a href="https://github.com/jatinkataria94/Asteroid-Detection/blob/master/asteroid_ANN.py" rel="noopener  ugc nofollow" target="_blank"><div class="jt ab dw"><div class="ju ab jv cl cj jw"><h2 class="bd hj fi z dy jx ea eb jy ed ef hh bi translated">jatinkataria 94/小行星探测</h2><div class="ka l"><p class="bd b fp z dy jx ea eb jy ed ef dx translated">github.com</p></div></div><div class="kb l"><div class="kh l kd ke kf kb kg jn js"/></div></div></a></div><h1 id="9e31" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">数据处理</h1><p id="3af1" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">我们将使用第1部分中处理的数据。处理后的数据集由20个选定的特征组成，我们需要对小行星是否有潜在危险进行分类。目标变量分为两类。</p><h1 id="352e" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">导入包</h1><p id="ea82" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">Keras是一个构建神经网络的简单工具。它是一个基于tensorflow、theano或cntk后端的高级框架。</p><pre class="je jf jg jh fd ll lm ln lo aw lp bi"><span id="0820" class="lq kj hi lm b fi lr ls l lt lu">#Dependencies</span><span id="ec75" class="lq kj hi lm b fi lv ls l lt lu">import tensorflow as tf<br/>import keras<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Dropout<br/>from keras import optimizers<br/>from sklearn.metrics import  confusion_matrix,classification_report,matthews_corrcoef</span></pre><h1 id="8eef" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">构建神经网络</h1><p id="f67a" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">我们将定义一个包含我们的神经网络模型架构的函数，以便稍后可以轻松地调用它。</p><pre class="je jf jg jh fd ll lm ln lo aw lp bi"><span id="bd52" class="lq kj hi lm b fi lr ls l lt lu">def NNmodel(init_mode,act,opt,n_top_features=n_top_features):</span><span id="489c" class="lq kj hi lm b fi lv ls l lt lu">#Setting random.seed for numpy and tensorflow to ensure reproducible results<br/>np.random.seed(42)<br/>tf.random.set_seed(42)</span><span id="8a1e" class="lq kj hi lm b fi lv ls l lt lu"># building a linear stack of layers with the sequential model</span><span id="9f59" class="lq kj hi lm b fi lv ls l lt lu">model = Sequential()</span><span id="4869" class="lq kj hi lm b fi lv ls l lt lu"># hidden layer</span><span id="0f42" class="lq kj hi lm b fi lv ls l lt lu">model.add(Dense(16,input_dim=n_top_features, kernel_initializer=init_mode, activation=act))</span><span id="13a7" class="lq kj hi lm b fi lv ls l lt lu">model.add(Dropout(0.2))</span><span id="6004" class="lq kj hi lm b fi lv ls l lt lu">model.add(Dense(16, kernel_initializer=init_mode,activation=act))</span><span id="c6ae" class="lq kj hi lm b fi lv ls l lt lu">model.add(Dropout(0.2))</span><span id="c1fd" class="lq kj hi lm b fi lv ls l lt lu"># output layer</span><span id="9664" class="lq kj hi lm b fi lv ls l lt lu">model.add(Dense(1, activation='sigmoid'))</span><span id="1c89" class="lq kj hi lm b fi lv ls l lt lu"># compiling the sequential model</span><span id="ea46" class="lq kj hi lm b fi lv ls l lt lu">model.compile(loss='binary_crossentropy', metrics=['acc'], optimizer=opt)</span><span id="6b7a" class="lq kj hi lm b fi lv ls l lt lu">return model</span></pre><ul class=""><li id="60d4" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated"><strong class="ih hj"> kernel_initializer: </strong>神经网络需要从一些权重开始，然后迭代地将它们更新为更好的值。这一项是用于初始化权重的函数。(由NNmodel函数的<strong class="ih hj"> init_mode </strong>参数设置)</li><li id="1731" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated"><strong class="ih hj">激活函数:</strong>这是一个传递函数，用于将一层的输出映射到另一层。(由NNmodel functiom的<strong class="ih hj"> act </strong>参数设定)。对于输出层，激活可以是“sigmoid”(二进制类)或“softmax”(多类)。</li><li id="a656" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated"><strong class="ih hj">优化算法:</strong>它们通过响应损失函数的输出更新模型，将损失函数和模型参数联系在一起。他们通过使用权重将你的模型塑造成最精确的形式。(由NNmodel功能的<strong class="ih hj"> opt </strong>参数设定)</li><li id="7e1d" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated"><strong class="ih hj">损失:</strong>计算得到关于模型权重的梯度，并通过反向传播相应地更新这些权重。(' binary_crossentropy '用于二进制类；多类的“分类交叉熵”)</li></ul><p id="4f1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的数据集中，输入有20个维度(使用了20个顶级特征)，输出有2个值(二进制类)。因此，输入和输出层的维数分别为20和1(但是对于多类，输出层的维数等于类的数目)。在我们的神经网络中，我们使用两个16维的隐藏层。</p><p id="4ace" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注意</strong> -选择隐藏层和节点的数量很困难，但是有一些经验法则会有所帮助。</p><div class="jp jq ez fb jr js"><a href="https://stats.stackexchange.com/questions/181/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw" rel="noopener  ugc nofollow" target="_blank"><div class="jt ab dw"><div class="ju ab jv cl cj jw"><h2 class="bd hj fi z dy jx ea eb jy ed ef hh bi translated">前馈神经网络的隐层和节点数如何选择？</h2><div class="jz l"><h3 class="bd b fi z dy jx ea eb jy ed ef dx translated">begingroup $我意识到这个问题已经被回答了，但是我不认为现存的答案真正涉及到这个问题…</h3></div><div class="ka l"><p class="bd b fp z dy jx ea eb jy ed ef dx translated">stats.stackexchange.com</p></div></div><div class="kb l"><div class="mk l kd ke kf kb kg jn js"/></div></div></a></div><p id="ae36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好地理解神经网络，请参考这些全面解释该主题的文章:</p><div class="jp jq ez fb jr js"><a href="https://towardsdatascience.com/simple-introduction-to-neural-networks-ac1d7c3d7a2c" rel="noopener follow" target="_blank"><div class="jt ab dw"><div class="ju ab jv cl cj jw"><h2 class="bd hj fi z dy jx ea eb jy ed ef hh bi translated">神经网络简介</h2><div class="jz l"><h3 class="bd b fi z dy jx ea eb jy ed ef dx translated">神经网络的详细概述，有大量的例子和简单的图像。</h3></div><div class="ka l"><p class="bd b fp z dy jx ea eb jy ed ef dx translated">towardsdatascience.com</p></div></div><div class="kb l"><div class="ml l kd ke kf kb kg jn js"/></div></div></a></div><div class="jp jq ez fb jr js"><a href="https://towardsdatascience.com/comprehensive-introduction-to-neural-network-architecture-c08c6d8e5d98" rel="noopener follow" target="_blank"><div class="jt ab dw"><div class="ju ab jv cl cj jw"><h2 class="bd hj fi z dy jx ea eb jy ed ef hh bi translated">神经网络体系结构综合介绍</h2><div class="jz l"><h3 class="bd b fi z dy jx ea eb jy ed ef dx translated">神经架构、激活函数、损失函数、输出单元的详细概述。</h3></div><div class="ka l"><p class="bd b fp z dy jx ea eb jy ed ef dx translated">towardsdatascience.com</p></div></div><div class="kb l"><div class="mm l kd ke kf kb kg jn js"/></div></div></a></div><div class="jp jq ez fb jr js"><a href="https://towardsdatascience.com/neural-network-optimization-7ca72d4db3e0" rel="noopener follow" target="_blank"><div class="jt ab dw"><div class="ju ab jv cl cj jw"><h2 class="bd hj fi z dy jx ea eb jy ed ef hh bi translated">神经网络优化</h2><div class="jz l"><h3 class="bd b fi z dy jx ea eb jy ed ef dx translated">涵盖优化器，动量，自适应学习率，批量标准化，等等。</h3></div><div class="ka l"><p class="bd b fp z dy jx ea eb jy ed ef dx translated">towardsdatascience.com</p></div></div><div class="kb l"><div class="mn l kd ke kf kb kg jn js"/></div></div></a></div><h1 id="cf0e" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">Keras超参数调谐</h1><p id="2c79" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">可以通过Python中的scikit-learn库使用Keras的深度学习模型。这将允许您利用scikit-learn库的强大功能进行模型超参数优化。</p><p id="1799" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要使用<strong class="ih hj">顺序Keras模型</strong>执行随机搜索，您必须通过使用<strong class="ih hj"> Scikit-Learn API </strong>的<strong class="ih hj"> Keras包装器</strong>将这些模型转换成<strong class="ih hj"> sklearn兼容的估计器</strong>:【更多细节请参考<a class="ae mo" href="http://keras.io/scikit-learn-api" rel="noopener ugc nofollow" target="_blank">文档</a>。Keras库为深度学习模型提供了一个方便的包装器，可用作scikit-learn中的分类或回归估计器。</p><pre class="je jf jg jh fd ll lm ln lo aw lp bi"><span id="0617" class="lq kj hi lm b fi lr ls l lt lu">batch_size = 64<br/>epochs = 10</span><span id="a057" class="lq kj hi lm b fi lv ls l lt lu">from keras.wrappers.scikit_learn import KerasClassifier<br/>model_CV = KerasClassifier(build_fn=NNmodel, epochs=epochs, <br/>                           batch_size=batch_size, verbose=1)</span><span id="f3d8" class="lq kj hi lm b fi lv ls l lt lu"># define the search parameters<br/>init_mode = ['he_uniform','glorot_uniform']<br/>act=['relu','selu','tanh']<br/>opt=['rmsprop','adam']<br/>param_distributions={'init_mode':init_mode,'act':act,'opt':opt}</span><span id="6116" class="lq kj hi lm b fi lv ls l lt lu">#Use RandomizedSearchCV to tune hyperparameters<br/>rand = RandomizedSearchCV(estimator=model_CV, param_distributions=param_distributions, n_jobs=-1, cv=3,random_state=42,verbose=10)<br/>rand_result = rand.fit(X_train_sfs_scaled, y_train)</span><span id="e31f" class="lq kj hi lm b fi lv ls l lt lu"># print results<br/>print(f'Best Accuracy for {rand_result.best_score_} using {rand_result.best_params_}')</span><span id="6142" class="lq kj hi lm b fi lv ls l lt lu">#Store the best search results in NNmodel parameters<br/>init_mode=rand_result.best_params_['init_mode']<br/>act=rand_result.best_params_['act']<br/>opt=rand_result.best_params_['opt']</span></pre><p id="ffc3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们通过<strong class="ih hj"> build_fn </strong>参数将函数名(NNmodel)传递给KerasClassifier类。我们还传递了额外的参数<strong class="ih hj"> epochs=10 </strong>和<strong class="ih hj"> batch_size=64 </strong>。这些被自动打包并传递给由KerasClassifier类内部调用的<strong class="ih hj"> fit() </strong>函数。</p><p id="63e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们数据集的优化超参数结果是:<strong class="ih hj"> {init_mode='glorot_uniform '，act='tanh '，opt='adam'} </strong></p><h1 id="48dc" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">培训模式</h1><p id="9e18" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">我们将使用调整后的超参数，并使模型符合训练数据。</p><pre class="je jf jg jh fd ll lm ln lo aw lp bi"><span id="f73d" class="lq kj hi lm b fi lr ls l lt lu">labels=['Non-Hazardous','Hazardous']</span><span id="177a" class="lq kj hi lm b fi lv ls l lt lu">NNperformance(init_mode,act,opt,n_top_features,epochs,<br/>              batch_size,labels,X_train_sfs_scaled,<br/>              y_train,X_test_sfs_scaled, y_test)</span></pre><p id="4c76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们需要指定类标签并适应模型。模型训练的历史由每个时期之后的模型精度和损失组成，其可以被可视化为学习曲线以评估模型性能。</p><h2 id="2e4c" class="lq kj hi bd kk mp mq mr ko ms mt mu ks iq mv mw kw iu mx my la iy mz na le nb bi translated">模型性能</h2><p id="a9db" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">为了评估和可视化模型性能，我们创建了三个函数:</p><ul class=""><li id="5cfa" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated"><strong class="ih hj">学习曲线:</strong>在每个时期后绘制<strong class="ih hj"> </strong>模型精度和损失，以了解模型是否合适。</li><li id="fbeb" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated"><strong class="ih hj"> PerformanceReports: </strong>绘制混淆矩阵和分类报告，评估模型的召回率和准确率。</li><li id="ca8f" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated"><strong class="ih hj"> NNPerformance: </strong>将模型拟合到训练数据，进行预测，调用上述两个函数进行性能评估。</li></ul><pre class="je jf jg jh fd ll lm ln lo aw lp bi"><span id="d821" class="lq kj hi lm b fi lr ls l lt lu">def LearningCurve(history):</span><span id="58b8" class="lq kj hi lm b fi lv ls l lt lu"># summarize history for accuracy</span><span id="057e" class="lq kj hi lm b fi lv ls l lt lu">plt.subplot(211)<br/>plt.plot(history.history['acc'])<br/>plt.plot(history.history['val_acc'])<br/>plt.title('model accuracy')<br/>plt.ylabel('accuracy')<br/>plt.xlabel('epoch')<br/>plt.legend(['train', 'validation'], loc='upper left')<br/>plt.show()</span><span id="1651" class="lq kj hi lm b fi lv ls l lt lu"># summarize history for loss</span><span id="0148" class="lq kj hi lm b fi lv ls l lt lu">plt.subplot(212)<br/>plt.plot(history.history['loss'])<br/>plt.plot(history.history['val_loss'])<br/>plt.title('model loss')<br/>plt.ylabel('loss')<br/>plt.xlabel('epoch')<br/>plt.legend(['train', 'validation'], loc='upper left')<br/>plt.show()</span><span id="d433" class="lq kj hi lm b fi lv ls l lt lu">def PerformanceReports(conf_matrix,class_report,labels):</span><span id="a5cc" class="lq kj hi lm b fi lv ls l lt lu">ax= plt.subplot()<br/>sns.heatmap(conf_matrix, annot=True,ax=ax)<br/># labels, title and ticks<br/>ax.set_xlabel('Predicted labels')<br/>ax.set_ylabel('True labels')<br/>ax.set_title('Confusion Matrix')<br/>ax.xaxis.set_ticklabels(labels)<br/>ax.yaxis.set_ticklabels(labels[::-1])<br/>plt.show()<br/>sns.heatmap(pd.DataFrame(class_report).iloc[:-1, :].T, annot=True)<br/>plt.show()</span><span id="456d" class="lq kj hi lm b fi lv ls l lt lu">def NNperformance(init_mode,act,opt,n_top_features,epochs,<br/>                  batch_size,labels,X_train_sfs_scaled, <br/>                  y_train,X_test_sfs_scaled, y_test):</span><span id="c544" class="lq kj hi lm b fi lv ls l lt lu">    np.random.seed(42)  <br/>    tf.random.set_seed(42)<br/>    <br/>    #fit the keras model on the dataset<br/>    start_time = timeit.default_timer()<br/>    <br/>    model=NNmodel(init_mode,act,opt,n_top_features)<br/>    history=model.fit(X_train_sfs_scaled, y_train, epochs=epochs,   <br/>                      batch_size=batch_size,validation_data=<br/>                      (X_test_sfs_scaled, y_test),shuffle=True)</span><span id="187c" class="lq kj hi lm b fi lv ls l lt lu">    scores_train = model.evaluate(X_train_sfs_scaled, y_train)<br/>    scores_test = model.evaluate(X_test_sfs_scaled, y_test)<br/>    print('Train Accuracy: %.2f' % (scores_train[1]*100))<br/>    print('Test Accuracy: %.2f' % (scores_test[1]*100))<br/>    <br/>    # make class predictions with the model<br/>    y_pred = model.predict_classes(X_test_sfs_scaled)<br/>    <br/>    cm=confusion_matrix(y_test,y_pred)<br/>    cr=classification_report(y_test, y_pred, target_names=labels,<br/>                             output_dict=True)<br/>    mcc= matthews_corrcoef(y_test, y_pred)<br/>    print('Matthews Correlation Coefficient: ',mcc)</span><span id="ddf3" class="lq kj hi lm b fi lv ls l lt lu">    PerformanceReports(cm,cr,labels)</span><span id="4e1e" class="lq kj hi lm b fi lv ls l lt lu">    LearningCurve(history)         <br/>            <br/>    elapsed = timeit.default_timer() - start_time<br/>    print('Execution Time for deep learning model: %.2f minutes'%<br/>           (elapsed/60))</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nc"><img src="../Images/722cc9a99a273fba37e9e62c81a35123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*-gPi4xbhghnU7qjPYD1dRQ.png"/></div></figure><div class="je jf jg jh fd ab cb"><figure class="nd ji ne nf ng nh ni paragraph-image"><img src="../Images/b3b70af423c2c54672e1f3e87f5b4613.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*Ue-1CXzsiR-CZxZ1BMWHgA.png"/></figure><figure class="nd ji nj nf ng nh ni paragraph-image"><img src="../Images/10bc4a76041ea489129b86cf2f14c590.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*ZhuUu1K3RiCx2HUOS0zDLw.png"/></figure></div><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nk"><img src="../Images/4df037047bac61c171de732c8d16dc0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*fWbp9tEAMT7i8UX_r2Djhw.png"/></div></figure><ul class=""><li id="6f88" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated">查看学习曲线，该模型是很好的拟合，因为训练和测试准确度和损失的曲线彼此接近。</li><li id="fade" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated"><strong class="ih hj">精度— 64% </strong></li><li id="648d" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated"><strong class="ih hj">召回率— 74% </strong></li></ul><p id="55bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从模型性能图中可以看出，尽管精确度很高，但对危险小行星的精确度和召回率却很低。这可以解释为，<strong class="ih hj">数据是不平衡的</strong>，因为小行星的危险等级仅占目标变量的<strong class="ih hj"> 0.22% </strong>。(摘自第1部分)</p><h1 id="de89" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">纠正阶级不平衡</h1><p id="c7ee" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated">我们可以使用各种技术，如SMOTE、未遂事件、随机抽样等来处理类不平衡。我们使用了BorderlineSMOTE来解决这个问题，但您可以尝试这些不同的整流器，看看有什么变化。</p><pre class="je jf jg jh fd ll lm ln lo aw lp bi"><span id="d6ba" class="lq kj hi lm b fi lr ls l lt lu">from imblearn.over_sampling import SMOTE,RandomOverSampler,BorderlineSMOTE<br/>from imblearn.under_sampling import NearMiss,RandomUnderSampler</span><span id="e6ce" class="lq kj hi lm b fi lv ls l lt lu">smt = SMOTE()<br/>nr = NearMiss()<br/>bsmt=BorderlineSMOTE(random_state=42)<br/>ros=RandomOverSampler(random_state=42)<br/>rus=RandomUnderSampler(random_state=42)</span><span id="fa58" class="lq kj hi lm b fi lv ls l lt lu">X_train_bal, y_train_bal = bsmt.fit_sample(X_train_sfs_scaled, y_train)<br/><br/>  <br/>NNperformance(init_mode,act,opt,n_top_features,epochs,<br/>              batch_size,labels,X_train_bal,<br/>              y_train_bal,X_test_sfs_scaled, y_test)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nl"><img src="../Images/99935d4434edbd5004a89240fc7ec755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uvvpn_1Eq4eqsrw0xEewcA.png"/></div></div></figure><div class="je jf jg jh fd ab cb"><figure class="nd ji nm nf ng nh ni paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><img src="../Images/51fa26532d01c0996696429e6597b876.png" data-original-src="https://miro.medium.com/v2/resize:fit:912/format:webp/1*sITmcuYN4exCRIcbe9_jIA.png"/></div></figure><figure class="nd ji nn nf ng nh ni paragraph-image"><img src="../Images/9e2936d27945bcba590f78d7a59d53e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*f7RiTTg2y4Ov6VwLHPZcyg.png"/></figure></div><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es no"><img src="../Images/3277bcd807689f0b2effa252bc897c4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*z0kcOrTX2_ObJpzg5ZQcvA.png"/></div></figure><ul class=""><li id="9930" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated">查看学习曲线，该模型是很好的拟合，因为训练和测试准确度和损失的曲线彼此接近。</li><li id="fd94" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated"><strong class="ih hj">精度— 64% </strong></li><li id="897c" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated"><strong class="ih hj">召回率— 100%(比74%有所提高)</strong></li></ul><p id="31a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们将平衡神经网络模型的性能与平衡传统分类器的性能进行比较→</p><p id="51da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">来自第1部分(传统分类器)</p><p id="ca7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(<a class="ae mo" rel="noopener" href="/@jatin.kataria94/detecting-potentially-hazardous-asteroids-using-deep-learning-part-1-9873a0d97ac8">https://medium . com/@ jatin . kataria 94/detecting-potential-hazardous-asteroids-using-deep-learning-part-1-9873 a 0d 97 AC 8</a>)</p><ul class=""><li id="dd8b" class="lw lx hi ih b ii ij im in iq ly iu lz iy ma jc mb mc md me bi translated"><strong class="ih hj">精度— 23.2% </strong></li><li id="918c" class="lw lx hi ih b ii mf im mg iq mh iu mi iy mj jc mb mc md me bi translated"><strong class="ih hj">召回— 100% </strong></li></ul><p id="1a01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我们可以清楚地看到，神经网络的精度(64%)比传统分类器的精度(23.2%)高得多。这显示了深度学习在处理庞大数据集时的威力。</strong></p><h1 id="a9af" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">在你走之前</h1><p id="4c7c" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated"><strong class="ih hj"> <em class="np">感谢</em> </strong>的阅读！请随意将这种方法应用到您的分类问题中。如果你有任何困难或疑问，请在下面评论。非常感谢你的支持。如果你想和我联系，打jatin.kataria94@gmail.com找我。</p></div></div>    
</body>
</html>