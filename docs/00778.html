<html>
<head>
<title>Prediction Intervals in Forecasting: Quantile Loss Function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测中的预测区间:分位数损失函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/prediction-intervals-in-forecasting-quantile-loss-function-18f72501586f?source=collection_archive---------1-----------------------#2019-09-05">https://medium.com/analytics-vidhya/prediction-intervals-in-forecasting-quantile-loss-function-18f72501586f?source=collection_archive---------1-----------------------#2019-09-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/f6a5ce13862c2d9975455a1d046aa15a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X5GFQ8CRyL6BaLrY4VSl3w.png"/></div></div></figure><div class=""/><p id="0d09" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在大多数现实世界的预测问题中，我们预测中的不确定性提供了重要的价值。了解预测的范围，而不仅仅是点估计，可以显著改善许多商业应用的决策过程。</p><h1 id="f054" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">什么是预测区间？</h1><p id="af27" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">预测区间是对预测不确定性的量化。它提供了一个结果变量估计的概率上限和下限。</p><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es kr"><img src="../Images/2dba2939f5472f1f59c162933872acb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/0*epNbbq2VJbIUn_LB.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">预测值、实际值和预测区间的关系。<br/>摘自《模型输出预测区间估计的机器学习方法》，2006年。</figcaption></figure><p id="2d43" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">尽管大多数模型输出是精确的，并且接近观察值，但是输出本身是随机变量，因此具有分布。为了了解我们的结果正确的可能性，预测区间是必要的。这种可能性决定了可能值的区间。</p><h1 id="0552" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">分位数回归损失函数</h1><p id="fe8c" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">机器学习模型通过最小化(或最大化)目标函数来工作。一个目标函数将我们试图解决的问题转化为一个数学公式，通过模型最小化。顾名思义，分位数回归损失函数用于预测分位数。分位数是一个值，低于这个值，一组中的一部分观察值就会下降。例如，分位数为0.9的预测在90%的情况下会过度预测。</p><blockquote class="la lb lc"><p id="011f" class="iq ir ld is b it iu iv iw ix iy iz ja le jc jd je lf jg jh ji lg jk jl jm jn hb bi translated">给定一个预测<em class="ht"> yi^p </em>和结果<em class="ht">易</em>，分位数<em class="ht"> q </em>的平均回归损失为</p></blockquote><figure class="ks kt ku kv fd hk er es paragraph-image"><div class="er es lh"><img src="../Images/87b9b99c73c11fa1114067fdcc169a40.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*fMTBPWvbyk9dG2oh8cnnTA.png"/></div></figure><p id="5fc5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于一组预测，损失将是其平均值。</p><p id="0ff6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">直观理解</strong></p><p id="a1c4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上面的回归损失方程中，由于<em class="ld"> q </em>的值在0和1之间，所以第一项在过预测时将为正并占优势，<em class="ld"> yip </em> &gt; <em class="ld"> yi </em>，第二项在欠预测时将占优势，<em class="ld"> yip </em> &lt; <em class="ld"> yi </em>。对于等于0.5的<em class="ld"> q </em>，欠预测和过预测将受到相同因子的惩罚，并获得中值。<em class="ld"> q </em>的值越大，与预测不足相比，过度预测受到的惩罚越多。对于等于0.75的<em class="ld"> q </em>，过预测将被罚因子0.75，欠预测将被罚因子0.25。然后，该模型将尝试避免过度预测，避免过度预测的难度大约是预测不足的三倍，并且将获得0.75分位数。</p><p id="12b3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">为什么要用分位数损失？</strong></p><p id="ed14" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最小二乘回归的预测区间基于残差(y-y _ hat)在独立变量值之间具有恒定方差的假设。我们不能相信违背这一假设的线性回归模型。我们也不能说使用非线性函数或基于树的模型可以更好地模拟这种情况，从而抛弃将线性回归模型作为基线的想法。这就是分位数损失和分位数回归的用武之地，因为基于分位数损失的回归提供了合理的预测区间，即使残差具有非常数方差或非正态分布。</p><h1 id="9d4d" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">示例:梯度推进回归的预测区间</h1><p id="a6de" class="pw-post-body-paragraph iq ir ht is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">该损失函数可用于计算神经网络或基于树的模型中的预测区间。</p><p id="9f74" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面的示例展示了如何使用GradientBoostingRegressor的scikit-learn实现使用分位数回归来创建预测区间。在下面的例子中，我们预测加入噪声后的函数f(x)=xcos(x)。这个例子大量借用了<a class="ae li" href="http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_quantile.html" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/auto _ examples/ensemble/plot _ gradient _ boosting _ quantile . html</a>的内容，因此，我只展示了y的上下分位数的分位数损失的实现。</p><pre class="ks kt ku kv fd lj lk ll lm aw ln bi"><span id="afba" class="lo jp ht lk b fi lp lq l lr ls">alpha = 0.95</span><span id="c610" class="lo jp ht lk b fi lt lq l lr ls">clf = GradientBoostingRegressor(loss='quantile', alpha=alpha,<br/>                                n_estimators=250, max_depth=3,<br/>                                learning_rate=.1, min_samples_leaf=9,<br/>                                min_samples_split=9)</span><span id="38fa" class="lo jp ht lk b fi lt lq l lr ls">clf.fit(X, y)</span><span id="c22b" class="lo jp ht lk b fi lt lq l lr ls"># Make the prediction on the meshed x-axis<br/>y_upper = clf.predict(xx)</span><span id="aadd" class="lo jp ht lk b fi lt lq l lr ls">clf.set_params(alpha=1.0 - alpha)<br/>clf.fit(X, y)</span><span id="faff" class="lo jp ht lk b fi lt lq l lr ls"># Make the prediction on the meshed x-axis<br/>y_lower = clf.predict(xx)</span><span id="f57b" class="lo jp ht lk b fi lt lq l lr ls">clf.set_params(loss='ls')<br/>clf.fit(X, y)</span><span id="6bde" class="lo jp ht lk b fi lt lq l lr ls"># Make the prediction on the meshed x-axis<br/>y_pred = clf.predict(xx)</span></pre><figure class="ks kt ku kv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lu"><img src="../Images/f7ca918d61944156c6418d21cbd37ee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y71z4Oe2fjHvFpp1Q6Kzug.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">对f(x)=xcos(x)使用分位数损失(梯度推进回归量)的预测区间</figcaption></figure><p id="b0c4" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上图显示了使用分位数损失计算的90%预测区间，其中上限在q = 0.95处构建，下限使用q = 0.05构建。</p><p id="edff" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">参考文献</strong></p><ol class=""><li id="5c84" class="lv lw ht is b it iu ix iy jb lx jf ly jj lz jn ma mb mc md bi translated"><a class="ae li" href="https://towardsdatascience.com/regression-prediction-intervals-with-xgboost-428e0a018b" rel="noopener" target="_blank">https://towards data science . com/regression-prediction-intervals-with-xgboost-428 e0a 018 b</a></li><li id="1232" class="lv lw ht is b it me ix mf jb mg jf mh jj mi jn ma mb mc md bi translated"><a class="ae li" href="https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0" rel="noopener ugc nofollow" target="_blank">https://heart beat . fritz . ai/5-regression-loss-functions-all-machine-learners-should-know-4fb 140 e 9 D4 b 0</a></li><li id="e7f8" class="lv lw ht is b it me ix mf jb mg jf mh jj mi jn ma mb mc md bi translated"><a class="ae li" href="https://www.evergreeninnovations.co/blog-quantile-loss-function-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://www . evergreenwinnovations . co/blog-quantile-loss-function-for-machine-learning/</a></li></ol></div></div>    
</body>
</html>