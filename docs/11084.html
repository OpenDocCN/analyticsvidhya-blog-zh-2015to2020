<html>
<head>
<title>Your Guide for Logistic Regression with Titanic Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">泰坦尼克号数据集的逻辑回归指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/your-guide-for-logistic-regression-with-titanic-dataset-784943523994?source=collection_archive---------1-----------------------#2020-11-17">https://medium.com/analytics-vidhya/your-guide-for-logistic-regression-with-titanic-dataset-784943523994?source=collection_archive---------1-----------------------#2020-11-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/fd3049db9cc111a2899996b36ef9b9ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*M2o_Ja8E2vRJ5u2O"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">朱莉·科索拉波娃在<a class="ae hv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><div class=""/><p id="aac7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated"><span class="l ju jv jw bm jx jy jz ka kb di"> N </span> ext我们要说的机器学习算法是<strong class="ix hz"> <em class="kc"> logistic回归</em> </strong>(也叫Sigmoid函数)。当我写下的时候，我会学到更多，像你一样。所以，我们来一趟逻辑回归，多标记一个算法为<strong class="ix hz"> DONE </strong>！</p></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><blockquote class="kk kl km"><p id="a434" class="iv iw kc ix b iy iz ja jb jc jd je jf kn jh ji jj ko jl jm jn kp jp jq jr js hb bi translated">在统计学中，逻辑模型(或logit模型)用于模拟某个类别或事件存在的概率，如通过/失败、赢/输、活着/死了或健康/生病。</p></blockquote><p id="5272" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">维基百科是这样解释的。看起来很简单，对吧？这个模型的目的是给我们0或1。该模型预测一个介于0和1之间的值，该值显示了这种情况的概率。</p><p id="8fa1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> <em class="kc">逻辑回归不预测连续值。逻辑回归预测某事是真是假</em> </strong>。</p><p id="357f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们来看一个例子。事实上，这是一个非常著名的例子。泰坦尼克号数据集。你有不止一个特征，通过逻辑回归你可以预测他们是否死亡。如果模型预测的值是0.79，这意味着这个人79%活着，21%死了。</p><p id="3757" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">当概率大于等于0.5时，二进制值为1，当概率小于0.5时，二进制值为0。所以，我上面提到的人将被归类为1，活着。模型返回1(真)。</strong></p></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><p id="f217" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">逻辑回归图看起来像介于0和1之间的“S ”,如下所示:</p><figure class="kr ks kt ku fd hk er es paragraph-image"><div class="er es kq"><img src="../Images/6e1c3252ae4b424d53d0e9e3dcea2472.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*pd_Iaq4P8llKi_3iloNhEA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><a class="ae hv" href="https://machinelearningmastery.com/logistic-regression-for-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/logistic-regression-for-machine-learning/</a></figcaption></figure><p id="a819" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">曲线表示一个案件的概率</strong>。</p><p id="8733" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">看起来类似于线性回归，但事实上，它不是。线性回归是一种预测算法。另一方面，逻辑回归是一种分类算法。线性回归算法使用最小二乘法来拟合数据的最佳直线，但逻辑回归不能使用该方法。所以，它需要另一个。逻辑回归使用<strong class="ix hz"> <em class="kc">【最大似然】</em> </strong>来拟合数据的最佳直线。</p><h2 id="3782" class="kv kw hy bd kx ky kz la lb lc ld le lf jg lg lh li jk lj lk ll jo lm ln lo lp bi translated">什么是最大可能性？！</h2><blockquote class="kk kl km"><p id="e8f6" class="iv iw kc ix b iy iz ja jb jc jd je jf kn jh ji jj ko jl jm jn kp jp jq jr js hb bi translated">最大似然估计涉及将问题视为优化或搜索问题，其中我们寻求一组参数，这些参数导致最适合数据样本的联合概率(<em class="hy"> X </em>)。</p></blockquote><p id="7c96" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我会保持简单。我不会用华丽的词语来形容它。我会按照我的理解来写。它在泰坦尼克号数据集上计算一个人活着的可能性，然后另一个，然后另一个，在所有计算结束后，模型乘以所有这些可能性，<em class="kc">将S形线拟合到数据</em>。<strong class="ix hz">不断计算，直到找到最佳的S形线。</strong></p><p id="65e7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">想了解更多可以看看这篇文章。</p><div class="hh hi ez fb hj lq"><a href="https://machinelearningmastery.com/what-is-maximum-likelihood-estimation-in-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="lr ab dw"><div class="ls ab lt cl cj lu"><h2 class="bd hz fi z dy lv ea eb lw ed ef hx bi translated">机器学习的最大似然估计的温和介绍-机器学习…</h2><div class="lx l"><h3 class="bd b fi z dy lv ea eb lw ed ef dx translated">密度估计是从一个样本中估计一个观察值的概率分布的问题</h3></div><div class="ly l"><p class="bd b fp z dy lv ea eb lw ed ef dx translated">machinelearningmastery.com</p></div></div><div class="lz l"><div class="ma l mb mc md lz me hp lq"/></div></div></a></div><p id="4bc3" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">或者，你可以看这个视频。</p><figure class="kr ks kt ku fd hk"><div class="bz dy l di"><div class="mf mg l"/></div></figure></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><p id="afcf" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们将在Titanic数据集上实现逻辑回归。我们也不确定这种算法是否是该数据集的最佳匹配，但我们会一起找出答案。</p><p id="5fc4" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据集可通过Seaborn图书馆获得。如果您还没有安装Seaborn库，您可以在命令行中安装:</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="1653" class="kv kw hy mi b fi mm mn l mo mp">pip3 install seaborn <strong class="mi hz"># check Seaborn documentations for details</strong></span></pre><p id="d886" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们可以导入我们的库:</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="ef56" class="kv kw hy mi b fi mm mn l mo mp">import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from seaborn import load_dataset <strong class="mi hz"># this method will help us to #download the Titanic dataset</strong><br/>%matplotlib inline <strong class="mi hz"># if you use jupyter notebook</strong><br/>plt.style.use('ggplot') <strong class="mi hz"># check for more with plt.style.available</strong></span></pre><p id="b7b6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正在下载数据集:</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="d930" class="kv kw hy mi b fi mm mn l mo mp">data = load_dataset("titanic")<br/>data</span></pre><figure class="kr ks kt ku fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mq"><img src="../Images/1ac266ca37c0fcaad26b6a72621e80fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PuIETDlJseugo4BJ0nwwFw.png"/></div></div></figure><p id="92af" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的代码之后，您必须看到类似这样的内容。如果一切正常，你应该知道这不仅仅是一篇文章，我们将只使用逻辑回归。我们首先清理数据，然后将其可视化，然后将实现逻辑回归。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="3e55" class="kv kw hy mi b fi mm mn l mo mp">data.info()</span></pre><figure class="kr ks kt ku fd hk er es paragraph-image"><div class="er es mr"><img src="../Images/4071ecd99062eebeecd5737cff20093d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1064/format:webp/1*NuVvqo0oD_6gQM0IqqoPvQ.png"/></div></figure><p id="5fd0" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如您所见，我们在“年龄”、“上船”、“甲板”、“上船_城镇”列中有空值。我们将放弃其中一些，并处理其余的。</p><p id="51d7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">某些列具有相同的值，但值类型或名称不同。比如‘谁’、‘性’、‘成人_男性’。我不想他们出现在我的模型里。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="8337" class="kv kw hy mi b fi mm mn l mo mp">columns = ['alive', 'alone', 'embark_town', 'who', 'adult_male', 'deck']<br/>data_2 = data.drop(columns, axis=1)</span></pre><p id="d933" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">“列”列表包含我要从数据集中删除的名称。“drop”方法将删除它们。axis=1，意味着我们希望as列被删除。</p><p id="3b82" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> <em class="kc">我把我的新数据集赋给另一个变量。如果需要，可以使用“inplace = True”对原始数据进行永久更改。</em> </strong></p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="230d" class="kv kw hy mi b fi mm mn l mo mp">data_2.describe(include='all').T</span></pre><figure class="kr ks kt ku fd hk er es paragraph-image"><div class="er es ms"><img src="../Images/4242c36af5004d32b5cd045b84e0f484.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*SISU4oPkF_sYrsb-HyRwOA.png"/></div></figure><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="7df5" class="kv kw hy mi b fi mm mn l mo mp">print(f"Max value of age column : {data_2['age'].max()}")<br/>print(f"Min value of age column : {data_2['age'].min()}")<br/>&gt;&gt; Max value of age column : 80.0<br/>&gt;&gt; Min value of age column : 0.42</span></pre><p id="cf9e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以用0到80之间的值对年龄列进行分类。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="67d4" class="kv kw hy mi b fi mm mn l mo mp">bins = [0, 5, 17, 25, 50, 80]<br/>labels = ['Infant', 'Kid', 'Young', 'Adult', 'Old']<br/>data_2['age'] = pd.cut(data_2['age'], bins = bins, labels=labels)</span></pre><p id="fe66" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">熊猫的‘切’法会让我们做出自己的分类。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="da9b" class="kv kw hy mi b fi mm mn l mo mp">pd.DataFrame(data_2['age'].value_counts())</span></pre><figure class="kr ks kt ku fd hk er es paragraph-image"><div class="er es mt"><img src="../Images/2012bc97dbc5e323d340ab2e38cafaff.png" data-original-src="https://miro.medium.com/v2/resize:fit:188/format:webp/1*r2n5P0szVeUo1ab3Fo4zGw.png"/></div></figure><p id="c5fe" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Wola！我们可以看到成年人占大多数。</p><p id="35e7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">“年龄”列中仍有空值。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="ecb4" class="kv kw hy mi b fi mm mn l mo mp">data_3['age'].mode()[0]<br/>&gt;&gt; 'Adult'</span></pre><p id="5762" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以用该列的模式填充空值。这是一个选择，这就是我要做的！</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="6661" class="kv kw hy mi b fi mm mn l mo mp">data_4 = data_3.fillna({'age' : data_3['age'].mode()[0]})</span></pre><p id="1ed0" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们已经完成了“年龄”一栏。是啊！“登上”专栏的时间到了。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="6b26" class="kv kw hy mi b fi mm mn l mo mp">data_2['embarked'].unique()<br/>&gt;&gt; array(['S', 'C', 'Q', nan], dtype=object)</span></pre><p id="6255" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们的‘上船’栏目显然包含‘S、C、Q’和‘nan’。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="ffb6" class="kv kw hy mi b fi mm mn l mo mp">print(f"How many 'S' on embarked column : {data_2[data_2['embarked'] == 'S'].shape[0]}")<br/>print(f"How many 'C' on embarked column : {data_2[data_2['embarked'] == 'C'].shape[0]}")<br/>print(f"How many 'Q' on embarked column : {data_2[data_2['embarked'] == 'Q'].shape[0]}")<br/>&gt;&gt; How many 'S' on embarked column : 644<br/>&gt;&gt; How many 'C' on embarked column : 168<br/>&gt;&gt; How many 'Q' on embarked column : 77</span></pre><p id="764d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">看起来我们可以使用这个列的模式来填充nan值。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="18e2" class="kv kw hy mi b fi mm mn l mo mp">data_3 = data_2.fillna({'embarked' : 'S'})</span><span id="9660" class="kv kw hy mi b fi mu mn l mo mp">data_4[['pclass', 'survived']].groupby(['pclass']).sum().sort_values(by='survived')</span></pre><figure class="kr ks kt ku fd hk er es paragraph-image"><div class="er es mv"><img src="../Images/ec45a93c375e18bf93e39499c627ae8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:244/format:webp/1*_pAfKUhj74_eMi9gp9QCmw.png"/></div></figure><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="46e5" class="kv kw hy mi b fi mm mn l mo mp">data_4[['sex', 'survived']].groupby(['sex']).sum().sort_values(by='survived')</span></pre><figure class="kr ks kt ku fd hk er es paragraph-image"><div class="er es mw"><img src="../Images/9feaad92179c6b3bdd60341d0796e4b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/format:webp/1*P3eabAC54LD1LSIdD-2iXQ.png"/></div></figure><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="7896" class="kv kw hy mi b fi mm mn l mo mp">bins = [-1, 7.9104, 14.4542, 31, 512.330]<br/>labels = [’low’, 'medium-low’, 'medium’, 'high’]<br/>data_4[’fare’] = pd.cut(data_4["fare"], bins = bins, labels = labels)</span></pre><p id="efc6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们也可以对票价列进行分类，就像上面的代码一样。</p><figure class="kr ks kt ku fd hk er es paragraph-image"><div class="er es mx"><img src="../Images/7d8719cf0d45c2f4a58453a2f8d74be7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*pSw1j3ww2wZwdqVsAMHheA.png"/></div></figure><p id="4aa1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们数据集的最终版本。</p><p id="36b2" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们也应该删除“class ”,因为它与pclass相同，而pclass已经是数字。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="113a" class="kv kw hy mi b fi mm mn l mo mp">data_5 = data_4.drop('class', axis=1)</span><span id="078e" class="kv kw hy mi b fi mu mn l mo mp">sns.distplot(data_5['survived'])</span></pre><figure class="kr ks kt ku fd hk er es paragraph-image"><div class="er es my"><img src="../Images/d58d6f918b830a7249d2e1069c851178.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*lfwhKtlxntEG0YULEsgfOA.png"/></div></figure><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="5e13" class="kv kw hy mi b fi mm mn l mo mp">plt.figure(figsize=(20, 10))<br/>plt.subplot(321)<br/>sns.barplot(x = 'sibsp', y = 'survived', data = data_5)<br/>plt.subplot(322)<br/>sns.barplot(x = 'fare', y = 'survived', data = data_5)<br/>plt.subplot(323)<br/>sns.barplot(x = 'pclass', y = 'survived', data = data_5)<br/>plt.subplot(324)<br/>sns.barplot(x = 'age', y = 'survived', data = data_5)<br/>plt.subplot(325)<br/>sns.barplot(x = 'sex', y = 'survived', data = data_5)<br/>plt.subplot(326)<br/>sns.barplot(x = 'embarked', y = 'survived', data = data_5);</span></pre><figure class="kr ks kt ku fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mz"><img src="../Images/e6a8dd0dce65c1c2efdef95e098709ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W_ITr5xSM6HSi87YNJ51VA.png"/></div></div></figure><p id="487b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，机器学习模型<strong class="ix hz">讨厌</strong>非数值。我们不能把它们放到我们的训练和测试数据中。我们需要将它们转换成数值。你有两个选择:标签编码器，和熊猫get_dummies方法。这个我就不细说了。我将使用get_dummies。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="7b7f" class="kv kw hy mi b fi mm mn l mo mp">dummies = ['fare', 'age', 'embarked', 'sex']<br/>dummy_data = pd.get_dummies(data_5[dummies])</span></pre><p id="4147" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们把它打开一点。“dummies”包含我们要转换成数值的列名。它们中的每个变量都将成为一列，它们的缺失将被定义为0，和1，不管它们对那个乘客是否为真。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="31a8" class="kv kw hy mi b fi mm mn l mo mp">dummy_data.shape<br/>&gt;&gt; (891, 10)</span></pre><p id="792d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将连接两个数据框，并删除旧的列。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="ee89" class="kv kw hy mi b fi mm mn l mo mp">data_6 = pd.concat([data_5, dummy_data], axis = 1)<br/>data_6.drop(dummies, axis=1, inplace=<strong class="mi hz">True</strong>)</span></pre><p id="977f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们有891行，18列。我们准备好建立我们的模型。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="beb8" class="kv kw hy mi b fi mm mn l mo mp">from sklearn.model_selection import train_test_split<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.metrics import accuracy_score, confusion_matrix</span></pre><p id="9726" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为我们为我们的模型导入了必要的库，所以我们准备好了。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="60ec" class="kv kw hy mi b fi mm mn l mo mp">X = data_6.drop('survived', axis = 1)<br/>y = data_6['survived']<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)</span><span id="5ebe" class="kv kw hy mi b fi mu mn l mo mp"><strong class="mi hz"># X contains independent values, y contains dependent value</strong></span></pre><p id="ede1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">模型构建:</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="bfd1" class="kv kw hy mi b fi mm mn l mo mp">log_reg = LogisticRegression()<br/>log_reg.fit(X_train, y_train)<br/>y_pred = log_reg.predict(X_test)<br/>y_pred</span></pre><p id="a39d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">y_pred长这样:</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="5b21" class="kv kw hy mi b fi mm mn l mo mp">array([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,<br/>       0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,<br/>       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,<br/>       1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0,<br/>       1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,<br/>       0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,<br/>       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0,<br/>       1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,<br/>       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,<br/>       0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,<br/>       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,<br/>       0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,<br/>       0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0,<br/>       0, 0, 0, 0, 0, 0, 1, 1, 1])</span></pre><p id="15c7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以检查我们的模型的准确性分数。</p><pre class="kr ks kt ku fd mh mi mj mk aw ml bi"><span id="4969" class="kv kw hy mi b fi mm mn l mo mp">accuracy_score(y_pred, y_test)<br/>&gt;&gt; 0.8067796610169492<br/>confusion_matrix(y_pred, y_test)<br/>&gt;&gt; array([[158,  31],<br/>          [ 26,  80]])</span><span id="116f" class="kv kw hy mi b fi mu mn l mo mp"><strong class="mi hz"># 31 + 26 = 57 wrong prediction</strong></span></pre><p id="523a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">说实话，不好看。我们可以做得更好。我们可以为这个数据集尝试不同的模型。但是这篇文章是关于逻辑回归的。就像我之前说的，我们不知道逻辑回归是否是泰坦尼克号数据集的正确选择。如果你已经做了很长时间，你可能会。但我没有。</p><p id="8850" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">好消息是，现在您知道了逻辑回归算法，以及如何实现它！</strong></p></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><div class="kr ks kt ku fd lq"><a href="https://machinelearningmastery.com/what-is-maximum-likelihood-estimation-in-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="lr ab dw"><div class="ls ab lt cl cj lu"><h2 class="bd hz fi z dy lv ea eb lw ed ef hx bi translated">机器学习的最大似然估计的温和介绍-机器学习…</h2><div class="lx l"><h3 class="bd b fi z dy lv ea eb lw ed ef dx translated">密度估计是从一个样本中估计一个观察值的概率分布的问题</h3></div><div class="ly l"><p class="bd b fp z dy lv ea eb lw ed ef dx translated">machinelearningmastery.com</p></div></div><div class="lz l"><div class="ma l mb mc md lz me hp lq"/></div></div></a></div><figure class="kr ks kt ku fd hk"><div class="bz dy l di"><div class="mf mg l"/></div></figure><div class="hh hi ez fb hj lq"><a href="https://machinelearningmastery.com/logistic-regression-for-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="lr ab dw"><div class="ls ab lt cl cj lu"><h2 class="bd hz fi z dy lv ea eb lw ed ef hx bi translated">机器学习的逻辑回归-机器学习掌握</h2><div class="lx l"><h3 class="bd b fi z dy lv ea eb lw ed ef dx translated">逻辑回归是机器学习从统计学领域借用的另一种技术。它是最受欢迎的…</h3></div><div class="ly l"><p class="bd b fp z dy lv ea eb lw ed ef dx translated">machinelearningmastery.com</p></div></div><div class="lz l"><div class="na l mb mc md lz me hp lq"/></div></div></a></div></div></div>    
</body>
</html>