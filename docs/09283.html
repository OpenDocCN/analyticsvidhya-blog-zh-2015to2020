<html>
<head>
<title>A Simple Introduction to Sequence to Sequence Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">序列对序列模型的简单介绍</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-simple-introduction-to-sequence-to-sequence-models-b34ebdf113a5?source=collection_archive---------15-----------------------#2020-08-31">https://medium.com/analytics-vidhya/a-simple-introduction-to-sequence-to-sequence-models-b34ebdf113a5?source=collection_archive---------15-----------------------#2020-08-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/532f939f75f8ff676007c66b7ad95a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*yYo4QF_FMBSGW5GsZBRfsw.gif"/></div></div></figure><h1 id="eabf" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">概观</h1><p id="0cca" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在这篇文章中，我会给你一个序列对序列模型的概述，这种模型在不同的任务中非常流行，如机器翻译、视频字幕、图像字幕、问题回答等。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es km"><img src="../Images/ac217d169cf662674f6881bd1761d29a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6i8zvWmaXpXx48yl.jpg"/></div></div></figure><p id="5ee1" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">先决条件:读者应该已经熟悉神经网络，特别是递归神经网络(RNNs)。此外，了解LSTM或GRU模型更佳。如果你不熟悉LSTM，我更希望你读读<a class="ae kx" rel="noopener" href="/analytics-vidhya/lstm-long-short-term-memory-5ac02af47606"><em class="kw">【LSTM】——长期短期记忆</em> </a> <em class="kw">。</em></p><h1 id="7317" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">简介:</h1><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/3f548839ba67c0c7fa91c0f12e5da21a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1338/format:webp/0*EuUcrUwNMDEf_x3d.jpg"/></div></figure><h1 id="97b9" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">序列到序列模型的用例</h1><p id="e649" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">序列对序列模型是您每天面对的众多系统的基础。例如，seq2seq模型支持Google Translate、语音设备和在线聊天机器人等应用。以下是一些应用:</p><ul class=""><li id="7d94" class="kz la hi jq b jr kr jv ks jz lb kd lc kh ld kl le lf lg lh bi translated"><em class="kw">机器翻译</em> —谷歌2016年的一篇<a class="ae kx" href="https://arxiv.org/pdf/1409.3215.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>展示了seq2seq模型的翻译质量如何“接近或超过目前所有公布的结果”。</li></ul><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es li"><img src="../Images/5d31f6df8062386334830702bcff71c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*7b2qzUwUYRLBkNXy.png"/></div></figure><ul class=""><li id="286a" class="kz la hi jq b jr kr jv ks jz lb kd lc kh ld kl le lf lg lh bi translated"><em class="kw">语音识别</em>——另一篇谷歌<a class="ae kx" href="https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0233.PDF" rel="noopener ugc nofollow" target="_blank">论文</a>在语音识别任务上对比了现有的seq2seq模型。</li></ul><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lj"><img src="../Images/fce9edf8312d565c55ccd59303aec425.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vWsikdfobYuM2DoH.jpg"/></div></div></figure><p id="157b" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">这些只是seq2seq被视为最佳解决方案的一些应用。该模型可用作任何基于序列的问题的解决方案，尤其是输入和输出具有不同大小和类别的问题。</p><p id="0d4f" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">我们将在下面详细讨论模型结构。</p><h1 id="f641" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">编码器-解码器架构:</h1><p id="c07c" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">用于构建Seq2Seq模型的最常见架构是编码器-解码器架构。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lk"><img src="../Images/eba4b727146cbe846535f53bcb4832e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VJHsFRb6emNfZ1Qb.png"/></div></div></figure><p id="6401" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated"><a class="ae kx" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sutskever%2C+I" rel="noopener ugc nofollow" target="_blank"> Ilya Sutskever </a>用神经网络进行序列对序列学习的模型</p><p id="6964" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">顾名思义，有两个组件——编码器和解码器。</p><h2 id="7a1b" class="ll ir hi bd is lm ln lo iw lp lq lr ja jz ls lt je kd lu lv ji kh lw lx jm ly bi translated">编码器:</h2><ul class=""><li id="e26e" class="kz la hi jq b jr js jv jw jz lz kd ma kh mb kl le lf lg lh bi translated">编码器和解码器都是LSTM模型(或者有时是GRU模型)</li><li id="d22e" class="kz la hi jq b jr mc jv md jz me kd mf kh mg kl le lf lg lh bi translated">编码器读取输入序列，并在所谓的<strong class="jq hj">内部状态向量</strong>或<strong class="jq hj">上下文向量</strong>中总结信息(在LSTM的情况下，这些被称为隐藏状态和单元状态向量)。我们丢弃编码器的输出，只保留内部状态。该上下文向量旨在封装所有输入元素的信息，以便帮助解码器做出准确的预测。</li><li id="c35f" class="kz la hi jq b jr mc jv md jz me kd mf kh mg kl le lf lg lh bi translated">使用以下公式计算隐藏状态<em class="kw"> h_i </em>:</li></ul><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/d4b39fd16dd14e3d91518e1644f90df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/0*wlQo78PHSEYnn-4T.png"/></div></div></figure><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/28c8aeec725fe5a3fd1401b7c0b0c203.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*6kOiIylTsRZeUpKu.png"/></div></div></figure><p id="044a" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">LSTM一个接一个地读取数据。因此，如果输入是一个长度为“t”的序列，我们说LSTM以“t”个时间步长读取它。</p><p id="8796" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">1.Xi =时间步长I的输入序列</p><p id="f8af" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">2.hi和ci = LSTM在每个时间步长保持两种状态(“h”代表隐藏状态，“c”代表单元状态)。这些组合在一起，就是LSTM在时间步I的内部状态</p><p id="8490" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">3.Yi =时间步长I的输出序列。Yi实际上是通过使用softmax激活生成的整个词汇的概率分布。因此，每个Yi是代表概率分布的大小为“vocab_size”的向量。</p><h2 id="1bec" class="ll ir hi bd is lm ln lo iw lp lq lr ja jz ls lt je kd lu lv ji kh lw lx jm ly bi translated">解码器:</h2><ul class=""><li id="d035" class="kz la hi jq b jr js jv jw jz lz kd ma kh mb kl le lf lg lh bi translated">解码器是LSTM，其初始状态被初始化为编码器LSTM的最终状态，即编码器最终单元的上下文向量被输入到解码器网络的第一个单元。利用这些初始状态，解码器开始产生输出序列，并且这些输出也被考虑用于将来的输出。</li><li id="3109" class="kz la hi jq b jr mc jv md jz me kd mf kh mg kl le lf lg lh bi translated">几个LSTM单元的堆栈，其中每个单元在时间步长t预测一个输出y_t</li><li id="8bf6" class="kz la hi jq b jr mc jv md jz me kd mf kh mg kl le lf lg lh bi translated">每个递归单元接受来自前一个单元的隐藏状态，并产生和输出它自己的隐藏状态。</li><li id="e157" class="kz la hi jq b jr mc jv md jz me kd mf kh mg kl le lf lg lh bi translated">使用以下公式计算任何隐藏状态<em class="kw"> h_i </em>:</li></ul><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/b31f46815df827f957e1fab0c52b421e.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/0*Gxw-YefM5l2_R9AF.png"/></div></figure><ul class=""><li id="4522" class="kz la hi jq b jr kr jv ks jz lb kd lc kh ld kl le lf lg lh bi translated">使用以下公式计算时间步长<em class="kw"> t </em>的输出<em class="kw"> y_t </em>:</li></ul><figure class="kn ko kp kq fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/c2415674ecd42608dae05bc29dfcb2ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*pl57H41--vLxBOS3.png"/></div></figure><ul class=""><li id="59f3" class="kz la hi jq b jr kr jv ks jz lb kd lc kh ld kl le lf lg lh bi translated">我们使用当前时间步长的隐藏状态以及相应的权重W(S)来计算输出。<a class="ae kx" href="https://www.youtube.com/watch?v=LLux1SW--oM" rel="noopener ugc nofollow" target="_blank"> Softmax </a>用于创建一个概率向量，该向量将帮助我们确定最终输出(如问答问题中的单词)。</li></ul><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/fe96aa28cf9c31e53155d7086437f704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Jtdv0rQNWABf5aDO.png"/></div></div></figure><p id="d285" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">我们将在输出序列中添加两个令牌，如下所示:</p><h2 id="3f7b" class="ll ir hi bd is lm ln lo iw lp lq lr ja jz ls lt je kd lu lv ji kh lw lx jm ly bi translated">示例:</h2><p id="a1f1" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">约翰正在努力工作。</p><p id="c212" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">最重要的一点是，解码器的初始状态(h0，c0)被设置为编码器的最终状态。这直观地意味着解码器被训练成根据编码器编码的信息开始生成输出序列。</p><p id="582b" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">最后，根据每个时间步的预测输出计算损耗，误差随时间反向传播，以更新网络参数。用足够多的数据对网络进行长时间的训练会产生非常好的预测结果。</p><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/4125000bc834d0c8d664aa1f40165fa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*dO-zaIZc87T6Ci1K.png"/></div></div></figure><p id="7719" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">整体编码器-解码器架构</p><ul class=""><li id="88f4" class="kz la hi jq b jr kr jv ks jz lb kd lc kh ld kl le lf lg lh bi translated">在推理过程中，我们一次生成一个单词。</li><li id="3435" class="kz la hi jq b jr mc jv md jz me kd mf kh mg kl le lf lg lh bi translated">解码器的初始状态被设置为编码器的最终状态。</li><li id="06b2" class="kz la hi jq b jr mc jv md jz me kd mf kh mg kl le lf lg lh bi translated">解码器的初始输入总是起始令牌。</li><li id="6622" class="kz la hi jq b jr mc jv md jz me kd mf kh mg kl le lf lg lh bi translated">在每个时间步，我们保留解码器的状态，并将其设置为下一个时间步的初始状态。</li><li id="22f5" class="kz la hi jq b jr mc jv md jz me kd mf kh mg kl le lf lg lh bi translated">在每个时间步，预测的输出作为下一个时间步的输入。</li><li id="de8e" class="kz la hi jq b jr mc jv md jz me kd mf kh mg kl le lf lg lh bi translated">当解码器预测到结束标记时，我们中断循环。</li></ul><h2 id="aa0c" class="ll ir hi bd is lm ln lo iw lp lq lr ja jz ls lt je kd lu lv ji kh lw lx jm ly bi translated">编码器-解码器模型的缺点:</h2><p id="3619" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">这种架构有两个主要缺点，都与长度有关。</p><ol class=""><li id="9396" class="kz la hi jq b jr kr jv ks jz lb kd lc kh ld kl mn lf lg lh bi translated">首先，和人类一样，这种架构的记忆非常有限。LSTM的最后一个隐藏状态，我们称之为<strong class="jq hj"> S或W </strong>，是你试图把你要翻译的整个句子塞进去的地方。<strong class="jq hj"> S或W </strong>通常只有几百个单位(读作:浮点数)长——你越是试图强行进入这个固定维度的向量，神经网络就被迫损失越多。根据要求神经网络执行的“有损压缩”来考虑神经网络有时是非常有用的。</li><li id="1a08" class="kz la hi jq b jr mc jv md jz me kd mf kh mg kl mn lf lg lh bi translated">第二，根据一般经验，神经网络越深，就越难训练。对于递归神经网络，序列越长，神经网络沿时间维度越深。这导致消失梯度，其中来自递归神经网络学习的目标的梯度信号在向后行进时消失。即使有专门设计的rnn来帮助防止渐变消失，例如LSTM，这仍然是一个基本问题。</li></ol><figure class="kn ko kp kq fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/d30712dcbd413017cc7d3f7a9d89c32b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/0*tz6m62tl4-sDDS-a.png"/></div></div></figure><p id="d751" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">此外，对于更健壮和更长的句子，我们有像<strong class="jq hj">注意力模型</strong>和<strong class="jq hj">变形金刚</strong>这样的模型。</p><p id="c5eb" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">这里的<a class="ae kx" href="https://github.com/prasoons075/Deep-Learning-Codes/tree/master/Encoder%20Decoder%20Model" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj"> <em class="kw">这个</em> </strong> </a>是我的GitHub储存库，用于完整的单词级以及字符级编码器-解码器模型。</p><h1 id="a7ad" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">参考</h1><p id="1576" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">1.<a class="ae kx" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener ugc nofollow" target="_blank">https://blog . keras . io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras . html</a></p><p id="a61c" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">2.<a class="ae kx" href="https://arxiv.org/abs/1409.3215" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1409.3215</a></p><p id="0bbb" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">3.<a class="ae kx" href="https://arxiv.org/abs/1406.1078" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1406.1078</a></p><p id="8f78" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated">4.<a class="ae kx" href="https://towardsdatascience.com/word-level-english-to-marathi-neural-machine-translation-using-seq2seq-encoder-decoder-lstm-model-1a913f2dc4a7" rel="noopener" target="_blank">https://towards data science . com/word-level-English-to-Marathi-neural-machine-translation-using-seq 2 seq-encoder-decoder-lstm-model-1a 913 F2 DC 4a 7</a></p></div><div class="ab cl mp mq gp mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="hb hc hd he hf"><p id="cc5f" class="pw-post-body-paragraph jo jp hi jq b jr kr jt ju jv ks jx jy jz kt kb kc kd ku kf kg kh kv kj kk kl hb bi translated"><em class="kw">原载于2020年8月31日</em><a class="ae kx" href="https://www.analyticsvidhya.com/blog/2020/08/a-simple-introduction-to-sequence-to-sequence-models/" rel="noopener ugc nofollow" target="_blank"><em class="kw">https://www.analyticsvidhya.com</em></a><em class="kw">。</em></p></div></div>    
</body>
</html>