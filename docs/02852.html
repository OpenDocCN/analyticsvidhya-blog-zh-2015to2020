<html>
<head>
<title>LSTM- Long Short-Term Memory</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LSTM-长短期记忆</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/lstm-long-short-term-memory-5ac02af47606?source=collection_archive---------7-----------------------#2020-01-05">https://medium.com/analytics-vidhya/lstm-long-short-term-memory-5ac02af47606?source=collection_archive---------7-----------------------#2020-01-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/0f328788502d748ee635e1b5c9f52e8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H3aobLAmfePsS26QhY3YFQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">我是蝙蝠侠</figcaption></figure><p id="e40b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">长短期记忆网络——通常简称为“lstm”——是一种特殊的RNN，能够学习长期依赖性。它们是由<a class="ae js" href="http://www.bioinf.jku.at/publications/older/2604.pdf" rel="noopener ugc nofollow" target="_blank">hoch Reiter&amp;schmid Huber(1997)</a>介绍的，并在随后的工作中被许多人提炼和推广。它们在各种各样的问题上表现得非常好，现在被广泛使用。</p><p id="f94a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">LSTMs的明确设计是为了避免长期依赖问题。长时间记住信息实际上是他们的默认行为，而不是他们努力学习的东西！</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jt"><img src="../Images/ae64181e6db9e7efe5e5b9d2a9bd534b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vcM4KAieoUCDa4Xp.png"/></div></div></figure><p id="0e44" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">稍后我们将一步一步地浏览LSTM图。现在，让我们试着适应我们将要使用的符号。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jy"><img src="../Images/d4bdaa79e3a4e19e4528dd2c1dac8bb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gu0SeGRj6t6Vhf0g2JT0nA.png"/></div></div></figure><p id="2dac" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在上图中，每条线都承载一个完整的向量，从一个节点的输出到其他节点的输入。粉色圆圈代表逐点操作，如向量加法，而黄色方框是学习过的神经网络层。行合并表示连接，而行分叉表示其内容被复制，并且副本被移动到不同的位置。</p><h1 id="5543" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">LSTM按部就班地走过</h1><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/ba0bd086eae30305b554e209dad0282d.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*Gwpe2UYn8Iws8_8LEviDeg.png"/></div></figure><ol class=""><li id="368a" class="ky kz hi iw b ix iy jb jc jf la jj lb jn lc jr ld le lf lg bi translated"><strong class="iw hj">忘门</strong>(权重对应W_f): <br/>首先我们有忘门(W_f)。这个门决定哪些信息应该被丢弃或保留。来自先前隐藏状态(h(t-1))的信息和来自当前输入(x(t))的信息通过sigmoid函数传递。值介于0和1之间。越接近0表示忘记，越接近1表示保留。</li></ol><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/38d9276ee2e89d034b13283d79297b9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*QeCVt60RU5M5BhQieE6Cnw.png"/></div></figure><p id="c05d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 2。输入门</strong>(权重对应W_i): <br/>为了更新单元状态，我们有了输入门。首先，我们将之前的隐藏状态(h(t-1))和当前输入(x(t))传递给一个sigmoid函数。它通过将值转换为0和1之间的值来决定哪些值将被更新。0表示不重要，1表示重要。您还可以将隐藏状态(h(t-1))和当前输入(x(t))传递到tanh函数中，以挤压-1和1之间的值来帮助调节网络。然后将双曲正切输出乘以sigmoid输出。sigmoid输出将决定哪些信息对tanh输出很重要。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es li"><img src="../Images/cea7d6005177577e8e60d4fc41b3094f.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*DPAqmwfbhJeJuN9sW1-iog.png"/></div></figure><p id="df32" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 3。细胞状态</strong> <br/>现在我们应该有足够的信息来计算细胞状态。首先，单元格状态逐点乘以遗忘向量。如果乘以接近0的值，这有可能会丢失单元状态中的值。然后，我们从输入门获取输出，进行逐点加法，将细胞状态更新为神经网络认为相关的新值。这给了我们新的细胞状态(c(t))。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/fed7b995489a95265cfa66241d40aec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/format:webp/1*A-wTgSHfXNcNm9UNWiKR1A.png"/></div></figure><p id="0d5a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 4。输出门</strong> <br/>最后我们有输出门。输出门决定下一个隐藏状态应该是什么。请记住，隐藏状态包含以前输入的信息。隐藏状态也用于预测。首先，我们将先前的隐藏状态和当前输入传递给一个sigmoid函数。然后，我们将新修改的单元格状态传递给tanh函数。我们将双曲正切输出乘以sigmoid输出来决定隐藏状态应该携带什么信息。输出是隐藏状态。然后，新的单元状态和新的隐藏被带入下一个时间步骤。</p><p id="3e76" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">回顾一下,“遗忘之门”决定了哪些内容与之前的步骤相关。输入门决定从当前步骤添加哪些相关信息。输出门决定下一个隐藏状态应该是什么。</p><h1 id="e7ea" class="jz ka hi bd kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw bi translated">参考资料:</h1><div class="lj lk ez fb ll lm"><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener  ugc nofollow" target="_blank"><div class="ln ab dw"><div class="lo ab lp cl cj lq"><h2 class="bd hj fi z dy lr ea eb ls ed ef hh bi translated">了解LSTM网络</h2><div class="lt l"><h3 class="bd b fi z dy lr ea eb ls ed ef dx translated">2015年8月27日发布人类不是每秒钟都从零开始思考。当你读这篇文章时，你…</h3></div><div class="lu l"><p class="bd b fp z dy lr ea eb ls ed ef dx translated">colah.github.io</p></div></div></div></a></div><p id="c43f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae js" href="https://en.wikipedia.org/wiki/Long_short-term_memory" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Long_short-term_memory</a></p></div></div>    
</body>
</html>