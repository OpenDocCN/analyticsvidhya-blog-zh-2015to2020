<html>
<head>
<title>Decision Trees and Ensembling Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树和集成方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decision-trees-and-ensembling-methods-cbdd918e3193?source=collection_archive---------14-----------------------#2019-10-26">https://medium.com/analytics-vidhya/decision-trees-and-ensembling-methods-cbdd918e3193?source=collection_archive---------14-----------------------#2019-10-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/5d7f894d59c3023e8f2fef75d5b2cd8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lz5mD1xmkaOppAqXCnCpSA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">学分:<a class="ae iu" href="http://www.forestryengland.uk" rel="noopener ugc nofollow" target="_blank"> www.forestryengland.uk </a></figcaption></figure><p id="43ad" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">决策树还是传统模型？</p><p id="e23c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果预测值和目标变量之间的关系是近似线性的，那么传统方法可能会在数据上表现得更好，并且优于各种树模型。</p><p id="c3f7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一方面，如果模型非常复杂并且遵循非线性模式，决策树可能会优于传统方法。此外，在项目需要更多的可解释性和可视化的情况下，决策树可能是首选，因为这些可以在“流程图”类型的图表中表示，并且比较容易解释。</p><p id="cfe8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">决策树的优势:</strong></p><ul class=""><li id="83f2" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">比大多数模型更容易解释，包括线性回归</li><li id="25ae" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">可以以“流程”或“组织”图的形式可视化，这使得它易于理解，即使对于外行人也是如此</li><li id="2f64" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">决策树倾向于更好地模拟人类实际思考的方式</li><li id="bcdc" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">无需创建虚拟变量即可处理定性变量的能力</li></ul><p id="caf9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">决策树的缺点:</strong></p><ul class=""><li id="c860" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">不具有与其他回归和分类模型(如线性回归和逻辑回归)相同的预测准确性</li><li id="a541" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">不太容易概括，数据中的一个小变化就可能破坏最终的模型</li></ul><p id="4b5f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">考虑到上面提到的缺点，还有其他技术可以应用于决策树，从而大大提高它们的预测能力。这些是装袋，随机森林和助推。</p><p id="2ab9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">装袋:</strong></p><p id="a08e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Bagging或Bootstrap Aggregation正在生成数量为<strong class="ix hj"> <em class="kh"> n </em> </strong>的决策树，并用数据集的<strong class="ix hj"> <em class="kh"> n </em> </strong>个样本对它们进行训练(样本应该从单个数据集中随机抽取。带替换！).</p><p id="cf73" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然这种方法可以改进许多估计器的结果，但它对决策树特别有用。由于它们的高方差，对一组观察值进行平均已被证明可以减少方差而不影响偏倚！</p><p id="0f65" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">就像所谓的“群体智慧”一样，虽然有些人低估了，有些人高估了，但当我们平均这些猜测时，误差的总和往往会减少。</p><p id="7a3d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">虽然Bagging是一种提高决策树预测准确性的好方法，但它也带来了负面影响，即我们失去了模型的可解释性。使用单个树，很容易解释和可视化模型中使用的最佳特征，但是在对数百或数千个特征进行平均后，我们放弃了模型的可解释性…</p><p id="3da6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">随机森林:</strong></p><p id="453f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">与袋装决策树非常相似，随机森林由一组袋装决策树组成，但稍加调整，结果就会大不相同。</p><p id="c175" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">不是在同一个树上测试自举训练样本，而是在决策树的每个分裂处选择特征(预测器)的随机样本。这保证了被测试的模型彼此不同，因为它们不会总是使用最重要的特征来进行树的第一次分裂。</p><p id="30ce" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，不同的模型彼此之间的相关性较低，一旦取平均值，就会产生更有影响的变化，从而产生变化较小的模型。</p><p id="4613" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">袋装决策树和随机森林的主要区别在于从总共<strong class="ix hj"><em class="kh">【p】</em></strong>个预测器<strong class="ix hj"> <em class="kh">中选择一个预测器子集大小<strong class="ix hj"><em class="kh">【m】</em></strong>。</em> </strong>通常对于随机森林，我们使用<strong class="ix hj"> m </strong> = <strong class="ix hj"> √p </strong>，这样可以减少预测误差。当<strong class="ix hj"><em class="kh">m</em></strong>=<strong class="ix hj"><em class="kh">p</em></strong>我们有一个常规的袋装决策树模型。</p><p id="948c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">增压:</strong></p><p id="7875" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Boosting也以类似的方式工作，但树是通过使用来自先前树的预测误差信息来顺序生长的。模型适合数据的修改版本，随着数据的迭代，每棵树被迫专注于那些错误或误分类。</p><p id="ab5a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">根据树的性能，赋予它权重以指示其性能。权重将在最后用于平均结果并得出最佳模型。</p><p id="8cd0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在Python的Scikit-Learn上实现不同的技术:</p><pre class="ki kj kk kl fd km kn ko kp aw kq bi"><span id="bf1a" class="kr ks hi kn b fi kt ku l kv kw"># Import Models from Sklearn<br/>from sklearn.datasets import make_classification<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier</span><span id="2976" class="kr ks hi kn b fi kx ku l kv kw"># Set Features and Target Variable<br/>X, y = make_classification(n_samples=750, n_features=20, random_state=42)</span><span id="791d" class="kr ks hi kn b fi kx ku l kv kw"># Train / Test Split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y,<br/>                                                    random_state=42,<br/>                                                    stratify=y)</span><span id="d192" class="kr ks hi kn b fi kx ku l kv kw"># Decision Tree Classifier<br/>dt = DecisionTreeClassifier(random_state=42)<br/>dt.fit(X_train, y_train)<br/>dt.score(X_test, y_test)</span><span id="9458" class="kr ks hi kn b fi kx ku l kv kw"># OUTPUT<br/>0.8670212765957447</span><span id="4dcd" class="kr ks hi kn b fi kx ku l kv kw"># Bagging Classifier<br/>bag = BaggingClassifier(n_estimators=100, random_state=42)<br/>bag.fit(X_train, y_train)<br/>bag.score(X_test, y_test)</span><span id="eb13" class="kr ks hi kn b fi kx ku l kv kw"># OUTPUT<br/>0.898936170212766</span><span id="eb7b" class="kr ks hi kn b fi kx ku l kv kw"># Random Forest Classifier<br/>rf = RandomForestClassifier(n_estimators=100, random_state=42)<br/>rf.fit(X_train, y_train)<br/>rf.score(X_test, y_test)</span><span id="005a" class="kr ks hi kn b fi kx ku l kv kw"># OUTPUT<br/>0.898936170212766</span><span id="7792" class="kr ks hi kn b fi kx ku l kv kw"># Adaptative Boost Classifier<br/>ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)<br/>ada.fit(X_train, y_train)<br/>print(ada.score(X_test, y_test))</span><span id="2b56" class="kr ks hi kn b fi kx ku l kv kw"># OUTPUT<br/>0.8457446808510638</span><span id="014c" class="kr ks hi kn b fi kx ku l kv kw">#Gradient Boost Classifier<br/>gb = GradientBoostingClassifier(n_estimators=100, random_state=42)<br/>gb.fit(X_train, y_train)<br/>gb.score(X_test, y_test)</span><span id="0555" class="kr ks hi kn b fi kx ku l kv kw"># OUTPUT<br/>0.9095744680851063</span></pre></div></div>    
</body>
</html>