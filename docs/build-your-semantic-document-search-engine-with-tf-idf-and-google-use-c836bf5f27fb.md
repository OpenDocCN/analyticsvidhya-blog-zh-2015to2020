# ç”¨ TF-IDF å’Œ Google-USE æ„å»ºä½ çš„è¯­ä¹‰æ–‡æ¡£æœç´¢å¼•æ“

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/build-your-semantic-document-search-engine-with-tf-idf-and-google-use-c836bf5f27fb?source=collection_archive---------3----------------------->

![](img/52dd1a38422cd37cd6a15649c623dbf9.png)

æ¥æº: [inmyownterms](http://inmyownterms.com/six-document-search-engines-use/)

è®©æˆ‘ä»¬æ¥äº†è§£ä¸€ä¸‹å¦‚ä½•ä½¿ç”¨ python è¯­è¨€æ¥æ„å»ºä¸€ä¸ªæ–‡æ¡£æœç´¢å¼•æ“ã€‚

åœ¨æœ¬å¸–ä¸­ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨ [20newsgroup å¼€æºæ•°æ®é›†](http://qwone.com/~jason/20Newsgroups/)æ„å»ºä¸€ä¸ª**è¯­ä¹‰æ–‡æ¡£æœç´¢å¼•æ“**ã€‚

# å…ˆå†³æ¡ä»¶

*   [Python 3.5](https://www.python.org/) +
*   [pip 19](https://pypi.org/project/pip/) +æˆ– pip3
*   [NLTK](https://www.nltk.org/)
*   [Scikit-learn](https://scikit-learn.org/stable/)
*   [TensorFlow-GPU](https://www.tensorflow.org)

# 1.åšå¥½å‡†å¤‡

å¯¹äºè¿™ä¸ªèŒä½ï¼Œæˆ‘ä»¬å°†éœ€è¦ä¸Šè¿°å…ˆå†³æ¡ä»¶**ï¼Œ**å¦‚æœä½ è¿˜æ²¡æœ‰ï¼Œè¯·åšå¥½å‡†å¤‡ã€‚

# 2.æ•°æ®æ”¶é›†

è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ 20 ä¸ªæ–°é—»ç»„æ•°æ®é›†æ¥åˆ†æç»™å®šè¾“å…¥å…³é”®è¯/å¥å­è¾“å…¥çš„æ–‡æœ¬æœç´¢å¼•æ“ã€‚

20 ä¸ªæ–°é—»ç»„æ•°æ®é›†æ˜¯å¤§çº¦ 11K ä¸ªæ–°é—»ç»„æ–‡æ¡£çš„é›†åˆï¼Œå¹³å‡åˆ†å¸ƒåœ¨ 20 ä¸ªä¸åŒçš„æ–°é—»ç»„ä¸­ã€‚

```
news = pd.read_json('[https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/newsgroups.json](https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/newsgroups.json)')
```

## 2.1 æ•°æ®æ¸…ç†:

åœ¨è¿›å…¥æ¸…ç†é˜¶æ®µä¹‹å‰ï¼Œæˆ‘ä»¬ä»æ–‡æœ¬ä¸­æ£€ç´¢æ–‡æ¡£çš„ä¸»é¢˜ã€‚

```
for i,txt in enumerate(news['content']):
    subject = re.findall('Subject:(.*\n)',txt)
    if (len(subject) !=0):
        news.loc[i,'Subject'] =str(i)+' '+subject[0]
    else:
        news.loc[i,'Subject'] ='NA'
df_news =news[['Subject','content']]
```

ç°åœ¨ï¼Œæˆ‘ä»¬ä»æ–‡æœ¬å†…å®¹å’Œæ•°æ®é›†çš„ä¸»é¢˜ä¸­åˆ é™¤ä¸éœ€è¦çš„æ•°æ®ã€‚

```
df_news.content =df_news.content.replace(to_replace='from:(.*\n)',value='',regex=True) ##remove from to email 
df_news.content =df_news.content.replace(to_replace='lines:(.*\n)',value='',regex=True)
df_news.content =df_news.content.replace(to_replace='[!"#$%&\'()*+,/:;<=>?@[\\]^_`{|}~]',value=' ',regex=True) #remove punctuation except
df_news.content =df_news.content.replace(to_replace='-',value=' ',regex=True)
df_news.content =df_news.content.replace(to_replace='\s+',value=' ',regex=True)    #remove new line
df_news.content =df_news.content.replace(to_replace='  ',value='',regex=True)                #remove double white space
df_news.content =df_news.content.apply(lambda x:x.strip())  # Ltrim and Rtrim of whitespace
```

## 2.2 æ•°æ®é¢„å¤„ç†

é¢„å¤„ç†æ˜¯æˆ‘ä»¬å¤„ç†ä»»ä½•æ–‡æœ¬æ¨¡å‹çš„ä¸»è¦æ­¥éª¤ä¹‹ä¸€ã€‚åœ¨æ­¤é˜¶æ®µï¼Œæˆ‘ä»¬å¿…é¡»æŸ¥çœ‹æ•°æ®çš„åˆ†å¸ƒæƒ…å†µï¼Œéœ€è¦ä»€ä¹ˆæŠ€æœ¯ä»¥åŠåº”è¯¥æ¸…ç†å¤šæ·±ã€‚

## å°å†™å­—æ¯

å°†æ–‡æœ¬è½¬æ¢æˆå°å†™å½¢å¼ã€‚å³'**ç‹—'**å˜æˆ'**ç‹—**

```
df_news['content']=[entry.lower() for entry in df_news['content']]
```

## å•è¯æ ‡è®°åŒ–

å•è¯æ ‡è®°åŒ–æ˜¯å°†å¥å­åˆ†æˆå•è¯å½¢å¼çš„è¿‡ç¨‹ã€‚

â€œ**çº¦ç¿°åœ¨è½¨é“**ä¸Šè·‘â€â†’â€œ**çº¦ç¿°**â€ã€â€œ**æ˜¯**â€ã€â€œ**è·‘**â€ã€â€œä¸­çš„**â€ã€â€œ**ä¸­çš„**â€ã€â€œ**è½¨é“**â€**

```
df_news['Word tokenize']= [word_tokenize(entry) for entry in df_news.content]
```

## åœæ­¢è¨€è¯­

åœç”¨è¯æ˜¯æœ€å¸¸è§çš„è¯ï¼Œå®ƒä¸ä¼šç»™æ–‡æ¡£å‘é‡å¸¦æ¥ä»»ä½•é™„åŠ å€¼ã€‚äº‹å®ä¸Šï¼Œåˆ é™¤è¿™äº›å°†å¢åŠ è®¡ç®—å’Œç©ºé—´æ•ˆç‡ã€‚NLTK åº“æœ‰ä¸€ä¸ªä¸‹è½½åœç”¨è¯çš„æ–¹æ³•ã€‚

![](img/c07e2a457bd4cedd5adaa29910de1fc0.png)

## å•è¯è¯æ±‡åŒ–

å¼•ç†åŒ–æ˜¯ä¸€ç§å°†å•è¯ç®€åŒ–ä¸ºå•è¯çš„è¯æ ¹åŒä¹‰è¯çš„æ–¹æ³•ã€‚ä¸è¯å¹²æå–ä¸åŒï¼Œè¯æ±‡åŒ¹é…ç¡®ä¿ç¼©å‡åçš„å•è¯å†æ¬¡æˆä¸ºè¯å…¸ä¸­çš„å•è¯(åŒä¸€ç§è¯­è¨€ä¸­çš„å•è¯)ã€‚WordNetLemmatizer å¯ç”¨äºå¯¹ä»»ä½•å•è¯è¿›è¡Œè¯æ±‡åŒ–ã€‚

å³**å²©çŸ³â†’å²©çŸ³ï¼Œæ›´å¥½â†’å¥½ï¼Œè¯­æ–™åº“â†’è¯­æ–™åº“**

è¿™é‡Œåˆ›å»ºäº† wordLemmatizer å‡½æ•°æ¥åˆ é™¤ä¸€ä¸ª**å•å­—ç¬¦**ã€**åœç”¨è¯**å’Œ**å•è¯ã€‚**

```
# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun
def wordLemmatizer(data):
    tag_map = defaultdict(lambda : wn.NOUN)
    tag_map['J'] = wn.ADJ
    tag_map['V'] = wn.VERB
    tag_map['R'] = wn.ADV
    file_clean_k =pd.DataFrame()
    for index,entry in enumerate(data):

        # Declaring Empty List to store the words that follow the rules for this step
        Final_words = []
        # Initializing WordNetLemmatizer()
        word_Lemmatized = WordNetLemmatizer()
        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.
        for word, tag in pos_tag(entry):
            # Below condition is to check for Stop words and consider only alphabets
            if len(word)>1 and word not in stopwords.words('english') and word.isalpha():
                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])
                Final_words.append(word_Final)
            # The final processed set of words for each iteration will be stored in 'text_final'
                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)
                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)
                file_clean_k=file_clean_k.replace(to_replace ="\[.", value = '', regex = True)
                file_clean_k=file_clean_k.replace(to_replace ="'", value = '', regex = True)
                file_clean_k=file_clean_k.replace(to_replace =" ", value = '', regex = True)
                file_clean_k=file_clean_k.replace(to_replace ='\]', value = '', regex = True)
    return file_clean_k
```

é€šè¿‡ä½¿ç”¨è¿™ä¸ªå‡½æ•°ï¼ŒèŠ±è´¹äº†å¤§çº¦ **13 å°æ—¶**çš„æ—¶é—´æ¥æ£€æŸ¥å’Œè¯æ¡åŒ– 20 ä¸ªæ–°é—»ç»„æ•°æ®é›†çš„ 11K ä¸ªæ–‡æ¡£çš„å•è¯ã€‚åœ¨ä¸‹é¢æ‰¾åˆ°è¿™ä¸ªå•è¯çš„ JSON æ–‡ä»¶ã€‚

```
[https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/WordLemmatize20NewsGroup.json](https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/WordLemmatize20NewsGroup.json)
```

## 2.3 æ•°æ®å·²å‡†å¤‡å°±ç»ª

æŸ¥çœ‹å¹²å‡€æ•°æ®çš„ç¤ºä¾‹-

```
df_news.Clean_Keyword[0]
```

![](img/73e7a4167f0017cc75df9b2343e379a3.png)

# 3.æ–‡æ¡£æœç´¢å¼•æ“

åœ¨è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬ç”¨ä¸‰ç§æ–¹æ³•æ¥ç†è§£æ–‡æœ¬åˆ†æã€‚

1.ä½¿ç”¨ **TF-IDF** çš„æ–‡æ¡£æœç´¢å¼•æ“

2.å¸¦æœ‰**è°·æ­Œé€šç”¨è¯­å¥ç¼–ç å™¨**çš„æ–‡æ¡£æœç´¢å¼•æ“

## 3.1 ä½¿ç”¨[ä½™å¼¦ç›¸ä¼¼åº¦](https://en.wikipedia.org/wiki/Cosine_similarity)è®¡ç®—æ’å

è¿™æ˜¯æ ¹æ®è¾“å…¥å…³é”®å­—/å¥å­è®¡ç®—æ–‡æ¡£æ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼æ€§çš„æœ€å¸¸è§çš„åº¦é‡ã€‚æ•°å­¦ä¸Šï¼Œå®ƒæµ‹é‡çš„æ˜¯åœ¨å¤šç»´ç©ºé—´ä¸­æŠ•å½±çš„ä¸¤ä¸ªå‘é‡çš„è§’åº¦ b/w çš„ä½™å¼¦å€¼ã€‚

![](img/4a415cd5889ab06dceaf78570f4384fe.png)

è¦æŸ¥è¯¢çš„é»‘ç™½æ–‡æ¡£çš„ä½™å¼¦ç›¸ä¼¼åº¦

åœ¨ä¸Šå›¾ä¸­ï¼Œç©ºé—´ä¸­æœ‰ 3 ä¸ªæ–‡æ¡£å‘é‡å€¼å’Œä¸€ä¸ªæŸ¥è¯¢å‘é‡ã€‚å½“æˆ‘ä»¬è®¡ç®— 3 ä¸ªæ–‡æ¡£çš„ä½™å¼¦ç›¸ä¼¼åº¦ b/w æ—¶ã€‚æœ€ç›¸ä¼¼å€¼å°†æ˜¯ä¸‰ä¸ªæ–‡æ¡£ä¸­çš„ D3 æ–‡æ¡£ã€‚

# 1.ä½¿ç”¨ TF-IDF çš„æ–‡æ¡£æœç´¢å¼•æ“:

[**TF-IDF**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) ä»£è¡¨**â€œè¯é¢‘â€”é€†æ–‡æ¡£é¢‘â€**ã€‚è¿™æ˜¯ä¸€ç§è®¡ç®—æ¯ä¸ªå•è¯çš„æƒé‡çš„æŠ€æœ¯ï¼Œè¡¨ç¤ºè¯¥å•è¯åœ¨æ–‡æ¡£å’Œè¯­æ–™åº“ä¸­çš„é‡è¦æ€§ã€‚è¯¥ç®—æ³•ä¸»è¦ç”¨äºä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬æŒ–æ˜é¢†åŸŸã€‚

## æœ¯è¯­é¢‘ç‡(TF)

å•è¯åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„æ¬¡æ•°é™¤ä»¥æ–‡æ¡£ä¸­çš„æ€»å•è¯æ•°ã€‚æ¯ä¸ªæ–‡æ¡£éƒ½æœ‰å…¶è¯é¢‘ã€‚

![](img/87ac28bd18de9ce28e20848c7143bca9.png)

## åå‘æ•°æ®é¢‘ç‡(IDF)

æ—¥å¿—ä¸­çš„æ–‡æ¡£æ•°é™¤ä»¥åŒ…å«å•è¯ ***w*** çš„æ–‡æ¡£æ•°ã€‚é€†æ•°æ®é¢‘ç‡å†³å®šäº†è¯­æ–™åº“ä¸­æ‰€æœ‰æ–‡æ¡£ä¸­ç¨€æœ‰è¯çš„æƒé‡ã€‚

![](img/2c4cee08c4949ce478536fe6f799f7fd.png)

æœ€åï¼Œ **TF-IDF** å°±æ˜¯ TF ä¹˜ä»¥ IDFã€‚

```
**TF-IDF = Term Frequency (TF) * Inverse Document Frequency (IDF)**
```

![](img/44cf9d26b4295a628fe18837da398987.png)

æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ [Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) æä¾›çš„ç±»ï¼Œè€Œä¸æ˜¯è‡ªå·±æ‰‹åŠ¨å®ç° [TF-IDF](http://www.tfidf.com/) ã€‚

## ä½¿ç”¨ Sklearn çš„ TfidfVectorizer ç”Ÿæˆ TF-IDF

å¯¼å…¥åŒ…:

```
import pandas as pd
import numpy as np
import os 
import re
import operator
import nltk 
from nltk.tokenize import word_tokenize
from nltk import pos_tag
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import defaultdict
from nltk.corpus import wordnet as wn
from sklearn.feature_extraction.text import TfidfVectorizer
```

TF-IDF

```
from sklearn.feature_extraction.text import TfidfVectorizer
import operator## Create Vocabulary
vocabulary = set()for doc in df_news.Clean_Keyword:
    vocabulary.update(doc.split(','))vocabulary = list(vocabulary)# Intializating the tfIdf model
tfidf = TfidfVectorizer(vocabulary=vocabulary)# Fit the TfIdf model
tfidf.fit(df_news.Clean_Keyword)# Transform the TfIdf model
tfidf_tran=tfidf.transform(df_news.Clean_Keyword)
```

ä¸Šé¢çš„ä»£ç å·²ç»åˆ›å»ºäº†æ•´ä¸ªæ•°æ®é›†çš„ TF-IDF æƒé‡ï¼Œç°åœ¨å¿…é¡»åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥ä¸ºè¾“å…¥æŸ¥è¯¢ç”Ÿæˆä¸€ä¸ªå‘é‡ã€‚

## ä¸ºæŸ¥è¯¢/æœç´¢å…³é”®å­—åˆ›å»ºå‘é‡

```
def gen_vector_T(tokens):Q = np.zeros((len(vocabulary)))    
    x= tfidf.transform(tokens)
    #print(tokens[0].split(','))
    for token in tokens[0].split(','):
        #print(token)
        try:
            ind = vocabulary.index(token)
            Q[ind]  = x[0, tfidf.vocabulary_[token]]
        except:
            pass
    return Q
```

## ç”¨äºè®¡ç®—çš„ä½™å¼¦ç›¸ä¼¼å‡½æ•°

```
def cosine_sim(a, b):
    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))
    return cos_sim
```

## ä½™å¼¦ç›¸ä¼¼åº¦é»‘ç™½æ–‡æ¡£åˆ°æŸ¥è¯¢å‡½æ•°

```
def cosine_similarity_T(k, query):
    preprocessed_query = preprocessed_query = re.sub("\W+", " ", query).strip()
    tokens = word_tokenize(str(preprocessed_query))
    q_df = pd.DataFrame(columns=['q_clean'])
    q_df.loc[0,'q_clean'] =tokens
    q_df['q_clean'] =wordLemmatizer(q_df.q_clean)
    d_cosines = []

    query_vector = gen_vector_T(q_df['q_clean'])
    for d in tfidf_tran.A:
        d_cosines.append(cosine_sim(query_vector, d))

    out = np.array(d_cosines).argsort()[-k:][::-1]
    #print("")
    d_cosines.sort()
    a = pd.DataFrame()
    for i,index in enumerate(out):
        a.loc[i,'index'] = str(index)
        a.loc[i,'Subject'] = df_news['Subject'][index]
    for j,simScore in enumerate(d_cosines[-k:][::-1]):
        a.loc[j,'Score'] = simScore
    return a
```

## æµ‹è¯•åŠŸèƒ½

```
cosine_similarity_T(10,â€™computer scienceâ€™)
```

![](img/710a08fd464236eb2387baadbad27831.png)

**å…³äºâ€œè®¡ç®—æœºç§‘å­¦â€å•è¯çš„å‰ 5 ä¸ªç›¸ä¼¼æ€§æ–‡æ¡£çš„ç»“æœ**

# 2.å¸¦æœ‰è°·æ­Œé€šç”¨å¥å­ç¼–ç å™¨çš„æ–‡æ¡£æœç´¢å¼•æ“

## è°·æ­Œä½¿ç”¨ç®€ä»‹

é¢„å…ˆè®­ç»ƒçš„[é€šç”¨è¯­å¥ç¼–ç å™¨](https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html)åœ¨ [Tensorflow-hub](https://www.tensorflow.org/hub/) ä¸­å…¬å¼€ã€‚å®ƒæœ‰ä¸¤ç§å˜åŒ–ï¼Œå³ä¸€ç§ç”¨ [**å˜å‹å™¨ç¼–ç å™¨**](https://tfhub.dev/google/universal-sentence-encoder-large/5) è®­ç»ƒï¼Œå¦ä¸€ç§ç”¨ [**æ·±åº¦å¹³å‡ç½‘ç»œ(DAN)**](https://tfhub.dev/google/universal-sentence-encoder/4) è®­ç»ƒã€‚å®ƒä»¬æ˜¯åœ¨å¤§å‹è¯­æ–™åº“ä¸Šé¢„å…ˆè®­ç»ƒçš„ï¼Œå¯ä»¥ç”¨äºå„ç§ä»»åŠ¡(æƒ…æ„Ÿåˆ†æã€åˆ†ç±»ç­‰)ã€‚è¿™ä¸¤è€…åœ¨å‡†ç¡®æ€§å’Œè®¡ç®—èµ„æºéœ€æ±‚ä¹‹é—´æœ‰ä¸€ä¸ªæŠ˜è¡·ã€‚è™½ç„¶å…·æœ‰å˜æ¢å™¨ç¼–ç å™¨çš„æ–¹æ³•å…·æœ‰æ›´é«˜çš„ç²¾åº¦ï¼Œä½†æ˜¯å®ƒåœ¨è®¡ç®—ä¸Šæ›´æ˜‚è´µã€‚ä½¿ç”¨ DNA ç¼–ç çš„æ–¹æ³•åœ¨è®¡ç®—ä¸ŠèŠ±è´¹è¾ƒå°‘ï¼Œå¹¶ä¸”å‡†ç¡®æ€§ä¹Ÿç¨ä½ã€‚

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨ç¬¬äºŒä¸ªä¸¹é€šç”¨å¥å­ç¼–ç å™¨ï¼Œå¯åœ¨è¿™ä¸ªç½‘å€:- [è°·æ­Œä½¿ç”¨ä¸¹æ¨¡å‹](https://tfhub.dev/google/universal-sentence-encoder/4)

ä¸¤ä¸ªæ¨¡å‹éƒ½ä»¥ä¸€ä¸ªå•è¯ã€å¥å­æˆ–æ®µè½ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºä¸€ä¸ª **512** ç»´åº¦å‘é‡ã€‚

![](img/05ec3cbaab87c1fbf74e57a4ce29143b.png)

ä¸€ä¸ªåŸå‹è¯­ä¹‰æ£€ç´¢ç®¡é“ï¼Œç”¨äºæ–‡æœ¬ç›¸ä¼¼æ€§ã€‚

åœ¨ä½¿ç”¨å¼ é‡æµæ¢çº½æ¨¡å‹ä¹‹å‰ã€‚

**å…ˆå†³æ¡ä»¶:**

```
!pip install --upgrade tensorflow-gpu
 #Install TF-Hub.
!pip install tensorflow-hub
!pip install seaborn
```

ç°åœ¨å¯¼å…¥åŒ…:

```
import pandas as pd
import numpy as np
import re, string
import os 
import tensorflow as tf
import tensorflow_hub as hub
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics.pairwise import linear_kernel
```

ä»è°ƒç”¨ç›´æ¥ URL çš„ [TensorFlow-hub](https://tfhub.dev/google/universal-sentence-encoder/4) ä¸‹è½½æ¨¡å‹:

```
! curl -L -o 4.tar.gz "[https://tfhub.dev/google/universal-sentence-encoder/4?tf-hub-format=compressed](https://tfhub.dev/google/universal-sentence-encoder/4?tf-hub-format=compressed)"or
module_url = "[https://tfhub.dev/google/universal-sentence-encoder/4](https://tfhub.dev/google/universal-sentence-encoder/4)"
```

åŠ è½½è°·æ­Œä¸¹é€šç”¨å¥å­ç¼–ç å™¨

```
#Model load through local path:module_path ="/home/zettadevs/GoogleUSEModel/USE_4"
%time model = hub.load(module_path)#Create function for using model training
def embed(input):
    return model(input)
```

## ç”¨ä¾‹ 1:-å•è¯è¯­ä¹‰

```
WordMessage =[â€˜big dataâ€™, â€˜millions of dataâ€™, â€˜millions of recordsâ€™,â€™cloud computingâ€™,â€™awsâ€™,â€™azureâ€™,â€™saasâ€™,â€™bankâ€™,â€™accountâ€™]
```

![](img/7acadeae27507f5c1ac810032013f41c.png)

## ç”¨ä¾‹ 2:å¥å­è¯­ä¹‰

```
SentMessage =['How old are you?','what is your age?','how are you?','how you doing?']
```

![](img/8f7eff0ac3646c46794abaec7fb68b8a.png)

## ç”¨ä¾‹ 3:å•è¯ã€å¥å­å’Œæ®µè½è¯­ä¹‰

```
word ='Cloud computing'Sentence = 'what is cloud computing'Para =("Cloud computing is the latest generation technology with a high IT infrastructure that provides us a means by which we can use and utilize the applications as utilities via the internet."
        "Cloud computing makes IT infrastructure along with their services available 'on-need' basis." 
        "The cloud technology includes - a development platform, hard disk, computing power, software application, and database.")Para5 =(
    "Universal Sentence Encoder embeddings also support short paragraphs. "
    "There is no hard limit on how long the paragraph is. Roughly, the longer "
    "the more 'diluted' the embedding will be.")Para6 =("Azure is a cloud computing platform which was launched by Microsoft in February 2010."
       "It is an open and flexible cloud platform which helps in development, data storage, service hosting, and service management."
       "The Azure tool hosts web applications over the internet with the help of Microsoft data centers.")
case4Message=[word,Sentence,Para,Para5,Para6]
```

![](img/bec7f52e225b3494e8e4d2e82c630410.png)

# è®­ç»ƒæ¨¡å‹

è¿™é‡Œï¼Œæˆ‘ä»¬ä»¥æ‰¹å¤„ç†æ–¹å¼è®­ç»ƒæ•°æ®é›†ï¼Œå› ä¸ºç”Ÿæˆæ•°æ®é›†çš„å›¾å½¢éœ€è¦å¾ˆé•¿çš„æ‰§è¡Œæ—¶é—´ã€‚å› æ­¤ï¼Œæ›´å¥½åœ°è®­ç»ƒæ‰¹é‡æ•°æ®ã€‚

```
Model_USE= embed(df_news.content[0:2500])
```

**ä¿å­˜æ¨¡å‹**ï¼Œä»¥ä¾¿é‡ç”¨æ¨¡å‹ã€‚

```
exported = tf.train.Checkpoint(v=tf.Variable(Model_USE))
exported.f = tf.function(
    lambda  x: exported.v * x,
    input_signature=[tf.TensorSpec(shape=None, dtype=tf.float32)])tf.saved_model.save(exported,'/home/zettadevs/GoogleUSEModel/TrainModel')
```

**ä»è·¯å¾„:**åŠ è½½æ¨¡å‹

```
imported = tf.saved_model.load(â€˜/home/zettadevs/GoogleUSEModel/TrainModel/â€™)
loadedmodel =imported.v.numpy()
```

**æ–‡ä»¶æœç´¢åŠŸèƒ½:**

```
def SearchDocument(query):
    q =[query]
    # embed the query for calcluating the similarity
    Q_Train =embed(q)

    #imported_m = tf.saved_model.load('/home/zettadevs/GoogleUSEModel/TrainModel')
    #loadedmodel =imported_m.v.numpy()
    # Calculate the Similarity
    linear_similarities = linear_kernel(Q_Train, con_a).flatten() 
    #Sort top 10 index with similarity score
    Top_index_doc = linear_similarities.argsort()[:-11:-1]
    # sort by similarity score
    linear_similarities.sort()
    a = pd.DataFrame()
    for i,index in enumerate(Top_index_doc):
        a.loc[i,'index'] = str(index)
        a.loc[i,'File_Name'] = df_news['Subject'][index] ## Read File name with index from File_data DF
    for j,simScore in enumerate(linear_similarities[:-11:-1]):
        a.loc[j,'Score'] = simScore
    return a
```

**æµ‹è¯•æœç´¢:**

```
SearchDocument('computer science')
```

![](img/27504e01cef8cb49cbf404063f2564cc.png)

æ‰¾åˆ°è¯¥é¡¹ç›®çš„[](https://github.com/zayedrais/DocumentSearchEngine)

# ****ç»“è®º:****

****åœ¨æœ¬æ•™ç¨‹çš„æœ€åï¼Œæˆ‘ä»¬å¾—å‡ºç»“è®ºï¼Œâ€œè°·æ­Œé€šç”¨å¥å­ç¼–ç å™¨â€æ¨¡å‹æä¾›äº†è¯­ä¹‰æœç´¢ç»“æœï¼Œè€Œ TF-IDF æ¨¡å‹ä¸çŸ¥é“å•è¯çš„æ„æ€ã€‚åªæ˜¯æ ¹æ®æ–‡æ¡£ä¸­å¯ç”¨çš„å•è¯ç»™å‡ºç»“æœã€‚****

******ä¸€äº›å‚è€ƒæ–‡çŒ®:******

*   ****[TF-IDF](https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089)****
*   ****[è°·æ­Œä½¿ç”¨](https://ai.googleblog.com/2018/05/advances-in-semantic-textual-similarity.html)****

# ****åˆ«å¿˜äº†ç»™æˆ‘ä»¬ä½ çš„ğŸ‘ï¼****