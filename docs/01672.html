<html>
<head>
<title>Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/natural-language-processing-faf89fffce56?source=collection_archive---------12-----------------------#2019-11-07">https://medium.com/analytics-vidhya/natural-language-processing-faf89fffce56?source=collection_archive---------12-----------------------#2019-11-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="3de7" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">定义:</h2></div><p id="4f10" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">自然语言处理(NLP)是语言学、计算机科学、信息工程和人工智能的一个子领域，涉及计算机和人类(自然)语言之间的交互，特别是如何对计算机进行编程以处理和分析大量自然语言数据。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/4eac11a6c3d01022a9ec7fc518e4cd31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZzwGjd6pMo6mt752BvGMPQ.jpeg"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">自然语言处理图像</figcaption></figure><h1 id="0170" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">自然语言处理的应用:</h1><p id="a00e" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">垃圾邮件过滤器(Gmail单独过滤垃圾邮件)。<br/>·自然语言生成(从图像或视频数据生成文本)。<br/>·语音识别(Google WebSpeech或Vocalware)。<br/>·文本摘要(Smmry或Reddit的autotldr给出句子摘要)。<br/>·情绪分析(Hater News给我们用户的情绪)。<br/>·自动预测(谷歌搜索预测用户搜索结果)。<br/>·机器翻译(谷歌翻译将语言从一种语言翻译成另一种语言)。</p><h1 id="2405" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">数据集信息</h1><p id="7f8d" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated"><strong class="iz hj">餐厅评论数据集:- </strong> <br/>该数据集包含以下2个变量的2000个数据点。<br/>1。查看<br/>2。喜欢的<br/><strong class="iz hj">喜欢的</strong>是数据集中的响应变量。<br/><strong class="iz hj">回顾</strong>是数据集中的特征变量。<br/>下载数据集。<a class="ae lg" href="https://www.kaggle.com/akram24/restaurant-reviews/download" rel="noopener ugc nofollow" target="_blank"> <strong class="iz hj">餐厅点评</strong> </a></p><h1 id="209e" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">阅读和探索数据集</h1><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="5031" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj"><em class="lr"># Reading the raw data</em><br/></strong>raw_data=open("Restaurant_Reviews.tsv").read()<br/>raw_data[1:1001]</span></pre><p id="1f23" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">Output</strong>:“eview \ t liked \ n多么……喜欢这个地方。\ t1 \信任不好。\ t0 \不好吃，质地也很差。\ t0 \在五月底的银行假日期间，在Rick Steve的推荐下来到了这家店，非常喜欢。\ t1 \菜单上的选择很棒，价格也不错。\ t1 \现在我生气了，我要我那该死的照片。\ t0 \老实说，味道没那么新鲜。)0 \该"</p><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="7dbe" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj"><em class="lr"># Importing the libraries</em><br/></strong>import numpy as np<br/>import matplotlib.pyplot as plt<br/>import pandas as pd</span><span id="55eb" class="lm kk hi li b fi ls lo l lp lq"><strong class="li hj"><em class="lr"># Importing the dataset</em><br/></strong>data=pd.read_csv("D:\\...<strong class="li hj">\\</strong>Restaurant_Reviews.tsv",delimiter="<strong class="li hj">\t</strong>",quoting=3)<br/>data.head(2)</span></pre><p id="75dc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lt"><img src="../Images/7ca44d9c1678a6e9e7182a11ab7e931d.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*Jub41e2DrLIdbDN6G77iPg.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">数据集样本</figcaption></figure><h1 id="e219" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">数据预处理</h1><h2 id="bb03" class="lm kk hi bd kl lu lv lw kp lx ly lz kt jg ma mb kv jk mc md kx jo me mf kz mg bi translated">1.去掉标点符号</h2><p id="78a1" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">更多信息请点击此处。<a class="ae lg" href="https://en.wikipedia.org/wiki/Punctuation" rel="noopener ugc nofollow" target="_blank"> <strong class="iz hj">标点</strong> </a> <br/>它去掉了所有的标点符号(！" #$% &amp; '()*+，-。/:;&lt; = &gt;？[]^_`{|}~)</p><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="5eaf" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj">import string</strong><br/>print(string.punctuation)</span></pre><p id="b053" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:(！" #$% &amp; '()*+，-。/:;&lt; = &gt;？[]^_`{|}~)</p><p id="ee6e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以上是所有的<strong class="iz hj">标点符号。</strong></p><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="945d" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj"><em class="lr"># Function to remove punctuation</em><br/>def</strong> remove_punct(text):<br/>    text_nopunct="".join([char <strong class="li hj">for</strong> char <strong class="li hj">in</strong> text <strong class="li hj">if</strong> char <strong class="li hj">not</strong> <strong class="li hj">in</strong> string.punctuation]) <em class="lr"># It will discard all punctuation</em><br/>    <strong class="li hj">return</strong> text_nopunct<br/>data["Review_clean"]=data["Review"].apply(<strong class="li hj">lambda</strong> x:remove_punct(x))<br/>data.head(2)</span></pre><p id="4be9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mh"><img src="../Images/c83a88d266af47bafa31d004da2c22d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*Q27Q5bkNI_5vTWH6wbbWKQ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">去除标点符号后的数据集样本</figcaption></figure><h2 id="a496" class="lm kk hi bd kl lu lv lw kp lx ly lz kt jg ma mb kv jk mc md kx jo me mf kz mg bi translated">2.标记化</h2><p id="0494" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">它把句子分解成单词。更多信息，请点击此处。<a class="ae lg" href="https://www.guru99.com/tokenize-words-sentences-nltk.html" rel="noopener ugc nofollow" target="_blank"> <strong class="iz hj">标记化</strong> </a></p><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="731f" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj">import re<br/><em class="lr"># Function to Tokenize  words</em><br/>def</strong> tokenize(text):<br/>    tokens=re.split('\W+', text) <em class="lr"># w+ means that either a word character (A-Z,a-z,0-9)</em><br/>    <strong class="li hj">return</strong> tokens<br/>data["Review_tokenized"]=data["Review_clean"].apply(<strong class="li hj">lambda</strong> x:tokenize(x))<br/>data.head(2)</span></pre><p id="7bc6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mi"><img src="../Images/d52b466cc2ea514d0d1fba60255fae4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*y3u7nSV8LuEAv0c9N2j6mQ.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">应用标记化后的数据集样本</figcaption></figure><h2 id="2f1a" class="lm kk hi bd kl lu lv lw kp lx ly lz kt jg ma mb kv jk mc md kx jo me mf kz mg bi translated">3.删除停用词</h2><p id="4e3b" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">它删除了<strong class="iz hj">停止字</strong>。<br/>更多信息点击此处<a class="ae lg" href="https://www.geeksforgeeks.org/removing-stop-words-nltk-python" rel="noopener ugc nofollow" target="_blank"> <strong class="iz hj">停用词</strong> </a> <strong class="iz hj">。</strong></p><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="8efa" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj">import nltk<br/></strong>stopwords=nltk.corpus.stopwords.words("english") <strong class="li hj"><em class="lr"># All englishwords</em></strong></span><span id="87c3" class="lm kk hi li b fi ls lo l lp lq"><strong class="li hj"><em class="lr"># Function to remove stopwords</em><br/>def</strong> remove_stopwords(tokenized_list):<br/>    text=[word <strong class="li hj">for</strong> word <strong class="li hj">in</strong> tokenized_list <strong class="li hj">if</strong> word <strong class="li hj">not</strong> <strong class="li hj">in</strong> stopwords] <strong class="li hj"><em class="lr"># To remove all stopwords</em><br/></strong>    <strong class="li hj">return</strong> text<br/>data["Review_nostop"]=data["Review_tokenized"].apply (<strong class="li hj">lambda</strong> x:remove_stopwords(x))<br/>data.head(2)</span></pre><p id="38f5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mj"><img src="../Images/0205fe2e01ca185c2ab215b07943272b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I8dK7musG4LNJzH2Bxkk2A.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">去除停用词后的数据集样本</figcaption></figure><h1 id="091c" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">堵塞物</h1><p id="3a83" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">它将一个单词简化为其词干形式。它去掉了足够的词，如“ing”、“ly”、“s”等。通过简单的基于规则的方法。它减少了单词集，但是实际的单词经常被忽略。例如:Entitling，Entitled- &gt; Entitl <br/>了解更多信息<a class="ae lg" href="https://www.geeksforgeeks.org/python-stemming-words-with-nltk/" rel="noopener ugc nofollow" target="_blank"> <strong class="iz hj">词干</strong> </a>。</p><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="4fa9" class="lm kk hi li b fi ln lo l lp lq">ps=nltk.PorterStemmer()<br/><strong class="li hj">def</strong> stemming(tokenized_text):<br/>    text=[ps.stem(word) <strong class="li hj">for</strong> word <strong class="li hj">in</strong> tokenized_text]<br/>    <strong class="li hj">return</strong> text<br/>data["Review_stemmed"]=data["Review_nostop"].apply(<strong class="li hj">lambda</strong> x:stemming(x))<br/>data.head(2)</span></pre><p id="f541" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mk"><img src="../Images/a7a6f806a56a17e818b4e32b2f454a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Y8MSiLF5HGxuFIvjNvo0g.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">应用词干后的数据集示例</figcaption></figure><h2 id="2430" class="lm kk hi bd kl lu lv lw kp lx ly lz kt jg ma mb kv jk mc md kx jo me mf kz mg bi translated">词汇化</h2><p id="2d45" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">它推导出一个单词的标准形式(“引理”)。即根形式。它比词干分析法要好，因为它使用了一种基于词典的方法，即对词根进行词法分析。<br/>了解更多信息<a class="ae lg" href="https://www.machinelearningplus.com/nlp/lemmatization-examples-python/" rel="noopener ugc nofollow" target="_blank"> <strong class="iz hj">词条化</strong> </a>。</p><p id="01b8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> nltk.download() <br/> </strong>为了下载<strong class="iz hj"> wordnet </strong>和不同的包对于不同的<strong class="iz hj"> NLP </strong>使用上面的代码。</p><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="ece6" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj">wn=nltk.WordNetLemmatizer()<br/></strong> <br/><strong class="li hj">def</strong> lemmatizing(tokenized_text):<br/>    text=[wn.lemmatize(word) <strong class="li hj">for</strong> word <strong class="li hj">in</strong> tokenized_text]<br/>    <strong class="li hj">return</strong> text<br/>data["Review_lemmatized"]=data["Review_nostop"].apply(<strong class="li hj">lambda</strong> x:lemmatizing(x))<br/>data.head(2)</span></pre><p id="6d97" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es ml"><img src="../Images/1e1315905e9babe5823e097408bd791c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A-41hglGR-MIrIMNLOflrw.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">应用术语化后的数据集样本</figcaption></figure><h1 id="f7ac" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">向量化数据:词汇袋</h1><p id="3c53" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">你需要把这些文本转换成一些数字或者数字的向量。词袋模型(BoW)是从文本中提取特征的最简单的方法。BoW将文本转换成文档中单词出现的矩阵。</p><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="a454" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj">from sklearn.feature_extraction.text import CountVectorizer<br/></strong>count_vect=CountVectorizer()<br/>X_counts=count_vect.fit_transform(data["Review"])<br/>print(X_counts.shape)</span></pre><p id="4046" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong> :(1000，2035)</p><h1 id="6f87" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">向量化数据:N-grams</h1><p id="287c" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">它是一系列大型文本或句子中n个项目的一组共现或连续序列。这里的项目可以是单词、字母和音节。1-gram也称为unigrams，因为它是句子中出现的唯一单词。Bigram(2-gram)是两个单词的组合。三元组是3个单词，以此类推。</p><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="8f58" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj">from sklearn.feature_extraction.text import CountVectorizer<br/></strong>ngram_vect=CountVectorizer(ngram_range=(2,2))<br/>X_counts=ngram_vect.fit_transform(data['Review'])<br/>print(X_counts.shape)<br/>print(ngram_vect.get_feature_names())</span></pre><p id="8a94" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong> :(1000，6691) <br/> ['10分钟'，' 10次'，' 100推荐'，' 100次'，' 11个99 '，' 12个可笑'，' 15分钟'，' 17个汉堡'，' 1979和'，' 20…]</p><h2 id="e5ce" class="lm kk hi bd kl lu lv lw kp lx ly lz kt jg ma mb kv jk mc md kx jo me mf kz mg bi translated">向量化数据:TF-IDF</h2><p id="0f7e" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">它计算一个单词在一个文档中出现的“相对频率”,与它在所有文档中出现的频率进行比较。对于识别每个文档中的“重要”单词(在该文档中出现频率高，在其他文档中出现频率低)，它比“术语频率”更有用。<br/> NLTK不支持tf-idf。因此，我们将使用scikit-learn。scikit-learn有一个内置的tf-Idf实现，而我们仍然利用NLTK的标记器和词干分析器来预处理文本。</p><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="f9e7" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj">from sklearn.feature_extraction.text import TfidfVectorizer<br/></strong>tfidf_vect=TfidfVectorizer()<br/>X_tfidf=tfidf_vect.fit_transform(data['Review'])<br/>print(X_tfidf.shape)<br/>print(tfidf_vect.get_feature_names())</span></pre><p id="6bed" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong> : (1000，2035) <br/> ['00 '，' 10 '，' 100 '，' 11 '，' 12 '，' 15 '，' 17 '，' 1979 '，' 20 '，' 2007 '，' 23 '，' 30 '，' 30s '，' 35 '，' 40 '，' 40分钟'，' 45 '，' 4th '，' 5lb '，' 70 '，' 85 '，' 90 '，' 99 '，'大约'，'以上'，，…]</p><h2 id="c4e7" class="lm kk hi bd kl lu lv lw kp lx ly lz kt jg ma mb kv jk mc md kx jo me mf kz mg bi translated">特征创建:</h2><p id="36ea" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">特征工程是使用数据的领域知识来创建使机器学习算法工作的特征的过程。它就像一门艺术，因为它需要领域知识，并且创建特征可能很困难，但是ML算法预测结果可能是富有成效的，因为它们可能与预测相关。</p><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="409f" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj">import string<br/><em class="lr"># Function to calculate length of message excluding space</em><br/></strong>data["Review_len"]=data["Review"].apply(<strong class="li hj">lambda</strong> x: len(x)-x.count(" "))<br/><br/><strong class="li hj">def</strong> count_punct(text):<br/>    count=sum([1 <strong class="li hj">for</strong> char <strong class="li hj">in</strong> text <strong class="li hj">if</strong> char <strong class="li hj">in</strong> string.punctuation])<br/>    <strong class="li hj">return</strong> round(count/(len(text)-text.count(" ")),3)*100<br/>data["punct%"] = data["Review"].apply(<strong class="li hj">lambda</strong> x:count_punct(x))<br/>data.head(2)</span></pre><p id="a6fd" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mm"><img src="../Images/987dd972e58117adbfb384a12bd54997.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q5d9emLRXtilFvla5X0EJA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">统计单词后的数据集样本</figcaption></figure><h1 id="9236" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">绘制数据集</h1><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="e001" class="lm kk hi li b fi ln lo l lp lq">bins=np.linspace(0,200,40)<br/>plt.hist(data[data['Liked']==1]['Review_len'],bins,alpha=0.5,normed=<strong class="li hj">True</strong>,label=1)<br/>plt.hist(data[data['Liked']==0]['Review_len'],bins,alpha=0.5,normed=<strong class="li hj">True</strong>,label=0)<br/>plt.legend(loc="upper right")<br/>plt.show()</span></pre><p id="5a15" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mn"><img src="../Images/8dc7498801ba444010ccdaec132a44e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*3reJG4y0A_GuDY2Xlrow5w.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">评论的直方图_长度</figcaption></figure><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="9e8b" class="lm kk hi li b fi ln lo l lp lq">bins=np.linspace(0,50,40)<br/>plt.hist(data[data['Liked']==1]['punct%'],bins,alpha=0.5,normed=<strong class="li hj">True</strong>,label=1)<br/>plt.hist(data[data['Liked']==0]['punct%'],bins,alpha=0.5,normed=<strong class="li hj">True</strong>,label=0)<br/>plt.legend(loc="upper right")<br/>plt.show()</span></pre><p id="eba7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mo"><img src="../Images/edd1beeee94a2e0d3f7d9f74f83300fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*9kU0UmmZ0dgYNeI5I1MoPA.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">标点符号百分比直方图</figcaption></figure><h2 id="e6aa" class="lm kk hi bd kl lu lv lw kp lx ly lz kt jg ma mb kv jk mc md kx jo me mf kz mg bi translated">型号选择</h2><p id="b56e" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">我们使用机器学习的集成方法，其中使用多个模型，并且它们的组合产生比单个模型更好的结果(支持向量机/朴素贝叶斯)。合奏方法是许多Kaggle比赛的首选。随机森林，即构建多个随机决策树，每棵树的集合用于最终预测。它可以用于分类以及回归问题。它遵循随机打包策略。</p><p id="e954" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">网格搜索</strong>:在给定的网格中彻底搜索所有参数组合，以确定最佳模型。</p><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="0099" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj">from</strong> <strong class="li hj">sklearn.ensemble</strong> <strong class="li hj">import</strong> <strong class="li hj">RandomForestClassifier</strong><br/><strong class="li hj">from</strong> <strong class="li hj">sklearn.model_selection</strong> <strong class="li hj">import</strong> GridSearchCV<br/>rf=RandomForestClassifier()<br/>param={'n_estimators':[10,150,300],'max_depth':[30,60,90,<strong class="li hj">None</strong>]}<br/>gs=GridSearchCV(rf,param,cv=5,n_jobs=-1)<em class="lr"># For parallelizing the speech</em><br/>gs_fit=gs.fit(X_counts,data['Liked'])<br/>pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score',ascending=<strong class="li hj">False</strong>).head()</span></pre><p id="f529" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mp"><img src="../Images/f85bf360a30f228525d36aeab80347b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2cussI3TF4r2fRuMVPMp4A.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">应用随机森林分类器后X_counts值的样本</figcaption></figure><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="d3e8" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj">from</strong> <strong class="li hj">sklearn.ensemble</strong> <strong class="li hj">import</strong> RandomForestClassifier<br/><strong class="li hj">from</strong> <strong class="li hj">sklearn.model_selection</strong> <strong class="li hj">import</strong> GridSearchCV<br/>rf=RandomForestClassifier()<br/>param={'n_estimators':[10,150,300],'max_depth':[30,60,90,<strong class="li hj">None</strong>]}<br/>gs=GridSearchCV(rf,param,cv=5,n_jobs=-1)<em class="lr"># For parallelizing the speech</em><br/>gs_fit=gs.fit(X_tfidf,data['Liked'])<br/>pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score',ascending=<strong class="li hj">False</strong>).head()</span></pre><p id="0339" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mq"><img src="../Images/bfeeae12d738454416c9c9f41db226f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Me8Uvrg1zEWR-vcFWPZRgA.png"/></div></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">应用随机森林分类器后X_TFIDF值的示例</figcaption></figure><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="ddad" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj"># Spliting the dataset into X and y variables<br/></strong>corpus=X_counts<br/>X = corpus.toarray()<br/>y = data.iloc[:, 1].values<br/><br/><strong class="li hj"><em class="lr"># Splitting the dataset into the Training set and Test set</em><br/>from</strong> <strong class="li hj">sklearn.model_selection</strong> <strong class="li hj">import</strong> train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)<br/><br/><strong class="li hj"><em class="lr"># Fitting Random Forest Classifier to the Training set</em><br/>from</strong> <strong class="li hj">sklearn.ensemble</strong> <strong class="li hj">import</strong> RandomForestClassifier<br/>classifier = RandomForestClassifier()<br/>gs.fit(X_train, y_train)<br/><br/><strong class="li hj"><em class="lr"># Predicting the Test set results</em><br/></strong>y_pred = gs.predict(X_test)<br/><br/><strong class="li hj"><em class="lr"># Making the Confusion Matrix</em><br/>from</strong> <strong class="li hj">sklearn.metrics</strong> <strong class="li hj">import</strong> confusion_matrix<br/>cm = confusion_matrix(y_test, y_pred)<br/>print(cm)</span></pre><p id="fd2b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mr"><img src="../Images/8165dbffa2a0771b32d30a5fa517ceb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*dn019p_Dfv2uHOdKLGev4g.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">混淆矩阵</figcaption></figure><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="ddd6" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj">import</strong> <strong class="li hj">seaborn</strong> <strong class="li hj">as</strong> <strong class="li hj">sns</strong><br/>sns.heatmap(cm,annot=<strong class="li hj">True</strong>)<br/>plt.figure(figsize=(10,10))</span></pre><p id="c714" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es ms"><img src="../Images/68d2503ed2022d76e0b1ee858f953115.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*A4QqSkSAiU2vXySjxHSQ4g.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">混乱矩阵图</figcaption></figure><pre class="ju jv jw jx fd lh li lj lk aw ll bi"><span id="9de6" class="lm kk hi li b fi ln lo l lp lq"><strong class="li hj">from</strong> <strong class="li hj">sklearn.metrics</strong> <strong class="li hj">import</strong> classification_report<br/>cr=classification_report(y_test,y_pred)<br/>print(cr) </span></pre><p id="224f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">输出</strong>:</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mt"><img src="../Images/9e5563825cac143a7d0e9ef6639fddd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1170/format:webp/1*_5owZZRz5aXo_R06JDXRVg.png"/></div><figcaption class="kf kg et er es kh ki bd b be z dx translated">分类报告值</figcaption></figure><h1 id="cb87" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated">结论</h1><p id="7ff7" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated">因此，我们通过使用随机森林分类器获得了65%的准确性。由于唯一性，我们不能获得超过60%的准确性。为了提高，我们需要使用评级系统，该系统大大提高了我们训练数据中的信息，以便我们可以获得更高的准确性</p><h2 id="d28d" class="lm kk hi bd kl lu lv lw kp lx ly lz kt jg ma mb kv jk mc md kx jo me mf kz mg bi translated"><strong class="ak"> Github要点:</strong></h2><p id="18f9" class="pw-post-body-paragraph ix iy hi iz b ja lb ij jc jd lc im jf jg ld ji jj jk le jm jn jo lf jq jr js hb bi translated"><a class="ae lg" href="https://gist.github.com/Priyanka-Makineni/aa69202e874594648792a03106da4211" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/Priyanka-makin Eni/aa 69202 e 874594648792 a 03106 da 4211</a></p><p id="8ad5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">我的Github: </strong></p><div class="mu mv ez fb mw mx"><a href="https://github.com/Priyanka-Makineni/Data-science-codes" rel="noopener  ugc nofollow" target="_blank"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hj fi z dy nc ea eb nd ed ef hh bi translated">priyanka-makin Eni/数据科学代码</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">数据是新的科学。数据科学掌握着“答案”</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">github.com</p></div></div></div></a></div><h1 id="a110" class="kj kk hi bd kl km kn ko kp kq kr ks kt io ku ip kv ir kw is kx iu ky iv kz la bi translated"><strong class="ak">参考文献:</strong></h1><ol class=""><li id="b973" class="ng nh hi iz b ja lb jd lc jg ni jk nj jo nk js nl nm nn no bi translated">分析Vidhya</li><li id="739f" class="ng nh hi iz b ja np jd nq jg nr jk ns jo nt js nl nm nn no bi translated">极客为极客</li><li id="46e1" class="ng nh hi iz b ja np jd nq jg nr jk ns jo nt js nl nm nn no bi translated">向数据科学进军</li></ol></div></div>    
</body>
</html>