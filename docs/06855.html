<html>
<head>
<title>How the decision tree takes the decisions?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树如何做出决策？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-the-decision-tree-takes-the-decisions-31749439deac?source=collection_archive---------20-----------------------#2020-06-04">https://medium.com/analytics-vidhya/how-the-decision-tree-takes-the-decisions-31749439deac?source=collection_archive---------20-----------------------#2020-06-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/228c736cf368e8a22fcb34f92d66910a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hIBiL3i4ON_Nwx-TUo496Q.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="http://www.theguardian.pe.ca/" rel="noopener ugc nofollow" target="_blank"> www.theguardian.pe.ca </a></figcaption></figure><p id="49b8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">决策树是一种非常流行的监督机器学习算法，用于回归和分类。决策树是随机森林、XGBoost等更健壮的机器学习模型的构建块。这是一个非常简单的机器学习模型，即使对于外行来说也很容易理解。有多种决策树算法可用，例如ID3、C4.5和CART用于小数据集，BOAT用于大数据集。</p><p id="6fe8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">要理解决策树，了解决策树的结构很重要。下面是对其结构的简单阐述。在机器学习的语言中，第一个分裂节点被称为根节点，进一步分裂的节点被称为内部节点。如果没有进一步的数据分割，那么它被称为叶节点。到达叶节点的最长路径称为决策树的深度。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/c9c5b8b7d6078962f4de06e3d6a51c08.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*FfKPq1MawUfKpSn8ex9aJA.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">决策树结构</figcaption></figure><p id="9993" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们用一个小的心脏病数据集，并对其应用决策树算法。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jy"><img src="../Images/0eca58cb6d962ffed78f7fc117501f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*i_26fthMelQTdXp8ruSQQw.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">数据及其决策树</figcaption></figure><p id="76b1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里问题就来了，决策树怎么知道70.5 kg是拆分数据的最佳点！让我们弄清楚。</p><p id="fc6d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第一步:将权重按升序排序，计算所有的中点。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jz"><img src="../Images/bac0c91fbe9a40a2df091c4d0edeb37a.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*gJLaCONqD8c_kYZGk0FMVw.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">寻找中点</figcaption></figure><p id="4c2f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第二步:以53 kg为拆分点拆分数据，计算基尼指数。基尼系数的公式如下，其中p代表是或否的概率。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es ka"><img src="../Images/3218f5f70d5087fbc5dbbb13d199d710.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/format:webp/1*xEIN0i5tbQHlNi7RqQLm1g.jpeg"/></div></figure><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kb"><img src="../Images/1158c5b4d0890a66abab62d8f4fe3787.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*w2CHBousiZQKD1yIhmcpig.jpeg"/></div></figure><p id="d196" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">步骤3:对每个分割点重复步骤2。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kc"><img src="../Images/a275320193ba85714b43f580dd801906.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*Ieh0OzO5ToV4D9afczJ1RQ.jpeg"/></div></figure><p id="593a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由于基尼指数衡量节点的杂质，决策树算法选择杂质最小的分裂点。在我们的例子中，0.187是最小的杂质，因此相应的70.5 kg是最佳分裂点。决策树算法重复它以创建具有更高深度的决策树。</p><p id="c361" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我希望这篇文章对你理解决策树的内部工作有所帮助。</p></div></div>    
</body>
</html>