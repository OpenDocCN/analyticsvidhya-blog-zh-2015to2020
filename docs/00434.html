<html>
<head>
<title>Predicting the Testing duration of the Mercedes-Benz during Manufacturing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测梅赛德斯-奔驰在制造过程中的测试持续时间</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mercedes-benz-greener-manufacturing-85d7621b5b08?source=collection_archive---------0-----------------------#2019-06-14">https://medium.com/analytics-vidhya/mercedes-benz-greener-manufacturing-85d7621b5b08?source=collection_archive---------0-----------------------#2019-06-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="49e9" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">更快的测试将有助于降低排放和更高效的生产线</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/44238a99ad8a0a64bf81b6dbaab4db8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YUtRgwlpEOV6MYSehf2Iew.jpeg"/></div></figure><p id="028f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">作者注:这是我用应用人工智能课程完成梅赛德斯-奔驰绿色制造项目后的博客。在这个博客中，我将写下这个方法，并展示我的项目的结果。所有的作品都是原创的，可以使用/扩展/传播。</p><h1 id="dae8" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated"><strong class="ak">业务问题:</strong></h1><p id="ba7b" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">自1886年第一辆汽车——奔驰专利汽车问世以来，梅赛德斯-奔驰一直代表着重要的汽车创新。例如，这包括带有防撞缓冲区的乘客安全单元、安全气囊和智能辅助系统。梅赛德斯-奔驰每年申请近2000项专利，使该品牌成为欧洲高档汽车制造商的领导者。戴姆勒的梅赛德斯-奔驰汽车是高档汽车行业的领导者。凭借丰富的功能和选项，客户可以选择他们梦想中的定制梅赛德斯-奔驰。。</p><p id="9f67" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">为了确保每一个独特的汽车配置在上路前的安全性和可靠性，戴姆勒的工程师们开发了一个强大的测试系统。但是，如果没有强大的算法方法，为如此多可能的功能组合优化测试系统的速度是复杂和耗时的。作为世界上最大的高档汽车制造商之一，安全和效率是戴姆勒生产线的重中之重。</p><p id="1f96" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在这场比赛中，戴姆勒正在挑战卡格勒，以解决维度的诅咒，并减少汽车在测试台上花费的时间。竞争对手将使用代表梅赛德斯-奔驰汽车功能不同排列的数据集来预测通过测试所需的时间。获胜的算法将有助于加快测试速度，从而在不降低戴姆勒标准的情况下降低二氧化碳排放。</p><p id="9145" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">问题陈述:</strong>预测目标变量y Iie汽车需要通过测试的时间(秒)。</p><p id="5f9a" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">现实世界/业务目标和约束:</strong> <br/> 1:用高值R(决定系数)预测以秒为单位的时间。<br/> 2:没有严格的延迟限制。</p><h1 id="f6b6" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">数据描述:</h1><p id="bd51" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">该数据集包含一组匿名的变量，每个变量代表一辆奔驰汽车的定制功能。例如，变量可以是4WD、附加空气悬架或平视显示器。</p><p id="7ea3" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">地面实况标记为“y ”,代表汽车通过每个变量测试的时间(秒)。</p><h2 id="6f0d" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated"><strong class="ak">机器学习问题公式化:</strong></h2><p id="06ee" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated"><strong class="jh hj">数据描述:</strong>例如，变量可以是4WD、附加空气悬架或平视显示器。</p><p id="ad1f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">地面实况标记为“y ”,代表汽车通过每个变量测试的时间(秒)。</p><p id="5478" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">文件描述:</strong> <br/>带字母的变量是分类变量。带有0/1的变量是二进制值。train.csv —训练集test.csv —测试集，您必须以正确的格式预测此文件sample_submission.csv —示例提交文件中“ID”的“y”变量。下面是下载文件的链接。</p><div class="lm ln ez fb lo lp"><a href="https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/data" rel="noopener  ugc nofollow" target="_blank"><div class="lq ab dw"><div class="lr ab ls cl cj lt"><h2 class="bd hj fi z dy lu ea eb lv ed ef hh bi translated">梅赛德斯-奔驰绿色制造</h2><div class="lw l"><h3 class="bd b fi z dy lu ea eb lv ed ef dx translated">你能减少一辆奔驰花在测试台上的时间吗？</h3></div><div class="lx l"><p class="bd b fp z dy lu ea eb lv ed ef dx translated">www.kaggle.com</p></div></div><div class="ly l"><div class="lz l ma mb mc ly md jd lp"/></div></div></a></div><h2 id="2564" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">E.D.A关于<strong class="ak">奔驰绿色制造</strong>数据集:</h2><p id="e41a" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">因为我们只有378个特征，所以我们做了<strong class="jh hj">列分析</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es me"><img src="../Images/c7ec39ba9e213b1070c404c5bd615958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KKKmKbCr3RPEF-Fi0FGq0w.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated"><strong class="bd kd">如我们所见，我们有8个分类特征具有超过EN 2值，其余的是二进制数字特征</strong></figcaption></figure><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="a7b6" class="ky kc hi mo b fi ms mt l mu mv">Columns containing the unique values :  [0, 1]<br/>['X10', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49', 'X50', 'X51', 'X52', 'X53', 'X54', 'X55', 'X56', 'X57', 'X58', 'X59', 'X60', 'X61', 'X62', 'X63', 'X64', 'X65', 'X66', 'X67', 'X68', 'X69', 'X70', 'X71', 'X73', 'X74', 'X75', 'X76', 'X77', 'X78', 'X79', 'X80', 'X81', 'X82', 'X83', 'X84', 'X85', 'X86', 'X87', 'X88', 'X89', 'X90', 'X91', 'X92', 'X94', 'X95', 'X96', 'X97', 'X98', 'X99', 'X100', 'X101', 'X102', 'X103', 'X104', 'X105', 'X106', 'X108', 'X109', 'X110', 'X111', 'X112', 'X113', 'X114', 'X115', 'X116', 'X117', 'X118', 'X119', 'X120', 'X122', 'X123', 'X124', 'X125', 'X126', 'X127', 'X128', 'X129', 'X130', 'X131', 'X132', 'X133', 'X134', 'X135', 'X136', 'X137', 'X138', 'X139', 'X140', 'X141', 'X142', 'X143', 'X144', 'X145', 'X146', 'X147', 'X148', 'X150', 'X151', 'X152', 'X153', 'X154', 'X155', 'X156', 'X157', 'X158', 'X159', 'X160', 'X161', 'X162', 'X163', 'X164', 'X165', 'X166', 'X167', 'X168', 'X169', 'X170', 'X171', 'X172', 'X173', 'X174', 'X175', 'X176', 'X177', 'X178', 'X179', 'X180', 'X181', 'X182', 'X183', 'X184', 'X185', 'X186', 'X187', 'X189', 'X190', 'X191', 'X192', 'X194', 'X195', 'X196', 'X197', 'X198', 'X199', 'X200', 'X201', 'X202', 'X203', 'X204', 'X205', 'X206', 'X207', 'X208', 'X209', 'X210', 'X211', 'X212', 'X213', 'X214', 'X215', 'X216', 'X217', 'X218', 'X219', 'X220', 'X221', 'X222', 'X223', 'X224', 'X225', 'X226', 'X227', 'X228', 'X229', 'X230', 'X231', 'X232', 'X234', 'X236', 'X237', 'X238', 'X239', 'X240', 'X241', 'X242', 'X243', 'X244', 'X245', 'X246', 'X247', 'X248', 'X249', 'X250', 'X251', 'X252', 'X253', 'X254', 'X255', 'X256', 'X257', 'X258', 'X259', 'X260', 'X261', 'X262', 'X263', 'X264', 'X265', 'X266', 'X267', 'X269', 'X270', 'X271', 'X272', 'X273', 'X274', 'X275', 'X276', 'X277', 'X278', 'X279', 'X280', 'X281', 'X282', 'X283', 'X284', 'X285', 'X286', 'X287', 'X288', 'X291', 'X292', 'X294', 'X295', 'X296', 'X298', 'X299', 'X300', 'X301', 'X302', 'X304', 'X305', 'X306', 'X307', 'X308', 'X309', 'X310', 'X311', 'X312', 'X313', 'X314', 'X315', 'X316', 'X317', 'X318', 'X319', 'X320', 'X321', 'X322', 'X323', 'X324', 'X325', 'X326', 'X327', 'X328', 'X329', 'X331', 'X332', 'X333', 'X334', 'X335', 'X336', 'X337', 'X338', 'X339', 'X340', 'X341', 'X342', 'X343', 'X344', 'X345', 'X346', 'X348', 'X349', 'X350', 'X351', 'X352', 'X353', 'X354', 'X355', 'X356', 'X357', 'X358', 'X359', 'X360', 'X361', 'X362', 'X363', 'X364', 'X365', 'X366', 'X367', 'X368', 'X369', 'X370', 'X371', 'X372', 'X373', 'X374', 'X375', 'X376', 'X377', 'X378', 'X379', 'X380', 'X382', 'X383', 'X384', 'X385']<br/>--------------------------------------------------<br/>Columns containing the unique values :  [0]<br/>['X11', 'X93', 'X107', 'X233', 'X235', 'X268', 'X289', 'X290', 'X293', 'X297', 'X330', 'X347']<br/>--------------------------------------------------</span><span id="d4d4" class="ky kc hi mo b fi mw mt l mu mv">WHEN WE DID INTEGER COLUMN ANALYSIS WE OBSERVE THAT THERE ARE SOME COLUMNS WHICH ARE HAVING ONLY 0 VALUES SO WE DISCARD THOSE COLUMNS</span></pre><p id="a5d0" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在我们只有7个分类特征，所以我们对它们进行了<strong class="jh hj">单变量分析</strong>:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es mx"><img src="../Images/c1de133dba686539e957432cea7519dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JZaafIvEMCuVvWNwzLUE6w.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">正如我们在该图中看到的，y值均匀分布在(X0)的每个类别中，我们还可以观察到大多数y(目标变量)值在80和150之间。</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es mx"><img src="../Images/0e890eff99beb3802dd4f7ec22d597d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uAWX6dDG4-hxLHHSkYOKeg.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">正如我们在该图中看到的，y值均匀分布在(X1)的每个类别中，我们还可以观察到大多数y(目标变量)值在80和150之间。</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es mx"><img src="../Images/160eea629c1e78ec3f255562dcac9234.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HuWK8ZtUVrNqqrlGPAwtcQ.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">正如我们在该图中看到的，y值在每一类(X2)中都是均匀分布的，我们还可以观察到大多数y(目标变量)值在75到138之间。</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es mx"><img src="../Images/da818d44aa3994f57cec8abf3c5b0fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vufXPhn-4tKfYh9KmWcgYQ.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">正如我们在该图中看到的，y值在每一类(X3)中都是均匀分布的，我们还可以观察到大多数y(目标变量)值在70到135之间。</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es mx"><img src="../Images/d855a1fbb14672679708bb33cdd4a329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jy3CnHnLLwC9oFCkZcqJdg.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">正如我们在该图中所看到的，y的值不是均匀分布在(X4)的每个类别中，它主要出现在a和d中，我们还可以观察到大多数y(目标变量)值在80和120之间。</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es mx"><img src="../Images/3814a35b09faac2b289be1d45d7e93da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QaUURNcnJGap-xCR6wvNfA.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">正如我们在该图中所看到的，y值均匀分布于除g、h、x、y之外的所有类别(x5 ),我们还可以观察到大多数y(目标变量)值在70和130之间。</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es mx"><img src="../Images/5a569841ce35027f668babe7753abbaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aDHeIkwvwpjYRLzqEbDjzQ.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">正如我们在该图中看到的，y值均匀分布在(x6)的每个类别中，我们还可以观察到大多数y(目标变量)值在80和150之间。</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es mx"><img src="../Images/1c9f7fa26abad58275b74c7288d45e96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ju_rPrUhHmz0ukCoZk2Tnw.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">正如我们在该图中看到的，y值均匀分布在(x8)的每个类别中，我们还可以观察到大多数y(目标变量)值在75和130之间。</figcaption></figure><h2 id="54b3" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">二元变量:</h2><p id="eff3" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">现在我们可以看看二元变量。像我们以前看到的那样，有不少。让我们从获得每个变量中0和1的数量开始。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es my"><img src="../Images/b28033ed5480ac85f001f1c047c7a411.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*iZwXStKuzWHwotjJepUvQQ.png"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">有些列只有0值，因此我们将丢弃它们，我们还观察到数据非常稀疏</figcaption></figure><p id="6c0b" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">对目标变量y的一些分析:</p><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="1fc9" class="ky kc hi mo b fi ms mt l mu mv">#SOME STATISTICS TARGET VARIABLE ON Y<br/>count    4209.000000<br/>mean      100.669318<br/>std        12.679381<br/>min        72.110000<br/>25%        90.820000<br/>50%        99.150000<br/>75%       109.010000<br/>max       265.320000      (MAY BE OUTLIER)<br/>Name: y, dtype: float64</span></pre><p id="ab10" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们画出y的CDF</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mz"><img src="../Images/f6a8b7cc29a0acc5352c263410d138c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*Q2NYdnyYSz1g8Al1Ytg5nA.png"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">这里我们还可以看到，大部分y值都在70-180之间，还有一个异常值为256.32</figcaption></figure><p id="486b" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">让我们绘制目标分布图:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mz"><img src="../Images/e4a6e17d0e3c9c59369dde11f770ed28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*6ltW0WjDCqypp4q4KqKNOA.png"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">有一点向右倾斜，所以这里我们可以看到右边的大部分异常点可以大于145</figcaption></figure><p id="d068" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">让我们找出离群值:</strong></p><p id="a39b" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在这里，我们已经找到了基于观察上述图的异常值，所以我们将设置我们的阈值为150。</p><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="8479" class="ky kc hi mo b fi ms mt l mu mv">[150.43 169.91 154.87 265.32 158.53 154.43 160.87 150.89 152.32 167.45<br/> 154.16 158.23 153.51 165.52 155.62]</span><span id="ff03" class="ky kc hi mo b fi mw mt l mu mv">          NOW</span><span id="906e" class="ky kc hi mo b fi mw mt l mu mv">Removing outliers based on above information and setting 150 as a threshold value . . . . . . . . . . . . . . . . . . . . <br/>(4194, 378)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mz"><img src="../Images/1f399b8f3a0bab7bc9a0767db3c68c5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1126/format:webp/1*QVwoP2JmUVajg-1jNh-LPg.png"/></div><figcaption class="mj mk et er es ml mm bd b be z dx translated">现在，在去除异常值(即去除大于150的值)后，它看起来好多了</figcaption></figure><h2 id="90be" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">预测功能:</h2><p id="5642" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">探索目标变量和预测变量之间的关联:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es na"><img src="../Images/e5be3cde8b980fda2ddb6f7f4bcbdbd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qd7yU_piONUUO5lfYarQWw.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated"><strong class="bd kd">X0与目标有最好的关系。(64%) </strong></figcaption></figure><h2 id="a7ab" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">E.d .结论:</h2><p id="df22" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">1:包含唯一值的列:[0]我们必须删除这些只包含零的列['X11 '，' X93 '，' X107 '，' X233 '，' X235 '，' X268 '，' X289 '，' X290 '，' X293 '，' X297 '，' X330 '，' X339 '，' X347']</p><p id="c833" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">2:我们也尝试去除异常值。</p><h2 id="fc2b" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">数据准备:</h2><p id="c83b" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">1:在这种情况下，我们删除这些列。['X11 '，' X93 '，' X107 '，' X233 '，' X235 '，' X268 '，' X289 '，' X290 '，' X293 '，' X297 '，' X330 '，' X339 '，' X347']</p><p id="cd1e" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">2:通过使用带有函数np.percentile(ys，[20，80])的四分位数再次找到异常值，并删除行216，678，1031，1135，1340，2346，2365，2874，2892，3077。</p><p id="f737" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">3:用标签编码器预处理[“X0”、“X1”、“X2”、“X3”、“X4”、“X5”、“X6”、“X8”]。</p><p id="4daf" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">4:随机训练和测试分割(80:20):</p><h2 id="4dc2" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">特征工程:</h2><p id="8171" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">这是解决机器学习问题的最重要的一步，因为这是通过良好的特征工程改善结果的唯一方法。</p><p id="dd84" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因此，在这里，我们通过在不在“y”、“X0”、“X1”、“X2”、“X3”、“X4”、“X5”、“X6”、“X8”中的列上实现，使用tsvd、pca、ica、grp、srp作为新功能，组件数量为10。</p><h2 id="ec4b" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">实施模型:</h2><p id="e468" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated"><strong class="jh hj"> 1 —因此，我实施的第一个模型是带有超参数调整的决策树回归:</strong></p><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="9ebb" class="ky kc hi mo b fi ms mt l mu mv">hyper = [1, 5, 10, 50, 100, 500, 1000]<br/>param_grid = {‘max_depth’ :hyper}<br/>lasso_model = DecisionTreeRegressor()<br/>grid_search = GridSearchCV(lasso_model,param_grid, cv=5,scoring=’r2')<br/>grid_search.fit(X_train, Y_train)<br/>print(“Best SGDR alpha: “, grid_search.best_params_)<br/>print(“Best SGDR score: “, grid_search.best_score_)</span><span id="82f2" class="ky kc hi mo b fi mw mt l mu mv">Best SGDR alpha:  {'max_depth': 5}<br/>Best SGDR score:  0.6169689489084359</span><span id="c616" class="ky kc hi mo b fi mw mt l mu mv">The R2 score for DEPTH = 5.000000 ON TEST DATA is 0.599141%</span></pre><p id="7b77" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> 2 —我实施的第二个模型是XGB_Regression，带有超参数调整:</strong></p><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="4140" class="ky kc hi mo b fi ms mt l mu mv">x_cfl=XGBRegressor()<br/>prams={<br/> ‘learning_rate’:[0.01,0.03,0.05,0.1,0.15,0.2],<br/> ‘n_estimators’:[100,200,500,1000,2000],<br/> ‘max_depth’:[3,5,8,10],<br/> ‘colsample_bytree’:[0.1,0.3,0.5,1],<br/> ‘subsample’:[0.1,0.3,0.5,1]<br/> }<br/>first_xgb=GridSearchCV(x_cfl,prams,verbose=10,n_jobs=-1,scoring=’r2')<br/>first_xgb.fit(X_train, Y_train)</span></pre><p id="b760" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">使用第二代i7完成上述超参数调整需要300分钟。</p><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="85b0" class="ky kc hi mo b fi ms mt l mu mv">print (first_xgb.best_params_)</span><span id="a244" class="ky kc hi mo b fi mw mt l mu mv">{'colsample_bytree': 0.5, 'learning_rate': 0.03, 'max_depth': 3, 'n_estimators': 200, 'subsample': 1}</span><span id="e06a" class="ky kc hi mo b fi mw mt l mu mv">The R2 score of XGBRegressor ON TEST DATA is 0.609613%</span></pre><p id="3c45" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj"> 3 —现在，我实施的第三个模型是Lasso回归和超参数调整:</strong></p><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="3f5b" class="ky kc hi mo b fi ms mt l mu mv">hyper = []<br/>i = 0.0000001<br/>while(i&lt;=1000000000):<br/> hyper.append(np.round(i,7))<br/> i *= 10</span><span id="ced0" class="ky kc hi mo b fi mw mt l mu mv">lasso_grid = {“alpha” : hyper}<br/>lasso_model = Lasso(fit_intercept=True,normalize=True)<br/>grid_search = GridSearchCV(lasso_model, lasso_grid, cv=5, scoring=’r2')<br/>grid_search.fit(X_train, Y_train)<br/>print(“Best lasso alpha: “, grid_search.best_params_)<br/>print(“Best lasso score: “, grid_search.best_score_)</span><span id="7f58" class="ky kc hi mo b fi mw mt l mu mv">Best lasso alpha:  {'alpha': 0.01}<br/>Best lasso score:  0.611276230109042</span><span id="fbce" class="ky kc hi mo b fi mw mt l mu mv">The R2 score for ALPHA = 0.010000 ON TEST DATA is 0.584410%</span><span id="cca1" class="ky kc hi mo b fi mw mt l mu mv">+--------------------------+----------+--------------------------+<br/>| MODEL  |  R^2| R^2(KAGGLE_SCORE_PUBLIC) |R^2(KAGGLE_SCORE_PRIVATE) |<br/>+--------------------------+----------+--------------------------+<br/>|Lasso Regression   | 0.58441  | 0.51797  |          0.52655     <br/>+--------------------------+----------+--------------------------+-</span><span id="f916" class="ky kc hi mo b fi mw mt l mu mv">|Decision Tree Regression | 0.599141 |   0.53522  |  0.54524   <br/>+--------------------------+----------+--------------------------      <br/>| XGB Regression          | 0.608679 | 0.5457   |    0.55275          |<br/>+--------------------------+----------+-----------------------\</span></pre><h2 id="28f2" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">结果:在第一种方法中，我们在kaggle上得到(私人分数:0.5457)和(公共分数:0.55275)。</h2><p id="0297" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">我把我的作业提交给了应用A。当然，他们建议我做更多的特征工程来提高分数。</p><h1 id="1e57" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">第二种方法</h1><h2 id="852e" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">具有不同特征工程的第二种方法增加得分R (Coef。决心)。</h2><h2 id="6693" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">E.d .第二种方法:</h2><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es nb"><img src="../Images/a1ef23d83aca5b039e04600a0cde2784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1p94tCgm5KdpV0mG.png"/></div></div></figure><p id="3a80" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">正如我们在上面的图中所看到的，异常值大约接近140 °,这是安全点，为了不过度拟合，我们将像前面的方法一样在150°之后移除异常值。</strong></p><h1 id="69a5" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">预处理:</h1><p id="207c" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">1: Ist我们将根据上述观察结果移除异常值:</p><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="ea0b" class="ky kc hi mo b fi ms mt l mu mv">We removed these outlier on the basis of above observation<br/>[150.43 169.91 154.87 265.32 158.53 154.43 160.87 150.89 152.32 167.45<br/> 154.16 158.23 153.51 165.52 155.62]<br/>Removing outliers based on above information and setting 150 as a threshold value . . . . . . . . . . . . . . . . . . . . <br/>(4194, 378)</span></pre><p id="2d86" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">2:与第一种方法一样，也删除零值列。</p><p id="5322" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">3:再次为每一列的方差设置阈值0.01，并且也将它们移除。被移除的列也被从所有临时数据帧中移除。结果是有146列用于移除。从原始数据帧中删除要素后，更新df_num数据帧。</p><p id="e67f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">3:获得与目标列y相关的重要特征的字典。一些与目标变量相关的重要特征。基于实验变化以0.25作为阈值。</p><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="edb9" class="ky kc hi mo b fi ms mt l mu mv">Important Features with there respective correlations are  <br/> --------------------------------------------------------- <br/> {'X28': -0.261548387853112, 'X29': -0.3979846718424932, 'X54': -0.39362263688450944, 'X76': -0.39362263688450944, 'X80': -0.2566304628986176, 'X118': 0.2911340078121633, 'X119': 0.2911340078121633, 'X127': -0.5359508861669307, 'X136': 0.39362263688450944, 'X162': -0.3809601526804208, 'X166': -0.3469061103890677, 'X178': -0.3105490342608788, 'X185': -0.25654857309239765, 'X232': -0.3979846718424932, 'X234': -0.27530886410908445, 'X250': -0.32318814896929704, 'X261': 0.6184684577479749, 'X263': 0.3979846718424932, 'X272': -0.36779944561534245, 'X275': 0.29297093005751373, 'X276': -0.37663134331800774, 'X279': -0.3979846718424932, 'X313': -0.3453785698372581, 'X314': 0.6371978536813558, 'X316': -0.2747484119054767, 'X328': -0.3839243197734775, 'X348': -0.25754835598033654, 'X378': -0.27115936517391354}</span><span id="273b" class="ky kc hi mo b fi mw mt l mu mv">1: This states that X29, X54, X76, X127, X136, X162, X166, X178, X232, X250, X261, X263, X272, X276, X279, X313, X314, X328 are important features later we will select using some selection techniques.</span><span id="9f7b" class="ky kc hi mo b fi mw mt l mu mv">2:But , YOU MUST SEE THAT SOME FEATURES ARE HAVING SAME CORRELATIONS THAT COULD INDICATE THE POSSIBLE DUPLICATE FEATURES. Lets check them too .</span></pre><p id="f062" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">4:我们还删除了一些重复的列，它们显示了大约的相关性。一</p><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="47a9" class="ky kc hi mo b fi ms mt l mu mv">Dublicates: <br/> ['X232', 'X279', 'X35', 'X37', 'X113', 'X134', 'X147', 'X222', 'X76', 'X324', 'X84', 'X244', 'X119', 'X146', 'X226', 'X326', 'X360', 'X247'</span></pre><p id="4116" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">检查一组重复特征之间的相关性，并准备高度相关的对。同样，0.9的相关阈值是在多次实验后判断和采用的，即我们丢弃那些特征。</p><p id="ab08" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">5:标签编码分类特征，如第一种方法。</p><p id="9453" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">到目前为止，我们已经删除了许多特性，即列，因此我们将使用XGBoost和随机森林回归器来查找其余列的特性重要性。</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nc"><img src="../Images/118d657d159509e1e44a37b66a780fb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/0*1czZ79tFI0d2tx6J.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nc"><img src="../Images/32dab2c4fec80656a7152225e93aad3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/0*cvLUsttuyLVi651O.png"/></div></figure><h2 id="5803" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">现在让我们使用交互变量进行一些特征工程:</h2><p id="7e65" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">双向交互:['X314_plus_X315']，["X10_plus_X54"]，["X10_plus_X29"]。三方互动:['X118_plus_X314_plus_X315']。</p><p id="bc3a" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">再次检查相关性！</p><ul class=""><li id="45ae" class="nd ne hi jh b ji jj jl jm jo nf js ng jw nh ka ni nj nk nl bi translated">获取数字数据帧，找到所有相关性非常高的特征，并对其进行检查。也如上所述制造它们的对。</li><li id="a625" class="nd ne hi jh b ji nm jl nn jo no js np jw nq ka ni nj nk nl bi translated">结果是63个高度相关的特征的列表。<strong class="jh hj">同样，0.95的值是通过实验判断和确定的，没有经验法则来确定阈值。</strong></li></ul><p id="4344" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">现在让我们看看XGBoost和随机森林回归器的新交互特性的重要性。</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es nr"><img src="../Images/47ccea08183919c999439b2ec714c552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*layyQmmKge6A8CmN.png"/></div></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es ns"><img src="../Images/0550c68117b44371478c1981cb408458.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UnSapJnHFWuHe6lV.png"/></div></div><figcaption class="mj mk et er es ml mm bd b be z dx translated"><strong class="bd kd">XG boost的特性重要性</strong></figcaption></figure><p id="1068" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">正如我们在上面的功能图中所看到的，它显示了我们新的交互功能比其他许多令人印象深刻的功能更重要。</p><h2 id="c69a" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated"><strong class="ak">模型实现:</strong></h2><p id="871e" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">正如我们在第一种方法中看到的，XGboost很好地处理了这个数据集，而且我们的数据集不是很大，功能也较少，这就是我们选择实现XGboost的原因。</p><p id="9325" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">培训代码片段:</strong></p><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="d0bf" class="ky kc hi mo b fi ms mt l mu mv">x_train, x_valid, y_train, y_valid = train_test_split(train_X, train_y, test_size=0.2, random_state=420)<br/>d_train = xgb.DMatrix(x_train, label=y_train)<br/>d_valid = xgb.DMatrix(x_valid, label=y_valid)<br/>d_test = xgb.DMatrix(test)<br/>xgb_params = {<br/> ‘n_trees’: 500, <br/> ‘eta’: 0.0050,<br/> ‘max_depth’: 3,<br/> ‘subsample’: 0.95,<br/> ‘objective’: ‘reg:linear’,<br/> ‘eval_metric’: ‘rmse’,<br/> ‘base_score’: np.mean(train_y), # base prediction = mean(target)<br/> ‘silent’: 1 }<br/>def xgb_r2_score(preds, dtrain):<br/> labels = dtrain.get_label()<br/> return ‘r2’, r2_score(labels, preds)<br/>watchlist = [(d_train, ‘train’), (d_valid, ‘valid’)]<br/>clf = xgb.train(xgb_params, d_train, 1050 , watchlist, early_stopping_rounds=70, feval=xgb_r2_score, maximize=True, verbose_eval=10)</span></pre><p id="b470" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">结果:在第三种方法中，我们在kaggle上得到了(私人分数:0.55282)和(公共分数:0.55709)。</strong></p><h1 id="d44f" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">第三种方法</h1><p id="d48c" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated"><strong class="jh hj">结合第一项和第二项特征工程，提高得分R (Coef。【决心之光】(T15)</strong></p><h2 id="4761" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">注意:</h2><p id="b446" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">在这里，我们所做的是，我们采用第二种方法中用于训练模型的相同最终数据，并通过在不在“y”、“X0”、“X1”、“X2”、“X3”、“X4”、“X5”、“X6”、“X8”中的列上实施，将tsvd、pca、ica、grp、srp作为新特征应用，所采用的组件数量是3个不同的实验，此处未显示，以最大化这些特征组合的得分。</p><p id="3e62" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">现在让我们一起来看看特征的重要性，即交互特征和tsvd、pca、ica、grp、srp与XGBoost和随机森林回归，哪个特征存在于哪里。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nt"><img src="../Images/6343f36889802f42de49fa2e48c958a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*26NDMxRJR22FYbPLaP7s1g.png"/></div></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="er es nu"><img src="../Images/3aed7bffe1e007f3a604e40e44b69d14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fd_kBTBIMlNAjzA_EHySNA.png"/></div></div></figure><p id="188f" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">正如我们在上面的特性图中所看到的，它显示了由两种不同的特性工程方法所产生的特性的重要性，这两种特性看起来都很重要，令人印象深刻，但是两个最重要的特性来自交互特性化。</p><p id="24ae" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">训练代码片段:</strong></p><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="61fe" class="ky kc hi mo b fi ms mt l mu mv">x_train, x_valid, y_train, y_valid = train_test_split(train_X, train_y, test_size=0.2, random_state=420)<br/>d_train = xgb.DMatrix(x_train, label=y_train)<br/>d_valid = xgb.DMatrix(x_valid, label=y_valid)<br/>d_test = xgb.DMatrix(test)<br/>xgb_params = {<br/> ‘n_trees’: 500, <br/> ‘eta’: 0.0050,<br/> ‘max_depth’: 3,<br/> ‘subsample’: 0.95,<br/> ‘objective’: ‘reg:linear’,<br/> ‘eval_metric’: ‘rmse’,<br/> ‘base_score’: np.mean(train_y), # base prediction = mean(target)<br/> ‘silent’: 1<br/>}<br/>def xgb_r2_score(preds, dtrain):<br/> labels = dtrain.get_label()<br/> return ‘r2’, r2_score(labels, preds)<br/>watchlist = [(d_train, ‘train’), (d_valid, ‘valid’)]<br/>clf = xgb.train(xgb_params, d_train, 1050 , watchlist, early_stopping_rounds=70, feval=xgb_r2_score, maximize=True, verbose_eval=10)</span></pre><p id="72f9" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><strong class="jh hj">测试代码片段:</strong></p><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="2929" class="ky kc hi mo b fi ms mt l mu mv">d_train = xgb.DMatrix(train_X, label=train_y)<br/>#d_valid = xgb.DMatrix(x_valid, label=y_valid)<br/>d_test = xgb.DMatrix(test)<br/>xgb_params = {<br/> ‘n_trees’: 500, <br/> ‘eta’: 0.0050,<br/> ‘max_depth’: 3,<br/> ‘subsample’: 0.95,<br/> ‘objective’: ‘reg:linear’,<br/> ‘eval_metric’: ‘rmse’,<br/> ‘base_score’: np.mean(train_y), <br/> ‘silent’: 1<br/>}<br/>def xgb_r2_score(preds, dtrain):<br/> labels = dtrain.get_label()<br/> return ‘r2’, r2_score(labels, preds)<br/>watchlist = [(d_train, ‘train’)]<br/>clf = xgb.train(xgb_params, d_train, 1050 , watchlist, early_stopping_rounds=70, feval=xgb_r2_score, maximize=True, verbose_eval=10)</span></pre><p id="c186" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在第三种方法中，我们在kaggle上得到了(私人分数:0.55077)和(公共分数:0.55744)。</p><h2 id="42b5" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">KAGGLE评分结果:</h2><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="d0e5" class="ky kc hi mo b fi ms mt l mu mv">+--------------+-----KAGGLE SCORE RESULT:----------+---------------</span><span id="9e39" class="ky kc hi mo b fi mw mt l mu mv">|APPROACH_NO | Private_Score | Public_Score  | Feature_engineering      |<br/>+--------------+---------------+---------------+--------------------<br/>|1st_approach | 0.5457  | 0.55275  | SVD,PCA,GRP,SRP,ICA <br/>        |<br/>|2nd_approach | 0.55282 | 0.55709  | Interaction_variable</span><span id="85fc" class="ky kc hi mo b fi mw mt l mu mv">|3rd_approach | 0.55077 | 0.55744  | Combine_of_1st_2nd_approach   </span></pre><h1 id="211d" class="kb kc hi bd kd ke kf kg kh ki kj kk kl io km ip kn ir ko is kp iu kq iv kr ks bi translated">接下来的步骤:</h1><p id="a71b" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated"><strong class="jh hj">1—-&gt;&gt;探索性数据分析:</strong> <br/>答:数据加载和清洗。<br/> B:浏览数据集中出现的分类和二元变量列。<br/><strong class="jh hj">2—-&gt;&gt;数据准备:</strong> <br/>答:找出离群值并丢弃。<br/> B:删除只有一个值的列。<br/> C:将每个类别值转换成标签。<br/> D:随机训练测试分割(80:20)比例。<br/> E:我们做了奇异值分解、主成分分析、GRP、SRP和ICA，并从每种方法中提取3个分量作为特征。女:我们还做了交互式可变特征工程。<br/><strong class="jh hj">3———&gt;&gt;机器学习模型。</strong> <br/> A:超参数调优的Lasso回归:<br/> B:超参数调优的决策树回归<br/> C:超参数调优的XGB回归:</p><h2 id="c8ff" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">观察结果:</h2><p id="bc1c" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated"><strong class="jh hj">答:</strong>当我们应用以上三个回归模型时，我们知道如果我们使用生产套索回归模型<br/>在时间复杂度和R矩阵方面是最好的。<br/> <strong class="jh hj"> B: </strong>如果我们只关心R矩阵那么XGB最好。如果我们采用在XGB和LASSO回归之间取得平衡的模型，那么我们可以使用决策树。<br/> <strong class="jh hj"> D: </strong>我们观察到，使用简单的特征工程，如交互变量，我们会得到最好的分数。</p><h2 id="0c36" class="ky kc hi bd kd kz la lb kh lc ld le kl jo lf lg kn js lh li kp jw lj lk kr ll bi translated">结论:</h2><p id="d750" class="pw-post-body-paragraph jf jg hi jh b ji kt ij jk jl ku im jn jo kv jq jr js kw ju jv jw kx jy jz ka hb bi translated">在这个案例研究中，作为我在应用人工智能课程中的一部分，我学到了(学会学习)以及如何从零开始解决问题。在这个项目中，数据集是真正匿名的(没有名称提示)，v.dirty数据集需要一个好的EDA标签，预处理等。</p><p id="d1b8" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这个案例研究的最重要的方面是特征工程，这是改善我们结果的唯一方法，我认为它暗示了整个机器学习，因为我们的重模型总是不能像特征工程那样为我们做改进。在这个案例研究中，我用交互变量做特征工程，这非常有趣。</p><p id="b42e" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我还观察到方法1和2中的特征，当我们将它们组合时，分数下降。有可能一些特征再次相互关联并使分数下降。反复消除相关特征也是本案例研究的重要任务。我认为我们可以通过消除相关特征和超参数调整来改进我们的第三种方法。</p><p id="c33d" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">这就是项目的全部内容。我希望您喜欢这个案例研究。在这里，我们可以通过做更多特征工程以及其他回归模型和一些其他高级技术来改进我们的模型。在这个案例研究项目中，我们的目标是对机器学习行业问题有所了解，并做出最佳模型。</p><p id="3bb0" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">感谢阅读！</p><p id="8cb1" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><em class="nv">参考文献</em></p><p id="b5c9" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae nw" href="https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-mercedes" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/sudalairajkumar/simple-exploration-notebook-Mercedes</a></p><p id="2bbe" class="pw-post-body-paragraph jf jg hi jh b ji jj ij jk jl jm im jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated"><a class="ae nw" href="https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/37700" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/Mercedes-benz-greener-manufacturing/discussion/37700</a></p></div></div>    
</body>
</html>