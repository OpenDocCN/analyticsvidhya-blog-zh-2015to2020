# 当单词嵌入变得疯狂时。

> 原文：<https://medium.com/analytics-vidhya/when-word-embeddings-go-berserk-aa43ca927611?source=collection_archive---------16----------------------->

当我在学习吴恩达的序列模型课程时，我看到了一篇名为《男人对于电脑程序员就像女人对于家庭主妇？去偏置词嵌入”。因为这个话题很吸引人，而吴恩达的解释又太简短，所以我决定自己去探索一下。

在网上搜索，阅读了几篇论文后，我整理了一个像教程一样的故事列表，今天我想和你分享。

## 首先:机器就像婴儿

![](img/370396fb21e2e8d436c5815285a2f302.png)

可爱的小机器人

就像孩子如何观察他们的父母，然后模仿他们的行为一样，机器和算法正在观察我们，以找出他们应该如何表现自己。因此，如果我们把自己点着，从窗户跳出去，降落在下面两层楼，敲下某人的头，然后笑着离开，他们可能会认为我们很享受这个过程，发现他们做得很好，很有道德。

这正是计算机如何从我们以前的思维模式中学习的情况(但稍微温和一点),无论它们有多古老和错误，然后将它们复制到我们的现代社会中。本文中讨论的旧的和以前的思维模式是种族歧视、性别偏见，以及总的来说，将具有特定能力的人归类为不同类别的信念，因此阻止他们渴望新的技能、职业甚至生活方式。

## 第二:婴儿是我们的下一代，所以要照顾好他们

当我们走向社会，扮演不同的角色，尽最大努力向我们的父母展示我们已经很好地学习了他们教给我们的东西，机器试图向我们展示它们如何能像我们一样表现出色。所以让我们假设我们想教一个机器人如何与其他人互动。当我们在谷歌新闻、维基百科和所有其他包含世界各地文本的资源的大型文本语料库上训练它时，它会尽可能多地吸收字典中每个单词的细微差别。所以当我们把机器人放在一个房间里，和一个白人、一个女人和一个非裔美国人在一起时，它可能会递给女人一个洗衣液，给男人一瓶啤酒，并引导非裔美国人去平原收割棉花。

## 第三:我不在乎我的宝宝，那又怎样？

你可能会说这样的话，“来吧！如果给女人一个清洁剂又有什么关系呢？！所以她放下杯子，拿了一瓶啤酒。”，或者“冷静点，这只是个机器人，那又怎么样？”。

嗯，我们必须记住的第一件事是，机器学习算法实际上正在带走这个世界，以及我们每天拥有和使用的所有系统，如广告、社交媒体、求职等。它们不仅复制了我们的旧思想和思维模式，还通过使偏见更加普遍来放大它们。

因此，如果我们不关心我们的算法，他们会继续[给黑皮肤的人贴上大猩猩的标签](https://www.theguardian.com/technology/2018/jan/12/google-racism-ban-gorilla-black-people)，[给女性电脑程序员和数学家看清洁和家务工作](https://www.researchgate.net/publication/332465701_Algorithmic_Privacy_and_Gender_Bias_Issues_in_Google_Ad_Settings)，认为[所有穆斯林都支持恐怖主义](https://twitter.com/HindMakki/status/698191175820378113/photo/1)，等等。

## 第四:我们如何创造婴儿的简史。(别误会，算法和机器学习宝宝)

回到 2013 年，像托马斯·米科洛夫和他的团队这样的谷歌大脑和头脑正在尽最大努力让他们的搜索算法更好地回答人们的问题。因此，在结合了一些旧技术之后，他们发明了一种叫做 word2vec 的算法，这种算法属于自动编码器算法的范畴。自动编码器通常就像压缩器，它们接收我们数据的一个明确表示，比如一个物体的图片，然后输出另一个表示，该表示具有我们给它的输入的主要方面和特征。它们主要由三部分组成:编码器、拥有压缩数据的潜在空间或瓶颈，以及试图尽可能将压缩数据恢复到原始格式的解码器。在另一篇文章中有更多的内容，但对于本教程，您必须记住的是，**自动编码器是无监督的深度学习算法，它保留输入** **的最重要方面，并尝试返回尽可能与输入**对应的输出。这实际上是为什么它们传统上用于降维，但现在它们已经发展并用于机器和深度学习的所有不同领域。

![](img/6058a127f4b7017dda28564faf80ead6.png)

自动编码器

理解了自动编码器的基本结构之后，我们就可以更好地理解 Word2Vec 了。因为 Word2Vec 是一个自动编码器，所以它是一个无监督的算法，可以自己学习每个输入的最重要的方面，而无需预先定义标签。因此，当我们给它一个句子时，对于该句子中的每个单词，它输出一系列代表这些单词最重要方面的数字，称为单词嵌入。就像当我们给它这个句子，“一只可爱的猫和狗”，它会产生五个不同的向量，这些向量的值是从句子本身推断出来的。而且它们是用数字表示的，因为计算机只能理解数字而不能理解其他任何东西。(在[这里](/@mitra.mirshafiee/step-by-step-introduction-to-word-embeddings-and-bert-embeddings-1779c8cc643e)了解更多关于单词嵌入的信息)

## 嵌入能做什么？

有了这些很酷的嵌入技术，人们可以做很多事情。他们可以根据每个人的搜索推荐产品，找到每个人不同兴趣之间的隐藏联系，甚至他们有时可以通过问类似的问题来玩算法。

计算机回答这些问题的方式非常简单。我们知道每个单词都有自己的嵌入，每个嵌入由一系列称为向量的数字组成。为了形象化，让我们假设我们的单词嵌入只是三维的，并且只代表每个单词的三个方面。

![](img/360bcf8223e82265229b91ce49ec71c9.png)

我们如何形成类比

当我们问它“男人”对“女人”是什么，就像“女孩”对什么一样，它减去第一对向量以获得它们意义之间的差异，然后将这个差异向量添加到“男孩”的向量，并试图找到与计算结果最接近的向量。

这就是有趣的地方。当科学家们兴致勃勃地询问机器不同的类比，并用它们来推断文本的含义时，他们遇到了一个引起他们注意的类比。**果然是“男人”对“电脑程序员”就像“女人”对什么一样？**

在看到答案之前，他们期望类似程序员或编码员的东西。但有趣的是，机器的回答就像一个生活在 70 年代的人认为电脑是上帝派来的，只能由被选中的男性先知来操作。所以它回答说，“家庭主妇”。

在进一步探索之后，他们发现了其他中性词，它们的向量更接近某一特定性别。下面是这些单词的列表。

![](img/de9b7fdc32ea9be7b6552ef20e97948e.png)

一些带有性别倾向的词语

为了进一步检验这种推测，并查看性别对是否有任何共同点，他们选择了 10 个不同的性别对，如(她，他)，并计算他们的差异，然后计算所有这些差异的主成分分析(右侧蓝色图表显示了前 3 个主成分)。他们还随机选择了 10 对单词 1000 次，并计算它们差异的平均值，然后计算它们的主成分(左侧图表显示了它们差异的前 3 个主成分)。

![](img/27082be0a7ca591062d2ea5c19f9c9f1.png)

在左图中，很明显，第一个组件明显大于其他两个组件。另一方面，10 个随机对的主成分分析具有更渐进的衰减，由此，我们可以得出结论，性别差异是具体的，并且可以作为所有这些对的特征来指出。所以有了这些方向，我们就可以把性别差异从性别中性词中剔除，并根据性别来中和它们。因此，为了消除单词中的所有偏见和差异，他们首先需要将每个单词投影到性别差异子空间上，以识别包含这种偏见的那部分意思。

## 去偏置的两个主要步骤

## 步骤 1:识别性别子空间

*W:所有词集
Di:每个词集
i:每个词集的均值
B:偏差子空间(C 的前 3 行)*

![](img/8a6e9b09d5d7fa6ca7b0521f54e7676a.png)

## 步骤 2a:中和和均衡

![](img/42d17fab5a3102ab6025ad97db4457dc.png)

中和与均衡的快速定义

在找到有偏向的特征和方向后，我们必须识别每个词的哪些方面与性别子空间或(方向)相关，然后将其从我们的性别中性词中取出。首先，为了识别每个单词中的偏差，我们找到其单词向量在性别子空间上的投影。如果你不熟悉数学，请阅读以下内容:

**把一个向量投影到一个子空间:**假设你想把一个向量 v 投影到一个子空间 b 上，你要做的，就是在子空间 b 中找到离 v 最近的向量，所以作为一个简单的例子，如果 v 在子空间中，它的投影就是它自己。作为一般定义，我们通过下面的等式将 v 投影到 B 上:

![](img/e0ba6143706331079209a796eca2c3ba.png)![](img/778523792ffa0af163b70733ddcc8e22.png)

请参见下图，以获得更好的直觉:

![](img/e1987eb535a6b3aeede0ceab0adaf95f.png)

如果我们假设我们的子空间的跨度是“plane_spans”中的向量，我们要投影到它上面的向量是“vector”。通过计算“矢量”和跨度的点积，我们得到“矢量”投影的每个方向的大小，然后乘以跨度，我们将得到一个与跨度方向相同的矢量。以下是计算投影的简单 python 代码:

1.  **中和:**从所有性别中性词中取出偏向方向。对于这一步，我们计算每个单词在 N(一组像‘计算机程序员’、‘保姆’等必须被中和的单词)中的投影。)到偏置子空间(B)上。然后从单词向量中减去投影，输出中和后的单词嵌入。
    *N:要中和的词
    wʙ:将每个词向量投影到 b(性别子空间)*

![](img/7ba57966b05f8aa238efe00f3fccc46f.png)

1.  **均衡:**确保每组单词(等式组)到中性词的距离相同。例如，如果{ '祖母'，'祖父' }和{ '女孩'，'男孩' }是两个相等集合，在相等之后，两个集合中的两个单词都将与单词'保姆'等距。
    ε:所有等式集合的集合
    E:等式集合
    :每个等式集合的平均值

![](img/51ed18f9860928ac94c19d506c7e49fe.png)

Equalize 将 B 之外的每组单词等同于它们的简单平均值， **v** 。正如这篇论文的作者所建议的，更容易将它分别视为 **wʙ** 和垂直于 b 并等于 **w-wʙ** 的 w。那些等式集合中的所有单词嵌入都等于它们的平均值( **v** )，并且在 b 内它们被居中，因此被移动到平均值 0( **wʙ- ʙ** )，然后被缩放(通过乘以除法的根式和分母下的等式**，因此它们都具有单位长度。**

## 为什么词向量在偏置子空间内居中。

好吧，让我们假设我们有一对像{ '女性'，'男性' }并且我们想要使他们与单词程序员的距离相等。碰巧的是，当我们把这两个词都投射到 B 上时，“女性”有更大的投射，因此也有性别成分。所以在偏置子空间中使这些单词居中之后，它们变得对称平衡。
正如作者告诉我们的那样:如果我们只是简单地在偏差方向上缩放每个向量的分量而不居中，**男性和女性将具有完全相同的嵌入**，并且我们将丢失诸如父亲:男性::母亲:女性之类的类比。

## 中和和均衡后

经过前面的两步，我们可以观察到，对于一个等式集合(E)中的每一个单词(如 e1，e2)，到每一个中性词(w)的距离都是相等的。或者用数学术语更详细地说:

![](img/16a81b7e089edbdbf0b8cce2c83bb792.png)

## 步骤 2b:软去偏置

在最后一步中，通过定义线性变换，我们倾向于最小化每个性别中立单词在性别子空间上的投影，同时保持所有单词向量和它们的变换版本之间的相似性。换句话说，我们试图提出一种转换，这种转换可以对性别中立的单词产生最大的影响，但不是所有嵌入在 **W** 中的单词。因此，为了以数学形式呈现这种直觉，我们使用点积并写出下面的等式:

![](img/2ca69040ca0de4fe97da410e39099981.png)

λ越大，该等式的第二部分就越重要，因此，我们就越关注如何使 N 在 b 上的投影最小化，λ值越大，我们就能得到与硬去偏置步骤完全相同的结果。还有，最后我们把向量 T 归一化，所以它有单位长度。

## 我们究竟如何在海量数据集中区分所有性别专用词和性别中性词？

在这篇论文中，作者使用了 300 万字的谷歌新闻数据集。这是一个很大的单词，他们不得不把所有中性词分开，比如“程序员”、“保姆”等等。不同性别的人，比如“女商人”、“演员”等等。所以他们只能消除偏见。因为性别特定的单词比另一组少得多，所以更有效的方法是指定其中一些单词，然后说任何不特定性别的单词都是中性的。

为了使这个过程更容易，他们首先指定了谷歌新闻词汇子集的大约 200 个性别特定的单词，大小约为 26，000 个单词，然后训练一个线性 SVM 来自己找出其余的性别特定的单词。最后，他们总共掌握了大约 6500 个与性别相关的词汇。

## 去偏置的结果

最后，在尝试了硬去偏置和软去偏置之后，他们再次生成了类似于(他，她)方向的配对，并要求人们说出他们是否认为这些配对是性别刻板印象。对于这两种方法，被确定为定型的配对数量显著减少。(例如，在对 word2News 进行硬去偏后，被识别为原型的对的百分比从%19 下降到了 6%)