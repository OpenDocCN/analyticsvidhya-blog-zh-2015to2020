<html>
<head>
<title>Deep Dive Into Word2Vec</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深入研究Word2Vec</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-dive-into-word2vec-7fcefa765c17?source=collection_archive---------3-----------------------#2019-09-13">https://medium.com/analytics-vidhya/deep-dive-into-word2vec-7fcefa765c17?source=collection_archive---------3-----------------------#2019-09-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/6df751c225d2121297ea9cdb88846647.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sGxKvELB8t0Q2AUUh15yyQ.png"/></div></div></figure><blockquote class="iq ir is"><p id="7eab" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> Word2vec </strong>是一组用于产生单词嵌入的相关模型。这些模型是浅层的两层神经网络，被训练来重建单词的语言上下文。Word2vec将大型文本语料库作为其输入，并产生一个向量空间，通常具有数百个维度，语料库中的每个唯一单词都被分配一个空间中的相应向量。单词向量位于向量空间中，使得语料库中共享共同上下文的单词在空间中彼此靠近。在本文中，我们将看到嵌入如何帮助找到相似/不相似的单词。</p></blockquote><h1 id="f249" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">为什么是单词向量？</h1><p id="b4a7" class="pw-post-body-paragraph it iu hi iw b ix kq iz ja jb kr jd je ks kt jh ji ku kv jl jm kw kx jp jq jr hb bi translated">首先，我们需要知道为什么我们需要把一个单词转换成一个向量？那么什么是矢量呢？</p><blockquote class="iq ir is"><p id="bb27" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">矢量</strong>——既有方向又有大小的量，尤其是确定空间中一点相对于另一点的位置。</p></blockquote><p id="86fb" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">如果某物有大小和方向，我们可以用它来做数学运算，如果我们可以用一个词的矢量形式来做数学运算，那么它创造了在N维空间中比较一个词和另一个词的可能性。</p><p id="4b47" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">好吧，这可能是一个很大的开始，让我举一个简单的例子来证明当单词被认为是数字而不是简单的字母组合时，数学是如何创造奇迹的。</p><p id="5464" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated"><strong class="iw hj">动物相似性和简单线性代数</strong></p><p id="22a3" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">我们将从英语的一小部分开始:动物词汇。我们的任务是能够找到这些词和它们所指的生物之间的相似之处。为此，我们可以从制作一些动物及其特征的电子表格开始。例如:</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ky"><img src="../Images/453ecdfccaa8ea924964d8e49c15e11a.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*0GaAusRXkWUCRDXqYdicbg.png"/></div></figure><p id="1e65" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">这张电子表格将一些动物与两个数字联系起来:它们的可爱程度和大小，两者的范围都是从0到100。(这些值本身是随机取的。)</p><p id="c661" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">这些值为我们提供了确定哪些动物相似所需的一切(至少，我们在数据中包含的属性相似)。现在如果我们试着回答一个问题:哪种动物最像水豚？您可以逐个查看这些值，并进行数学计算来做出评估，但是将数据可视化为二维空间中的点可以非常直观地找到答案:</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/af2935b6427cf8af8b1df5d10123fcf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*lDd5kPhw7yPtok2QKPguPA.png"/></div></figure><p id="da8c" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">剧情向我们展示了与水豚最接近的动物是熊猫(同样，就其主观大小和可爱程度而言)。计算两点“相距多远”的一种方法是找出它们的欧几里德距离。</p><p id="dbf8" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">你可以把这个箭头理解为狼蛛和仓鼠在大小和可爱程度上的关系。在同一个图中，我也调换了这个箭头(这次是红色的)，所以它的原点是“鸡”箭头的末端离“小猫”最近我们发现的是，和鸡差不多大但更可爱的动物是…一只小猫。打个比方来说:</p><pre class="kz la lb lc fd le lf lg lh aw li bi"><span id="5c73" class="lj jt hi lf b fi lk ll l lm ln">Tarantulas are to hamsters as chickens are to kittens.</span></pre><p id="0b81" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">现在我们有了为什么我们需要把一个单词转换成一个向量的基本概念，我们继续理解如何在一个N维空间中得到一个单词的向量。</p><h1 id="07ca" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">词向量从何而来？</h1><p id="d94d" class="pw-post-body-paragraph it iu hi iw b ix kq iz ja jb kr jd je ks kt jh ji ku kv jl jm kw kx jp jq jr hb bi translated">此时一个很好的问题是，<em class="iv">这些维度和权重从何而来？！</em>生成词向量有两种常见方式:</p><ol class=""><li id="5379" class="lo lp hi iw b ix iy jb jc ks lq ku lr kw ls jr lt lu lv lw bi translated">单词/上下文共现的计数</li><li id="2795" class="lo lp hi iw b ix lx jb ly ks lz ku ma kw mb jr lt lu lv lw bi translated">给定单词的上下文预测(skip-gram神经网络模型，即word2vec)</li></ol><h1 id="7c36" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">这些单词在一个N维矩阵中是如何表示的？</strong></h1><blockquote class="iq ir is"><p id="0b51" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="hi">步骤1- </em> </strong>我们一键编码每个单词。这里假设词汇表是10K单词，并且每个单词用一维(1*10k)的一键编码来表示。</p></blockquote><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/18d61a42f9aa1d037570ecadbe557178.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VQdMQ1nAkUKKoLHtk-R8Rw.png"/></div></div></figure><p id="713f" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">例如，Man(5391)在一个热编码中只有一个1，其余的都是其他单词。也表示为O5391。</p><p id="0e1a" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">现在，如果我们有一个矩阵，其中每个单词都被分类，并用基于其维度的数字评级来表示。例如，男性性别被归类为男性为-1，女性为+1。同样地，国王的性别是男性，所以会有一个负值，而女王是女性会有一个正值。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/8a30eaf3366b5d40e9d8d5e9f7800dc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IsKkCAn87nFrZtR7Hg5zPw.png"/></div></div></figure><p id="df62" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">所以这里我们有对应于维度的行，维度有对应于它的列的数值，但是实际上，像性别、皇家和所有这些行很难解释，但是它们只是随着这个概念而发展。</p><blockquote class="md"><p id="6f3a" class="me mf hi bd mg mh mi mj mk ml mm jr dx translated"><strong class="ak">可视化2D中的单词嵌入:</strong>这可以使用用于降维的TSNE来完成。</p></blockquote><figure class="mn mo mp mq mr ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/15396050455029b04fe82616827bbf09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pLi8sROXPKNuEuXawwl08Q.png"/></div></div></figure><blockquote class="iq ir is"><p id="d548" class="it iu iv iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">步骤2- </strong>当我们必须找到对应于单词的N维向量时，我们将嵌入矩阵乘以单词的独热编码形式，并且我们得到的输出是该单词的N维向量。</p></blockquote><p id="0928" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">在这里，为了找到对应于一个单词的300维向量，我们将这个嵌入矩阵乘以该单词的一个热表示，该热表示是10k*1，并且因为10k是唯一单词的组合，所以1热表示将只有“1”出现一次，所以当与这个嵌入矩阵相乘时，我们将得到(300*1)矩阵，该矩阵将具有所有300维的值(一个单词离其他维有多远)。所以，每当我们需要一个词的这种表示时，我们就使用这种方法。</p><blockquote class="md"><p id="d42e" class="me mf hi bd mg mh mi mj mk ml mm jr dx translated">E.OJ = EJ(E =嵌入矩阵，O = 1-热表示)</p></blockquote><figure class="mn mo mp mq mr ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/06b33750e85ec92a91c69f17689af3b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fDIascaXjzMuoY7dmmLwJg.png"/></div></div></figure><p id="414c" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">现在我们已经掌握了单词嵌入，我们可以深入到Word2Vec的概念中。</p><h1 id="cda0" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><span class="l ms mt mu bm mv mw mx my mz di"> W </span> <strong class="ak"> ord2Vec : </strong></h1><p id="8ffc" class="pw-post-body-paragraph it iu hi iw b ix kq iz ja jb kr jd je ks kt jh ji ku kv jl jm kw kx jp jq jr hb bi translated">我们将保持语料库简单，因为它有助于我们轻松理解每一步，而且我们可以清晰地可视化关系，使其更加具体。<strong class="iw hj"> CBOW(连续单词包)和Skip-Gram </strong>是两种最流行的单词嵌入框架。在CBOW中，在所选单词的上下文(周围单词)中出现的单词被用作输入，中间或所选单词被用作目标。在Skip-Gram中正好相反，这里中间的单词试图预测它前面和后面的单词。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es na"><img src="../Images/fddddcde6f9cd247a26271590b2a4117.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7DidClA6gbkKp9w46P3GJA.png"/></div></div></figure><p id="78fa" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">考虑由从“a”到“z”的字符顺序组成的文本[a，b，c，d，e，f，g，h，I，j，k，l，m，n，o，p，q，r，s，t，u，v，w，x，y，z]，进一步让整数0到25代表相应的字母。在保持窗口大小为2的情况下实现CBOW，我们可以看到“a”与“b”和“c”相关，“b”与“a”、“c”和“d”相关，依此类推。由此可见，表示输入单词的一个热向量的维数为[26，1]。在模型的帮助下，我们将找到大小为[10，1]的密集分布向量。嵌入向量的大小是任意选择的。</p><p id="658a" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">下图说明了上下文和目标词。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nb"><img src="../Images/8744ec591bdadeed5bcdaea14e30095c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GEzwA77YB9J9q-0GvWyQFw.png"/></div></div><figcaption class="nc nd et er es ne nf bd b be z dx translated">向左。CBOW和Skip-gram模型的排列。没错。字符和相应的整数组合成输入和目标列表。对于每个条目，第一列代表输入字符，每个子列表中的其余条目是输入的目标。</figcaption></figure><p id="8305" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">想象关系的模式是很容易的。当它们在两个字符的范围内同时出现时，它们是相关的或者共现是真实的。下面的表格和地图描述了这种模式。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ng"><img src="../Images/5d6a0485969ded4aefcc4acb133fcacf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KXS35mdm6mSISUw9rkTPEw.png"/></div></div><figcaption class="nc nd et er es ne nf bd b be z dx translated">向左。一起出现的字符(没有完全可视化)。没错。黑色区域表示没有同现，白色区域表示在给定窗口大小为2 #的情况下字符一起出现。还要注意对角线对称，这意味着从a到b的同现意味着从b到a的同现，但是这些关系在不同的轴上描述。</figcaption></figure><p id="a8d3" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">Word2Vec是一个概率模型。该模型的关键组件是两个权重矩阵。第一矩阵(w1)的行和第二矩阵(w2)的列分别嵌入输入单词和目标单词。给定所选择的输入单词，这两个单词向量的乘积然后被用于获得成为目标单词的概率。在训练时，使用梯度下降优化这些嵌入向量，使得真实目标的概率最大化。显然，矩阵w1和w2是因式分解的概率矩阵，它非常类似于共生矩阵。</p><p id="3e07" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">下图对模型进行了说明和解释。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div class="ab fe cl nh"><img src="../Images/31573030d1a65a61fbd5e375cc6c1f55.png" data-original-src="https://miro.medium.com/v2/format:webp/1*b31hiO4ynbDLRrXWEFF4aQ.png"/></div><figcaption class="nc nd et er es ne nf bd b be z dx translated">Word2Vec模型示意图。输入和输出向量的大小为V，而隐藏向量(嵌入)的大小为n .(来源:<a class="ae ni" href="https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html" rel="noopener ugc nofollow" target="_blank">https://lilian Weng . github . io/lil-log/2017/10/15/learning-word-embedding . html</a>)</figcaption></figure><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nj"><img src="../Images/47965d27f79e3a2fde45d25688c968ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vPEYQa6jXJjCZcYW.png"/></div></div><figcaption class="nc nd et er es ne nf bd b be z dx translated">在此图中，模型正在学习将6个字符(“a”到“f”)嵌入到三维嵌入向量中。<strong class="bd ju"> A. </strong>这些字符的共现矩阵的窗口大小为2，共现表示为存在或不存在。(大小6，6) <strong class="bd ju"> B. </strong>权重矩阵w1 —转置。矩阵w1的每一行是一个字的嵌入向量，因为一个热码向量唯一地选择一个热码向量的矩阵w1(或W1转置的列)的对应行<strong class="bd ju"> C. </strong>，每一列表示一个字/项<strong class="bd ju">d .</strong>W1和输入矩阵的乘积导致矩阵h(隐藏层)。这里，整个输入矩阵作为单位矩阵，并简单地将权重矩阵w1作为隐藏矩阵h传递。然而，单位矩阵不是必须的，输入矩阵(以及隐藏层)的顺序和大小可以不同<strong class="bd ju"> E. </strong>权重矩阵w2-转置(大小6，3)。矩阵w2的每一列近似地表示目标词<strong class="bd ju"> F. </strong>隐藏层— h，与之前描述的w1相同<strong class="bd ju">g .</strong>w2的每一行—具有一列隐藏的转置乘积(其嵌入输入词)输出大小为vocab的长度的分数<strong class="bd ju"> H. </strong>如上所述，w2 _转置的所有行与1列隐藏的相互作用—产生一列分数矩阵，w2 _转置和‘h’的总乘积是矩阵S(大小为6，6) 【T18一列中的每个条目被转换成概率<strong class="bd ju"> J. </strong>概率矩阵-该矩阵的一列中的每个项目表示在给定输入单词的情况下成为目标单词的概率<strong class="bd ju"> L. </strong>误差-真实索引位置处的概率将被最大化，通过比较分配给对应于真实目标的索引的该概率来计算误差<strong class="bd ju">m .</strong>Back prop-在开始时随机初始化权重，并且所得的概率对于所有目标来说是低的和相似的。在训练、梯度的反向传播和权重的优化之后，概率在目标周围是密集的，而在其他地方接近于0。</figcaption></figure><p id="48f1" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated"><strong class="iw hj">余弦相似度</strong></p><p id="5f0a" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">在不同的距离度量中，余弦相似度更直观，在word2vec中使用最多。它是两个向量的归一化点积，这个比值定义了它们之间的角度。具有相同方向的两个向量的余弦相似度为1，90°的两个向量的相似度为0，直径方向相反的两个向量的相似度为-1，与它们的大小无关。</p><figure class="kz la lb lc fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nk"><img src="../Images/a1cef446d7df8184e54c4bf5ba117767.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P8MAOCL1lyiVbANHNmFGHA.png"/></div></div><figcaption class="nc nd et er es ne nf bd b be z dx translated">左上:余弦距离的等式右上:余弦距离从1到-1变化，对应的角度从0到180度变化。左下角:从c到所有字符的余弦距离。右下角:用角度表示的相同距离。注‘c’与其自身最为相似。</figcaption></figure><h1 id="47c3" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">参考:</strong></h1><div class="nl nm ez fb nn no"><a href="https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa" rel="noopener follow" target="_blank"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hj fi z dy nt ea eb nu ed ef hh bi translated">单词嵌入和Word2Vec简介</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">单词嵌入是文档词汇表最流行的表示方法之一。它能够捕捉…的上下文</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">towardsdatascience.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc io no"/></div></div></a></div><div class="nl nm ez fb nn no"><a href="https://dzone.com/articles/introduction-to-word-vectors" rel="noopener  ugc nofollow" target="_blank"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hj fi z dy nt ea eb nu ed ef hh bi translated">单词向量介绍- DZone AI</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">词向量代表了我们在分析词与词之间关系的能力上的一个重大飞跃…</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">dzone.com</p></div></div><div class="nx l"><div class="od l nz oa ob nx oc io no"/></div></div></a></div><div class="nl nm ez fb nn no"><a href="https://towardsdatascience.com/word2vec-made-easy-139a31a4b8ae" rel="noopener follow" target="_blank"><div class="np ab dw"><div class="nq ab nr cl cj ns"><h2 class="bd hj fi z dy nt ea eb nu ed ef hh bi translated">Word2vec变得简单</h2><div class="nv l"><h3 class="bd b fi z dy nt ea eb nu ed ef dx translated">这篇文章是word2vec的一个简单而深入的指南。在本文中，我们将实现word2vec模型</h3></div><div class="nw l"><p class="bd b fp z dy nt ea eb nu ed ef dx translated">towardsdatascience.com</p></div></div><div class="nx l"><div class="oe l nz oa ob nx oc io no"/></div></div></a></div><p id="35e5" class="pw-post-body-paragraph it iu hi iw b ix iy iz ja jb jc jd je ks jg jh ji ku jk jl jm kw jo jp jq jr hb bi translated">艾利森·帕里什的《理解单词向量:【https://gist.github.com/aparrish/2f56... T2】</p></div></div>    
</body>
</html>