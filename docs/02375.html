<html>
<head>
<title>How I learned to stop worrying and trust the script.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我是如何学会不再担心并相信剧本的。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-i-learned-to-stop-worry-and-trust-the-script-f428101ee167?source=collection_archive---------12-----------------------#2019-12-13">https://medium.com/analytics-vidhya/how-i-learned-to-stop-worry-and-trust-the-script-f428101ee167?source=collection_archive---------12-----------------------#2019-12-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div class="er es hg"><img src="../Images/a0f716debaa4cd702cd5e5981a751106.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*G32e55S_IWVeaSDKpZCsjA.png"/></div></figure><div class=""/><div class=""><h2 id="271c" class="pw-subtitle-paragraph im ho hp bd b in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd dx translated">或者如何在python中自动创建特定的文本分类模型</h2></div><p id="2e3f" class="pw-post-body-paragraph je jf hp jg b jh ji iq jj jk jl it jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">互联网很棒，我们可以找到任何主题的信息。然后，困难的部分是组织它们，并根据您感兴趣的主题将这些数据分类到正确的桶中。一个经典的方法是浏览所有内容，阅读它，以某种方式将你提取的主题存储在某个地方(在你的脑海中，post-it，csv文件…)，然后手动将所有这些文档分类到正确的桶中。如果你的语料库不太大，这还可以，但是一旦你有十几个长度变化很大的文档，这就成了一个很大的负担。此外，您可能只对文档中的一个主题感兴趣，但仍然需要阅读全部内容，并且您的分类可能不一致。肯定有更好的方法来利用我们的时间… </p><p id="7306" class="pw-post-body-paragraph je jf hp jg b jh ji iq jj jk jl it jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><em class="ka">牢记这些要点，我编写了一个脚本，用于文本数据语料库的自动提取和分类。除了创建这个解决方案，其他人的目标是获得一些关于数据废弃、API和空间的实践经验。</em></p><h1 id="bcb6" class="kb kc hp bd kd ke kf kg kh ki kj kk kl iv km iw kn iy ko iz kp jb kq jc kr ks bi translated"><strong class="ak">1-查找数据</strong></h1><p id="0d1d" class="pw-post-body-paragraph je jf hp jg b jh kt iq jj jk ku it jm jn kv jp jq jr kw jt ju jv kx jx jy jz hb bi translated">对于任何数据科学项目，最困难的部分通常是找到一个有意义的数据集，该数据集具有足够的内容和代表性数据，以实际上表现良好。幸运的是，至少对于NLP项目来说，很多人愿意用近300种语言发表大约3000万篇文章。在过去的18年里，人们在一个开放的在线平台上自由地发表文章并互相更正。我当然指的是在线百科全书维基百科。该平台提供了一个关于许多主题的记录良好、相当可靠且组织良好的百科全书，因此可以用作NLP项目的数据提取的参考。首先，这些数据必须从维基百科中删除。由于维基百科页面有这样的格式<a class="ae ky" href="https://en.wikipedia.org/wiki/Turing_machine," rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Turing_machine,</a>其中‘en’代表英语，而‘Turing _ machine’是你想要探索的主题，你可以动态地创建一些你将用来获取数据的URL(起初我想使用维基百科API，但是在创建这个脚本时没有一个被支持)。这样，脚本将遍历html页面，提取span部分中的每个字幕，并在以后将其用作数据集的标签。</p><p id="bebf" class="pw-post-body-paragraph je jf hp jg b jh ji iq jj jk jl it jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">这是通过这个函数使用beautifulsoup完成的:</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="f556" class="li kc hp le b fi lj lk l ll lm">######################<br/>##this function takes as input the <br/>##language and the subject you want to create a classifier for<br/>## it will then get the ad hoc url in wkipedia<br/>## then it will scrape each span to get its title as a category<br/>#################### <br/>def get_category(language, subject):                                    </span><span id="7004" class="li kc hp le b fi ln lk l ll lm">    wikipedia.set_lang(language)                           <br/>    source = urllib.request.urlopen(wikipedia.page(subject).url).read()                         </span><span id="2d91" class="li kc hp le b fi ln lk l ll lm">    soup = bs.BeautifulSoup(source,'lxml')                               </span><span id="56ee" class="li kc hp le b fi ln lk l ll lm">    soup_txt = str(soup.body)                           <br/>    category = []                           <br/>    for each_span in soup.find_all('span', {'class':'mw-headline'}):                                   <br/>        soup = BeautifulSoup(str(each_span).replace(' ','_'), "html.parser").getText()                                      </span><span id="681c" class="li kc hp le b fi ln lk l ll lm">        category.append(soup)                           <br/>    return category</span></pre><p id="c0ac" class="pw-post-body-paragraph je jf hp jg b jh ji iq jj jk jl it jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">然后，一旦你有了标签，你就需要相应的数据。这是通过以下方式实现的:</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="d3a7" class="li kc hp le b fi lj lk l ll lm">#################### <br/>## this function takes as input the language and the subject <br/>## it gets the text for each span and cleans it <br/>## then it will output a list containing each clean sentence #################### <br/>def get_data(language, subject): <br/>    wikipedia.set_lang(language) <br/>    source = urllib.request.urlopen(wikipedia.page(subject).url).read() <br/>    soup = bs.BeautifulSoup(source,’lxml’) <br/>    soup_txt = str(soup.body) <br/>    div = [] <br/>    for each_span in soup.find_all(‘span’, {‘class’:’mw-headline’}):     </span><span id="af97" class="li kc hp le b fi ln lk l ll lm">        str(each_span).replace(‘ ‘,’_’) <br/>        div.append(str(each_span)) <br/>    filter_tag = [] <br/>    i = 0 <br/>    while i &lt; len(div)-1: <br/>        start = div[i] <br/>        end = div[i+1] <br/>        text = soup_txt[soup_txt.find(start)+len(start):soup_txt.rfind(end)] <br/>        soup = str(BeautifulSoup(text, “html.parser”)) <br/>        soup = re.sub(“([\(\[]).*?([\)\]])”, “\g&lt;1&gt;\g&lt;2&gt;”, soup) <br/>        soup = BeautifulSoup(soup, “html.parser”) <br/>        soup = re.compile(r’&lt;img.*?/&gt;’).sub(‘’, str(soup.find_all(‘p’))) <br/>        soup = BeautifulSoup(soup, “html.parser”) <br/>        soup = (re.sub(“[^a-zA-Z,.;:!0–9]”,” “,soup.getText()).replace(‘[‘,’’).replace(‘]’,’’).lstrip().rstrip().lower())<br/>        clean_text = re.sub(‘ +’, ‘ ‘,soup).replace(‘,’,’ ‘)      </span><span id="820f" class="li kc hp le b fi ln lk l ll lm">        filter_tag.append(clean_text) <br/>        i += 1 <br/>   return filter_tag</span></pre><p id="ce0a" class="pw-post-body-paragraph je jf hp jg b jh ji iq jj jk jl it jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">最后，所有东西都通过函数组合在一起:</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="7d5e" class="li kc hp le b fi lj lk l ll lm">#################### <br/>## this function takes as input the list of filter tags<br/>## it will output a list containing the number of sentences in each category <br/>#################### <br/>def get_len_list(filter_tag): <br/>    filtered_text = [] <br/>    len_list = [] <br/>    i = 0 <br/>    while i &lt; len(filter_tag): <br/>        doc = nlp(filter_tag[i]) <br/>        text = [sent.string.strip() for sent in doc.sents] <br/>        filtered_text.append(text) <br/>        len_list.append(len(filtered_text[i])) <br/>        i += 1 <br/>    return filtered_text,len_list</span></pre><p id="2c22" class="pw-post-body-paragraph je jf hp jg b jh ji iq jj jk jl it jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">这将创建一个基于主题和语言的had hoc pandas数据框架。在每个区间中提取的每个句子被提取，并且它所链接的字幕被用作标签。</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="ec9e" class="li kc hp le b fi lj lk l ll lm">#################### <br/>##this function takes as input the len_list and the list of category ## it will output a dataframe containing the label (category) for each sentence <br/>#################### <br/>def generate_dataset(len_list, category): <br/>    i = 0 <br/>    label_list = [] <br/>    while i &lt; len(len_list): <br/>        j = 0 <br/>        if len_list[i] != 0: <br/>            while j != len_list[i]: <br/>            label_list.append(category[i].lower()) <br/>            j += 1 <br/>        i += 1 <br/>    flat_list = [item for sublist in get_len_list(get_data(language, subject))[0] for item in sublist] <br/>    data = {‘text’: flat_list,’label’: label_list} <br/>    df = pd.DataFrame.from_dict(data) print(df.head()) <br/>    print(‘Repartition of labels:’, df[‘label’].iloc[0]) <br/>    print(‘Data Shape:’, df.shape) <br/>    return df</span></pre><h1 id="d079" class="kb kc hp bd kd ke kf kg kh ki kj kk kl iv km iw kn iy ko iz kp jb kq jc kr ks bi translated">2-训练模型</h1><p id="a3eb" class="pw-post-body-paragraph je jf hp jg b jh kt iq jj jk ku it jm jn kv jp jq jr kw jt ju jv kx jx jy jz hb bi translated">一旦有了带有标注的特殊数据集，现在就该挑选模型并对其进行训练了。起初，我想用LSTM进行分类实验，但结果并不太好，我想这是因为我没有太多的数据。所以我选择了一种更经典的方法，使用一些众所周知的NLP操作。</p><p id="8291" class="pw-post-body-paragraph je jf hp jg b jh ji iq jj jk jl it jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">首先，我去掉了给定语言的停用词(spacy提出了英语、法语、德语、西班牙语、葡萄牙语、意大利语和荷兰语的内置列表)。</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="7c1f" class="li kc hp le b fi lj lk l ll lm">#################### <br/>## this function takes as input the language <br/>## it will output the list of stopwords for this language as a list #################### <br/>def get_stop_words(language): <br/>    if language == ‘en’: <br/>        spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS <br/>    if language == ‘fr’: <br/>        spacy_stopwords = spacy.lang.fr.stop_words.STOP_WORDS <br/>    if language == ‘de’: <br/>        spacy_stopwords = spacy.lang.de.stop_words.STOP_WORDS <br/>    if language == ‘es’: <br/>        spacy_stopwords = spacy.lang.es.stop_words.STOP_WORDS <br/>    if language == ‘pt’: <br/>        spacy_stopwords = spacy.lang.pt.stop_words.STOP_WORDS <br/>    if language == ‘it’: <br/>        spacy_stopwords = spacy.lang.it.stop_words.STOP_WORDS <br/>    if language == ‘nl’: <br/>        spacy_stopwords = spacy.lang.nl.stop_words.STOP_WORDS <br/>    return spacy_stopwords</span></pre><p id="4380" class="pw-post-body-paragraph je jf hp jg b jh ji iq jj jk jl it jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">然后，我应用了词汇化，只保留了单词的词根。我可以使用词干，但是它不太适合拉丁语。</p><pre class="kz la lb lc fd ld le lf lg aw lh bi"><span id="d8d9" class="li kc hp le b fi lj lk l ll lm">#################### <br/>## this function takes as input the language on the generated dataset <br/>## it performs some last cleaning and data visualsation on the dataset <br/>#################### <br/>def clean_dataset(language, df): <br/>    srce_labels = df.label.values.tolist() <br/>    srce_text = df.text.values.tolist() <br/>    spacy_stopwords = get_stop_words(language) <br/>    clean_text = [] <br/>    text = [] <br/>    i = 0 <br/>    while i &lt; len(srce_text): <br/>        extract = [] <br/>        doc = nlp(srce_text[i]) <br/>        for token in doc: <br/>            extract.append(token.lemma_)             </span><span id="e73f" class="li kc hp le b fi ln lk l ll lm">        clean_text.append(“,”.join(extract).replace(“,”,” “).replace(“ “,” “)) <br/>        i += 1 <br/>    print(‘Number of stop words: %d’ % len(spacy_stopwords)) <br/>    i = 0 <br/>    while i &lt; len(clean_text): <br/>        doc = nlp(clean_text[i]) <br/>        tokens = [token.text for token in doc if not token.is_stop]     </span><span id="dbb2" class="li kc hp le b fi ln lk l ll lm">        text.append(“,”.join(tokens).replace(“,”,” “).replace(“ “,” “).replace(“ “,” “).replace(“-PRON-”,” “).rstrip().lstrip()) <br/>        i += 1 <br/>    data = {‘text’: text,’label’: srce_labels} <br/>    df = pd.DataFrame.from_dict(data) <br/>    df = df.dropna() <br/>    return df</span></pre><p id="cc04" class="pw-post-body-paragraph je jf hp jg b jh ji iq jj jk jl it jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">我应用了更多的操作来得到一个更干净的数据集，适合NLP操作。</p><p id="4c03" class="pw-post-body-paragraph je jf hp jg b jh ji iq jj jk jl it jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">问题是模型理解数字，而不是单词，所以我决定使用单词包方法将这些单词转换成数字。</p><p id="d957" class="pw-post-body-paragraph je jf hp jg b jh ji iq jj jk jl it jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">然后在一个随机森林中应用网格搜索，以获得这个给定主题在这个给定语言中可能的最佳模型。然后，模型被保存为pickle格式，并可以被调用进行分类。</p><h1 id="a744" class="kb kc hp bd kd ke kf kg kh ki kj kk kl iv km iw kn iy ko iz kp jb kq jc kr ks bi translated">3-结论</h1><p id="b505" class="pw-post-body-paragraph je jf hp jg b jh kt iq jj jk ku it jm jn kv jp jq jr kw jt ju jv kx jx jy jz hb bi translated">这个小项目让我有机会尝试许多不同的新概念和新技术，从使用最新库的经典NLP操作到web报废和公共API操作以及其他库，如argparse或pickle。我想通过添加虚拟环境和docker容器来不断改进这一点，使它们更容易复制和共享，并在这个问题上与LSTM一起做好实验。但至少有了这个项目，我有了一个一致的、非常强大的方法来为7种语言中给定主题的文本语料库分类创建自动模型。这就是我如何学会不再担心并相信剧本的。</p><h1 id="af9b" class="kb kc hp bd kd ke kf kg kh ki kj kk kl iv km iw kn iy ko iz kp jb kq jc kr ks bi translated">4向前进</h1><p id="ac21" class="pw-post-body-paragraph je jf hp jg b jh kt iq jj jk ku it jm jn kv jp jq jr kw jt ju jv kx jx jy jz hb bi translated">到目前为止，我认为当前模型的几个缺点很容易解决:</p><ul class=""><li id="e9f9" class="lo lp hp jg b jh ji jk jl jn lq jr lr jv ls jz lt lu lv lw bi translated">使用TF-IDF代替countvectorizer可能会得到更一致的矢量化结果</li><li id="6390" class="lo lp hp jg b jh lx jk ly jn lz jr ma jv mb jz lt lu lv lw bi translated">使用进化算法，通过例如TPOT库而不是网格搜索来创建更好的分类模型</li></ul><p id="38d2" class="pw-post-body-paragraph je jf hp jg b jh ji iq jj jk jl it jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><em class="ka">资源库可以在这里找到:</em><a class="ae ky" href="https://github.com/elBichon/blue_orchid" rel="noopener ugc nofollow" target="_blank"><em class="ka"/></a></p></div></div>    
</body>
</html>