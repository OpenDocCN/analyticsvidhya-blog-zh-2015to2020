<html>
<head>
<title>Long Short-Term Memory Decoded</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">长短期记忆解码</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/long-short-term-memory-decoded-9041fe06235f?source=collection_archive---------14-----------------------#2020-04-14">https://medium.com/analytics-vidhya/long-short-term-memory-decoded-9041fe06235f?source=collection_archive---------14-----------------------#2020-04-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="10de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated">除非你一直生活在岩石下，否则你可能听说过人工智能<strong class="ih hj"> </strong>以及它将如何在不久的将来接管世界。但是它到底能给我们的日常生活带来什么好处呢？</p><blockquote class="jm jn jo"><p id="41eb" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated">如果你对人工智能和机器学习不熟悉，可以看看我以前的文章，了解一些基本的背景知识:</p></blockquote><div class="jt ju ez fb jv jw"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/a-beginners-guide-to-machine-learning-6f6ac495d8eb"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">机器学习初学者指南</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">塔里克·伊尔沙德</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">medium.co</p></div></div><div class="kf l"><div class="kg l kh ki kj kf kk kl jw"/></div></div></a></div><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es km"><img src="../Images/6372123303a2a04d57248816d55dd9fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BHHkvR8b0M4tc5p8ugVl7w.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">【https://www.javatpoint.com/subsets-of-ai T4】</figcaption></figure><p id="8cea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看上面AI分支的示意图，有一条路径是通过<strong class="ih hj">机器学习</strong>通向<strong class="ih hj">深度学习</strong>。在人工智能的一个子类中的这个子类中(是的，这两个子类是有意的)，有一个更具体的领域<strong class="ih hj">长短期记忆(LSTM) </strong>。</p><p id="80b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LSTM <strong class="ih hj"> </strong>在AI中只占很小一部分。在我们到达那里之前，让我们看一看导致它的更广泛的人工智能子领域！</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es lc"><img src="../Images/0d8b03e233a89f686b73276991311741.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*bd7Sz7Un3Z9xLQ6doVydbw.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><a class="ae lb" href="https://blog.knoldus.com/data-processing-and-using-ml-supervised-classification-algorithm-and-find-accuracy/" rel="noopener ugc nofollow" target="_blank">https://blog . knol dus . com/data-processing-and-using-ml-supervised-class ification-algorithm-and-find-accuracy/</a></figcaption></figure><ol class=""><li id="eac5" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated"><strong class="ih hj">机器学习</strong>是人工智能的一个分支，它基于机器可以自己从输入的数据中学习的想法。然后，他们可以在最少的人工干预下，根据给定的数据识别模式并做出决策。简单地说，机器学习的目标是让机器在没有明确编程的情况下行动。</li><li id="faad" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><strong class="ih hj">深度学习</strong>是机器学习的一个分支，自学系统通过算法发现的模式，使用现有数据对新数据进行预测。这个过程是由模拟人脑神经元的人工神经网络完成的。看起来像是:</li></ol><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es lr"><img src="../Images/fc24450619791dff9492787b3d3f8808.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3fA77_mLNiJTSgZFhYnU0Q@2x.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><a class="ae lb" href="https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6" rel="noopener" target="_blank">https://towards data science . com/applied-deep-learning-part-1-artificial-neural-networks-d 7834 f 67 a4f 6</a></figcaption></figure><p id="05ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">神经网络有三层:输入层、隐藏层和输出层。</p><ol class=""><li id="9d32" class="ld le hi ih b ii ij im in iq lf iu lg iy lh jc li lj lk ll bi translated"><strong class="ih hj">输入层</strong>将初始数据带入系统，供后续层处理。</li><li id="4614" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><strong class="ih hj">隐藏层</strong>是所有“深度学习”实际发生的地方，它隐藏在输入层和输出层之间。隐藏层通过对加权输入执行计算来产生净输入，然后应用非线性激活函数来产生最终输出。需要注意的是，为了让它成为深度学习，必须有一个以上的隐藏层。</li><li id="6a54" class="ld le hi ih b ii lm im ln iq lo iu lp iy lq jc li lj lk ll bi translated"><strong class="ih hj">输出层</strong>简单地产生给定输入的结果。</li></ol><p id="21b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，当试图使用传统的神经网络时，你可能会遇到一个关键问题:它们不能使用从以前的试验中获得的信息来帮助通知他们未来的决策。因此，传统的神经网络不能处理<strong class="ih hj">序列数据</strong>。</p><p id="e11f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是为什么我们有<strong class="ih hj">循环神经网络</strong>！</p><h1 id="a02d" class="ls lt hi bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">递归神经网络</h1><p id="0c5b" class="pw-post-body-paragraph if ig hi ih b ii mq ik il im mr io ip iq ms is it iu mt iw ix iy mu ja jb jc hb bi translated">rnn是一种深度学习，包含<strong class="ih hj">循环</strong>，允许之前的信息持续存在——就像在人脑中一样。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es mv"><img src="../Images/a241229f6a57608152ac1c4f7be88d17.png" data-original-src="https://miro.medium.com/v2/resize:fit:230/format:webp/1*pbf8lq2ep4DLJKpo4XvxWg.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><a class="ae lb" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a></figcaption></figure><p id="3513" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">左图显示了一个RNN，其中<strong class="ih hj">隐藏状态(A) </strong>接受<strong class="ih hj">输入(xt) </strong>并产生一个<strong class="ih hj">输出值(ht) </strong>。RNN和传统神经网络之间的区别在于附加到隐藏状态的回路。</p><p id="b788" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">仅从这个图中可能很难理解循环的概念，所以让我们<strong class="ih hj">展开</strong>循环:</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es mw"><img src="../Images/ce9ed6a2d5d360bb512d44d26069f785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NKhwsOYNUT5xU7Pyf6Znhg.png"/></div></div></figure><p id="c173" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">添加回路简单地将一个<strong class="ih hj">前馈神经网络</strong>转变为一系列神经网络，这些神经网络都能够从先前的神经网络中获取信息。本质上，从一个神经网络的隐藏状态收集的信息被向前传递到下一个神经网络，使其能够在确定其应该产生的<strong class="ih hj">输出值</strong>时使用该信息。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es mx"><img src="../Images/17ad7c25dd4c4bc1bfdd9f6761677f40.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/1*McjuacX236J8oUz1kscx5A.gif"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><a class="ae lb" href="https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9" rel="noopener" target="_blank">https://towards data science . com/illustrated-guide-to-recurrent-neural-networks-79 E5 EB 8049 c 9</a></figcaption></figure><p id="efef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">看待RNN的另一种方式是把这个回路想象成一条路，允许信息从一个隐藏状态流向下一个隐藏状态。这显示在左图中，其中移动的蓝色圆圈是先验信息。</p><p id="60fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如前所述，rnn由于其循环性，在处理顺序数据时非常有用。正因为如此，有各种各样有用的RNN应用，从语言建模到T2图像字幕。如果你想了解更多关于使用RNNs的知识，请访问Andrej Kaparthy的博客文章:</p><div class="jt ju ez fb jv jw"><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">递归神经网络的不合理有效性</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">递归神经网络(rnn)有一些神奇的东西。我仍然记得当我训练我的第一个循环…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">karpathy.github.io</p></div></div><div class="kf l"><div class="my l kh ki kj kf kk kl jw"/></div></div></a></div><p id="8db7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，rnn也不是完美的。他们也面临一个重大问题:<strong class="ih hj">短期记忆</strong>。这意味着来自早期神经网络的信息不会持久，也不能被更晚的神经网络使用。这是由于<strong class="ih hj">消失梯度问题</strong>。</p><h2 id="a43f" class="mz lt hi bd lu na nb nc ly nd ne nf mc iq ng nh mg iu ni nj mk iy nk nl mo nm bi translated">反向传播和消失梯度问题</h2><p id="dc01" class="pw-post-body-paragraph if ig hi ih b ii mq ik il im mr io ip iq ms is it iu mt iw ix iy mu ja jb jc hb bi translated">为了理解消失梯度问题，你必须首先理解<strong class="ih hj">反向传播</strong>，用于训练RNNs的算法。反向传播是<strong class="ih hj">监督学习</strong>的一个例子，其中RNN将预测已经<strong class="ih hj">标记的数据点</strong>的输出。损失函数然后将预测与正确的输出进行比较，并输出误差值。最后，RNN使用误差值进行反向传播——计算RNN中每个节点的<strong class="ih hj">梯度</strong>。</p><p id="5292" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">梯度是允许RNNs通过调整其<strong class="ih hj">内部权重</strong>来学习的值。渐变<strong class="ih hj">在反向传播时收缩</strong>，因为渐变相对于前一个节点的渐变计算其值。如果之前的梯度很小，那么现在的梯度会更小。这使得对内部权重的调整更小，这意味着随着反向传播的发生，节点将学习得越来越少，并且最早的节点将由于梯度消失而几乎不学习。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es nn"><img src="../Images/dfaa31b4865cf64318b83578b4aeb02a.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*fzqkHug33Wvr4RVLF0SemQ.gif"/></div></figure><p id="6bf4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">消失的梯度导致RNNs无法处理<strong class="ih hj">长期依赖</strong>。下面的例子证明了为什么这是一个问题:</p><blockquote class="jm jn jo"><p id="d482" class="if ig jp ih b ii ij ik il im in io ip jq ir is it jr iv iw ix js iz ja jb jc hb bi translated"><em class="hi">比方说，我们想预测文本中的最后一个单词:“我在法国长大……我说一口流利的法语</em><em class="hi">。”由于最近的先验信息，RNN人会意识到这个单词是一种语言，但是是什么语言呢？RNN需要更多的上下文来把法国和这个词联系起来。</em></p></blockquote><p id="d2c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">谢天谢地，<strong class="ih hj">长短期记忆</strong>没有这个问题！</p><h1 id="1b01" class="ls lt hi bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">长短期记忆(LSTM)</h1><p id="aef3" class="pw-post-body-paragraph if ig hi ih b ii mq ik il im mr io ip iq ms is it iu mt iw ix iy mu ja jb jc hb bi translated">LSTM网络(lstm)是一种特殊类型的RNNs，通过操纵其记忆状态来减轻消失梯度问题。他们这样做的能力在于他们的<strong class="ih hj">架构</strong>。</p><p id="4631" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重新检查RNNs，单个<strong class="ih hj">单元</strong>的架构如下所示:</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es no"><img src="../Images/55b57e77cb260db681f04c6dca0d49c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*KxrxyB10ZbOc3xjDneQdhA.gif"/></div></div></figure><p id="ef0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">前一隐藏状态(ht-1) </strong>与<strong class="ih hj">输入(xt) </strong>结合形成<strong class="ih hj">向量[(ht-1)+(xt)] </strong>。向量包含关于先前输入和当前输入的信息。然后向量被一个<strong class="ih hj">双曲正切函数</strong>激活(产生一个从-1到1的值)以形成<strong class="ih hj">新的隐藏状态(ht) </strong>。新的隐藏状态然后移动到下一个单元格以完成相同的过程。</p><p id="e927" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相比之下，LSTMs的架构要复杂得多:</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es np"><img src="../Images/8842872c9b6bb29b04d4296cafbbd021.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jYpwdiMdh8XIrYy-wejZDw.png"/></div></div></figure><h2 id="010e" class="mz lt hi bd lu na nb nc ly nd ne nf mc iq ng nh mg iu ni nj mk iy nk nl mo nm bi translated">核心概念</h2><p id="0a7e" class="pw-post-body-paragraph if ig hi ih b ii mq ik il im mr io ip iq ms is it iu mt iw ix iy mu ja jb jc hb bi translated">什么使得LSTMs绕过短期记忆被称为<strong class="ih hj">细胞状态</strong>。这是图顶部穿过整个单元格的<strong class="ih hj">水平线</strong>。它可以被认为是沿着整个链条直接传输相关信息的高速公路。这意味着来自第一个单元的信息可以一直到达链的末端。</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es nq"><img src="../Images/db50c4dc0951900f2b33ab0386022ee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*a6FEUZOlcx4peSLv3eZVnw.png"/></div></div></figure><p id="0a6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LSTMs能够添加或删除单元状态。这种操作通过<strong class="ih hj">门</strong>进行并受其调节:神经网络决定哪些信息应该关于细胞状态。门由一个<strong class="ih hj"> sigmoid函数层</strong>和一个<strong class="ih hj">逐点乘法层</strong>组成。</p><p id="f215" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">sigmoid函数层产生一个从0到1的数字，该数字描述每种成分应该通过的量。值0对应于不允许任何东西通过，而值1对应于允许任何东西通过。</p><p id="f533" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LSTM中的单个单元包含三种类型的门:<strong class="ih hj">遗忘门</strong>、<strong class="ih hj">输入门</strong>和<strong class="ih hj">输出门</strong>。</p><h2 id="5b7a" class="mz lt hi bd lu na nb nc ly nd ne nf mc iq ng nh mg iu ni nj mk iy nk nl mo nm bi translated">忘记大门</h2><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es no"><img src="../Images/679d3448da623b8870bbe49e24bad940.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*bHvQiI-Ez9fVzzvAPpzb_Q.gif"/></div></div></figure><p id="6280" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">回到LSTM的架构，遗忘门是第一步，因为它决定了来自<strong class="ih hj">先前单元状态(Ct-1) </strong>的什么信息将被丢弃或保留。<strong class="ih hj">先前隐藏状态(ht-1) </strong>与<strong class="ih hj">输入(xt) </strong>结合形成<strong class="ih hj">向量[(ht-1)+(xt)] </strong>。然后，它通过sigmoid函数层，再次为前一个单元状态中的每个数字生成一个介于0和1之间的值，0表示要丢弃，1表示要保留。下面，该过程总结为一个等式:</p><blockquote class="nr"><p id="1ca7" class="ns nt hi bd nu nv nw nx ny nz oa jc dx translated">ft = sigmoid(Wf * [ht-1，xt] + bf)</p></blockquote><h2 id="e7a1" class="mz lt hi bd lu na ob nc ly nd oc nf mc iq od nh mg iu oe nj mk iy of nl mo nm bi translated">输入门</h2><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es no"><img src="../Images/4b998166afe74dd24d7e77935b1a5341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*EWfammOGJ4zaAYs9oB8AbQ.gif"/></div></div></figure><p id="dcba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后向量进入下一步:输入门。输入门决定哪些新信息将存储在单元状态中。这个过程有两个步骤。首先，向量通过sigmoid函数层来确定哪些值将使用上述过程进行更新。接下来，向量通过一个双曲正切函数，使值的范围从-1到1，创建一个<strong class="ih hj">候选向量(C̃t) </strong>。最后，sigmoid函数输出与双曲正切函数输出相乘，其中sigmoid函数输出决定双曲正切函数输出保留哪些值。下面，该过程总结为两个等式:</p><blockquote class="nr"><p id="eb4f" class="ns nt hi bd nu nv nw nx ny nz oa jc dx translated">it = sigmoid(Wi * [ht-1，xt] + bi)</p><p id="102d" class="ns nt hi bd nu nv nw nx ny nz oa jc dx translated">C̃t = tanh(Wc * [ht-1，xt] +bC)</p></blockquote><h2 id="9827" class="mz lt hi bd lu na ob nc ly nd oc nf mc iq od nh mg iu oe nj mk iy of nl mo nm bi translated">细胞状态</h2><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es no"><img src="../Images/f7b92d288a94f0113b630c27be16220e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*cmv5EOAd6iWMzWvHrZbl-w.gif"/></div></div></figure><p id="30f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在有足够的信息将旧的单元状态更新为<strong class="ih hj">新的单元状态(Ct) </strong>。我们首先将旧的单元状态乘以<strong class="ih hj">遗忘门输出(ft) </strong>，它会遗忘我们之前认为不重要的信息。然后，我们将<strong class="ih hj">输入门输出(it) </strong>乘以候选值，根据我们之前决定更新的程度来缩放这些值。最后，我们将缩放后的候选值与更新后的单元状态相加，得到新的单元状态。下面，该过程总结为一个等式:</p><blockquote class="nr"><p id="020b" class="ns nt hi bd nu nv nw nx ny nz oa jc dx translated">Ct = ft * Ct-1 + it * C̃t</p></blockquote><h2 id="c26f" class="mz lt hi bd lu na ob nc ly nd oc nf mc iq od nh mg iu oe nj mk iy of nl mo nm bi translated">输出门</h2><figure class="kn ko kp kq fd kr er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es no"><img src="../Images/a99d9b7240f290e123b5dd1d616675c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*S9n8E_9MwvZUc6Nb0REpMw.gif"/></div></div></figure><p id="f044" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们需要决定下一个隐藏层。首先，向量通过sigmoid函数层，以确定应该输出哪个更新的单元状态值。结果是<strong class="ih hj">输出门输出【ot】</strong>。然后，我们通过压缩-1和1之间的值的tanh函数传递单元格状态。最后，我们将这两个输出相乘来确定<strong class="ih hj">最终输出</strong>和下一个隐藏层。下面，该过程总结为两个等式:</p><blockquote class="nr"><p id="094c" class="ns nt hi bd nu nv nw nx ny nz oa jc dx translated">ot = sigmoid(Wo * [ht-1，xt] +bo)</p><p id="d793" class="ns nt hi bd nu nv nw nx ny nz oa jc dx translated">ht = ot * tanh(Ct)</p></blockquote><p id="5430" class="pw-post-body-paragraph if ig hi ih b ii og ik il im oh io ip iq oi is it iu oj iw ix iy ok ja jb jc hb bi translated">总之，LSTM使用门从单元状态的存储器中过滤出不重要的信息，这样相关的信息就可以通过LSTM的单元链传递。</p><p id="e326" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们知道了什么是LSTMs以及它们是如何工作的，让我们来看看它们的一些应用！</p><h1 id="32c3" class="ls lt hi bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">应用程序</h1><p id="de43" class="pw-post-body-paragraph if ig hi ih b ii mq ik il im mr io ip iq ms is it iu mt iw ix iy mu ja jb jc hb bi translated">LSTM在现实世界中有无数的应用，从手写识别到机器人控制。其两个主要应用是<strong class="ih hj">语音识别</strong>和<strong class="ih hj">时间序列预测</strong>。</p><h2 id="437b" class="mz lt hi bd lu na nb nc ly nd ne nf mc iq ng nh mg iu ni nj mk iy nk nl mo nm bi translated">1.语音识别</h2><p id="5e1d" class="pw-post-body-paragraph if ig hi ih b ii mq ik il im mr io ip iq ms is it iu mt iw ix iy mu ja jb jc hb bi translated">你有没有想过，你的电脑怎么有能力把你的话翻译成文本，或者Siri怎么能听懂你在说什么？嗯，这部分要归功于LSTMs。</p><p id="516c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了识别和处理人类的声音，机器使用了一种叫做自动语音识别(ASR)的过程。ARS的图表如下所示:</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es ol"><img src="../Images/bb3d951f403b2dacd0ab0262cd193a57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*3ewEn2i7mik047s2s4tiZA.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><a class="ae lb" href="https://towardsdatascience.com/hello-world-in-speech-recognition-b2f43b6c5871" rel="noopener" target="_blank">https://towards data science . com/hello-world-in-speech-recognition-B2 f 43 b 6 c 5871</a></figcaption></figure><p id="115f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<strong class="ih hj">卷积神经网络(CNN) </strong>层从频谱图向量输入中提取特征并创建特征向量之后，LSTM层需要处理这些特征向量并向<strong class="ih hj">全连接层</strong>提供输出。</p><p id="d6c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于LSTM的能力，它能够<strong class="ih hj">分析</strong>语音识别机器的输出，并确定所识别的单词在其上下文中是否有意义。这有助于语音识别机器创建正确的句子。</p><h2 id="e6c6" class="mz lt hi bd lu na nb nc ly nd ne nf mc iq ng nh mg iu ni nj mk iy nk nl mo nm bi translated">2.时间序列预测</h2><p id="65e1" class="pw-post-body-paragraph if ig hi ih b ii mq ik il im mr io ip iq ms is it iu mt iw ix iy mu ja jb jc hb bi translated">一个<strong class="ih hj">时间序列</strong>就是一系列按时间顺序排列的数据点。在时间序列中，时间是<strong class="ih hj">自变量</strong>。时间序列的一个突出例子是<strong class="ih hj">股价图</strong>，如下所示:</p><figure class="kn ko kp kq fd kr er es paragraph-image"><div class="er es om"><img src="../Images/8a87d7e030c2417acd335d9e6ee3c8b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*D5s10sW2HUYbh2L0mAoOjw.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><a class="ae lb" href="https://www.investopedia.com/ask/answers/081314/whats-most-expensive-stock-all-time.asp" rel="noopener ugc nofollow" target="_blank">https://www . investopedia . com/ask/answers/081314/whats-most-friendly-stock-all-time . ASP</a></figcaption></figure><p id="760f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于图表显示了<em class="jp"> BRK/A </em>的股票价格在一段设定的时间内是如何变化的，所以它是一个时间序列。</p><p id="b59c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LSTMs能够<strong class="ih hj">预测</strong>因变量，在这种情况下，股票价格，随着时间的推移将如何变化。</p><p id="35ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于我们之前讨论过的LSTM体系结构，lstm能够在已经给出的一系列数据点中寻找<strong class="ih hj">模式</strong>。它能够使用从时间序列最开始发现的模式来预测股票价格的变化。这就是它如此有效的原因。如果您有兴趣深入了解构建时间序列预测器背后的理论和编码，请查看Marco Peixeiro的文章:</p><div class="jt ju ez fb jv jw"><a href="https://towardsdatascience.com/the-complete-guide-to-time-series-analysis-and-forecasting-70d476bfe775" rel="noopener follow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">时间序列分析和预测完全指南</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">理解移动平均线，指数平滑，平稳性，自相关，SARIMA，并应用这些技术在…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">towardsdatascience.com</p></div></div><div class="kf l"><div class="on l kh ki kj kf kk kl jw"/></div></div></a></div><h2 id="e6e9" class="mz lt hi bd lu na nb nc ly nd ne nf mc iq ng nh mg iu ni nj mk iy nk nl mo nm bi translated">奖金:预测棒球标志</h2><p id="2058" class="pw-post-body-paragraph if ig hi ih b ii mq ik il im mr io ip iq ms is it iu mt iw ix iy mu ja jb jc hb bi translated">对于像我一样的棒球迷来说，你可能知道当跑垒员在垒道上时，三垒教练会给他们是否偷球的信号。随着最近关于太空人通过摄像机偷取标志的新闻，我们现在可以看到偷取标志能给对方球队带来多大的优势。</p><p id="d3dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是仅仅记录和回顾这些迹象是不够的。为了理解这些符号，你必须<strong class="ih hj">破译</strong>哪些信号对应什么，同时忽略虚张声势。</p><p id="2f22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">幸运的是，对于任何有抱负的偷牌者来说，马克·罗伯(Mark Rober)和贾布里勒(Jabrils)一起，两个YouTubers和工程师，创建了一个<strong class="ih hj">机器学习应用</strong>，当给定教练信号的输入时，它使用<strong class="ih hj"> LSTMs </strong>来确定偷牌的迹象，以及球员是否在多次比赛中偷牌。</p><p id="276a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你想深入了解他们的项目，他们制作的视频链接如下:</p><figure class="kn ko kp kq fd kr"><div class="bz dy l di"><div class="oo op l"/></div></figure></div><div class="ab cl oq or gp os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="hb hc hd he hf"><p id="be03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢您花时间阅读我关于LSTMs的文章。我希望你喜欢它，并学到了新的东西！如果你想做一些自己的研究，我建议你去看看这些网站(我也把它们作为资料来源):</p><div class="jt ju ez fb jv jw"><a href="https://arxiv.org/abs/1703.07090" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">用于大词汇量连续语音识别的深度LSTM</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">递归神经网络，特别是长短期记忆(LSTM)神经网络，是序列学习的有效网络</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">arxiv.org</p></div></div></div></a></div><div class="jt ju ez fb jv jw"><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">了解LSTM网络</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">2015年8月27日发布人类不是每秒钟都从零开始思考。当你读这篇文章时，你…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">colah.github.io</p></div></div></div></a></div><div class="jt ju ez fb jv jw"><a href="https://content.sciendo.com/view/journals/jaiscr/9/4/article-p235.xml" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">用于语音识别的深度神经网络的性能评估:RNN、LSTM和GRU</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">摘要深层神经网络(DNN)只不过是具有许多隐藏层的神经网络。dnn正在变得流行…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">content.sciendo.com</p></div></div></div></a></div><div class="jt ju ez fb jv jw"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/guide-to-lstms-for-beginners-ac9d1fc86176"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">LSTMs初学者指南。</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">在这个博客中，你将了解什么是LSTM，为什么我们需要它，以及LSTM的内部建筑概况。</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">medium.com</p></div></div><div class="kf l"><div class="ox l kh ki kj kf kk kl jw"/></div></div></a></div><div class="jt ju ez fb jv jw"><a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21" rel="noopener follow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">LSTM和GRU的图解指南:一步一步的解释</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">嗨，欢迎来到长短期记忆(LSTM)和门控循环单位(GRU)的图解指南。我是迈克尔…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">towardsdatascience.com</p></div></div><div class="kf l"><div class="oy l kh ki kj kf kk kl jw"/></div></div></a></div><div class="jt ju ez fb jv jw"><a href="https://towardsdatascience.com/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9" rel="noopener follow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">递归神经网络图解指南</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">理解直觉</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">towardsdatascience.com</p></div></div><div class="kf l"><div class="oz l kh ki kj kf kk kl jw"/></div></div></a></div><div class="jt ju ez fb jv jw"><a href="https://towardsdatascience.com/its-deep-learning-times-a-new-frontier-of-data-a1e9ef9fe9a8" rel="noopener follow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">这是深度学习时代:数据的新前沿</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">图像、音乐、情感等等</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">towardsdatascience.com</p></div></div><div class="kf l"><div class="pa l kh ki kj kf kk kl jw"/></div></div></a></div><div class="jt ju ez fb jv jw"><a href="https://towardsdatascience.com/using-lstms-to-forecast-time-series-4ab688386b1f" rel="noopener follow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">利用LSTMs预测时间序列</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">有几种时间序列预测技术，如自回归(AR)模型、移动平均(MA)模型…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">towardsdatascience.com</p></div></div><div class="kf l"><div class="pb l kh ki kj kf kk kl jw"/></div></div></a></div><div class="jt ju ez fb jv jw"><a href="https://www.forbes.com/sites/bernardmarr/2018/10/01/what-is-deep-learning-ai-a-simple-guide-with-8-practical-examples/#43ab7c578d4b" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">什么是深度学习AI？包含8个实例的简单指南</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">人工智能、机器学习和深度学习是当今最热门的词汇。本指南…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">www.forbes.com</p></div></div><div class="kf l"><div class="pc l kh ki kj kf kk kl jw"/></div></div></a></div><div class="jt ju ez fb jv jw"><a href="https://www.technologyreview.com/artificial-intelligence/" rel="noopener  ugc nofollow" target="_blank"><div class="jx ab dw"><div class="jy ab jz cl cj ka"><h2 class="bd hj fi z dy kb ea eb kc ed ef hh bi translated">人工智能</h2><div class="kd l"><h3 class="bd b fi z dy kb ea eb kc ed ef dx translated">AI是什么？这是对建造能够推理、学习和智能行动的机器的追求，而这才刚刚开始。我们…</h3></div><div class="ke l"><p class="bd b fp z dy kb ea eb kc ed ef dx translated">www.technologyreview.com</p></div></div><div class="kf l"><div class="pd l kh ki kj kf kk kl jw"/></div></div></a></div></div></div>    
</body>
</html>