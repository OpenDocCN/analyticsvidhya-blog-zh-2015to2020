<html>
<head>
<title>Essential Math For Understanding Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解神经网络的基本数学</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/essential-math-for-understanding-neural-networks-1173be7e1219?source=collection_archive---------12-----------------------#2020-02-19">https://medium.com/analytics-vidhya/essential-math-for-understanding-neural-networks-1173be7e1219?source=collection_archive---------12-----------------------#2020-02-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6923" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你好，人类同胞，我希望我能帮助你学习机器学习。我们将涵盖你可能需要完全理解神经网络背后的算法和概念的所有主题。我们将学习:偏导数、多维图形、标量、向量、矩阵、张量和一些最重要的概率分布。对于所有列出的主题，我将给出一个机器学习的例子，以及它是如何应用的。注意，在本文中我不会解释神经网络背后的概念；我打算通过掌握必要的数学知识，让你对NN <br/>的学习体验更容易消化，这些知识应用在NN后面的算法中，或者至少在正确的方向上引导你。</p><p id="0662" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们从<strong class="ih hj">偏导数</strong>的基础知识开始。它们在机器学习中很常见，尤其是在用于调整神经网络权重的反向传播中。<br/>我们需要在反向传播中使用偏导数的原因是为了找出比重或偏差如何影响损失函数。区分偏导数和普通导数的符号是一个非常艺术的d符号，∂.<br/>假设我们有一个多变量函数:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/07d2ea2369df43841755277bd1f2e12c.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/0*D5a8MyFTopEWIUkD"/></div></figure><p id="d910" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">计算关于x的偏导数看起来像这样:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jl"><img src="../Images/ee1da6f8110281c4fa2e651cc7388410.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/0*X9ZOQjXO1Vqzi6V3"/></div></figure><p id="46f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们计算上面的表达式时，我们把y和z看作常数。<br/>我们知道，如果我们在一元微积分中有普通导数df/dx，它说的是:对于x的微小变化，函数的输出会改变多少。但是我们如何解释，如果我们有一个多元函数f(x，y) = x，y？dx仍然表示输入变量x的微小变化，df当然仍然表示函数的最终变化。但是x不是我们唯一可以移动的方向，我们也在多维空间中，我们也可以在y方向移动。现在，我们可以计算df/dy，它告诉我们，对于y方向上的小位移，输出如何变化。我们可以得出结论，这两个导数都没有告诉我们函数是如何变化的，这就是我们称它们为偏导数的原因。这是一个二元函数在3d空间中的样子:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jm"><img src="../Images/a7c37de5374aa3aac0aff3967ac17988.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*PiPANl_ulx89KDv267tthg.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">f(x，y)=x y</figcaption></figure><p id="4439" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">单变量导数更正式的定义是这样的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jr"><img src="../Images/4cde60537076a3149bf6aacd44f77989.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/1*AUaCpnN2UO_kYFAWVGHSEQ.gif"/></div></figure><p id="e2dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们选择某个函数的两个值f(x₀和f(x₁，并想计算它们之间割线的斜率，我们可以使用游程公式。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es js"><img src="../Images/750e41b0748e0b1d20531fd6813f78fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/1*1LbMQTPGrbiNAtnJbOrJ4Q.gif"/></div></figure><p id="c50d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们知道，如果f(x₀和f(x₁之间的距离变得很小，我们会得到f(x₀).值的切线斜率如果我们改写公式，用δx表示一些小的推动，我们得到:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jt"><img src="../Images/f47f271e400d75e875b1621a32e017ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:376/1*SaJYMelW20jL8_3PPWxsOw.gif"/></div></figure><p id="5921" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是我们不能把δx设为0，因为我们会得到0/0，这是一种未定义的形式，这也是我们需要使用极限的原因，因为δx接近于0，但绝不是0。所以当我们计算函数在某点的导数时，我们得到了该点切线的斜率。对于对x的偏导数，形式上的定义非常相似:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ju"><img src="../Images/0cde24020113a50a7134b90568a34d5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/1*lhgAGTssElwVgBvnZt_aCg.gif"/></div></figure><p id="7bdf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重要的是要注意，标记为微小输入的δx被添加到不同的输入变量中，这取决于我们所取的偏导数。但是当我们计算一个多元函数的偏导数时，图形解释是什么呢？如果我们假设有两个变量函数f(x，y)=…，并计算x在某个特定点的偏导数，我们可以通过在图形上画一个平面作为常数y值，并测量最终切割的斜率来实现这一点。为了获得更好的视觉效果，我推荐你看这个<a class="ae jv" href="https://youtu.be/dfvnCHqzK54" rel="noopener ugc nofollow" target="_blank">视频</a>。</p><p id="326e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">偏导数的另一个方面是用于反向传播的<strong class="ih hj">多变量链式法则</strong>。我将向您介绍开始理解它的基础知识。单变量链法则规定，复合函数f(g(x))的导数是f’(g(x))* g’(x)。现在让我们看看这个例子:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jw"><img src="../Images/5d1d06ce47d0df90a3fbb37dcd642ff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/1*s0JPi5WeBQ8OgNCRqSiy-A.gif"/></div></figure><p id="7c4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">推导这个函数看起来像这样:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jx"><img src="../Images/e82a4fe5264fa0c4ef6ce7973fe96551.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/1*HYYfuYNtVNjE1ICksyCdGw.gif"/></div></figure><p id="1414" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们实际上是这样做的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jy"><img src="../Images/0e6f077fa821829b7dc877d6c1d27470.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/1*hxJ9ikPScbjNIvM07mJuQw.gif"/></div></figure><p id="df33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在看多变量链式法则应该比较容易，下面的等式*里就用到了。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jz"><img src="../Images/c2aea835fafc4e35bede9178da357812.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/1*VESr-2qN-uU06emEso1RoA.gif"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">多变量链式法则</figcaption></figure><p id="6fc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在神经网络中，当我们试图计算一个函数对某个变量的偏导数，但该变量依赖于它之前的变量时，就会出现这种情况。在反向传播的情况下，函数c的成本相对于连接kᵗʰ神经元和jᵗʰ神经元的Lᵗʰ层中的权重的偏导数看起来像这样(“a”表示lᵗʰ层中的mᵗʰ神经元):</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ka"><img src="../Images/2a40779da60f2df59093d7ddc6e9780a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*r0ty5V4WMpWwClZkDPZrEg.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">*成本函数相对于权重的偏导数</figcaption></figure><p id="49d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在为了更好地解释和形象化上面的等式，我们将定义什么是<strong class="ih hj">梯度下降</strong>。函数的梯度是对函数的所有偏导数进行分组的一种方式。假设我们有这个函数:f(x，y)=x sin(y)。对x的偏导数是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kb"><img src="../Images/55ab2c14fa3027f7f00aaadd900ee41a.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/1*328aFUEMua-aaJuZzleWTA.gif"/></div></figure><p id="e1d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">关于y的偏导数是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kc"><img src="../Images/703e5878d2b0a04ab43067f5fba31a55.png" data-original-src="https://miro.medium.com/v2/resize:fit:220/1*csMB_4M6RDQuCeKOeAXsew.gif"/></div></figure><p id="c246" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们用符号nabla表示函数f的梯度:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kd"><img src="../Images/19614c2b6ebf1d53dd6c4788b23367c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/1*OP9pSYBLSR-NBh_w3THKlQ.gif"/></div></figure><p id="d1d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以称这个函数为向量值函数，它以二维点为输入，输出一个二维向量。如果我们用四维输入来做这件事，我们会得到四维输出。看梯度的一个有用的方法是作为一个算子，它的形式是一个向量，充满了对不同变量的偏导数(在机器学习中，这个数字可以是几百万)，我们可以用一个函数乘以它，我们可以看到如何得到我上面写的格式。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ke"><img src="../Images/91bf00a13a67f2240ef0f6f746760f33.png" data-original-src="https://miro.medium.com/v2/resize:fit:188/1*6VsPeD4crrkcKiC4R6R-UQ.gif"/></div></figure><p id="df9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">函数的梯度是最陡上升的路径，在几何解释中，是我们需要走的路径，以便用最少的步骤达到局部最大值。既然我们知道梯度是什么，定义梯度下降就更容易了，这是最陡下降的路径，我们通过采取与梯度的负值成比例的步骤来得到它，换句话说，找到局部最小值的方法是最小化损失函数的方法。我建议你查看<a class="ae jv" href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives" rel="noopener ugc nofollow" target="_blank">汗学院</a>网站，了解更多关于偏导数和多变量微积分的信息，以及3Blue1Brown Youtube频道。关于反向传播和详细解释的更多信息，请查阅这本<a class="ae jv" href="http://neuralnetworksanddeeplearning.com/chap2.html" rel="noopener ugc nofollow" target="_blank">书</a>。</p><p id="8683" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的旅程中，我们必须包括部分<strong class="ih hj">线性代数</strong>，神经网络中的所有数据点都是张量的形式，这使得在编码环境中编写和计算单元计算非常高效。数据集、图像都是矩阵形式，每一行都有相同的长度，即相同的列数，因此我们可以说数据是矢量化的，每一行代表特定的数据输入。行可以一次或成批地提供给模型。让我们从标量开始。一个<strong class="ih hj">标量</strong>只是一个单一的数字，当我们定义它时，它将被记为n∈R，其中n是直线的斜率。<br/>一个<strong class="ih hj">向量</strong>是一个数字数组，我们可以把向量中的每个数字看作一个定义不同轴上坐标的数字。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kf"><img src="../Images/13f9babbdad03f2779b87f706d201b0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:144/1*HmzyXB6vdFLH-gD8BIZ57w.gif"/></div></figure><p id="d9ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">x的第一个元素是x₁，第二个是x₂…如果我们还想说向量x中存储的是什么样的数，即如果每个元素都是实数，向量有n个元素，那么向量就位于取ℝⁿ.ℝn次的笛卡尔积(两个集合a和b的笛卡尔积记为A×B，定义为:A×B = { (a，b) | aϵA和bϵB })形成的集合中<br/>矩阵是数字的二维数组，所以每个元素由两个索引来标识，而不是只有一个，但是在神经网络中，我们需要X-D数组，它们被称为<strong class="ih hj">张量</strong>。假设我们有一个名为a的张量，我们想通过写Aᵢⱼₖ.来确定a在坐标(I，j，k)上的元素<br/>矩阵的一个重要操作是转置，它是矩阵在一条对角线上的镜像，这条对角线称为主对角线。它从左上角向下向右延伸。我们将矩阵a的转置表示为A^T，定义如下:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kg"><img src="../Images/8a50a1139a48990bfcfcc4eb8c3baa84.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/1*ldESUvv7WNMCtI7oFO304A.gif"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kh"><img src="../Images/56458536bebacbcd1896baa1d16b49f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/1*3YHG2l26JsXr7rD6UgKS5g.gif"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">例子</figcaption></figure><p id="ceb2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以把矩阵加在一起，只要它们有相同的形状，只需把它们对应的元素相加C = A+B其中Cᵢⱼ=Aᵢⱼ+Bᵢⱼ.我们可以在矩阵中添加一个标量，或者只通过对矩阵的每个元素执行该操作来乘以一个标量。在处理神经网络时更流行的符号中，我们允许添加一个矩阵和一个向量，这将给出另一个矩阵:C = A + b，其中Cᵢⱼ=Aᵢⱼ+Bⱼ.换句话说，向量b被添加到矩阵的每一行。这样，我们就不需要定义一个带有“b”的矩阵，在做加法之前将它复制到每一行中。神经网络中最常见的运算是两个矩阵相乘。两个矩阵A和B的矩阵乘积就是第三个矩阵c，规则是矩阵A的列数必须和矩阵B的行数相同。如果矩阵A的形状是k×n，B是n×g，那么矩阵C的形状是k×g，乘积运算定义为:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es js"><img src="../Images/9ba95aa39cc2760655a979d7ee7f2a7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/1*kf7ZAWKB8gwkoCt2LwRkoA.gif"/></div></figure><p id="4375" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你想知道更多，请参考数学文献或这本<a class="ae jv" href="http://www.deeplearningbook.org/contents/linear_algebra.html" rel="noopener ugc nofollow" target="_blank">书</a>。</p><p id="3a6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将讨论机器学习中常见的几个概率概念。<br/> <strong class="ih hj">均方误差</strong>是一种统计方法，测量估计值和被估计值之间的均方差，在机器学习中，我们用它作为损失函数，进而从标注数据中找出我们的猜测有多大误差。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ki"><img src="../Images/3ae63d7234e6ab5adb6ad918805b0984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/1*D-7K2fUG8Ev3iA_81svXjw.gif"/></div></figure><p id="2688" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们将我们的yᵢ想象为分散在二维图上的点的结果(由我们的神经网络预测的数据),将yᵢ'想象为位于最佳分散数据之后的线上的标记数据，换句话说，最小化误差，我们的目标是找到线y=Mx+B。在每一批之后，我们计算均方误差，并试图在下一批中将其最小化，我们的目标当然是使其尽可能接近零，这意味着我们预测的点尽可能接近y = Mx+B线。</p><p id="6c35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一个流行的评估损失的方法是<strong class="ih hj">交叉熵</strong>，它是对给定随机变量或事件集的两个概率分布之间的差异的度量。它输出一个介于0和1之间的概率值，并且随着预测的概率偏离标记的数据而增加。我们可以用这个公式计算损失，其中p(x)是期望概率，q(x)是实际概率。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kj"><img src="../Images/4956b7748b1921fa0f8d8f54d9b66b47.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*pWTSaQHB5Gfxa3SNtMVGww.png"/></div></figure><p id="9a16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果具有softmax激活函数的最后一层输出给定类别的这些概率:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kk"><img src="../Images/8416cc86b757f604b69608c381ef9462.png" data-original-src="https://miro.medium.com/v2/resize:fit:150/1*4XyYZ7HV7lWfhsY_kGg96Q.gif"/></div></figure><p id="bcf0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们针对该特定训练示例的一键编码数据是:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kl"><img src="../Images/2029681bdf20b278a117a7ed8d27e3ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:152/1*6M31YtOHXZeSqOBiIlFibA.gif"/></div></figure><p id="455d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我们可以看到预测的分布是否与标记值不同，并且我们想要计算这种差异，我们可以通过使用交叉熵来这样做，然后微调神经网络的权重和偏差，这样我们就可以最小化这种差异。在上面交叉熵公式中的求和运算符之前，我们必须加上负号，因为对数函数对小于1的输入输出负值，这种情况始终存在，因为我们要处理softmax函数输出的概率。</p><p id="605b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们来列举一些激活函数及其图形。我们不使用线性函数作为激活函数的主要原因是我们不能使用反向传播来训练模型。线性函数的导数是常数，与输入无关。这就是我们使用非线性函数的原因。我将列出其中的几个，但我不会深入，也不会通过利弊；为此，我建议您参考这篇文章。<br/>乙状结肠功能。一个Sigmoid函数的范围是&lt; 0，1 &gt;。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="kn ko di kp bf kq"><div class="er es km"><img src="../Images/40bf146952f907b0c3512f7d05a6d3c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zaaRuOw3qTUWFSp0N3r_Bw.png"/></div></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">f(x)=1/(1+e^(-x))</figcaption></figure><p id="2945" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">双曲正切函数。TanH的范围是[-1，1]。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kr"><img src="../Images/ab776d625008e97035a2927122d13d44.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HTWfKAcKXJJv7A7AF9smzA.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">f(x)=tanh(x)</figcaption></figure><p id="ad14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ReLU函数代表校正的线性单位，公式非常简单max(0，x)，它类似于Sigmoid函数，但避免了消失梯度问题。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ks"><img src="../Images/58d9cf8885e025e1e1175b1c023d0e2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*UquDyvK41LPQsiUrqDSflw.png"/></div><figcaption class="jn jo et er es jp jq bd b be z dx translated">f(x)=max(0，x)</figcaption></figure><p id="c11e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们浏览了机器学习中一些最常用的数学概念，简要解释了它们，并链接了一些文献以获取特定主题的更多信息。还有很多东西我没有提到，但我希望这可以成为你旅程的一个好的起点，或者只是一个好的记忆刷新。感谢您的阅读。</p></div></div>    
</body>
</html>