<html>
<head>
<title>Deep Q Stock Trading: Object Oriented R Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深Q股票交易:面向对象的R代码</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/object-oriented-r-code-for-deep-q-stock-trading-90a8f73b2aac?source=collection_archive---------8-----------------------#2020-04-03">https://medium.com/analytics-vidhya/object-oriented-r-code-for-deep-q-stock-trading-90a8f73b2aac?source=collection_archive---------8-----------------------#2020-04-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2a322919185f9b25e55383c3ee7bd842.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6LJdLrG180pN_OKL0GvUgw.png"/></div></div></figure></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="e8b4" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">对所做事情的回忆</h1><p id="3aad" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">在过去的系列文章中，我们运行了一个实验来测试强化学习框架可以成功地学习交易模拟和真实股票数据的假设。如果你没有检查它。<a class="ae kt" rel="noopener" href="/@alexeybnk/can-reinforcement-learning-trade-stock-implementation-in-r-8cd54d13165c"> <strong class="jx hj">第一部分。</strong> </a></p><p id="ea61" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">为了更快地收敛，我们修改了几个东西:在神经网络中增加递归，形成奖励，以及生成演示以获得奖励。实验表明，基于潜力的奖励塑造是最有效的。<a class="ae kt" rel="noopener" href="/@alexeybnk/improving-q-learning-agent-trading-stock-by-adding-recurrency-and-reward-shaping-b9e0ee095c8b"> <strong class="jx hj">第二部分。</strong> </a></p><p id="196b" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">那时我发布了神经网络的代码来帮助你开始你的项目。这一次，我贴出了整个实验的代码，这些代码是我用自己最喜欢的R语言编写的，并通过R6类进行了丰富，使其更容易理解。即使您的日常编码关注使用Python、Java或C，您也可能会发现R6的OOP非常方便。我希望你们会喜欢它的高层次。查看我的<a class="ae kt" href="https://github.com/alexmosc/deep_q_trading" rel="noopener ugc nofollow" target="_blank"> <strong class="jx hj">代码库</strong> </a>，克隆运行！</p></div><div class="ab cl iq ir gp is" role="separator"><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv iw"/><span class="it bw bk iu iv"/></div><div class="hb hc hd he hf"><h1 id="846f" class="ix iy hi bd iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">实验逻辑</h1><p id="89e3" class="pw-post-body-paragraph jv jw hi jx b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">一旦你按照<a class="ae kt" href="https://github.com/alexmosc/deep_q_trading/blob/master/README.md" rel="noopener ugc nofollow" target="_blank">的建议</a>安装了软件包，就可以随意从命令行、R控制台或RStudio IDE运行<code class="du kz la lb lc b">main.R</code>。如果你想完全控制选项，RStudio是你的首选，它是一个方便舒适的编辑器。</p><pre class="ld le lf lg fd lh lc li lj aw lk bi"><span id="58ad" class="ll iy hi lc b fi lm ln l lo lp">setwd('C:/R_study/reinforcement/rl_classes') # set your working directory  </span><span id="2f51" class="ll iy hi lc b fi lq ln l lo lp">## Classes <br/>source('NN_class.R') <br/>source('DATA_class.R') <br/>source('RB_class.R') <br/>source('TRAIN_class.R')</span></pre><p id="549e" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">在R中，你必须设置一个工作目录来指示程序你的文件和目录的位置，你肯定应该首先这样做。因此，其他脚本的导入将会顺利进行。</p><p id="7320" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">按照您在脚本中观察到的顺序执行接下来的步骤:生成数据对象，然后重放缓冲区对象，神经网络对象，最后是训练对象。这些对象依赖于在前面步骤中创建的属性和方法。即，重放缓冲器对象从数据对象中取出一段数据字段。</p><pre class="ld le lf lg fd lh lc li lj aw lk bi"><span id="0e2a" class="ll iy hi lc b fi lm ln l lo lp">Dat &lt;- Data$new()</span></pre><p id="531f" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">我们创建一个类<code class="du kz la lb lc b">Data</code>的对象，并在这个对象上调用一个方法来获取时间序列。你有几个选择:合成噪音、合成信号和雅虎的真实股票数据。默认情况下，合成信号将是你的时间序列作为一个玩具问题。</p><pre class="ld le lf lg fd lh lc li lj aw lk bi"><span id="050c" class="ll iy hi lc b fi lm ln l lo lp">Dat$synthetic_signal( <br/>stepsize = 0.1 <br/>, noise_sd = 0.0 <br/>, noise_sd2 = 0.0 <br/>, n = 20000 )</span></pre><p id="0b76" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">您可以创建一个非常简单的(正弦)信号。通过增加<code class="du kz la lb lc b">_sd</code>标准偏差值选择更复杂的信号。</p><pre class="ld le lf lg fd lh lc li lj aw lk bi"><span id="aa60" class="ll iy hi lc b fi lm ln l lo lp">Dat$make_features(max_lag_power = 6)</span></pre><p id="fc06" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">为了将任务环境状态置于可测量的空间中，创建以不同顺序的时间序列差的形式实现的输入特征。</p><pre class="ld le lf lg fd lh lc li lj aw lk bi"><span id="1c35" class="ll iy hi lc b fi lm ln l lo lp">Nn &lt;- NN$new(lstm_seq_length = 8L) </span><span id="0678" class="ll iy hi lc b fi lq ln l lo lp">Nn$compile_nn( <br/>loss = 'mse' <br/>, metrics = 'mse' <br/>, optimizer = 'adam'<br/>) </span><span id="9b12" class="ll iy hi lc b fi lq ln l lo lp">Nn2 &lt;- Nn$clone()</span></pre><p id="4b0c" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">除了在为LSTM层创建输入时将使用多少时间序列时间步长之外，神经网络结构不可从类对象外部修改。</p><p id="b241" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">由于学习q值的<strong class="jx hj">双</strong>逻辑，我们需要2个随机触发的神经网络(你可以在<code class="du kz la lb lc b">Train</code>方法中查看这段代码)。</p><pre class="ld le lf lg fd lh lc li lj aw lk bi"><span id="73e8" class="ll iy hi lc b fi lm ln l lo lp">Rb &lt;- RB$new( <br/>buffer_size = 512 <br/>, priority_alpha = 0.1 <br/>) </span><span id="8c65" class="ll iy hi lc b fi lq ln l lo lp">Rb$init_rb()</span></pre><p id="f195" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">Replay buffer是一个<code class="du kz la lb lc b">data.table</code>对象，它存储过去的轨迹，也是调整我们的NN模型的数据源。进行优先化采样需要额外的参数，该参数定义了采样期间RB行上的概率分布的锐度。</p><pre class="ld le lf lg fd lh lc li lj aw lk bi"><span id="0fd6" class="ll iy hi lc b fi lm ln l lo lp">Log &lt;- Logs$new()</span></pre><p id="f00c" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">关于神经网络和代理行为的所有相关信息都在一个日志类对象中。您将看到它是如何被利用的，并且您可能想要以自己的风格来绘制它。</p><pre class="ld le lf lg fd lh lc li lj aw lk bi"><span id="4c16" class="ll iy hi lc b fi lm ln l lo lp">Tr &lt;- Train$new() </span><span id="43ed" class="ll iy hi lc b fi lq ln l lo lp">Tr$run( <br/>test_mode = F <br/>, batch_size = 64 <br/>, discount_factor = 0.99 <br/>, learn_rate = 0.001 <br/>, max_iter = 5000 <br/>, min_trans_cost = 0 <br/>, print_returns_every = 100 <br/>, magic_const = 1<br/>)</span></pre><p id="cd5f" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">开始了。训练开始了。我建议你阅读一下Q学习参数，这样你就能熟练掌握这一部分。一个有趣的参数是<code class="du kz la lb lc b">print_returns_every</code>,它控制着在这个过程中多长时间弹出一次中级培训报告。该报告可能如下所示:</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/ea573d66a340875bd24136fcc65f9bfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*74LteJnBmDQliR4iNYOX4Q.png"/></div></div></figure><p id="2358" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">你可以把长时间的训练放在一边，但要不时地查看屏幕，了解这一过程进行得有多顺利。</p><p id="77a1" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">当循环停止时，您还将获得代理性能演变的摘要。如果你愿意，可以进行多次训练，每次神经网络都会从上次训练结束时的状态开始进化。当对结果满意时，在一个<code class="du kz la lb lc b">Nn</code>对象上调用<code class="du kz la lb lc b">$save()</code>方法来存储模型。</p><p id="6201" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">塑造奖励和演示文稿还没有写好。</p><p id="39b4" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">在这一点上，我将完成这篇文章，希望这些脚本将顺利地进入你的技术堆栈，你将不仅有一个编码时间，而是一个教育窗口。</p><p id="dbba" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">花些时间阅读类中的方法来修改一个神经网络结构，输入你自己的数据，或者改变实验逻辑。</p><p id="ccd5" class="pw-post-body-paragraph jv jw hi jx b jy ku ka kb kc kv ke kf kg kw ki kj kk kx km kn ko ky kq kr ks hb bi translated">祝强化学习好运！</p></div></div>    
</body>
</html>