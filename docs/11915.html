<html>
<head>
<title>Confusion Matrix, Precision , Recall and F1-Score</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">混淆矩阵、精确度、回忆和F1分数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/confusion-matrix-precision-recall-and-f1-score-d5f340e38cca?source=collection_archive---------9-----------------------#2020-12-23">https://medium.com/analytics-vidhya/confusion-matrix-precision-recall-and-f1-score-d5f340e38cca?source=collection_archive---------9-----------------------#2020-12-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/ac2ad672c48513ec1c058e9475877494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fBud_PNPhzHDw3kP.jpeg"/></div></div></figure><blockquote class="iq"><p id="3230" class="ir is hi bd it iu iv iw ix iy iz ja dx translated">为什么我们有并使用不同的机器学习指标？？</p></blockquote><p id="0e83" class="pw-post-body-paragraph jb jc hi jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx ja hb bi translated">我们中的一些人，包括我自己，曾经问过这个问题“为什么有各种各样的ML模型？”，为什么我们不使用一个并坚持使用那个度量标准呢？？</p><p id="95ed" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">你可以这样想，就像我们不能根据一条鱼的飞翔能力来判断它，不能根据一只鸟的游泳能力来判断它，不能根据一只蜗牛的奔跑能力来判断它，不能根据一匹马的爬树能力来判断它一样，每一种动物都有它们的长处，它们在这些地方表现得特别好，所以它们也有它们的弱点。这适用于<strong class="jd hj">机器学习指标</strong>，然后留给我们作为数据科学家或机器学习工程师来决定哪个指标最适合我们正在工作的领域。</p><p id="568e" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">例如，我们正在训练一个欺诈检测模型，在与利益相关者举行了几次会议后，我们得出结论，每个客户对公司来说都是宝贵的，公司希望在我们的模型不会造成问题的情况下满足所有客户的需求，然后我们作为一名数据科学家专注于将误报降至最低，因为如果我们的模型预测某项交易是欺诈交易并阻止了用户，用户就会变得愤怒，并转向另一家服务提供商、银行或我们公司可能属于的任何领域。但是等等！！！…什么是误报？，别担心，你会在这个帖子里明白的。</p><p id="21dc" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">混淆矩阵、精确分数、召回分数和F1分数都是分类度量。我记得我第一次听说混淆矩阵的时候，名字中的混淆这个词让我想到了“我需要一段时间才能弄明白”。如果你和我一样，在你的头脑中，只是把其中的困惑清空，因为我们会揭开它的神秘面纱…哈哈😃</p><h2 id="8633" class="kd ke hi bd kf kg kh ki kj kk kl km kn jm ko kp kq jq kr ks kt ju ku kv kw kx bi translated">混淆矩阵</h2><h2 id="261f" class="kd ke hi bd kf kg kh ki kj kk kl km kn jm ko kp kq jq kr ks kt ju ku kv kw kx bi translated">因此，作为开始，我们将解释我所谓的…<strong class="ak">混乱矩阵的4根柱子</strong> …</h2><p id="250a" class="pw-post-body-paragraph jb jc hi jd b je ky jg jh ji kz jk jl jm la jo jp jq lb js jt ju lc jw jx ja hb bi translated">让我们回到我们的二元分类问题，我们可以预测欺诈或非欺诈、垃圾邮件或火腿、流失或停留、0或1以及许多其他可能性，我们将以此作为四个支柱的基础，它们是:</p><ol class=""><li id="3173" class="ld le hi jd b je jy ji jz jm lf jq lg ju lh ja li lj lk ll bi translated">正确肯定</li><li id="f0b4" class="ld le hi jd b je lm ji ln jm lo jq lp ju lq ja li lj lk ll bi translated">假阳性</li><li id="7f94" class="ld le hi jd b je lm ji ln jm lo jq lp ju lq ja li lj lk ll bi translated">正确否定</li><li id="6f8e" class="ld le hi jd b je lm ji ln jm lo jq lp ju lq ja li lj lk ll bi translated">假阴性</li></ol><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="ab fe cl lv"><img src="../Images/30a3dad6b4396eab131e12c0e6d94936.png" data-original-src="https://miro.medium.com/v2/format:webp/1*FMJLMBO-VW0GuVTSOmpvGQ.png"/></div></figure><p id="bfb0" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">知道积极的是1，消极的是0，所以让我们深入到混淆矩阵的4个组成部分</p><p id="928b" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated"><strong class="jd hj">亲提示</strong>:</p><pre class="lr ls lt lu fd lw lx ly lz aw ma bi"><span id="1a75" class="kd ke hi lx b fi mb mc l md me">A good trick I've employed to be able to understand immediately what this four pillars stand for and not <br/>get confused by how they sound is to know that, the first part i.e the part with the True and False is the <br/>part that tells us the "validity of the second part" while the second part i.e the part with the Positive <br/>and Negative tells us "what the model predicts".<br/><br/>So if we hear about False Positive we know that "the model predicts positive" i.e 1 but the validity of <br/>that is False, meaning what the model predicts is wrong. Also a True Negative means that our model predicts a Negative i.e 0 and the validity of that is True, meaning what our model predicts is correct.</span></pre><p id="fb23" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">真正的肯定:当我们的预测是肯定的，即1，并且这是真的，那么我们就说这个预测是真正的肯定</p><p id="fbaa" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated"><strong class="jd hj">假阳性</strong>当我们的预测为正(即1)且为假时，我们称该预测为假阳性</p><p id="e424" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">当我们的预测是负的，即0，并且这是真的时，我们说预测是真的负的</p><p id="08d1" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated"><strong class="jd hj">假阴性</strong>当我们的预测是负的，即0，并且是假的时，我们说预测是假阴性</p><p id="40a5" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">接下来，我们将讨论<strong class="jd hj">费率</strong>，包括:</p><ul class=""><li id="5ec7" class="ld le hi jd b je jy ji jz jm lf jq lg ju lh ja mf lj lk ll bi translated">真实阳性率</li><li id="7aea" class="ld le hi jd b je lm ji ln jm lo jq lp ju lq ja mf lj lk ll bi translated">假阳性率</li><li id="c386" class="ld le hi jd b je lm ji ln jm lo jq lp ju lq ja mf lj lk ll bi translated">真实负利率</li><li id="9e8e" class="ld le hi jd b je lm ji ln jm lo jq lp ju lq ja mf lj lk ll bi translated">假阴性率</li></ul><p id="7639" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">但在此之前，让我们真正领会一下<strong class="jd hj">积极</strong> &amp; <strong class="jd hj">消极</strong>的含义:</p><p id="a82c" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">阳性=真阳性和假阴性</p><p id="6cad" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">阴性=真阴性和假阳性</p><p id="b1b4" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">您会发现这非常直观，因为假阴性的预测意味着数据点是正数据点，而假阳性的预测意味着数据点是负数据点</p><p id="f91e" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated"><strong class="jd hj">真阳性率</strong>:</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="ab fe cl lv"><img src="../Images/5a6532883f2db63a3164da5d60a46b57.png" data-original-src="https://miro.medium.com/v2/format:webp/1*1kgzhZaxzUdsurr5OCNYkg.png"/></div></figure><p id="a25a" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated"><strong class="jd hj">假阳性率</strong>:</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="ab fe cl lv"><img src="../Images/b9c9043fe588a95aa0c55f127d9f7c70.png" data-original-src="https://miro.medium.com/v2/format:webp/1*CHre6QiatQfn4btxp1i6Ew.png"/></div></figure><p id="2c4f" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated"><strong class="jd hj">真负率</strong>:</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="ab fe cl lv"><img src="../Images/ad85dfb57772f0e4f33cc125838f67ce.png" data-original-src="https://miro.medium.com/v2/format:webp/1*idX7U20Hpex4-byr1BmkOA.png"/></div></figure><p id="cfa5" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated"><strong class="jd hj">假阴性率</strong>:</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="ab fe cl lv"><img src="../Images/81d2be92735f39ca193dc84973825384.png" data-original-src="https://miro.medium.com/v2/format:webp/1*AXq0HZ8_KtplBh_WOZkvNw.png"/></div></figure><p id="307c" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">因此，一个好的分类器应该具有高TPR、高TNR、低FPR和低FNR。</p><p id="2621" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">要知道混淆矩阵不仅限于二元分类问题，它也可以扩展到多类问题。在一个多类问题中，“主对角线”上的数字是我们想要的高度，而“非对角线”上的数字是我们想要减少到最近的最小值。</p><p id="52c7" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">在下图中，画一条从左上角到右下角的线，这条线是<strong class="jd hj">主对角线</strong>，不在这条线上的每个部分是<strong class="jd hj">非对角线</strong></p><p id="7921" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">下面是多类问题的混淆矩阵</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="ab fe cl lv"><img src="../Images/ad79efe08175ed6d687d6e3a8fb7fac8.png" data-original-src="https://miro.medium.com/v2/format:webp/1*AuZytGWf-MfXPIHPoi9-AQ.png"/></div></figure><h2 id="c1ee" class="kd ke hi bd kf kg kh ki kj kk kl km kn jm ko kp kq jq kr ks kt ju ku kv kw kx bi translated">优点:</h2><ol class=""><li id="2d9a" class="ld le hi jd b je ky ji kz jm mg jq mh ju mi ja li lj lk ll bi translated">很好地处理不平衡。</li></ol><h2 id="6b0f" class="kd ke hi bd kf kg kh ki kj kk kl km kn jm ko kp kq jq kr ks kt ju ku kv kw kx bi translated">缺点:</h2><ol class=""><li id="017f" class="ld le hi jd b je ky ji kz jm mg jq mh ju mi ja li lj lk ll bi translated">不需要预测概率。</li></ol><h2 id="440e" class="kd ke hi bd kf kg kh ki kj kk kl km kn jm ko kp kq jq kr ks kt ju ku kv kw kx bi translated">混乱矩阵从零开始</h2><pre class="lr ls lt lu fd lw lx ly lz aw ma bi"><span id="e610" class="kd ke hi lx b fi mb mc l md me">###############################<br/>#Code Input                   #<br/>###############################</span><span id="43dd" class="kd ke hi lx b fi mj mc l md me">import numpy as np<br/>from sklearn.metrics import confusion_matrix<br/>np.random.seed(0)<br/><br/>targets = np.random.randint(low=0,high=2,size=100)<br/>y_hats = np.random.randint(low=0,high=2,size=100)<br/><br/>print("Sklearn Confusion Matrix:",confusion_matrix(targets,y_hats),sep="\n")<br/><br/>def customConfusionMatrix(targets,preds):<br/>    TP = 0<br/>    FP = 0<br/>    TN = 0<br/>    FN = 0<br/>    for y,y_hat in zip(targets,preds):<br/>        if y==1 and y_hat==1:<br/>            TP += 1<br/>        elif y==0 and y_hat==0:<br/>            TN += 1<br/>        elif y==1 and y_hat==0:<br/>            FN += 1<br/>        elif y==0 and y_hat==1:<br/>            FP += 1<br/>    return np.array([[TN,FP],<br/>                     [FN,TP]])<br/>print("Custom Confusion Matrix:",customConfusionMatrix(targets,y_hats),sep="\n")</span><span id="456d" class="kd ke hi lx b fi mj mc l md me">###############################<br/>#Output                       #<br/>###############################</span><span id="a16a" class="kd ke hi lx b fi mj mc l md me">Sklearn Confusion Matrix:<br/>[[24 20]<br/> [31 25]]<br/>Custom Confusion Matrix:<br/>[[24 20]<br/> [31 25]]</span></pre><h2 id="0258" class="kd ke hi bd kf kg kh ki kj kk kl km kn jm ko kp kq jq kr ks kt ju ku kv kw kx bi translated">精确度和召回率</h2><p id="2ca9" class="pw-post-body-paragraph jb jc hi jd b je ky jg jh ji kz jk jl jm la jo jp jq lb js jt ju lc jw jx ja hb bi translated">精确度和召回率是非常好的信息检索指标。他们都更关心积极阶层，而不关心消极阶层。</p><p id="ff08" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated"><strong class="jd hj">精度</strong>(特异性):</p><p id="8fde" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">精确度直观地意味着<strong class="jd hj">在模型分类或声明为阳性的所有点中，有多少百分比实际上是阳性的？</strong></p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="ab fe cl lv"><img src="../Images/15a499b3f2608019a38219b22065faa7.png" data-original-src="https://miro.medium.com/v2/format:webp/1*XPuNAQzVS6ToFkvte_R9OQ.png"/></div></figure><p id="6313" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated"><strong class="jd hj">回忆</strong>(灵敏度):</p><p id="d5c2" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">另一方面，回忆说<strong class="jd hj">在所有实际上积极的点中，模型能够检测或预测的百分比是多少？</strong>。你可以看到，召回率与我们在混淆矩阵部分讨论的真实阳性率相同，因为TP和FN都是阳性。</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="ab fe cl lv"><img src="../Images/938ee81142f1db12a5e0afc1ec403f4e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*VwLsHKgG9qbwieXbbL-qsg.png"/></div></figure><p id="1710" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">回忆告诉我们<strong class="jd hj">我们的模型对正类有多敏感</strong>，我们看到它也被称为<strong class="jd hj">敏感度</strong></p><p id="7300" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">精度和召回指标可以从scikit-learn导入，使用</p><p id="2cfa" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">精确度和召回率都在0到1之间，越高越好。</p><pre class="lr ls lt lu fd lw lx ly lz aw ma bi"><span id="ff38" class="kd ke hi lx b fi mb mc l md me">###############################<br/>#Code Input                   #<br/>###############################</span><span id="699c" class="kd ke hi lx b fi mj mc l md me">from sklearn.metrics import precision_score , recall_score</span></pre><h2 id="321f" class="kd ke hi bd kf kg kh ki kj kk kl km kn jm ko kp kq jq kr ks kt ju ku kv kw kx bi translated">精确和从头开始回忆</h2><pre class="lr ls lt lu fd lw lx ly lz aw ma bi"><span id="5f11" class="kd ke hi lx b fi mb mc l md me">###############################<br/>#Code Input                   #<br/>###############################</span><span id="184d" class="kd ke hi lx b fi mj mc l md me">import numpy as np<br/>from sklearn.metrics import precision_score , recall_score<br/>np.random.seed(0)<br/><br/>targets = np.random.randint(low=0,high=2,size=100)<br/>y_hats = np.random.randint(low=0,high=2,size=100)<br/><br/>sklearn_precision = precision_score(targets,y_hats)<br/>print("Sklearn Precision = ",sklearn_precision)<br/><br/>sklearn_recall = recall_score(targets,y_hats)<br/>print("Sklearn Recall = ",sklearn_recall)<br/><br/>def customPrecision(targets,preds):<br/>    TP = 0<br/>    FP = 0<br/>    for y,y_hat in zip(targets,preds):<br/>        if y==1 and y_hat==1:<br/>            TP += 1<br/>        elif y == 0 and y_hat==1:<br/>            FP +=1<br/>            <br/>    return TP / (TP + FP)<br/><br/>print("Custom Precision = ",customPrecision(targets,y_hats))<br/><br/>def customRecall(targets,preds):<br/>    TP = 0<br/>    FN = 0<br/>    for y,y_hat in zip(targets,preds):<br/>        if y==1 and y_hat==1:<br/>            TP += 1<br/>        elif y == 1 and y_hat==0:<br/>            FN +=1<br/>            <br/>    return TP / (TP + FN)<br/><br/>print("Custom Recall = ",customRecall(targets,y_hats))</span><span id="8ab0" class="kd ke hi lx b fi mj mc l md me">###############################<br/>#Output                       #<br/>###############################</span><span id="12eb" class="kd ke hi lx b fi mj mc l md me">Sklearn Precision =  0.5555555555555556<br/>Sklearn Recall =  0.44642857142857145<br/>Custom Precision =  0.5555555555555556<br/>Custom Recall =  0.44642857142857145</span></pre><h2 id="9712" class="kd ke hi bd kf kg kh ki kj kk kl km kn jm ko kp kq jq kr ks kt ju ku kv kw kx bi translated">F1分数</h2><p id="8ce5" class="pw-post-body-paragraph jb jc hi jd b je ky jg jh ji kz jk jl jm la jo jp jq lb js jt ju lc jw jx ja hb bi translated">F1分数是一个试图结合精确度和召回率的指标</p><p id="79aa" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">f1分数指标可从scikit-learn导入，使用</p><pre class="lr ls lt lu fd lw lx ly lz aw ma bi"><span id="4d22" class="kd ke hi lx b fi mb mc l md me">###############################<br/>#Code Input                   #<br/>###############################</span><span id="bedf" class="kd ke hi lx b fi mj mc l md me">from sklearn.metrics import f1_score</span></pre><p id="8f26" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">f1分数也在0到1之间，越高越好。</p><p id="0d4d" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">F1分数的公式为</p><figure class="lr ls lt lu fd ij er es paragraph-image"><div class="ab fe cl lv"><img src="../Images/4e38bf456d73ead0d4f9f3c2466bfdeb.png" data-original-src="https://miro.medium.com/v2/format:webp/1*oIBYC7gX7aOMNaoQ4NMvRQ.png"/></div></figure><h2 id="1252" class="kd ke hi bd kf kg kh ki kj kk kl km kn jm ko kp kq jq kr ks kt ju ku kv kw kx bi translated">F1分数从零开始</h2><pre class="lr ls lt lu fd lw lx ly lz aw ma bi"><span id="63b4" class="kd ke hi lx b fi mb mc l md me">###############################<br/>#Code Input                   #<br/>###############################</span><span id="9282" class="kd ke hi lx b fi mj mc l md me">import numpy as np<br/>from sklearn.metrics import f1_score<br/>np.random.seed(0)<br/><br/>targets = np.random.randint(low=0,high=2,size=100)<br/>y_hats = np.random.randint(low=0,high=2,size=100)<br/><br/>sklearn_f1_score = f1_score(targets,y_hats)<br/><br/>def customF1Score(targets,preds):<br/>    def customPrecision(targets,preds):<br/>        TP = 0<br/>        FP = 0<br/>        for y,y_hat in zip(targets,preds):<br/>            if y==1 and y_hat==1:<br/>                TP += 1<br/>            elif y == 0 and y_hat==1:<br/>                FP +=1<br/><br/>        return TP / (TP + FP)<br/><br/>    def customRecall(targets,preds):<br/>        TP = 0<br/>        FN = 0<br/>        for y,y_hat in zip(targets,preds):<br/>            if y==1 and y_hat==1:<br/>                TP += 1<br/>            elif y == 1 and y_hat==0:<br/>                FN +=1<br/><br/>        return TP / (TP + FN)<br/>    precision = customPrecision(targets,preds)<br/>    recall = customRecall(targets,preds)<br/>    <br/>    return 2 * (precision * recall) / (precision + recall)<br/><br/><br/>print("Sklearn F1_Score = ",sklearn_f1_score)<br/>print("Custom F1_Score = ",customF1Score(targets,y_hats))</span><span id="cf6f" class="kd ke hi lx b fi mj mc l md me">###############################<br/>#Output                       #<br/>###############################</span><span id="81ef" class="kd ke hi lx b fi mj mc l md me">Sklearn F1_Score =  0.4950495049504951<br/>Custom F1_Score =  0.4950495049504951</span></pre><p id="62b0" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">感谢您的阅读，我希望我已经让您对一些分类指标有了一些了解。一点点的激励将会被欣赏，你可以通过鼓掌来做到这一点👏。我也乐于接受问题和建议。你可以与朋友和其他人分享，或者在你最喜欢的社交媒体平台上发布，这样有需要的人可能会偶然发现。</p><p id="660b" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">您可以通过以下方式联系我:</p><p id="84ea" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">领英:<a class="ae mk" href="https://www.linkedin.com/in/temiloluwa-awoyele/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/temiloluwa-awoyele/</a></p><p id="54b1" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">推特:【https://twitter.com/temmyzeus100 T2】</p><p id="3fc4" class="pw-post-body-paragraph jb jc hi jd b je jy jg jh ji jz jk jl jm ka jo jp jq kb js jt ju kc jw jx ja hb bi translated">github:【https://github.com/temmyzeus T4】</p></div></div>    
</body>
</html>