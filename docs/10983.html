<html>
<head>
<title/>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1/>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decision-tree-in-python-a667af9943eb?source=collection_archive---------2-----------------------#2020-11-12">https://medium.com/analytics-vidhya/decision-tree-in-python-a667af9943eb?source=collection_archive---------2-----------------------#2020-11-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="323a" class="hg hh hi bd hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie bi translated">PYTHON 中的决策树</h2><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es if"><img src="../Images/e31ea2a66e3f5fd5be15e54f61e1fe39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iWzy084DvmZ8fTHQPuE-Aw.png"/></div></div></figure><p id="4aa6" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">决策树是机器学习领域中最基本的分类和回归算法之一。</p><p id="0aa5" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">但是在进行算法之前，我们先讨论一下任何机器学习模型的生命周期。该图解释了从零开始创建机器学习模型，然后进一步调整超参数以提高其准确性，决定该模型的部署策略，并在部署后设置日志记录和监控框架</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es jm"><img src="../Images/3093180a90e0d2ce2d8929efcbfcf359.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*uExGw5F575qRpIaLMzoT9A.png"/></div></figure><p id="4bd6" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">决策树算法是机器学习中最通用的算法之一，它可以执行分类和回归分析。它非常强大，非常适合复杂的数据集。除此之外，这很容易理解。该算法的工作原理是根据一些规则和条件将整个数据集划分为树状结构，然后根据这些条件进行预测。</p><p id="2f76" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">让我们看一个简单的例子，假设现在是星期五晚上，你不能决定是出去还是呆在家里。让决策树帮你决定吧。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es jn"><img src="../Images/fb175ffbe179f92e4fc18d11d151e407.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*GBz01XJf7FqzTx6tnDmFVg.png"/></div></figure><ul class=""><li id="cf5e" class="jo jp hi it b iu iv iy iz hs jq hw jr ia js jl jt ju jv jw bi translated">它根据给定的条件选择一个根节点，例如，我们的根节点被选择为 time &gt;10 pm。</li><li id="9270" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">然后，根节点根据给定的条件被分割成子节点。上图中右边的子节点满足了条件，所以不再问更多的问题。</li><li id="5bed" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">左边的子节点不满足条件，所以它再次根据新的条件被拆分。</li><li id="1496" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">这个过程一直持续到所有的条件都满足，或者如果你已经预先定义了树的深度，例如，我们的树的深度是 3，当所有的条件都满足时，它就到达了那里。</li></ul><h2 id="56cd" class="hg hh hi bd hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie bi translated">回归决策树</h2><p id="7d46" class="pw-post-body-paragraph ir is hi it b iu kc iw ix iy kd ja jb hs ke jd je hw kf jg jh ia kg jj jk jl hb bi translated">当用决策树执行回归时，我们试图将 X 的给定值分成不同的和不重叠的区域，例如，对于一组可能的值 X1，X2，…，Xp；我们将试着把它们分成几个不同的、互不重叠的区域:R1、R2。。。，RJ。对于落入区域 Rj 的给定观察，预测等于区域 Rj 中每个训练观察(x)的响应(y)值的平均值。R1、R2 等地区。。。，RJ 以减少下列残差平方和的方式被选择:</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es kh"><img src="../Images/cb164ee04674e1b085cea159374129b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*jSAY50B8nXi3xYXSRqduMA.png"/></div></figure><p id="2a66" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">其中，yrj(第二项)是区域“j”中所有响应变量的平均值。</p><h2 id="5e7d" class="hg hh hi bd hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie bi translated">递归二进制分裂(贪婪方法)</h2><p id="9bed" class="pw-post-body-paragraph ir is hi it b iu kc iw ix iy kd ja jb hs ke jd je hw kf jg jh ia kg jj jk jl hb bi translated">如上所述，我们尝试将 X 值划分为 j 个区域，但是尝试将每组 X 值放入 j 个区域在计算时间方面非常昂贵。因此，决策树选择自顶向下的贪婪方法，其中基于给定的条件将节点分成两个区域，即不是每个节点都将被分裂，而是满足条件的节点将被分裂成两个分支。它被称为贪婪，因为它在该时间点的给定步骤进行最佳分割，而不是在接下来的步骤中为更好的树寻找分割步骤。它决定一个阈值(比如说 s)来将观测值分成不同的区域(j ),使得 Xj≥s 和 Xj <s/></p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es ki"><img src="../Images/2275273387a8e46182f3ee3e9c344de9.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*_Uur9xriAJE7dglgJLHYBA.png"/></div></figure><p id="a3a5" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">这里对于上面的方程，j 和 s 被发现使得这个方程具有最小值。基于 s 和 j 的值选择区域 R1、R2，使得上面的等式具有最小值。类似地，基于具有相同逻辑的某些条件，更多的区域从上面创建的区域中分离出来。这一直持续到达到停止标准(预定义)为止。一旦所有区域都被分割，就可以根据该区域的观测值的平均值进行预测。</p><p id="7906" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">上述过程很有可能过度拟合训练数据，因为它非常复杂。</p><h2 id="ef16" class="hg hh hi bd hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie bi translated">树木修剪</h2><p id="1e31" class="pw-post-body-paragraph ir is hi it b iu kc iw ix iy kd ja jb hs ke jd je hw kf jg jh ia kg jj jk jl hb bi translated">树修剪是修剪完整的树(通过上述过程获得)以减少数据的复杂性和变化的方法。正如我们正则化线性回归一样，我们也可以通过添加一个新项来正则化决策树模型。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es kj"><img src="../Images/2b05b9e9952c0d44a09b309bfdaad32f.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*3fHMZ7lK7zrQSzL_uOH85Q.png"/></div></figure><p id="6aa2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">其中，T 是作为完整树 T0 的子集的子树，α是非负调整参数，其随着树长度的增加而惩罚 MSE。通过使用交叉验证，选择这样的α和 T 值，使我们的模型给出最低的测试错误率。这就是决策树回归模型的工作原理。现在让我们看看使用决策树进行分类的工作算法。贪婪算法根据动手机器学习书籍“贪婪算法贪婪地在顶级搜索最优分割，然后在每一级重复该过程。它不检查分离是否会导致几个级别以下的最低可能杂质。贪婪算法通常会产生一个相当好的解决方案，但并不保证它就是最佳解决方案。”</p><h2 id="c273" class="hg hh hi bd hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie bi translated">后期修剪</h2><p id="d98b" class="pw-post-body-paragraph ir is hi it b iu kc iw ix iy kd ja jb hs ke jd je hw kf jg jh ia kg jj jk jl hb bi translated">后剪枝，也称为反向剪枝，是先生成决策树，然后删除不重要分支的过程。交叉验证数据集用于检查修剪的效果，并测试扩展节点是否会带来改进。如果有任何改进，则我们继续扩展该节点，否则，如果准确度降低，则该节点不被扩展，并且应该在叶节点中被转换。</p><h2 id="77d2" class="hg hh hi bd hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie bi translated">预修剪</h2><p id="70fe" class="pw-post-body-paragraph ir is hi it b iu kc iw ix iy kd ja jb hs ke jd je hw kf jg jh ia kg jj jk jl hb bi translated">预修剪，也称为前向修剪，阻止不重要的分支生成。当生成树时，它使用一个条件来决定何时应该过早地终止一些分支的分裂。</p><h1 id="23d5" class="kk hh hi bd hj kl km kn hn ko kp kq hr kr ks kt hv ku kv kw hz kx ky kz id la bi translated">分类树</h1><p id="2ddc" class="pw-post-body-paragraph ir is hi it b iu kc iw ix iy kd ja jb hs ke jd je hw kf jg jh ia kg jj jk jl hb bi translated">回归树用于定量数据。在定性数据或分类数据的情况下，我们使用分类树。在回归树中，我们基于 RSS 标准来分割节点，但在分类中，它是使用分类错误率、基尼杂质和熵来完成的。让我们详细了解一下这些术语。</p><h1 id="b9bd" class="kk hh hi bd hj kl km kn hn ko kp kq hr kr ks kt hv ku kv kw hz kx ky kz id la bi translated"><strong class="ak">熵</strong></h1><p id="e8ab" class="pw-post-body-paragraph ir is hi it b iu kc iw ix iy kd ja jb hs ke jd je hw kf jg jh ia kg jj jk jl hb bi translated">熵是数据随机性的度量。换句话说，它给出了数据集中存在的杂质。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es lb"><img src="../Images/c0098d65d176e1b04b3c0cfe8b5193af.png" data-original-src="https://miro.medium.com/v2/resize:fit:754/format:webp/1*1UvvWBi-SxhZW2Fj9SjwZw.png"/></div></figure><p id="76bf" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">当我们将节点分成两个区域，并在这两个区域中放置不同的观测值时，主要目标是减少熵，即减少区域中的随机性，并将我们的数据比前一个节点中的数据划分得更清楚。如果分裂节点没有导致熵减少，我们尝试基于不同的条件分裂，或者我们停止。当一个区域包含具有相同标签的数据时，该区域是干净的(低熵)，如果存在标签的混合，则该区域是随机的(高熵)。假设有“m”个观察值，我们需要将它们分为第 1 类和第 2 类。假设类别 1 有‘n’个观察值，类别 2 有‘m-n’个观察值。</p><p id="be2d" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">p= n/m，q = m-n/m = 1-p</p><p id="83b9" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">那么，给定集合的熵是:</p><pre class="ig ih ii ij fd lc ld le lf aw lg bi"><span id="66a5" class="hg hh hi ld b fi lh li l lj lk">E = -p*log2(p) – q*log2(q)</span></pre><p id="61b7" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">当所有的观察值都属于类别 1 时，那么 p = 1，所有的观察值都属于类别 2，那么 p =0，在这两种情况下 E =0，因为在类别中没有随机性。如果一半的观测值属于第 1 类，另一半属于第 2 类，那么 p =1/2，q =1/2，熵最大，E =1。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es ll"><img src="../Images/968eed45dcd990cce1ea050d354c157c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*zwJpLjDOzMU0bQwLw3rYKg.png"/></div></figure><h2 id="bbfc" class="hg hh hi bd hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie bi translated">信息增益</h2><p id="a90f" class="pw-post-body-paragraph ir is hi it b iu kc iw ix iy kd ja jb hs ke jd je hw kf jg jh ia kg jj jk jl hb bi translated">信息增益计算分裂节点后熵的减少。这是分裂前后熵的差别。获得的信息越多，去除的熵就越多。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es lm"><img src="../Images/a41c9fb461635d0f39b9f7b446032782.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*pcSqWmmRJ1RTeVofx3w-VA.png"/></div></div></figure><p id="31e1" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">其中，T 是拆分前的父节点，X 是从 T 拆分的节点。</p><p id="3953" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">基于熵和信息增益值分裂的树看起来像:</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es ln"><img src="../Images/63c91cedad20b30cdcf5d196d8f4f5d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*weo5WUVgPvKd5Z7kK16KAg.png"/></div></div></figure><h2 id="9e57" class="hg hh hi bd hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie bi translated">Ginni 杂质</h2><p id="9bc2" class="pw-post-body-paragraph ir is hi it b iu kc iw ix iy kd ja jb hs ke jd je hw kf jg jh ia kg jj jk jl hb bi translated">根据维基百科的说法，“基尼不纯度是一种衡量从集合中随机选择的元素被错误标记的频率，如果它是根据标签在子集中的分布随机标记的话。”它的计算方法是将给定观察值分类到正确类别的概率与该特定观察值分类到错误类别时所有概率的总和相乘。假设有 k 个类，一个观察值属于类“I”</p><p id="98ed" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">Ginni 杂质值介于 0 和 1 之间，0 表示没有杂质，1 表示随机分布。选择 Ginni 杂质最少的节点作为要分割的根节点。</p><p id="fdc1" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">根据 ginni 杂质值分割的树看起来像:</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es ln"><img src="../Images/63c91cedad20b30cdcf5d196d8f4f5d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*weo5WUVgPvKd5Z7kK16KAg.png"/></div></div></figure><h1 id="a7a2" class="kk hh hi bd hj kl km kn hn ko kp kq hr kr ks kt hv ku kv kw hz kx ky kz id la bi translated">决策树的不同算法</h1><ul class=""><li id="adb6" class="jo jp hi it b iu kc iy kd hs lo hw lp ia lq jl jt ju jv jw bi translated">ID3(迭代二分法) :它是用于构建分类决策树的算法之一。它使用信息增益作为寻找根节点和分裂它们的标准。它只接受分类属性。</li><li id="a7f3" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">C4.5:它是 ID3 算法的扩展，比 ID3 更好，因为它处理连续值和离散值。它也用于分类目的。</li><li id="db59" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">分类和回归算法(CART):这是用于构造决策树的最流行的算法。它使用 ginni 杂质作为选择根节点的默认计算，但是也可以使用“熵”作为标准。这种算法既适用于回归问题，也适用于分类问题。我们将在 pyhton 实现中使用这个算法。</li></ul><p id="2eab" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">熵和 Ginni 杂质可以可逆地使用。这对结果影响不大。虽然，ginni 比熵更容易计算，因为熵有一个对数项计算。这就是为什么 CART 算法使用 ginni 作为默认算法的原因。</p><p id="dd11" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">如果我们绘制 ginni 与熵图，我们可以看到它们之间没有太大的差异:</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es lr"><img src="../Images/cb6eb37ce33a4468aefc6c78da8c5eeb.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*0fdqSdobiVxQDV73BcZZhg.png"/></div></figure><p id="e0f1" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">决策树的优势:</p><ul class=""><li id="7f09" class="jo jp hi it b iu iv iy iz hs jq hw jr ia js jl jt ju jv jw bi translated">它可用于回归和分类问题。</li><li id="a9fa" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">决策树非常容易掌握，因为分裂规则已经讲得很清楚了。</li><li id="670c" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">复杂的决策树模型在可视化时非常简单。这可以通过观想来理解。</li><li id="a7bc" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">不需要缩放和标准化。</li></ul><p id="aa39" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">决策树的缺点:</p><ul class=""><li id="d13a" class="jo jp hi it b iu iv iy iz hs jq hw jr ia js jl jt ju jv jw bi translated">由于贪婪的方法，数据的微小变化都会导致模型的不稳定。</li><li id="ba03" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">对于决策树来说，过度拟合的概率非常高。</li><li id="7dc2" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">与其他分类算法相比，训练决策树模型需要更多的时间。</li></ul><h1 id="8df7" class="kk hh hi bd hj kl km kn hn ko kp kq hr kr ks kt hv ku kv kw hz kx ky kz id la bi translated">交叉验证</h1><p id="1d64" class="pw-post-body-paragraph ir is hi it b iu kc iw ix iy kd ja jb hs ke jd je hw kf jg jh ia kg jj jk jl hb bi translated">假设您使用任何特定的算法在给定的数据集上训练模型。您尝试使用相同的训练数据来查找训练模型的准确性，并发现准确性为 95%，甚至可能是 100%。这是什么意思？你的模型准备好预测了吗？答案是否定的，为什么？因为你的模型已经根据给定的数据进行了自我训练，也就是说，它知道这些数据，并且已经很好地概括了这些数据。但是当你试图预测一组新的数据时，它很可能会给你很差的准确性，因为它以前从未见过这些数据，因此它不能很好地概括这些数据。这就是过拟合的问题。为了解决这样的问题，交叉验证应运而生。交叉验证是一种重采样技术，其基本思想是将训练数据集分为两部分，即训练和测试。在第一部分(训练)中，您尝试训练模型，在第二部分(测试)中，即模型看不到的数据，您进行预测并检查您的模型在这方面的工作情况。如果模型对您的测试数据具有良好的准确性，这意味着模型没有过度拟合训练数据，可以信任预测，而如果它的准确性较差，则我们的模型不可信，我们需要调整我们的算法。</p><p id="e374" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">让我们看看交叉验证的不同方法:</p><ul class=""><li id="6d7a" class="jo jp hi it b iu iv iy iz hs jq hw jr ia js jl jt ju jv jw bi translated">坚持方法:</li></ul><p id="3562" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">这是最基本的 CV 技术。它只是将数据集分成两组训练集和测试集。训练数据集用于训练模型，然后将测试数据拟合到训练好的模型中以进行预测。我们检查准确性，并在此基础上评估我们的模型。使用这种方法是因为它的计算成本较低。但是基于拒绝集的评估可能有很大的差异，因为它很大程度上取决于哪些数据点最终出现在训练集中，哪些数据点出现在测试数据中。每次这个划分发生变化，评价都会不一样。</p><ul class=""><li id="6061" class="jo jp hi it b iu iv iy iz hs jq hw jr ia js jl jt ju jv jw bi translated">k 倍交叉验证</li></ul><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es ls"><img src="../Images/ffaa94210038cb686362ce77788b0353.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*JSGdOrkAtyqEtYlNp0Mheg.png"/></div></figure><h1 id="1237" class="kk hh hi bd hj kl km kn hn ko kp kq hr kr ks kt hv ku kv kw hz kx ky kz id la bi translated">用 Python 实现</h1><p id="f304" class="pw-post-body-paragraph ir is hi it b iu kc iw ix iy kd ja jb hs ke jd je hw kf jg jh ia kg jj jk jl hb bi translated">我们将使用 Sklearn 模块来实现决策树算法。Sklearn 使用 CART(分类和回归树)算法，默认情况下，它使用 Gini 杂质作为分割节点的标准。</p><p id="b797" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">导入所需模块</p><blockquote class="lt lu lv"><p id="7766" class="ir is lw it b iu iv iw ix iy iz ja jb lx jc jd je ly jf jg jh lz ji jj jk jl hb bi translated">导入熊猫 as pd <br/>从 sklearn.tree 导入 graphviz <br/>决策树分类器，从 sklearn 导出 _graphviz <br/>导入树<br/>从 sklearn.model_selection 导入 train_test_split，GridSearchCV <br/>从 sklearn.metrics 导入 accuracy_score，confusion_matrix，roc_curve，roc_auc_score <br/>从 sklearn.externals.six 导入 StringIO <br/>从 IPython.display 导入图片<br/></p><p id="51ae" class="ir is lw it b iu iv iw ix iy iz ja jb lx jc jd je ly jf jg jh lz ji jj jk jl hb bi translated">data = PD . read _ CSV(" wine quality _ red . CSV ")<br/>数据</p></blockquote><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es ma"><img src="../Images/e7477c92be00109573f581223362d1d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iE5A93T4NhUeGrl0lJ_bgw.png"/></div></div></figure><pre class="ig ih ii ij fd lc ld le lf aw lg bi"><span id="2de3" class="hg hh hi ld b fi lh li l lj lk">X <strong class="ld mb">=</strong> data.drop(columns <strong class="ld mb">=</strong> 'quality')<br/>y <strong class="ld mb">=</strong> data['quality']</span><span id="fa02" class="hg hh hi ld b fi mc li l lj lk">where X is Independent values and y is dependent values</span><span id="0faa" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">Splitting dataset into Test data and Training data</strong></span><span id="1be3" class="hg hh hi ld b fi mc li l lj lk">x_train,x_test,y_train,y_test <strong class="ld mb">=</strong> train_test_split(X,y,test_size <strong class="ld mb">=</strong> 0.30, random_state<strong class="ld mb">=</strong> 355)</span><span id="e7d5" class="hg hh hi ld b fi mc li l lj lk">In []:</span><span id="cfaa" class="hg hh hi ld b fi mc li l lj lk"><em class="lw">#let's first visualize the tree on the data without doing any pre processing<br/></em>clf <strong class="ld mb">=</strong> DecisionTreeClassifier()<br/>clf.fit(x_train,y_train)</span><span id="85c2" class="hg hh hi ld b fi mc li l lj lk">Out[]:</span><span id="6f7a" class="hg hh hi ld b fi mc li l lj lk"><em class="lw">DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,<br/>                       max_features=None, max_leaf_nodes=None,<br/>                       min_impurity_decrease=0.0, min_impurity_split=None,<br/>                       min_samples_leaf=1, min_samples_split=2,<br/>                       min_weight_fraction_leaf=0.0, presort=False,<br/>                       random_state=None, splitter='best')</em></span><span id="7641" class="hg hh hi ld b fi mc li l lj lk"><em class="lw"># create a dot_file which stores the tree structure</em></span><span id="072f" class="hg hh hi ld b fi mc li l lj lk">dot_data <strong class="ld mb">=</strong> export_graphviz(clf,feature_names <strong class="ld mb">=</strong> feature_name,rounded <strong class="ld mb">=</strong> <strong class="ld mb">True</strong>,filled <strong class="ld mb">=</strong> <strong class="ld mb">True</strong>)</span><span id="5a9a" class="hg hh hi ld b fi mc li l lj lk"><em class="lw"># Draw graph</em></span><span id="fb4e" class="hg hh hi ld b fi mc li l lj lk">graph <strong class="ld mb">=</strong> pydotplus.graph_from_dot_data(dot_data)<br/>graph.write_png("myTree.png")</span><span id="6d89" class="hg hh hi ld b fi mc li l lj lk"><em class="lw"># Show graph<br/></em>Image(graph.create_png())</span></pre><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es md"><img src="../Images/e952261d44fde00f26a8d0822f6955d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wBkl7qW3sAOjwNPdpZpZHA.png"/></div></div></figure><p id="de02" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">让我们来理解上面的树:</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es me"><img src="../Images/20ca914bc9e1f361743767ace0db329c.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*DTUMZSnyKV1mKXTK9UTPnA.png"/></div></figure><ul class=""><li id="17d3" class="jo jp hi it b iu iv iy iz hs jq hw jr ia js jl jt ju jv jw bi translated">第一个值指示选择根节点的列和条件，根节点将被进一步拆分。</li><li id="a8b8" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">第二个值给出了所选节点的基尼系数</li><li id="6269" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">samples 给出了节点中在该时间点的观察次数</li><li id="0c76" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">方括号内的值表示每个类别(输出)中存在的观测值的数量，即在上面给出的图中，第 1 类中有 8 个观测值，第 2 类中有 38 个，第 3 类中有 468 个，依此类推。</li></ul><pre class="ig ih ii ij fd lc ld le lf aw lg bi"><span id="bf6d" class="hg hh hi ld b fi lh li l lj lk"><strong class="ld mb">In[]:</strong><br/>clf.score(x_train,y_train)</span><span id="7848" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">Out[]:</strong><br/>1.0</span><span id="789d" class="hg hh hi ld b fi mc li l lj lk">In []:<br/>py_pred <strong class="ld mb">=</strong> clf.predict(x_test)</span><span id="98fb" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">In[]:</strong><br/><em class="lw"># accuracy of our classification tree<br/></em>clf.score(x_test,y_test)</span><span id="53c7" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">Out[]:</strong><br/>0.5791666666666667</span></pre><p id="f7b9" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">现在，我们还没有对数据进行任何预处理，也没有进行任何超参数调整。</p><p id="f68c" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">让我们做所有这些，看看我们的分数如何提高</p><h1 id="af12" class="kk hh hi bd hj kl km kn hn ko kp kq hr kr ks kt hv ku kv kw hz kx ky kz id la bi translated">什么是超参数？</h1><p id="9925" class="pw-post-body-paragraph ir is hi it b iu kc iw ix iy kd ja jb hs ke jd je hw kf jg jh ia kg jj jk jl hb bi translated">我们可以在下面看到，决策树分类器算法采用所有这些参数，也称为超参数。</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es mf"><img src="../Images/54a3d1e3a0a057ef40b12d619be76322.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*vhN_OWz0z4p_ImRdsrK2EA.png"/></div></figure><p id="c6d0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">让我们看看最重要的参数(根据 sklearn 文档) :</p><h2 id="581f" class="hg hh hi bd hj hk hl hm hn ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie bi translated">因素</h2></div><div class="ab cl mg mh gp mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="hb hc hd he hf"><ul class=""><li id="54d7" class="jo jp hi it b iu iv iy iz hs jq hw jr ia js jl jt ju jv jw bi translated">标准:字符串，可选(默认值="gini ")</li><li id="fbf0" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated"><code class="du mn mo mp ld b">The function to measure the quality of a split. Supported criteria are "gini" for the Gini impurity and "entropy" for the information gain.</code></li><li id="7ff7" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">splitter : string，可选(default="best ")用于在每个节点选择拆分的策略。支持的策略是选择最佳分割的“最佳”和选择最佳随机分割的“随机”。</li><li id="7ab8" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">max_depth : int 或 None，可选(默认值=None)树的最大深度。如果没有，则扩展节点，直到所有叶子都是纯的，或者直到所有叶子包含少于 min_samples_split 样本。</li><li id="83f5" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">min_samples_split : int，float，optional(默认值=2)拆分内部节点所需的最小样本数:</li><li id="e2d0" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">如果是 int，那么就把<code class="du mn mo mp ld b">min_samples_split</code>当做最小数。</li><li id="0633" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">如果是 float，那么<code class="du mn mo mp ld b">min_samples_split</code>是一个分数，<code class="du mn mo mp ld b">ceil(min_samples_split * n_samples)</code>是每次分割的最小样本数。</li><li id="e441" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">..versionchanged:: 0.18 增加了分数的浮点值。</li><li id="04fb" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">min_samples_leaf : int，float，optional(默认值=1)叶节点所需的最小样本数。任何深度的分裂点只有在左和右分支的每一个中留下至少<code class="du mn mo mp ld b">min_samples_leaf</code>个训练样本时才会被考虑。这可能具有平滑模型的效果，尤其是在回归中。</li><li id="8057" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">如果是 int，那么考虑<code class="du mn mo mp ld b">min_samples_leaf</code>为最小数。</li><li id="c0de" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">如果是 float，那么<code class="du mn mo mp ld b">min_samples_leaf</code>是一个分数，而<code class="du mn mo mp ld b">ceil(min_samples_leaf * n_samples)</code>是每个节点的最小样本数。</li><li id="18a8" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">max_features : int、float、string 或 None，可选(默认值=None)寻找最佳分割时要考虑的要素数量:</li></ul><pre class="ig ih ii ij fd lc ld le lf aw lg bi"><span id="de6e" class="hg hh hi ld b fi lh li l lj lk">If int, then consider `max_features` features at each split.<br/>If float, then `max_features` is a fraction and<br/>`int(max_features * n_features)` features are considered at each split.<br/>   - If "auto", then `max_features=sqrt(n_features)`.<br/>   - If "sqrt", then `max_features=sqrt(n_features)`.<br/>   - If "log2", then `max_features=log2(n_features)`.<br/>   - If None, then `max_features=n_features`.</span></pre><ul class=""><li id="d597" class="jo jp hi it b iu iv iy iz hs jq hw jr ia js jl jt ju jv jw bi translated">注意:直到找到节点样本的至少一个有效分区，对分割的搜索才会停止，即使它需要有效地检查不止<code class="du mn mo mp ld b">max_features</code>个特征。</li><li id="0adb" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">random_state : int，RandomState instance 或 None，可选(默认=None)如果 int，random_state 是随机数生成器使用的种子；如果是 RandomState 实例，random_state 是随机数生成器；如果没有，随机数生成器就是<code class="du mn mo mp ld b">np.random</code>使用的 RandomState 实例。</li><li id="6b4c" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">max_leaf_nodes : int 或 None，可选(默认值=None)以最佳优先方式用<code class="du mn mo mp ld b">max_leaf_nodes</code>生成一棵树。最佳节点被定义为杂质的相对减少。如果没有，则无限数量的叶节点。</li><li id="71e5" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">min _ infinity _ decrease:float，可选(默认值=0。)如果该分裂导致杂质减少大于或等于该值，则该节点将被分裂。</li><li id="a11e" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">min _ infinity _ split:float，(默认值=1e-7)树生长中提前停止的阈值。如果一个节点的杂质高于阈值，它就会分裂，否则它就是一片叶子。</li><li id="3cbe" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">class_weight : dict，dict 列表，“平衡”或 None，default=None 与表单<code class="du mn mo mp ld b">{class_label: weight}</code>中的类相关联的权重。如果没有给定，所有类的权重都应该是 1。对于多输出问题，可以按照与 y 的列相同的顺序提供字典列表。</li><li id="ebba" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated">预排序:bool，可选(默认值=False)</li><li id="33a8" class="jo jp hi it b iu jx iy jy hs jz hw ka ia kb jl jt ju jv jw bi translated"><code class="du mn mo mp ld b">Whether to presort the data to speed up the finding of best splits in fitting. For the default settings of a decision tree on large datasets, setting this to true may slow down the training process. When using either a smaller dataset or a restricted depth, this may speed up the training.</code></li></ul><p id="47d0" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">当我们进行超参数调整时，我们基本上是试图找到那些超参数的集合和值，这些集合和值将为我们提供具有最大精确度的模型。让我们继续努力改进我们的模型。</p><p id="86e8" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">我们将从扩展数据开始</p><pre class="ig ih ii ij fd lc ld le lf aw lg bi"><span id="8e2b" class="hg hh hi ld b fi lh li l lj lk"><strong class="ld mb">In[]:</strong><br/>scalar <strong class="ld mb">=</strong> StandardScaler()<br/>x_transform <strong class="ld mb">=</strong> scalar.fit_transform(X)</span><span id="c48a" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">In[]:</strong><br/>x_train,x_test,y_train,y_test <strong class="ld mb">=</strong> train_test_split(x_transform,y,test_size <strong class="ld mb">=</strong> 0.30, random_state<strong class="ld mb">=</strong> 355)</span></pre><p id="76b2" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">虽然我们的数据集相对较小，但让我们使用 PCA 进行特征选择，看看它是否能提高我们的准确性。</p><pre class="ig ih ii ij fd lc ld le lf aw lg bi"><span id="0c3c" class="hg hh hi ld b fi lh li l lj lk">In []:<br/><strong class="ld mb">from</strong> sklearn.decomposition <strong class="ld mb">import</strong> PCA<br/><strong class="ld mb">import</strong> numpy <strong class="ld mb">as</strong> np<br/>pca <strong class="ld mb">=</strong> PCA()<br/>principalComponents <strong class="ld mb">=</strong> pca.fit_transform(x_transform)<br/>plt.figure()<br/>plt.plot(np.cumsum(pca.explained_variance_ratio_))<br/>plt.xlabel('Number of Components')<br/>plt.ylabel('Variance (%)') <em class="lw">#for each component<br/></em>plt.title('Explained Variance')<br/>plt.show()</span></pre><figure class="ig ih ii ij fd ik er es paragraph-image"><div class="er es mq"><img src="../Images/f86130644759d024a3d8c0f84a785b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*rRlHx8QZl2KeU-XPiNiQcg.png"/></div></figure><p id="1bc3" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">我们可以看到，大约 95%的差异是由 8 个部分解释的。因此，在我们的算法中，我们不使用所有 11 列作为输入，而是使用这 8 个主成分。</p><pre class="ig ih ii ij fd lc ld le lf aw lg bi"><span id="4636" class="hg hh hi ld b fi lh li l lj lk"><strong class="ld mb">In [170]:<br/></strong>pca <strong class="ld mb">=</strong> PCA(n_components<strong class="ld mb">=</strong>8)<br/>new_data <strong class="ld mb">=</strong> pca.fit_transform(x_transform)<br/>principal_x <strong class="ld mb">=</strong> pd.DataFrame(new_data,columns<strong class="ld mb">=</strong>['PC-1','PC-2','PC-3','PC-4','PC-5','PC-6','PC-7','PC-8'])</span><span id="428d" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">In []:</strong></span><span id="4b40" class="hg hh hi ld b fi mc li l lj lk">principal_x</span></pre><p id="990f" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">Out[]:</p><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es mr"><img src="../Images/cd09c88ffa96928299fbaa53f546f2ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HWNk-oPeCZQuyPsa-dcNtQ.png"/></div></div></figure><p id="4164" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">1599 行× 8 列</p><pre class="ig ih ii ij fd lc ld le lf aw lg bi"><span id="5343" class="hg hh hi ld b fi lh li l lj lk"><strong class="ld mb">In[]:</strong></span><span id="e033" class="hg hh hi ld b fi mc li l lj lk"><em class="lw"># let's see how well our model perform on this new data</em></span><span id="7c75" class="hg hh hi ld b fi mc li l lj lk">x_train,x_test,y_train,y_test <strong class="ld mb">=</strong> train_test_split(principal_x,y,test_size <strong class="ld mb">=</strong> 0.30, random_state<strong class="ld mb">=</strong> 355)</span><span id="0e66" class="hg hh hi ld b fi mc li l lj lk"><em class="lw">#let's first visualize the tree on the data without doing any pre processing</em></span><span id="2892" class="hg hh hi ld b fi mc li l lj lk">clf <strong class="ld mb">=</strong> DecisionTreeClassifier()<br/>clf.fit(x_train,y_train)<br/>clf.score(x_test,y_test)</span><span id="431e" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">Out[]:</strong></span><span id="c7bd" class="hg hh hi ld b fi mc li l lj lk">0.58125</span></pre><p id="1944" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">我们的测试准确度有一点提高。太好了！！</p><p id="7788" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">现在让我们尝试使用 GridSearchCV 算法来调优一些超参数。</p><p id="0b70" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">GridSearchCV 是一种用于调整超参数的方法。我们可以传递不同的超参数值作为网格搜索的参数。它对传递的不同参数的组合进行详尽的生成。使用交叉验证分数，网格搜索返回模型表现最佳的超参数组合。</p><pre class="ig ih ii ij fd lc ld le lf aw lg bi"><span id="ea3a" class="hg hh hi ld b fi lh li l lj lk"><strong class="ld mb">In []:</strong><em class="lw"><br/># we are tuning three hyperparameters right now, we are passing the different values for both parameters</em></span><span id="2695" class="hg hh hi ld b fi mc li l lj lk">grid_param <strong class="ld mb">=</strong> <br/>{<br/>'criterion': ['gini', 'entropy'],<br/>'max_depth' : range(2,32,1),<br/>'min_samples_leaf' : range(1,10,1),<br/>'min_samples_split': range(2,10,1),<br/>'splitter' : ['best', 'random']<br/>}</span><span id="4bb1" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">In []:</strong><br/>grid_search <strong class="ld mb">=</strong> GridSearchCV(estimator<strong class="ld mb">=</strong>clf,param_grid<strong class="ld mb">=</strong>grid_param,cv<strong class="ld mb">=</strong>5,n_jobs <strong class="ld mb">=-</strong>1)</span><span id="d0b3" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">In []:</strong><br/>grid_search.fit(x_train,y_train)</span><span id="04e3" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">Out[]:</strong></span></pre><figure class="ig ih ii ij fd ik er es paragraph-image"><div role="button" tabindex="0" class="il im di in bf io"><div class="er es ms"><img src="../Images/2b71302842818d3e47eccd28b52348a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*va-wLCYv2jRKyyZUBZQVoA.png"/></div></div></figure><pre class="ig ih ii ij fd lc ld le lf aw lg bi"><span id="f719" class="hg hh hi ld b fi lh li l lj lk"><strong class="ld mb">In [ ]:<br/></strong>best_parameters <strong class="ld mb">=</strong> grid_search.best_params_</span><span id="b894" class="hg hh hi ld b fi mc li l lj lk">print(best_parameters)</span><span id="19ac" class="hg hh hi ld b fi mc li l lj lk">{'criterion': 'entropy', 'max_depth': 24, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'random'}</span><span id="a9d2" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">In []</strong>:</span><span id="a918" class="hg hh hi ld b fi mc li l lj lk">grid_search.best_score_</span><span id="d3c2" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">Out[]:</strong></span><span id="ae2a" class="hg hh hi ld b fi mc li l lj lk">0.5933869526362824</span><span id="e216" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">In []:</strong></span><span id="6b72" class="hg hh hi ld b fi mc li l lj lk">clf <strong class="ld mb">=</strong> DecisionTreeClassifier(criterion <strong class="ld mb">=</strong> 'entropy', max_depth <strong class="ld mb">=</strong>24, min_samples_leaf<strong class="ld mb">=</strong> 1, min_samples_split<strong class="ld mb">=</strong> 2, splitter <strong class="ld mb">=</strong>'random')</span><span id="4485" class="hg hh hi ld b fi mc li l lj lk">clf.fit(x_train,y_train)</span><span id="f360" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">Out[]:</strong></span><span id="f629" class="hg hh hi ld b fi mc li l lj lk">DecisionTreeClassifier(class_weight=None, criterion='entropy',     max_depth=24,max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None,<br/>min_samples_leaf=1, min_samples_split=2,<br/>min_weight_fraction_leaf=0.0, presort=False,<br/>random_state=None, splitter='random')</span><span id="0c13" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">In []:<br/></strong>clf.score(x_test,y_test)</span><span id="4959" class="hg hh hi ld b fi mc li l lj lk"><strong class="ld mb">Out[]:</strong><br/>0.6041666666666666</span></pre><p id="3a72" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">太好了！！使用 Gridsearch 后，我们的测试分数有所提高。</p><p id="d8d6" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">注意:我们必须明白在 gridSearch 中给出所有的超参数并不能保证得到最好的结果。我们必须对参数进行反复试验才能得到完美的分数。</p><p id="3087" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">欢迎您尝试调整更多的参数，并尝试提高更多的准确性。</p><p id="9719" class="pw-post-body-paragraph ir is hi it b iu iv iw ix iy iz ja jb hs jc jd je hw jf jg jh ia ji jj jk jl hb bi translated">让我们想象一下这棵树:</p><pre class="ig ih ii ij fd lc ld le lf aw lg bi"><span id="0879" class="hg hh hi ld b fi lh li l lj lk"><strong class="ld mb">In []:</strong></span><span id="8405" class="hg hh hi ld b fi mc li l lj lk">feature_name<strong class="ld mb">=</strong>list(X.columns)<br/>class_name <strong class="ld mb">=</strong> list(y_train.unique())<br/><em class="lw"># create a dot_file which stores the tree structure<br/></em>dot_data <strong class="ld mb">=</strong> export_graphviz(clf , rounded <strong class="ld mb">=</strong> <strong class="ld mb">True</strong>, filled <strong class="ld mb">=</strong> <strong class="ld mb">True</strong>)<br/><em class="lw"># Draw graph<br/></em>graph <strong class="ld mb">=</strong> pydotplus.graph_from_dot_data(dot_data)<br/><em class="lw">#graph.write_png("tree.png")<br/># Show graph<br/></em>Image(graph.create_png())</span></pre><div class="mt mu ez fb mv mw"><a href="https://drive.google.com/file/d/1sHrF1HIxiG-IAtxk5xjIBlfkAVuBdVPI/view?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab dw"><div class="my ab mz cl cj na"><h2 class="bd mb fi z dy nb ea eb nc ed ef nd bi translated">download.png</h2><div class="ne l"><h3 class="bd b fi z dy nb ea eb nc ed ef dx translated">编辑描述</h3></div><div class="nf l"><p class="bd b fp z dy nb ea eb nc ed ef dx translated">drive.google.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl ip mw"/></div></div></a></div></div></div>    
</body>
</html>