<html>
<head>
<title>Please use Pyspark for Hyper-parameter tuning to binge-watch more</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">请使用Pyspark进行超参数调谐，以便尽情观看更多内容</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/fine-tuning-at-scale-for-free-2a5c40eedaa2?source=collection_archive---------2-----------------------#2020-02-20">https://medium.com/analytics-vidhya/fine-tuning-at-scale-for-free-2a5c40eedaa2?source=collection_archive---------2-----------------------#2020-02-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/1df05960f6cbc2d2a20fd10d2ce30b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HnkV00tsbmS32axZ.png"/></div></div></figure><div class=""/><div class=""><h2 id="14d5" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh dx translated">免费使用Pyspark在大规模机器学习模型中进行超参数调整</h2></div><p id="11df" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">经过几个月的追踪和审查，您终于获得了一直等待的数据转储。您已经完成了必要的预处理步骤、一些特征工程和一些模型选择。经过一段时间的试验，你选择了你的最佳算法，执行超参数调整部分，装满你的大杯咖啡，然后你等待，等待，等待……然后像一个永无止境的故事一样等待结果。在任何典型的数据科学家/机器学习工程师的日常工作中，这都是一个非常普遍的现象。我确信这被认为是整个ML生命周期中最无聊和单调的任务，如果有的话。我偶然发现了一个很酷的方法，可以在很大程度上减少这种等待时间。这是纯粹的魔法。就好像看了这篇《你是巫师哈利！。不管怎样，魔法有个名字，它是…..等着吧…..<strong class="jk hu">并行化</strong>。而且要用的法术是<strong class="jk hu"> Pyspark </strong>。</p><p id="0a79" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated"><strong class="jk hu">超参数调整</strong>无非是寻找合适的超参数集，以达到高精度和准确度。优化超参数是构建机器学习模型最棘手的部分之一。超参数调整的主要目的是找到模型参数的最佳点，以便获得更好的性能。</p><p id="9d51" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在典型的经典(我指的是使用Scikit-Learn for ML(无意冒犯))机器学习管道中进行超参数调整的两种最常见的方法是</p><blockquote class="ke kf kg"><p id="93b7" class="ji jj kh jk b jl jm iu jn jo jp ix jq ki js jt ju kj jw jx jy kk ka kb kc kd hb bi translated"><strong class="jk hu">网格搜索</strong></p><p id="4ac0" class="ji jj kh jk b jl jm iu jn jo jp ix jq ki js jt ju kj jw jx jy kk ka kb kc kd hb bi translated">在网格搜索中，我们尝试超参数值的预设列表的每个组合，并评估每个组合的模型。</p><p id="12b1" class="ji jj kh jk b jl jm iu jn jo jp ix jq ki js jt ju kj jw jx jy kk ka kb kc kd hb bi translated"><strong class="jk hu">随机搜索</strong></p><p id="d94e" class="ji jj kh jk b jl jm iu jn jo jp ix jq ki js jt ju kj jw jx jy kk ka kb kc kd hb bi translated">随机搜索是一种技术，其中使用超参数的随机组合来为构建的模型找到最佳解决方案。</p></blockquote><p id="b65c" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">虽然在机器学习实践中不如网格搜索常见，但对于某些类型的问题，随机搜索已被证明可以在更少的函数评估内找到与网格搜索相等或更好的值。对于我的大多数项目来说，这也是我的选择。所以，我在这篇文章中解释了随机搜索的魔力。</p><p id="01da" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">为了那些没有集群网格可供支配的初露头角的数据科学家，我使用了Databricks社区版免费服务器，并在帖子中提供了一个玩具示例。<a class="ae kl" href="https://towardsdatascience.com/quick-start-guide-to-spark-with-databricks-d2d24a2f995d" rel="noopener" target="_blank">这里的</a>是一个关于如何建立一个免费集群并使用它的便捷教程。谁能想到规模上的微调可以免费实现？</p><h1 id="8640" class="km kn ht bd ko kp kq kr ks kt ku kv kw iz kx ja ky jc kz jd la jf lb jg lc ld bi translated">如何的想法</h1><p id="278b" class="pw-post-body-paragraph ji jj ht jk b jl le iu jn jo lf ix jq jr lg jt ju jv lh jx jy jz li kb kc kd hb bi translated">Spark最擅长使用键值对。它实现并行化的方式是通过一个特定的键将所有数据发送到单个节点。键及其相关数据的子集将进入集群中的每个节点进行处理。这对我们数据科学家来说是个问题。我们需要所有节点上的所有数据，以便能够进行随机搜索和拟合模型。或者任何相关的任务。重点关注以下内容，因为这是一个至关重要的技巧。一个街头智能解决方案是多次复制所有数据，并为每个复制分配一个replication_id。这样，我们可以按replication_id分组，并且可以将包含完整数据的每个复制发送到群集的每个节点。这个巧妙的小技巧使我们有可能利用PySpark并行化的魔力来大规模执行我们的机器学习任务。你可能会问，我们如何在spark集群上使用pyspark中的pandas和sklearn来使用标准的机器学习代码。为了让数据科学家能够利用大数据的价值，Spark在0.7版本中添加了Python API，支持<a class="ae kl" href="https://docs.databricks.com/spark/latest/spark-sql/udf-in-python.html" rel="noopener ugc nofollow" target="_blank">用户定义函数</a>。这些用户定义的函数一次操作一行，因此序列化和调用开销很大。放心吧，<strong class="jk hu">熊猫_udf </strong>来救援了。构建在<a class="ae kl" href="https://arrow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Apache Arrow </a>之上的Pandas UDFs为您带来了两全其美——完全用Python定义低开销、高性能UDF的能力。Spark版本2.3.1中引入了这一功能。更多关于这个<a class="ae kl" href="https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="0aae" class="km kn ht bd ko kp kq kr ks kt ku kv kw iz kx ja ky jc kz jd la jf lb jg lc ld bi translated">密码</h1><p id="51c5" class="pw-post-body-paragraph ji jj ht jk b jl le iu jn jo lf ix jq jr lg jt ju jv lh jx jy jz li kb kc kd hb bi translated">以下代码可以在Databricks community Edition free spark集群上运行。得到它<a class="ae kl" href="https://community.cloud.databricks.com/login.html" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><ol class=""><li id="391f" class="lj lk ht jk b jl jm jo jp jr ll jv lm jz ln kd lo lp lq lr bi translated">导入库必需的库。</li></ol><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="c41c" class="mb kn ht lx b fi mc md l me mf">from sklearn import datasets<br/>import pandas as pd<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.metrics import accuracy_score<br/>import  pyspark.sql.functions as F<br/>import random<br/>from pyspark.sql.types import *<br/>from sklearn.model_selection import train_test_split</span></pre><p id="fa22" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">2.制作一个有10000条记录的玩具分类示例。</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="b3bb" class="mb kn ht lx b fi mc md l me mf">X,y = datasets.make_classification(n_samples=10000, n_features=4, n_informative=2, n_classes=2, random_state=1,shuffle=True)</span></pre><p id="eef2" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">3.将X和y放入一个熊猫数据框中。</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="8942" class="mb kn ht lx b fi mc md l me mf">training_data = pd.DataFrame(X)<br/>training_data['target'] = y</span></pre><p id="665e" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">4.将pandas数据框架转换为spark数据框架。</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="3e34" class="mb kn ht lx b fi mc md l me mf">train_spark_df = spark.createDataFrame(training_data)<br/>train_spark_df = train_spark_df.toDF(*['c0', 'c1', 'c2', 'c3', 'target'])</span></pre><p id="322f" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">5.创建一个只有一列复制id的spark数据框。</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="70ad" class="mb kn ht lx b fi mc md l me mf">replication_df = spark.createDataFrame(pd.DataFrame(list(range(1,1001)),columns=['replication_id']))</span></pre><p id="551a" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">6.在两个数据帧之间应用交叉连接，以获得完整数据作为值、复制id作为键的键-值对。</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="58af" class="mb kn ht lx b fi mc md l me mf">replicated_train_df = train_spark.crossJoin(replication_df)</span></pre><p id="a02a" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">7.为UDF函数定义一个输出模式(这是Pyspark所期望的)</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="6031" class="mb kn ht lx b fi mc md l me mf">outSchema = StructType([StructField('replication_id',IntegerType(),True),StructField('Accuracy',DoubleType(),True),StructField('num_trees',IntegerType(),True),StructField('depth',IntegerType(),True),StructField('criterion',StringType(),True)])</span></pre><p id="e753" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">8.现在来看看实际的UDF函数。它看起来像一个普通的python函数，只是在它的顶部添加了一个额外的装饰器。</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="568d" class="mb kn ht lx b fi mc md l me mf"><a class="ae kl" href="http://twitter.com/F" rel="noopener ugc nofollow" target="_blank">@F</a>.pandas_udf(outSchema, F.PandasUDFType.GROUPED_MAP)<br/>def run_model(pdf):</span><span id="8c2d" class="mb kn ht lx b fi mg md l me mf">    ## this is how we are picking random set of parameters.<br/>    num_trees =  random.choice(list(range(50,500)))<br/>    depth = random.choice(list(range(2,10)))<br/>    criterion = random.choice(['gini','entropy'])</span><span id="c0ea" class="mb kn ht lx b fi mg md l me mf">    replication_id = pdf.replication_id.values[0]</span><span id="c239" class="mb kn ht lx b fi mg md l me mf">    # Split X and y<br/>    X = pdf[['c0', 'c1', 'c2', 'c3']]<br/>    y = pdf['target']</span><span id="6f11" class="mb kn ht lx b fi mg md l me mf">    # Test train split<br/>    Xtrain,Xcv,ytrain,ycv = train_test_split(X, y, test_size=0.33, random_state=42)</span><span id="2a62" class="mb kn ht lx b fi mg md l me mf">    # Initilize model and fit    <br/>    clf = RandomForestClassifier(n_estimators=num_trees, max_depth = depth, criterion =criterion)<br/>    clf.fit(Xtrain,ytrain)</span><span id="0704" class="mb kn ht lx b fi mg md l me mf">    # Get the accuracies <br/>    accuracy = accuracy_score(clf.predict(Xcv),ycv)</span><span id="8d4f" class="mb kn ht lx b fi mg md l me mf">    # Return result as a pandas data frame               <br/>res=pd.DataFrame({'replication_id':replication_id,'Accuracy':accuracy,'num_trees':num_trees,'depth':depth,'criterion':criterion}, index=[0])<br/>    return res</span></pre><p id="bf38" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">请在这里阅读一下“<strong class="jk hu">F . pandasudftype . grouped _ MAP</strong>”如何工作<a class="ae kl" href="https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="7e89" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">9.一旦你有了UDF，现在是时候把它应用到我们的专栏了。</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="aa7f" class="mb kn ht lx b fi mc md l me mf">results = replicated_train_df.groupby("replication_id").apply(run_model)</span></pre><p id="67ff" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">10.上述语句不会实际处理数据，但会保存要应用于列的指令集。当我们请求对show()、count()等数据进行操作时，真正的处理就开始了。</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="0ab6" class="mb kn ht lx b fi mc md l me mf">results.sort(F.desc("Accuracy")).show()</span></pre><p id="7434" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">输出:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/f337ddbebcda2a657105d83089c6f32c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dZmNtphDBh7vKqxayeLMmA.png"/></div></div></figure><p id="cc48" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">该处理花费了<strong class="jk hu"> 10.19分钟</strong>。为了比较普通python有多好，这里有一个纯Python的等价物，我们试图使用sklearn内置的RandomizedSearchCV函数来实现1000次迭代。</p><pre class="ls lt lu lv fd lw lx ly lz aw ma bi"><span id="8bdb" class="mb kn ht lx b fi mc md l me mf"># Load libraries<br/>from scipy.stats import uniform<br/>from sklearn import linear_model, datasets<br/>from sklearn.model_selection import RandomizedSearchCV<br/>import random<br/>from sklearn.ensemble import RandomForestClassifier<br/>import time</span><span id="f831" class="mb kn ht lx b fi mg md l me mf">X,y = datasets.make_classification(n_samples=10000, n_features=4, n_informative=2, n_classes=2, random_state=1,shuffle=True)</span><span id="bc04" class="mb kn ht lx b fi mg md l me mf">n_estimators =  [int(x) for x in range(50,501)]<br/>max_depth = [int(x) for x in range(2,11)]<br/>criterion = ['gini','entropy']</span><span id="8d73" class="mb kn ht lx b fi mg md l me mf">hyperparameters = dict(n_estimators=n_estimators, max_depth=max_depth, criterion= criterion)</span><span id="469c" class="mb kn ht lx b fi mg md l me mf">clf = RandomForestClassifier()</span><span id="3254" class="mb kn ht lx b fi mg md l me mf">clf = RandomizedSearchCV(clf, hyperparameters, random_state=1, n_iter=1000, verbose=0, n_jobs=-1)</span><span id="438e" class="mb kn ht lx b fi mg md l me mf">start_time = time.time()<br/>best_model = clf.fit(X, y)<br/>print("--- %s seconds ---" % (time.time() - start_time))</span></pre><p id="32cd" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">输出:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mi"><img src="../Images/e6203da4adb06542bb5d1b86a4c19087.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Vt7DQBtT3uCD0OfMH60Tg.png"/></div></div></figure><p id="c099" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">你可以看到<strong class="jk hu">纯python花了38.3分钟，而pyspark花了10.2分钟。</strong>这已经是相当大的进步了。Pyspark的处理时间会进一步减少，而python在处理更多数据时会花费更长时间(比如将总记录数从1000000增加到10000)。</p><p id="2cd4" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">以下是所有代码的要点，供您复制粘贴之用。</p><figure class="ls lt lu lv fd hk"><div class="bz dy l di"><div class="mj mk l"/></div></figure><p id="778e" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">愿这给你赋予Pypark的力量，让你在未来的机器学习中不再浪费不必要的时间。</p><p id="5a09" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">关于疯狂观看的部分。如果您可以在超参数调优上节省时间，那么您显然可以在网飞上花费更多时间。随便说说；)</p><blockquote class="ke kf kg"><p id="3088" class="ji jj kh jk b jl jm iu jn jo jp ix jq ki js jt ju kj jw jx jy kk ka kb kc kd hb bi translated"><em class="ht">关于我</em></p></blockquote><p id="3217" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我是<a class="ae kl" href="https://wavelabs.ai/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Wavelabs.ai </a>的资深AI专家。我们Wavelabs帮助您利用人工智能(AI)来彻底改变用户体验并降低成本。我们使用人工智能独特地增强您的产品，以达到您的全部市场潜力。我们试图将尖端研究引入您的应用中。</p><p id="d87c" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">欢迎访问<a class="ae kl" href="https://wavelabs.ai/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> Wavelabs.ai </a>了解更多信息。</p></div><div class="ab cl ml mm gp mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="hb hc hd he hf"><p id="6d45" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">嗯，这都是在这个职位。感谢阅读:)</p><p id="68bc" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">保持好奇！</p><p id="3491" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">你可以在<a class="ae kl" href="https://www.linkedin.com/in/rehan-a-18675296?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我。</p></div></div>    
</body>
</html>