<html>
<head>
<title>Why and How PCA implemented</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么以及如何实施认证后活动</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/why-and-how-pca-implemented-5d2d9f282a94?source=collection_archive---------24-----------------------#2019-12-11">https://medium.com/analytics-vidhya/why-and-how-pca-implemented-5d2d9f282a94?source=collection_archive---------24-----------------------#2019-12-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="eebb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi jd translated">“维度的诅咒”，在我的母语(泰卢固语)中被翻译为“kolathala yoka saapam”听起来像好莱坞配音的恐怖电影，这种现象通常被称为在处理高维数据时出现的问题，这些问题在低维中并不存在。</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es jm"><img src="../Images/cc3abd80b9cc15de31bfb452497a6627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*RfStsKyHeVChMSqIkgRrBw.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">分类器或回归器的预测能力首先随着使用的维度/特征数量的增加而增加，然后降低</figcaption></figure><p id="4981" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在糟糕的情况下，数据集可能有大量的要素或维度。随着特征数量的增加，可视化(更高的维度，不可能)或训练变得更加复杂，并且模型可能过度拟合数据，导致性能不佳</p><p id="8777" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但在特征数据集较少的情况下，数据量较小，因此需要存储。冗余较少，因此精度提高。此外，该模型运行速度更快</p><p id="e0ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，我们如何克服这个问题呢？，降维就来了</p><h1 id="4203" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">维度还原</h1><p id="b54c" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">降维是降低数据集中的维度的过程，这意味着在不丢失太多信息的情况下将高维数据转换为低维数据。</p><p id="cce3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">降维有两类技术:特征消除和特征提取。主成分分析属于特征提取技术。</p><p id="7d81" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">降维工具有很多。但是PCA比较老但是很受欢迎</p></div><div class="ab cl lb lc gp ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="hb hc hd he hf"><h1 id="0229" class="jy jz hi bd ka kb li kd ke kf lj kh ki kj lk kl km kn ll kp kq kr lm kt ku kv bi translated">主成分分析</h1><p id="619e" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">PCA(主成分分析)是一种用于特征提取的技术，主要用作探索性数据分析的工具和用于建立预测模型。PCA的思想很简单——将大数据集(k维)的维数降低到一个更小的(d维),其中d<k/></p><p id="b0a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于给定的数据集，变量之间的线性相关性被分解为独立的变量，这些变量相互独立且正交，它们被称为主成分。这种变换以这样一种方式定义，即第一个主分量具有最大可能的方差(更多地分布在数据中),而每个随后的分量在与前面的分量正交且不相关的约束下又具有最大可能的方差</p><p id="1e51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于给定的高维数据集，PCA过程包括:</p><ul class=""><li id="e31c" class="ln lo hi ih b ii ij im in iq lp iu lq iy lr jc ls lt lu lv bi translated">使数据标准化</li><li id="b5ab" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">求标准化数据的协方差矩阵。</li><li id="4c0d" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">为获得的协方差矩阵计算特征向量和相应的特征值。</li><li id="631b" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">按照特征值降序排列特征向量。</li><li id="22a7" class="ln lo hi ih b ii lw im lx iq ly iu lz iy ma jc ls lt lu lv bi translated">选择第一个d特征向量，这将是新的d维，将原来的k维数据点转换成d维。</li></ul><p id="b8d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这可以由sci-kit learn通过以下代码轻松实现</p><pre class="jn jo jp jq fd mb mc md me aw mf bi"><span id="93e0" class="mg jz hi mc b fi mh mi l mj mk">from sklearn.decomposition import PCA as sklearnPCA<br/>sklearn_pca = sklearnPCA(n_components=2)<br/>Y_sklearn = sklearn_pca.fit_transform(X_std)</span></pre><p id="85dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上面的代码n_components=2中，我们采用了2个主分量，这有助于2维可视化，但是为了训练模型，我们需要更多具有最多信息的特征，然后我们需要计算所解释的方差，并使用按降序排序的特征值来选择值</p><figure class="jn jo jp jq fd jr er es paragraph-image"><div class="er es ml"><img src="../Images/8687f287c61280e58a59734bf76b79f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/1*r1oB33KuA3ky_FJIuLOvNg.gif"/></div></figure><p id="d861" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，在图像中，红线代表PDF，黄线代表CDF，当我们取一个主成分(PC 1)时，解释的方差是23%,而第二个主成分(PC2)是19%,它们合起来覆盖了42%的数据。假设我们想要包含90%的数据，我们从13个主成分中取出前10个主成分</p><p id="e64d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了计算解释方差，我们需要按降序排列的特征值(λ1，λ2，…λ13)。仅包含一个主成分的解释方差为λ1/(λ1 + λ2 + … + λ13)=23%。因为只有第二主成分是λ2/(λ1 + λ2 + … + λ13)=19%。对于90%的数据是(λ1 + λ2 + … + λ10)/(λ1 + λ2 + … + λ13)=91%。</p><h1 id="9ad2" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">结论</h1><p id="4cb4" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">我希望这篇文章对你有帮助！欢迎建议</p><h1 id="4cbd" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">参考</h1><p id="dc4f" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated"><a class="ae mm" href="http://sebastianraschka.com/Articles/2014_pca_step_by_step.html" rel="noopener ugc nofollow" target="_blank">http://sebastianraschka . com/Articles/2014 _ PCA _ step _ by _ step . html</a></p><p id="803b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">https://en.wikipedia.org/wiki/Principal_component_analysis<a class="ae mm" href="https://en.wikipedia.org/wiki/Principal_component_analysis" rel="noopener ugc nofollow" target="_blank"/></p><p id="8206" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae mm" href="https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c" rel="noopener" target="_blank">https://towards data science . com/a-一站式主成分分析-5582fb7e0a9c </a></p></div></div>    
</body>
</html>