<html>
<head>
<title>Introduction to PySpark — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark简介—第2部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-pyspark-part-2-6d6113e31592?source=collection_archive---------15-----------------------#2020-07-10">https://medium.com/analytics-vidhya/introduction-to-pyspark-part-2-6d6113e31592?source=collection_archive---------15-----------------------#2020-07-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/fae9fb0a807ed3b0c8999ad76e889b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/0*2jYXsGugpWPUAndD.png"/></div></figure><p id="7ad0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在<a class="ae jk" rel="noopener" href="/@anveshrithaas/getting-started-with-apache-spark-part-1-91b379204ae0">之前的博客</a>中，我们首先介绍了Apache Spark，为什么它是首选，它的特性和优势，架构以及它的工业用例。在本文中，我们将开始使用py Spark——使用Python的Apache Spark！到本文结束时，您将会更好地理解什么是PySpark，为什么我们选择python作为Spark，它的特性和优点，然后是在您自己的计算机上设置PySpark的快速安装指南。最后，本文将阐明Spark中的一些重要概念，以便更进一步。</p><h1 id="fada" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">PySpark是什么？</h1><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es kj"><img src="../Images/abd7e3d3bb070619cd51b1ac1b540fff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*TwdqU3ATPwkJZLM9.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">资料来源:Databricks</figcaption></figure><p id="999c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">正如我们已经讨论过的，Apache Spark也支持Python和其他语言，这使得那些更习惯使用Python for Apache Spark的开发人员更加容易。与Spark的原生语言Scala相比，Python是一种相对更容易学习和使用的编程语言，许多人更喜欢用它来开发Spark应用程序。众所周知，Python是许多数据分析工作负载的事实语言。虽然Apache Spark是当今使用最广泛的大数据框架，但Python是使用最广泛的编程语言之一，尤其是在数据科学领域。那么为什么不整合它们呢？这就是py Spark——python for Spark的用武之地。为了用Apache Spark支持Python，发布了PySpark。由于许多数据科学家和分析师将python用于其丰富的库，将它与Spark集成是两全其美。在开源社区的大力支持下，PySpark是使用Py4j库开发的，通过Python与Apache Spark中的rdd接口。高速数据处理、强大的缓存、实时和内存计算以及低延迟是PySpark优于其他数据处理框架的一些特性。</p><h1 id="520d" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">为什么选择Python做Spark？</h1><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es kw"><img src="../Images/ddde848ceccfcbea4c94d41c596c5d40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*D5-uKOxN43o7T7dd.png"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">来源:becominghuman.ai</figcaption></figure><p id="92e3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">与其他编程语言相比，Python更容易学习和使用，这要归功于它的语法和标准库。Python是一种动态类型语言，这使得Spark的rdd能够保存多种类型的对象。此外，Python有一套广泛而丰富的库，用于各种实用程序，如机器学习、自然语言处理、可视化、本地数据转换等等。</p><p id="c7e9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">虽然python有许多用于数据分析和操作的库，如Pandas、NumPy、SciPy，但是这些库是内存相关的，并且依赖于单节点系统。因此，对于处理万亿字节和千兆字节级的大型数据集来说，这并不理想。对于熊猫，可伸缩性是一个问题。在实时或接近实时的数据流的情况下，大量的数据需要被带到一个集成的空间进行转换、处理和分析，Pandas不是一个最佳的选择。相反，我们需要一个框架，通过分布式和流水线处理的方式，更快、更有效地完成工作。这就是PySpark发挥作用的地方。</p><p id="d1af" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="kx">注:</em></strong><em class="kx">Spark数据帧可以用toPandas()函数转换成Pandas数据帧。但是当分布式Spark数据帧被转换时，所有数据都被带入单个节点驱动器存储器。因此，它对于大型数据集来说效率不高。</em></p><h1 id="dedf" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated"><strong class="ak"> PySpark安装和设置</strong></h1><figure class="kk kl km kn fd ij er es paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="er es ky"><img src="../Images/0aa827fedbb1908b1fc8597ac8d59211.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iRtpDYbOnI49VBfnWrfCJA.jpeg"/></div></div></figure><p id="fab1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">以下是在Windows中安装和设置Spark的步骤。</p><h2 id="d7e3" class="kz jm hi bd jn la lb lc jr ld le lf jv ix lg lh jz jb li lj kd jf lk ll kh lm bi translated">安装先决条件</h2><ol class=""><li id="7201" class="ln lo hi io b ip lp it lq ix lr jb ls jf lt jj lu lv lw lx bi translated"><strong class="io hj">安装Java </strong></li></ol><ul class=""><li id="2e3d" class="ln lo hi io b ip iq it iu ix ly jb lz jf ma jj mb lv lw lx bi translated">确保您的计算机中已经安装了Java(版本7或更高版本)</li><li id="2cd6" class="ln lo hi io b ip mc it md ix me jb mf jf mg jj mb lv lw lx bi translated">设置环境变量并将其添加到PATH变量中。</li></ul><p id="3272" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果你还没有安装Java，参考<a class="ae jk" href="https://www.guru99.com/install-java.html" rel="noopener ugc nofollow" target="_blank">本安装指南</a>来完成。</p><p id="9d3b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 2。安装Python </strong></p><ul class=""><li id="3123" class="ln lo hi io b ip iq it iu ix ly jb lz jf ma jj mb lv lw lx bi translated">确保您已经安装了Python(版本2.7或更高版本)</li></ul><p id="a386" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">要安装Python for Windows，请参考<a class="ae jk" href="https://www.howtogeek.com/197947/how-to-install-python-on-windows/" rel="noopener ugc nofollow" target="_blank">本安装指南</a></p><p id="9d9d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 3。安装火花</strong></p><ul class=""><li id="9289" class="ln lo hi io b ip iq it iu ix ly jb lz jf ma jj mb lv lw lx bi translated">转到<a class="ae jk" href="http://spark.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank"> Apache Spark主页</a></li><li id="cb1e" class="ln lo hi io b ip mc it md ix me jb mf jf mg jj mb lv lw lx bi translated">选择最新的稳定Spark版本，对于包类型，选择预构建的Apache Hadoop</li><li id="1dc8" class="ln lo hi io b ip mc it md ix me jb mf jf mg jj mb lv lw lx bi translated">并下载。tgz文件，方法是单击步骤3中的链接(如下所示)</li></ul><figure class="kk kl km kn fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/5851fc2d044ac90510edd075105c30c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*pDE3j3obymiTxMR-imW24Q.png"/></div></figure><ul class=""><li id="0eb5" class="ln lo hi io b ip iq it iu ix ly jb lz jf ma jj mb lv lw lx bi translated">将下载的zip文件解压到所选目录下的一个文件夹中，并将其命名为“spark”(我的文件夹的路径是C:\spark，因为我已将其存储在c盘中)</li><li id="b388" class="ln lo hi io b ip mc it md ix me jb mf jf mg jj mb lv lw lx bi translated">设置环境变量:</li></ul><pre class="kk kl km kn fd mi mj mk ml aw mm bi"><span id="9ca1" class="kz jm hi mj b fi mn mo l mp mq">SPARK_HOME = C:\spark\spark-3.0.0-bin-hadoop2.7</span></pre><ul class=""><li id="482d" class="ln lo hi io b ip iq it iu ix ly jb lz jf ma jj mb lv lw lx bi translated">将路径添加到path环境变量中:</li></ul><pre class="kk kl km kn fd mi mj mk ml aw mm bi"><span id="c19c" class="kz jm hi mj b fi mn mo l mp mq">C:\spark\spark-3.0.0-bin-hadoop2.7\bin</span></pre><p id="1b7a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 4。安装winutils.exe</strong></p><ul class=""><li id="9e4c" class="ln lo hi io b ip iq it iu ix ly jb lz jf ma jj mb lv lw lx bi translated">去<a class="ae jk" href="https://github.com/steveloughran/winutils" rel="noopener ugc nofollow" target="_blank">https://github.com/steveloughran/winutils</a>下载你的Hadoop版本的winutils.exe(我的例子是hadoop-2.7.1)</li><li id="ffe8" class="ln lo hi io b ip mc it md ix me jb mf jf mg jj mb lv lw lx bi translated">将winutils.exe文件移动到SPARK_HOME中的bin文件夹(在我的例子中是C:\ SPARK \ SPARK-3 . 0 . 0-bin-Hadoop 2.7 \ bin)</li><li id="25d3" class="ln lo hi io b ip mc it md ix me jb mf jf mg jj mb lv lw lx bi translated">设置环境变量(与SPARK_HOME相同):</li></ul><pre class="kk kl km kn fd mi mj mk ml aw mm bi"><span id="4c83" class="kz jm hi mj b fi mn mo l mp mq">HADOOP_HOME = C:\spark\spark-3.0.0-bin-hadoop2.7</span></pre><p id="392e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> 5。验证PySpark安装</strong></p><ul class=""><li id="7ae0" class="ln lo hi io b ip iq it iu ix ly jb lz jf ma jj mb lv lw lx bi translated">要验证pyspark是否安装成功，请打开命令提示符，键入PySpark并执行它以进入PySpark shell。如果安装正确，您应该会看到类似下面的输出</li></ul><figure class="kk kl km kn fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/f77348496094c8c77fedef10ed0af7b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*N5WG5UkO4q5-rqSdGlAFiQ.png"/></div></figure><p id="d227" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="kx">注意:</em> </strong> <em class="kx">为了避免以后的错误和修复一些安装Spark后的bug，按照</em> <a class="ae jk" rel="noopener" href="/big-data-engineering/how-to-install-apache-spark-2-x-in-your-pc-e2047246ffc3"> <em class="kx">这篇文章</em> </a>中的可选步骤(步骤4)进行操作</p><p id="5571" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">PySpark安装成功，现在我们可以开始了！但是在我们深入探讨这个问题之前，Spark中有几个重要的概念和术语，在进一步深入之前，您必须了解它们。</p><h2 id="b374" class="kz jm hi bd jn la lb lc jr ld le lf jv ix lg lh jz jb li lj kd jf lk ll kh lm bi translated">SparkContext、SparkConf和SparkSession</h2><p id="0645" class="pw-post-body-paragraph im in hi io b ip lp ir is it lq iv iw ix mr iz ja jb ms jd je jf mt jh ji jj hb bi translated">SparkContext是Spark功能的入口点。Spark应用程序应该做的第一件事是创建一个定义如何访问集群的SparkContext对象。当执行Spark应用程序时，驱动程序启动，SparkContext初始化。Spark驱动程序应用程序生成的SparkContext允许Spark应用程序使用资源管理器访问Spark集群。</p><p id="8ad2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">首先，为了创建SparkContext，应该创建定义Spark应用程序配置的SparkConf。SparkConf用于配置Spark，它包含由驱动程序传递给SparkContext的各种配置参数。一些常用的参数是master参数和appName参数，前者给出它所连接的集群的URL，后者给出作业的名称。SparkConf和SparkContext的创建过程如下。</p><pre class="kk kl km kn fd mi mj mk ml aw mm bi"><span id="4f36" class="kz jm hi mj b fi mn mo l mp mq">#Creating SprarkConf object to configure Spark to run locally <br/>conf = SparkConf().setMaster("local").setAppName("SparkBlog")</span><span id="9037" class="kz jm hi mj b fi mu mo l mp mq">#Creating SparkContext<br/>sc = SparkContext(conf)</span></pre><p id="bc54" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从Spark 2.0开始，引入了SparkSession作为统一入口点，消除了显式创建SparkConf和SparkContext的需要，因为它们封装在SparkSession中。SparkContext中可用的所有功能现在都可以在SparkSession中使用，从而取代了SparkContext和SparkConf。SparkSession创建如下。</p><pre class="kk kl km kn fd mi mj mk ml aw mm bi"><span id="e1b0" class="kz jm hi mj b fi mn mo l mp mq">spark = SparkSession.builder().master(“local”)\<br/>.appName(“example of SparkSession”)\<br/>.config(“spark.some.config.option”, “config-value”)\<br/>.getOrCreate()</span></pre><p id="cd1c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="kx">注意:</em> </strong> <em class="kx">如果使用pyspark shell，SparkContext变量</em><strong class="io hj"><em class="kx">【sc】</em></strong><em class="kx">和SparkSession变量</em><strong class="io hj"><em class="kx">【spark】</em></strong><em class="kx">会自动创建。您不需要显式创建它们。</em></p><h2 id="43a5" class="kz jm hi bd jn la lb lc jr ld le lf jv ix lg lh jz jb li lj kd jf lk ll kh lm bi translated">弹性分布式数据集</h2><p id="fd07" class="pw-post-body-paragraph im in hi io b ip lp ir is it lq iv iw ix mr iz ja jb ms jd je jf mt jh ji jj hb bi translated">弹性分布式数据集(rdd)是PySpark的构建块。RDD是Apache Spark的基本数据结构。它是一个不可变的对象集合，可以跨集群进行拆分，以促进内存中的数据存储，从而提高效率。RDD是一种无模式的数据结构，可以处理结构化和非结构化数据。虽然rdd可以跨集群中的各种节点进行逻辑分区，但rdd上的操作也可以拆分并并行执行，从而实现更快、更可扩展的并行处理。RDD是一个分布式内存抽象，它的一个显著特征是高度弹性。这是因为数据块是跨集群中的多个节点复制的，因此它们能够从任何问题中快速恢复，因为即使一个执行器节点出现故障，数据仍将由其他节点处理，这使得它具有高度的容错能力。此外，rdd遵循延迟评估，也就是说，执行不会立即开始，而是仅在需要时才触发。可以在rdd上执行两个基本且重要的操作——转换和操作，我们将在下一篇博客中讨论。</p><p id="60ed" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">总而言之，我们从PySpark的概述开始，为什么它被用于Spark，它在哪些方面优于其他python库。我们看到了如何在您的计算机上安装它，然后以SparkContext、SparkConf、SparkSession的概述结束，最后是Spark的主干RDDs！我希望现在你已经知道它到底是什么了。还有更多需要学习的内容，我们将在本系列的后续文章中介绍。查看下一篇关于rdd操作的文章,在那里你会得到一个更清晰的画面。快乐学习！</p></div><div class="ab cl mv mw gp mx" role="separator"><span class="my bw bk mz na nb"/><span class="my bw bk mz na nb"/><span class="my bw bk mz na"/></div><div class="hb hc hd he hf"><p id="1f2c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">查看本系列中的其他博客</p><p id="cb9e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" rel="noopener" href="/@anveshrithaas/getting-started-with-apache-spark-part-1-91b379204ae0"> <strong class="io hj"> <em class="kx">第一部分Apache Spark入门</em> </strong> </a></p><p id="b2b1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" rel="noopener" href="/@anveshrithaas/understanding-spark-rdds-part-3-3b1b9331652a"> <strong class="io hj"> <em class="kx">第三部分—了解Spark RDDs </em> </strong> </a></p><p id="48ca" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" rel="noopener" href="/@anveshrithaas/machine-learning-in-pyspark-part-4-5813e831922f"> <strong class="io hj"> <em class="kx">第四部分PySpark中的机器学习</em> </strong> </a></p><p id="9e9f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><a class="ae jk" rel="noopener" href="/@anveshrithaas/end-to-end-machine-learning-pipeline-on-databricks-part-5-c10273e2cd88"> <strong class="io hj"> <em class="kx">第五部分——数据块上的端到端机器学习流水线</em> </strong> </a></p></div></div>    
</body>
</html>