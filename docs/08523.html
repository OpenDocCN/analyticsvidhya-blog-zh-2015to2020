<html>
<head>
<title>Question Answering Using BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用BERT回答问题</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-bert-f9aa4075cf4f?source=collection_archive---------8-----------------------#2020-08-02">https://medium.com/analytics-vidhya/introduction-to-bert-f9aa4075cf4f?source=collection_archive---------8-----------------------#2020-08-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="07f1" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">开始将BERT语言模型应用于您自己的业务问题的实用指南。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/d65450666243b12edb2dabe10c478d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8u1XwtWRGsyHMNe6zRFwUQ.png"/></div></div></figure><p id="b0bc" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> BERT </strong>，<em class="kf">来自Transformer的双向编码器表示，</em>是Google推出的最先进的语言模型，可用于尖端的自然语言处理(NLP)任务。</p><p id="9c48" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">阅读完本文后，您将对BERT有一个基本的了解，并将能够在您自己的业务应用程序中使用它。如果你熟悉Python，对机器学习有个大概的了解，会很有帮助。</p><p id="61e5" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我将在本文中介绍的BERT模型有:</p><ul class=""><li id="ba5b" class="kg kh hi jl b jm jn jp jq js ki jw kj ka kk ke kl km kn ko bi translated">二元或多类分类</li><li id="41e2" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">回归模型</li><li id="4a79" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">问答应用程序</li></ul><p id="e81e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">伯特简介</strong></p><p id="11e7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">BERT是在整个维基百科(约25亿字)以及一本书的语料库(约8亿字)上接受训练的。为了利用BERT，您不必重复这个计算密集型过程。</p><p id="712f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">BERT以一种前所未有的语言模型的方式将迁移学习方法引入自然语言处理领域。</p><p id="d216" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">迁移学习</strong></p><p id="1060" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">迁移学习是一个过程，其中为一般任务开发的机器学习模型可以被重用为特定业务问题的起点。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ku"><img src="../Images/0b96ff18f6cb1c2571b6713b486a64d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Fe6ioF_5f_FdOGpv"/></div></div></figure><p id="8cde" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">假设你想教一个叫阿曼达的人如何参加SAT考试，她不会说英语。第一步是尽可能彻底地教阿曼达英语。然后，你可以针对SAT更具体地教她。</p><p id="a6f1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在机器学习模型的背景下，这种想法被称为迁移学习。迁移学习的第一部分是前期训练(类似于第一次教阿曼达英语)。预培训完成后，你可以专注于一项具体的任务(比如教阿曼达如何参加SAT考试)。这是一个称为微调的过程，即改变模型，使其适合您的特定业务问题。</p><p id="feff" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">伯特预培训</strong></p><p id="e85f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这是关于BERT培训前流程的快速介绍。出于实用目的，您可以使用预先训练的BERT模型，而不需要执行此步骤。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ku"><img src="../Images/e78b7699089f2543672a25889025268e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BfYtYF2RHMErhVcC"/></div></div></figure><p id="a6d8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">BERT接受两个文本块作为输入。在上面的简化示例中，我将这两个输入称为句子1和句子2。在针对BERT的预训练中，在大约一半的训练示例中，句子2故意不跟随句子1。</p><p id="a8ce" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">第一句以一个特殊符号[CLS]开始，两句都以另一个特殊符号[SEP]结束。BERT词汇表中的每个单词都有一个标记。如果一个单词不在词汇表中，BERT会把这个单词分成多个记号。在给伯特输入句子之前，15%的标记被屏蔽。</p><p id="d0f8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">预训练过程是迁移学习的第一步，就像按照BERT模型教授英语，以便它可以用于各种需要英语知识的任务。这是通过给BERT的两个练习任务来完成的:</p><ol class=""><li id="c283" class="kg kh hi jl b jm jn jp jq js ki jw kj ka kk ke kv km kn ko bi translated">预测屏蔽(隐藏)令牌。为了说明，单词“favorite”和“To”在上图中被屏蔽了。作为预训练的一部分，BERT将尝试预测这些屏蔽的令牌。这类似于我们给正在学习英语的学生布置的“填空”任务。当学生试着填入所缺的单词时，他会学习这门语言。这被称为掩蔽语言模型(MLM)。</li><li id="4877" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kv km kn ko bi translated">BERT还试图预测句子2在逻辑上是否跟在句子1后面，以便更深入地理解句子依存关系。在上面的例子中，句子2是句子1的逻辑延续，所以预测是正确的。输出端的特殊标记[CLS]用于此任务。</li></ol><p id="a2c1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">BERT预训练模型有许多变体。最常见的是BERT Large和BERT Base:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es kw"><img src="../Images/3bf4bf54078ff594abe9f0ee08e533b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pXYck51dzMcDqREO-xReEg.png"/></div></div></figure><p id="d3ec" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">伯特微调</strong></p><p id="cd4c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">微调是迁移学习的下一部分。对于特定的任务，如文本分类或问答，您可以在小得多的数据集上执行增量训练。这将调整预训练模型的参数。</p><h2 id="647b" class="kx ky hi bd kz la lb lc ld le lf lg lh js li lj lk jw ll lm ln ka lo lp lq lr bi translated"><strong class="ak">用例</strong></h2><p id="99ef" class="pw-post-body-paragraph jj jk hi jl b jm ls ij jo jp lt im jr js lu ju jv jw lv jy jz ka lw kc kd ke hb bi translated">为了展示BERT的实际用途，我在下面提供了两个例子。GitHub和Google Colab中都提供了代码和文档。您可以使用这两个选项中的任何一个来跟随并亲自尝试一下！</p><ol class=""><li id="4579" class="kg kh hi jl b jm jn jp jq js ki jw kj ka kk ke kv km kn ko bi translated"><strong class="jl hj">文本分类或回归</strong></li></ol><p id="030b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这是tweets二进制分类的样本代码。这里我们有两种类型的推文，与灾难相关的推文(target = 1)和正常推文(target = 0)。我们微调了BERT基本模型，将推文分为这两类。</p><p id="3c7b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">GitHub:<a class="ae lx" href="https://github.com/sanigam/BERT_Medium" rel="noopener ugc nofollow" target="_blank">https://github.com/sanigam/BERT_Medium</a></p><p id="5556" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">Google Colab:<a class="ae lx" href="https://colab.research.google.com/drive/1ARH9dnugVuKjRTNorKIVrgRKitjg051c?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://Colab . research . Google . com/drive/1 arh 9 dnugvukjrtnorkivrgrkitjg 051 c？usp =分享</a></p><p id="d584" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">通过使用函数<em class="kf"> bert_model_creation() </em>中的适当参数值，该代码可用于多类分类或回归。代码提供了参数值的详细信息。如果您愿意，可以在此函数中添加额外的密集层。</p><p id="a4a2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 2。用于问答的BERT</strong></p><p id="09f6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这个<strong class="jl hj"> </strong>是BERT的另一个有趣的用例，在这里您向BERT模型中输入一段话和一个问题。它可以根据文章中给出的信息找到问题的答案。在这段代码中，我使用的是BERT大模型，该模型已经在斯坦福问答数据集(SQuAD)上进行了微调。你将看到如何使用这个微调的模型从给定的段落中得到答案。</p><p id="6759" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">GitHub:<a class="ae lx" href="https://github.com/sanigam/BERT_QA_Medium" rel="noopener ugc nofollow" target="_blank">https://github.com/sanigam/BERT_QA_Medium</a></p><p id="c530" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">Google Colab:<a class="ae lx" href="https://colab.research.google.com/drive/1ZpeVygQJW3O2Olg1kZuLnybxZMV1GpKK?usp=sharing" rel="noopener ugc nofollow" target="_blank">https://Colab . research . Google . com/drive/1 zpevygqjw 3 o 2 olg 1k zulnybxzmv 1 gpkk？usp =共享</a></p><p id="c15a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">此使用案例的示例:</p><p id="fb67" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><em class="kf">“约翰是一个10岁的男孩。他是罗伯特·史密斯的儿子。伊丽莎白·戴维斯是罗伯特的妻子。她在加州大学伯克利分校教书。索菲娅·史密斯是伊丽莎白的女儿。她就读于加州大学戴维斯分校"</em></p><p id="ac89" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">问题— <em class="kf">“约翰的妹妹上哪所大学？”</em></p><p id="ef94" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">当这两个输入被传入时，模型返回正确的答案<em class="kf">“UC Davis”</em></p><p id="a345" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这个例子证明了BERT可以理解语言结构，处理跨句子的依赖关系。它可以应用简单的逻辑来回答问题(例如，找出约翰的妹妹是谁)。请注意，你的文章可以比上面的例子长得多，但是问题和文章的总长度不能超过512个单词。如果你的文章超过这个长度，代码会自动截断多余的部分。</p><p id="447a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">除了上面显示的例子之外，代码还提供了一些例子，总共有3篇文章和22个问题。其中一段是我的伯特文章的一个版本。你会看到伯特问答能够回答任何能从文章中得到答案的问题。您可以为自己的问答应用程序定制代码。</p><p id="3396" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">希望这为您在自己的实际应用中使用BERT提供了一个良好的开端。如果您有任何问题或反馈，请随时告诉我！</p></div></div>    
</body>
</html>