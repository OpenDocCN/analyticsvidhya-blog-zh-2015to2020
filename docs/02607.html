<html>
<head>
<title>BERT in keras (tensorflow 2.0) using tfhub/huggingface</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">keras中的BERT(tensor flow 2.0)使用tfhub/huggingface</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bert-in-keras-tensorflow-2-0-using-tfhub-huggingface-81c08c5f81d8?source=collection_archive---------2-----------------------#2019-12-25">https://medium.com/analytics-vidhya/bert-in-keras-tensorflow-2-0-using-tfhub-huggingface-81c08c5f81d8?source=collection_archive---------2-----------------------#2019-12-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/3eae09172e7ae881569c4bec56ad0e01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6TMTbPVp5BnnDmu9NS2-uw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">(承蒙:杰伊·阿拉玛)</figcaption></figure><p id="9f93" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">最近，已经发布了相当多的深度信念网络或图形生成模型，如elmo、gpt、ulmo、bert等。这是自glove、fasttext、word2vec形式的预训练嵌入出现以来的一个重大突破。虽然单词嵌入有助于创建文本序列的密集表示，但这些信念网络使之成为可能</p><ul class=""><li id="b0e2" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">为了克服nlp任务缺乏训练数据的挑战</li><li id="5015" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">微调各种nlp任务，如实体识别模型、情感分析、问题回答。</li></ul><h1 id="bc56" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">为什么是伯特？</h1><figure class="lf lg lh li fd ij er es paragraph-image"><div class="er es le"><img src="../Images/c4e363597487b6f6ea765783cc0a3a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*IOskqRtq3UOjvchtFxe-AA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">伯特是深度双向的，开放的GPT是单向的，埃尔莫是浅双向的。</figcaption></figure><p id="da7d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">BERT是第一个<em class="lj">深度双向</em>、<em class="lj">无监督</em>语言表示，仅使用纯文本语料库进行预训练。为什么这很重要？两个原因:</p><ul class=""><li id="ea0c" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">具有多重表征的词的上下文表征，如多义词的情况。以鼹鼠为例——a .它是一种动物，b .它是一个间谍。其他模型是上下文无关的，也就是说，在两种上下文中，它们将返回相同的嵌入。</li><li id="e6a7" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">完全连接的双向模型——不可能通过简单地根据每个单词的前一个<em class="lj">和后一个</em>单词来训练双向模型，因为这将允许被预测的单词在多层模型中间接“看到自己”。为了解决这个问题，BERT使用了一种简单的技术来屏蔽输入中的一些单词，然后双向调节每个单词来预测被屏蔽的单词。</li></ul><p id="8d0d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">关于BERT的详细了解，可以直接从<a class="ae lk" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank"> google-ai </a>和<a class="ae lk" href="http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/" rel="noopener ugc nofollow" target="_blank">这里</a>阅读。</p><h1 id="c135" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">BERT的实现</h1><p id="f00a" class="pw-post-body-paragraph iu iv hi iw b ix ll iz ja jb lm jd je jf ln jh ji jj lo jl jm jn lp jp jq jr hb bi translated">如果你喜欢直接行动起来，不再痛苦，这里有一款<a class="ae lk" href="https://colab.research.google.com/drive/1IubZ3T7gqD09ZIVmJapiB5MXUnVGlzwH#scrollTo=7LbVjtktg3Ln" rel="noopener ugc nofollow" target="_blank"> colab笔记本</a>可以开始使用。你最头疼的问题来自于将你的文本特征转换成上面的格式。这个<em class="lj"> _get_inputs </em>函数将帮助您做到这一点。它可以在colab笔记本中找到。</p><p id="524e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">实施通常需要两个步骤:</p><ul class=""><li id="3cb2" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">按照bert的要求获取输入，即输入id、输入掩码和输入段。这是我通过创建一个名为<em class="lj"> _get_inputs </em>的函数实现的</li><li id="558a" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">将预训练的bert模型作为一个层添加到您自己的模型中</li></ul><p id="b017" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">第一次看这些输入可能会感到困惑。这可能是一个简单的解释。给出一个类似“我想嵌入。也许现在”。伯特·托肯泽会把它转换成</p><p id="e41b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">["[CLS]，"我"，"想要"，"要"，" em "，"床# "，"[SEP]，"可能"，"现在"，" SEP]"]。</p><p id="46b6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里‘CLS’和‘SEP’是保留字，用来标记句子或序列的分隔。在文本上使用convert_to_ids，我们可以得到<em class="lj"> input_ids </em>。假设我们固定15个单词作为标准形状。上面的序列只包含10。剩下的5个单词已经被填充。<em class="lj"> input_masks </em>就是这些填充的单元格。<em class="lj">输入_段</em>代表分离。第一句将被标记为“0”，第二句将被标记为“1”。给定数据集和要使用的标记器，所有这些都在get inputs函数中执行。我使用过Google Q &amp;上的数据集，这是一个来自Kaggle在colab的竞赛(关于如何获得数据集。可以在这里勾选<a class="ae lk" rel="noopener" href="/@muralim1585/kaggle-colab-link-download-analyze-d25da2f618e9"/>。</p><p id="4fe5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">以下是实现keras模型的片段</p><h2 id="1e4d" class="lq kh hi bd ki lr ls lt km lu lv lw kq jf lx ly ku jj lz ma ky jn mb mc lc md bi translated">使用TFhub</h2><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/5bf128eac36abd810bd923b8aa38c164.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*45GsUSVnWKNYTaGoDXFgjA.png"/></div></div></figure><p id="cc43" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于tf 2.0，hub.module()将不起作用。我们需要使用hub.keraslayer。这是用于版本上的互联网。要关闭互联网，请使用hub.load —查看tfhub 中的<a class="ae lk" href="https://www.tensorflow.org/hub/common_issues" rel="noopener ugc nofollow" target="_blank">常见问题</a></p><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/937ac915371a9b86c7729f955462a857.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ZXojnfvgtZWHMs8WCTE-w.png"/></div></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/3ac01266dbb1773226bacae198464103.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*svunNjU1rRj-NGcxnhjzeA.png"/></div></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/fb505857e625633677c9822aa6402dd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HBNUPF5zLeeUp3sYImsJmg.png"/></div></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/56c09a7ecec26f2ab26c03fb96211d14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vpTF2ZRb9C6t0PBvv3WZ_Q.png"/></div></div></figure><h2 id="5443" class="lq kh hi bd ki lr ls lt km lu lv lw kq jf lx ly ku jj lz ma ky jn mb mc lc md bi translated">使用拥抱脸/变形金刚</h2><p id="4072" class="pw-post-body-paragraph iu iv hi iw b ix ll iz ja jb lm jd je jf ln jh ji jj lo jl jm jn lp jp jq jr hb bi translated">HuggingFace是一家初创公司，它创建了一个“变形金刚”包，通过这个包，我们可以在许多预先训练好的模型之间无缝切换，而且我们还可以在pytorch和keras之间移动。查看<a class="ae lk" href="https://github.com/huggingface/transformers" rel="noopener ugc nofollow" target="_blank">这里</a>了解更多关于这个令人敬畏的创业公司的信息</p><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es me"><img src="../Images/282e8a60c5637e877d423c296cd5f95e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AXEqe68xMg8Z0FkgnGoTBg.png"/></div></div></figure><p id="9aca" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">嵌入层也差不多。神奇的是' TFBertModel '模块从变形金刚包。事实上，在不同型号之间切换非常容易。例如，要获取“roberta”，只需访问“TFRoberataModel”。型号名称中的TF表示TF 2.0兼容性。</p><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mj"><img src="../Images/6eec42dfd2b46d579fa8750b426c5fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZSzp3kwFTRZqprJvZWauRQ.png"/></div></div></figure><figure class="lf lg lh li fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/745c1e2d62591988037e6f2ec2561bcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5rE5WvY4zVeHn1Spp8pfAg.png"/></div></div></figure><h2 id="1f4e" class="lq kh hi bd ki lr ls lt km lu lv lw kq jf lx ly ku jj lz ma ky jn mb mc lc md bi translated">常见问题或错误</h2><ul class=""><li id="f16e" class="js jt hi iw b ix ll jb lm jf ml jj mm jn mn jr jx jy jz ka bi translated">Bert要求输入张量为“int32”。请注意输入图层的dtype是如何标记为“int32”的。</li><li id="52e2" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">在序列输出的情况下，Bert输出3D阵列，在混合输出的情况下，输出1D阵列。您将需要使用一个层来将其转换成您想要的输出。在这里，我使用了GlobalAveragePooling1D将其转换为1D输出数组。</li><li id="6877" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">如果您使用tfhub来实现bert，其中一些将与tf2不兼容。只选择那些有清晰的文档说明如何使用的产品，如示例中所示的产品。</li><li id="fb67" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">人们很难在keras中确定数据集的输入形状。简单的方法是排列[句子/批量_大小，字数，嵌入_尺寸]。这里，embed_dim是每个单词通过不同嵌入的维度输出。伯特的例子是768。</li></ul><h2 id="ed92" class="lq kh hi bd ki lr ls lt km lu lv lw kq jf lx ly ku jj lz ma ky jn mb mc lc md bi translated">其他有用的链接:</h2><p id="fad6" class="pw-post-body-paragraph iu iv hi iw b ix ll iz ja jb lm jd je jf ln jh ji jj lo jl jm jn lp jp jq jr hb bi translated"><a class="ae lk" href="https://www.kaggle.com/maroberti/fastai-with-transformers-bert-roberta" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/maroberti/fastai-with-transformers-Bert-Roberta</a><br/><a class="ae lk" href="https://www.kaggle.com/mobassir/jigsaw-google-q-a-eda/notebook" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/mobas sir/jigsaw-Google-q-a-EDA/notebook</a><br/><a class="ae lk" href="https://analyticsindiamag.com/bert-classifier-with-tensorflow-2-0/" rel="noopener ugc nofollow" target="_blank">https://analyticsindiamag . com/Bert-classifier-with-tensor flow-2-0/</a><br/><a class="ae lk" href="https://analyticsindiamag.com/step-by-step-guide-to-implement-multi-class-classification-with-bert-tensorflow/" rel="noopener ugc nofollow" target="_blank">https://analyticsindiamag . com/step-by-step-guide-to</a></p><p id="4a74" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">伯特解释</p><p id="710c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae lk" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">http://jalammar.github.io/illustrated-transformer/</a><br/><a class="ae lk" href="https://towardsdatascience.com/how-bert-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5bbee1b6dbdb" rel="noopener" target="_blank">https://towards data science . com/how-BERT-leverage-attention-mechanism-and-transformer-to-learn-word-contextual-relations-5b bee 1 b 6 dbdb</a><br/><a class="ae lk" href="https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/" rel="noopener ugc nofollow" target="_blank">https://mccormickml . com/2019/05/14/BERT-word-embeddings-tutorial/</a></p></div></div>    
</body>
</html>