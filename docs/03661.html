<html>
<head>
<title>Credit Risk and Machine Learning Concepts -6</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">信用风险和机器学习概念-6</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/credit-risk-and-machine-learning-concepts-6-15adee7c0454?source=collection_archive---------22-----------------------#2020-02-12">https://medium.com/analytics-vidhya/credit-risk-and-machine-learning-concepts-6-15adee7c0454?source=collection_archive---------22-----------------------#2020-02-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="481a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个领域的机器学习组件是什么？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/7f9b97ac9aa06f61214f193bda3e1b74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*hfY6AAQIgaevv5_tsjRa7g.png"/></div></figure><p id="3d3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设计AI/ML解决方案的标准方法通常包括创建决策树，该决策树作为设计思维行为驱动的开发的结果，映射出功能和动作序列。这不是序列图的层次，而是流程和决策流的映射，否则人会这样做，但规则是直接编码的，以使自动化流程具有额外的洞察力，并且能够在几乎即时的时间内一次考虑比人类多得多的数据。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jl"><img src="../Images/7cb809bc7e6898752c996395f766d2d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*jYNbuZobheKwiibk1DSMZA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">决策树的示例</figcaption></figure><p id="e95a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面介绍的模型标志着风险建模实践的巨大飞跃。之前提出的所有方法都预先假定模型的参数独立于观察数据集<em class="jq"> D </em>，因此<em class="jq"> PD = p(x|⊖，D) = P(x|⊖) </em></p><p id="bcb3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">非参数模型释放这个假设，这样<em class="jq"> ⊖ </em> = <em class="jq"> f </em> ( <em class="jq"> D </em>)。模型的解释力，即<em class="jq"> ⊖ </em>能够捕捉到的关于数据的信息量，也取决于<em class="jq"> D </em>的基数，即数据越多，模型越精确。</p><p id="7ef0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策/分类树由一系列基于协变量向量的条件是/否子句组成，用于对客户进行分组。因此，神经网络的作用是基于多个变量快速地将一个实体与已经失败或无法偿还债务的其他实体一起分类。</p><p id="884e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正在建模的新方法采用查看净营运营运资本(NOWC)和一些其他趋势的方法，以确定支付能力，以及可用的支付历史、行业趋势、经济趋势、法院诉讼和持续经营的验证。NOWC是一个衡量公司用营运资产偿还所有营运负债的能力的比率。这是一个重要的指标，因为它显示了公司的杠杆和流动资产的数量。</p><p id="7e6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它还显示了一家公司如何利用其资源运营，以及该公司如何有效地适应意外事件和新机遇。这在等式本身中是显而易见的。</p><p id="ad21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">净营运营运资本公式的计算方法是从营运资产中减去营运负债，如下所示:</p><p id="b917" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现金+应收账款+存货—应付账款+预提费用或流动经营资产—流动经营负债。</p><p id="7676" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这一指标更多地与现金流而不是净营运资本计算联系在一起，因为NWC包括所有的流动资产和流动负债。正因为如此，NOWC经常被用来计算自由现金流量。比如说。：</p><p id="10db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设Bob的运输和供应公司的资产负债表上有以下资产:</p><ul class=""><li id="0378" class="jr js hi ih b ii ij im in iq jt iu ju iy jv jc jw jx jy jz bi translated">现金:10万美元</li><li id="9699" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">应收账款:20 000美元</li><li id="2b12" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">库存/修复:50万美元</li><li id="9239" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">应付账款:300 000美元</li><li id="db08" class="jr js hi ih b ii ka im kb iq kc iu kd iy ke jc jw jx jy jz bi translated">应计费用:100 000美元</li></ul><p id="665b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">鲍勃将按如下方式计算他的NOWC:</p><p id="ad20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">$100,000 + $20,000 + $500,000 — $300,000 — $100,000 = $220,000</p><p id="7840" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这意味着鲍勃可以只用他的一部分流动资产来偿还他所有的流动负债。因此，如果他的卖主或债权人要求同时偿还他的所有债务，他将能够还清这些债务，并且仍然有大量的流动资产来经营企业。</p><p id="8e16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">净营运资本(NOWC)是营运流动资产超过营运流动负债的部分。在大多数情况下，它等于现金加应收账款加存货减应付账款减应计费用。</p><p id="9b06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">经营性流动资产是指(a)支持业务运营所需的资产，以及(b)预计在未来12个月内转换为现金的资产。它们不包括当前的金融投资。</p><p id="2ecd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">经营性流动负债是指(a)为开展业务而承担的负债，以及(b)预计在未来12个月内偿还的负债。它们不包括任何流动贷款或有息负债。</p><p id="255a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">净营运资本不同于(净)营运资本，后者等于流动资产减去流动负债。NOWC是计算自由现金流的中间输入。自由现金流等于经营现金流减去经营资产总投资减去净营运资本投资。</p><p id="d2c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">决策分类</strong></p><p id="623c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">决策树或分类树由一系列基于协变量向量的条件子句(通常是/否子句)组成，这些子句将实体分类成组。以二元分类树为例，如“规模”,包括报告的全职员工人数和趋势、企业年龄、总净值减去无形资产和趋势、包括争议和破产保护或重组申请在内的任何县判决、包括主要高管和任何负面犯罪活动和法庭案件在内的贸易和新闻报道情绪分析。</p><p id="344b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如在决定水果标识的图中一样，每个节点将客户集分成不同的子集，直到到达结束节点。客户被细分为许多类别，必须给这些类别分配一个违约概率(PD)。这不是简单地给一个客户打分，而是根据观察到的数据对“可能违约”的分类进行调整和再调整。因此，这不允许对同一类别的客户进行区别对待。一个较小的缺点是，由于该模型不是基于任何统计假设，因此无法评估具有统计相关性的框架的稳定性。因此，稳健性与训练样本的良好性相关联。该模型有助于构建变量之间潜在的复杂或非线性关系:例如，协变量可能只在树的某个节点上相关，并且只与特定的客户子集相关。能够清晰地模拟协变量之间的相互作用，这是决策树的另一个优势，不幸的是，如果变量之间的相互作用是先验已知的，这将变得特别有用。</p><p id="b758" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">神经网络</strong></p><p id="2ae2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">神经网络得名于这样一个事实，即它们天真地似乎模拟了大脑的工作方式。神经网络更正式的定义是多层感知器。感知器(见下图)是系统的基本单元，由<em class="jq"> n </em>个轴突和一个节点构成，分别代表<em class="jq"> n </em>个权重和一个基本运算，它接受输入并产生加权输入运算的输出。从每个节点输出的值通过平滑函数进行“过滤”,该函数重新调整输出的比例，以使结果不会在多个层之间发散，从而使处理单元负担过重。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kf"><img src="../Images/0d239a37f59e13c35ea3369c307dbc08.png" data-original-src="https://miro.medium.com/v2/resize:fit:674/format:webp/1*rjpPP2_FrVAqs7thx8OozA.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">感知器的简单例子</figcaption></figure><p id="1d50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">事实上，它们可以通过协变量<em class="jq"> x </em>的多个向量的一系列级联矩阵乘法来描述。模型的输出可以被称为PD，通过层中的一系列中间节点(连接的一系列感知器)与输入相关，这些节点接收向量<em class="jq"> x </em>或其他节点的输出作为输入，并依次向一个或多个下游节点或最终输出输出值。没有对网络内关系的结构进行初步假设。事实上，从初始状态开始，网络可以用违约/未违约公司的不同样本进行“训练”。基本上，训练是一个称为反向传播的迭代过程，通过该过程，一种算法评估节点连接的最佳权重，比较网络当前状态的结果。</p><p id="3c8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">建议对净营运资本(NOWC)法进行修改。这减少了各种信用评级机构使用的多达24个特征。对一组最初移动的4年财务数据进行趋势评估。员工数量、收入、规模和经营年限将是评级评估的初始限定条件，否则，如果未达到阈值，信用咨询状态将在完成评级后返回，或者如果公司不再活跃(处于清算、解散、破产状态)，则信用受到质疑。财务数据的及时性因素“新鲜度”随着其变得越来越陈旧，会对信用风险因素产生负面影响。</p><p id="6cc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">部门评估和一般商业状况将是下一次评估。这被建模为神经网络模型的多层感知器结构，类似于下面的框架:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/95031d999fcb53d5ecebf2af1c9ba59e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*1PqhHyA-XhJjSmXPOT0pmQ.png"/></div></figure><p id="5fef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">蓝线表示沿着网络传播输入的前馈流，而橙色箭头表示路径反向传播过程。</p><p id="975d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后给出了目前正在开发中的这种神经网络的一些R代码。2017年米兰理工大学的加布里埃尔·博诺米·博塞吉亚撰写的基于供应链的信誉论文作为理学硕士的重要内容。一些其他内容也来源于该参考文献。</p><p id="8aca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我关于信用风险和机器学习的博客的第六部分。下一部分将考虑失败和失败公司的剖析，以及NOWC神经网络感知器方法可能已经表明了什么，可通过以下链接获得:</p><p id="3d98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae kg" rel="noopener" href="/@geoff.leigh19/credit-risk-and-machine-learning-concepts-7-fdc9eb8dcd14?source=friends_link&amp;sk=f3142319d5eb06540512264f587623e4">信用风险和机器学习概念-7 </a></p><p id="6598" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">前5期可以在这里找到:</p><p id="cc63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae kg" rel="noopener" href="/@geoff.leigh19/credit-risk-and-machine-learning-concepts-85ef47c978c7?source=friends_link&amp;sk=5249acc679330bd64c76bcae1dc074d1">https://medium . com/@ Geoff . Leigh 19/credit-risk-and-machine-learning-concepts-85ef 47 c 978 c 7？source = friends _ link&amp;sk = 5249 ACC 679330 BD 64 c 76 bcae 1 DC 074d 1</a></p><p id="d76c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae kg" rel="noopener" href="/@geoff.leigh19/credit-risk-and-machine-learning-concepts-2-fc37e1a05183?sk=94ef606e1c60e2cf1522b9c38a5e144e">https://medium . com/@ Geoff . Leigh 19/credit-risk-and-machine-learning-concepts-2-fc 37 E1 a 05183？sk = 94ef 606 E1 c 60 e2cf 1522 b 9 c 38 a5 e 144 e</a></p><p id="072a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae kg" rel="noopener" href="/analytics-vidhya/credit-risk-and-machine-learning-concepts-3-d2bb2f39d843">https://medium . com/analytics-vid hya/credit-risk-and-machine-learning-concepts-3-D2 bb 2 f 39d 843</a></p><p id="969e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae kg" rel="noopener" href="/analytics-vidhya/credit-risk-and-machine-learning-concepts-4-3c44b479a3d1?source=friends_link&amp;sk=cf6fe8b0a96d01c68971f72cbc179229">https://medium . com/analytics-vid hya/credit-risk-and-machine-learning-concepts-4-3c 44 b 479 a3 d 1？source = friends _ link&amp;sk = cf 6 Fe 8 b 0 a 96d 01 c 68971 f 72 bcc 179229</a></p><p id="b816" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae kg" rel="noopener" href="/analytics-vidhya/credit-risk-and-machine-learning-concepts-5-88f2dc1e18e2?source=friends_link&amp;sk=2a4015bc86ee6071716865356ffb1a0d">https://medium . com/analytics-vid hya/credit-risk-and-machine-learning-concepts-5-88 F2 DC 1e 18 e 2？source = friends _ link&amp;sk = 2a 4015 BC 86 ee 6071716865356 ffb1 a0d</a></p><p id="d3b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">示例R代码片段:</p><pre class="je jf jg jh fd kh ki kj kk aw kl bi"><span id="60d7" class="km kn hi ki b fi ko kp l kq kr">&gt;</span><span id="1032" class="km kn hi ki b fi ks kp l kq kr">&gt; ####</span><span id="6025" class="km kn hi ki b fi ks kp l kq kr">&gt; #</span><span id="fb82" class="km kn hi ki b fi ks kp l kq kr">&gt; # load library</span><span id="6e43" class="km kn hi ki b fi ks kp l kq kr">&gt; #</span><span id="2488" class="km kn hi ki b fi ks kp l kq kr">&gt; ####</span><span id="fbf3" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="6001" class="km kn hi ki b fi ks kp l kq kr">&gt; library ( tensorflow )</span><span id="c8c5" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="e096" class="km kn hi ki b fi ks kp l kq kr">&gt; ####</span><span id="9081" class="km kn hi ki b fi ks kp l kq kr">&gt; #</span><span id="13a8" class="km kn hi ki b fi ks kp l kq kr">&gt; # Import data</span><span id="8f1b" class="km kn hi ki b fi ks kp l kq kr">&gt; #</span><span id="3c5e" class="km kn hi ki b fi ks kp l kq kr">&gt; ####</span><span id="942f" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="16f8" class="km kn hi ki b fi ks kp l kq kr">&gt; companies &lt;- read.csv (“ companies _ large _ norm.csv “,header =T)</span><span id="25da" class="km kn hi ki b fi ks kp l kq kr">&gt; companies &lt;- companies [,c (1 ,2 ,8:19) ]</span><span id="86e8" class="km kn hi ki b fi ks kp l kq kr">&gt; active &lt;- companies [ which ( companies $ Active == 1) ,]</span><span id="b13a" class="km kn hi ki b fi ks kp l kq kr">&gt; default &lt;- companies [ which ( companies $ Default == 1) ,]</span><span id="996b" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="a0b9" class="km kn hi ki b fi ks kp l kq kr">&gt; ### SAMPLE 5000 active and 5000 default for testing purposes</span><span id="4e05" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="62e1" class="km kn hi ki b fi ks kp l kq kr">&gt; smp _act &lt;- sample ( nrow ( active ), size = 5000)</span><span id="2cda" class="km kn hi ki b fi ks kp l kq kr">&gt; smp _def &lt;- sample ( nrow ( default ), size = 5000)</span><span id="7556" class="km kn hi ki b fi ks kp l kq kr">&gt; test &lt;- rbind ( active [smp _act ,], default [smp_def ,])</span><span id="af10" class="km kn hi ki b fi ks kp l kq kr">&gt; test &lt;- test [ sample ( nrow ( test )) ,]</span><span id="0bac" class="km kn hi ki b fi ks kp l kq kr">&gt; train &lt;- rbind ( active [-smp _act , ], default [-smp_def , ])</span><span id="dc29" class="km kn hi ki b fi ks kp l kq kr">&gt; train &lt;- train [ sample ( nrow ( train )) ,]</span><span id="e29d" class="km kn hi ki b fi ks kp l kq kr">&gt; train _ active &lt;- active [-smp _act ,]</span><span id="476a" class="km kn hi ki b fi ks kp l kq kr">&gt; train _ default &lt;- default [-smp_def ,]</span><span id="4586" class="km kn hi ki b fi ks kp l kq kr">&gt; test _ values &lt;- as.matrix ( test [ ,3: ncol ( test )])</span><span id="564f" class="km kn hi ki b fi ks kp l kq kr">&gt; test _ labels &lt;- as.matrix ( test [ ,1:2])</span><span id="c7dd" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="95af" class="km kn hi ki b fi ks kp l kq kr">&gt; # ####</span><span id="e2cc" class="km kn hi ki b fi ks kp l kq kr">&gt; #</span><span id="0a39" class="km kn hi ki b fi ks kp l kq kr">&gt; # build the mlayer perceptron</span><span id="6737" class="km kn hi ki b fi ks kp l kq kr">xxix</span><span id="61ec" class="km kn hi ki b fi ks kp l kq kr">xxx APPENDIX I. SCRIPTS</span><span id="8a40" class="km kn hi ki b fi ks kp l kq kr">&gt; #</span><span id="6c27" class="km kn hi ki b fi ks kp l kq kr">&gt; # ####</span><span id="b80d" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="d93b" class="km kn hi ki b fi ks kp l kq kr">&gt; n_ classes &lt;- 2L</span><span id="657a" class="km kn hi ki b fi ks kp l kq kr">&gt; n_ nodes _l1 &lt;- 50L</span><span id="e50e" class="km kn hi ki b fi ks kp l kq kr">&gt; n_ nodes _l2 &lt;- 25L</span><span id="60c5" class="km kn hi ki b fi ks kp l kq kr">&gt; keep _ prob = 0.97</span><span id="fa9c" class="km kn hi ki b fi ks kp l kq kr">&gt; x_ length &lt;- as.integer ( ncol ( companies ) -2)</span><span id="85c0" class="km kn hi ki b fi ks kp l kq kr">&gt; ### Initialize weigths and biases with values</span><span id="fff8" class="km kn hi ki b fi ks kp l kq kr">&gt; from truncated std. normal</span><span id="ec55" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="86ae" class="km kn hi ki b fi ks kp l kq kr">&gt; weight _ variable &lt;- function ( shape ) {</span><span id="2ca5" class="km kn hi ki b fi ks kp l kq kr">+ initial &lt;- tf$ truncated _ normal (shape , stddev = 0.1)</span><span id="49bc" class="km kn hi ki b fi ks kp l kq kr">+ tf$ Variable ( initial )</span><span id="7482" class="km kn hi ki b fi ks kp l kq kr">+ }</span><span id="1b70" class="km kn hi ki b fi ks kp l kq kr">&gt; bias _ variable &lt;- function ( shape ) {</span><span id="5027" class="km kn hi ki b fi ks kp l kq kr">+ initial &lt;- tf$ constant (0.1 , shape = shape )</span><span id="5969" class="km kn hi ki b fi ks kp l kq kr">+ tf$ Variable ( initial )</span><span id="14dc" class="km kn hi ki b fi ks kp l kq kr">+ }</span><span id="23e3" class="km kn hi ki b fi ks kp l kq kr">&gt; ### define connections within the network</span><span id="ed5f" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="1470" class="km kn hi ki b fi ks kp l kq kr">&gt; x &lt;- tf$ placeholder (tf$ float32 , shape (NULL , x_ length ))</span><span id="22cb" class="km kn hi ki b fi ks kp l kq kr">&gt; hl1 _W &lt;- tf$ Variable (tf$ truncated _ normal ( shape (x_length , n_ nodes _l1),</span><span id="3c57" class="km kn hi ki b fi ks kp l kq kr">+ stddev + name &gt; hl1 _b &lt;- tf$ Variable (tf$ zeros ( shape (n_ nodes _l1)), name = “B_hl_1”)</span><span id="9117" class="km kn hi ki b fi ks kp l kq kr">&gt; hl2 _W &lt;- tf$ Variable (tf$ truncated _ normal ( shape (n_ nodes _l1 , n_ nodes _l2),</span><span id="f72c" class="km kn hi ki b fi ks kp l kq kr">+ stddev + name &gt; hl2 _b &lt;- tf$ Variable (tf$ zeros ( shape (n_ nodes _l2)), name = “B_hl_2”)</span><span id="3b37" class="km kn hi ki b fi ks kp l kq kr">&gt; out _W &lt;- tf$ Variable (tf$ truncated _ normal ( shape (n_ nodes _l1 , n_ classes ),</span><span id="01ac" class="km kn hi ki b fi ks kp l kq kr">+ stddev + name &gt; out _b &lt;- tf$ Variable (tf$ zeros ( shape (n_ classes )), name = “B_ outupt _ layer “)</span><span id="5306" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="6d4f" class="km kn hi ki b fi ks kp l kq kr">&gt; ### define activation and droupout function</span><span id="fbb9" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="1313" class="km kn hi ki b fi ks kp l kq kr">&gt; l1 &lt;- tf$ add(tf$ matmul (x,hl1 _W),hl1 _b)</span><span id="f249" class="km kn hi ki b fi ks kp l kq kr">&gt; l1 &lt;- tf$nn$ relu (l1)</span><span id="fb29" class="km kn hi ki b fi ks kp l kq kr">&gt; drop1 &lt;- tf$nn$ dropout (l1 , keep _ prob )</span><span id="9174" class="km kn hi ki b fi ks kp l kq kr">&gt; l2 &lt;- tf$ add(tf$ matmul (drop1 ,hl2 _W),hl2 _b)</span><span id="4568" class="km kn hi ki b fi ks kp l kq kr">&gt; l2 &lt;- tf$nn$ relu (l2)</span><span id="e136" class="km kn hi ki b fi ks kp l kq kr">&gt; drop2 &lt;- tf$nn$ dropout (drop1 , keep _ prob )</span><span id="4bf9" class="km kn hi ki b fi ks kp l kq kr">&gt; out &lt;- tf$ add (tf$ matmul (l1 ,out _W),out _b)</span><span id="cfc9" class="km kn hi ki b fi ks kp l kq kr">&gt; y &lt;- tf$nn$ softmax (out )</span><span id="b32a" class="km kn hi ki b fi ks kp l kq kr">&gt; ### placeholder for output variable</span><span id="2653" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="0fa6" class="km kn hi ki b fi ks kp l kq kr">&gt; y_ &lt;- tf$ placeholder (tf$ float32 , shape (NULL , n_ classes ))</span><span id="7d83" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="421c" class="km kn hi ki b fi ks kp l kq kr">&gt; ### error function</span><span id="f24c" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="9a76" class="km kn hi ki b fi ks kp l kq kr">&gt; cross _ entropy &lt;- tf$ reduce _ mean (-tf$ reduce _sum (y_ * tf$ log(y),</span><span id="fd1a" class="km kn hi ki b fi ks kp l kq kr">xxxi</span><span id="50e5" class="km kn hi ki b fi ks kp l kq kr">+ &gt;</span><span id="eb29" class="km kn hi ki b fi ks kp l kq kr">&gt; ### declaration of the backpropagation</span><span id="15ab" class="km kn hi ki b fi ks kp l kq kr">&gt; ### optimization algorithm ( ADaptive Moment estimation )</span><span id="aaf8" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="a66f" class="km kn hi ki b fi ks kp l kq kr">&gt; optimizer &lt;- tf$ train $ AdamOptimizer ()</span><span id="bd2f" class="km kn hi ki b fi ks kp l kq kr">&gt; train _ step &lt;- optimizer $ minimize ( cross _ entropy )</span><span id="8989" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="3b8e" class="km kn hi ki b fi ks kp l kq kr">&gt; ### store and reload partially trained model if needed</span><span id="9dd6" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="3a1e" class="km kn hi ki b fi ks kp l kq kr">&gt; # do not run first time</span><span id="fbc0" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="99fb" class="km kn hi ki b fi ks kp l kq kr">&gt; # loader = tf$ train $ import _ meta _ graph (“ folder “)</span><span id="26a5" class="km kn hi ki b fi ks kp l kq kr">&gt; # loader $ restore (sess , tf$ train $ latest _ checkpoint (“ folder “))</span><span id="253f" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="fe11" class="km kn hi ki b fi ks kp l kq kr">&gt; # saver $ restore (sess , “ folder “)</span><span id="deb5" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="15e2" class="km kn hi ki b fi ks kp l kq kr">&gt; ### do not run when loading</span><span id="e698" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="84b7" class="km kn hi ki b fi ks kp l kq kr">&gt; init &lt;- tf$ global _ variables _ initializer ()</span><span id="7316" class="km kn hi ki b fi ks kp l kq kr">&gt; sess &lt;- tf$ Session ()</span><span id="0366" class="km kn hi ki b fi ks kp l kq kr">&gt; sess $ run( init )</span><span id="5187" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="2a35" class="km kn hi ki b fi ks kp l kq kr">&gt; ### Define accuracy metrics</span><span id="2061" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="1405" class="km kn hi ki b fi ks kp l kq kr">&gt; correct _ prediction &lt;- tf$ equal (tf$ argmax (y, 1L), tf$ argmax (y_, 1L))</span><span id="bcc4" class="km kn hi ki b fi ks kp l kq kr">&gt; accuracy &lt;- tf$ reduce _ mean (tf$ cast ( correct _ prediction , tf$ float32 ))</span><span id="d720" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="1a73" class="km kn hi ki b fi ks kp l kq kr">&gt; ### Define summary statistics to be monitor training</span><span id="594c" class="km kn hi ki b fi ks kp l kq kr">&gt; ### ( accuracy and and weigths )</span><span id="59fd" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="4714" class="km kn hi ki b fi ks kp l kq kr">&gt; summary &lt;- tf$ summary $ scalar (“ accuracy “, accuracy )</span><span id="b980" class="km kn hi ki b fi ks kp l kq kr">&gt; summary _ CrossEntropy &lt;- tf$ summary $ scalar (“ cross entropy “,</span><span id="0b18" class="km kn hi ki b fi ks kp l kq kr">+ &gt; summary _hl1 _W &lt;- tf$ summary $ histogram (“ weights1 “,hl1_W)</span><span id="101f" class="km kn hi ki b fi ks kp l kq kr">&gt; summary _hl2 _W &lt;- tf$ summary $ histogram (“ weights2 “,hl2_W)</span><span id="1784" class="km kn hi ki b fi ks kp l kq kr">&gt; summary _ output _ layer _W &lt;- tf$ summary $ histogram (“ weightsOut “,out_W)</span><span id="a09f" class="km kn hi ki b fi ks kp l kq kr">&gt; summary _w &lt;- c( summary _hl1_W,</span><span id="f6f2" class="km kn hi ki b fi ks kp l kq kr">+ summary _hl2_W,</span><span id="34df" class="km kn hi ki b fi ks kp l kq kr">+ summary _ output _ layer _W</span><span id="2c7c" class="km kn hi ki b fi ks kp l kq kr">+ )</span><span id="92e2" class="km kn hi ki b fi ks kp l kq kr">&gt; summary _ weights &lt;- tf$ summary $ merge ( summary _w)</span><span id="9133" class="km kn hi ki b fi ks kp l kq kr">&gt; log _ writer &lt;- tf$ summary $ FileWriter ( paste0 (“ folder “,j))</span><span id="caf6" class="km kn hi ki b fi ks kp l kq kr">&gt; saver &lt;- tf$ train $ Saver ()</span><span id="99e4" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="d7e6" class="km kn hi ki b fi ks kp l kq kr">&gt; ####</span><span id="f62a" class="km kn hi ki b fi ks kp l kq kr">&gt; #</span><span id="c54b" class="km kn hi ki b fi ks kp l kq kr">&gt; # Train the model</span><span id="6066" class="km kn hi ki b fi ks kp l kq kr">&gt; #</span><span id="1d41" class="km kn hi ki b fi ks kp l kq kr">&gt; ####</span><span id="c63a" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="90cb" class="km kn hi ki b fi ks kp l kq kr">&gt; for (i in 1:5000) {</span><span id="d176" class="km kn hi ki b fi ks kp l kq kr">xxxii APPENDIX I. SCRIPTS</span><span id="67ac" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="7177" class="km kn hi ki b fi ks kp l kq kr">+ ### random selection of the training batch</span><span id="0e4f" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="c1c4" class="km kn hi ki b fi ks kp l kq kr">+ train &lt;- rbind ( train _ default ,</span><span id="4558" class="km kn hi ki b fi ks kp l kq kr">+ train _ active [ sample ( nrow ( train _ default )) ,])</span><span id="9dbd" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="34c7" class="km kn hi ki b fi ks kp l kq kr">+ ### shuffle</span><span id="8e9f" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="0328" class="km kn hi ki b fi ks kp l kq kr">+ train &lt;- train [ sample ( nrow ( train )) ,]</span><span id="10bd" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="28d9" class="km kn hi ki b fi ks kp l kq kr">+ ### separate input (col 1 and 2) from labels ( remaining columns )</span><span id="b910" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="ea7b" class="km kn hi ki b fi ks kp l kq kr">+ train _ values &lt;- as.matrix ( train [ ,3: ncol ( train )])</span><span id="797d" class="km kn hi ki b fi ks kp l kq kr">+ train _ labels &lt;- as.matrix ( train [ ,1:2])</span><span id="ab91" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="6eaa" class="km kn hi ki b fi ks kp l kq kr">+ batch _xs &lt;- train _ values</span><span id="e14b" class="km kn hi ki b fi ks kp l kq kr">+ batch _ys &lt;- train _ labels</span><span id="6454" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="4b19" class="km kn hi ki b fi ks kp l kq kr">+ sess $ run( train _step ,</span><span id="f00a" class="km kn hi ki b fi ks kp l kq kr">+ feed _ dict = dict (x = batch _xs , y_ = batch _ys))</span><span id="021e" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="7995" class="km kn hi ki b fi ks kp l kq kr">+ ### save accuracy on testing sample each 10 iteration</span><span id="fa98" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="2e6a" class="km kn hi ki b fi ks kp l kq kr">+ if(i %% 10 == 0){</span><span id="563a" class="km kn hi ki b fi ks kp l kq kr">+ accuracy = sess $ run( summary ,</span><span id="8f12" class="km kn hi ki b fi ks kp l kq kr">+ feed _ dict = dict (x = test _ values , y_ = test _ labels ))</span><span id="7fbc" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="537b" class="km kn hi ki b fi ks kp l kq kr">+ log_ writer $ add _ summary ( accuracy , i)</span><span id="b98c" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="f0e9" class="km kn hi ki b fi ks kp l kq kr">+ error = sess $ run ( summary _ CrossEntropy ,</span><span id="f338" class="km kn hi ki b fi ks kp l kq kr">+ feed _ dict = dict (x = test _ values , y_ = test _ labels ))</span><span id="b318" class="km kn hi ki b fi ks kp l kq kr">+ log_ writer $ add _ summary (error , i)</span><span id="09bb" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="32de" class="km kn hi ki b fi ks kp l kq kr">+ weigths = sess $ run ( summary _ weights )</span><span id="9f19" class="km kn hi ki b fi ks kp l kq kr">+ log_ writer $ add _ summary (c, global _ step = i)</span><span id="af42" class="km kn hi ki b fi ks kp l kq kr">+ }</span><span id="5bfd" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="98d1" class="km kn hi ki b fi ks kp l kq kr">+ ### save entire model each 100 iterations</span><span id="fc62" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="374e" class="km kn hi ki b fi ks kp l kq kr">+ if(i %% 100 == 0){</span><span id="4cbb" class="km kn hi ki b fi ks kp l kq kr">+ saver $ save (sess , “ folder “, global _ step =i)</span><span id="e85a" class="km kn hi ki b fi ks kp l kq kr">+</span><span id="2632" class="km kn hi ki b fi ks kp l kq kr">+ }</span><span id="ebe2" class="km kn hi ki b fi ks kp l kq kr">+ }</span><span id="f4c5" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="28ca" class="km kn hi ki b fi ks kp l kq kr">&gt; ####</span><span id="42a2" class="km kn hi ki b fi ks kp l kq kr">&gt; #</span><span id="754b" class="km kn hi ki b fi ks kp l kq kr">&gt; # Show stats</span><span id="1887" class="km kn hi ki b fi ks kp l kq kr">&gt; #</span><span id="891c" class="km kn hi ki b fi ks kp l kq kr">&gt; ####</span><span id="bb9e" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="de8e" class="km kn hi ki b fi ks kp l kq kr">&gt; ### print confusion matrix , balanced accuracy and accuracy</span><span id="b0f0" class="km kn hi ki b fi ks kp l kq kr">xxxiii</span><span id="5c32" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="b8cd" class="km kn hi ki b fi ks kp l kq kr">&gt; prediction = tf$ equal (tf$ argmax (y, 1L), 2L)</span><span id="bb24" class="km kn hi ki b fi ks kp l kq kr">&gt; pred = sess $ run(tf$ argmin (y, 1L), feed _ dict = dict (x = test _values ,</span><span id="0d7d" class="km kn hi ki b fi ks kp l kq kr">+ &gt; conf.mat &lt;- table ( Predictions =pred , Actual = test _ labels [ ,1])</span><span id="afc8" class="km kn hi ki b fi ks kp l kq kr">&gt; conf.mat</span><span id="9caa" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="ede9" class="km kn hi ki b fi ks kp l kq kr">&gt; bal.accuracy &lt;- ( conf.mat [1 ,1]/sum( conf.mat [ ,1]) +</span><span id="c09d" class="km kn hi ki b fi ks kp l kq kr">+ conf.mat [2 ,2]/ sum( conf.mat [ ,2]))/2</span><span id="a424" class="km kn hi ki b fi ks kp l kq kr">&gt; bal.accuracy</span><span id="3a16" class="km kn hi ki b fi ks kp l kq kr">&gt;</span><span id="1fe3" class="km kn hi ki b fi ks kp l kq kr">&gt; sess $ run( accuracy , feed _ dict = dict (x = test _values , y_ = test _ labels ))</span><span id="678f" class="km kn hi ki b fi ks kp l kq kr">&gt;</span></pre></div></div>    
</body>
</html>