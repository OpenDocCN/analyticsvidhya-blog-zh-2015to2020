<html>
<head>
<title>Memories in Neural Networks?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络中的记忆？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/memories-in-neural-networks-d650e4593689?source=collection_archive---------39-----------------------#2020-05-13">https://medium.com/analytics-vidhya/memories-in-neural-networks-d650e4593689?source=collection_archive---------39-----------------------#2020-05-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9437" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大家好！这是我在一个月内完成<em class="jd">深度学习纳米学位</em>旅程中的第十篇文字！我已经完成了该学位总共六个模块中第四个模块的51%。今天，我们将讨论神经网络(LSTMs)对内存的<em class="jd">需求。</em></p><h2 id="f7b8" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">第15天</h2><p id="9bac" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">今天的模块在<em class="jd">列表</em>上。这些是构成循环神经网络的细胞。让我继续前面关于<em class="jd"> RNNs </em>的谈话。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es ke"><img src="../Images/c74fe8e9fac006b781eec2c472abd1c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IskhSlel33LZPmwRymPaIQ.png"/></div></div></figure><h2 id="b62e" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">RNNs</h2><p id="b988" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">在处理<em class="jd"> RNNs </em>时，我们会遇到两个主要问题，<em class="jd">消失梯度</em>T52<em class="jd">爆炸梯度</em>。</p><p id="5ef6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于<strong class="ih hj"> <em class="jd">消失渐变</em> </strong>，我们使用<em class="jd"> LSTMs </em>，稍后会详细介绍。对于爆炸渐变，我们使用一个术语叫做<em class="jd">渐变剪辑</em>。</p><h2 id="f644" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">剪裁渐变</h2><p id="0231" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">这就是我们可以解决<em class="jd">爆炸渐变</em>问题的地方。我们使用这个函数，基本上就是说，如果一个梯度高于某个阈值，就把这个值设置为已经设置好的阈值。给出了使用裁剪的代码表示。请注意'<em class="jd">剪辑</em>'是一个值。</p><pre class="kf kg kh ki fd kq kr ks kt aw ku bi"><span id="b9dc" class="je jf hi kr b fi kv kw l kx ky">#Gradient Clipping<br/>nn.utils.clip_grad_norm_(net.parameters(), clip)</span></pre><h1 id="93f3" class="kz jf hi bd jg la lb lc jk ld le lf jo lg lh li jr lj lk ll ju lm ln lo jx lp bi translated">长短期记忆细胞(LSTM)</h1><p id="955d" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">在<em class="jd"> Sepp Hochreiter的关于<em class="jd"> LSTM </em>的原始论文中，他向科学界介绍了算法和方法，他解释说<em class="jd">长期记忆</em>指的是学习到的权重，<em class="jd">短期记忆</em>指的是随着时间的每一步而变化的门控细胞状态值。</em></p><p id="0261" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">lstm</em>的基本工作是避免<strong class="ih hj"> <em class="jd">信息</em> </strong>丢失。通过存储信息并且在一些时间戳内不暴露它，然后在需要时暴露它。<em class="jd"> LSTM </em> <em class="jd">单元格</em>是<em class="jd">完全可微分的</em>，这意味着我们可以计算所有值的梯度。在该单元内执行的操作是<em class="jd"> Sigmoid、双曲正切、乘法&amp;加法。</em></p><blockquote class="lq lr ls"><p id="7af2" class="if ig jd ih b ii ij ik il im in io ip lt ir is it lu iv iw ix lv iz ja jb jc hb bi translated">LSTM网络非常适合于<a class="ae lw" href="https://en.wikipedia.org/wiki/Classification_in_machine_learning" rel="noopener ugc nofollow" target="_blank">分类</a>、<a class="ae lw" href="https://en.wikipedia.org/wiki/Computer_data_processing" rel="noopener ugc nofollow" target="_blank">处理</a>和<a class="ae lw" href="https://en.wikipedia.org/wiki/Predict" rel="noopener ugc nofollow" target="_blank">基于<a class="ae lw" href="https://en.wikipedia.org/wiki/Time_series" rel="noopener ugc nofollow" target="_blank">时间序列</a>数据做出预测</a>，因为时间序列中的重要事件之间可能存在未知持续时间的滞后。LSTMs被开发来处理在训练传统rnn时可能遇到的<a class="ae lw" href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="noopener ugc nofollow" target="_blank">消失梯度问题</a>。在许多应用中，对间隙长度的相对不敏感性是LSTM相对于RNNs、<a class="ae lw" href="https://en.wikipedia.org/wiki/Hidden_Markov_models" rel="noopener ugc nofollow" target="_blank">隐马尔可夫模型</a>和其他序列学习方法的优势。[ <a class="ae lw" href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" rel="noopener ugc nofollow" target="_blank"> <em class="hi">需要引用</em> </a> ]</p></blockquote><h2 id="6e3c" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">为什么？</h2><p id="74e3" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">LSTMs的需求是，rnn，即递归神经网络，往往会在长时间内丢失存储在其中的信息。例如，<em class="jd"> RNN的</em>不会记得10步前做出的决定，为此，我们使用<em class="jd"> LSTMs </em>。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es lx"><img src="../Images/b5eb4da7174149f00aea7817b4e55890.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-Om0SN5RODgbSuV0VWbRQ.png"/></div></div><figcaption class="ly lz et er es ma mb bd b be z dx translated">LSTM细胞</figcaption></figure><p id="b5c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了存储、使用和删除信息，它使用<strong class="ih hj"> <em class="jd">门</em> </strong>来执行所有的操作。</p><h2 id="ee17" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">盖茨</h2><p id="8e36" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">因此，在一个<em class="jd"> LSTM单元中使用了四个门。</em></p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es mc"><img src="../Images/a1866c5811e0cee857bf2c940dc09336.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ucDt_K0e-J4G0EN4GfTmHQ.png"/></div></div><figcaption class="ly lz et er es ma mb bd b be z dx translated">Gates盖茨代表</figcaption></figure><ul class=""><li id="6351" class="md me hi ih b ii ij im in iq mf iu mg iy mh jc mi mj mk ml bi translated"><strong class="ih hj">学习门:</strong>它主要接收<em class="jd">短时记忆</em>和<em class="jd">事件</em>，它<strong class="ih hj"> <em class="jd">加入</em> </strong>它们。然后它忽略了一部分。<br/> <strong class="ih hj">数学上</strong>，我们把两个内存相乘，然后应用一个激活函数。但是我们如何忘记一些数据呢？我们对它应用一个遗忘因子。</li></ul><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es mm"><img src="../Images/97598bfcd8a6fc56a84b7265b8170a51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IBvEVJNiPsjeh8piTvkJQw.png"/></div></div><figcaption class="ly lz et er es ma mb bd b be z dx translated">学习门</figcaption></figure><ul class=""><li id="70a9" class="md me hi ih b ii ij im in iq mf iu mg iy mh jc mi mj mk ml bi translated"><strong class="ih hj">遗忘门:</strong>它接收<em class="jd">长期记忆</em>并决定保留哪个部分。<br/> <strong class="ih hj">数学上</strong>，记忆与遗忘因子相乘。</li><li id="72f3" class="md me hi ih b ii mn im mo iq mp iu mq iy mr jc mi mj mk ml bi translated"><strong class="ih hj">记忆门:</strong>它接收<em class="jd">长期记忆</em>和<em class="jd">短期记忆</em>，并决定将哪些内容保存在一起。<br/><strong class="ih hj"/>数学上，我们从<em class="jd">学习门</em>和<em class="jd">遗忘门</em>获得输出，然后将它们相加。</li><li id="f320" class="md me hi ih b ii mn im mo iq mp iu mq iy mr jc mi mj mk ml bi translated"><strong class="ih hj">使用门:</strong>它接收刚从<em class="jd">遗忘门</em>出来的<em class="jd">长期记忆</em>和刚从<em class="jd">学习门</em>出来的<em class="jd">短期记忆</em>以获得新的<em class="jd">短期记忆</em>和<em class="jd">输出</em>。<br/></li></ul><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es ms"><img src="../Images/e4728c70dfce8bfcec98e10f037c0596.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tbCEk07YUmrtbiadfzvuhg.png"/></div></div><figcaption class="ly lz et er es ma mb bd b be z dx translated">使用门</figcaption></figure><blockquote class="lq lr ls"><p id="cc34" class="if ig jd ih b ii ij ik il im in io ip lt ir is it lu iv iw ix lv iz ja jb jc hb bi translated">如何<em class="hi">计算</em><strong class="ih hj"><em class="hi">遗忘因子</em> </strong>？我们利用短期记忆和当前事件，运行一个小的线性神经网络，得到输出。</p></blockquote><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es mt"><img src="../Images/b346fc16cfae7edc1038813ad199e6fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7h65IV7bIh2omModwqOsRg.png"/></div></div><figcaption class="ly lz et er es ma mb bd b be z dx translated">遗忘因子</figcaption></figure><blockquote class="mu"><p id="9eb1" class="mv mw hi bd mx my mz na nb nc nd jc dx translated">之所以这样设计LSTMs，是因为它有效。</p></blockquote><pre class="ne nf ng nh ni kq kr ks kt aw ku bi"><span id="286d" class="je jf hi kr b fi kv kw l kx ky">#Code Representation of an LSTM<br/>nn.LSTM(input_size, n_hidden, n_layers, dropout, batch_first)</span></pre><h2 id="2959" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">窥视孔连接</h2><p id="ba1f" class="pw-post-body-paragraph if ig hi ih b ii jz ik il im ka io ip iq kb is it iu kc iw ix iy kd ja jb jc hb bi translated">我们看到，在先前设计的LSTM中，没有长时记忆在寻找和获得遗忘因子中的作用，因此，这里有窥视孔连接。在这些实验中，我们在得到遗忘因子的同时，也把它放入长期记忆中，还有在细胞中进行的所有其他操作中。我们这样做也是为了让细胞知道它的长期记忆。</p><figure class="kf kg kh ki fd kj er es paragraph-image"><div role="button" tabindex="0" class="kk kl di km bf kn"><div class="er es nj"><img src="../Images/7d6c5048bf9ad5b9f08559f5fbff4de4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yMTmaO0Np5eI47Uvp1fPIw.png"/></div></div><figcaption class="ly lz et er es ma mb bd b be z dx translated">窥视孔连接</figcaption></figure></div><div class="ab cl nk nl gp nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="hb hc hd he hf"><p id="83b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我一天能走的路。我很期待这个模块的项目，<em class="jd">生成电视脚本</em>，这肯定会很有趣。不管怎样，下次再见！</p></div></div>    
</body>
</html>