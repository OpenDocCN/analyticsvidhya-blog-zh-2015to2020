<html>
<head>
<title>Understanding Attention In Transformers Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解变形金刚模型中的注意力</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-attention-in-transformers-models-57bada0cce3e?source=collection_archive---------0-----------------------#2020-11-26">https://medium.com/analytics-vidhya/understanding-attention-in-transformers-models-57bada0cce3e?source=collection_archive---------0-----------------------#2020-11-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div class="er es hg"><img src="../Images/5a0a4493496477d005118d5a912977e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*_9ySRzHD_tyCe0rr8Vz_Gg.png"/></div></figure><div class=""/><p id="50fe" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我希望在我的项目开始前就知道。</p><p id="ea7c" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我认为建立一个语言翻译器会很酷。起初，我想我会利用一个<code class="du jk jl jm jn b"><strong class="io hq">recurrent neural network</strong></code> ( <code class="du jk jl jm jn b"><strong class="io hq">RNN</strong></code>)或者一个<code class="du jk jl jm jn b"><strong class="io hq">LSTM</strong></code>来这样做。但是在我做研究的时候，我开始看到一些文章提到<code class="du jk jl jm jn b"><strong class="io hq">transformer model</strong></code>是NLP的新技术。</p><p id="9e0a" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">所以我的兴趣达到了顶峰，我认为为我的项目构建一个转换器可能会很酷。</p><p id="e3de" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，现在我必须尽可能多地获取信息，努力学习如何实现它。但更重要的是，我真的很想了解它们是如何工作的。</p><p id="f520" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我找到了开始这一切的研究论文——<a class="ae jo" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="io hq"><em class="jp">关注是你需要的全部</em> </strong> </a>。它是在2017年出版的，所以它是相当新的。但是当我读它的时候，我知道我需要一些帮助来理解一些概念。</p><p id="ae95" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我陷入困境的部分是位置编码和自我关注。所以我继续寻找，试图找到其他人是如何解释这些概念的。</p><p id="017e" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我能找到的最好的博客文章是Jay Alammar的<a class="ae jo" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> <strong class="io hq"> <em class="jp">插图变压器</em> </strong> </a>。如果你像我一样是视觉学习者，你会发现这是无价的。在那里我找到了Jay的另一个帖子的链接，<a class="ae jo" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/" rel="noopener ugc nofollow" target="_blank"> <strong class="io hq"> <em class="jp">可视化神经机器翻译模型(Seq2seq模型的力学注意)</em> </strong> </a>。</p><p id="bf6e" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这篇文章中，我将探讨<code class="du jk jl jm jn b"><strong class="io hq">self-attention</strong></code>的概念。这将有望成为在研究transformer模型时试图理解这一概念的任何人的资源。</p><p id="88ee" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我不会在这篇文章中谈论变形金刚，请注意。但是这里有一张架构的图片，这样你就可以知道注意力子层与模型的关系。</p><p id="58ab" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">变压器模型架构</strong></p><figure class="jr js jt ju fd hk er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es jq"><img src="../Images/a64c5a488f34ba9703ed5d7c8406f0ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YZsMZmXiLybDUzJtrYsQSA.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated"><strong class="bd kd">来源:</strong> <a class="ae jo" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kd">注意力是你所需要的一切</strong> </a></figcaption></figure><p id="7143" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">那么，为什么关注？</strong></p><p id="0214" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这是变形金刚里的专用酱！它解决了并行化和序列中相距较远的单词的上下文信息丢失的问题。这些都是像<code class="du jk jl jm jn b"><strong class="io hq">RNNs</strong></code>和<code class="du jk jl jm jn b"><strong class="io hq">LSTMs</strong></code>这样的<code class="du jk jl jm jn b"><strong class="io hq">seq2seq</strong></code>型号的问题。</p><p id="e9ad" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这些模型中出现这些问题的主要原因是它们需要一个接一个地处理序列中的每个单词。这意味着他们需要更长的时间来训练。如果我们可以并行处理所有的单词会怎么样？</p><p id="5f35" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">另一个问题是上下文和结构的推导方式。这些模型首先将第一个字(令牌)输入编码器，然后用随机起始状态计算一个新的<code class="du jk jl jm jn b"><strong class="io hq">state</strong></code>。然后在下一个时间步骤中，使用下一个单词和上一个状态创建一个新状态。重复这个过程，直到所有的单词都用完，此时产生固定长度的上下文向量。然后，上下文向量被传递给解码器以生成输出。</p><p id="93fe" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">问题是，在计算最终的上下文向量时，关于早期状态的上下文信息可能会丢失，因为早期的计算无法访问后面的单词。此外，固定大小的上下文向量可能不够大，不足以保留所有信息。</p><p id="414a" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">那么变压器是如何解决这些问题的呢？</strong></p><p id="cc25" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">转换器的编码器和解码器消耗整个序列，并并行处理所有单词(嵌入)。由于这种并行性，训练时间减少了。第二，这种并行处理意味着可以从所有单词中计算出上下文，从而得到更完整的上下文。</p><p id="5b99" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">线性变换</strong></p><p id="2bc1" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当一系列嵌入被传递到变换器的输入端(编码器和解码器)时，首先发生的是每个嵌入经历三个独立的线性变换，产生三个向量— <code class="du jk jl jm jn b"><strong class="io hq">query</strong></code>、<code class="du jk jl jm jn b"><strong class="io hq">key</strong></code>和<code class="du jk jl jm jn b"><strong class="io hq">value</strong></code>。当输入向量(嵌入)乘以3个权重矩阵时，发生这些变换。适当的重量是通过训练学会的。下图表示的是一个矢量<code class="du jk jl jm jn b"><strong class="io hq">sequence length</strong></code>为2，<code class="du jk jl jm jn b"><strong class="io hq">embedding size</strong></code>为4。</p><p id="7d7b" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">为什么是三次线性变换？</strong></p><p id="6f96" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">由于每个<code class="du jk jl jm jn b"><strong class="io hq">weight matrix</strong></code>都是用随机权重初始化的，因此每个合成向量都了解一些关于正在处理的嵌入(单词)的不同信息。这在计算注意力分数时很重要，因为我们不希望仅仅得到向量本身的点积。</p><figure class="jr js jt ju fd hk er es paragraph-image"><div class="er es ke"><img src="../Images/37cbf3cd797edc50a2a83fe3a0acbd8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1162/format:webp/1*7qLcv6Sd-ft5nvjmmXJgvw.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated"><strong class="bd kd">来源:</strong> <a class="ae jo" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kd">图示变压器</strong> </a></figcaption></figure><p id="90d9" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在我们有了序列中每个嵌入的3个向量，我们可以计算<code class="du jk jl jm jn b"><strong class="io hq">attention score</strong></code>。注意力得分衡量序列中的一个单词与所有其他单词之间的关系强度。</p><p id="612d" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">计算注意力的步骤</strong></p><p id="ab44" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">1.获取一个单词的查询向量，用序列中每个单词的<code class="du jk jl jm jn b"><strong class="io hq">key vector</strong></code>的转置来计算它的<code class="du jk jl jm jn b"><strong class="io hq">dot product</strong></code>——包括它自己。这是<code class="du jk jl jm jn b"><strong class="io hq">attention score</strong></code>或<code class="du jk jl jm jn b"><strong class="io hq">attention weight</strong></code>。<br/> 2。然后将每个结果除以关键向量维数的平方根。这就是<code class="du jk jl jm jn b"><strong class="io hq">scaled attention score</strong></code>。<br/> 3。通过一个<code class="du jk jl jm jn b"><strong class="io hq">softmax</strong></code>函数传递它们，以便值包含在0和1之间。<br/> 4。取每个<code class="du jk jl jm jn b"><strong class="io hq">value vectors</strong></code>并用<code class="du jk jl jm jn b"><strong class="io hq">sofmax</strong></code>函数的输出计算<code class="du jk jl jm jn b"><strong class="io hq">dot product</strong></code>。<br/> 5。将所有的<code class="du jk jl jm jn b"><strong class="io hq">wighted value vectors</strong></code>加在一起。</p><p id="d468" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">注意下图中我们正在对<code class="du jk jl jm jn b"><strong class="io hq">seq_length x embedding_size</strong></code>矩阵进行矩阵运算。这显示了嵌入大小为3的两个单词的玩具示例。</p><figure class="jr js jt ju fd hk er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es kf"><img src="../Images/d42337c5118f94e28227df8837edd512.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dSwckeG028obZPWafgJrmw.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated"><strong class="bd kd">来源:</strong> <a class="ae jo" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kd">图解变压器</strong> </a></figcaption></figure><p id="ffd3" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du jk jl jm jn b"><strong class="io hq">dot product</strong></code>产生一个<code class="du jk jl jm jn b"><strong class="io hq">seq_length x seq_length</strong></code>矩阵。想想<code class="du jk jl jm jn b"><strong class="io hq">correlation matrix</strong></code>，任何两个成员之间的关系都可以通过它们的交集找到。在这种情况下，成员是单词，它们的交集是<code class="du jk jl jm jn b"><strong class="io hq">attention scores</strong></code>。</p><p id="9ace" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">将<code class="du jk jl jm jn b"><strong class="io hq">value matrix</strong></code>乘以<code class="du jk jl jm jn b"><strong class="io hq">attention matrix</strong></code>再次得到一个<code class="du jk jl jm jn b"><strong class="io hq">seq_length x embedding_size</strong></code>矩阵。这个矩阵保存了每次嵌入的<code class="du jk jl jm jn b"><strong class="io hq">contextual information</strong></code>。</p><p id="a46c" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">多头自我关注</strong></p><p id="9577" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我上面描述的是<code class="du jk jl jm jn b"><strong class="io hq">single-head self-attention</strong></code>。在实践中我们使用<code class="du jk jl jm jn b"><strong class="io hq">multi-head self-attention</strong></code>。使用<code class="du jk jl jm jn b"><strong class="io hq">multi-head self-attention</strong></code>，每个单词都由几个注意头处理。在最初的论文中，他们使用了八个，这是我在这里使用的。</p><figure class="jr js jt ju fd hk er es paragraph-image"><div class="er es kg"><img src="../Images/4a57bd6b98c15f619ff50101d8687d6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*bAFj3xZ7UjXe5x2LFB9RPA.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated"><strong class="bd kd">来源:</strong> <a class="ae jo" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kd">注意力是你所需要的一切</strong> </a></figcaption></figure><p id="7156" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du jk jl jm jn b"><strong class="io hq">query</strong></code>、<code class="du jk jl jm jn b"><strong class="io hq">key,</strong></code>和<code class="du jk jl jm jn b"><strong class="io hq">value</strong></code>向量按头数划分，每段通过不同的头。这产生了八个向量，然后将它们连接在一起，并通过乘以另一个权重矩阵进行变换，使得合成向量是输入嵌入向量的大小。</p><p id="9288" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">来自<a class="ae jo" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的一切</a>:</p><figure class="jr js jt ju fd hk er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es kh"><img src="../Images/e071a3d4f5fe1016b9f73dfb7310b29b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wOFfYYQ4RNA3hKDI_g8aOg.png"/></div></div></figure><figure class="jr js jt ju fd hk er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es ki"><img src="../Images/420918d578dc960a791a8cc06ec7e811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rpMx8CnQJlEXwW5tYOYOiw.png"/></div></div></figure><p id="03cf" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在使用<code class="du jk jl jm jn b"><strong class="io hq">softmax</strong></code>功能之前，我们需要对<code class="du jk jl jm jn b"><strong class="io hq">attention score</strong></code>应用一个遮罩。掩模将是一个<code class="du jk jl jm jn b"><strong class="io hq">padding mask</strong></code>或一个<code class="du jk jl jm jn b"><strong class="io hq">padding mask</strong></code>和一个<code class="du jk jl jm jn b"><strong class="io hq">look-ahead mask</strong></code>的组合。</p><p id="5fa0" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">输入到<code class="du jk jl jm jn b"><strong class="io hq">encoder</strong></code>和<code class="du jk jl jm jn b"><strong class="io hq">decoder</strong></code>的序列必须长度相同。因为句子的长度是可变的，所以必须要么截断句子，要么填充句子，使它们都是指定的长度。结果，填充可能被<code class="du jk jl jm jn b"><strong class="io hq">attention</strong></code>机制解释为有意义的，导致<br/>不可靠的学习。</p><p id="18ff" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了减轻这种影响，应用了一个<code class="du jk jl jm jn b"><strong class="io hq">padding mask</strong></code>,以便当添加到<code class="du jk jl jm jn b"><strong class="io hq">attention scores,</strong></code>时，填充位置中的值变得很小以至于可以忽略。这通过将掩模中的位置设置为接近<code class="du jk jl jm jn b"><strong class="io hq">negative infinity</strong></code>的值来实现。</p><p id="00d0" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">填充掩码被认为是可选的，但是我相信大多数实现都选择使用它。</p><p id="a6ad" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">变形金刚是<code class="du jk jl jm jn b"><strong class="io hq">autoregressive models</strong></code>。它们查看序列中所有先前的输入，以便预测下一个输出(<code class="du jk jl jm jn b"><strong class="io hq">token</strong></code>)。但是因为与<code class="du jk jl jm jn b"><strong class="io hq">RNNs</strong></code>不同，它们一次接收整个序列，所以需要有一种方法来限制对任何位置的注意力，以便只关注序列中在它之前的部分。这是因为期望模型在没有峰值的情况下推断下一个位置。如果它能够看到下一个位置，那么它就会复制它。</p><p id="9b00" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这就是<code class="du jk jl jm jn b"><strong class="io hq">look-ahead mask</strong></code>的作用。序列中出现在<code class="du jk jl jm jn b"><strong class="io hq">query</strong></code>字右边的所有<code class="du jk jl jm jn b"><strong class="io hq">attention scores</strong></code>字都被屏蔽。这限制了<code class="du jk jl jm jn b"><strong class="io hq">query</strong></code>字关注它自己和序列中左边的所有字。</p><p id="e253" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><code class="du jk jl jm jn b"><strong class="io hq">look-ahead mask</strong></code>仅应用于每个解码器层的第一关注子层。这是因为解码器是推理发生的地方。因此，这是模型不应该能够预见的地方。</p><p id="7e2b" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">它是如何工作的？</p><p id="ba42" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">1.创建一个遮罩，其中对角线上方的右上角三角形的值都是1，其余的值都是0。<br/> 2。将遮罩乘以<code class="du jk jl jm jn b"><strong class="io hq"><em class="jp">-1e9</em></strong></code>。<br/> 3。将遮罩添加到<code class="du jk jl jm jn b"><strong class="io hq">attention matrix</strong></code>中。</p><figure class="jr js jt ju fd hk er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es kj"><img src="../Images/668fd137a739a55c8af030a38bd0c8e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BOV347XrFbUudP3PN-BuGQ.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated"><strong class="bd kd">注意力面具+注意力矩阵</strong></figcaption></figure><p id="d17c" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">接下来，沿着<code class="du jk jl jm jn b"><strong class="io hq">key</strong></code>列应用softmax，将这些值转换为<code class="du jk jl jm jn b"><strong class="io hq">probability distributions</strong></code>。将<code class="du jk jl jm jn b"><strong class="io hq">softmax</strong></code>应用于<code class="du jk jl jm jn b"><strong class="io hq">attention matrix</strong></code>后，所有这些极小的值都将变为零。这有什么关系呢？</p><figure class="jr js jt ju fd hk er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es kk"><img src="../Images/916526165b7a81cc199d7ed38310d89c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ChozqC-zk1qmq9WIenVmA.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated"><strong class="bd kd">带softmax的注意力矩阵</strong></figcaption></figure><p id="32dc" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在图中的玩具<code class="du jk jl jm jn b"><strong class="io hq">attention matrix</strong></code>中，我标记了两个轴[ <code class="du jk jl jm jn b"><strong class="io hq"><em class="jp">T1,…,Tn</em></strong></code> ]。列代表记号(单词)<code class="du jk jl jm jn b"><strong class="io hq">keys</strong></code>，行是记号<code class="du jk jl jm jn b"><strong class="io hq">queries</strong></code>。any、<code class="du jk jl jm jn b"><strong class="io hq">query <em class="jp">Ti,</em></strong></code>的关注度得分是其所在行相对于<code class="du jk jl jm jn b"><strong class="io hq">key</strong></code>列[ <code class="du jk jl jm jn b"><strong class="io hq"><em class="jp">T1,…,Tn</em></strong></code> ]的值。注意最上面的<code class="du jk jl jm jn b"><strong class="io hq">query,</strong></code> <code class="du jk jl jm jn b"><strong class="io hq"><em class="jp">T1</em></strong></code>只能得到最左边的<code class="du jk jl jm jn b"><strong class="io hq">key</strong></code> <code class="du jk jl jm jn b"><strong class="io hq">T1</strong></code>的分数。<code class="du jk jl jm jn b"><strong class="io hq">T2,...,T5</strong></code>没有有意义的信息(值太低)。<code class="du jk jl jm jn b"><strong class="io hq">query</strong></code> <code class="du jk jl jm jn b"><strong class="io hq">T2</strong></code>可以访问<code class="du jk jl jm jn b"><strong class="io hq">keys</strong></code> <code class="du jk jl jm jn b"><strong class="io hq">[T2, $T1]</strong></code>的集合。依此类推，直到最后一个字，<code class="du jk jl jm jn b"><strong class="io hq">T5,</strong></code>即访问整个序列。</p><p id="c9e6" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">接下来，计算<code class="du jk jl jm jn b"><strong class="io hq">attention weight matrix</strong></code>和<code class="du jk jl jm jn b"><strong class="io hq">value matrix</strong></code>之间的<code class="du jk jl jm jn b"><strong class="io hq">dot product</strong></code>。这具有将权重应用于值的效果。如果您继续下去，您会注意到<code class="du jk jl jm jn b"><strong class="io hq"><em class="jp">T1</em></strong></code>嵌入中的值只受第1行的<code class="du jk jl jm jn b"><strong class="io hq"><em class="jp">T1</em></strong></code> <code class="du jk jl jm jn b"><strong class="io hq">attention weights</strong></code>的影响。<code class="du jk jl jm jn b"><strong class="io hq"><em class="jp">T2</em></strong></code>嵌入只影响第2行的<code class="du jk jl jm jn b"><strong class="io hq">attention weights</strong></code>、<code class="du jk jl jm jn b"><strong class="io hq"><em class="jp">T1</em></strong></code>和<em class="jp"> T2 </em> 。如此下去，直到到达最后的嵌入(序列中的最后一个单词)，其嵌入受到最后一行中所有权重的影响。自回归就是这样实现的。</p><p id="b366" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">该图显示了刚才描述的内容。</p><figure class="jr js jt ju fd hk er es paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="er es kl"><img src="../Images/3efdb8385cffcf424fd35cde1d031440.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CIDimhoGr4_geHV9vJaFNA.png"/></div></div><figcaption class="jz ka et er es kb kc bd b be z dx translated"><strong class="bd kd">计算加权值矩阵</strong></figcaption></figure><p id="7a7f" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这些步骤发生在每个注意头中。下一步是连接所有的加权矩阵，并通过线性变换来恢复原始的输入维数。</p><p id="e4b0" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这就是所有需要注意的地方。</p><p id="ec3b" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">还有一件事。注意力面具并不总是单独使用。请记住，还需要考虑填充。因此，如果我们选择使用填充遮罩，则<code class="du jk jl jm jn b"><strong class="io hq">look-ahead mask</strong></code>将与填充遮罩一起应用。</p><p id="3c86" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">自我关注公式</strong>:</p><p id="7d46" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这是包括面具在内的自我关注公式。</p><figure class="jr js jt ju fd hk er es paragraph-image"><div class="er es km"><img src="../Images/e4026368951158ea74cbd6f79b93918a.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*asfa2FcVDfMYYmGFJM4AdA.png"/></div></figure><p id="2885" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">关注度计算的表示:</strong></p><figure class="jr js jt ju fd hk er es paragraph-image"><div class="er es kn"><img src="../Images/2e80000dd0a190d64c41228e941c7148.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*exxp9293eJywC11yggdD7g.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated">来源:<a class="ae jo" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a></figcaption></figure><p id="546d" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hq">结论</strong></p><p id="2227" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在本文中，我们描述了<code class="du jk jl jm jn b"><strong class="io hq">self-attention mechanism</strong></code>，它是<code class="du jk jl jm jn b"><strong class="io hq">transformer model</strong></code>的核心和灵魂。我们谈到了</p><ol class=""><li id="1872" class="ko kp hp io b ip iq it iu ix kq jb kr jf ks jj kt ku kv kw bi translated">关注的原因</li><li id="3ca9" class="ko kp hp io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated">计算关注度的步骤</li><li id="313c" class="ko kp hp io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated">对填充遮罩的需求</li><li id="ecd2" class="ko kp hp io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated">前瞻掩码如何在解码器中解决自回归问题</li><li id="7057" class="ko kp hp io b ip kx it ky ix kz jb la jf lb jj kt ku kv kw bi translated">直观地演示了前瞻遮罩的工作原理。</li></ol><p id="d404" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">谢谢你坚持到现在。我希望你喜欢这本书，并且它对理解注意力有所帮助。</p></div><div class="ab cl lc ld gp le" role="separator"><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh li"/><span class="lf bw bk lg lh"/></div><div class="hb hc hd he hf"><p id="43f2" class="pw-post-body-paragraph im in hp io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">资源</p><ul class=""><li id="13d7" class="ko kp hp io b ip iq it iu ix kq jb kr jf ks jj lj ku kv kw bi translated">阿希什·瓦斯瓦尼等人的作品</li><li id="e766" class="ko kp hp io b ip kx it ky ix kz jb la jf lb jj lj ku kv kw bi translated">杰·阿拉玛的《变形金刚》</li><li id="d214" class="ko kp hp io b ip kx it ky ix kz jb la jf lb jj lj ku kv kw bi translated"><a class="ae jo" href="https://blog.exxactcorp.com/a-deep-dive-into-the-transformer-architecture-the-development-of-transformer-models/" rel="noopener ugc nofollow" target="_blank">深入了解变压器架构——变压器模型的开发</a></li></ul></div></div>    
</body>
</html>