<html>
<head>
<title>IMDB MOVIE GENRE TAG PREDICTION</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">IMDB电影类型标签预测</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/imdb-movie-genre-tag-prediction-4ee71a0aa9bd?source=collection_archive---------7-----------------------#2020-04-06">https://medium.com/analytics-vidhya/imdb-movie-genre-tag-prediction-4ee71a0aa9bd?source=collection_archive---------7-----------------------#2020-04-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="6087" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇博客中，我们将知道如何使用经典和深度学习技术进行多标签分类。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/fda78d71dd6a18ee3b1b01d8b34222f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/0*ddvPZf_X9in4wa90.jpeg"/></div></figure><p id="13e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">电影情节包含了很多关于电影的信息。这些信息对于构建自动系统来为电影创建标签是很有价值的。</p><p id="8f82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">自动标记系统可以帮助推荐引擎改进相似电影的检索，以及帮助观众预先知道从电影中可以期待什么。</p><p id="6ecf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将尝试使用电影情节进行类型标签预测。</p><p id="ce22" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据集:<a class="ae jl" href="https://www.kaggle.com/cryptexcode/mpst-movie-plot-synopses-with-tags" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/cryptexcode/mpst-movie-plot-synopses-with-tags</a></p><p id="b825" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">加载数据集:</strong></p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jm jn l"/></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es jo"><img src="../Images/c7e64d16a022ff0b455b84771f22e12e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*inzT2XIdDmyrDoj12W88Vg.png"/></div></div></figure><p id="9ac2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以看到有14828部电影。电影类型是多标签的。所以，这个分类就是<strong class="ih hj">多标签分类问题</strong>。</p><p id="68ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">预处理电影剧情_梗概</strong></p><p id="a6ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在此阶段，我们将进行以下操作:</p><ol class=""><li id="a457" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">从plot_synopsis中删除所有停用词。</li><li id="cc69" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">堵塞物</li><li id="bf42" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">以小写形式转换绘图。</li><li id="e5c3" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">删除html标签</li><li id="73b4" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">删除https链接。</li><li id="ba5c" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">删除除字母数字字符以外的所有字符。</li></ol><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jm jn l"/></div></figure><p id="fb7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">预处理标签:</strong></p><p id="f141" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">去除重复的流派标签是非常重要的。预处理标签后，我得到了71个不同的标签，而不是141个。</p><p id="f389" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我去掉了空格，用空格代替了'，'。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jm jn l"/></div></figure><p id="ff5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们进入正题。</p><p id="89ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是一个多标签分类的问题。</p><p id="4587" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">多标签分类</strong>:多标签分类为每个样本分配一组目标标签。这可以被认为是预测一个数据点的不相互排斥的属性，例如与一个文档相关的主题。</p><p id="d5ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考:<a class="ae jl" href="https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2017/08/introduction-to-multi-label-class ification/</a></p><p id="0489" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">方法:</strong></p><p id="6161" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于任何类型的分类，我们都必须将文本转换成向量。有很多策略。</p><p id="555b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">参考:<a class="ae jl" rel="noopener" href="/@paritosh_30025/natural-language-processing-text-data-vectorization-af2520529cf7">https://medium . com/@ paritosh _ 30025/natural-language-processing-text-data-矢量化-af2520529cf7 </a></p><p id="9c5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">绩效指标</strong></p><p id="3292" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">“微f1分数”:</strong> <br/>通过计算总的真阳性、假阴性和假阳性来计算全局指标。当我们有<strong class="ih hj">阶级不平衡时，这是一个更好的衡量标准。</strong></p><p id="986a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请参考:<a class="ae jl" href="http://rushdishams.blogspot.com/2011/08/micro-and-macro-average-of-precision.html" rel="noopener ugc nofollow" target="_blank">http://rushdishams . blogspot . com/2011/08/micro-and-macro-average-of-precision . html</a></p><p id="41f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">文本特征化</strong></p><ol class=""><li id="0c6a" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated"><strong class="ih hj"> 3克Tf-Idf </strong>，最小文档频率10，最大特征20000。</li></ol><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jm jn l"/></div></figure><p id="f5ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj"> LDA(话题建模)</strong>拥有10个n话题。这是一个自动识别文本对象中存在的主题并导出文本语料库所展示的隐藏模式的过程。<a class="ae jl" href="https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2016/08/初学者指南-主题建模-python/ </a></p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jm jn l"/></div></figure><p id="3010" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">模型<br/>对于多标签分类，我们在基本模型上使用一个vs rest。</strong></p><p id="9ace" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">基本型号:</strong></p><ol class=""><li id="4a07" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated"><strong class="ih hj">朴素贝叶斯作为文本分类的基准。因此，建议采用这种模式。我们将首先在Tf-idf上应用朴素贝叶斯。</strong></li></ol><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jm jn l"/></div></figure><p id="e0bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺点(朴素贝叶斯)</strong>:它假设特征是独立。但是在我们的例子中，有很多特征，这些假设可能会被违反。</p><p id="efcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.<strong class="ih hj">Tf-IDF上的逻辑回归</strong>:我们将应用逻辑回归，因为它在高维数据上非常有效。</p><p id="86ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">逻辑回归试图找到一个将两个类分开的超平面，即假设数据是线性可分的。</p><p id="0d66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">随着维数的增加，线性可分数据的机会增加。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jm jn l"/></div></figure><p id="3e4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">设置正确的阈值:</strong></p><p id="878f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">阈值默认为<strong class="ih hj"> 0.5 </strong>。这意味着如果输出概率大于0.5，则它被分类为1，否则为0。</p><p id="6503" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将检查多个阈值，并选择一个给出最高<strong class="ih hj">微f1 </strong>测量值的阈值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jp jq di jr bf js"><div class="er es kh"><img src="../Images/1308bc589b2ad16f0dcf08ef00e9849c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*8pB9h5wjFqeygw8f03qjuA.png"/></div></div></figure><p id="7f12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">得到适当的阈值<strong class="ih hj">后，微f1 </strong>有所提高。</p><p id="5cfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了增加f1，我们将应用具有10个n主题的<strong class="ih hj">主题建模</strong> (LDA)。我们合并Tf-idf和LDA并应用逻辑回归。</p><p id="5a1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。LDA + Tf-idf的逻辑回归:</strong></p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jm jn l"/></div></figure><p id="0140" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。LDA+Tf-idf上的轻型GBM:</strong></p><p id="039b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Light GBM是一个基于决策树算法的快速、分布式、高性能梯度提升框架，用于排序、分类和许多其他机器学习任务。</p><p id="31b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">增强技术是一种复杂的技术，它试图通过误差的附加减少来最小化偏差。</p><p id="5307" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，不建议对多标签分类使用复杂的模型，因为这会增加时间复杂度。</p><p id="e675" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我们的数据集很小，所以我们将尝试一次。</p><p id="e8d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用随机搜索CV来获得最佳超参数:</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jm jn l"/></div></figure><p id="3b1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">尝试深度学习方法:</strong></p><p id="4074" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习是一种非常强大的数据特征化技术。所以，我们也尝试了这个。</p><p id="77c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> LSTM: </strong>长短期记忆网络——通常被称为“LSTM”，是一种特殊的RNN，能够学习长期依赖关系。</p><p id="bb2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">LSTM处理序列信息。</p><p id="fb7f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了应用LSTM，我们首先将预处理图转换成秩矩阵。</p><p id="0e5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">以下是步骤:</strong></p><ol class=""><li id="6026" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">删除出现在不到10个文档中的所有单词。</li><li id="52fc" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">根据每个词出现的频率来排列它们。</li><li id="65b7" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">用预处理图中单词各自的等级替换单词。</li><li id="4651" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">应用填充以使每个地块大小相等。</li><li id="ba03" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">对此数据集应用LSTM。</li></ol><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jm jn l"/></div></figure><p id="3c0a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">训练模式:</strong></p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="jm jn l"/></div></figure><p id="f362" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">缺点</strong>:深度学习模型需要大数据集来训练。</p><p id="fad3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结论:</strong></p><ol class=""><li id="8f8a" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">我从上面链接中提到的原始研究论文中获得了大约5%的显著提高。</li><li id="5551" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">尝试大量的特征和主题建模提高了我的分数。</li><li id="f8ed" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">黑客使用阈值是非常有效的，提高了分数。</li><li id="53db" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">标签需要预处理，因为它包含多个重复的标签，前面有一个空格</li><li id="7c9d" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">我使用了每一种可能的方法，即Tfidf、矩阵分解技术和深度学习。</li><li id="f23a" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">最终测试的最佳微f1为0.408。</li><li id="f06b" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">给定的数据集非常小，特征图的维数很大。由于缺乏足够的数据点，深度学习表现不佳。</li><li id="f031" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">我用数据集指定的训练和测试分割来训练模型。</li><li id="2b43" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">我担心数据泄露。</li><li id="fc61" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">决策树因为维数高而表现最差。</li></ol><h1 id="c361" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">参考资料:</h1><p id="ea18" class="pw-post-body-paragraph if ig hi ih b ii lg ik il im lh io ip iq li is it iu lj iw ix iy lk ja jb jc hb bi translated"><a class="ae jl" href="https://www.kaggle.com/cryptexcode/mpst-movie-plot-synopses-with-tags" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/cryptexcode/mpst-movie-plot-synopses-with-tags</a><a class="ae jl" href="https://www.analyticsvidhya.com/blog/2017/08/introduction-to-multi-label-classification/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2017/08/introduction-to-multi-label-class ification/</a></p><p id="8de9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">未来工作:</strong></p><p id="c474" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">BERT是用于自然语言处理的很好的深度学习技术。BERT的关键技术创新是将Transformer(一种流行的注意力模型)的双向训练应用于语言建模。我们可以在体裁分类上试试。</p></div></div>    
</body>
</html>