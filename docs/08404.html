<html>
<head>
<title>GPT-3: Whats, Hows &amp; The Takeaways</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GPT-3:什么，怎么样&amp;要点是什么</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gpt-3-whats-hows-where-bdc15d204867?source=collection_archive---------12-----------------------#2020-07-28">https://medium.com/analytics-vidhya/gpt-3-whats-hows-where-bdc15d204867?source=collection_archive---------12-----------------------#2020-07-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/21c8b2d90f54a1cd36576faab216b42c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*9pt5d4D7aGJqUqpra1tMzg.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">GPT 3号是巨大的！！</figcaption></figure><p id="48fe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我第一次听说GPT-3时，我的第一印象是它一定是GPT-2 +更多计算+更多数据。鉴于GPT-2本身就是GPT +更多的计算+更多的数据+很少的聪明黑客，这不是一个坏的预期。事实证明，GPT 3号也是如此。但是，这并没有削弱GPT 3号和它的前辈在过去3年中取得的成就。盲目地向语言模型注入更多的东西在达到一定限度后是没有好处的，并且会吸引大量的工程/数据/合规性挑战来处理。</p><p id="0760" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这篇文章讨论了以下内容:</p><ul class=""><li id="a5fd" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">GPT-3在实现基于文本的任务的通用性方面是一个值得称赞的尝试，并且是对AGI的一个有希望的贡献，它使用了新的学习范式— <a class="ae jx" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">变压器架构</a>和语境学习(<a class="ae jx" href="https://arxiv.org/pdf/1703.03400.pdf" rel="noopener ugc nofollow" target="_blank">元学习</a></li><li id="63db" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">从文献中改编一些最佳实践和线索，这对从事实际应用语言建模的ML从业者来说会很方便</li></ul></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><h1 id="cb9e" class="kk kl hi bd km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh bi translated">关于GPT-3的思考</h1><p id="d4ae" class="pw-post-body-paragraph iq ir hi is b it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn hb bi translated">除了GPT命名法之外，看看相应白皮书的名称，我们可以了解GPT模型是如何随着时间的推移而发展的，以及它们相对于前辈的贡献。</p><ul class=""><li id="0272" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated"><a class="ae jx" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">提高语言理解</a> (GPT) — <strong class="is hj"> 117M参数</strong> — <strong class="is hj"> ~400MB大小</strong></li><li id="22a6" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><a class="ae jx" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">语言模型是无监督的多任务学习器</a>(GPT-2)——<strong class="is hj">1.5B参数</strong>——<strong class="is hj">~ 5GB大小</strong></li><li id="7854" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated"><a class="ae jx" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank">语言模型是少拍学习器</a> (GPT-3) — <strong class="is hj"> 175B参数</strong>—<strong class="is hj">500 GB大小</strong></li></ul><p id="a09f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">出人意料(还是没有？)，学习组件的核心在所有迭代中保持不变— <strong class="is hj">一个单向的语言建模目标</strong>，尽管最近对它进行了改进(例如<a class="ae jx" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT </a>)</p><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es ln"><img src="../Images/94a365d70eeab5c87d887ed0a2adde55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yyb4yBP6M2k6JJhWI-WPlA.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">将符号上的联合概率学习为条件概率的乘积</figcaption></figure><p id="1576" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了完整起见，引用架构变更:</p><p id="85c4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> GPT到GPT-2 </strong></p><blockquote class="lw lx ly"><p id="e14a" class="iq ir lz is b it iu iv iw ix iy iz ja ma jc jd je mb jg jh ji mc jk jl jm jn hb bi translated">该模型在很大程度上遵循了开放人工智能GPT模型(拉德福德等人，2018年)的细节，并进行了一些修改。层标准化(Ba等人，2016年)被移动到每个子块的输入，类似于预激活残差网络(he等人，2016年)，并且在最终自注意块之后添加了额外的层标准化。使用修正的初始化，该初始化考虑了剩余路径上的累积和模型深度。我们在初始化时将残差层的权重缩放1/ √ N，其中N是残差层的数量。词汇量扩大到50257。我们还将上下文大小从512个令牌增加到1024个令牌，并使用了更大的批量512。</p></blockquote><p id="c70f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">GPT 2号对GPT 3号</p><blockquote class="lw lx ly"><p id="6df4" class="iq ir lz is b it iu iv iw ix iy iz ja ma jc jd je mb jg jh ji mc jk jl jm jn hb bi translated">我们的基本预训练方法，包括模型、数据和训练，类似于新GPT协议中描述的流程，相对直接地扩大了模型大小、数据集大小和多样性以及训练长度。我们对情境学习的使用也类似于GPT新协议，但在这项工作中，我们系统地探索了情境中不同的学习设置。我们使用与GPT-2 [RWC+19]相同的模型和架构，包括其中描述的修改的初始化、预规范化和可逆令牌化，除了我们在变换器的层中使用交替的密集和局部带状稀疏注意模式，类似于稀疏变换器。</p></blockquote><p id="9d39" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">从可用性的角度来看:</p><ul class=""><li id="b70d" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">GPT-2侧重于<strong class="is hj">零触发</strong>设置，在该设置中，语言模型执行某些任务的能力在针对所述语言建模目标进行优化后直接进行测试</li><li id="5e07" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">GPT-3专注于更实用和可扩展的范例——<strong class="is hj">单镜头</strong>和<strong class="is hj">少镜头</strong>设置以及零镜头。这使得GPT-3在SOTA排行榜上遥遥领先于大多数特定任务的微调模型</li></ul><p id="32e1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">更多关于元学习、零元学习、少元学习的内容在另一篇文章中。</p><h2 id="0924" class="md kl hi bd km me mf mg kq mh mi mj ku jb mk ml ky jf mm mn lc jj mo mp lg mq bi translated">GPT-3及其推广</h2><p id="5519" class="pw-post-body-paragraph iq ir hi is b it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn hb bi translated">在高层次上，特定于任务的学习者针对<strong class="is hj"> p(输出|输入)</strong>进行优化，并且通过针对<strong class="is hj"> p(输出|输入，任务)进行优化来实现泛化。</strong>最近的架构通过将输入、输出和任务规范都指定为符号序列来实现这一点。例如，翻译请求可以表述为(<em class="lz">翻译成法语、英语文本、法语文本)</em>，阅读理解任务可以表述为(<em class="lz">回答问题、文档、问题、答案)— </em> <strong class="is hj">上下文学习</strong></p><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es mr"><img src="../Images/0fab34ffec764c5c39527e6b5f62bb32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cFA675tMeirJikbGwnnPoQ.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">GPT-3在42个标记任务上的综合表现——证明其在环境中的学习能力</figcaption></figure><p id="22fe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">GPT-3基于最近的主张，即足够大的转换器架构改善了文本合成和跨多个任务的下游性能，以及与许多下游任务密切相关的日志丢失遵循随着规模而改善的平稳趋势的证据。GPT-3假设并证明了<em class="lz">由于情境学习涉及吸收模型参数中的许多技能和任务，因此情境学习能力可能会随着规模扩大而显示出类似的强劲增长。</em></p><figure class="lo lp lq lr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="er es ms"><img src="../Images/7b1a4143964d883a304d7186c7a41b4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vkq9gKPa9RyA2_BPaao7TQ.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">显示GPT-3推理时间行为的零发、一发和少发插图</figcaption></figure><ul class=""><li id="4db9" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">在推理过程中，模型期望一组示例(上下文、样本输入和输出)将模型引导到当前的问题设置中</li><li id="8add" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">GPT-3使用这些例子来推断新的查询输入，而不在过程中反向传播梯度</li><li id="1172" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">模型的推理时间行为取决于问题类型</li><li id="43fe" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">例如，在涉及从几个选项中选择一个正确完成的任务上，我们提供K个上下文和正确完成的例子，后面是仅一个上下文的例子，并且比较每个完成的LM可能性。对于大多数任务，我们比较每个令牌的可能性(以归一化长度)，但是在少数数据集(ARC、OpenBookQA和RACE)上，我们通过归一化每个完成的无条件概率，通过计算<em class="lz"> P(完成|上下文)/P(完成|答案上下文)</em>，获得了额外的好处，其中答案上下文是字符串“答案:”或“A:”并用于提示完成应该是答案，但在其他方面是通用的。</li></ul><h1 id="7153" class="kk kl hi bd km kn mt kp kq kr mu kt ku kv mv kx ky kz mw lb lc ld mx lf lg lh bi translated"><strong class="ak">给传销从业者的建议</strong></h1><h2 id="b766" class="md kl hi bd km me mf mg kq mh mi mj ku jb mk ml ky jf mm mn lc jj mo mp lg mq bi translated"><strong class="ak">面向潜在的GPT-3用户</strong></h2><p id="e42a" class="pw-post-body-paragraph iq ir hi is b it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn hb bi translated">虽然列出GPT-3可以使用和不可以使用的广泛情况是不可行的，但在接听电话时考虑这些要点是值得的。这些建议是基于这样一个假设，即我们不会很快获得输入文本的GPT-3特征向量，除非他们开源整个GPT-3模型。</p><ul class=""><li id="cb89" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">如果您的用例与互联网上可用的免费文本有很大关系，并且/或者用于GPT-3培训的数据集与您的用例相关，则该用例类似于GPT-3显示出可观结果的那些任务，您可以尝试一下</li><li id="d342" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">如果您的用例需要开放+专有数据，并且用例符合GPT-3协议的设置，那么GPT-3协议的使用可能是有限的或特定于用例的</li><li id="5673" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">对于完全由专有数据驱动的案例，OOTB GPT-3可能没有太多实际帮助</li></ul><h2 id="a548" class="md kl hi bd km me mf mg kq mh mi mj ku jb mk ml ky jf mm mn lc jj mo mp lg mq bi translated">对于ML工程师和黑客来说</h2><p id="414c" class="pw-post-body-paragraph iq ir hi is b it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn hb bi translated">对我来说，关键的一点是GPT-3的核心是经典语言建模客观，但在很少/没有监督的情况下击败SOTA图表确实令人鼓舞</p><ul class=""><li id="ac82" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated">当你有足够的数据来微调具有语言模型目标的GPT式架构时，我们可以在定制数据集上取得惊人的结果</li><li id="32f8" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">LM自适应需要仔细照看微调过程，并识别提高微调模型性能的无监督辅助任务</li><li id="e58d" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">它极大地有助于建立日志丢失和目标用例之间的关联。正如在GPT-3和其他最近的论文中观察到的，日志丢失与大多数文本下游任务相关——这应该涵盖了我们今天解决的大多数问题</li><li id="64be" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">对于人工检查不容易访问数据的用例，通过观察日志丢失和辅助任务的性能来盲调语言模型是一种可行的嗅探测试，也是一种利用数据的方法，而不是在受限数据上使用OOTB语言模型。要获得更多关于选择辅助任务和参考论文的帮助，请查看附加的帖子(有点过时，但应该是一个好的起点)</li><li id="8bf5" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">建立一个可靠的特征提取器是大多数文本任务的关键，其中标记数据是稀疏的。GPT-3及其前身概述了如何实现这一目标。参考<a class="ae jx" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank"> GPT </a>论文了解方法，参考<a class="ae jx" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>、<a class="ae jx" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>了解有用的技巧和改进</li></ul><div class="my mz ez fb na nb"><a rel="noopener follow" target="_blank" href="/ether-labs/https-medium-com-ether-labs-attention-architecture-1-60caf491fc8e"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">【Part-1】你需要哪方面的关注(架构)？</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">面向NLP任务的变压器架构最新进展综述</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">medium.com</p></div></div><div class="nk l"><div class="nl l nm nn no nk np ik nb"/></div></div></a></div><div class="my mz ez fb na nb"><a rel="noopener follow" target="_blank" href="/ether-labs/part-2-which-attention-architecture-do-you-need-c327aaa771f8"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">【Part-2】你需要哪方面的关注(架构)？</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">面向NLP任务的变压器架构最新进展综述</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">medium.com</p></div></div><div class="nk l"><div class="nq l nm nn no nk np ik nb"/></div></div></a></div><h1 id="0673" class="kk kl hi bd km kn mt kp kq kr mu kt ku kv mv kx ky kz mw lb lc ld mx lf lg lh bi translated">结论</h1><p id="95ed" class="pw-post-body-paragraph iq ir hi is b it li iv iw ix lj iz ja jb lk jd je jf ll jh ji jj lm jl jm jn hb bi translated">虽然GPT-3有其自身的局限性，有很多改进的空间，但就模型大小、训练数据大小和智能训练策略而言，这是一个惊人的壮举。虽然微调和处理这样的大规模模型还不是大多数ML实践者所能达到的，但关键的收获是GPT-3及其前身强化了这样一个事实，即基本语言建模范式本身足以用于大多数下游任务，其中标记数据集的可用性是一个挑战。</p><p id="d652" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">很乐意讨论您是否有任何可以通过语言模型和微调解决的用例。</p><p id="148b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">快乐学习！！</p></div></div>    
</body>
</html>