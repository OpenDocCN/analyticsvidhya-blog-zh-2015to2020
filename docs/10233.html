<html>
<head>
<title>K Nearest Neighbors (K-NN) with numpy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有numpy的K个最近邻(K-NN)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-nearest-neighbors-k-nn-with-numpy-6c52cbe28093?source=collection_archive---------5-----------------------#2020-10-10">https://medium.com/analytics-vidhya/k-nearest-neighbors-k-nn-with-numpy-6c52cbe28093?source=collection_archive---------5-----------------------#2020-10-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="6a38" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">ML算法的群集返回K-NN作为最简单的算法</p></blockquote><p id="b346" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">K-NN可以说是用于分类和回归的最简单的机器学习算法。构建模型包括仅存储训练数据集，从而减少训练时间。这里我们将了解如何使用KNN进行分类。我们将在kaggle 的<a class="ae jk" href="https://www.kaggle.com/rtatman/did-it-rain-in-seattle-19482017" rel="noopener ugc nofollow" target="_blank">西雅图降雨数据集上使用numpy实现KNN。是有真假标签的二元分类。</a></p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jl"><img src="../Images/b63e538755efeef169e148f95414b56d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*obT6-5r6G-j3Tf1RyDxhJQ.jpeg"/></div></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">图一。下雨了吗？</figcaption></figure></div><div class="ab cl kb kc gp kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="hb hc hd he hf"><p id="8af3" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">第一步:导入库</strong></p><pre class="jm jn jo jp fd ki kj kk kl aw km bi"><span id="a122" class="kn ko hi kj b fi kp kq l kr ks">import pandas as pd<br/>import numpy as np<br/>from collections import Counter<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import classification_report, accuracy_score</span></pre><p id="2a3b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">第二步:读取数据并预处理</strong></p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es kt"><img src="../Images/5f9a2c8ec72c46ceb73aa37aeca871b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*U9O2qIOW_cnDmQkL"/></div></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">图二。照片由<a class="ae jk" href="https://unsplash.com/@franki?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">弗兰基·查马基</a>在<a class="ae jk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><pre class="jm jn jo jp fd ki kj kk kl aw km bi"><span id="7b64" class="kn ko hi kj b fi kp kq l kr ks">#Read the csv in the form of a dataframe<br/>df= pd.read_csv("data.csv")<br/>df.head()</span></pre><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es ku"><img src="../Images/955703ab79e8e6f5412931a4afabe557.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*fW--Z5kdznTavIlau7cf0g.png"/></div></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">图3。浏览一下数据</figcaption></figure><pre class="jm jn jo jp fd ki kj kk kl aw km bi"><span id="4e4d" class="kn ko hi kj b fi kp kq l kr ks">#Removing the null values<br/>df.dropna(axis=0, inplace=True)<br/>#Reset the index to avoid error<br/>df.reset_index(drop=True, inplace=True)<br/>y = df['RAIN'].replace([False,True],[0,1])<br/>#Removing Date feature and Rain because it is our label<br/>df.drop(['RAIN','DATE'],axis=1,inplace=True) </span></pre><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es kv"><img src="../Images/d25e44f70d2849785eff4a12c1c5b60c.png" data-original-src="https://miro.medium.com/v2/resize:fit:510/format:webp/1*e2kqUNkVyitSJC9CqfseJQ.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">图4。预处理后的训练数据</figcaption></figure><pre class="jm jn jo jp fd ki kj kk kl aw km bi"><span id="fc00" class="kn ko hi kj b fi kp kq l kr ks">#Splitting the data to train(75%) and test(25%)<br/>x_train,x_test,y_train,y_test=train_test_split(df,y,test_size=0.25)</span></pre><p id="746a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">步骤3:实现欧几里德距离以找到最近的邻居</strong></p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es kw"><img src="../Images/1c19b3755494cc0a10132a24037f567c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CyaKPCFb_ywnY1bHj6gQng.jpeg"/></div></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">图5。将数据分类成两个群</figcaption></figure><p id="dbf7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">让我们试着理解图5。首先，我们在两个聚类中有三个特征，类似于我们的西雅图降雨量数据。让我们假设聚类1在不下雨时是数据点，聚类2在下雨时是数据点。前两个集群可以被认为是我们的训练数据。两个聚类之外的三个数据点是我们必须找到标签的测试数据。</p><p id="6146" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">现在，我们将使用<strong class="il hj">欧几里德距离</strong>来计算训练数据和测试数据之间的距离。我们也可以使用不同的方法来计算距离，如<strong class="il hj">曼哈顿距离、闵可夫斯基距离</strong>等。</p><p id="b89a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">欧氏距离:- </strong>用于求两点间的直线距离。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es kx"><img src="../Images/39d4153f9aab5a3cee21819fc2d72664.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*vyxN5mMaR3oeUG126nmE5Q.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">x和y之间的欧几里德距离</figcaption></figure><p id="6f39" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">由于KNN是非参数的，即它不对输入的概率分布做任何假设，我们从训练数据中找到测试数据点的距离，并且考虑接近测试数据的数据点来分类我们的测试数据。让我们借助一张图片来看看这一点</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es ky"><img src="../Images/f8403f3f18eeb6b71e23fac3dffba26b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*UAe9ujfJWsAkbKEP6xYQ8g.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">图6。计算训练数据和测试数据之间的欧几里德距离</figcaption></figure><p id="4299" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">我们看到，在图像中，每个特征有5个数据点，2个分类在一个聚类中，2个分类在另一个聚类中，1个尚未标记。一旦我们计算出这些数据点之间的欧几里德距离，我们就可以对这些数据点所在的聚类进行分类。</p><p id="7268" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">KNN中的K表示我们必须考虑的计数，假设K为5，在这种情况下，它将考虑前5个最短距离，在此之后，具有更多频率的类将被视为测试数据的类。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es kz"><img src="../Images/9704bb041f8c506f817bb3e137a1ed69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*muVDQP-z7jX-2RJh3rjr-A.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">图7。k为5时的最近邻居</figcaption></figure><p id="70ad" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">为了理解K的目的，我们只取了一个自变量，如图7所示，具有2个标签，即二进制分类，并且在计算距离之后，我们标记了5个最近的邻居，因为K被赋予值5。现在我们可以看到，蓝色标签的频率(蓝色为3，红色为2)高于红色标签的频率，我们将测试数据点标记为蓝色。找出K的值是K-NN中的一项任务，因为它可能需要通过尝试不同的值并基于这些值评估模型来进行多次迭代。</p><pre class="jm jn jo jp fd ki kj kk kl aw km bi"><span id="3b2c" class="kn ko hi kj b fi kp kq l kr ks">def KNN(x,y,k):<br/>    dist = [] <br/>    #Computing Euclidean distance<br/>    dist_ind = np.sqrt(np.sum((x-y)**2, axis=1)) <br/>    #Concatinating the label with the distance<br/>    main_arr = np.column_stack((train_label,dist_ind))<br/>    #Sorting the distance in ascending order<br/>    main = main_arr[main_arr[:,1].argsort()] <br/>    #Calculating the frequency of the labels based on value of K<br/>    count = Counter(main[0:k,0])<br/>    keys, vals = list(count.keys()), list(count.values())<br/>    if len(vals)&gt;1:<br/>        if vals[0]&gt;vals[1]:<br/>            return int(keys[0])<br/>        else:<br/>            return int(keys[1])<br/>    else:<br/>        return int(keys[0])</span></pre><p id="1ae1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">第四步:计算准确度和分类报告</strong></p><pre class="jm jn jo jp fd ki kj kk kl aw km bi"><span id="784b" class="kn ko hi kj b fi kp kq l kr ks">print(classification_report(pred,train_label))</span></pre><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es la"><img src="../Images/1dd2413c607b7edbdd0306522be7920e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*cHVFB6MNSSwP1MVPbMHo7w.png"/></div></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">图8。带有训练数据的分类报告</figcaption></figure><pre class="jm jn jo jp fd ki kj kk kl aw km bi"><span id="2155" class="kn ko hi kj b fi kp kq l kr ks">print(classification_report(pred,test_label))</span></pre><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es lb"><img src="../Images/010d5663a381bbf7c19bc0b0cb5aa80d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1318/format:webp/1*qNg9Ux15k33aoW5rHqLx2Q.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">图9。测试数据分类报告</figcaption></figure><p id="d9a9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">我们在训练数据上取得了96%的准确率，在测试数据上取得了94%的准确率，不错！</p></div><div class="ab cl kb kc gp kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="hb hc hd he hf"><h1 id="0191" class="lc ko hi bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">何时应用欧氏距离的KNN</h1><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es lz"><img src="../Images/2b2e307b4ae1619e5bd8316351299d04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jYjSpLaD45Za2OnH"/></div></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">图10。由<a class="ae jk" href="https://unsplash.com/@juniorferreir_?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">朱尼尔·费雷拉</a>在<a class="ae jk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="b070" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">一个很常见的问题，什么时候应用哪种算法？</p><p id="3a46" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">答案并不简单，但它取决于我们手头的数据和任务，至于欧氏距离的KNN，当我们拥有连续数据类型的结构化数据时，我们可以希望获得更好的结果。这也是一个假设，我们可以通过模型的多种实现来证明。</p></div><div class="ab cl kb kc gp kd" role="separator"><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg kh"/><span class="ke bw bk kf kg"/></div><div class="hb hc hd he hf"><p id="5ef0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">使用的参考资料:- </strong></p><div class="ma mb ez fb mc md"><a href="https://brilliant.org/wiki/k-nearest-neighbors/" rel="noopener  ugc nofollow" target="_blank"><div class="me ab dw"><div class="mf ab mg cl cj mh"><h2 class="bd hj fi z dy mi ea eb mj ed ef hh bi translated">k-最近邻|精彩的数学和科学维基</h2><div class="mk l"><h3 class="bd b fi z dy mi ea eb mj ed ef dx translated">k-最近邻(或简称为k-NN)是一种简单的机器学习算法，它通过使用输入的k…</h3></div><div class="ml l"><p class="bd b fp z dy mi ea eb mj ed ef dx translated">brilliant.org</p></div></div><div class="mm l"><div class="mn l mo mp mq mm mr jv md"/></div></div></a></div><div class="ma mb ez fb mc md"><a href="https://scikit-learn.org/stable/modules/neighbors.html#classification" rel="noopener  ugc nofollow" target="_blank"><div class="me ab dw"><div class="mf ab mg cl cj mh"><h2 class="bd hj fi z dy mi ea eb mj ed ef hh bi translated">1.6.最近邻-sci kit-了解0.23.2文档</h2><div class="mk l"><h3 class="bd b fi z dy mi ea eb mj ed ef dx translated">为无监督和有监督的基于邻居的学习方法提供功能。无监督的最近邻居…</h3></div><div class="ml l"><p class="bd b fp z dy mi ea eb mj ed ef dx translated">scikit-learn.org</p></div></div></div></a></div><p id="6b9b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated"><strong class="il hj">Github上提供的代码:- </strong></p><div class="ma mb ez fb mc md"><a href="https://github.com/Akshit2409/KNN/blob/main/KNNWithNumpy.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="me ab dw"><div class="mf ab mg cl cj mh"><h2 class="bd hj fi z dy mi ea eb mj ed ef hh bi translated">阿克什特2409/KNN</h2><div class="mk l"><h3 class="bd b fi z dy mi ea eb mj ed ef dx translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="ml l"><p class="bd b fp z dy mi ea eb mj ed ef dx translated">github.com</p></div></div><div class="mm l"><div class="ms l mo mp mq mm mr jv md"/></div></div></a></div></div></div>    
</body>
</html>