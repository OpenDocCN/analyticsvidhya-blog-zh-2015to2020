<html>
<head>
<title>Detailed Solution to Mercedes Benz Green Manufacturing Competition on Kaggle</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">卡格尔奔驰绿色制造竞赛的详细解决方案</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/detailed-solution-to-mercedes-benz-green-manufacturing-competition-on-kaggle-7b85c84a3ff5?source=collection_archive---------15-----------------------#2020-05-02">https://medium.com/analytics-vidhya/detailed-solution-to-mercedes-benz-green-manufacturing-competition-on-kaggle-7b85c84a3ff5?source=collection_archive---------15-----------------------#2020-05-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="753b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">目录:</strong></h1><ol class=""><li id="1447" class="jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju bi translated">商业问题</li><li id="8395" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">问题陈述</li><li id="bcdc" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">数据准备</li><li id="7378" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">探索性数据分析</li><li id="49cc" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">特征工程</li><li id="bcfd" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">数据预处理</li><li id="42f0" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">特征选择</li><li id="f49e" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">建模</li><li id="c882" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">摘要</li><li id="535b" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">测试数据预测</li><li id="af6b" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">结论和未来工作</li><li id="e8f4" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">参考</li></ol><h1 id="c9ef" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak"> 1。业务问题:</strong></h1><p id="b731" class="pw-post-body-paragraph ka kb hi jf b jg jh kc kd ji jj ke kf jk kg kh ki jm kj kk kl jo km kn ko jq hb bi translated">车辆测试是汽车制造过程中的一个重要方面。每辆车在交付给客户之前都必须通过一定的标准。梅赛德斯·奔驰提供各种各样的定制车辆。每辆车都必须经过测试，以确保车辆满足安全要求和排放标准。由于定制的原因，每个型号需要不同的试验台配置。由于型号较多，需要进行大量的试验。更多的测试导致在测试台上花费更多的时间，增加了梅赛德斯-奔驰的成本，并产生更多的二氧化碳(温室污染气体)。</p><p id="7c58" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">在Kaggle上举办的梅赛德斯-奔驰绿色制造竞赛旨在通过开发一种机器学习模型来优化车辆测试过程，该模型可以预测车辆在测试台上花费的时间。最终目标是减少花费在测试台上的时间，从而减少测试阶段的二氧化碳排放。这项研究的数据集是由奔驰公司提供的。数据可以从这个<a class="ae ku" href="https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/data" rel="noopener ugc nofollow" target="_blank">链接</a>下载。</p><h1 id="5888" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak"> 2。问题陈述:</strong></h1><p id="2d4c" class="pw-post-body-paragraph ka kb hi jf b jg jh kc kd ji jj ke kf jk kg kh ki jm kj kk kl jo km kn ko jq hb bi translated">任务是开发一个机器学习模型，可以根据车辆配置预测汽车在测试台上花费的时间。车辆配置被定义为为特定车辆选择的一组定制选项和特征。问题背后的动机是，一个准确的模型将能够通过允许具有相似测试配置的汽车连续运行来减少测试车辆所花费的总时间。这个问题是一个有监督的机器学习回归任务，因为它涉及到通过对有标签的训练数据进行学习来基于一组独立变量预测连续的目标变量。</p><p id="3b85" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">评估度量是R平方，也称为决定系数。它量化了由特征解释的目标变量的变化百分比。r平方值可以介于0和1之间。R平方的最佳可能值为1，表示目标变量的所有变化都可以由输入要素来解释。</p><h1 id="0fa0" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak"> 3。数据准备:</strong></h1><p id="86aa" class="pw-post-body-paragraph ka kb hi jf b jg jh kc kd ji jj ke kf jk kg kh ki jm kj kk kl jo km kn ko jq hb bi translated">梅赛德斯-奔驰为此比赛提供了两个数据集，即train.csv和test.csv。文件train.csv是机器学习模型必须在其上开发的标记数据集。test.csv文件是进行预测的数据集。训练和测试数据都包含377个特征，这些特征代表了车辆测试阶段的车辆配置。这些特征有诸如“X0”、“X1”、“X2”等名称。有一个特征“ID ”,代表分配给每个车辆试验的ID。这些特征是匿名的，没有任何物理表示。数据描述表明，这些功能是配置选项，如悬架设置、自适应巡航控制、全轮驱动和许多不同的选项，它们共同定义了一个汽车模型。下图显示了训练数据的子集。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="c6d1" class="le ig hi la b fi lf lg l lh li">#Load dataset<br/>data = pd.read_csv('train.csv')<br/>data.head()</span></pre><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lj"><img src="../Images/be5917a332e7f341582481bca9dce40d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XMjN34ixPc6pafew3qG_Hg.jpeg"/></div></div></figure><p id="f1e0" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">有377个特征，其中368个是二元的，8个是分类的，1个是连续的。目标变量y是连续值，代表车辆进行试验所用的时间，单位为秒。数据集中不存在缺失值。train.csv文件分为训练集和验证集。下图显示了该操作的代码。</p><pre class="kv kw kx ky fd kz la lb lc aw ld bi"><span id="7d78" class="le ig hi la b fi lf lg l lh li">#Separate the dependent and independent features<br/>X = data.drop(columns=['y'])<br/>Y = data['y']</span><span id="17f8" class="le ig hi la b fi lr lg l lh li">#Split the dataset<br/>X_train, X_val, y_train, y_val = train_test_split(X, Y, <br/>test_size=0.2, random_state=25)</span><span id="87f3" class="le ig hi la b fi lr lg l lh li">#Concatenate X_train and y_train<br/>train_data = pd.merge(X_train,y_train.to_frame(),left_index=True, right_index=True)<br/>train_data.head()</span></pre><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es ls"><img src="../Images/a8619da6e7d24438a381e24686ee5159.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c2ycORHccBYdBpzlkGpgHw.jpeg"/></div></div></figure><p id="7445" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">对上述代码中提到的train_data执行探索性数据分析(EDA)。</p><h1 id="9d17" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak"> 4。探索性数据分析:</strong></h1><p id="0853" class="pw-post-body-paragraph ka kb hi jf b jg jh kc kd ji jj ke kf jk kg kh ki jm kj kk kl jo km kn ko jq hb bi translated"><strong class="jf hj"> 4.1。分析目标/因变量:</strong></p><p id="7292" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">下图包含目标变量的直方图和箱线图。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lt"><img src="../Images/b66ef632a810a92285b9659ad6b5ee34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nQuRGWyBjjUroHprAcAU_A.jpeg"/></div></div></figure><p id="786a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">目标变量的平均值为100秒。目标值高于137.5的点可以从箱线图中推断为异常点。对于这个比赛，高于150的点被分类为异常点，并且它们被从训练集中移除。</p><p id="105e" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 4.2。分析分类变量:</strong></p><p id="fc7a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">有8个分类特征，即X0、X1、X2、X3、X4、X5、X6和X8。对于这些特征中的每一个，绘制了唯一值的计数直方图和唯一值的箱线图。</p><p id="b712" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">4.2.1。X0特性:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lu"><img src="../Images/3e98e836ca14408fb960f6538c13f110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ESAXsTNZ8CPaBTf8QgNIoA.jpeg"/></div></div></figure><p id="dcfb" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">从上述图中观察到:</p><ol class=""><li id="54ea" class="jd je hi jf b jg kp ji kq jk lv jm lw jo lx jq jr js jt ju bi translated">aa，ab，g和ac只出现一次。</li><li id="b649" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">z、y、t、o、f、n、s、al、m、ai、e、ba、aq、am、u、I、ad和b的箱线图几乎相同。这些类别的平均值接近93。</li><li id="4564" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">ak、x、j、w、af、at、a、ax、I、au、as、r和c的箱线图几乎相同。这些类别平均值接近110。</li><li id="55f5" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">因此，在X0的不同类别之间似乎存在分组。</li></ol><p id="1d38" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 4.2.2。X1功能:</strong></p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es ly"><img src="../Images/401deb10e759d5b643c7e4602d7dcd18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MDq6B69PAGJ-5CncDdiCHA.jpeg"/></div></div></figure><p id="d515" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">从上述图中观察到:</p><ol class=""><li id="e06a" class="jd je hi jf b jg kp ji kq jk lv jm lw jo lx jq jr js jt ju bi translated">X1的大多数类别的平均值为100。</li><li id="f27d" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">X1类别的y与其他类别明显分开。</li></ol><p id="29a2" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 4.2.3。X2专题:</strong></p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lz"><img src="../Images/1165678c069da2bb6e6ac225cf8b6ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BV3o82pNgQS3JqRxFN9ikQ.jpeg"/></div></div></figure><p id="44d7" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">从上述图中观察到:</p><ol class=""><li id="1581" class="jd je hi jf b jg kp ji kq jk lv jm lw jo lx jq jr js jt ju bi translated">ae类别在X2占据主导地位。39%的X2值为ae。</li><li id="0e96" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">与X0相似，X2似乎也有分组。X2的分组比X0少。</li><li id="701e" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">X2的大多数类别的平均值接近97。</li></ol><p id="6da6" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 4.2.4。X3专题:</strong></p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lt"><img src="../Images/063a371b85de0e0b6f4723132f0c946e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eKi8Ws5_OrXS3deAFFkOjw.jpeg"/></div></div></figure><p id="4e32" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">从上述图中观察到:</p><ol class=""><li id="7ff6" class="jd je hi jf b jg kp ji kq jk lv jm lw jo lx jq jr js jt ju bi translated">c类在X3占主导地位。X3 46%的值是c</li><li id="71a9" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">几乎所有X3的分类都有100的平均值。</li><li id="81f1" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">在X3的各个类别中，因变量y的变化似乎较小。X3的大多数类别的箱线图是匹配的。</li></ol><p id="6d35" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">4.2.5。X4特性:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es ma"><img src="../Images/b339e924ff300432f10c6abd2a861752.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1o49jp_0loHba1z2r2Wowg.jpeg"/></div></div></figure><p id="cdbd" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">从上述图中观察到:</p><ol class=""><li id="ca6b" class="jd je hi jf b jg kp ji kq jk lv jm lw jo lx jq jr js jt ju bi translated">X4中d品类占主导地位。X4的99.9%的值是d。</li><li id="d782" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">必须删除此功能，因为该功能中不存在差异。</li></ol><p id="7521" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 4.2.6。X5特性:</strong></p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mb"><img src="../Images/104afb13c991ee23b5048b4fd8723f1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jELU5AoXuzq-57RKjUSVFA.jpeg"/></div></div></figure><p id="8ff2" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">从上述图中观察到:</p><ol class=""><li id="f1bb" class="jd je hi jf b jg kp ji kq jk lv jm lw jo lx jq jr js jt ju bi translated">x，h，g，y和u在数据中很少出现。</li><li id="2a72" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">X5所有类别的平均值接近98。</li><li id="71e4" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">在X5的类别中，因变量y的变化似乎较小。X5的大多数类别的箱线图是匹配的。</li></ol><p id="bf2e" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 4.2.7。X6特性:</strong></p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mc"><img src="../Images/ec6729f0794abca25bbb1e72b237fdcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yHEjITt7JAWddAFU_aS_RQ.jpeg"/></div></div></figure><p id="94c6" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">从上述图中观察到:</p><ol class=""><li id="954e" class="jd je hi jf b jg kp ji kq jk lv jm lw jo lx jq jr js jt ju bi translated">j、g、d和I的箱线图相互匹配。</li><li id="cdb4" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">X6的大多数类别的平均值在100左右。</li><li id="c4d4" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">在X6的类别中，因变量y的变化似乎较小。X6的大多数类别的箱线图是匹配的。</li></ol><p id="2934" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">4.2.8。X8特性:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es md"><img src="../Images/9a4e32ca64195e8b9259251fefdd0079.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rwURTKarp4k_OnJPk_4YqA.jpeg"/></div></div></figure><p id="d201" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">从上述图中观察到:</p><ol class=""><li id="ac81" class="jd je hi jf b jg kp ji kq jk lv jm lw jo lx jq jr js jt ju bi translated">X8的大多数类别的平均值接近100。</li><li id="2b0e" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">在X8的类别中，因变量y的变化似乎较小。X8的大多数类别的箱线图是匹配的。</li></ol><p id="1782" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj">分类特征总结:</strong></p><p id="876b" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">具有X3、X5、X6、X8，并且在一定程度上X1显示了在不同级别中非常相似的分布。</p><p id="40e5" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 4.3。分析二进制变量:</strong></p><p id="b44f" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">下图显示了每个二元特征中零的百分比。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es me"><img src="../Images/3435280ce7e025e0b12a7f316b78e7e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fhs1HSMrnoL_I3y0xb9Mqg.jpeg"/></div></div></figure><p id="bb1e" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">99%的值为0或1的二进制特征在建立任何机器学习模型时都没有多大用处。因此，这些要素将从数据集中删除。在训练集中，有146个这样的二元特征。</p><p id="7cdd" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">在这些二元特征中，某些特征是重复的。为所有特征绘制相关矩阵。相关系数大于99%的要素将从数据集中删除。在训练集中，识别了27个这样的特征。</p><p id="f56b" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">移除冗余和重复特征后，二进制特征的数量为195(368–146–27)。</p><p id="07c0" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 4.4。分析“ID”特征:</strong></p><p id="3b92" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">下图是ID特征的Reg-plot。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mf"><img src="../Images/c076f51c3802a044f7383776ea02b543.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLb1r94pFbP1OZUw0pzCrQ.jpeg"/></div></div></figure><p id="6763" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">y是上图中的目标变量。y相对于ID略微减小。因此，ID在估计目标变量时是一个有用的特征。</p><p id="f4b5" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 5。特征工程:</strong></p><p id="e985" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">特征工程是建立机器学习模型之前的主要步骤之一。在此步骤中，从现有特征设计特征。在此竞赛中，数据集中的二元特征用于设计合成特征。创建新功能有两种方式，如下所示。</p><ol class=""><li id="f008" class="jd je hi jf b jg kp ji kq jk lv jm lw jo lx jq jr js jt ju bi translated">每个二元特征被添加到剩余的二元特征之一。将发展的特征和目标变量之间的相关性与阈值进行比较。如果相关系数大于阈值，则开发的特征包括在数据集中。本次比赛选择的阈值为0.65。下面是这个过程的代码。</li></ol><figure class="kv kw kx ky fd lk"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="acf4" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">因为有195个二元特征，所以对于每个二元特征，将有194个合成特征。在这194个特征中，挑选相关系数(目标变量和合成特征之间)大于0.65的特征。运行上述代码后获得的特征数是5。</p><p id="4b64" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">2.每个二元特征被添加到剩余的两个二元特征中。将发展的特征和目标变量之间的相关性与阈值进行比较。如果相关系数大于阈值，则开发的特征包括在数据集中。本次比赛选择的阈值为0.65。下面是这个过程的代码。</p><figure class="kv kw kx ky fd lk"><div class="bz dy l di"><div class="mg mh l"/></div></figure><p id="1df5" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">运行上述代码后获得的特征数是354。</p><p id="ca93" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">因此，从以上两个步骤获得的总特征是359。在这359个特征中，某些特征是重复的。识别并移除45个重复特征。因此，从特征工程获得的总特征是314。</p><h1 id="4120" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak"> 6。数据预处理:</strong></h1><p id="96cd" class="pw-post-body-paragraph ka kb hi jf b jg jh kc kd ji jj ke kf jk kg kh ki jm kj kk kl jo km kn ko jq hb bi translated"><strong class="jf hj"> 6.1。分类特征的标签编码:</strong></p><p id="7d6e" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">在标签编码中，特定特征的每个唯一类别被分配一个编号。对于本次竞赛，出现次数少于或等于5次的类别将被忽略。为每个分类特征创建一个以关键字为类别、以值为标签的字典。标签从1开始分配。为字典中不存在的类别分配了0标签。</p><p id="3f68" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 6.2。归一化非二进制特征</strong></p><p id="1315" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">为了确保所有特征都在相同的范围内，所有非二进制特征都被归一化为具有0和1之间的值。非二进制特征包括ID、X0、X1、X2、X3、X5、X6和X8。</p><p id="bd9a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 7。功能选择:</strong></p><p id="9f70" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">经过EDA、特征工程和数据预处理后，训练集中的特征数为517。特征的数量很大。如果所有的特征都作为任何机器学习模型的输入，那么很有可能会遇到维数灾难。模型的训练时间随着特征数量的增加和模型复杂度的增加而增加。</p><p id="d964" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">为了克服上述障碍，可以进行特征选择以挑选出那些在它们之间具有高度可变性并且在预测目标变量中有用的特征。随机森林算法用于特征选择。随机森林回归算法在所有517个特征上被训练。该模型也被调整以获得最佳的超参数集。下表列出了最佳超参数集的值。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mi"><img src="../Images/5551b02631d074bf16d45a12b4906fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nM4ATRcfFQr4VXvqKtcJuQ.jpeg"/></div></div></figure><p id="9391" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">随机森林回归器的sklearn实现有一个获取特征重要性的方法。下面是具有相对重要性的前10项功能的条形图。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div class="er es mj"><img src="../Images/385b691362aaf98d9a1913f9e5f90c5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*L5j3Nq6c1X3Kjdbo3iZE1Q.jpeg"/></div></figure><p id="7dc9" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">前10个特征主要包括在特征工程过程中创建的合成特征。X119、X232和ID是已经存在的特征。该模型针对不同的算法在前10个或20个特征上进行训练，这确保避免了维数灾难问题，并且时间复杂度显著降低。</p><h1 id="3a56" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak"> 8。造型:</strong></h1><p id="5d9b" class="pw-post-body-paragraph ka kb hi jf b jg jh kc kd ji jj ke kf jk kg kh ki jm kj kk kl jo km kn ko jq hb bi translated">该模型基于不同的算法进行训练，即线性回归、支持向量机(SVM)、k-最近邻(k-NN)、决策树、随机森林、XGBoost、多层感知器(深度学习)和各种形式的堆叠。对于每个算法，都要进行超参数调整，以获得最佳的超参数集。</p><p id="fbda" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 8.1。线性回归:</strong></p><p id="a93f" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">sklearn的SGDRegressor类用于训练线性回归模型。下表列出了线性回归的最佳超参数集。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mk"><img src="../Images/41d996f8c4f78b64cede498690dd91fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o-Toi-qeqTEhJoY6v9AgKQ.jpeg"/></div></div></figure><p id="01fe" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">获得的性能指标如下:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es ml"><img src="../Images/dfc9b87b6cb96ccbbc776788c9d79012.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FlpAkDovPHRY4hJCB0mtlw.jpeg"/></div></div></figure><p id="a96c" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 8.2。支持向量机:</strong></p><p id="0cca" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">8.2.1。线性SVM: </p><p id="0a8d" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">sklearn的LinearSVR类用于训练线性SVM模型。下表列出了该模型的最佳超参数集。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mk"><img src="../Images/e01c6e91242d86f62d298bcf247e220a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LVGpsSvGuKF33eLNQ0fg0g.jpeg"/></div></div></figure><p id="b7cb" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">获得的性能指标如下:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mm"><img src="../Images/58112d19862464174a9e3a00747a76df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SFF5we_AMsPBZrq5OFAJow.jpeg"/></div></div></figure><p id="437b" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 8.2.2。内核SVM: </strong></p><p id="e496" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">sklearn的SVR类用于训练核SVM模型。使用的核是径向基函数(RBF)。</p><p id="6ff3" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">RBF核定义如下:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div class="er es mn"><img src="../Images/457ab50bbe24484357b97f56cc69db95.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*kcV1MAIIIAfJCya7uwAZDQ.jpeg"/></div></figure><p id="707a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">下表列出了该模型的最佳超参数集。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mo"><img src="../Images/f9264b64aa93d87be69103492a220ddb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*99uA9XNpRyeq4LJ33d2lxw.jpeg"/></div></div></figure><p id="3740" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">获得的性能指标如下:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mp"><img src="../Images/05e3228ff1d179b8812c7d8d0e754163.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jrQ_a0CuEBhmsqUaROLkUw.jpeg"/></div></div></figure><p id="4f1a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 8.3。k-最近邻:</strong></p><p id="1d92" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">sklearn的KNeighborsRegressor类用于训练k-NN模型。下表列出了该模型的最佳超参数集。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mp"><img src="../Images/070cd978f50e5aac403b69230e455f2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3MgJefrj7eC-r91VaVFiNg.jpeg"/></div></div></figure><p id="af1c" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">获得的性能指标如下:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mo"><img src="../Images/d4487c410ad4c68cde6817f981c01066.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O2DheMaxhRr4OkStTnqk0g.jpeg"/></div></div></figure><p id="1e24" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 8.4。决策树:</strong></p><p id="f002" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">sklearn的DecisionTreeRegressor类用于训练决策树模型。下表列出了该模型的最佳超参数集。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mk"><img src="../Images/02d7f5027d0b9fa649e5764057ce8ddd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UXsNWFLb8_VuPRX8RoAN-Q.jpeg"/></div></div></figure><p id="da44" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">获得的性能指标如下:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mo"><img src="../Images/fb9c1c2dade24011a4000eb44e15f8aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0oG7Ryv57FZyQ6BGwxxM5Q.jpeg"/></div></div></figure><p id="2745" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 8.5。随机森林:</strong></p><p id="50c3" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">sklearn的RandomForestRegressor类用于训练随机森林模型。下表列出了该模型的最佳超参数集。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mo"><img src="../Images/7fed6dabfc85f6ab31d405c01a799e1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LhquJ5NmkBoZOBeMFmBNpA.jpeg"/></div></div></figure><p id="279a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">获得的性能指标如下:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mo"><img src="../Images/8f3e7831002b78f2127bd21ec473012e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bRyDYnivLhNRSL-8Et_X0Q.jpeg"/></div></div></figure><p id="4b32" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 8.6。XGBoost: </strong></p><p id="42fa" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">xgboost库的XGBRegressor类用于训练XGBoost模型。下表列出了该模型的最佳超参数集。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mq"><img src="../Images/3b1a66b19c4df7cb6415b3d79890d8eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u70vM4D6xkyhGcQsBe6sbw.jpeg"/></div></div></figure><p id="2523" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">获得的性能指标如下:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mq"><img src="../Images/e348ebc531560f328fa52612c8de3380.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XMXWkdbBbSruYj-ru5fpbQ.jpeg"/></div></div></figure><p id="6e68" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 8.7。多层感知器(MLP): </strong></p><p id="8921" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">该模型考虑了从特征选择中获得的前8个特征。该模型由输入层、输出层和5个隐含层组成。模型架构如下图所示。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mr"><img src="../Images/0f97432afd5f200ffa2185f71499c25f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pc1hfcw6ECpbrr_hOBZ9Qw.jpeg"/></div></div></figure><p id="0eae" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">模型总结如下:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div class="er es ms"><img src="../Images/5a3f9fe96d98ffe5c39c1283dd662d68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*UXBdhbNlyYvajyEkChcZrQ.jpeg"/></div></figure><p id="544f" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">MLP模型是使用Keras库构建的。获得的性能指标如下:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mm"><img src="../Images/621c15efc55314c03f1920d995ee8924.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O0LQA54A1SwOmK-d4jZyzw.jpeg"/></div></div></figure><p id="2fd4" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 8.8。堆叠:</strong></p><p id="57d6" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">Ensemble是一种机器学习模型，它结合了在同一训练数据集上训练的两个或更多模型的预测。堆叠是一种集成学习技术，它通过元分类器或元回归器将多个分类或回归模型(称为基础模型)结合起来。基础模型在完整的训练集上被训练，然后元模型在这些基础模型的输出上被训练。</p><p id="4d80" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">所采用的不同种类的堆叠是用k-fold交叉验证的堆叠、用StackingCVRegressor的堆叠和用整体装袋的堆叠。</p><p id="0a35" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 8.8.1。k重交叉验证叠加:</strong></p><p id="cc5b" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">k重交叉验证技术包括将训练数据集分成k组，然后在维持集或测试集上使用k组示例中的每一组，而剩余的示例用作训练集。这意味着k个不同的模型被训练和评估。训练数据集中的每个样本都被分配到一个单独的组中，并一直留在该组中，直到模型被训练好。因此，训练数据集中的每个样本在维持集中出现一次，在训练集中出现k-1次。</p><p id="a091" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">出折叠(OOF)预测是在k折叠交叉验证过程中对维持集进行的预测。聚集的OOF预测提供了有关模型在不用于训练模型时如何对训练数据集中的每个示例执行的信息。</p><p id="4602" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">对每个基础模型执行k倍交叉验证过程，并且收集所有OOF预测。对于每个模型，将训练数据同样分割成k个折叠。因此，对于每个基本模型，训练数据集中的每个示例都有预测。(这些预测是从k重交叉验证技术中的维持集获得的)</p><p id="9d62" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">k重交叉验证堆叠中使用的程序可总结如下:</p><p id="9763" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">1.整个数据集(train.csv)分为训练数据集和验证数据集。</p><p id="9d60" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">2.对于每个基本型号:</p><p id="3623" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">a.对训练数据集进行k重交叉验证。</p><p id="fa3d" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">b.收集每个维持集的OOF预测。这些预测被用作训练元模型的输入特征。</p><p id="211b" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">c.对在k重交叉验证期间训练的每个k模型的验证数据集进行预测。因此，验证数据集有k个预测。这k个预测的平均值作为验证数据集的输出。</p><p id="789a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">3.元模型是根据基础模型的预测来训练的。元模型的特征数量等于基础模型的数量。</p><p id="a67a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">上述过程如下图所示。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mt"><img src="../Images/392b143cd27276c1bd21eefa78b75dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IHaLHQz8IGLbl6vq0HFl1g.jpeg"/></div></div></figure><p id="880f" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">下表列出了本次比赛中使用的模型。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mu"><img src="../Images/dbfaece446e3b5c0f6d9b96a0eb14bdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HkgzIWazvon8debX2QJYNw.jpeg"/></div></div></figure><p id="38b7" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">获得的性能指标如下:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mo"><img src="../Images/09e97f88fe318ca3a50f3d9152fdef81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9T1x3e5qwFRq0zlhbAsmFA.jpeg"/></div></div></figure><p id="885d" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 8.8.2。使用StackingCVRegressor进行堆叠:</strong></p><p id="8b8a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">mlxtend库的StackingCVRegressor类实现了k重交叉验证的堆叠。StackingCVRegressor可以与sklearn库的GridSearchCV结合使用，用于调优基模型和元模型的超参数。</p><p id="a05b" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">下图显示了StackingCVRegressor的流程图。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div class="er es mv"><img src="../Images/70eb5c8f98e13a91f9cd51c2f28e9259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*_oxJ0ygZAxFGclQ4vGxRsA.jpeg"/></div><figcaption class="mw mx et er es my mz bd b be z dx translated">图片来源:<a class="ae ku" href="http://rasbt.github.io/mlxtend/user_guide/regressor/StackingCVRegressor/" rel="noopener ugc nofollow" target="_blank">http://rasbt . github . io/mlx tend/user _ guide/regressor/StackingCVRegressor/</a></figcaption></figure><p id="96a5" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">数据集被分成k倍。在k个连续回合中，k-1个折叠用于拟合第一级回归量，剩余的1个折叠用于进行预测。然后，预测值被叠加，并作为输入输入到二级回归变量，称为元回归变量。元回归模型被训练并用于生成最终预测。</p><p id="5cbd" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">下表列出了本次比赛中使用的模型。使用GridSearchCV对基本模型和元模型的max_depth参数和XGBoost进行了超调优。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es na"><img src="../Images/25ca756e96dfad804fc817f131cbc328.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ASbbUhPNtTwAsW_fC0f-NA.jpeg"/></div></div></figure><p id="8d12" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">获得的性能指标如下:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es mo"><img src="../Images/c260c99192a1700ce063abbb3d8290d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fAQkz40bcsp_Q165dGCUFw.jpeg"/></div></div></figure><p id="3e9a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 8.8.3。使用集合打包或自举聚合的堆叠:</strong></p><p id="9ea1" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">这种建模方法与使用k-fold交叉验证的叠加非常相似，只是稍有修改。在堆叠k重交叉验证的情况下，完整的训练数据用于k重交叉验证。在使用集合装袋进行堆叠的情况下，仅提供一个数据样本进行k倍交叉验证。对于这个比赛，90%的训练数据被取样并提供给交叉验证。</p><p id="5c4d" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">整体装袋堆垛中使用的程序可总结如下:</p><p id="85be" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">1.整个数据集(train.csv)分为训练数据集和验证数据集。</p><p id="1ddb" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">2.对于每个基本型号:</p><p id="6451" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">a.从训练数据集中抽取90%的数据。对采样的训练数据集进行k重交叉验证。未被采样的数据被称为样本外数据集。</p><p id="8cb7" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">b.收集每个维持集的OOF预测。在k重交叉验证期间，对每个k模型的样本外数据集进行预测。因此，样本外数据集有k个预测。这k个预测的平均值作为样本外数据集的输出。聚集的OOF预测和样本外数据集的输出一起用作训练元模型的输入特征。</p><p id="17c6" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">c.对在k重交叉验证期间训练的每个k模型的验证数据集进行预测。因此，验证数据集有k个预测。这k个预测的平均值作为验证数据集的输出。</p><p id="a87e" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">3.元模型是根据基础模型的预测来训练的。元模型的特征数量等于基础模型的数量。</p><p id="d4cf" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">上述过程如下图所示。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es nb"><img src="../Images/579933e484b6f1e1a7e267232c7d2806.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sj0W63gHQcm17NTafkkUzQ.jpeg"/></div></div></figure><p id="2e5a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">下表列出了本次比赛中使用的模型。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es nc"><img src="../Images/1e7469046f0c39e2434877cc9178b028.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cwUP9rtfpNlndDYyE5zOYQ.jpeg"/></div></div></figure><p id="c7f2" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">验证R平方小于0.595的基本模型将被删除。然后，元模型根据剩余基础模型的预测进行拟合。</p><p id="7229" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">获得的性能指标如下:</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es nd"><img src="../Images/ea7eae5fbc48ee939fce4aae7013dcee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O654NGC5m9b_1U7yVfACgA.jpeg"/></div></div></figure><p id="c67d" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 9。概要:</strong></p><p id="f944" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">不同算法的验证R平方值总结如下。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es ne"><img src="../Images/9cd873b3dbc735a2c89b01b9ef04a21e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vcDbpaYn3Y0KEhyRBasedw.jpeg"/></div></div></figure><p id="1f7a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 10。测试数据预测:</strong></p><p id="e25a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">test.csv文件提供了预测所需的数据。线性回归和线性SVM的验证R平方远低于其他模型。因此，在对试验数据进行预测时，这些模型被忽略。</p><p id="53d4" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">当堆叠模型与集合装袋一起使用时，观察到测试数据的最佳得分。在进行预测时，考虑了除MLP以外的所有基础模型的平均值。下面是Kaggle评分截图。</p><figure class="kv kw kx ky fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es nf"><img src="../Images/215205de67b8952cac9e0e7e79f0754c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kruGv7BOoXYvEaqhX31h5A.jpeg"/></div></div></figure><p id="1d25" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 11。结论和未来工作:</strong></p><p id="e36f" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">考虑到除MLP之外的所有模型的平均值，用系综打包获得的堆积模型的私人得分是0.55246。在Jupyter笔记本上写的全部代码可在<a class="ae ku" href="https://github.com/NitishVSawant/Mercedes-Benz-Green-Manufacturing" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><p id="ae70" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">为改进该解决方案，可以做出如下改进:</p><ol class=""><li id="5d83" class="jd je hi jf b jg kp ji kq jk lv jm lw jo lx jq jr js jt ju bi translated">对分类变量使用不同种类的编码</li><li id="22f9" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">增加堆叠中基本型号的数量</li><li id="e34e" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">CNN-LSTM模型的使用</li><li id="e230" class="jd je hi jf b jg jv ji jw jk jx jm jy jo jz jq jr js jt ju bi translated">使用特征工程开发新特征</li></ol><p id="9eea" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated"><strong class="jf hj"> 12。参考文献:</strong></p><p id="6c04" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">1.<a class="ae ku" href="https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-mercedes" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/sudalairajkumar/simple-exploration-notebook-Mercedes</a></p><p id="3a42" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">2.<a class="ae ku" href="https://www.kaggle.com/deadskull7/78th-place-solution-private-lb-0-55282-top-2" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/deads kull 7/78 th-place-solution-private-l b-0-55282-top-2</a></p><p id="907e" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">3.<a class="ae ku" href="https://www.kaggle.com/umbertogriffo/deep-learning" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/umbertogriffo/deep-learning</a></p><p id="949a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">4.<a class="ae ku" href="https://www.kaggle.com/eikedehling/stack-of-svm-elasticnet-xgboost-rf-0-55" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/eike dehling/stack-of-SVM-elastic net-xgboost-RF-0-55</a></p><p id="7c8b" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">5.<a class="ae ku" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></p><p id="b88c" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">6。https://machinelingmastery . com/out-of-fold-predictions-in-machine-learning/</p><p id="f72a" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">7 .<a class="ae ku" href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" rel="noopener ugc nofollow" target="_blank">。https://www . analyticsvidhya . com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/</a></p><p id="89db" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">领英简介:【https://www.linkedin.com/in/nitish-sawant-a991b8154/ T2】</p><p id="9ba7" class="pw-post-body-paragraph ka kb hi jf b jg kp kc kd ji kq ke kf jk kr kh ki jm ks kk kl jo kt kn ko jq hb bi translated">电子邮件:menitishsawant@gmail.com</p></div></div>    
</body>
</html>