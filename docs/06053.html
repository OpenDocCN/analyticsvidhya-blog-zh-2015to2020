<html>
<head>
<title>Information from parts of words: subword models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">来自部分词的信息:子词模型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/information-from-parts-of-words-subword-models-e5353d1dbc79?source=collection_archive---------10-----------------------#2020-05-10">https://medium.com/analytics-vidhya/information-from-parts-of-words-subword-models-e5353d1dbc79?source=collection_archive---------10-----------------------#2020-05-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="73bc" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">这个故事包括4个部分:纯字符级模型的动机，子词模型:字节对编码和朋友，混合字符和词级模型，以及快速文本嵌入。这个故事是对<a class="ae ix" href="https://www.youtube.com/watch?v=9oTHFx0Gg3Q&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=12" rel="noopener ugc nofollow" target="_blank">斯坦福CS224N: NLP与深度学习| Winter 2019 |第12讲—子词模型</a>的总结。</h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es iy"><img src="../Images/170562f8f4510e5ac88905a2774f6822.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xNFqM9hJSB6i2XTR7vAQNw.png"/></div></div></figure><p id="cfed" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">这个故事包括四个部分。:纯字符级模型、子词模型的动机:字节对编码和友元、混合字符和词级模型以及快速文本嵌入。这个故事是<a class="ae ix" href="https://www.youtube.com/watch?v=9oTHFx0Gg3Q&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=12" rel="noopener ugc nofollow" target="_blank">Stanford cs 224n:NLP with Deep Learning | Winter 2019 |讲座12 —子词模型</a>的总结。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="e163" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">1.纯角色级别的模型</h1><p id="83a9" class="pw-post-body-paragraph jk jl hi jm b jn lf ij jp jq lg im js jt lh jv jw jx li jz ka kb lj kd ke kf hb bi translated"><em class="lk">为什么我们需要角色级别的模型？因为有些语言没有分词，</em>比如中文。即使是有词段的语言，他们也以不同的方式分割词，比如德语和荷兰语包含许多复合词。此外，我们需要处理大量的、开放的词汇，比如在线评论包含大量常见的非正式拼写。</p><p id="3d5a" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj">在字符级模型中，单词嵌入可以由字符嵌入组成。</strong>角色级模型至少有3个好处<strong class="jm hj"> : 1)。它可以为未知单词生成嵌入；2).相似的拼写共享相似的嵌入；3).它解决了OOV问题。</strong>字符级模型在某些情况下工作成功，这有点令人惊讶，因为传统上，音素或字母不是语义单位。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="34f4" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">2.子字模型:字节对编码和朋友</h1><h2 id="cc90" class="ll ko hi bd kp lm ln lo kt lp lq lr kx jt ls lt kz jx lu lv lb kb lw lx ld ly bi translated">2.1字节对编码</h2><p id="e76f" class="pw-post-body-paragraph jk jl hi jm b jn lf ij jp jq lg im js jt lh jv jw jx li jz ka kb lj kd ke kf hb bi translated"><em class="lk">字节对编码(BPE)最初是一种压缩算法</em>，它将最常见的字节对编码成一个新的字节。字节对编码的起源和字节对这个名称与自然语言处理或神经网络无关，但BPE的使用已经成为一种非常标准和成功的方式，用于向<strong class="jm hj"> <em class="lk">表示单词片段，允许你在实际使用有限词汇的同时拥有无限的有效词汇。</em>T19】</strong></p><p id="8d21" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">在压缩中，你用BPE做的是，你有一个字节的东西的集合，你寻找最频繁的2字节序列，然后你把这个2字节序列作为一个新元素添加到你的可能值的字典中。这意味着你可以有257个不同的字节值</p><p id="22a9" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">所以从本质上来说，这项工作建议我们可以应用这种压缩算法，并把它作为一种产生有用单词的方式，不严格地用字节来做，尽管有名字，而是用字符和字符n-grams来做。</p><p id="a92e" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">最常见的处理字符和字符n-gram的方法是，有Unicode，你可以表示所有这些可爱的字母，像加拿大因纽特人的音节之类的。但是Unicode实际上有一个问题，那就是实际上有很多Unicode字符(可能有200，000个),而且有点多。所以我们只挑选最常见的字符n元语法。</p><p id="82c5" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> BPE是一种分词算法</strong>，尽管它是以自底向上聚类的方式完成的。<strong class="jm hj"> <em class="lk">它以数据中所有(Unicode)字符的单字词汇表开始。它将最频繁的n元语法对封装到一个新的n元语法中。BPE有一个目标词汇量，当你达到它的时候就会停下来。它对单词进行确定性的最长分段，而分段仅在由一些先前的分词器(通常是用于机器翻译的Moses分词器)识别的单词内进行。<strong class="jm hj"> <em class="lk"> BPE自动为系统决定词汇。</em> </strong></em></strong></p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es lz"><img src="../Images/621777761d9e5b12016ca6860ce42681.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OrhMLzcmWhUTfuZR2-SFsA.png"/></div></div></figure><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es ma"><img src="../Images/c4b69c23f9f9884237df3b038c491fd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OF7-IMDWjxoNB76lyeFzZw.png"/></div></div></figure><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mb"><img src="../Images/f13a188268c7286e9bb2000de22ff33d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6ucVXiDhALKRq4P_wNG9AA.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated">来源:<a class="ae ix" href="https://www.youtube.com/watch?v=9oTHFx0Gg3Q&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=12" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="2677" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">例如，我们开始构建这些常见的字母序列，因此像<code class="du mg mh mi mj b">est</code>这样的常见位，但也只是常见的单词，像英语中的<code class="du mg mh mi mj b">that</code>这样的东西将很快聚集在一起，成为我们词汇的一个单元。所以通常我们所做的是决定我们想要使用的词汇量。例如，我想使用8000个单词的词汇量。我们一直这样做，直到我们的词汇量达到8000个。这意味着我们的词汇中会有所有的单个字母，因为我们是从它们开始的，它会有像<code class="du mg mh mi mj b">es</code>、<code class="du mg mh mi mj b">est</code>这样的单词的共同序列，但也有完整的单词，只要有共同的单词，像<code class="du mg mh mi mj b">the</code>、<code class="du mg mh mi mj b">too</code>、<code class="du mg mh mi mj b">with</code>等等，都是我们词汇的一部分。</p><p id="9d7c" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">因此，当我们有了一段文本，<em class="lk">我们可以做一个确定性的最长的单词分段。在这种情况下，我们将假定</em>是我们的<code class="du mg mh mi mj b">eest</code>字块。到目前为止，一段输入文本，我们变成了单词段。例如，我们想做一个机器翻译(MT)任务，然后我们就像使用单词一样在我们的MT系统中运行它，但实际上它是单词的片段，然后在输出端，我们只是根据需要将它们连接起来。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="c570" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">2.子字模型:字节对编码和朋友</h1><h2 id="ecbb" class="ll ko hi bd kp lm ln lo kt lp lq lr kx jt ls lt kz jx lu lv lb kb lw lx ld ly bi translated">2.2词块模型和句子块模型</h2><p id="959d" class="pw-post-body-paragraph jk jl hi jm b jn lf ij jp jq lg im js jt lh jv jw jx li jz ka kb lj kd ke kf hb bi translated">谷歌NMT(GNMT)使用BPE模型的变体:<strong class="jm hj">单词块模型和句子块模型</strong>。t <strong class="jm hj"> <em class="lk">不是char n-gram计数，而是使用贪婪近似来最大化语言模型对数似然来选择片段</em> </strong>和<em class="lk">添加n-gram，最大程度地减少困惑</em>。</p><p id="421e" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj">词块模型对词内进行分词:首先对词进行分词，然后应用BPE。</strong>如果我们需要先标记成单词，这可能会有问题，因为<em class="lk">然后我们需要为每种语言准备一个标记器</em>，这是一项繁重的工作。因此，也许我们可以从字符序列出发，保留空白，并将其视为成簇过程的一部分，你只需构建你的单词片段，这些片段通常在它们的一侧或另一侧有空格，因为单词中的内容通常是更常见的簇，你可以构建这些簇，这被证明是非常成功的。<strong class="jm hj">所以在SentencePiece模型中，空白作为一个特殊的标记(_)被保留，并正常分组。</strong> <em class="lk">你可以通过将碎片连接起来并记录到空格中来反转事情的结局</em>。</p><p id="dcfd" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">BERT 使用了工件模型的变体。BERT不处理单词，而是处理单词片段。所以它的词汇量很大，比较常见的词都在词汇表里，比如Fairfax。不过它涵盖了所有的词，对于OOV来说，它使用了词块的概念。因此，如果我想要一个代表OOV单词<code class="du mg mh mi mj b">hypatia</code>的符号，我会把它拼凑起来。<strong class="jm hj"> <em class="lk">有一个</em> </strong> <code class="du mg mh mi mj b"><strong class="jm hj"><em class="lk">h</em></strong></code> <strong class="jm hj"> <em class="lk">的表示，然后在BERT版本中，不同于Google NMT版本，非起始词段在开始用两个哈希表示</em> </strong>，所以我可以把它和<code class="du mg mh mi mj b">h</code><code class="du mg mh mi mj b">##yp</code>等放在一起，所以<code class="du mg mh mi mj b">h</code>、<code class="du mg mh mi mj b">##yp</code>、<code class="du mg mh mi mj b">##ati</code>、<code class="du mg mh mi mj b">##a</code>将是我对<code class="du mg mh mi mj b">hypatia</code>的表示。所以实际上，我有单词向量，四个单词的片段，然后我必须知道如何处理它们。最简单也是最常见的方法是，我对其中的4个进行平均。很明显你还可以做其他事情。你可以用ConvNet和max-pool或者你可以运行一个小LSTM或者别的什么来组合一个表示。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="fdbb" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">3.混合字符和单词级模型</h1><h2 id="c4ce" class="ll ko hi bd kp lm ln lo kt lp lq lr kx jt ls lt kz jx lu lv lb kb lw lx ld ly bi translated">3.1 CNN +人物层面的公路网</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mk"><img src="../Images/334d82396fad75d49b2b90224c33ca7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kSvIc4TlaOoAgKCz0LleHg.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated">模型架构。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=9oTHFx0Gg3Q&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=12" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es ml"><img src="../Images/b31b3c4518cab05aa05c11d2b5cdc58b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_-U4gCK0DgzhhaxGobG0tA.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated">来源:<a class="ae ix" href="https://www.youtube.com/watch?v=9oTHFx0Gg3Q&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=12" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="8779" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">让我们研究一下论文&lt;<a class="ae ix" href="http://arxiv.org/abs/1508.06615" rel="noopener ugc nofollow" target="_blank">字符感知神经语言模型</a> &gt;。为了编码子词相关性，解决词级模型的稀有词问题，用更少的参数获得可比较的表达能力，并导出跨多种语言有效的强大、健壮的语言模型，作者试图通过从字符开始并想要利用相关子词和稀有词的排序来建立良好的语言模型。</p><p id="557f" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">我们从用字符表示的单词开始。我们有嵌入到卷积网络中的字符，然后我们有向上的字符。因此，如果我们一次一个字符，每个字符都有一个字符嵌入，那么就会有一个卷积层，它有各种滤波器，可以处理2、3和4克字符的字符序列，所以你会得到单词部分的表示。然后，从这些卷积网络中，随时间推移进行最大轮询，这实际上有点像选择这些n元语法中的哪一个最能代表一个单词的含义。</p><figure class="iz ja jb jc fd jd"><div class="bz dy l di"><div class="mm mn l"/></div></figure><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mo"><img src="../Images/8fbeb8641ef87e979cc62cefdb6818b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*kJLAfna0spTYyrfHkJK-MA.png"/></div><figcaption class="mc md et er es me mf bd b be z dx translated">高速公路网。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=9oTHFx0Gg3Q&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=12" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="f942" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">然后，字符n-grams的输出表示被输入到<strong class="jm hj">高速公路网。</strong>一个<a class="ae ix" href="https://arxiv.org/abs/1505.00387" rel="noopener ugc nofollow" target="_blank">高速公路网</a>模拟n元语法交互并应用转换，同时传递原始信息。它的功能类似于LSTM记忆细胞。</p><figure class="iz ja jb jc fd jd"><div class="bz dy l di"><div class="mm mn l"/></div></figure><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mp"><img src="../Images/94d425150211e2d1b272b7d1a116e6a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*lPXQ0aXaT_Vn01bKhzwZVA.png"/></div><figcaption class="mc md et er es me mf bd b be z dx translated">来源:<a class="ae ix" href="https://www.youtube.com/watch?v=9oTHFx0Gg3Q&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=12" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="f84d" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">然后，高速公路网络在单词级别的输出进入长短期记忆网络，在该网络中，分层softmax处理大量输出词汇，并通过时间对其进行截短的反向传播训练。</p><figure class="iz ja jb jc fd jd"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="87e7" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">结果显示，高速公路网络确实捕捉到了单词的语义。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mq"><img src="../Images/76399dfe0f38ee984f13b159cd63b773.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PdjuLodLgjUmpSkmEMW_Sw.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated">来源:<a class="ae ix" href="https://www.youtube.com/watch?v=9oTHFx0Gg3Q&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=12" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="1284" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">左图中，五个字上到最上面:<code class="du mg mh mi mj b">while</code>、<code class="du mg mh mi mj b">his</code>、<code class="du mg mh mi mj b">you</code>、<code class="du mg mh mi mj b">richard</code>、<code class="du mg mh mi mj b">trading</code>。根据计算出的单词表示，它会询问还有哪些单词与它最相似。顶部是单词级LSTM模型的输出。有趣的是，首先，你记得他们有一些字符嵌入，通过卷积层和最大池。如果在那个时候你问什么东西是最相似的(第二部分，LSTM-查尔(高速公路之前))，那基本上它仍然是记住关于人物的事情。所以和<code class="du mg mh mi mj b">while</code>最相似的词就是<code class="du mg mh mi mj b">chile</code>、<code class="du mg mh mi mj b">whole</code>、<code class="du mg mh mi mj b">meanwhile</code>、<code class="du mg mh mi mj b">white</code>。至少对于第一类来说，它们都以<code class="du mg mh mi mj b">LE</code>结尾。<em class="lk">所以你只是得到了这个字符序列的相似度</em>。根本不是真的在做有意义的事。<em class="lk">但有趣的是，当他们通过高速公路层时，高速公路层成功地学会了如何将这些字符序列表示转换成能够捕捉含义的东西</em>。对于单词<code class="du mg mh mi mj b">while</code>，其最相似的单词是<code class="du mg mh mi mj b">meanwhile</code>、<code class="du mg mh mi mj b">whole</code>、<code class="du mg mh mi mj b">though</code>和<code class="du mg mh mi mj b">nevertheless</code>。</p><p id="9be7" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">本文的要点是:1)本文质疑了使用单词嵌入作为神经语言建模输入的必要性；<strong class="jm hj"> 2) CNN加公路网超过字符可以提取丰富的语义和结构信息；</strong> 3)您可以构建“积木”来获得细致入微且功能强大的模型！</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="13cd" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">3.混合字符和单词级模型</h1><h2 id="a856" class="ll ko hi bd kp lm ln lo kt lp lq lr kx jt ls lt kz jx lu lv lb kb lw lx ld ly bi translated">3.2混合神经机器翻译(NMT)</h2><p id="ebf8" class="pw-post-body-paragraph jk jl hi jm b jn lf ij jp jq lg im js jt lh jv jw jx li jz ka kb lj kd ke kf hb bi translated"><a class="ae ix" href="https://arxiv.org/abs/1604.00788" rel="noopener ugc nofollow" target="_blank">混合NMT </a>大部分在单词层面翻译，只在需要的时候才转到字符层面。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mr"><img src="../Images/6f1ed6769285fb5cb4914e60836cf455.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vc8JpPUQGwDl1rlhi36IDw.png"/></div></div><figcaption class="mc md et er es me mf bd b be z dx translated">混合NMT模型结构。来源:<a class="ae ix" href="https://www.youtube.com/watch?v=9oTHFx0Gg3Q&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=12" rel="noopener ugc nofollow" target="_blank">课程幻灯片</a></figcaption></figure><p id="d3e2" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">这是一个混合架构，具有单词级和字符级。我们想要建立一个混合模型，因为看起来建立一个翻译起来相对更快更好的东西更实际。</p><p id="a6db" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">因此，我们的想法是主要建立一个单词级别的神经机器翻译系统，但是当我们遇到罕见或看不见的单词时，我们也可以处理字符级别的东西。事实证明，它能成功地提高性能。<em class="lk">所以我们的想法是这样的，我们要运行一个相当标准的，关注LSTM神经机器翻译系统的序列对序列。它实际上是一个4级深度系统。</em>我们将在<em class="lk">用合理的16，000个单词来运行这个。</em>因此，对于常见的单词，我们只有输入到神经机器翻译模型中的单词表示，但对于不在词汇表中的单词，我们将通过使用字符级LSTM为它们计算出单词表示，相反，当我们开始在另一边生成单词时，我们有一个词汇量为16，000的softmax。</p><p id="946b" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">它可能只会产生像“un <unk> chat”、<em class="lk">这样的词，但其中一个词是UNK符号。如果它生成了UNK符号，我们就把这个</em> <strong class="jm hj"> <em class="lk">隐藏的表示</em> </strong> <em class="lk">作为初始输入输入到字符级LSTM，然后我们有字符级LSTM，</em>然后我们有字符级LSTM生成一个字符序列，直到它生成一个停止符号，我们用它来生成单词。因此，我们最终得到了这种由8个LSTM层组成的混合堆栈。</unk></p><p id="b3ab" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">这是一项决定性的技术，在训练时你知道来源和目标，这意味着在训练时，我们已经决定了我们的词汇。所以对于输入端和输出端，我们都知道哪些单词不在我们的词汇表中。因此，如果它不在我们的词汇表中，对于输入，我们运行左上角的蓝色lstm，对于输出，我们运行上图中右上角的红色lstm。</p><p id="981d" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">解码包含两个阶段:用于<unk>的字级波束搜索和字符级波束搜索。</unk></p><p id="e08a" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><a class="ae ix" href="https://arxiv.org/abs/1604.00788" rel="noopener ugc nofollow" target="_blank">论文</a>中的结果显示，它在尝试填充生僻字方面比复制机制实现了超过2 BLEU 的改进。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="213b" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">4.单词嵌入的字符</h1><h2 id="b9a6" class="ll ko hi bd kp lm ln lo kt lp lq lr kx jt ls lt kz jx lu lv lb kb lw lx ld ly bi translated">4.1快速文本嵌入</h2><p id="eb6c" class="pw-post-body-paragraph jk jl hi jm b jn lf ij jp jq lg im js jt lh jv jw jx li jz ka kb lj kd ke kf hb bi translated">对于单词嵌入，您应该能够使用字符或单词片段做一些有用的事情。这就是快速文本嵌入的用武之地。<a class="ae ix" href="https://arxiv.org/pdf/1607.04606.pdf" rel="noopener ugc nofollow" target="_blank"> FastText </a>的目标是下一代高效的类似word2vec的单词表示库，<strong class="jm hj">，但更适用于生僻字和有大量词法的语言。</strong> <strong class="jm hj"> <em class="lk">它是word2vec跳格模型的扩展，带有字符n-grams。</em> </strong> FastText嵌入将一个单词表示为用边界符号扩充的char n-grams以及整个单词。例如，单词“where”被表示成<code class="du mg mh mi mj b">&lt;wh</code>、<code class="du mg mh mi mj b">whe</code>、<code class="du mg mh mi mj b">her</code>、<code class="du mg mh mi mj b">ere</code>、<code class="du mg mh mi mj b">re&gt;</code>和<code class="du mg mh mi mj b">&lt;where&gt;</code>。注意<code class="du mg mh mi mj b">&lt;her&gt;</code>或<code class="du mg mh mi mj b">&lt;her</code>与‘她’不同。前缀、后缀和整个单词是特殊的。它将单词表示为这些表示的总和。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ms"><img src="../Images/d10b6cb5f7f23d292700a8f8dd901628.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*vlRLLBSSYIWfLUmAyMkRCQ.png"/></div><figcaption class="mc md et er es me mf bd b be z dx translated">上下文中的单词分数。</figcaption></figure><p id="a0c8" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">上下文得分中单词的等式表明，它不是共享表示或所有n元语法，而是使用“散列轨迹”来具有固定数量的向量。例如，您将单词“where”表示为一组n-gram。你需要一个边界符号(分别是<code class="du mg mh mi mj b">&lt;</code>和<code class="du mg mh mi mj b">&gt;</code>)来表示单词的开始和结束。所以单词<code class="du mg mh mi mj b">where</code>的三个字母可以是<code class="du mg mh mi mj b">&lt;wh</code>、<code class="du mg mh mi mj b">whe</code>、<code class="du mg mh mi mj b">her</code>、<code class="du mg mh mi mj b">ere</code>和<code class="du mg mh mi mj b">re&gt;</code>。此外，还有一个用于整个单词的额外按钮，即<code class="du mg mh mi mj b">&lt;where&gt;</code>。所以<code class="du mg mh mi mj b">where</code>用6个向量表示。记住，word2vec计算上下文表示向量和中心单词表示向量之间的点积。FastText完全做同样的事情，但是中心单词表示向量将是这6个向量。在计算完每个向量后，FasText最终将它们相加，生成用于<code class="du mg mh mi mj b">where</code>的单词嵌入。</p><p id="97c0" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"><em class="lk"/></strong><a class="ae ix" href="https://arxiv.org/pdf/1607.04606.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="jm hj"><em class="lk">论文</em> </strong> </a> <strong class="jm hj"> <em class="lk">中的结果表明，对于英语来说，FastText嵌入并不比CBOW得到更好的效果。但是对于另一种有更多词法的语言(s.t. German)，快速文本嵌入表现更好</em> </strong>。</p></div><div class="ab cl kg kh gp ki" role="separator"><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl km"/><span class="kj bw bk kk kl"/></div><div class="hb hc hd he hf"><h1 id="94d0" class="kn ko hi bd kp kq kr ks kt ku kv kw kx io ky ip kz ir la is lb iu lc iv ld le bi translated">参考</h1><ol class=""><li id="dcab" class="mt mu hi jm b jn lf jq lg jt mv jx mw kb mx kf my mz na nb bi translated"><a class="ae ix" href="https://www.youtube.com/watch?v=9oTHFx0Gg3Q&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=12" rel="noopener ugc nofollow" target="_blank">斯坦福CS224N: NLP与深度学习| Winter 2019 |讲座12 —子词模型</a></li></ol><p id="762e" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">2.<a class="ae ix" href="https://arxiv.org/abs/1508.07909" rel="noopener ugc nofollow" target="_blank">具有子词单元的生僻字的神经机器翻译</a></p><p id="4ec7" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">3.<a class="ae ix" href="https://arxiv.org/abs/1808.06226" rel="noopener ugc nofollow" target="_blank"> SentencePiece:一个简单且独立于语言的子词记号化器和去记号化器，用于神经文本处理</a></p><p id="4aa5" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">4.<a class="ae ix" href="https://arxiv.org/abs/1508.06615" rel="noopener ugc nofollow" target="_blank">字符感知神经语言模型</a></p><p id="f2a5" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">5.<a class="ae ix" href="https://arxiv.org/abs/1604.00788" rel="noopener ugc nofollow" target="_blank">用混合单词-字符模型实现开放词汇神经机器翻译</a></p><p id="8498" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">6.<a class="ae ix" href="https://arxiv.org/pdf/1607.04606.pdf" rel="noopener ugc nofollow" target="_blank">用子词信息丰富词向量</a></p></div></div>    
</body>
</html>