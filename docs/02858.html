<html>
<head>
<title>Machine Learning: Implementing various regression algorithms to predict Boston house prices</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:实现各种回归算法来预测波士顿房价</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-implementing-various-regression-algorithms-to-predict-boston-house-prices-c87f961de981?source=collection_archive---------13-----------------------#2020-01-05">https://medium.com/analytics-vidhya/machine-learning-implementing-various-regression-algorithms-to-predict-boston-house-prices-c87f961de981?source=collection_archive---------13-----------------------#2020-01-05</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e7b841bf87f613fa2d97a3b011e995c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aXtmIR7VTSzhRVX-IiPh5w.jpeg"/></div></div></figure><h1 id="93b1" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">简介</strong></h1><p id="3e74" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在这篇文章中，我们使用了各种回归算法来预测波士顿的房价。<a class="ae km" href="http://lib.stat.cmu.edu/datasets/boston" rel="noopener ugc nofollow" target="_blank">波士顿住房数据集</a>包括由美国共识服务收集的关于影响波士顿地区业主自用房屋价格的各种因素的数据。这些因素包括人均犯罪率、离查尔斯河的远近、一氧化氮浓度、每所房子的房间数量、高速公路的可达性、税收、下层社会人口的比例等。被视为描述价格，这些价格以1000美元表示。房屋的价格将被预测，因此，我们的目标或因变量是房屋的价格，相关特征或自变量将从影响价格的所有因素中选择，而数据探索过程。很明显，这是一个回归问题，因为需要预测连续值，所以实现了各种回归算法来达到同样的目的。</p><h1 id="2fa6" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">数据探索</strong></h1><p id="8d3d" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">数据集的来源是卡内基梅隆大学的StatLib。数据集由506个实例和14列组成，其中13列是要素，1列是目标。图1是Python中data.describe()命令获得的数据集的描述，图2是source提供的列的描述:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kn"><img src="../Images/6c54097eefc32c6cdf928e8dd3f353c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fG-ghm1ogb7zJ7WDaqlmPw.jpeg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图1:数据描述</figcaption></figure><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/b0e3b09b8ea9c9a50ed435567e09d7d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*dxpLt1GJMHq9qgntgmZ2ug.jpeg"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图2:列描述</figcaption></figure><p id="caaa" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">图1提供了所有列中数据分布的信息。该表用于了解所选功能和目标列的统计信息，以便在数据预处理中采取必要的步骤。下一步是寻找可以使用data.isnull()获得的nan/null值。Python中的sum()。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lc"><img src="../Images/ce0cc69146b2a4c3eb1bac418c9f2a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:218/format:webp/1*AG1RF-qpov_na0F89vtxUg.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图3总空值</figcaption></figure><p id="65b9" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">从图3中可以清楚地看到，数据集中没有空值，因此现在可以对其进行处理以供进一步研究。必须检查目标值中的异常值，因此，为了达到同样的目的，数据的直方图绘制在图4中。可以注意到，在该图中，medv列数据呈正态分布，很少有异常值，即medv列数据值为50的点。这些异常值可以通过计算medv的所有数据点的Z分数来去除。z得分定义为:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/41968f4be43833db99efe33a2717fcdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/1*MwQ3_emugYa3MD7UJHKekg.png"/></div></figure><p id="12b9" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">这里，x是数据点，μ是平均值，σ是标准差。离群值将给出最大的Z分数，因此，可以通过设置某个阈值来消除它。在Python scipy.stats.zscore()中，命令用于找出Z得分，阈值2.7用于移除这些异常值。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es le"><img src="../Images/60923758f552fc05955aa99f1fe12195.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*vwSd6Qy4Cig7Ps2SsIy_zw.jpeg"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图4 medv的分布</figcaption></figure><p id="88d7" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">下一步是选择实施回归分析的独立变量。为了选择要素，将使用Python中的data.corr()命令计算协方差矩阵，该矩阵将包含每个列相对于所有其他列的协方差。可以使用所有的特征，但是这可能导致过拟合问题，此外，相对于彼此具有相同协方差的特征将产生冗余问题。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/0ad82d2e9534626e543117df3222d826.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EW7FdJAl6ugumP1518zXtQ.jpeg"/></div></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图5协方差矩阵</figcaption></figure><p id="f9cd" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">从图5中可以看出，indus、rm、tax、ptratio和lstat的协方差接近0.5或大于0.5，因此我们可以绘制一个散点图来进行更详细的研究。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/bed303da9da88322f254d65ccb58c2c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*PBFRD7UUjdvfkM_fR8_Icg.jpeg"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图6协方差&gt; 4.8的要素散点图</figcaption></figure><p id="4903" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">图6中散点图的最后一行提供了自变量“rm”、“lstat”和“ptratio”与因变量“medv”高度相关的证据。此外，这些特征的协方差大于0.5，如图5所示。因此，特征“每个住宅的平均房间数”、“低地位人口百分比”和“学生教师比率”用于预测波士顿自有住房的中值价格。</p><h1 id="72f0" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">所选算法的简要概述</strong></h1><p id="6520" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><strong class="jq hj">线性回归</strong></p><p id="74b2" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">简单线性回归模型通过一条最佳拟合直线来估计自变量和因变量之间的关系。通过数据拟合该线，使得残差的平方和最小。一条直线的方程式如下:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lh"><img src="../Images/b0878492c0d1983db7943d8df0bafc98.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*SpBS8sNNlMe9WV3JjJVzsA.png"/></div></figure><p id="64ff" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">这里，y是目标值，x是特征，β0是线的y截距，β1是斜率，ε是误差。下面的示例图解释了如何使用线性线来实现线性回归。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es li"><img src="../Images/837c6dabdc3799b9e50f682f88f64d42.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*YO4-NrN90ygMROem3nv02w.png"/></div></figure><p id="a25c" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">代价函数:</strong>线性回归的代价函数由残差平方和给出。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/fde29e5a6fb35cb5dd462af3fd6eefa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*VrdXKJ4LZIxVHY7fnaFueg.png"/></div></figure><p id="ad26" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">多项式回归</strong></p><p id="c844" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">当因变量和自变量之间的关系是非线性的时，使用多项式回归，这是解释数据方差的非线性线，因此使用n次多项式方程来估计该关系。多项式回归方程由下式给出:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/00ed696a0c25aa87a2d289264a3ea65c.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*3PCYBlvFSxuzy296e7vrUQ.png"/></div></figure><p id="c77c" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">这里，y代表因变量，α0，α1，α2，…，αn是权重或参数，x是自变量，ε是残差。随着阶数n的增加，误差减小。但是，阶数应该仔细选择，因为非常高的阶数可能会导致过拟合。下面的示例图解释了如何使用4次多项式方程来实现多项式回归。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es ll"><img src="../Images/8e30dc95d1e6856b8dc59b316dcd2085.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*aP_HpNDi43l_H7OSSG9xYQ.jpeg"/></div></figure><p id="b649" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">成本函数:</strong>多项式回归的成本函数由残差平方和给出:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lm"><img src="../Images/1ae2eacdcc479385114276226d35c7c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*R7-A-hr9CuCY0XVWBd82Ng.png"/></div></figure><p id="2cf8" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">回归树</strong></p><p id="dcb4" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">回归树基于决策树的概念。在这种情况下，通过具有节点和叶子的二叉树进行预测，并且在每个节点处，二元条件决定树中的流向，并且一旦到达叶子就进行预测。节点的值代表独立变量的不同区域/组，节点的叶的值是特定区域的平均输出。节点和叶子的那些值被认为降低了成本函数。使用以下虚拟数据可以更好地解释该算法:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/bdf559c15748d9b3381694548e9cf335.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*XnRmnznDRSmOd1FdVQlbWQ.jpeg"/></div></figure><p id="3868" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">从散点图可以清楚地看出，这里不能使用线性或多项式回归，因为残差会非常高。因此，实现了回归树算法:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/0e3e5eae210935551ea5035c4719f2df.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*22R6MkpXjhsEnajdC3T2iA.jpeg"/></div></figure><p id="0bba" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">从二叉树和x区域的输出中获得输出&lt;4.5 can be explained using the following tree:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lo"><img src="../Images/eaa00751fc404fee1f3cdb454eac0137.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*jrEaIKRzEOzz3fTjOh_kRw.jpeg"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">Figure 7 Decision tree</figcaption></figure><p id="dc3a" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">In Figure 7, nodes viz x&lt;1.5, x&lt;2.5 are regions and circled values are leaves which are mean of values in the particular region. If x value is less than 1.5 than output is y=46205 otherwise, x will be checked in every node until it reaches a leaf value.</p><p id="596d" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">成本函数:</strong>成本函数是在所有区域中计算的成本函数的总和，是第k区域中的平均响应，并且是目标值，则成本函数定义为:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lp"><img src="../Images/108329f13e656b5dbc68f989ed8a836f.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/format:webp/1*sdOVGeSAdbdHpwAeO3SdjA.png"/></div></figure><p id="c5de" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">支持向量回归</strong></p><p id="1446" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">支持向量回归是解决线性和非线性回归问题的一种非常强大的算法。在支持向量回归中，决策边界形成在作为阈值的超参数ε的范围内，并且SVR试图在该决策边界内包括尽可能多的数据点。SVR确保误差在某个阈值内。最接近边界线的向量称为支持向量。SVR中的核函数有助于处理非线性可分数据，而无需特别将数据转换到新的z空间。决策边界内的超平面或线做出新的预测，并且超平面的方程给出为:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/9192bf05f4f6a6c22387dc7d58dcaaf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/format:webp/1*0cN1-v5RD6ePNIFNhSXnIQ.png"/></div></figure><p id="a8c5" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">其中y是目标，x是训练样本，w垂直于超平面。决策边界试图最小化:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/8b0d186ad7aead013ba160e21b3f53dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*QVNLy72fxBkipwhrKl9G3g.png"/></div></figure><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/bbe2902083dba06fadbd8fefdbf79c20.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*mSWYK6h0APlO5DoItbcrSg.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图8支持向量回归</figcaption></figure><p id="39fc" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">神经网络回归</strong></p><p id="a7c2" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">神经网络由神经元组成，并且神经网络的最简单形式是感知器，其具有输入，与每个输入、偏差和网络输入相关联的权重被给出为:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es li"><img src="../Images/67136ba58093822038d05afbe6e2866d.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*KKDpf4zmjI7TnkTxhDO98A.png"/></div></figure><p id="4df2" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">净输入被传递到激活函数，该函数根据阈值给出输出。更复杂的神经网络有许多神经元，形成输入层、隐藏层和输出层。在训练神经网络时，执行前馈操作以获得输出。然后，计算输出误差，并进一步反向传播该误差。这些等式是:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lt"><img src="../Images/bf11296e79785ec482f8077e57505ba3.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*OTPAJ-_5I78iu0Ywls2LXA.png"/></div></figure><p id="bc27" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">z是输出，σ是<strong class="jq hj"/><strong class="jq hj"/>激活函数。最后，执行梯度下降以达到成本函数的最小值，并更新权重和偏差。新的权重和偏差由下式给出:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/f35d07e14204594fb8880f507850e204.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*qjMjRWNYgmszigqSN5I8sw.png"/></div></figure><p id="1fb6" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">w，b分别是更新权重和偏差，C是代价函数，是学习率。在神经网络回归中，输入层和隐藏层中可以有多个神经元，但输出层中只有一个神经元。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lv"><img src="../Images/9bbdaec4962639c94a1f083f5db8a6ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JEucoGogJUFwzUClcRWZhg.jpeg"/></div></div></figure><p id="7f89" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">代价函数:</strong>神经网络回归的代价函数由均方误差给出:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/cf7084bc4d92879b780f5b46eddaa4aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*fd3Dk8M4zzIdPK3coneGzA.png"/></div></figure><h1 id="3fb4" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">应用回归算法</strong></h1><p id="ffa6" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated"><strong class="jq hj">线性回归</strong></p><p id="514c" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">数据预处理:</strong>从图5中的协方差矩阵可以看出，每套住宅的房间数(rm)和低人口比例(lstat)与我们的目标值高度相关。rm的协方差值为0.70，因此它显示出正趋势，lstat的协方差值为-0.74，因此它与目标medv负相关。在下面的散点图中也很明显。</p><div class="ko kp kq kr fd ab cb"><figure class="lx ij ly lz ma mb mc paragraph-image"><img src="../Images/c5900f764d7b002a42bb893964f2f745.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*MHlr-noPEPX8PuMgM8K1rw.jpeg"/></figure><figure class="lx ij md lz ma mb mc paragraph-image"><img src="../Images/db62a6fe2da9a4e050581db92699b5f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*tAApZvCbVD5AJqKVpid-ww.jpeg"/></figure></div><p id="ed40" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">可以选择任何一个要素来执行线性回归，因为这两个要素都与目标高度相关，但是lstat稍好一些，因此选择lstat要素。</p><p id="062e" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">应用模型:</strong>数据预处理后，下一步是将数据分成训练集和测试集，最后训练模型。sklearn.linear_model。LinearRegression类帮助执行线性回归。应用了10重交叉验证，图9描述了在该过程中获得的误差。下面的图是通过线性回归得到的。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es me"><img src="../Images/4198466e68b1325036a7527a768a5469.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*6Q9isN0bZNk6IcU4YEni3g.jpeg"/></div></figure><p id="b124" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">多项式回归</strong></p><p id="39b3" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">数据预处理:</strong>为了实现多项式回归，需要对输入数据进行转换，以获得特征的高阶项，例如，对于4次多项式，项是必需的。因此，测试和训练数据集的特征向量都使用sk learn . preprocessing . Polynomial features进行转换。多项式回归也在特征lsat上实现。然而，该模型可以通过曲线实现非线性关系。</p><p id="069f" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">应用模型:</strong>多项式回归也是用sklearn.linear_model实现的。线性回归类和误差如图10所示。在应用该模型之后，获得了下面的图。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es me"><img src="../Images/6af37cdcd38814cee98ac84eaeeac49a.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*g096dOXS_yOVweL06FmpEA.jpeg"/></div></figure><p id="13a4" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">从上面的图中可以看出，与直线相比，多项式函数形成的曲线是更好的数据估计。</p><p id="c563" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">回归树</strong></p><p id="b802" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">数据预处理:</strong>在回归树中，可以使用三个选择的特征来进行预测。然而，我们需要检查一些特性的差异，以确定是否有任何特性压倒了其他特性。rm特征的方差为0.49，lstat的方差为50.8。因此，lsat特征的方差很容易超过rm，如下面的散点图所示:</p><div class="ko kp kq kr fd ab cb"><figure class="lx ij mf lz ma mb mc paragraph-image"><img src="../Images/88c6aa2466b4b3f94516b4aeaa8992da.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*ArHXUWVK7QkRYMNnke4eJw.jpeg"/></figure><figure class="lx ij mf lz ma mb mc paragraph-image"><img src="../Images/0eec01da920897792072ff152a4b7edc.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*z1M0eAPhj67mbQOQeUHAgw.png"/></figure></div><p id="33e4" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">为了解决这个问题，在特征数据上实现了sk learn . preprocessing . standard scaler，以将平均值降低到零，将标准偏差降低到一。</p><p id="dffe" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">应用模型:</strong>使用sk learn . tree . decision tree regressor实现回归树模型。下面的图是从测试数据集获得的，该散点图中的点表示测试数据集中的medv值和回归树预测的medv值:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/3b544c1b870b31760c21e1aa8e9515d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*NreECsbMQYDGnsoKlCkDjQ.jpeg"/></div></figure><p id="ae55" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">从这个散点图可以看出，许多预测输出值与实际值重叠。然而，一些预测的输出不重叠并且远离目标值，并且这些值会导致残留误差。</p><p id="2c2f" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">支持向量回归</strong></p><p id="011c" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">数据预处理:</strong>SVR的数据预处理类似于回归树的数据预处理。</p><p id="f4dc" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">应用模型:</strong>使用sklearn.svm.SVR实现SVR模型，该模型采用参数，即kernel、gamma和c。此处使用的内核是径向基函数(rbf ),由下式给出:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/f5b99733973db60d2e2b9c6a7be6b06e.png" data-original-src="https://miro.medium.com/v2/resize:fit:352/format:webp/1*vEgwavpzbtiZjzTEKr_wTg.png"/></div></figure><p id="3d29" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">这里γ是0.10。以下图从测试数据集获得，散点图中的点表示测试数据集值中的输出medv和SVR预测的medv值:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/8d770092e15cd0642edd5d18d9b12ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*d3b3DZajXDW3PAuuqG6hSg.jpeg"/></div></figure><p id="675f" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">在上面的散点图中，可以看到大部分预测输出与实际输出值重叠，或者预测值非常接近实际值，这证明该模型优于回归树。图12显示了在交叉验证过程中获得的错误。</p><p id="f2ce" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">神经网络回归:</strong></p><p id="3eb9" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">数据预处理:</strong>支持向量回归的数据预处理类似于回归树。</p><p id="3496" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">应用模型:</strong>使用neural_network实现神经网络模型。MLPRegressor采用参数viz hidden_layer_sizes、activation和max_iter。两个隐藏层使用64和32个神经元，这里使用的激活函数是一个校正线性单元(relu ),最大迭代集是1000。relu激活函数的公式为:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/09e8875fc198b1f590053d1d5beecf14.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*xM-vvieqI07k3wSXyOWHeA.png"/></div></figure><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/1012a002e2c2ce1fbb6c1cf07f2c0b63.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*k1Z6XB_dqGnb-EwPdcgWCA.png"/></div></figure><p id="83b4" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">神经网络产生的散点图非常类似于SVR，并且同样好，这就是这两种模型产生的误差非常相似的原因。</p><h1 id="9d7d" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">精度对比</strong></h1><p id="a66f" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">对于下面提到的所有性能指标，假设y，h分别是输入的实际和预测输出值，图14中给出了每个模型在10倍交叉验证中获得的RMSE和MAPE误差的平均值</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/5463c8bf2143607f5d12ec1209261cb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*tVyZcz8FdfA-M2OgxLEzgA.png"/></div><figcaption class="ks kt et er es ku kv bd b be z dx translated">图14各种模型的误差均值</figcaption></figure><p id="c3e2" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">均方根误差(RMSE): </strong>取残差平方和，然后取总和的平方根计算。RMSE越小，模型越好。除此之外，RMSE还描述了最佳拟合线的误差标准偏差，即最佳拟合线的残余误差值的分布。该公式由下式给出:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/0191730d2c19a5c2416e34a958498125.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*UM8Wam2D5aF-3eXWy_QBxA.png"/></div></figure><p id="078e" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">线性模型具有最大RMSE，因此，与该数据集的其他模型相比，它是最差的模型，因为它具有最大的残差。多项式回归模型表现良好，因为数据是非线性的，所以由多项式模型形成的曲线降低了残差，回归树也同样良好。SVR模型具有非常低的RMSE，因为rbf核在解释自变量和因变量之间的非线性关系方面做得非常好。与产生最小残差的其他模型相比，神经网络模型具有最小的RMSE，并且由该模型做出的大多数预测非常接近实际输出值。</p><p id="3af2" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated"><strong class="jq hj">平均绝对百分比误差(MAPE): </strong>是预测输出相对于实际输入的百分比误差的平均值。它需要绝对值，所以正误差不会抵消负误差。MAPE不仅比较不同的模型，而且有助于轻松了解特定模型的表现如何。该公式由下式给出:</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es ml"><img src="../Images/dc4cb16120d9be9569d2710e05ad5274.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*pVDrTovxQ3iJR1e0nHQPkg.png"/></div></figure><p id="9f68" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">线性模型具有最大MAPE，这意味着它具有最大数量的错误预测值。神经网络模型的预测误差仅为14.24%，这使其成为该数据集的最佳预测模型。</p><h1 id="f067" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">代号:</strong></h1><p id="f3a5" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">代码请参考下面的GitHub链接。</p><div class="mm mn ez fb mo mp"><a href="https://github.com/khushwant18/Regression-analysis" rel="noopener  ugc nofollow" target="_blank"><div class="mq ab dw"><div class="mr ab ms cl cj mt"><h2 class="bd hj fi z dy mu ea eb mv ed ef hh bi translated">khushwant 18/回归分析</h2><div class="mw l"><h3 class="bd b fi z dy mu ea eb mv ed ef dx translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="mx l"><p class="bd b fp z dy mu ea eb mv ed ef dx translated">github.com</p></div></div><div class="my l"><div class="mz l na nb nc my nd io mp"/></div></div></a></div><h1 id="9819" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak">参考文献</strong></h1><p id="cc5d" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">Harrison d .和Rubin feld d . l .，《享乐价格和对清洁空气的需求》，J. Environ。经济学与管理学，第5卷，81–102页，1978年。可用:<a class="ae km" href="http://lib.stat.cmu.edu/datasets/boston" rel="noopener ugc nofollow" target="_blank">http://lib.stat.cmu.edu/datasets/boston</a></p><p id="d5d8" class="pw-post-body-paragraph jo jp hi jq b jr kx jt ju jv ky jx jy jz kz kb kc kd la kf kg kh lb kj kk kl hb bi translated">房屋图片:<a class="ae km" href="https://news.northeastern.edu/2019/07/30/greater-boston-housing-report-card-underscores-housing-challenges-in-the-greater-boston-area/" rel="noopener ugc nofollow" target="_blank">https://news . northeast . edu/2019/07/30/greater-Boston-housing-report-card-下划线-housing-challenges-in-the-greater-Boston-area/</a></p></div></div>    
</body>
</html>