<html>
<head>
<title>Effect of learning rate for training convergence.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习率对训练收敛的影响。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/effect-of-learning-rate-for-training-convergence-2d6801a95956?source=collection_archive---------6-----------------------#2019-12-08">https://medium.com/analytics-vidhya/effect-of-learning-rate-for-training-convergence-2d6801a95956?source=collection_archive---------6-----------------------#2019-12-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex if ig ih ii"><div class="bz dy l di"><div class="ij ik l"/></div></figure><h1 id="74ca" class="il im hi bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">介绍</h1><p id="b2a7" class="pw-post-body-paragraph jj jk hi jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">在这篇文章中，我将分享我对学习率的不同值在训练时如何影响收敛的见解，在训练我们的模型时，我们应该从学习率图中推断出什么，以及基于此我们应该如何更新我们的学习率。</p><h1 id="80d7" class="il im hi bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">概观</h1><p id="7979" class="pw-post-body-paragraph jj jk hi jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">我们将创建一个样本数据，并使用特定的权重来创建目标值。一旦我们有了目标值，我们将添加高斯噪声。现在我们的任务是预测导致目标产生噪音的权重。在此过程中，我们将不断改变学习速度以达到目标，并观察其效果。以下是我将遵循的6个步骤</p><ol class=""><li id="2d6c" class="kh ki hi jl b jm kj jq kk ju kl jy km kc kn kg ko kp kq kr bi translated">创建示例数据</li><li id="a52a" class="kh ki hi jl b jm ks jq kt ju ku jy kv kc kw kg ko kp kq kr bi translated">选择特定重量</li><li id="5ff5" class="kh ki hi jl b jm ks jq kt ju ku jy kv kc kw kg ko kp kq kr bi translated">获得添加了噪声的目标并将其可视化。</li><li id="d14c" class="kh ki hi jl b jm ks jq kt ju ku jy kv kc kw kg ko kp kq kr bi translated">首先使用均方差进行预测，看看我们离输出有多远。</li><li id="1f1f" class="kh ki hi jl b jm ks jq kt ju ku jy kv kc kw kg ko kp kq kr bi translated">使用梯度下降优化到达目标。</li><li id="4243" class="kh ki hi jl b jm ks jq kt ju ku jy kv kc kw kg ko kp kq kr bi translated">更新学习率并观察其效果。</li></ol><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es kx"><img src="../Images/ddc70f0a76cfa575480a9d36c3cd561e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2LbJiUlCuNdPq1oG6vmS7g.png"/></div></div></figure><h1 id="cfbf" class="il im hi bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">履行</h1><p id="f054" class="pw-post-body-paragraph jj jk hi jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">我们将在整个实现中使用pytorch、matplotlib和numpy。</p><p id="2d53" class="pw-post-body-paragraph jj jk hi jl b jm kj jo jp jq kk js jt ju li jw jx jy lj ka kb kc lk ke kf kg hb bi translated">首先，我们将创建一个维度为[100，2]的样本数据，其中第一列代表来自均匀分布的一些数据，第二列是有偏差的数据。</p><pre class="ky kz la lb fd ll lm ln lo aw lp bi"><span id="e950" class="lq im hi lm b fi lr ls l lt lu">import torch<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>from torch import nn</span><span id="2f9c" class="lq im hi lm b fi lv ls l lt lu">n=100<br/>x = torch.ones(n,2)      #sample data of shape [100,2]<br/>x[:,0].uniform_(-1.,1) <br/>x[:5]</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es lw"><img src="../Images/6d1aad45666eb4b5f3f6a925f818d34c.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*io0fXW5vZYtmYidFLD86ng.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">抽样资料</figcaption></figure><p id="194f" class="pw-post-body-paragraph jj jk hi jl b jm kj jo jp jq kk js jt ju li jw jx jy lj ka kb kc lk ke kf kg hb bi translated">现在我们将创建一个张量，其中3。是权重，2是偏差，粗略地说，我们称之为权重矩阵。<strong class="jl hj">这是算法最终要学习的矩阵</strong>。</p><pre class="ky kz la lb fd ll lm ln lo aw lp bi"><span id="edf1" class="lq im hi lm b fi lr ls l lt lu">a = torch.Tensor([3.,2])   #weights to be learned by the model.</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es mb"><img src="../Images/1bbc982378037226d423c4d7313a540d.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*nS3HGzwmE6MEyDqHYTaquQ.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">权重矩阵</figcaption></figure><p id="ad27" class="pw-post-body-paragraph jj jk hi jl b jm kj jo jp jq kk js jt ju li jw jx jy lj ka kb kc lk ke kf kg hb bi translated">在这里，我们将我们的权重矩阵乘以样本数据来获得预测，然后添加一些噪声。现在这个y是目标标签。</p><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es mc"><img src="../Images/37cbd54b8f5b865272a559c92832f5f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/1*CYbGGRvnlgasFCBPAybB9w.gif"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">样本数据和重量的乘积(x@a)</figcaption></figure><pre class="ky kz la lb fd ll lm ln lo aw lp bi"><span id="3621" class="lq im hi lm b fi lr ls l lt lu">y = x@a + torch.rand(n)   #target label to be predicted<br/>plt.scatter(x[:,0], y);</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es md"><img src="../Images/6e921ea9b44d0c0d8d55f74c926dfb76.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*yVjwZ2ERasI_RxRLfTsIzA.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">目标y可视化</figcaption></figure><p id="f415" class="pw-post-body-paragraph jj jk hi jl b jm kj jo jp jq kk js jt ju li jw jx jy lj ka kb kc lk ke kf kg hb bi translated">这里，我们将使用随机选择的[-1，1]权重矩阵进行第一次预测。同时，我们也将使用均方差来计算损失。</p><pre class="ky kz la lb fd ll lm ln lo aw lp bi"><span id="3c89" class="lq im hi lm b fi lr ls l lt lu"><strong class="lm hj">def</strong> mse(y_hat, y):              #mean squared error <br/>    <strong class="lm hj">return</strong> ((y_hat-y)**2).mean()</span><span id="07fe" class="lq im hi lm b fi lv ls l lt lu">a = torch.Tensor([-1.,1])               #initializing random weights<br/><br/>y_hat = x@a                    #first target prediction<br/>mse(y_hat, y)</span><span id="3d8d" class="lq im hi lm b fi lv ls l lt lu">plt.scatter(x[:,0],y)<br/>plt.scatter(x[:,0],y_hat);</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es me"><img src="../Images/f7d52a66dcbb9ca2a61c0f99baa16c28.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*aMqZwZtlLz9b4gSVYGwk2Q.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">第一目标预测图。</figcaption></figure><h1 id="6201" class="il im hi bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">梯度下降</h1><p id="c244" class="pw-post-body-paragraph jj jk hi jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">我们将使用pytorch编写梯度下降实现，以便自动计算梯度。上面随机创建的权重矩阵<strong class="jl hj"> a </strong>我们会传入nn。pytorch的参数模块，自动计算在<strong class="jl hj"> a </strong>上完成的所有操作的梯度，这些梯度可以使用<strong class="jl hj"> a.grad </strong>访问。我们将预测y，计算损失，反向传播损失，更新权重矩阵。这个过程我们将做100次，我们预计在100次迭代结束时，我们将接近目标标签值。</p><pre class="ky kz la lb fd ll lm ln lo aw lp bi"><span id="b0d3" class="lq im hi lm b fi lr ls l lt lu">a = nn.Parameter(a);<br/><br/><strong class="lm hj">def</strong> update():<br/>    y_hat = x@a<br/>    loss = mse(y, y_hat)<br/>    <strong class="lm hj">if</strong> t % 10 == 0: print(loss)<br/>    loss.backward()<br/>    <strong class="lm hj">with</strong> torch.no_grad():<br/>        a.sub_(lr * a.grad)<br/>        a.grad.zero_()</span><span id="9119" class="lq im hi lm b fi lv ls l lt lu">lr = 1e-1 <br/><strong class="lm hj">for</strong> t <strong class="lm hj">in</strong> range(100): <br/>    update()</span><span id="a562" class="lq im hi lm b fi lv ls l lt lu">plt.scatter(x[:,0],y)<br/>plt.scatter(x[:,0],x@a.detach());</span></pre><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es mf"><img src="../Images/77f0178687f8055151fb56aa7b95c1e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*qVB-RU8SMcqXQkTWQqO5Lw.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">100次迭代后，预测y(橙色)和目标y(蓝色)。</figcaption></figure><h1 id="17fb" class="il im hi bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">更新学习率</h1><p id="a2e5" class="pw-post-body-paragraph jj jk hi jl b jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg hb bi translated">我们将通过使用<strong class="jl hj"> lr=0.1 </strong>和<strong class="jl hj"> lr=1.01 </strong>来查看学习率的效果</p><figure class="ky kz la lb fd ii"><div class="bz dy l di"><div class="ij ik l"/></div></figure><p id="7d81" class="pw-post-body-paragraph jj jk hi jl b jm kj jo jp jq kk js jt ju li jw jx jy lj ka kb kc lk ke kf kg hb bi translated">正如我们从左侧视频中看到的，当我们使用0.1 的低<strong class="jl hj">学习率时，模型在<strong class="jl hj"> 70个历元</strong>左右达到最小值需要时间。</strong></p><figure class="ky kz la lb fd ii"><div class="bz dy l di"><div class="ij ik l"/></div></figure><p id="e2db" class="pw-post-body-paragraph jj jk hi jl b jm kj jo jp jq kk js jt ju li jw jx jy lj ka kb kc lk ke kf kg hb bi translated">另一方面，如果我们使用略高的0.7的<strong class="jl hj">学习率</strong>，模型在大约8个时期内达到最小值。</p><figure class="ky kz la lb fd ii"><div class="bz dy l di"><div class="ij ik l"/></div></figure><p id="f0f7" class="pw-post-body-paragraph jj jk hi jl b jm kj jo jp jq kk js jt ju li jw jx jy lj ka kb kc lk ke kf kg hb bi translated">而1.01的更高的<strong class="jl hj">学习率</strong>将模型推向发散。</p><h1 id="c718" class="il im hi bd in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji bi translated">结论</h1><figure class="ky kz la lb fd ii er es paragraph-image"><div class="er es mg"><img src="../Images/72aa347aecc86effa90ca2d2de4e8b87.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*s4vL6V_1M4SoT1sw0PsF8A.png"/></div></figure><figure class="ky kz la lb fd ii er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es mh"><img src="../Images/78103ea61eba151712b78fb250091b87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QKsTr6hwD4uVegBkRPi0eQ.png"/></div></div></figure><p id="5805" class="pw-post-body-paragraph jj jk hi jl b jm kj jo jp jq kk js jt ju li jw jx jy lj ka kb kc lk ke kf kg hb bi translated">正如我们从左图中看到的，当达到收敛<strong class="jl hj">时，损失函数开始波动</strong>。这可以从下图中得到解释，该图显示<strong class="jl hj">损失曲线在底部</strong>是平坦的，这导致了损失函数的波动。</p></div><div class="ab cl mi mj gp mk" role="separator"><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn mo"/><span class="ml bw bk mm mn"/></div><div class="hb hc hd he hf"><p id="10e3" class="pw-post-body-paragraph jj jk hi jl b jm kj jo jp jq kk js jt ju li jw jx jy lj ka kb kc lk ke kf kg hb bi translated">最后，我要感谢fastai @ Josh FP对这个话题的解释。<strong class="jl hj"> ❤️喜欢，分享，留下你的评论</strong></p></div></div>    
</body>
</html>