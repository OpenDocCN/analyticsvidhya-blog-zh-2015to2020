<html>
<head>
<title>A Bouquet of Sequence to Sequence Architectures for Implementing Machine Translation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一束用于实现机器翻译的序列到序列体系结构</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-bouquet-of-sequence-to-sequence-architectures-for-implementing-machine-translation-5d13b286df5?source=collection_archive---------16-----------------------#2020-04-16">https://medium.com/analytics-vidhya/a-bouquet-of-sequence-to-sequence-architectures-for-implementing-machine-translation-5d13b286df5?source=collection_archive---------16-----------------------#2020-04-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/bb09fff551f6ad735805cd1263d09ca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ChleIiclAWwIJYTSmG5G-g.jpeg"/></div></div></figure><p id="3429" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在本文中，我们将讨论实现机器翻译的各种可能的序列到序列架构。尽管我们将尝试解决的主要问题是机器翻译，但是相同的架构，稍加修改，也适用于其他机器学习用例，例如但不限于:</p><p id="2f3a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">文本摘要—生成输入文本摘要的模型</p><p id="5102" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">问题回答—为输入问题提供答案的模型</p><p id="b2d3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对话——生成序列中下一个对话/话语的模型</p><p id="571a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">文档分类—将输入文档分类为体育、政治、金融等的模型</p><p id="2ba0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，掌握为机器翻译设计序列到序列架构的艺术将额外武装您，使您能够毫不费力地处理上述任何用例。</p><p id="ec42" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">简介</strong></p><p id="38a1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">顾名思义，机器翻译是一种机器学习模型，可以帮助我们将文本从一种语言转换为另一种语言。在这篇文章中，我们将学习把英语句子翻译成法语。我将这篇文章评为相当高级，因此我希望你有递归神经网络的基础知识，包括长短期记忆和门控递归单元——因为我们将使用这些作为构建序列到序列架构的构建模块。此外，对自然语言处理的基本概念，即单词嵌入，标记化，词汇，语料库等。将帮助您自信地浏览这篇文章。事不宜迟，现在让我们深入有趣的序列到序列架构世界。</p><p id="a0b1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本文将讨论以下序列到序列架构。</p><p id="b196" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">经典的多对多架构</p><p id="4238" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">具有嵌入和双向层的多对多架构</p><p id="cf4b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">编码器-解码器架构</p><p id="d0aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我将用自己写的架构图和代码片段来补充文本，以确保我能公正地解释这些概念。</p><p id="5f00" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">经典多对多架构</strong></p><p id="f7b0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">经典的多对多架构是最简单的seq2seq架构，也是最容易理解的。它的输出和输入数量相同。然而，具有与输入相同数量的输出是一种限制，因为语言翻译很少具有与输入句子相同数量的句子，因此，与其他体系结构相比，充满这种限制的经典体系结构不能很好地执行机器翻译。通过添加不同种类的循环单元(即RNN、LSTM或GRU)或者通过增加隐藏层的深度或者通过改变RNN层中的维度数量。</p><p id="5413" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面的涂鸦代表了一个简单的经典seq2seq架构</p><p id="70c8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo"> Doodle:经典的多对多架构</em></p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jp"><img src="../Images/a1512856f316841e40968c920e1a5f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SSwyeUINgQVNKe65N8CEQw.jpeg"/></div></div></figure><p id="1d78" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">代码片段:经典的多对多架构</em></p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="0c58" class="jz ka hi jv b fi kb kc l kd ke">import tensorflow as tf</span><span id="2c01" class="jz ka hi jv b fi kf kc l kd ke">from tensorflow.python.keras.models import Sequential</span><span id="ffdc" class="jz ka hi jv b fi kf kc l kd ke">from tensorflow.keras.models import Model</span><span id="8bbb" class="jz ka hi jv b fi kf kc l kd ke">from tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, TimeDistributed</span><span id="2b63" class="jz ka hi jv b fi kf kc l kd ke">def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):</span><span id="7f31" class="jz ka hi jv b fi kf kc l kd ke">learning_rate = 0.01</span><span id="c461" class="jz ka hi jv b fi kf kc l kd ke">model = Sequential([</span><span id="140c" class="jz ka hi jv b fi kf kc l kd ke">SimpleRNN(256, input_shape=input_shape[1:], return_sequences=True),</span><span id="a1f7" class="jz ka hi jv b fi kf kc l kd ke">TimeDistributed(Dense(french_vocab_size, activation='softmax'))</span><span id="ade3" class="jz ka hi jv b fi kf kc l kd ke">])</span><span id="9890" class="jz ka hi jv b fi kf kc l kd ke">model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,</span><span id="1e71" class="jz ka hi jv b fi kf kc l kd ke">optimizer=tf.keras.optimizers.Adam(learning_rate),</span><span id="8a1a" class="jz ka hi jv b fi kf kc l kd ke">metrics=['accuracy'])</span><span id="2f81" class="jz ka hi jv b fi kf kc l kd ke">return model</span><span id="f4db" class="jz ka hi jv b fi kf kc l kd ke">tmp_x = pad(preproc_english_sentences, max_french_sequence_length)</span><span id="0ad4" class="jz ka hi jv b fi kf kc l kd ke">tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))</span><span id="237e" class="jz ka hi jv b fi kf kc l kd ke">simple_rnn_model = simple_model(tmp_x.shape, max_french_sequence_length, english_vocab_size, french_vocab_size)</span><span id="bf07" class="jz ka hi jv b fi kf kc l kd ke">simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)</span></pre><p id="1744" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">嵌入&amp;双向层</strong>的多对多架构</p><p id="e6ac" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过包含嵌入层和双向RNN，可以进一步支持经典的多对多架构。嵌入层帮助将输入句子标记转换成多维嵌入。嵌入可以被认为是一种矢量化的单词表示，其中具有相似语义的单词在向量空间中彼此更接近。已知具有嵌入层的模型产生更好的自然语言处理模型。</p><p id="7646" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">双向层是简单RNN的扩展，其中细胞状态在两个方向流动(向前和向后),如附带的涂鸦所示。双向层基于这样的概念，即除了当前输入单词之外，序列中的过去和未来单词都对当前翻译(或任何其他自然语言处理任务)有影响。</p><p id="5e03" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">嵌入层和双向RNN这两种增强功能预计将对模型的输出产生积极影响。</p><p id="96cc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面的涂鸦代表了一个简单的经典seq2seq架构，具有嵌入和双向RNN层。</p><p id="98ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">涂鸦:嵌入双向层的多对多架构&amp;</em></p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kg"><img src="../Images/d23da01f0a6e9fc60bbfd204e2baabb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*12ElVK2pbuM7rbWkFAm1PQ.jpeg"/></div></div></figure><p id="b2bb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">代码片段:嵌入&amp;双向层的多对多架构</em></p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="1d43" class="jz ka hi jv b fi kb kc l kd ke">import tensorflow as tf</span><span id="b4f1" class="jz ka hi jv b fi kf kc l kd ke">from tensorflow.python.keras.models import Sequential</span><span id="b700" class="jz ka hi jv b fi kf kc l kd ke">from tensorflow.keras.models import Model</span><span id="5942" class="jz ka hi jv b fi kf kc l kd ke">from tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, TimeDistributed, Embedding, Bidirectional</span><span id="a523" class="jz ka hi jv b fi kf kc l kd ke">embedding_dim = 256</span><span id="67bc" class="jz ka hi jv b fi kf kc l kd ke">def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):</span><span id="aa5b" class="jz ka hi jv b fi kf kc l kd ke">learning_rate = 0.005</span><span id="86ca" class="jz ka hi jv b fi kf kc l kd ke">model = Sequential([</span><span id="92f8" class="jz ka hi jv b fi kf kc l kd ke">Embedding(english_vocab_size+1, embedding_dim, input_length=input_shape[1], input_shape=input_shape[1:]),</span><span id="8846" class="jz ka hi jv b fi kf kc l kd ke">Bidirectional(GRU(256, return_sequences=True)),</span><span id="253f" class="jz ka hi jv b fi kf kc l kd ke">TimeDistributed(Dense(french_vocab_size, activation='softmax'))</span><span id="04ac" class="jz ka hi jv b fi kf kc l kd ke">])</span><span id="7746" class="jz ka hi jv b fi kf kc l kd ke">model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,</span><span id="c717" class="jz ka hi jv b fi kf kc l kd ke">optimizer=tf.keras.optimizers.Adam(learning_rate),</span><span id="669a" class="jz ka hi jv b fi kf kc l kd ke">metrics=['accuracy'])</span><span id="bf85" class="jz ka hi jv b fi kf kc l kd ke">return model</span><span id="c01e" class="jz ka hi jv b fi kf kc l kd ke">tmp_x = pad(preproc_english_sentences, max_french_sequence_length)</span><span id="c918" class="jz ka hi jv b fi kf kc l kd ke">embed_rnn_model = embed_model(tmp_x.shape, max_french_sequence_length, english_vocab_size, french_vocab_size)</span><span id="8126" class="jz ka hi jv b fi kf kc l kd ke">embed_rnn_model.summary()</span><span id="8fd0" class="jz ka hi jv b fi kf kc l kd ke">embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)</span></pre><p id="0dff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">编码器解码器架构</strong></p><p id="33c3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">编码器<strong class="is hj"> </strong>解码器架构解决了传统多对多架构的局限性。编码器/解码器结构能够生成长度不同于输入英语句子的法语句子。这样就克服了输入&amp;输出句子长度相同的限制。经典架构的另一个限制是它一次翻译一个单词，这有时会导致不精确的翻译。一种更自然的翻译方法是大爆炸翻译法，即。一次读完整个句子，然后一起翻译。编码器/解码器架构使用后一种方法——它首先将输入句子编码成一个编码向量，然后用它来生成输出句子。警告——编码器解码器架构仅对较短的句子有效，因为将较长句子的所有信息保留在编码器向量中可能不可行；一些信息即将丢失。这个问题是通过使用注意机制来解决的，我计划在以后的文章中讨论这个问题。</p><p id="7b2f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下面的涂鸦代表了一个编码器解码器架构。</p><p id="e275" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">涂鸦:编码器解码器架构</em></p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kh"><img src="../Images/992639c63e37aee890a5f339ddcb26ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vzN2ElWmDXTMlp6HLncyJA.jpeg"/></div></div></figure><p id="f195" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">代码片段:编码器解码器架构</em></p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="77bd" class="jz ka hi jv b fi kb kc l kd ke">import tensorflow as tf</span><span id="2cad" class="jz ka hi jv b fi kf kc l kd ke">from tensorflow.python.keras.models import Sequential</span><span id="4419" class="jz ka hi jv b fi kf kc l kd ke">from tensorflow.keras.models import Model</span><span id="1848" class="jz ka hi jv b fi kf kc l kd ke">from tensorflow.keras.layers import Input, Dense, SimpleRNN, GRU, TimeDistributed, Embedding</span><span id="57f4" class="jz ka hi jv b fi kf kc l kd ke">embedding_dim = 256</span><span id="649a" class="jz ka hi jv b fi kf kc l kd ke">def encdec1_model(encoder_input_shape, decoder_input_shape, output_sequence_length, english_vocab_size, french_vocab_size):</span><span id="82d9" class="jz ka hi jv b fi kf kc l kd ke">learning_rate = 0.005</span><span id="d1eb" class="jz ka hi jv b fi kf kc l kd ke">encoder_inputs = Input(shape=encoder_input_shape[1:])</span><span id="e64b" class="jz ka hi jv b fi kf kc l kd ke">en_x = Embedding(english_vocab_size+1, embedding_dim)(encoder_inputs)</span><span id="6f8c" class="jz ka hi jv b fi kf kc l kd ke">_, state_h, state_c = LSTM(french_vocab_size, return_state=True)(en_x)</span><span id="fc9d" class="jz ka hi jv b fi kf kc l kd ke">encoder_states = [state_h, state_c]</span><span id="81c6" class="jz ka hi jv b fi kf kc l kd ke">decoder_inputs = Input(shape=decoder_input_shape[1:])</span><span id="f3fd" class="jz ka hi jv b fi kf kc l kd ke">dec_x = Embedding(french_vocab_size+1, embedding_dim)(decoder_inputs)</span><span id="2f6f" class="jz ka hi jv b fi kf kc l kd ke">decoder_outputs = LSTM(french_vocab_size, return_sequences=True)(dec_x, initial_state=encoder_states)</span><span id="4237" class="jz ka hi jv b fi kf kc l kd ke">decoder_outputs = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(decoder_outputs)</span><span id="3fae" class="jz ka hi jv b fi kf kc l kd ke">model = Model([encoder_inputs, decoder_inputs], decoder_outputs)</span><span id="be4c" class="jz ka hi jv b fi kf kc l kd ke">model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,</span><span id="0406" class="jz ka hi jv b fi kf kc l kd ke">optimizer=tf.keras.optimizers.Adam(learning_rate),</span><span id="2c47" class="jz ka hi jv b fi kf kc l kd ke">metrics=['accuracy'])</span><span id="cc2d" class="jz ka hi jv b fi kf kc l kd ke">return model</span><span id="c236" class="jz ka hi jv b fi kf kc l kd ke">tmp_x = pad(preproc_english_sentences, max_english_sequence_length)</span><span id="fbdb" class="jz ka hi jv b fi kf kc l kd ke">tmp_y = pad(preproc_french_sentences, max_french_sequence_length)</span><span id="b48a" class="jz ka hi jv b fi kf kc l kd ke">tmp_y = tmp_y.reshape((-1, preproc_french_sentences.shape[-2]))</span><span id="5cb0" class="jz ka hi jv b fi kf kc l kd ke">encdec1_rnn_model = encdec1_model(tmp_x.shape, tmp_y.shape, max_french_sequence_length, english_vocab_size, french_vocab_size)</span><span id="2be3" class="jz ka hi jv b fi kf kc l kd ke">encdec1_rnn_model.summary()</span><span id="d4e2" class="jz ka hi jv b fi kf kc l kd ke">encdec1_rnn_model.fit([tmp_x, tmp_y], preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)</span></pre><p id="5472" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如你从doodle中注意到的，解码器部分(右边的RNN)类似于简单的RNN架构——编码器部分(左边的RNN)是我们以前没有见过的额外部分。解码器部分不是使用英语句子作为输入，而是取法语句子作为输入，而编码器部分取英语句子作为输入。编码器解码器架构的主要原则是当前预测不仅依赖于输入的英语句子，还依赖于迄今预测的法语句子。但是有一个问题，在推理过程中，我们没有可用的法语翻译。因此，在推理过程中，我们不是输入实际的法语句子，而是将前一个解码器单元的输出馈送给每个解码器单元。另一方面，第一解码器单元将被馈送<bos>或<go>(指示句子开始的标签)，它还将被馈送来自最后一个编码器的单元状态(如果我们使用LSTM作为RNN单元，则包括长期和短期存储状态)。输出序列/单词将继续生成，直到算法预测到<eos>或达到最大法语句子长度。最大法语句子长度是可配置的，但通常我们将其设置为培训期间使用的法语句子的最大长度。</eos></go></bos></p><p id="4455" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你一定注意到了，我们在训练中使用了备用交叉熵作为损失函数。还有其他优化技术，如波束搜索，已知性能更好，但实施起来不太简单。关于波束搜索的讨论超出了本文的范围。</p><p id="565a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">评估指标</strong></p><p id="5e4c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">尽管我没有提供任何评估指标，但机器翻译中非常流行的一种技术是Bleu Score，它基于我们能够在实际的人类和模型预测的翻译中找到多少相似的单字、双字、三字等。</p><p id="2a59" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">用于评估各种架构结果的Bleu分数可以使用NLTK库生成，如下面的代码片段所示:</p><pre class="jq jr js jt fd ju jv jw jx aw jy bi"><span id="342a" class="jz ka hi jv b fi kb kc l kd ke">from nltk.translate.bleu_score import sentence_bleu</span><span id="cf4f" class="jz ka hi jv b fi kf kc l kd ke">import statistics as stats</span><span id="51c6" class="jz ka hi jv b fi kf kc l kd ke">bleu_score_list = []</span><span id="4785" class="jz ka hi jv b fi kf kc l kd ke">bleu_score_list = [sentence_bleu([modified_french_sentences[100000+x]], logits_to_text(final_pred[x][0], french_tokenizer)) for x in range(100)]</span><span id="47a4" class="jz ka hi jv b fi kf kc l kd ke">print(stats.mean(bleu_score_list))</span></pre><p id="16c7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">后记</strong></p><p id="9391" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">请注意，我故意没有在本文中解释输入文本的预处理，因为它本身就是一个庞大的主题。但是，请记住，您几乎总是需要实现所有的文本预处理步骤，如标记句子，填充句子，将所有字母转换为小写，删除标点符号，词条，词干等。在开始机器翻译之前。你可能想要抓住任何自然语言处理的初学者指南来掌握这些概念。</p><p id="1ab7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在不久的将来，我将努力在github上上传附带的策划代码，以便您可以在您的计算机上下载它，并尝试各种seq2seq架构。</p><p id="cbad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我真心希望你喜欢这篇文章——如果你能在下面留下评价/反馈，我将不胜感激。如果您遇到了任何其他有趣的seq2seq体系结构，并且没有在本文中涉及，请告诉我；我非常乐意将它们收录到未来的出版物中。</p></div><div class="ab cl ki kj gp kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="hb hc hd he hf"><p id="68f3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">学分:</p><ol class=""><li id="2f8f" class="kp kq hi is b it iu ix iy jb kr jf ks jj kt jn ku kv kw kx bi translated">阿努杰·库马尔(<a class="ae ky" href="https://www.linkedin.com/in/anujchauhan/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/anujchauhan/</a>)合著</li><li id="b337" class="kp kq hi is b it kz ix la jb lb jf lc jj ld jn ku kv kw kx bi translated">Aurelien Geron —使用Scikit-Learn和TensorFlow进行机器实践学习</li><li id="6877" class="kp kq hi is b it kz ix la jb lb jf lc jj ld jn ku kv kw kx bi translated">v . Kishore Ayyadevara——神经网络与Keras食谱</li><li id="f843" class="kp kq hi is b it kz ix la jb lb jf lc jj ld jn ku kv kw kx bi translated">Udacity —自然语言处理纳米学位，深度学习纳米学位</li><li id="3642" class="kp kq hi is b it kz ix la jb lb jf lc jj ld jn ku kv kw kx bi translated">Coursera —实践与深度学习专业化中的TensorFlow</li><li id="45cc" class="kp kq hi is b it kz ix la jb lb jf lc jj ld jn ku kv kw kx bi translated">杰森·布朗尼—<a class="ae ky" href="https://machinelearningmastery.com/calculate-bleu-score-for-text-python/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/calculate-bleu-score-for-text-python/</a></li></ol></div></div>    
</body>
</html>