<html>
<head>
<title>Linear Algebra- How it is used in AI ?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性代数-它是如何在人工智能中使用的？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-algebra-how-uses-in-artificial-intelligence-2e1e001c65?source=collection_archive---------1-----------------------#2020-08-27">https://medium.com/analytics-vidhya/linear-algebra-how-uses-in-artificial-intelligence-2e1e001c65?source=collection_archive---------1-----------------------#2020-08-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/a15a571b65aa2d5b283eadc2223a0af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q2zjmycki5oXkDd3Agoogw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">如何在 AI 中使用数学或线性代数对象(向量、矩阵和张量)来存储不同维度的数据。</strong></figcaption></figure><div class=""/><p id="55e7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">了解线性代数在人工智能中的应用。</strong></p><p id="ed02" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">线性代数(数学对象)在人工智能中的应用。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es jt"><img src="../Images/b1ab9a272d2fa50128277bcd636ddf0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*X8qBAtF4njXcianHRMuYqQ.jpeg"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">AI 中的子字段</strong></figcaption></figure><p id="6561" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">人工智能不是一个单一的学科，它有子领域，如学习(机器学习和深度学习)，使用 NLP 的通信，知识表示和推理，问题解决，不确定知识和推理。</p><p id="cd88" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这篇文章中，将解释对象及其属性是如何在人工智能的子领域 ML，NLP，DL 等中使用的。，算法。</p><p id="8a6b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">描述可以应用 LA 对象的子领域概念。</p><p id="3240" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">浏览每个子领域，解释相关主题以及如何应用。下图解释了我们在人工智能中应用线性代数的领域。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es jy"><img src="../Images/8861775f97d61ef905ec5f69e97835ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*G7taeqJ2WSnxhDRFR3vSQA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">应用于这些人工智能领域的 LA 对象</strong></figcaption></figure><p id="44ad" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jz translated"><span class="l ka kb kc bm kd ke kf kg kh di"> N </span></p><p id="a25c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的图表中，使用了其他子领域，如问题解决、知识表示和知识推理，但在学习(ML/DL)和 NLP 中使用的不多。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es ki"><img src="../Images/8d27688daf53fd2664f44df0f51030c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*0iKeL_rWX5l8kce26OV1Cg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">描述这些子字段中的 LA 对象&amp;属性</strong></figcaption></figure><p id="db16" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">线性代数或数学对象是向量、矩阵和张量。根据数据的维度，你必须选择正确的对象来存储和处理，标题图描述了这一点。</p><p id="45e6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在开始如何在 AI 中使用数学对象之前，最好先刷新一下<a class="ae kj" rel="noopener" href="/@syedshafiuddin/linear-algebra-in-artificial-intelligence-quantum-computing-c61ea629367c?source=friends_link&amp;sk=e23b456585fb059a08c54b8222ec63fa">线性代数</a>。</p><p id="c124" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">数据表示</strong>:用数学对象向量、矩阵、张量来解释。</p><p id="980d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">数据集</strong>:是实例或数据点或对象的集合。每个示例都是一组功能。每个示例是一行，特征是一列。</p><p id="3865" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">设计矩阵:</strong>数据集可以通过设计矩阵来描述。设计矩阵是每行包含不同示例的矩阵。例如:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es kk"><img src="../Images/889fbc799f20b849c738bacf96c54d1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*MpsfHmqhni5s2mAL2kXKDQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">设计矩阵表示</strong></figcaption></figure><p id="9fb4" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果数据没有特定的顺序，即每个示例/行的列不相同。在这种情况下，我们把包含“m”个元素的集合描述为具有不同向量大小的集合。</p><p id="195f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在监督学习中，数据集包含标签或目标以及特征集合。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es kl"><img src="../Images/047eba2b5407ba091a9fddd4e6307760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*8sLnhdNXBob3EFHH38zcJA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">监督学习的设计矩阵</strong></figcaption></figure><p id="7eeb" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">数据处理</strong>:在我们在 ML 算法或人工智能的任何子领域中使用数据集之前，有必要准备好数据集(净化&amp;过滤)。</p><p id="4b3c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有 3 种形式的数据处理均值减法、归一化和 PCA &amp;白化。这些表格在下图中简单描述。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es km"><img src="../Images/4efafb30feac9ef056b0bbf259332dfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*3JrCtLBOKZDTjOePDByEVw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">通过 Numpy 解释的数据处理操作</strong></figcaption></figure><p id="2fd9" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这三者形成访问矩阵并产生所需的。第三种形式的 PCA 用于降维，它完全在纯线性代数中工作，下面的算法描述它。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kn"><img src="../Images/6aba5e72da8f188fd61e5e4a5cae9fa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BbX45LAhgllA-BlhzZ6SKw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">训练数据的 PCA 算法</strong></figcaption></figure><p id="7099" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在数据选择、工程、数据清理等过程中使用的一些操作。、argmin 和 argmax 是数据处理中的运算。它作用于矩阵和向量，并分别选择最小或最大的行或列。</p><p id="8a5d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里轴可以是列或行。轴 0(零)表示列，轴 1(一)表示行。</p><p id="f60a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> argmin </strong>:返回一个轴上最小值的索引。</p><p id="cd81" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> argmax </strong>:返回轴上最大值的索引。</p><p id="cce3" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">机器学习(ML) : ML </strong>是一种基于算法的方法，它从训练数据中学习，并对看不见的数据做出决策。最大似然中有许多算法用于监督和非监督学习。</p><p id="dfb5" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">LA 概念如何应用于 ML-回归算法:</strong>这里描述线性代数如何应用于回归分析。通过线性多元回归算法解释概念。下图描述了 ML 和 DL 中的 LA 概念。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es ko"><img src="../Images/c16d8e4f6ac8cf8e4a03474483a88385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*tvX9M0hItOYu_i9wr7P77w.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">ML 和 DL 中的 LA 对象、属性和用法</strong></figcaption></figure><p id="b58b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">回归分析</strong>用向量、矩阵及其性质来解释。</p><p id="8a74" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">什么是回归</strong>？它是一种统计技术，用于估计因变量和自变量之间的关系。</p><p id="6f6e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">回归分析最常见的形式是<strong class="ix hz">线性回归</strong></p><p id="66aa" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面的等式将描述简单和多元线性回归。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kp"><img src="../Images/02dc17bd6d65038c966a0032234ea345.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*nyvl75auqgv2DHIm5tQ1cw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">简单&amp;多元回归举例</strong></figcaption></figure><p id="a85d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这种技术预测连续的反应——例如，预测股票价格、房租等。,.</p><p id="7a23" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">残差</strong>:在机器学习/统计术语中，是目标变量的观测值与估计值之差。</p><p id="5aed" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">符号如下所示:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kq"><img src="../Images/06395d39a49c80e0cb6fa58c6866d9b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8UZtsIsZwznGf5ExcyXk4Q.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">目标变量</strong>的观测值&amp;估计值的符号</figcaption></figure><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es kr"><img src="../Images/c4e0f6606d7aa9dfc7099a1e1ed2195e.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*sYlGbFI3CmBmlSR9JBaCYQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">多元回归中的残差</strong></figcaption></figure><p id="3a9a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">残差平方和</strong>:我们把残差定义为<strong class="ix hz">‘r’。</strong></p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es ks"><img src="../Images/e654c94964f19ecb4727b206411759a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*S47n-tcBeCjx_JPrl94S3w.png"/></div></figure><p id="6fc2" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">最小二乘法</strong>:最小二乘法是标准方法，它使残差<strong class="ix hz">的平方和最小化。</strong></p><p id="af37" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">普通最小二乘法(OLS)或线性最小二乘法通过最小化残差平方和来估计回归模型中的参数。它通过使观察值和预测值(或拟合值或估计值)之间的 SSE 最小化的数据点画一条线。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es kt"><img src="../Images/7249043b5f0af872f0ce6a5484d735ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*hHzi9ltbIfCaWwWcUjeWrQ.png"/></div></figure><p id="fcb3" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最重要的应用是数据拟合。</p><p id="cbba" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">数据拟合</strong>:它是构造一个曲线拟合或数学函数的过程，对一组数据点进行最佳拟合。</p><p id="c0f5" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">曲线拟合可以是线性的，也可以是非线性的。下文描述了两条曲线。</p><p id="e560" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">线性曲线</strong>:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es ku"><img src="../Images/317482bc6c634a151d0e6f73fd83ae45.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*sfKFYFIpvo4zg_tDp6dTQg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">线性曲线</strong></figcaption></figure><p id="a018" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在引入回归分析之后，让我们定义它的损失和成本函数。</p><p id="1a29" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">损失函数</strong>:线性回归的损失函数定义如下</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kv"><img src="../Images/a99cf9a6d82ef6010c0220d77551ece1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1bH6iqNn9mH6q_cdaDogTA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">回归的损失函数</strong></figcaption></figure><p id="42a6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对参数求导求参数。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kw"><img src="../Images/5651caf2bdee1a253ca6bdace7dbdee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ANSCaA6zEr6xZqRzljRJJA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">通过对损失函数应用梯度来寻找权重或参数</strong></figcaption></figure><p id="7827" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">什么是正则化</strong>:为了避免过拟合问题，正则化技术被用来缩小参数的大小。这可以通过将罚值(参数和的函数)添加到成本函数中来实现。L1、L2、辍学和最大范数约束用于 DL，而 L1、L2、L1+L2 用于 ML。</p><p id="5d7a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你使用神经网络进行最大似然算法，你可以应用以上 4 种正则化技术。</p><p id="f937" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> L2 正规化</strong>:这是最常见的正规化形式。它可以通过直接在目标中惩罚所有参数的平方值来实现。</p><p id="ad2f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> L1 正则化</strong>:每个权重 w 我们给目标函数加上 param*|w|项，L1、L2 都定义如下:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kx"><img src="../Images/e9eaaf5c06145a6b016e211be73a51e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uwsJXw2k7fX0-Fnm1j_NPw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">广义正则化</strong></figcaption></figure><p id="083f" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">在用于正则化的机器学习中使用向量范数:</strong></p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ky"><img src="../Images/7a716e2ea648626d8f4cccc0787b115a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RTP6Uud03-76vn3MW29NUA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">正则化中的向量范数，以避免过拟合问题</strong></figcaption></figure><p id="29c5" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jz translated"><span class="l ka kb kc bm kd ke kf kg kh di">D</span><strong class="ix hz">eep Learning(DL)</strong>:它是 ML 的一个分支，深度学习文本、图像或视频。像图像或视频这样的非结构化数据可以使用 DL 来处理。DL 有许多应用，如图像处理(使用 CNN 的计算机视觉)、视频处理(使用 RNNs 的计算机视觉)、文本处理(使用 RNNs 的 NLP、LSTMs)等。，我们可以结合强化学习(DEEP RL)。</p><p id="5fdc" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">DL 的灵感来源于神经元。一个神经元与多个神经元连接，并在该神经元上应用激活功能。</p><p id="8050" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">向量、矩阵和张量是在 DL 领域使用的对象。下图是神经网络的示例，描述了输入、神经元、层、前馈传播、反向传播等。,</p><p id="3997" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">深度学习涉及许多数学学科，本文考虑线性代数。描述数学对象在每个阶段是如何被使用的。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es kz"><img src="../Images/5015cb52fcb456e181780f1decd6c785.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*mdfdWHY1YKGDbBRzqRCNbA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">常见神经网络架构</strong></figcaption></figure><p id="0acf" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">输入</strong>:输入以向量、矩阵或张量的形式输入到神经网络。最后，每个数据对象/样本将在向量中。这里的输入是一个 n 维向量。它是数据集中的一个例子或数据点。</p><p id="8544" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">神经元或节点</strong>:这里我们对前一层的输入和权重或连接应用激活函数。它是一组相互连接的自然或人工神经元，使用数学或计算模型进行信息处理，基于连接主义的计算方法。</p><p id="d8d1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">连接:</strong>生物神经元的连接被建模为权重。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es la"><img src="../Images/04ed174d9d607be0de64133a58e1ebb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:204/format:webp/1*-O1dcdCK9H2a65hWMyEDeA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">每个神经元将连接到下一层的其他神经元</strong></figcaption></figure><p id="a69e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">层</strong>:每一层包含一组神经元，如下图所示。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lb"><img src="../Images/f1cebbe856c75323e61dbaaf14bafc8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:92/format:webp/1*iC3A70z2AB_zLNFbEmoVgg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">层包含神经元，并将在矢量级操作。</strong></figcaption></figure><p id="5b8b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">前馈传播</strong>:这些被称为深度前馈网络或前馈神经网络或多层感知器(MLPs)。这些被称为前馈，因为信息流从 x 开始通过被评估的函数，通过用于定义 f 的中间计算，最后到达输出 y。</p><p id="c6ee" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">前馈神经网络之所以称为网络，是因为它们通常由许多不同的函数组合而成。</p><p id="532c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">举个例子，我们的网络有三个功能连接成一个链，形成</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lc"><img src="../Images/3c1a3a2597dbce81052f5b0c07a1c448.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*tl6Tv6xk1llz8ydzhFQQXA.png"/></div></figure><p id="cf1c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这些链结构被用作神经网络的结构。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ld"><img src="../Images/bc5212b00fe8f7fcf16b66801a35aa98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XzHF1od7nPVNwznYOtts9Q.png"/></div></div></figure><p id="ddcf" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看如何在前馈网络中应用向量和矩阵。</p><ol class=""><li id="1cbe" class="le lf hy ix b iy iz jc jd jg lg jk lh jo li js lj lk ll lm bi translated">向量化输入、权重和偏差:x:n 维的输入向量；下一层 n 行 m 个神经元的 w-权重矩阵，下一层 m 个神经元的偏差。</li></ol><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es ln"><img src="../Images/bbdce87a03d7127a6a2f1e0d530614f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*EtY-Dp76BuehkaIMfBbkyA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">输入、权重和偏差的整体计算进入临时变量 Z </strong></figcaption></figure><p id="e4e7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">由此可以得出结论</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lo"><img src="../Images/5eba9ab19f72086667189a7a9eeb4c2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*B67PWLTNygiGIN92x8JIQw.png"/></div></figure><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lp"><img src="../Images/53fa87d3a313ca2331bb766ce43b60d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*l2qlikVBrrDt9VBwDOuUCw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">广义方法</strong></figcaption></figure><p id="e9a1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.将中间变量 Z 应用于激活函数</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lq"><img src="../Images/e0056907f241904aec283299e109dc48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*5_U_GdYunB99DoGohaO20w.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">前馈到下一层</strong></figcaption></figure><p id="2872" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.重复上述步骤，并以正向方式将结果馈送到下一层。</p><p id="8ba2" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">每个神经元的中间计算和激活函数如下:</p><p id="5d5b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">考虑一个输入 2 个特征、3 个隐藏层和 1 个输出层的神经网络的例子，具有 3，5，4，2，1 个隐藏单元。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lr"><img src="../Images/a19c831ba9267e6693dd84269cfe644c.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*CKPFbKxpBrRwa9sl7atj6A.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">具有 1 个输入、4 个隐藏和 1 个输出层的神经网络</strong></figcaption></figure><p id="3fd6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们将向量、矩阵运算用于正向传播。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es ls"><img src="../Images/6b22970cf5e76346fb7cea002a16ca35.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*rotZZhUj3IxY7BgQDgO-7A.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">正向传播 4 层</strong></figcaption></figure><p id="5d9e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">了解你的矩阵维度</strong>:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lt"><img src="../Images/81cb904106008bd01a520937bc81f085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-jtPNve_sJSWOvbpocxtg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">矩阵的维数，前馈传播中的矢量</strong></figcaption></figure><p id="effb" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">前馈传播</strong> =矩阵-矢量积法则，矩阵与激活函数相加。</p><p id="4c83" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">反向传播</strong> =矩阵演算+线性代数乘积规则——将在下一篇文章中介绍。</p><p id="92a6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">自然语言处理(NLP): </strong> NLP 关注的是人和计算机之间的交互，特别是如何对计算机进行编程，以处理和分析大量的自然语言数据。</p><p id="60c1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里我们描述用于 NLP 的 Word2Vector (W2V)技术。在 Word2Vec 中，每个不同的单词都有一个特定的数字列表，称为矢量。基于 W2V，我们可以应用向量属性来检查向量之间的相似性和语义相似性。</p><p id="1f23" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在 NLP 中，我们使用的向量和矩阵如下:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lu"><img src="../Images/056345ece9774fc2e7260db1b18f200e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*McU6rcIG0wiwGhXEO0Jz5w.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">W2V 算法中使用的向量和矩阵</strong></figcaption></figure><p id="4af8" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">W2V 用于自然语言处理中的许多任务，它是将单词捕获到向量中的基础。自然语言文本=离散符号序列</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lv"><img src="../Images/197d29ab32bab4b379684bddb4aee63c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*4-WatC4hH3RXlUFM5Lh0Ug.png"/></div></figure><p id="fdaa" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">基于单词的上下文/使用产生密集的矢量表示。</p><p id="be86" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">什么是目标&amp;上下文单词</strong>:考虑一个上下文窗口大小=2 的文本实例。以下描述</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es lw"><img src="../Images/f69b42cddaa868e3c11c92f91b29319d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*Xv_440LJd4krTuudyRauEg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">上下文和目标/当前单词</figcaption></figure><p id="e72b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">如何表示一热表示法？</strong></p><p id="32b7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">词汇</strong>:特征向量中编码的单词集合称为词汇，所以向量的维数等于词汇的大小。简而言之，|V| =词汇表的大小。</p><p id="03ef" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">假设我们的文本数据集包含以下几行</p><ol class=""><li id="ae5e" class="le lf hy ix b iy iz jc jd jg lg jk lh jo li js lj lk ll lm bi translated">“可爱的小猫发出呜呜声，然后…</li><li id="8d94" class="le lf hy ix b iy lx jc ly jg lz jk ma jo mb js lj lk ll lm bi translated">“可爱的毛茸茸的猫呜呜叫着…”</li><li id="ab6e" class="le lf hy ix b iy lx jc ly jg lz jk ma jo mb js lj lk ll lm bi translated">"小猫喵喵叫着，她.."</li><li id="a4bf" class="le lf hy ix b iy lx jc ly jg lz jk ma jo mb js lj lk ll lm bi translated">“那只毛茸茸的大狗跑过来咬了……”</li></ol><p id="00ec" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从这 4 个句子的基础词汇:{比特，可爱，毛茸茸，大声，喵喵叫，咕噜，跑，小} — 8 是词汇长度。让我们定义目标词和上下文词。</p><p id="0939" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">目标词:<strong class="ix hz">小猫</strong>，语境词:{可爱，咕噜，小，喵喵}</p><p id="8c27" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">目标词:<strong class="ix hz">猫</strong>，语境词:{可爱，毛茸茸，喵喵叫}</p><p id="d178" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">目标词:<strong class="ix hz">狗</strong>，上下文词:{大声，毛茸茸，跑，咬}。</p><p id="81ae" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们把词汇长度表示为一个向量<strong class="ix hz"> 8。</strong></p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mc"><img src="../Images/2ddaa28ed2719afda476577ca23dbba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CnEWrp6L9fmBlhY-Pxo4VQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">作为向量的词</strong></figcaption></figure><p id="46ce" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将向量定义为当上下文单词出现时指定“1”，否则在向量的维度上指定“0”。</p><p id="f662" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">检查向量之间的相似性</strong>:为了检查相似性，我们可以使用内积(或)余弦作为相似性核心。</p><p id="cf54" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Sim(小猫，猫)=余弦(小猫，猫)~ 0.58；Sim(小猫，小狗)=余弦(小猫，小狗)~ 0.00；Sim(猫，狗)=余弦(猫，狗)~0.29</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es md"><img src="../Images/44939adf147bfdcd664b6064dc427852.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*-rOCIzPo5SSilc3ag3Au8A.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">向量之间的余弦、点和叉积</figcaption></figure><p id="5dd1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">嵌入矩阵</strong>:嵌入矩阵可以定义为行- &gt;目标单词和列- &gt;上下文单词的数量是上下文窗口的长度</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es me"><img src="../Images/e71607a753f0a8aa09d1c8542c7f52fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*XrCoRvEsxjIVZRYqjd1y1Q.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">嵌入矩阵维数</figcaption></figure><p id="fd8a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">行是单词向量，所以我们可以用一个热向量来检索它们</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es mf"><img src="../Images/992ae1c91f536bbaddb2070da8662f2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*xuWGXgDL5VTvgpueaf-tIA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">用一个热键表示单词</strong></figcaption></figure><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es mg"><img src="../Images/106a19f8d62f4c2688c49a98a7d07faf.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*0gyeFlhyQKzfcL8UhtvaRQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">以行为目标词的嵌入矩阵及其上下文词</strong></figcaption></figure><p id="3096" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">构建嵌入矩阵的算法</strong>:</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mh"><img src="../Images/0dd46926018458fee608c81f9dd0c432.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kuiz4kf7-Kpbk_D2ezusOA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">构建嵌入矩阵的步骤</strong></figcaption></figure><p id="15d7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">捕捉单词含义的向量。也可以称为 Word2Vec，Word Emebedding。以下是算法</p><ol class=""><li id="bfd0" class="le lf hy ix b iy iz jc jd jg lg jk lh jo li js lj lk ll lm bi translated"><strong class="ix hz"> Skip-gram (SG) </strong>:预测由目标单词给出的上下文单词</li><li id="b688" class="le lf hy ix b iy lx jc ly jg lz jk ma jo mb js lj lk ll lm bi translated"><strong class="ix hz">连续单词包(CBOW) </strong>:预测上下文单词给出的目标单词</li><li id="23a1" class="le lf hy ix b iy lx jc ly jg lz jk ma jo mb js lj lk ll lm bi translated"><strong class="ix hz"> Glove </strong>:利用全局共现统计。Glove 由一个加权最小二乘模型组成，该模型对全局单词-单词共现计数进行训练。</li></ol><p id="18b1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以上 3 种算法在《线性代数》中的用法说明。</p><p id="1ee9" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">步骤 1:跳格(SG) </strong>:跳格(SG)模型的目标是最大化平均对数概率</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/333f53614124dea218a724bed5aef992.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*Ku-1gvKCEiwAXKMmmiJ2AA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">描述上下文和目标词</figcaption></figure><p id="72f3" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">步骤 2 </strong>:投射到词汇 Softmax</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es mj"><img src="../Images/1d65e54029a7a9095a18e38de2d2944b.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*xsAq7r36dkhmLsnhBrG6XQ.png"/></div></figure><p id="7642" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第三步:学会估计上下文单词的可能性</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es mk"><img src="../Images/915c5dd55203405ec678ae31a65489b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*MotBdj64ls6ZPysKEP9EQQ.png"/></div></figure><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es ml"><img src="../Images/6a55abdea8f8158a6d5c855b5a287745.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*-OEGK085NH3kXBCKdBGSSQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">跳格图</strong></figcaption></figure><p id="fa40" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">连续词包(CBOW) </strong>:根据上下文词预测目标词或当前词。它的可能性分布是</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es mm"><img src="../Images/481d0cb5481d05d740d198f8f4a7cd58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*TYKQYxexzat0_XYoQgJkmA.png"/></div></figure><ul class=""><li id="1590" class="le lf hy ix b iy iz jc jd jg lg jk lh jo li js mn lk ll lm bi translated">投射回词汇大小/最大值</li></ul><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es mo"><img src="../Images/09665321e0e4a37b04f23ba2669a0407.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*NzcBXkxi_aOfRnAAC3YIAg.png"/></div></figure><ul class=""><li id="806b" class="le lf hy ix b iy iz jc jd jg lg jk lh jo li js mn lk ll lm bi translated">嵌入上下文单词，添加它们。</li></ul><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es mp"><img src="../Images/244152b7511a53916a28a81981d5a409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*DheXstC9pcARNa5HZIeuCg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">在 LA </strong>中以向量矩阵乘积规则的 softmax 形式表示当前单词</figcaption></figure><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es ki"><img src="../Images/c838d74beade2b02060a75784210b9ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*MJnD2wAxJR4P2yhUWPFJrA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv"> CBOW </strong></figcaption></figure><p id="c1a1" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz"> GLOVE </strong>:和 word2vec 一样，GLOVE 是一组捕捉语义信息(即单词的含义)的向量。它由一个加权最小二乘模型组成，该模型对全局单词-单词共现计数进行训练。</p><p id="8647" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Glove 利用了全局发生统计。</p><p id="85d7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">共现矩阵</strong>:我们使用下面的语料库定义这个矩阵。</p><p id="91ec" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我喜欢深度学习；我喜欢 NLP 我喜欢飞行。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es mq"><img src="../Images/224d8896c7ac3920175401e3e508c2e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1302/format:webp/1*-GlKas3GpeG_1BtLqXdyFw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">同现矩阵</strong></figcaption></figure><p id="58b6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">设 X 为单词-单词共现计数矩阵。</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mr"><img src="../Images/a004943149c9f65cadcb36bcee88cf88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*awxGh9BGp_h3zarY17hVQw.png"/></div></div></figure><p id="d740" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">像 word2vec 中的情况一样，每个字有两个向量，输入(v)和输出(u)</p><figure class="ju jv jw jx fd hk er es paragraph-image"><div class="er es ms"><img src="../Images/f3543599a698ce7ace86fae36157d2ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*wS6EV3N2jG1QC8r2_PpOHg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated"><strong class="bd hv">手套模型的成本函数</strong></figcaption></figure><p id="9cbb" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">结论</strong>:描述了线性代数如何应用于人工智能的各个领域，在我们进入 ML、DL 或 NLP 之前，最好对线性代数感兴趣。我试图从算法的角度讲述如何应用线性代数的东西，我希望它能给我更多地参与线性代数的力量。</p><p id="bf1b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">线性代数促进了其他学科的发展，如在 DL 语言中大量使用反向传播的矩阵微积分。</p><p id="6abf" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">感谢您阅读这篇文章，如果有任何错误，请留言，并感谢您的反馈。</p><p id="8734" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">参考文献</strong>:</p><ol class=""><li id="a348" class="le lf hy ix b iy iz jc jd jg lg jk lh jo li js lj lk ll lm bi translated">《人工智能:一种现代方法》,作者斯图尔特·罗素，彼得·诺维格，</li><li id="4bf0" class="le lf hy ix b iy lx jc ly jg lz jk ma jo mb js lj lk ll lm bi translated"><a class="ae kj" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">深度学习书籍</a>作者:伊恩·古德菲勒、约舒阿·本吉奥和亚伦·库维尔</li><li id="8461" class="le lf hy ix b iy lx jc ly jg lz jk ma jo mb js lj lk ll lm bi translated"><a class="ae kj" href="https://en.wikipedia.org/wiki/Regression_analysis" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Regression_analysis</a></li><li id="b420" class="le lf hy ix b iy lx jc ly jg lz jk ma jo mb js lj lk ll lm bi translated"><a class="ae kj" href="http://web.stanford.edu/class/cs224n/" rel="noopener ugc nofollow" target="_blank">http://web.stanford.edu/class/cs224n/</a></li><li id="a6c8" class="le lf hy ix b iy lx jc ly jg lz jk ma jo mb js lj lk ll lm bi translated"><a class="ae kj" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">向量空间中单词表示的有效估计</a></li><li id="ad83" class="le lf hy ix b iy lx jc ly jg lz jk ma jo mb js lj lk ll lm bi translated"><a class="ae kj" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/projects/glove/</a></li></ol></div></div>    
</body>
</html>