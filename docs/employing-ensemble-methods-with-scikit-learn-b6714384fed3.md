# 使用带有 Scikit-learn 的集成方法

> 原文：<https://medium.com/analytics-vidhya/employing-ensemble-methods-with-scikit-learn-b6714384fed3?source=collection_archive---------17----------------------->

![](img/8d65ebd053fe341bb77053d51d8f1f46.png)

来源:[https://www . shutterstock . com/video/clip-29137192-绿色-森林-松树-仙女-未动-云杉](https://www.shutterstock.com/video/clip-29137192-green-forest-pine-trees-fairy-untouched-spruce)

尽管每天都有大量的机器学习框架和库被引入[，scikit-learn](https://scikit-learn.org/stable/tutorial/basic/tutorial.html) 仍然轻松地保持着它的流行。特别是，scikit-learn 具有对集成学习极其全面的支持，这是一种减轻过拟合的重要技术。集成学习是最有趣的概念之一。我将讨论几种重要类型的集成学习模型的发展过程。

# **目录**

01)决策树和随机森林——集成学习的理想模块

02)平均和提升技术

03)装袋和粘贴

04)构建助推模型

05)利用模型堆叠

在这篇博客中，我将讨论如何使用集成学习来提高鲁棒性并减少对训练数据的过度拟合，然后我们将继续讨论不同类型的集成学习技术，稍后讨论平均提升、投票、堆叠，然后讨论 [scikit-learn](https://scikit-learn.org/stable/) 中对集成学习的内置支持，最后我们将讨论一个实际场景。

让我们快速熟悉术语[集成学习](https://scikit-learn.org/stable/modules/ensemble.html)的定义，它是一种机器学习技术，其中几个学习者被组合起来以获得比任何一个单独的学习者更好的性能。集成学习模型寻求引导群体的智慧。将几个学习者结合起来会比单个学习者有更好的表现。集成指的是可以在同一数据集上训练的多个不同的学习者。

当我们开发集成学习时，重要的问题出现了

01.用什么样的个别学习者？

02.个人学习者应该如何训练？

03.个别学习者应该如何组合？

当你建立一个集成学习模型时，模型中的个体学习者或预测者绝对可以是任何类型，选择算法没有限制。但是你应该选择每个学习者的表现尽可能不同于其他学习者。如果我们考虑一个分类问题，有很多算法可以使用，如逻辑回归，支持向量机，k-最近邻，朴素贝叶斯。

决策树更常用。决策树的集合是一个随机森林。随机森林使得建立不相关的学习者变得容易。

当你使用集成学习模型时，你不仅仅使用 2 或 3 个预测器，你可能有 100，200，500 个预测器。如果你希望每个单独的预测器在集合中尽可能的不同，决策树有助于做到这一点，这是集合学习者经常随机选择决策树的主要原因。

现在第二个问题的答案是，如果学习者是不同的，每个学习者都可以在整个数据集上进行训练。对于相似的模型:-每个模型可以在训练数据的随机样本上被训练，并且它也可以使用一组随机的特征来训练不同的模型。

第三，集合的最终预测必须以某种有意义的方式组合几个模型的所有预测，并且有许多不同的技术可以用于此，如果我们以一个案例为例，如果你正在处理一个分类模型，你可以使用**硬投票**(针对单个学习者的多数投票)一个替代方案是**软投票**(概率加权平均)，另一个奇特的方法是**堆叠**(训练附加模型以组合来自单个学习者的预测)

现在我们对什么是集成学习有了完整的了解..！

你可以用两种方式训练你的集合，一种是**平均**(并行训练预测器并平均单个预测器的分数)，另一种是**提升**(按顺序训练预测器，每个预测器从早期错误中学习)。平均和提升指的是如何训练预测器。当我们讨论集成学习技术时，我们关注投票(单个预测者的多数投票是集成的最终预测)和堆叠(在单个预测上拟合模型以获得集成的最终预测)。

**平均:-** 并行训练多个学习者，从一个学习者获得个体预测，集合的最终预测是个体预测的平均值，投票可以被认为是一种平均技术。通常，使用决策树或随机森林来建立不同的模型。在训练数据的不同样本上训练模型，**打包**(有替换的样本数据)，**粘贴**(无替换的样本数据)。

**Boosting:-** 依次训练多个学习器，每个模型从以前模型的错误中学习，我们可以调整每个模型的学习率或贡献，增加一个学习器可以提高模型的准确性。

因为学习者是按顺序应用的，但是每一个额外的学习者都会有所贡献，所以 Boosting 的扩展性不如平均技术。有两种最受欢迎的提升技术，它们是**自适应提升或 AdaBoost** (每个模型更关注前一个模型出错的训练实例)当前一个模型的错误在数据输入下一个模型时被加重，其他受欢迎的提升是**梯度提升**(每个模型依次适合前一个模型的剩余误差)

当您处理相同的训练数据时，很难建立不同数量的模型例如，如果您使用逻辑回归进行分类，则很难使几个模型具有相同的训练数据这就是决策树和随机森林发挥作用的地方。决策树是一种高方差算法，这意味着对训练数据的敏感性，当您对训练数据进行不同的采样时，您可以使用决策树建立非常不同的模型。

**决策树:-** 基于 x 变量的阈值构建树的 ML 模型。与基于规则的树不同，因为阈值是由训练确定的。决策树在训练数据上建立了一个树形结构，有助于根据规则做出决策。

**随机森林:-** 决策树的集合(集合)，其中个体树在训练数据的不同随机子集上被训练。

集成学习也可以用于减轻对训练数据的过度拟合。当机器学习模型在训练数据上表现出色，但在实际使用中表现不佳时，我们称之为模型中的过拟合。简单来说，我们可以说模型已经记住了训练数据，而不是从中学习重要的特征。过度拟合模型是一种训练误差较低但在现实世界中效果不佳的模型。这个模型有很高的测试误差。过度拟合的原因是偏差-方差权衡中的次优选择。过度拟合的模型具有:-高方差误差，低偏差误差。(低偏差-对基础数据的假设很少，高偏差-对基础数据的假设较多)&(高方差-当训练数据更改时模型会发生显著变化，低方差-当训练数据更改时模型不会发生太大变化)

减轻过度拟合的方法

01)正则化-惩罚复杂模型

02)交叉验证—不同的培训和验证阶段

03)辍学(仅 NNs 在训练期间有意关闭一些神经元

集成学习是减轻过拟合的重要技术。有了集成学习，你就不依赖于一个算法或一个模型。你有一套不同的预测，这有助于减轻过度拟合。

该编码了！

这是到回购协议的[链接](https://github.com/vidush5/Ensemble-learning-),由这里的代码组成。我使用 scikit-learn 投票分类器方法开发了一个简单的投票方法。

在下一篇博客中，我将讨论使用平均方法实现集成学习，使用 boosting 方法实现集成学习，以及使用模型堆叠方法实现集成学习。

注意安全！