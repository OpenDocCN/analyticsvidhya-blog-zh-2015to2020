<html>
<head>
<title>Overview of optimizers for DNN: when and how to choose which optimizer — Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DNN优化器概述:何时以及如何选择哪个优化器—第1部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/overview-of-optimizers-for-dnn-when-and-how-to-choose-which-optimizer-951084b72061?source=collection_archive---------6-----------------------#2020-04-10">https://medium.com/analytics-vidhya/overview-of-optimizers-for-dnn-when-and-how-to-choose-which-optimizer-951084b72061?source=collection_archive---------6-----------------------#2020-04-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="b2c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个故事里，我想<strong class="ih hj">回顾一下深度神经网络</strong>的优化方法的发展<em class="jd">【DNN】</em><strong class="ih hj">分享一下使用优化器</strong>的建议。</p><blockquote class="je jf jg"><p id="a042" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">这是我的故事的第一部分。如果你熟悉所有的优化器并想练习使用优化器，请查看<a class="ae jk" rel="noopener" href="/@shengfang/overview-of-optimizers-for-dnn-when-and-how-to-choose-which-optimizer-part-2-16524dedbfd2">第2部分</a>。</p></blockquote><p id="1668" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">你能在第1部分和第2部分找到什么:</strong></p><ol class=""><li id="6143" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">从直观的角度简要回顾流行的优化。(第一部分)</li><li id="1036" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">流行的自适应优化器<em class="jd"> Adam </em>的缺点。(第二部分)</li><li id="44d8" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">关于联合使用不同优化器以获得更好性能的建议。(第二部分)</li></ol><p id="cf20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">谁可能感兴趣:</strong></p><ol class=""><li id="9669" class="jl jm hi ih b ii ij im in iq jn iu jo iy jp jc jq jr js jt bi translated">想简单了解一下从<em class="jd"> SGD </em>到<em class="jd"> Nadam </em>的优化者。(第一部分)</li><li id="f93b" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">想要练习如何使用它们。(第二部分)</li></ol></div><div class="ab cl jz ka gp kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hb hc hd he hf"><h1 id="d3e6" class="kg kh hi bd ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld bi translated">优化的直观视角</h1><p id="d368" class="pw-post-body-paragraph if ig hi ih b ii le ik il im lf io ip iq lg is it iu lh iw ix iy li ja jb jc hb bi translated"><em class="jd"> DNN </em>优化的目标是找到最佳参数w，使损失函数<em class="jd"> f(w，x，y) </em>最小，利用下面的<em class="jd"> f(w) </em>进行简化，服从<em class="jd"> x，y </em>，其中<em class="jd"> x </em>为数据，<em class="jd"> y </em>为标号。梯度下降(<em class="jd"> GD </em>)是机器学习最常用的优化方法。在这个方法中，我们需要另一个参数叫做学习率，<em class="jd"> α </em>。</p><p id="b1cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们开始梯度下降过程，在每个批次/步骤<em class="jd"> t: </em></p><figure class="lk ll lm ln fd lo er es paragraph-image"><div role="button" tabindex="0" class="lp lq di lr bf ls"><div class="er es lj"><img src="../Images/5ab50eaff660efae99be093c505a6e0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gRWsicLDp8_HSnWaW2krhw.png"/></div></div></figure><p id="9718" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，让我们用这4个步骤来回顾不同的优化。</p><h1 id="446b" class="kg kh hi bd ki kj lv kl km kn lw kp kq kr lx kt ku kv ly kx ky kz lz lb lc ld bi translated">优化器综述</h1><blockquote class="je jf jg"><p id="07c2" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">批量梯度下降(BGD)</p></blockquote><p id="3e83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd"> BGD </em>仅针对一次参数更新，计算整个训练数据集的梯度。收敛可能会非常缓慢。如果训练数据集太大而无法填充到内存中，<em class="jd"> BGD </em>变得难以处理。此外，<em class="jd"> BGD </em>不兼容在线更新模型，例如，动态更新新数据。</p><blockquote class="je jf jg"><p id="914c" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">随机梯度下降(<em class="hi"> SGD </em>)</p></blockquote><p id="7c38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd"> SGD </em>用一个数据计算梯度。计算变得更快，但是梯度下降的过程变得波动。仅从一个数据计算的梯度下降方向不是全局稳定的。它甚至可以与真正的梯度下降方向相反。</p><blockquote class="je jf jg"><p id="69ae" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">小批量梯度下降</p></blockquote><p id="63a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">新币</em>和<em class="jd"> BGD </em>之间的权衡是小批量梯度下降。这个优化器使用部分数据(<em class="jd"> n &gt; 1 </em>)来计算梯度并更新参数<em class="jd"> w </em>。这是现代机器学习训练过程中最常用的设置。在本故事的其余部分，<em class="jd">新币</em>表示小批量的<em class="jd">新币。</em></p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es ma"><img src="../Images/516b6ee3ef6613e1ebe8b71753653b35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*eDDjx7qJHxoctQeURLcbcw.png"/></div></figure><blockquote class="je jf jg"><p id="ba13" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">带动量的SGD</p></blockquote><p id="c1ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">动机</strong> : <em class="jd"> SGD </em>在峡谷状曲面曲线中优化有困难。很容易堵在鞍点。图1[1]显示了不同优化器在鞍点处的峡谷状曲线上的行为。红点的最终位置称为鞍点。不要担心图像中的新名称，我将在故事的其余部分介绍它们。</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es mb"><img src="../Images/6a4275c097be09041e4c5c0683454b17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/1*QBY1jeAWGhsiMkEbkDXmBw.gif"/></div><figcaption class="mc md et er es me mf bd b be z dx translated">图1:鞍点</figcaption></figure><p id="333c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图2和图3 [1]是中间有局部最小值的等高线图。带闪光的线条是优化的痕迹。对于图2，很容易发现竖步大，横步小。但是我们需要的是沿着水平方向的一大步和沿着垂直方向的一小步，如图3所示。图3中应用的技术是call momentum，它累积过去的梯度来确定当前的梯度，以加速收敛。</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es mg"><img src="../Images/317538fe9da6f54b0c3e19e21dbe1e2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*-iZ-UpsUJVOvtnUl1UIqxw.png"/></div></figure><p id="fa0b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解决方案</strong>:带动量的<em class="jd"> SGD的实现使用了指数平均的方法。指数率<em class="jd"> β </em>通常设定为0.9。</em></p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es mh"><img src="../Images/2d005569e2f855d27f010c57ab577fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*qgKMa-kLqG7136fFlMIFow.png"/></div></figure><blockquote class="je jf jg"><p id="66ee" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">带内斯特罗夫加速的SGD(内斯特罗夫加速梯度，NAG)</p></blockquote><p id="ab99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">动机</strong>:一个深度学习模型有几十万个参数。当我们在这样的高维空间中优化参数时，很容易陷入局部极小值。如果我们能给优化器前瞻的能力，它给我们更多的机会跳出局部最小值。<em class="jd">带内斯特罗夫加速度的SGD</em>用预测近似值<em class="jd"> w </em>计算电流梯度。通过将电流<em class="jd"> w </em>与先前的更新值相加来进行近似。那么当前更新值被认为是近似值的校正。更多的解释可以在[2]和[3]中找到。</p><p id="59f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解决方案:</strong></p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es mi"><img src="../Images/446f3847af0f6f8f270e1bc9d52625b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*zJHm09TMgB9Hn1oyUThgPA.png"/></div></figure><blockquote class="je jf jg"><p id="f151" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">阿达格拉德</p></blockquote><p id="c0ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">动机</strong>:动量赋予<em class="jd"> SGD </em>根据梯度历史调整更新值的能力。但是对于<em class="jd"> w </em>的所有参数，学习率是相同的。我们可以根据参数的重要性用不同的学习率来更新参数吗？在网络的训练过程中，我们可以对那些与频繁特征相关的参数进行慢速更新，而对那些与非频繁特征相关的参数进行快速更新。所以所有的参数都能以相似的节奏收敛。我们用二阶动量来实现这个命题。我们对一个参数的所有过去梯度的平方求和。该参数的重要性通过将总体学习率除以总和来衡量。</p><p id="f38c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解决方案:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es mj"><img src="../Images/e658859d45d4a9cfd95c55f1ec118ce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*dmytdUhkVT5OPhdY7Tfgxw.png"/></div></figure><blockquote class="je jf jg"><p id="dd1a" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">AdaDelta/RMSprop</p></blockquote><p id="06ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">动机</strong>:在<em class="jd"> AdaGrad </em>中计算出的二阶动量是所有历史的积累，当训练过程较长时变得极其巨大。这导致更新值无限接近0，模型无法收敛。为了克服这个问题，我们计算二阶动量的指数平均值来代替所有过去梯度的总和。</p><p id="ccd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解决方案:</strong></p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es mk"><img src="../Images/645b03013c9fab647168a24708c298b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*scm9ot8eEZjEYqXBz3cCtA.png"/></div></figure><blockquote class="je jf jg"><p id="1657" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">圣经》和《古兰经》传统中）亚当（人类第一人的名字</p></blockquote><p id="9169" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">动机</strong> : <em class="jd"> Adam </em>是最常用的优化器，因为它结合了<em class="jd"> SGD和momentum </em>以及<em class="jd"> RMSProp </em>。</p><p id="1cb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解决方案:</strong></p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es ml"><img src="../Images/90fd6782401e555c6a571849882a439a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*RX7u9a-X3ZuCoyfx3sL6tw.png"/></div></figure><blockquote class="je jf jg"><p id="1689" class="if ig jd ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">那达慕</p></blockquote><p id="b4ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">动机</strong>:是否<em class="jd">亚当</em>结合了之前讲过的所有方法？我们似乎忘记了<em class="jd">内斯特罗夫加速梯度</em>。让我们整合它，这个优化器叫做Nadam。</p><p id="49e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">解决方案</strong>:</p><figure class="lk ll lm ln fd lo er es paragraph-image"><div class="er es mm"><img src="../Images/7b25cbd25321712b7f2521254da223b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*U0Tskv7N_hrbLMPQ2JwECw.png"/></div></figure></div><div class="ab cl jz ka gp kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hb hc hd he hf"><p id="5f2a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们已经用直观的视角回顾了DNN的大多数优化器。一些从业者认为优化者像<em class="jd">亚当</em>、<em class="jd"> SGD带动量</em>、<em class="jd">等</em>。<em class="jd"> SGD </em>是否带有学习率调度程序。我的意见是可以也可以不可以，预定学习率可以和<em class="jd"> SGD带动量</em>有一样的效果，但是不能更新不同学习率的参数。</p><p id="0153" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于新从业者，建议使用<em class="jd">亚当</em>。但是为什么越来越多的研究人员在他们的论文中使用<em class="jd">带有动量</em>和预定学习速率的SGD呢？<em class="jd">亚当</em>怎么了？如何使用<em class="jd"> SGD配合动量</em>达到更好的性能？请检查这个故事的第二部分。</p></div><div class="ab cl jz ka gp kb" role="separator"><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke kf"/><span class="kc bw bk kd ke"/></div><div class="hb hc hd he hf"><h2 id="4de2" class="mn kh hi bd ki mo mp mq km mr ms mt kq iq mu mv ku iu mw mx ky iy my mz lc na bi translated"><strong class="ak">参考</strong>:</h2><ol class=""><li id="1240" class="jl jm hi ih b ii le im lf iq nb iu nc iy nd jc jq jr js jt bi translated">南鲁德，梯度下降优化算法概述，<a class="ae jk" href="https://ruder.io/optimizing-gradient-descent/index.html" rel="noopener ugc nofollow" target="_blank">https://ruder.io/optimizing-gradient-descent/index.html</a></li><li id="a8c3" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">G.Hinton，N.Sricastava，K. Swersky，机器学习的神经网络，<a class="ae jk" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf" rel="noopener ugc nofollow" target="_blank">http://www . cs . Toronto . edu/~ tij men/CSC 321/slides/lecture _ slides _ le C6 . pdf</a></li><li id="1b84" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">https://cs231n.github.io/neural-networks-3/#sgd<a class="ae jk" href="https://cs231n.github.io/neural-networks-3/#sgd" rel="noopener ugc nofollow" target="_blank"/></li><li id="acef" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">【https://zhuanlan.zhihu.com/p/32230623 T4】</li><li id="1276" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">萨尚克·j·雷迪，萨坦·卡莱，桑基夫·库马尔，《亚当和超越的融合》，<a class="ae jk" href="https://openreview.net/forum?id=ryQu7f-RZ" rel="noopener ugc nofollow" target="_blank">https://openreview.net/forum?id=ryQu7f-RZ</a></li><li id="6685" class="jl jm hi ih b ii ju im jv iq jw iu jx iy jy jc jq jr js jt bi translated">Nitish Shirish Keskar，Richard Socher，通过从Adam切换到SGD提高泛化性能，<a class="ae jk" href="https://arxiv.org/abs/1712.07628" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1712.07628</a></li></ol></div></div>    
</body>
</html>