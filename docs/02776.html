<html>
<head>
<title>Understanding Convolution Neural Networks -Part I</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解卷积神经网络-第一部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-convolution-neural-networks-part-i-e86c14a34be3?source=collection_archive---------16-----------------------#2020-01-01">https://medium.com/analytics-vidhya/understanding-convolution-neural-networks-part-i-e86c14a34be3?source=collection_archive---------16-----------------------#2020-01-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="eaa4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我将解释用于构建卷积神经网络的主要模块，然后从头开始构建卷积神经网络。CNN已经被证明在图像分类、分割、目标检测等方面工作得相当好。与具有较少参数的全连接神经网络相比，CNN往往表现得相当好。让我们定义一些我们将在本文中使用的术语。</p><p id="77d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">f =过滤器尺寸</p><p id="bbc5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">n_filers =过滤器数量</p><p id="4c89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">p =填充</p><p id="35b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">s =步幅</p><p id="7b02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">为什么选择CNN？</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/750e2fb9df9407eadd5958c1b9483003.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*i9Nb7PYZ1SAQJxCQJUsxBw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图1.1 10个3 x 3滤波器的卷积运算。</figcaption></figure><p id="b2a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在图1.1中，假设我们的过滤器大小为3，过滤器数量为10。输出卷的大小为30 x 30 x 10。该卷积层中的参数总数为[3 x 3 +1] x 10 = 100。现在让我们假设这是一个完全连接的神经网络，参数的数量将是[32 x 32 x 3]x[30 x 30 x 10]≃2700万。这需要训练很多参数，而且计算量很大。由于<strong class="ih hj">参数共享</strong>和s <strong class="ih hj">连接的稀疏性</strong>，卷积层的参数数量较少。参数共享意味着想象一个负责检测图像中垂直边缘的滤波器，在卷积运算期间，该滤波器将在整个图像上滑动，寻找垂直边缘。这意味着在图像的一部分有用的特征检测器，例如在这种情况下的垂直边缘检测器，可能在图像的另一部分也有用，因此权重被共享。连接的稀疏性意味着在每一层中，每个输出值只依赖于少量的输入，如图1.2所示。这使得CNN更容易过度拟合，并且可以用较小的训练集来训练。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jp"><img src="../Images/4642c9bf6c390273c76527e83487e66a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oeLOgg1QuoiA5CwFHmXmrg.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图1.2图像I与滤波器K的卷积[Ihab Sami Mohamed，2017]</figcaption></figure><p id="c83d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在图1.2中，卷积运算期间，输出图像中的值<strong class="ih hj"> 4 </strong>仅取决于输入图像中红色窗口中的9个值。这就是所谓的连接稀疏性。</p><p id="9fee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">卷积层</strong></p><p id="6319" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在卷积层，卷积滤波器遍历图像并计算矩阵(图像中像素与图像上卷积窗口的元素乘积)。卷积运算缩小图像，图像边界的像素在输出中仅使用一次，因为它们在卷积窗口中不重叠。这可能导致图像边缘的信息丢失。为了避免这个问题，使用了填充。卷积层的超级参数是:填充，步幅和过滤器大小。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es ju"><img src="../Images/cb0744caabcb4a3ad3c378c11181bf73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yPOMbCq9-w4w_brZ_MI-4w.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图1.3使用3 x 3滤波器的输入图像的卷积运算“x”。</figcaption></figure><p id="5afe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在图1.3中，大小为3 x3的输入图像部分与大小为3 x 3的滤波器进行卷积，在每个位置对像素值进行逐元素乘法运算。<strong class="ih hj">有效卷积</strong>表示无填充，<strong class="ih hj">相同卷积</strong>表示填充，使得输出大小与输入大小相同。按照惯例，在计算机视觉中卷积滤波器通常是奇数。如果填充相同，p = f-1/2，其中f是滤波器大小。<strong class="ih hj">步进卷积</strong>是指将卷积窗口在图像上移动s行和s列。</p><p id="74d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">填充</strong></p><p id="f721" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它在图像的边框上添加零。例如，如果一个(n x n)形状的图像被填充了一个数量p，它将产生如下图1.4所示的图像</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jv"><img src="../Images/bf062b3ae9e88cf7dd7e68555cd30f30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_UfqeawiaMTo-WxDosdiuA.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图1.4 3 x 3的图像用数量2填充。</figcaption></figure><p id="858f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">填充有助于恢复卷积运算过程中可能丢失的边缘信息。它允许您使用卷积层，而不必收缩体积的高度和宽度。这对于建立更深层次的网络很重要，因为否则当你进入更深层次时，高度/宽度会缩小。一个重要的特例是<strong class="ih hj">相同卷积</strong>，其中高度/宽度在一层之后被精确保留。它帮助我们在图像的边缘保留更多的信息。如果没有填充，下一层的值很少会受到图像边缘像素的影响。</p><p id="828d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">跨步</strong></p><p id="8f96" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">跨距意味着卷积滤波器在计算下一个值之前跳跃多长时间。如果图像是(n×n)，步距是2，滤波器是(f×f)，滤波器将在水平和垂直方向上以步距2的重叠对图像进行卷积。在图1.5中，过滤器将计算红色窗口像素的元素乘积，然后它将在水平方向移动2个像素，并与蓝色窗口和绿色窗口卷积。然后垂直移动2个像素，直到图像结束。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jw"><img src="../Images/3feb8b9411f0ffdf706084cd55a24056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b7zIjwcrsYN4krqU_qiHtg.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图1.5滤波器为3 x 3、步长为2的卷积运算。</figcaption></figure><p id="648c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">热卢层</strong></p><p id="4d00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它使用ReLU(整流线性激活函数),如果大于零，则直接返回输入提供的值，如果输入为零或小于零，则返回零值。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jx"><img src="../Images/1d5af6238b33d56232ad29e2649b776a.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*ckK3yXubqnjNus5w8Ieowg.png"/></div></figure><p id="8509" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中Z是输入。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jy"><img src="../Images/a4ad9fc97e13a77373d9a527f639237a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*KlPuLUtNHI6mZyAzrqdrJA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图1.6在3 x 3输入图像上重新激活。</figcaption></figure><p id="922a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ReLU是隐藏层的默认选择。它使学习更快，因为它防止梯度变为零，从而减慢训练。</p><p id="f3b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">最大池层</strong></p><p id="d160" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Maxpooling计算image (n x n)中窗口(f x f)的最大值。过滤器尺寸<em class="jz"> f </em>和步幅<em class="jz"> s </em>是该层的超参数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ka"><img src="../Images/b1b73083c72af0b2ccb57786a95b6183.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*TP7ybQkWSHrV_nd_qTB1UA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图1.7过滤器大小为2 x 2、步幅为2的最大池。</figcaption></figure><p id="0165" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">池(POOL)层减少了输入的高度和宽度。它有助于减少计算，并有助于使特征检测器对其在输入中的位置更加不变。最大池化背后的想法是，它在窗口中选取最大值，该值可能是任何特定的特性，因此它会保留在最大池输出中。假设在左上角的2 x 2窗口中检测到一个垂直边缘，则取最大值来保留该特征。如果没有检测到，那么最大值仍然很小，并且该特征没有首先被检测到。值得注意的一点是，max pool layer没有用于训练反向传播的参数。</p><p id="ccd5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">展平图层</strong></p><p id="bc1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它对输入向量执行“展平”。它将n维特征矩阵转换为一个向量，该向量可以输入到一个完全连接的神经网络分类器中。展平层没有任何要学习的参数。</p><p id="fcc2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献</strong></p><p id="6c16" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">伊哈卜·萨米·穆罕默德·穆罕默德。<em class="jz">使用激光测距仪和机器学习技术检测和跟踪托盘</em>。Diss。硕士论文，欧洲高级机器人学硕士Plus (EMARO+)，热那亚大学，2017。</p></div></div>    
</body>
</html>