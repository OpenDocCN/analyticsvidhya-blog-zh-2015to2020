<html>
<head>
<title>Keyword Extraction Techniques using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python的关键词提取技术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/keyword-extraction-techniques-using-python-edea5fc35678?source=collection_archive---------6-----------------------#2020-01-14">https://medium.com/analytics-vidhya/keyword-extraction-techniques-using-python-edea5fc35678?source=collection_archive---------6-----------------------#2020-01-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7b28ac4279ee4740fc70e41c849cc5e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3wS3ei31qVuL1AKX"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">罗曼·维涅斯在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="30e6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将深入讨论TF-IDF和LDA。</p><p id="ff84" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi jt translated"><span class="l ju jv jw bm jx jy jz ka kb di"> K </span>关键词提取的任务是自动识别最能描述文档主题的术语。关键字是表示文档中包含的最相关信息的术语。<br/>自动关键词提取的方法可以是监督式、半监督式或无监督式。在研究新闻文章时，关键字是一个重要的组成部分，因为它们提供了文章内容的简明表达。</p><p id="5e66" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">写这篇文章的想法是打破常规，在现有信息的基础上分享一些额外的信息。文章以问答的形式撰写，涵盖了所有相关的话题以及关于这个话题的常见问题。</p><p id="f7ff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们将讨论一种监督方法:TF-IDF和一种非监督方法:LDA。</p><p id="37cf" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">先决条件:</strong>对Sklearn库的基本了解将是一个额外的好处。但是仍然给出了代码示例，以便更好地理解主题。</p><p id="acbc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于我们的监督学习方法，需要理解某些概念。已经用代码示例对它们进行了深入的解释。<em class="kc">现在，要从纯文本中提取关键字，我们需要</em> <strong class="ix hj"> <em class="kc">对每个单词进行标记，并对单词进行编码，以构建一个词汇表</em> </strong> <em class="kc">，这样就可以开始提取了。因此解释了某些概念，以便您对主要方法有更好的了解。</em></p><p id="17ff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">词袋的概念<br/> </strong>根据wiki词袋是一种简化的表示，用于<a class="ae iu" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank">自然语言处理</a>和<a class="ae iu" href="https://en.wikipedia.org/wiki/Information_retrieval" rel="noopener ugc nofollow" target="_blank">信息检索</a> (IR)。在这个模型中，一个文本(比如一个句子或者一个文档)被表示为它的单词的<a class="ae iu" href="https://en.wikipedia.org/wiki/Multiset" rel="noopener ugc nofollow" target="_blank">包(multiset) </a>，不考虑语法甚至词序，但是保持<a class="ae iu" href="https://en.wikipedia.org/wiki/Multiplicity_(mathematics)" rel="noopener ugc nofollow" target="_blank">多样性</a>。示例:</p><p id="8d1c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">“那是最糟糕的时代”= [1，1，1，0，1，1，0，0] <br/>“那是智慧的时代”= [1，1，1，0，1，0，0，1，0] <br/>“那是愚蠢的时代”= [1，1，1，0，0，0，1]</p><p id="8587" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">计数矢量器的概念</strong><br/><strong class="ix hj">计数矢量器</strong>提供了一种简单的方法，既可以标记一组文本文档，构建已知单词的词汇表，还可以使用该词汇表对新文档进行编码。</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="ab80" class="km kn hi ki b fi ko kp l kq kr">text = [“The quick brown fox jumped over the lazy dog.”]<br/># create the transform<br/>vectorizer = CountVectorizer()</span><span id="3a56" class="km kn hi ki b fi ks kp l kq kr"># tokenize and build vocab<br/>vectorizer.fit(text)</span><span id="c098" class="km kn hi ki b fi ks kp l kq kr">print("output = ",vectorizer.vocabulary_)<br/>output = {‘dog’: 1, ‘fox’: 2, ‘over’: 5, ‘brown’: 0, ‘quick’: 6, ‘the’: 7, ‘lazy’: 4, ‘jumped’: 3}</span><span id="b5e7" class="km kn hi ki b fi ks kp l kq kr"># encode document<br/>vector = vectorizer.transform(text)</span><span id="2755" class="km kn hi ki b fi ks kp l kq kr">print(vector.shape)<br/>(1, 8)</span><span id="3fe1" class="km kn hi ki b fi ks kp l kq kr">print('output = ',vector.toarray())<br/>output = [[1 1 1 1 1 1 1 2]]</span></pre><p id="a485" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">问:为什么我们不能使用计数矢量器方法提取关键词？</strong>“它”这个词与“平等”有同样的重要性。但是单词“它”并没有给出关于文档的任何额外信息。</p><h1 id="8742" class="kt kn hi bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated"><strong class="ak"> 1。TF-IDF </strong></h1><p id="9966" class="pw-post-body-paragraph iv iw hi ix b iy lq ja jb jc lr je jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">TF-IDF代表项频率和逆项频率。这种方法有助于理解文档中某个单词的重要性。<br/>维基对TF-IDF的定义是，它是一个数字统计，旨在反映一个单词对集合中的<a class="ae iu" href="https://en.wikipedia.org/wiki/Document" rel="noopener ugc nofollow" target="_blank">文档</a>或<a class="ae iu" href="https://en.wikipedia.org/wiki/Text_corpus" rel="noopener ugc nofollow" target="_blank">语料库</a>的重要性。在信息检索的搜索、<a class="ae iu" href="https://en.wikipedia.org/wiki/Text_mining" rel="noopener ugc nofollow" target="_blank">文本挖掘</a>和<a class="ae iu" href="https://en.wikipedia.org/wiki/User_modeling" rel="noopener ugc nofollow" target="_blank">用户建模</a>中，它经常被用作<a class="ae iu" href="https://en.wikipedia.org/wiki/Weighting_factor" rel="noopener ugc nofollow" target="_blank">权重因子</a>。</p><p id="e6e1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在让我们把它分解一下:</p><p id="26b1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 1.1。</strong> <strong class="ix hj">词频(TF) <br/>词频</strong>是给定的<strong class="ix hj">词</strong>或查询在文档中出现的次数。其计算方法如下:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es lv"><img src="../Images/d9869729cf41719a4d32549ab7edea56.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*k_NiA-jSRe1rjdUB7s_H_w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">检索词频率</figcaption></figure><p id="7275" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里n代表该术语出现的次数，除以文档中的总术语数。</p><p id="f023" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 1.2。逆文档频率(IDF) <br/> </strong>它被计算为语料库中<strong class="ix hj">文档</strong>的数量的对数除以特定术语出现的<strong class="ix hj">文档</strong>的数量。其计算方法如下:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/8feafbe54e82035f2a5ff8dc56e67700.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*GAKFp0TM4d2O9js_sFDC1g.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">综合资料的文件（intergrated Data File）</figcaption></figure><p id="7447" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里n是文档总数，除以包含该术语的文档数。</p><p id="1f50" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> 1.3 .词频-逆文档频率(TF-IDF) <br/> </strong>就是上面给定的两个词的相乘。它的代表是:</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/a7b7de89ec841902886ce238afb159cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*ioNfD5xtiJtgLwOGbhrCFg.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">TF-IDF</figcaption></figure><p id="4669" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是TF-IDF是如何帮助关键词提取的呢？为什么TF或IDF本身的效率不足以提取关键词？ <br/>答:通过一个例子可以更好地理解TF-IDF在关键词提取中的工作:<br/>让我们假设有10个关于足球的文档。这里面会有很多常见的词，比如“a”、“the”等等，还会有“Pele”，可能会有一些不太常用的词，比如“supercalifragilisticecialidocious”，这是一个“神奇的或难以置信的”术语。</p><p id="a476" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，如果我们通过关键词提取的术语频率，显然术语a的计数会高于Pele或稀有词。例如:</p><blockquote class="ly lz ma"><p id="2333" class="iv iw kc ix b iy iz ja jb jc jd je jf mb jh ji jj mc jl jm jn md jp jq jr js hb bi translated">术语频率:<br/>a-100<br/>Pele-12<br/>supercalifilisticiexpalidocious-1<br/><strong class="ix hj">因此术语频率本身对提取关键字没有帮助，因为通过这种方法，单词“a”获得了最大的重要性</strong></p></blockquote><p id="a984" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一个有趣的想法可能是，如果我们把这些文件颠倒过来，那么单词“Pele”的重要性就会增加，所以让我们试试IDF:</p><blockquote class="ly lz ma"><p id="4f75" class="iv iw kc ix b iy iz ja jb jc jd je jf mb jh ji jj mc jl jm jn md jp jq jr js hb bi translated">逆项频率:<br/>a-log(10/10)= 0<br/>Pele-log(10/4)= 0.397<br/>supercalifilisticelidiocious-log(10/1)= 1<br/><strong class="ix hj">因此IDF本身对提取关键字没有帮助，因为它会重视很少使用的词。因此，TF和IDF本身的效率并不高。</strong></p></blockquote><p id="b0d6" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但现在让我们看看他们如何作为TF-IDF共同工作:</p><blockquote class="ly lz ma"><p id="b6e8" class="iv iw kc ix b iy iz ja jb jc jd je jf mb jh ji jj mc jl jm jn md jp jq jr js hb bi translated">TF-IDF = TF * IDF<br/>TF-IDF:<br/>a-100 * 0 = 0<br/>Pele-12 * 0.397 = 4.775<br/>supercalifilisticepalidocious-1 * 1 = 1<br/><strong class="ix hj">在这里我们可以看到，通过TF-TDF，像“贝利”这样的词变得更加重要，因为文件都在谈论足球。因此，TF-IDF在关键词提取方面是有效的。</strong></p></blockquote><p id="66b2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了更好地理解使用TF-IDF的关键字提取，请查看此<a class="ae iu" href="https://github.com/AjayJohnAlex/Keyword-Extraction/blob/master/model.py" rel="noopener ugc nofollow" target="_blank">代码</a>。要查看其实施情况，请访问此处的。</p><h1 id="7045" class="kt kn hi bd ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp bi translated">2.潜在狄利克雷分配</h1><p id="3eca" class="pw-post-body-paragraph iv iw hi ix b iy lq ja jb jc lr je jf jg ls ji jj jk lt jm jn jo lu jq jr js hb bi translated">LDA代表潜在狄利克雷分配，用于主题建模。现在，主题建模是使用无监督学习来提取出现在文档集合中的主要主题(表示为一组单词)的任务。因此，使用LDA提取的主要主题将是我们的关键词。</p><p id="bceb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">问:什么是LDA？<br/>  A .根据定义:<br/> <strong class="ix hj"> <em class="kc">潜在狄利克雷分配(Latent Dirichlet allocation)是一种生成统计模型，它允许用未观察到的组来解释多组观察值，从而解释为什么数据的某些部分是相似的</em> </strong> <em class="kc">。</em> <br/>用更简单的话来说:<br/> <strong class="ix hj"> <em class="kc"> LDA想象一组固定的话题。每个主题代表一组单词。而LDA的目标是将所有文档以某种方式映射到主题，使得每个文档中的单词大部分被那些虚构的主题</em> </strong>捕获。<br/>现在LDA的工作有两个假设:<br/> 1。主题是由经常一起使用的单词集合构成的。<br/> 2。这些文档是由多个主题混合而成的。然后，这些主题根据它们的概率分布生成单词。由于LDA是一种无监督学习方法，所以主题由我们来解释。</p><h2 id="09e3" class="km kn hi bd ku me mf mg ky mh mi mj lc jg mk ml lg jk mm mn lk jo mo mp lo mq bi translated"><strong class="ak"> LDA假设→每个文档→混合主题→每个主题→混合单词</strong></h2><figure class="kd ke kf kg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/54ddffa676ea1d592a1e4e123a7e3b53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v4ES-1KdsMKw23tABSHnfA.png"/></div></div></figure><p id="481d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="kc">如果你仍然困惑，请在评论区添加它，如果你正在寻找视频解释，请观看</em> <a class="ae iu" href="https://youtu.be/3mHy4OSyRf0" rel="noopener ugc nofollow" target="_blank"> <em class="kc">这个</em> </a> <em class="kc">。但是如果你明白…让我们继续。</em></p><p id="92b5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">问:LDA是如何工作的？</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/053c1ca94c42ba758ab3110e0d7c8389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PxLA7EqueQMfDz3VS5Zsqw.png"/></div></div></figure><p id="6e7a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">A.现在让我们通过一个例子来理解这一点。现在，如果我们的输入是1000个文档的dataframe列，例如:df['Description']，那么使用Count Vectoriser或TF-IDF vector ser，我们会得到一个文档术语矩阵。</p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="d1f3" class="km kn hi ki b fi ko kp l kq kr">cv = CountVectorizer(max_df=0.9,min_df=2, stop_words='english')<br/>dtm  = cv.fit_transform(df['Description'])</span></pre><p id="d31d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="kc">什么是文档术语矩阵(dtm)？<br/></em></strong>DTM是一个矩阵，描述了术语在文档集合中出现的频率。</p><figure class="kd ke kf kg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/de7426ad7af06148d5b4120a94fc2b3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w9FXydQEYfrp5Wntq6fDrg.png"/></div></div></figure><p id="dae2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一旦创建了dtm，我们就得到一个(1000，no_of_unique_words)的形状。然后，我们可以使用scikit learn的潜在Dirichlet分配库来指定我们认为文档可能具有的组件(主题)的数量。使用LDA的拟合方法，我们得到(无主题，无唯一单词)的形状。</p><p id="b533" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过使用循环中的<strong class="ix hj">，我们提取了每个主题中的热门词汇。这些热门词汇是每个主题的关键词。这些关键词给了我们关于这些文件的信息。因此，如果我们有一份关于动作电影评论的文件，最热门的词将是</strong></p><pre class="kd ke kf kg fd kh ki kj kk aw kl bi"><span id="7c3e" class="km kn hi ki b fi ko kp l kq kr"><strong class="ki hj">from</strong> <strong class="ki hj">sklearn.decomposition</strong> <strong class="ki hj">import</strong> LatentDirichletAllocation<br/>lda = LatentDirichletAllocation(n_components=5,random_state=101)<br/><br/>lda_fit  = lda.fit(dtm)<br/><em class="kc"><br/># understanding each topics top 10 common words </em> <br/><strong class="ki hj">for</strong> id_value, value <strong class="ki hj">in</strong> enumerate(lda_fit.components_):<br/>   print(f"The topic would be <strong class="ki hj">{id_value}</strong>") <br/>  print([cv.get_feature_names()[index] <strong class="ki hj">for</strong> index <strong class="ki hj">in</strong> value.argsort()   [-10:]])<br/>   print("<strong class="ki hj">\n</strong>")</span></pre><p id="c147" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">执行这段代码后，我们得到5个列表，每个列表的长度为[no_of_unique_words],从主题中最重要的单词到最不重要的单词排序。因此，我们可以从所有主题中获取主题词来创建我们的关键词。有关LDA的更深入的文章，请参考<a class="ae iu" href="https://towardsdatascience.com/the-complete-guide-for-topics-extraction-in-python-a6aaa6cedbbc" rel="noopener" target="_blank">和</a>。</p><p id="291e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">LDA也用于分类。一个例子可以在这个<a class="ae iu" href="https://github.com/AjayJohnAlex/NaturalLanguageProcessing/blob/master/LDA_with_nlp.ipynb" rel="noopener ugc nofollow" target="_blank">代码</a>中看到。</p><p id="4605" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果对这篇文章有任何疑问，请在评论区添加。我很乐意尽快回答他们。我也会相应地对文章进行充分的修改。</p></div></div>    
</body>
</html>