# åŸºäºç¥ç»ç½‘ç»œçš„é»„é‡‘ä»·æ ¼é¢„æµ‹

> åŸæ–‡ï¼š<https://medium.com/analytics-vidhya/gold-price-forecast-using-neural-networks-37ac4e6600c1?source=collection_archive---------10----------------------->

> æ­¥éª¤ 1:-å…³äºæ•°æ®é›†å’ŒåŸºæœ¬æ“ä½œçš„ä¿¡æ¯
> 
> æ­¥éª¤ 2:-æ•°æ®é›†çš„å¯è§†åŒ–ã€‚
> 
> æ­¥éª¤ 3:-åˆ¶ä½œç”¨äºé¢„æµ‹çš„çª—å£æ•°æ®é›†
> 
> æ­¥éª¤ 4:-å»ºç«‹ç¥ç»ç½‘ç»œå¹¶åœ¨çª—å£æ•°æ®é›†ä¸Šè®­ç»ƒå’ŒéªŒè¯

ipynb ç¬”è®°æœ¬æ–‡ä»¶å’Œæ•°æ®é›†çš„é“¾æ¥å¦‚ä¸‹

[](https://github.com/akshayardeshana/Gold-forecasting) [## akshayardeshana/é»„é‡‘é¢„æµ‹

### é€šè¿‡åœ¨ GitHub ä¸Šåˆ›å»ºå¸æˆ·ï¼Œä¸º akshayardeshana/é»„é‡‘é¢„æµ‹å‘å±•åšå‡ºè´¡çŒ®ã€‚

github.com](https://github.com/akshayardeshana/Gold-forecasting) 

> æ­¥éª¤ 1:-å…³äºæ•°æ®é›†å’ŒåŸºæœ¬æ“ä½œçš„ä¿¡æ¯

è¯¥æ•°æ®é›†åŒ…å«æœ‰å…³é»„é‡‘å’Œç™½é“¶ä»·æ ¼çš„ä¿¡æ¯ã€‚

```
import pandas as pdgold_data=pd.read_csv('/content/drive/MyDrive/dataset/Goldsorted_final.csv')gold_data
```

![](img/61476ae5d405d614a0200973c1aac839.png)

é»„é‡‘æ•°æ®é›†

åœ¨è¿™ä¸ªæ•°æ®é›†ä¸­ï¼Œä»·æ ¼æ˜¯ä» 2019 å¹´ 8 æœˆ 22 æ—¥åˆ° 2020 å¹´ 9 æœˆ 28 æ—¥ã€‚

> æ­¥éª¤ 2:-æ•°æ®é›†çš„å¯è§†åŒ–

**æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ TensorFlow 2.3.0 ç‰ˆæœ¬ã€‚**

```
import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltprint(tf.__version__)
```

ä¸ºè¿™ä¸ªæƒ…èŠ‚å†™ä¸€ä¸ªå‡½æ•°ã€‚

```
def plot_series(time, series, format="-", start=0, end=None): plt.plot(time[start:end], series[start:end], format) plt.xlabel("index") plt.ylabel("Gold 999(AM Price)") plt.grid(True)
```

ä¸ºå¯è§†åŒ–ç»˜åˆ¶å›¾è¡¨ã€‚

```
import csvtime_step = []goldinfo = []with open('/content/drive/My Drive/dataset/Goldsorted_final.csv') as csvfile:reader = csv.reader(csvfile, delimiter=',') next(reader) for row in reader: goldinfo.append(float(row[2])) time_step.append(int(row[0]))series = np.array(goldinfo)time = np.array(time_step)plt.figure(figsize=(10, 6))plot_series(time, series)
```

![](img/4797d5e1564eeab32d45b73f0074e258.png)

ä¸Šé¢æˆ‘ä»¬ç”¨ä¸€ä¸ªæŒ‡æ•°ä½œä¸ºæ—¶é—´ï¼Œç”¨é»„é‡‘ 999(AM ä»·æ ¼)åˆ—ä½œä¸ºå€¼ã€‚

> æ­¥éª¤ 3:-åˆ¶ä½œç”¨äºé¢„æµ‹çš„çª—å£æ•°æ®é›†

æˆ‘ä»¬ä½¿ç”¨çª—å£æ•°æ®é›†çš„æ¦‚å¿µæ¥åˆ¶ä½œè¿™ä¸ªæ•°æ®é›†ã€‚

æœ‰å…³çª—å£æ•°æ®é›†çš„æ›´å¤šä¿¡æ¯ï¼Œæ‚¨å¯ä»¥å‚è€ƒä¸‹é¢çš„é“¾æ¥â€¦

 [## TF . data . dataset | tensor flow Core v 2 . 3 . 0

### è¡¨ç¤ºä¸€ä¸ªæ½œåœ¨çš„å¤§å‹å…ƒç´ é›†ã€‚

www.tensorflow.org](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window) 

åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨çª—å£å¤§å°ä¸º 10ï¼Œè¿™æ„å‘³ç€ 1 åˆ° 10 çš„ç´¢å¼•ç”¨ä½œè¾“å…¥å€¼ Xï¼Œ11 çš„å€¼ç”¨ä½œæ ‡ç­¾ yã€‚ä½¿ç”¨ TensorFlowï¼Œæˆ‘ä»¬å¯ä»¥å¾ˆå®¹æ˜“åœ°åšåˆ°è¿™ä¸€ç‚¹ã€‚

```
split_time = 200time_train = time[:split_time]x_train = series[:split_time]time_valid = time[split_time:]x_valid = series[split_time:]window_size = 10batch_size = 4shuffle_buffer_size = 77def windowed_dataset(series, window_size, batch_size, shuffle_buffer): dataset = tf.data.Dataset.from_tensor_slices(series) dataset = dataset.window(window_size + 1, shift=1,      drop_remainder=True) dataset = dataset.flat_map(lambda window: window.batch(window_size + 1)) dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1])) dataset = dataset.batch(batch_size).prefetch(1) return dataset
```

ä¸Šé¢ä»£ç ç‰‡æ®µä¸­çš„ windowed_dataset å‡½æ•°ç”¨äºåˆ›å»ºä¸€ä¸ªçª—å£æ•°æ®é›†ã€‚

è®©æˆ‘ä»¬è°ƒç”¨ windowed_dataset å‡½æ•°â€¦

```
dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)
```

> æ­¥éª¤ 4:-å»ºç«‹ç¥ç»ç½‘ç»œå¹¶åœ¨çª—å£æ•°æ®é›†ä¸Šè®­ç»ƒå’ŒéªŒè¯

æˆ‘ä»¬æ­£åœ¨ä½¿ç”¨ ***å¯†é›†å±‚*** å’Œ ***æ•´æµçº¿æ€§å•å…ƒ*** æ¿€æ´»åŠŸèƒ½ã€‚

è¿™ä¸ªé¡¹ç›®ä¸­ä½¿ç”¨çš„ä¼˜åŒ–å™¨æ˜¯ ***Adam*** ã€‚Adam æ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼Œå¯ç”¨äºæ ¹æ®è®­ç»ƒæ•°æ®è¿­ä»£æ›´æ–°ç½‘ç»œæƒé‡ã€‚

æœ¬é¡¹ç›®ä½¿ç”¨çš„æŸå¤±å‡½æ•°ä¸º[***MSE***](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-error)ã€‚

```
model = tf.keras.models.Sequential([tf.keras.layers.Dense(64, input_shape=[window_size], activation="relu"),tf.keras.layers.Dense(32, input_shape=[window_size], activation="relu"),tf.keras.layers.Dense(16, input_shape=[window_size], activation="relu"),tf.keras.layers.Dense(8, activation="relu"),tf.keras.layers.Dense(1)])model.compile(loss="mse", optimizer=tf.keras.optimizers.Adam())model1=model.fit(dataset,epochs=1000)
```

```
forecast=[]for time in range(len(series) - window_size): forecast.append(model.predict(series[time:time + window_size][np.newaxis]))forecast = forecast[split_time-window_size:]results = np.array(forecast)[:, 0, 0]plt.figure(figsize=(10, 6))plot_series(time_valid, x_valid)plot_series(time_valid, results)
```

![](img/d0781b2d7ba755148af4561676ab6d32.png)

```
tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()
```

![](img/ebca026cb2f04d33c4f695e6442f9eda.png)

å¤ªæ£’äº†ã€‚ğŸ˜œ