<html>
<head>
<title>Data Science and Machine learning- A Foreword</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学和机器学习-前言</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/data-science-and-machine-learning-a-foreword-1a5bdc583ed7?source=collection_archive---------24-----------------------#2020-06-11">https://medium.com/analytics-vidhya/data-science-and-machine-learning-a-foreword-1a5bdc583ed7?source=collection_archive---------24-----------------------#2020-06-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/76eefadb720ee3fda0876c6e7aa2e128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TZCWfdsfcBIxMGtFphzzVg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">前言方法</figcaption></figure><p id="fe06" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">谷歌的无人驾驶汽车和机器人受到了很多媒体的关注，但该公司真正的未来在于机器学习，这项技术可以让计算机变得更加智能和个性化。</p><p id="ac67" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><em class="js">–埃里克·施米特(谷歌董事长)</em></p><p id="f178" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="js">数据科学</em> </strong>是利用数学、统计学和计算机科学从数据中产生洞察的领域。</p><p id="02dc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">与科学的许多其他领域一样，数据科学通过假设、证明、否定假设、切片、组合数据和生成观察结果来理解给定的问题。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/4ca21e856dfcf3df17c4d5de799cee29.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*9IlGFF0DJ51A6biIY5DOQA.png"/></div></figure><p id="c549" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">机器学习是人工智能的子集。它是通过使用各种算法来执行的，这些算法执行某些任务，而无需显式编程。</p><p id="1c6d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">深度学习是机器学习的一个子集，它更年轻，更先进，更复杂，计算成本更高。</p><p id="b429" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">基于要解决的问题和解决的方法，机器学习有两种主要类型。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jy"><img src="../Images/40f33461c4b713d4c03a3aefb113833b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8fdw2uuvzdLm1_CyDjY-hA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">有人监督与无人监督</figcaption></figure><ol class=""><li id="1323" class="jz ka hi iw b ix iy jb jc jf kb jj kc jn kd jr ke kf kg kh bi translated">监督学习:这需要标签数据来执行任务，给定的任务是在标签数据的监督下执行的。</li><li id="ad77" class="jz ka hi iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">无监督学习:这不需要带标签的数据，给定的任务是自己计算出数据并生成趋势。</li></ol><p id="a3f4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">基于数据类型和任务，监督学习有2个子类型。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kn"><img src="../Images/566e899ea8923a373e362dc6b4bb0ea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*TUy3lCMdOxBgQJ-HwYtYjA.png"/></div></figure><p id="0008" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">回归:一种发现自变量和因变量之间关系的技术。它要求因变量连续。</p><p id="7e25" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">b)分类:将对象分为不同类别的技术。它要求因变量是离散的。</p><p id="a53e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">无监督学习有两个主要的子类型</p><p id="0631" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">a)聚类:这是一种将相似的对象分组在一起的技术</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ko"><img src="../Images/103e3d76a31cf045b6b003bfbc036f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RKgaqmRXqnR5Y-56Xizmug.png"/></div></div></figure><p id="9705" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">b)降维:这是一种用于减少数据中的维度以降低复杂性而不丢失信息的技术。</p><p id="eaa7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">算法是机器和深度学习中使用的构建模块。</p><p id="6042" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">有不少算法。这里讨论几个吧。</p><ol class=""><li id="a80a" class="jz ka hi iw b ix iy jb jc jf kb jj kc jn kd jr ke kf kg kh bi translated">线性回归</li><li id="c650" class="jz ka hi iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">逻辑回归</li><li id="6273" class="jz ka hi iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">支持向量机</li><li id="85cf" class="jz ka hi iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">k-最近邻</li><li id="70b4" class="jz ka hi iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">k均值聚类</li><li id="8cf6" class="jz ka hi iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">决策图表</li><li id="be91" class="jz ka hi iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">随机森林</li><li id="08ec" class="jz ka hi iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">装袋和增压</li><li id="e413" class="jz ka hi iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">主成分分析</li><li id="3e5a" class="jz ka hi iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">神经网络</li></ol><p id="6230" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们来谈谈算法</p><p id="06ff" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">线性回归:</strong>可以说是最古老、最著名的统计技术。这用于找出一个因变量和一个或多个自变量之间的线性关系。</p><p id="57d5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">线性回归的目标是找到大致通过实际点的“最佳拟合线”，同时减少实际点和预测点之间的距离。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kp"><img src="../Images/5b0cc172620100f963eabe6c22216ed2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*md7XN9nXzyjcOXmL7LRzpw.png"/></div></div></figure><p id="cc38" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这条线的方程式是Y = B0 + B1X</p><p id="22ee" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里Y是因变量，X是自变量，B0，B1是我们需要找到的值。</p><p id="36a4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">梯度下降是一种更新B0、B1值的方法，以找到“最佳拟合线”,同时降低成本函数或损失函数，或通常称为误差。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kq"><img src="../Images/a402afb31299751e729a82f285362f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*TlI_MgITNGXdY0PnfwmG5w.jpeg"/></div></figure><p id="723e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在线性回归中，误差称为均方误差。MSE计算为所有预测点和实际点之间的差值的平均值。</p><p id="fa95" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">评估线性回归模型有多种方法</p><p id="d987" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">a)均方误差</p><p id="febf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">b)均方根误差</p><p id="6844" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">c)绝对均方误差</p><p id="ea0d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">d) R2 ( R平方)</p><p id="ea6f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">e)调整后的R2</p><p id="0e4b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">逻辑回归:</strong>最通用、最常用的分类技术。逻辑回归是一种监督分类算法。</p><p id="d99d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">该算法的目标是将因变量分成不同的类。这个概念叫做<em class="js">最大似然估计。</em></p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kr"><img src="../Images/59793fc7f884de91ef0a3fea8b45a7c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*-SNsvXZN9q_qFcbkuJXutw.png"/></div></figure><p id="8b0c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">由于因变量本质上是离散的，应用回归没有意义。因此，它使用称为logit函数的转换函数将预测输出转换为离散类。</p><p id="d884" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">划分类别的直线方程形成一条“S”形线。</p><p id="d93a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里，成本函数称为对数损失函数，用于计算误分类的成本。目标是降低成本函数，梯度下降用于更新“S”形线的值。</p><p id="c1df" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">逻辑回归几乎没有评估指标</p><p id="0eff" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">a)混淆矩阵</p><p id="60c0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">b) F1分数</p><p id="43bf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">AUC-ROC</p><p id="8ad2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">支持向量机:</strong>这是一种监督分类算法，将变量的类别分开。</p><p id="3e8c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">该算法的目标是找到分隔类别的超平面，同时增加超平面和决策边界之间的距离裕度，该决策边界是在每个类别中最近的向量的帮助下形成的，这些向量被称为支持向量。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/6459841eb38a1a2b065b4b8896fee00b.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*eGL8X-7TGViiD_33tPPPHQ.png"/></div></figure><p id="9078" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">支持向量确定超平面的位置和方向。</p><p id="6c9d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这里，我们希望最大化超平面和点之间的距离裕度，以便将来的新点舒适地落入特定的类别。</p><p id="c9a5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里的代价函数叫做<em class="js">铰链损耗。当实际类别和预测类别不同时，计算</em>成本函数。错误分类越低，成本函数越低。使用梯度，我们更新平面方程中的权重。</p><p id="7b1f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于非线性可分数据，寻找超平面变得复杂且不直观。SVM有一个额外的参数叫做<em class="js">内核</em>，可以将高维转换成低维。</p><p id="ef21" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">有几个像多项式，径向，径向基函数等核函数…</p><p id="42f7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">决策树</strong>:决策树是一种监督分类和回归算法，也称为CART算法，意思是可以用于分类和回归问题。它构成了所有基于树的高级模型的基础。因此，理解决策树对于任何DS/ML爱好者来说都是至关重要的。所以，让我们试着去理解。</p><p id="d2b9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">顾名思义，它遵循一种树形的方法，为给定的问题做出决策。模型的目标是获得沿途形成的纯节点和决策。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kt"><img src="../Images/f5547c71608d041bd3d6eb081e16ae7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mC4l0X7jgjJx0i6wHUksqg.png"/></div></div></figure><p id="4dc7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">基于目标变量数据类型的决策树有两种类型</p><ol class=""><li id="dfd0" class="jz ka hi iw b ix iy jb jc jf kb jj kc jn kd jr ke kf kg kh bi translated">分类变量决策树</li><li id="80ac" class="jz ka hi iw b ix ki jb kj jf kk jj kl jn km jr ke kf kg kh bi translated">连续变量决策树</li></ol><p id="81af" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在我们开始之前，让我们先了解一些术语</p><p id="8893" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">a)根节点:数据特征进入树的位置</p><p id="9d76" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">b)决策节点:在此做出拆分节点的决策</p><p id="bda6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">c)叶节点:在此停止分裂成其他子节点。</p><p id="8659" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">d)修剪:当我们删除树的子节点时。</p><p id="c6d0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">现在，让我们看看决策树是如何工作的。</p><p id="0a28" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">模型的准确性取决于变量的选择和分裂决策。决策树必须首先决定变量，并决定内部节点的变量及其分裂，以在最后给我们最好的纯节点。通过使用决策树模型中的以下算法之一进行选择。</p><p id="ad92" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">他们是</p><ol class=""><li id="1792" class="jz ka hi iw b ix iy jb jc jf kb jj kc jn kd jr ke kf kg kh bi translated">ID3-构建树时采用自顶向下的方法。</li></ol><p id="a4fa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">2.c 4.5-ID3的高级版本</p><p id="8786" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">3.分类和回归树</p><p id="b441" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">4.CHAID卡方自动交互检测在计算分类树时执行多级分裂</p><p id="4c18" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">5.MARS-多元自适应回归样条</p><p id="7a4e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">还有一些标准(指标)是变量选择和分割过程的一部分。</p><p id="6755" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">a)熵:它是节点中数据随机性的度量。熵越高，模型性能越低。因此，我们需要具有较低熵的变量来进一步构建树</p><p id="5b41" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">b)信息增益:信息增益与决策时的熵有关。这给出了节点关于变量的多少信息。因此，我们需要从节点获得更高的信息增益值。</p><p id="a222" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">ID3算法同时使用熵和信息增益来建模。</p><p id="bf64" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">基尼指数</p><p id="2f7e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">d)基尼系数</p><p id="f77f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">e)方差的减少</p><p id="e8e9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">f)卡方检验</p><p id="48dd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">未完待续…</p></div></div>    
</body>
</html>