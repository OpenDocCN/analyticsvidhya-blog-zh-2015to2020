<html>
<head>
<title>K-means Clustering in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的K-means聚类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/k-means-clustering-in-python-80b7ac147eee?source=collection_archive---------3-----------------------#2020-07-02">https://medium.com/analytics-vidhya/k-means-clustering-in-python-80b7ac147eee?source=collection_archive---------3-----------------------#2020-07-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="e816" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">拾取数据科学</h2><div class=""/><div class=""><h2 id="83a0" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">一个简单的，无监督的ML例子</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/a825cf912cb045ee4949792e42bd6c09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oodd7p5BShqt40tJ"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">亨利·洛伦扎托在<a class="ae jw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="e700" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">如果一个人太深入，对机器学习(ML)概念的描述可能会迅速升级。让我们暂时远离杂草。相反，我们将漫步于基本机器学习(ML)算法领域，并回顾一些与无监督ML相关的问题。</p><h2 id="4c6d" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">我们将回顾:</h2><ul class=""><li id="df19" class="ln lo hi jz b ka lp kd lq kg lr kk ls ko lt ks lu lv lw lx bi translated">什么是K-means聚类，它的优势是什么？</li><li id="05d6" class="ln lo hi jz b ka ly kd lz kg ma kk mb ko mc ks lu lv lw lx bi translated">描述K均值聚类算法</li><li id="53fc" class="ln lo hi jz b ka ly kd lz kg ma kk mb ko mc ks lu lv lw lx bi translated">如何用Python实现K-means聚类</li></ul><h1 id="06e3" class="md ku hi bd kv me mf mg kz mh mi mj ld ix mk iy lg ja ml jb lj jd mm je lm mn bi translated">无监督学习</h1><p id="6be4" class="pw-post-body-paragraph jx jy hi jz b ka lp is kc kd lq iv kf kg mo ki kj kk mp km kn ko mq kq kr ks hb bi translated">聚类是无监督学习的一个例子。无监督学习是一种机器学习，它在没有提供标签和最少人工监督的情况下处理数据中以前未检测到的模式。</p><p id="39c1" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">与通常使用标记数据的监督学习技术相比，无监督学习允许对输入的概率密度建模(我们稍后将简化这个定义)。K-means是这些技术中最基本的一种。</p><h2 id="bd1b" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">什么是集群？</h2><p id="e71b" class="pw-post-body-paragraph jx jy hi jz b ka lp is kc kd lq iv kf kg mo ki kj kk mp km kn ko mq kq kr ks hb bi translated">在ML中，聚类是将数据点分成特定组的一个例子，这种方式是将相似的数据点分组在一起。任何一个组中的数据点与同一组中的数据点更相似，而与其他组中的数据点不相似。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es mr"><img src="../Images/b55e4c33c76ea564fa9a24855720aeed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*XHaBSYNxwAs9eMcEUCX9Pg.jpeg"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><a class="ae jw" href="https://commons.wikimedia.org/wiki/File:Sockenzoo.jpg" rel="noopener ugc nofollow" target="_blank">埃尔克·韦奇格</a> / <a class="ae jw" href="http://Elke Wetzig (Elya) / CC BY-SA (https://creativecommons.org/licenses/by-sa/3.0)" rel="noopener ugc nofollow" target="_blank"> CC BY-SA </a></figcaption></figure></div><div class="ab cl ms mt gp mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="hb hc hd he hf"><blockquote class="mz"><p id="1294" class="na nb hi bd nc nd ne nf ng nh ni ks dx translated">聚类基本上是基于数据之间的相似性和不相似性来收集数据。例如，花卉种类的分组可以是聚类的一个例子。</p></blockquote></div><div class="ab cl ms mt gp mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="hb hc hd he hf"><h1 id="c9fd" class="md ku hi bd kv me nj mg kz mh nk mj ld ix nl iy lg ja nm jb lj jd nn je lm mn bi translated">k均值聚类</h1><p id="795b" class="pw-post-body-paragraph jx jy hi jz b ka lp is kc kd lq iv kf kg mo ki kj kk mp km kn ko mq kq kr ks hb bi translated">K-means聚类算法的目标是简单地将数据分组，使得从每个点到聚类平均点的距离平方和最小。</p><p id="64cc" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">K-means被认为是一种线性算法。我们使用欧几里得距离，即欧几里得空间中两点之间的“普通”直线距离，作为计算数据之间相似性的度量。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es no"><img src="../Images/9edee391165b7383bc7dc9f2ad207b17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iIDmj4Zjb5HzMy7x7G0S0g.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">相似？字母“u”的古希腊变体，截屏来自<a class="ae jw" href="https://en.wiktionary.org/wiki/Appendix:Variations_of_%22u%22" rel="noopener ugc nofollow" target="_blank">wikitionary</a>/<a class="ae jw" href="https://creativecommons.org/licenses/by-sa/3.0" rel="noopener ugc nofollow" target="_blank">CC BY-SA 3.0</a></figcaption></figure><h1 id="f975" class="md ku hi bd kv me mf mg kz mh mi mj ld ix mk iy lg ja ml jb lj jd mm je lm mn bi translated">K-均值聚类算法</h1><p id="b566" class="pw-post-body-paragraph jx jy hi jz b ka lp is kc kd lq iv kf kg mo ki kj kk mp km kn ko mq kq kr ks hb bi translated">K均值算法在迭代过程中工作:</p><ul class=""><li id="fc7c" class="ln lo hi jz b ka kb kd ke kg np kk nq ko nr ks lu lv lw lx bi translated">选择k的某个值，例如要创建的聚类数。</li><li id="3694" class="ln lo hi jz b ka ly kd lz kg ma kk mb ko mc ks lu lv lw lx bi translated">初始化数据中的K个“质心”或起始点。</li><li id="c5b0" class="ln lo hi jz b ka ly kd lz kg ma kk mb ko mc ks lu lv lw lx bi translated">创建聚类，将每个点指定给最近的聚类质心。</li><li id="c368" class="ln lo hi jz b ka ly kd lz kg ma kk mb ko mc ks lu lv lw lx bi translated">尝试改进集群。将每个质心移动到其簇的中心。</li><li id="7b48" class="ln lo hi jz b ka ly kd lz kg ma kk mb ko mc ks lu lv lw lx bi translated">重复上述两个步骤，直到你的质心收敛。</li><li id="86be" class="ln lo hi jz b ka ly kd lz kg ma kk mb ko mc ks lu lv lw lx bi translated">结束算法。</li></ul><h2 id="a66c" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">K-means的优缺点</h2><p id="d0cf" class="pw-post-body-paragraph jx jy hi jz b ka lp is kc kd lq iv kf kg mo ki kj kk mp km kn ko mq kq kr ks hb bi translated">优势</p><ul class=""><li id="f0c2" class="ln lo hi jz b ka kb kd ke kg np kk nq ko nr ks lu lv lw lx bi translated">实现起来非常简单——构建模型几乎比绘制结果更快</li><li id="4f0f" class="ln lo hi jz b ka ly kd lz kg ma kk mb ko mc ks lu lv lw lx bi translated">非常快</li><li id="178b" class="ln lo hi jz b ka ly kd lz kg ma kk mb ko mc ks lu lv lw lx bi translated">算法很容易适应新的例子</li><li id="f8e2" class="ln lo hi jz b ka ly kd lz kg ma kk mb ko mc ks lu lv lw lx bi translated">收敛是有保证的(不要与完全准确相混淆)</li><li id="c468" class="ln lo hi jz b ka ly kd lz kg ma kk mb ko mc ks lu lv lw lx bi translated">扩展到大型数据集</li></ul><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ns"><img src="../Images/6cc9e1e9c9c3d1494baf6bae09e56054.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gYV8pT-okTmFPQB5-CYCzQ.jpeg"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">六个不同长度的辣椒，三种颜色| <a class="ae jw" href="https://www.pikrepo.com/fccvu/six-bell-peppers" rel="noopener ugc nofollow" target="_blank"> pikrepo </a></figcaption></figure><p id="e695" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">不足之处</p><ul class=""><li id="34d2" class="ln lo hi jz b ka kb kd ke kg np kk nq ko nr ks lu lv lw lx bi translated">手动决定k的值是困难的(辣椒代表3种颜色，两种或三种大小，曲线和直线…？)</li><li id="38bb" class="ln lo hi jz b ka ly kd lz kg ma kk mb ko mc ks lu lv lw lx bi translated">异常值是有问题的，我们很快就会看到——在我们的数据图的左下角</li><li id="d27c" class="ln lo hi jz b ka ly kd lz kg ma kk mb ko mc ks lu lv lw lx bi translated">对初始值的依赖</li><li id="b41a" class="ln lo hi jz b ka ly kd lz kg ma kk mb ko mc ks lu lv lw lx bi translated">不同的大小和密度会使集群变得复杂</li></ul></div><div class="ab cl ms mt gp mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="hb hc hd he hf"><h1 id="b2fa" class="md ku hi bd kv me nj mg kz mh nk mj ld ix nl iy lg ja nm jb lj jd nn je lm mn bi translated">使用python实现</h1><p id="492c" class="pw-post-body-paragraph jx jy hi jz b ka lp is kc kd lq iv kf kg mo ki kj kk mp km kn ko mq kq kr ks hb bi translated">包括pandas、matplotlib和sklearn在内的库对于扩展python的内置功能以支持K-means非常有用。</p><pre class="jh ji jj jk fd nt nu nv nw aw nx bi"><span id="7658" class="kt ku hi nu b fi ny nz l oa ob">from sklearn import datasets<br/>import matplotlib.pyplot as plt<br/>import pandas as pd<br/>from sklearn.cluster import KMeans</span></pre><h2 id="e9a9" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">数据集</h2><p id="83e6" class="pw-post-body-paragraph jx jy hi jz b ka lp is kc kd lq iv kf kg mo ki kj kk mp km kn ko mq kq kr ks hb bi translated">我们将通过使用sklearn库中提供的IRIS数据集来简化我们的数据采集。该数据由来自三种鸢尾(刚毛鸢尾、海滨鸢尾和杂色鸢尾)的每一种的50个样本组成。数据集描述了每个样本的四个特征:萼片和花瓣的长度和宽度。</p><pre class="jh ji jj jk fd nt nu nv nw aw nx bi"><span id="12e7" class="kt ku hi nu b fi ny nz l oa ob"># loading data<br/>iris = datasets.load_iris()</span></pre><p id="0300" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">定义目标和预测值</p><pre class="jh ji jj jk fd nt nu nv nw aw nx bi"><span id="2c7d" class="kt ku hi nu b fi ny nz l oa ob">X = iris.data[:, :2]<br/>y = iris.target</span></pre><p id="829b" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">让我们看看我们的数据是如何分布的。我们将使用matplotlib的散点图来可视化我们的数据。</p><pre class="jh ji jj jk fd nt nu nv nw aw nx bi"><span id="434a" class="kt ku hi nu b fi ny nz l oa ob">plt.scatter(X[:,0], X[:,1], c=y, cmap='gist_rainbow')<br/>plt.xlabel('Sepa1 Length')<br/>plt.ylabel('Sepal Width')</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es oc"><img src="../Images/69843f9ee20913c61c4c959b1b3677cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*9JhYZbNiYYcQ3EdAJ-ErFQ.png"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated">可视化三个数据分组</figcaption></figure><p id="6884" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">现在，让我们使用sklearn库提供的K-means算法来拟合这些数据。正如我们在上面的图中看到的，有三个集群，因此我们将k设置为3。</p><pre class="jh ji jj jk fd nt nu nv nw aw nx bi"><span id="f474" class="kt ku hi nu b fi ny nz l oa ob"># creating Kmeans object using  KMeans()<br/>kmean = KMeans(n_clusters = 3, random_state=1)</span><span id="6d3b" class="kt ku hi nu b fi od nz l oa ob"># Fit on data<br/>kmean.fit(X)<br/>KMeans(algorithm='auto', <br/>       copy_x=True, <br/>       init='k-means++', # selects initial cluster centers<br/>       max_iter=300,<br/>       n_clusters=3, <br/>       n_init=10, <br/>       n_jobs=None, <br/>       precompute_distances='auto',<br/>       random_state=1, <br/>       tol=0.0001, # min. tolerance for distance between clusters<br/>       verbose=0)</span></pre><p id="6651" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">让我们画出由模型决定的聚类中心。</p><pre class="jh ji jj jk fd nt nu nv nw aw nx bi"><span id="6bab" class="kt ku hi nu b fi ny nz l oa ob"># instantiate a variable for the centers<br/>centers = kmean.cluster_centers_</span><span id="1d71" class="kt ku hi nu b fi od nz l oa ob"># print the cluster centers<br/>print(centers)</span></pre><p id="05fd" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated"><em class="oe">[[6.81276596 3.07446809]<br/>【5.006 3.428】<br/>【5.7735891 2.69245283】]</em></p><h2 id="705d" class="kt ku hi bd kv kw kx ky kz la lb lc ld kg le lf lg kk lh li lj ko lk ll lm ho bi translated">原始数据与聚类结果的比较</h2><p id="9cb4" class="pw-post-body-paragraph jx jy hi jz b ka lp is kc kd lq iv kf kg mo ki kj kk mp km kn ko mq kq kr ks hb bi translated">现在，让我们将聚类数据与原始数据一起绘制出来，看看我们的模型预测与原始的“真实”数据聚类的一致性如何。</p><pre class="jh ji jj jk fd nt nu nv nw aw nx bi"><span id="4a8b" class="kt ku hi nu b fi ny nz l oa ob">new_labels = kmean.labels_<br/># Plot the identified clusters and compare<br/>fig, axes = plt.subplots(1, 2, figsize=(12,7))<br/>axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='gist_rainbow', edgecolor='k', s=150)<br/>axes[1].scatter(X[:, 0], X[:, 1], c=new_labels, cmap='jet', edgecolor='k', s=150)<br/>axes[0].set_xlabel('Sepal length')<br/>axes[0].set_ylabel('Sepal width')<br/>axes[1].set_xlabel('Sepal length')<br/>axes[1].set_ylabel('Sepal width')<br/>axes[0].set_title('Original')<br/>axes[1].set_title('Predicted')</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es of"><img src="../Images/7f0813c3e6787a2f844ff6379b7699d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i2nyPEe4AnwQrmZk4EPgxA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">比较原始聚类和预测聚类的可视化效果</figcaption></figure></div><div class="ab cl ms mt gp mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="hb hc hd he hf"><p id="d534" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">将原始数据聚类的可视化与模型的预测聚类进行比较，我们可以看到一个聚类相当准确(其中“原始”图中的红色是由“预测”图中的绿色估计的)。其他两个集群不容易区分。</p><h1 id="03a1" class="md ku hi bd kv me mf mg kz mh mi mj ld ix mk iy lg ja ml jb lj jd mm je lm mn bi translated">结论</h1><p id="ca83" class="pw-post-body-paragraph jx jy hi jz b ka lp is kc kd lq iv kf kg mo ki kj kk mp km kn ko mq kq kr ks hb bi translated">在本文中，我们讨论了K-means聚类算法在Python中的实现。我们使用sklearn IRIS数据集来训练和测试一个模型，目的是区分三种植物物种。然后，我们对数据的散点图可视化进行着色，表示样本观察的宽度和长度特征。</p><p id="4f67" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">K-means聚类在一系列应用中可能是有用的，包括客户细分、文档分类和威胁检测。但是，当原始数据中的聚类之间存在明显的重叠或相关性时，其他算法可能更适合该任务。</p><p id="fe56" class="pw-post-body-paragraph jx jy hi jz b ka kb is kc kd ke iv kf kg kh ki kj kk kl km kn ko kp kq kr ks hb bi translated">你用K-means分析过哪些数据科学问题？在这里评论，或者在<a class="ae jw" href="https://www.linkedin.com/in/darganj/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>上联系我。</p></div></div>    
</body>
</html>