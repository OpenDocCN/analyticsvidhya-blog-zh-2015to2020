# 保存并加载您的模型，以便在 PyTorch 中继续培训

> 原文：<https://medium.com/analytics-vidhya/saving-and-loading-your-model-to-resume-training-in-pytorch-cb687352fa61?source=collection_archive---------0----------------------->

![](img/9dcbff051b3a9c00143748880b543d18.png)

由 [Unsplash](https://unsplash.com?utm_source=medium&utm_medium=referral) 上的[émile Perron](https://unsplash.com/@emilep?utm_source=medium&utm_medium=referral)拍摄的照片

我刚刚完成了对深度学习模型的训练，以创建歌词的嵌入，并在尝试从特定状态恢复训练我的模型时遇到了多个问题。因此，我决定写一篇博客，与社区分享我的心得。
所以在这篇文章中，我们将讨论如何以检查点的形式保存你的模型，以及如何加载它们来恢复训练你的模型。

# 为什么呢？

那么，你可能会问的第一个问题是，为什么你甚至需要恢复训练模型？！模型不是应该只有在我们完成训练后才被保存吗？？！
想象一下这样一种情况，你已经训练了你的模型几个小时，突然机器崩溃了，或者你失去了与你训练模型的远程 GPU 的连接。灾难对吗？考虑另一种情况，您为某些时期训练了您的模型，这已经花费了相当多的时间，但是您对性能不满意，并且您希望为更多时期训练它。

> 进入检查点和恢复训练！

很明显，需要保存中间模型状态，并有一个恢复训练的机制。我们将这些中间模型状态称为检查点(您可能已经猜到了)。在进入如何保存它们的细节之前，让我们先看看它们是由什么组成的！

# 检查点的内容是什么？

作为一个中间模型状态，我们需要从这里恢复训练，我们需要确保我们保存了模型在训练过程中使用的所有信息。

1.  **模型参数**
2.  **历元数**
    保存历元数是一个很好的方法，这样可以记录并跟踪我们总共运行了多少个历元。
3.  **优化器参数**
    是。你没看错。您需要保存优化器参数，尤其是当您使用 Adam 作为优化器时。Adam 是一种自适应学习率方法，也就是说，它可以计算不同参数的个人学习率，如果你想从你离开的地方继续训练，你就需要这些参数！

# 保存检查点

现在我们知道了内容，让我们保存检查点。Pytorch 使得保存检查点变得非常容易。

注意*。pt* 或*。pth* 是使用 PyTorch 保存文件的常用和推荐的文件扩展名。

让我们看一下上面的代码块。它将状态保存到指定的检查点目录。除此之外，如果该模型是迄今为止最好的模型，则同样的检查点也被复制到另一个目录中以跟踪该最佳模型。

要使用上述函数，请在训练循环中添加以下代码行

这将模型保存在所需的位置，以后可以使用下一节中的函数读取该位置。

# 加载检查点

与保存检查点非常相似，PyTorch 中的函数帮助加载检查点。

上述函数读取检查点文件，并将先前保存的模型状态和优化器状态加载到模型和优化器的实例中。加载状态本质上意味着它将模型/优化器参数设置为保存的检查点中存在的值。

在开始训练循环之前，您可以通过添加以下代码行来加载模型以恢复训练。

基本上，首先初始化您的模型和优化器，然后使用 load checkpoint 函数更新状态字典。

现在，您可以简单地将这个模型和优化器传递到您的训练循环中，您会注意到模型从它停止的地方恢复训练。您可以通过查看每个时期之后的损失值来确认这一点，这是之前观察到的时期(训练停止之前)的延续。

大概就是这样。
感谢阅读。我希望这有所帮助。请不要犹豫，在评论区纠正任何错误。我真的很想学习和提高。