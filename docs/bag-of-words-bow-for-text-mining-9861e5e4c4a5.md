# 用于文本处理的单词包(BoW)

> 原文：<https://medium.com/analytics-vidhya/bag-of-words-bow-for-text-mining-9861e5e4c4a5?source=collection_archive---------12----------------------->

![](img/530f87d38d2f3486aa1cecc7c9d376ce.png)

读者们好，又到了一个……我们开始吧！

*实时信息有很多表现形式，比如数字、图片、视频、文本等等..这次让我们讨论一下课文。*

*很长一段时间以来，文本表示一直在帮助人类，我们希望它能帮助我们的机器学习建立模型和做出决策。为什么不呢？！由于我们无法让机器直接理解文本，所以我们创建了数字，并创建了能够理解矢量表示中的这些数字并做出决策的模型。*

*文本的一个这样的表示是* **单词包(BoW)。**

*在我们进入这个主题之前，请花一点时间为自己想想，你已经得到了一堆文档，这些文档的文本是用与你相似的未知语言编写的。你如何试图找出相似的文件？您可以尝试观察文档中相似的单词和符号，并尝试将它们与其他文档进行匹配。具有最多数量的这些独特的单词/符号的文档是最相似的文档，并且具有与您开始使用的文档更相似的内容。(现在这是一个例子，请不要让你的思想流向模式识别)。当我们教我们的机器寻找相似的文档时，我们使用了类似的方法。*

直觉上，我们称这种方法为单词“包”,因为我们更关心文本中出现的独特单词，而不是它们出现的顺序。然而，我们必须采取某些措施来清除文本中一些最重复的单词，如 a、an、the 等。，并过滤掉像标点符号这样的语法，我将在这次讨论中进一步讨论。

***只要记住这一点:*** *模型只关心已知单词是否出现在文档中，而不关心在文档中的什么地方*

*文本的单词包表示描述了单词在文档中的出现，它涉及两件事:*

1.  已知单词的词汇表。
2.  已知单词存在的度量。

直觉上，我们认为，如果文档具有相似的内容，并且仅内容就足以使我们了解文档的含义，则文档是相似的。

**顿悟由于上面的陈述:** *两个文档相似，如果它们有更多独特的共同之处的话。该模型不关心单词的含义、它们的上下文以及它们在文档中出现的顺序。*

*让我们从一个例子开始，通过获取一些句子并为其生成向量来理解。*

***陈述 1:*** “约翰喜欢看电影。玛丽也喜欢电影。”

***陈述二:*** “约翰也喜欢看足球比赛。”

**生成的向量可以输入到你的机器学习算法中。**

*这两个句子也可以用单词集合来表示。*

```
1\. ['John', 'likes', 'to', 'watch', 'movies.', 'Mary', 'likes', 'movies', 'too.']2\. ['John', 'also', 'likes', 'to', 'watch', 'football', 'games']
```

*此外，对于每个句子，删除多次出现的单词，并使用单词计数来表示。*

```
1\. {"John":1,"likes":2,"to":1,"watch":1,"movies":2,"Mary":1,"too":1}2\. {"John":1,"also":1,"likes":1,"to":1,"watch":1,"football":1,   "games":1}
```

*假设这些句子是文档的一部分，下面是我们整个文档的组合词频。两个句子都考虑进去了。*

```
{"John":2,"likes":3,"to":2,"watch":2,"movies":2,"Mary":1,"too":1,  "also":1,"football":1,"games":1}
```

*来自文档中所有单词的上述词汇，以及它们各自的字数，将被用于创建每个句子的向量。*

**向量的长度将始终等于词汇量。在这种情况下，向量长度是 11。**

*为了用一个向量来表示我们原来的句子，每个向量都用全零初始化—***【0，0，0，0，0，0，0，0，0，0，0，0】**

接着是与我们的词汇表中的每个单词进行迭代和比较，如果句子中有那个单词，就增加向量值。

```
John likes to watch movies. Mary likes movies too.[1, 2, 1, 1, 2, 1, 1, 0, 0, 0]John also likes to watch football games.[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]
```

例如，在句子 1 *中，单词* `*likes*` *出现在第二个位置，并且出现了两次。因此，句子* 1 *的向量的第二个元素将是*2:***【1，2，1，1，2，1，1，0，0】***

**向量总是与我们词汇量的大小成正比。**

**一个生成的词汇量很大的大文档可能会产生一个有很多 0 值的向量。这叫做* ***稀疏矢量*** *。稀疏向量在建模时需要更多的内存和计算资源。对于传统算法来说，大量的位置或维度会使建模过程非常具有挑战性。**

# *编写 BoW 算法*

*我们代码的输入将是多个句子，输出将是向量。*

**输入数组是这样的:**

```
*["John waited for the bus", "The bus was late", "Manny and Sabrina took the cab","I looked for Manny and Sabrina at the cab rental","Manny and Sabrina arrived at the cab rental early but waited until noon for the cab"]*
```

# *第一步:*语义的标记化**

*我们将从删除句子中的 **停用词** *开始。**

***停用词** *是没有包含足够意义的词，在没有我们的算法的情况下使用，并且当我们提到诸如“一个人”、“一个苹果”等任何东西时使用它们。, .我们不希望这些词占用我们的数据库空间，(或)消耗宝贵的处理时间。因此，我们可以通过存储一个你认为是停用词的单词列表，如 a、an、the 等，轻松地删除它们。，**

***记号化***是将一个字符串序列分解成句子中的单词、关键词、短语、符号等元素的过程，称为* **记号** *。这些记号可以是单个的单词、短语，甚至是整个句子。在标记化的过程中，一些类似标点符号的字符被丢弃。**

```
*def word_extraction(sentence):    
    ignore = ['a', "the", "is"]    
    words = re.sub("[^\w]", " ",  sentence).split()    
    clean_txt = [w.lower() for w in words if w not in ignore]
    return clean_txt*
```

**为了更健壮地实现停用词，可以使用 python***nltk****库。***

**每种语言都有一组预定义的单词。**

****例如:****

**`import nltk from nltk.corpus
import stopwords
set(stopwords.words('english'))`**

**由于我们的句子是英语的，我们将停用词设置为“英语”。**

# **第二步:对所有句子应用标记化**

```
**def tokenize(sentences):    
    words = []    
    for sntnce in sentences:        
    w = word_extraction(sntnce)        
    words.extend(w)            
    words = sorted(list(set(words)))    
    return words**
```

**该方法迭代所有的句子，并在每次迭代中将提取的单词添加到一个数组中。**

***该方法的输出将是:***

```
**['and', 'arrived', 'at', 'cab', 'but', 'early', 'for', 'i', 'john', 'late', 'looked', 'manny', 'noon', 'sabrina', 'rental', 'the', 'took', 'bus', 'until', 'waited', 'was']**
```

# **第三步:建立词汇并生成向量**

***使用步骤 1 和 2 中定义的方法创建文档词汇表，并从句子中提取单词。***

```
**def generate_bow(allsntncs):        
    vocab = tokenize(allsntncs)    
    print("Word List for Document \n{0} \n".format(vocab));for sentence in allsntncs:        
    words = word_extraction(sentence)        
    bag_vector = numpy.zeros(len(vocab))        
for wd in words:            
    for i,word in enumerate(vocab):                
        if word == wd:                     
           bag_vector[i] += 1                            
print("{0}\n{1}\n".format(sentence,numpy.array(bag_vector)))**
```

***这里是我们代码的定义输入和执行:***

```
**allsntncs = ["John waited for the bus bus", "The bus was late", "Manny and Sabrina took the cab","I looked for Manny and Sabrina at the cab rental","Manny and Sabrina arrived at the cab rental early but waited until noon for the cab"]generate_bow(allsntncs)**
```

***每个句子的输出向量是:***

```
**Output:John waited for the bus bus[0\. 0\. 0\. 0\. 0\. 0\. 1\. 0\. 1\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 2\. 0\. 1\. 0.]The bus was late[0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 1\. 0\. 0\. 0\. 0\. 0\. 1\. 0\. 1\. 0\. 0\. 1.]Manny and Sabrina took the cab[1\. 0\. 0\. 1\. 0\. 0\. 0\. 0\. 0\. 0\. 0\. 1\. 0\. 1\. 0\. 0\. 1\. 0\. 0\. 0\. 0.]I looked for Manny and Sabrina at the cab rental[1\. 0\. 1\. 1\. 0\. 0\. 1\. 1\. 0\. 0\. 1\. 1\. 0\. 1\. 1\. 0\. 0\. 0\. 0\. 0\. 0.]Manny and Sabrina arrived at the cab rental early but waited until noon for the cab[1\. 1\. 1\. 2\. 1\. 1\. 1\. 0\. 0\. 0\. 0\. 1\. 1\. 1\. 1\. 0\. 0\. 0\. 1\. 1\. 0.]**
```

***如你所见，* ***每一个句子都与我们在步骤 1 中生成的单词表进行了对比。基于该比较，向量元素值可以递增*** *。这些向量可以在 ML 算法中用于文档分类和预测。***

***我们编写了代码并生成了向量，但现在让我们通过理解以下内容来更好地理解这个单词包:***

# **弓的局限性**

1.  ****语义含义** : *基本的* ***鞠躬*** *方法不考虑单词在文档中的含义。它完全忽略了使用它的上下文，所以同一个单词可以根据上下文(或)附近的单词在多个地方使用。***
2.  ****矢量大小** : *对于一个大文档来说，矢量大小可能会很大，从而导致大量的计算和时间。重要的是，您可能需要根据与您的用例的相关性来忽略单词。***

***不要把我们的句子分成一个单词(一个单词)，你可以分成两个单词(两个单词或两个单词)。有时，双元表示法似乎比单元表示法好得多。这些通常可以用* **N-gram 符号** *来表示。***

# **管理词汇**

***随着词汇量的增加，文档的向量表示也随之增加。***

***你可以想象，对于一个非常大的语料库，比如数百本书/文档，向量的长度可能是数千或数百万个位置。此外，每个文档可能包含词汇表中很少的已知单词。***

***这会产生一个有很多零分数的向量，称为* **稀疏向量** *(或)* **稀疏表示**。**

***稀疏向量在建模时需要更多的内存和计算资源，并且大量的位置或维度会使建模过程对传统算法来说非常具有挑战性，从而导致在使用词袋模型时减少词汇量的压力。***

***有一些简单的文本清理技术可以作为第一步，例如:***

*   ***忽略案例***
*   ***忽略标点符号***
*   ***忽略不包含太多信息的常用词，称为停用词，如“a”、“of”等。***
*   **修复拼错的单词。**
*   ***使用* **词干算法将单词还原为词干(例如，从“playing”还原为“play”)——*porter stemmer****(or)****Lancaster stemmer****。***
*   ***词汇化不同于词干化，它适当地减少了词形变化，确保词根属于该语言。在词条化中，词根词称为* ***词条*** *。一个词条(复数词条或词条)是一组单词的规范形式、词典形式或引用形式。***

**一个更复杂的方法是创建一个分组单词的词汇表。这改变了词汇表的范围，并允许单词包从文档中获取更多的含义。**

***在这种方法中，每个单词或记号称为一个“克”。创建一个由两个单词对组成的词汇表，又被称为二元模型。同样，只有出现在语料库中的二元模型被建模，而不是所有可能的二元模型。***

***好消息！你不必在需要的时候编写****，因为它已经是许多可用框架的一部分，比如 sci-kit learn 中的* **CountVectorizer** *。*****

*****我们之前的代码可以替换为:*****

```
****from sklearn.feature_extraction.text import CountVectorizervectorizer = CountVectorizer()
X = vectorizer.fit_transform(allsentences)print(X.toarray())****
```

*******提示:*** *理解框架中的库是如何工作的，理解它们背后的方法总是好的。你对概念理解得越好，你就能更好地利用框架。*****

****现在说了这么多，在我的下一部中再见…和平！****