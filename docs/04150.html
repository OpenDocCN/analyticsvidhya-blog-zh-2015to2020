<html>
<head>
<title>Data Cleaning in Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的数据清洗</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/data-cleaning-in-natural-language-processing-1f77ec1f6406?source=collection_archive---------2-----------------------#2020-03-07">https://medium.com/analytics-vidhya/data-cleaning-in-natural-language-processing-1f77ec1f6406?source=collection_archive---------2-----------------------#2020-03-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="fb44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该职位将通过NLP数据处理的基础。我们将浏览NLP空间中用于数据清理的最流行的库，并提供在您的项目中重用的代码</p><p id="f20d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将使用谷歌新闻数据集(<a class="ae jd" href="https://www.kaggle.com/therohk/million-headlines#abcnews-date-text.csv" rel="noopener ugc nofollow" target="_blank">下载</a>)，这是一个包含两列的csv文件:</p><ul class=""><li id="da7f" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated"><strong class="ih hj">发布日期</strong></li><li id="a9a3" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated"><strong class="ih hj">头条_正文</strong></li></ul><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es js"><img src="../Images/e97b3f62c67764831d275032f8b1b5b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JCY04wRUJ7KKrI_w3uEdHw.png"/></div></div></figure><p id="55ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据有多种应用，例如</p><ul class=""><li id="e185" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">创建word2vec模型(Google word2vec在1000亿个单词上训练模型)[ <a class="ae jd" href="https://code.google.com/archive/p/word2vec/" rel="noopener ugc nofollow" target="_blank">链接</a></li><li id="8d40" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">创建交易系统来分类好/坏情绪</li></ul><p id="870a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有应用程序的共同点是数据清理，通常一个正确或不正确形成的句子上的原始数据并不总是可取的，因为它包含许多不需要的组件，如null/html/links/URL/e moji/stop words等。在这篇博客中，我将介绍用于清理自然语言文本的最常用的数据处理方法</p><h1 id="bae8" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated"><strong class="ak">固定空值</strong></h1><p id="f1f0" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">最常见的也是您应该检查的第一件事是空值，有各种工具可以做到这一点。</p><p id="2887" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们从处理多少空值的基本概述开始。这可以通过运行以下命令来确定:</p><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="b101" class="lm kf hi li b fi ln lo l lp lq">train.isnull().sum()</span></pre><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es lr"><img src="../Images/75cb242ced3e8a00624c2937b222a236.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i-FOhgy8xqNcB4BXoSsowA.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">(注意:本博客中使用的实际数据集没有任何空值，这只是为了演示)</figcaption></figure><p id="d425" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面给了你一个在pandas数据帧的每一列中处理多少个空值的概述。</p><p id="1fe8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步是定义一个值来替换或删除数据集中的空值，要删除和创建一个非空值的新数据帧，可以使用下面的代码:</p><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="bab6" class="lm kf hi li b fi ln lo l lp lq">train[pd.notnull(train["headline_text"])]</span></pre><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es lw"><img src="../Images/c6ee5e5d23254812c00ae48d90f2e71f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CHAP6AKSC4jxxyOsoPDMkg.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">这为您提供了一个没有空标题文本的数据帧</figcaption></figure><p id="8ecd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您也可以决定将空值替换为对您的算法有意义的值，例如，在我的例子中，我想让所有的headline_text在没有值的地方有文本作为“忽略文本”。我可以使用。熊猫中的fillna()方法:</p><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="6b39" class="lm kf hi li b fi ln lo l lp lq">train.headline_text.fillna("IGNORE TEXT")</span></pre><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es lx"><img src="../Images/cea1866b13f84d079024e2637fe1ed0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qse-rb5Al_65d4HV8ntoyg.png"/></div></div></figure><p id="764a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">熊猫图书馆的另一个流行工具是。dropna()对于Null/NaN/NaT值非常有用。它的参数是非常可定制的</p><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="5379" class="lm kf hi li b fi ln lo l lp lq">train.dropna(axis=0, how="any", thresh=None, subset=None, inplace=False).shape</span></pre><ul class=""><li id="5759" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">axis=0/1，0表示删除行，1表示删除列</li><li id="7ba7" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">how=all/any，all表示所有值都为空，any表示即使只有一个值为空</li><li id="aa42" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">thresh=最大丢弃数量的阈值</li><li id="3c2d" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">subset =要扫描的行列范围</li></ul><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es ly"><img src="../Images/272292b119ff977346e6386c6fcaf51b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nBWOL06Z_EsGBvTVBH-FTw.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">请注意dropna()函数是如何从dataframe中删除2行数据的，因为它们都有空值</figcaption></figure><h1 id="585a" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">正在删除URL</h1><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="1476" class="lm kf hi li b fi ln lo l lp lq">def remove_URL(headline_text):<br/>    url = re.compile(r'https?://\S+|www\.\S+')<br/>    return url.sub(r'', headline_text)</span></pre><p id="74cb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用上面的函数删除任何url(以https:// www。来自正文)</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es lz"><img src="../Images/4c720baf6c39d34005e72901f366844a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E43q0cWrBPAiiFPr4LBsnQ.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">注意url是如何使用？应用()</figcaption></figure><h1 id="5f85" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated"><strong class="ak">删除HTML标签</strong></h1><p id="e1d2" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">你可以使用下面的函数通过正则表达式移除html标签</p><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="1a37" class="lm kf hi li b fi ln lo l lp lq">def remove_html(headline_text):<br/>    html=re.compile(r'&lt;.*?&gt;')<br/>    return html.sub(r'',headline_text)<br/><br/>train['headline_text'] = train['headline_text'].apply(remove_html)</span></pre><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es lw"><img src="../Images/f13b55233ab7ca9d3a531bdf6a8b11dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qc38Sv2v-6q4yQjtwXB6Fg.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">注意HTML标签是如何从文本中移除的</figcaption></figure><h1 id="06c1" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">删除图片/标签/符号/表情符号</h1><p id="2060" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">通常在处理真实世界的自由文本时，你会发现你的文本包含了大量的笑脸、表情符号、图片等等，这些都基于你获取数据集的平台。这就要求我们有一个函数可以过滤掉这些特殊的字符序列</p><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="b788" class="lm kf hi li b fi ln lo l lp lq">def remove_emojis(data):<br/>    emoj = re.compile("["<br/>        u"\U0001F600-\U0001F64F"  # emoticons<br/>        u"\U0001F300-\U0001F5FF"  # symbols &amp; pictographs<br/>        u"\U0001F680-\U0001F6FF"  # transport &amp; map symbols<br/>        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)<br/>        u"\U00002500-\U00002BEF"  # chinese char<br/>        u"\U00002702-\U000027B0"<br/>        u"\U00002702-\U000027B0"<br/>        u"\U000024C2-\U0001F251"<br/>        u"\U0001f926-\U0001f937"<br/>        u"\U00010000-\U0010ffff"<br/>        u"\u2640-\u2642" <br/>        u"\u2600-\u2B55"<br/>        u"\u200d"<br/>        u"\u23cf"<br/>        u"\u23e9"<br/>        u"\u231a"<br/>        u"\ufe0f"  # dingbats<br/>        u"\u3030"<br/>                      "]+", re.UNICODE)<br/>    return re.sub(emoj, '', data)</span><span id="ed2c" class="lm kf hi li b fi ma lo l lp lq">train['headline_text'] = train['headline_text'].apply(remove_emoji)</span></pre><h1 id="d376" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">删除标点符号</h1><p id="58d0" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">对于英语语言的清理，标点符号通常作为自由文本的一部分出现，通常不会增加模型的价值，可以使用下面的函数将它们从我们的数据集中删除</p><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="8cb0" class="lm kf hi li b fi ln lo l lp lq">def remove_punct(headline_text):<br/>    table=str.maketrans('','',string.punctuation)<br/>    return headline_text.translate(table)</span><span id="7e9c" class="lm kf hi li b fi ma lo l lp lq">train['headline_text'] = train['headline_text'].apply(remove_punct)</span></pre><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es mb"><img src="../Images/4d4eac38d858a53fc5a4c8053f99d5b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zeE_hM24y4TMuMXhWui_Fw.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">注意()是如何从文本中删除的</figcaption></figure><h1 id="62cc" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">文本标记化</h1><p id="4768" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">在大多数用例中，下一个有用的步骤是从句子中提取文本，通常有多种可能性，下面我们使用最流行的自然语言工具库之一<a class="ae jd" href="https://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> nlkt </a></p><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="23e6" class="lm kf hi li b fi ln lo l lp lq">import nltk.data</span><span id="9760" class="lm kf hi li b fi ma lo l lp lq">##Load language Specific .pickle file</span><span id="703c" class="lm kf hi li b fi ma lo l lp lq">tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')<br/>spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')</span><span id="2963" class="lm kf hi li b fi ma lo l lp lq">##Different type of tokenizer</span><span id="c424" class="lm kf hi li b fi ma lo l lp lq">from nltk.tokenize import regexp_tokenize <br/>from nltk.tokenize import RegexpTokenizer <br/>from nltk.tokenize import WordPunctTokenizer <br/>from nltk.tokenize import PunktWordTokenizer <br/>from nltk.tokenize import TreebankWordTokenizer <br/>from nltk.tokenize import word_tokenize</span><span id="31aa" class="lm kf hi li b fi ma lo l lp lq">##Sample initialization of token </span><span id="ab90" class="lm kf hi li b fi ma lo l lp lq">tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')</span><span id="1537" class="lm kf hi li b fi ma lo l lp lq">##Define Normalization <br/>normalization = None<br/>normalization = 'stemmer'<br/>normalization = 'lemmatizer'</span><span id="feae" class="lm kf hi li b fi ma lo l lp lq">##Define Vectorizer <br/>vectorizer = 'countvectorizer'<br/>vectorizer = 'tfidfvectorizer'</span></pre><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es mc"><img src="../Images/ee2f3d827b9f1db1fc19af73cce59529.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e8uentcFh-Cz0IV6eV2VBA.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">注意句子是如何被转换成文本标记的</figcaption></figure><h1 id="0ae2" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">正常化</h1><p id="bcdd" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">清除社交媒体文本时的一个常见预处理步骤是归一化。文本规范化是将文本转换成规范(标准)形式的过程。例如，单词“gooood”和“gud”可以转换为“good”，这是它的规范形式。</p><p id="ea6a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jd" href="https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html" rel="noopener ugc nofollow" target="_blank"> Ref - I </a></p><p id="36b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">示例:</p><p id="e7c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2moro，2mrrw，tomrw →明天</p><p id="c23d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b4 →之前</p><p id="feaf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在python中，这可以使用nltk库来完成</p><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="0e5f" class="lm kf hi li b fi ln lo l lp lq">def stem_tokens(tokens):<br/>    stemmer = nltk.stem.PorterStemmer()<br/>    tokens = [stemmer.stem(token) for token in tokens]<br/>    return tokens</span><span id="462e" class="lm kf hi li b fi ma lo l lp lq">def lemmatize_tokens(tokens):<br/>    lemmatizer = nltk.stem.WordNetLemmatizer()<br/>    tokens = [lemmatizer.lemmatize(token) for token in tokens]<br/>    return tokens</span><span id="0b19" class="lm kf hi li b fi ma lo l lp lq">def normalize_tokens(normalization):<br/>    if normalization is not None:<br/>        if normalization == 'stemmer':<br/>            train['text'] = train['text'].apply(stem_tokens)<br/>        elif normalization == 'lemmatizer':<br/>            train['text'] = train['text'].apply(lemmatize_tokens)<br/>        <br/>normalize_tokens(normalization)</span></pre><h1 id="e1fc" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">停用词</h1><p id="86f3" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">在英语中，你通常需要删除所有不必要的停用词，nlkt库包含一个停用词包，可以用来过滤掉文本中的停用词。通过下面的代码可以看到这个列表</p><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="cfca" class="lm kf hi li b fi ln lo l lp lq">import nltk<br/>from nltk.corpus import stopwords<br/> set(stopwords.words('english'))</span></pre><p id="02ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">{ '我们自己'，'她的'，'之间'，'你自己'，'但是'，'又'，'有'，'关于'，'曾经'，'期间'，'出'，'非常'，'有'，'有'，'自己'，'一个'，'自己'，'你的'，'这样'，'成'，'最'，'本身'，'其他'，'关'，'是'，' s '，' am '，'或'谁'，'作为'，'从'，'他'，'每个'，'这个'，'他们'，'自己'，'直到'，'下面'，'是'，'我们'，'这些'，'你的'，'他的'，' 将'，'上'，'做'，'你们自己'，'然后'，'那个'，'因为'，'什么'，'过'，'为什么'，'所以'，'能'，'没有'，'现在'，'下'，'他'，'你'，'自己'，'有'，'刚刚'，'在'，'太'，'只有'，'我'，'几个'，'谁'，' t '，'被'，'如果'，'他们的'，'我的'，'反对'，'一个'，'被'，'做'，'它'，'如何'，'进一步'，'曾'，'这里'，'比' }</p><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="4cea" class="lm kf hi li b fi ln lo l lp lq">def remove_stopwords(text):<br/>    words = [w for w in text if w not in stopwords.words('english')]<br/>    return wordstrain['headline_text'] = train['headline_text'].apply(remove_stopwords)</span></pre><p id="925d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据输入示例:</p><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="4f2e" class="lm kf hi li b fi ln lo l lp lq">Input : ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', <br/>'off', 'the', 'stop', 'words', 'filtration', '.']<br/></span><span id="ffb5" class="lm kf hi li b fi ma lo l lp lq">Output : ['This', 'sample', 'sentence', ',', 'showing', 'stop',<br/>'words', 'filtration', '.']</span></pre><h1 id="52eb" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">向量化您的代码</h1><p id="c2d5" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated"><code class="du md me mf li b"><a class="ae jd" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction" rel="noopener ugc nofollow" target="_blank">sklearn.feature_extraction</a></code>模块可用于从由文本和图像等格式组成的数据集中提取机器学习算法支持的格式的特征</p><figure class="jt ju jv jw fd jx er es paragraph-image"><div role="button" tabindex="0" class="jy jz di ka bf kb"><div class="er es mg"><img src="../Images/6afe36202e22a7a2a0268921265d3407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C32OakrZxsXvyeat5Yf_sw.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">根据您的使用情况，您需要选择ngram来创建特征向量</figcaption></figure><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="3230" class="lm kf hi li b fi ln lo l lp lq">## Default CountVectorizer</span><span id="e97d" class="lm kf hi li b fi ma lo l lp lq">from sklearn.feature_extraction.text import CountVectorizer<br/>corpus = [<br/>    'This is the first document.',<br/>    'This document is the second document.',<br/>    'And this is the third one.',<br/>    'Is this the first document?',<br/>]<br/>vectorizer = CountVectorizer()<br/>X = vectorizer.fit_transform(corpus)<br/>print(vectorizer2.get_feature_names())<br/>print(X.toarray())</span></pre><p id="e476" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="f8b1" class="lm kf hi li b fi ln lo l lp lq">## CountVectorizer(with ngram=2)</span><span id="18a7" class="lm kf hi li b fi ma lo l lp lq">vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))<br/>X2 = vectorizer2.fit_transform(corpus)<br/>print(vectorizer2.get_feature_names())<br/>print(X2.toarray())</span></pre><p id="5fa9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</p><p id="5a99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现实生活中的用法:</p><pre class="jt ju jv jw fd lh li lj lk aw ll bi"><span id="f230" class="lm kf hi li b fi ln lo l lp lq"># Vectorizationin funcation<br/>def vectorize(vectorizer):<br/>    if vectorizer == 'countvectorizer':<br/>        print('countvectorizer')<br/>        vectorizer = CountVectorizer()<br/>        train_vectors = vectorizer.fit_transform(train['text'])<br/>        test_vectors = vectorizer.transform(test['text'])<br/>    elif vectorizer == 'tfidfvectorizer':<br/>        print('tfidfvectorizer')<br/>        vectorizer = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))<br/>        train_vectors = vectorizer.fit_transform(train['text'])<br/>        test_vectors = vectorizer.transform(test['text'])<br/>    return train_vectors, test_vectors<br/>train_vectors, test_vectors = vectorize(vectorizer)</span></pre><p id="7e5f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望上面的编译能帮助一个初学NLP的初学者</p></div></div>    
</body>
</html>