<html>
<head>
<title>BIG DATA — IMPLEMENTATION</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">大数据—实施</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/big-data-implementation-43871736491e?source=collection_archive---------12-----------------------#2020-06-03">https://medium.com/analytics-vidhya/big-data-implementation-43871736491e?source=collection_archive---------12-----------------------#2020-06-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f4ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本博客讨论了云平台中大数据的实施、Hadoop和Spark概述、其使用案例、存储、处理、分析、可视化等。在生产系统中</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/7096a6111cadde89367c196df0bc6a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nfqAEstLZRC3VymIbOYUaQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">云中的大数据架构</figcaption></figure><p id="d4fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇博客的目的是了解Hadoop和Spark技术(概述)以及生产系统中的用例和执行。本博客将帮助您在云环境中设计大数据项目的架构。内容针对新项目(从头开始开发)，讨论数据类型、工具解释、用例、解决方案解释等。,.</p><p id="3789" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">本博客内容</strong>:</p><p id="deaf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我将介绍Hadoop和Spark平台、Hadoop的优缺点、可能支持ELT /ETL的用例、Hive:分区、数据仓库、SQL查询、NOSQL数据库、Hadoop发行版、流处理、Drill(查询引擎)用例、AWS中的执行。我介绍了5个用例，分别是Hive中的ELT处理、使用SQOOP的数据摄取、使用连接器和Spark SQL的NOSQL/SQL数据处理、流处理—日志处理和钻取用例。</p><p id="d5b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于使用案例和大规模数据，我们需要决定是否可以使用大数据。</p><p id="90b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我涵盖了Hadoop中的2个用例、Spark中的2个用例以及Drill中的1个用例。</p><p id="05b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过查看标题图，您会对AWS中的Hadoop有所了解。</p><p id="deb9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在云计算中执行大数据项目的步骤。</p><ol class=""><li id="6c78" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">行程安排</li><li id="5ffe" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">数据采集</li><li id="3768" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">存储和处理(执行分析或分析)</li><li id="fb17" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">导出结果</li><li id="2cc4" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">数据可视化</li></ol><p id="2816" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你必须从头到尾阅读这篇博客，然后你会对在云环境中使用大数据平台有一个清晰的了解，这可能有助于考虑你的需求或使用案例。</p><p id="a3ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我从介绍数据来源开始这篇博客。</p><p id="36b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您的应用程序可能有不同的架构，但这是用于Hadoop、Spark、NOSQL平台的通用架构。</p><p id="a61b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">调度器</strong>:调度器定期运行，从不同的数据源获取数据，并将压缩数据(例如:zip格式)移动到AWS-S3。</p><p id="85e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据收集</strong>:数据可以从不同的来源收集，如平面文件、数据库、NOSQLs、DWHs、Web &amp;社交媒体(通过编写抓取器来提取数据)，主要存储在AWS-S3等数据湖中，然后转移到HDFS。</p><p id="3ac3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦数据移动到HDFS(这是主存储)，处理大数据的故事就开始了。</p><p id="6beb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在处理数据之前，我们需要了解我们将要处理的数据类型。通常有3种不同类型的数据:</p><p id="d71a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1) <strong class="ih hj">结构化</strong>:从RDBMS和DWHs生成的数据(它包含结构和数据)</p><p id="5c89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2) <strong class="ih hj">半结构化</strong>:由NOSQLs、XML、JSON文件生成的数据。(与半结构和数据混合)</p><p id="2430" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3) <strong class="ih hj">非结构化</strong>:数据中没有结构。例如:CSV，TXT，图像，视频等。,</p><p id="ad69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦我们理解了数据，我们必须选择工具来处理。我们将从了解Hadoop和Spark平台中的工具开始。</p><p id="81c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们从Hadoop框架及其生态系统开始。</p><p id="a638" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi kh translated">adoop是一个基于java的分布式并行处理开源软件框架，用于存储大规模数据，并在具有不同类型模式的商用硬件集群上运行应用程序。它为任何类型的数据提供大容量存储、巨大的处理能力以及处理几乎无限的并发任务或工作的能力。</p><p id="eae6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi kh translated"><span class="l ki kj kk bm kl km kn ko kp di"> H </span> adoop是分布式系统，因为框架将文件分割成大的数据块，并将它们分布在集群中的节点上。Hadoop然后并行处理数据，其中节点只处理它有权访问的数据。这使得处理比在更传统的体系结构(如RDBMS)中更有效、更快。</p><p id="f1df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Hadoop中有2个主要组件1)HDFS(用于存储)2) Map Reduce(处理)。看标题图。</p><p id="58f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> HDFS </strong> — Hadoop分布式文件系统，允许存储数据(处理前&amp;和处理后)和结果。HDFS提供了操作和管理文件的命令。</p><p id="132f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Map Reduce (MR) </strong> : MR将处理数据。每一个操作或作业转换成MR。首先它会执行Map，然后减少。两者都是独立的实体。</p><p id="b012" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在MR中工作要复杂得多，你必须编写自己的映射器并减少组件，以克服这种复杂性，生态系统就出现了。在生产系统中，生态系统被认为是有用的，即使生态系统内部转换成Map减少MR。直接处理MR将较少被考虑。</p><p id="9488" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Hadoop解决了任何分布式数据处理框架面临的3个主要问题:</p><p id="55b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1) <strong class="ih hj">并行化</strong> <em class="kq"> : </em>一次计算数据的子集(将数据分成子集)。</p><p id="c048" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2) <strong class="ih hj">分发</strong>:分发数据</p><p id="0368" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3) <strong class="ih hj">容错</strong>:处理组件或节点故障。</p><p id="9119" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">生态系统</strong>:今天介绍了各种生态系统，我将列出最常见的。</p><ol class=""><li id="bf77" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated"><strong class="ih hj"> Hive (DWH) </strong>:它是DWH的基础设施，允许我们以SQL命令的形式操作数据。有各种类型的特性用于导入/导出、DDL、DML、存储、分区(静态&amp;动态)、桶、from语句、多插入语句等。，你必须存储和操作结构化数据。它为大型表提供了专门的查询和汇总。</li><li id="b384" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">SQOOP :这是一个数据摄取工具，用于将数据导入RDBMS/HDFS/从RDBMS/导出数据。</li><li id="cbc9" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">这是一个流处理工具，当数据到达时，你必须处理和存储。</li><li id="2e15" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">猪:顾名思义，它可以吃任何东西，复杂的查询可以通过分成子语句来解决。它是用于并行处理的高级脚本数据流语言和执行框架。</li><li id="094e" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj"> HBase </strong>:它是一个分布式的、面向非SQL列的数据库。我们主要使用JAVA-API来存储和操作HBase中的数据。</li><li id="46a5" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj"> OOZIE </strong>:基于服务器的工作流调度系统，管理Hadoop作业。</li><li id="78d0" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj"> Zookeeper </strong>:在Hadoop环境下控制和协调系统，为大型分布式系统提供分布式<a class="ae kr" href="https://en.wikipedia.org/wiki/Configuration_management" rel="noopener ugc nofollow" target="_blank"> c </a>配置服务、<a class="ae kr" href="https://en.wikipedia.org/wiki/Synchronization_(computer_science)" rel="noopener ugc nofollow" target="_blank"> s </a>同步服务和命名注册表(端口)。</li><li id="bbc7" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj"> Ambari </strong>:这是一个基于web的集群管理系统，用于为hdfs、mapreduce和Hadoop生态系统监控、管理和配置Hadoop集群。它提供了一个仪表板，用于查看您的Hadoop集群运行状况。</li></ol><p id="09bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi kh translated">请注意，对于大规模数据，每个生态系统都有单独的用例。例如，要将数据从RDBMS摄取到Hadoop，您可以使用SQOOP，要处理非结构化数据进行复杂查询，您可以使用PIG，要存储大型SQL表，Hive是最佳工具。</p><p id="d2e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">Hadoop中的用例</strong></p><p id="a0cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">hive——使用分区和桶以及<strong class="ih hj"> ELT </strong>流程的用例。在美国以州和城市的方式存储数据。</p><p id="8bd8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我们必须了解数据仓库及其用途。</p><p id="66f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">什么是数据仓库？</p><p id="00ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据仓库(DWH)是向所有用户提供数据以进行分析、报告等的地方。一个数据仓库，也可以称为企业数据仓库(EDW)。</p><p id="ca66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">什么是DWH，什么不是？</p><p id="73fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据仓库是一个数据库，其数据包含操作数据的副本。这些数据可以从多个数据源获得，对决策制定非常有用。DWH不包含原始数据。DWH不仅包含历史数据，还可以由分析和报告数据组成。交易数据不存储在DWH。</p><p id="e09f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">DWH的做法</strong>:</p><p id="3fd4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">传统的数据仓库设计可以使用两种常见的数据建模方法之一:维度模型(也称为星型模式)或规范化模型。</p><p id="ea39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">具有事实表的维度模型——数据库中关于实体(表)的不可变值或度量。它由事实和维度表组成，其中每个事实都与一个或多个维度相关联。事实表，维度方法增加了维度(即属性)——系统中实体的详细信息表，为事实提供了上下文。</p><p id="5db9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一种标准化的数据仓库方法，涉及一种非常类似于OLTP DBs的设计。表是由实体创建的，旨在保留第三范式—属性完全依赖于主键。</p><p id="3ae4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用维度方法，数据库针对查询和分析进行了优化，执行分析查询更容易、更快，而规范化方法使更新信息更容易。</p><p id="d289" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下图描述了传统和Hadoop级别数据仓库架构的高级架构。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ks"><img src="../Images/7d3027da159efdf74d6f223dbaafe51a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*XC-FQuzVYomGAx2WuZCZfg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">高级传统数据仓库架构</figcaption></figure><p id="51d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们浏览一下传统的数据仓库架构。</p><p id="61a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">传统的EDW由以下组件组成:操作源系统、数据暂存区、数据仓库，将在下面稍加解释。</p><p id="aeb2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">运营源系统</strong>:这些是捕获业务事务的在线事务处理数据库(OLTP)和运营数据存储。</p><p id="1c59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据暂存区</strong>:作为存储区和ETL/ELT处理的平台。数据从可操作的源系统(OSS)中提取出来并移动到暂存区，在加载到最终的数据仓库模式之前，数据在暂存区中进行转换。转换可能包括重复数据删除、规范化数据、清理数据、丰富数据等操作。分级不是固定的体系结构，而是会根据所使用的工具、处理模型和类似因素而变化。例如，在ETL模型中，临时区域是DWH、平面文件或供应商特定基础设施外部的一组关系表。在ELT模型中，暂存区是DWH本身中的一组表，在数据加载到最终表之前，在这里进行转换。该区域严格用于数据处理—用户永远不会访问暂存区域中的数据来进行查询、报告等操作。在我们的用例中，对暂存区中的数据进行分区，我将在下面进行解释。</p><p id="ba04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> EDW(数据仓库)</strong>:数据仓库是供所有用户分析、报告等使用的地方。DW中的模式是为数据分析而设计的，这通常涉及到访问多个记录。</p><p id="3f50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在该体系结构的各层之间移动数据并执行转换是用DI(数据集成)工具实现的。DI工具如Pentaho，Talend，SAP，IBM等。</p><p id="519c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下段落将解释使用Hadoop设计DW(数据仓库)及其组件。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kt"><img src="../Images/486ac9dfdc3990cda8608e1fa29e3bd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QlkjSUtKyyCZalnwt_dXOg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">高级Hadoop数据仓库架构</figcaption></figure><p id="56b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们浏览一下高级Hadoop数据仓库架构。</p><p id="66b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">运营源系统</strong>:这里是OLTP数据库，数据可以从Sqoop摄取工具中提取。此外，Hadoop中还添加了不同的结构化数据源，如web日志、机器数据或社交媒体源。这些数据源(网络日志、机器数据、社交媒体源)通常难以用传统的DWH管理，但使用Hadoop可以轻松处理。这些数据源被称为基于事件的数据，通常使用Flume或Kafka或Spark流数据等工具。</p><p id="c71c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单来说:OSS——提取数据的SQOOP，基于事件的系统——Flume、Kafka或Spark Streaming。</p><p id="2ed5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">暂存层</strong>:源数据(操作系统或基于事件的系统)在被转换(以期望的形式)和加载到目标系统之前将被接收到HDFS。可以使用Map Reduce、Hive、Pig或Spark进行转换。</p><p id="d1fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">分析</strong>:转换后，数据将被移入目标系统。目标系统可以是蜂巢或者黑斑羚或者出口到DWH做进一步分析。</p><p id="9f4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">分析的可视化</strong>:可以通过BI工具、分析、可视化工具访问数据。在传统的DWH中，分析工具从不访问暂存区中的数据。数据分析可以在DWH中运行，也可以在Hadoop中运行。</p><p id="28ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">工作流:我们的处理很可能由一个工作流管理工具来编排，比如Oozie。</p><p id="d961" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">用例描述</strong>:我有一家冷饮公司，业务遍及美国50个州，我想知道每个州的每月销售额。</p><p id="dc99" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是需要考虑实现的步骤。</p><ol class=""><li id="bece" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">在每月第一天上午12:00之后从每个州获取数据:为此，我们需要在每月第一天上午12:00运行调度程序。</li><li id="cd75" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">收集数据湖(AWS -S3)中的数据并转移到HDFS。(shell脚本中的这些动人语句)</li><li id="dc6e" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">最初将所有数据加载到配置单元表，即临时表中</li><li id="4b55" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">从临时表中使用多插入语句和分区(动态)来加载州级别和城市的记录</li><li id="a14c" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">编写一个查询来查找每个分区州在城市中的最高销售额</li><li id="5080" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">使用BI工具进行可视化。</li></ol><p id="7f51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Hadoop中的数据建模和存储由其他东西组成(如反规范化、更新数据、存储格式和压缩、分区等。，)设计蜂巢DWH时要考虑的问题，这里对蜂巢分区进行了详细解释。</p><p id="78ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是标准且常见的大数据端到端使用情形。同样，这一过程可以扩展到我的公司分支所在的世界各地。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ku"><img src="../Images/5b1ed675570ad3aa0da3708ae2a45fb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*unYrE6sogU8R2BgLQM7XoA.jpeg"/></div></div></figure><p id="d07e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们简单了解一下Hive中的分区？</p><p id="f9f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据美国的州和城市划分数据。</p><p id="52bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">关于分区</strong>:</p><p id="44d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对单个或多个列上的表数据或数据集进行分区。通过允许跳过评估查询不需要的部分数据，对数据进行分区消除了大量不必要的输入/输出操作。</p><p id="e4ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如:如果表在名为“<strong class="ih hj"> State </strong>”(美国)的列上分区，我们将只查询特定的州，而不是整个表。如果总数据包含600万条记录，而纽约州有50000条记录，查询只需要评估纽约州，那么它必须处理50000条记录，而不是600万条记录。</p><p id="0af7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">分区大小:</strong></p><p id="8c06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">平均分区大小至少是HDFS块大小的几倍。常见的块大小是64 MB或128 MB，根据前面的分析，1 GB是平均分区大小。如果数据在某一天超过1 GB左右，那么按小时分区。如果每天的数据少于1 GB，则分区为每周或每月。</p><p id="63e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据整合:</strong></p><p id="0a02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据集成(DI)是一种跨数据存储移动数据或使数据可用的技术。DI可以包括提取、验证、移动、清理、标准化、转换和加载。</p><p id="9b9b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> ETL过程</strong>:一种非常常见的处理类型是提取-转换-加载处理管道——将数据加载到Hadoop中，以某种方式转换以满足业务需求，然后将其移动到系统中进行进一步处理或分析或可视化等。,</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kv"><img src="../Images/f3744298bee17f907a1faabd480de246.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*2CtCTStTmZ-KgqmvddG1ew.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><strong class="bd kw"> ETL流程</strong></figcaption></figure><p id="4739" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ETL过程依次遵循三个步骤。</p><ol class=""><li id="8a33" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">提取-从数据源提取的数据</li><li id="f2e7" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">转型—满足业务需求的流程</li><li id="2b39" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">加载—将数据加载到端点以供进一步处理(DWH或其他位置)。</li></ol><p id="0483" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> ELT流程</strong>:数据集成的另一种处理。在这里，数据从数据源中提取出来，不经转换就加载到staging中。之后，数据在staging中进行转换，然后加载到数据仓库中。在ELT过程中，步骤相同，但顺序不同1)提取2)加载3)转换。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kx"><img src="../Images/c36c3fe089413a36ff0d11ea37532595.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*28oVJ1IueeHbffNB82VDfg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">ELT过程</figcaption></figure><p id="00ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上述划分<strong class="ih hj">的例子中，ELT </strong>过程可以定义为:</p><p id="70a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">提取</strong>:从数据源提取数据(即我们用例的输入)，即从世界各地的所有分支机构提取数据。</p><p id="47b0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Load </strong>:然后将数据加载到数据仓库中，也就是我们例子中的Hive。</p><p id="9d52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">转换</strong>:根据国家、州和城市对数据进行处理、验证和过滤，使用分区存储或加载相应的国家、州和城市数据。</p><p id="ba69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> SQOOP-HIVE用例</strong></p><p id="ecf3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">问题</strong>:分析产品销量(找出本季度销量最高的产品)。制造商是一家跨国公司。</p><p id="2604" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">用例描述</strong>:从不同的RDBMS、DWH现有系统(全球所有分支机构和不同地区)收集数据，在Hadoop中统一数据并分析数据和可视化结果。</p><p id="f0f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Sqoop用法</strong>:使用Sqoop数据摄取工具从不同的数据库系统收集数据。它将来自不同数据源的数据接收到HDFS。通常，从我们的OLTP数据存储中获取数据到Hadoop。Sqoop可以连接多个表中的数据，并将其存储到HDFS中。</p><p id="c244" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Hive用法:</strong>将数据加载到暂存表中，对数据进行分区，启动查询</p><p id="022c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">执行</strong>的步骤:</p><ol class=""><li id="d32d" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">使用Sqoop import语句从DWH RDBMS导入数据</li><li id="7cf1" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">将数据加载到HDFS(如果您知道结构并且不需要进一步过滤或分区，您可以加载到hive)</li><li id="cefd" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">使用配置单元外部表(临时)来映射数据</li><li id="1644" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">根据您的方便，从暂存表中过滤数据或映射到不同的分区</li><li id="f79c" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">在分区上执行查询以获得结果(可以使用sqoop export语句将结果或数据导出到不同的RDBMS或DWH系统)</li><li id="119e" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">存储结果，并转发给BI团队以直观显示结果。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ky"><img src="../Images/d9f5f77f1f502a6c15b80b3a4328112e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vvOw5NnNPyWmQbrTbTIbhg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">Sqoop用于从RDBMS/ DWH导入数据集，在HIVE中处理并将结果导出到不同的RDBMS或DWH</figcaption></figure><p id="7798" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">地图减少缺点:</p><ol class=""><li id="49b0" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">作业结果需要先存储在HDFS，然后才能被其他作业使用。由于这个原因，MR不适用于迭代算法。</li><li id="281f" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">由于范式的两个步骤，许多类型的问题不容易适应，并且将每个问题分解成一系列这两个操作可能太困难。</li><li id="fb8f" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">Hadoop是一个相当低级的框架，许多高级工具(导入、导出、操纵数据、实时处理等。，)更复杂，以适应，他们带来额外的复杂性，这是复杂的任何环境。</li><li id="70f2" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">Map减少了每次从磁盘中读取数据，对于每次操作，MR将从HDFS存储或接收数据，这不是Spark会做的内存概念。</li></ol><p id="71a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Hadoop发行版:</strong></p><p id="b2c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Hadoop发行版可以基于Hadoop及其生态系统轻松管理和安装所需的包。</p><p id="9a3d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">什么是Hadoop发行版</strong>:由于Hadoop是开源的，许多公司开发了超越原始开源代码的专有发行版。通常Hadoop发行版通常包括Hadoop Common、HDFS、Hadoop MapReduce、YARN。</p><p id="16f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Hadoop发行版为客户提供了额外的价值，并解决了原始代码的问题。通常，供应商关注可靠性、稳定性和技术支持，并提供定制配置来完成特定任务。</p><p id="fa46" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于猪脚本、Hive脚本的开发，查看HDFS、S3的文件，Hbase查询可以使用“<strong class="ih hj"><em class="kq"/></strong>”。Hue是一个开源的SQL数据库助手。你可以做以下事情。它可以被认为是DBs和DWHs的IDE。</p><p id="7cdb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">像专家一样编写SQL，查找和连接您的数据，直观地发现见解，连接到所有数据库，等等。，以下是Hue的截图。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kz"><img src="../Images/7bb3ad0551961eec70594929df81b724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tzTY9UsnE4a3UstS0EEA8g.png"/></div></div></figure><p id="cd41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">前三个行业选择是Cloudera、Hortonworks和MapR即使微软Azure的HDInsight、AWS elastic mapreduce、IBM开放平台、pivotal大数据套件等。,</p><p id="55fb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们必须发布任何一个用于开发和生产的Hadoop发行版。上一节我选择了云端的AWS EMR (Elastic mapreduce)。</p><p id="98f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi kh translated"><span class="l ki kj kk bm kl km kn ko kp di"> S </span> park:仅是数据处理引擎(无存储)，是大规模数据处理的统一分析引擎。实际上，它通常被定义为一个快速、通用的分布式计算平台。</p><p id="1361" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark建立在Scala(函数式面向对象语言)之上。Spark速度快，易于使用，通用性强，可以在任何地方运行(意味着在Hadoop、Mesos、Kubernetes上运行，独立运行或在云中运行)。</p><p id="dfcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">spark it的优点是可以访问不同的数据源(如DWH、NOSQL、RDBMS、平面文件),并通过连接器轻松连接。</p><p id="ef55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark将类似MapReduce的批处理功能、实时数据处理功能、类似SQL的结构化数据处理、图形算法和机器学习结合在一个框架中。这是满足大部分大数据需求的一站式商店。</p><p id="830a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark给处理时间带来了一些开销。当处理大量数据时，这种开销可以忽略不计；但是，如果您有一个可以由单个节点处理的数据集，那么最好使用其他一些框架。Spark不适合OLTP(在线事务处理)应用程序(快速、大量、原子事务)。它更适合在线分析处理OLAP:批处理和数据挖掘。</p><p id="06f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">火花要点&amp;火花模块</strong>:</p><ol class=""><li id="ce5a" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated"><strong class="ih hj">模块</strong>:</li></ol><p id="fea6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Spark Core </strong> : Spark致力于称为RDD的概念(在Spark 2.0之前)，Spark 2.0之后致力于数据集，但仍然支持rdd。你必须检查版本及其局限性。</p><p id="c1d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Spark SQL </strong>:如果您熟悉SQL，那么您可以轻松使用Spark，因为它支持以编程方式以及API级别运行SQL查询。</p><p id="2abe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">使用SQL查询</strong>:</p><p id="9903" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">val</strong>sqlDF<strong class="ih hj">=</strong>spark . SQL(" SELECT * FROM table ")</p><p id="1278" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用API:</p><p id="e03a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">val empdata frame = spark . read . JSON(" data/emply oees . JSON ")</p><p id="8206" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">empDataFrame.select("name ")。显示()</p><p id="8a61" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Spark-SQL中，数据是以Dataframe的形式操作的(就像RDBMS表一样)。数据框提供了所有的sql操作。</p><p id="1ef9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Spark-Streaming </strong>:流式传输允许我们在数据到达时进行处理。当数据到来时，你必须立即处理和存储，Spark允许我们结合流和SQL，灵活地处理数据。</p><p id="8587" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Spark-ML </strong>:旨在提供实用的可扩展且简单的机器学习。如果您的项目需要建立预测、分类数据模型，那么您不需要寻找另一个工具spark include本身。它还为线性代数、统计、数据处理等提供了特征、管道、持久性和实用程序。,</p><p id="c605" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它支持其他模块，如GraphX、结构化流等。，更多详情请访问https://spark.apache.org/<a class="ae kr" href="https://spark.apache.org/" rel="noopener ugc nofollow" target="_blank"/></p><p id="adb1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.通过使用spark——减少了许多代码行，没有Map和Reduce (Hadoop框架的)之类的东西。</p><p id="eadd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一般来说，Spark可以被视为Hadoop的继承者和替代者。</p><p id="3493" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark正在将Hadoop框架的许多功能统一到一个统一的平台中。许多角色和功能可以在一个平台上协同工作。</p><p id="0c94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark可以很容易地淘汰Hadoop生态系统中的工具，如Mahout、Giraph、Pig、Hive、Sqoop，因为Spark可以很容易地用其统一的架构风格进行替换。例如:Mahout可以用Spark MLlib代替，Apache Pig，Apache Sqoop可以很容易地用Spark Core和Spark SQL代替</p><p id="0ba0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">Spark中的用例</strong></p><p id="2ffa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">用例是统一来自NOSQL和RDBMS的数据，并对其进行分析。</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es la"><img src="../Images/ddd57c7bdb00fc7b01b2ba60728c5d8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*EUmWJDiUUQrVT3oICf2p_g.jpeg"/></div></figure><p id="73f2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用Spark连接器、连接不同数据库的连接器、NOSQL、搜索引擎、平面文件等可以轻松处理这个用例。,</p><p id="d013" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">连接器允许我们从RDBMS、NOSQLs中读取/写入数据。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lb"><img src="../Images/0deb73ffcaf70009ecdff33e9398317c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*DYA2tZEc-xMNDteTa3oLYw.png"/></div></figure><p id="be9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">no SQL的介绍</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lc"><img src="../Images/cead542a744e264a998221a364fef827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*SQGCVCcjvedRLGBHcLlp7g.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">NOSQL的类别</figcaption></figure><p id="da15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">NoSQL(不仅仅是结构化查询语言)是一个用来描述那些应用于非结构化数据的数据存储的术语。生产系统在分布式系统中选择NoSQL。通常，NoSQL数据存储的强大之处在于，随着数据的增长，实现的解决方案可以通过向分布式系统添加额外的机器(节点)来扩展。</p><p id="dec2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">NoSQL有4个主要类别</p><ol class=""><li id="f677" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated"><strong class="ih hj">列族存储</strong>:其用途是读写原始时间序列数据<strong class="ih hj">例如:Cassandra，HBase </strong></li><li id="9c62" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">键值对</strong>:键值对，其中的值可以是复杂的混合数据结构(如:文档)<strong class="ih hj">例如:Redis，Voldemort </strong></li><li id="d06e" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">文档存储</strong>:用于文档(每个文档中可以不同)，半结构化:通常是键和值(json格式)。例如:MongoDB，CouchDB </li><li id="f082" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated"><strong class="ih hj">面向图形</strong>:用于命名实体、语义查询、关联数据集。<strong class="ih hj"> Ex: Neo4j，FlockDB </strong></li></ol><p id="7d0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的图片(Databrick的)清楚地提到，Spark可以很容易地连接到任何DB | NO SQL | Flat文件。</p><p id="ea7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark不会提供连接器，而数据库提供商必须提供。可以检查一下Cassandra，Mongo DB等的连接器。，在他们各自的官方网站。</p><p id="f5cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">必须执行以下步骤来完成此用例</p><p id="5b68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1)将数据从每个NOSQL或RDBMS读入Spark-SQL数据帧</p><p id="21a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">示例:从Cassandra读取数据</p><p id="7828" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">val user frame = sqlcontext . read . format(" org . Apache . spark . SQL . Cassandra ")。</p><p id="58d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">选项(Map("table"-&gt;"user_visits "，" keyspace" -&gt; "shafi_join_test "))。负荷</p><p id="ef25" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这样，您就可以从所需的NOSQL或RDBMS中读取数据</p><p id="87c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2)一旦你获得了与供应商(Mongo DB、Cassandra、Mysql、Oracle、SQL Server等)相关的所有所需数据框架。,)</p><p id="8765" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">iii)您可以合并数据帧或rdd</p><p id="08b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">iv)一旦您将所有需要的数据框合并到一个RDD或数据框中，您就可以通过应用sql查询或rdd转换开始进行分析(如果您正在使用rdd)。</p><p id="e09a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">v)您可以在任何需要的地方导出结果。</p><p id="e517" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">vi)您可以使用任何BI工具以图形方式查看结果。</p><p id="bdc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请记住，除非你有大量的数据，否则不要使用Spark。</p><p id="6e84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">Spark的第二个用例是流媒体</strong>:</p><p id="96ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在开始流用例之前，让我们详细讨论一下流。</p><p id="8912" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">什么是流</strong>:并行处理大量数据具有巨大的优势，有时也有一些用例需要更实时(或接近实时)的数据处理，即在数据到达时进行处理，而不是通过批处理。这种处理称为“流处理”。</p><p id="7334" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">连续处理传入数据的流处理流；这些流将处理数据，只要它们保持运行，因为它与对固定数据集进行操作的批处理相反。</p><p id="f0ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">批处理—对固定数据集进行操作。</p><p id="848b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">流处理-对到达数据进行操作。</p><p id="6769" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">流处理框架</strong>:流行的流处理框架有Apache Spark、Apache Flume、Samza、Flink、Storm、Apache Kinesis stream、Apache Apex。</p><p id="298d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">流处理的一些例子是:</p><p id="4233" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分析社交媒体馈送(例如:检测更新趋势)、分析金融馈送(例如:异常检测)、分析机器数据馈送(例如:在异常情况下发出警报)、分析视频游戏使用馈送等。,</p><p id="c4e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我们所提到的，流处理指的是连续处理传入数据的系统，并且将继续处理传入数据，直到应用程序停止。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ld"><img src="../Images/552902db79491e1dcd2ee54af4d7e958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*NSu8-EO2oEIfYlnKFvlwBA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">各种应用的流处理</figcaption></figure><p id="8318" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用例是找出网上购物市场中最畅销的产品(类别和品牌)。这个用例将及时更新产品销售到仪表板，比如说几个小时。当数据及时到达时，将进行流处理，然后更新到仪表板。卖方需要此使用案例，以便计划提前预订商品，供客户使用。</p><p id="52bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下步骤要求进行流式处理。</p><ol class=""><li id="a344" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">获取日志文件中的数据:日志文件中的每个条目都包含项目细节，如售出/替换/等等。、、、创建日期时间、用户标识、地址、支付方式等。,</li><li id="3c4a" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">我们将在日志中记录条目，并处理出售、替换等项目。，分开</li><li id="714b" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">处理完数据流后，我们将把信息更新到DWH/DB/HDFS，从这里开始应用程序更新到仪表板。</li></ol><p id="339b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark确实给大数据领域带来了一场革命。Spark能有效利用内存，执行同等任务的速度比Hadoop的MapReduce快100倍。</p><p id="4181" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用spark感觉就像使用本地Scala、Java或Python集合，但是Spark引用分布在许多节点或集群中的数据。</p><p id="2e6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark解决了上述Map Reduce缺点。Spark使用名为内存执行模型的概念，减少了MR每次从磁盘读取的时间，因为与MR jobs相比，Spark可以将作业的执行速度提高100倍。</p><p id="1e97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">阿帕奇演习</strong>:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es le"><img src="../Images/d2eefe7eb757422ab20e8cf292d0587d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*frxpFkCCaDNm3v_deo6ANw.png"/></div></div></figure><p id="d9e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">什么是Apache Drill: Apache Drill是用于Hadoop、NoSQL和云存储的无模式SQL查询引擎。它使用标准的ANSI SQL。</p><p id="8291" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Drill使用一种通用语言<strong class="ih hj">“SQL”方便了对各种不同数据源和格式的特别分析。</strong></p><p id="3c87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Drill是通用的，支持各种文件格式、数据源和云区域。Drill查询文件格式，如CSV、TSV、拼花、PSV或其他分隔数据、JSON、AVRO、Hadoop序列文件、Apache和web服务器日志、日志文件、PCAP、PCAP-NG。</p><p id="3a74" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Drill查询不同的数据源，如Hive，Hbase，Kafka的流数据，Mongo DB，Map-R DB，开放式时间序列数据库，任何具有JDBC驱动程序的RDBMS。</p><p id="cb49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，Drill支持分布式文件系统，如HDFS、MapR-FS、AWS -S3。</p><p id="4291" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Drill使只具备SQL或BI工具(如Tableau)知识的分析师能够分析和查询他们的数据，而不必转换数据或将其移动到集中的数据存储。</p><p id="35b3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">钻机的动力</strong>:</p><p id="1c1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是<strong class="ih hj">敏捷性</strong>:无需数据加载、模式创建和维护、转换等，您就可以获得更快的洞察力。, )</p><p id="45fa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">灵活</strong>:您可以分析非关系数据存储中的多结构和嵌套数据，您可以直接工作，无需转换或限制数据。</p><p id="225d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">熟悉度</strong>:熟悉SQL和BI工具(Tableau、Qlikview、Excel等。,)</p><p id="285b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Drill不需要在查询数据之前创建模式，这也很容易做到。您可以使用Drill查询原始数据。它提供了复杂数据的JSON文档模型和列执行引擎。</p><p id="471f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">连接不同数据存储的查询</strong>:单个查询可以连接来自多个数据存储的数据。例如，您可以将MongoDB中的用户配置文件集合与Hadoop中的事件日志目录结合起来。</p><p id="8284" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">演练用例:从非结构化数据源收集数据，并对数据进行分析。</p><p id="4aa0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用例描述:在我们的用例中，数据已经以json文件格式从不同的系统中提取出来。我们需要在集群中分析这些数据(由于json文件形式的大规模数据)。</p><p id="a54d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是执行数据分析的步骤:</p><ol class=""><li id="ed90" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">检查已经预先定义的工具，如Hive，Pig和Spark。</li><li id="1900" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">比方说hive:您需要在即席查询之前首先定义模式，所以Hive不适合</li><li id="7476" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">Pig:它是一种脚本语言:你必须热衷于拉丁脚本，并且需要将复杂的查询划分到多个语句中。</li><li id="2d75" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">Spark Core (or )Spark — SQL:您已经清理了数据，然后定义了模式。</li><li id="91ed" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">如果您考虑钻取，那么没有必要检查上述语句，只需对文件进行查询，并立即获得查询结果。例如:查询json数据:选择SELECT timestamp FROM '<strong class="ih hj">JSON/sales . JSON `</strong>其中location _ id = ' 1</li><li id="1c49" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">使用标准SQL语句执行所需的分析，最后连接到任何BI工具以可视化数据。就这些。</li><li id="908c" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">通过使用Apache Drill只需要标准的SQL。如果您有大规模的数据，那么您需要形成钻取集群，然后进行分析。</li></ol><p id="6227" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后一步是在云中执行，我选择AWS弹性MapReduce分布。</p><p id="d614" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae kr" href="https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-launch.html" rel="noopener ugc nofollow" target="_blank">https://docs . AWS . Amazon . com/EMR/latest/release guide/EMR-spark-launch . html</a>—点击此链接创建集群、配置spark、hadoop等</p><ol class=""><li id="8479" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">使用您的凭证登录AWS控制台<a class="ae kr" href="https://console.aws.amazon.com/elasticmapreduce/" rel="noopener ugc nofollow" target="_blank">https://console.aws.amazon.com/elasticmapreduce/</a></li><li id="27e3" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">点击“创建集群”-&gt;给出集群名称，选择启动模式、s3位置、软件配置、硬件配置、安全性等。，以下是截图。如果您想自己选择软件，请单击“进入高级选项”。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lf"><img src="../Images/403dab244fa34975f8fe92242800ac3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oyo7aeGU29EXK4UWDhpv9Q.png"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lg"><img src="../Images/f983ef6d25a13bc11233f6319a38774a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JFVcIGWLWKPQ5K26V7HOGw.png"/></div></div></figure><p id="54ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在AWS中执行应用程序后，请确保您必须停止集群，否则计费将继续，直到您停止集群。</p><p id="c52d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在配置集群和节点数量的选择上有些力不从心，作为架构师，您必须考虑数据大小、执行时间、完成此任务所需节点数量的计算等。，注意这里要多加小心。</p><p id="2a85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">可视化数据</strong>:数据分析完成后，BI团队或分析师开始将分析数据可视化，以图片的形式呈现数据。有很多方法可以将数据可视化到不同的报告中(图形、直方图、条形图、饼图等)。,).市场上有很多可用的工具，如Qlikview、Cognos、Tableau、SAP BI等。,</p><p id="48cc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">何去何从</strong>:以独立模式启动do R &amp; D，阅读Spark和Hadoop相关工具的文档，根据你要思考的用例&amp;考虑哪些工具可以满足你的需求。尝试创建您自己的用例并应用于Spark &amp; Hadoop，遵循不同的架构和样本，成为专家。</p><p id="19bc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，我没有把代码分享给用例，只是高层次的架构和通常的方法。许多事情需要涵盖，但在一个单一的博客无法涵盖。</p><p id="243c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读这篇文章，请留下评论或错误等。,</p></div></div>    
</body>
</html>