<html>
<head>
<title>Multicollinearity — How to fix it?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多重共线性-如何解决？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multicollinearity-how-to-fix-it-905b110d1968?source=collection_archive---------15-----------------------#2019-10-19">https://medium.com/analytics-vidhya/multicollinearity-how-to-fix-it-905b110d1968?source=collection_archive---------15-----------------------#2019-10-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/3e64699bff523b60a32f9010e02969e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/0*vfp6kQVYykZUUVrZ"/></div></figure><p id="682d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这篇文章将回答诸如什么是多重共线性？多重共线性会产生哪些问题？何时必须修复多重共线性？以及如何修复多重共线性？</p><p id="9bae" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">回归时我们必须注意的一个重要方面是多重共线性。查看这篇<a class="ae jk" href="https://machinelearningmind.com/2019/10/06/introduction-to-linear-regression-e-commerce-dataset/" rel="noopener ugc nofollow" target="_blank">文章</a>，找到多元线性回归和因变量/自变量的解释。</p><p id="df67" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">只是复习一下，</p><ul class=""><li id="ac98" class="jl jm hi io b ip iq it iu ix jn jb jo jf jp jj jq jr js jt bi translated">因变量是我们想要预测的变量。</li><li id="a2b2" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">自变量是用来预测因变量的变量。</li><li id="a252" class="jl jm hi io b ip ju it jv ix jw jb jx jf jy jj jq jr js jt bi translated">我们回归的目标是找出哪些自变量可以用来预测因变量。</li></ul><h2 id="5a9c" class="jz ka hi bd kb kc kd ke kf kg kh ki kj ix kk kl km jb kn ko kp jf kq kr ks kt bi translated">什么是多重共线性？</h2><p id="ae8e" class="pw-post-body-paragraph im in hi io b ip ku ir is it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj hb bi translated">让我们看看什么是多重共线性，以及为什么我们应该担心它。</p><p id="3480" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">一个变量成为独立变量的条件之一是它必须<em class="kz">独立于其他变量</em>。也就是说，我们不能用其他独立变量来推导这个变量的值。</p><p id="bfa9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">正如我们在我以前的文章中看到的，因变量相对于自变量的方程可以写成</p><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es la"><img src="../Images/c60c2142d68d16e479f0958a879fbdb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/0*-qQ1n_Gv4YrOVrSn"/></div></figure><p id="f0c6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们举一个贷款数据的例子。</p><p id="5e23" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果X1 =贷款总额，X2 =本金，X3 =利息。</p><p id="d109" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以通过(X2 + X3)求出X1的值。这表明X1、X2和X3之间存在很强的多重共线性。</p><p id="b89e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们的自变量(X1)不完全是<em class="kz">独立的</em>。</p><p id="9706" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果你看一下这个等式，你可以看到X1和m1在一起，m1是X1的系数。</p><p id="cf0a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于线性回归，系数(m1)代表当你保持<em class="kz">所有其他自变量不变</em>时，自变量(X1)每变化1个单位，因变量(y)的平均变化。</p><p id="6eff" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">例如，在<a class="ae jk" href="https://machinelearningmind.com/2019/10/14/feature-elimination-using-p-values/" rel="noopener ugc nofollow" target="_blank">上一篇</a>中，我们看到预测医疗费用的等式为<br/> <strong class="io hj">【预测费用】</strong> = ( <strong class="io hj">年龄</strong>x 255.3)+(<strong class="io hj">BMI</strong>x 318.62)+(<strong class="io hj">儿童</strong> x 509.21) + ( <strong class="io hj">吸烟者</strong> x 23240) -( <strong class="io hj">地区_东南</strong></p><p id="f058" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在<strong class="io hj">吸烟者</strong>的情况下，系数为23240。这意味着如果这个人是吸烟者，预测的费用将增加23，240英镑，如果这个人是不吸烟者，预测的费用将减少23，240英镑(假设所有其他变量都不变)。</p></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h2 id="3fd4" class="jz ka hi bd kb kc kd ke kf kg kh ki kj ix kk kl km jb kn ko kp jf kq kr ks kt bi translated">多重共线性会产生哪些问题？</h2><p id="6ce0" class="pw-post-body-paragraph im in hi io b ip ku ir is it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj hb bi translated">在我们的贷款例子中，我们看到X1是X2和X3的总和。由于这种关系，当X1发生变化时，我们不能期望X2或X3的值是<strong class="io hj"> <em class="kz">常数</em> </strong>。<br/>所以，在这种情况下我们不能完全相信系数值(m1)。我们不知道X1对因变量的确切影响。</p><p id="dc16" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在文章<a class="ae jk" href="https://machinelearningmind.com/2019/10/14/feature-elimination-using-p-values/" rel="noopener ugc nofollow" target="_blank">使用p值进行特征消除</a>中，我们讨论了p值以及我们如何使用该值来查看特征/自变量是否具有统计显著性。<br/>由于多重共线性会降低系数的准确性，我们可能无法信任p值来识别具有统计显著性的独立变量。</p></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h2 id="2cbe" class="jz ka hi bd kb kc kd ke kf kg kh ki kj ix kk kl km jb kn ko kp jf kq kr ks kt bi translated">何时必须修复多重共线性？</h2><p id="8026" class="pw-post-body-paragraph im in hi io b ip ku ir is it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj hb bi translated"><strong class="io hj">好消息</strong>是多重共线性仅影响系数和p值，但不影响模型预测因变量的能力。</p><p id="4482" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这意味着，如果您只关心预测值，就不必担心多重共线性。但在一些商业案例中，我们实际上必须关注单个自变量对因变量的影响。那么在这种情况下，我们必须减少数据中的多重共线性。</p><p id="b3aa" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们了解了什么是多重共线性，以及它会导致什么问题。现在我们来看看如何修复它。</p></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><h2 id="e6e1" class="jz ka hi bd kb kc kd ke kf kg kh ki kj ix kk kl km jb kn ko kp jf kq kr ks kt bi translated">如何修复多重共线性？</h2><p id="ad78" class="pw-post-body-paragraph im in hi io b ip ku ir is it kv iv iw ix kw iz ja jb kx jd je jf ky jh ji jj hb bi translated">一旦确定多重共线性对您来说是一个问题并需要解决它，您就需要关注方差膨胀因子(VIF)。VIF值帮助我们识别独立变量之间的相关性。</p><p id="7128" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">开始之前，您必须了解VIF的范围以及它表示的多重共线性的级别。</p><p id="5e5d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们通常试图将多重共线性保持在中等水平。因此，我们必须确保自变量具有VIF值&lt; 5.</p><p id="edb1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Let’s take the loan data example.</p><p id="fd59" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Loan data has the following columns,<br/> <strong class="io hj"> loan_amnt </strong>:批准的贷款金额<br/> <strong class="io hj"> total_pymnt </strong>:迄今支付的总金额<br/> <strong class="io hj"> total_rec_prncp </strong>:迄今支付的本金总额<br/> <strong class="io hj"> total_rec_int </strong>:迄今支付的利息总额<br/> <strong class="io hj">期限</strong>:贷款期限<br/> <strong class="io hj"/></p><pre class="lb lc ld le fd lm ln lo lp aw lq bi"><span id="ccc2" class="jz ka hi ln b fi lr ls l lt lu"># Load csv into DataFrame <br/>data = pd.read_csv("loan.csv") </span><span id="6a99" class="jz ka hi ln b fi lv ls l lt lu"># Data Cleanup <br/>data.term = data['term'].apply(lambda x:int(x.replace('months',''))) data.int_rate = data.int_rate.apply(lambda x:float(x.replace('%',''))) <br/>data = data[data['loan_status']!='Current'] <br/>data['loan_status'] = data['loan_status'].map({'Charged Off':1, 'Fully Paid':0}) </span><span id="d2fd" class="jz ka hi ln b fi lv ls l lt lu"># Separating independent variables into X and dependent variable into y <br/>X = data[['loan_amnt','total_pymnt','total_rec_prncp','total_rec_int','term','int_rate']] <br/>y = data['loan_status'] <br/>X = sm.add_constant(X) </span><span id="576d" class="jz ka hi ln b fi lv ls l lt lu">X.head()</span></pre><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/638ac021ecdf1264db9f9f074692a4f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/0*r76chz5FREA8sfXN"/></div></figure><p id="5d5a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了了解变量之间的相关性，我们使用了heatmap()</p><pre class="lb lc ld le fd lm ln lo lp aw lq bi"><span id="bbac" class="jz ka hi ln b fi lr ls l lt lu">sns.heatmap(X.corr(), annot=True)</span></pre><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es if"><img src="../Images/439a87ce42d9c19454b3994f60c0613e.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/0*XEFuz4ttEFk6JtSl"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">绝对值越高，相关性越强</figcaption></figure><p id="989c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">但是，当列数很高时，这是不可行的。</p><p id="3728" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们计算每个独立列的VIF值。</p><pre class="lb lc ld le fd lm ln lo lp aw lq bi"><span id="e8a8" class="jz ka hi ln b fi lr ls l lt lu"># Function to calculate VIF<br/>def calculate_vif(data):<br/>    vif_df = pd.DataFrame(columns = ['Var', 'Vif'])<br/>    x_var_names = data.columns<br/>    for i in range(0, x_var_names.shape[0]):<br/>        y = data[x_var_names[i]]<br/>        x = data[x_var_names.drop([x_var_names[i]])]<br/>        r_squared = sm.OLS(y,x).fit().rsquared<br/>        vif = round(1/(1-r_squared),2)<br/>        vif_df.loc[i] = [x_var_names[i], vif]<br/>    return vif_df.sort_values(by = 'Vif', axis = 0, ascending=False, inplace=False)</span></pre><p id="059e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们拟合一个线性回归模型，并检查系数。此外，计算VIF值</p><pre class="lb lc ld le fd lm ln lo lp aw lq bi"><span id="0872" class="jz ka hi ln b fi lr ls l lt lu">lm = sm.OLS(y, X).fit() <br/>print("Coeffients: \n{0}".format(lm.params)) <br/>calculate_vif(X)</span></pre><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/acacaa8f20be067eae9c29b20adf049c.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/0*sUHdckMBox2u0P4J"/></div></figure><p id="6df4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在请忽略“const”列。我们可以看到很低的系数，因为这些变量对因变量的影响很小。但我们不是来讨论这个的。让我们关注VIF价值观。</p><p id="3bd1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以看到<strong class="io hj"> total_pymnt </strong>，<strong class="io hj"> total_rec_prncp </strong>，<strong class="io hj"> total_rec_int </strong>有VIF &gt; 5(极端多重共线性)。这是显而易见的，因为<strong class="io hj">total _ pym nt = total _ rec _ pr NCP+total _ rec _ int</strong></p><p id="137a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了减少多重共线性，让我们删除具有最高VIF的列并检查结果。</p><pre class="lb lc ld le fd lm ln lo lp aw lq bi"><span id="69c7" class="jz ka hi ln b fi lr ls l lt lu"># Dropping total_pymnt as VIF was highest <br/>X.drop(['total_pymnt'], axis=1, inplace=True) </span><span id="cd86" class="jz ka hi ln b fi lv ls l lt lu">lm = sm.OLS(y, X).fit() <br/>print("Coeffients: \n{0}".format(lm.params))</span><span id="8583" class="jz ka hi ln b fi lv ls l lt lu">calculate_vif(X)</span></pre><figure class="lb lc ld le fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/97995c0d4f592319dffaf87c59684eba.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/0*kg4i9LsmJI5KDPsO"/></div></figure><p id="b124" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果你注意到，删除'<strong class="io hj"> total_pymnt </strong>'只改变了与它相关的变量的VIF值(<strong class="io hj"> total_rec_prncp </strong>，<strong class="io hj"> total_rec_int </strong>)。</p><p id="b3c3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">减少多重共线性前后的自变量系数。<br/>两者之间有显著变化。<br/><strong class="io hj">total _ rec _ pr NCP:</strong>-0.000089-&gt;-0.000069<br/><strong class="io hj">total _ rec _ int:</strong>-0.000007-&gt;0.000015</p><p id="062a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因此，最终我们成功地将多重共线性降低到中等水平，现在我们的因变量有了VIF &lt; 5.</p><p id="f7d8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">That’s it for this post!.</p></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><p id="43cf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Here’s my <a class="ae jk" href="https://github.com/fahadanwar10/LinearRegression" rel="noopener ugc nofollow" target="_blank"> GitHub </a>用于Jupyter笔记本的线性回归。请随时查看，并在回复中提出更多减少多重共线性的方法。</p><p id="8895" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">感谢您的阅读！</p><p id="4528" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">请在<a class="ae jk" href="http://www.medium.com/@fahadanwar10" rel="noopener"> Medium </a>查看我的其他帖子并关注我。</p></div><div class="ab cl lf lg gp lh" role="separator"><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk ll"/><span class="li bw bk lj lk"/></div><div class="hb hc hd he hf"><p id="038e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kz">原载于2019年10月19日</em><a class="ae jk" href="https://machinelearningmind.com/2019/10/19/multicollinearity-how-to-fix-it/" rel="noopener ugc nofollow" target="_blank"><em class="kz">https://machinelearningmind.com</em></a><em class="kz">。</em></p></div></div>    
</body>
</html>