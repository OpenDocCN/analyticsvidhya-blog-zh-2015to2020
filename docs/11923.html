<html>
<head>
<title>Decision Trees on mark! || Need — Why — When || Quick Hands-On || Conclude</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">马克的决策树！||需要—为什么—何时||快速动手||总结</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/decision-trees-on-mark-need-why-when-quick-hands-on-conclude-ce10dac51e3?source=collection_archive---------17-----------------------#2020-12-23">https://medium.com/analytics-vidhya/decision-trees-on-mark-need-why-when-quick-hands-on-conclude-ce10dac51e3?source=collection_archive---------17-----------------------#2020-12-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="912d" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">决策树的直觉！</h1><p id="62f5" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我妹妹总是说，我很坏…说到撒谎！我问她怎么能得出这样的结论。她引用了以下几点:</p><ul class=""><li id="8356" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated"><strong class="jf hj">嗯，当你撒谎的时候，你在说这句话的时候会露出一丝微笑，这意味着你在积极主动地思考你正在编造的故事。</strong></li><li id="d6fa" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated"><strong class="jf hj">然后你突然加深了你的声音，这是一个很大的迹象！… </strong>等等。</li></ul><p id="c24e" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">我的目的不是告诉你我是如何撒谎的😅，但是使用这个例子，现在假设<strong class="jf hj">两个特征</strong>为<strong class="jf hj">微笑 _ 而 _ 说谎(</strong>真或假<strong class="jf hj"> ) </strong>和<strong class="jf hj">沉重 _ 声音(</strong>真或假<strong class="jf hj">)</strong>她就能够推断出<strong class="jf hj">目标变量(<em class="ku">说谎或不说谎</em> ) </strong>。</p><blockquote class="kv kw kx"><p id="a7d0" class="jd je ku jf b jg kd ji jj jk ke jm jn ky kr jq jr kz ks ju jv la kt jy jz ka hb bi translated">因此，决策树是一种基于<em class="hi">树的算法，它在“if-else”语句逻辑条件下工作。</em>它既可以用于<strong class="jf hj">分类问题，也可以用于</strong>回归问题。此外，决策树算法的输入可以是连续的，也可以是分类的。</p></blockquote><h1 id="b697" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">DT 中涉及的组件:</h1><ul class=""><li id="4422" class="kb kc hi jf b jg jh jk jl jo lb js lc jw ld ka ki kj kk kl bi translated"><strong class="jf hj">根节点</strong>:这是任何决策树创建的基础节点。该属性使用通常称为分裂技术的统计方法最终确定，以降低特定特征的数据集中的杂质。</li><li id="c13c" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated"><strong class="jf hj">内部节点</strong>:仍在进程中或仍有可能被分裂出去的子节点。</li><li id="6f71" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated"><strong class="jf hj">叶节点</strong>:没有更多子节点的最终节点，使得最大杂质已经被模型移除。</li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es le"><img src="../Images/3141fc333fe988d102bf1bf492e1943a.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*ZZ5jUKaIGLCvwPyUQ6TKig.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">决策树命名法</figcaption></figure></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><h1 id="196c" class="if ig hi bd ih ii lx ik il im ly io ip iq lz is it iu ma iw ix iy mb ja jb jc bi translated">DT 中的分裂技术；</h1><ul class=""><li id="3a19" class="kb kc hi jf b jg jh jk jl jo lb js lc jw ld ka ki kj kk kl bi translated">ID3--&gt;(<em class="ku">迭代二分法 3 </em>)</li><li id="c603" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">购物车-&gt; ( <em class="ku">分类和回归树</em>)</li></ul><p id="a4a6" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">这是决定在哪个特征上执行<strong class="jf hj">分割</strong> <strong class="jf hj">的两个主要标准。而且，这一切都归结为<strong class="jf hj"> <em class="ku">分裂</em> </strong>时获得的杂质。因此，在分割之后，我们得到一个更加同质的分割，然后为根节点选择的特性就是我们想要的。</strong></p><p id="ca83" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">决策树的一般算法可以描述如下:</p><ol class=""><li id="b3ea" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka mc kj kk kl bi translated"><em class="ku">挑选最佳属性/特征。最佳属性是能够最好地拆分或分离数据的属性。</em></li><li id="e640" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">询问相关的问题。</li><li id="cce2" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated"><em class="ku">遵循答案路径。</em></li><li id="7527" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">继续第一步，直到找到答案。</li></ol><p id="6898" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">最好的拆分是将两个不同的标签分成两组。大概是这样的:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es md"><img src="../Images/cf578a246399fc3f7672dadb0d6ba51a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oJAz2-xOoxTa8wt66He_Ag.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">DT 步骤的直观表示</figcaption></figure><p id="a7c6" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">现在，我们如何继续选择<strong class="jf hj">最佳分裂特征</strong>，我如何决定<strong class="jf hj">天气</strong>是导致最低杂质的特征。嗯，我们有两个<strong class="jf hj">指标</strong>来计算。</p><h2 id="d6bd" class="mi ig hi bd ih mj mk ml il mm mn mo ip jo mp mq it js mr ms ix jw mt mu jb mv bi translated">决策树的假设:</h2><ul class=""><li id="b97a" class="kb kc hi jf b jg jh jk jl jo lb js lc jw ld ka ki kj kk kl bi translated">最初，所有的训练集都被认为是根。</li><li id="1f66" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">特征最好是分类的，如果是连续的，那么它们就是离散的。</li><li id="fa32" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">记录基于属性值递归分布。</li><li id="3e48" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">通过使用<em class="ku">统计</em> ( <strong class="jf hj"> <em class="ku"> ID3 或 CART </em> </strong>)方法来完成在根节点或内部节点中的属性</li></ul><h2 id="3b95" class="mi ig hi bd ih mj mk ml il mm mn mo ip jo mp mq it js mr ms ix jw mt mu jb mv bi translated">迭代二分法 3:</h2><blockquote class="kv kw kx"><p id="1243" class="jd je ku jf b jg kd ji jj jk ke jm jn ky kr jq jr kz ks ju jv la kt jy jz ka hb bi translated">ID3 是一种自上而下生长树的贪婪算法，在每个节点选择最能对本地训练示例进行分类的属性。这个过程继续进行，直到树完美地分类了训练样本，或者直到所有属性都被使用。</p></blockquote><p id="6e80" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">简而言之，该算法的步骤是:</p><ol class=""><li id="4157" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka mc kj kk kl bi translated">选择最佳属性→ A。</li><li id="151a" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">分配<strong class="jf hj">一个</strong>作为<strong class="jf hj">节点</strong>的决策属性(<strong class="jf hj">测试用例</strong>)。</li><li id="5922" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">对于 A 的每个值，创建一个新的<strong class="jf hj">节点</strong>的后代。</li><li id="1682" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">将定型示例排序到适当的后代节点叶。如果<em class="ku">示例被完美分类为</em>，那么<strong class="jf hj">停止</strong>否则迭代新的叶节点。</li></ol><p id="6091" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">ID3 使用<strong class="jf hj">信息增益</strong>作为杂质的评估指标。为了精确定义信息增益，我们需要定义一个信息论中常用的叫做<em class="ku">熵</em> 的<strong class="jf hj">度量，它度量一组例子中<em class="ku">杂质</em>的水平。数学上，它被定义为:</strong></p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es mw"><img src="../Images/cb81182449f20c34cae29ecb970e0906.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*yM1XYauGwscd78PFJOdjLg.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">计算杂质的熵</figcaption></figure><p id="d1b8" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">现在，给定熵作为训练样本样本中杂质的度量，我们现在可以将<em class="ku"> i </em> <strong class="jf hj"> <em class="ku">信息增益</em>定义为属性在分类训练数据</strong>中的有效性的度量。信息增益，<strong class="jf hj"> <em class="ku">一个属性 A 的增益(S，A) </em>，相对于一个示例的样本<em class="ku"> S </em> </strong>，定义为:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es mx"><img src="../Images/243e3b0f2ded7c984902628a3d8a7080.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*gaJG_-QIfCMr8aovfem2Qw.png"/></div></figure><p id="cd7c" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">其中<em class="ku">值(A) </em>是属性 A 的所有可能值的集合，而<strong class="jf hj"> Sv </strong>是属性<em class="ku"> A </em>具有值<em class="ku"> v </em>的<em class="ku"> S </em>的子集。所以，简而言之:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es my"><img src="../Images/6a09546637d1c7d98e5caf02d8001a12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2OcO5LQE8_mNaKRFUZ2dKg.png"/></div></div></figure><p id="c106" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">数学示例:<strong class="jf hj">我们想弄清楚，<em class="ku"> A1 或 A2 </em>，哪个属性通过给出最高的信息增益度量来给我们最好的分割！？</strong></p><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es mz"><img src="../Images/9aaf443040a6d4814f33a22e8d5e5553.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BfZ_JOdIXeDJ4_Ghm1s8mQ.png"/></div></div></figure><p id="9e6d" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">因此，<strong class="jf hj"> Gain(S，A1) </strong>比<strong class="jf hj"> Gain(S，A2) </strong>具有更高的熵，这表明在<strong class="jf hj">属性 A1 </strong>上发生分裂时获得了更多的同质性。</p><h2 id="a288" class="mi ig hi bd ih mj mk ml il mm mn mo ip jo mp mq it js mr ms ix jw mt mu jb mv bi translated">CART(分类和回归树):</h2><p id="c719" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">CART 是将节点分成根/内部节点的另一种技术。它使用实际上是成本函数的 GINI 指数的概念来减少杂质。<em class="ku">就我个人而言，我觉得 GINI 指数/得分比 ID3 或 C4.5/5.0 更友好，也更容易获得</em></p><p id="b842" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">它被给出为:</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es na"><img src="../Images/934cf202c4bec8d255b6af614a73a3e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*xZ0MLfk1EFFEQwPtnqM5aA.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated"><strong class="bd ih">p(I)</strong>——选择类别为<strong class="bd ih"> i </strong>的数据点的概率</figcaption></figure><p id="2d44" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">该算法的工作原理如下:</p><ul class=""><li id="a8ba" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">首先，我们使用“<strong class="jf hj"> 1-(p +q)”计算子节点的<strong class="jf hj"> Gini 杂质</strong>。</strong></li><li id="fcc9" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">然后，我们使用该分裂的每个节点的<strong class="jf hj">加权基尼系数</strong>来计算分裂的<strong class="jf hj">基尼系数</strong>。</li></ul><h2 id="1032" class="mi ig hi bd ih mj mk ml il mm mn mo ip jo mp mq it js mr ms ix jw mt mu jb mv bi translated">快速示例:</h2><p id="c23c" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">因此，如果我们采取同样的上述例子，我想显示如何分裂计算使用 GINI 指数法手工。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nb"><img src="../Images/25df2ff69180b19ed3a1f3d2a1bf13f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*-VM4Jfh5t3fkVIJsnnR6qw.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">GINI 分数的手工实验</figcaption></figure><p id="ab54" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">我想到用手做 GINI 计算，这样读者应该能够得到我试图解释和展示的东西！。</p><p id="74e4" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">所以，我们做了一个主题回顾:</p><ul class=""><li id="e7c8" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">决策树介绍和直觉</li><li id="f88e" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">决策树的组成部分</li><li id="42f0" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">决策树的假设</li><li id="e31c" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">拆分方法(<em class="ku"> ID3 </em>和<em class="ku"> CART </em></li><li id="5c55" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">分裂技术的解释和数学计算</li></ul><p id="6b41" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">现在，我最喜欢的部分来了。</p></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><h1 id="1888" class="if ig hi bd ih ii lx ik il im ly io ip iq lz is it iu ma iw ix iy mb ja jb jc bi translated">动手操作:</h1><p id="9a99" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">出于实验和实际操作的目的，我们将使用<strong class="jf hj">天平数据集</strong>，其中目标变量是一个多类标签(L- &gt; <strong class="jf hj">左</strong>，B- &gt; <strong class="jf hj">天平</strong>，R- &gt; <strong class="jf hj">右</strong>)。</p><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nc"><img src="../Images/e746bf3b0cb53d4ed8975849d9e1bc3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*slN-GaeqPzKGQqbKymqi8A.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">秤</figcaption></figure><p id="92be" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">此处，目标是根据<em class="ku">左重量、左距离、右重量、右距离等特征预测天平尖端移动的方向。</em></p><p id="164e" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">附上<a class="ae nd" href="https://github.com/mishra-atul5001/Data-Science-Medium-Content/blob/master/Decision%20Trees%20handsOn.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>的链接。这个数据集来自<strong class="jf hj"> <em class="ku"> UCI 机器学习知识库。</em>T41】</strong></p><p id="efbc" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">我们处理这个问题的方法如下:</p><ol class=""><li id="194f" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka mc kj kk kl bi translated">最初，我们将尝试查看是否需要进行任何数据插补/检查缺失值/数据类型/信息等。</li><li id="9e47" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">然后，我们将看到目标变量水平的分布作为一个推论，然后是一些可视化！</li><li id="93fe" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">接着建模，然后检查作为我们<strong class="jf hj">评估指标的<strong class="jf hj"> F1 分数</strong>！</strong></li><li id="6378" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">然后，我们将使用<strong class="jf hj"> GridSearchCV </strong>对我们的<strong class="jf hj">决策树模型</strong>进行一点改进，作为超参数调整的一部分，以寻求更好的结果。</li><li id="5e0e" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">然后我们以结果和评论结束。</li></ol><h2 id="069a" class="mi ig hi bd ih mj mk ml il mm mn mo ip jo mp mq it js mr ms ix jw mt mu jb mv bi translated">加载/分析和准备数据！：</h2><ul class=""><li id="4167" class="kb kc hi jf b jg jh jk jl jo lb js lc jw ld ka ki kj kk kl bi translated">在加载数据时，我们发现缺少列名，这在我们进行任何类型的分析时都是一个问题。因此在熊猫中，如果我们知道序列，我们可以传递一个列名为<strong class="jf hj">的列表</strong>。</li></ul><pre class="lf lg lh li fd ne nf ng nh aw ni bi"><span id="fb10" class="mi ig hi nf b fi nj nk l nl nm">balance_data = pd.read_csv('<a class="ae nd" href="https://archive.ics.uci.edu/ml/machine-learning-'+'databases/balance-scale/balance-scale.data',sep=" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-'+'databases/balance-scale/balance-scale.data',sep=</a> ',', header = None,<br/>names =<strong class="nf hj">['class_name','left_weight','left_distance','right_weight','right_distance'])</strong></span></pre><ul class=""><li id="438c" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">我们的数据看起来像这样:</li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nn"><img src="../Images/a0b675fe58a2a15c348fd5573ba0f38e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*LEwqDK-Dc0k-hznKw0HwWA.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">balance_data.head()</figcaption></figure><ul class=""><li id="c71c" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">继续，我们查看了加载的数据集的<strong class="jf hj">形状</strong>和<strong class="jf hj">信息</strong>！</li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es no"><img src="../Images/ab83dcbed99d56f46a527c12499bce78.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*xdyKoiUJKqoY_u9CdcvFZQ.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">print("数据集长度: "，len(balance_data)) <br/> print("数据集形状: "，balance _ data . Shape)<br/>print(balance _ data . info())</figcaption></figure><ul class=""><li id="45fa" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">所以，形状是<strong class="jf hj"> (625，5) </strong>和<strong class="jf hj">，没有空值，看起来是</strong>。此外，由于这是一个标准数据集，我们的<strong class="jf hj">预测变量</strong>是数字格式并且<strong class="jf hj">目标变量</strong>是一个<strong class="jf hj">多类变量！</strong></li><li id="f14e" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">让我们看看我们的<strong class="jf hj">目标变量，即 class_name </strong>中有多少个级别，以及它们的<strong class="jf hj">分布对这 625 行贡献了多少！</strong></li></ul><pre class="lf lg lh li fd ne nf ng nh aw ni bi"><span id="00ba" class="mi ig hi nf b fi nj nk l nl nm"># Exploring the Target Variable</span><span id="a043" class="mi ig hi nf b fi np nk l nl nm">target_var = balance_data.iloc[:,0]<br/>print(target_var.value_counts())</span><span id="410c" class="mi ig hi nf b fi np nk l nl nm"># Let's check the Class Name Distribution</span><span id="26c1" class="mi ig hi nf b fi np nk l nl nm">print('% i Class L',round(target_var.value_counts()[0]/len(target_var)*100,2))<br/>print('% i Class R',round(target_var.value_counts()[1]/len(target_var)*100,2))<br/>print('% i Class B',round(target_var.value_counts()[2]/len(target_var)*100,2))</span></pre><ul class=""><li id="5a57" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">结果:</li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nq"><img src="../Images/74a75088564c139d61a40c6d0aa85c94.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*fKicsGCG4ACihJXaCsHAGA.png"/></div></figure><ul class=""><li id="3be9" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">实例数:<strong class="jf hj"> 625 </strong> ( <em class="ku"> 49 平衡，288 左，288 右</em>)</li><li id="ae1a" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">类别分布:</li></ul><ol class=""><li id="862b" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka mc kj kk kl bi translated"><strong class="jf hj">为 46.08% </strong>为 L</li><li id="d6c7" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated"><strong class="jf hj">为 46.08% </strong>为 R</li><li id="c3ff" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated"><strong class="jf hj">为 7.84% </strong>为 B</li></ol><ul class=""><li id="8189" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">所以，我们可以看到<strong class="jf hj"> <em class="ku">天平</em> </strong>天平出现的次数非常少。我们应该认为这是<strong class="jf hj">不平衡的情况吗？</strong> <em class="ku">在评论区告诉我和补救方法一样！</em></li><li id="69bf" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">既然一切看起来都很好，我们将尝试一个<strong class="jf hj"> pair plot </strong>来理解特性之间的依赖关系！</li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nr"><img src="../Images/cb727770e2a5b4ff0a9628a56001decf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RRy2ZXXz6ktOIJGL6zQJkw.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx translated"># pair plot<br/>SNS . pair plot(balance _ data)；<br/> plt.title('成对绘图！');</figcaption></figure><ul class=""><li id="1692" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">显然，我们可以看到特性之间没有依赖关系！。同样，我想<em class="ku">分享一小段代码</em>，使用它<strong class="jf hj">您可以找到数据框中缺失值的数量，以及缺失值的百分比和该特征的类型！</strong></li></ul><pre class="lf lg lh li fd ne nf ng nh aw ni bi"><span id="7c92" class="mi ig hi nf b fi nj nk l nl nm"># Checking for Missing values</span><span id="8d63" class="mi ig hi nf b fi np nk l nl nm">def check_missing_values_function(df):<br/>    missing_values = df.isnull().sum()<br/>    missing_values_percent =round(100*(df.isnull().sum()/len(df)),2)<br/>    dtype = df.dtypes<br/>    info_df = pd.DataFrame({'Missing Value Count':missing_values,'% of Missing':missing_values_percent,'DTYPE': dtype})<br/>    info_df.sort_values(by = '% of Missing',inplace=True,ascending=False)<br/>    return info_df</span><span id="add6" class="mi ig hi nf b fi np nk l nl nm">check_missing_values_function(balance_data) # Calling the function on your DF</span></pre><ul class=""><li id="97f4" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">结果:</li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es ns"><img src="../Images/2670869fc3d9de0430da564009666a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*rCFLJjkd_wvLDzdI5CR4Ag.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated"><strong class="bd ih">长得好看呐！！</strong></figcaption></figure><ul class=""><li id="fe17" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">现在，为了支持我们的<em class="ku"> pairplot 独立结果，我们也用热图绘制了一个关联矩阵！</em></li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nt"><img src="../Images/2bed9c20063440f0387263c93b81de7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*TMQ0khW0VK-T5wp-StRHVw.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">sns.heatmap(balance_data.corr()，annot = True)；<br/> plt.xticks(旋转= 30) <br/> plt.yticks(旋转= 30) <br/> plt.title('相关图！');</figcaption></figure><h2 id="7492" class="mi ig hi bd ih mj mk ml il mm mn mo ip jo mp mq it js mr ms ix jw mt mu jb mv bi translated">建模:</h2><ul class=""><li id="fd00" class="kb kc hi jf b jg jh jk jl jo lb js lc jw ld ka ki kj kk kl bi translated">在建模之前，如果你认为我现在可能已经对数据进行了缩放，那么我就去建模！。好吧，来回答这个:<strong class="jf hj"> <em class="ku">决策树和随机森林不受特征量的影响，因此它不是必需的。</em>T3】</strong></li><li id="4b68" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">为什么？:嗯，<strong class="jf hj">树的一个节点通过比较一个特征</strong>(最好地分割数据集)和一个阈值<strong class="jf hj"/>将你的数据分割成 2 组。<strong class="jf hj"> <em class="ku">阈值</em> </strong>没有规则化(因为要保持树的高度小)，所以不受不同尺度的影响。</li><li id="dbe3" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">现在，让我们分开我们的数据。让我们按 70:30 的格式来定吧</li></ul><pre class="lf lg lh li fd ne nf ng nh aw ni bi"><span id="d958" class="mi ig hi nf b fi nj nk l nl nm"># Separating the target variable <br/>predictor_variables = balance_data.values[:, 1:5] <br/>target_variables = balance_data.values[:, 0]</span><span id="1b65" class="mi ig hi nf b fi np nk l nl nm"># Splitting the dataset into train and test <br/>X_train, X_test, y_train, y_test = train_test_split(predictor_variables, target_variables, test_size = 0.3, random_state = 100)</span></pre><ul class=""><li id="c398" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">让我们开始建立我们的模型！。从<strong class="jf hj">推车/GINI 索引</strong>方法开始。</li></ul><pre class="lf lg lh li fd ne nf ng nh aw ni bi"><span id="3ef2" class="mi ig hi nf b fi nj nk l nl nm"># Modelling using the GINI INDEX Score methodology<br/>clf_gini = DecisionTreeClassifier(criterion = "gini",<br/>                                  random_state = 100,max_depth=3, <br/>                                  min_samples_leaf=5)</span><span id="5f05" class="mi ig hi nf b fi np nk l nl nm"># Performing training <br/>clf_gini.fit(X_train, y_train)</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nu"><img src="../Images/a9df9f91c73f46b7a590b20aa100472c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*UHuU1C-mGTtVHGIJ7pTr6A.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">GINI 指数 DT 分类器</figcaption></figure><ul class=""><li id="1c4d" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">为了评估任何一种结果。我们的衡量标准应该是什么！。是的，你答对了！。我们正在讨论将<strong class="jf hj">准确性和 F1 分数</strong>作为我们的评估指标。所以，让我们看看他们如何使用<strong class="jf hj">混淆矩阵</strong>，这样我们就可以得到最终结果的要点！</li></ul><pre class="lf lg lh li fd ne nf ng nh aw ni bi"><span id="cdce" class="mi ig hi nf b fi nj nk l nl nm"># Making predictions on the TEST SET<br/>y_pred = clf_gini.predict(X_test)</span><span id="fca8" class="mi ig hi nf b fi np nk l nl nm"># Since, this is a MultiClass Classification Problem, We need to see the CLASSIFICATION REPORT<br/>print("Confusion Matrix: ")<br/>print(confusion_matrix(y_test, y_pred))<br/>print()<br/>print ("Accuracy : ",accuracy_score(y_test,y_pred)*100) <br/>print()<br/>print("Report : ", classification_report(y_test, y_pred))</span></pre><ul class=""><li id="c3dc" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">结果:</li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nv"><img src="../Images/ae180d5b3a69dd16c07f6772f2c8a68a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*eLWvEgGHS6mrumKjFenXzQ.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">混淆矩阵</figcaption></figure><ul class=""><li id="c5c2" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">所以，我们得到<strong class="jf hj">准确性</strong>在某处<strong class="jf hj"> 73% </strong>和<strong class="jf hj">F1-分数</strong>在<strong class="jf hj"> 51% </strong>左右，这是相当低的！</li><li id="7c98" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">让我们试试<strong class="jf hj">熵/ID3 </strong>方法，看看我们是否能得到更好的结果！</li></ul><pre class="lf lg lh li fd ne nf ng nh aw ni bi"><span id="0bed" class="mi ig hi nf b fi nj nk l nl nm"># Decision tree with entropy <br/>clf_entropy = DecisionTreeClassifier(criterion = "entropy", <br/>                                     random_state = 100,max_depth = 3, <br/>                                     min_samples_leaf = 5)</span><span id="bfbf" class="mi ig hi nf b fi np nk l nl nm"># Performing training<br/>clf_entropy.fit(X_train, y_train)</span><span id="f5e1" class="mi ig hi nf b fi np nk l nl nm"># Making predcitions on the TEST SET<br/>y_pred = clf_entropy.predict(X_test)</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nw"><img src="../Images/c71e0204ceea090948835f1460865d1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*S_lOm2imkEfPnFbxIVoATA.png"/></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">熵分裂的 DT 分类器</figcaption></figure><ul class=""><li id="9619" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">测评结果！</li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es nx"><img src="../Images/f2181b21b23b293d878325a7744e68be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*xhngcwxWeefXjOlWceDE7A.png"/></div></figure><ul class=""><li id="0510" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">哦！哦！减少—减少—减少。现在你能想到别的什么，我们能在哪里增加我们的<strong class="jf hj">准确性</strong>。🤔💭</li></ul></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><h2 id="f484" class="mi ig hi bd ih mj mk ml il mm mn mo ip jo mp mq it js mr ms ix jw mt mu jb mv bi translated">超参数调谐！</h2><ul class=""><li id="ccb0" class="kb kc hi jf b jg jh jk jl jo lb js lc jw ld ka ki kj kk kl bi translated">现在，在我们开始构建一个更好的模型之前，我们应该选择哪种<strong class="jf hj"> <em class="ku">分割方法？</em> </strong> <em class="ku">这是</em><strong class="jf hj"><em class="ku">GINI</em></strong><em class="ku">方法，因为这是我们的模型分裂得更好的地方，我们能够实现高端结果！</em></li><li id="54fe" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">有多种方法可以执行这种调整。最短和最好的方法之一是使用<strong class="jf hj"> GridSearchCV </strong>并应用某些参数的值范围。</li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es ny"><img src="../Images/517cc1c754946ea2c307cea7c1c5437a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3CgPZYgVMvvAaKpN_sWz4g.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">CV 方法论的可视化工作！</figcaption></figure><ul class=""><li id="d73c" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">这里我们将使用我们的评估指标作为<strong class="jf hj"> F1 分数！</strong>。所以当我们要看最好成绩的时候，我们指的是 F1 的分数，而不是准确性！</li></ul><pre class="lf lg lh li fd ne nf ng nh aw ni bi"><span id="9a03" class="mi ig hi nf b fi nj nk l nl nm">param_grid = {<br/>    'max_depth': [2,5,8,11,14,17,20],<br/>    'min_samples_leaf': [5,10,15,20],<br/>    'min_samples_split': [10,20,30],<br/>    'max_features': [1, 4]<br/>}</span><span id="2c86" class="mi ig hi nf b fi np nk l nl nm">dt_hyper = DecisionTreeClassifier(class_weight='balanced',criterion = "entropy")</span><span id="e6c2" class="mi ig hi nf b fi np nk l nl nm">grid_search = GridSearchCV(estimator = dt_hyper, param_grid = param_grid,refit=['f1_score'] ,<br/>                          cv = 5, n_jobs=-1, return_train_score=True,verbose=1)<br/>grid_search.fit(X_train, y_train)</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es nz"><img src="../Images/d78bc42f39c4496dcd82df2fe02c7810.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V3y0NfJ_R8ikbX5_ykg9rg.png"/></div></div></figure><ul class=""><li id="98dd" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">结果:</li></ul><pre class="lf lg lh li fd ne nf ng nh aw ni bi"><span id="d283" class="mi ig hi nf b fi nj nk l nl nm">print('We can get accuracy of',round(grid_search.best_score_,2),'using',grid_search.best_params_)</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es oa"><img src="../Images/9772aa57e633f6520dc36cddfa62e344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1lDnz_3D3RWEiWaBUT-_Og.png"/></div></div></figure><ul class=""><li id="bfee" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">让我们建立我们的最终模型:</li></ul><pre class="lf lg lh li fd ne nf ng nh aw ni bi"><span id="f1d8" class="mi ig hi nf b fi nj nk l nl nm"># Modelling using the GINI INDEX Score methodology<br/>dt_final_model = DecisionTreeClassifier(criterion = "gini",random_state = 100,max_depth=11, min_samples_leaf=5,min_samples_split=10,max_features = 4)</span><span id="1ea0" class="mi ig hi nf b fi np nk l nl nm"># Performing training <br/>dt_final_model.fit(X_train, y_train)</span><span id="1dbf" class="mi ig hi nf b fi np nk l nl nm"># Making predcitions on the TEST SET<br/>y_pred = dt_final_model.predict(X_test)</span><span id="ee36" class="mi ig hi nf b fi np nk l nl nm"># Since, this is a MultiClass Classification Problem, We need to see the CLASSIFICATION REPORT<br/>print("Confusion Matrix: ")<br/>print(confusion_matrix(y_test, y_pred))<br/>print()<br/>print ("Accuracy : ",round(accuracy_score(y_test,y_pred)*100,2)) <br/>print()<br/>print("Report : ", classification_report(y_test, y_pred)) <br/>print()<br/>print('F1 Score: ',round(f1_score(y_test, y_pred, average='micro')*100,2))</span><span id="2668" class="mi ig hi nf b fi np nk l nl nm"># 'micro':<br/># Calculate metrics globally by counting the total true positives, false negatives and false positives.</span></pre><ul class=""><li id="7e7c" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">最终混乱矩阵与结果！</li></ul><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es ob"><img src="../Images/8c9ea6104c395ebe91e1723e8339d5ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*am_GIlXtJEWTMqSYk164gQ.png"/></div></figure><p id="9c79" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">太好了！实际上，我们得到了一个更好的工作模型，准确率超过了 81%。现在，如果我们在行数方面有更多的数据，这个复杂的模型可以了解更多！。</p><ul class=""><li id="fe79" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">这种超调可以在任何类型的模型上执行，无论是随机森林/逻辑回归/XG Boost 等。</li></ul><h1 id="ea37" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结论</h1><ul class=""><li id="886c" class="kb kc hi jf b jg jh jk jl jo lb js lc jw ld ka ki kj kk kl bi translated">最后，在我们做 EDA/建模/超调的这个阶段，特性怎么样？</li><li id="2a25" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">我们如何知道哪个特性对建模更重要。我们能以某种方式看出<strong class="jf hj">特征的重要性吗？</strong></li><li id="b41b" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">是的，使用<strong class="jf hj">T3 . model . feature _ importances _T5</strong></li></ul><pre class="lf lg lh li fd ne nf ng nh aw ni bi"><span id="2939" class="mi ig hi nf b fi nj nk l nl nm"># Creating feature name vs feature value df<br/>features = [dt_final_model.feature_importances_]<br/>feature_names = ['left_weight','left_distance','right_weight','right_distance']<br/>feature_imp_df = pd.DataFrame(columns=feature_names,data= features).reset_index(drop=True)</span><span id="892c" class="mi ig hi nf b fi np nk l nl nm">feature_df = pd.DataFrame(feature_imp_df.T.reset_index())<br/>feature_df = feature_df.rename(columns = {'index':'feature_names',0:'feature_imp_value'}).sort_values(by = 'feature_imp_value').reset_index(drop=True)<br/>feature_df</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div class="er es oc"><img src="../Images/9e7bd126e718bde1a636caeaff95a1c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*Vd-7nyMZ348PR05664DeHg.png"/></div></figure><ul class=""><li id="c1cc" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">我们可以看到，建模时最重要的特征是<strong class="jf hj">右 _ 距离</strong>，最不重要的是<strong class="jf hj">右 _ 重量。</strong></li><li id="19f0" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">让我们计划一下吧！</li></ul><pre class="lf lg lh li fd ne nf ng nh aw ni bi"><span id="4854" class="mi ig hi nf b fi nj nk l nl nm">plt.figure(figsize=(12,8))<br/>sns.barplot(x = feature_df.feature_names,y = feature_df.feature_imp_value);<br/>plt.rcParams.update({'font.size': 22})</span></pre><figure class="lf lg lh li fd lj er es paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="er es od"><img src="../Images/77abeb30111973bc8075164cbd1f7431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GqsPdWi0IyZqFlR6eydW0A.png"/></div></div><figcaption class="lm ln et er es lo lp bd b be z dx translated">特征重要性图！</figcaption></figure><ul class=""><li id="47f8" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">您可以看到，当我们使用<strong class="jf hj">熵值法</strong>时，存在<strong class="jf hj"> 3% </strong>的精确度差异，但是，F1 分数，也就是我们应该寻找的权衡，似乎更适合每个职业！</li><li id="7860" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">我们尝试了网格搜索，然后使用 GINI 方法，找到最好的超参数！</li><li id="eaae" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated">在这里，我们的超参数调整更侧重于获得更好的宏观 F1 分数！-&gt; <strong class="jf hj">总体 72% </strong></li></ul></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><h1 id="3558" class="if ig hi bd ih ii lx ik il im ly io ip iq lz is it iu ma iw ix iy mb ja jb jc bi translated">DT 的优点:</h1><ol class=""><li id="8bd0" class="kb kc hi jf b jg jh jk jl jo lb js lc jw ld ka mc kj kk kl bi translated">决策树需要较少的数据准备工作。</li><li id="9ea9" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">如上所述，决策树不需要数据的标准化/缩放。</li><li id="d050" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">数据中缺少的值也不会在很大程度上影响构建决策树的过程。</li><li id="5303" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">他们不受离群值的影响！。这是一个很大的安慰。</li><li id="71e9" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">决策树模型非常直观，易于向技术团队和利益相关者解释。</li><li id="c505" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">人们可以得到特征重要性作为结果来解释模型的驱动因素并得出结论性的结果。</li></ol><h1 id="7264" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">DT 的缺点:</h1><ol class=""><li id="92d4" class="kb kc hi jf b jg jh jk jl jo lb js lc jw ld ka mc kj kk kl bi translated">数据的微小变化会导致决策树结构的巨大变化，从而导致不稳定。<strong class="jf hj">高方差！</strong></li><li id="07c8" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">决策树训练相对昂贵，因为复杂性和花费的时间更多。</li><li id="d1db" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">决策树算法不适用于应用回归和预测连续值。</li><li id="9b5f" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka mc kj kk kl bi translated">决策树有时倾向于<strong class="jf hj">过度拟合</strong>，因此我们需要对多个 DT 的结果进行平均，这导致了<strong class="jf hj">随机森林的概念！</strong></li></ol><ul class=""><li id="46dc" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated">大部分<strong class="jf hj">面试问题</strong>仅从<em class="ku">正反</em>中提到的几点提出。像<strong class="jf hj">从 CART 和 ID3 中拆分节点这样的技术问题请解释！</strong></li></ul></div><div class="ab cl lq lr gp ls" role="separator"><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv lw"/><span class="lt bw bk lu lv"/></div><div class="hb hc hd he hf"><p id="1b40" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">我希望你喜欢这篇带有简要解释的日志，并能快速掌握决策树算法的要点！</p><p id="3be0" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">如果你相信的话，请鼓掌，在那之前请保持关注，保持安全，保持微笑。！</p><p id="a446" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kr jq jr js ks ju jv jw kt jy jz ka hb bi translated">给我接通电话:</p><ul class=""><li id="b7d9" class="kb kc hi jf b jg kd jk ke jo kf js kg jw kh ka ki kj kk kl bi translated"><a class="ae nd" href="https://github.com/mishra-atul5001" rel="noopener ugc nofollow" target="_blank"> Github </a></li><li id="1378" class="kb kc hi jf b jg km jk kn jo ko js kp jw kq ka ki kj kk kl bi translated"><a class="ae nd" href="https://www.linkedin.com/in/atul-mishra-5001/" rel="noopener ugc nofollow" target="_blank">领英</a></li></ul><h1 id="a86b" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">快乐学习🌟</h1></div></div>    
</body>
</html>