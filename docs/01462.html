<html>
<head>
<title>Multi Domain NLP using Bidirectional LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用双向LSTM的多域自然语言处理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/multi-category-multi-domain-nlp-90690eda607a?source=collection_archive---------21-----------------------#2019-10-23">https://medium.com/analytics-vidhya/multi-category-multi-domain-nlp-90690eda607a?source=collection_archive---------21-----------------------#2019-10-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/99779355a2455e00e8e830f15c6d9e30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WvsR9s2LbnXK_mV5cG0-XA.jpeg"/></div></div></figure><p id="2697" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我开始这个项目是想知道我们是否可以给<strong class="is hj">下一个单词预测</strong>添加不同的上下文，或者也称为语言建模的任务是预测下一个单词是什么。</p><p id="1a48" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">大多数<strong class="is hj">下一个单词预测</strong>只能预测一种类型的上下文。当预测一个句子时，用两个不同类型的数据集一起训练，这个句子通常没有意义。</p><p id="10ea" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了简单起见，让我们从具有3种类别双域下一个单词预测器开始。以预测每个类别下一个单词以及混合类别的下一个单词预测。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jo"><img src="../Images/7be85e0811bc7e4d21f90510198f69e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*CkDVxa0zYEQS_McOmJdS3A.png"/></div></figure><h1 id="9397" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">双域(下一个单词预测器)</h1><h2 id="4b80" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jb ky kz kh jf la lb kl jj lc ld kp le bi translated">使用的数据集:</h2><p id="1dcd" class="pw-post-body-paragraph iq ir hi is b it lf iv iw ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn hb bi translated">数据集A:笑话(来自？？？添加链接)<br/>数据集B:报价(来自？？？添加链接)</p><p id="ec5c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">相似的数据集在做双域(下一个单词预测器)时会失去它的目的。因此，在这种情况下，使用两个非常不同的数据集“笑话”和“引用”数据集。以便人们能够以不同的风格和语调来识别预测的句子。</p><blockquote class="lk ll lm"><p id="63de" class="iq ir ln is b it iu iv iw ix iy iz ja lo jc jd je lp jg jh ji lq jk jl jm jn hb bi translated">数据集A: sample →“为什么从来没见过大象躲在树上？因为他们非常擅长这个"<br/>数据集B: sample →"答应我你会永远记得:你比你相信的更勇敢，比你看起来的更强大，比你想象的更聪明"</p></blockquote><p id="ec35" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将准备同一个数据集的两个副本，其中一个数据集的<strong class="is hj">标记为类别</strong>，其他数据集的<strong class="is hj">标记为无类别。</strong>我们总共将使用4组数据集。</p><blockquote class="lk ll lm"><p id="93b2" class="iq ir ln is b it iu iv iw ix iy iz ja lo jc jd je lp jg jh ji lq jk jl jm jn hb bi translated">T <!-- -->总数据集=已标记笑话+未标记笑话+已标记报价+未标记报价。</p></blockquote><h2 id="3d1b" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jb ky kz kh jf la lb kl jj lc ld kp le bi translated">处理数据集</h2><p id="f101" class="pw-post-body-paragraph iq ir hi is b it lf iv iw ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn hb bi translated">让我们清理数据集并对其进行记号化，为了便于说明，让我们使用长度为4的序列。假设<strong class="is hj"> X1 </strong>总是<strong class="is hj">类别标签</strong>。</p><blockquote class="lk ll lm"><p id="5f53" class="iq ir ln is b it iu iv iw ix iy iz ja lo jc jd je lp jg jh ji lq jk jl jm jn hb bi translated">例如:[X1] [X2] [X3] [X4]预测[Y1] <br/>例如:[类别标签] [第一个词] [第二个词] [第三个词]→[下一个词]</p></blockquote><p id="2dbe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">类别为“<joke>”、“<quote>”和“<notag>”</notag></quote></joke></p><p id="38da" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">数据集引用例如:“答应我你会永远记住:你比你相信的更勇敢，比你看起来的更强大，比你想象的更聪明”</p><blockquote class="lr"><p id="065d" class="ls lt hi bd lu lv lw lx ly lz ma jn dx translated">[# @ Quote][答应][我][你会]→[总是]<br/>[# @ Quote][我][你会][总是]→[记住:]<br/>[# @ Quote][你会][总是][记住:]→[你是]<br/>[# @ Quote&gt;][总是][记住:][你是]→[更勇敢]<br/>[# @ Quote&gt;][记住:][你是][更勇敢]→[比]</p></blockquote><h1 id="13e7" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke mb kg kh ki mc kk kl km md ko kp kq bi translated">型号(LSTM双向)</h1><p id="4464" class="pw-post-body-paragraph iq ir hi is b it lf iv iw ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn hb bi translated"><strong class="is hj">使用的库:</strong> <br/> Keras将是使用的主库。Keras是用Python编写的前端层，运行在其他流行的深度学习工具包之上，如<strong class="is hj"> TensorFlow、Theano </strong>和<strong class="is hj">微软认知工具包(CNTK) </strong>。您用Keras编写的任何代码都可以在这些后端中的任何一个上运行，而无需对代码做任何修改。通过TensorFlow/Theano /CNTK，Keras可以在CPU和GPU上无缝运行。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="ab fe cl me"><img src="../Images/4f4a1a1c761088871e44ae53492f1e2e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*OxAgYCBDKyXYLiWWUoKUcQ.png"/></div></figure><p id="9ca5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在Keras中，我们可以简单地将多层堆叠起来，为此我们需要将模型初始化为<code class="du mf mg mh mi b">Sequential()</code>。</p><h2 id="e9b2" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jb ky kz kh jf la lb kl jj lc ld kp le bi translated">LSTM模式:</h2><figure class="jp jq jr js fd ij er es paragraph-image"><div class="ab fe cl me"><img src="../Images/a5c2ae6129e31e2da530b4b96256525f.png" data-original-src="https://miro.medium.com/v2/format:webp/1*e4_3OBFWnPU7oi0hXBiVWQ.png"/></div></figure><p id="c2d3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">第一个sigmoid激活功能是<strong class="is hj">遗忘门</strong>。应该从先前的小区状态(Ct-1)中忘记哪些信息。第二个sigmoid和第一个tanh激活函数是我们的<strong class="is hj">输入门</strong>。哪些信息应该保存到单元状态或者应该被忘记？最后一个sigmoid是<strong class="is hj">输出门</strong>并突出显示哪些信息应该进入下一个<strong class="is hj">隐藏状态</strong>。</p><h1 id="3be3" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">双向LSTMs</h1><p id="6f6d" class="pw-post-body-paragraph iq ir hi is b it lf iv iw ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn hb bi translated">双向递归神经网络(RNNs)的思想很简单。它包括复制网络中的第一循环层，使得现在有两层并排，然后将输入序列原样作为输入提供给第一层，并将输入序列的反向副本提供给第二层。</p><p id="23eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">双向提供序列的使用最初在语音识别领域是合理的，因为有证据表明整个话语的上下文被用于解释所说的内容，而不是线性解释。</p><p id="2840" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Keras通过<a class="ae mj" href="https://keras.io/layers/wrappers/#bidirectional" rel="noopener ugc nofollow" target="_blank">双向</a>层包装器支持双向LSTMs。这个包装器将一个循环层(例如第一个LSTM层)作为参数。</p><p id="203c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它还允许您指定合并模式，即前向和后向输出在传递到下一层之前应该如何组合。这些选项包括:</p><ul class=""><li id="417c" class="mk ml hi is b it iu ix iy jb mm jf mn jj mo jn mp mq mr ms bi translated"><em class="ln">sum</em>’:输出相加在一起。</li><li id="990e" class="mk ml hi is b it mt ix mu jb mv jf mw jj mx jn mp mq mr ms bi translated"><em class="ln">mul</em>’:输出相乘。</li><li id="8ddf" class="mk ml hi is b it mt ix mu jb mv jf mw jj mx jn mp mq mr ms bi translated"><em class="ln"> concat </em>':输出被连接在一起(默认)，为下一层提供双倍的输出。</li><li id="b18a" class="mk ml hi is b it mt ix mu jb mv jf mw jj mx jn mp mq mr ms bi translated"><em class="ln"> ave </em>':取输出的平均值。</li></ul><p id="f906" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">默认模式是连接，这是双向LSTMs研究中常用的方法。</p><h1 id="dfcd" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">LSTM参数选择</h1><h2 id="4cb6" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jb ky kz kh jf la lb kl jj lc ld kp le bi translated">隐藏节点的数量:</h2><blockquote class="lr"><p id="fb93" class="ls lt hi bd lu lv lw lx ly lz ma jn dx translated">隐藏节点数= Ns/(α∫(Ni+No))</p></blockquote><p id="2b10" class="pw-post-body-paragraph iq ir hi is b it my iv iw ix mz iz ja jb na jd je jf nb jh ji jj nc jl jm jn hb bi translated">Ni =输入神经元的数量。<br/>否=输出神经元的数量。Ns =训练数据集中的样本数。<br/> αα =任意比例因子，通常为2-10。</p><p id="b40f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae mj" href="http://www.solver.com/training-artificial-neural-network-intro" rel="noopener ugc nofollow" target="_blank">建议</a>将alpha设置为5到10之间的值，值2通常不会过度拟合。</p><h2 id="dfa9" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jb ky kz kh jf la lb kl jj lc ld kp le bi translated">辍学率:</h2><p id="5f40" class="pw-post-body-paragraph iq ir hi is b it lf iv iw ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn hb bi translated">每个LSTM层都应该有一个下降层。该层将通过在训练期间忽略随机选择的神经元来帮助防止过度拟合，并因此降低对单个神经元的特定权重的敏感性。20%通常用作保持模型准确性和防止过度拟合之间的良好折衷。</p><h2 id="8f56" class="kr ju hi bd jv ks kt ku jz kv kw kx kd jb ky kz kh jf la lb kl jj lc ld kp le bi translated">激活层:</h2><p id="3122" class="pw-post-body-paragraph iq ir hi is b it lf iv iw ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn hb bi translated">要添加的最后一层是激活层。使用哪种激活功能同样取决于应用。对于我们手头的问题，我们有多个类，但一次只能出现一个类。对于这些类型的问题，一般来说，<a class="ae mj" href="https://en.wikipedia.org/wiki/Softmax_function" rel="noopener ugc nofollow" target="_blank"> softmax激活函数</a>效果最好，因为它允许我们(和你的模型)将输出解释为概率。</p><h1 id="3af1" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak">预测:</strong></h1><p id="cfac" class="pw-post-body-paragraph iq ir hi is b it lf iv iw ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn hb bi translated">给定4个输入，该模型能够预测下一个单词。那么我们如何预测一整句话呢？当生成一个完整的句子时，我们将继续预测下一个完整的句子，直到它形成一个完整的句子。</p><p id="ff41" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">样本:<br/>1)[# @ Joke][我][am][a]→[fan]<br/>2)[# @ Joke][am][a][fan]→[of]<br/>3)[# @ Joke][a][fan][of]→[talking]</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/e411291adf473b196a1c8c01e439ba88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*Kml4tck6Mz5vvaZ21nkS7w.png"/></div></figure><h1 id="56d8" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">(多类别+多领域)自然语言处理</h1><p id="0783" class="pw-post-body-paragraph iq ir hi is b it lf iv iw ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn hb bi translated">让我们再深入一步，一个领域可以有多个类别。让我们将笑话作为域A。域A可以表示为笑话、滑稽、笑声等。域A也可以用一个或多个标签来表示，例如[US]+[笑话]，[dark]+[幽默]+[笑话]。</p><p id="dc46" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用上述两域(下一个单词预测器)方法，将创建大量重复的数据集并增加训练成本。<br/>【美国笑话】带域A数据集的标签。<br/>【黑色幽默】带域A数据集标签。<br/>【黑色幽默笑话】带域A数据集标签。</p><p id="0b6f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果有更多领域和更多标签，我们的多类别+多领域NLP将变得非常低效且难以训练。</p><p id="01a7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因为我们可以预测下一个单词。对标签到域名做预测怎么样？</p><blockquote class="lr"><p id="0030" class="ls lt hi bd lu lv lw lx ly lz ma jn dx translated">[第一标签] [第二标签] [第三标签] [第四标签]→[域]<br/>[空][空][空][# @笑话]→[域A]<br/>[空][空][空]→[域A]<br/>[空][空][# @黑暗][# @幽默]→[域A]<br/>[空][空][# @我们][# @笑话]→[域A]</p></blockquote><p id="f8fa" class="pw-post-body-paragraph iq ir hi is b it my iv iw ix mz iz ja jb na jd je jf nb jh ji jj nc jl jm jn hb bi translated">现在输入将是:<br/>【第一个标签】【第二个标签】【第三个标签】【第四个标签】【第一个字】【第二个字】【第三个字】→【下一个字】</p><p id="35fe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用在两个域中训练的相同模型(下一个词预测器)<br/>第一步:[第一个标签] [第二个标签] [第三个标签] [第四个标签]→[域] <br/>第二步:[预测的域] [第一个词] [第二个词][第三个词]→[下一个词]</p><p id="f2d6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如何达到想要的状态？记住我们之前训练的模型，现在让我们创建一个名为“TaggedDataset”的附加数据集。假设身份域为#D@Domain</p><blockquote class="lr"><p id="4c8a" class="ls lt hi bd lu lv lw lx ly lz ma jn dx translated">样本TagDataSet: <br/> empty，empty，empty，#@joke，#D@joke <br/> empty，empty，#@US，#@joke，#D@joke <br/> empty，empty，#@dark，# @幽默，#D@joke</p></blockquote><p id="66ca" class="pw-post-body-paragraph iq ir hi is b it my iv iw ix mz iz ja jb na jd je jf nb jh ji jj nc jl jm jn hb bi translated">还记得之前用4个特征训练双域(下一个单词预测器)模型来预测下一个工作。现在我们的TagDataSet也使用4个特征来预测领域。标签数据集可以被训练添加到双域(下一个单词预测器)数据集中。</p><h1 id="f33c" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">高级(多类别+多领域)自然语言处理</h1><p id="2a4e" class="pw-post-body-paragraph iq ir hi is b it lf iv iw ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn hb bi translated">让我们再深入一步，4个单词序列没有保留下一个单词的一些句子上下文。长期短期记忆或LSTM循环神经网络能够学习和记忆长序列的输入。</p><p id="cebc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如何在给定较少输入数据的情况下增加句子上下文，以预测下一个单词。如何求解给定的10个特征来预测下一个单词，以及在使用该模型时我们能否使用只有4个特征的输入。</p><p id="23d8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">答案是填充，让我们用长度达到10个序列的序列来训练模型。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ne"><img src="../Images/b83e5e82f53cddf7a901261d8303242b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hTnUdQFDvTkOrf-oC8yX4g.png"/></div></div></figure></div></div>    
</body>
</html>