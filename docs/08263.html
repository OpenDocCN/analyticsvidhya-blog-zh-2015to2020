<html>
<head>
<title>Understanding Recurrent Neural Network (RNN) and Long Short Term Memory(LSTM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解循环神经网络(RNN)和长短期记忆(LSTM)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/undestanding-recurrent-neural-network-rnn-and-long-short-term-memory-lstm-30bc1221e80d?source=collection_archive---------1-----------------------#2020-07-23">https://medium.com/analytics-vidhya/undestanding-recurrent-neural-network-rnn-and-long-short-term-memory-lstm-30bc1221e80d?source=collection_archive---------1-----------------------#2020-07-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="454f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我们将了解循环神经网络和长短期记忆。我们将介绍基本原理及其工作原理。所以让我们先了解一下。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/b40c66c627ee0c54e2e93951f385f872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/0*H6gn0w2taQBQHgFZ"/></div></figure><p id="3b24" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RNN代表循环神经网络。它是一种包含记忆的神经网络，最适合于顺序数据。苹果的Siri和谷歌的语音搜索都使用RNN。让我们讨论一下RNN的一些基本概念。</p><p id="fe8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在深入了解之前，让我们先了解一下<strong class="ih hj">正向传播</strong>和<strong class="ih hj">反向传播</strong></p><h1 id="0a53" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">向前和向后传播</h1><p id="4c78" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated"><strong class="ih hj">前向传播</strong>:这是最简单的一种神经网络。数据只在从输入层到隐藏层再到输出层的正向流动。它可能包含一个或多个隐藏层。所有节点都完全连接。我们进行前向传播以获得模型的输出，并检查其准确性并获得误差。</p><p id="2db2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">反向传播</strong>:反向传播法用于训练神经网络。如果有大量的隐藏层，它可以被称为深度神经网络。一旦正向传播完成，我们计算误差。这个误差然后被反向传播到网络以更新权重。</p><p id="7668" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们通过神经网络反向寻找误差相对于权重的偏导数(损失函数)。这个偏导数现在乘以学习率来计算步长。这个步长被加到原始权重上以计算新的权重。这就是神经网络在训练过程中的学习方式。</p><h1 id="9f16" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">1.什么是RNN？</h1><p id="1840" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">递归神经网络是具有内部存储器的前馈神经网络的推广。RNN本质上是递归的，因为它对每个数据输入执行相同的功能，而当前输入的输出取决于过去的一次计算。产生输出后，它被复制并发送回循环网络。为了做出决定，它考虑当前的输入和从先前的输入中学习到的输出。</p><p id="28d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与前馈神经网络不同，rnn可以使用其内部状态(记忆)来处理输入序列。这使得它们适用于诸如未分段的、连接的手写识别或语音识别之类的任务。在其他神经网络中，所有的输入都是相互独立的。但是在RNN，所有的输入都是相互关联的。</p><p id="c630" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">最适合顺序数据</strong></p><p id="d4ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RNN最适合顺序数据。它可以处理任意的输入/输出长度。RNN使用它的内存来处理任意的输入序列。</p><p id="ad82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这使得RNNs最适合预测单词序列中的下一个单词。就像人类的大脑一样，特别是在对话中，人们更看重信息的新近性来预测句子。</p><p id="b873" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">受过翻译文本训练的RNN可能会知道，如果“dog”前面有单词“hot ”,则应该有不同的翻译。</p><p id="be91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> RNN有内存</strong></p><p id="aca1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RNN有记忆能力。它记忆以前的数据。在做出决定时，它会考虑当前的输入，以及它从先前接收的输入中学到了什么。前一步骤的输出被作为输入提供给当前步骤，形成反馈回路。</p><p id="d11d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，它使用一组当前输入和先前状态来计算其当前状态。这样，信息循环往复。</p><p id="4d2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简而言之，我们可以说RNN有两个输入，现在和最近的过去。这很重要，因为数据序列包含了关于接下来会发生什么的关键信息，这就是为什么RNN可以做其他算法不能做的事情。</p><h1 id="74fb" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">不同类型的RNN有:</h1><ul class=""><li id="1e06" class="kp kq hi ih b ii kk im kl iq kr iu ks iy kt jc ku kv kw kx bi translated"><strong class="ih hj">一对一RNN </strong></li><li id="3632" class="kp kq hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">一对多RNN </strong></li><li id="c2cb" class="kp kq hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">多对一RNN </strong></li><li id="c887" class="kp kq hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated"><strong class="ih hj">多对多RNN </strong></li></ul><p id="f40b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将回顾RNN的基本概念，然后，继续探讨不同类型的RNN，并深入探讨它们。</p><h1 id="596c" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">RNN的类型</h1><p id="a916" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">所以我们建立了递归神经网络，也称为RNNs，是一类允许以前的输出用作输入，同时具有隐藏状态的神经网络。RNN模型主要用于自然语言处理和语音识别领域。让我们看看它的类型:</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ld"><img src="../Images/979ea28525c8e7b10a9fe3e9a437ae63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VO4DW_vN7ldqgEZg.png"/></div></div></figure><h1 id="bfa9" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">一对一RNN</h1><p id="5f0e" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">一对一RNN (Tx=Ty=1)是最基本和最传统的神经网络类型，为单个输入提供单个输出，如上图所示。它也被称为香草神经网络。它用于解决常规的机器学习问题。</p><h1 id="cba1" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">一对多</h1><p id="4020" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">一对多(Tx=1，Ty&gt;1)是一种RNN架构，适用于单输入多输出的情况。其应用的一个基本例子是音乐生成。在音乐生成模型中，RNN模型用于从单个音符(单个输入)生成音乐作品(多个输出)。</p><h1 id="cca3" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">多对一</h1><p id="c3d8" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">多对一RNN架构(Tx&gt;1，Ty=1)通常被视为情感分析模型的常见示例。顾名思义，当需要多个输入来给出单个输出时，使用这种模型。</p><p id="35af" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以Twitter情感分析模型为例。在那个模型中，一个文本输入(单词作为多个输入)给出它的固定情感(单个输出)。另一个例子可以是电影评级模型，该模型将评论文本作为输入，以提供从1到5的电影评级。</p><h1 id="0c3c" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">多对多</h1><p id="fa14" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">显而易见，多对多RNN (Tx&gt;1，Ty&gt;1)体系结构接受多个输入并给出多个输出，但是多对多模型可以有两种，如上所示:</p><p id="df15" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1.Tx=Ty:</p><p id="152f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是指输入层和输出层具有相同大小的情况。这也可以理解为每个输入都有一个输出，在命名实体识别中可以找到一个常见的应用。</p><p id="327a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.Tx！=Ty:</p><p id="ba89" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">多对多体系结构也可以用输入和输出层大小不同的模型来表示，这种RNN体系结构最常见的应用是在机器翻译中。例如，“我爱你”，英语中的三个神奇单词在西班牙语中只能翻译成两个，“te amo”。因此，机器翻译模型能够返回比输入字符串更多或更少的单词，因为在后台存在不相等的多对多RNN体系结构。</p><p id="57db" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">消失和爆炸渐变</strong></p><p id="985d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们先来了解一下什么是梯度？</p><p id="3587" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">梯度</strong>:如上文反向传播部分所述，梯度是相对于其输入的偏导数。梯度衡量的是，如果你稍微改变输入，函数的输出会改变多少。你也可以把梯度想成一个函数的斜率。梯度越高，斜率越陡，模型可以学习得越快。如果斜率几乎为零，模型就停止学习。梯度只是测量所有权重相对于误差变化的变化。</p><p id="c5a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">RNN的梯度问题</strong></p><p id="9ec3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在训练RNN算法时，梯度有时会变得太小或太大。因此，在这种情况下，RNN算法的训练变得非常困难。因此，会出现以下问题:</p><ol class=""><li id="245d" class="kp kq hi ih b ii ij im in iq li iu lj iy lk jc ll kv kw kx bi translated">性能差</li></ol><p id="fc19" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.低精度</p><p id="5e45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.训练周期长</p><p id="1e7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">爆炸梯度:</strong>当我们赋予权重高的重要性时，就会出现爆炸梯度问题。在这种情况下，梯度值变得太大，斜率往往呈指数增长。这可以通过以下方法解决:</p><ol class=""><li id="bb63" class="kp kq hi ih b ii ij im in iq li iu lj iy lk jc ll kv kw kx bi translated">身份初始化</li></ol><p id="3e97" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.截断反向传播</p><p id="ae51" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.渐变剪辑</p><p id="877b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">渐变消失:</strong>当渐变的值太小，模型停止学习或因此花费太长时间时，会出现这个问题。这可以通过以下方法解决:</p><ol class=""><li id="e2cf" class="kp kq hi ih b ii ij im in iq li iu lj iy lk jc ll kv kw kx bi translated">重量初始化</li></ol><p id="3910" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.选择正确的激活功能</p><p id="c4ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.LSTM(长短期记忆)解决渐变消失问题的最好方法是使用LSTM(长短期记忆)。</p><h1 id="e883" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">2.什么是LSTM(长短期记忆)</h1><p id="0be5" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">正如我们所讨论的，RNN不能长时间记忆数据，并开始忘记以前的输入。为了克服这个问题的消失和爆炸梯度LSTM使用。它们被用作短期记忆学习的解决方案。同样在RNN，当添加新信息时，RNN完全修改现有信息。RNN不能够区分重要或不那么重要的信息。而在LSTM，当添加新信息时，对现有信息的修改很小，因为LSTM包含决定信息流的门。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es lm"><img src="../Images/06125dceaf0e9479d4902e927088d122.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*e-dNcg1nvZ7cdH6h.png"/></div></div></figure><p id="f4c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些门决定哪些数据是重要的并且在将来可能是有用的，哪些数据必须被擦除。这三个门是输入门、输出门和遗忘门。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="le lf di lg bf lh"><div class="er es ln"><img src="../Images/cd9c148d8ddde28d23d3b656a8d760e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uxsA-z8ARItF5tPN.png"/></div></div></figure><ul class=""><li id="f097" class="kp kq hi ih b ii ij im in iq li iu lj iy lk jc ku kv kw kx bi translated">遗忘门:这个门决定哪些信息是重要，哪些信息应该被存储，哪些信息应该被遗忘。它去除了神经元细胞中不重要的信息。这导致了性能的优化。这个门有两个输入，一个是前一个单元产生的输出，另一个是当前单元的输入。将所需的偏差和权重相加并相乘，然后将sigmoid函数应用于该值。生成一个介于0和1之间的值，并基于此决定保留哪些信息。如果值为0，遗忘门将删除该信息，如果值为1，则该信息很重要，必须记住。</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lo"><img src="../Images/42f44ceddc6ec514b42ea904946c214e.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/0*Q3gFSOj55lBt-2B3.png"/></div></figure><ul class=""><li id="ddf1" class="kp kq hi ih b ii ij im in iq li iu lj iy lk jc ku kv kw kx bi translated">输入门:这个门用来给神经元细胞增加信息。它负责使用类似sigmoid的激活函数将什么值添加到单元格中。它创建了一个必须添加的信息数组。这是通过使用另一个名为tanh的激活函数来完成的。它生成一个介于-1和1之间的值。sigmoid函数充当过滤器，并调节哪些信息必须添加到单元格中。</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lp"><img src="../Images/f0f1d74d65a7bc3d4f0115d76790fdd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/0*GUx9Ls9TWHWBIqQU.png"/></div></figure><ul class=""><li id="c1d0" class="kp kq hi ih b ii ij im in iq li iu lj iy lk jc ku kv kw kx bi translated">输出门:该门负责从当前单元中选择重要信息并显示为输出。它使用范围从-1到1的双曲正切函数创建一个值向量。它使用以前的输出和当前输入作为调节器，其中也包括sigmoid函数，并决定哪些值应该显示为输出。</li></ul><h1 id="3acd" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">LSTM的挤压/激活功能</h1><ol class=""><li id="668a" class="kp kq hi ih b ii kk im kl iq kr iu ks iy kt jc ll kv kw kx bi translated">逻辑(sigmoid):输出范围从0到1。</li></ol><p id="cd4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.双曲正切(tanh):输出范围从-1到1。</p><h1 id="a1c8" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">提高模型性能的技巧:</h1><ul class=""><li id="e909" class="kp kq hi ih b ii kk im kl iq kr iu ks iy kt jc ku kv kw kx bi translated">我们可以通过添加更多的层来提高我们的模型性能。与具有较少数量的宽层相比，总是优选具有更多(密集)层。</li><li id="e7a7" class="kp kq hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">但是，我们必须注意不要过度拟合数据，为此我们可以尝试使用各种正则化方法。</li><li id="258a" class="kp kq hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">批处理规范化:批处理规范化(batchnorm)是一种提高神经网络性能和准确性的技术。每个批次都会发生批次标准化。这就是为什么，它被称为批量规范化。我们在应用激活函数之前对一层的输出进行归一化(均值= 0，标准偏差= 1)，然后将其馈入神经网络中的下一层。因此，我们不是将网络的输入标准化，而是将网络中每个隐藏层的输入标准化。</li></ul><p id="1cbf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">感谢阅读！将来我也会写更多的<strong class="ih hj">深度学习</strong> <strong class="ih hj">文章</strong>。<a class="ae lq" rel="noopener" href="/@vijay_choubey"> <strong class="ih hj">关注</strong> </a>我来了解一下。我也是一名自由职业者，如果有一些与数据相关的项目的自由职业工作，请随时通过<a class="ae lq" href="https://www.linkedin.com/in/vijay-choubey-3bb471148/" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj"> Linkedin </strong> </a>联系。没有什么比做真正的项目更好的了！</p><h1 id="140a" class="jm jn hi bd jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj bi translated">如果你喜欢这篇文章，请鼓掌！</h1></div></div>    
</body>
</html>