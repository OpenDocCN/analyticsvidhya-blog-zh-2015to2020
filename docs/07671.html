<html>
<head>
<title>Autonomous driving simulations using Twin Delayed Deep Deterministic (TD3) Deep Reinforcement Learning algorithm with LSTM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用具有LSTM的双延迟深度确定性(TD3)深度强化学习算法的自动驾驶模拟</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/autonomous-driving-simulations-using-twin-delayed-deep-deterministic-td3-deep-reinforcement-c846bf6882e5?source=collection_archive---------14-----------------------#2020-07-03">https://medium.com/analytics-vidhya/autonomous-driving-simulations-using-twin-delayed-deep-deterministic-td3-deep-reinforcement-c846bf6882e5?source=collection_archive---------14-----------------------#2020-07-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/74538f60af965ea8392b4288b8b59564.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Px_FjekA_2QnpOw3hL7FeQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">城市地图</figcaption></figure><p id="a86e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">由于复杂的道路场景和密集的周围动态物体，自动驾驶</strong>具有挑战性。因此，高度智能的决策对于处理复杂场景至关重要。</p><p id="23ff" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">Twin Delayed Deep Deterministic(TD3)</strong>算法(<a class="ae js" href="https://arxiv.org/pdf/1706.02275.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1706.02275.pdf</a>)是一种最先进的、非策略的深度强化学习算法，可以高效地应用于连续动作空间，如自动驾驶场景。</p><p id="db2b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">TD3使用行动者评价方法，其中行动者功能指定给定环境当前状态的行动。批评值函数指定一个信号(TD误差)来批评演员所做的动作。TD3使用两个评论家网络和一个演员网络。TD3算法还为每个代理的演员和评论家使用单独的目标神经网络。TD3算法的细节在<a class="ae js" href="https://github.com/monimoyd/P2_S9" rel="noopener ugc nofollow" target="_blank">https://github.com/monimoyd/P2_S9</a>中解释</p><p id="7f2a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这篇文章中，提供了一个带有道路的城市地图以及一辆用于自动驾驶的汽车。目标如下:</p><ol class=""><li id="0c76" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated">让汽车尽可能多的行驶在路上</li><li id="e9d0" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr jy jz ka kb bi translated">在最短的时间内达到目标。</li></ol><p id="05bc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我已经为这个项目使用了Kivy环境</p><p id="5535" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae js" href="https://kivy.org/doc/stable/installation/installation-windows.html" rel="noopener ugc nofollow" target="_blank">https://kivy . org/doc/stable/installation/installation-windows . html</a>。</p><p id="7fc2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Github链接:</p><p id="a9d4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><a class="ae js" href="https://github.com/monimoyd/EndGameAssignment" rel="noopener ugc nofollow" target="_blank">https://github.com/monimoyd/EndGameAssignment</a></p><p id="c6c1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">该项目的亮点如下:</p><ul class=""><li id="537f" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr kh jz ka kb bi translated">创建了一个基于Kivy环境的模拟健身房环境</li><li id="7e83" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">使用双延迟深度确定性(TD3)算法进行训练</li><li id="211d" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">使用基于LSTM和卷积神经网络(CNN)的DNN模型。经验的随机序列被从重放缓冲器中取出作为对LSTM的输入</li><li id="daa3" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">对于状态空间，遮罩与一辆三角形旋转汽车以及一个基于汽车在该步骤中所做操作的编号分数叠加在一起</li><li id="e4bb" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">最初对动作进行随机采样，以填充重放缓冲区</li><li id="0678" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">随机动作和策略动作的混合是基于探索因子ε值来完成的，该探索因子ε值在每一集都减少</li><li id="05fa" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">每两次训练后，进行评估</li><li id="d634" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">在每次培训和评估中都会计算指标(总奖励、上路/越野次数、目标命中次数、边界命中次数)</li><li id="71bc" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">每集存储后的演员和评论家模型</li><li id="1a60" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">根据对收集的评估和培训指标的分析，我选择了合适的测试模型</li><li id="d2ce" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">调整学习率、权重衰减等参数以克服代理循环效应</li></ul><h1 id="f640" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">一.环境</h1><p id="273a" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">代理(即car)根据策略和环境从当前状态采取行动，告知下一个状态并给予奖励。</p><h1 id="d334" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">a.观察状态空间</h1><p id="331e" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">观察状态空间由对应于道路上汽车当前状态的图像组成，如下所示:</p><ul class=""><li id="1b63" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr kh jz ka kb bi translated">80×80的数字阵列是通过在当前汽车位置周围取40个像素由沙子构成的</li><li id="59e6" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">一个等腰三角形(即只有两条边相等)在汽车移动方向上旋转，叠加在图像上。等腰三角形有助于在自然界中发现不对称特征</li><li id="7de7" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">还叠加了一个数字分数(0-5 ),代表汽车在这一步的表现。编号分数如下表所示:</li></ul><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ll"><img src="../Images/8e78bf2107c52a9580160f65d99ea867.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4gLTCAMQ1YOCZCisRY1QMQ.png"/></div></div></figure><p id="b943" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面显示了一些示例图像:</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/42a3854bffa458d87a0be2d51ee6fbbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:160/format:webp/1*SSn-_WHv5hYbXbGhYocgFw.png"/></div></figure><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/9267596b01e7d5aaa9b89d20970fd15c.png" data-original-src="https://miro.medium.com/v2/resize:fit:160/format:webp/1*Idy8KP_sxLOlFPlmiK7sTA.png"/></div></figure><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/9a93eb24e4ce30895dd1e621e3f0a40a.png" data-original-src="https://miro.medium.com/v2/resize:fit:160/format:webp/1*7-z8OQkWiPYMxJAYGn2kSA.png"/></div></figure><p id="5711" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">该图像由CNN和LSTM处理。此外，在状态空间中使用了以下附加属性</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/7ab359fc4573c80e11abd20b7f4141a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1njWGNSsI7CBz0rptpAPag.png"/></div></div></figure><h1 id="f584" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">b.行动空间:</h1><p id="a724" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">的动作空间由两个动作组成。沿x轴的速度</p><p id="d616" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">动作描述旋转沿x轴的旋转角度。它可以有连续的值-3到3个沿x轴的速度位移。它可以具有0.4到2.4之间值</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ls"><img src="../Images/3634b8a7e63b58575dd83fb035a10ea4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aP3RWRioc5a1nvIXNAbLkw.png"/></div></div></figure><p id="d676" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">注意:在策略网络中，策略被实现来为旋转和速度给出-5到5之间的连续值。在应用到汽车之前，该值被标准化为表中为每个动作指定的范围</p><h1 id="62df" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">c.奖励:</h1><p id="7e86" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">奖励是由环境在代理人的每一步给予的</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/fe73c5c16ed1cdfb58c9e8fb7e3efa39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RWfSRFkDVgd5n2_PQdrqrw.png"/></div></div></figure><p id="b746" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">每次训练都有固定的2500步。剧集结束后，变量设置为真。</p><h1 id="4d29" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">二。方法和解决方案</h1><h1 id="9a59" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">I .模拟健身房环境，以封装Kivy环境</h1><p id="96f9" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">Kivy环境没有提供像reset，step这样的方法，这对于任何RL项目来说都是非常容易的。为了解决这个问题，我基于Python提供的多进程队列和事件机制创建了一个与Kivy交互的模拟健身房环境。真实的Kivy环境在一个单独的过程上工作，而TD3训练在一个单独的过程上工作</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lu"><img src="../Images/fabc76bf8d024506ebdfb881a1d88ee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UkmwnrkjtHjGIoa4hzvZLQ.png"/></div></div></figure><p id="1ce4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">TD3列车过程首先启动，它将启动Kivy环境。有一个模拟的体育馆环境，TD3 Train进程可以调用类似env.reset()的方法来重置环境，调用env.step(action)来采取行动并获得下一个状态。内部模拟的健身房环境使用事件和消息队列与真实的Kivy环境交互。</p><h1 id="8aa4" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">二。演员网络</h1><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lv"><img src="../Images/ba993698c150515f8a4ec5b8d780815d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5DGznygZo7l8-bZ1eJ6CoA.png"/></div></div></figure><h1 id="cc88" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">参与者输入:</h1><p id="204f" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">Actor网络将输入作为两个元素的元组i. 80x80 numpy数组，表示沙子，叠加等腰三角形，旋转方向与汽车相同，数字得分(1–5)ii。第二个元素是具有4个参数的Numpy数组，它们是A .汽车的角度b .汽车到目标的负方向c .当前汽车位置到目标的距离和前一汽车位置与目标的距离之差除以4 d .标志on_road，其值1表示汽车在道路上，而-1表示汽车关闭</p><h1 id="6014" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">卷积层:</h1><p id="55ef" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">有5个卷积层用于变换道路像素输入。除了最后一层，每一层都使用16个3×3的滤波器，步长为2，ReLU激活。最后一层具有步幅为1的16个3x3滤波器</p><h1 id="02f1" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">间隙层:</h1><p id="2f9e" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">在转换成16x1x1的5个卷积层之后添加全局平均池层</p><h1 id="1e50" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">LSTM层:</h1><p id="d6a4" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">LSTM层取一维数组并编码成32个矢量张量的隐层</p><h1 id="9485" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">FC层:</h1><p id="b0ac" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">有三个完全连接的层。</p><ul class=""><li id="3139" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr kh jz ka kb bi translated">第一层层采用来自LSTM的隐藏层输出，与4个额外参数附加状态信息(角度、到目标的方向、到目标的距离差、道路标志值)连接，并转换成64个1D张量和应用的ReLU激活</li><li id="0372" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">第二层连接第一层输出，and输出是128 1d张量和应用的ReLU激活</li><li id="78f8" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">第三层输出从第二层转换到1个张量，在该张量上应用tanh并乘以max_action以获得actor输出</li></ul><h1 id="fa8a" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">三。评论家网络</h1><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/4b425b7707ec04dc5baae97d6882bcf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mCKdjiD3pJx2jHf53LQjgg.png"/></div></div></figure><h1 id="9eaf" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">评论家意见:</h1><p id="6759" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">Critic网络将输入作为二元元组i. 80x80 numpy数组，表示沙子，叠加等腰三角形，旋转方向与汽车相同，数字得分为(1–5)ii。第二个元素是具有4个参数的Numpy数组，这些参数是A .汽车的角度b .汽车到目标的方向c .当前汽车位置到目标的距离和前一汽车位置与目标的距离之差除以4 d .标志on_road，其值1表示汽车在道路上，0表示汽车不在道路上i. Actor输出(旋转、速度)</p><h1 id="573d" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">卷积层:</h1><p id="113c" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">有5个卷积层用于变换道路像素输入。除了最后一层，每一层都使用16个3×3的滤波器，步长为2，ReLU激活。最后一层具有步幅为1的16个3x3滤波器</p><h1 id="bc10" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">间隙层:</h1><p id="5514" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">在转换成16x1x1的5个卷积层之后添加全局平均池层</p><h1 id="b77e" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">LSTM层:</h1><p id="1e15" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">LSTM层取一维数组并编码成32个矢量张量的隐层</p><h1 id="d5cd" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">FC层:</h1><ul class=""><li id="889c" class="jt ju hi iw b ix lg jb lh jf lx jj ly jn lz jr kh jz ka kb bi translated">第一层层采取隐藏层输出形式LSTM连接4个额外的状态信息(角度，方向目标，距离目标的差异，道路标志值)和行动(旋转，速度)转换成64 1D张量和应用ReLU激活</li><li id="ace0" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">第二层连接第一层输出，and输出是128 1d张量和应用的ReLU激活</li><li id="657f" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">第三层输出从第二层变换到1张量，这是Critic的Q值</li></ul><h1 id="0c75" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">三。使用的工作流程</h1><p id="ff35" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">下面的流程图显示了所使用的工作流程。</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/9053420925cab515f4a86992c8b4c2a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QYJQPNqql5BFG4U7KCkhgA.png"/></div></div></figure><p id="745e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">一、跑一集2500步二。在每集标记了集号后，执行训练并保存模型，以便可以检索用于后期分析。收集指标iii。每两个训练周期后，评估500步10次，收集指标iv。对指标进行分析，并选择最佳模型</p><h1 id="6843" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">四。培养</h1><h1 id="ac8b" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">a.训练期间使用的超参数</h1><p id="5052" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">我使用Adam optimizer对演员和评论家网络进行了优化，具有以下超级参数:</p><pre class="lm ln lo lp fd mb mc md me aw mf bi"><span id="0fe2" class="mg kj hi mc b fi mh mi l mj mk">Batch Size: 128<br/>    Number of time steps : 500000<br/>    Steps per episode: 2500<br/>    Discount Rate (gamma) : 0.99<br/>    Soft update of target parameters for Polyak Averaging (Tau) : 0.005<br/>    Initial warmup episodes without learning: 10000 timesteps<br/>    Number of learning steps for each environment step : 128<br/>    Exploration Noise : 0.1<br/>    Policy Noise : 0.2<br/>    Actor Learning Rate: 1e-5<br/>    Actor Weight Decay: 0.01<br/>    Critic Learning Rate: 2e-5</span></pre><h1 id="7836" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">b.用于改进培训的技术</h1><p id="aee9" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">为了改进训练，我使用了以下技巧</p><h2 id="c366" class="mg kj hi bd kk ml mm mn ko mo mp mq ks jf mr ms kw jj mt mu la jn mv mw le mx bi translated">一.热身:</h2><p id="c923" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">初始10000个时间戳通过从动作空间中随机选择动作，重放缓冲区被随机填充。这将有助于更好的探索</p><h2 id="53b5" class="mg kj hi bd kk ml mm mn ko mo mp mq ks jf mr ms kw jj mt mu la jn mv mw le mx bi translated">二。LSTM:</h2><p id="756c" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">由于汽车所采取的步骤是一个序列问题，因此在网络中使用LSTM来提高性能。当使用LSTM时，不是从重放缓冲器中随机抽取样本，而是使用经验序列。由于我为每集使用了固定数量的时间步长，因此连续剧集的记录会按顺序存储在重播缓冲区中，使用以下代码可以很容易地获得特定剧集的记录:</p><ul class=""><li id="cd98" class="jt ju hi iw b ix iy jb jc jf jv jj jw jn jx jr kh jz ka kb bi translated">从已完成的剧集列表中随机选择一集。</li><li id="47d1" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">从所选剧集中随机选择一个时间步长。确保选择的时间步长在0和2500–256之间</li><li id="66d0" class="jt ju hi iw b ix kc jb kd jf ke jj kf jn kg jr kh jz ka kb bi translated">我没有选择连续体验，而是选择了两次体验间隔，例如，如果当前播放了100集，随机选择的集是10，开始序列是100，则从重放缓冲区中选择用于训练的记录id是:</li></ul><p id="6d08" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">[100 <em class="my"> 2500 + 100，100 </em> 2500 + 102，100 <em class="my"> 2500 + 104，…100 </em> 2500 + 356]</p><h2 id="34f9" class="mg kj hi bd kk ml mm mn ko mo mp mq ks jf mr ms kw jj mt mu la jn mv mw le mx bi translated">四。勘探系数ε:</h2><p id="fdf1" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">对于剧集探索，我使用了初始化为0.9的epsilon值，并且在每一集之后，epsilon值减少0.05，直到它达到0.2。在0和1之间生成一个随机数，如果它小于ε值，则从随机策略中采取下一个动作，否则从基于DNN的策略网络中采取动作</p><h2 id="23db" class="mg kj hi bd kk ml mm mn ko mo mp mq ks jf mr ms kw jj mt mu la jn mv mw le mx bi translated">动词 （verb的缩写）高斯噪声:</h2><p id="ec8a" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">平均值为0、标准偏差(sigma)为0.1的高斯噪声已添加到探索状态的操作中。</p><h2 id="913a" class="mg kj hi bd kk ml mm mn ko mo mp mq ks jf mr ms kw jj mt mu la jn mv mw le mx bi translated">不及物动词在标记有剧集编号的每集后分别保存模型:</h2><p id="5b39" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">每集结束后，该集使用的模型将分别为演员和评论家保存。由于我已经运行了200集，因此有200个演员和评论家模型的实例被保存。基于在培训期间收集的指标分析结果和在培训2期后完成的评估，有助于决定哪种模型做得最好，以及哪种模型用于部署。</p><h2 id="49c6" class="mg kj hi bd kk ml mm mn ko mo mp mq ks jf mr ms kw jj mt mu la jn mv mw le mx bi translated">动词 （verb的缩写）评估:</h2><p id="e9a8" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">有两种评估模式:</p><p id="28c8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">i. Eval:这在两个训练周期之后使用，以通过仅使用迄今为止学习的策略来评估策略网络。每个评估由500个时间步组成，每个时间步运行10次</p><p id="868c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">二。全面评估(测试):—此模式用于演示和部署。目前，当达到目标时，done变量设置为True。(对于运行全集stop_on_hitting_goal为False。但是理想地，在测试时间而不是随机位置期间，它应该在路上，但是还没有实现)。默认情况下，完全评估处于演示模式，如果不想处于演示模式，可以在map.py中将变量full_eval_demo_mode更改为False</p><h1 id="cbc5" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">三。对收集的指标进行分析</h1><p id="2aa1" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">在每集结束后的培训和评估阶段收集的指标如下:第三次上路。越野计数四。击中边界计数与击中目的地目标计数</p><p id="b303" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对收集的指标进行绘图，如下所示:</p><h1 id="ddc9" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">a.培训情节与总体奖励</h1><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/e40f0471bcc5cbb2804f65349a66ff6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*O0uBnbVFQvCw4WyyxtyyIA.png"/></div></figure><h1 id="1df5" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">分析:</h1><p id="f419" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">从该图可以明显看出，总报酬最初依次增加，在125和160之间达到峰值，然后开始下降</p><h1 id="17b8" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">b.训练情节(即训练后10次对500步进行评估)与平均总奖励的对比</h1><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/2876c570842fbf9fe065b15ebce7d274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*FyVWWUBa-nTSjnHKg_mJow.png"/></div></figure><h1 id="6d9f" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">分析:</h1><p id="2448" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">从该图可以明显看出，总报酬最初依次增加，在120和165之间达到峰值，然后开始下降</p><h1 id="c784" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">c.训练情节vs路上百分比</h1><p id="73c2" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">使用公式计算在途百分比</p><p id="fbc2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">上路百分比=上路次数/(上路次数+越野次数)* 100</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es na"><img src="../Images/41554fff4d3c03c5634c9c3d59fb707a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*i1S9GNye5D5gGBCdb66Htg.png"/></div></figure><h1 id="8d7a" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">分析:</h1><p id="081b" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">从图中可以看出，在路上的百分比是波动的，但随着时间的推移，即使在波动，波动的幅度也会变大。在125到160集左右，振幅处于最高范围，这意味着car在这些集期间表现更好</p><h1 id="8c43" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">d.训练后的评估图(即训练后10次500步的评估)与平均道路百分比</h1><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es na"><img src="../Images/9c8407337b1cbbbd02b0a17250805929.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*T_k5LC6lrsfQ_ye0s6Z4LQ.png"/></div></figure><h1 id="d86c" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">分析:</h1><p id="3bfa" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">从图中可以看出，在路上的百分比随着情节的增加而增加，在125和160之间达到峰值，然后下降。与训练相比，我可以看到更少的波动，所以看起来策略网络更稳定，但是在训练期间，因为我仍然使用随机样本，所以系统可能有点不稳定</p><h1 id="610e" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">e.训练集与边界命中计数的对比图</h1><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es nb"><img src="../Images/2838bc08f60f9e6f160a7384f017246b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*kYR-ON9iFyNR9SvKm6tceg.png"/></div></figure><h1 id="f0e5" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">分析:</h1><p id="fee4" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">从图中，我可以看到边界命中计数的波动，但波动的幅度在最初的情节中更大，但在后来的情节中更小(尽管我也可以看到一些峰值)。</p><h1 id="6f5d" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">f.训练情节(即训练情节后10次评估500步)与平均命中界限的关系图</h1><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es nc"><img src="../Images/36fab17033c602b81bba635c09378faa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*Xig9_9xS1LMp46EVQM4cfw.png"/></div></figure><h1 id="209d" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">分析:</h1><p id="46d4" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">从图中可以明显看出，边界命中次数的平均值随着剧集的增加而降低，在110到150集的范围内最低，之后有一个小的峰值，但在后面的剧集中基本稳定</p><h1 id="2d04" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">g.训练情节与进球次数</h1><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/572a76c19c84aad49bccaba290dc1db8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*h-T3hiJmaOe7ZetYPkLxKA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">分析:</figcaption></figure><p id="3347" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从剧情来看，很明显，在某些情节中进球次数是1，而在其他情况下是0。只有175集左右有目标计数2</p><h1 id="a656" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">h.训练集后的评估图(即训练集后10次500步的评估)与平均边界命中数</h1><figure class="lm ln lo lp fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/3c2a6c80ae4f8026a956b9e6e58865a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*XEKZ8kbLjv13zC1JYsz1WQ.png"/></div></figure><h1 id="a273" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">分析:</h1><p id="a376" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">从剧情来看，很明显，大多数剧集的平均命中率为0，有些剧集为0.1。我可以在第35、125、145、155、175集附近观察到0.2峰值。在接近65和接近200的两种情况下，我可以看到0.3的峰值。</p><h1 id="509a" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">不及物动词最佳模型的选择</h1><p id="cbc8" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">从分析中可以看出，汽车的最佳性能在120和175之间。接下来，我检查了实际指标，发现在第116、120、122、128、130、134、140、142、144、147、148、150集，特别是表演奖励非常好</p><p id="f23e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">接下来，我加载了与该集对应的演员和评论家模型，仅使用策略网络对2500个步骤进行了5次全面评估，并计算了获得的平均奖励，还观察了car的表现，并在如下表格中注意到:</p><figure class="lm ln lo lp fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ne"><img src="../Images/ba6a5a75cfad3d630151e7607d3b1126.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wCPKKyG19lLngty9fAYeLw.png"/></div></div></figure><p id="4455" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从所有的观察中，我发现虽然第120集有最高的奖励，但汽车移动不是很流畅。所以我决定从第147集选择演员，评论家模型，它有很好的平均报酬，汽车移动也很平稳。我使用了第147集的模型进行部署、演示和任何进一步的微调。我还存储了所有其他的模型权重，以便用于将来的分析</p><h1 id="615b" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">四。培训期间面临的问题以及我如何处理</h1><h1 id="c9e1" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">a.汽车撞上边界并停留在那里:</h1><p id="bc33" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">为了处理这种情况，一旦汽车在0像素边界内，我将给予-50的严重惩罚，并将汽车移动到一个随机位置开始。因为我每集都使用固定的步数，所以我在到达边界后继续</p><h1 id="1854" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">b.汽车在同一个地方盘旋:</h1><p id="6a82" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">这是我遇到的主要问题之一。当旋转/速度的动作值变得接近+5或-5并保持不变时，就会出现这种情况。当没有足够的随机样本来探索时会发生这种情况，并且在训练期间，从重放缓冲器中取出样本，这些样本被馈送到策略网络，并且策略网络还没有学习。我采取了以下方法来解决这个问题</p><h1 id="b954" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">a.</h1><p id="99f9" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">我最初取了10000个随机经验，并使用初始化为0.9的探索因子epsilon，每集减少0.05，直到它达到0.2。这确保重放缓冲区中总是有足够的随机经验。</p><h1 id="826f" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">b.</h1><p id="6c62" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">每当360度旋转发生时，无论是顺时针还是逆时针，汽车代理商都会受到惩罚，奖励-50。</p><h1 id="0386" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">c.</h1><p id="dda3" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">Adam优化器的默认学习值为0.001。我把演员的学习率改为1e-5，评论家改为2e-5。我还为演员使用了权重衰减(L2正则化)</p><h1 id="86d1" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">动词 （verb的缩写）未来的改进</h1><h1 id="4886" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">a.为状态空间选择更大的维度:</h1><p id="39b7" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">我选择了一个80x80的沙盘，上面叠加了等腰三角形和国家等级。可以尝试更大的尺寸，比如160x160，这可以提供更好的性能</p><h1 id="7ec2" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">b.多辆汽车:</h1><p id="bdcd" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">为了尝试在道路上行驶的多辆汽车，可以使用MADDPG(多代理深度确定性算法)。</p><h1 id="5105" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">c.卡尔曼滤波器:</h1><p id="88d5" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">代替LSTM，卡尔曼滤波器也可以用于具有更好的性能</p><h1 id="48d4" class="ki kj hi bd kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf bi translated">不及物动词感谢</h1><p id="78d3" class="pw-post-body-paragraph iu iv hi iw b ix lg iz ja jb lh jd je jf li jh ji jj lj jl jm jn lk jp jq jr hb bi translated">本文是基于我在人工智能学院的泛视觉人工智能项目中的项目工作。感谢艾的学校。</p></div></div>    
</body>
</html>