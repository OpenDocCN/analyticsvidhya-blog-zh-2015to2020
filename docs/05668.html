<html>
<head>
<title>Teaching AI Video Games — Deep Q Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能视频游戏教学——深度Q网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/teaching-ai-to-play-games-deep-q-networks-572df36edc9a?source=collection_archive---------18-----------------------#2020-04-28">https://medium.com/analytics-vidhya/teaching-ai-to-play-games-deep-q-networks-572df36edc9a?source=collection_archive---------18-----------------------#2020-04-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/253587cd8a3e30faf6743c7b2f3ad323.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kBDexxPK8vur9DeL"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">卡尔·劳在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="c094" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在今天的文章中，我将向您介绍深度Q网络的热门话题及其工作原理。我将回顾我的模型，并解释构建强化学习算法的关键概念。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/adcd63117b249c5cc29075ff6d8fdaff.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/1*jCMEOUabAOFwNVHp0ot4_Q.gif"/></div></figure><h2 id="2b72" class="jy jz hi bd ka kb kc kd ke kf kg kh ki jg kj kk kl jk km kn ko jo kp kq kr ks bi translated">创建游戏</h2><p id="5ab4" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">在这个游戏中，我们将扮演一架喷气式飞机，它可以采取4种行动——向上、向下、向前、向后——并需要躲避尽可能多的导弹。目标是通过存活最长时间来获得尽可能高的回报。</p><p id="7663" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了创建这个游戏，我使用了pygame，这是一个为编写视频游戏而设计的跨平台Python模块集。</p><p id="79e7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于这个特定的游戏，你不需要太多的模块/库，只需要pygame和内置的模块，比如time和random。</p><p id="818a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们使用精灵设置了3个类；敌人、玩家和云(可选)。完整代码请查看我的Github知识库，了解更多关于pygame的信息请查看本教程:【https://realpython.com/pygame-a-primer/<a class="ae iu" href="https://realpython.com/pygame-a-primer/" rel="noopener ugc nofollow" target="_blank"/></p><h2 id="cc3a" class="jy jz hi bd ka kb kc kd ke kf kg kh ki jg kj kk kl jk km kn ko jo kp kq kr ks bi translated">它是如何工作的？</h2><p id="eb33" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">当谈到强化学习时，它只是一个代理应该如何在一个环境中采取行动，以最大化回报(得分)。Markov决策过程(MDPs)是一个用于建模主体决策的框架。MDP是强化学习的核心概念。要了解基础知识或者什么RL和dqn先看这个:<strong class="ix hj"> <em class="ky"> </em> </strong> <a class="ae iu" rel="noopener" href="/analytics-vidhya/how-i-built-an-algorithm-to-takedown-atari-games-a13d3b3def69"> <strong class="ix hj"> <em class="ky">我是如何构建一个算法来搞垮雅达利游戏的！</em> </strong> </a></p><h2 id="9edb" class="jy jz hi bd ka kb kc kd ke kf kg kh ki jg kj kk kl jk km kn ko jo kp kq kr ks bi translated">马尔可夫决策过程</h2><p id="5a2a" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">马尔可夫决策过程几乎用于每一个强化学习问题。为了掌握MDPs的概念，我们需要研究<strong class="ix hj">马尔可夫性质</strong>和<strong class="ix hj">马尔可夫过程</strong>。</p><blockquote class="kz la lb"><p id="4d94" class="iv iw ky ix b iy iz ja jb jc jd je jf lc jh ji jj ld jl jm jn le jp jq jr js hb bi translated"><strong class="ix hj"> <em class="hi">马氏地产</em> </strong></p></blockquote><p id="2a89" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">马尔可夫性质声明:“给定现在，未来独立于过去”</p><p id="c5aa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正式的定义是:</p><p id="ae26" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">P[St+1|St]=P[St+1|S1，…，St]</p><p id="a86d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">本质上，这告诉我们先前的状态和事件不需要知道未来的状态——当前的状态捕获了所有必要的信息。了解了这一点，我们就能理解价值观并做出决策。</p><blockquote class="kz la lb"><p id="692a" class="iv iw ky ix b iy iz ja jb jc jd je jf lc jh ji jj ld jl jm jn le jp jq jr js hb bi translated"><strong class="ix hj"> <em class="hi">马氏奖励流程</em> </strong></p></blockquote><p id="8b5f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一个MRP就是一个元组(<strong class="ix hj"> <em class="ky"> S </em> </strong>，<strong class="ix hj"> <em class="ky"> P </em> </strong>，<strong class="ix hj"> <em class="ky"> R </em> </strong>，<strong class="ix hj"> 𝛾 </strong>)其中<strong class="ix hj"> <em class="ky"> S </em> </strong>是一个有限状态空间，<strong class="ix hj"> <em class="ky"> P </em> </strong>是状态转移概率，<strong class="ix hj"> <em class="ky"> R </em> </strong>是一个奖励函数其中</p><p id="0b60" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"><em class="ky">r</em></strong><em class="ky">s</em>= 𝔼[<strong class="ix hj"><em class="ky">r</em></strong><em class="ky">t+1</em>|<strong class="ix hj"><em class="ky">s</em></strong><em class="ky">t</em>=<em class="ky">s</em>，</p><p id="9110" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">它表示我们期望在此刻从状态<em class="ky"> S </em>得到多少即时回报，而<strong class="ix hj"> 𝛾 </strong>【伽马】<strong class="ix hj"> </strong>是贴现因子，它告诉我们的代理应该关心未来回报多少。比如gamma为0或者接近于0，我们的代理人就会只关心当前的奖励，变得短视。如果gamma更接近1，我们的代理人会关心未来的回报，并最大化它能生存多久，即使这意味着牺牲短期回报。</p><blockquote class="kz la lb"><p id="360b" class="iv iw ky ix b iy iz ja jb jc jd je jf lc jh ji jj ld jl jm jn le jp jq jr js hb bi translated"><strong class="ix hj"> <em class="hi">马氏决策过程</em> </strong></p></blockquote><p id="37ff" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一个<strong class="ix hj"> <em class="ky">马尔可夫决策过程</em> </strong>是一个马尔可夫回报过程的扩展，因为它包含了一个代理必须做出的决策。环境中的所有状态都是马尔可夫的。</p><p id="2c41" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一个马尔可夫决策过程由一个元组(<strong class="ix hj"> <em class="ky"> S </em> </strong>，<strong class="ix hj"> <em class="ky"> A </em> </strong>，<strong class="ix hj"> <em class="ky"> P </em> </strong>，<strong class="ix hj"> <em class="ky"> R </em> </strong>，<strong class="ix hj"> 𝛾 </strong>)组成</p><p id="9d0d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中:</p><ul class=""><li id="c3dd" class="lf lg hi ix b iy iz jc jd jg lh jk li jo lj js lk ll lm ln bi translated"><strong class="ix hj"> <em class="ky"> S </em> </strong>是一组状态(有限的)</li><li id="6955" class="lf lg hi ix b iy lo jc lp jg lq jk lr jo ls js lk ll lm ln bi translated"><strong class="ix hj"> <em class="ky">一个</em> </strong>是一组动作(有限的)</li><li id="17d6" class="lf lg hi ix b iy lo jc lp jg lq jk lr jo ls js lk ll lm ln bi translated"><strong class="ix hj"> <em class="ky"> P </em> </strong>是一个状态转移的概率(一个新状态将发生的可能性有多大)</li><li id="94ac" class="lf lg hi ix b iy lo jc lp jg lq jk lr jo ls js lk ll lm ln bi translated"><strong class="ix hj"> <em class="ky"> R </em> </strong>是奖励功能(基于动作和状态的积极或消极奖励)</li><li id="d8a6" class="lf lg hi ix b iy lo jc lp jg lq jk lr jo ls js lk ll lm ln bi translated"><strong class="ix hj"> 𝛾 </strong>是一个贴现因子(短视回报对未来回报)</li></ul><blockquote class="kz la lb"><p id="c4aa" class="iv iw ky ix b iy iz ja jb jc jd je jf lc jh ji jj ld jl jm jn le jp jq jr js hb bi translated"><strong class="ix hj"> <em class="hi">贝尔曼方程</em> </strong></p></blockquote><p id="a522" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在我们有了MDP，我们可以用贝尔曼方程来确定每个状态和动作的值。贝尔曼是在给定状态下采取正确行动的一个关键概念。</p><blockquote class="lt"><p id="95d9" class="lu lv hi bd lw lx ly lz ma mb mc js dx translated">价值函数:V(s) = maxa(R(s,a)+𝛾V(s')</p></blockquote><p id="7feb" class="pw-post-body-paragraph iv iw hi ix b iy md ja jb jc me je jf jg mf ji jj jk mg jm jn jo mh jq jr js hb bi translated">上面的等式告诉我们，给定状态<strong class="ix hj"> s </strong>的值等于<strong class="ix hj"> (s，a) </strong>中最大行动的回报加上状态<strong class="ix hj">s’</strong>的贴现值。其中<strong class="ix hj">s’</strong>是如果我们采取行动<strong class="ix hj"> a. </strong>我们最终会达到的状态</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/3e18748fc337e798a5ec4068d00a5260.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*baE0v930QPD_lEu8.png"/></div></div></figure><p id="749c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个等式告诉我们一个状态-动作对的Q值。上面的等式只适用于没有不确定性的环境。如果是一个随机环境，上面的等式就不成立。为了说明随机性，我们稍微修改了一下方程，加入了下一个状态的转移概率和预期回报。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mj"><img src="../Images/5697ea535d6f6ac03cd568e4ac26c5e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MSvF3f27dacKCvwl.png"/></div></div></figure><p id="87cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> <em class="ky">注:</em> </strong> <em class="ky">对于包括我们这个游戏在内的很多强化问题来说，算出每个状态的值是不可伸缩的——一次发生的事情太多了，会占用大量的计算能力。因此，我们必须使用神经网络来逼近Q值和状态值。通过计算TD误差来更新神经网络。</em></p><h2 id="f763" class="jy jz hi bd ka kb kc kd ke kf kg kh ki jg kj kk kl jk km kn ko jo kp kq kr ks bi translated">强化学习和政策</h2><p id="bc2e" class="pw-post-body-paragraph iv iw hi ix b iy kt ja jb jc ku je jf jg kv ji jj jk kw jm jn jo kx jq jr js hb bi translated">在强化学习中，我们有两个主要组件:环境(我们的游戏)和代理(喷气机)。每当代理执行一个动作时，环境使用<strong class="ix hj"> MRP </strong>给代理一个<strong class="ix hj">奖励</strong>，奖励可以是积极的，也可以是消极的，这取决于该动作在特定状态下有多好。代理的目标是在给定每一种可能状态的情况下，学习什么样的行为能使奖励最大化。对于这个特定的游戏，我们不会给代理任何负面的奖励，相反，当喷气式飞机与导弹相撞时，这一集就结束了。代理人每存活一个<strong class="ix hj">时间步</strong>就会获得+1奖励。在这个过程中，智能体会选择特定的策略和特定的行为方式，这被称为智能体的<strong class="ix hj">策略</strong>。</p><h2 id="7a09" class="jy jz hi bd ka kb kc kd ke kf kg kh ki jg kj kk kl jk km kn ko jo kp kq kr ks bi translated">建筑</h2><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es mk"><img src="../Images/9a8f18d0ecb88e9befdb6aafb0ff5e30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*YLxMb2M6719cLRAN.jpg"/></div></figure><p id="ef14" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我的模型的神经网络由3个完全连接的层和256个神经元组成。该模型不需要CNN或任何预处理，因为我们可以在不需要图像检测的情况下获得所有东西的状态和位置。此外，我们在每一层之后应用一个<strong class="ix hj"> ReLU activation </strong>函数，该函数使0以下的所有值变平，并对0以上的所有值保持线性。由于ReLU非常简单，它允许更快的计算，因此，更少的训练时间。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/beece110971e4357cd4ce2de4017bcbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7STvtYrpXp0wdWM1.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">ReLU激活功能</figcaption></figure><p id="1960" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们还堆叠了<strong class="ix hj"> 4帧</strong>，这样模型就可以检测到运动的变化。没有堆叠帧，模型将无法准确预测未来事件。例如，想象一幅两辆车面对面的画面。只有一帧，你无法判断汽车是在移动还是停着。因此，你无法预测汽车是否会发生碰撞。但是，如果给你4帧画面，你可以很容易地识别运动，并预测将要发生什么。</p></div><div class="ab cl mm mn gp mo" role="separator"><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr ms"/><span class="mp bw bk mq mr"/></div><div class="hb hc hd he hf"><p id="b924" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">如果你喜欢读这篇文章，请关注我，关注我未来的文章。此外，请随意与他人分享这篇文章！</strong></p><p id="0cfb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在Medium和<a class="ae iu" href="https://www.linkedin.com/in/sumeet-pathania-93b052194/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上关注我，了解我在人工智能方面的最新进展。</p><p id="dfb1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果你想知道我目前在做什么，以及我在人工智能和类似项目上的经历，请免费订阅我的时事通讯！<a class="ae iu" href="http://eepurl.com/gFbCFX" rel="noopener ugc nofollow" target="_blank">http://eepurl.com/gFbCFX</a></p></div></div>    
</body>
</html>