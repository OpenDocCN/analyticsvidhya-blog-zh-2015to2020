<html>
<head>
<title>Neural Machine Translation for Hindi-English: Sequence to sequence learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">印地语-英语的神经机器翻译:序列到序列学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-machine-translation-for-hindi-english-sequence-to-sequence-learning-1298655e334a?source=collection_archive---------1-----------------------#2020-01-06">https://medium.com/analytics-vidhya/neural-machine-translation-for-hindi-english-sequence-to-sequence-learning-1298655e334a?source=collection_archive---------1-----------------------#2020-01-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/64739fd521f65a3f5e91d954c8978578.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WWXJ0w6YByfPA9KKmDx2Ug.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片鸣谢:<a class="ae iu" href="https://www.betranslated.com/blog/how-good-is-google-translate/" rel="noopener ugc nofollow" target="_blank">https://www . be translated . com/blog/how-good-is-Google-translate/</a></figcaption></figure><p id="678e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你好，机器学习者！去年夏天，我在一个机器翻译项目上有一次惊人的经历。我认为这将是一个伟大的想法，与任何人分享，谁是这个领域的新手，像我一样奋斗，一年前，也与人谁做了类似的研究。本文阐述了使用序列到序列学习方法，利用LSTMs(长短期记忆)构建基本神经机器翻译(NMT)模型的各个方面。虽然有许多关于NMT的论文和博客，但这是强调一些直观功能的另一种尝试，也是执行类似NLP任务的一步一步的指南。在本文中，我试图在技术和非技术细节之间保持平衡。希望有帮助。所以让我们开始吧！</p><p id="95e4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们将关注NMT的以下方面:</p><ol class=""><li id="3ae0" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">语料库——预处理</li><li id="d960" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">准备培训数据</li><li id="bdd6" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">单词嵌入</li><li id="79d4" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">序列对序列学习</li><li id="3529" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">预测:波束搜索</li><li id="f3ea" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">结果:BLEU评分</li></ol><blockquote class="kh ki kj"><p id="4cdd" class="iv iw kk ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated">注意:为了理解翻译步骤，我强烈建议查看LSTMs上这个令人惊叹的<a class="ae iu" href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">博客</a>。</p></blockquote><h1 id="160c" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">第一部分:语料库——预处理</h1><p id="7dce" class="pw-post-body-paragraph iv iw hi ix b iy lm ja jb jc ln je jf jg lo ji jj jk lp jm jn jo lq jq jr js hb bi translated">构建该翻译模型选择的语言为<em class="kk">印地语</em>和英语，平行语料库从<a class="ae iu" href="http://www.cfilt.iitb.ac.in/iitb_parallel/" rel="noopener ugc nofollow" target="_blank"> <em class="kk"> IIT孟买印地语-英语平行语料库</em> </a>中获得。这是一个印地语-英语平行语料库，包含1492827对句子。为了了解两种语言中的单词分布，下面分别展示了<a class="ae iu" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4176592/" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> <em class="kk">齐夫定律</em></strong></a><strong class="ix hj"><em class="kk"/></strong>的情节:</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/6e19daa55cf9e98745a32b33c30dbfc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vyw3h8frOraqU9fE-w_KMw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">齐夫词频定律</figcaption></figure><p id="6933" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">除了处理两种语言和语义不同的语言的挑战之外，还需要其他预处理任务，例如:</p><ol class=""><li id="2d2a" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">处理句子长度<strong class="ix hj">异常值</strong>(一些句子被组合在一起，导致整个序列长度为2000，这在语法上是不符合逻辑的！去掉这些总是更好，特别是在这种情况下，有4个这样的句子。)</li><li id="83e6" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">基本文本<strong class="ix hj">清理</strong>:文件中的错误数据(引用其他语言)、版权声明和电子邮件地址(无需翻译)、标点符号、数字、转换为小写字母等..</li><li id="d3c2" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">将“START_”和“_END”添加到目标句子中(英语)。我们将在后面的章节中看到这样做的原因。</li></ol><p id="9742" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">预处理消耗了你大部分的时间，尤其是。从数据科学的角度来看，你可能需要对数据进行详细的研究，例如发现平均句子长度、异常值、词频的Zipf定律等..这种分析提供了更好的见解，有助于做出更好的决策。例如，我发现两种语言的平均句子长度分别是14和15，这让我考虑长度不超过30的句子。在继续设计模型之前，您可能需要做出许多类似的决定。这就是为什么我建议对数据预处理阶段给予足够的重视。</p><h1 id="b2c1" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">第二部分:准备培训数据</h1><p id="34ea" class="pw-post-body-paragraph iv iw hi ix b iy lm ja jb jc ln je jf jg lo ji jj jk lp jm jn jo lq jq jr js hb bi translated">我们仍然有文本格式的数据。我们需要让它为训练我们的模型做好机器准备。因此，在模型设计之前，我们将执行<strong class="ix hj">标记化</strong>和<strong class="ix hj">索引</strong>(您可以使用<a class="ae iu" href="https://www.nltk.org/_modules/nltk/tokenize.html" rel="noopener ugc nofollow" target="_blank"> NLTK </a>标记化器或其他可从<a class="ae iu" href="https://indic-nlp-library.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> <em class="kk"> IndicNLP </em> </a>库中获得的工具。我选择手动操作。)对于标记化，我们会找出两种语言中所有的独特词。这将决定索引数组的维数。现在，创建3个<a class="ae iu" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html" rel="noopener ugc nofollow" target="_blank"> numpy </a>数组，一个用于编码器输入，一个用于解码器输入，一个用于解码器目标。我们将用它来索引每个单词。</p><figure class="ls lt lu lv fd ij"><div class="bz dy l di"><div class="lw lx l"/></div></figure><p id="f5c3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的步骤中，维度30和32是因为我们已经决定的最大句子长度。编码器为30(印地语)，解码器为32(英语)。解码器限制为32，因为“START_”和“_END”被附加到目标句子(在本例中为英语)的开头和结尾，所以解码器具有停止条件，该条件要么是遇到“_END ”,要么是达到最大字数限制。此外，使用“START_”是因为解码器输出将超前一个时间步长。</p><figure class="ls lt lu lv fd ij"><div class="bz dy l di"><div class="lw lx l"/></div></figure><p id="854f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这一步听起来很复杂，但我们所做的是，打破句子，并为每个独特的单词分配一个整数，主要是创建一个字典。下面的例子可以帮助解释这一点:</p><pre class="ls lt lu lv fd ly lz ma mb aw mc bi"><span id="70d8" class="md kp hi lz b fi me mf l mg mh">Sentence:       This is my home</span><span id="a5e2" class="md kp hi lz b fi mi mf l mg mh">Tokenization:   ['This','is','my','home']</span><span id="bfb3" class="md kp hi lz b fi mi mf l mg mh">Indexing:       This -&gt; 1<br/>                is -&gt; 2<br/>                my -&gt; 3<br/>                home -&gt; 4</span></pre><blockquote class="kh ki kj"><p id="0880" class="iv iw kk ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated"><strong class="ix hj">注意:</strong>下一节只针对有兴趣了解单词如何嵌入特征向量的人。如果你很清楚这一点，请跳到第四部分。</p></blockquote><h1 id="bbf5" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">第三部分:词语嵌入</h1><p id="bc83" class="pw-post-body-paragraph iv iw hi ix b iy lm ja jb jc ln je jf jg lo ji jj jk lp jm jn jo lq jq jr js hb bi translated">这是我最喜欢的部分！为了在文本数据上训练我们的模型，我们需要为每个单词建立一个<strong class="ix hj">特征化的</strong>表示，也称为单词<strong class="ix hj">嵌入</strong>。这个想法是学习一组特征和它们的值，这样我们就有了单词的密集向量表示。</p><p id="efb3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">解释单词嵌入最常见的例子是“<em class="kk">男人-女人-国王-王后</em>”的例子。如果我们有一个10000的词汇量，并且这些词在语料库中的位置是:<em class="kk">【男人-5545，女人-9678，国王-4426，王后-7523】</em>。我们首先为这些单词创建<a class="ae iu" rel="noopener" href="/@michaeldelsole/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179"> <em class="kk">个独热编码向量</em> </a>。如果‘man’在大小为10，000的词汇表中位于位置5545，则‘man’的独热向量是0的10，000维向量，只有一个条目为‘1’，即位于位置5545，表示为<em class="kk"> O₅₅₄₅ </em>。现在，如果我们通过一些特征来定义这些单词，比如说50个，我们可以为语料库中的每个单词定义所有50个特征的值的矩阵。“嵌入矩阵”的非正式表示如下:</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mj"><img src="../Images/ed35b5ac30fac5418ebd79b856644902.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2I2lVtV0Np9prbUqzeJa6w.png"/></div></div></figure><p id="1190" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个矩阵是随机初始化的。“性别”、“皇家”和“亲切”是50个特征中的3个。因此，这个矩阵中的每个单词都是一个50维向量。例如，如果男性性别被认为是-1，女性被认为是1 ( <em class="kk">在这个例子中只是为了区分，这是一个完全无偏的随机假设</em>)，在嵌入矩阵中，单词‘Man’对于‘Gender’特征的值是-1，而‘King’对于同一特征的值是-0.99。这表明“男人”和“国王”在性别上非常相似。因此，当这些单词向量将被放置在50维向量空间中时，如果所有其他特征也相似，则向量之间很可能有更大的<em class="kk">余弦相似度</em>。现在，为了找到‘man’的单词嵌入，将嵌入矩阵(e)乘以‘man’的独热编码向量(<em class="kk"> O₅₅₄₅ </em>):</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/708b3f700ac1ab7cba63e7921b506140.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cxUdwj_ZwwQpYIuE2A-mAA.png"/></div></div></figure><p id="7856" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">结果是‘人’的50维向量表示。这被称为“人”这个词的“嵌入”。同样，这个语料库中的所有单词都表示为50维向量。一些比较知名的预训练单词嵌入有<a class="ae iu" href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener ugc nofollow" target="_blank"><em class="kk">word 2 vec</em></a><a class="ae iu" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank"><em class="kk">Glove</em></a><a class="ae iu" href="https://fasttext.cc/" rel="noopener ugc nofollow" target="_blank"><em class="kk">fast text</em></a>。我选择不使用这些(没有原因，只是想试验和探索手动训练方法)。</p><p id="64d5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Keras 提供了一种与上面提到的略有不同的方法。在预定义的类别'<em class="kk">'嵌入'</em>中，keras从嵌入矩阵(E)中仅取出对应于输入单词的列，并直接产生该列作为单词嵌入。这避免了大矩阵E(在词汇大小很大的情况下)和独热码编码向量的相乘。每个输入整数都被视为从嵌入矩阵中挑选相关列的索引。这使得嵌入过程更快。我个人选择使用这种方法，你总是可以省去这个麻烦，并且使用预先训练的嵌入来获得更好的结果。</p><h1 id="39b3" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">第四部分:序列到序列学习</h1><blockquote class="kh ki kj"><p id="e222" class="iv iw kk ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated">注意:在继续之前，可能值得看一看Keras的这个<a class="ae iu" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener ugc nofollow" target="_blank">博客</a>，关于序列到序列的学习。当他们做基于字符的嵌入时，我使用了基于单词的方法。</p></blockquote><p id="7fac" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当要在读取整个序列后预测输出时，使用序列到序列模型。基本的序列到序列模型具有编码器-解码器架构。该模型有两个LSTMs，编码器和解码器各一个。序列对序列模型的一般工作方式概述如下:</p><ol class=""><li id="5c7d" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">将源序列(印地语)的嵌入向量一次一个字地输入编码器网络。</li><li id="280d" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><em class="kk">将</em>输入的句子编码成固定维的状态向量。在这一步，我们从编码器LSTM获得<em class="kk">隐藏的</em>和<em class="kk">单元状态</em>，并将其馈送给解码器LSTM。</li><li id="b07f" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">这些状态被解码器视为<em class="kk">初始状态</em>。此外，它还有目标词(英语)的嵌入向量。</li><li id="da20" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated"><em class="kk">解码</em>输出翻译的句子，一次一个字。在该步骤中，解码器的输出被发送到整个目标词汇上的<em class="kk"> softmax </em>层。</li></ol><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ml"><img src="../Images/5a192f215eccc45e8f7c7d51dc68691f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AqSX-ubB8oZ-QLEsAVHXsg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">编码器-解码器结构，改编自<strong class="bd kq"> towardsdatascience </strong>博客上的Sequence to sequence，最初灵感来自<a class="ae iu" href="https://arxiv.org/pdf/1406.1078.pdf" rel="noopener ugc nofollow" target="_blank"> Cho等人</a></figcaption></figure><p id="9be5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">编码器LSTM: </strong>在结构上，LSTM层位于<em class="kk">嵌入</em>层之后，因此将嵌入向量作为输入。这里需要注意的重要一点是，输入序列的长度不同。因此，为了保持输入的恒定长度，计算句子的最大长度(这里是30)，并相应地选择输入矩阵的维数。编码器LSTM处理输入序列并返回内部状态。在这个阶段，输入序列被映射到固定维度的状态向量，该状态向量被进一步馈送到解码器LSTM作为其初始状态。</p><figure class="ls lt lu lv fd ij"><div class="bz dy l di"><div class="lw lx l"/></div></figure><p id="fb9f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">解码器LSTM: </strong>解码器LSTM将编码器映射的状态向量作为输入，然后训练它输出翻译，一次一个单词。这个LSTM预测目标序列的下一个单词，给定序列中先前翻译的单词。它基本上使用来自编码器的状态向量作为初始状态，产生未来偏移一个时间步长的目标序列。这种学习方法也被称为<a class="ae iu" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener ugc nofollow" target="_blank"> <em class="kk">师逼迫</em> </a>法。</p><figure class="ls lt lu lv fd ij"><div class="bz dy l di"><div class="lw lx l"/></div></figure><p id="a1bb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">模型:</strong>解码器LSTM的输入通过嵌入层，在这里为英语单词创建单词嵌入。为编码器和解码器网络选择的嵌入维数都是50。这意味着我们指定了源词汇表和目标词汇表中每个单词的特征数量。LSTM()调用的第一个参数是单位数，即输出空间的维数。在这种情况下，单元数选择为50(与嵌入大小相同)。每个概率分布使用目标词汇表上的<em class="kk"> softmax </em>来表示。LSTM输出使用密集连接的网络层<em class="kk">密集</em>包装。最后，模型摘要如下所示:</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/d63d3e3e0bf893497840a0d5bb74006e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zdkpAdxbXABvCmag5Uwn-Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">模型摘要</figcaption></figure><p id="32a9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">训练:</strong>模型是在GPU上训练的，原本有100万个句子对。像这样的大数据集，用通常的<em class="kk"> model.fit() </em>，我亲身经历了几次内存耗尽错误，即使是在GPU机器上。我建议看看Keras <a class="ae iu" href="https://keras.io/models/sequential/" rel="noopener ugc nofollow" target="_blank"> <strong class="ix hj"> <em class="kk">生成器</em> </strong> </a> (fit_generator)或者其他框架中的等效方法。</p><blockquote class="mn"><p id="584e" class="mo mp hi bd mq mr ms mt mu mv mw js dx translated">对于100万个句子对，我只训练了我的模型10个纪元！:)</p></blockquote><p id="506d" class="pw-post-body-paragraph iv iw hi ix b iy mx ja jb jc my je jf jg mz ji jj jk na jm jn jo nb jq jr js hb bi translated">使用的训练参数的详细说明可以在my <a class="ae iu" href="https://github.com/richaranjan23/My_Projects/tree/master/MSc_dissertation" rel="noopener ugc nofollow" target="_blank"> Github </a>的文档中找到。敬请期待“<em class="kk"> final_thesis.pdf </em>”。</p><h1 id="fa16" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">第五节:预测—波束搜索</h1><p id="7988" class="pw-post-body-paragraph iv iw hi ix b iy lm ja jb jc ln je jf jg lo ji jj jk lp jm jn jo lq jq jr js hb bi translated">现在模型训练好了，我们来做一些预测吧！理想情况下，给定一个输入的印地语句子，该模型估计不同的对应英语翻译的概率，即<em class="kk"> P(Y | X)其中(Y = y⟨1⟩，y⟨2⟩，…。</em>)y⟨tʸ⟩。为了挑选最可能的翻译，如果从该分布中随机地对输出进行采样，对于最初的几次，可以获得好的翻译。但是随着越来越多的采样，翻译准确度可能会变差。所以，为了最大化条件概率:</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nc"><img src="../Images/1ddecbd58e49b68c0d045e85254e6122.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3F_DCQEeIAzvSWflbs3KSw.png"/></div></div></figure><p id="1c45" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用<em class="kk">波束搜索</em>算法。该算法的目的是“限制源语言和目标语言之间可能的单词排序，以便进行有效的搜索”。基本的从左到右波束搜索以如下方式工作:</p><ol class=""><li id="236a" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">如果我们选择波束宽度= B = 3，在第一步中，仅给定输入x，即<em class="kk">【p(y‹₁›|x】</em>，模型评估第一个单词的概率。在这一步中，输入的印地语句子通过编码器LSTM，解码器LSTM的第一步将是一个覆盖英语词汇中所有可能性的<em class="kk"> softmax </em>输出。现在，对于第一个单词，在所有可能的翻译中，选出了前三个。该算法为接下来的步骤存储这三个选择。</li><li id="3789" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">在该步骤中，对于所挑选的三个选项中的每一个，估计下一个选择，即<em class="kk"> P(y‹₂›|X，y‹₁›) </em>。网络硬连线第一个单词<em class="kk"> y‹₁› </em>。在这一步之后，我们将条件概率计算为:</li></ol><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nd"><img src="../Images/81e7c333a4071053f8597cc0153aca6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ZrAFLox2CKfzHS7DFI07w.png"/></div></div></figure><p id="798a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.同样，在第三步中，网络片段将硬连接前面步骤中计算的前两个字。而如果B &gt; 3，则模型遵循相同的过程输出它去的最可能的句子，直到遇到<em class="kk"> &lt; EOS &gt; </em>或者达到最大字数限制。</p><p id="eef1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我使用的波束宽度为2。这是我的光束搜索功能的样子:</p><figure class="ls lt lu lv fd ij"><div class="bz dy l di"><div class="lw lx l"/></div></figure><p id="dece" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">实现波束搜索的完整代码可以在<a class="ae iu" href="https://github.com/richaranjan23/My_Projects/blob/master/MSc_dissertation/NMT_final_code/predictions/pred_with_beam.py" rel="noopener ugc nofollow" target="_blank">这里</a>找到。下面给出了一个使用波束搜索显示预测的例子:</p><blockquote class="mn"><p id="c549" class="mo mp hi bd mq mr ms mt mu mv mw js dx translated"><em class="ne">来源:कालबैक फोन </em></p><p id="83ee" class="mo mp hi bd mq mr ms mt mu mv mw js dx translated"><em class="ne">目标:回拨电话</em></p><p id="cf29" class="mo mp hi bd mq mr ms mt mu mv mw js dx translated"><em class="ne">带贪婪搜索的翻译:回拨电话</em></p><p id="cf76" class="mo mp hi bd mq mr ms mt mu mv mw js dx translated"><em class="ne">翻译用波束搜索(B=2): [('回拨电话'，0.195772)，('电话对比'，0.003202)] </em></p></blockquote><p id="69da" class="pw-post-body-paragraph iv iw hi ix b iy mx ja jb jc my je jf jg mz ji jj jk na jm jn jo nb jq jr js hb bi translated">波束搜索的优点是显示预测的前两个概率，然后验证转换是否有意义。</p><blockquote class="kh ki kj"><p id="c86f" class="iv iw kk ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated">注意:一旦你有了一个训练好的模型，你就可以进行预测，不管有没有波束搜索。我试图在本<a class="ae iu" href="https://github.com/richaranjan23/My_Projects/blob/master/MSc_dissertation/NMT_final_code/hindi_eng_seq2seq_word_1M-BEAM-transfer_learning.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>中记录这两种方法的基本步骤。</p></blockquote><h1 id="53f3" class="ko kp hi bd kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll bi translated">第六部分:结果——BLEU分数</h1><p id="27ea" class="pw-post-body-paragraph iv iw hi ix b iy lm ja jb jc ln je jf jg lo ji jj jk lp jm jn jo lq jq jr js hb bi translated">下面显示了通过该模型获得的一些示例翻译，以及相应的Google翻译输出:</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nf"><img src="../Images/3c18700ea566267d3c669baee78e2d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ukkhHg1DJATYkAMLKcwe_A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">模型翻译与谷歌翻译输出的比较</figcaption></figure><p id="232b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">通过BLEU分数计算来评估整体模型性能。在<strong class="ix hj"> 4克</strong>精密系统上，发现最佳训练和测试BLEU分数分别为<strong class="ix hj"> 0.33 </strong>和<strong class="ix hj"> 0.30 </strong>。</p><p id="2d9d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我很想对这些数据进行一些抽样调查。我对大约25个句子进行了分析，并与谷歌翻译的结果进行了比较。该模型在n-gram精度系统中获得的BLEU分数与Google的分数相当！(当然，我们必须考虑这样一个事实，即谷歌可以访问这两种语言的巨大语料库，而在我的情况下，我只在100万个句子对上训练了我的模型)。BLEU分数比较总结如下:</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ng"><img src="../Images/6d603d6d09faa8b476817bfc7440c4bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Be6Uh82CTBnwoOdA2bM-vw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">模型翻译和谷歌翻译结果之间的n-gram BLEU分数比较</figcaption></figure><p id="b79e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我确实意识到这需要大量的阅读，但我认为这个项目是一次重要的学习经历，我希望我的发现也能帮助你。完整的代码和文档可以在<a class="ae iu" href="https://github.com/richaranjan23/My_Projects/tree/master/MSc_dissertation" rel="noopener ugc nofollow" target="_blank">这里</a>找到。请随时提供反馈和改进建议。</p><blockquote class="kh ki kj"><p id="78ea" class="iv iw kk ix b iy iz ja jb jc jd je jf kl jh ji jj km jl jm jn kn jp jq jr js hb bi translated">注意:如果你看一下我的实现，请注意大多数文件会有两个版本，“有和没有波束搜索”。谁不想一步一步来呢？:)祝你阅读愉快，如果你喜欢，请鼓掌！:)</p></blockquote></div></div>    
</body>
</html>