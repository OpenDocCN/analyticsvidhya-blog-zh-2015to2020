<html>
<head>
<title>Building an Intrusion Detection System using KDD Cup’99 Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用KDD杯99数据集构建入侵检测系统</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/building-an-intrusion-detection-model-using-kdd-cup99-dataset-fb4cba4189ed?source=collection_archive---------2-----------------------#2020-01-12">https://medium.com/analytics-vidhya/building-an-intrusion-detection-model-using-kdd-cup99-dataset-fb4cba4189ed?source=collection_archive---------2-----------------------#2020-01-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/bb861fb274f3f3507588343bed45ad4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hieyhEVcAhwEN09TF2ZXCg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片来源:谷歌</figcaption></figure><p id="6865" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">网络入侵是计算机网络上任何未经授权的活动。检测网络入侵的软件旨在保护计算机网络免受未经授权的用户，可能包括内部人员。</p><h1 id="ab45" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">问题陈述:-</h1><p id="6e2f" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">在这个项目中，我们将构建一个网络入侵检测器，一个能够区分“坏”连接(称为入侵或攻击)和“好”或正常连接的预测模型。该数据集包括在军事网络环境中模拟的各种入侵。用于构建入侵探测器的数据由麻省理工学院林肯实验室准备和管理。目的是调查和评估入侵检测的研究。有关如何收集数据的更多详细信息，以及数据中涉及的各种功能的描述，您可以访问以下链接:</p><div class="kv kw ez fb kx ky"><a href="http://kdd.ics.uci.edu/databases/kddcup99/task.html" rel="noopener  ugc nofollow" target="_blank"><div class="kz ab dw"><div class="la ab lb cl cj lc"><h2 class="bd hj fi z dy ld ea eb le ed ef hh bi translated">KDD杯-99任务描述</h2><div class="lf l"><h3 class="bd b fi z dy ld ea eb le ed ef dx translated">本文档改编自论文《基于成本的数据挖掘建模与评估及其在欺诈中的应用》</h3></div><div class="lg l"><p class="bd b fp z dy ld ea eb le ed ef dx translated">kdd.ics.uci.edu</p></div></div></div></a></div><h1 id="39e2" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">构建入侵检测系统:</h1><h1 id="9d5b" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">㈠导入数据:</h1><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="9a08" class="lq jt hi lm b fi lr ls l lt lu">data = pd.read_csv('kddcup.data_10_percent_corrected', names = features, header=None)</span><span id="5605" class="lq jt hi lm b fi lv ls l lt lu">print('The no of data points are:',data.shape[0])<br/>print('='*40)<br/>print('The no of features are:',data.shape[1])<br/>print('='*40)<br/>print('Some of the features are:',features[:10])</span><span id="7587" class="lq jt hi lm b fi lv ls l lt lu">The no of data points are: 494021<br/>========================================<br/>The no of features are: 42<br/>========================================<br/>Some of the features are: ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land', 'wrong_fragment', 'urgent', 'hot']</span></pre><p id="6225" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">数据集由4，94，021个数据点和42个要素组成。功能列表及其详细信息可从以下链接获得:</p><p id="c7b4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names<a class="ae lw" href="http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names" rel="noopener ugc nofollow" target="_blank"/></p><p id="da94" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">不同类别的输出标签如下所示:</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="ea97" class="lq jt hi lm b fi lr ls l lt lu">output = data['intrusion_type'].values<br/>labels = set(output)</span><span id="48ca" class="lq jt hi lm b fi lv ls l lt lu">print('The different type of output labels are:',labels)<br/>print('='*125)<br/>print('No. of different output labels are:', len(labels))</span><span id="1888" class="lq jt hi lm b fi lv ls l lt lu">The different type of output labels are: {'neptune.', 'multihop.', 'warezmaster.', 'portsweep.', 'smurf.', 'land.', 'teardrop.', 'nmap.', 'guess_passwd.', 'normal.', 'perl.', 'spy.', 'satan.', 'ftp_write.', 'loadmodule.', 'pod.', 'back.', 'buffer_overflow.', 'phf.', 'rootkit.', 'warezclient.', 'imap.', 'ipsweep.'}<br/>====================================================================<br/>No. of different output labels are: 23</span></pre><p id="3c85" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">正如我们所看到的，该数据总共有23个不同的输出类，其中“正常”类代表良好的连接，而其余22个类代表不同类型的不良连接。</p><h1 id="557f" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">(二)数据清理</h1><p id="640b" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">处理数据集时涉及的一个重要步骤是在使用数据进行数据分析和构建模型之前清理可用数据。数据清理过程中涉及的一些重要步骤是<strong class="iw hj"> <em class="lx">移除/输入空值</em> </strong>和<strong class="iw hj"> <em class="lx">从数据集中移除重复项</em> </strong>。</p><p id="1ea1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">检查空值:- </strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="dea8" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">print('Null values in dataset are',len(data[data.isnull().any(1)]))</strong></span><span id="ea29" class="lq jt hi lm b fi lv ls l lt lu">Null values in the dataset are:  0</span></pre><p id="25ac" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">检查重复值:- </strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="f6b4" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">data.drop_duplicates(subset=features, keep='first', inplace = True)</strong></span><span id="b9bb" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">data.shape</strong><br/>(145586, 42)</span><span id="ad7b" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">data.to_pickle('data.pkl')</strong></span></pre><p id="7fb2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在最后一部分中，我们将数据存储到一个pickle文件中。这样做是为了在我们再次需要原始数据的情况下，我们可以直接从pickle文件加载它，而不需要经历导入原始数据并再次清理它的过程。</p><h1 id="9d04" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">㈢探索性数据分析</h1><p id="e697" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">E <strong class="iw hj">探索性数据分析</strong> ( <strong class="iw hj"> EDA </strong>)是一种分析数据集以总结其主要特征的方法，通常采用可视化方法。一个<a class="ae lw" href="https://en.wikipedia.org/wiki/Statistical_model" rel="noopener ugc nofollow" target="_blank">统计模型</a>可能被使用，也可能不被使用，但是EDA主要是为了看看数据能告诉我们什么，而不仅仅是正式的建模。</p><p id="5c72" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面，我们使用了一些python库，如matplotlib、pandas和seaborn来执行EDA。我们还构建了一些用于创建双变量分析图的效用函数。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="ba74" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">plt.figure(figsize=(20,15))<br/>class_distribution = data['intrusion_type'].value_counts()<br/>class_distribution.plot(kind='bar')<br/>plt.xlabel('Class')<br/>plt.ylabel('Data points per Class')<br/>plt.title('Distribution of yi in train data')<br/>plt.grid()<br/>plt.show()</strong></span><span id="d2d5" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx"># ref: arg sort </em><a class="ae lw" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html" rel="noopener ugc nofollow" target="_blank"><em class="lx">https://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html</em></a></span><span id="8093" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx"># -(train_class_distribution.values): the minus sign will give us in decreasing order</em></span><span id="633d" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">sorted_yi = np.argsort(-class_distribution.values)<br/>for i in sorted_yi:<br/>    print('Number of data points in class', i+1,':', class_distribution.values[i], '(', np.round((class_distribution.values[i]/data.shape[0]*100), 3), '%)')</strong></span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/32f5573af3e94aee7e6d3e084192f1ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*daGHTZzzuVDCIFuDCmSP2Q.png"/></div></div></figure><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="c723" class="lq jt hi lm b fi lr ls l lt lu">Number of data points in class normal : 87832 ( 60.33 %)<br/>Number of data points in class neptune : 51820 ( 35.594 %)<br/>Number of data points in class back : 968 ( 0.665 %)<br/>Number of data points in class teardrop : 918 ( 0.631 %)<br/>Number of data points in class satan : 906 ( 0.622 %)<br/>Number of data points in class warezclient : 893 ( 0.613 %)<br/>Number of data points in class ipsweep : 651 ( 0.447 %)<br/>Number of data points in class smurf : 641 ( 0.44 %)<br/>Number of data points in class portsweep : 416 ( 0.286 %)<br/>Number of data points in class pod : 206 ( 0.141 %)<br/>Number of data points in class nmap : 158 ( 0.109 %)<br/>Number of data points in class guess_passwd : 53 ( 0.036 %)<br/>Number of data points in class buffer_overflow : 30 ( 0.021 %)<br/>Number of data points in class warezmaster : 20 ( 0.014 %)<br/>Number of data points in class land : 19 ( 0.013 %)<br/>Number of data points in class imap : 12 ( 0.008 %)<br/>Number of data points in class rootkit : 10 ( 0.007 %)<br/>Number of data points in class loadmodule : 9 ( 0.006 %)<br/>Number of data points in class ftp_write : 8 ( 0.005 %)<br/>Number of data points in class multihop : 7 ( 0.005 %)<br/>Number of data points in class phf : 4 ( 0.003 %)<br/>Number of data points in class perl: 3 ( 0.002 %)<br/>Number of data points in class spy : 2 ( 0.001 %)</span></pre><p id="d47b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">观察:- </strong></p><ul class=""><li id="9bf6" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">大多数数据点来自“正常”(良好连接)类别，约占60.33 %。</li><li id="26a7" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">在属于坏连接的类别中，分类为“海王星”(35.594 %)和“背。”(0.665 %)的数据点数最多。</li><li id="0561" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">类“rootkit”，“load_module。”，“ftp_write。”，“多跳。”，“phf。”，“perl。”还有"间谍"数据点数量最少，每类少于10个数据点。</li><li id="92fa" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">这个数据集是高度不平衡的，因此我们将需要建立一个模型，该模型应该能够准确地对属于这些不同类的点进行分类。</li></ul><h1 id="24dc" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">问题的性能指标:-</h1><ul class=""><li id="1d42" class="lz ma hi iw b ix kq jb kr jf mn jj mo jn mp jr me mf mg mh bi translated">我们将使用<strong class="iw hj"> <em class="lx">混淆矩阵</em> </strong>，因为它将帮助我们确定一个模型对属于23个类别中每一个类别的数据点进行分类的能力。</li><li id="3d16" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">连同混淆矩阵，我们还将计算<strong class="iw hj"> <em class="lx">精度，回忆</em> </strong>和<strong class="iw hj">加权</strong><strong class="iw hj"><em class="lx">f1-分数</em> </strong>来确定最佳模型。</li></ul><p id="cb0e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">另一个重要指标:</strong></p><p id="e846" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于这个问题，我们希望我们的FPR尽可能低。这是因为“正常”连接因被误分类为“坏”连接而被丢弃的严重性低于被误分类为“正常”连接的“坏”连接，后者可能会导致安全威胁。</p><ul class=""><li id="e6f0" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">对于这个入侵检测问题，TPR和FPR可以描述如下</li></ul><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/4739ae1f2418adb82d2727c0eaf3e200.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0v3WWBj4BnWaACAgvjxFrw.png"/></div></div></figure><ul class=""><li id="1645" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">因此，在对数据应用不同的ML技术的同时，除了计算混淆矩阵和f1分数，我们还将计算TPR和FPR分数，这将有助于我们选择最佳模型。</li></ul><h1 id="b6f1" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">单变量分析:-</h1><ol class=""><li id="b9e0" class="lz ma hi iw b ix kq jb kr jf mn jj mo jn mp jr mr mf mg mh bi translated"><strong class="iw hj"> <em class="lx">持续时间:- </em> </strong></li></ol><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="cf82" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">import seaborn as sns<br/>plt.figure(figsize=(20,16))<br/>sns.set(style="whitegrid")<br/>ax = sns.violinplot(x="intrusion_type", y="duration", data=data, fliersize=None)<br/>plt.xticks(<br/>    rotation=45, <br/>    horizontalalignment='right',<br/>    fontweight='light',<br/>    fontsize='x-large'  <br/>)</strong></span><span id="5478" class="lq jt hi lm b fi lv ls l lt lu">(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]), &lt;a list of 23 Text xticklabel objects&gt;)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/aa8bf80d62a50632c777b4fe5f8617f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7uEJ0iVu08byjVaPcEi4oA.png"/></div></div></figure><ul class=""><li id="d91d" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">使用箱线图和violin图的单变量分析没有给出任何清晰和令人满意的结果。</li><li id="60c4" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">因此，我们将使用双变量分析的配对图，或者我们也可以使用主成分分析/TSNE来减少维数并进行双变量/三变量分析。</li></ul><h1 id="ba90" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">使用pairplot的双变量分析:-</h1><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="57f9" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">def pairplot(data, label, features=[]):</strong></span><span id="70d4" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx">'''<br/>    This function creates pairplot taking 4 features from our dataset as default parameters along with the output variable<br/>    '''</em></span><span id="b0ac" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">sns.pairplot(data, hue=label, height=4, diag_kind='hist',   vars=features, plot_kws={'alpha':0.6, 's':80, 'edgecolor':'k'})</strong></span></pre><p id="d00c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">上述函数从我们的数据集中提取了4个要素，并绘制了16个二元图，在16个图的每个图中有2个要素的不同组合，如下所示。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/e920f605fae824345220f78db7c64d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LfSHobO8bcSHzmk6nh8BeA.png"/></div></div></figure><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mu"><img src="../Images/de3bc4eb737dd1f8a80fc429fd8a4626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZhzbKwTApXOKOL_PhB6cow.png"/></div></div></figure><p id="1138" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">类似地，可以绘制具有不同特征组合的许多这样的配对图。</p><p id="b243" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">来自成对图的观察:- </strong></p><ul class=""><li id="56f2" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">没有一个配对图能够显示不同输出类别之间的任何线性可分性/几乎线性可分性。</li></ul><h1 id="ade0" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">双变量分析的TSNE:-</h1><p id="0531" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">(<strong class="iw hj">t-SNE</strong>)t-分布式随机邻居嵌入是一种非线性降维<strong class="iw hj">算法</strong>，用于探索高维数据。它将多维数据映射到适合人类观察的两个或多个维度。</p><p id="1b46" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面我们写了一个函数，通过指定我们选择的困惑值和迭代次数，可以用来绘制TSNE图。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="15a5" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">def tsne_func(data, label, no_components, perplexity_value, n_iter_value):</strong></span><span id="ab6d" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx">'''<br/>    This function applies TSNE on the original dataset with  no_components, perplexity_value, n_iter_value as the TSNE parameters <br/>    and transforms the original dataset into TSNE transformed feature space with the tsne dataset containing number of features <br/>    equal to the value specified for no_components and also plots the scatter plot of the transformed data points along with <br/>    their class label <br/>    '''</em></span><span id="d1be" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">print('TSNE with perplexity={} and no. of iterations={}'.format(perplexity_value, n_iter_value))</strong></span><span id="444c" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">tsne = TSNE(n_components=no_components, perplexity=perplexity_value, n_iter=n_iter_value)</strong></span><span id="6fd3" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">tsne_df1 = tsne.fit_transform(data)</strong></span><span id="f359" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">print(tsne_df1.shape)</strong></span><span id="4315" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">tsne_df1 = np.vstack((tsne_df1.T, Y)).T</strong></span><span id="e64e" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">tsne_data1 = pd.DataFrame(data=tsne_df1, columns=['feature1', 'feature2', 'Output'])</strong></span><span id="13de" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">sns.FacetGrid(tsne_data1, hue='Output', size=6).map(plt.scatter, 'feature1', 'feature2').add_legend()</strong></span><span id="5358" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">plt.show()</strong></span></pre><ol class=""><li id="dcdd" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr mr mf mg mh bi translated">困惑值=100，迭代值=500的TSNE图(2D ):</li></ol><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mv"><img src="../Images/4b47851f20d4d895256a166d4cf8198b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l1sDtvyIzE5eA1B7btQgHA.png"/></div></div></figure><p id="358d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">2.困惑值=50，迭代值=1000的TSNE图(2D ):</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/7ef5308acad4cc40d6f603b6c20d94f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0fuT_ZFAnYMdaqgYAnWFIw.png"/></div></div></figure><h2 id="caa7" class="lq jt hi bd ju mx my mz jy na nb nc kc jf nd ne kg jj nf ng kk jn nh ni ko nj bi translated">观察:-</h2><p id="a10d" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">从以上两个图中，我们可以得出结论，在TSNE变换的二维空间中，任何两个或两个以上的类别之间不存在线性可分性。</p><h1 id="8b29" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">(V)列车测试分割:-</h1><p id="e546" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">下面，我们执行了训练-测试分割，其中我们将整个数据集分为两部分，其中训练数据占总数据的75%，测试数据由属于剩余25%数据的点组成。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="0c76" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">from sklearn.model_selection import train_test_split</strong></span><span id="6485" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">X_train, X_test, Y_train, Y_test = train_test_split(data.drop('intrusion_type', axis=1), data['intrusion_type'], stratify=data['intrusion_type'], test_size=0.25)</strong></span><span id="9c42" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">print('Train data')<br/>print(X_train.shape)<br/>print(Y_train.shape)<br/>print('='*20)<br/>print('Test data')<br/>print(X_test.shape)<br/>print(Y_test.shape)</strong></span><span id="bc2f" class="lq jt hi lm b fi lv ls l lt lu">Train data<br/>(109189, 41)<br/>(109189,)<br/>====================<br/>Test data<br/>(36397, 41)<br/>(36397,)</span></pre><p id="9626" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们没有将训练数据进一步分成训练集和交叉验证集的原因是，在构建模型时，我们将使用网格搜索CV对训练数据进行K-fold交叉验证。</p><h1 id="0166" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">(VI)预处理我们数据中的特征</h1><ol class=""><li id="b2ee" class="lz ma hi iw b ix kq jb kr jf mn jj mo jn mp jr mr mf mg mh bi translated"><strong class="iw hj">使用一键编码对分类数据进行矢量化:- </strong></li></ol><p id="7ac5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们的数据集有3个分类特征，即<strong class="iw hj"> <em class="lx">协议、服务</em></strong><em class="lx">&amp;</em><strong class="iw hj"><em class="lx">标志</em> </strong>，我们将使用one-hot编码对其进行矢量化，如下所示。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="8ac7" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">1. Protocol_type:-</strong><br/>--------------------</span><span id="ee41" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">protocol = list(X_train['protocol_type'].values)<br/>protocol = list(set(protocol))<br/>print('Protocol types are:', protocol)</strong></span><span id="eceb" class="lq jt hi lm b fi lv ls l lt lu">Protocol types are: ['udp', 'tcp', 'icmp']</span><span id="a7ec" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">from sklearn.feature_extraction.text import CountVectorizer<br/>one_hot = CountVectorizer(vocabulary=protocol, binary=True)<br/>train_protocol = one_hot.fit_transform(X_train['protocol_type'].values)<br/>test_protocol = one_hot.transform(X_test['protocol_type'].values)</strong></span><span id="597a" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">print(train_protocol[1].toarray())<br/>print(train_protocol.shape)</strong></span><span id="272e" class="lq jt hi lm b fi lv ls l lt lu">[[0 1 0]]<br/>(109189, 3)</span></pre><p id="292d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">同样，我们也对<strong class="iw hj"> <em class="lx">服务</em> </strong>和<strong class="iw hj"> <em class="lx">标志</em> </strong>特征应用一键编码。</p><p id="550a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 2。标准化功能:- </strong></p><p id="9774" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">数据标准化是重新调整一个或多个属性的过程，使它们的平均值为0，标准差为1。</p><p id="f4f8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面是一个函数，它将其中一个特征作为参数，并对其应用标准化。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="d8d4" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">def feature_scaling(X_train, X_test, feature_name):</strong><br/> <br/> <em class="lx">'''<br/> This function performs standardisation on the features<br/> '''</em></span><span id="dfa9" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">scaler = StandardScaler()<br/> scaler1 =      scaler.fit_transform(X_train[feature_name].values.reshape(-1,1))<br/> scaler2 = scaler.transform(X_test[feature_name].values.reshape(-1,1))<br/> <br/> return scaler1, scaler2</strong></span><span id="4230" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">1. Duration :-<br/>---------------</strong></span><span id="ed99" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">duration1, duration2 = feature_scaling(X_train, X_test, 'duration')</strong></span><span id="02c8" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">print(duration1[1])</strong></span><span id="37e7" class="lq jt hi lm b fi lv ls l lt lu">[-0.10631]</span><span id="3a55" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">2. src_bytes :-<br/>------------------</strong></span><span id="8387" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">src_bytes1, src_bytes2 = feature_scaling(X_train, X_test, 'src_bytes')</strong></span><span id="91d4" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">print(src_bytes1[1])</strong></span><span id="5b9e" class="lq jt hi lm b fi lv ls l lt lu">[-0.02721124]</span><span id="a292" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">2. dst_bytes :-<br/>------------------</strong></span><span id="da2d" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">dst_bytes1, dst_bytes2 = feature_scaling(X_train, X_test, 'dst_bytes')</strong></span><span id="353c" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">print(dst_bytes1[1])</strong></span><span id="5190" class="lq jt hi lm b fi lv ls l lt lu">[-0.03568432]</span></pre><p id="6aaa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">同样，我们将对数据集中的其他连续要素应用标准化。</p><h1 id="1b45" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">(VII)合并特征以准备最终数据</h1><p id="2895" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">在对特征进行矢量化和标准化之后，我们将合并这些特征以获得我们的最终训练和测试数据集，这些数据集将被馈送到我们的ML模型以用于训练和评估目的。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="0d69" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">from scipy.sparse import hstack</strong></span><span id="78ae" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">X_train_1 = hstack((duration1, train_protocol, train_service, train_flag, src_bytes1, dst_bytes1, land1.T, wrong_fragment1, urgent1, hot1, num_failed_logins1, logged_in1.T, num_compromised1, root_shell1, su_attempted1, num_root1, num_file_creations1, num_shells1, num_access_files1, is_host_login1.T, is_guest_login1.T, count1, srv_count1, serror_rate1, srv_serror_rate1, rerror_rate1, srv_rerror_rate1, same_srv_rate1, diff_srv_rate1, srv_diff_host_rate1, dst_host_count1, dst_host_srv_count1, dst_host_same_srv_rate1, dst_host_diff_srv_rate1, dst_host_same_src_port_rate1, dst_host_srv_diff_host_rate1, dst_host_serror_rate1, dst_host_srv_serror_rate1, dst_host_rerror_rate1, dst_host_srv_rerror_rate1))</strong></span><span id="4d04" class="lq jt hi lm b fi lv ls l lt lu">X_train_1.shape</span><span id="1435" class="lq jt hi lm b fi lv ls l lt lu">(109189, 116)</span><span id="3959" class="lq jt hi lm b fi lv ls l lt lu">--------------------------------------------------------------------</span><span id="6c44" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">X_test_1 = hstack((duration2, test_protocol, test_service, test_flag, src_bytes2, dst_bytes2, land2.T, wrong_fragment2, urgent2, hot2, num_failed_logins2, logged_in2.T, num_compromised2, root_shell2, su_attempted2, num_root2, num_file_creations2, num_shells2, num_access_files2, is_host_login2.T, is_guest_login2.T, count2, srv_count2, serror_rate2, srv_serror_rate2, rerror_rate2, srv_rerror_rate2, same_srv_rate2, diff_srv_rate2, srv_diff_host_rate2, dst_host_count2, dst_host_srv_count2, dst_host_same_srv_rate2, dst_host_diff_srv_rate2, dst_host_same_src_port_rate2, dst_host_srv_diff_host_rate2, dst_host_serror_rate2, dst_host_srv_serror_rate2, dst_host_rerror_rate2, dst_host_srv_rerror_rate2))</strong></span><span id="9a0e" class="lq jt hi lm b fi lv ls l lt lu">X_test_1.shape</span><span id="61bd" class="lq jt hi lm b fi lv ls l lt lu">(36,397, 116)</span></pre><h1 id="81ad" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">进一步探讨我们的问题:-</h1><p id="b37b" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">(I)我们将在我们的数据集上应用以下分类器，并评估它们的性能:- <br/> <br/> 1。朴素贝叶斯<br/> 2。逻辑回归<br/> 3。SVM <br/> 4。决策树<br/> 5。随机森林<br/> 6。GBDT / XGBoost</p><p id="971c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">(ii)基于我们将从上述分类器获得的性能度量分数，我们将对我们的数据集应用以下特征工程技术，以获得一些附加特征:</p><ul class=""><li id="352f" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated"><strong class="iw hj"> 1。聚类特性:- </strong>我们将在数据集上应用聚类，并将聚类值作为附加特性添加到数据集。</li><li id="b84f" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated"><strong class="iw hj"> 2。PCA变换的特征:- </strong>我们将在数据集上应用PCA，并将前5个PCA特征作为附加特征添加到我们的数据集。</li><li id="6757" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated"><strong class="iw hj"> 3。使用现有特征的特征工程:- </strong>我们将从数据中创建新特征，如下所示:<br/> (i)添加2个特征:(例如，new _ feature _ 1 = src _ bytes+dst _ bytes)<br/>(ii)减去2个特征，(例如，new _ feature _ 2 = ABS(src _ bytes-dst _ bytes)。</li></ul><p id="f2f2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">(iii)然后，我们将在数据集2上应用来自数据集1的最佳性能分类器，并评估它们的性能。</p><h1 id="ef08" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">(八)应用机器学习模型</h1><p id="d890" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">下面是一些在模型构建阶段用于不同目的的函数。</p><p id="7dcd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">功能_1 : </strong> -</p><p id="961b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">下面的函数打印了一个混淆矩阵热图，它将帮助我们确定模型对属于不同类别的数据点进行分类的能力。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="13d7" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">import datetime as dt<br/>from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score<br/>from sklearn.model_selection import GridSearchCV<br/>from sklearn.externals import joblib</strong></span><span id="a695" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">def confusion_matrix_func(Y_test, y_test_pred):</strong><br/>    <br/>    <em class="lx">'''<br/>    This function computes the confusion matrix using Predicted and Actual values and plots a confusion matrix heatmap<br/>    '''</em></span><span id="ca54" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">C = confusion_matrix(Y_test, y_test_pred)<br/>    cm_df = pd.DataFrame(C)</strong></span><span id="029a" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">labels = ['back', 'butter_overflow', 'loadmodule', 'guess_passwd', 'imap', 'ipsweep', 'warezmaster', 'rootkit', <br/>'multihop', 'neptune', 'nmap', 'normal', 'phf', 'perl', 'pod', 'portsweep', 'ftp_write', 'satan', 'smurf', 'teardrop', 'warezclient', 'land']</strong></span><span id="1909" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">plt.figure(figsize=(20,15))<br/>    sns.set(font_scale=1.4)<br/>    sns.heatmap(cm_df, annot=True, annot_kws={"size":12}, fmt='g',       xticklabels=labels, yticklabels=labels)<br/>    plt.ylabel('Actual Class')<br/>    plt.xlabel('Predicted Class')<br/>    <br/>    plt.show()</strong></span></pre><p id="3679" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">功能_2 :-</p><p id="39df" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">该函数拟合从训练数据的网格搜索CV获得的最佳估计值，并预测训练数据以及测试数据的性能，还计算拟合模型和预测输出所需的总时间。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="a1dc" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">def model(model_name, X_train, Y_train, X_test, Y_test):</strong></span><span id="f83e" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx">'''<br/> Fits the model on train data and predict the performance on train and test data.<br/> '''</em></span><span id="f2d9" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">print('Fitting the model and prediction on train data:')<br/>    start = dt.datetime.now()<br/>    model_name.fit(X_train, Y_train)<br/>    y_tr_pred = model_name.predict(X_train)<br/>    print('Completed')<br/>    print('Time taken:',dt.datetime.now()-start)<br/>    print('='*50)<br/>    <br/>    results_tr = dict()</strong></span><span id="654e" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">y_tr_pred = model_name.predict(X_train)    <br/>    results_tr['precision'] = precision_score(Y_train, y_tr_pred,         average='weighted')<br/>    results_tr['recall'] = recall_score(Y_train, y_tr_pred, average='weighted')<br/>    results_tr['f1_score'] = f1_score(Y_train, y_tr_pred, average='weighted')<br/>    <br/>    results_test = dict()</strong></span><span id="919c" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">print('Prediction on test data:')<br/>    start = dt.datetime.now()<br/>    y_test_pred = model_name.predict(X_test)<br/>    print('Completed')<br/>    print('Time taken:',dt.datetime.now()-start)<br/>    print('='*50)<br/>    <br/>    print('Performance metrics:')<br/>    print('='*50)<br/>    print('Confusion Matrix is:')<br/>    confusion_matrix_func(Y_test, y_test_pred)<br/>    print('='*50)<br/>    results_test['precision'] = precision_score(Y_test, y_test_pred, average='weighted')<br/>    print('Precision score is:')<br/>    print(precision_score(Y_test, y_test_pred, average='weighted'))<br/>    print('='*50)<br/>    results_test['recall'] = recall_score(Y_test, y_test_pred, average='weighted')<br/>    print('Recall score is:')<br/>    print(recall_score(Y_test, y_test_pred, average='weighted'))<br/>    print('='*50)<br/>    results_test['f1_score'] = f1_score(Y_test, y_test_pred, average='weighted')<br/>    print('F1-score is:')<br/>    print(f1_score(Y_test, y_test_pred, average='weighted'))<br/>    <em class="lx"># add the trained  model to the results</em><br/>    results_test['model'] = model<br/>    <br/>    return results_tr, results_test</strong></span></pre><p id="44a9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">功能_3 :- </strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="a465" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">def print_grid_search_attributes(model):</strong><br/>    <br/>    <em class="lx">'''<br/>    This function prints all the grid search attributes<br/>    '''</em><br/>    <br/>    <strong class="lm hj">print('---------------------------')<br/>    print('|      Best Estimator     |')<br/>    print('---------------------------')<br/>    print('\n\t{}\n'.format(model.best_estimator_))</strong></span><span id="1d15" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx"># parameters that gave best results while performing grid search</em></span><span id="2204" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">print('---------------------------')<br/>    print('|     Best parameters     |')<br/>    print('---------------------------')<br/>    print('\tParameters of best estimator : \n\n\t{}\n'.format(model.best_params_))</strong></span><span id="d526" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx">#  number of cross validation splits</em></span><span id="6232" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">print('----------------------------------')<br/>    print('|   No of CrossValidation sets   |')<br/>    print('----------------------------------')<br/>    print('\n\tTotal number of cross validation sets: {}\n'.format(model.n_splits_))</strong></span><span id="d2d6" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx"># Average cross validated score of the best estimator, from the Grid Search</em></span><span id="0670" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">print('---------------------------')<br/>    print('|        Best Score       |')<br/>    print('---------------------------')<br/>    print('\n\tAverage Cross Validate scores of best estimator : \n\n\t{}\n'.format(model.best_score_))</strong></span></pre><p id="7e99" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">上面的函数打印与网格搜索相关的属性，如no_of_splits、best_estimator、best_parameters和best_score。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="b65b" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">def tpr_fpr_func(Y_tr, Y_pred):</strong></span><span id="5aa9" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx">'''</em><br/><em class="lx">    This function computes the TPR and FPR scores using the actual and predicetd values.</em><br/><em class="lx">    '''</em></span><span id="7406" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">results = dict()</strong></span><span id="cffe" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">Y_tr = Y_tr.to_list()<br/>    tp = 0; fp = 0; positives = 0; negatives = 0; length = len(Y_tr)<br/>    for i in range(len(Y_tr)):<br/>        if Y_tr[i]=='normal.':<br/>            positives += 1<br/>        else:<br/>            negatives += 1<br/>            <br/>    for i in range(len(Y_pred)):<br/>        if Y_tr[i]=='normal.' and Y_pred[i]=='normal.':<br/>            tp += 1<br/>        elif Y_tr[i]!='normal.' and Y_pred[i]=='normal.':<br/>            fp += 1<br/>            <br/>    tpr = tp/positives<br/>    fpr = fp/negatives<br/>    <br/>    results['tp'] = tp; results['tpr'] = tpr; results['fp'] = fp; results['fpr'] = fpr<br/>    <br/>    return results</strong></span></pre><p id="0544" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">上述函数使用实际值和预测值计算TP、FP、TPR和FPR。</p><h1 id="566f" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">模型1:-高斯朴素贝叶斯</h1><p id="4895" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">我们将应用于数据集的第一个模型是高斯朴素贝叶斯模型。模型中涉及的超参数是“var_smoothing ”,也称为“Laplace Smoothing ”,用于避免由于非常小的特征值导致的数值不稳定性。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="e81c" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">hyperparameter = {'var_smoothing':[10**x for x in range(-9,3)]}</strong></span><span id="5684" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">from sklearn.naive_bayes import GaussianNB</strong></span><span id="b541" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">nb = GaussianNB()<br/>nb_grid = GridSearchCV(nb, param_grid=hyperparameter, cv=5, verbose=1, n_jobs=-1)</strong></span><span id="3a97" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">nb_grid_results = model(nb_grid, X_train_1.toarray(), Y_train, X_test_1.toarray(), Y_test)</strong></span><span id="46fa" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">--------------------------------------------------------------------</strong></span><span id="2955" class="lq jt hi lm b fi lv ls l lt lu">Fitting the model and prediction on train data:<br/>Fitting 5 folds for each of 12 candidates, totalling 60 fits</span><span id="80c7" class="lq jt hi lm b fi lv ls l lt lu">[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.<br/>[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    8.5s<br/>[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   12.1s finished</span><span id="2a8c" class="lq jt hi lm b fi lv ls l lt lu">Completed<br/>Time taken: 0:00:17.167590<br/>==================================================<br/>Prediction on test data:<br/>Completed<br/>Time taken: 0:00:00.712164<br/>==================================================<br/>Performance metrics:<br/>==================================================<br/>Precision score is:<br/>0.9637974665033534<br/>==================================================<br/>Recall score is:<br/>0.974201170426134<br/>==================================================<br/>F1-score is:<br/>0.9679678294214985</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nk"><img src="../Images/b32f9cd5d50371ade7c0af0cd701eb1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eMLrXLDk5cp65pc-aPO3Ug.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">NB混淆矩阵</figcaption></figure><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="5e88" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">print_grid_search_attributes(nb_grid)</strong></span><span id="2a86" class="lq jt hi lm b fi lv ls l lt lu">--------------------------------------------------------------------</span><span id="df3b" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx">---------------------------<br/>|      Best Estimator     |<br/>---------------------------</em></span><span id="51e0" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx">	GaussianNB(priors=None, var_smoothing=10)</em></span><span id="6ae4" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx">---------------------------<br/>|     Best parameters     |<br/>---------------------------<br/>	Parameters of best estimator : </em></span><span id="cf15" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx">	{'var_smoothing': 10}</em></span><span id="db79" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx">----------------------------------<br/>|   No of CrossValidation sets   |<br/>----------------------------------</em></span><span id="3063" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx">	Total number of cross validation sets: 5</em></span><span id="a147" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx">---------------------------<br/>|        Best Score       |<br/>---------------------------</em></span><span id="a12c" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx">	Average Cross Validate scores of best estimator : </em></span><span id="157d" class="lq jt hi lm b fi lv ls l lt lu"><em class="lx">	0.9729551511599154</em></span></pre><p id="bfc6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从NB分类器获得的最终结果如下:</p><p id="1d68" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">训练结果:- </em> </strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="3022" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">tpr_fpr_train</strong></span><span id="dbe6" class="lq jt hi lm b fi lv ls l lt lu">{'fp': 2225,<br/> 'fpr': 0.051367886413482625,<br/> 'tp': 65483,<br/> 'tpr': 0.9940644260254425}</span><span id="3133" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">nb_grid_results_tr</strong></span><span id="02b1" class="lq jt hi lm b fi lv ls l lt lu">{'f1_score': 0.9671813437943309,<br/> 'precision': 0.9632732426450655,<br/> 'recall': 0.9738984696260612}</span></pre><p id="9d17" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">测试结果:- </em> </strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="143c" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">tpr_fpr_test</strong></span><span id="1045" class="lq jt hi lm b fi lv ls l lt lu">{'fp': 710, 'fpr': 0.04917238035875061, 'tp': 21814, 'tpr': 0.9934420256853994}</span><span id="ec5a" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">nb_grid_results_test</strong></span><span id="fc22" class="lq jt hi lm b fi lv ls l lt lu">{'f1_score': 0.9679678294214985,<br/> 'model': &lt;function __main__.model(model_name, X_train, Y_train, X_test, Y_test)&gt;,<br/> 'precision': 0.9637974665033534,<br/> 'recall': 0.974201170426134}</span></pre><p id="1473" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">观察来自NB的量词:- </em> </strong></p><ul class=""><li id="0fef" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">测试数据总共有36397个点。其中，21958个点属于正常连接，其余14439个点属于不良连接。</li><li id="486c" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">在21958个正常连接点中，21814个(99.34%)被朴素贝叶斯分类器正确分类。</li><li id="d4dc" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">在属于不良连接的14439个点中，海王星类具有最高数量的数据点12955，其中12954(99.99%)被正确分类。</li><li id="828b" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">在具有非常少数量的数据点的类中，guess_passwd类被分类为具有(12/13) 92.30%的准确度，butter_overflow类具有(6/7) 85.71%的准确度，warezmaster类具有(4/5) 80%的准确度，land类具有(4/5) 80%的准确度，imap类具有(0/3) 0%的准确度，loadmodule类具有(1/2) 50%的准确度，rootkit类具有(0/2) 0%的准确度，多跳类</li><li id="5d79" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">虽然朴素贝叶斯分类器能够对具有0.9670的高f1分数的点进行分类，但是我们可以提前使用更高级的非线性和线性分类器，并且我们将尝试对具有更高f1分数的正常和不良连接进行分类。</li><li id="d0cd" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">误报:710</li><li id="a397" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">假阳性率:0.049</li><li id="9031" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">真阳性:21814</li><li id="3d24" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">真阳性率:0.9934</li><li id="3c60" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">由于训练和测试分数几乎相似且很高，我们可以说模型是<em class="lx">“既不过拟合也不欠拟合”。</em></li></ul><h1 id="49ce" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">模型2:-决策树分类器</h1><p id="3ff3" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">决策树分类器是一种非线性ML分类器，它使用多个线/平面/超平面来做出决策，并对属于不同类别的点进行分类，类似于if-else语句。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="5f22" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">hyperparameter = {'max_depth':[5, 10, 20, 50, 100, 500], 'min_samples_split':[5, 10, 100, 500]}</strong></span><span id="ffa3" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">from sklearn.tree import DecisionTreeClassifier</strong></span><span id="8a99" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">decision_tree = DecisionTreeClassifier(criterion='gini', splitter='best',class_weight='balanced')<br/>decision_tree_grid = GridSearchCV(decision_tree, param_grid=hyperparameter, cv=3, verbose=1, n_jobs=-1)</strong></span><span id="b3aa" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">decision_tree_grid_results = model(decision_tree_grid, X_train_1.toarray(), Y_train, X_test_1.toarray(), Y_test)</strong></span><span id="7b57" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">--------------------------------------------------------------------</strong></span><span id="7585" class="lq jt hi lm b fi lv ls l lt lu">Fitting the model and prediction on train data:<br/>Fitting 3 folds for each of 24 candidates, totalling 72 fits</span><span id="61ab" class="lq jt hi lm b fi lv ls l lt lu">[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.<br/>[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.8s<br/>[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:   36.3s finished</span><span id="79b1" class="lq jt hi lm b fi lv ls l lt lu">Completed<br/>Time taken: 0:00:36.574308<br/>==================================================<br/>Prediction on test data:<br/>Completed<br/>Time taken: 0:00:00.018077<br/>==================================================<br/>Performance metrics:<br/>==================================================<br/>Precision score is:<br/>0.9986638296866037<br/>==================================================<br/>Recall score is:<br/>0.9985713108223205<br/>==================================================<br/>F1-score is:<br/>0.9986068375429693</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nl"><img src="../Images/16dfd782501df00a6f987486d280a46e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BlDa2BeHKNxQeM4hGqkqAw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">DT_1混淆矩阵</figcaption></figure><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="004a" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">print_grid_search_attributes(decision_tree_grid)</strong></span><span id="939e" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">--------------------------------------------------------------------</strong></span><span id="5d8c" class="lq jt hi lm b fi lv ls l lt lu">---------------------------<br/>|      Best Estimator     |<br/>---------------------------</span><span id="0388" class="lq jt hi lm b fi lv ls l lt lu">	DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=50,<br/>                       max_features=None, max_leaf_nodes=None,<br/>                       min_impurity_decrease=0.0, min_impurity_split=None,<br/>                       min_samples_leaf=1, min_samples_split=5,<br/>                       min_weight_fraction_leaf=0.0, presort=False,<br/>                       random_state=None, splitter='best')</span><span id="5800" class="lq jt hi lm b fi lv ls l lt lu">---------------------------<br/>|     Best parameters     |<br/>---------------------------<br/>	Parameters of best estimator : </span><span id="26d4" class="lq jt hi lm b fi lv ls l lt lu">	{'max_depth': 50, 'min_samples_split': 5}</span><span id="17df" class="lq jt hi lm b fi lv ls l lt lu">----------------------------------<br/>|   No of CrossValidation sets   |<br/>----------------------------------</span><span id="c471" class="lq jt hi lm b fi lv ls l lt lu">	Total number of cross validation sets: 3</span><span id="a591" class="lq jt hi lm b fi lv ls l lt lu">---------------------------<br/>|        Best Score       |<br/>---------------------------</span><span id="cd6d" class="lq jt hi lm b fi lv ls l lt lu">	Average Cross Validate scores of best estimator : </span><span id="1275" class="lq jt hi lm b fi lv ls l lt lu">	0.9983056901336215</span></pre><p id="4378" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从DT分类器获得的最终结果如下:</p><p id="b806" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">训练结果:- </em> </strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="a552" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">decision_tree_grid_results_tr</strong></span><span id="6c30" class="lq jt hi lm b fi lv ls l lt lu">{'f1_score': 0.9997583211262271,<br/> 'precision': 0.9997729384543836,<br/> 'recall': 0.9997527223438258}</span><span id="3e5a" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">dt_tpr_fpr_train</strong></span><span id="295b" class="lq jt hi lm b fi lv ls l lt lu">{'fp': 0, 'fpr': 0.0, 'tp': 65853, 'tpr': 0.9996812095819292}</span></pre><p id="5752" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">测试结果:- </em> </strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="895f" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">decision_tree_grid_results_test</strong></span><span id="8633" class="lq jt hi lm b fi lv ls l lt lu">{'f1_score': 0.99860727686375,<br/> 'model': &lt;function __main__.model(model_name, X_train, Y_train, X_test, Y_test)&gt;,<br/> 'precision': 0.9986657857309603,<br/> 'recall': 0.9985713108223205}</span><span id="2f2d" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">dt_tpr_fpr_test</strong></span><span id="e1c5" class="lq jt hi lm b fi lv ls l lt lu">{'fp': 19, 'fpr': 0.001315880601149664, 'tp': 21937, 'tpr': 0.9990436287457874}</span></pre><p id="c469" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">来自DT分类器的观察:- </em> </strong></p><ul class=""><li id="2617" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">在21958个正常连接点中，21937个(99.90%)被决策树分类器正确分类。</li><li id="fbb4" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">在属于不良连接的14439个点中，海王星类具有最高数量的数据点12955，其中12953(99.98%)被正确分类。</li><li id="85ba" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">在具有非常少数量的数据点的类中，guess_passwd类以(13/13) 100%的准确度分类，butter_overflow类以(6/7) 85.71%的准确度分类，warezmaster类以(5/5) 100%的准确度分类，land类以(4/5) 80%的准确度分类，imap类以(3/3) 100%的准确度分类，load module类以(0/2) 0%的准确度分类，rootkit类以(1/2) 50%的准确度分类，多点分类</li><li id="f00d" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">与所有先前的分类器相比，决策树分类器能够对具有0.9986的较高f1分数的点进行分类。</li><li id="3869" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">真阳性= 21937</li><li id="3893" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">TPR = 0.9990</li><li id="7bf8" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">假阳性= 19</li><li id="0314" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">FPR = 0.0013</li><li id="c904" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">与上述所有模型相比，DT分类器具有最低的FPR和最高的TPR。</li><li id="9bcc" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">因此，我们可以说，与像逻辑回归这样的线性分类器相比，像DT这样的非线性ML模型能够以更好的方式从数据集中学习模式。</li></ul><h1 id="c3a3" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">模型_3 :- XGBoost分类器</h1><p id="1171" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">XGBoost分类器使用增强的概念，该概念使用多个弱学习器(即深度为1或2的决策树)，其中每个新的树从先前弱学习器所犯的错误中学习，并且类似地多个这样的弱学习器(100的倍数)组合在一起以形成强的最终模型。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="7d48" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">hyperparameter = {'max_depth':[2, 3, 5, 7, 10], 'n_estimators': [10, 50, 100, 200, 500]}</strong></span><span id="6138" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">from xgboost import XGBClassifier</strong></span><span id="2ea9" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">xgb = XGBClassifier(objective='multi:softprob')<br/>xgb_grid = GridSearchCV(xgb, param_grid=hyperparameter, cv=3, verbose=1, n_jobs=-1)</strong></span><span id="dea5" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">xgb_grid_results = model(xgb_grid, X_train_1.toarray(), Y_train, X_test_1.toarray(), Y_test)</strong></span><span id="76c4" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">--------------------------------------------------------------------</strong></span><span id="3710" class="lq jt hi lm b fi lv ls l lt lu">Fitting the model and prediction on train data:<br/>Fitting 3 folds for each of 25 candidates, totalling 75 fits</span><span id="3c33" class="lq jt hi lm b fi lv ls l lt lu">[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.<br/>[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 51.3min<br/>[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed: 143.6min finished</span><span id="755a" class="lq jt hi lm b fi lv ls l lt lu">Completed<br/>Time taken: 0:19:06.398788<br/>==================================================<br/>Prediction on test data:<br/>Completed<br/>Time taken: 0:00:10.396326<br/>==================================================<br/>Performance metrics:<br/>==================================================<br/>Precision score is:<br/>0.9993660928288938<br/>==================================================<br/>Recall score is:<br/>0.9995054537461878<br/>==================================================<br/>F1-score is:<br/>0.9994282483855045</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nm"><img src="../Images/95c235875b974e2f29a8445458a6f50e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_tabOTQKS7oZt3cXnZ_FNQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">XGB_1混淆矩阵</figcaption></figure><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="2c1f" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">print_grid_search_attributes(xgb_grid)</strong></span><span id="abf8" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">--------------------------------------------------------------------</strong></span><span id="ddeb" class="lq jt hi lm b fi lv ls l lt lu">---------------------------<br/>|      Best Estimator     |<br/>---------------------------</span><span id="e540" class="lq jt hi lm b fi lv ls l lt lu">	XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=5,<br/>min_child_weight=1, missing=None, n_estimators=500, n_jobs=1,<br/>nthread=None, objective='multi:softprob', random_state=0,<br/>reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,<br/>silent=None, subsample=1, verbosity=1)</span><span id="48f7" class="lq jt hi lm b fi lv ls l lt lu">---------------------------<br/>|     Best parameters     |<br/>---------------------------<br/>	Parameters of best estimator : </span><span id="60a1" class="lq jt hi lm b fi lv ls l lt lu">	{'max_depth': 5, 'n_estimators': 500}</span><span id="685d" class="lq jt hi lm b fi lv ls l lt lu">----------------------------------<br/>|   No of CrossValidation sets   |<br/>----------------------------------</span><span id="815c" class="lq jt hi lm b fi lv ls l lt lu">	Total number of cross validation sets: 3</span><span id="ea3d" class="lq jt hi lm b fi lv ls l lt lu">---------------------------<br/>|        Best Score       |<br/>---------------------------</span><span id="91fd" class="lq jt hi lm b fi lv ls l lt lu">	Average Cross Validate scores of best estimator : </span><span id="bfeb" class="lq jt hi lm b fi lv ls l lt lu">	0.9992123748729268</span></pre><p id="2499" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从XGBoost分类器获得的最终结果如下:</p><p id="040a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">训练结果:- </em> </strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="8141" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">xgb_grid_results_tr</strong></span><span id="23e3" class="lq jt hi lm b fi lv ls l lt lu">{'f1_score': 0.9999816952580419,<br/> 'precision': 0.9999817609521351,<br/> 'recall': 0.9999816831365796}</span><span id="9d20" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">xgb_tpr_fpr_train</strong></span><span id="f857" class="lq jt hi lm b fi lv ls l lt lu">{'fp': 0, 'fpr': 0.0, 'tp': 65873, 'tpr': 0.9999848195039014}</span></pre><p id="f9f6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">测试结果:- </em> </strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="2e27" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">xgb_grid_results_test</strong></span><span id="72d9" class="lq jt hi lm b fi lv ls l lt lu">{'f1_score': 0.9994282483855045,<br/> 'model': &lt;function __main__.model(model_name, X_train, Y_train, X_test, Y_test)&gt;,<br/> 'precision': 0.9993660928288938,<br/> 'recall': 0.9995054537461878}</span><span id="83aa" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">xgb_tpr_fpr_test</strong></span><span id="08b6" class="lq jt hi lm b fi lv ls l lt lu">{'fp': 12,<br/> 'fpr': 0.0008310824849366299,<br/> 'tp': 21955,<br/> 'tpr': 0.9998633755351125}</span></pre><p id="9aee" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">来自XGBoost分类器的观察值:- </em> </strong></p><ul class=""><li id="9eff" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">在21958个正常连接点中，21955个(99.98%)被XGBoost分类器正确分类。</li><li id="90f1" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">在属于不良连接的17409个点中，海王星类具有最高数量的数据点12955，其中12955(100.0%)被正确分类。</li><li id="3091" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">在具有非常少数量的数据点的类中，guess_passwd类被分类为具有(12/13) 92.30%的准确度，butter_overflow类具有(7/7) 100.0%的准确度，warezmaster类具有(4/5) 80%的准确度，land类具有(5/5) 100%的准确度，imap类具有(3/3) 100%的准确度，loadmodule类具有(0/2) 0%的准确度，rootkit类具有(0/2) 0%的准确度，class</li><li id="f2f1" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">与上述所有模型相比，XGBoost分类器能够以最高的f1分数(0.9994)对不同的类进行分类。</li><li id="992b" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">真阳性= 21955</li><li id="f120" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">TPR = 0.9998</li><li id="a062" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">假阳性= 12</li><li id="2048" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">FPR = 0.00083</li><li id="39e7" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">与所有以前的模型相比，XGBoost分类器具有最高的TPR和最低的FPR。</li><li id="ba25" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">由于f1-score、tpr和for等训练和测试指标几乎相似，因此该模型不会过度拟合。</li></ul><p id="9ef5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">同样，我们应用了剩余的型号&amp;下面是它们性能的总结:</em> </strong></p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nn"><img src="../Images/4111a4d91f9fae13a226801dcf3b11fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LOvrcLHtahsn3hVu66eufw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">结果</figcaption></figure><h1 id="9699" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">上述所有分类者的观察结果:-</h1><ul class=""><li id="cd98" class="lz ma hi iw b ix kq jb kr jf mn jj mo jn mp jr me mf mg mh bi translated">如果我们将正常连接点视为1类，将属于所有其他22个不良连接类的点视为第2类，那么XGBoost分类器是最好的分类器，因为它具有0.9998和0.00083的TPR和FPR。</li><li id="1acc" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">虽然XGBoost分类器比r f分类器具有更好的f1_score，但是如果我们深入研究混淆矩阵分数的细节，我们可以观察到两个分类器在我们的数据集上对不同类别的攻击表现相似。</li><li id="c6f4" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">RF分类器的TPR和FPR分别为0.9998和0.0013。</li><li id="e24d" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">与XGBoost分类器相比，RF和DT分类器用于训练+评估的总时间更少。</li><li id="1dae" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">所有分类器显示的一个共同模式是，类rootkit、ftp_write和load module被大多数分类器误分类为类Normal。</li><li id="8c3f" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">我们将向数据集添加更多的特征，并尝试提高分类器的性能。</li><li id="2940" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">由于DT、RF和XGBoost具有最佳性能，我们将在现有的+特征工程数据上提前使用这3个分类器。</li></ul><p id="2237" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">由于f1-score、tpr和fpr等训练和测试指标对于训练和测试数据集具有几乎相似的分数，因此模型不会过度拟合。</em>T3】</strong></p><h1 id="8838" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">(IX)增加新功能</h1><ol class=""><li id="c0dd" class="lz ma hi iw b ix kq jb kr jf mn jj mo jn mp jr mr mf mg mh bi translated"><strong class="iw hj">聚类特征(使用迷你批处理方式):</strong></li></ol><p id="4150" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">聚类是一种无监督的ML技术，它将相似(较近)的点分组到同一个聚类中，将不相似(较远)的点分组到不同的聚类中。我们在这个问题中使用聚类的原因是，如果KMeans算法最终将属于同一类别的点分组到同一个聚类中，那么它将允许我们的模型学习一个对分类测试数据点非常重要的新特征。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="db9d" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">from sklearn.cluster import MiniBatchKMeans<br/>import numpy as np</strong></span><span id="02de" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">kmeans = MiniBatchKMeans(n_clusters=23, random_state=0, batch_size=128, max_iter=100)<br/>kmeans.fit(X_train_1)</strong></span><span id="0cc8" class="lq jt hi lm b fi lv ls l lt lu">MiniBatchKMeans(batch_size=128, compute_labels=True, init='k-means++', init_size=None, max_iter=100, max_no_improvement=10,<br/>n_clusters=23, n_init=3, random_state=0,reassignment_ratio=0.01, tol=0.0, verbose=0)</span><span id="bd31" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">train_cluster = kmeans.predict(X_train_1)<br/>test_cluster = kmeans.predict(X_test_1)</strong></span><span id="f583" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">--------------------------------------------------------------------</strong></span><span id="c087" class="lq jt hi lm b fi lv ls l lt lu">print('Length of train cluster',len(train_cluster))<br/>print(train_cluster)</span><span id="7fc7" class="lq jt hi lm b fi lv ls l lt lu">Length of train cluster 109189<br/>array([8, 0, 1, ..., 4, 4, 1], dtype=int32)</span><span id="e6ee" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">--------------------------------------------------------------------</strong></span><span id="8cf9" class="lq jt hi lm b fi lv ls l lt lu">print('Length of test cluster',len(train_cluster))<br/>print(test_cluster)</span><span id="c90a" class="lq jt hi lm b fi lv ls l lt lu">Length of test cluster 109189<br/>array([ 1, 22,  8, ...,  0, 17,  8], dtype=int32)</span></pre><p id="30dd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 2。PCA特性:- </strong></p><p id="d94f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">PCA是一种降维ML技术，它将给定的d维数据集转换成d’维，其中每个新特征(主成分)是基于它们携带的方差或信息的量获得的。我们将把前5个PCA特征添加到我们的数据集中。(我们可以增加或减少，并测试它们是否能提高性能)</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="c90c" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">from sklearn.decomposition import PCA</strong></span><span id="84eb" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">pca = PCA(n_components=5)<br/>pca.fit(X_train_1.toarray())<br/>pca_train = pca.transform(X_train_1.toarray())<br/>pca_test = pca.transform(X_test_1.toarray())</strong></span><span id="9df4" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">--------------------------------------------------------------------</strong></span><span id="4460" class="lq jt hi lm b fi lv ls l lt lu">print(pca_train.shape)<br/>print(pca_test.shape)</span><span id="ffcf" class="lq jt hi lm b fi lv ls l lt lu">(109189, 5)<br/>(36397, 5)</span></pre><p id="9daa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> 3。附加功能工程:- </strong></p><ul class=""><li id="517f" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">我们将从数据中创建新特征，如:<br/> (i)添加2个现有特征，(例如，new _ feature _ 1 = src _ bytes+dst _ bytes)<br/>(ii)减去2个现有特征，(例如，new _ feature _ 2 = ABS(src _ bytes-dst _ bytes)。</li></ul><p id="f7be" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">(a)<em class="lx">src _ bytes+dst _ bytes</em></strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="4744" class="lq jt hi lm b fi lr ls l lt lu">feature_src_dst_1 = src_bytes1 + dst_bytes1<br/>feature_src_dst_2 = src_bytes2 + dst_bytes2</span><span id="63de" class="lq jt hi lm b fi lv ls l lt lu">feature_src_dst_1.shape<br/>(109189, 1)</span></pre><p id="89c0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">(b)</strong><strong class="iw hj"><em class="lx">src _ bytes—dst _ bytes</em></strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="375d" class="lq jt hi lm b fi lr ls l lt lu">feature_src_dst_3 = src_bytes1 - dst_bytes1<br/>feature_src_dst_4 = src_bytes2 - dst_bytes2</span><span id="d9a0" class="lq jt hi lm b fi lv ls l lt lu">feature_src_dst_3.shape<br/>(109189, 1)</span></pre><p id="acdf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"><em class="lx">(c)same _ SRV _ rate+diff _ SRV _ rate:-</em></strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="a042" class="lq jt hi lm b fi lr ls l lt lu">feature_5 = same_srv_rate1 + diff_srv_rate1<br/>feature_6 = same_srv_rate2 + diff_srv_rate2</span><span id="c756" class="lq jt hi lm b fi lv ls l lt lu">feature_5.shape<br/>(109189, 1)</span></pre><p id="99bc" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"><em class="lx">(d)dst _ host _ same _ SRV _ rate+dst _ host _ diff _ SRV _ rate:-</em>T29】</strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="e358" class="lq jt hi lm b fi lr ls l lt lu">feature_7 = dst_host_same_srv_rate1 + dst_host_diff_srv_rate1<br/>feature_8 = dst_host_same_srv_rate2 + dst_host_diff_srv_rate2</span><span id="383c" class="lq jt hi lm b fi lv ls l lt lu">feature_7.shape<br/>(109189, 1)</span></pre><p id="fcc0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">使用额外的4个特征将聚类和PCA特征添加到我们的数据集中:- </strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="d7de" class="lq jt hi lm b fi lr ls l lt lu">X_train_2 = hstack((X_train_1, pca_train, train_cluster.T, feature_src_dst_1, feature_src_dst_3, feature_5, feature_7))</span><span id="b77a" class="lq jt hi lm b fi lv ls l lt lu">X_test_2 = hstack((X_test_1, pca_test, test_cluster.T, feature_src_dst_2, feature_src_dst_4, feature_6, feature_8))</span><span id="ec7a" class="lq jt hi lm b fi lv ls l lt lu">print('Train data:')<br/>print(X_train_2.shape)<br/>print('='*30)<br/>print('Test data:')<br/>print(X_test_2.shape)</span><span id="c28b" class="lq jt hi lm b fi lv ls l lt lu">Train data:<br/>(109189, 126)<br/>==============================<br/>Test data:<br/>(36397, 126)</span></pre><h1 id="a925" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">(X)应用机器学习模型</h1><p id="47cb" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">我们将在数据集2上应用以下3个模型，因为它们是数据集1上表现最好的模型:</p><ol class=""><li id="ea7e" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr mr mf mg mh bi translated">决策图表</li><li id="223f" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr mr mf mg mh bi translated">随机森林</li><li id="8529" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr mr mf mg mh bi translated">XGBoost</li></ol><h1 id="6735" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">型号:- XGBoost分类器</h1><p id="bb59" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">下面我们在dataset_2上应用了XGBoost分类器并评估了性能。</p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="a02a" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">from xgboost import XGBClassifier<br/>from sklearn.model_selection import RandomizedSearchCV</strong></span><span id="3673" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">hyperparameter = {'max_depth':[2, 3, 5, 7, 10], 'n_estimators': [10, 50, 100, 200, 500]}</strong></span><span id="ecc2" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">xgb = XGBClassifier(objective='multi:softprob', n_jobs=-1)<br/>xgb_grid = RandomizedSearchCV(xgb, param_distributions=hyperparameter, cv=3, verbose=1, n_jobs=-1)</strong></span><span id="e561" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">xgb_grid_results2 = model(xgb_grid, X_train_2.toarray(), Y_train, X_test_2.toarray(), Y_test)</strong></span><span id="97f1" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">--------------------------------------------------------------------</strong></span><span id="6c18" class="lq jt hi lm b fi lv ls l lt lu">Fitting the model and prediction on train data:<br/>Fitting 3 folds for each of 10 candidates, totalling 30 fits</span><span id="ab6a" class="lq jt hi lm b fi lv ls l lt lu">[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.<br/>[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 57.3min finished</span><span id="ef62" class="lq jt hi lm b fi lv ls l lt lu">Completed<br/>Time taken: 2:00:09.666172<br/>==================================================<br/>Prediction on test data:<br/>Completed<br/>Time taken: 0:00:24.416710<br/>==================================================<br/>Performance metrics:<br/>==================================================<br/>==================================================<br/>Precision score is:<br/>0.9994189203758796<br/>==================================================<br/>Recall score is:<br/>0.999450504162431<br/>==================================================<br/>F1-score is:<br/>0.9994241935579559</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nl"><img src="../Images/2ddb0e2f418bc4b41f33d87180c88fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ykc5lt03FyEzReuQoHkw3Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">XGB混淆矩阵</figcaption></figure><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="5a0c" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">print_grid_search_attributes(xgb_grid)</strong></span><span id="c1a8" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">--------------------------------------------------------------------</strong></span><span id="8157" class="lq jt hi lm b fi lv ls l lt lu">---------------------------<br/>|      Best Estimator     |<br/>---------------------------</span><span id="27e0" class="lq jt hi lm b fi lv ls l lt lu">XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,<br/>min_child_weight=1, missing=None, n_estimators=200, n_jobs=-1,<br/>nthread=None, objective='multi:softprob', random_state=0,<br/>reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,<br/>silent=None, subsample=1, verbosity=1)</span><span id="3d2e" class="lq jt hi lm b fi lv ls l lt lu">---------------------------<br/>|     Best parameters     |<br/>---------------------------<br/>	Parameters of best estimator : </span><span id="437c" class="lq jt hi lm b fi lv ls l lt lu">	{'n_estimators': 200, 'max_depth': 3}</span><span id="674a" class="lq jt hi lm b fi lv ls l lt lu">----------------------------------<br/>|   No of CrossValidation sets   |<br/>----------------------------------</span><span id="e739" class="lq jt hi lm b fi lv ls l lt lu">	Total number of cross validation sets: 3</span><span id="dba4" class="lq jt hi lm b fi lv ls l lt lu">---------------------------<br/>|        Best Score       |<br/>---------------------------</span><span id="87f3" class="lq jt hi lm b fi lv ls l lt lu">	Average Cross Validate scores of best estimator : </span><span id="c70c" class="lq jt hi lm b fi lv ls l lt lu">	0.999166582714376</span></pre><p id="4d74" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从XGBoost_2分类器获得的最终结果如下:</p><p id="3fae" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">训练结果:- </em> </strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="8766" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">xgb_grid_results_tr</strong></span><span id="e8be" class="lq jt hi lm b fi lv ls l lt lu">{'f1_score': 0.9999816952580419,<br/> 'precision': 0.9999817609521351,<br/> 'recall': 0.9999816831365796}</span><span id="667e" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">xgb_tpr_fpr_train</strong></span><span id="4383" class="lq jt hi lm b fi lv ls l lt lu">{'fp': 0, 'fpr': 0.0, 'tp': 65873, 'tpr': 0.9999848195039014}</span></pre><p id="1761" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">测试结果:- </em> </strong></p><pre class="lh li lj lk fd ll lm ln lo aw lp bi"><span id="d2b5" class="lq jt hi lm b fi lr ls l lt lu"><strong class="lm hj">xgb_grid_results_test</strong></span><span id="0e42" class="lq jt hi lm b fi lv ls l lt lu">{'f1_score': 0.9994241935579559,<br/> 'model': &lt;function __main__.model(model_name, X_train, Y_train, X_test, Y_test)&gt;,<br/> 'precision': 0.9994189203758796,<br/> 'recall': 0.999450504162431}</span><span id="1c1e" class="lq jt hi lm b fi lv ls l lt lu"><strong class="lm hj">xgb_tpr_fpr_test</strong></span><span id="85ce" class="lq jt hi lm b fi lv ls l lt lu">{'fp': 12,<br/> 'fpr': 0.0008310824849366299,<br/> 'tp': 21955,<br/> 'tpr': 0.9998633755351125}</span></pre><p id="a968" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">来自XGBoost_2分类器的观察:- </em> </strong></p><ul class=""><li id="66de" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">这种XG Boost分类器能够以大约99.94的更好精度和大约0.9994的高f1分数对点进行分类，这与第一个XGB分类器的性能相似。</li><li id="52b1" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">真阳性= 21955</li><li id="179e" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">TPR = 0.9998</li><li id="c495" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">假阳性= 12</li><li id="5e5b" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">FPR = 0.00083</li><li id="c7b8" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated">这种XGB分类器的TPR为99.98%，FPR为0.083%，与XGB1模型(0.083%)相同。</li></ul><p id="e647" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lx">同样，我们在同一数据集上应用了DT和RF模型，下面是获得的结果。</em>T3】</strong></p><p id="64f4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">以下是应用于数据集2的3个模型的结果。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es no"><img src="../Images/7c30a111cc876f07f9d32b57e9b8ad1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ef8M2W_V2GBSWQH2cbLVmg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">结果</figcaption></figure><h1 id="7357" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">从上述3个模型中得出的重要观察结果:-</h1><ul class=""><li id="758f" class="lz ma hi iw b ix kq jb kr jf mn jj mo jn mp jr me mf mg mh bi translated"><strong class="iw hj"> <em class="lx">根据我们从上述3个模型中获得的性能分数，我们可以得出结论，添加新功能增加了TPR分数，因为“正常”类别分数的正确分类数增加了，但这也导致了所有3个模型的FPR分数增加，这是不可取的。</em> </strong></li></ul><h1 id="7ec8" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">(XI)总结结果并做出结论:-</h1><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es np"><img src="../Images/854b6492131a2b33585ec370d6cddd87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*skLHJ5bkG95U8xmAeZwMXQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">结果</figcaption></figure><ul class=""><li id="7114" class="lz ma hi iw b ix iy jb jc jf mb jj mc jn md jr me mf mg mh bi translated">所有模型在训练和测试数据上具有非常接近的性能分数，因此它们没有过度拟合。</li></ul><h2 id="0acd" class="lq jt hi bd ju mx my mz jy na nb nc kc jf nd ne kg jj nf ng kk jn nh ni ko nj bi translated">XGBoost _ 1型号是我们入侵检测的最佳型号，因为它的f1测试得分最高，为0.9994，TPR为99.98%，FPR最低，为0.083%。</h2><p id="c146" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">这使我们结束了这个有趣的案例研究，我们使用KDD杯99数据集并应用不同的ML技术来构建一个网络入侵检测系统，该系统能够以良好的精度分类好的和坏的连接，同时减少误报的数量。</p><p id="4ae6" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">要获得完整的代码片段，您可以访问我下面的GitHub库，在那里，我也尝试通过将22个不良类别合并为一个“不良”类别，将这个入侵检测问题作为二进制分类问题来解决。</p><div class="kv kw ez fb kx ky"><a href="https://github.com/Saurabh2805/my_ml" rel="noopener  ugc nofollow" target="_blank"><div class="kz ab dw"><div class="la ab lb cl cj lc"><h2 class="bd hj fi z dy ld ea eb le ed ef hh bi translated">Saurabh2805/my_ml</h2><div class="lf l"><h3 class="bd b fi z dy ld ea eb le ed ef dx translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="lg l"><p class="bd b fp z dy ld ea eb le ed ef dx translated">github.com</p></div></div><div class="nq l"><div class="nr l ns nt nu nq nv io ky"/></div></div></a></div><h1 id="177b" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">参考资料:</h1><ul class=""><li id="01de" class="lz ma hi iw b ix kq jb kr jf mn jj mo jn mp jr me mf mg mh bi translated">要确定使用的性能指标和分类器，请下载PDF:-<a class="ae lw" href="https://www.researchgate.net/publication/309038723_A_review_of_KDD99_dataset_usage_in_intrusion_detection_and_machine_learning_between_2010_and_2015" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/publication/309038723 _ A _ review _ of _ KDD 99 _ dataset _ usage _ in _ intrusion _ detection _ and _ machine _ learning _ between _ 2010 _ and _ 2015</a></li><li id="2021" class="lz ma hi iw b ix mi jb mj jf mk jj ml jn mm jr me mf mg mh bi translated"><a class="ae lw" href="https://arxiv.org/pdf/1811.05372" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1811.05372</a></li></ul><h1 id="c29e" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">谢谢你。</h1></div></div>    
</body>
</html>