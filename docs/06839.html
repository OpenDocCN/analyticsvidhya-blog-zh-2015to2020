<html>
<head>
<title>Video Content-Based Advertisement Recommendation Using Text Classification Technique</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于视频内容的文本分类广告推荐</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/video-content-based-advertisement-recommendation-using-nlp-4512430c169e?source=collection_archive---------4-----------------------#2020-06-04">https://medium.com/analytics-vidhya/video-content-based-advertisement-recommendation-using-nlp-4512430c169e?source=collection_archive---------4-----------------------#2020-06-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/87987d2d3fdacf1cdc744bc7560b3b12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IHjL2giQ_pP7-rAY"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">亚伦·塞巴斯蒂安在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h2 id="51a2" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">为什么是广告推荐系统？：</h2><ul class=""><li id="f7e5" class="jt ju hi jv b jw jx jy jz jg ka jk kb jo kc kd ke kf kg kh bi translated">广告是一种营销传播形式，用于鼓励或操纵受众继续或采取一些新的行动。</li><li id="0729" class="jt ju hi jv b jw ki jy kj jg kk jk kl jo km kd ke kf kg kh bi translated">我们每天看到多少广告？几十个？几百个？或者几千个？</li><li id="c513" class="jt ju hi jv b jw ki jy kj jg kk jk kl jo km kd ke kf kg kh bi translated">一天结束时我们还记得多少？</li><li id="ab24" class="jt ju hi jv b jw ki jy kj jg kk jk kl jo km kd ke kf kg kh bi translated">现代人平均每天接触大约5000个广告。(扬凯洛维奇公司— 2007年)</li></ul><p id="19af" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated">让我们从理解术语开始</p><h2 id="cd44" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">概述:</h2><p id="7b2e" class="pw-post-body-paragraph kn ko hi jv b jw jx kq kr jy jz kt ku jg le kw kx jk lf kz la jo lg lc ld kd hb bi translated">今天，每个人都可以在他们的手机、笔记本电脑上轻松访问流媒体内容，视频已经成为互联网上非常重要和流行的内容。如今，人们制作自己的内容并上传到流媒体平台上，因此与文本、音频和图像数据集相比，视频数据集的规模变得非常庞大。根据许多用户的说法，他们对在线流媒体播放时显示的无关广告感到恼火，所以他们不得不等到视频结束后才能避免这些广告。因此，我们的目标是显示与视频内容相关的广告。我们可以通过准确的在线广告来推动业务，这些广告通过检测视频的主题与内容相关。例如，我们可以将一些提供虚拟主机的公司放在网站开发教程视频上，而不是任何随机的视频上。因此，客户可以更有目的和更准确地向该主题发布广告，同时，用户不会被广告打扰，同时，他们可以观看整个视频或访问客户的网站以获得更多信息，这将吸引更多的客户。</p><h2 id="049d" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">文本分类:</h2><p id="94cc" class="pw-post-body-paragraph kn ko hi jv b jw jx kq kr jy jz kt ku jg le kw kx jk lf kz la jo lg lc ld kd hb bi translated">自然语言处理所要解决的一个经典问题就是分析原始文本的内容并确定其所属的类别。文本分类有着广泛的应用，如情感分析、主题标注、垃圾邮件检测和意图检测。</p><h2 id="09ca" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">推荐系统:</h2><p id="084b" class="pw-post-body-paragraph kn ko hi jv b jw jx kq kr jy jz kt ku jg le kw kx jk lf kz la jo lg lc ld kd hb bi translated">推荐/推荐系统的一个流行技术是<strong class="jv hj">基于内容的过滤。</strong>这里的内容是指视频的内容或属性。因此，基于内容的过滤的思想是使用某些关键字标记视频，在数据库中查找这些关键字，并推荐与该视频内容相关的广告。</p><h1 id="73dc" class="lh iw hi bd ix li lj lk jb ll lm ln jf lo lp lq jj lr ls lt jn lu lv lw jr lx bi translated">收集数据:</h1><p id="39d3" class="pw-post-body-paragraph kn ko hi jv b jw jx kq kr jy jz kt ku jg le kw kx jk lf kz la jo lg lc ld kd hb bi translated">我的目标是Youtube上可用的视频数据。收集的数据分为六大类:旅游博客、科技、食品、制造、历史、艺术和音乐。当处理像这样的定制机器学习问题时，我发现收集我的数据非常有用，如果不是简单地令人满意的话。对于这个问题，我需要一些关于属于不同类别的视频的元数据。在这里，我们将创建两个数据集，一个是视频数据，另一个是广告数据。</p><ol class=""><li id="af98" class="jt ju hi jv b jw kp jy ks jg ly jk lz jo ma kd mb kf kg kh bi translated"><strong class="jv hj">视频数据:</strong></li></ol><p id="eef9" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated">我将使用<strong class="jv hj"> Youtube API v3 </strong>。它是由谷歌自己创建的，通过一段专门为我们这样的程序员编写的代码与Youtube进行交互。前往<a class="ae iu" href="https://console.developers.google.com/" rel="noopener ugc nofollow" target="_blank"> Google开发者控制台</a>，创建一个示例项目并开始。我选择这样做的原因是，我需要收集成千上万的样本，我发现使用任何其他技术都不可能做到这一点。API的文档非常简单，在使用了超过8个电子邮件帐户来补偿所需的配额后，我收集了以下数据并将其存储在一个. csv文件中。如果您希望将该数据集用于您的项目，您可以在此处下载它<a class="ae iu" href="https://github.com/rishikonapure/Advertisement-Recommendation/blob/master/Videos_data.csv" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj">。</strong>T11】</a></p><p id="5a15" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated"><strong class="jv hj"> 2。广告数据:</strong></p><p id="43fa" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated">我已经收集了来自www.adforum.com的广告数据。它建立了一个广告公司简介和广告的数据库。根据网站上的信息，它目前提供超过20，000家机构的简介，以及来自世界各地110个国家的超过90，000种不同广告的可搜索图书馆。我通过网络抓取收集了这些广告数据。您可以通过此<a class="ae iu" href="https://www.pmg.com/blog/tutorial-crawl-youtube-channel/" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj">链接遵循程序。</strong> </a> <strong class="jv hj"> </strong>或者直接从我的GitHub repo <a class="ae iu" href="https://github.com/rishikonapure/Advertisement-Recommendation" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj">这里下载。</strong>T25】</a></p><h1 id="f6b9" class="lh iw hi bd ix li lj lk jb ll lm ln jf lo lp lq jj lr ls lt jn lu lv lw jr lx bi translated">导入数据:</h1><ul class=""><li id="5cec" class="jt ju hi jv b jw jx jy jz jg ka jk kb jo kc kd ke kf kg kh bi translated">首先，让我们导入必要的库。</li></ul><pre class="mc md me mf fd mg mh mi mj aw mk bi"><span id="bff8" class="iv iw hi mh b fi ml mm l mn mo">import pandas as pd<br/>import nltk<br/>#nltk.download()<br/>from nltk.corpus import stopwords<br/>import re<br/>import string<br/>from nltk.tokenize import word_tokenize<br/>from nltk.corpus import stopwords<br/>from nltk.stem.wordnet import WordNetLemmatize</span></pre><ul class=""><li id="7c74" class="jt ju hi jv b jw kp jy ks jg ly jk lz jo ma kd ke kf kg kh bi translated">接下来，我们将CSV文件中的数据加载到pandas数据帧中</li></ul><pre class="mc md me mf fd mg mh mi mj aw mk bi"><span id="09b1" class="iv iw hi mh b fi ml mm l mn mo"># Import Data<br/>vdata = pd.read_csv(‘Videos_data.csv’)<br/>vdata = data.iloc[:, 1:] # Remove extra un-named column<br/>vdata.head(10)</span><span id="660a" class="iv iw hi mh b fi mp mm l mn mo">#import data<br/>adata = pd.read_csv(‘collected_sports_data.csv’ )<br/>adata.head(10)</span></pre><h1 id="c86d" class="lh iw hi bd ix li lj lk jb ll lm ln jf lo lp lq jj lr ls lt jn lu lv lw jr lx bi translated">数据预处理和清理:</h1><p id="be34" class="pw-post-body-paragraph kn ko hi jv b jw jx kq kr jy jz kt ku jg le kw kx jk lf kz la jo lg lc ld kd hb bi translated">我的数据预处理过程的第一步是处理丢失的数据。因为丢失的值应该是文本数据，所以没有办法估算它们，因此唯一的选择是删除它们。接下来，我们将对剩余数据执行自然语言处理文本清理技术，以获得干净和所需的数据。这种方法分为以下几个步骤:</p><p id="9114" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated"><strong class="jv hj"> <em class="mq">转换成小写:</em> </strong>让所有文本保持相同的格式是一个好习惯，但是，将大写单词转换成小写不会改变单词的意思，例如，“足球”和“橄榄球”在语义上是相同的。</p><p id="7fbe" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated"><strong class="jv hj"> <em class="mq">删除数值和标点:</em> </strong>数值和标点中使用的特殊字符($，！等等。)无助于确定正确的类别。</p><p id="cf3b" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated"><strong class="jv hj"> <em class="mq">去除多余的空格:</em> </strong>这样每个单词都用一个空格隔开，否则在分词过程中可能会出现问题。</p><p id="50bf" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated"><strong class="jv hj"> <em class="mq">记号化成单词:</em> </strong>这是指将一个文本串拆分成一个‘记号’列表，其中每个记号就是一个单词。例如，句子“他坐在一棵树下”将被转换为['他'，'坐'，'下'，'一'，'树']。</p><p id="8614" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated"><strong class="jv hj"> <em class="mq">去除非字母词和停用词:</em> </strong>停用词是指像and、the、is等词。，这是学习如何造句时的重要词汇，但对我们的预测分析没有用处。</p><p id="eb51" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated"><strong class="jv hj"> <em class="mq">词汇化:</em> </strong>在词汇化中，我们把一个词转换成它的基本意思。例如，单词“正在播放”和“已播放”将被转换为它们的基本格式“播放”。</p><pre class="mc md me mf fd mg mh mi mj aw mk bi"><span id="2338" class="iv iw hi mh b fi ml mm l mn mo"># Change to lowercase<br/>vdata[‘Title’] = vdata[‘Title’].map(lambda x: x.lower())<br/>vdata[‘Description’] = vdata[‘Description’].map(lambda x: x.lower())</span><span id="9440" class="iv iw hi mh b fi mp mm l mn mo"># Remove numbers<br/>vdata[‘Title’] = vdata[‘Title’].map(lambda x: re.sub(r’\d+’, ‘’, x))<br/>vdata[‘Description’] = vdata[‘Description’].map(lambda x: re.sub(r’\d+’, ‘’, x))</span><span id="7069" class="iv iw hi mh b fi mp mm l mn mo"># Remove Punctuation<br/>vdata[‘Title’] = vdata[‘Title’].map(lambda x: x.translate(x.maketrans(‘’, ‘’, string.punctuation)))<br/>vdata[‘Description’] = vdata[‘Description’].map(lambda x: x.translate(x.maketrans(‘’, ‘’, string.punctuation)))</span><span id="a7df" class="iv iw hi mh b fi mp mm l mn mo"># Remove white spaces<br/>vdata[‘Title’] = vdata[‘Title’].map(lambda x: x.strip())<br/>vdata[‘Description’] = vdata[‘Description’].map(lambda x: x.strip())</span><span id="bfd2" class="iv iw hi mh b fi mp mm l mn mo"># Tokenize into words<br/>vdata[‘Title’] = vdata[‘Title’].map(lambda x: word_tokenize(x))<br/>vdata[‘Description’] = vdata[‘Description’].map(lambda x: word_tokenize(x))<br/> <br/># Remove non alphabetic tokens<br/>vdata[‘Title’] = vdata[‘Title’].map(lambda x: [word for word in x if word.isalpha()])<br/>vdata[‘Description’] = vdata[‘Description’].map(lambda x: [word for word in x if word.isalpha()])</span><span id="0fc5" class="iv iw hi mh b fi mp mm l mn mo"># filter out stop words<br/>stop_words = set(stopwords.words(‘english’))<br/>vdata[‘Title’] = vdata[‘Title’].map(lambda x: [w for w in x if not w in stop_words])<br/>vdata[‘Description’] = vdata[‘Description’].map(lambda x: [w for w in x if not w in stop_words])</span><span id="49bd" class="iv iw hi mh b fi mp mm l mn mo"># Word Lemmatization<br/>lem = WordNetLemmatizer()<br/>vdata[‘Title’] = vdata[‘Title’].map(lambda x: [lem.lemmatize(word,”v”) for word in x])<br/>vdata[‘Description’] = vdata[‘Description’].map(lambda x: [lem.lemmatize(word,”v”) for word in x])</span><span id="ddb2" class="iv iw hi mh b fi mp mm l mn mo"># Turn lists back to string<br/>vdata[‘Title’] = vdata[‘Title’].map(lambda x: ‘ ‘.join(x))<br/>vdata[‘Description’] = vdata[‘Description’].map(lambda x: ‘ ‘.join(x))</span></pre><p id="155b" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated">为了从文本中提取数据作为特征并以数字格式表示它们，一种非常常见的方法是<strong class="jv hj">对它们进行矢量化</strong>。Scikit-learn库包含用于此目的的“TF-IDF矢量器”。<strong class="jv hj"> TF-IDF </strong>(词频-逆文档频率)计算每个词在多个文档内部和跨文档的频率，以识别每个词的重要性。</p><pre class="mc md me mf fd mg mh mi mj aw mk bi"><span id="6780" class="iv iw hi mh b fi ml mm l mn mo"># TF-IDF<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>tfidf_title = TfidfVectorizer(sublinear_tf=True, min_df=5, norm=’l2', encoding=’latin-1', ngram_range=(1, 2), stop_words=’english’)<br/>tfidf_desc = TfidfVectorizer(sublinear_tf=True, min_df=5, norm=’l2', encoding=’latin-1', ngram_range=(1, 2), stop_words=’english’)<br/>labels = vdata.Category<br/>features_title = tfidf_title.fit_transform(vdata.Title).toarray()<br/>features_description = tfidf_desc.fit_transform(vdata.Description).toarray()<br/>print(‘Title Features Shape: ‘ + str(features_title.shape))<br/>print(‘Description Features Shape: ‘ + str(features_description.shape))</span><span id="5ca1" class="iv iw hi mh b fi mp mm l mn mo">Title Features Shape: (9999, 2637)<br/>Description Features Shape: (9999, 4858)</span></pre><h1 id="851d" class="lh iw hi bd ix li lj lk jb ll lm ln jf lo lp lq jj lr ls lt jn lu lv lw jr lx bi translated">数据分析和特征探索:</h1><p id="fbdf" class="pw-post-body-paragraph kn ko hi jv b jw jx kq kr jy jz kt ku jg le kw kx jk lf kz la jo lg lc ld kd hb bi translated">我已经决定显示类的分布，所以检查不平衡的样本数量。</p><pre class="mc md me mf fd mg mh mi mj aw mk bi"><span id="2a39" class="iv iw hi mh b fi ml mm l mn mo"># Plotting class distribution<br/>vdata[‘Category’].value_counts().sort_values(ascending=False).plot(kind=’bar’, y=’Number of Samples’, <br/> title=’Number of samples for each class’)</span></pre><figure class="mc md me mf fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/8a2b6e8e224331a43733e6183d095324.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*oX1qb0bAZ8_KovoPzAHaAA.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">每类的样本数量</figcaption></figure><p id="8d43" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated">现在，我们将使用标题和描述特性为每个类找到最相关的单词和双词。我们将使用这些单字和双字作为关键词，从广告数据集中提取数据。</p><ul class=""><li id="3883" class="jt ju hi jv b jw kp jy ks jg ly jk lz jo ma kd ke kf kg kh bi translated">使用标题功能的每个类别的最佳5个关键字</li></ul><pre class="mc md me mf fd mg mh mi mj aw mk bi"><span id="f224" class="iv iw hi mh b fi ml mm l mn mo"># Best 5 keywords for each class using Title Feaures<br/>from sklearn.feature_selection import chi2<br/>import numpy as np<br/>N = 10<br/>for current_class in list(le.classes_):<br/> current_class_id = le.transform([current_class])[0]<br/> features_chi2 = chi2(features_title, labels == current_class_id)<br/> indices = np.argsort(features_chi2[0])<br/> feature_names = np.array(tfidf_title.get_feature_names())[indices]<br/> unigrams = [v for v in feature_names if len(v.split(‘ ‘)) == 1]<br/> bigrams = [v for v in feature_names if len(v.split(‘ ‘)) == 2]<br/> print(“# ‘{}’:”.format(current_class))<br/> print(“Most correlated unigrams:”)<br/> print(‘-’ *30)<br/> print(‘. {}’.format(‘\n. ‘.join(unigrams[-N:])))<br/> print(“Most correlated bigrams:”)<br/> print(‘-’ *30)<br/> print(‘. {}’.format(‘\n. ‘.join(bigrams[-N:])))<br/> print(“\n”)</span></pre><p id="c4de" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated">输出:</p><pre class="mc md me mf fd mg mh mi mj aw mk bi"><span id="298d" class="iv iw hi mh b fi ml mm l mn mo"># 'art and music':<br/>Most correlated unigrams:<br/>------------------------------<br/>. musical<br/>. live<br/>. travel<br/>. arts<br/>. video<br/>. paint<br/>. official<br/>. music<br/>. art<br/>. theatre<br/>Most correlated bigrams:<br/>------------------------------<br/>. art challenge<br/>. avengers endgame<br/>. theatre company<br/>. theatre official<br/>. theatre congolais<br/>. capitol theatre<br/>. musical theatre<br/>. work theatre<br/>. official music<br/>. music video<br/></span><span id="0cc5" class="iv iw hi mh b fi mp mm l mn mo"># 'food':<br/>Most correlated unigrams:<br/>------------------------------<br/>. street<br/>. recipe<br/>. taste<br/>. healthy<br/>. try<br/>. foods<br/>. eat<br/>. snack<br/>. cook<br/>. food<br/>Most correlated bigrams:<br/>------------------------------<br/>. cook guy<br/>. sam cook<br/>. try hiho<br/>. eat snack<br/>. emmy eat<br/>. healthy snack<br/>. snack amp<br/>. taste test<br/>. kid try<br/>. street food<br/></span><span id="c62a" class="iv iw hi mh b fi mp mm l mn mo"># 'history':<br/>Most correlated unigrams:<br/>------------------------------<br/>. archaeologist<br/>. rap<br/>. anthropologist<br/>. anthropological<br/>. archaeologists<br/>. discoveries<br/>. archaeological<br/>. archaeology<br/>. history<br/>. anthropology<br/>Most correlated bigrams:<br/>------------------------------<br/>. cultural anthropology<br/>. history documentary<br/>. concepts anthropology<br/>. world history<br/>. forensic anthropology<br/>. history channel<br/>. rap battle<br/>. epic rap<br/>. battle history<br/>. archaeological discoveries<br/></span><span id="028f" class="iv iw hi mh b fi mp mm l mn mo"># 'manufacturing':<br/>Most correlated unigrams:<br/>------------------------------<br/>. additive<br/>. lean<br/>. production<br/>. factory<br/>. manufacturer<br/>. business<br/>. printer<br/>. process<br/>. print<br/>. manufacture<br/>Most correlated bigrams:<br/>------------------------------<br/>. manufacture industry<br/>. manufacture tour<br/>. manufacture engineer<br/>. future manufacture<br/>. advance manufacture<br/>. manufacture plant<br/>. lean manufacture<br/>. additive manufacture<br/>. manufacture business<br/>. manufacture process<br/></span><span id="0ada" class="iv iw hi mh b fi mp mm l mn mo"># 'science and technology':<br/>Most correlated unigrams:<br/>------------------------------<br/>. robots<br/>. primitive<br/>. technologies<br/>. quantum<br/>. robotics<br/>. compute<br/>. computers<br/>. science<br/>. computer<br/>. technology<br/>Most correlated bigrams:<br/>------------------------------<br/>. course computer<br/>. quantum computers<br/>. future technology<br/>. university science<br/>. quantum compute<br/>. science amp<br/>. amp technology<br/>. primitive technology<br/>. computer science<br/>. science technology<br/></span><span id="945d" class="iv iw hi mh b fi mp mm l mn mo"># 'travel':<br/>Most correlated unigrams:<br/>------------------------------<br/>. viewfinder<br/>. manufacture<br/>. tip<br/>. expedia<br/>. trip<br/>. blogger<br/>. vlog<br/>. travellers<br/>. blog<br/>. travel<br/>Most correlated bigrams:<br/>------------------------------<br/>. start travel<br/>. travel light<br/>. travel salesman<br/>. travel guide<br/>. expedia viewfinder<br/>. viewfinder travel<br/>. travel blogger<br/>. tip travel<br/>. travel vlog<br/>. travel blog</span></pre><ul class=""><li id="daba" class="jt ju hi jv b jw kp jy ks jg ly jk lz jo ma kd ke kf kg kh bi translated">使用描述功能的每个类别的最佳5个关键字</li></ul><pre class="mc md me mf fd mg mh mi mj aw mk bi"><span id="b1c5" class="iv iw hi mh b fi ml mm l mn mo"># Best 5 keywords for each class using Description Features<br/>from sklearn.feature_selection import chi2<br/>import numpy as np<br/>N = 10<br/>for current_class in list(le.classes_):<br/> current_class_id = le.transform([current_class])[0]<br/> features_chi2 = chi2(features_description, labels == current_class_id)<br/> indices = np.argsort(features_chi2[0])<br/> feature_names = np.array(tfidf_desc.get_feature_names())[indices]<br/> unigrams = [v for v in feature_names if len(v.split(‘ ‘)) == 1]<br/> bigrams = [v for v in feature_names if len(v.split(‘ ‘)) == 2]<br/> print(“# ‘{}’:”.format(current_class))<br/> print(“Most correlated unigrams:”)<br/> print(‘-’ *30)<br/> print(‘. {}’.format(‘\n. ‘.join(unigrams[-N:])))<br/> print(“Most correlated bigrams:”)<br/> print(‘-’ *30)<br/> print(‘. {}’.format(‘\n. ‘.join(bigrams[-N:])))<br/> print(“\n”)</span></pre><p id="8ca2" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated">输出:</p><pre class="mc md me mf fd mg mh mi mj aw mk bi"><span id="0c9b" class="iv iw hi mh b fi ml mm l mn mo"># 'art and music':<br/>Most correlated unigrams:<br/>------------------------------<br/>. spotify<br/>. album<br/>. draw<br/>. listen<br/>. arts<br/>. official<br/>. paint<br/>. music<br/>. art<br/>. theatre<br/>Most correlated bigrams:<br/>------------------------------<br/>. work theatre<br/>. official video<br/>. live capitol<br/>. theatre passaic<br/>. passaic nj<br/>. capitol theatre<br/>. click listen<br/>. production connexion<br/>. official music<br/>. music video<br/></span><span id="b8c2" class="iv iw hi mh b fi mp mm l mn mo"># 'food':<br/>Most correlated unigrams:<br/>------------------------------<br/>. delicious<br/>. recipes<br/>. taste<br/>. healthy<br/>. recipe<br/>. foods<br/>. eat<br/>. snack<br/>. cook<br/>. food<br/>Most correlated bigrams:<br/>------------------------------<br/>. httpbitlyznbqjw come<br/>. httpbitlycomhihofans update<br/>. sign httpbitlycomhihofans<br/>. series httpbitlyznbqjw<br/>. update hiho<br/>. special offer<br/>. hiho special<br/>. come play<br/>. sponsor series<br/>. street food<br/></span><span id="ae1f" class="iv iw hi mh b fi mp mm l mn mo"># 'history':<br/>Most correlated unigrams:<br/>------------------------------<br/>. rap<br/>. anthropologist<br/>. ancient<br/>. archaeologist<br/>. archaeologists<br/>. discoveries<br/>. archaeological<br/>. history<br/>. archaeology<br/>. anthropology<br/>Most correlated bigrams:<br/>------------------------------<br/>. begin april<br/>. season begin<br/>. decide erb<br/>. erb season<br/>. history decide<br/>. episode epic<br/>. epic rap<br/>. battle history<br/>. rap battle<br/>. archaeological discoveries<br/></span><span id="9cc6" class="iv iw hi mh b fi mp mm l mn mo"># 'manufacturing':<br/>Most correlated unigrams:<br/>------------------------------<br/>. machine<br/>. plant<br/>. manufacturers<br/>. manufacturer<br/>. printers<br/>. factory<br/>. printer<br/>. process<br/>. print<br/>. manufacture<br/>Most correlated bigrams:<br/>------------------------------<br/>. manufacture facility<br/>. manufacture industry<br/>. manufacture company<br/>. manufacture unit<br/>. advance manufacture<br/>. process make<br/>. lean manufacture<br/>. additive manufacture<br/>. manufacture business<br/>. manufacture process<br/></span><span id="327d" class="iv iw hi mh b fi mp mm l mn mo"># 'science and technology':<br/>Most correlated unigrams:<br/>------------------------------<br/>. technologies<br/>. future<br/>. robots<br/>. robotics<br/>. compute<br/>. quantum<br/>. computers<br/>. science<br/>. computer<br/>. technology<br/>Most correlated bigrams:<br/>------------------------------<br/>. new technology<br/>. artificial intelligence<br/>. cloud compute<br/>. future technology<br/>. university science<br/>. quantum computers<br/>. primitive technology<br/>. quantum compute<br/>. computer science<br/>. science technology<br/></span><span id="8f5a" class="iv iw hi mh b fi mp mm l mn mo"># 'travel':<br/>Most correlated unigrams:<br/>------------------------------<br/>. stay<br/>. expedia<br/>. tip<br/>. adventure<br/>. blogger<br/>. vlog<br/>. travellers<br/>. trip<br/>. blog<br/>. travel<br/>Most correlated bigrams:<br/>------------------------------<br/>. becomingfilipino travel<br/>. instagram taesungsayshi<br/>. travel video<br/>. travel world<br/>. travel vlog<br/>. tip travel<br/>. start travel<br/>. expedia viewfinder<br/>. travel blogger<br/>. travel blog</span></pre><h1 id="9002" class="lh iw hi bd ix li lj lk jb ll lm ln jf lo lp lq jj lr ls lt jn lu lv lw jr lx bi translated">建模和培训:</h1><p id="5e19" class="pw-post-body-paragraph kn ko hi jv b jw jx kq kr jy jz kt ku jg le kw kx jk lf kz la jo lg lc ld kd hb bi translated">数据建模用于定义和分析支持机器学习模型所需的数据，并将其存储到数据库中，以对其执行所需的操作。</p><p id="38d2" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated">提取<strong class="jv hj">标题</strong>和<strong class="jv hj">描述</strong>的特征，然后连接以构建最终的特征矩阵</p><pre class="mc md me mf fd mg mh mi mj aw mk bi"><span id="552a" class="iv iw hi mh b fi ml mm l mn mo">from sklearn.model_selection import train_test_split<br/>from sklearn.naive_bayes import MultinomialNB<br/>from sklearn import linear_model<br/>from sklearn.ensemble import AdaBoostClassifier</span><span id="463c" class="iv iw hi mh b fi mp mm l mn mo">X_train, X_test, y_train, y_test = train_test_split(vdata.iloc[:, 1:3], vdata[‘Category’], random_state = 0)<br/>X_train_title_features = tfidf_title.transform(X_train[‘Title’]).toarray()<br/>X_train_desc_features = tfidf_desc.transform(X_train[‘Description’]).toarray()<br/>features = np.concatenate([X_train_title_features, X_train_desc_features], axis=1)</span></pre><figure class="mc md me mf fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/85f8f093426bec57bfdf3e3457e845d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dOKP7w1zhtAAYkKbMlsNsQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">X_train.head()</figcaption></figure><h1 id="3b50" class="lh iw hi bd ix li lj lk jb ll lm ln jf lo lp lq jj lr ls lt jn lu lv lw jr lx bi translated">绩效评估:</h1><p id="202d" class="pw-post-body-paragraph kn ko hi jv b jw jx kq kr jy jz kt ku jg le kw kx jk lf kz la jo lg lc ld kd hb bi translated">(详细代码请参考本项目的GitHub链接。)</p><pre class="mc md me mf fd mg mh mi mj aw mk bi"><span id="8427" class="iv iw hi mh b fi ml mm l mn mo">precision    recall  f1-score   support</span><span id="952e" class="iv iw hi mh b fi mp mm l mn mo">         art and music       0.92      0.95      0.94       415<br/>                  food       0.97      0.95      0.96       449<br/>               history       0.97      0.95      0.96       418<br/>         manufacturing       0.95      0.99      0.97       398<br/>science and technology       0.95      0.94      0.94       414<br/>                travel       0.97      0.95      0.96       406</span><span id="9b08" class="iv iw hi mh b fi mp mm l mn mo">              accuracy                           0.96      2500<br/>             macro avg       0.96      0.96      0.96      2500<br/>          weighted avg       0.96      0.96      0.96      2500</span></pre><h1 id="0122" class="lh iw hi bd ix li lj lk jb ll lm ln jf lo lp lq jj lr ls lt jn lu lv lw jr lx bi translated">提出建议:</h1><p id="e5ac" class="pw-post-body-paragraph kn ko hi jv b jw jx kq kr jy jz kt ku jg le kw kx jk lf kz la jo lg lc ld kd hb bi translated">现在我们终于可以看到我们的推荐系统在运行了。</p><pre class="mc md me mf fd mg mh mi mj aw mk bi"><span id="e6d9" class="iv iw hi mh b fi ml mm l mn mo">def find(dec,k):<br/> r=[]<br/> for i in dec.index:<br/> if k in dec[‘Meta Description 1’][i]:<br/> r.append(dec[‘Original Url’][i])<br/> return r</span><span id="9d9e" class="iv iw hi mh b fi mp mm l mn mo">adata=adata[[‘Original Url’, ‘Meta Description 1’]]</span><span id="c6ad" class="iv iw hi mh b fi mp mm l mn mo">#Search unigram keyword which is extracted from videos data.</span><span id="9fa2" class="iv iw hi mh b fi mp mm l mn mo">result=find(adata, “travel”) <br/>for i in result:<br/> print(“ Url Link “,i)</span></pre><p id="c066" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated">在这里，我们只需输入一个与视频相关的unigram(关键字),瞧！我们的函数收集与之对应的<code class="du mt mu mv mh b">results[]</code> <strong class="jv hj"> </strong>，在屏幕上得到我们的广告推荐。</p><p id="eb1d" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated">输出中的以下链接重定向到广告视频。</p><pre class="mc md me mf fd mg mh mi mj aw mk bi"><span id="69ea" class="iv iw hi mh b fi ml mm l mn mo">Url Link  <a class="ae iu" href="https://www.adforum.com/creative-work/ad/player/34458334/time-travel/directv" rel="noopener ugc nofollow" target="_blank">https://www.adforum.com/creative-work/ad/player/34458334/time-travel/directv</a><br/> Url Link  <a class="ae iu" href="https://www.adforum.com/creative-work/ad/player/34465481/rugby-world/gullivers-sports-travel" rel="noopener ugc nofollow" target="_blank">https://www.adforum.com/creative-work/ad/player/34465481/rugby-world/gullivers-sports-travel</a><br/> Url Link  <a class="ae iu" href="https://www.adforum.com/creative-work/ad/player/34485545/travel/ole" rel="noopener ugc nofollow" target="_blank">https://www.adforum.com/creative-work/ad/player/34485545/travel/ole</a><br/> Url Link  <a class="ae iu" href="https://www.adforum.com/creative-work/ad/player/34488240/new-zealand/expedia-com" rel="noopener ugc nofollow" target="_blank">https://www.adforum.com/creative-work/ad/player/34488240/new-zealand/expedia-com</a><br/> Url Link  <a class="ae iu" href="https://www.adforum.com/creative-work/ad/player/34485545/travel/ole" rel="noopener ugc nofollow" target="_blank">https://www.adforum.com/creative-work/ad/player/34485545/travel/ole</a><br/> Url Link  <a class="ae iu" href="https://www.adforum.com/creative-work/ad/player/34488240/new-zealand/expedia-com" rel="noopener ugc nofollow" target="_blank">https://www.adforum.com/creative-work/ad/player/34488240/new-zealand/expedia-com</a><br/> Url Link  <a class="ae iu" href="https://www.adforum.com/creative-work/ad/player/34517107/leave-your-mark/under-armour" rel="noopener ugc nofollow" target="_blank">https://www.adforum.com/creative-work/ad/player/34517107/leave-your-mark/under-armour</a></span></pre></div><div class="ab cl mw mx gp my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="hb hc hd he hf"><h1 id="21e1" class="lh iw hi bd ix li nd lk jb ll ne ln jf lo nf lq jj lr ng lt jn lu nh lw jr lx bi translated">结论:</h1><p id="8a79" class="pw-post-body-paragraph kn ko hi jv b jw jx kq kr jy jz kt ku jg le kw kx jk lf kz la jo lg lc ld kd hb bi translated">所提出的系统从视频中提取特征，如标题、描述和标签。基于这些提取的特征，我们打算使用分类模型产生分类标签。分析产生的关于广告数据集的标签我们打算在视频上提供与视频主题相关的广告。</p><h1 id="abda" class="lh iw hi bd ix li lj lk jb ll lm ln jf lo lp lq jj lr ls lt jn lu lv lw jr lx bi translated">进一步改进:</h1><ul class=""><li id="fc33" class="jt ju hi jv b jw jx jy jz jg ka jk kb jo kc kd ke kf kg kh bi translated">因为我们不能肯定地说我们从视频中提取的元数据是正确的。我们也找不到一些视频的描述或标题与视频中显示的一致。在这种情况下，我们的系统无法显示相关广告。</li><li id="2364" class="jt ju hi jv b jw ki jy kj jg kk jk kl jo km kd ke kf kg kh bi translated">我们可以通过从实际视频中提取特征来克服这个缺点，比如文本、运动等。并且使用这些特征来识别视频的内容。我将很快发布这个教程。</li></ul></div><div class="ab cl mw mx gp my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="hb hc hd he hf"><p id="dd52" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated">我希望这对你和对我一样有益。完整的代码可以在我的Github上找到。</p><p id="9250" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated">GitHub项目的<a class="ae iu" href="https://github.com/rishikonapure/Advertisement-Recommendation" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj">链接</strong> </a>。LinkedIn <a class="ae iu" href="https://www.linkedin.com/in/rushikesh-konapure/" rel="noopener ugc nofollow" target="_blank"> <strong class="jv hj">简介</strong> </a>。</p></div><div class="ab cl mw mx gp my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="hb hc hd he hf"><p id="63dc" class="pw-post-body-paragraph kn ko hi jv b jw kp kq kr jy ks kt ku jg kv kw kx jk ky kz la jo lb lc ld kd hb bi translated">希望你喜欢这个教程。感谢阅读..！</p></div></div>    
</body>
</html>