<html>
<head>
<title>Implementing an AdaBoost classifier from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始实现AdaBoost分类器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/implementing-an-adaboost-classifier-from-scratch-e30ef86e9f1b?source=collection_archive---------7-----------------------#2020-03-30">https://medium.com/analytics-vidhya/implementing-an-adaboost-classifier-from-scratch-e30ef86e9f1b?source=collection_archive---------7-----------------------#2020-03-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="33b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我们将看看强大的集成学习方法AdaBoost。我们将看到这个算法背后的数学。我会尽可能简单地解释数学，这样就容易理解了。最后，我们将用python从头开始编写一个AdaBoost分类器。那我们开始吧。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/6bf5fbe08486d5c36027391b4aa83746.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i5qvEcVE8qiNELuHfUjA9g.jpeg"/></div></div></figure><h2 id="0fc5" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated">集合方法:</h2><p id="3eeb" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">顾名思义，集成方法试图以这样一种方式将多个模型的结果组合在一起，使它们的表现比单个模型好得多。有两种方法可以实现这一点。即装袋和增压。</p><p id="9bc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Bagging中，每个模型都在数据的随机子集上进行训练，以便每个模型都可以捕捉数据的不同方面/行为。然后在预测时，取每个模型预测的平均值来给出输出。RandomForest方法是Bagging的一个例子(随机数来自随机子集)。</p><p id="ecfe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在助推方面，方法是不同的。在这里，每个后续的模型，而训练给予被先前的模型错误分类的数据更多的重要性。然后进行加权投票来决定输出。AdaBoost就是一个例子。</p><p id="3d26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果这一切听起来很复杂，那么不要担心。我举个简单的例子。假设你和你妹妹准备历史考试。当然，因为这是集体学习，所以作弊是允许的。😛</p><p id="72e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设你们都随机选择了两本不同的涵盖教学大纲的书，并为此进行了学习。然后在考试的时候，你们会写一个你们双方都认同的答案。如果你决定这样做，那么你已经遵循装袋方法。但是假设你采用了不同的方法。在学习的时候，你注意到你的姐姐在法国大革命和莫卧儿时代很软弱。所以当你学习的时候，你会特别关注这些章节，因为你知道如果这些章节出现问题，你姐姐很可能会出错。所以你最好做好准备。你现在所做的正是升压所做的。</p><h1 id="4d08" class="kp jq hi bd jr kq kr ks jv kt ku kv jz kw kx ky kc kz la lb kf lc ld le ki lf bi translated">AdaBoost:</h1><p id="1261" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">不同的增强方法遵循不同的技术，以向后续模型传达这些数据点是被错误分类的数据点。在AdaBoost中，我们使用权重来实现这些目标。</p><p id="8558" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，让我们定义两个最重要的事情是需要建立任何模型ie。假设/预测函数和损失/误差函数。</p><p id="3e33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设函数f(x)定义为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lg"><img src="../Images/d9c757fc137ea27aa13dbf2a69774f3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*btY1hRa2KTO9utZvxhLRUA.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">(等式1)</figcaption></figure><p id="d677" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们将<em class="ll"> fm(x) </em>定义为具有<em class="ll"> m </em>预测值的函数，将<em class="ll"> fm-1(x) </em>定义为具有<em class="ll"> m-1 </em>预测值的函数，那么很容易证明</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lm"><img src="../Images/1d78f56786989c9a7d55a856f44cf87a.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*axlffwW9UezT4vkRJR6t1w.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">(等式2)</figcaption></figure><p id="2d77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在来看看损失函数。AdaBoost使用指数损失函数，即</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ln"><img src="../Images/83cdd1b3822758c92a8d287e97e82e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/format:webp/1*RRLsXYfuUwPLNbM7hBUPfg.png"/></div></figure><p id="8d1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有一点你应该记住，AdaBoost使用{-1，1}作为二进制标签，而不是{0，1}。这将如何帮助我们，我稍后会解释。</p><p id="32f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">根据等式(2)，我们可以将误差函数展开为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lo"><img src="../Images/7729d1350f6197aef5a2362167f54be2.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*HuEJfKCQ_1H6sDCZD0jvtA.png"/></div></figure><p id="c2da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于<em class="ll">【e^(-yfm-1(xᵢ】)</em>不依赖于<em class="ll"/>和<em class="ll"> Gm </em>我们可以用一些常数来代替它们，比如w <em class="ll"> ᵢ </em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lp"><img src="../Images/46c0ddf1269881518f660b7888e6a7c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*wF29phouqMwo8qGmWuoRSw.png"/></div></figure><p id="69be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些<em class="ll"> wᵢ </em>将成为传递给<em class="ll"> Gm </em>以适应功能的权重。在<em class="ll"> Gm </em>完成拟合后，我们现在必须找到<em class="ll"> αₘ.的值</em></p><p id="68de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">很容易说明表达式-y <em class="ll"> ᵢαₘGm(xᵢ) </em>是<em class="ll"> -αₘ </em>如果y <em class="ll"> ᵢ </em> = <em class="ll"> Gm(xᵢ) </em>并且是<em class="ll"> αₘ </em>如果<em class="ll"> </em> y <em class="ll"> ᵢ！=Gm(xᵢ).记住这一点，我们可以将误差函数改写为</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lq"><img src="../Images/7f99f0dc227405d9a9705f46fd4cf08d.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*tuCFwmlYX9hQjEJKr6P5eA.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">(等式3)</figcaption></figure><p id="0e85" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我定义总重量<em class="ll"> Tw </em>为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/1e47768c0d47c6d0b981628bd7edd809.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*vulPca9itjmCTyrjXC3BQg.png"/></div></figure><p id="80e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">误差权重<em class="ll"> Ew </em>为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/1aafad6660f7eaa7174138c84ee04ff1.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*-kFe-swp-fEPYkQRxNUNNg.png"/></div></figure><p id="84f3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么我可以将等式-(3)改写为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/66f1cf7fcba523fa9a91e947360d74cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*MOUsxaJccaKKrGRFZujcig.png"/></div></figure><p id="bebf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这变成了太多的变数。但相信我，我们越来越接近了。现在我们来微分<em class="ll"> Err </em> w.r.t <em class="ll"> αₘ </em>得到极小点。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/ed367fcb63dc485b5c786b7ae052b1bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*S86gkKjsJ-hECUyaDSLaJg.png"/></div></figure><p id="618c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">等于d <em class="ll"> Err/dαₘ </em> = 0我们有</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/192e004d76163b42b46aab144bffb677.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*InM_b_vou2iILPK1kvI1Ww.png"/></div></figure><p id="dfd8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们两边都有原木</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/f4d3f3f281f8dce155faefc7393d6342.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*-HTnl59NHm9fpKNQuvqtzQ.png"/></div></figure><p id="5da4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在哪里</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/62ea77e442555e8156d085f9461821c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*STjbKrThSnyavx-8BQmZJQ.png"/></div></figure><p id="827b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们完成了我们的推导。在继续讨论算法之前，让我解释一些事情。</p><p id="4d2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1)看看我们的假设函数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lg"><img src="../Images/5ab5bf424db4fbfb3d596be58a6e1f48.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*Z64MYjxR1yPciJMY8gz23g.png"/></div></figure><p id="9a29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注意<em class="ll"> Gm(x) </em>只输出<em class="ll"> {-1，1} </em>。然后，通过与<em class="ll"> αₘ </em>相乘，该输出被缩放到某个正值或负值。因此<em class="ll"> αₘ </em>被称为信心，因为我们对那个特定预测器的输出表现出如此大的信心。</p><p id="d206" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2)<em class="ll">αₘgm(x)</em>将产生一系列正/负值。将所有这些相加，我们将得到一个或正或负的和。如果它是正的，那么我们说输出是1，否则它是-1。</p><p id="ec08" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要带走的东西是<em class="ll"> αₘGm(x) </em>可以是正的，也可以是负的。但是如果更多的分类器说它是正的，那么总和将变成正的。所以我们遵循大多数分类者的主张。这就是我们所说的加权投票。</p><h2 id="7949" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated">算法:</h2><pre class="je jf jg jh fd ly lz ma mb aw mc bi"><span id="e0a2" class="jp jq hi lz b fi md me l mf mg"><strong class="lz hj">Fit:</strong></span><span id="ae70" class="jp jq hi lz b fi mh me l mf mg">Step-1 : Initialize weights. wi = C , i = 1,2,..N</span><span id="a3d3" class="jp jq hi lz b fi mh me l mf mg">This constant can be anything. I will be using 1/N as my constant. Any constant you pick will give exact same performance given it doesn’t cause overflow.</span><span id="962a" class="jp jq hi lz b fi mh me l mf mg">Step-2: For m = 1 to M:</span><span id="4dcb" class="jp jq hi lz b fi mh me l mf mg">    a) Fit classifier Gm with weights w</span><span id="7085" class="jp jq hi lz b fi mh me l mf mg">    b) Compute errₘ = SUM(wi*I(yi!=Gm(xi) ) / SUM(wi)</span><span id="ee33" class="jp jq hi lz b fi mh me l mf mg">    c) Compute αₘ = log( (1-errₘ)/errₘ )</span><span id="c788" class="jp jq hi lz b fi mh me l mf mg">    d) Update the weights wi = wi * exp(αₘ*I(yi!=Gm(xi))</span><span id="9166" class="jp jq hi lz b fi mh me l mf mg"><strong class="lz hj">Predict:</strong></span><span id="906b" class="jp jq hi lz b fi mh me l mf mg">f(x) = sign( SUM (αₘ*Gm(x)) )</span></pre><p id="2d0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们把它编码。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mi mj l"/></div></figure><p id="07e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们生成一个数据集来检查我们的性能。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mi mj l"/></div></figure><pre class="je jf jg jh fd ly lz ma mb aw mc bi"><span id="b7fa" class="jp jq hi lz b fi md me l mf mg">Performance: 84.7926267281106 %<br/>Confusion Matrix:<br/> [[94 15]<br/> [18 90]]</span></pre><p id="4c35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为健全性检查，我们可以与sklearn实现进行比较，</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="mi mj l"/></div></figure><pre class="je jf jg jh fd ly lz ma mb aw mc bi"><span id="30fd" class="jp jq hi lz b fi md me l mf mg">Performance: 84.7926267281106 %<br/>Confusion Matrix:<br/> [[94 15]<br/> [18 90]]</span></pre><p id="c78b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如所料，它们产生了相同的结果。有一点你可能会注意到，对于sklearn，我使用的是algorithm=" <em class="ll"> SAMME" </em>。实际上，有两种AdaBoost算法用于分类。萨姆。当弱学习者产生连续输出时使用R (像逻辑回归产生属于一个类的数据点的概率)。但是当我们使用决策树分类器时，产生离散输出的算法，我们应该使用的是<em class="ll"> SAMME </em>。我们已经实现的算法是用于2类分类的<em class="ll"> SAMME </em>的变体。</p><p id="b119" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ll">(注意SAMME和SAMME都有。r也可以处理连续和离散预测器。但是SAMME被设计成与离散预测器和SAMME一起工作。r代表连续预测值)</em></p><p id="3c3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">多类分类的算法与这里给出的算法非常相似，只是有一些微小的变化。如果多类分类涉及k个类，则y不是在{-1，1}之间变化的单个变量，而是变成k维向量。类似地，f(x)也输出一个k维向量。所以在误差函数中没有做</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mk"><img src="../Images/cc576765398dc912783ef9d9ef725c97.png" data-original-src="https://miro.medium.com/v2/resize:fit:166/format:webp/1*-Df7_f3d_6HNltpF3wbtaA.png"/></div></figure><p id="602b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们取y和f(x)向量的点积</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ml"><img src="../Images/9b27e7ee7d19688313f09e17618669e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:196/format:webp/1*U-AQh6_hZ9udo4SbfGvd7g.png"/></div></figure><p id="2a64" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后我们尝试最小化这个新的误差函数。你可以通过这个github链接找到用AdaBoost进行多类分类的完整代码。</p><pre class="je jf jg jh fd ly lz ma mb aw mc bi"><span id="e32d" class="jp jq hi lz b fi md me l mf mg"><a class="ae mm" href="https://github.com/Samarendra109/ML-Models/blob/master/ensemble/AdaBoostClassifier.py" rel="noopener ugc nofollow" target="_blank">https://github.com/Samarendra109/ML-Models/blob/master/ensemble/AdaBoostClassifier.py</a></span></pre><p id="639c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">AdaBoost是一种非常强大的技术，也可以用于回归。对于回归而非指数损失函数，我们使用平方误差函数的变体。在sklearn中，它被实现为AdaBoostRegressor。</p><p id="e201" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，在您离开之前，在使用AdaBoostClassifier时，您应该记住以下几点。</p><h2 id="fd26" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated"><strong class="ak">收敛:</strong></h2><p id="49b9" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">有时在某个点之后，无论你增加多少<em class="ll"> n_estimators </em>，AdaBoost分类器的性能都不会提高。</p><p id="8b72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您为每个估计器绘制<em class="ll"> errₘ </em>(在sklearn实现中，这些值出现在<em class="ll"> estimator_errors_ </em>中)，那么您将得到一个类似这样的图形。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/0d14c524d62b13c93e12414152936a88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W_GXq1v2NaQ4CjXZKhNHug.png"/></div></div></figure><p id="c73a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您会注意到<em class="ll"> errₘ </em>值收敛到<em class="ll"> 0.5 </em>(对于<em class="ll"> k </em>类分类，该值将为<em class="ll">(1–1/k)</em>)。让我们看看当<em class="ll"> errₘ </em>变成<em class="ll"> 0.5 </em>时会发生什么。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mn"><img src="../Images/c3d99085b900f39d6c2a7224619a66ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*0e1duOJcMOihOAEtaMYs2g.png"/></div></figure><p id="2f1e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">置信值变为0，因此不计算估计量的贡献。在这之后，无论你增加多少n_estimators的值，性能都不会进一步提高。如果你绘制置信值<em class="ll"> αₘ.，你也可以看到同样的情况</em>(估算器_权重_在sklearn实现中)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/392494f589d92230f6792375980a7d3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LRE8L4xBGDaWvXAPqYlH9Q.png"/></div></div></figure><p id="cbcc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实践中，如果你的模型在增加了n个估计量后仍然没有改善，那么你可能需要传递一个比你现在使用的更强的估计量。</p><h2 id="410d" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated"><strong class="ak">过拟合:</strong></h2><p id="46d2" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">正如我们看到的，每次一个点被错误分类，它的权重就会不断增加，直到某个分类器正确预测它。因此，如果你的数据有离群值或噪音，那么AdaBoost将迫使模型，直到它被正确分类。换句话说，它对噪音非常敏感，很容易过度拟合。</p><p id="a4a4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单点说，你和你妹妹明天有数学考试。你姐姐试图学习一个特定的问题，但无论她如何努力都无法理解。所以你所做的是，你只是一个字一个字地记住那个问题和它的答案。你不知道的是，你的妹妹实际上无法解决这个问题，因为这个问题有一个印刷错误。但是现在这变得危险了，因为你已经记住了答案，而不是学习公式，如果考试中出现类似的问题，你很可能会写错答案，因为你会应用你记住的内容。</p><p id="03ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个故事的寓意是，如果你打算使用AdaBoost，那么首先从数据中删除离群值和噪声。</p><p id="531f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就是这样。现在你可以继续使用你自己的AdaBoostClassifier来解决你想要的问题。</p><h1 id="111b" class="kp jq hi bd jr kq kr ks jv kt ku kv jz kw kx ky kc kz la lb kf lc ld le ki lf bi translated">结论:</h1><p id="16c9" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">AdaBoost算法是一个巨大的讨论话题。定期发布研究论文来调整和改进该算法。我已经尽量用简洁的方式解释了，但是还有更多东西需要学习。谢谢你一直读到最后。如果你喜欢，那就鼓掌吧。如果您有任何疑问/建议，请在回复中提出。</p></div></div>    
</body>
</html>