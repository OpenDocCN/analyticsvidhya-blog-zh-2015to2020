<html>
<head>
<title>Why GPUs are more suited for Deep Learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么GPU更适合深度学习？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/why-gpus-are-more-suited-for-deep-learning-3f823d66f5e4?source=collection_archive---------11-----------------------#2020-09-25">https://medium.com/analytics-vidhya/why-gpus-are-more-suited-for-deep-learning-3f823d66f5e4?source=collection_archive---------11-----------------------#2020-09-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/6be8dc9ee518a8d2141c6e1f696ca820.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*frB_04yhcct5SL3F6Kp4ZQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源(英伟达)</figcaption></figure><p id="7065" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在过去的十年中，我们已经看到GPU越来越多地出现在HPC(高性能计算)和最受欢迎的游戏领域。GPU年复一年地改进，现在它们能够做一些令人难以置信的伟大事情，但在过去几年中，由于深度学习，它们受到了更多的关注。</p><p id="ea17" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">由于深度学习模型花费大量时间进行训练，即使强大的CPU也不足以在给定时间内处理如此多的计算，这就是GPU由于其<strong class="iw hj">并行性</strong>而胜过CPU的地方。但是在深入研究之前，让我们先了解一些关于GPU的事情。</p><h1 id="8772" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">什么是GPU？</h1><p id="d70b" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">GPU或“图形处理单元”是整个计算机的迷你版本，但仅专用于特定的任务，不像CPU同时执行多个任务。GPU自带处理器，嵌入到自己的主板上，与v-ram或视频ram相耦合，还有适当的散热设计，用于通风和冷却。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es kv"><img src="../Images/ec28006fc3d7425d02ab13e7540ddbcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*kcdEyDXjByjfubkqbNKQ_w.jpeg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源(游戏玩家关系)</figcaption></figure><p id="03f1" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在术语“图形处理单元”中，“图形”是指在2d或3d空间上的指定坐标处渲染图像的术语。视口或视点是观看者观看物体的视角，取决于所使用的投影类型。光栅化和光线跟踪是渲染3d场景的一些方法，这两个概念都基于称为透视投影的投影类型。什么是透视投影？简而言之，这是一种在视图平面或画布上形成图像的方式，平行线会聚到一个称为“投影中心”的会聚点，随着物体远离视点，它看起来更小，这正是我们的眼睛在现实世界中描绘的，这也有助于理解图像的深度，这就是为什么它产生真实图像的原因。<br/>此外，GPU还处理复杂的几何图形、矢量、光源或照明、纹理、形状等。<br/>现在我们对GPU有了一个基本的概念，让我们理解为什么它被大量用于深度学习。</p><h1 id="abab" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">为什么GPU更适合深度学习？</h1><p id="d37c" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">GPU最令人钦佩的特性之一是并行计算进程的能力，这是<strong class="iw hj">并行计算</strong>概念的切入点。一个CPU通常以一种顺序的方式完成它的任务，这个CPU可以被分成几个核心，每个核心每次承担一个任务。假设CPU有两个内核，那么通过实现多任务，两个不同的任务进程可以在这两个内核上运行。但是这些过程仍然以串行方式执行。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es la"><img src="../Images/3e3c7d5d526dab8023a88aa3d6dd24f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/0*2qUyfUOlNX2g6Sws.jpg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源(示例)</figcaption></figure><p id="845a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这并不意味着CPU不够好，事实上CPU在处理与不同操作相关的不同任务方面非常出色，如处理操作系统、处理电子表格、播放高清视频、提取大zip文件，所有这些都在同一时间进行。有些事情GPU根本做不到。</p><h2 id="50e4" class="lb jt hi bd ju lc ld le jy lf lg lh kc jf li lj kg jj lk ll kk jn lm ln ko lo bi translated">区别在哪里？</h2><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lp"><img src="../Images/7c17c575100ce30fa7bd508d6d0a23e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yh_3QpOHplLu9cYv.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源(英伟达)</figcaption></figure><p id="582f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">正如前面所讨论的，CPU被划分为多个内核，以便它们可以同时承担多个任务，而在GPU中，它将拥有成百上千个内核，所有这些都专用于一个任务，这些都是更频繁执行的简单计算，并且彼此独立。并且两者都将频繁需要的数据存储到各自的高速缓冲存储器中，从而遵循“<strong class="iw hj">位置引用</strong>的原则。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/87563f68b9a12c4ce7136906f6b6f58d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/0*stJaJ3BaH_rEMFlR.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源(英伟达)</figcaption></figure><p id="eb40" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">有许多软件和游戏可以利用GPU来执行。这背后的想法是仅使任务或应用程序代码的某些部分并行，而不是整个过程，即由于大多数任务的过程必须以顺序方式执行，例如，登录到系统或应用程序不需要并行。当有部分执行可以并行完成时，可以简单地转移到GPU进行处理，同时在CPU中执行顺序任务，然后任务的两个部分再次组合在一起。</p><p id="febf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在GPU市场上有两个主要参与者，即Amd和Nvidia，Nvidia GPUs被广泛用于深度学习，因为它们在表单软件、驱动程序、cuda、cudnn方面有广泛的支持，因此在人工智能和深度学习方面，Nvidia长期以来一直是先驱。<br/>神经网络被称为<strong class="iw hj">令人尴尬的并行</strong>，这意味着神经网络中的计算可以很容易地并行执行，并且它们彼此独立。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/a8f644087e0bf2b7a79144b641908d64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/0*iO_dkbbE3MHJyL1f.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源(数据科学中心)</figcaption></figure><p id="dd9d" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">一些计算，如计算每层的权重和激活函数，反向传播可以并行进行，也有许多研究论文可用。<br/>Nvidia GPU配备了称为<strong class="iw hj"> cuda </strong>内核的专用内核，有助于加速深度学习。</p><h2 id="2f17" class="lb jt hi bd ju lc ld le jy lf lg lh kc jf li lj kg jj lk ll kk jn lm ln ko lo bi translated">什么是Cuda？</h2><p id="a7b3" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">CUDA是2007年推出的“计算统一设备架构”的缩写，通过这种方式，您可以实现并行计算，并以优化的方式充分利用您的GPU能力，从而在执行任务时获得更好的性能。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ls"><img src="../Images/c7b47b73aca05e8176547665b1e06496.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OGBATkaIjPDosXjw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源(英伟达)</figcaption></figure><p id="252a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">cuda toolkit是一个完整的包，它由开发环境组成，用于构建使用GPU的应用程序，该工具包主要包含c/c++编译器、调试器、库。cuda运行时也有自己的驱动程序，这样它就可以与GPU通信。Cuda也是一种编程语言，专门用于指导GPU执行任务，也称为GPU编程。下面是一个简单的hello word程序，让你了解一下cuda代码的样子。</p><pre class="kw kx ky kz fd lt lu lv lw aw lx bi"><span id="c4f1" class="lb jt hi lu b fi ly lz l ma mb">/* hello world program in cuda *\<br/>#include&lt;stdio.h&gt;<br/>#include&lt;stdlib.h&gt;<br/>#include&lt;cuda.h&gt;</span><span id="cfb9" class="lb jt hi lu b fi mc lz l ma mb">__global__ void demo() {<br/>    printf("hello world!,my first cuda program");<br/>}</span><span id="b9f9" class="lb jt hi lu b fi mc lz l ma mb">int main() {<br/>    printf("From main!\n");<br/>    demo&lt;&lt;&lt;1,1&gt;&gt;&gt;();<br/>    return 0;<br/>}</span></pre><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es md"><img src="../Images/f945654d27213f1f2c84966370c7d1d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*UD6zVgvaFahAKPiqrIijFg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">输出</figcaption></figure><h2 id="b12d" class="lb jt hi bd ju lc ld le jy lf lg lh kc jf li lj kg jj lk ll kk jn lm ln ko lo bi translated">cuDNN是什么？</h2><figure class="kw kx ky kz fd ij er es paragraph-image"><div class="er es me"><img src="../Images/f3d5cc1c564bedfe0ff2ffc3000828ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/0*L0SrGvjrQtkIXA2b.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源(超级Linuxer)</figcaption></figure><p id="4bf5" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">cuDNN是一个经过GPU优化的神经网络库，可以充分利用Nvidia GPU，该库包括卷积、前向和后向传播、激活函数和池的实现。这是一个必须的库，没有它你就不能使用GPU来训练神经网络。</p><h2 id="04f5" class="lb jt hi bd ju lc ld le jy lf lg lh kc jf li lj kg jj lk ll kk jn lm ln ko lo bi translated">张量核的巨大飞跃！</h2><p id="8943" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">早在2018年，Nvidia就推出了新的GPU系列，即2000系列，也称为RTX，这些卡配备了专门用于深度学习的张量核心，并基于Volta架构。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mf"><img src="../Images/6fcb82ae88af4e6c738872863e05b2b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*yb7ZQ51_TknPXlDo.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源(Anand Tech)</figcaption></figure><p id="74bf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">张量核是执行4 x 4 FP16矩阵的矩阵乘法和以半精度与4 x 4矩阵FP16或FP32相加的特定核，输出将产生具有全精度的4 x 4 FP16或FP32矩阵。<br/>注意:“FP”代表浮点，要了解更多关于浮点和精度的信息，请查看此<a class="ae mg" href="https://blogs.nvidia.com/blog/2019/11/15/whats-the-difference-between-single-double-multi-and-mixed-precision-computing/" rel="noopener ugc nofollow" target="_blank">博客</a>。</p><p id="e20b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">正如Nvidia所说，基于volta <br/>架构的新一代tensor内核比基于pascal架构的cuda内核快得多<br/>，这为加速深度学习提供了巨大的推动力。</p><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mh"><img src="../Images/93e8e56be296524bf60a77d05df9a4d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*R97T2aR7YPH4Cjco.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源(英伟达)</figcaption></figure><p id="10d7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在写这篇博客的时候，Nvidia宣布了最新的3000系列GPU产品，这些产品采用了Ampere架构，将张量内核的性能提高了2倍，还带来了新的精度值，如TF32(张量浮点32)，FP64(浮点64)。TF32的工作原理与FP32相同，但加速高达20倍，因此Nvidia声称模型的推理或训练时间将从数周减少到数小时。</p><h2 id="f1f5" class="lb jt hi bd ju lc ld le jy lf lg lh kc jf li lj kg jj lk ll kk jn lm ln ko lo bi translated">Amd vs英伟达</h2><figure class="kw kx ky kz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mi"><img src="../Images/3a20c7c5f56469506faa5f55fb55b5a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UuHLDm-saHWrgWmH.jpg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源(汤姆的硬件)</figcaption></figure><p id="8550" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">Amd GPUs对于游戏来说是不错的，但是一旦深度学习进入画面，那么Nvidia就遥遥领先了。这并不意味着Amd GPUs不好，这是由于软件优化和驱动程序没有积极更新，在Nvidia方面，他们有更好的驱动程序，经常更新，在cuda的顶部，cudnn有助于加速计算。<br/>一些知名的库如tensorflow，pytorch支持cuda，这意味着<br/>可以使用GTX 1000系列的entery级GPU。在Amd方面，它对其GPU的软件支持非常少。在硬件方面，Nvidia推出了专用张量内核。Amd有ROCm用于加速，但它不像张量核那样好，而且许多深度学习库不支持ROCm，从过去几年来看，在性能方面没有明显的飞跃。<br/>由于所有这些原因，英伟达在深度学习方面表现出色。</p><h2 id="898e" class="lb jt hi bd ju lc ld le jy lf lg lh kc jf li lj kg jj lk ll kk jn lm ln ko lo bi translated">摘要</h2><p id="1de2" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">综上所述，我们已经清楚地了解到，就GPU而言，Nvidia是市场的领导者，但我真的希望Amd在未来能够迎头赶上，或者至少在即将推出的GPU系列中做出一些显著的改进，因为他们已经在CPU方面做得很好，例如锐龙系列。<br/>随着我们在深度学习、机器学习和HPC领域取得新的创新和突破，未来几年GPU的应用范围将会非常广泛。GPU加速对于许多进入该领域的开发人员和学生来说总是很方便，因为他们的价格也变得越来越实惠。也要感谢广大的社区，他们也为AI和HPC的发展做出了贡献。</p></div></div>    
</body>
</html>