<html>
<head>
<title>Getting it to top 6% in Kaggle’s MNIST Digit Recognizer from scratch -3.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让它在Kaggle的MNIST数字识别器中从零开始达到6 %- 3。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/getting-it-to-top-6-in-kaggles-mnist-digit-recognizer-from-scratch-3-8b11b79958a2?source=collection_archive---------24-----------------------#2020-03-31">https://medium.com/analytics-vidhya/getting-it-to-top-6-in-kaggles-mnist-digit-recognizer-from-scratch-3-8b11b79958a2?source=collection_archive---------24-----------------------#2020-03-31</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="cb1e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">让我们的CNN更强大</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ix"><img src="../Images/f0c74a0b0c83a511564cdc3c7a35c99a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*yeMVdGja2Qw46itfNq795A.png"/></div><figcaption class="jf jg et er es jh ji bd b be z dx translated">机器学习:吴恩达·库塞拉</figcaption></figure><p id="01d9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 3.1批量标准化</strong></p><p id="82af" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在第1部分中，我们通过在0到1之间重新调整图像的像素值来规范化我们的输入层。以类似的方式，我们甚至可以使用批处理规范化来规范化提供给隐藏层的输入。批量标准化有几个优点，但最有希望的是需要较少的历元数，隐藏层的学习更加相互独立，并且具有较小的协方差偏移，即它防止模型过拟合。我们可以直接在我们的模型中包括批量标准化层，使用:</p><pre class="iy iz ja jb fd kf kg kh ki aw kj bi"><span id="feaf" class="kk kl hi kg b fi km kn l ko kp">model.add(BatchNormalization())</span></pre><p id="bdfb" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">更多关于批量规格化<a class="ae kq" href="https://towardsdatascience.com/batch-normalization-in-neural-networks-1ac91516821c" rel="noopener" target="_blank"> <em class="kr">这里</em> </a>。</p><p id="0700" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 3.2辍学——调整我们的深层神经网络</strong></p><p id="99cc" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在神经网络的训练阶段，隐藏层中的一些节点根据其神经元值被随机丢弃(我们需要指定范围在0到1之间的丢弃率)。这允许网络在每次运行时在结构上不同，从而防止过度拟合。你可以从0.4的辍学率开始，然后继续做一点试验，看看它在哪里找到一个最佳点。我通常倾向于0.4-0.6之间。我们可以如下实现丢弃层:</p><pre class="iy iz ja jb fd kf kg kh ki aw kj bi"><span id="8827" class="kk kl hi kg b fi km kn l ko kp">model.add(Dropout(rate))</span></pre><p id="82d7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">更多关于辍学<a class="ae kq" href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="kr">这里</em> </a>。</p><p id="85f4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 3.3随着培训的进行，学习率下降</strong></p><p id="88f1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们可以在一组固定的历元之后降低学习率。这一点很重要，因为当我们在第1部分和第2部分中训练模型时，准确性不断波动，也就是说，随着学习的进展，准确性开始下降而不是上升。因此，有必要降低学习率，以防止其超过最优值。我们可以实现自己的学习率调度程序，如下所示:</p><pre class="iy iz ja jb fd kf kg kh ki aw kj bi"><span id="d096" class="kk kl hi kg b fi km kn l ko kp">from keras.callbacks import *</span><span id="e169" class="kk kl hi kg b fi ks kn l ko kp">LearningRateScheduler(lambda x: droppingrate**x)</span></pre><p id="db2b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 3.4数据扩充</strong></p><p id="142c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">深网总是渴求数据，你提供给网络的数据越多，它就越准确。数据扩充是任何机器学习模型中最关键的一步，尤其是如果我们正在构建复杂的神经网络。随着训练参数数量的增加，相应地需要更多的数据。TensorFlow提供了一个实时扩充数据的选项，可实施为:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="daab" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">让我们用以上4个步骤来构建我们的CNN:</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="kt ku l"/></div></figure><p id="ff68" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">该模型在Kaggle上的得分为0.99728，排名在150以下(前7%)。</p><blockquote class="kv kw kx"><p id="66c8" class="jj jk kr jl b jm jn ij jo jp jq im jr ky jt ju jv kz jx jy jz la kb kc kd ke hb bi translated">在实现任何神经网络时，这4个步骤是至关重要的，并且肯定能使你的CNN模型更加健壮。</p></blockquote><p id="ec4a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 3.5组装不同型号:</strong></p><p id="978e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">运行该模型5次，您将看到每次运行的训练和测试分数都不同。因此，不能保证相同的模型每次运行都能达到0.99728的准确度。因此，最好是:</p><p id="2983" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">a.运行模型几次迭代</p><p id="40b2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">b.尝试不同的模型，通过试验一些隐藏层、内核大小、池层、学习率、辍学率、密集层中的神经元等。选择具有较好验证分数的模型并集成它们。</p><p id="b2c4" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">保持模板不变，仅通过将卷积层中的核大小从3改变为(2，4，5)并集合不同模型的输出，我能够得到0.99771的分数，排名为120，即截至2020年3月31日的前6%。</p></div></div>    
</body>
</html>