<html>
<head>
<title>Understanding of Matrix Factorization (MF) and Singular Value Decomposition (SVD)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解矩阵分解和奇异值分解</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-of-matrix-factorization-mf-and-singular-value-decomposition-svd-1a38c2d5bbaa?source=collection_archive---------4-----------------------#2020-01-24">https://medium.com/analytics-vidhya/understanding-of-matrix-factorization-mf-and-singular-value-decomposition-svd-1a38c2d5bbaa?source=collection_archive---------4-----------------------#2020-01-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="da35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇博客中，我将简要介绍矩阵分解的理论方面，比如分解的目的和好处，用简单的术语介绍矩阵分解的应用，以便对矩阵分解有一个清晰的了解。</p><p id="a1ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除此之外，它还涵盖了奇异值分解(SVD)的一步一步的推导，解释清楚每一步。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/f871f06a2c8baf10c879cb1be037756d.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*Q4xiAUm_o00BoEOtximtLg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">目录</figcaption></figure><h1 id="aeb0" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">矩阵分解</strong></h1><p id="ce66" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">矩阵分解<strong class="ih hj">矩阵分解</strong>是将一个矩阵近似为一个矩阵的乘积。它们用于实现高效的矩阵算法。</p><blockquote class="ks kt ku"><p id="5656" class="if ig kv ih b ii ij ik il im in io ip kw ir is it kx iv iw ix ky iz ja jb jc hb bi translated"><em class="hi">“一般的想法是，你可以将所有的结果‘因子’矩阵相乘，得到原始矩阵。”</em></p></blockquote><p id="b6e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如:6 = 2 * 3，其中2，3被视为6的因子，我们可以通过这些因子的乘积再次生成6(即2，3)。</p><p id="9476" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以类似的方式，<strong class="ih hj"> A = B.C </strong>，A可以表示为两个低维矩阵B，C的乘积，这里k =维数(超参数)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kz"><img src="../Images/dedc815f71ea9b31ca8120106a8ff18c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*5aVVECdbyazk1TMcqT51fA.png"/></div></figure><h2 id="04b4" class="la jq hi bd jr lb lc ld jv le lf lg jz iq lh li kd iu lj lk kh iy ll lm kl ln bi translated"><strong class="ak"> a)分解矩阵的目的和方法</strong></h2><p id="47f2" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">有许多不同的矩阵分解技术，每一种都可以在一类特定的问题中找到用途。</p><blockquote class="ks kt ku"><p id="0eb1" class="if ig kv ih b ii ij ik il im in io ip kw ir is it kx iv iw ix ky iz ja jb jc hb bi translated">简而言之，这是两大类分解:<br/> a)一些只适用于‘n×n’矩阵<br/> b)一些适用于更一般的‘m×n’矩阵</p></blockquote><p id="27a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 1)解线性方程组</strong>，其中大部分矩阵分解成2部分。</p><ul class=""><li id="2888" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated">LU分解将矩阵分解为下三角和上三角矩阵。</li><li id="9625" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">QR分解将矩阵A分解成正交矩阵Q和上三角矩阵r的乘积A = QR。</li><li id="851f" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">Cholskey分解等。</li><li id="bf75" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">非负矩阵分解(NMF)将矩阵A分解成两个具有非负元素的矩阵。(A也必须有非负元素)</li></ul><p id="09e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2)基于特征值的分解</strong>，大多适用于方阵。其中矩阵分解成3个部分(最终旋转、缩放、初始旋转)。</p><ul class=""><li id="99bd" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated">PCA是一种使用特征分解来获得变换矩阵的变换。</li><li id="f30b" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">奇异值分解(SVD)将任何维数的矩阵分解为3部分USV。</li></ul><p id="11c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">许多其他可能的方法是可用的。请参考<a class="ae mc" href="https://en.wikipedia.org/wiki/Matrix_decomposition" rel="noopener ugc nofollow" target="_blank">这里的</a>。</p><h2 id="4e29" class="la jq hi bd jr lb lc ld jv le lf lg jz iq lh li kd iu lj lk kh iy ll lm kl ln bi translated"><strong class="ak"> b)分解矩阵有什么好处？</strong></h2><ul class=""><li id="327b" class="lo lp hi ih b ii kn im ko iq md iu me iy mf jc lt lu lv lw bi translated">矩阵分解，将一个矩阵分解成两个比原始矩阵更容易求解的矩阵。这不仅使问题容易解决，而且减少了计算机计算答案所需的时间。</li><li id="e1d7" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">矩阵分解主要用于以简单的方式或快速的方式求解线性系统。</li><li id="c60c" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">矩阵因式分解减少了计算机存储矩阵的空间，而不是存储大的非因式分解矩阵(A)，我们可以使用更少的存储空间来存储它的因子(即B，C)，当矩阵的秩小时，有时甚至更小。</li></ul><h2 id="1ae5" class="la jq hi bd jr lb lc ld jv le lf lg jz iq lh li kd iu lj lk kh iy ll lm kl ln bi translated">c)应用</h2><ol class=""><li id="39a6" class="lo lp hi ih b ii kn im ko iq md iu me iy mf jc mg lu lv lw bi translated">缺失/不完整数据的插补。</li><li id="9485" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc mg lu lv lw bi translated">成像:分割和噪声去除。</li><li id="3e50" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc mg lu lv lw bi translated">文本挖掘/主题建模。</li><li id="e733" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc mg lu lv lw bi translated">使用协同过滤的推荐。</li><li id="400b" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc mg lu lv lw bi translated">特征脸。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mh"><img src="../Images/e6ed310358c90ac05760a62418e1f466.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*7Y-Q1cPzzIc5ia6nAg2THQ.png"/></div></figure><p id="b86a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MF的流行技术之一是SVD，它以平滑的方式被完整地覆盖。</p><h1 id="bec6" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">奇异值分解的推导</strong></h1><p id="2e07" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">SVD是实(或)复矩阵的因式分解，它通过极坐标分解的扩展将标准方阵的特征分解推广到任何<strong class="ih hj"> m x n </strong>矩阵。</p><p id="1cfd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">换句话说，SVD将任何维矩阵近似为3个低维矩阵，通过“旋转和缩放”保持最大方差，其形式为具有“前k个本征向量和本征值”的矩阵USV。</p><blockquote class="mi"><p id="e9b9" class="mj mk hi bd ml mm mn mo mp mq mr jc dx translated">A = U S V '(最终旋转||缩放||初始旋转)</p></blockquote><p id="d461" class="pw-post-body-paragraph if ig hi ih b ii ms ik il im mt io ip iq mu is it iu mv iw ix iy mw ja jb jc hb bi translated">其中，U =左奇异值矩阵，S =奇异值矩阵，以及</p><p id="5d0c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">v =右奇异值矩阵。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mx"><img src="../Images/f29cd9a8c36e6c82a995a972a4dfc8fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/0*p_It-wWbFvcaqK-n"/></div></figure><ul class=""><li id="1b5f" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated">旋转矩阵由变换的数据点组成，其特征向量构成奇异矩阵U或v</li><li id="3205" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">缩放矩阵具有按因子缩放的值</li></ul><p id="6dcc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">u，V是正交矩阵，表示空间的旋转或反射，它们是彼此正交的特征向量。</p><blockquote class="ks kt ku"><p id="cae5" class="if ig kv ih b ii ij ik il im in io ip kw ir is it kx iv iw ix ky iz ja jb jc hb bi translated">特征向量的主要目的是保持向量的方向。</p></blockquote><p id="575c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">s是对角矩阵，代表每个值按值‘σ’的缩放。</p><h2 id="646a" class="la jq hi bd jr lb lc ld jv le lf lg jz iq lh li kd iu lj lk kh iy ll lm kl ln bi translated">a)如何计算U，S，V？</h2><p id="d03b" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">因为u，v是正交矩阵，我们有一个性质<strong class="ih hj"> U.Uᵀ =1 </strong>和<strong class="ih hj"> V.Vᵀ =1 </strong>并且那些u，v矩阵包含<strong class="ih hj"> A.Aᵀ </strong>和<strong class="ih hj"> Aᵀ.的特征向量答</strong></p><p id="5eec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(Aᵀ.的本征向量a)构成v的列,(A.Aᵀ)的本征向量构成u的列</p><p id="8deb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">同样，<strong class="ih hj"> S </strong>中的奇异值是来自A.Aᵀ(或)Aᵀ.的特征值的平方根A中的这些‘σ’值按降序排列。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es my"><img src="../Images/ece2e76fb384f3cfb52ea6ee946401cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*wduvDZqR02qlqXzEeqHfuw.png"/></div></figure><p id="3a71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">将矩阵的所有数据点投影到低维空间中，使得这些特征向量在该维中保持最大方差，并且与每个特征向量相关联的每个特征值描述了“在该维中解释了多少%的方差”。</p><p id="b777" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">注:</strong></p><ol class=""><li id="7278" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc mg lu lv lw bi translated">λ~ σ(即特征值相当于奇异值的平方)。</li><li id="b393" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc mg lu lv lw bi translated">如果W是矩阵，那么特征向量可以通过W. <strong class="ih hj"> x </strong> = λ来计算。x (or) W.x= σ。x</li></ol><p id="3ec9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中，x=特征向量，λ (or) σ =特征/奇异值。</p><p id="332b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们开始推导U和s。</p><p id="2b60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">语句:(A.Aᵀ)。x = λ。x <br/> </strong>要找到u，先从A.Aᵀ的特征向量‘x’开始</p><p id="8a17" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">证明:</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mz"><img src="../Images/4b5bc5c15f2755f2faf321cac3581d67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*0tLpL6fOi1jrB_f0ZhsKhQ.png"/></div></figure><h2 id="e304" class="la jq hi bd jr lb lc ld jv le lf lg jz iq lh li kd iu lj lk kh iy ll lm kl ln bi translated"><strong class="ak"> b)寻找特征值(</strong> λ) <strong class="ak"> : </strong></h2><p id="6dc3" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">如果x不为零，那么这个特征方程(W- λ。我)。x = 0会有解。</p><p id="3dec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">W.x- λ.I.x = 0 ……(‘w’不过是A.Aᵀ矩阵)。</p><p id="63c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">矩阵的行列式(W- λ。I)必须等于零。从而从特征方程的解中，<strong class="ih hj"> det |W- λ。I|=0 </strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es na"><img src="../Images/5fb80c17d2bef745ecf36a514c59d7d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*JVqMujk_-qER4g-3jVL9Jw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">例子:<strong class="bd jr"> W — λ。我</strong></figcaption></figure><p id="5bd1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">det |W- λ。I| <strong class="ih hj"> = </strong> λ -12 λ + 16 <strong class="ih hj"> = </strong> 0</p><p id="e172" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">∴求上述方程的根，我们得到a的特征值</p><p id="da04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi">λ = 4, −2.</p><p id="dd8c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">求解这个行列式方程，我们得到特征值'λ',从中我们可以导出奇异值'σ'。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="ab fe cl nb"><img src="../Images/9773d109d9aa3f7308fe06bb8a1edff2.png" data-original-src="https://miro.medium.com/v2/0*FozgqEcfE_zBPrM4"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><strong class="bd jr">哎呀！</strong></figcaption></figure><h2 id="6762" class="la jq hi bd jr lb lc ld jv le lf lg jz iq lh li kd iu lj lk kh iy ll lm kl ln bi translated">c)找到特征向量(x):</h2><ul class=""><li id="5465" class="lo lp hi ih b ii kn im ko iq md iu me iy mf jc lt lu lv lw bi translated">将这些λ代入方程(W- λ)。我)。x=0，我们得到矩阵的第一种形式为(W-4。我)。x和(W+2I)。x</li><li id="69c5" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated">构建增广矩阵(W+2I ),并通过执行行操作转换为行梯队形式。[ <a class="ae mc" href="https://en.wikipedia.org/wiki/Gaussian_elimination#Example_of_the_algorithm" rel="noopener ugc nofollow" target="_blank">高斯·乔丹消去法</a></li></ul><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nc"><img src="../Images/afde787eaf1dcbdff607bfa3a970575a.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*UDVT--ExWwYQ5Mqm0pxiYA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated"><strong class="bd jr">特征向量的计算示例</strong></figcaption></figure><p id="2d6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于每个‘λ’值，我们得到一个特征向量。</p><p id="6186" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，最后得到的向量就是w(即A.Aᵀ)的本征向量。</p><p id="2eb7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有这些向量被组合成特征向量矩阵，这是我们的左/右奇异矩阵U或v。</p><p id="32da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">详细计算参见<a class="ae mc" href="https://www.scss.tcd.ie/Rozenn.Dahyot/CS1BA1/SolutionEigen.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ih hj">本</strong> </a>。</p><p id="8254" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重复同样的过程，计算Aᵀ.的特征值和特征向量一个可以做‘V’的。</p><p id="d8bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献:</strong></p><ul class=""><li id="6831" class="lo lp hi ih b ii ij im in iq lq iu lr iy ls jc lt lu lv lw bi translated"><a class="ae mc" href="https://www.scss.tcd.ie/Rozenn.Dahyot/CS1BA1/SolutionEigen.pdf" rel="noopener ugc nofollow" target="_blank">https://www.scss.tcd.ie/Rozenn.dah yot/cs1ba 1/solution eigen . pdf</a></li><li id="6fde" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated"><a class="ae mc" href="http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm" rel="noopener ugc nofollow" target="_blank">http://web . MIT . edu/be . 400/www/SVD/Singular _ Value _分解. htm </a></li><li id="226c" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated"><a class="ae mc" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Singular_value_decomposition</a></li><li id="307e" class="lo lp hi ih b ii lx im ly iq lz iu ma iy mb jc lt lu lv lw bi translated"><a class="ae mc" href="https://www.quora.com/What-are-the-different-kinds-of-matrix-decomposition-and-what-purpose-does-each-one-serve" rel="noopener ugc nofollow" target="_blank">https://www . quora . com/What-the-different-species-of-matrix-decomposition-and-What-purpose-does-one-serve</a></li></ul></div></div>    
</body>
</html>