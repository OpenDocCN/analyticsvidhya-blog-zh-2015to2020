<html>
<head>
<title>A Beginner’s Guide for Dimensionality Reduction using Principal Component Analysis(PCA).</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用主成分分析(PCA)降维的初学者指南。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-beginners-guide-for-dimensionality-reduction-using-principle-component-analysis-pca-c4c515ae49c1?source=collection_archive---------3-----------------------#2019-11-01">https://medium.com/analytics-vidhya/a-beginners-guide-for-dimensionality-reduction-using-principle-component-analysis-pca-c4c515ae49c1?source=collection_archive---------3-----------------------#2019-11-01</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="308a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在将数据放入模型之前，将数据可视化。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/9306ab10c0ccf8eb297a3743ffa6da22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zzfWCFhbhjDDPjnU"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><a class="ae jt" href="https://unsplash.com/@victor_g?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">维克多</a>在<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="b6d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在进入正题之前，我有一个问题要问你。问题是——“<strong class="ih hj"/>你有没有想过，当你第一次在学校或大学学习这些数学概念时，你为什么还要学习它们，比如直线方程、平面、特征值、特征向量等等，或者它们在现实世界中有什么应用。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jv"><img src="../Images/ac946b34c3fcf953b4d9a224755f374c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*a7APKqis_TPuh_2ThMwOUw.gif"/></div></figure><p id="e279" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">坦率地说，当时我也不知道，但我向你保证，我们将一起解决这个谜:)</p><p id="7b25" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在解释这些话题的所有可能性中，今天我将尝试从数据科学|机器学习的角度来解释它们。让我们看一下目录:</p><ol class=""><li id="3825" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated">为什么是PCA？</li><li id="c74a" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">主成分分析的几何直觉</li><li id="6419" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">PCA(方差最大化)的数学目标函数</li><li id="860e" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">PCA的替代公式(距离最小化)</li><li id="a675" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">特征值和特征向量</li><li id="13ae" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">PCA代码</li><li id="289c" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">PCA的局限性</li><li id="75b2" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">五氯苯甲醚的其他用途</li></ol><blockquote class="kk kl km"><p id="1cb3" class="if ig ju ih b ii ij ik il im in io ip kn ir is it ko iv iw ix kp iz ja jb jc hb bi translated"><strong class="ih hj">先决条件</strong>:我假设你知道一些基础知识，比如什么是数据集，数据集中的特征是什么，以及一些高中数学知识。没什么！。</p></blockquote><p id="4ab2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ju">为什么是PCA？</em>T13】</strong></p><p id="13de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设你有一个非常高维的数据集，比如MNIST数据集，它有784个维度。如果我说“可视化这些数据”呢。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kq"><img src="../Images/cb97f17258162a148d3673e97831360c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*eqFtPOrZ7VQeToMS"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">照片由<a class="ae jt" href="https://unsplash.com/@patrickian4?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">帕特里克·福尔</a>在<a class="ae jt" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</figcaption></figure><p id="3f4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当然，我们人类无法想象超过3个维度。这就是PCA发挥作用的地方。</p><p id="c1e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了可视化，PCA还有其他的用途，我们会在继续学习这个概念的时候看到。</p><p id="23d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="ju">几何直觉</em> </strong></p><p id="ee41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">出于解释目的，让我们采用2D数据集来代替784。假设我们有一个包含两个特征f1和f2的数据集。<strong class="ih hj"> f1 </strong>代表头发的黑度<strong class="ih hj"> f2 </strong>代表身高(我知道这没有意义但是请多包涵！)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kr"><img src="../Images/20b803bc702c4d958d0dd6da3d1a6b2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DhLsI_qOvJwNXmyydWfv2A.jpeg"/></div></div></figure><p id="4e1c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个数据是二维的，因为我们有两个特征。如您所见，分布在特征f2上的数据点比分布在f1上的数据点多。</p><blockquote class="kk kl km"><p id="a75d" class="if ig ju ih b ii ij ik il im in io ip kn ir is it ko iv iw ix kp iz ja jb jc hb bi translated">价差的数学术语叫做方差</p></blockquote><p id="0dde" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们被迫从2D到1D，我们可以将这些点投影到特征f2上，并且简单地说这是数据的1D表示，因为<strong class="ih hj">分布/方差</strong>更多地在f2上。这是简单的权利。</p><blockquote class="kk kl km"><p id="c61a" class="if ig ju ih b ii ij ik il im in io ip kn ir is it ko iv iw ix kp iz ja jb jc hb bi translated">这次我们来看一个稍微棘手的数据:</p></blockquote></div><div class="ab cl ks kt gp ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="hb hc hd he hf"><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es kz"><img src="../Images/82f094fbc8472f4dadf7156f694e8cb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GwKAw4Oyv8YDXvFvRO8Ifw.jpeg"/></div></div></figure><p id="c11e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如您所见，在这种情况下，f1和f2保持相同的价差/方差。</p><p id="1ee9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果选择f1或f2，将会丢失50%的信息。在这种情况下，你会怎么做？</p></div><div class="ab cl ks kt gp ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="hb hc hd he hf"><p id="9d1b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个想法是，我们将轴的f1和f2分别旋转为f1 '和f2 '，其中f1' ⊥ f2 '，这样f1 '比f2 '具有最大的扩散(见下图):</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es la"><img src="../Images/d3056e70701632b17af8bbac64052e52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OSn9HCGQ78cljjgkB3BeKw.jpeg"/></div></div></figure><p id="5f4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我们找到了具有最大展开f1’的轴。现在我们可以放下F2’并将Xi的(数据点)完全投射到f1’上。我们的目标现在实现了，即从二维走向一维。</p><p id="95f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是PCA的基本功能。它试图找到正确的方向，在这个方向上我们可以保留更多的数据信息，并删除不必要的维度。</p><blockquote class="kk kl km"><p id="1c87" class="if ig ju ih b ii ij ik il im in io ip kn ir is it ko iv iw ix kp iz ja jb jc hb bi translated">现在我们来看看这个伟大想法背后的数学原理:</p></blockquote><p id="540a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">T3【PCA】数学目标函数T5】</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lb"><img src="../Images/8c444c813ff22872bf337d59ff07d5ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DqIQ9tihiTSek90zIoc-bA.jpeg"/></div></div></figure><p id="6c5d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">符号和先决条件公式:</p><ol class=""><li id="e8dc" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated">我们将f1’表示为“u1 ”,即最大方差轴，因为大多数在线解释更喜欢这种符号。</li><li id="2e71" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">所以，我们想找到单位向量u1，即||u1||=1，它保持最大方差。</li><li id="9417" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">u1 = u1^T.Xi上Xi的投影(这是我们想弄清楚的。)</li><li id="b061" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">我们假设数据是列标准化的，这意味着均值(<strong class="ih hj"> μ </strong> )=0，标准差(<strong class="ih hj"> σ </strong> )=1。</li></ol><blockquote class="kk kl km"><p id="0406" class="if ig ju ih b ii ij ik il im in io ip kn ir is it ko iv iw ix kp iz ja jb jc hb bi translated">来到数学方程式:</p></blockquote><p id="0351" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">方差公式:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lc"><img src="../Images/16074d0b9dc748e6371ea56e5814af65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rtp5Wba7zUWZ99MTPaJD5g.jpeg"/></div></div></figure><p id="5779" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们将上述公式与我们的问题联系起来:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ld"><img src="../Images/7b9298fbfb05af5d740ab979e9fd1ef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lzKdqsud-fAnk3hEHsy9_Q.jpeg"/></div></div></figure><p id="ddd3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们想要找出u1，使得投射到u1上的Xi的扩散/方差最大。</p><p id="66ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以我们要求的函数如下(见左边的方程)。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lc"><img src="../Images/7a294ad86987460e5d3fb8ee196d9409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jAVd_4aZysSKZcbB8Eu0kQ.jpeg"/></div></div></figure><p id="1342" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我们假设数据是列标准化的，这意味着mean=0，所以上述等式<strong class="ih hj"> </strong>中的<strong class="ih hj"> u1^T. x̄ </strong>部分变为0。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lc"><img src="../Images/4dc6df488030d288c79de137f096e3e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qoj6wHbgDpwyJILPu-XeDw.jpeg"/></div></div></figure><p id="cc20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，这将是最终的优化函数，我们要解决的是，在u1是单位向量的约束下，找到方差最大的u1。</p></div><div class="ab cl ks kt gp ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="hb hc hd he hf"><p id="b4a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">五氯苯甲醚的替代配方:(距离最小化)</strong></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es le"><img src="../Images/43518817d7ff40b0882c8665db9484a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*XGaA7KWUlhWZLIezYEBIHA.gif"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">参考:https://stats.stackexchange.com/a/140579</figcaption></figure><p id="7dc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上图中可以看出，在这个距离公式中，我们必须将Xi点到u1的距离最小化。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lc"><img src="../Images/77888fa3c6c48e9c3c18fef38a26c701.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KBpLOw9JO0Ttc3pAUo5b-A.jpeg"/></div></div></figure><p id="dcc5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">相邻距离不过是Xi到u1的投影，斜边是Xi的长度。现在我们可以很容易地从毕达哥拉斯定理中找出距离b/w Xi和u1，即di。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lc"><img src="../Images/4d9967bfd8cf63922b3cf659188811ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SZ6xwRNPTGxXXC5cIbktvg.jpeg"/></div></div></figure><p id="5430" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，最终的距离优化函数如下，其中我们希望通过最小化点Xi到u1的距离来找到u1。</p><p id="4973" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">之前的优化函数是方差最大化，而这个是距离最小化。</p><p id="3279" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">特征值和特征向量</strong></p><p id="6aea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我不打算在这里解释定义<em class="ju">(你有你的朋友谷歌！)</em>，不过我会解释为什么这些在PCA中有用。</p><blockquote class="kk kl km"><p id="0034" class="if ig ju ih b ii ij ik il im in io ip kn ir is it ko iv iw ix kp iz ja jb jc hb bi translated">前提概念:协方差矩阵。</p></blockquote><p id="39ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的优化函数的解可以通过使用由(λ1，λ2，λ3…表示的特征值和向量来获得。，λd)和(V1，V2，V3，…Vd)。记住特征值是标量。</p><p id="4981" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Sklearn库中有一个简单的函数，你必须给它一个协方差矩阵，它会返回相应的特征值和向量，其中λ1&gt;λ2&gt;λ3&gt;…&gt;λd。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lc"><img src="../Images/5ebedfe4635091c2eabf69552920f630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4KD4CXxdl_0xgXN85gzORA.jpeg"/></div></div></figure><p id="1c28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这意味着对应于λ1的向量，即V1具有解释的最高方差，然后是λ2，即V2，它是解释的方差第二大的向量，依此类推。</p><p id="4378" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以我们试图从优化函数中得到的u1，就是这里的V1。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lc"><img src="../Images/a02a8c025ab334b7650fd53ffa001dd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cs_kSaAeFaIJNIcX6Dam1Q.jpeg"/></div></div></figure><p id="62ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个非常好的性质是，每一对本征向量都是相互垂直的。</p><p id="0d84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这意味着如果我们取V1和V2，即对应于最高特征值的最高两个向量，这类似于得到f1 '和f2 '。类似地，我们可以将“d”表示为700或300或100或2或1，或者您想要减少到的任何数量的维度，它将返回数据集的顶部维度。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lf lg l"/></div></figure><p id="ce33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好吧！！理论说够了。让我们深入到代码部分。</p><p id="70c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">PCA代码</strong></p><blockquote class="kk kl km"><p id="8f17" class="if ig ju ih b ii ij ik il im in io ip kn ir is it ko iv iw ix kp iz ja jb jc hb bi translated"><strong class="ih hj">手动执行:</strong></p></blockquote><p id="a7fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">导入一些有用的库:</p><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="2746" class="lm ln hi li b fi lo lp l lq lr">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span></pre><p id="fc2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正在加载数据集:</p><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="fe8b" class="lm ln hi li b fi lo lp l lq lr">d0 = pd.read_csv(‘./mnist_train.csv’)<br/>l = d0[‘label’]<br/>d = d0.drop(“label”,axis=1)</span></pre><p id="1eea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">检查形状:</p><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="1ab7" class="lm ln hi li b fi lo lp l lq lr">print(d.shape)<br/>print(l.shape)</span></pre><p id="20a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(42000，784) <br/> (42000，)</p><p id="977e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你所看到的，有784个维度，每个维度代表图像中的一个像素</p><p id="d40c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看图像在该数据集中是什么样子的:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/80382158f327c289855cda54044fd603.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*r7rGGPOB-imXeF0r5-E4-g.png"/></div></figure><p id="ffe9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，正如我之前所说，我们必须将数据标准化:</p><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="41ee" class="lm ln hi li b fi lo lp l lq lr">from sklearn.preprocessing import StandardScaler<br/>standardized_data = StandardScaler().fit_transform(data)</span></pre><p id="faa3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">构建协方差矩阵:</p><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="b5b5" class="lm ln hi li b fi lo lp l lq lr">sample_data = standardized_data<br/>covar_matrix = np.matmul(sample_data.T , sample_data)</span><span id="61c7" class="lm ln hi li b fi lt lp l lq lr">print ( “The shape of variance matrix = “, covar_matrix.shape)</span><span id="b2d6" class="lm ln hi li b fi lt lp l lq lr">The shape of variance matrix =  (784, 784)</span></pre><p id="bca1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">寻找特征值和向量:</p><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="5dc3" class="lm ln hi li b fi lo lp l lq lr">from scipy.linalg import eigh<br/>values, vectors = eigh(covar_matrix, eigvals=(782,783))</span><span id="bb88" class="lm ln hi li b fi lt lp l lq lr">print(“Shape of eigen vectors = “,vectors.shape)<br/># converting the eigen vectors into (2,d) shape for easyness of further computations<br/>vectors = vectors.T</span><span id="dfa8" class="lm ln hi li b fi lt lp l lq lr">print(“Updated shape of eigen vectors = “,vectors.shape)<br/># here the vectors[1] represent the eigen vector corresponding 1st principal eigen vector<br/># here the vectors[0] represent the eigen vector corresponding 2nd principal eigen vector</span></pre><blockquote class="kk kl km"><p id="8bff" class="if ig ju ih b ii ij ik il im in io ip kn ir is it ko iv iw ix kp iz ja jb jc hb bi translated">特征向量的形状= (784，2) <br/>更新的特征向量的形状= (2，784)</p></blockquote><p id="f23a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过向量-向量乘法将原始数据样本投影到由两个主特征向量形成的平面上:</p><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="d4a6" class="lm ln hi li b fi lo lp l lq lr">import matplotlib.pyplot as plt<br/>new_coordinates = np.matmul(vectors, sample_data.T)</span><span id="0fa3" class="lm ln hi li b fi lt lp l lq lr">print (“ resultant new data points’ shape “, vectors.shape, “X”, sample_data.T.shape,” = “, new_coordinates.shape</span></pre><blockquote class="kk kl km"><p id="7f98" class="if ig ju ih b ii ij ik il im in io ip kn ir is it ko iv iw ix kp iz ja jb jc hb bi translated">生成的新数据点的形状为(2，784) X (784，15000) = (2，15000)</p></blockquote><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="6f95" class="lm ln hi li b fi lo lp l lq lr"># appending label to the 2d projected data<br/>new_coordinates = np.vstack((new_coordinates, labels)).T</span><span id="ed92" class="lm ln hi li b fi lt lp l lq lr"># creating a new data frame for ploting the labeled points.<br/>dataframe = pd.DataFrame(data=new_coordinates, columns=(“1st_principal”, “2nd_principal”, “label”))<br/>print(dataframe.head())</span><span id="b4ab" class="lm ln hi li b fi lt lp l lq lr">1st_principal  2nd_principal  label<br/>0      -5.558661      -5.043558    1.0<br/>1       6.193635      19.305278    0.0<br/>2      -1.909878      -7.678775    1.0<br/>3       5.525748      -0.464845    4.0<br/>4       6.366527      26.644289    0.0</span></pre><p id="82a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">绘图:</p><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="7bd6" class="lm ln hi li b fi lo lp l lq lr"># ploting the 2d data points with seaborn<br/>import seaborn as sn<br/>sn.FacetGrid(dataframe, hue=”label”, size=6).map(plt.scatter, ‘1st_principal’, ‘2nd_principal’).add_legend()<br/>plt.show()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/ec75fc7bc1200cc3d112bc610d56a2cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*NUTO_RS6G2xUza2Ibs-cCw.png"/></div></figure><blockquote class="kk kl km"><p id="3c1f" class="if ig ju ih b ii ij ik il im in io ip kn ir is it ko iv iw ix kp iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="hi"> Sklearn的实现:</em> </strong></p></blockquote><p id="becc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Sklearn有一个名为decomposition的包，可以简化我们的任务，而不是编写这么多代码:</p><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="dfdb" class="lm ln hi li b fi lo lp l lq lr"># initializing the pca<br/>from sklearn import decomposition<br/>pca = decomposition.PCA()</span><span id="c15d" class="lm ln hi li b fi lt lp l lq lr"># configuring the parameteres<br/># the number of components = 2<br/>pca.n_components = 2<br/>pca_data = pca.fit_transform(sample_data)</span><span id="224c" class="lm ln hi li b fi lt lp l lq lr"># pca_reduced will contain the 2-d projects of simple data<br/>print(“shape of pca_reduced.shape = “, pca_data.shape)</span></pre><p id="a444" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">pca_reduced.shape = (15000，2)的形状</p><p id="4e5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">绘图:</p><pre class="je jf jg jh fd lh li lj lk aw ll bi"><span id="3487" class="lm ln hi li b fi lo lp l lq lr"># attaching the label for each 2-d data point <br/>pca_data = np.vstack((pca_data.T, labels)).T</span><span id="997a" class="lm ln hi li b fi lt lp l lq lr"># creating a new data fram which help us in ploting the result data<br/>pca_df = pd.DataFrame(data=pca_data, columns=(“1st_principal”, “2nd_principal”, “label”))<br/>sn.FacetGrid(pca_df, hue=”label”, size=6).map(plt.scatter, ‘1st_principal’, ‘2nd_principal’).add_legend()<br/>plt.show()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/c869ce63a2416e9849e13e93a12d588b.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*8Hh_QfJS3RwUlQPVRbXGZA.png"/></div></figure><blockquote class="kk kl km"><p id="f64f" class="if ig ju ih b ii ij ik il im in io ip kn ir is it ko iv iw ix kp iz ja jb jc hb bi translated">解释的差异与维度数量:</p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lv"><img src="../Images/ca64b0a64b237310b250ed1992ac6b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*E9Fl79jd4t37h5lp_9lFag.png"/></div></figure><p id="7fe0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如您所见，仅使用200个特征/尺寸就可以解释或保留90%的差异。因此，不要使用所有的784个维度来建模，你可以选择200个特征或者更好，即使你选择400个维度，你也可以保留99%的信息。</p><p id="9173" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">PCA的局限性</strong></p><blockquote class="kk kl km"><p id="2637" class="if ig ju ih b ii ij ik il im in io ip kn ir is it ko iv iw ix kp iz ja jb jc hb bi translated">一些失败案例包括:</p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/c08a647ce909e0b80e9020c1844a9380.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*rgN8gX4r9QCX7QtlExT_cA.jpeg"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">参赛作品:<a class="ae jt" href="https://qr.ae/TWozmU" rel="noopener ugc nofollow" target="_blank">https://qr.ae/TWozmU</a></figcaption></figure><ol class=""><li id="de79" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated">PCA关注于寻找包含可能的最高方差的数据集的正交投影，以便在数据集的变量之间“寻找隐藏的线性相关性”。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/d235c8bacc6a93c3c3577ced2a33ebbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*Y_xlpeE--Q32BYkfofwE0Q.jpeg"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">参赛:https://qr.ae/TWozmU</figcaption></figure><p id="19a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是如果你的特征不是线性的，比如说是螺旋形或者其他形状，那么PCA就不是你的最佳选择。</p></div><div class="ab cl ks kt gp ku" role="separator"><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx ky"/><span class="kv bw bk kw kx"/></div><div class="hb hc hd he hf"><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lc"><img src="../Images/8d13d6f6def5e080c0c2b6aea3ba45ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G60R9v_uuOJDQtpu7LIqNw.jpeg"/></div></div></figure><p id="8f82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.如果你的数据集遵循一个很好的结构，比如正弦波，如果你把它投影到V1，我们就失去了关于结构的重要信息，这些信息可能在机器学习任务中有用，比如特征工程。</p><p id="38dc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">五氯苯甲醚的其他用途</strong></p><ol class=""><li id="c6e8" class="jw jx hi ih b ii ij im in iq jy iu jz iy ka jc kb kc kd ke bi translated"><strong class="ih hj">缩减大小</strong>:当我们有太多的数据，我们将使用随机森林、XGBoost等过程密集型算法处理数据时，我们需要消除冗余。</li><li id="e771" class="jw jx hi ih b ii kf im kg iq kh iu ki iy kj jc kb kc kd ke bi translated">不同的视角:有时视角的改变比减少更重要。</li></ol><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/e545aed98e687821633e8f72f5b1ef77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*12o7cQLSn_fRGxIKSnYAOQ.jpeg"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">参考:https://qr.ae/TWo65s<a class="ae jt" href="https://qr.ae/TWo65s" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure><blockquote class="kk kl km"><p id="e22c" class="if ig ju ih b ii ij ik il im in io ip kn ir is it ko iv iw ix kp iz ja jb jc hb bi translated">如果你发现本博客有任何错误，欢迎在评论框中讨论。</p></blockquote><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jv"><img src="../Images/99e4988c660a8ca1a1c930012bd934a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*yxjn47pbUXoAF3GaB6RESA.gif"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><a class="ae jt" href="http://gph.is/19Mx2oF" rel="noopener ugc nofollow" target="_blank">http://gph.is/19Mx2oF</a></figcaption></figure><p id="0b56" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将会张贴越来越多的内容，以更简单的方式解释各种复杂的主题。直到那时再见:)</p><p id="eea8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在Linkedin上关注我:<a class="ae jt" href="https://www.linkedin.com/in/dileep-teja-473088141/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/dileep-teja-473088141/</a></p></div></div>    
</body>
</html>