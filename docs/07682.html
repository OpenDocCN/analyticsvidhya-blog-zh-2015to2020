<html>
<head>
<title>Deep Learning: An Intuitive guide from a beginner to an expert level</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习:从初学者到专家水平的直观指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-learning-an-intuitive-guide-from-a-beginner-to-an-expert-level-999454d8234a?source=collection_archive---------25-----------------------#2020-07-03">https://medium.com/analytics-vidhya/deep-learning-an-intuitive-guide-from-a-beginner-to-an-expert-level-999454d8234a?source=collection_archive---------25-----------------------#2020-07-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="401a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">神经元、感知器、神经网络和反向传播初学者指南。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/bb5982a32d343b26439f301d928f897f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*39VPIiyY3em1uMkqJy37NA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作者图片</figcaption></figure><p id="3abe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">众所周知，机器学习是人工智能的一个分支，在这个分支中，我们赋予机器学习的能力，而无需使用一些数据进行显式编程。在本文中，我们将讨论深度学习。深度学习是一套特殊的机器学习算法，使用人工神经网络。人工神经网络是一种受生物神经网络启发的先进计算系统。这就是为什么说它们描绘了人类的智慧。由于易于获得大计算能力和数据集，深度学习可以用于各种领域，如自然语言处理(NLP)、计算机视觉、欺诈检测、模式识别、语音识别、生物信息学等，因此它们在各种领域都有巨大的需求，如银行部门、医疗保健部门、教育部门、娱乐部门等。所以让我们开始理解深度学习的基础。深度学习本身是一个非常大的领域，并且在不断增长，因此我们需要在掌握这个领域之前学习很多概念。在这篇文章中，我将只涉及深度学习的基础知识。在以后的文章中，我将深入这个领域，并涵盖每个深度学习爱好者必须知道的所有主要流行词汇。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="2f81" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">内容-</h1><ol class=""><li id="9fb0" class="ky kz hi ih b ii la im lb iq lc iu ld iy le jc lf lg lh li bi translated">人工神经元如何受到生物神经元的启发？</li><li id="9c89" class="ky kz hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">逻辑回归的神经元表示</li><li id="fc2b" class="ky kz hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">感知器</li><li id="361c" class="ky kz hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">多层感知器(MLP)</li><li id="e40f" class="ky kz hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">训练单个感知器模型</li><li id="6d01" class="ky kz hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">训练多层感知器</li><li id="9b54" class="ky kz hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">反向传播算法</li><li id="6c1e" class="ky kz hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">结论</li><li id="7f3b" class="ky kz hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">信用</li></ol></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="9e86" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">人工神经元如何受到生物神经元的启发？</h1><p id="11ae" class="pw-post-body-paragraph if ig hi ih b ii la ik il im lb io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">神经元是大脑的基本工作单位。在大脑中，几个神经元相互连接，在激活时执行一项特殊的任务。生物神经元有三个部分:称为树突(一种细胞体)的输入单元和称为轴突的输出单元。树突向神经元提供输入。每个神经元都有一个细胞体，在那里对它们进行特定类型的操作，然后产生输出。这种输出通过轴突传递给其他单位。随着这些轴突与其他几个单元连接，它形成了一个被称为生物神经网络的网络。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lr"><img src="../Images/74d3c4a79559c3f0277b9a95fac52a57.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*3tZibzP1TPzpbGSjB8vKIg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">生物神经元(来源:维基百科)</figcaption></figure><p id="1824" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">人工神经网络就是受这个概念的启发。人工神经元是与生物神经元非常相似的数学函数。像树突一样，人工神经元有输入单元，像轴突一样，人工神经元有输出单元，像细胞体对输入进行一些操作，人工神经元也有数学函数，对输入进行一些操作。如上图所示，一些枝晶比其他的更粗。为了在人工神经元中描述同样的事情，我们给每个输入单元分配权重。所以基本上权重有助于决定哪个输入更重要。然后，存在于人工神经元内部的被称为激活函数的函数获取这些输入和权重，然后对它们进行处理，最后产生输出，该输出然后以与生物神经元相同的方式被传送到输出单元。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ls"><img src="../Images/255725c19ffc71929b7433a50fed6e33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/1*Gy3pNinQ_mxTz_hH6o76CA.gif"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">人工神经元(来源:cnl.salk.edu)</figcaption></figure><p id="43e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设神经元的输入由向量X(x1，x2，x2)表示，权重由向量W(w1，w2，w3)表示，那么神经元将首先计算它们的点积，该点积将是x1.w1+x2.w2+x3.w3。该点积也称为输入的加权和。然后这个点积将通过激活函数‘f’。因此，输出“y”将是f(x1.w1+x2.w2+x3.w3)。根据我们选择的激活函数，我们将得到不同的输出。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="9579" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">逻辑回归的神经元表示</h1><p id="587e" class="pw-post-body-paragraph if ig hi ih b ii la ik il im lb io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">逻辑回归是最简单的分类算法之一。它使用sigmoid函数(也称为逻辑函数，用σ表示)来计算给定输入的输出。然后基于输出，我们解释输入的类别。对于输入X权重W和偏差b，输出将为sigmoid(W'X+b)。我们可以用一个简单的神经元来表示逻辑回归。在人工神经元中，如果我们使用sigmoid函数作为激活函数，它将表现为逻辑回归。这样，逻辑回归可以成为神经网络的一层。我们可以从随机初始化这些权重开始，然后使用梯度下降算法训练它们。(在以后的博客中，我将讨论一些用于权重初始化及其训练的智能技术)。在神经网络中堆叠更多这样的函数可以帮助它学习输入和输出之间的复杂关系。这就是神经网络的力量所在。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lt"><img src="../Images/de1e6849c952caaac4dc77c04254d12b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*mGyAkZ2wm4LmtK6GMY2-gA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">逻辑回归的神经元表示(来源:mc.ai)</figcaption></figure></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="c82d" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">感知器-</h1><p id="4a1e" class="pw-post-body-paragraph if ig hi ih b ii la ik il im lb io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">感知器是一种用于二进制分类的监督学习算法。它是一个单层神经网络，使用Heaviside阶跃函数进行激活。如果输入为负，阶跃函数输出为0；如果输入为0或正，阶跃函数输出为1。当加权输入给感知器时，它使用阶跃函数进行计算，然后输出0或1。然后，它更新权重以提高准确性。在这篇博客的后面，我们将了解这些权重是如何更新的。感知机是弗兰克·罗森布拉特在1958年发明的。一个有趣的事实是，它是作为机器而不是软件被发明的。感知器是一个简单的人工神经元，以阶跃函数作为激活函数。我们将使用这些感知器来创建一个复杂的神经网络，它将有能力在输入和输出之间形成复杂的关系。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lu"><img src="../Images/87139314542328d526be32bbacebd6f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*6K2w2TpQmHyphnv5fSvxUg.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">亥维赛阶梯函数(来源:维基百科)</figcaption></figure></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="6065" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">多层感知器-</h1><p id="2541" class="pw-post-body-paragraph if ig hi ih b ii la ik il im lb io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">多层感知器(MLP)是一个完全连接的神经网络，这意味着当前层的所有节点都连接到下一层的所有节点。它由一个输入层、几个隐藏层和最后一个输出层组成。隐藏层是那些其输入依赖于先前层的输出的层。如果一个MLP只有一个隐藏层，它通常被称为香草网络。为了简单起见，让我们从两个隐藏层开始。为了在继续之前把事情弄清楚，让我们讨论一些我们将使用的符号。</p><p id="c2bd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第I层第j个神经元上的Fᵢⱼ:函数</p><p id="fe58" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从第I个神经元到第j个神经元的Wₖᵢⱼ:权重和“k”代表下一层</p><p id="2118" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第I层到第j层的Oᵢⱼ:输出</p><p id="39a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了简单起见，我没有在图中标出所有的重量。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lv"><img src="../Images/ad987a820d6dc3cf0fa41a8f9eeb2118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*59d6z5IbMKQhBIjHK7IG1w.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">多层感知器(作者图片)</figcaption></figure><p id="798b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们使用矩阵来表示层之间的权重，那么它将是NXM维数，其中N和M分别表示每层的神经元数量。因此，输入层和第一隐藏层之间的权重矩阵的大小将为6×4，第一隐藏层和第二隐藏层之间的权重矩阵的大小将为4×3，第二隐藏层和最终层之间的权重矩阵的大小将为3×1。</p><p id="683a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MLP使用非线性激活函数来为每个神经元生成输出。基本上，MLP首先工作，我们初始化所有权重，然后根据当前权重和给定数据点，MLP计算输出，然后根据该输出，使用实际输出计算损耗。之后，它使用梯度下降算法迭代更新权重以最小化损失(或者使用它的一些变体或优化器，我将在未来的博客中介绍)。</p><p id="582a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是MLP的工作原理。让我们深入了解它，理解它工作背后的所有数学原理。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="a09d" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">训练单个感知器模型:</h1><p id="87d2" class="pw-post-body-paragraph if ig hi ih b ii la ik il im lb io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">让我们训练一个单一的感知器模型来执行回归任务。我们有一个由D表示的数据集，由输入X(x1，x2…xn)和相应的输出Y(y1，y2…yn)组成。假设我们使用relu函数作为所有神经元的激活函数，即神经元的输入小于零，那么它的输出也将是零，否则它的输出将与输入相同。为了简单起见，让我们假设我们正在随机初始化权重。当我们在解决回归问题时，让我们使用均方误差函数作为损失函数。这里W'X+b代表输入的加权和</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/a3d8bcef51c891f037d2e121217c470a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vMtwueSIeJbg3dN1tUJomA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作者图片</figcaption></figure><p id="f1d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们目标是最小化损失函数，表示为-</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lx"><img src="../Images/6ba4137ab2648ab0d8d3508a28e725d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RduqbiGQsi19d-zKNvxnEg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作者图片</figcaption></figure><p id="448a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以我们的目标是找到W使得损失(L)最小。让我们用梯度下降算法来做这件事。对于梯度下降，我们需要计算损失函数对重量(w)的导数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/df30a4d5e4231abf1d9af346b14d223d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*RjWfhDhKgwo4ByQ6Ul01gw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作者图片</figcaption></figure><p id="1f9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设r为学习率(0.001)，使用梯度下降权重将更新为</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lz"><img src="../Images/4cbfa09e2e4d9e7127a24e4f21116f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*hVwzB3v7sRLLwZB6umvt7A.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作者图片</figcaption></figure><p id="776d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重复该过程直到收敛，即重复该过程直到W(新的)和W(旧的)变得几乎相同。</p><p id="27d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到目前为止，我们还没有做任何独特的事情。这个过程后面是所有的优化问题，都是用梯度下降法解决的。我们要做的独特的事情是，我们将使用微分链法则，而不是直接计算损失对重量的导数。在这个阶段，这可能看起来没有用，因为我们可以直接计算导数，但相信我，当我们训练多层感知器时，这将非常非常有用。所以我们不用直接计算损失对重量的导数，而是用这个-</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ma"><img src="../Images/2b4173164ae8194692fdd55e1cc215cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*Vu5_ARpuVDUA1L4_dNU-Mw.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作者图片</figcaption></figure><p id="cef7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为L是“y(pred)”的函数，所以L相对于“y(pred)”的导数可以容易地计算，并且因为y(pred)是W的函数，所以W相对于y(pred)的导数也可以容易地计算。我们将使用这个等式来更新权重。</p><p id="fd10" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当达到收敛时，我们找到最佳权重以最小化损失的任务就完成了。因此我们的模型被训练。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="6bfd" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">训练一个多层感知器(MLP)模型</h1><p id="c67d" class="pw-post-body-paragraph if ig hi ih b ii la ik il im lb io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">训练MLP与训练单个感知机模型是一样的。在MLP，方程式可能看起来非常复杂，但是如果你仔细阅读这篇博客，你会很容易理解它们。我试图用尽可能简单的方式来解释它们。让我们再看一次我们之前用过的图表</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lv"><img src="../Images/ad987a820d6dc3cf0fa41a8f9eeb2118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*59d6z5IbMKQhBIjHK7IG1w.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作者图片</figcaption></figure><p id="2454" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们已经在上面的MLP中讨论了权重矩阵的维数。如果我们把它们加起来，我们就得到了6*4+4*3+3*1，也就是39个重量。因为它们有很多权重，所以我只计算几个权重来直观地说明如何计算每一层的权重。</p><p id="9aed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于MLP，我们也使用与前一个案例相同的假设，即我们随机初始化权重，对所有神经元使用relu函数，并使用均方误差来计算总损失。当输入给这个网络时，第一隐藏层中的神经元被激活，然后它们激活第二隐藏层的神经元，并且以这种方式使用前向传播产生输出。然后使用这个输出损失的计算类似于我们对单感知器的计算</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lx"><img src="../Images/6ba4137ab2648ab0d8d3508a28e725d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RduqbiGQsi19d-zKNvxnEg.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作者图片</figcaption></figure><p id="3372" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的目标是找到最佳W，使这种损失最小。为此，我们可以像前面讨论的一样，再次使用梯度下降。我们在这里的主要挑战是计算损失对重量的导数。正如我之前提到的，我们会用微分的链式法则来计算所有这些导数。使用链式法则，我们可以很容易地计算这些导数，然后在梯度下降算法中使用它们。我们将从最后一层开始计算导数，然后向内移动，因为许多这些导数将重复。</p><p id="6b29" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用链式法则计算的W₃₁₁、W₃₂₁和W₃₃₁损失的导数为-</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mb"><img src="../Images/eb149107003a2391c250917fcc9784c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*JqkxVNU3Q0eMMmNj_i9L7Q.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作者图片</figcaption></figure><p id="b57a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在往回走，我们需要计算W₂₁₁、W₂₁₂、W₂₁₃、W₂₁₄、W₂₂₁、W₂₂₂、W₂₂₃、W₂₂₄ W₂₃₁、w和w的损失导数。</p><p id="6570" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我会用几个例子来展示如何计算这些导数，其余的可以用同样的直觉来计算-</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mc"><img src="../Images/ce4f50b429f5095d1c9612a70d1874df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*6PX7muO4h8RKOprXlqcvfQ.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作者图片</figcaption></figure><p id="965c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在往回走，我们需要计算损失对这些重量的导数-</p><p id="a8f1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">W₁₁₁、W₁₂₁、W₁₃₁、W₁₄₁、W₁₅₁、W₁₆₁、W₁₁₂、W₁₂₂</p><p id="be27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们仔细看我们的MLP图，很清楚从输入到输出有3条路径。在这种情况下计算导数可以通过这个图像很容易理解。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es ly"><img src="../Images/f01f0718c3540164e1728c1d2dc3832a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*pTW9yhgksPebRgKYftv27Q.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作者图片</figcaption></figure><p id="ca52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的图像中，很明显这次计算导数将会非常复杂。因此我只计算一个导数，其余的可以用同样的方法计算-</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es md"><img src="../Images/65ff3b875909067368f61631b0657601.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*-hy-cfVygIjZquwLd5fvnA.png"/></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作者图片</figcaption></figure><p id="46f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重复同样的过程，我们可以用同样的方式计算剩余的导数。</p><p id="d2ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，使用上述步骤，可以计算这种多层感知所需的所有导数。现在，我们可以执行梯度下降算法来更新权重，直到收敛。这样，一个MLP就被训练出来了。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es me"><img src="../Images/42453cef00f41d5840b3457e025e8c46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x3GE5MtgXMuz2mwWk6HAtw.jpeg"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作者图片</figcaption></figure></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="f2d5" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">反向传播算法</h1><p id="d99f" class="pw-post-body-paragraph if ig hi ih b ii la ik il im lb io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">到目前为止，你一定很清楚，为了训练一个MLP，我们需要计算很多导数。很多只是重复而已。因此，为了避免计算的冗余和优化，我们有一个算法，使用动态规划来计算导数。这种算法被称为反向传播算法。它从最后一层到内层应用了我们前面讨论过的链式法则，以避免冗余的计算和记忆。其余所有步骤与我们之前讨论的相同。</p><blockquote class="mf mg mh"><p id="0802" class="if ig mi ih b ii ij ik il im in io ip mj ir is it mk iv iw ix ml iz ja jb jc hb bi translated">简单地说，反向传播是链式法则和动态规划(记忆化)的结合。</p></blockquote></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="a857" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">结论-</h1><p id="749a" class="pw-post-body-paragraph if ig hi ih b ii la ik il im lb io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">在这篇博客中，我讨论了深度学习的基础知识。我们看到了人工神经元如何受到生物神经元的启发，什么是感知器，感知器如何工作，如何使用反向传播算法训练MLP(深度神经网络)。</p></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><h1 id="1810" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">学分-</h1><ol class=""><li id="ed9b" class="ky kz hi ih b ii la im lb iq lc iu ld iy le jc lf lg lh li bi translated"><a class="ae mm" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Artificial_neural_network</a></li><li id="44f8" class="ky kz hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">【https://en.wikipedia.org/wiki/Artificial_neural_network T2】号</li><li id="1d05" class="ky kz hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated">【http://citeseerx.ist.psu.edu/viewdoc/download? T4】doi = 10 . 1 . 1 . 335 . 3398&amp;rep = re P1&amp;type = pdf</li><li id="e495" class="ky kz hi ih b ii lj im lk iq ll iu lm iy ln jc lf lg lh li bi translated"><a class="ae mm" href="http://www.cs.stir.ac.uk/courses/ITNP4B/lectures/kms/4-MLP.pdf" rel="noopener ugc nofollow" target="_blank">http://www . cs . stir . AC . uk/courses/itnp 4b/lectures/kms/4-MLP . pdf</a></li></ol></div><div class="ab cl jt ju gp jv" role="separator"><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy jz"/><span class="jw bw bk jx jy"/></div><div class="hb hc hd he hf"><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/b64aadec932ebc34abae145e6fb34bfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QsrtkGoTtadYZUj3eKELgA.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">作者图片</figcaption></figure><p id="7479" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那都是我这边的。感谢阅读这篇文章。使用的少数图片的来源被提及，其余的是我的创作。请随意发表评论，建议改正和改进。通过Linkedin与我联系，或者你可以给我发邮件，地址是sahdevkansal02@gmail.com。我期待听到您的反馈。如果你想阅读更多这样的文章，请关注我的媒体。</p></div></div>    
</body>
</html>