<html>
<head>
<title>ML12: A Detailed PyTorch Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML12:详细的 PyTorch 教程</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ml12-59d2a56737ac?source=collection_archive---------21-----------------------#2020-12-16">https://medium.com/analytics-vidhya/ml12-59d2a56737ac?source=collection_archive---------21-----------------------#2020-12-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="1e7e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">关于张量运算的一切</h2></div><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="5e3c" class="jg jh hi jc b fi ji jj l jk jl">Read time: 30 min</span><span id="90a3" class="jg jh hi jc b fi jm jj l jk jl">Complete code on Colab: <a class="ae jn" href="https://bit.ly/3oWWYws" rel="noopener ugc nofollow" target="_blank">https://bit.ly/3oWWYws</a></span></pre><p id="980a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">本文旨在面对 NN/DL 问题时，快速获取所需的代码。为了简化起见，我们只讨论代码的主要部分。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><blockquote class="kr ks kt"><p id="6705" class="jo jp ku jq b jr js ij jt ju jv im jw kv jy jz ka kw kc kd ke kx kg kh ki kj hb bi translated"><strong class="jq hj"> <em class="hi">概述</em> </strong></p><p id="28bd" class="jo jp ku jq b jr js ij jt ju jv im jw kv jy jz ka kw kc kd ke kx kg kh ki kj hb bi translated"><strong class="jq hj"> <em class="hi"> (1) </em> </strong> <a class="ae jn" href="#2b94" rel="noopener ugc nofollow"> <strong class="jq hj"> <em class="hi">设置</em> </strong> </a></p><p id="742e" class="jo jp ku jq b jr js ij jt ju jv im jw kv jy jz ka kw kc kd ke kx kg kh ki kj hb bi translated"><strong class="jq hj"><em class="hi">(2)</em></strong><a class="ae jn" href="#f9d2" rel="noopener ugc nofollow"><strong class="jq hj"><em class="hi">PyTorch 基础知识 01</em></strong></a><em class="hi"><br/># # 01:一个辅助函数— describe() <br/> ## 02: zeros()、one()、arange() <br/> ## 03: rand()、randn()、randint() <br/> ## 04:从 Python 列表到 py torch 张量<br/> ## 05:张量类型— float、long <br/>float()，。long()，。int() <br/> ## 07:张量类型--。类型()&amp;。dtype <br/> ## 08: PyTorch 和 NumPy —from _ NumPy()<br/># # 09:py torch 和 NumPy—。numpy() <br/> ## 10:张量大小--。size() &amp;。形状<br/> ## 11:分度&amp;切片— 1 <br/> ## 12:分度&amp;切片— 2 <br/> ## 13:分度&amp;切片— 3 </em></p><p id="fb6d" class="jo jp ku jq b jr js ij jt ju jv im jw kv jy jz ka kw kc kd ke kx kg kh ki kj hb bi translated"><strong class="jq hj"><em class="hi">(3)</em></strong><a class="ae jn" href="#2c25" rel="noopener ugc nofollow"><strong class="jq hj"><em class="hi">py torch 基础知识 02</em></strong><em class="hi"><br/># # 01:加法— add()，。add_()，。fill()，。zero_() <br/> ## 02:乘法— 1 <br/> ## 03:乘法— 2 <br/> ## 04:乘法— 3 <br/> ## 05:更多数学函数— mean()、sum()、sqrt()、sqrt_()、abs()、acos() <br/> ## 06:更多数学函数— eq()、equal()、ge()、gt() <br/> ## 07:基于维度的张量运算:。view()，shape()，。t()，transpose() <br/> ## 08:基于维度的张量运算:squeeze() &amp;。unsqueeze() <br/> ## 09:基于维度的张量运算:cat()<br/># # 10:基于维度的张量运算:split()【chunk()<br/># # 11:索引到存储器— 1 <br/> ## 12:索引到存储器— 2 <br/> ## 13:连续张量— 1 <br/> ## 14:连续张量— 2 <br/> ## 15:连续张量— 3 <br/> ##normal()、非零()、expand()、expand_as() <br/> ## 17:其他函数:argmax()、max()、min()、sort() <br/> ## 18:其他函数:eye()、zeros_like()、ones_like() </em></a></p><p id="96cc" class="jo jp ku jq b jr js ij jt ju jv im jw kv jy jz ka kw kc kd ke kx kg kh ki kj hb bi translated"><strong class="jq hj"><em class="hi">(4)</em></strong><a class="ae jn" href="#a361" rel="noopener ugc nofollow"><strong class="jq hj"><em class="hi">py torch 基础知识 03</em></strong></a><em class="hi"><br/># # 01:GPU 上的张量——。cuda() &amp;。CPU()<br/># # 02:GPU 上的张量—to(device = ' cuda ')<br/># # 03:GPU 上的张量—一个错误<br/># # 04:GPU 上的张量—to(torch . device(" CPU "))<br/># # 05:CPU&amp;GPU 之间通过矩阵乘法精心制作的速度差<br/> ## 06:计算图— 1 <br/> ## 07:计算图— 2 <br/> ## 08:计算图— 3</em></p><p id="4a1a" class="jo jp ku jq b jr js ij jt ju jv im jw kv jy jz ka kw kc kd ke kx kg kh ki kj hb bi translated"><strong class="jq hj"> <em class="hi"> (5) </em> </strong> <a class="ae jn" href="#7f1c" rel="noopener ugc nofollow"> <strong class="jq hj"> <em class="hi">汇总</em> </strong> </a></p><p id="244a" class="jo jp ku jq b jr js ij jt ju jv im jw kv jy jz ka kw kc kd ke kx kg kh ki kj hb bi translated"><strong class="jq hj"><em class="hi">(6)</em></strong><a class="ae jn" href="#3933" rel="noopener ugc nofollow"><strong class="jq hj"><em class="hi">参考文献</em> </strong> </a></p></blockquote></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><p id="3ece" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们已经在 ML03 和 ML06 中介绍了 PyTorch 和 Colab 的一些先决条件，现在是时候深入实施 PyTorch 了。</p><div class="ky kz ez fb la lb"><a href="https://merscliche.medium.com/ml06-893e4cb389c6" rel="noopener follow" target="_blank"><div class="lc ab dw"><div class="ld ab le cl cj lf"><h2 class="bd hj fi z dy lg ea eb lh ed ef hh bi translated">ML06:谷歌 Colab 上的 PyTorch</h2><div class="li l"><h3 class="bd b fi z dy lg ea eb lh ed ef dx translated">免费访问英伟达特斯拉 T4</h3></div><div class="lj l"><p class="bd b fp z dy lg ea eb lh ed ef dx translated">merscliche.medium.com</p></div></div><div class="lk l"><div class="ll l lm ln lo lk lp lq lb"/></div></div></a></div><div class="ky kz ez fb la lb"><a href="https://becominghuman.ai/ml03-9de2f0dbd62d" rel="noopener  ugc nofollow" target="_blank"><div class="lc ab dw"><div class="ld ab le cl cj lf"><h2 class="bd hj fi z dy lg ea eb lh ed ef hh bi translated">ML03: PyTorch vs. Tensorflow</h2><div class="li l"><h3 class="bd b fi z dy lg ea eb lh ed ef dx translated">py troch——一个有前途的人工神经网络框架</h3></div><div class="lj l"><p class="bd b fp z dy lg ea eb lh ed ef dx translated">becominghuman.ai</p></div></div><div class="lk l"><div class="lr l lm ln lo lk lp lq lb"/></div></div></a></div><p id="f0c5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">读者可以浏览完整代码的整个大纲，但我们不会在本文中涵盖所有内容。在这里查看完整的代码:<br/>【https://bit.ly/3oWWYws T21】</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="2b94" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated"><strong class="ak"> <em class="mj"> (1)设置</em> </strong></h1><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="96b7" class="jg jh hi jc b fi ji jj l jk jl">#### (1) Setup<br/><br/>import torch</span><span id="797e" class="jg jh hi jc b fi jm jj l jk jl"># "Runtime" -&gt; "Change runtime type" -&gt; "Hardware accelerator" -&gt; Choose "GPU"</span><span id="5161" class="jg jh hi jc b fi jm jj l jk jl">print(torch.cuda.is_available()); print('\n')<br/>!nvidia-smi</span></pre><p id="69d6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">CUDA 是一种特殊的 API，允许在 GPU 上进行计算。CUDA 是英伟达建立的，只能在英伟达的 GPU 上使用。[3]</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="f9d2" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated"><strong class="ak"> <em class="mj"> (2) PyTorch 基础知识 01 </em> </strong></h1><h2 id="5f46" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated"><em class="mj"> ## 01:一个助手函数— describe() </em></h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="967d" class="jg jh hi jc b fi ji jj l jk jl">## 01: A helper function -- describe()</span><span id="f082" class="jg jh hi jc b fi jm jj l jk jl">def describe(x):<br/>print("Type: {}".format(x.type()))<br/>print("Dtype: {}".format(x.dtype))<br/>print("Shape/size: {}".format(x.shape))<br/>print("Values: \n{}".format(x))</span></pre><p id="3c69" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">从现在开始，我们创建的这个函数将对我们有很大帮助。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="19bf" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 02:零()、一()、一个整数()</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="7733" class="jg jh hi jc b fi ji jj l jk jl">## 02: zeros(), ones(), arange()</span><span id="acf8" class="jg jh hi jc b fi jm jj l jk jl">describe(torch.zeros(2,3)); print('\n')<br/>describe(torch.ones(2,3)); print('\n')<br/>describe(torch.ones(2,2,3)); print('-----')</span><span id="5712" class="jg jh hi jc b fi jm jj l jk jl">x = torch.arange(6)<br/>describe(x)</span></pre></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="7638" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated"># 04:从 Python 列表到 PyTorch 张量</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="068f" class="jg jh hi jc b fi ji jj l jk jl">## 04: From Python lists to PyTorch tensors</span><span id="0ee1" class="jg jh hi jc b fi jm jj l jk jl">import numpy as np</span><span id="8e57" class="jg jh hi jc b fi jm jj l jk jl">x = torch.tensor([[1,2,3],<br/>                 [4,5,6]])<br/>describe(x); print('-----')<br/># Type: torch.LongTensor (torch.int64)</span><span id="8196" class="jg jh hi jc b fi jm jj l jk jl">x = torch.Tensor([[1,2,3],<br/>                 [4,5,6]])<br/>describe(x); print('-----')<br/># Type: torch.FloatTensor (torch.float32)</span><span id="5240" class="jg jh hi jc b fi jm jj l jk jl">x = torch.Tensor(np.array([[1,2,3],<br/>                 [4,5,6]]))<br/>describe(x)<br/># Type: torch.FloatTensor (torch.float32)</span></pre><p id="0d45" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">稍后我们将看到将 NumPy 数组转换为 PyTorch 张量。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="d093" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 05:张量类型—浮点型，长整型</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="e855" class="jg jh hi jc b fi ji jj l jk jl">## 05: Tensor types -- float, long</span><span id="7616" class="jg jh hi jc b fi jm jj l jk jl">'''<br/>1. The default type of torch.Tensor() is torch.FloatTensor()<br/>2. The default type of torch.tensor() is torch.LongTensor()<br/>3. Type: float, long, double and so on<br/>4. Two ways to define types: like FloatTensor() and LongTensor() OR like dtype= torch.int64<br/>'''<br/>x = torch.ones(2,3)<br/>describe(x); print('\n')<br/># Type: torch.FloatTensor</span><span id="7971" class="jg jh hi jc b fi jm jj l jk jl">x = torch.Tensor([[1,2,3],<br/>                  [4,5,6]])<br/>describe(x); print('\n')<br/># Type: torch.FloatTensor</span><span id="2de5" class="jg jh hi jc b fi jm jj l jk jl">x = torch.FloatTensor([[1,2,3],<br/>                       [4,5,6]])<br/>describe(x); print('-----')<br/># Type: torch.FloatTensor</span><span id="215a" class="jg jh hi jc b fi jm jj l jk jl"># ---------------------------</span><span id="2515" class="jg jh hi jc b fi jm jj l jk jl">x = torch.arange(6)<br/>describe(x); print('\n')<br/># Type: torch.FloatTensor</span><span id="83c2" class="jg jh hi jc b fi jm jj l jk jl">x = torch.tensor([[1,2,3],<br/>                  [4,5,6]])<br/>describe(x); print('\n')<br/># Type: torch.LongTensor</span><span id="232b" class="jg jh hi jc b fi jm jj l jk jl">x = torch.LongTensor([[1,2,3],<br/>                      [4,5,6]])<br/>describe(x)<br/># Type: torch.LongTensor</span></pre><p id="30e0" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">弄清楚数据的类型很重要。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="6fc1" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 06:张量类型-。float()，。long()，。int()</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="0766" class="jg jh hi jc b fi ji jj l jk jl">## 06: Tensor types -- .float(), .long(), .int()</span><span id="455f" class="jg jh hi jc b fi jm jj l jk jl">x = torch.LongTensor([[1,2,3],<br/>                      [4,5,6]])</span><span id="95a2" class="jg jh hi jc b fi jm jj l jk jl">x = x.long()<br/>describe(x)<br/>print(x.dtype); print('-----')</span><span id="5be1" class="jg jh hi jc b fi jm jj l jk jl">x = torch.tensor([[1,2,3],<br/>                  [4,5,6]], dtype= torch.int64)<br/>describe(x)<br/>print(x.dtype); print('-----')</span><span id="da36" class="jg jh hi jc b fi jm jj l jk jl">x = x.float()<br/>describe(x)<br/>print(x.dtype); print('-----')</span><span id="6c08" class="jg jh hi jc b fi jm jj l jk jl">p = torch.randn(3,3).to(torch.int)<br/>q = torch.randint(0,5,(3,3)).to(torch.float)<br/>print(p); print(q); print(q.dtype)</span></pre><p id="7d56" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">不同数据类型之间的转换。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="a92a" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 08: PyTorch 和 NumPy — from_numpy()</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="9800" class="jg jh hi jc b fi ji jj l jk jl">## 08: PyTorch and NumPy -- from_numpy()<br/>'''<br/>1. Tensor in PyTorch is pretty similar to matrix in Numpy!<br/>2. from_numpy()<br/>'''<br/>import numpy as np<br/>def describe_numpy(x):<br/>  print("Dtype: {}".format(x.dtype))<br/>  print("Shape/size: {}".format(x.shape))<br/>  print("Values: \n{}".format(x))</span><span id="b1ed" class="jg jh hi jc b fi jm jj l jk jl">x = torch.Tensor(np.array([[1,2,3],<br/>                 [4,5,6]]))<br/>describe(x); print('-----')<br/># Type: torch.FloatTensor (torch.float32)</span><span id="2d26" class="jg jh hi jc b fi jm jj l jk jl">a = np.ones(5)<br/>b = torch.from_numpy(a)<br/>np.add(a, 1, out= a)<br/>print(a)<br/>print(b); print('\n')</span><span id="31b7" class="jg jh hi jc b fi jm jj l jk jl">npy = np.random.rand(2,3)<br/>describe(torch.from_numpy(npy)); print('\n')</span><span id="d70c" class="jg jh hi jc b fi jm jj l jk jl">x = torch.tensor([1.0, 2.0, 3.0, 4.0]) <br/>y = torch.tensor(np.array([1.0, 2.0, 3.0, 4.0])) <br/>print(x.dtype); print(y.dtype)</span></pre><p id="1a25" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">PyTorch 中的 Tensor 的作用就像 NumPy 中的 array 一样，我们可以很容易地在这两种矢量化数据类型之间进行切换。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="5c10" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated"># 09: PyTorch 和 NumPy。numpy()</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="c6b8" class="jg jh hi jc b fi ji jj l jk jl">## 09: PyTorch and NumPy -- .numpy()<br/>'''<br/>1. .numpy() &amp; from_numpy()<br/>'''<br/>x_tensor = torch.randn(2,3)<br/>y_numpy = np.random.randn(2,3)</span><span id="8255" class="jg jh hi jc b fi jm jj l jk jl">x_numpy = x_tensor.numpy()<br/>y_tensor = torch.from_numpy(y_numpy)</span><span id="a37e" class="jg jh hi jc b fi jm jj l jk jl">describe(x_tensor); print('\n')<br/>describe_numpy(x_numpy); print('-----')</span><span id="41b6" class="jg jh hi jc b fi jm jj l jk jl">describe_numpy(y_numpy); print('\n')<br/>describe(y_tensor)</span></pre></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="ebf3" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated"># 10:张量大小-。size() &amp;。形状</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="70e6" class="jg jh hi jc b fi ji jj l jk jl">## 10: Tensor size -- .size() &amp; .shape<br/>'''<br/>1. .shape &amp; .size() are pretty much the same. <br/>2. However, .shape must be used on immutable data type, like "tuple."<br/>'''<br/>x = torch.tensor([[1,2,3],<br/>                  [4,5,6]], dtype= torch.int64)</span><span id="0fb0" class="jg jh hi jc b fi jm jj l jk jl">print(x.size()); print(x.shape)<br/>print(x.size() == x.shape); print("-----")</span><span id="8c47" class="jg jh hi jc b fi jm jj l jk jl">print(x.shape[0], x.shape[1])</span></pre><p id="9e85" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">每当我们得到一个张量时，看看张量的形状，不管它是一维的，二维的，三维的，四维的，甚至更高维的。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="ddf3" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 11:索引和切片— 1</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="a8db" class="jg jh hi jc b fi ji jj l jk jl">## 11: Indexing and slicing -- 1<br/>x1 = torch.ones(2,4,3)<br/>print(x1[0]); print('\n')<br/>x2 = torch.ones(4,3)</span><span id="c2f1" class="jg jh hi jc b fi jm jj l jk jl">y = torch.randn(4,3)<br/>print(y); print('\n')<br/>print(y[:,2])</span></pre></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="0782" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 12:索引和切片— 2</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="924d" class="jg jh hi jc b fi ji jj l jk jl">## 12: Indexing &amp; slicing -- 2<br/>a = torch.arange(6)<br/>describe(a); print('-----')</span><span id="90e8" class="jg jh hi jc b fi jm jj l jk jl">x = torch.arange(6).view(2,3)</span><span id="db28" class="jg jh hi jc b fi jm jj l jk jl">describe(x); print('\n')</span><span id="ce12" class="jg jh hi jc b fi jm jj l jk jl">describe(x[:1,:2]); print('\n')</span><span id="a393" class="jg jh hi jc b fi jm jj l jk jl">describe(x[0,1])</span></pre></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="2c25" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">(3) PyTorch 基础知识 02</h1><h2 id="9945" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 01:加法— add()，。add_()，。fill()，。零 _()</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="d473" class="jg jh hi jc b fi ji jj l jk jl">## 01: Addition -- add(), .add_(), .fill(), .zero_()<br/>'''<br/>1. torch.add(input, value, out= None)<br/>'''<br/>x = torch.ones(2,3)<br/>y = torch.randn(2,3)</span><span id="0908" class="jg jh hi jc b fi jm jj l jk jl">z1 = x + y<br/>print(z1); print('-----')<br/>z2 = torch.add(x, x)<br/>z3 = torch.add(x, 2)<br/>torch.add(x, 2, out= x)</span><span id="96a3" class="jg jh hi jc b fi jm jj l jk jl">describe(z1); print('\n')<br/>describe(z2); print('\n')<br/>describe(z3); print('\n')<br/>describe(x); print('-----')</span><span id="41a7" class="jg jh hi jc b fi jm jj l jk jl">a = torch.Tensor([[1,2], [3,4]])<br/># For out-place addition<br/>b = torch.add(a,5)<br/>print(b); print('\n')<br/># For in-place addition<br/>print(a.add_(5)); print('-----')</span><span id="26dd" class="jg jh hi jc b fi jm jj l jk jl"># In-place filling<br/>a.fill_(5)<br/>describe(a); print('-----')</span><span id="54d2" class="jg jh hi jc b fi jm jj l jk jl">a = torch.ones(3,2)<br/>a.zero_()<br/>print(a)</span></pre><p id="0955" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">注意<strong class="jq hj"> <em class="ku">出位</em> </strong>方法和<strong class="jq hj"> <em class="ku">入位</em> </strong>方法差别很大。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="af4e" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 02:乘法— 1</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="d736" class="jg jh hi jc b fi ji jj l jk jl">## 02: Multiplication -- 1<br/>'''<br/>1. element-wise multiplication VS. matrix multiplication<br/>2. Element-wise multiplication methods: * , .mul(), .mul_(). They are all the same.<br/>3. Matrix multiplication methods: @, .matmul(), mm(), .mm(). They are all the same.<br/>4. Transpose function .t()<br/>'''<br/>a = torch.Tensor([[1,2], [3,4]])<br/>b = torch.Tensor([[7,8], [5,6]])</span><span id="4b33" class="jg jh hi jc b fi jm jj l jk jl">c1 = a * b<br/>print(c1)</span><span id="022c" class="jg jh hi jc b fi jm jj l jk jl"># mul(b) for out-place multiplication<br/>c2 = a.mul(b)<br/>print(c2)</span><span id="ee46" class="jg jh hi jc b fi jm jj l jk jl"># mul_(b) for in-place multiplication<br/>a.mul_(b)<br/>print(a); print('-----')</span><span id="41d2" class="jg jh hi jc b fi jm jj l jk jl">a = torch.Tensor([[1,2], [3,4]])<br/>b = torch.Tensor([[7,8], [5,6]])</span><span id="5ef9" class="jg jh hi jc b fi jm jj l jk jl">c3 = a @ b.t()<br/>print(c3)<br/>c4 = a.matmul(b.t())<br/>print(c4)<br/>c5 = torch.mm(a, b.t()); print('\n')<br/>print(c5)<br/>c6 = a.mm(b.t())<br/>print(c6); print('-----')</span><span id="2c50" class="jg jh hi jc b fi jm jj l jk jl">x = torch.Tensor([[1,2], [3,4], [5,6]]) # 3 x 2<br/>y = torch.Tensor([[1,1], [4,3], [6,5]]) # 3 x 2<br/>z = torch.Tensor([[3,1], [2,1]]) # 2 x 1<br/>print(x * y)<br/>print(x * y @ z)</span></pre><p id="7367" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated"><em class="ku">逐元素乘法</em> VS. <em class="ku">矩阵乘法</em></p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="4db2" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 03:乘法— 2</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="51c1" class="jg jh hi jc b fi ji jj l jk jl">## 03: Multiplication -- 2<br/>x1 = torch.arange(6).view(2,3)<br/>describe(x1); print('\n')</span><span id="cd6f" class="jg jh hi jc b fi jm jj l jk jl">x2 = torch.ones(3,2)<br/>x2[:, 1] += 1<br/>describe(x2); print('-----')</span><span id="89e3" class="jg jh hi jc b fi jm jj l jk jl">x1 = x1.float()<br/>y = torch.mm(x1, x2)<br/>describe(y); print('\n')</span><span id="0642" class="jg jh hi jc b fi jm jj l jk jl">x3 = torch.ones(2,3)<br/>x3[1, :] += 1<br/>y = torch.mm(x1, x3.t())<br/>describe(y)</span></pre></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="67e8" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 07:基于维度的张量运算:。view()，shape()，。t()，转置()，。转置()</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="6f4a" class="jg jh hi jc b fi ji jj l jk jl">## 07: Dimension based tensor operations: .view(), reshape(), .t(), transpose(), .transpose()<br/>'''<br/>1. Changing the shape<br/>'''<br/>x = torch.arange(6)<br/>describe(x); print('-----')</span><span id="c2c4" class="jg jh hi jc b fi jm jj l jk jl">y = x.view(2,3)<br/>describe(y); print('\n')<br/>z = x.reshape(2,3)<br/>describe(z); print('-----')</span><span id="6dda" class="jg jh hi jc b fi jm jj l jk jl">describe(y.t()); print('\n')<br/>describe(torch.transpose(y,0,1)); print('-----')</span><span id="2675" class="jg jh hi jc b fi jm jj l jk jl">a = torch.ones(3, 2)<br/>a_t = torch.transpose(a, 0, 1)<br/>print(a.shape,a_t.shape)</span><span id="c86d" class="jg jh hi jc b fi jm jj l jk jl">a = torch.ones(3, 2)<br/>a_t = a.transpose(0, 1)<br/>print(a.shape,a_t.shape)</span></pre></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="f150" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 08:基于维度的张量运算:squeeze() &amp;。unsqueeze()</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="8566" class="jg jh hi jc b fi ji jj l jk jl">## 08: Dimension based tensor operations: squeeze() &amp; .unsqueeze()<br/>'''<br/>1. torch.squeeze(input, dim= None, out= None) <br/>2. torch.squeeze() removes 1-D tensors.<br/>3. torch.unsqueeze() adds virtual 1-D tensors.<br/>'''<br/>x = torch.zeros(2,1,2,1,2)<br/>print(x.size())</span><span id="d224" class="jg jh hi jc b fi jm jj l jk jl">y = torch.squeeze(x)<br/>print(y.size()); print('\n')</span><span id="761a" class="jg jh hi jc b fi jm jj l jk jl">y = torch.squeeze(x,0)<br/>print(y.size())<br/>y = torch.squeeze(x,1)<br/>print(y.size())<br/>y = torch.squeeze(x,2)<br/>print(y.size())<br/>z1 = torch.squeeze(x,3)<br/>print(z1.size()); print('\n')</span><span id="dc9f" class="jg jh hi jc b fi jm jj l jk jl">z2 = z1.unsqueeze(0)<br/>print(z2.size())<br/>z1.unsqueeze_(0)<br/>print(z1.size())</span></pre></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="1996" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 09:基于维度的张量运算:cat()，stack()</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="d3a0" class="jg jh hi jc b fi ji jj l jk jl">## 09: Dimension based tensor operations: cat(), stack()<br/>'''<br/>1. Joining<br/>'''<br/>x = torch.arange(6).view(2,3)<br/>describe(x); print('-----')</span><span id="9f6c" class="jg jh hi jc b fi jm jj l jk jl">describe(torch.cat([x,x], dim= 0)); print('-----')</span><span id="86f8" class="jg jh hi jc b fi jm jj l jk jl">describe(torch.cat([x,x], dim= 1)); print('-----')</span><span id="991c" class="jg jh hi jc b fi jm jj l jk jl">describe(torch.stack([x,x]))</span></pre></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="e3ca" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 10:基于维度的张量运算:split()，chunk()</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="867b" class="jg jh hi jc b fi ji jj l jk jl">## 10: Dimension based tensor operations: split(), chunk()<br/>'''<br/>1. Splitting<br/>'''<br/>y = torch.rand(3,2)<br/>print(y); print('-----')<br/>splitted_1 = y.split(split_size= 1, dim= 0)<br/>splitted_2 = y.chunk(chunks= 3, dim= 0)<br/>print(splitted_1)<br/>print(splitted_2)</span></pre></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="d7ab" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 11:存储索引— 1</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="e04b" class="jg jh hi jc b fi ji jj l jk jl">## 11: Indexing into storage -- 1<br/>'''<br/>1. .storage() &amp; id(x.storage())<br/>'''<br/>points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])<br/>points_storage = points.storage()</span><span id="7f7c" class="jg jh hi jc b fi jm jj l jk jl">print(points_storage)<br/>print(points_storage[0])<br/>print(points.storage()[0]); print('-----')</span><span id="02f7" class="jg jh hi jc b fi jm jj l jk jl">points_storage[0] = 2.0<br/>print(points); print('-----')</span><span id="99de" class="jg jh hi jc b fi jm jj l jk jl">points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])<br/>print(points)<br/>points_t = points.t()<br/>print(points_t)</span><span id="bb8e" class="jg jh hi jc b fi jm jj l jk jl">print(id(points.storage()) == id(points_t.storage())) # storage ID<br/>print(points.data_ptr() == points_t.data_ptr()) # data pointer</span></pre><p id="cca4" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们看<em class="ku">。t() </em>只是在给定的内存空间上检索数据的不同方式，而不是创建新的内存空间。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="a361" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">(4) PyTorch 基础 03</h1><h2 id="3517" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated"># # 01:GPU 上的张量--。cuda() &amp;。cpu()</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="f34e" class="jg jh hi jc b fi ji jj l jk jl">## 01: Tensor on GPU -- .cuda() &amp; .cpu()<br/><br/>print(torch.cuda.is_available()); print('\n')</span><span id="4ef5" class="jg jh hi jc b fi jm jj l jk jl">describe(torch.randn(2,3, device= "cuda:0")); print('\n')</span><span id="6275" class="jg jh hi jc b fi jm jj l jk jl">x = torch.ones(4,3); y = torch.randn(4,3)<br/>x = x.cuda(); y = y.cuda()<br/>print(x+y); print('\n')</span><span id="3813" class="jg jh hi jc b fi jm jj l jk jl">x = x.cpu()<br/>print(x); print('-----')</span><span id="f39b" class="jg jh hi jc b fi jm jj l jk jl">print(torch.cuda.is_available()); print('\n')</span><span id="2a67" class="jg jh hi jc b fi jm jj l jk jl"># preferred method: device agnostic tensor instantiation<br/>device = torch.device("cuda" if torch.cuda.is_available() else "cpu")<br/>print(device); print('\n')</span><span id="a8e1" class="jg jh hi jc b fi jm jj l jk jl">x = torch.rand(3,3).to(device)<br/>describe(x)</span></pre><p id="552c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">请记住，从 GPU  来回移动数据是很昂贵的。因此，典型的过程包括在 GPU 上进行许多可并行计算，然后<strong class="jq hj"> <em class="ku">将最终结果传回</em></strong>CPU。这将允许您充分利用 GPU。[3]</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="1312" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 05:通过矩阵乘法精心设计 CPU 和 GPU 之间的速度差异</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="a87c" class="jg jh hi jc b fi ji jj l jk jl">## 05: Elaborate speed difference between CPU &amp; GPU by matrix multiplication</span><span id="ba0b" class="jg jh hi jc b fi jm jj l jk jl">import timeit</span><span id="3ce4" class="jg jh hi jc b fi jm jj l jk jl"># The tensors are on CPU<br/>a = torch.rand(1000, 1000)<br/>b = torch.rand(1000, 1000)<br/>%timeit a.matmul(b)<br/># 10 loops, best of 3: 28.1 ms per loop<br/>print('\n')</span><span id="ddd9" class="jg jh hi jc b fi jm jj l jk jl"># Move the tensors to GPU<br/>a = a.cuda()<br/>b = b.cuda()<br/>%timeit a.matmul(b)<br/># 1000 loops, best of 3: 563 µs per loop</span></pre><figure class="ix iy iz ja fd my er es paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="er es mx"><img src="../Images/f749a5e1f11a6a5874cabc4685fb0da9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w3aAFe4o_2ymbGA4oZZZZA.png"/></div></div><figcaption class="ne nf et er es ng nh bd b be z dx translated">图 GPU 惊人的计算能力。</figcaption></figure><p id="149c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">563 秒/ 28.1 毫秒= 2.0%</p><p id="fcbc" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">GPU 对 NN/DL 的贡献有多显著！</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h2 id="8ed0" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">## 08:计算图表— 3</h2><pre class="ix iy iz ja fd jb jc jd je aw jf bi"><span id="2456" class="jg jh hi jc b fi ji jj l jk jl">## 08: Computational graph -- 3<br/>from torch.autograd import Variable<br/>x = Variable(torch.ones(2,2), requires_grad= True)<br/>print(x); print('\n')</span><span id="7759" class="jg jh hi jc b fi jm jj l jk jl">y = x + 2<br/>print(y)<br/>print(y.data)<br/>print(y.grad_fn); print('\n')</span><span id="b669" class="jg jh hi jc b fi jm jj l jk jl">z = y * y<br/>print(z.data)<br/>print(z.grad_fn); print('\n')</span><span id="3ba1" class="jg jh hi jc b fi jm jj l jk jl">t = torch.mean(z)<br/>print(t); print('\n')</span><span id="b80c" class="jg jh hi jc b fi jm jj l jk jl">t.backward()<br/>print(z.grad)<br/>print(y.grad)<br/>print(x.grad)</span></pre><p id="d287" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">分解计算图有助于深入了解 PyTorch 的工作原理。PyTorch 使用<em class="ku">动态计算图</em>，而 TensorFlow 使用<em class="ku">静态计算图</em>。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="7f1c" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">(5)总结</h1><p id="4202" class="pw-post-body-paragraph jo jp hi jq b jr ni ij jt ju nj im jw jx nk jz ka kb nl kd ke kf nm kh ki kj hb bi translated">现在，我们已经讨论了开始用张量表示一切所需的大部分内容。</p><p id="7891" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">有了 PyTorch 的基本用例，下一步是探索构建 NN/DL 模型的模块，如<em class="ku"> torch.nn </em>，然后在真实世界的数据上实现这些代码。</p></div><div class="ab cl kk kl gp km" role="separator"><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp kq"/><span class="kn bw bk ko kp"/></div><div class="hb hc hd he hf"><h1 id="3933" class="ls jh hi bd lt lu lv lw lx ly lz ma mb io mc ip md ir me is mf iu mg iv mh mi bi translated">(6)参考文献</h1><p id="5867" class="pw-post-body-paragraph jo jp hi jq b jr ni ij jt ju nj im jw jx nk jz ka kb nl kd ke kf nm kh ki kj hb bi translated">[1]史蒂文斯，e .，安提卡，l .和托马斯，V. (2020 年)。用 PyTorch 进行深度学习。纽约州纽约市:曼宁出版公司。</p><p id="e746" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[2]托马斯和帕西(2019 年)。PyTorch 深度学习动手。英国伯明翰:Packt 出版公司。</p><p id="d12a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[3] Rao，d .和 McMahan，B. (2019 年)。用 PyTroch 进行自然语言处理。加利福尼亚州:奥赖利媒体。</p><p id="cdcd" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">[4] Subramanian，V. (2018)。用 PyTorch 进行深度学习。英国伯明翰:Packt 出版公司。</p><h2 id="9fe5" class="jg jh hi bd lt mk ml mm lx mn mo mp mb jx mq mr md kb ms mt mf kf mu mv mh mw bi translated">(中文)</h2><p id="51c6" class="pw-post-body-paragraph jo jp hi jq b jr ni ij jt ju nj im jw jx nk jz ka kb nl kd ke kf nm kh ki kj hb bi">[5] 張校捷 (2020)。深入淺出 PyTorch：從模型到源碼。北京，中國：電子工業。</p><p id="5f98" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi">[6] 集智俱樂部 (2019)。深度學習原理與 PyTorch 實戰。北京，中國：人民郵電。</p><p id="e92a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi">[7] 邢夢來等人 (2018)。深度学习框架 PyTorch 快速开发与实战。北京，中國：電子工業。</p></div></div>    
</body>
</html>