<html>
<head>
<title>Machine Learning: Decision Tree Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习:决策树分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-decision-tree-classification-1a248e95bc03?source=collection_archive---------31-----------------------#2020-06-13">https://medium.com/analytics-vidhya/machine-learning-decision-tree-classification-1a248e95bc03?source=collection_archive---------31-----------------------#2020-06-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7351c7b525d334d3c4bd3509ad52dfd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xmZis4W43nLXwDJLwOCRcA.png"/></div></div></figure><p id="e2dc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们大多数人都觉得决策树很难，但它是机器学习中最强大的技术之一。这很容易实现，属于监督学习技术。顾名思义，它是一棵树，但没有真正的根和绿叶。它是通过经历各种条件来构建树，形成称为头节点的根和称为末端叶的叶，从而将数据分解成小段。</p><blockquote class="jp jq jr"><p id="b4bd" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">决策树:</strong></p><p id="6702" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">决策树以树结构的形式建立分类或回归模型。这是监督学习技术。他们有两种类型的决策树</p><p id="6a48" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">分类树。</p><p id="5649" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">回归树。</p><p id="45ce" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">决策树是使用递归划分来构建。这种方法也称为分而治之。它将数据集分解成越来越小的子集，同时增量地开发相关的决策树。</p></blockquote><p id="e36c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">决策树是类似流程图的结构:</em></p><figure class="jw jx jy jz fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jv"><img src="../Images/9e26e108c7c3253c72eb22e5ea997e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FRSjUmdSxyuJfPJxHXPdbQ.png"/></div></div></figure><blockquote class="jp jq jr"><p id="64e8" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">关于决策树分类结构的描述:</p><p id="c467" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">I)它是一个树状结构的流程图。</p><p id="7d22" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">ii)内部节点表示特征。</p><p id="757a" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">iii)分支代表决策规则。</p><p id="480c" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">iv)最顶层的节点称为根节点。</p><p id="c64f" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">v)叶节点以二进制(0，1)表示最终决策或结果，描述0表示不发生事件，1表示发生事件。</p><p id="9215" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">这种最终结构有助于决策。</p></blockquote><p id="e567" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是，我们首先想到的是树的形成是如何发生的，以及它实际遵循的程序是什么？</p><p id="5df2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">那么，我们来谈谈决策树算法——</em></p><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es ka"><img src="../Images/bb271c1d3bd462bd37b452f46bef0b3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*cd5Png1_SedmQxX7OhEIPw.png"/></div><figcaption class="kb kc et er es kd ke bd b be z dx translated"><strong class="bd kf">决策树模型准备。</strong></figcaption></figure><p id="3d6a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">现在，让我们详细讨论每个特征选择步骤:- </em></p><blockquote class="jp jq jr"><p id="0dc6" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj"> <em class="hi">信息增益:- </em> </strong></p><p id="77d3" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><em class="hi">信息增益是熵的减少。信息增益根据给定的属性值计算数据集分割前的熵和分割后的平均熵之差。</em></p><p id="d839" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><em class="hi">数学实现- </em></p></blockquote><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es kg"><img src="../Images/54c22ce705fe4ac3995d2c8b2eac00d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*bB7C9Q9G-ZOPIO6Ed1XVqw.png"/></div></figure><blockquote class="jp jq jr"><p id="7a44" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><em class="hi">其中，Pi是D中任意元组属于类Ci的概率。</em></p></blockquote><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es kh"><img src="../Images/ad549556ab23f59fdc78c703ceea1889.png" data-original-src="https://miro.medium.com/v2/resize:fit:910/format:webp/1*aPG_DDys8gJIfmoXzOpKxA.png"/></div></figure><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es ki"><img src="../Images/f9ab0383710fd74f05fe2de9aa9ec64e.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*LOKmFZVlsIs6G4iB-6ARMw.png"/></div></figure><blockquote class="jp jq jr"><p id="cf13" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><em class="hi">在节点N()选择信息增益最高的属性A Gain(A)作为分裂属性。</em></p><p id="ab17" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj"> <em class="hi">增益比- </em> </strong></p><p id="45d5" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><em class="hi">对于多结果的属性，信息增益是有偏的。这意味着它更喜欢具有大量不同值的属性。</em></p></blockquote><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es kj"><img src="../Images/a0163fc9684a3043ca05ab2071dee525.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*uXnI3pgfiIrzl8OvfGh8IA.png"/></div></figure><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es kk"><img src="../Images/4c1cb64b616fbd92b8feb4a3f95e08ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*_UqEo2mF5etFRuS6JG7mTQ.png"/></div></figure><blockquote class="jp jq jr"><p id="c839" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><em class="hi">选择增益比最高的属性作为分割属性。</em></p><p id="ae3f" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj"> <em class="hi">基尼指数- </em> </strong></p><p id="faa9" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><em class="hi">基尼系数考虑了每个属性的二元分割。你可以计算每个分区的杂质的加权和。如果属性A上的二元分割将数据D划分为D1和D2，则D的基尼指数为:</em></p></blockquote><div class="jw jx jy jz fd ab cb"><figure class="kl ij km kn ko kp kq paragraph-image"><img src="../Images/fb606c7fbb983a17453ff188d221bafa.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*IKfi5LhkxLX2rZxn3ap6cw.png"/></figure><figure class="kl ij kr kn ko kp kq paragraph-image"><img src="../Images/b9c9165fe159fe387f8dbab610eb8220.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*qS-wKJxaLSFNm28xbkmtYQ.png"/></figure></div><blockquote class="jp jq jr"><p id="1a64" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><em class="hi">其中，pi是D中一个元组属于类Ci的概率。</em></p></blockquote><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/8c147d5d6bac9e6024b78eff8fa4a893.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*wt57Jgw7iwAqGH910nht6w.png"/></div></figure><blockquote class="jp jq jr"><p id="8176" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><em class="hi">选择基尼系数最小的属性作为分裂属性。</em></p></blockquote><p id="0a6a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> <em class="jo">我们来看一个例子——</em></strong></p><figure class="jw jx jy jz fd ij er es paragraph-image"><div class="er es kt"><img src="../Images/8af732abc5b8467a387a4f393d74b239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*2-mNxn0EDZwgssEMNM92gQ.png"/></div></figure><blockquote class="jp jq jr"><p id="22b3" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated"><strong class="is hj">结论- </strong></p><p id="2f8f" class="iq ir jo is b it iu iv iw ix iy iz ja js jc jd je jt jg jh ji ju jk jl jm jn hb bi translated">决策树很容易解释和可视化。它可以很容易地捕捉非线性模式。但是，对噪声数据敏感。它会过度拟合噪声数据。决策树偏向于不平衡数据集，因此建议在创建决策树之前平衡数据集。数据中的微小变化(或差异)会导致不同的决策树。这可以通过打包和提升算法来减少。</p></blockquote><blockquote class="ku"><p id="b0d6" class="kv kw hi bd kx ky kz la lb lc ld jn dx translated">感谢您抽出宝贵的时间来阅读本博客！！！</p></blockquote></div></div>    
</body>
</html>