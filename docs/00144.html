<html>
<head>
<title>Learn how to Build Neural Networks from Scratch in Python for Digit Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解如何用Python从头开始构建用于数字识别的神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/neural-networks-for-digits-recognition-e11d9dff00d5?source=collection_archive---------0-----------------------#2018-10-12">https://medium.com/analytics-vidhya/neural-networks-for-digits-recognition-e11d9dff00d5?source=collection_archive---------0-----------------------#2018-10-12</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/94ebbf40d69a4802596d583eed4026ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FGGge_GilZ_KJYaoryaxkA.png"/></div></div></figure><p id="1492" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">吴恩达的机器学习课程仍然是成千上万有抱负的数据科学家的敲门砖和门户。但是如何用现代语言来实现他的教导的问题经常成为人们的一个障碍。</p><p id="4ef2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这就是为什么我继续写下我对每周课程的想法，以及如何用Python实现他的所有教导。</p><p id="6fae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我的上一篇文章中，我们看到了像逻辑回归这样的简单算法是如何被用来识别手写数字的。我们得到了<code class="du jp jq jr js b">95.08%</code>的精度！但是，请记住，逻辑回归是一个线性分类器，因此不能形成复杂的边界。</p><h1 id="00cf" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">我们将在这篇文章中讨论的内容</h1><p id="ab8f" class="pw-post-body-paragraph iq ir hi is b it kr iv iw ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn hb bi translated">因此，在这篇博文中，我们将了解神经网络如何用于相同的任务。由于神经网络可以适应更复杂的非线性边界，我们应该看到我们的分类器精度也有所提高。</p><p id="40ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这部分帖子基于吴恩达的机器学习课程第5周的内容。你可以在这里访问编程练习和数据集<a class="ae jo" href="https://s3.amazonaws.com/spark-public/ml/exercises/on-demand/machine-learning-ex4.zip" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="3ca1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里，我不打算解释反向传播之类的概念，因为这将偏离为你提供课程的Pythonic式翻译的目标。每个概念本身都可以很快变成一篇博文，老实说，我认为吴恩达在解释这些概念方面做得非常好。</p><blockquote class="kw kx ky"><p id="8377" class="iq ir kz is b it iu iv iw ix iy iz ja la jc jd je lb jg jh ji lc jk jl jm jn hb bi translated">在开始编程练习之前，我们强烈建议观看<a class="ae jo" href="https://www.coursera.org/learn/machine-learning/home/week/5" rel="noopener ugc nofollow" target="_blank">视频讲座</a>，并完成相关主题的复习题。</p></blockquote><h1 id="a8c9" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">1.前馈传播</h1><p id="0442" class="pw-post-body-paragraph iq ir hi is b it kr iv iw ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn hb bi translated">我们首先使用已经给定的权重实现神经网络的前馈传播。然后，我们将实现反向传播算法来为自己学习参数。这里我们互换使用术语权重和参数。</p><h1 id="3625" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated"><strong class="ak"> 1.1可视化数据:</strong></h1><p id="2ce6" class="pw-post-body-paragraph iq ir hi is b it kr iv iw ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn hb bi translated">每个训练示例都是手指的20像素乘20像素的灰度图像。每个像素由一个浮点数表示，表示该位置的灰度强度。20×20像素网格被“展开”成400维向量。这些训练示例中的每一个都成为我们的数据矩阵<code class="du jp jq jr js b">X</code>中的一行。这给了我们一个5000乘400的矩阵<code class="du jp jq jr js b">X</code>，其中每一行都是一个手写数字图像的训练示例。训练集的第二部分是一个5000维的向量<code class="du jp jq jr js b">y</code>，它包含训练集的标签。</p><pre class="ld le lf lg fd lh js li lj aw lk bi"><span id="e765" class="ll ju hi js b fi lm ln l lo lp">from scipy.io import loadmat<br/>import numpy as np<br/>import scipy.optimize as opt<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span><span id="616e" class="ll ju hi js b fi lq ln l lo lp"># reading the data<br/>data = loadmat('ex4data1.mat')<br/>X = data['X']<br/>y = data['y']</span><span id="bb06" class="ll ju hi js b fi lq ln l lo lp"># visualizing the data<br/>_, axarr = plt.subplots(10,10,figsize=(10,10))<br/>for i in range(10):<br/>    for j in range(10):<br/>       axarr[i,j].imshow(X[np.random.randint(X.shape[0])].\<br/>reshape((20,20), order = 'F'))          <br/>       axarr[i,j].axis('off')</span></pre><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es lr"><img src="../Images/2bb0b31d1045f9f258912f9718057d0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*WJdmUan0j7Hb1dbvaN6ixA.png"/></div></figure><h1 id="5397" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">1.2模型表示</h1><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es ls"><img src="../Images/c80e44e8bc1c25831efa89ae5a95d0a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*l78dvvJFf0cOJnXTJglR7A.png"/></div></figure><p id="8dee" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们的神经网络有三层——输入层、隐藏层和输出层。请记住，输入将是“展开”的20 x 20灰度图像，以形成400个输入特征，我们将把这些特征输入到神经网络中。所以我们的输入层有400个神经元。此外，隐藏层有25个神经元，输出层有10个神经元，对应于我们的模型预测的10个数字(或类别)。上图中的+1代表偏置项。</p><p id="f344" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">已经向我们提供了一组已经训练好的网络参数。这些存储在ex4weights.mat中，将被加载到<code class="du jp jq jr js b">theta1</code>和<code class="du jp jq jr js b">theta2</code>中，然后展开成一个向量<code class="du jp jq jr js b">nn_params</code>。参数的尺寸大小适合于在第二层具有25个单元和10个输出单元(对应于10个数字类别)的神经网络。</p><pre class="ld le lf lg fd lh js li lj aw lk bi"><span id="8a96" class="ll ju hi js b fi lm ln l lo lp">weights = loadmat('ex4weights.mat')<br/>theta1 = weights['Theta1']    #Theta1 has size 25 x 401<br/>theta2 = weights['Theta2']    #Theta2 has size 10 x 26</span><span id="4d42" class="ll ju hi js b fi lq ln l lo lp">nn_params = np.hstack((theta1.ravel(order='F'), theta2.ravel(order='F')))    #unroll parameters</span><span id="7a10" class="ll ju hi js b fi lq ln l lo lp"># neural network hyperparameters<br/>input_layer_size = 400<br/>hidden_layer_size = 25<br/>num_labels = 10<br/>lmbda = 1</span></pre><h1 id="1629" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">1.3前馈和成本函数</h1><p id="8622" class="pw-post-body-paragraph iq ir hi is b it kr iv iw ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn hb bi translated">首先，我们将实现成本函数，然后是神经网络的梯度(为此我们使用反向传播算法)。回想一下，具有正则化的神经网络的成本函数是</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/7ee4c06a41c60b139e0896dcafd3ae97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JWdzW4eqS1SXF-vYuGOOoA.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">正则化神经网络的代价函数</figcaption></figure><p id="3187" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中<code class="du jp jq jr js b">h(x(i))</code>如图2所示计算，K = 10是可能标签的总数。注意<code class="du jp jq jr js b">h(x(i)) = a(3)</code>是输出单元的激活。此外，鉴于原始标签(在变量y中)是1、2、…、10，为了训练神经网络，我们需要将标签重新编码为只包含值0或1的向量，这样</p><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/69387b5502d8430996b0dde209e3d9cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*4Yt_mE02LQMStgv4Ms-eAA.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">一键编码</figcaption></figure><p id="6540" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这个过程被称为一键编码。我们这样做的方法是使用“熊猫库”中的<code class="du jp jq jr js b">get_dummies </code>函数。</p><p id="d753" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">乙状结肠功能</strong></p><pre class="ld le lf lg fd lh js li lj aw lk bi"><span id="294d" class="ll ju hi js b fi lm ln l lo lp">def sigmoid(z):<br/>    return 1/(1+np.exp(-z))</span></pre><h2 id="cbf6" class="ll ju hi bd jv lz ma mb jz mc md me kd jb mf mg kh jf mh mi kl jj mj mk kp ml bi translated">价值函数</h2><figure class="ld le lf lg fd ij"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="6b07" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用给定的权重调用<code class="du jp jq jr js b">nnCostFunc </code>给我们成本。</p><pre class="ld le lf lg fd lh js li lj aw lk bi"><span id="923e" class="ll ju hi js b fi lm ln l lo lp">nnCostFunc(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda)</span></pre><p id="73d6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你应该看到费用大概在<code class="du jp jq jr js b">0.383770</code>左右。</p><h1 id="bb1a" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">2反向传播</h1><p id="931f" class="pw-post-body-paragraph iq ir hi is b it kr iv iw ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn hb bi translated">在这部分练习中，您将实现反向传播算法来计算神经网络的梯度。一旦你计算出梯度，你将能够通过使用先进的优化器如<code class="du jp jq jr js b">fmincg</code>最小化成本函数来训练神经网络。</p><h2 id="7b1f" class="ll ju hi bd jv lz ma mb jz mc md me kd jb mf mg kh jf mh mi kl jj mj mk kp ml bi translated">2.1 Sigmoid梯度</h2><p id="23b4" class="pw-post-body-paragraph iq ir hi is b it kr iv iw ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn hb bi translated">我们将首先实现sigmoid梯度函数。sigmoid函数的梯度可计算如下</p><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es mo"><img src="../Images/a96ff5a0132daed95415ba18693930bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*ExqiMON0DyZo-YOqSGfSBQ.png"/></div></figure><pre class="ld le lf lg fd lh js li lj aw lk bi"><span id="1cc0" class="ll ju hi js b fi lm ln l lo lp">def sigmoidGrad(z):<br/>    return np.multiply(sigmoid(z), 1-sigmoid(z))</span></pre><h2 id="324a" class="ll ju hi bd jv lz ma mb jz mc md me kd jb mf mg kh jf mh mi kl jj mj mk kp ml bi translated">2.2随机初始化</h2><p id="4f47" class="pw-post-body-paragraph iq ir hi is b it kr iv iw ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn hb bi translated">训练神经网络时，随机初始化<a class="ae jo" href="https://stackoverflow.com/a/20029817/" rel="noopener ugc nofollow" target="_blank">对称破缺</a>的参数很重要。这里，我们随机初始化对应于隐藏层和输出层的名为<code class="du jp jq jr js b">initial_theta1</code>和<code class="du jp jq jr js b">initial_theta2</code>的参数，并像前面一样展开成一个向量。</p><figure class="ld le lf lg fd ij"><div class="bz dy l di"><div class="mm mn l"/></div></figure><h2 id="a2c7" class="ll ju hi bd jv lz ma mb jz mc md me kd jb mf mg kh jf mh mi kl jj mj mk kp ml bi translated">2.3反向传播</h2><p id="00ee" class="pw-post-body-paragraph iq ir hi is b it kr iv iw ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn hb bi translated">一旦你掌握了诀窍，反向传播算法就不是那么复杂了。<br/>我强烈建议你在backprop 上多次观看Andrew的<a class="ae jo" href="https://www.coursera.org/learn/machine-learning/lecture/1z9WW/backpropagation-algorithm" rel="noopener ugc nofollow" target="_blank">视频。</a></p><p id="fe17" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">总之，我们通过循环每个训练示例来做以下事情。计算正向传播以获得输出激活<code class="du jp jq jr js b">a3</code>。<br/> 2。计算误差项<code class="du jp jq jr js b">d3 </code>，这是通过从我们的计算输出<code class="du jp jq jr js b">a3</code>中减去实际输出而获得的。<br/> 3。对于隐藏层，误差项<code class="du jp jq jr js b">d2</code>可计算如下:</p><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es mp"><img src="../Images/b83683283d3f803c68ee179470708691.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*ZKiU_LPTPTbQ4dD_rPDw0Q.png"/></div></figure><p id="22ad" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4.累积<code class="du jp jq jr js b">delta1</code>和<code class="du jp jq jr js b">delta2</code>中的梯度。<br/> 5。通过用<code class="du jp jq jr js b">m</code>除以(步骤4的)累积梯度，获得神经网络的梯度。<br/> 6。将正则项添加到梯度中。</p><figure class="ld le lf lg fd ij"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="bcf8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">顺便说一下，如果您可以使用高度矢量化的实现，可以消除上面代码中的for循环。但是对于那些不熟悉反向投影的人来说，使用for-loop可以获得更好的理解。使用初始参数运行上述函数会给出<code class="du jp jq jr js b">nn_backprop_Params</code>，我们将在执行梯度检查时使用它。</p><pre class="ld le lf lg fd lh js li lj aw lk bi"><span id="8574" class="ll ju hi js b fi lm ln l lo lp">nn_backprop_Params = nnGrad(nn_initial_params, input_layer_size, hidden_layer_size, num_labels, X, y, lmbda)</span></pre><h2 id="6710" class="ll ju hi bd jv lz ma mb jz mc md me kd jb mf mg kh jf mh mi kl jj mj mk kp ml bi translated">2.4梯度检查</h2><p id="46b5" class="pw-post-body-paragraph iq ir hi is b it kr iv iw ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn hb bi translated">为什么我们需要梯度检查？以确保我们的反向传播算法没有错误，并按预期工作。我们可以用以下公式近似计算成本函数的导数:</p><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es mq"><img src="../Images/5316f844b51eced7cd047610fe6b29c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*zXZPYRGD-3CU8Q219OJnXQ.png"/></div></figure><p id="ab66" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">使用反向投影和数值近似计算的梯度应该至少符合4个有效数字，以确保我们的反向投影实现没有错误。</p><figure class="ld le lf lg fd ij"><div class="bz dy l di"><div class="mm mn l"/></div></figure><pre class="ld le lf lg fd lh js li lj aw lk bi"><span id="61fa" class="ll ju hi js b fi lm ln l lo lp">checkGradient(nn_initial_params,nn_backprop_Params,input_layer_size, hidden_layer_size, num_labels,X,y,lmbda)</span></pre><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/aff4ce6de134eeccebc5c11ecd05f985.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xqYyA6KnX8smSDw-g31XuA.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">梯度检查的输出</figcaption></figure><h2 id="981b" class="ll ju hi bd jv lz ma mb jz mc md me kd jb mf mg kh jf mh mi kl jj mj mk kp ml bi translated">2.5使用<code class="du jp jq jr js b">fmincg</code>学习参数</h2><p id="71c3" class="pw-post-body-paragraph iq ir hi is b it kr iv iw ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn hb bi translated">成功实现神经网络成本函数和梯度计算后，下一步是使用<code class="du jp jq jr js b">fmincg </code>为神经网络学习一组好的参数。<code class="du jp jq jr js b">theta_opt</code>包含我们刚刚学习的展开参数，我们滚动这些参数以获得<code class="du jp jq jr js b">theta1_opt</code>和<code class="du jp jq jr js b">theta2_opt</code>。</p><figure class="ld le lf lg fd ij"><div class="bz dy l di"><div class="mm mn l"/></div></figure><h2 id="4d08" class="ll ju hi bd jv lz ma mb jz mc md me kd jb mf mg kh jf mh mi kl jj mj mk kp ml bi translated">2.6使用学习参数进行预测</h2><p id="60a5" class="pw-post-body-paragraph iq ir hi is b it kr iv iw ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn hb bi translated">是时候通过计算模型的准确性来看看我们新学习的参数表现如何了。回想一下，当我们使用像逻辑回归这样的线性分类器时，我们得到了95.08%的准确率。神经网络应该会给我们一个更好的精度。</p><figure class="ld le lf lg fd ij"><div class="bz dy l di"><div class="mm mn l"/></div></figure><pre class="ld le lf lg fd lh js li lj aw lk bi"><span id="99a1" class="ll ju hi js b fi lm ln l lo lp">pred = predict(theta1_opt, theta2_opt, X, y)<br/>np.mean(pred == y.flatten()) * 100</span></pre><p id="a522" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这应该给出一个<code class="du jp jq jr js b">96.5%</code>的值(由于随机初始化，这个值可能会有大约1%的变化)。值得注意的是，通过调整超参数，我们仍然可以获得更好的精度。</p><h1 id="934d" class="jt ju hi bd jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq bi translated">结束注释</h1><p id="c17e" class="pw-post-body-paragraph iq ir hi is b it kr iv iw ix ks iz ja jb kt jd je jf ku jh ji jj kv jl jm jn hb bi translated">我们刚刚看到了神经网络如何用于执行数字识别等复杂任务，并在此过程中了解了反向传播算法。</p><p id="ad5b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">谢谢你能走到这一步。如果你喜欢我的作品，给我一个(或几个)掌声。</p></div></div>    
</body>
</html>