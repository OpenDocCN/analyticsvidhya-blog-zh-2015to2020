<html>
<head>
<title>COVID-19 X-Ray Image Classification with mxnet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于mxnet的新冠肺炎X射线图像分类</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/covid-19-xray-image-classification-with-mxnet-f09c752a8e67?source=collection_archive---------16-----------------------#2020-03-22">https://medium.com/analytics-vidhya/covid-19-xray-image-classification-with-mxnet-f09c752a8e67?source=collection_archive---------16-----------------------#2020-03-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="4968" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">3个月内，全球几乎所有国家都确诊了冠状病毒</strong></h1><p id="3f84" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">3月11日，世界卫生组织终于宣布，新冠肺炎疾病的传播已经达到了疫情的水平。自那时以来，局势进一步恶化，导致许多国家数千人死亡，特别是在意大利。</p><p id="f8b2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">新冠肺炎病毒与SARS病毒非常相似，SARS是一种在21世纪初出现并迅速消失的疾病，导致大约8000人受到感染，800人死亡。这两种病毒都属于冠状病毒家族，这是医学研究人员非常熟悉的:其中一些病毒非常常见，通常不会给患者带来大问题。</p><p id="5d0f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">新冠肺炎传播非常快，医生需要更多的检测工具。测试程序是基于聚合酶链式反应，需要一些时间，因此，一个便宜、可靠和或多或少自动化的诊断解决方案的问题摆在了桌面上。此外，死亡的进一步传播取决于我们以低水平的假阳性检测它的能力，更重要的是，尽可能低水平的假阴性(未确诊的病人将继续感染其他人，因为将不采取预防措施)。</p><p id="d676" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">中国人工智能/人工智能巨头已经开始实施人工智能系统，帮助医生诊断新冠肺炎。在下文中，我们将介绍一个基于患者胸部x射线扫描的新冠肺炎检测的简单研究。然而，由于这个问题是非常新的，并且没有x射线图像数据的官方来源，从人工智能的角度来看，我们使用的数据集的质量不是很好，因此我们开发的分类器的准确性相当低。如果没有对医疗数据使用的严格监管，它甚至无法接近与医院合作的中国领先技术公司的准确性。</p><p id="3b96" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们将用于训练和测试的数据来自[1]。开发我们的分类器的框架是Apache mxnet和Gluon。我们将看到不同的数据扩充技术、不同的损失和不同的超参数如何影响分类的质量。</p><h1 id="7d54" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">资料组</h1><p id="1ac0" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们使用的数据集非常不平衡，包含x光图像，之前标记为</p><ul class=""><li id="c82a" class="kg kh hi jf b jg kb jk kc jo ki js kj jw kk ka kl km kn ko bi translated">新冠肺炎:124张图片</li><li id="2464" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">SARS: 11张图片</li><li id="389f" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">链球菌:6张图片</li><li id="8c13" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">ARDS: 4幅图像</li><li id="ee5c" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">肺囊虫:2张图片</li><li id="1586" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">无发现:2张图片</li></ul><p id="f5ae" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这些扫描中的一些显然取自与大多数图像不同的分布。在许多标签上，我们可以看到外部标签。因此，我们从数据集中删除了一些图像。此外，出于我们的目的，我们将把这个数据集聚集到两个类中。</p><h1 id="0878" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">数据预处理和扩充</h1><p id="1008" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们使用了以下数据预处理和数据扩充技术:</p><ul class=""><li id="c53b" class="kg kh hi jf b jg kb jk kc jo ki js kj jw kk ka kl km kn ko bi translated">图像向左/向右翻转</li><li id="a8cb" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">中间作物</li><li id="c7a0" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">完全没有标准化:我们照原样对待图像</li><li id="02a6" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">ImageNet标准化:使用ImageNet数据集的平均值和标准偏差</li><li id="838b" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">对该数据集的平均值进行归一化:相同的统计数据，但在给定的数据集上进行计算</li></ul><p id="57ae" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">提前说一句，没有标准化并没有做好工作，而使用x射线扫描数据集的统计数据进行标准化已经显示出最好的F1度量结果。第二个和第三个程序的平均值和标准偏差值非常接近。</p><h1 id="5b3d" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">使用mxnet的基本分类器</h1><p id="c542" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">首先，导入必要的库</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="c6bc" class="ld ig hi kz b fi le lf l lg lh">import mxnet as mx</span><span id="2c3c" class="ld ig hi kz b fi li lf l lg lh">import numpy as np</span><span id="04a8" class="ld ig hi kz b fi li lf l lg lh">import os, time, shutil</span><span id="5db7" class="ld ig hi kz b fi li lf l lg lh">from mxnet import gluon, image, init, nd</span><span id="81ac" class="ld ig hi kz b fi li lf l lg lh">from mxnet import autograd as ag</span><span id="c6d9" class="ld ig hi kz b fi li lf l lg lh">from mxnet.gluon import nn</span><span id="70fc" class="ld ig hi kz b fi li lf l lg lh">from mxnet.gluon.data.vision import transforms</span><span id="8a37" class="ld ig hi kz b fi li lf l lg lh">from gluoncv.utils import makedirs</span><span id="844b" class="ld ig hi kz b fi li lf l lg lh">from gluoncv.model_zoo import get_model</span><span id="0a2a" class="ld ig hi kz b fi li lf l lg lh">from gluoncv.loss import FocalLoss</span></pre><p id="d70e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们需要设置超参数，并告诉系统我们将使用GPU进行训练</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="9fb2" class="ld ig hi kz b fi le lf l lg lh">classes = 2</span><span id="0a20" class="ld ig hi kz b fi li lf l lg lh">epochs = 100</span><span id="bd0c" class="ld ig hi kz b fi li lf l lg lh">lr = 0.001</span><span id="25c5" class="ld ig hi kz b fi li lf l lg lh">per_device_batch_size = 1</span><span id="a961" class="ld ig hi kz b fi li lf l lg lh">momentum = 0.9</span><span id="ce20" class="ld ig hi kz b fi li lf l lg lh">wd = 0.0001</span><span id="d77f" class="ld ig hi kz b fi li lf l lg lh">lr_factor = 0.75</span><span id="8a6d" class="ld ig hi kz b fi li lf l lg lh">lr_steps = [10, 20, 30, np.inf]</span><span id="e2d2" class="ld ig hi kz b fi li lf l lg lh">num_gpus = 1</span><span id="852c" class="ld ig hi kz b fi li lf l lg lh">num_workers = 8</span><span id="5df2" class="ld ig hi kz b fi li lf l lg lh">ctx = [mx.gpu(i) for i in range(num_gpus)] if num_gpus &gt; 0 else [mx.cpu()]</span><span id="78b0" class="ld ig hi kz b fi li lf l lg lh">batch_size = per_device_batch_size * max(num_gpus, 1)</span></pre><p id="4f3c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们将裁剪图像以避免考虑它们侧面的标签，并尝试不同的标准化。例如，为了避免粘贴太多代码</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="b7dd" class="ld ig hi kz b fi le lf l lg lh">transform_train = transforms.Compose([</span><span id="7ffe" class="ld ig hi kz b fi li lf l lg lh">transforms.CenterCrop(crop_size),transforms.RandomFlipLeftRight(),transforms.ToTensor(),</span><span id="b5fa" class="ld ig hi kz b fi li lf l lg lh">transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])</span><span id="5c59" class="ld ig hi kz b fi li lf l lg lh"><br/>transform_test = transforms.Compose([</span><span id="83f4" class="ld ig hi kz b fi li lf l lg lh">transforms.CenterCrop(crop_size),transforms.ToTensor(),</span><span id="c4f7" class="ld ig hi kz b fi li lf l lg lh">transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])</span></pre><p id="9dd1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们还定义了我们的数据加载器。由于数据集非常小且不平衡，我们放弃了创建验证集的必要性，仅使用训练集和测试集来限制自己:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="3d19" class="ld ig hi kz b fi le lf l lg lh">path = folder</span><span id="68d5" class="ld ig hi kz b fi li lf l lg lh">train_path = os.path.join(path, 'train')</span><span id="ff98" class="ld ig hi kz b fi li lf l lg lh">test_path = os.path.join(path, 'test')</span><span id="158a" class="ld ig hi kz b fi li lf l lg lh">train_data = gluon.data.DataLoader(gluon.data.vision.ImageFolderDataset(train_path).transform_first(transform_train), batch_size=batch_size, shuffle=True, num_workers=num_workers)</span><span id="c6a2" class="ld ig hi kz b fi li lf l lg lh">test_data = gluon.data.DataLoader(gluon.data.vision.ImageFolderDataset(test_path).transform_first(transform_test), batch_size=batch_size, shuffle=False, num_workers = num_workers)</span></pre><p id="c8af" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在我们定义预训练模型:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="985b" class="ld ig hi kz b fi le lf l lg lh">model_name = ‘ResNet34_v2’</span><span id="05a3" class="ld ig hi kz b fi li lf l lg lh">finetune_net = get_model(model_name, pretrained=True)</span><span id="8a7d" class="ld ig hi kz b fi li lf l lg lh">with finetune_net.name_scope():</span><span id="903f" class="ld ig hi kz b fi li lf l lg lh">finetune_net.output = nn.Dense(classes)</span><span id="a173" class="ld ig hi kz b fi li lf l lg lh">finetune_net.output.initialize(init.Xavier(), ctx = ctx)</span><span id="f266" class="ld ig hi kz b fi li lf l lg lh">finetune_net.collect_params().reset_ctx(ctx)</span><span id="bbdf" class="ld ig hi kz b fi li lf l lg lh">finetune_net.hybridize()</span></pre><p id="beee" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">最后，我们定义训练器来训练网络，F1作为要遵循的度量，以及两个损失函数供选择:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="55c9" class="ld ig hi kz b fi le lf l lg lh">trainer = gluon.Trainer(finetune_net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': momentum, 'wd': wd})</span><span id="f6ec" class="ld ig hi kz b fi li lf l lg lh">metric = mx.metric.F1()</span><span id="b63d" class="ld ig hi kz b fi li lf l lg lh">#L = gluon.loss.SoftmaxCrossEntropyLoss()</span><span id="7e5c" class="ld ig hi kz b fi li lf l lg lh">L = FocalLoss(num_class = classes)</span></pre><p id="0e66" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们还定义了一个函数来获得准确性和F1指标，并运行训练和测试迭代:</p><pre class="ku kv kw kx fd ky kz la lb aw lc bi"><span id="ac18" class="ld ig hi kz b fi le lf l lg lh">for epoch in range(epochs):</span><span id="bf12" class="ld ig hi kz b fi li lf l lg lh">if epoch == lr_steps[lr_counter]:</span><span id="a7a4" class="ld ig hi kz b fi li lf l lg lh">trainer.set_learning_rate(trainer.learning_rate*lr_factor)</span><span id="53a4" class="ld ig hi kz b fi li lf l lg lh">lr_counter += 1</span><span id="eee4" class="ld ig hi kz b fi li lf l lg lh">train_loss = 0</span><span id="073d" class="ld ig hi kz b fi li lf l lg lh">metric.reset()</span><span id="0160" class="ld ig hi kz b fi li lf l lg lh">for i, batch in enumerate(train_data):</span><span id="da8f" class="ld ig hi kz b fi li lf l lg lh">data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0, even_split=False)</span><span id="21a3" class="ld ig hi kz b fi li lf l lg lh">label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0, even_split=False)</span><span id="33b2" class="ld ig hi kz b fi li lf l lg lh">with ag.record():</span><span id="d2ad" class="ld ig hi kz b fi li lf l lg lh">outputs = [finetune_net(X) for X in data]</span><span id="86eb" class="ld ig hi kz b fi li lf l lg lh">loss = [L(yhat, y) for yhat, y in zip(outputs, label)]</span><span id="e162" class="ld ig hi kz b fi li lf l lg lh">for l in loss:</span><span id="c2a4" class="ld ig hi kz b fi li lf l lg lh">   l.backward()</span><span id="82d3" class="ld ig hi kz b fi li lf l lg lh">trainer.step(batch_size)</span><span id="f4f4" class="ld ig hi kz b fi li lf l lg lh">train_loss += sum([l.mean().asscalar() for l in loss]) / len(loss)</span><span id="a874" class="ld ig hi kz b fi li lf l lg lh">metric.update(label, outputs)</span><span id="d2bb" class="ld ig hi kz b fi li lf l lg lh">_, train_f1 = metric.get()</span><span id="b582" class="ld ig hi kz b fi li lf l lg lh">train_loss /= num_batch</span><span id="93eb" class="ld ig hi kz b fi li lf l lg lh">val_acc, val_F1 = test(finetune_net, test_data, ctx)</span></pre><h1 id="3640" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">损失函数</h1><p id="6f5f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们已经用两个损失函数进行了实验:经典交叉熵损失函数和焦点损失函数。</p><p id="a002" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">焦点损失首先被引入用于物体检测问题，但是发现它对于分类也是非常有效的。最初的问题，触发了新的损失函数的开发，是非常简单的:由于前景-背景类别不平衡，一级物体检测器落后于两级物体检测器。</p><p id="5ede" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在[2]中，作者已经提出根据神经网络置信度来降低损失函数的权重，缩小来自易于学习的样本的训练贡献，从而处理类不平衡。</p><h1 id="2a57" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结果和讨论</h1><p id="7721" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">给定小规模的训练和测试集以及小数量的训练时期，分类的结果与工业中可接受的结果相差甚远。</p><figure class="ku kv kw kx fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lj"><img src="../Images/b2d40ce0942ad38081ad15e30587fe0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pR6SCcfpSFKMUN2enzq4Mw.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">不同作物值和不同标准化技术的训练损失</figcaption></figure><p id="2d48" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">然而，即使从这些结果中我们也可以得出一些结论:</p><ul class=""><li id="e05f" class="kg kh hi jf b jg kb jk kc jo ki js kj jw kk ka kl km kn ko bi translated">焦点损失显示出好得多的结果。</li></ul><figure class="ku kv kw kx fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lv"><img src="../Images/1deb91ccfbe8b77619804a510cff9dde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_eEtbmTEZ3JKC64ANVoktQ.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">红色:焦点损失，蓝色:交叉熵损失</figcaption></figure><ul class=""><li id="7272" class="kg kh hi jf b jg kb jk kc jo ki js kj jw kk ka kl km kn ko bi translated">中央裁剪为500(对于大多数图像来说，大约是图像大小的30%)比裁剪为200(图像大小的10%-15%)要好。</li><li id="5f84" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">使用当前数据集的平均值对数据集进行归一化比使用ImageNet平均值更好(根据最终的F1指标);然而，ImageNet归一化显示了训练期间损失函数的最低结果。</li></ul><figure class="ku kv kw kx fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lw"><img src="../Images/e48d4b1c4c3dcf58af4ff27bc9ab76fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wEgnKAfe5WcWh-gBr6Dipg.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">红色:裁剪到200，蓝色:裁剪到500</figcaption></figure><ul class=""><li id="3eba" class="kg kh hi jf b jg kb jk kc jo ki js kj jw kk ka kl km kn ko bi translated">更复杂的预训练模型ResNext101_32x4d比ResNet34_v2的性能更高。</li></ul><figure class="ku kv kw kx fd lk er es paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="er es lx"><img src="../Images/36c69f4e4c95df23438d972f89d2deed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MapAY43RCXnU20Ru0wawNw.png"/></div></div><figcaption class="lr ls et er es lt lu bd b be z dx translated">对于数据集，使用其平均值进行归一化，ResNext101_32x4d模型显示出比更简单的模型更差的性能。需要更多培训？</figcaption></figure><h1 id="853f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结论和进一步措施</h1><p id="4836" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">mxnet是相当高性能和易于使用的。新冠肺炎病可以在患者的x光扫描中检测出来，但是需要更多的训练数据和更多的实验来获得可接受的精确度和召回率。</p><h2 id="0e4e" class="ld ig hi bd ih ly lz ma il mb mc md ip jo me mf it js mg mh ix jw mi mj jb mk bi translated">如何改进这种模式？</h2><ul class=""><li id="6d79" class="kg kh hi jf b jg jh jk jl jo ml js mm jw mn ka kl km kn ko bi translated">使用更多图像</li><li id="9f33" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">利用领域信息预处理图像</li><li id="c0ce" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">延长训练时间</li><li id="f748" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">测试其他神经网络架构</li><li id="b6eb" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">测试其他职业的目标(目前，新冠肺炎对其余的)</li></ul><p id="094d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">[1]<a class="ae mo" href="https://github.com/ieee8023/covid-chestxray-dataset" rel="noopener ugc nofollow" target="_blank">https://github.com/ieee8023/covid-chestxray-dataset</a></p><p id="9dc0" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">[2]https://arxiv.org/abs/1708.02002<a class="ae mo" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>