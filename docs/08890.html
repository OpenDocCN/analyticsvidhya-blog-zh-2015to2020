<html>
<head>
<title>Speech Analytics Part -3, Creating a SpeechToText Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语音分析第3部分，创建语音文本分类器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/speech-analytics-part-3-creating-a-speechtotext-classifier-e46b7befe240?source=collection_archive---------20-----------------------#2020-08-17">https://medium.com/analytics-vidhya/speech-analytics-part-3-creating-a-speechtotext-classifier-e46b7befe240?source=collection_archive---------20-----------------------#2020-08-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4fbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">制作声音模型与制作数据、NLP或计算机视觉非常相似。最重要的部分是了解声波的基础知识，以及我们如何对其进行预处理以将其放入模型中。</p><p id="bb47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以看看这个系列之前的<a class="ae jd" rel="noopener" href="/@divalicious.priya/speech-analytics-part-1-basics-of-speech-analytics-37ba6d5904e2">第一部分</a>和<a class="ae jd" rel="noopener" href="/@divalicious.priya/speech-analytics-part-2-sound-analytics-in-torchaudio-7645a3dd192d">第二部分</a>，了解我们是如何处理声波的。</p><p id="f975" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用本次比赛中使用的<a class="ae jd" href="https://www.kaggle.com/c/tensorflow-speech-recognition-challenge/data?select=train.7z" rel="noopener ugc nofollow" target="_blank">数据集</a>来创建我们的SpeechToText分类器。数据集由12个单词的若干次出现组成——“是”、“否”、“上”、“下”、“左”、“右”、“开”、“关”、“停止”、“开始”、“沉默”和未知的声音。假设我们将创建一个分类器模型，该模型将只能预测12个单词中的任何一个。</p><p id="bd66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在这里找到整个<a class="ae jd" href="https://github.com/divapriya/Speech-Analytics-SpeechToText/blob/master/BasicModelAudio_part2.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a></p><h2 id="ebc7" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">步骤1:浏览子文件夹来阅读我们的声音文件</h2><figure class="jz ka kb kc fd kd"><div class="bz dy l di"><div class="ke kf l"/></div></figure><h2 id="68d8" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">步骤2:剪切和填充声音文件</h2><p id="e7a8" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">分类器模型的一个关键要求是每个单词的输入长度必须相同。因此，我们将确保在出现时间不等于16秒的单词的末尾砍掉额外的时间并填充静音。</p><figure class="jz ka kb kc fd kd"><div class="bz dy l di"><div class="ke kf l"/></div></figure><h2 id="cfc9" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">步骤3:从声波中提取特征</h2><p id="9c66" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">要深入了解这一点，请参考我以前的<a class="ae jd" rel="noopener" href="/@divalicious.priya/speech-analytics-part-2-sound-analytics-in-torchaudio-7645a3dd192d">博客。对于这个问题，我们将提取声音的MFCC特征，并将它们用作我的分类器模型的输入。</a></p><figure class="jz ka kb kc fd kd"><div class="bz dy l di"><div class="ke kf l"/></div></figure><h2 id="6a40" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">步骤4:模型架构</h2><p id="9aef" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">假设每个单词都由音素组成，模型需要做的第一步是从整个单词中提取必要的特征/音素。因此，我们将使用CNN模型来捕捉这些特征。下一步，我们需要研究所有的特征/音素，并将单词分类到其中一个类别。序列在这里起着重要的作用。因此，我们将增加一个LSTM层。</p><p id="9ec9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最终的架构看起来像:</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es kl"><img src="../Images/ad1d8ee05e3870c13be3023effe48b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*is_gPF8wJlAReSiB_M9eSQ.png"/></div></figure><figure class="jz ka kb kc fd kd"><div class="bz dy l di"><div class="ke kf l"/></div></figure><h2 id="71e2" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">第五步:拟合模型</h2><figure class="jz ka kb kc fd kd"><div class="bz dy l di"><div class="ke kf l"/></div></figure><h2 id="ffb6" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">第六步:通过麦克风录下一个单词</h2><p id="79c0" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">让我们学习如何从麦克风获取输入</p><figure class="jz ka kb kc fd kd"><div class="bz dy l di"><div class="ke kf l"/></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">通过麦克风记录输入</figcaption></figure><h2 id="78a0" class="je jf hi bd jg jh ji jj jk jl jm jn jo iq jp jq jr iu js jt ju iy jv jw jx jy bi translated">步骤7:将录音转换成文本</h2><p id="34d1" class="pw-post-body-paragraph if ig hi ih b ii kg ik il im kh io ip iq ki is it iu kj iw ix iy kk ja jb jc hb bi translated">我们将使用上面创建的模型来预测来自麦克风的输入记录</p><figure class="jz ka kb kc fd kd"><div class="bz dy l di"><div class="ke kf l"/></div></figure><p id="d88a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在这里找到整个<a class="ae jd" href="https://github.com/divapriya/Speech-Analytics-SpeechToText/blob/master/BasicModelAudio_part2.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a></p><p id="c050" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是你制造SpeechToText引擎的第一步。实际上，这种架构非常复杂，并且考虑到它需要学习的单词数量，需要很高的计算能力(我们最终的模型应该能够预测任何单词，而不仅仅是12个单词)。</p><p id="efa2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用的一些著名架构有:-</p><ol class=""><li id="f83b" class="ks kt hi ih b ii ij im in iq ku iu kv iy kw jc kx ky kz la bi translated">双向LSTMs + CTC</li><li id="5dd0" class="ks kt hi ih b ii lb im lc iq ld iu le iy lf jc kx ky kz la bi translated">基于注意力的Seq2Seq模型</li></ol><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lg"><img src="../Images/9f6c2a015ea822c851ef4750df5fed51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-2SQNkJ4Bzhr2A4N.jpg"/></div></div><figcaption class="ko kp et er es kq kr bd b be z dx translated">CTC模式</figcaption></figure><p id="6af1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请等待本文的第4部分来详细介绍这种架构。</p></div></div>    
</body>
</html>