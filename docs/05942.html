<html>
<head>
<title>Gradient Descent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/gradient-descent-5195ec6182bc?source=collection_archive---------20-----------------------#2020-05-06">https://medium.com/analytics-vidhya/gradient-descent-5195ec6182bc?source=collection_archive---------20-----------------------#2020-05-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="02bc" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">通过实例学习(第二部分)</h1><p id="e38a" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">我们已经确立了梯度下降背后的思想，即减少误差平方和(SSR)以获得线性系统系数的最佳值，我们通过获得SSR相对于“截距”系数的一阶导数(<em class="kb"> 𝛽 </em> 0)来实现这一点。所以，我建议在阅读本教程之前先阅读本教程的<a class="ae kc" href="https://www.landofsciences.com/posts/0/gradient-descent-part1/" rel="noopener ugc nofollow" target="_blank">第一部分</a>。</p><p id="dca1" class="pw-post-body-paragraph jd je hi jf b jg kd ji jj jk ke jm jn jo kf jq jr js kg ju jv jw kh jy jz ka hb bi translated"><a class="ae kc" href="https://www.landofsciences.com/posts/0/gradient-descent-part2/" rel="noopener ugc nofollow" target="_blank">阅读更多… </a></p></div></div>    
</body>
</html>