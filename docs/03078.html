<html>
<head>
<title>Pre-processing of Topically Coherent Text Segments in Python 💬</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中主题连贯文本段的预处理💬</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/pre-processing-of-topically-coherent-text-segments-in-python-58f9b258596c?source=collection_archive---------4-----------------------#2020-01-15">https://medium.com/analytics-vidhya/pre-processing-of-topically-coherent-text-segments-in-python-58f9b258596c?source=collection_archive---------4-----------------------#2020-01-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="f1c2" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">如何使用自然语言工具包预处理一组抄本并将其转换成数字表示</h2></div><p id="a2a1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">完整的<strong class="iz hj"> Jupyter笔记本</strong>和文件可在我的<a class="ae jt" href="https://github.com/maziarizadi/TextPreProcessingPy" rel="noopener ugc nofollow" target="_blank"> <strong class="iz hj"> GitHub页面</strong> </a>获得。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h2 id="2f19" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">介绍</h2><p id="96cd" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">文本文档，如长录音和会议记录，通常由主题连贯的文本片段组成，每个片段包含一定数量的文本段落。在每一个主题连贯的片段中，人们会期望单词的使用比跨片段的使用表现出更一致的词汇分布。<strong class="iz hj">自然语言处理(NLP) </strong>，更具体地说，将文本线性划分成主题片段可用于文本分析任务，例如信息检索中的段落检索、文档摘要和话语分析。在当前的练习中，我们将回顾如何编写Python代码来<strong class="iz hj">预处理</strong>一组抄本并且<strong class="iz hj">将它们转换成适合输入到<strong class="iz hj">主题分割算法</strong>中的数值表示</strong>。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es lb"><img src="../Images/ebded4b8abcabc0f3892c045e78bf18f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*PGB0w1JZslqA-hM0xGrmJw.gif"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated"><a class="ae jt" href="https://s3.amazonaws.com/codecademy-content/courses/NLP/Natural_Language_Processing_Overview.gif" rel="noopener ugc nofollow" target="_blank">图像的来源</a></figcaption></figure><p id="c57a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这篇文章的来源来自我在莫纳什大学完成的<strong class="iz hj">数据科学研究生文凭的一部分作业。我也做了一些改动，让原来的任务更有趣。</strong></p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h2 id="c6e0" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">什么是使用案例，NLP如何提供帮助？</h2><p id="1073" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">现在有很多求职网站，包括seek.com.au和au.indeed.com。这些求职网站都管理着一个求职系统，求职者可以根据关键词、薪水和类别来搜索相关的工作。通常，广告工作的类别由广告商(例如，雇主)手动输入。类别分配可能会出错。<strong class="iz hj">因此，错误类别的工作将无法获得相关候选群体的足够曝光度</strong>。</p><p id="4504" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">随着文本分析的进步，自动工作分类将变得可行，并且可以向潜在的广告客户提供合理的工作类别建议。这有助于减少人工数据输入错误，增加相关候选人的职位曝光率，还可以改善求职网站的用户体验。为了做到这一点，我们需要一个<strong class="iz hj">自动招聘广告分类</strong>系统，它在现有的招聘广告数据集上进行训练，具有标准化的工作类别，<strong class="iz hj">预测新输入的招聘广告的类别标签</strong>。</p><blockquote class="lr"><p id="a813" class="ls lt hi bd lu lv lw lx ly lz ma js dx translated">当前示例涉及处理工作广告文本数据的第一步，即，将工作广告文本解析成更合适的格式。</p></blockquote><p id="3a35" class="pw-post-body-paragraph ix iy hi iz b ja mb ij jc jd mc im jf jg md ji jj jk me jm jn jo mf jq jr js hb bi translated">我们提供的招聘广告数据包含大量以简单txt格式表示的冗余信息。我们应该对招聘广告文本数据进行适当的预处理，以提高分类算法的性能。</p></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="1070" class="mg kc hi bd kd mh mi mj kh mk ml mm kl io mn ip ko ir mo is kr iu mp iv ku mq bi translated">问题陈述💡</h1><p id="6ba1" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">我们需要编写Python代码来提取一组表示每个招聘广告内容的单词(例如，unigrams ),然后<strong class="iz hj">将每个广告描述转换为数字表示</strong> : count vector，它可以直接用作许多分类算法的输入。</p><h2 id="7921" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">我们将采取什么步骤？</h2><ul class=""><li id="400e" class="mr ms hi iz b ja kw jd kx jg mt jk mu jo mv js mw mx my mz bi translated">提取<br/>数据文件<code class="du na nb nc nd b">data.txt</code>中所有招聘广告的id和描述(约500条招聘广告)。</li><li id="da4d" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated">将这些招聘广告文本作为稀疏计数向量进行处理和存储。</li></ul><p id="bee2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了实现上述目标，我们将:</p><ul class=""><li id="1b12" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated">排除长度小于4的单词</li><li id="3519" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated">使用提供的停用词列表(即停用词_en.txt)删除停用词</li><li id="ce73" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated">删除在一个招聘广告描述中只出现一次的单词，将其保存(无重复)为一个<code class="du na nb nc nd b">txt</code>文件(参考所需的输出)</li><li id="da51" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated">从生成的词汇表中排除这些单词</li><li id="cd2e" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated">找到100多个广告<br/>描述中出现的常用词，保存为<code class="du na nb nc nd b">txt</code>文件(参考所需输出)</li><li id="3671" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated">在生成的词汇表中排除它们</li></ul><p id="37b4" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们不会:</p><ul class=""><li id="4028" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated">生成多词短语(即搭配，名词短语)</li></ul><p id="27a1" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本练习结束时，我们将获得以下几项输出，包括它们的要求:</p><p id="ca28" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi nm translated"><span class="l nn no np bm nq nr ns nt nu di"> 1。</span> <code class="du na nb nc nd b">vocab.txt</code>:包含以下格式的单字词汇:<code class="du na nb nc nd b">word_string:integer_index</code></p><ul class=""><li id="ba0a" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated">词汇表中的单词必须按字母顺序排列。这个文件是解释稀疏编码的关键。例如，在下面的例子中，单词abbie是词汇表中的第12个单词(对应的integer_index = 11)(注意，下面的数字和单词不是指示性的)。</li></ul><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es nv"><img src="../Images/f2987425612b8e4ada501d5e72b8394b.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*IamDO4qHbniGJWVAXSRuyQ.jpeg"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">vocab.txt文件输出格式</figcaption></figure><p id="6c3e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi nm translated"><span class="l nn no np bm nq nr ns nt nu di"> 2。</span> <code class="du na nb nc nd b">highFreq.txt</code>该文件包含在100多个广告描述中出现的常用词。在输出<code class="du na nb nc nd b">txt</code>文件中，每行应该只包含一个单词。单字的顺序基于它们的频率，即包含该词的广告的数量，从高到低。</p><p id="f9fc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi nm translated"><span class="l nn no np bm nq nr ns nt nu di"> 3。</span> <code class="du na nb nc nd b">lowFreq.txt</code>该文件包含按字母顺序在一个招聘广告描述中只出现一次的单词。在输出的<code class="du na nb nc nd b">txt</code>文件中，每行应该包含一个单词。</p><p id="841b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi nm translated"><span class="l nn no np bm nq nr ns nt nu di"> 4。</span> <code class="du na nb nc nd b">sparse.txt</code>该文件的每一行对应一个广告。所以，他们从<code class="du na nb nc nd b">advertisement ID</code>开始。每行的其余部分是以逗号分隔的<code class="du na nb nc nd b">word_index:word_freq</code>形式的相应描述的稀疏表示。行的顺序必须与输入文件中广告的顺序相匹配。</p><p id="3b8e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">注:</strong> <code class="du na nb nc nd b">word_freq</code>这里指的是unigram在相应描述中的出现频率，而不是整个文档。例如，在广告12612628的描述中，单词编号11(根据上面的例子是‘abbie ’)恰好出现一次(编号不是指示性的) :</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es nw"><img src="../Images/764923683b48a9904acf1302cbb84f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cUkXXD2uaCQhY06U2et3gQ.png"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">sparse.txt文件输出格式</figcaption></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h1 id="e28b" class="mg kc hi bd kd mh mi mj kh mk ml mm kl io mn ip ko ir mo is kr iu mp iv ku mq bi translated">⛳️解决方案</h1><p id="1e97" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">所以我们总是从导入所需的库开始。鉴于这项工作的性质，需要做到以下几点:</p><h2 id="d406" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">导入库</h2><ul class=""><li id="8faf" class="mr ms hi iz b ja kw jd kx jg mt jk mu jo mv js mw mx my mz bi translated"><strong class="iz hj">正则表达式</strong></li></ul><p id="888c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">第一个是正则表达式，简称为ReGex。如果你还没有用过它们，我强烈建议你拿起它，做一些很酷的事情。再往下，我已经提供了一些开始的细节。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="aee4" class="kb kc hi nd b fi ob oc l od oe"># Regular Expressions (ReGeX)</span><span id="b845" class="kb kc hi nd b fi of oc l od oe">import re</span></pre><ul class=""><li id="bcab" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated"><strong class="iz hj">自然语言工具包</strong></li></ul><p id="b393" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">NLTK是构建Python程序来处理人类语言数据的领先平台。它提供了易于使用的界面，如WordNet，以及一套用于分类、标记化、词干化、标记、解析和语义推理的文本处理库，以及工业级NLP库的包装器。</p><p id="2949" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du na nb nc nd b">nltk.probability</code>提供了表示和处理概率信息的类，比如<code class="du na nb nc nd b">FreqDist</code>，我们稍后会用到。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="92b4" class="kb kc hi nd b fi ob oc l od oe"># Natural Language Toolkit</span><span id="4532" class="kb kc hi nd b fi of oc l od oe">import nltk</span><span id="d682" class="kb kc hi nd b fi of oc l od oe">from nltk.probability import *</span><span id="c8d4" class="kb kc hi nd b fi of oc l od oe">from nltk.corpus import stopwords</span></pre><ul class=""><li id="8fc7" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated"><strong class="iz hj"> Itertools </strong></li></ul><p id="a3b0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Python <code class="du na nb nc nd b">itertools</code>模块是处理迭代器的工具集合。简单地说，迭代器是可以在<code class="du na nb nc nd b">for</code>循环中使用的数据类型。Python中最常见的迭代器是list。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="0288" class="kb kc hi nd b fi ob oc l od oe"># Functions creating iterators for efficient looping</span><span id="d922" class="kb kc hi nd b fi of oc l od oe">import itertools</span><span id="485c" class="kb kc hi nd b fi of oc l od oe">from itertools import chain</span><span id="e8ca" class="kb kc hi nd b fi of oc l od oe">from itertools import groupby</span></pre><h2 id="fd68" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">让我们写一些代码🔥</h2><p id="1a6f" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">我们先导入数据。GitHub上有一个名为<code class="du na nb nc nd b">data.txt</code>的文件供你参考。我把它保存在本地电脑上，和我的Jupyter笔记本文件放在同一个文件夹里。</p><p id="a393" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在读取文件之前，为了方便起见，我们定义了一个空列表，并将其命名为<code class="du na nb nc nd b">data</code>。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="2108" class="kb kc hi nd b fi ob oc l od oe">data = []</span></pre><p id="1a76" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后我们简单的读取<code class="du na nb nc nd b">data.txt</code>并保存在列表<code class="du na nb nc nd b">data</code>中。确保你定义了<strong class="iz hj">编码格式</strong> <code class="du na nb nc nd b">utf8</code>，否则你可能会得到一个错误。</p><ul class=""><li id="30d1" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated">样本误差:</li></ul><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="c888" class="kb kc hi nd b fi ob oc l od oe">UnicodeDecodeError: 'charmap' codec can't decode byte 0x9d in position 260893: character maps to &lt;undefined&gt;</span></pre><p id="fd73" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">另一个<strong class="iz hj">考虑事项</strong>是，我们使用<code class="du na nb nc nd b">.lower()</code>函数直接将文本转换成lower以保持一致性。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="a26d" class="kb kc hi nd b fi ob oc l od oe">with open('data.txt', encoding="utf8") as f:<br/>    data = f.read().lower()</span></pre><h2 id="6ce5" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">格式化和清理✂️🔨 📌</h2><p id="29a0" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">现在，我们需要开始标记文本的过程。将一个字符序列分成几个部分的任务称为标记化。</p><p id="a099" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，我们必须移除文本中的所有噪音，比如/-*#@或任何其他非单词字符或多余的空格，我们使用强大的<code class="du na nb nc nd b">ReGex</code>工具来完成。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es og"><img src="../Images/8ab2fa98859c724ca5cb350eee10ac93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zrjjtCTZQSbnZXiMSrHJng.jpeg"/></div></div></figure><p id="ef5e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了使用ReGex运行格式化，需要采取两个步骤；</p><p id="3124" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">(1)创建模式，</p><p id="9395" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">(2)使用Python代码运行模式并找到匹配。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="c853" class="kb kc hi nd b fi ob oc l od oe"># (1) create a pattern for REGEX to find and keep matching words only</span><span id="aabd" class="kb kc hi nd b fi of oc l od oe">pattern = re.compile(r"[a-zA-Z]+(?:[-'][a-zA-Z]+)?")</span><span id="c790" class="kb kc hi nd b fi of oc l od oe"># (2)tokenise the words: match the pattern to file's content <br/># and tokenize the content</span><span id="565f" class="kb kc hi nd b fi of oc l od oe">tokenised = pattern.findall(data)</span></pre><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es oh"><img src="../Images/467ab59e3a79f2cb8d4cc962944e379f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*PIKVuAWpPhpXWfbDXiKsfQ.gif"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">图像<a class="ae jt" href="https://www.google.com/url?sa=i&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjf4MHO4YTnAhWLyDgGHW9YDI0QjRx6BAgBEAQ&amp;url=http%3A%2F%2Fnlp.cs.tamu.edu%2F&amp;psig=AOvVaw0kkE5JmXeahMHoc6Uvi0S9&amp;ust=1579148397647336" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="7cac" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Regex上有很多在线资源，但我发现最有趣的是https://regex101.com/。它不仅可以帮助您将文本与模式相匹配，还可以提供简短而有价值的内容。在图1中，我在他们的页面上提供了一个简单的功能列表。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es oi"><img src="../Images/b2a25f3088565f0816481c2465d7ee1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pwcYEMluczE-AXfHdrjbcw.jpeg"/></div></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">图1，<code class="du na nb nc nd b"><a class="ae jt" href="https://regex101.com/" rel="noopener ugc nofollow" target="_blank">regex101</a>.com</code>提供的功能</figcaption></figure><p id="d663" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">对Python有用的正则表达式资源:</strong></p><ul class=""><li id="e616" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated"><a class="ae jt" href="https://www.dataquest.io/blog/regular-expressions-data-scientists/" rel="noopener ugc nofollow" target="_blank"> Python正则表达式数据科学教程</a></li><li id="29c8" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated"><a class="ae jt" href="https://docs.python.org/3/library/re.html" rel="noopener ugc nofollow" target="_blank"> Python 3 re模块文档</a></li><li id="2afa" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated"><a class="ae jt" href="https://regex101.com/" rel="noopener ugc nofollow" target="_blank">在线正则表达式测试器和调试器</a></li></ul><h2 id="2f28" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated"><strong class="ak">索引标记化列表📇</strong></h2><p id="882b" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">现在，我已经根据每个招聘广告中的<code class="du na nb nc nd b">id</code>和<code class="du na nb nc nd b">title</code>对令牌进行了索引:</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="e8cc" class="kb kc hi nd b fi ob oc l od oe"># pass the length of the 'tokenised' series into a variable</span><span id="ea25" class="kb kc hi nd b fi of oc l od oe">tokenised_len = len(tokenised)</span><span id="2c39" class="kb kc hi nd b fi of oc l od oe"><br/># indexing the tokens based on the position of "id" and "title"</span><span id="e5e7" class="kb kc hi nd b fi of oc l od oe">indexes = [i for i, v in enumerate(tokenised) if v=='id' and i+1 &lt; tokenised_len and tokenised[i+1]=='title']</span></pre><p id="b917" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我们从<code class="du na nb nc nd b"><strong class="iz hj">itertools</strong></code> <strong class="iz hj"> </strong> <a class="ae jt" href="https://docs.python.org/3/library/itertools.html" rel="noopener ugc nofollow" target="_blank"> recipes </a>创建一个函数，该函数遍历令牌列表，并且<strong class="iz hj">创建一个子列表，以包括仅与一个作业广告相关的令牌</strong>。输出将是一个数据字典。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="627a" class="kb kc hi nd b fi ob oc l od oe"># from itertools recipes<br/>def pairwise(iterable, fillvalue=None):<br/>    """<br/>       This function iterates through the list of tokens and <br/>       creates sub list to include tokens related to one job ad only<br/>    """<br/>    a, b = iter(iterable), iter(iterable)<br/>    next(b, None)<br/>    return itertools.zip_longest(a, b, fillvalue=fillvalue)</span><span id="5153" class="kb kc hi nd b fi of oc l od oe"><br/># pairwise based on indexes in the last block and store in the 'tokenised' as a list</span><span id="4499" class="kb kc hi nd b fi of oc l od oe">tokenised = [tokenised[i:j] for i,j in pairwise(indexes)]</span></pre><p id="414e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了创建数据字典，我使用了Python <code class="du na nb nc nd b"><strong class="iz hj">itertools</strong></code>。<strong class="iz hj"> Jason Rigdel </strong>对Python中的<code class="du na nb nc nd b"><strong class="iz hj">itertools</strong></code> <strong class="iz hj"> </strong>这一话题做了很好的解释，并提供了一组例子。</p><ul class=""><li id="6bdf" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated"><a class="ae jt" rel="noopener" href="/@jasonrigden/a-guide-to-python-itertools-82e5a306cdf8">Python ITER tools指南</a></li></ul><p id="9d78" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">但是，这个列表包含了很多功能词，比如“to”、“in”、“the”、“is”等等。</p><blockquote class="lr"><p id="5308" class="ls lt hi bd lu lv lw lx ly lz ma js dx translated">这些功能词通常<em class="oj">对文本的语义没有太大贡献</em>，除了在文本分析中增加数据的维度。</p><p id="466a" class="ls lt hi bd lu lv lw lx ly lz ma js dx translated">另外，请注意，我们的目标通常是建立一个预测分类模型。因此，我们对报告的含义比对语法更感兴趣。因此，我们可以选择删除那些单词，这是你的下一个任务。</p></blockquote><p id="2bf1" class="pw-post-body-paragraph ix iy hi iz b ja mb ij jc jd mc im jf jg md ji jj jk me jm jn jo mf jq jr js hb bi translated">我将通过保留那些包含3个以上字符的标记来排除所有少于4个字符的标记，并将其余的标记添加到一个名为<code class="du na nb nc nd b">to_remove</code>的列表中。这个列表将被添加到通用英语<code class="du na nb nc nd b">stopwords</code>列表中。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="63d3" class="kb kc hi nd b fi ob oc l od oe">tokenised = [[word if len(word) &gt; 3 else "to_remove" for word in job] for job in tokenised]</span></pre><h2 id="5f76" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">删除停用词✂️</h2><p id="2ac6" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">停用词携带<em class="ok">少量词汇内容</em>。</p><blockquote class="lr"><p id="7e0d" class="ls lt hi bd lu lv lw lx ly lz ma js dx translated">它们经常是英语中的功能词，例如，冠词、代词、助词等等。在NLP和IR中，我们通常从词汇表中排除停用词。否则，我们将面临<a class="ae jt" href="https://towardsdatascience.com/the-curse-of-dimensionality-f07c66128fe1" rel="noopener" target="_blank">维度诅咒</a>。</p></blockquote><p id="6a29" class="pw-post-body-paragraph ix iy hi iz b ja mb ij jc jd mc im jf jg md ji jj jk me jm jn jo mf jq jr js hb bi translated">也有一些例外，比如句法分析像解析，我们选择保留那些功能词。但是，您将通过使用<strong class="iz hj"> NLTK </strong>中的停用词列表来删除上面列表中的所有停用词，它是:</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="ae8e" class="kb kc hi nd b fi ob oc l od oe">nltk.download('stopwords')</span><span id="5f85" class="kb kc hi nd b fi of oc l od oe">stopwords_list = stopwords.words('english')</span></pre><p id="5849" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于这个例子，我已经在我的GitHub上提供了<code class="du na nb nc nd b">stopwords_en.txt</code>文件，您可以从那里下载。我们首先将上面创建的<code class="du na nb nc nd b">to_remove</code>列表添加到<code class="du na nb nc nd b">stopwords_en.txt</code>文件中，读取该文件，然后将它们保存为<code class="du na nb nc nd b">set()</code>。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="4422" class="kb kc hi nd b fi ob oc l od oe"># adding'to_removed' string to the list of stopwords</span><span id="71f8" class="kb kc hi nd b fi of oc l od oe">stopwords = []</span><span id="55ba" class="kb kc hi nd b fi of oc l od oe">with open('stopwords_en.txt',"a") as f:<br/>    f.write("\nto_remove") #\n to shift to next line</span><span id="005e" class="kb kc hi nd b fi of oc l od oe"><br/>with open('stopwords_en.txt') as f:<br/>    stopwords = f.read().splitlines() #reading stopwords line and create stopwords as a list</span><span id="3d72" class="kb kc hi nd b fi of oc l od oe"># convert stopwords into set</span><span id="0a38" class="kb kc hi nd b fi of oc l od oe">stopwordsset = set(stopwords)</span></pre><p id="7398" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">你可能想知道为什么我们把<code class="du na nb nc nd b">stopwords</code>保存为<code class="du na nb nc nd b">set</code>。这是个好问题……Python<code class="du na nb nc nd b">set</code>比<code class="du na nb nc nd b">list</code>更好，因为<code class="du na nb nc nd b">set</code>在搜索大量<strong class="iz hj"> <em class="ok">可散列</em> </strong>项目方面比列表运行得快得多。</p><p id="93f9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来，我创建了一个名为<code class="du na nb nc nd b">purifier()</code>的函数，它通过移除<code class="du na nb nc nd b">stopwords</code>来净化令牌，然后运行<code class="du na nb nc nd b">tokenised</code>列表。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="14f6" class="kb kc hi nd b fi ob oc l od oe">def purifier(tokenList,remove_token):<br/>    """<br/>        This function takes two input (list of current tokens <br/>        and list of tokens to be removed)<br/>        The function converts the list into set to improve the <br/>        performance<br/>        and return a list of sets each of which include purified <br/>        tokens and remove_token lists are removed<br/>    """<br/>    return [set(word for word in job if word not in remove_token) for job in tokenList]</span><span id="0cda" class="kb kc hi nd b fi of oc l od oe"># running the 'purifier' function</span><span id="5c77" class="kb kc hi nd b fi of oc l od oe">tokenised = purifier(tokenised,stopwordsset)</span></pre><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es ol"><img src="../Images/fa8424bd4e4259c95abd7207204e6a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*cWfW76Tdy3iWAoKeW5rRtg.png"/></div><figcaption class="ln lo et er es lp lq bd b be z dx translated">照片来自shutterstock图书馆</figcaption></figure><p id="3980" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来是删除在一个招聘广告描述中只出现一次的<code class="du na nb nc nd b">words</code>，将它们(无重复)保存为txt文件(参考所需输出)。为此，您需要从生成的词汇表中排除这些单词。</p><p id="141a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了做到这一点，我们首先使用<code class="du na nb nc nd b"><strong class="iz hj">chain()</strong></code>功能将所有招聘广告中的所有单词列成一个列表。在“<a class="ae jt" rel="noopener" href="/@jasonrigden/a-guide-to-python-itertools-82e5a306cdf8">Python ITER tools</a>指南”中，有一个关于<code class="du na nb nc nd b">chain()</code>函数如何工作的很好的解释。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="5b2a" class="kb kc hi nd b fi ob oc l od oe">stop_wrds_removed_words = list(chain.from_iterable([word for word in job] for job in tokenised))</span></pre><p id="73fa" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将单词列表转换为集合以删除重复项并创建词汇集合</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="37b6" class="kb kc hi nd b fi ob oc l od oe">stop_wrds_removed_vocab = set(stop_wrds_removed_words)</span></pre><p id="b031" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来是通过<code class="du na nb nc nd b"><strong class="iz hj">FreqDisrt()</strong></code>函数中的单词来统计令牌的数量。</p><blockquote class="om on oo"><p id="908f" class="ix iy ok iz b ja jb ij jc jd je im jf op jh ji jj oq jl jm jn or jp jq jr js hb bi translated">FreqDist类用于对“频率分布”进行编码，它计算实验的每个结果出现的次数。它是<code class="du na nb nc nd b">nltk.probability</code>模块下的一个类。</p><p id="3cf8" class="ix iy ok iz b ja jb ij jc jd je im jf op jh ji jj oq jl jm jn or jp jq jr js hb bi translated">根据<a class="ae jt" href="https://devopedia.org/text-corpus-for-nlp" rel="noopener ugc nofollow" target="_blank"> developedia </a>的说法，通常，每个文本语料库都是文本源的集合。对于各种NLP任务，有几十个这样的语料库。本文忽略语音语料库，只考虑文本形式的语料库。在我们的例子中，文本语料库指的是所有工作广告的组合(…而不是每个工作单独)。</p></blockquote><p id="2eae" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面的line函数计算一个单词在整个<code class="du na nb nc nd b">corpus</code>中出现的次数，而不管它在哪个ad中。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="8077" class="kb kc hi nd b fi ob oc l od oe">fd = FreqDist(stop_wrds_removed_words)</span></pre></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h2 id="65b0" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">低频令牌</h2><p id="7651" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">为了找到不太频繁的令牌，我创建了一个只出现过一次的令牌列表，并将该列表转换为<code class="du na nb nc nd b">set</code>以提高性能。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="311d" class="kb kc hi nd b fi ob oc l od oe">once_only = set([k for k, v in fd.items() if v == 1])</span><span id="e95b" class="kb kc hi nd b fi of oc l od oe"># sort the set into alphabetical order</span><span id="bde5" class="kb kc hi nd b fi of oc l od oe">once_only = sorted(once_only)</span><span id="4175" class="kb kc hi nd b fi of oc l od oe"><br/>set(once_only)</span></pre><p id="319d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了创建<code class="du na nb nc nd b">lowFreq.txt</code>文件，我已经将在一个招聘广告描述中出现“仅一次”的单词的排序<code class="du na nb nc nd b">set</code>保存到一个同名文件中。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="03a9" class="kb kc hi nd b fi ob oc l od oe">out_file = open("lowFreq.txt", 'w')<br/>for d in once_only:<br/>    out_file.write(''.join(d) + '\n')<br/>out_file.close()</span></pre><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es os"><img src="../Images/d4ce25e2dfa65ae46764a298fa807449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rm8HmCWd017_LbUSKgh3Tg.png"/></div></div></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><h2 id="3c40" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">高频令牌</h2><p id="db0a" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">在这个阶段，我重复上面相同的步骤，但是，这一次的目的是找到高频词并把它们保存在名为<code class="du na nb nc nd b">highFreq.txt</code>的文件中。<br/>我首先通过运行我们之前定义的<code class="du na nb nc nd b">purifier()</code>函数，从令牌的<code class="du na nb nc nd b">list</code>中移除<code class="du na nb nc nd b">lowFreq</code>令牌。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="08d7" class="kb kc hi nd b fi ob oc l od oe">tokenised = purifier(tokenised,once_only)</span></pre><p id="eaa6" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下一步是在移除了<code class="du na nb nc nd b">once_only</code>个单词之后创建一个新的<code class="du na nb nc nd b">list</code>个单词。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="23da" class="kb kc hi nd b fi ob oc l od oe">LowFreqRemoved_Words = list(chain.from_iterable([word for word in job] for job in tokenised))</span><span id="46e0" class="kb kc hi nd b fi of oc l od oe"><br/>LowFreqRemoved_vocab = set(LowFreqRemoved_Words)</span><span id="db34" class="kb kc hi nd b fi of oc l od oe"><br/>LowFreqRemoved_fd = FreqDist(LowFreqRemoved_Words)</span></pre><p id="e0c7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于高频词，我选择了100个阈值。你可以根据你的工作环境选择任何门槛。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="6c42" class="kb kc hi nd b fi ob oc l od oe">highFreq = set([k for k, v in LowFreqRemoved_fd.items() if v &gt; 100])</span></pre><p id="cca3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，将在100多个招聘广告描述中出现的高频词的排序列表保存到一个文件中。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="e8ad" class="kb kc hi nd b fi ob oc l od oe">out_file = open("highFreq.txt", 'w')<br/>for d in highFreq:<br/>    out_file.write(''.join(d) + '\n')<br/>out_file.close()</span></pre><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es ot"><img src="../Images/174bc5c558a8ce9268166aa2f6d7e370.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6GLm05BN8s3TMBUMjQO1zw.png"/></div></div></figure><p id="91ff" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们再次运行<code class="du na nb nc nd b">purifier()</code>函数来删除<code class="du na nb nc nd b">highFreq</code>数据集并创建一个新的<code class="du na nb nc nd b">list</code>。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="04de" class="kb kc hi nd b fi ob oc l od oe">tokenised = purifier(tokenised,highFreq)</span><span id="6d74" class="kb kc hi nd b fi of oc l od oe"><br/>HighFreqRemoved_words = list(chain.from_iterable([word for word in job] for job in tokenised))</span><span id="e2d0" class="kb kc hi nd b fi of oc l od oe">HighFreqRemoved_vocab = set(HighFreqRemoved_words)</span></pre><h2 id="7a3d" class="kb kc hi bd kd ke kf kg kh ki kj kk kl jg km kn ko jk kp kq kr jo ks kt ku kv bi translated">注意</h2><p id="1031" class="pw-post-body-paragraph ix iy hi iz b ja kw ij jc jd kx im jf jg ky ji jj jk kz jm jn jo la jq jr js hb bi translated">你可能想知道在我的代码中<code class="du na nb nc nd b">words</code>和<code class="du na nb nc nd b">vocab</code>列表有什么不同，为什么每次我创建一个<code class="du na nb nc nd b">words</code>列表，然后一个<code class="du na nb nc nd b">vocab</code>也被创建。原因还要追溯到Python中<code class="du na nb nc nd b">list</code>和<code class="du na nb nc nd b">set</code>的区别。然而底线是在<code class="du na nb nc nd b">vocab</code>中每个单词只被列出一次，而<code class="du na nb nc nd b">words</code>可能有重复。</p><p id="8594" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来是一个简单的检查点，用于查看令牌的提纯进度:</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="71a6" class="kb kc hi nd b fi ob oc l od oe">print(f"Length of words: {len(stop_wrds_removed_words)}")</span><span id="2813" class="kb kc hi nd b fi of oc l od oe">print(f"Length of vocab: {len(stop_wrds_removed_vocab)}")</span><span id="4104" class="kb kc hi nd b fi of oc l od oe">print(f"Length of LowFreqRemoved_Words: {len(LowFreqRemoved_Words)}")</span><span id="00d3" class="kb kc hi nd b fi of oc l od oe">print(f"Length of LowFreqRemoved_vocab: {len(LowFreqRemoved_vocab)}")</span><span id="7726" class="kb kc hi nd b fi of oc l od oe">print(f"Length of HighFreqRemoved_words: {len(HighFreqRemoved_words)}")</span><span id="d237" class="kb kc hi nd b fi of oc l od oe">print(f"Length of HighFreqRemoved_vocab: {len(HighFreqRemoved_vocab)}")</span></pre><p id="bd84" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">…这为我们提供了以下输出:</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="8e7e" class="kb kc hi nd b fi ob oc l od oe">Length of words: 474345<br/>Length of vocab: 18619<br/>Length of LowFreqRemoved_Words: 465779<br/>Length of LowFreqRemoved_vocab: 10053<br/>Length of HighFreqRemoved_words: 126491<br/>Length of HighFreqRemoved_vocab: 9103</span></pre></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="1a9a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">接下来是创建一个名为<code class="du na nb nc nd b">vocab.txt</code>的所有词汇的文件。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="b9a9" class="kb kc hi nd b fi ob oc l od oe">HighFreqRemoved_vocab = list(HighFreqRemoved_vocab)</span><span id="b4aa" class="kb kc hi nd b fi of oc l od oe"><br/># list of final vocabs</span><span id="78a1" class="kb kc hi nd b fi of oc l od oe">vocab = {HighFreqRemoved_vocab[i]:i for i in range(0,len(HighFreqRemoved_vocab))}</span></pre><p id="429f" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">构建一个函数来创建<code class="du na nb nc nd b">vocab.txt</code>文件，最后通过调用以下函数来构建和排序该文件:</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="d5a1" class="kb kc hi nd b fi ob oc l od oe">def vaocab_output(file):<br/>    with open (file, "a") as f:<br/>        for key in sorted(vocab.keys()):<br/>            f.write("%s:%s\n" % (key, vocab[key]))</span><span id="2c38" class="kb kc hi nd b fi of oc l od oe"># calling the function to build the file</span><span id="9fb9" class="kb kc hi nd b fi of oc l od oe">vaocab_output("vocab.txt")</span></pre><p id="f488" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du na nb nc nd b">vocab.txt</code>输出:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es ou"><img src="../Images/10261f935ac2fbfe2f500c1a974276b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z0WliTu2MnmQ4TX-acSSkA.png"/></div></div></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="79bc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">到目前为止，出于练习的目的，我尽量保持代码简单。然而，对于这一步，我创建了一段更复杂但更有效的代码。请在评论中留下任何问题，我保证他们会得到回答。</p><p id="da56" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后的活动是以逗号分隔的<code class="du na nb nc nd b">word_index:word_freq</code>的形式稀疏表示相应的描述并创建文件<code class="du na nb nc nd b">sparse.txt</code>。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="9536" class="kb kc hi nd b fi ob oc l od oe">data = {}<br/>id = None<br/>with open('data.txt', 'r',encoding="utf8") as f:<br/>    for i, line in enumerate(f): # create the iteration in the range of imported file's length<br/>        line = line.lower() <br/>        line = line.strip()<br/>        if not line:<br/>            continue<br/>        section = line.split(':')[0] # define 'section' as a method to manupilate each line based on how the line begins<br/>        content = ':'.join(line.split(':')[1:]).strip() # define 'content' a method to capture tokens<br/>        if section == 'id': # id section:<br/>            if id: # Error handle if theres some bad formatting: multiple ids<br/>                raise ValueError('unable to parse file at line %d, multiple ids' % i)<br/>            id = content[1:] # capture the job id<br/>            if id in data.keys():# Error handle if theres some bad formatting: duplicates<br/>                raise ValueError('unable to parse file at line %d, duplicate id' % i)<br/>        elif section == 'description': #capture job description per each job ad<br/>            if not id:# Error handle if theres some bad formatting: missing id<br/>                raise ValueError('unable to parse file at line %d, missing id' % i)<br/>            content = pattern.findall(line)<br/>            content = [value for value in content if len(value) &gt; 3] # remove short character token<br/>            content = [value for value in content if value not in stopwordsset] # remove stopwords<br/>            content = [value for value in content if value not in once_only] # remove lowFreq token<br/>            content = [value for value in content if value not in highFreq] # remove highFreq tokens<br/>            data[id] = content # creates data dictionary<br/>            id = None<br/>        elif section == 'title': # if the line start with 'title' do nothing<br/>            continue<br/>        else:<br/>            raise ValueError('unable to parse file at line %d, unexpected section name' % i)</span></pre><p id="d9a3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后构建<code class="du na nb nc nd b">sparse.txt</code>文件。</p><pre class="lc ld le lf fd nx nd ny nz aw oa bi"><span id="4545" class="kb kc hi nd b fi ob oc l od oe">with open('sparse.txt',"w") as f:<br/>    for jobID,content in data.items(): # go through data dictionary created in the last block<br/>        fd_parse = FreqDist(content) # count number of times each token occured in the same job ad<br/>        tmp = "" # create a placeholder for word_index:word_freq<br/>        for (x,y) in fd_parse.items(): # iterate through each frequencies<br/>            tmp += f"{vocab[x]}:{y}," # build the dictionary of word_index:word_freq in the placeholder<br/>        f.write(f"#{jobID},{tmp[:-1]}\n") # write in the file line by line</span></pre><p id="ea0c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><code class="du na nb nc nd b">sparse.txt</code>的输出:</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="er es ov"><img src="../Images/ff7c05bdc84a1a0045b5fb1c98c0d7ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*edqWIkdSj9hkE5uELDynEQ.png"/></div></div></figure></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="f454" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">进一步了解</p><ul class=""><li id="9259" class="mr ms hi iz b ja jb jd je jg nj jk nk jo nl js mw mx my mz bi translated"><a class="ae jt" href="https://www.kaggle.com/adamschroeder/countvectorizer-tfidfvectorizer-predict-comments" rel="noopener ugc nofollow" target="_blank">计数矢量器，tfidf矢量器，预测Kaggle上的评论教程</a></li><li id="f69d" class="mr ms hi iz b ja ne jd nf jg ng jk nh jo ni js mw mx my mz bi translated"><a class="ae jt" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" rel="noopener ugc nofollow" target="_blank">将一组文本文档转换成一个令牌计数矩阵</a></li></ul></div><div class="ab cl ju jv gp jw" role="separator"><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz ka"/><span class="jx bw bk jy jz"/></div><div class="hb hc hd he hf"><p id="7162" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">——结束—</p></div></div>    
</body>
</html>