<html>
<head>
<title>Optimizing Deep Neural Networks using MorphNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用MorphNet优化深度神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/optimizing-deep-neural-networks-using-morphnet-e71f74a89e17?source=collection_archive---------16-----------------------#2020-06-25">https://medium.com/analytics-vidhya/optimizing-deep-neural-networks-using-morphnet-e71f74a89e17?source=collection_archive---------16-----------------------#2020-06-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="656d" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="b37f" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">深度神经网络 (DNN)已经被有效地用于解决涉及图像、文本、语音等困难任务。以便将这种深度神经网络部署在诸如手机、汽车部件等资源受限的设备中。需要有一个优化的神经网络，没有多余的连接。这种优化的网络不应该具有比原始网络更低的精度。产生这种优化网络的一种方法是使用MorphNet。MorphNet由谷歌于2018年<strong class="jf hj">开发并开源。</strong></p><p id="7521" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">理解什么总是很重要的？为什么？又是怎么做到的？对于我们所经历的任何概念。说到这里，让我们试着理解一下MorphNet。</p><h1 id="a630" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">什么是MorphNet？</h1><ul class=""><li id="2c1b" class="kh ki hi jf b jg jh jk jl jo kj js kk jw kl ka km kn ko kp bi translated">MorphNet是一种在训练过程中学习深层网络结构的方法。</li><li id="97ca" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka km kn ko kp bi translated">它将一个工作的神经网络作为输入，产生一个更小、更快、性能更好的优化网络。</li></ul><h1 id="b6c0" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">为什么选择MorphNet？</h1><ul class=""><li id="3a17" class="kh ki hi jf b jg jh jk jl jo kj js kk jw kl ka km kn ko kp bi translated"><strong class="jf hj">目标规则化【MorphNet用于针对特定任务，如针对FLOP(每秒浮点运算数)或内存大小或延迟优化网络。</strong></li><li id="4df4" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka km kn ko kp bi translated">由MorphNet生成的优化网络仍然包含与原始网络相同的层数。这是<strong class="jf hj">拓扑变形</strong>。</li><li id="4f91" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka km kn ko kp bi translated">MorphNet是可扩展的，因为它可以直接应用于任何昂贵的复杂网络。此外，MorphNet不会产生任何推理开销。</li><li id="f490" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka km kn ko kp bi translated">MorphNet是可移植的，因为如果你传送稀疏化的网络就足够了。</li></ul><h1 id="56cb" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">MorphNet是如何工作的？</h1><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es kv"><img src="../Images/f85464a782d33d724ba4a6b33a9b3dfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ri22bVIQdN40XlYdnFK_iQ.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated">MorphNet —算法</figcaption></figure><ul class=""><li id="0a33" class="kh ki hi jf b jg kc jk kd jo ll js lm jw ln ka km kn ko kp bi translated">将待优化的工作神经网络称为种子网络。它可以是<a class="ae kb" href="https://en.wikipedia.org/wiki/Artificial_neural_network" rel="noopener ugc nofollow" target="_blank">人工神经网络</a>、卷积神经网络<a class="ae kb" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">等..</a></li><li id="b6f8" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka km kn ko kp bi translated">MorphNet通过<em class="lo">收缩</em>和<em class="lo">扩展</em>阶段的循环来优化神经网络。这个循环一直持续到修剪后的扩展网络的精度大于或等于种子网络。</li><li id="0e32" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka km kn ko kp bi translated">它首先通过<a class="ae kb" href="https://arxiv.org/pdf/1203.4580.pdf" rel="noopener ugc nofollow" target="_blank">稀疏约束优化</a>对神经网络进行正则化，并通过对修剪后的权重进行恢复和再训练来提高预测精度。</li></ul><h2 id="9b82" class="lp ig hi bd ih lq lr ls il lt lu lv ip jo lw lx it js ly lz ix jw ma mb jb mc bi translated">稀疏化</h2><figure class="kw kx ky kz fd la er es paragraph-image"><div role="button" tabindex="0" class="lb lc di ld bf le"><div class="er es md"><img src="../Images/533e4190e99fadc67cc40cecceb6c50f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*khxYRCr5_a9uBC2Ei72JGw.png"/></div></div><figcaption class="lh li et er es lj lk bd b be z dx translated">MorphNet算法——稀疏化</figcaption></figure><p id="2926" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">MorphNet识别低效神经元，并通过应用稀疏正则化器从网络中剪除它们，使得网络的总损失函数包括每个神经元的成本。我相信你在文章结束时会明白这一点。</p><h2 id="dc88" class="lp ig hi bd ih lq lr ls il lt lu lv ip jo lw lx it js ly lz ix jw ma mb jb mc bi translated">膨胀</h2><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es me"><img src="../Images/9f3f48996bf15195339aa1d76a415546.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*GzrAD5Nvo_W5kTB-twUacQ.png"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">MorphNet算法—扩展</figcaption></figure><ul class=""><li id="21b0" class="kh ki hi jf b jg kc jk kd jo ll js lm jw ln ka km kn ko kp bi translated">在<em class="lo">扩展阶段</em>中，使用一个宽度倍增器来均匀扩展所有层的尺寸。基本上，每一层中神经元及其连接的数量都会增加。</li><li id="468c" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka km kn ko kp bi translated">净效应是将计算资源从效率较低的网络部分重新分配到它们可能更有用的网络部分。</li></ul><h1 id="f91e" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">实施</strong></h1><p id="ec36" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">本节显示的代码段解释了将MorphNet配置到种子网络所需的必要步骤。</p><ul class=""><li id="6e12" class="kh ki hi jf b jg kc jk kd jo ll js lm jw ln ka km kn ko kp bi translated">稀疏约束优化需要稀疏约束。假设FLOP是稀疏性约束，最终目标是获得FLOP优化的神经网络。</li><li id="7020" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka km kn ko kp bi translated">第一步是根据我们的任务选择正则化子。在我们的情况下，它是失败的。MorphNet还提供其他正则项，如延迟、内存大小。</li><li id="ca34" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka km kn ko kp bi translated">然后，正则化器(实际上是稀疏正则化器)用种子网络的输出边界ops和正则化强度(超参数)初始化，正则化强度定义了网络必须被修剪的程度。修剪得越多，网络就越小。较小的网络需要较低的计算能力。</li><li id="add1" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka km kn ko kp bi translated">我们知道神经网络中的损失只不过是神经网络的预测误差。基于损失更新权重，以便尽可能地减少损失。这个正则化器计算相对于目标资源的神经元成本，在我们的例子中是翻牌。该成本被添加到神经网络的损失中，从而成为稀疏约束优化。</li></ul><figure class="kw kx ky kz fd la"><div class="bz dy l di"><div class="mf mg l"/></div></figure><ul class=""><li id="f6ce" class="kh ki hi jf b jg kc jk kd jo ll js lm jw ln ka km kn ko kp bi translated">加入损失后，网络的训练就发生了(稀疏化)。经过训练的网络的准确性可能较低，因为其中的神经元数量较少。这种经过修剪的稀疏网络可以被传送以实现便携性。</li></ul><figure class="kw kx ky kz fd la"><div class="bz dy l di"><div class="mf mg l"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">稀疏化</figcaption></figure><ul class=""><li id="8e66" class="kh ki hi jf b jg kc jk kd jo ll js lm jw ln ka km kn ko kp bi translated">在扩展阶段，使用一个宽度乘数来增加每层中的神经元，以达到更好的精度。正常训练网络就够了。</li></ul><figure class="kw kx ky kz fd la"><div class="bz dy l di"><div class="mf mg l"/></div><figcaption class="lh li et er es lj lk bd b be z dx translated">膨胀</figcaption></figure><h1 id="dfa9" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结果</h1><p id="7c6d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">以下是针对控制预测问题，在<a class="ae kb" href="https://en.wikipedia.org/wiki/Feedforward_neural_network" rel="noopener ugc nofollow" target="_blank">前馈神经网络</a>上集成MorphNet后的结果。</p><p id="cfcb" class="pw-post-body-paragraph jd je hi jf b jg kc ji jj jk kd jm jn jo ke jq jr js kf ju jv jw kg jy jz ka hb bi translated">从下面的图中，可以观察到种子网络和MorphNet优化网络的预测彼此高度重叠。</p><figure class="kw kx ky kz fd la er es paragraph-image"><div class="er es mh"><img src="../Images/2f53e4357ddef1b7b455bcac0711a887.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*dE1L8pF7fwz7lxdoVkVdww.png"/></div></figure><h1 id="3f92" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结论</h1><ol class=""><li id="7119" class="kh ki hi jf b jg jh jk jl jo kj js kk jw kl ka mi kn ko kp bi translated">MorphNet非常有效，因此在精度损失很少甚至没有损失的情况下，显著减少了模型大小/FLOPs。</li><li id="a9ba" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka mi kn ko kp bi translated">MorphNet虽然高效，但应用速度很快，也很容易实现。</li><li id="1094" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka mi kn ko kp bi translated">每个机器学习实践者都可以利用MorphNet来针对各种应用特定的约束优化模型。</li></ol><h1 id="5ea8" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">参考</h1><ul class=""><li id="aea3" class="kh ki hi jf b jg jh jk jl jo kj js kk jw kl ka km kn ko kp bi translated"><a class="ae kb" href="https://arxiv.org/abs/1711.06798" rel="noopener ugc nofollow" target="_blank">阿里尔·戈登、埃拉德·埃班、奥菲尔·纳丘姆、陈博、吴昊、吴昊、爱德华·蔡、<strong class="jf hj"> MorphNet: Fast &amp;深度网络的简单资源受限结构学习</strong>、2018 </a></li><li id="b953" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka km kn ko kp bi translated">【github.com/google-research/morph-net T4】</li><li id="c6aa" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka km kn ko kp bi translated"><a class="ae kb" href="https://ai.googleblog.com/2019/04/morphnet-towards-faster-and-smaller.html" rel="noopener ugc nofollow" target="_blank">ai . Google blog . com/2019/04/morphnet-forward-faster-and-small . html</a></li><li id="06ef" class="kh ki hi jf b jg kq jk kr jo ks js kt jw ku ka km kn ko kp bi translated">www.wikipedia.org</li></ul></div></div>    
</body>
</html>