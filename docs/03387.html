<html>
<head>
<title>Dense or Convolutional Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">密集或卷积神经网络</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/dense-or-convolutional-part-1-c75c59c5b4ad?source=collection_archive---------1-----------------------#2020-01-29">https://medium.com/analytics-vidhya/dense-or-convolutional-part-1-c75c59c5b4ad?source=collection_archive---------1-----------------------#2020-01-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="6d4e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">第1部分——建筑、几何、性能</h2></div><p id="96a2" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当设计深度神经网络(DNN)时，有几个顶级架构选择，其中之一是我应该使用卷积还是密集(又名。感知器，全连接，还是内积)层？</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es jt"><img src="../Images/521da1cb4ee36daa36d4b27e60659210.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gif1twJ_Z1rWFL4WBbvOww.jpeg"/></div></div></figure><p id="1f63" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在这一系列的文章中，我们将从架构、几何和性能的角度回顾这些层类型之间的差异。</p><p id="2f8d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在第一部分中，我们将设计并比较两个网络作为MNIST数字分类器的性能[1]。第一个网络是TensorFlow在教程[2]中提出的仅密集网络。第二个网络是DNN的一个标志，通常与MNIST数据集相关联:LeNet 5卷积神经网络(CNN) [3]。</p></div><div class="ab cl kf kg gp kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="hb hc hd he hf"><h1 id="ddf2" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">建筑评论</h1><p id="d5c8" class="pw-post-body-paragraph ix iy hi iz b ja le ij jc jd lf im jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">卷积层之所以被称为卷积层，是因为它们的工作原理类似于图像/视频处理和任何信号处理(声音、电信等)中使用的卷积滤波器。[4]中对卷积层做了很好的介绍。卷积可以是任何维度，但是在下文中，我们将集中在图像处理中使用的2D卷积，特别是在LeNet-5网络中。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lj"><img src="../Images/d716269d02bb051f61abc4b007b6c4f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-OGHjP3knCXPU12EHL-OA.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">卷积滤波器的等效密度</figcaption></figure><p id="7875" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">从体系结构的角度来看，任何单个卷积都可以由密集层代替，该密集层将对每个像素执行相邻像素的相同关联。这意味着每个像素一个神经元，仅在相邻像素上具有非零系数。卷积层正在实施参数共享:每个像素的处理是通过设计而不是通过学习来实现的。这意味着需要学习的参数数量大幅减少，但性能仍然非常出色，我们将在评估部分看到这一点。</p><p id="37eb" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还有其他与卷积层相关的几何特性，这将在第3部分讨论。</p></div><div class="ab cl kf kg gp kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="hb hc hd he hf"><h1 id="2241" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">模特们</h1><p id="a30b" class="pw-post-body-paragraph ix iy hi iz b ja le ij jc jd lf im jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">为了进行比较，我们将从TensorFlow教程[2]的密集模型以及基于Keras [5]的LeNet-5实现开始。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es lo"><img src="../Images/5b6d4bec1d20a0789ffca1892a5abca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/format:webp/1*VRSdBLOZPFM8P9hMAPrgMQ.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">用于MNIST分类的密集神经网络</figcaption></figure><p id="f5e3" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">密集实现</strong>基于一个大的512单元层，其后是最后一层计算对应于10个数字的10个类别中的每一个类别的softmax概率:</p><pre class="ju jv jw jx fd lp lq lr ls aw lt bi"><span id="1578" class="lu kn hi lq b fi lv lw l lx ly">modelDense0 = models.Sequential([<br/>  layers.Flatten(input_shape=(28, 28)),<br/>  layers.Dense(512, activation=activations.relu),<br/>  layers.Dropout(0.2),<br/>  layers.Dense(10, activation=activations.softmax)<br/>])</span></pre><p id="9a3d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">尺寸为28×28像素的输入图像被转换成展平层中的矢量，给出宽度为784的特征空间。</p><p id="815e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该DNN的系数的数量主要在第一层上，512个神经元全部连接到784个权重，即784*512 + 512 = 401 920个权重来计算，包括偏差。和总共407 050个系数。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es lz"><img src="../Images/0ed1be549f21ca55b9695b74792a6502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2vNmNwAuJ0jskGA4yk-pIg.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">MNIST训练集上密集神经网络的损失和精度</figcaption></figure><p id="14b7" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj"> LeNet-5实现</strong>有更多的层，但没有一层是一样大的:</p><pre class="ju jv jw jx fd lp lq lr ls aw lt bi"><span id="5288" class="lu kn hi lq b fi lv lw l lx ly">modelLeNet0 = models.Sequential([<br/>  layers.Conv2D(filters=6, kernel_size=(3, 3),<br/>    activation=activations.relu, <br/>  layers.AveragePooling2D(),<br/>  layers.Conv2D(filters=16, kernel_size=(3, 3),<br/>    activation=’relu’),<br/>  layers.AveragePooling2D(),</span><span id="45b1" class="lu kn hi lq b fi ma lw l lx ly">  layers.Flatten(),<br/>  layers.Dense(units=120, activation=activations.relu),<br/>  layers.Dense(units=84, activation=activations.relu),<br/>  layers.Dense(units=10, activation =activations.softmax)<br/>])</span></pre><p id="583e" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最初的实现是使用Tanh函数来激活，现在更频繁地使用ReLU，这导致更快的训练和更低的消失梯度概率。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div class="er es mb"><img src="../Images/472947d90e70ce49e417c160f31682d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*7xvZQNkegKHmXlbrlBaQnQ.png"/></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">LeNet5 CNN为MNIST分类</figcaption></figure><p id="4109" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有两个基于具有平均池的3×3滤波器的卷积层。特征空间因此从32×32×3减少到6×6×16。接着是2个隐藏的密集层，分别由120个和84个神经元组成，最后是同样的10个神经元的softmax层，用于计算概率。LeNet-5的系数总数为101 770，是稠密DNN的四分之一。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mc"><img src="../Images/a9fadeb0d15cc8856db2b45b0e8697d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PLjVxLfwDSqkRHglk-JB8Q.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">LeNet5神经网络在MNIST训练集上的损失和精度</figcaption></figure><p id="c52d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在MNIST测量了这两个基线网络的性能:</p><ul class=""><li id="5591" class="md me hi iz b ja jb jd je jg mf jk mg jo mh js mi mj mk ml bi translated">密集DNN，测试准确度= 97.5%</li><li id="7212" class="md me hi iz b ja mm jd mn jg mo jk mp jo mq js mi mj mk ml bi translated">LeNet-5 CNN，测试准确率= 98.5%</li></ul><p id="8f57" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">卷积神经网络在大小和性能方面已经有了明显的优势。唯一的缺点是训练时间较长，给定层数。在90年代，训练需要几天或几周的时间，现在只需要几分钟。</p></div><div class="ab cl kf kg gp kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="hb hc hd he hf"><h1 id="1d97" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">模型优化</h1><p id="d6b7" class="pw-post-body-paragraph ix iy hi iz b ja le ij jc jd lf im jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">上面评估的两个网络过度拟合，并且在对新样本进行测试时性能下降。这是可以观察到的:</p><ul class=""><li id="322c" class="md me hi iz b ja jb jd je jg mf jk mg jo mh js mi mj mk ml bi translated">训练和验证评估之间在损失和准确性上有很大差距</li><li id="5ad7" class="md me hi iz b ja mm jd mn jg mo jk mp jo mq js mi mj mk ml bi translated">在最初的急剧下降之后，验证损失随着训练时期而恶化</li></ul><p id="cf9b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这并不意外，因为训练样本的数量是60 000，这小于要训练的系数的数量。网络正在学习和记忆训练样本。</p><p id="1bd5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在下文中，我们将优化这两个网络，增加正则化并寻找最佳的大小-性能权衡。</p><h1 id="c8ed" class="km kn hi bd ko kp mr kr ks kt ms kv kw io mt ip ky ir mu is la iu mv iv lc ld bi translated">正规化</h1><p id="033b" class="pw-post-body-paragraph ix iy hi iz b ja le ij jc jd lf im jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">正则化是一组在训练期间加速收敛并避免过度拟合的技术。有几个正规化的家族:</p><p id="ffca" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">惩罚</strong>:将惩罚项添加到梯度反向传播中，以便将系数“推”向0。经典的惩罚是套索(基于L1范数)和脊(基于L2范数)，还有许多其他的改变使用的范数，还有结合套索和脊的弹性网[7]。</p><p id="b826" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">提前停止</strong>:假设过拟合发生在网络学习训练样本的时候，并且可以通过验证期间的性能下降观察到，当检测到这种拐点时，训练停止。</p><p id="9731" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">丢失</strong>【6】:对于每一批，输出的随机部分被无效，以避免相邻层部分之间的强依赖性。这种技术类似于决策树上的提升技术。</p><p id="a107" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">数据扩充</strong>:通过几何变换(平移、缩放、旋转……)或过滤(模糊)，从现有样本中创建更多的训练样本。</p><p id="3c32" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">模型尺寸缩减</strong>倾斜系数数量与训练样本数量的比值。</p><p id="c02a" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在上面的密集模型中，两个密集层之间已经有一个落差。鉴于观察到的过拟合，我们应用了原始压差论文[6]的建议:输入压差为20%，两层之间压差为50%。根据下面的损耗和精度曲线观察，过拟合要低得多，密集网络的性能现在为98.5%，高达LeNet5！</p><p id="fad5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在LeNet5网络上，我们还研究了正则化的影响。在1990年代创建时，基于处罚的正规化是一个热门话题。但是，直到2016年才知道退学。使用网格搜索，我们测量并调整了弹性网(L1-L2组合)和漏失的正则化参数。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mw"><img src="../Images/50405907d7af7136e72eec0b073a5dfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zZgmWGPSUHpK-VcgRqxMpA.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">LeNet5具有可调压差，可实现99.4%的精度</figcaption></figure><p id="efff" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们发现最佳的一组参数是:</p><ul class=""><li id="1856" class="md me hi iz b ja jb jd je jg mf jk mg jo mh js mi mj mk ml bi translated">对于惩罚:参数λ= 10–5的第一密集层上的L2正则化，导致99.15%的测试精度</li><li id="d022" class="md me hi iz b ja mm jd mn jg mo jk mp jo mq js mi mj mk ml bi translated">对于dropout: dropout应用于前两个密集层的输入，参数为40%和30%，导致<strong class="iz hj">测试精度为99.4% </strong></li></ul><p id="7096" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">Dropout的性能更好，也更容易调整。</p><h1 id="63de" class="km kn hi bd ko kp mr kr ks kt ms kv kw io mt ip ky ir mu is la iu mv iv lc ld bi translated">模型尺寸优化</h1><p id="2f51" class="pw-post-body-paragraph ix iy hi iz b ja le ij jc jd lf im jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">由于我们需要比较密集网络和卷积网络，因此使用最大的网络毫无意义。事实上，对任何CNN都有一个基于密集架构的等价物。在[6]中，在具有2048个单位的两个致密层的MNIST上报道了一些结果，准确度在99%以上。只看业绩不会导致公平的比较。</p><figure class="ju jv jw jx fd jy er es paragraph-image"><div role="button" tabindex="0" class="jz ka di kb bf kc"><div class="er es mx"><img src="../Images/494312de7106b531c51bc1dd760cb064.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ljy1wbwUNIBf-EgNzNXIJQ.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">作为图层#0大小的函数的密集DNN精度</figcaption></figure><p id="d4e8" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">您还可能有一些额外的要求来优化处理时间或成本。</p><p id="901b" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如上所述，减小网络大小也减少了过拟合。</p><p id="0415" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这就是为什么我们一直在寻找两个正则化网络的最佳性能-规模权衡。以下是我们的结果:</p><ul class=""><li id="e868" class="md me hi iz b ja jb jd je jg mf jk mg jo mh js mi mj mk ml bi translated"><strong class="iz hj">密网有漏失</strong>，有128个隐层单位，即<strong class="iz hj"> 101个770系数，测试准确率98% </strong></li><li id="d14c" class="md me hi iz b ja mm jd mn jg mo jk mp jo mq js mi mj mk ml bi translated"><strong class="iz hj"/>lenet 5网络，具有60和42个单元的密集隐层(初始的一半)，<strong class="iz hj"> 38 552个系数，测试精度99.2% </strong></li></ul><p id="d901" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">CNN是明显的赢家，它只用1/3的系数就能表现得更好</p></div><div class="ab cl kf kg gp kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="hb hc hd he hf"><h1 id="a02b" class="km kn hi bd ko kp kq kr ks kt ku kv kw io kx ip ky ir kz is la iu lb iv lc ld bi translated">结论</h1><p id="6919" class="pw-post-body-paragraph ix iy hi iz b ja le ij jc jd lf im jf jg lg ji jj jk lh jm jn jo li jq jr js hb bi translated">在本帖中，我们解释了基于密集的神经网络和具有卷积层的网络在架构上的共性和差异。我们已经证明，后者总是表现出色，并且系数数量较少。</p><p id="15ed" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们还展示了给定一些互联网上可用的模型，评估这些模型并对它们进行调整总是一个好主意。通过这一过程，您将验证所选模型是否符合您的实际需求，更好地了解其架构和行为，并且您可以应用一些设计时没有的新技术，例如LeNet5上的Dropout。</p><p id="f6d5" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个调查的代码和细节可以在<strong class="iz hj">笔记本</strong>(<a class="ae my" href="https://tonio73.github.io/data-science/cnn/CnnVsDense-Part1.html" rel="noopener ugc nofollow" target="_blank">HTML</a>/<a class="ae my" href="https://nbviewer.jupyter.org/urls/tonio73.github.io/data-science/cnn/CnnVsDense-Part1.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter</a>)【8】中找到。</p><p id="c9fc" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在下一部分中，我们将继续我们的比较，在第二部分中查看内部层的可视化，在第三部分中查看每个网络对几何变换的鲁棒性。</p><p id="c9f0" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">不要忘记在下面留下评论/反馈。你现在可以鼓掌了，然后继续讨论关于可解释性的第2部分。</p><p id="d7de" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">参考资料:</p><ol class=""><li id="a64e" class="md me hi iz b ja jb jd je jg mf jk mg jo mh js mz mj mk ml bi translated">Yann Lecun等人的MNIST数据集——http://yann.lecun.com/exdb/mnist/<a class="ae my" href="http://yann.lecun.com/exdb/mnist/" rel="noopener ugc nofollow" target="_blank"/></li><li id="43a8" class="md me hi iz b ja mm jd mn jg mo jk mp jo mq js mz mj mk ml bi translated">TensorFlow教程—<a class="ae my" href="https://www.tensorflow.org/tensorboard/get_started" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tensorboard/get_started</a>MNIST分类器的密集实现</li><li id="2101" class="md me hi iz b ja mm jd mn jg mo jk mp jo mq js mz mj mk ml bi translated">基于梯度的学习应用于文档识别，Lecun等人。<a class="ae my" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf" rel="noopener ugc nofollow" target="_blank">http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf</a></li><li id="df60" class="md me hi iz b ja mm jd mn jg mo jk mp jo mq js mz mj mk ml bi translated">卷积神经网络(CNN)初学者指南，suh yun Kim—<a class="ae my" href="https://towardsdatascience.com/a-beginners-guide-to-convolutional-neural-networks-cnns-14649dbddce8" rel="noopener" target="_blank">https://towardsdatascience . com/a—卷积神经网络初学者指南—CNNs-14649 dbddce 8</a></li><li id="de07" class="md me hi iz b ja mm jd mn jg mo jk mp jo mq js mz mj mk ml bi translated">LeNet实现tensor flow Keras—<a class="ae my" href="https://colab.research.google.com/drive/1CVm50PGE4vhtB5I_a_yc4h5F-itKOVL9" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/drive/1 CVM 50 PGE 4 vht b5 I _ a _ YC 4 H5 f-it kovl 9</a></li><li id="0015" class="md me hi iz b ja mm jd mn jg mo jk mp jo mq js mz mj mk ml bi translated">辍学:防止神经网络过度拟合的简单方法，Nitish Srivastava等人。<a class="ae my" href="http://jmlr.org/papers/v15/srivastava14a.html" rel="noopener ugc nofollow" target="_blank">http://jmlr.org/papers/v15/srivastava14a.html</a></li><li id="6146" class="md me hi iz b ja mm jd mn jg mo jk mp jo mq js mz mj mk ml bi translated">通过弹性网的正则化和变量选择，https://citeseerx.ist.psu.edu/viewdoc/summary?，邹慧和特雷弗·哈斯蒂doi=10.1.1.124.4696 </li></ol></div></div>    
</body>
</html>