<html>
<head>
<title>A basic introduction to Long Short-Term Memory Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">长短期记忆网络的基本介绍</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-basic-introduction-to-long-short-term-memory-networks-e408c00b3fc5?source=collection_archive---------29-----------------------#2020-04-07">https://medium.com/analytics-vidhya/a-basic-introduction-to-long-short-term-memory-networks-e408c00b3fc5?source=collection_archive---------29-----------------------#2020-04-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="5a00" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">“你永远无法通过对抗现有的现实来改变事情。要改变什么，就建立一个新的模型，让现有的模型过时。”<br/> ― <strong class="il hj">巴克明斯特富勒</strong></p></blockquote><p id="a4dd" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">在本系列的前一篇文章中，我们讨论了构成消失和爆炸渐变的RNNs的缺点。这些都可以通过长短期记忆(LSTM)网络来克服。</p><p id="de4b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">LSTM网络是特殊的RNN细胞，专门设计用于学习长期依赖性，而这是rnn所缺乏的。受生物大脑记忆长期依赖关系能力的启发，LSTM网络试图将其纳入人工网络。为了实现这一点，它在由四个门表示的每个时间步长传递一个单元状态。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div role="button" tabindex="0" class="jq jr di js bf jt"><div class="er es jk"><img src="../Images/afa865003e63d90b8ef5c831527b0f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*50pXHyqC0uX2zg53QfnJyg.png"/></div></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">LSTM建筑概述[1]</figcaption></figure><p id="ec8c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">第一个门是遗忘门，它决定下一步要传递的信息。它是通过与旧单元状态逐元素相乘并进一步应用sigmoid激活来计算的。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es ka"><img src="../Images/2bf8691009d668bccfca74a00a864df3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1164/format:webp/1*11eSyj6CwJNABORs9JPKBw.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">遗忘门f的数学表示</figcaption></figure><p id="2c82" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">输入门和缩放的新候选门共同确定将为下一状态保留的信息。将输入门(sigmoid函数)乘以新的候选门(tanh函数),以创建临时张量、缩放的新候选。这种方法有助于防止渐变消失的问题。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es kb"><img src="../Images/9a18385706b2aa6c52276e2031295a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*ecsyCculFey6dHZ96PwNXw.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">输入门I和新候选门g的数学表示</figcaption></figure><p id="c239" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">通过将缩放的新候选单元状态和缩放的旧单元状态相加来计算新单元状态，以便放大数据中的重要特征。最后一个门是输出门，它是一个sigmoid函数。输出生成过程包括通过双曲正切函数传递新的单元状态，并用输出门执行逐元素乘法以获得隐藏状态。这通过sigmoid和Softmax函数的非线性组合来产生最终输出[2]。</p><figure class="jl jm jn jo fd jp er es paragraph-image"><div class="er es kc"><img src="../Images/cf5327f6f45f7514af46f89c9455ec53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*VSEwkwiJB5EoYrI-xGUvAg.png"/></div><figcaption class="jw jx et er es jy jz bd b be z dx translated">输出门o的数学表示</figcaption></figure><p id="11fe" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">我们的人工神经网络系列到此结束。如果你能坚持到这里，谢谢你支持我的工作。我也会在其他领域写内容，所以请保持关注！</p></div><div class="ab cl kd ke gp kf" role="separator"><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki kj"/><span class="kg bw bk kh ki"/></div><div class="hb hc hd he hf"><p id="a88f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">[1] Kostadinov，S. &amp; Safari，一家奥莱利传媒公司，2018年。递归神经网络与Python快速入门指南第1版。，Packt出版公司。</p><p id="2de2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jh iv iw ix ji iz ja jb jj jd je jf jg hb bi translated">[2]朱利安博士，2018。用Pytorch进行深度学习快速入门指南:学习用Python训练和部署神经网络模型，伯明翰:Packt。</p></div></div>    
</body>
</html>