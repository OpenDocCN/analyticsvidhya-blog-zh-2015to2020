<html>
<head>
<title>Ensemble Learning : Simple Techniques Implemented On Image Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集成学习:在图像数据上实现的简单技术</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ensemble-learning-simple-techniques-implemented-on-image-data-4885797e12a2?source=collection_archive---------27-----------------------#2020-04-10">https://medium.com/analytics-vidhya/ensemble-learning-simple-techniques-implemented-on-image-data-4885797e12a2?source=collection_archive---------27-----------------------#2020-04-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/fb0013142eef60be5bde76003522c33d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*sSSHJeUE2WHp3xD35NoJ9w.png"/></div></figure><p id="f26a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">机器学习中的集成模型结合了来自多个模型的决策，以提高整体性能。这可以通过多种方式实现。这里我将实现两个简单的方法(对图像数据):</p><ol class=""><li id="dead" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jp jq jr js bi translated"><strong class="io hj">平均:</strong>使用多个模型对每个数据点进行预测。所有模型预测的平均值用于进行最终预测</li><li id="2c1e" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated"><strong class="io hj"> Max Voting: </strong>使用多个模型对每个数据点进行预测。每个模型的预测被认为是一次“投票”。我们从大多数模型中得到的预测被用作最终预测。</li></ol><p id="73c7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">在MNIST数据上的实现(python 3.6.9，keras 2.2.4) </strong></p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="982a" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#CNN models</strong></span><span id="d793" class="kh ki hi kd b fi kn kk l kl km">from keras.callbacks import ModelCheckpoint<br/>from keras.datasets import mnist<br/>from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Activation, Average<br/>from keras.losses import categorical_crossentropy<br/>from keras.models import Model, Input<br/>from keras.optimizers import Adam<br/>from keras.utils import to_categorical</span><span id="b4fa" class="kh ki hi kd b fi kn kk l kl km">from tensorflow.python.framework.ops import Tensor<br/>from scipy.stats import mode<br/>from typing import List<br/>import glob<br/>import numpy as np<br/>import os</span><span id="d237" class="kh ki hi kd b fi kn kk l kl km"># data processing<br/>def load_data():<br/>    <br/>    (x_train, y_train), (x_test, y_test) = mnist.load_data()<br/>    x_train = x_train / 255.<br/>    x_test = x_test / 255.<br/>    y_train = to_categorical(y_train, num_classes=10)<br/>    return x_train, x_test, y_train, y_test</span><span id="8a17" class="kh ki hi kd b fi kn kk l kl km">x_train, x_test, y_train, y_test = load_data()<br/>x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))<br/>x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))<br/>input_shape = x_train[0].shape<br/>model_input = Input(shape=input_shape)</span><span id="c564" class="kh ki hi kd b fi kn kk l kl km"># models(3) building<br/>def first(model_input: Tensor):<br/>    <br/>    x = Conv2D(96, kernel_size=(3, 3), activation='relu', padding = 'same')(model_input)<br/>    x = Conv2D(96, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(96, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = MaxPooling2D(pool_size=(3, 3), strides = 2)(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = MaxPooling2D(pool_size=(3, 3), strides = 2)(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(192, (1, 1), activation='relu')(x)<br/>    x = Conv2D(10, (1, 1))(x)<br/>    x = GlobalAveragePooling2D()(x)<br/>    x = Activation(activation='softmax')(x)<br/>    <br/>    model = Model(model_input, x, name='first')<br/>    return model</span><span id="9775" class="kh ki hi kd b fi kn kk l kl km">def second(model_input: Tensor):<br/>    <br/>    x = Conv2D(96, kernel_size=(3, 3), activation='relu', padding = 'same')(model_input)<br/>    x = Conv2D(96, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(96, (3, 3), activation='relu', padding = 'same', strides = 2)(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same', strides = 2)(x)<br/>    x = Conv2D(192, (3, 3), activation='relu', padding = 'same')(x)<br/>    x = Conv2D(192, (1, 1), activation='relu')(x)<br/>    x = Conv2D(10, (1, 1))(x)<br/>    x = GlobalAveragePooling2D()(x)<br/>    x = Activation(activation='softmax')(x)<br/>        <br/>    model = Model(model_input, x, name='second')<br/>    return model</span><span id="bc8d" class="kh ki hi kd b fi kn kk l kl km">def third(model_input: Tensor):<br/>    <br/>    #mlpconv block 1<br/>    x = Conv2D(32, (5, 5), activation='relu',padding='valid')(model_input)<br/>    x = Conv2D(32, (1, 1), activation='relu')(x)<br/>    x = Conv2D(32, (1, 1), activation='relu')(x)<br/>    x = MaxPooling2D((2,2))(x)<br/>    x = Dropout(0.5)(x)<br/>    <br/>    #mlpconv block2<br/>    x = Conv2D(64, (3, 3), activation='relu',padding='valid')(x)<br/>    x = Conv2D(64, (1, 1), activation='relu')(x)<br/>    x = Conv2D(64, (1, 1), activation='relu')(x)<br/>    x = MaxPooling2D((2,2))(x)<br/>    x = Dropout(0.5)(x)<br/>    <br/>    #mlpconv block3<br/>    x = Conv2D(128, (3, 3), activation='relu',padding='valid')(x)<br/>    x = Conv2D(32, (1, 1), activation='relu')(x)<br/>    x = Conv2D(10, (1, 1))(x)<br/>    <br/>    x = GlobalAveragePooling2D()(x)<br/>    x = Activation(activation='softmax')(x)<br/>    <br/>    model = Model(model_input, x, name='third')<br/>    return model</span><span id="14db" class="kh ki hi kd b fi kn kk l kl km">first_model = first(model_input)<br/>second_model = second(model_input)<br/>third_model = third(model_input)</span><span id="950c" class="kh ki hi kd b fi kn kk l kl km"># models compilation &amp; training<br/>def compile_and_train(model: Model, num_epochs: int): <br/>    <br/>    model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['acc']) <br/>    filepath = 'weights/' + model.name + '.hdf5'<br/>    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_weights_only=True,<br/>                                                 save_best_only=True, mode='auto', period=1)<br/>    history = model.fit(x=x_train, y=y_train, batch_size=32, <br/>                     epochs=num_epochs, verbose=1, callbacks=[checkpoint], validation_split=0.2)<br/>    return filepath</span><span id="29f6" class="kh ki hi kd b fi kn kk l kl km">NUM_EPOCHS = 5<br/>first_weight_file = compile_and_train(first_model, NUM_EPOCHS)<br/>second_weight_file = compile_and_train(second_model, NUM_EPOCHS)<br/>third_weight_file = compile_and_train(third_model, NUM_EPOCHS)</span><span id="813a" class="kh ki hi kd b fi kn kk l kl km"># models evaluation<br/>def evaluate_error(model: Model):<br/>    pred = model.predict(x_test, batch_size = 32)<br/>    pred = np.argmax(pred, axis=1)<br/>    error = np.sum(np.not_equal(pred, y_test))/ y_test.shape[0]  <br/>    return error</span><span id="2ef0" class="kh ki hi kd b fi kn kk l kl km">e1=evaluate_error(first_model); print(e1)<br/>e2=evaluate_error(second_model); print(e2)<br/>e3=evaluate_error(third_model); print(e3)</span></pre><blockquote class="ko kp kq"><p id="2897" class="im in kr io b ip iq ir is it iu iv iw ks iy iz ja kt jc jd je ku jg jh ji jj hb bi translated"><em class="hi">输出错误:</em></p><p id="4f20" class="im in kr io b ip iq ir is it iu iv iw ks iy iz ja kt jc jd je ku jg jh ji jj hb bi translated"><em class="hi">0.0083<br/>0.0112<br/>0.0113</em></p></blockquote><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="c1d6" class="kh ki hi kd b fi kj kk l kl km"><strong class="kd hj">#Ensemble models</strong></span><span id="86c8" class="kh ki hi kd b fi kn kk l kl km">all_models = [first_model, second_model, third_model]<br/>first_model.load_weights(first_weight_file)<br/>second_model.load_weights(second_weight_file)<br/>third_model.load_weights(third_weight_file)</span><span id="95ae" class="kh ki hi kd b fi kn kk l kl km">def ensemble_average(models: List [Model]): # averaging<br/>    <br/>    outputs = [model.outputs[0] for model in all_models]<br/>    y = Average()(outputs)<br/>    <br/>    model = Model(model_input, y, name='ensemble_average')<br/>    E = evaluate_error(model)<br/>    return E</span><span id="3a63" class="kh ki hi kd b fi kn kk l kl km">def ensemble_vote(models: List [Model]): # max-voting<br/>    <br/>    pred = []<br/>    yhats = [model.predict(x_test) for model in all_models]<br/>    yhats = np.argmax(yhats, axis=2)<br/>    yhats = np.array(yhats)<br/>    #print(yhats.shape)<br/>    for i in range(0,len(x_test)):<br/>        m = mode([yhats[0][i], yhats[1][i], yhats[2][i]])<br/>        pred = np.append(pred, m[0])<br/>    E = np.sum(np.not_equal(pred, y_test))/ y_test.shape[0]  <br/>    return E</span><span id="edf7" class="kh ki hi kd b fi kn kk l kl km">E1 = ensemble_average(all_models); print(E1)<br/>E2 = ensemble_vote(all_models); print(E2)</span></pre><blockquote class="ko kp kq"><p id="88dd" class="im in kr io b ip iq ir is it iu iv iw ks iy iz ja kt jc jd je ku jg jh ji jj hb bi translated"><em class="hi">输出错误:</em></p><p id="886b" class="im in kr io b ip iq ir is it iu iv iw ks iy iz ja kt jc jd je ku jg jh ji jj hb bi translated"><em class="hi"> 0.0061 <br/> 0.0068 </em></p></blockquote><p id="2a30" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">显然，集成学习在这里给出了更好的准确性。</p></div><div class="ab cl kv kw gp kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="hb hc hd he hf"><p id="8b43" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">参考资料:</p><ol class=""><li id="c2ef" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jp jq jr js bi translated"><a class="ae lc" href="https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/" rel="noopener ugc nofollow" target="_blank">https://www . analyticsvidhya . com/blog/2018/06/comprehensive-guide-for-ensemble-models/</a></li><li id="0af9" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated"><a class="ae lc" href="https://towardsdatascience.com/ensembling-convnets-using-keras-237d429157eb" rel="noopener" target="_blank">https://towards data science . com/ensembling-conv nets-using-keras-237d 429157 EB</a></li><li id="09f2" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated"><a class="ae lc" href="https://machinelearningmastery.com/horizontal-voting-ensemble/" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/horizontal-voting-ensemble/</a></li></ol></div></div>    
</body>
</html>