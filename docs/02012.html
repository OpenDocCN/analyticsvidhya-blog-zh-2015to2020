<html>
<head>
<title>Demystifying Deep Deterministic Policy Gradient (DDPG) and it’s implementation in ChainerRL and OpenAI-baselines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭秘深度确定性策略梯度(DDPG)及其在ChainerRL和OpenAI-baselines中的实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/demystifying-deep-deterministic-policy-gradient-ddpg-uding-chainerrl-and-openai-baselines-a087a11630cc?source=collection_archive---------5-----------------------#2019-11-26">https://medium.com/analytics-vidhya/demystifying-deep-deterministic-policy-gradient-ddpg-uding-chainerrl-and-openai-baselines-a087a11630cc?source=collection_archive---------5-----------------------#2019-11-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="b261" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">对流行的强化学习技术DDPG及其使用ChainerRL和Tensorflow轻松实现的深入解释。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/4555113c58ad4ab25bd7e1bb64445597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8smoa6HCH3oMCl7afigSkg.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://wallpaperplay.com/walls/full/7/9/f/90463.jpg" rel="noopener ugc nofollow" target="_blank">强化学习的根源</a></figcaption></figure><p id="cdda" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">深度确定性策略梯度或通常称为DDPG基本上是一种非策略方法，它学习Q函数和策略来迭代动作。它使用非策略数据和贝尔曼方程来学习Q函数，该Q函数反过来用于导出和学习策略。</p><h1 id="1dc4" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">学习过程:</h1><p id="6df2" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">学习过程与Q-learning密切相关，如果您知道最佳行动值函数<strong class="jq hj"> Q*(s，a) </strong>，则在该状态下采取的最佳行动可以使用a*(s)找到，即:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lh"><img src="../Images/730da48081f15b16e426c44ab4efcd58.png" data-original-src="https://miro.medium.com/v2/resize:fit:544/format:webp/1*BJtcN0buUO7s7QY-ePdZEQ.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">最佳行动估计</figcaption></figure><h1 id="4844" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">连续动作空间推导；</h1><p id="ff93" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">DDPG是专门为处理具有连续动作空间的环境而开发的，本质上就是估计<strong class="jq hj"> <em class="li"> max Q*(s，a)中的最大过度动作。</em> </strong></p><ol class=""><li id="a536" class="lj lk hi jq b jr js ju jv jx ll kb lm kf ln kj lo lp lq lr bi translated">在离散动作空间的情况下，可以分别估计每个动作的Q值，因此容易比较最佳最大值。</li><li id="c461" class="lj lk hi jq b jr ls ju lt jx lu kb lv kf lw kj lo lp lq lr bi translated">在连续动作空间的情况下，对于每个Q值的计算和单独比较变得非常穷尽，导致不稳定的目标值和不稳定的学习。更不用说，这样的过程是非常详尽和计算昂贵的。</li></ol><p id="17b6" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">基于Q-learning的算法，特别是DDPG使用以下方法来处理连续动作空间:</p><ul class=""><li id="ec39" class="lj lk hi jq b jr js ju jv jx ll kb lm kf ln kj lx lp lq lr bi translated">利用<strong class="jq hj">贝尔曼方程</strong>通过状态-动作/Q值获得给定状态的最佳动作。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/a4c971da5f5643e3b67a978e141c8919.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*rhyYvvCRL7Iwsze2kYocKg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">贝尔曼方程</figcaption></figure><blockquote class="lz ma mb"><p id="e9b0" class="jo jp li jq b jr js ij jt ju jv im jw mc jy jz ka md kc kd ke me kg kh ki kj hb bi translated">在等式<em class="hi">中，s’~ P</em>指的是从环境中通过概率分布<em class="hi"> P(.|s，a)。</em></p></blockquote><ul class=""><li id="e61e" class="lj lk hi jq b jr js ju jv jx ll kb lm kf ln kj lx lp lq lr bi translated">DDPG使用<strong class="jq hj">均方贝尔曼误差(MSBE) </strong>函数来估计Q*接近满足贝尔曼方程的程度，如下式所示:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mf"><img src="../Images/c4b49d0c2d168d7ec16c40236bfba27e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1wInrRDtWm9pmP_CjSZijw.png"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">均方贝尔曼误差值方程</figcaption></figure><ul class=""><li id="b5b9" class="lj lk hi jq b jr js ju jv jx ll kb lm kf ln kj lx lp lq lr bi translated">利用<strong class="jq hj">经验重放缓冲器</strong>，它是一组先前的经验，有助于为基于Q学习的近似器提供稳定的学习行为</li><li id="3300" class="lj lk hi jq b jr ls ju lt jx lu kb lv kf lw kj lx lp lq lr bi translated">DDPG还部署使用T <strong class="jq hj">目标网络</strong>来处理不稳定的目标值，使学习更加稳定。下面描述什么是<strong class="jq hj">目标</strong>，因为当我们最小化MSBE损失时，我们试图使Q函数更像这个目标。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mg"><img src="../Images/3ab5f49040e402472fd6f5d8250e05d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*-pNj_aRdCNTPF-iPEKhG2Q.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">目标值</figcaption></figure><ul class=""><li id="6087" class="lj lk hi jq b jr js ju jv jx ll kb lm kf ln kj lx lp lq lr bi translated">DDPG的目标网络是从主网络复制过来的，每更新一次主网络，就更新一次固定步数，通过Polyak平均:</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mh"><img src="../Images/c953b0cb22ab2744a0fc8b26e86428fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*Yhbn8-XrQzK4_-2yfRm6yg.png"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">目标网络更新— Polyak平均</figcaption></figure><p id="6386" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，DDPG通过使用<strong class="jq hj">目标策略网络</strong>来计算近似最大化<strong class="jq hj"> Q*(目标)的动作，来处理这种巨大的连续动作空间挑战和昂贵的计算。</strong></p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mi"><img src="../Images/536e64b68777ac6adf5a96c3ec1ad586.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/1*w_ajzIeIVhqec1c0wCerlQ.gif"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.salesforceblogger.com%2F2017%2F11%2F10%2Fdreamforce-2017-takeaways-on-einstein-analytics%2F&amp;psig=AOvVaw2KBvxZxSqHXcfIdi67LYau&amp;ust=1574756455707000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCJD096T3hOYCFQAAAAAdAAAAABAd" rel="noopener ugc nofollow" target="_blank">记下一些要点的时间</a></figcaption></figure><h1 id="c46b" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated"><strong class="ak">关键要点:</strong></h1><ul class=""><li id="8eb3" class="lj lk hi jq b jr lc ju ld jx mj kb mk kf ml kj lx lp lq lr bi translated">DDPG的q学习是通过用随机梯度下降最小化以下MSBE损失来执行的。</li><li id="d2c6" class="lj lk hi jq b jr ls ju lt jx lu kb lv kf lw kj lx lp lq lr bi translated">DDPG本质上是与政策无关的学习方法</li><li id="a5b2" class="lj lk hi jq b jr ls ju lt jx lu kb lv kf lw kj lx lp lq lr bi translated">它基本上是连续动作空间的Q学习。</li><li id="f676" class="lj lk hi jq b jr ls ju lt jx lu kb lv kf lw kj lx lp lq lr bi translated">它使用带有经验重放的目标网络进行稳定高效的学习。</li></ul><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mm"><img src="../Images/889f7c6f7e026ea59725da5a956e2722.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*IWWDXcgArwXEV3MtokJycw.gif"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://media.giphy.com/media/NvCueB7PUf840/giphy.gif" rel="noopener ugc nofollow" target="_blank">什么都明白了？如果不是，后退几步也无妨</a></figcaption></figure><h1 id="becb" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">实施:</h1><p id="659a" class="pw-post-body-paragraph jo jp hi jq b jr lc ij jt ju ld im jw jx le jz ka kb lf kd ke kf lg kh ki kj hb bi translated">我们将使用两个框架来实现DDPG:</p><ul class=""><li id="0368" class="lj lk hi jq b jr js ju jv jx ll kb lm kf ln kj lx lp lq lr bi translated">ChainerRL—<a class="ae jn" href="https://github.com/chainer/chainerrl" rel="noopener ugc nofollow" target="_blank"><em class="li">https://github.com/chainer/chainerrl</em></a></li><li id="f5e2" class="lj lk hi jq b jr ls ju lt jx lu kb lv kf lw kj lx lp lq lr bi translated">Tensorflow /OpenAi基线—<a class="ae jn" href="https://github.com/openai/baselines" rel="noopener ugc nofollow" target="_blank"><em class="li">https://github.com/openai/baselines</em></a></li></ul><blockquote class="lz ma mb"><p id="5fdd" class="jo jp li jq b jr js ij jt ju jv im jw mc jy jz ka md kc kd ke me kg kh ki kj hb bi translated">ChainerRL</p></blockquote><p id="eaa7" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">Chainer是一个新开发的基于DL的框架，它的特点是速度非常快，可以在Cupy(可能是GPU使用的numpy的更快版本)上运行，并支持GPU的并行化。通过以下方式直接安装chainerRL</p><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="6980" class="ms kl hi mo b fi mt mu l mv mw">pip install chainerrl</span></pre><p id="3ed9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在安装设置之后，RL算法可以直接与测试脚本一起运行。在这里，我们将使用下面给出的train_ddpg脚本运行ddpg</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mx my l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://gist.github.com/Ujwal2910/890afa79e7fbeaa7ce9cd2f5ae08d216" rel="noopener ugc nofollow" target="_blank">mujoco DDPG培训代码—链接</a></figcaption></figure><p id="4b07" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">或者你可以直接从这里使用训练代码- <a class="ae jn" href="https://github.com/chainer/chainerrl/blob/master/examples/mujoco/train_ddpg_gym.py" rel="noopener ugc nofollow" target="_blank"> train_ddpg.py </a>。</p><figure class="iy iz ja jb fd jc"><div class="bz dy l di"><div class="mx my l"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">定制环境的make_env函数</figcaption></figure><p id="495a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在该脚本中，您可以修改make_env()函数以适应您自己选择的环境，或者您也可以添加创建的环境文件，方法是直接导入它们，然后在函数本身中调用它们。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mz"><img src="../Images/46b8dec1656b74b88ea2a90ccca2d944.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*S3UZzw-mqDOWO0pizLb66w.gif"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated"><a class="ae jn" href="https://www.google.com/search?q=code+is+running+joy+gif&amp;tbm=isch&amp;ved=2ahUKEwjE-4ThgoXmAhXqi0sFHWorDYgQ2-cCegQIABAA&amp;oq=code+is+running+joy+gif&amp;gs_l=img.3...7568.8300..8658...0.0..0.424.1155.3-2j1......0....1..gws-wiz-img.8PEGyfrddso&amp;ei=JpvbXcTQAuqXrtoP6ta0wAg&amp;bih=890&amp;biw=1745&amp;rlz=1C1GCEV_enIN863IN863#imgrc=bJbMW1LuwC4zDM" rel="noopener ugc nofollow" target="_blank">理解和看到运行代码的快乐</a></figcaption></figure><blockquote class="lz ma mb"><p id="8a12" class="jo jp li jq b jr js ij jt ju jv im jw mc jy jz ka md kc kd ke me kg kh ki kj hb bi translated">Tensorflow /OpenAi基线</p></blockquote><p id="2ceb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">从OpenAI基线运行训练设置也相对容易和简单。首先克隆并安装TensorFlow，然后是基线包-</p><pre class="iy iz ja jb fd mn mo mp mq aw mr bi"><span id="b603" class="ms kl hi mo b fi mt mu l mv mw">git clone <a class="ae jn" href="https://github.com/openai/baselines.git" rel="noopener ugc nofollow" target="_blank">https://github.com/openai/baselines.git</a> #clone the repo<br/>cd baselines #change directory</span><span id="b5e7" class="ms kl hi mo b fi na mu l mv mw">pip install tensorflow-gpu==1.14 # if you have a CUDA-compatible gpu and proper drivers</span><span id="5d3e" class="ms kl hi mo b fi na mu l mv mw">pip install -e . # for baseline installation</span></pre><p id="0861" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">要运行DDPG，请使用以下命令—</p><blockquote class="lz ma mb"><p id="564e" class="jo jp li jq b jr js ij jt ju jv im jw mc jy jz ka md kc kd ke me kg kh ki kj hb bi translated">python-m baselines . run-alg = ddpg-env = half cheetah-v2-num _ time steps = 1e 6</p></blockquote><p id="5376" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在Mujoco环境中运行1M帧= 10M时间步长的算法。更多选项见帮助(<code class="du nb nc nd mo b">-h</code>)。欲了解更多信息，请点击此<a class="ae jn" href="https://github.com/openai/baselines/tree/master/baselines/ddpg" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ne"><img src="../Images/69e08eb933e3dafc121ec1016ebd2798.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/1*01iNFgf3y-Nix4Ew1_OAPQ.gif"/></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">你做得很好。现在运行一些RL算法</figcaption></figure><h1 id="e96d" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated"><strong class="ak">参考文献:</strong></h1><ul class=""><li id="6ad3" class="lj lk hi jq b jr lc ju ld jx mj kb mk kf ml kj lx lp lq lr bi translated"><a class="ae jn" href="https://spinningup.openai.com/en/latest/algorithms/ddpg.html" rel="noopener ugc nofollow" target="_blank">https://spinning up . open ai . com/en/latest/algorithms/ddpg . html</a></li><li id="465b" class="lj lk hi jq b jr ls ju lt jx lu kb lv kf lw kj lx lp lq lr bi translated"><a class="ae jn" href="https://github.com/chainer/chainerrl" rel="noopener ugc nofollow" target="_blank">https://github.com/chainer/chainerrl</a></li><li id="b751" class="lj lk hi jq b jr ls ju lt jx lu kb lv kf lw kj lx lp lq lr bi translated"><a class="ae jn" href="https://github.com/openai/baselines" rel="noopener ugc nofollow" target="_blank">https://github.com/openai/baselines</a></li></ul></div></div>    
</body>
</html>