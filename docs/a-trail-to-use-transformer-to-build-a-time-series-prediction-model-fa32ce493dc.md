# 使用 Transformer 建立时间序列预测模型的尝试

> 原文：<https://medium.com/analytics-vidhya/a-trail-to-use-transformer-to-build-a-time-series-prediction-model-fa32ce493dc?source=collection_archive---------12----------------------->

Transformer 模型已经广泛应用于翻译、摘要等各种自然语言处理任务中。典型 NLP 任务与时序任务有许多相似之处。时序任务的输入数据总是具有某种顺序(时间顺序)，而典型的文本也具有某种类似的顺序(语言顺序)。根据这样的相似性，Transformer 模型似乎有一些天然的优势来完成一些时间序列的工作。本文中的代码可以通过[链接](https://github.com/adonis1022/time_series_Transformer)来引用。

**变压器**

有很多文章介绍变压器模型。我简单说一下。Transformer 模型遵循典型的两级编码器-解码器结构，类似于以前的 seq2seq 模型。编码器部分将输入文本转换成向量，而解码器部分基于编码器部分的输出和先前的解码器输入产生目标语言。关于该模型的详细知识，请深入研究[杰伦的博客](http://jalammar.github.io/illustrated-transformer/)。

![](img/95524f1aadcc75ec9ad9b0270aed0896.png)

典型的变压器模型结构

**建模—编码器**

现在让我们开始建立一个基于 Transformer 的时间序列模型。典型的变换器输入总是跟随着嵌入层，因为输入是离散整数的向量，每个离散整数代表一个单词。但是对于时间序列模型，输入是一个连续数字的向量。将每个连续的数字转换成一个向量是没有意义的，因此不需要添加嵌入层。我想有两种方法可以解决这个问题。第一种方法是只把连续数的向量当作嵌入向量，也就是说我们可以把向量当作一维嵌入向量。第二种方法是使用矩阵将一维向量转换为某一维向量。我在笔记本上用的是前一种方法。

![](img/8cd0b09f0296a22238e4f60e4d2f2597.png)

编码器零件的代码

**建模—解码器**

我们可以用不同的方法来构造解码器部分。我们可以简单地使用变压器解码器作为我们的时序模型解码器部分。或者我们可以使用 LSTM 模型作为我们的解码器部分。为了连接变换器编码器和 LSTM 解码器，我使用输出矩阵的前两个维度作为 LSTM 解码器的初始 h 和 c 向量。然后我们应该决定解码器输入。有两种方法可以确定解码器输入。第一种是使用前一个位置的输出值。第二种是使用编码器输入的一些位置。哪种方法更好取决于数据的实际模式。如果数据有明显的季节性，最好使用第二种方法。

![](img/38e30494b066661c3465d19c46d4c30d.png)

解码器部分的编码器

![](img/a2d18b78f2ac19651b85e749c2730d85.png)

整个模型结构

**结论**

本文只是利用 Transformer 构建时间序列模型的一个简单尝试。建立时间序列模型还有许多其他方法。没有证据表明 NLP 模型比传统的时间序列模型表现更好，但本文可以为一些问题提供解决方案。