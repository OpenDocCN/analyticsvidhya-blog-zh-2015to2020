<html>
<head>
<title>To be(rt) or not to be(rt)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">生存还是毁灭</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/to-be-rt-or-not-to-be-rt-7b7b1f008d37?source=collection_archive---------18-----------------------#2020-05-26">https://medium.com/analytics-vidhya/to-be-rt-or-not-to-be-rt-7b7b1f008d37?source=collection_archive---------18-----------------------#2020-05-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="db4f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当前SOTA NLP模型的初学者友好漫游</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/8ab93b5dfbbab3ce444d1d02ea1ad73e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rfn6WmU8hCi44e56CtTRVQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated"><a class="ae jt" rel="noopener" href="/tensorflow/using-tensorflow-2-for-state-of-the-art-natural-language-processing-102445cda54a">图片信用</a></figcaption></figure><p id="d0e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有各种各样的NLP技术，从简单的单词包方法(例如TF-IDF)，到静态单词嵌入(例如word2vec，Glove)，到上下文化(即动态)单词嵌入。静态单词嵌入在不同的上下文中为相同的单词生成相同的嵌入。例如，这两个句子——“这部电影一点也不好”和“这个提议好得不像真的”有相同的单词“好”，但它们在这两个句子中有不同的意思。静态单词嵌入将为“good”返回相同的嵌入。另一方面，当前最先进的NLP技术BERT(来自变压器的双向编码器表示)使用变压器，它引入了注意机制，可以同时处理整个文本输入，以学习单词(或子单词)之间的上下文关系。因此，BERT会返回单词“good”的不同嵌入。同时，与只输出下游任务的向量的静态单词嵌入不同，BERT输出训练的模型和向量。这意味着我们可以为自己的用例微调一个预先训练好的转换器模型。</p><p id="0ffc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从tfhub，google-research，huggingface等都有相同模型的不同版本。我将使用Huggingface，因为他们可以很容易地在不同的模型之间切换(例如，BERT，DistilBERT，RoBERTa，XLNet。我在本文的第二节中介绍了这些变体之间的简单区别)并且这些模型与PyTorch和Keras兼容。</p><p id="add4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本教程将基于从<a class="ae jt" href="https://www.kaggle.com/c/tweet-sentiment-extraction/overview" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>获得的一条推特情感摘录。一个训练输入具有4个变量，<code class="du ju jv jw jx b">id</code>、【全文】、【情感标签】和最能代表情感标签的【所选文本】。给定只有全文和情感标签的新输入，该模型将从全文中提取最能代表相应情感的支持短语。情感分析在自然语言处理的许多方面都很普遍，例如理解聊天机器人上用户消息的语气，以便根据用户情绪提供适当的响应，并帮助对服务和故障进行下游分析。捕捉导致情感描述的支持短语有助于下游分析。</p><h1 id="7fea" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">I)用于变压器模型的通用管道</h1><ol class=""><li id="d447" class="kw kx hi ih b ii ky im kz iq la iu lb iy lc jc ld le lf lg bi translated">文本预处理</li><li id="ca84" class="kw kx hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">模特培训</li><li id="6e6c" class="kw kx hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">微调</li><li id="79bb" class="kw kx hi ih b ii lh im li iq lj iu lk iy ll jc ld le lf lg bi translated">推理</li></ol><h2 id="c696" class="lm jz hi bd ka ln lo lp ke lq lr ls ki iq lt lu km iu lv lw kq iy lx ly ku lz bi translated">1.让我们对数据进行预处理，使其与训练DistilBERT的数据相匹配。</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es ma"><img src="../Images/adc49dfc357eebd1ca98cd37909153d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*py_4IQGNaI2f6HK3c078uA.png"/></div></div></figure><h2 id="e866" class="lm jz hi bd ka ln lo lp ke lq lr ls ki iq lt lu km iu lv lw kq iy lx ly ku lz bi translated">2.模特培训</h2><p id="7266" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">对于输入句子中的每个标记，DistilBERT将输出一个768维向量(即嵌入。我使用默认的隐藏层大小，但这是可配置的)。DistilBERT的第一个输出是所有输入令牌的嵌入。这代表了DistilBERT对句子上下文中单词含义的“理解”,并作为微调步骤的输入。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es me"><img src="../Images/81c856d16cdef8eb02964884e4985904.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z-7gHAtocbcz_wIwjpvaxw.png"/></div></div></figure><h2 id="3d78" class="lm jz hi bd ka ln lo lp ke lq lr ls ki iq lt lu km iu lv lw kq iy lx ly ku lz bi translated">3.微调</h2><p id="7f8e" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">这些嵌入作为第二个模型的输入，我构建这个模型是为了让DistilBERT适应我的用例。为了提取支持短语，该模型将基于完整的文本和情感标签来预测记号的开始和结束索引(其是空间的，即单词所在的位置)。因此，我使用1D卷积层来保存这些空间信息。在展平和应用softmax之后，输出是开始和结束标记索引的一个热编码。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mf"><img src="../Images/660cf932e3548ac5871bc73c6b2a0107.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mYr38M_Cry7qND7B43gZ4g.png"/></div></div></figure><h2 id="b519" class="lm jz hi bd ka ln lo lp ke lq lr ls ki iq lt lu km iu lv lw kq iy lx ly ku lz bi translated">4.推理</h2><p id="35aa" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">我使用微调后的模型基于全文和情感标签来预测支持短语的位置。随后，我通过定位它们对应的索引来提取支持短语。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es me"><img src="../Images/7965db0e8f1994ae435990cb7a3b3b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rvOSeIwsEPLSR92wsYCPvw.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">对预测是如何做出的说明</figcaption></figure><h1 id="b01d" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">二)BERT模型的比较</h1><p id="bc5a" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq mb is it iu mc iw ix iy md ja jb jc hb bi translated">1.伯特</p><ul class=""><li id="370b" class="kw kx hi ih b ii ij im in iq mg iu mh iy mi jc mj le lf lg bi translated">通过给出屏蔽记号，BERT将使用两侧(目标记号之前和之后)来预测屏蔽记号，以便学习文本表示。</li></ul><p id="f96d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="mk">限制</em></p><ul class=""><li id="ec21" class="kw kx hi ih b ii ij im in iq mg iu mh iy mi jc mj le lf lg bi translated">[掩码]标记存在于训练阶段，而它不存在于预测阶段。</li><li id="39e3" class="kw kx hi ih b ii lh im li iq lj iu lk iy ll jc mj le lf lg bi translated">无法处理长文本序列。默认情况下，BERT最多支持512个令牌。通过忽略512个记号之后的文本或者将记号分成2个或更多个输入并分别预测来克服它。</li></ul><p id="5d81" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.蒸馏啤酒</p><ul class=""><li id="cb21" class="kw kx hi ih b ii ij im in iq mg iu mh iy mi jc mj le lf lg bi translated">使用一种称为蒸馏的技术，这种技术近似于谷歌的BERT，即一个较小的大型神经网络。这个想法是，一旦一个大的神经网络被训练，它的全部输出分布可以用一个较小的网络来近似。</li></ul><p id="a61c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.罗伯塔</p><ul class=""><li id="2437" class="kw kx hi ih b ii ij im in iq mg iu mh iy mi jc mj le lf lg bi translated">为了改进训练过程，RoBERTa从BERT的预训练中移除了下一句预测(NSP)任务，并引入了动态屏蔽，使得屏蔽的令牌在训练时期期间改变。</li></ul><p id="8213" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">4.XLNet</p><ul class=""><li id="0215" class="kw kx hi ih b ii ij im in iq mg iu mh iy mi jc mj le lf lg bi translated">通过使用置换语言建模(PLM)克服单向的文本编码方式，在置换语言建模(PLM)中，所有的标记都是以随机顺序预测的。这与BERT的屏蔽语言模型相反，在该模型中，仅预测屏蔽的(15%)标记。这也与传统的语言模型形成对比，在传统的语言模型中，所有的标记都是按顺序而不是随机顺序预测的。这有助于模型学习双向关系，从而更好地处理单词之间的依赖性和关系。</li><li id="e49a" class="kw kx hi ih b ii lh im li iq lj iu lk iy ll jc mj le lf lg bi translated">PLM不改变原始序列顺序，但在注意部分对其进行操作。</li><li id="4e2c" class="kw kx hi ih b ii lh im li iq lj iu lk iy ll jc mj le lf lg bi translated">使用隐藏状态处理长文本序列(来自Transformer-XL)</li></ul></div><div class="ab cl ml mm gp mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="hb hc hd he hf"><p id="485c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我希望这篇文章能让您直观地了解如何将BERT用于各种各样NLP应用，比如文本分类、问答系统、文本摘要等。更多细节请看拥抱脸的<a class="ae jt" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><blockquote class="ms mt mu"><p id="0d05" class="if ig mk ih b ii ij ik il im in io ip mv ir is it mw iv iw ix mx iz ja jb jc hb bi translated">我<!-- -->目前正在寻找一个数据科学家的机会，加入一个团队，让我能够将我的数据科学知识应用于现实生活中的商业挑战，从而带来商业价值。如果你觉得这篇文章有用，请点击下面的掌声按钮，它对我来说意义重大，也有助于其他人看到这个故事。如果你的数据科学团队正在扩大，请随时通过<a class="ae jt" href="https://www.linkedin.com/in/valerie-lim-yan-hui/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我。我很高兴分享更多关于我的个人资料:)</p></blockquote><p id="0b7a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">查看我为其他数据科学项目撰写的文章<a class="ae jt" rel="noopener" href="/@valerielimyh">也在这里</a>。</p><p id="32c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">保重，注意安全:)</p></div></div>    
</body>
</html>