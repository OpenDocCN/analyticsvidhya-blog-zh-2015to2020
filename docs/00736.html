<html>
<head>
<title>TV Stations Logo Recognition</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">电视台徽标识别</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/tv-stations-logo-recognition-20812f68e20a?source=collection_archive---------7-----------------------#2019-09-02">https://medium.com/analytics-vidhya/tv-stations-logo-recognition-20812f68e20a?source=collection_archive---------7-----------------------#2019-09-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="867c" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated"><em class="ix">实现部署在Flutter上的基于深度学习的logo检测应用。</em></h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es iy"><img src="../Images/25bbc4fcec65ae0bba853bae5e2c600b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KHPasdCIz-Embsu8HRX6jw.png"/></div></div></figure><p id="376f" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">人工智能正在彻底改变系统，并导致全球范式的转变。<br/>在巴西这样的国家，在强大的媒体环境下，这一概念似乎非常合适，因为公民无法了解他们的公共评估系统是如何工作的，例如:</p><ul class=""><li id="b086" class="kg kh hi jm b jn jo jq jr jt ki jx kj kb kk kf kl km kn ko bi translated">观众系统(一个不可靠机构的垄断)</li><li id="9260" class="kg kh hi jm b jn kp jq kq jt kr jx ks kb kt kf kl km kn ko bi translated">选举投票(由被操纵的机构进行)</li></ul><figure class="iz ja jb jc fd jd"><div class="bz dy l di"><div class="ku kv l"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">Beep应用程序项目</figcaption></figure><p id="ecf4" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> ⠀⠀⠀⠀⠀⠀⠀⠀⠀ </strong></p><p id="8146" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">我是Beep应用程序的一部分，该应用程序提出了让人们能够与受众数据进行交互的建议，并通过实时数据向广告商和媒体代理提供可信的信息。</p><p id="7f47" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ </strong></p><h1 id="b57d" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">在物体识别中，机器会有什么表现？</h1><p id="a5a7" class="pw-post-body-paragraph jk jl hi jm b jn ls ij jp jq lt im js jt lu jv jw jx lv jz ka kb lw kd ke kf hb bi translated">机器是神奇的，现在在识别图案、形状、颜色和人脸方面比人类更好。更重要的是，它们超越了我们的听力水平，可以在几秒钟内复制一个人的声音。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es lx"><img src="../Images/7b1d051fb0b0bd7fc27293b638f15639.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M2KE1WtGHwSgCxLpL9C9jA.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">大规模视觉识别挑战赛的目标检测任务。资料来源:image-net.org</figcaption></figure><p id="fa11" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">自2015年以来，在Imagenet比赛中，深度学习模型在对象识别方面的检测水平超过了人类。这种技术可以广泛用于监视、自动识别、利用无人机测绘资源等。</p><h1 id="6fca" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">如何自动化观众测量系统？</h1><p id="ccd4" class="pw-post-body-paragraph jk jl hi jm b jn ls ij jp jq lt im js jt lu jv jw jx lv jz ka kb lw kd ke kf hb bi translated">为了实现这个目标，我在<strong class="jm hj"> Darknet </strong>神经网络框架上训练了两个深度学习模型，第一个用于从<strong class="jm hj">近距离</strong>捕捉电视台标志，以获得更高的检测准确性，另一个用于<strong class="jm hj">更远距离</strong>以获得智能手机操作上的更多便利。</p><p id="5295" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">检测的标志对象:</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es ly"><img src="../Images/6445ab12ce51013068baed8dcb7cd73d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JyLjooL2kWyhAi3TAdTU7A.png"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">物体检测标志</figcaption></figure><h2 id="6238" class="lz lb hi bd lc ma mb mc lg md me mf lk jt mg mh lm jx mi mj lo kb mk ml lq mm bi translated">这里展示了最终训练好的卷积神经网络:</h2><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mn"><img src="../Images/d3fb8cbe4753021bdf7d6b019e9248e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*5dBLMGkC8a0KKAWWWifYUA.gif"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">用于标识检测的训练卷积神经网络</figcaption></figure><p id="a8b8" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">我决定使用约洛·V2(你只能看一次)，因为它的精确度很高，而且训练时间很短。我无法使用Yolo V3版本，因为在转换到TensorFlow lite (FlatBuffer模型)时不支持某些操作。我不得不在Yolo上使用“<strong class="jm hj"> Tiny </strong>”配置构建一个轻量级版本的模型，以便在移动设备上正常工作。</p></div><div class="ab cl mo mp gp mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="hb hc hd he hf"><h1 id="4f16" class="la lb hi bd lc ld mv lf lg lh mw lj lk io mx ip lm ir my is lo iu mz iv lq lr bi translated">GITHUB项目:</h1><p id="c15e" class="pw-post-body-paragraph jk jl hi jm b jn ls ij jp jq lt im js jt lu jv jw jx lv jz ka kb lw kd ke kf hb bi translated">这个项目与我的Github库有关:(<a class="ae na" href="https://github.com/leoitcode/tv-recognizer" rel="noopener ugc nofollow" target="_blank">链接</a>)</p><h1 id="8f54" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">第1部分:获取数据</h1><p id="658a" class="pw-post-body-paragraph jk jl hi jm b jn ls ij jp jq lt im js jt lu jv jw jx lv jz ka kb lw kd ke kf hb bi translated">对于近距离模型，我使用python库<strong class="jm hj">Google _ Images _ download</strong><a class="ae na" href="https://pypi.org/project/google_images_download/" rel="noopener ugc nofollow" target="_blank">(链接)</a>从Google Images获取数据，我已经在我的github web抓取脚本上演示了该用法，在该脚本中我获取了所有483个巴西电视台的徽标(<a class="ae na" href="https://github.com/leoitcode/webscraping" rel="noopener ugc nofollow" target="_blank">链接</a>)。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nb"><img src="../Images/53e6cbd78308d28d116b63e976eb401d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R443dU5Z1bZfqvaSTfC_nQ.jpeg"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">电视节目:2019年全球电视全国新闻</figcaption></figure><p id="750e" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">对于更远的距离模型，我从Youtube上的<strong class="jm hj"> </strong>电视节目的录制视频中获取数据，并在屏幕的角落上显示徽标。因此，我使用FFmpeg平台从每个电视台获得了至少2000幅全屏图像进行视频操作。</p><p id="7b72" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> ⠀⠀⠀⠀⠀⠀⠀⠀⠀ </strong></p><pre class="iz ja jb jc fd nc nd ne nf aw ng bi"><span id="77e8" class="lz lb hi nd b fi nh ni l nj nk">ffmpeg -i RECORD1.mp4 -vf “fps=1” -q:v 2 record_%03d.jpeg</span></pre><p id="3062" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">在这个命令中，我每秒钟从这个视频(RECORD1.mp4)中提取2000帧，连续生成图像record_xxx.jpg用于模型。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nl"><img src="../Images/b74bfd88fdffcdf69fbd22747fd7d249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*muHCTFz2g0O9UZKQRjLmyw.png"/></div></figure></div><div class="ab cl mo mp gp mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="hb hc hd he hf"><h1 id="7473" class="la lb hi bd lc ld mv lf lg lh mw lj lk io mx ip lm ir my is lo iu mz iv lq lr bi translated">第2部分:数据扩充</h1><p id="eb9d" class="pw-post-body-paragraph jk jl hi jm b jn ls ij jp jq lt im js jt lu jv jw jx lv jz ka kb lw kd ke kf hb bi translated">为此，我在Google Colab上使用了Fastai库深度学习的视觉模块。该过程在笔记本上有说明:</p><figure class="iz ja jb jc fd jd"><div class="bz dy l di"><div class="nm kv l"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">数据扩充脚本</figcaption></figure><p id="74be" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">这个笔记本应用所有类型的电视标志图像转换，如照明变化，大小，裁剪，视角，模糊等。数据扩充是一种人工扩展数据集并提高模型最终精度的方法。这里，Globo电视台的数据增强标志:</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nn"><img src="../Images/0b2b70ec6226974223d59ad1fe7729df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*R-COt-0EJ2CXefIXnjWAyA.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">Globo的徽标数据增强</figcaption></figure></div><div class="ab cl mo mp gp mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="hb hc hd he hf"><h1 id="04f4" class="la lb hi bd lc ld mv lf lg lh mw lj lk io mx ip lm ir my is lo iu mz iv lq lr bi translated"><strong class="ak">第3部分:注释</strong></h1><p id="e4cf" class="pw-post-body-paragraph jk jl hi jm b jn ls ij jp jq lt im js jt lu jv jw jx lv jz ka kb lw kd ke kf hb bi translated">因此，我为每个电视台准备了至少2000个增强标识图像。准备好训练了吗？还没有，训练过程需要一个称为注释的过程来特别识别屏幕上的徽标格式和位置。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es no"><img src="../Images/26bd6d70ea65d852ffb60c80ffb905c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*YsOU5FY2qr2gZaCtQ7qq_A.gif"/></div></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">Yolo标记工具</figcaption></figure><p id="74b1" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">之所以选择Yolo Mark，是因为在Darknet标准下可以很容易地得到注释。在每个图像上，软件生成一个包含边界框和类的坐标的txt文件。</p></div><div class="ab cl mo mp gp mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="hb hc hd he hf"><h1 id="483b" class="la lb hi bd lc ld mv lf lg lh mw lj lk io mx ip lm ir my is lo iu mz iv lq lr bi translated">第4部分:暗网培训</h1><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es np"><img src="../Images/95f16662f74e4c33b6eb012e2440648d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/0*LCEF65IMClFB5fff.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">电脑看到了什么？</figcaption></figure><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nq"><img src="../Images/b1ec1edcdbfaca24d65b8b0eaf37ce1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SOl5wNci56fcwmX6.png"/></div></div></figure><p id="5ae4" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> ⠀⠀⠀⠀⠀⠀⠀⠀⠀ </strong></p><p id="4ba9" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> ⠀⠀⠀⠀⠀⠀⠀⠀⠀ </strong></p><p id="ad8d" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">CNN(卷积神经网络)通过许多层处理图像，使用过滤器(内核)获得形状和颜色的模式，然后进行分类以识别对象是否属于某个特定类别。所有参数都可以配置用于训练。</p><p id="e840" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> ⠀⠀⠀⠀⠀⠀⠀⠀⠀ </strong></p><p id="a716" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj"> ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ </strong></p><p id="124c" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">我用AlexeyAB  Fork访问了Darknet，这是Darknet的一个流行版本，有更多的选项，它比原来的Darknet程序更快更准确。</p><div class="nr ns ez fb nt nu"><a href="https://github.com/AlexeyAB" rel="noopener  ugc nofollow" target="_blank"><div class="nv ab dw"><div class="nw ab nx cl cj ny"><h2 class="bd hj fi z dy nz ea eb oa ed ef hh bi translated">AlexeyAB -概述</h2><div class="ob l"><h3 class="bd b fi z dy nz ea eb oa ed ef dx translated">用于目标检测的暗网Yolo V3和V2神经网络。</h3></div><div class="oc l"><p class="bd b fp z dy nz ea eb oa ed ef dx translated">github.com</p></div></div><div class="od l"><div class="oe l of og oh od oi ji nu"/></div></div></a></div><p id="5525" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">为了开始训练，你基本上需要3个文件:</p><p id="7c78" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">1- (.数据文件)。为了给Darknet所有的图像文件，txt注释路径和类。</p><p id="0fdc" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">2- (darknet19_448.conv.23)。预先训练的权重用于将学习转移到新的模型中，以避免从头开始。</p><p id="f51f" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">3- (.CFG文件)配置文件。我使用了约洛V2微小的文件，并做了像过滤器，批量大小，细分参数的配置。</p><p id="2a36" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">于是，开始训练:</p><pre class="iz ja jb jc fd nc nd ne nf aw ng bi"><span id="39ab" class="lz lb hi nd b fi nh ni l nj nk">./darknet detector train logopaca.data cfg/logopaca_yolov2-tiny.cfg darknet19_448.conv.23 -map</span></pre><p id="7a11" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">我将选项“-map”放在命令上，以便可视化损失函数的历史，该函数显示来自假阳性和假阴性的惩罚分数。mAP indicator计算模型在特定范围内的平均精度，这对于评估最佳模型非常有用。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mn"><img src="../Images/204ec06df79f66c017d42a174e5f3328.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*5iE2wTgjjBQxiB1YZF_9ZA.png"/></div></figure><p id="ceca" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">我决定使用14000次迭代，而不是到期的后续迭代:</p><p id="6580" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">1-损失值的变化在14000处几乎为零，这表明与后面的迭代相比，过度拟合的机会减少了。</p><p id="2502" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">2-良好映射98.74%，这表明检测具有极好的精度。</p><p id="3298" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">3-F1分数为0.96，它考虑了精确度和召回率，以分析该模型在检测特定电视台标志时的表现，添加了假阴性结果。</p><p id="c06e" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">这是一份14000磅文件的简历，包含每个电视台的精度:</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es oj"><img src="../Images/90539e42eab433717a217ab3451a4311.png" data-original-src="https://miro.medium.com/v2/resize:fit:1384/format:webp/1*gjhL9_eSBfy1w9NNKTP2MA.png"/></div><figcaption class="kw kx et er es ky kz bd b be z dx translated">14000迭代权重文件的mAP计算。</figcaption></figure><p id="158f" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">结果，用了(。权重文件)我开始使用暗流将暗网转换为张量流格式:</p><div class="nr ns ez fb nt nu"><a href="https://github.com/thtrieu/darkflow" rel="noopener  ugc nofollow" target="_blank"><div class="nv ab dw"><div class="nw ab nx cl cj ny"><h2 class="bd hj fi z dy nz ea eb oa ed ef hh bi translated">thtrieu/darkflow</h2><div class="ob l"><h3 class="bd b fi z dy nz ea eb oa ed ef dx translated">将Darknet转换为TensorFlow。加载训练过的权重，使用TensorFlow进行调整，将常量图形定义导出到移动设备。</h3></div><div class="oc l"><p class="bd b fp z dy nz ea eb oa ed ef dx translated">github.com</p></div></div><div class="od l"><div class="ok l of og oh od oi ji nu"/></div></div></a></div><p id="f06c" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">这段代码转换成(ProtoBuf)格式:</p><pre class="iz ja jb jc fd nc nd ne nf aw ng bi"><span id="b724" class="lz lb hi nd b fi nh ni l nj nk">sudo flow — model logopaca_yolov2-tiny.cfg — load logopaca_yolov2-tiny_14000.weights — savepb</span></pre><blockquote class="ol om on"><p id="d828" class="jk jl oo jm b jn jo ij jp jq jr im js op ju jv jw oq jy jz ka or kc kd ke kf hb bi translated">来自暗流:保存<code class="du os ot ou nd b">.pb</code>文件时，旁边还会生成一个<code class="du os ot ou nd b">.meta</code>文件。这个<code class="du os ot ou nd b">.meta</code>文件是<code class="du os ot ou nd b">meta</code>字典中所有内容的JSON转储，其中包含了后期处理所需的信息，如<code class="du os ot ou nd b">anchors</code>和<code class="du os ot ou nd b">labels</code>。创建的<code class="du os ot ou nd b">.pb</code>文件可用于将图形迁移到移动设备(JAVA / C++ / Objective-C++)。</p></blockquote><p id="bb82" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">与。pb文件和。meta我开始把这个图转换成TensorFlow lite(。tflite)格式，使用UBUNTU中TensorFlow库的“tflite_convert ”:</p><pre class="iz ja jb jc fd nc nd ne nf aw ng bi"><span id="d1b3" class="lz lb hi nd b fi nh ni l nj nk">tflite_convert — graph_def_file=built_graph/logopaca_yolov2-tiny.pb — output_file=built_graph/logopaca_yolov2_tiny_far.lite — input_format=TENSORFLOW_GRAPHDEF — output_format=TFLITE — input_shape=1,416,416,3 — input_array=input — output_array=output — inference_type=FLOAT</span></pre></div><div class="ab cl mo mp gp mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="hb hc hd he hf"><h1 id="f978" class="la lb hi bd lc ld mv lf lg lh mw lj lk io mx ip lm ir my is lo iu mz iv lq lr bi translated"><strong class="ak">第5部分:在颤振上展开</strong></h1><p id="5aed" class="pw-post-body-paragraph jk jl hi jm b jn ls ij jp jq lt im js jt lu jv jw jx lv jz ka kb lw kd ke kf hb bi translated">Flutter在相机插件中增加了对图像流的支持。这提供了在相机预览中捕捉分离帧的能力。有了这个功能，我可以将这个流与一个tflite (TensorFlow Lite)插件连接起来，通过对象检测立即处理这些帧。</p><p id="f4cf" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">相机插件:<a class="ae na" href="https://pub.dev/packages/camera" rel="noopener ugc nofollow" target="_blank">https://pub.dev/packages/camera</a></p><p id="3958" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">首先，我在相机控制器中调用startImageStream，以获取<strong class="jm hj"><em class="oo">camera image img</em></strong>(图像格式)、(高度)、(宽度)和(<strong class="jm hj">平面</strong>)图像的字节数。</p><pre class="iz ja jb jc fd nc nd ne nf aw ng bi"><span id="00a6" class="lz lb hi nd b fi nh ni l nj nk">controller.startImageStream((CameraImage img) {<br/> if (!isDetecting) {<br/> isDetecting = true;</span><span id="850c" class="lz lb hi nd b fi ov ni l nj nk">int startTime = new DateTime.now().millisecondsSinceEpoch;</span></pre><p id="9573" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">tflite插件访问TensorFlow Lite API，该API在iOS和Android上都支持检测(<a class="ae na" href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="noopener ugc nofollow" target="_blank"> SSD </a>和<a class="ae na" href="https://pjreddie.com/darknet/yolov2/" rel="noopener ugc nofollow" target="_blank"> YOLO </a>)、<a class="ae na" href="https://phillipi.github.io/pix2pix/" rel="noopener ugc nofollow" target="_blank"> Pix2Pix </a>和<a class="ae na" href="https://github.com/tensorflow/models/tree/master/research/deeplab" rel="noopener ugc nofollow" target="_blank"> Deeplab </a>和<a class="ae na" href="https://www.tensorflow.org/lite/models/pose_estimation/overview" rel="noopener ugc nofollow" target="_blank"> PoseNet </a>)。</p><div class="nr ns ez fb nt nu"><a href="https://pub.dev/packages/tflite" rel="noopener  ugc nofollow" target="_blank"><div class="nv ab dw"><div class="nw ab nx cl cj ny"><h2 class="bd hj fi z dy nz ea eb oa ed ef hh bi translated">tflite |颤振包</h2><div class="ob l"><h3 class="bd b fi z dy nz ea eb oa ed ef dx translated">一个访问TensorFlow Lite API的Flutter插件。支持图像分类、对象检测(固态硬盘和YOLO)…</h3></div><div class="oc l"><p class="bd b fp z dy nz ea eb oa ed ef dx translated">公共开发</p></div></div><div class="od l"><div class="ow l of og oh od oi ji nu"/></div></div></a></div><p id="6bae" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">该插件具有函数<strong class="jm hj"><em class="oo">detectObjectOnFrame</em></strong><em class="oo"/>，该函数接收参数并处理识别:</p><pre class="iz ja jb jc fd nc nd ne nf aw ng bi"><span id="4b9e" class="lz lb hi nd b fi nh ni l nj nk">Tflite.detectObjectOnFrame(<br/> bytesList: img.planes.map((plane) {<br/> return plane.bytes;<br/> }).toList(),<br/> model: widget.model == yolo ? “YOLO” : “SSDMobileNet”,<br/> imageHeight: img.height,<br/> imageWidth: img.width,<br/> imageMean: widget.model == yolo ? 0 : 127.5,<br/> imageStd: widget.model == yolo ? 255.0 : 127.5,<br/> numResultsPerClass: 1,<br/> threshold: widget.model == yolo ? 0.2 : 0.4,<br/> ).then((recognitions) {<br/> int endTime = new DateTime.now().millisecondsSinceEpoch;<br/> print(“Detection took ${endTime — startTime}”);</span><span id="1393" class="lz lb hi nd b fi ov ni l nj nk">widget.setRecognitions(recognitions, img.height, img.width);</span><span id="f9a1" class="lz lb hi nd b fi ov ni l nj nk">isDetecting = false;<br/>});</span></pre><p id="b476" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">为了输出识别，我宁愿把预览放到<strong class="jm hj"> <em class="oo"> RotatedBox </em>和OverflowBox </strong>小部件中。第一个是根据移动方向旋转图像，如横向和纵向。第二种方法使预览完全适合手机屏幕大小。</p><pre class="iz ja jb jc fd nc nd ne nf aw ng bi"><span id="e533" class="lz lb hi nd b fi nh ni l nj nk">Widget build(BuildContext context) {<br/> if (controller == null || !controller.value.isInitialized) {<br/> return Container();<br/> }<br/> var tmp = MediaQuery.of(context).size;<br/> var screenH = math.max(tmp.height, tmp.width);<br/> var screenW = math.min(tmp.height, tmp.width);<br/> tmp = controller.value.previewSize;<br/> var previewH = math.max(tmp.height, tmp.width);<br/> var previewW = math.min(tmp.height, tmp.width);<br/> var screenRatio = screenH / screenW;<br/> var previewRatio = previewH / previewW;</span><span id="1cd1" class="lz lb hi nd b fi ov ni l nj nk">return RotatedBox(<br/> quarterTurns: MediaQuery.of(context).orientation == Orientation.landscape ? 3 : 0,<br/> child: OverflowBox(<br/> maxHeight:<br/> MediaQuery.of(context).orientation == Orientation.landscape ? screenW / previewW * previewH : screenH,<br/> maxWidth:<br/> MediaQuery.of(context).orientation == Orientation.landscape ? screenW : screenH / previewH * previewW,<br/> child: CameraPreview(controller),),<br/> );<br/> }<br/>}</span></pre><p id="6024" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">检测的边界框我从<a class="ae na" href="http://git.kumoh42.com/capstone2019/ticketdamoa/blob/9fbbce50fb7ac8d7db9c1c823810a8a1fe380b31/lib/view/widget/detection/camera_detection_widget.dart" rel="noopener ugc nofollow" target="_blank"> ticketdamoa </a>项目中获得了基础代码，并做了一些修改以提高设备兼容性，并在框上显示类名。</p><pre class="iz ja jb jc fd nc nd ne nf aw ng bi"><span id="74e3" class="lz lb hi nd b fi nh ni l nj nk">Widget build(BuildContext context) {<br/> List&lt;Widget&gt; _renderBoxes() {<br/> return results.map((re) {<br/> var _x = re[“rect”][“x”];<br/> var _w = re[“rect”][“w”];<br/> var _y = re[“rect”][“y”];<br/> var _h = re[“rect”][“h”];<br/> var scaleW, scaleH, x, y, w, h;</span><span id="3d81" class="lz lb hi nd b fi ov ni l nj nk">scaleH = screenW / previewW * previewH;<br/> scaleW = screenW;<br/> var difH = (scaleH — screenH) / scaleH;<br/> x = _x * scaleW;<br/> w = _w * scaleW;<br/> y = (_y — difH / 2) * scaleH;<br/> h = _h * scaleH;<br/> if (_y &lt; difH / 2) h -= (difH / 2 — _y) * scaleH;</span><span id="be30" class="lz lb hi nd b fi ov ni l nj nk">return Positioned(<br/> left: math.max(0, x),<br/> top: math.max(0, y),<br/> width: w,<br/> height: h,<br/> child: Container(<br/> padding: EdgeInsets.only(top: 5.0, left: 5.0),<br/> decoration: BoxDecoration(<br/> border: Border.all(<br/> color: Color.fromRGBO(37, 213, 253, 1.0),<br/> width: 3.0,<br/> ),<br/> ),<br/> child: Text(<br/> “${re[“detectedClass”]} ${(re[“confidenceInClass”] * 100).toStringAsFixed(0)}%”,<br/> style: TextStyle(<br/> color: Color.fromRGBO(37, 213, 253, 1.0),<br/> fontSize: 14.0,<br/> fontWeight: FontWeight.bold,</span></pre><p id="78ce" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated">最终结果:</p><p id="88ef" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj">近距离:</strong></p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ox"><img src="../Images/ca4ef14dcc7c7bea5ea59773b49df99e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*p4KqfwPOClLqeRKKcT1WIw.gif"/></div></figure><p id="0fb3" class="pw-post-body-paragraph jk jl hi jm b jn jo ij jp jq jr im js jt ju jv jw jx jy jz ka kb kc kd ke kf hb bi translated"><strong class="jm hj">远距离:</strong></p><figure class="iz ja jb jc fd jd"><div class="bz dy l di"><div class="ku kv l"/></div></figure></div></div>    
</body>
</html>