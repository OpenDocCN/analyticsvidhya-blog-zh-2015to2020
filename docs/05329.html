<html>
<head>
<title>A Complete Guide On Dimensionality Reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">降维完全指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/a-complete-guide-on-dimensionality-reduction-62d9698013d2?source=collection_archive---------1-----------------------#2020-04-18">https://medium.com/analytics-vidhya/a-complete-guide-on-dimensionality-reduction-62d9698013d2?source=collection_archive---------1-----------------------#2020-04-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><blockquote class="if ig ih"><p id="3071" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">嗨，伙计们，这篇文章的本质是给出一个直觉，并通过python给出一个降维的完整指导。我希望你喜欢读它，就像我喜欢为人们写它一样。</p></blockquote><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es jh"><img src="../Images/d1808761277a1574a41c35fad48e265a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PtaeFuNJxdZVz99WfYrpZA.png"/></div></div></figure><p id="ced4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">你知道吗，感兴趣的用户每天产生2.5万亿字节的数据？数据可以来自任何地方。假设数据是从</p><ul class=""><li id="5ae2" class="jw jx hi il b im in iq ir jt jy ju jz jv ka jg kb kc kd ke bi translated">像facebook、instagram这样的社交应用收集你喜欢的东西、分享、帖子、你最近去过的地方、你喜欢的餐馆等数据。</li><li id="b422" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">在线营销网站，如youtube、google，它保存了与您最近的搜索、兴趣、帖子浏览等相关的数据..,</li><li id="5d26" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">购物应用程序，如Myntra、amazon、flipkart，收集您购买的商品、产品的浏览量和点击率，以及您放在购物车中的产品等数据。,</li></ul><p id="5f9d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj"> I </strong>基于互联网的应用程序每天都在产生大量的数据。他们更多地参与数据收集、提炼和提取。然而，随着数据生成和提取的不断增加——可视化、控制和绘制模式将脱离我们的掌控。</p><p id="0852" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">例如，我们有1000个特征和100万个数据点。假设我们将它存储在一个矩阵格式中，其单元数为1000 * 100万= 1000万。这是非常好的，尤其是当我们有记忆限制的时候。我们收集了大量的数据来解决问题，这很好，但是处理和应用这些数据的模型并不容易。这就是降维发挥作用的地方。因此，不浪费时间，让我们深入了解这个概念。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es kk"><img src="../Images/38a0ed588c42bddad2fe6c722b1d2132.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*hM0-FieMKYTU9Inuz7rRXg.jpeg"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">四维数据表示</figcaption></figure><h2 id="af22" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jt la lb lc ju ld le lf jv lg lh li lj bi translated"><strong class="ak">目录:</strong></h2><h2 id="9bcb" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jt la lb lc ju ld le lf jv lg lh li lj bi translated"><strong class="ak">第一章:降维介绍</strong></h2><h2 id="09d8" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jt la lb lc ju ld le lf jv lg lh li lj bi translated"><strong class="ak">第二章:主成分分析</strong></h2><ul class=""><li id="a1f6" class="jw jx hi il b im lk iq ll jt lm ju ln jv lo jg kb kc kd ke bi translated">步骤2–1:引入和需要PCA。</li><li id="7f34" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">步骤2–2:工作方法。</li><li id="ff83" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">步骤2–3:优势和劣势。</li><li id="80b8" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">step _ 2–4:通过python代码片段对MNIST数据集进行PCA。</li></ul><h2 id="e3de" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jt la lb lc ju ld le lf jv lg lh li lj bi translated"><strong class="ak">第三章:线性判别分析</strong></h2><ul class=""><li id="32ea" class="jw jx hi il b im lk iq ll jt lm ju ln jv lo jg kb kc kd ke bi translated">步骤3–1:简介</li><li id="580f" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">步骤3–2:LDA的工作。</li><li id="a082" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">步骤3–3:LDA的扩展</li><li id="35ee" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">step _ 3–4:Python sk learn在IRIS数据集上实现LDA</li></ul><h2 id="2375" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jt la lb lc ju ld le lf jv lg lh li lj bi translated"><strong class="ak">第四章:T分布随机邻域嵌入</strong></h2><ul class=""><li id="bb1f" class="jw jx hi il b im lk iq ll jt lm ju ln jv lo jg kb kc kd ke bi translated">步骤4–1:SNE霸王龙简介。</li><li id="c494" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">第4步–第2步:工作方法。</li><li id="4a7c" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">步骤4–3:处理拥挤问题。</li><li id="837f" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">step _ 4–4:优点和缺点。</li><li id="829e" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">步骤4–5:通过python代码片段对MNIST数据集进行T-SNE分析。</li></ul><h2 id="c3bd" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jt la lb lc ju ld le lf jv lg lh li lj bi translated">第5章:数据规范化</h2><h2 id="3bd2" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jt la lb lc ju ld le lf jv lg lh li lj bi translated"><strong class="ak">第6章:结论</strong></h2></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><blockquote class="if ig ih"><p id="f105" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><em class="hi">第一章:降维介绍:</em></p></blockquote><p id="301f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi lw translated"><span class="l lx ly lz bm ma mb mc md me di">W</span>T20】什么是降维？</p><p id="4333" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">在统计学、机器学习、信息论中，降维或降维是将n维降维为k维的过程，其中k &lt;<n./></p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mf"><img src="../Images/9bde7fe4edb6a83b56cea2652a94ba77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S7bpbLCiMZA7-5cMSHLhCg.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">Dimensionality Reduction of Data.</figcaption></figure><h2 id="9dba" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jt la lb lc ju ld le lf jv lg lh li lj bi translated">Visualization of Data:</h2><p id="5658" class="pw-post-body-paragraph ii ij hi il b im lk io ip iq ll is it jt mg iw ix ju mh ja jb jv mi je jf jg hb bi translated">Data visualization brings easiness in understanding and increases effectiveness. The human mind learns fast from visuals than that of text and tables. It is applied to a large population, for e.g., one can remember dialogues and scenes of a movie which he might have watched years before, on the other hand, it is difficult for him to recall the subject he recently read.</p><p id="9eed" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">Now-a-days we have a good number of tools for data visualization tools, which are fast and effective. Data visualization creates a better selling strategy. Data visualization boosts the ability to process information in an easy and faster way to compare and make conclusions out of it.</p><p id="ddd5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">Let’s have a look at the data of various dimensions.</p><p id="8297" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj"> 1维数据:</strong></p><p id="8bf1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">在这里，我们通常称维度为特征。作为一个例子，我们已经采取了一个1D数组，并开始绘制一个数轴上的值。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mj"><img src="../Images/fb842803df8f55d87d83be2822664ff4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_taZorBMPyzSg-PNGgpydw.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">数据的1D表示法。</figcaption></figure><p id="c326" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">二维数据:</strong></p><p id="f7ed" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">现在我们已经有了一个2D阵列，并开始在X轴和Y轴上绘制数据，这两个轴是相互正交的。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mk"><img src="../Images/ef6af9cfa29be32738f5ade640cfdd5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fsmQMsTUmGlVocmPb25_2Q.png"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ml"><img src="../Images/8cb28f68ba8c61a866f2ac7a00eeb2ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*en5LtMej0RUqSQc7KJt-jA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">数据的2D表示法。</figcaption></figure><p id="25bc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">三维数据:</strong></p><p id="fe5f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">现在，我们将使用3D数组，并将其绘制在X、Y和Z轴上。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mm"><img src="../Images/2f4dd320717b41163d3612fe43a7e40f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R_Wp6voM7gh5sk88jabOVA.png"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mn"><img src="../Images/47c5ded02e9afaf275f4ed8158195c61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJsOLtRUWHfyPk5MOHaZCA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">数据的三维表示。</figcaption></figure><p id="e222" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">我们可以看到，随着维度的增加，数据的可视化变得越来越困难。</p><p id="f59b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">多维数据:</strong></p><p id="55f6" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">现在，对于N-D数据，我们需要N维空间，我们不能再把它可视化。因此，对于任何超过3D的数据的可视化，我们将使用称为<strong class="il hj">维度减少的技术将其减少到2或3维。</strong></p><h2 id="fa62" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jt la lb lc ju ld le lf jv lg lh li lj bi translated">降维的本质:</h2><p id="2532" class="pw-post-body-paragraph ii ij hi il b im lk io ip iq ll is it jt mg iw ix ju mh ja jb jv mi je jf jg hb bi translated">在微观层面上分析高维数据中的每一个维度是不可行的。我们可能需要几天或几个月的时间来进行任何有意义的分析，这需要大量的时间、金钱和人力，而这在我们的业务中是不被鼓励的。训练高维数据会给我们带来如下问题:</p><ul class=""><li id="2871" class="jw jx hi il b im in iq ir jt jy ju jz jv ka jg kb kc kd ke bi translated">存储数据所需的空间随着维度的增加而增加。</li><li id="578c" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">在训练模型时，维度越少，时间复杂度越低。</li><li id="57d8" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">随着维度的增加，过度拟合模型的可能性也会增加。</li><li id="5654" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">我们无法将高维数据可视化。通过降维，我们将把数据减少到2D或3D以便更好地可视化。</li><li id="c735" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">这将移除我们数据中的所有相关要素。</li></ul><h2 id="7104" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jt la lb lc ju ld le lf jv lg lh li lj bi translated">降维的组成部分:</h2><p id="b52b" class="pw-post-body-paragraph ii ij hi il b im lk io ip iq ll is it jt mg iw ix ju mh ja jb jv mi je jf jg hb bi translated">这里将详细讨论降维的两个主要组成部分。</p><p id="1c90" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">I)特征选择:</p><p id="1a65" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">大多数情况下，这些功能与我们的问题无关。例如，我们正在训练一个预测身高的模型，我们有一些特征数据(体重、颜色、痣、婚姻状况、性别)。我们可以看到，肤色、痣和婚姻状况等特征与人的身高无关，也就是说，与我们寻找身高的问题无关。因此，我们需要想出一个解决方案，找到对我们的任务最有用的特性。我们可以通过以下方式实现这一目标:</p><ul class=""><li id="aaca" class="jw jx hi il b im in iq ir jt jy ju jz jv ka jg kb kc kd ke bi translated">业务理解、领域知识和专家解决方案可以帮助我们选择影响响应变量(目标)的预测因素(特征)。但是，如果我们找不到有用的预测指标，或者错过了有用的特征，就有可能丢失信息。</li><li id="a3fb" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">我们建立了一个经典的最大似然模型，我们将根据与目标变量的相关性来选择特征。与具有低拟合度的特征相比，具有高拟合度的特征更可能被选择。</li><li id="c0f2" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">减少特征也可以帮助我们解决这个问题。假设使用主成分分析(将在后面的课程中讨论),它可以帮助我们找到将数据投影到更低维度的组件，同时减少信息损失。</li><li id="bd0c" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">另一种方法是移除所有相关的特征。例如，如果我们有与其他要素线性组合的要素(f1=2f2+3f3 ),那么它们不会向我们的数据添加任何附加信息。因此，这些特征对我们的模型训练不再有用。</li></ul><p id="6094" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">特征选择包括寻找原始数据的子集，使得它们的信息损失最小。它有以下三个策略:</p><ol class=""><li id="b25d" class="jw jx hi il b im in iq ir jt jy ju jz jv ka jg mo kc kd ke bi translated">过滤策略:获取更多数据信息的策略。</li><li id="0b90" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg mo kc kd ke bi translated">包装策略:基于模型的准确性，我们将选择特性。</li><li id="13ce" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg mo kc kd ke bi translated">嵌入式策略:基于模型预测误差，我们将决定是保留还是删除所选特征。</li></ol><p id="d8f7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">II)特征投影:</p><p id="c34b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">特征投影也称为特征提取，用于将高维空间中的数据转换到低维空间中。数据转换可以线性和非线性两种方式进行。</p><p id="8af7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">对于线性变换，我们有主成分分析(PCA)、线性判别分析(LDA ),对于非线性变换，我们应用T-SNE。</p></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><blockquote class="if ig ih"><p id="283f" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><em class="hi">第二章:主成分分析:</em></p></blockquote><p id="6476" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">在进入内容之前，让我们先来欣赏一下这项技术背后的数学家。</p><p id="a8fa" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">步骤2–1:引入和需要PCA: </strong></p><p id="2da8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">PCA是<strong class="il hj">卡尔·皮尔逊</strong>在1901年发明的，作为力学中主轴定理的类比；后来在20世纪30年代由哈罗德·霍特林独立开发并命名。根据应用领域的不同，它还被称为信号处理中的离散KLT，多变量质量控制中的霍特林变换，固有正交分解，<strong class="il hj"> X </strong>的奇异值分解(SVD)，特征值分解，因子分析，噪声和振动中的频谱分解，以及结构动力学中的经验模态分析。</p><p id="2ad7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">主成分分析主要用作探索性数据分析(EDA)和制作预测模型的工具。它通常用于可视化种群间的遗传距离和亲缘关系。PCA可以通过数据协方差(或相关性)矩阵的特征值分解或数据矩阵的奇异值分解来完成。</p><p id="bb5e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">步骤2–2:常设仲裁院的工作方法:</strong></p><p id="4cdc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">为了更好地理解PCA的工作原理，让我们以2D数据为例。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mk"><img src="../Images/ef6af9cfa29be32738f5ade640cfdd5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fsmQMsTUmGlVocmPb25_2Q.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">数据的2D表示法。</figcaption></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es mp"><img src="../Images/8c6cdfe4cf392ff35ea3191f954263b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*1BHI5olPI4te-7V2I_rFYg.png"/></div></figure><ol class=""><li id="72fc" class="jw jx hi il b im in iq ir jt jy ju jz jv ka jg mo kc kd ke bi translated">首先，我们将标准化数据，使平均值移动到原点，所有数据位于一个单位正方形内。</li><li id="ace6" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg mo kc kd ke bi translated">现在，我们将尝试对数据进行拟合。为此，我们将尝试随机线。现在我们将旋转这条线，直到它最适合数据。</li></ol><p id="9275" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">最终我们得到了下面的拟合(高度拟合)，它解释了一个特征的最大方差。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mq"><img src="../Images/814f7a71f6de4d59096324e4acf97001.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4XZzGobQIdYxmDwF4cfw8A.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">最佳拟合线。</figcaption></figure><p id="0429" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">PCA如何找到最佳拟合线？</strong></p><p id="25a1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">让我们用一个点来计算。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mr"><img src="../Images/088788cefe485604752a1cd0add4b0ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9gig0XkvbW-t3fnQyRQmgg.png"/></div></div></figure><p id="daa2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">现在，为了量化直线与数据的拟合程度，PCA将数据投影到直线上。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ms"><img src="../Images/571fa1f3316f136e27c1c2b5fd6fda3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8HDovvHDzayaKqapGiWsUQ.png"/></div></div></figure><p id="886e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">I)然后，它可以测量从数据到线的距离，并尝试找到一条最小化这些距离的线。</p><p id="a153" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">ii)或者它可以试图找到使从投影点到原点的距离最大化的线。</p><p id="dd83" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">数学直觉:</strong></p><p id="7ed2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">为了理解这项技术背后的数学原理，让我们回到单个数据点的概念。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es mt"><img src="../Images/f75739df62804a0dee5da8e4a28f291d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*Xh52nhLqUwQxSI4_UHFK_Q.png"/></div></figure><blockquote class="if ig ih"><p id="d812" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">将数据投影到直线上后，我们将得到一个直角三角形。现在从勾股定理我们得到A = B + C。</p><p id="7885" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">我们可以看到，B和C是成反比的。这意味着如果B变大，那么c一定变小，反之亦然。</p></blockquote><p id="56a4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">因此，PCA可以最小化到直线的距离，或者最大化从投影点到原点的距离。</p><blockquote class="if ig ih"><p id="c4dc" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated">更容易找到从投影点到原点的最大距离。因此，PCA找到了使从投影点到原点的平方距离之和最大化的最佳拟合线。</p></blockquote><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mu"><img src="../Images/eedaebd639540fdf0f3bda6293d93338.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HKnUTiunE0gOjB4wsMsfTA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">距离的最大平方和。</figcaption></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es mv"><img src="../Images/ba60fc7f7cdc7c1c706e66caa2102645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*H_IS64XL984-Jk8BR0FJOw.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">主成分分析的成本函数</figcaption></figure><p id="bad1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">注意:</strong>这里我们取距离的平方，这样负值不会抵消正值。</p><p id="f7b1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">现在我们得到了最佳拟合线<strong class="il hj"> y = mx + c. </strong>这叫做PC1(主成分1)。假设比例为4:1，这意味着X轴上有4个单位，Y轴上有1个单位，这说明数据主要分布在X轴上。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mw"><img src="../Images/184f047a6c09e762301c87c2045dda32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gBgPALX21brSfZg4rmnhSw.png"/></div></div></figure><p id="f735" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">来自勾股定理</p><p id="082e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">a = b + c =&gt; a = 4 + 1 =&gt; sqrt(17)=&gt; 4.12，但数据是按比例缩放的，因此我们用4.12除每一边，以获得单位向量。即，</p><p id="4b7a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">F1 = 4 / 4.12 = 0.97并且</p><p id="ea4f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">F2 = 1 / 4.12 = 0.242</p><p id="9e47" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">我们刚刚计算的单位向量称为<strong class="il hj">特征向量或PC1 </strong>，特征的<strong class="il hj">和</strong>比例(0.97 : 0.242)称为<strong class="il hj">加载分数。</strong></p><p id="85f4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">SS(PC1的距离)= PC1的特征值。</p><p id="29d4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">sqrt(PC1的特征值)=<strong class="il hj">PC1的奇异值</strong>。</p><p id="c5d8" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">现在我们对其他特征做同样的事情来获得主成分。为了投影数据，现在我们将旋转轴，使PC1与X轴平行(水平)。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mx"><img src="../Images/c32da9c27799cd5bb6bd4c09b469b394.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t28jQ52q5q-LZBV9goYCyw.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">旋转轴，使PC1保持水平</figcaption></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es my"><img src="../Images/f60bfd9420c765363647c38eace3db53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vr5-JbLzB6gKSRssZ8Kw5w.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">基于主成分的数据投影。</figcaption></figure><p id="3113" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">为了可视化，让我们根据两个主成分上的投影点来投影数据。</p><p id="b0a9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">我们可以看到它等于点的原始投影。</p><p id="497c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">如何计算变异？</strong></p><p id="5ecf" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">我们可以使用在PCA中计算的特征值来计算它。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es mz"><img src="../Images/c0145e20bcd86e7c446bb6344fe80f34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ti0f-Y8CA41FUYQ4en055A.png"/></div></div></figure><p id="348f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">假设PC1 = 0.83，PC2 = 0.17</p><p id="7867" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">现在，如果我们想将数据从2D转换到1D，我们选择要素1作为最终1D，因为它覆盖了总变化的83%。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es na"><img src="../Images/910487cc172a54370035dcd63ad312e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3PSD50Y0yJbl-Ff4ULobdA.png"/></div></div></figure><p id="749d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">这就是PCA的工作方式，它基于使用主成分获得的方差来估计为了降维而要消除的特征。</p><p id="8fb0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">Step _ 2–3:优缺点:</strong></p><p id="309a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">优势:</p><ul class=""><li id="34ed" class="jw jx hi il b im in iq ir jt jy ju jz jv ka jg kb kc kd ke bi translated">它会移除相关的特征。</li><li id="1989" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">提高模型效率。</li><li id="e129" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">减少过度拟合。</li><li id="3bf8" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">提高可视化。</li></ul><p id="09ae" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">缺点:</p><ul class=""><li id="95ef" class="jw jx hi il b im in iq ir jt jy ju jz jv ka jg kb kc kd ke bi translated">PCA是一种线性算法，它对多项式或其他复杂函数不太适用。我们可以知道如何使用核主成分分析来处理这些数据。</li><li id="856e" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">在PCA之后，如果我们不选择正确的维数来消除，我们可能会丢失很多信息。</li><li id="912f" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">较少的可解释性，因为原始特征转换为不像原始特征那样可读的主要成分。</li><li id="715a" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">它保留全局形状而不是局部形状。</li></ul><p id="6811" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">步骤2–4:通过python代码片段对MNIST数据集进行PCA:</strong></p><p id="1726" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">我已经从kaggle下载了<a class="ae nb" href="https://www.kaggle.com/c/digit-recognizer/data" rel="noopener ugc nofollow" target="_blank">数据(train.csv) </a>，其中包含了一张图片的784个像素值。csv格式。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nc"><img src="../Images/00a4ec2f292c8971c9bdadfbc5ce6df2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GAjw0OVV9J79i0nPJxPxlw.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">正在加载mnist数据</figcaption></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nd"><img src="../Images/eb6b3f068450d0f9f3dad46ede76b2c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qhbPQ9yV_PUUqQhasPMtxg.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">mnist图像的维度表示</figcaption></figure><p id="60a0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">作为预处理步骤，我们将数据标准化，使平均值移至原点，所有数据位于单位正方形内。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ne"><img src="../Images/e736f2f2f14874696eee8c04ded6a97a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oDxzZEpqAD0aDmYfj-eCYA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">数据标准化</figcaption></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nf"><img src="../Images/eecfb4dbd8aa10a13043aab07611927f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*me3YiCXeScWcdrOKkP6Rig.png"/></div></div></figure><p id="114f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">PCA可以以两种方式应用，一种是通过寻找特征向量，另一种是通过使用sklearn实现。在大多数情况下，两种实现都会给出相似的结果。</p><p id="8bde" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">方法-1: </strong></p><p id="7e6c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">这里我们将找到协方差矩阵，它实际上是用来寻找特征值和特征向量的。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ng"><img src="../Images/12952c7543f073733e686237c66d67ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K5w0jpDrkecXBzVY85DUgA.png"/></div></div></figure><p id="241b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">在将数据转换成二维数据后，我们将使用这两个特性来实现数据的可视化。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nh"><img src="../Images/8d15cac487c42a6b5c1b14c1eb8265ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vd2bFxg-19SRZ4wHbH4n9g.png"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ni"><img src="../Images/ca8c7302eb2b198b1248e53d17a461e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eIFJ3B-dkcasDXQ_kfXDQA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">通过特征向量的主成分分析</figcaption></figure><p id="8952" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">方法:2 </strong></p><p id="8ca1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">现在我们将使用PCA的sklearn实现，它实际上只用几行代码就完成了。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nj"><img src="../Images/b86bc6bbda296629348933cb025b57b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jM83luRQRdU4UCxg_8RjYA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">Sklearn的实现</figcaption></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nk"><img src="../Images/ba6c58eb6a7bed550285fdb7389240df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vPlvk0SyYarUNK3MnRAytA.png"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nl"><img src="../Images/cf25a106c7da136dd1296260d1f5173b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BRAaNax910V6oYWf2Zvhtw.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">通过sklearn进行主成分分析</figcaption></figure><p id="2ef5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">让我们看看每个特性解释的差异百分比。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nm"><img src="../Images/faf08cad5df59ba28358f9a86341000e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kWhGqyygIcc0XlTHx27I3A.png"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ng"><img src="../Images/55cf3efb49852429ed7165db13abac65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OezgmTRckY95Duylve5s5g.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">每个特征解释的差异百分比</figcaption></figure><p id="5945" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">如果我想保留80%的数据信息，那么我可以将维度减少到110。这些图将帮助我们选择要消除的特征的正确数量。</p></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><blockquote class="if ig ih"><p id="c7bf" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><em class="hi">第三章:线性判别分析(LDA): </em></p></blockquote><p id="ca9c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">步骤3–1:简介:</strong></p><p id="6839" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">LDA是机器学习和统计、模式识别中预处理步骤中最常用的降维技术。而该算法的目标是将数据集投影到具有类别可分离性的低维空间，以避免过拟合并降低机器的计算能力。</p><p id="1c56" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">最初的判别分析是由<strong class="il hj">罗纳德·费雪</strong>于1936年提出的，描述的是两类分类问题，后来由<strong class="il hj"> CR Rao </strong>于1948年推广为多类判别分析，现在称为线性判别分析或正态判别分析和判别函数分析。</p><p id="767b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">步骤3–2:LDA的工作:</strong></p><p id="e98b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">PCA和LDA都是线性归约技术，但是不同于PCA，LDA集中于最大化两组的可分性。</p><p id="700c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">LDA使用特征创建一个新的轴，并尝试将数据投影到一个新的轴上，以最大化两个类别或组的分离。这就是为什么LDA是一种监督学习算法，因为它利用目标值来寻找新的轴。</p><p id="f198" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">PCA试图找到最大化方差的分量，而另一方面LDA试图找到</p><p id="0569" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">I)最大化类别的可分性，并且</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nn"><img src="../Images/2923a38236a13ca978f90942f6c2613d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FRclC4o96iLtfCN1N5rU2A.png"/></div></div></figure><p id="398b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">ii)最小化类别之间的差异。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es no"><img src="../Images/39f97696e0b8d4231b570ef01c735fd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gq4TrWYnYk62qK9I-f-qsQ.png"/></div></div></figure><p id="65b9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">通过最小化方差，我们可以很好地分离各个组的聚类。因此，它和最大化群体的平均值一样重要。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es np"><img src="../Images/8ea6f571c33a41bae66a1328a08c4604.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GxKiZYOmCF3L5OO9WVVdQw.png"/></div></div></figure><p id="9b68" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">现在，LDA根据最大化以下公式的标准找到新的轴</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nq"><img src="../Images/bad737010d16246d44d328e5203ad5d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MQV5JJcRPPPXJletKe4f5Q.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">LDA的成本函数</figcaption></figure><p id="ede0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">超过2个类别的LDA:</strong></p><p id="047a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">考虑具有多于2组的数据，在这种情况下，LDA找到整个数据的平均值和各个组的中心，现在它试图最大化从中心平均值到各个组平均值的距离。为了更好地理解，请看以下三类数据。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nr"><img src="../Images/4d835665f2381b2fa2cf697fd43a8561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yZTKrfPFOhdwyumagpNNFA.png"/></div></div></figure><p id="da0e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">现在，我们可以找到一个平面来最好地分隔这三个组。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ns"><img src="../Images/ca64026974756d1b08e351ccd324fb26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8GOB6NBgKLiKeH6zj49OrA.png"/></div></div></figure><p id="444b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">算法:</strong></p><ol class=""><li id="8e8d" class="jw jx hi il b im in iq ir jt jy ju jz jv ka jg mo kc kd ke bi translated">计算数据集中不同类别的d维平均向量。</li><li id="4068" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg mo kc kd ke bi translated">计算散布矩阵(类间和类内散布矩阵)。</li><li id="95f4" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg mo kc kd ke bi translated">计算散射矩阵的特征向量(e1，e2，…，ed)和相应的特征值(λ1，λ2，…，λd)。</li><li id="5c96" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg mo kc kd ke bi translated">按特征值递减对特征向量进行排序，选择k个特征值最大的特征向量，形成d×k维矩阵W(其中每列代表一个特征向量)。</li><li id="d879" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg mo kc kd ke bi translated">使用这个d×k特征向量矩阵将样本变换到新的子空间上。这可以通过矩阵乘法来概括:Y=X×W(其中X是表示n个样本的n×d维矩阵，Y是新子空间中的变换后的n×k维样本)。</li></ol><ul class=""><li id="d124" class="jw jx hi il b im in iq ir jt jy ju jz jv ka jg kb kc kd ke bi translated"><strong class="il hj">步骤3–3:LDA的扩展:</strong></li></ul><p id="38e3" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">当分布的平均值是共享的(具有高方差的组)时，线性判别分析失败，因为LDA不可能找到使两个类线性分离的新轴。LDA在数据不是线性可分的情况下也会失败。在这种情况下，我们可以使用非线性判别分析。</p><ol class=""><li id="6344" class="jw jx hi il b im in iq ir jt jy ju jz jv ka jg mo kc kd ke bi translated"><strong class="il hj">二次判别分析(QDA): </strong>每一类都使用自己的方差(或有多个输入变量时的协方差)估计。</li><li id="c86c" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg mo kc kd ke bi translated"><strong class="il hj">灵活判别分析(FDA): </strong>使用非线性输入组合，如样条。</li><li id="d5d8" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg mo kc kd ke bi translated"><strong class="il hj">正则化判别分析(RDA): </strong>将正则化引入方差(实际上是协方差)的估计，调节不同变量对LDA的影响。</li></ol><ul class=""><li id="e035" class="jw jx hi il b im in iq ir jt jy ju jz jv ka jg kb kc kd ke bi translated"><strong class="il hj">Step _ 3–4:Python sk learn在IRIS数据集上实现LDA:</strong></li></ul><p id="5c9f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">让我们把用于LDA的虹膜数据集作为降维技术。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nt"><img src="../Images/b18b4ebe53fe0710eded624454c5bb6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mdJ53J64gWvo173xmKBkaQ.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">正在导入虹膜数据集。</figcaption></figure><p id="ac21" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">像PCA一样，LDA也可以使用sklearn实现。我们使用LDA将数据从4维减少到2维。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nu"><img src="../Images/e430810f6713d1d9335af9ca04cc2f6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KIpFPe4hv2otV4NHAhITyQ.png"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nv"><img src="../Images/4885c97eb9e84851a74becd084007e9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bcmU4DCge6U3bnhBpN_a5Q.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">虹膜数据集上的LDA。</figcaption></figure><p id="b38c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">为了知道PCA和LDA工作的不同，让我们看下面的图。其中PCA试图最大化方差，不像LDA试图最大化三个类别的可分性。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nw"><img src="../Images/1d62c33fb3df0865431add0f629ef0b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4HLbJ-CbzoNUWtYWEhMV0g.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">主成分分析与线性判别分析。</figcaption></figure><p id="6ed4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">我们可以看到这两个图之间的差异。在主成分分析中，数据有一些重叠，很难找到将两组分开的线。LDA可以帮助我们区分这三个组，因为它们在数据中的重叠较少。</p></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><blockquote class="if ig ih"><p id="9f19" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><em class="hi">第四章:T分布随机邻域嵌入(T-SNE): </em></p></blockquote><p id="d8a5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">步骤4–1:T-SNE简介:</strong></p><p id="ff52" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">T-SNE是一种经常用于可视化的机器学习算法，由<strong class="il hj">劳伦斯·范德马腾和杰弗里·辛顿</strong>(深度学习之父)开发。这是一种非线性降维技术，非常适合于在二维或三维的低维空间中嵌入用于可视化的高维数据。具体而言，它通过二维或三维点对每个高维对象进行建模，以这种方式，相似的对象通过附近的点进行建模，而不相似的对象通过远处的点以高概率进行建模。</p><p id="acea" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">T-SNE已经在广泛的应用中用于可视化，包括计算机安全研究、音乐分析、癌症研究、生物信息学和生物医学信号处理。它通常用于可视化由人工神经网络学习的高级表示。</p><p id="c00b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">步骤4–2:工作方法:</strong></p><p id="25d9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">在进入数学直觉之前，让我们学习一些与SNE霸王龙有关的术语。</p><p id="3936" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">邻域:</strong></p><p id="45da" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">点的邻域被定义为几何上彼此接近的点的簇。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nx"><img src="../Images/1f96b01df51e093434d22d813fd8b2c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dJWoq2Bi7wiJZngh4l5l1Q.png"/></div></div></figure><p id="17a3" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">N(X1) = { xj S.T xi，xj彼此更近}</p><p id="6797" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">嵌入:</strong></p><p id="0aec" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">嵌入是通过创建xi将高维空间中的点投影到低维空间中的过程。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ny"><img src="../Images/2c24abb329e747f31ab952f441aaa5c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FCzAOV5HwbjG4w0leDrF7Q.png"/></div></div></figure><p id="a460" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">随机:</strong></p><p id="af65" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">首先，T-SNE在成对的高维对象上构建概率分布，使得相似的对象具有被挑选的高概率，而不相似的点具有被挑选的极小概率。这就是T-SNE被称为随机(概率)的原因。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es nz"><img src="../Images/bedb90fa2fad346dfde704afdc99e198.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J9oQNse6lE0xnION76UcQQ.png"/></div></div></figure><p id="0e2c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">现在，T-SNE在低维地图中的点上定义了一个类似的概率分布，并且它最小化了关于地图中点的位置的两个分布之间的<strong class="il hj">kull back–lei bler散度</strong> (KL散度)。请注意，虽然原始算法使用对象之间的欧几里德距离作为其相似性度量的基础，但这应该适当地改变。</p><p id="6b16" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">所以现在T-SNE的成本函数是</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es oa"><img src="../Images/986ca728293b538a20280952058be784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5dq217Ucoe3qg5D2ZEuqmA.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">T-SNE成本函数</figcaption></figure><p id="9045" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">步骤4–3:处理拥挤问题:</strong></p><p id="5e7d" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">拥挤是一种情况，我们试图在一个更小的空间里投射点，由于空间不足，一切都变得混乱。例如，假设你在一辆城市公交车上，所有的座位都坐满了，但售票员仍然允许人们上车。在这种情况下会发生拥挤。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ob"><img src="../Images/a739e74bc798107fc19046ae378639f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dpedZOr8t_xJY0L1Q_1USA.png"/></div></div></figure><p id="4378" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj"> T分布:</strong></p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es oc"><img src="../Images/6151a5d4e646cb632afe0c10928f56a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NIPkbj0L0cMZ7AZ7Z5HCsw.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">正态分布</figcaption></figure><p id="e0c2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">通过使用高斯分布，所有低相似度值都落在曲线的尾部区域，我们可以看到尾部的空间非常小，不像t分布在曲线的两端更高，有更多的空间来保持这些簇的分离(就像公共汽车增加了几个座位)。最终，低相似性值与不同聚类的其他低相似性值混合在一起。为了克服这个问题，传统的SNE算法被T-SNE算法所取代，其中T表示T-分布。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es od"><img src="../Images/8100c2117fe90506e65da38a0a0f50a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XfUHxwpfwBXkbAWprLS-UQ.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">高斯分布与t分布</figcaption></figure><p id="aa3e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">Step _ 4–4:优缺点:</strong></p><p id="0a16" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">优势:</p><ul class=""><li id="17c6" class="jw jx hi il b im in iq ir jt jy ju jz jv ka jg kb kc kd ke bi translated">与PCA不同，T-SNE是一种非线性归约技术，这意味着它可以很好地处理任何多项式或非线性数据。</li><li id="d6bf" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">T-SNE能够保持局部和全局结构，而PCA试图将高D投影到低D，这解释了数据中的大部分差异。因此，它只关心全局结构。</li><li id="2df3" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">T-SNE广泛用于可视化任务。</li></ul><p id="c0a2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">缺点:</p><ul class=""><li id="c27e" class="jw jx hi il b im in iq ir jt jy ju jz jv ka jg kb kc kd ke bi translated">T-SNE具有二次时间和空间复杂度，因为我们正在寻找数据中每隔一个点的点之间的相似性(0(N)阶)。这是巨大的，尤其是当我们有时间和内存的限制。</li><li id="7251" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">不建议在处理大型数据集时使用T-SNE。</li><li id="c3d0" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">T-SNE是一种非参数映射方法，这意味着它没有将给定点映射到低维空间的显式函数。T-SNE基于点的邻域将点嵌入到低维中。因此，当一个测试数据点出现时，由于它以前不存在，我们需要重新训练整个T-SNE算法进行嵌入，这是很少使用的，因为它的二次时间复杂度。</li></ul><p id="4bdf" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">巴恩斯-胡特SNE (BHTSNE): </strong></p><p id="899c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">这项技术于2014年推出，与SNE霸王龙非常相似，但略有变化。该算法利用<strong class="il hj"> Barnes-Hut算法</strong>，天文学家使用该算法进行N体模拟，以近似对应点之间的力。</p><p id="86d9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">这些算法导致了优于标准T-SNE( O(N log N))计算优势，其远优于二次时间复杂度。这可以使用sklearn manifold轻松实现。TSNE库通过使用method = 'barnes-hut '属性。</p><p id="72a5" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">步骤4–5:通过python代码片段对MNIST数据集进行T-SNE:</strong></p><p id="6b69" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">我们已经看到了PCA对MNIST的作用。现在让我们用同样的数据集来试试SNE霸王龙。与PCA不同，T-SNE有两个参数:困惑度和n_iter。我们将尝试用这些参数的不同值来拟合数据。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es oe"><img src="../Images/b0c948c1fed37da99e7c3fbd39f7b83a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZNEFy8lwTZgmEA3UDZI5dg.png"/></div></div></figure><p id="9f3c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">困惑度= 30且n_iter = 1000:</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es of"><img src="../Images/265661f77cdd01537c731e39cd7e8367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U7Ojyax_hsOkDlYakct5Ew.png"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es og"><img src="../Images/cc30a0a24750acae23a54b3aa6f1da82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uXRx87qkqcI9KmmmvLS_NA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">困惑的= 30，马克西特=1000</figcaption></figure><p id="c95a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">困惑度= 50，n_iter = 1000:</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es oh"><img src="../Images/590eb51fb49f6015752a7bb483c5a3cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gziUPFhGCgOEHXxuFxTJ4Q.png"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es oi"><img src="../Images/d5ad92013aa4a856c14aaff23c0d5d6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DPCXY9jRHPbdUrPSElzZGw.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">困惑的= 50，马克西特=1000</figcaption></figure><p id="ac7a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">困惑度= 2，n_iter =1000</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es oj"><img src="../Images/8d625320f0cedae0842b54f1d008d42a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*grorQcXjGu0RgRY3yAi9WQ.png"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ok"><img src="../Images/1dbfe87b87cc479578dbb9d66b77ace0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-U1Zq80Clb7jPEuAsqK-mw.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">困惑的= 2，马克西特=1000</figcaption></figure><p id="58a1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">我们可以看到，困惑度降低了，数据与所有聚类混在一起。因此，选择正确的参数值总是很重要的。</p><p id="41c1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">困惑度= 100，n_iter =1000:</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es ol"><img src="../Images/d2e22b1e8cc6a0e5c63f2f7edb03fe89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xWzxShVi6KiePEjvEkjX7g.png"/></div></div></figure><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es om"><img src="../Images/89fe32fc5bd97b0222e8007a37c84fec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GALilbw38ZWRCWh5K0-ALA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">困惑的= 100，马克西特=1000</figcaption></figure><p id="6686" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">在所有的值中，困惑=100在我们的数据中表现良好。但是请注意，我们只在5000个数据点上尝试了T-SNE，因为它的时间复杂性需要很多时间。</p></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><blockquote class="if ig ih"><p id="79a7" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><em class="hi">第五章:数据规范化:</em></p></blockquote><p id="c5c9" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">通过数据规范化，我们可以将数据从无限范围转换到有限范围。数据标准化的主要目的是让你的数据一致清晰。一致性是指确保输出是可靠的，以便可以使用通用术语和格式识别相关数据。</p><p id="0dcc" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">通过数据规范化，我们可以将数据从无限范围转换到有限范围。数据标准化的主要目的是让你的数据一致清晰。一致性是指确保输出是可靠的，以便可以使用通用术语和格式识别相关数据。</p><p id="c023" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">降维前数据归一化的需求:</strong></p><p id="9bfe" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">假设有两个特征，其中一个特征的值在1到10的范围内(市场中每小时的购买者数量)，而另一个特征的值在50到1000的范围内(市场的访问者数量)。大概每小时的访客数是&gt;&gt;每小时的买家数。</p><p id="2909" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">由于像PCA这样的技术是基于最大化方差，如果我们在应用PCA来寻找特征向量之前不应用归一化，它们将更多地集中在较大值的维度上，因此特征向量将不会捕获其他维度中存在的信息。</p><p id="6e11" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">因此，特征标准化被用于去除诸如千克、厘米、毫米、升等刻度..归一化后，当我们绘制时，所有的数据都位于一个单位正方形内。</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es on"><img src="../Images/71e988eb9d4e5784518470edb482fc02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hHRqjc082zzeNKc414c3tg.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">数据标准化</figcaption></figure><p id="6005" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated"><strong class="il hj">数据归一化技术:</strong></p><p id="e54f" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">数据可以通过多种方式进行转换。一些最常用的技术是:</p><ol class=""><li id="2ea3" class="jw jx hi il b im in iq ir jt jy ju jz jv ka jg mo kc kd ke bi translated">线性缩放或最小-最大缩放:</li></ol><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es oo"><img src="../Images/0549022306aaadaa104646fbdd98da7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*Sn5CqaMKD4BY-dK-THRuQA.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">公式</figcaption></figure><p id="49d7" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">其中X =数据点</p><p id="c3a4" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">Xmin =最小数据点</p><p id="4f8a" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">Xmax =最大数据点</p><p id="90ab" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">当我们知道数据的近似上限和下限，并且很少或没有异常值时，我们将使用这种技术。当我们知道我们的数据在整个范围内近似均匀分布时。重新调整比例后，所有值将位于范围[0，1]内。</p><p id="5b0b" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">2.特征剪辑:</p><p id="0c9c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">当数据包含极端异常值时，这些技术将高于或低于某个值的特征值限制为固定值。例如，将所有超过120厘米的高度值裁剪为正好120厘米。这意味着我们将值压缩到一个固定的范围。</p><p id="90c2" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">3.对数标度:</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div class="er es op"><img src="../Images/0be6c2aa3c60b0a5a806296139f726f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*NCMhechXVVoLnNRbS0-jUA.png"/></div><figcaption class="kl km et er es kn ko bd b be z dx translated">公式</figcaption></figure><p id="adc0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">当数据的分布遵循幂律或者是帕累托分布(即一个值有很多点，而其他值只有很少点)时，使用它。例如，拿两部电影来说，一部受欢迎，另一部失败。与失败的电影相比，我们会在热门电影上获得更多的收视率。</p><p id="1565" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">4.z分数或标准化:</p><figure class="ji jj jk jl fd jm er es paragraph-image"><div role="button" tabindex="0" class="jn jo di jp bf jq"><div class="er es oq"><img src="../Images/0e253c5ddc67392d05b9920141bcbace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XESIyQlwbsLqoXigj-owBw.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">公式</figcaption></figure><p id="bf43" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">标准化后，平均值转换为0，标准差转换为1。当我们有一些异常值时，这是有用的，但不是极端的，我们需要剪辑。</p></div><div class="ab cl lp lq gp lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="hb hc hd he hf"><blockquote class="if ig ih"><p id="db47" class="ii ij ik il b im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg hb bi translated"><em class="hi">第六章:结论:</em></p></blockquote><p id="28f1" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">处理成千上万的要素是任何数据科学家的必备技能。我们每天生成的数据量是前所未有的，我们需要找到不同的方法来使用它。降维是一种非常有用的方法，对我来说非常有用，无论是在专业场合还是在机器学习黑客马拉松中。</p><p id="5258" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">此外，降维可能不是在所有情况下都能很好地工作。在这种情况下，我们需要打破常规，将数据转换到低维空间，同时对相同的数据使用不同的新约简算法也很重要。</p><p id="ac5e" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">今天到此为止。希望你已经对在现实世界中使用降维有了一些基本的概念。</p><h2 id="0c18" class="kp kq hi bd kr ks kt ku kv kw kx ky kz jt la lb lc ju ld le lf jv lg lh li lj bi translated">参考资料:</h2><ul class=""><li id="f741" class="jw jx hi il b im lk iq ll jt lm ju ln jv lo jg kb kc kd ke bi translated">https://sebastianraschka.com/Articles/2014_python_lda.html<a class="ae nb" href="https://sebastianraschka.com/Articles/2014_python_lda.html" rel="noopener ugc nofollow" target="_blank"/></li><li id="3aad" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated">https://www.youtube.com/watch?v=azXCzI57Yfc&amp;t = 786s</li><li id="cdcd" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated"><a class="ae nb" href="https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2892/pca-for-dimensionality-reduction-and-visualization/2/module-2-data-science-exploratory-data-analysis-and-data-visualization" rel="noopener ugc nofollow" target="_blank">https://www . applied ai course . com/lecture/11/applied-machine-learning-online-course/2892/PCA-for-dimensionalization-reduction-and-visualization/2/module-2-data-science-explorative-data-analysis-and-data-visualization</a></li><li id="d6a1" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated"><a class="ae nb" href="https://www.appliedaicourse.com/lecture/11/applied-machine-learning-online-course/2900/geometric-intuition-of-t-sne/2/module-2-data-science-exploratory-data-analysis-and-data-visualization" rel="noopener ugc nofollow" target="_blank">https://www . applied ai course . com/lecture/11/applied-machine-learning-online-course/2900/geometric-intuition-of-t-SNE/2/module-2-data-science-explorative-data-analysis-and-data-visualization</a></li><li id="11ad" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated"><a class="ae nb" href="https://en.wikipedia.org/wiki/Dimensionality_reduction" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Dimensionality_reduction</a></li><li id="5c1c" class="jw jx hi il b im kf iq kg jt kh ju ki jv kj jg kb kc kd ke bi translated"><a class="ae nb" href="https://www.youtube.com/watch?v=2cngQxtbkDc&amp;t=68s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=2cngQxtbkDc&amp;t = 68s</a></li></ul><p id="28ae" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">一旦我找到新的简化算法，我会更新这个页面。作为一名渴望成为数据科学家的人，我来到了这里。这包括我对这个概念的最后工作。谢谢你阅读我的文章。</p><p id="f8a0" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">可以查一下<strong class="il hj">。ipynb </strong>在我的<a class="ae nb" href="https://github.com/ChaitanyaNarva/dimensionality-reduction-on-mnist" rel="noopener ugc nofollow" target="_blank"> <strong class="il hj"> Github资源库中获取本文的完整代码片段。</strong> </a> <strong class="il hj"> </strong>你也可以查看我在kaggle的第一个关于<a class="ae nb" rel="noopener" href="/@chaitanyanarava/mercari-price-suggestion-challenge-66500ac1f88a"> <strong class="il hj">价格建议挑战</strong> </a>的案例研究。</p><p id="736c" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">请关注我，获取更多关于不同真实世界案例研究和数据科学文章的文章和实现！也可以通过<a class="ae nb" href="http://www.linkedin.com/in/sai-chaitanya-narava-20b3571a0" rel="noopener ugc nofollow" target="_blank"><strong class="il hj">LinkedIn</strong></a><strong class="il hj"/>和<a class="ae nb" href="https://github.com/ChaitanyaNarva/ChaitanyaNarva" rel="noopener ugc nofollow" target="_blank"> <strong class="il hj"> Github </strong> </a>与我联系</p><p id="2d73" class="pw-post-body-paragraph ii ij hi il b im in io ip iq ir is it jt iv iw ix ju iz ja jb jv jd je jf jg hb bi translated">我希望你喜欢读我的文章。这方面的学习永无止境，所以快乐学习吧！！签署再见:)</p></div></div>    
</body>
</html>