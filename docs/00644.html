<html>
<head>
<title>Feature Selection Methods with Code Examples</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">带有代码示例的特征选择方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-selection-methods-with-code-examples-a78439477cd4?source=collection_archive---------2-----------------------#2019-08-21">https://medium.com/analytics-vidhya/feature-selection-methods-with-code-examples-a78439477cd4?source=collection_archive---------2-----------------------#2019-08-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/e28fc1d3d1897e6ad5b576297e8377e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NtfbRctE3e7Nhb10GGlfVA.png"/></div></div></figure><h1 id="c9c9" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">为什么选择功能？</h1><p id="2f58" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">要素选择是在数据集中查找和选择最有用的要素的过程。这是机器学习管道中至关重要的一步。我们应该关心特征选择方法的原因与我们的模型中有不必要的特征的不良影响有关:</p><ul class=""><li id="033e" class="km kn hi jq b jr ko jv kp jz kq kd kr kh ks kl kt ku kv kw bi translated">过度拟合，降低测试集的泛化性能。</li><li id="f734" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl kt ku kv kw bi translated">降低训练速度</li><li id="0a41" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl kt ku kv kw bi translated">降低模型的可解释性</li></ul><h1 id="abf5" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">特征选择工具的主要类型</h1><p id="e82e" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">一般来说，有三种类型的特征选择工具(虽然我不知道是谁定义的):</p><ul class=""><li id="3904" class="km kn hi jq b jr ko jv kp jz kq kd kr kh ks kl kt ku kv kw bi translated"><strong class="jq hj"> <em class="lc">基于过滤:</em> </strong>过滤方法使用排序或分类算法来过滤掉那些不太有用的特征。</li><li id="05b4" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl kt ku kv kw bi translated"><strong class="jq hj"> <em class="lc">基于包装器:</em> </strong>包装器方法将一组特征的选择视为一个搜索问题。包装方法通常通过直接测试特性对模型性能的影响来选择特性。</li><li id="6e3f" class="km kn hi jq b jr kx jv ky jz kz kd la kh lb kl kt ku kv kw bi translated"><strong class="jq hj"> <em class="lc">嵌入式:</em> </strong>嵌入式方法使用具有内置特征选择方法的算法。例如，套索。</li></ul><p id="c7a8" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">现在，让我们更详细地看一下每种方法。</p><h1 id="8cff" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">1-基于过滤器的方法</h1><p id="81ab" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">过滤方法通常用作预处理步骤。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/9fd8f6345a66641e4b5472eb02346ecc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y51qvzsl_qZG7YOxlgMFsA.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated"><a class="ae lp" href="https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h2 id="e317" class="lq ir hi bd is lr ls lt iw lu lv lw ja jz lx ly je kd lz ma ji kh mb mc jm md bi translated">1.1 —差异阈值</h2><p id="c102" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">方差阈值移除值在不同观测值之间变化不大的要素(即其方差低于阈值)。这些功能没有什么价值。我们可以使用sklearn特征选择工具轻松应用这种方法。</p><pre class="lh li lj lk fd me mf mg mh aw mi bi"><span id="a5c0" class="lq ir hi mf b fi mj mk l ml mm">from sklearn.feature_selection import VarianceThreshold</span></pre><h2 id="6c3b" class="lq ir hi bd is lr ls lt iw lu lv lw ja jz lx ly je kd lz ma ji kh mb mc jm md bi translated"><strong class="ak"> 1.2 —相关阈值</strong></h2><p id="b009" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">相关性阈值移除与其他要素高度相关的要素(即其值的变化与另一个要素非常相似)。这些功能提供了冗余信息。</p><p id="67ab" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">首先，我们计算所有的成对相关性。然后，如果一对特征之间的相关性高于给定的阈值，我们将删除与其他特征具有较大平均绝对相关性的特征。</p><h2 id="f75e" class="lq ir hi bd is lr ls lt iw lu lv lw ja jz lx ly je kd lz ma ji kh mb mc jm md bi translated"><a class="ae lp" href="https://blog.datadive.net/selecting-good-features-part-i-univariate-selection/" rel="noopener ugc nofollow" target="_blank"> 1.3 —单变量特征选择</a></h2><p id="d359" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">单变量特征选择单独检查每个特征，以确定该特征与响应变量的关系强度。单变量选择有许多不同的选项。</p><ul class=""><li id="8f87" class="km kn hi jq b jr ko jv kp jz kq kd kr kh ks kl kt ku kv kw bi translated"><strong class="jq hj">皮尔逊相关性</strong></li></ul><p id="acf8" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">理解特征与响应变量关系的最简单方法之一是<a class="ae lp" href="http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient" rel="noopener ugc nofollow" target="_blank">皮尔逊相关系数</a>，它测量两个变量之间的线性相关性。</p><ul class=""><li id="5767" class="km kn hi jq b jr ko jv kp jz kq kd kr kh ks kl kt ku kv kw bi translated"><strong class="jq hj">卡方检验</strong></li></ul><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es mn"><img src="../Images/f4c0a8dc26a4b99ef3898e9e47e0e45f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*NMNKN3yt9z15ZmjncJYz6A.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated"><a class="ae lp" href="https://chrisalbon.com/machine_learning/feature_selection/chi-squared_for_feature_selection/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="a38a" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">在该方法中，我们计算目标和数值变量之间的卡方度量，并且仅选择具有最佳卡方值的期望数量的变量。</p><pre class="lh li lj lk fd me mf mg mh aw mi bi"><span id="0264" class="lq ir hi mf b fi mj mk l ml mm"><strong class="mf hj">from</strong> <strong class="mf hj">sklearn.feature_selection</strong> <strong class="mf hj">import</strong> SelectKBest<br/><strong class="mf hj">from</strong> <strong class="mf hj">sklearn.feature_selection</strong> <strong class="mf hj">import</strong> chi2<br/>chi2_selector = SelectKBest(chi2, k=<strong class="mf hj">2</strong>)<br/>X_kbest = chi2_selector.fit_transform(X, y)</span></pre><ul class=""><li id="ba77" class="km kn hi jq b jr ko jv kp jz kq kd kr kh ks kl kt ku kv kw bi translated"><a class="ae lp" href="https://chrisalbon.com/machine_learning/feature_selection/anova_f-value_for_feature_selection/" rel="noopener ugc nofollow" target="_blank"> <strong class="jq hj">方差分析F值</strong> </a></li></ul><p id="d9ac" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">如果特征是分类的，计算每个特征和目标向量之间的卡方(χ2)统计。但是，如果特征是定量的，则计算每个特征和目标向量之间的方差分析F值。</p><p id="396d" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">当我们按目标向量对数值特征进行分组时，F值分数检验每组的均值是否有显著差异。</p><pre class="lh li lj lk fd me mf mg mh aw mi bi"><span id="42b3" class="lq ir hi mf b fi mj mk l ml mm"><strong class="mf hj">from</strong> <strong class="mf hj">sklearn.feature_selection</strong> <strong class="mf hj">import</strong> SelectKBest<br/><strong class="mf hj">from</strong> <strong class="mf hj">sklearn.feature_selection</strong> <strong class="mf hj">import</strong> f_classif<br/>fvalue_selector = SelectKBest(f_classif, k=<strong class="mf hj">2</strong>)<br/>X_kbest = fvalue_selector.fit_transform(X, y)</span></pre><ul class=""><li id="6d46" class="km kn hi jq b jr ko jv kp jz kq kd kr kh ks kl kt ku kv kw bi translated"><strong class="jq hj">互信息和最大信息系数(MIC) </strong></li></ul><p id="4b19" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">相关性估计的一个更健壮的选项是<a class="ae lp" href="https://en.wikipedia.org/wiki/Mutual_information" rel="noopener ugc nofollow" target="_blank">互信息</a>，它测量变量之间的相互依赖性。</p><p id="8ff5" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">但是，由于两个原因，直接使用特性排名可能不太方便。首先，它不是一个度量，也没有归一化(即，不在固定范围内)，因此两个数据集之间的MI值可能是不可比的。第二，对于连续变量的计算可能是不方便的:通常变量需要通过宁滨离散化，但是互信息分数可能对箱选择相当敏感。</p><p id="b824" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated"><a class="ae lp" href="http://en.wikipedia.org/wiki/Maximal_information_coefficient" rel="noopener ugc nofollow" target="_blank">最大信息系数</a>是为解决这些缺点而开发的技术。它搜索最优宁滨，并将互信息得分转换为位于范围[0；1].在python中，MIC在<a class="ae lp" href="http://minepy.sourceforge.net/" rel="noopener ugc nofollow" target="_blank"> minepy </a>库中可用。</p><h1 id="789d" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 2 —基于包装器的方法</strong></h1><p id="28bd" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">包装器方法基于贪婪搜索算法，因为它们评估所有可能的特征组合，并为特定的机器学习算法选择产生最佳结果的组合。这种方法的缺点是测试所有可能的特征组合在计算上非常昂贵，特别是当特征集非常大时。</p><p id="6ef7" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">很明显，这是一种计算量很大的寻找性能最佳的特征子集的方法，因为它们必须多次调用学习算法。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/038a8b8b1004ae6c9a4a9c868cb42e74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4LvuQ-OW0i18TCeIDJyf5Q.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated"><a class="ae lp" href="https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><h2 id="6477" class="lq ir hi bd is lr ls lt iw lu lv lw ja jz lx ly je kd lz ma ji kh mb mc jm md bi translated"><strong class="ak"> 2.1 —向前搜索</strong></h2><p id="2f5a" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">在向前步进特征选择的第一阶段，针对每个特征评估分类器的性能。从所有特征中选择表现最佳的特征。</p><p id="0c05" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">在第二步中，结合所有其他特征来尝试第一个特征。选择产生最佳算法性能的两个特征的组合。该过程继续进行，直到选择了指定数量的特征。</p><pre class="lh li lj lk fd me mf mg mh aw mi bi"><span id="9774" class="lq ir hi mf b fi mj mk l ml mm">from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier<br/>from sklearn.metrics import roc_auc_score <br/><strong class="mf hj">from mlxtend.feature_selection import SequentialFeatureSelector </strong>feature_selector = SequentialFeatureSelector(RandomForestClassifier(n_jobs=-1),<br/>                          k_features=15, forward=True, <br/>                          verbose=2, scoring=’roc_auc’, cv=4)</span></pre><h2 id="208a" class="lq ir hi bd is lr ls lt iw lu lv lw ja jz lx ly je kd lz ma ji kh mb mc jm md bi translated"><strong class="ak"> 2.2 —向后搜索</strong></h2><p id="2d61" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">顾名思义，后退特征选择与我们在上一节中学习的前进特征选择正好相反。在后退特征选择的第一步中，以循环方式从特征集中移除一个特征，并评估分类器的性能。</p><p id="0b9f" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">产生最佳性能的功能集将被保留。在第二步中，再次以循环方式移除一个特征，并且评估除了这两个特征之外的所有特征组合的性能。此过程将一直持续到数据集中剩余指定数量的要素。</p><pre class="lh li lj lk fd me mf mg mh aw mi bi"><span id="0e68" class="lq ir hi mf b fi mj mk l ml mm">from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier<br/>from sklearn.metrics import roc_auc_score <br/>from mlxtend.feature_selection import SequentialFeatureSelector<strong class="mf hj"> </strong>feature_selector = SequentialFeatureSelector(RandomForestClassifier(n_jobs=-1),<br/>                          k_features=15, <strong class="mf hj">forward=False</strong>, <br/>                          verbose=2, scoring=’roc_auc’, cv=4)</span></pre><h2 id="b6ed" class="lq ir hi bd is lr ls lt iw lu lv lw ja jz lx ly je kd lz ma ji kh mb mc jm md bi translated"><strong class="ak"> 2.3 —递归特征消除</strong></h2><p id="a37f" class="pw-post-body-paragraph jo jp hi jq b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl hb bi translated">这是一种贪婪的优化算法，旨在找到性能最佳的特征子集。它重复地创建模型，并在每次迭代中保留性能最好或最差的特性。它用剩下的特征构造下一个模型，直到所有的特征都用完。然后，它根据要素被消除的顺序对其进行排序。</p><pre class="lh li lj lk fd me mf mg mh aw mi bi"><span id="136d" class="lq ir hi mf b fi mj mk l ml mm"><strong class="mf hj">&gt;&gt;&gt; from</strong> <strong class="mf hj">sklearn.datasets</strong> <strong class="mf hj">import</strong> make_friedman1<br/><strong class="mf hj">&gt;&gt;&gt; from</strong> <strong class="mf hj">sklearn.feature_selection</strong> <strong class="mf hj">import</strong> RFE<br/><strong class="mf hj">&gt;&gt;&gt; from</strong> <strong class="mf hj">sklearn.svm</strong> <strong class="mf hj">import</strong> SVR<br/><strong class="mf hj">&gt;&gt;&gt; </strong>X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)<br/><strong class="mf hj">&gt;&gt;&gt; </strong>estimator = SVR(kernel="linear")<br/><strong class="mf hj">&gt;&gt;&gt; </strong>selector = RFE(estimator, 5, step=1)<br/><strong class="mf hj">&gt;&gt;&gt; </strong>selector = selector.fit(X, y)</span></pre><h1 id="1d32" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated"><strong class="ak"> 3 —嵌入式方法</strong></h1><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/50d66ebb8f2c0cd4475f27c44bbdcc7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G1pChqYsCEas_j94byRsLw.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated"><a class="ae lp" href="https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="28a4" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">嵌入式方法使用具有内置特征选择方法的算法。比如Lasso和RF都有自己的特征选择方法。套索正则化强制许多要素权重为零。</p><p id="608f" class="pw-post-body-paragraph jo jp hi jq b jr ko jt ju jv kp jx jy jz ld kb kc kd le kf kg kh lf kj kk kl hb bi translated">我们还可以使用RandomForest根据特性的重要性来选择特性。我们也可以使用LightGBM。或者XGBoost对象，只要它有一个<code class="du mq mr ms mf b">feature_importances_</code>属性。</p><h1 id="511b" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">参考</h1><div class="mt mu ez fb mv mw"><a href="https://blog.datadive.net/selecting-good-features-part-i-univariate-selection/" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab dw"><div class="my ab mz cl cj na"><h2 class="bd hj fi z dy nb ea eb nc ed ef hh bi translated">特征选择-第一部分:单变量选择</h2><div class="nd l"><h3 class="bd b fi z dy nb ea eb nc ed ef dx translated">对特征选择/排序有很好的理解对于数据科学家或机器学习来说是一笔巨大的财富…</h3></div><div class="ne l"><p class="bd b fp z dy nb ea eb nc ed ef dx translated">blog.datadive.net</p></div></div><div class="nf l"><div class="ng l nh ni nj nf nk io mw"/></div></div></a></div><div class="mt mu ez fb mv mw"><a href="https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab dw"><div class="my ab mz cl cj na"><h2 class="bd hj fi z dy nb ea eb nc ed ef hh bi translated">结合实例介绍特征选择方法(或者如何选择合适的变量？)</h2><div class="nd l"><h3 class="bd b fi z dy nb ea eb nc ed ef dx translated">简介我学习机器学习的最好方法之一是用最好的数据对自己进行基准测试…</h3></div><div class="ne l"><p class="bd b fp z dy nb ea eb nc ed ef dx translated">www.analyticsvidhya.com</p></div></div><div class="nf l"><div class="nl l nh ni nj nf nk io mw"/></div></div></a></div><div class="mt mu ez fb mv mw"><a href="https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2" rel="noopener follow" target="_blank"><div class="mx ab dw"><div class="my ab mz cl cj na"><h2 class="bd hj fi z dy nb ea eb nc ed ef hh bi translated">每个数据科学家都应该知道的5种特征选择算法</h2><div class="nd l"><h3 class="bd b fi z dy nb ea eb nc ed ef dx translated">额外收获:是什么让一个优秀的足球运动员变得伟大？</h3></div><div class="ne l"><p class="bd b fp z dy nb ea eb nc ed ef dx translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="nm l nh ni nj nf nk io mw"/></div></div></a></div><div class="mt mu ez fb mv mw"><a href="https://chrisalbon.com/" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab dw"><div class="my ab mz cl cj na"><h2 class="bd hj fi z dy nb ea eb nc ed ef hh bi translated">克里斯·阿尔邦</h2><div class="nd l"><h3 class="bd b fi z dy nb ea eb nc ed ef dx translated">我是一名数据科学家，有十年应用统计学习、人工智能和软件的经验…</h3></div><div class="ne l"><p class="bd b fp z dy nb ea eb nc ed ef dx translated">chrisalbon.com</p></div></div></div></a></div><div class="mt mu ez fb mv mw"><a href="https://jasonlian.github.io/2017/03/13/ML2-Feature-Selection/" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab dw"><div class="my ab mz cl cj na"><h2 class="bd hj fi z dy nb ea eb nc ed ef hh bi">机器学习中的特征选择方法概述</h2><div class="nd l"><h3 class="bd b fi z dy nb ea eb nc ed ef dx">Feature Selection 是在模型构建过程中选择最相关、最有利于提高预测效果的特征子集的过程 机器学习中的特征选择（Feature Selection）也被称为 Variable Selection 或 Attribute…</h3></div><div class="ne l"><p class="bd b fp z dy nb ea eb nc ed ef dx translated">jasonlian.github.io</p></div></div></div></a></div><div class="mt mu ez fb mv mw"><a href="https://scikit-learn.org/stable/modules/feature_selection.html" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab dw"><div class="my ab mz cl cj na"><h2 class="bd hj fi z dy nb ea eb nc ed ef hh bi translated">1.13.功能选择-sci kit-了解0.21.3文档</h2><div class="nd l"><h3 class="bd b fi z dy nb ea eb nc ed ef dx translated">sklearn.feature_selection模块中的类可用于…上的特征选择/维度缩减</h3></div><div class="ne l"><p class="bd b fp z dy nb ea eb nc ed ef dx translated">scikit-learn.org</p></div></div><div class="nf l"><div class="nn l nh ni nj nf nk io mw"/></div></div></a></div><div class="mt mu ez fb mv mw"><a href="https://stackabuse.com/applying-wrapper-methods-in-python-for-feature-selection/" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab dw"><div class="my ab mz cl cj na"><h2 class="bd hj fi z dy nb ea eb nc ed ef hh bi translated">在Python中应用包装器方法进行要素选择</h2><div class="nd l"><h3 class="bd b fi z dy nb ea eb nc ed ef dx translated">在上一篇文章中，我们研究了如何使用过滤方法为机器选择特征…</h3></div><div class="ne l"><p class="bd b fp z dy nb ea eb nc ed ef dx translated">stackabuse.com</p></div></div></div></a></div><div class="mt mu ez fb mv mw"><a href="https://bookdown.org/max/FES/" rel="noopener  ugc nofollow" target="_blank"><div class="mx ab dw"><div class="my ab mz cl cj na"><h2 class="bd hj fi z dy nb ea eb nc ed ef hh bi translated">特征工程和选择:预测模型的实用方法</h2><div class="nd l"><h3 class="bd b fi z dy nb ea eb nc ed ef dx translated">预测建模的一个主要目标是找到一个可靠和有效的预测之间的关系。</h3></div><div class="ne l"><p class="bd b fp z dy nb ea eb nc ed ef dx translated">bookdown.org</p></div></div></div></a></div></div></div>    
</body>
</html>