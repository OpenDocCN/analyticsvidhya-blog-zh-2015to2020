<html>
<head>
<title>Deep Learning on Seinfeld</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于宋飞的深度学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/the-seinfeld-fc3c40d2910a?source=collection_archive---------15-----------------------#2019-09-23">https://medium.com/analytics-vidhya/the-seinfeld-fc3c40d2910a?source=collection_archive---------15-----------------------#2019-09-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5bf5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">《宋飞正传》一直是我最喜欢的电视剧之一，所以当我看到Kaggle上完整的<a class="ae jd" href="https://www.kaggle.com/thec03u5/seinfeld-chronicles" rel="noopener ugc nofollow" target="_blank">《宋飞正传》</a>数据集时，我无言以对。</p><p id="0f27" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">github笔记本电脑:</p><div class="je jf ez fb jg jh"><a href="https://github.com/ktzioumis/The-Seinfeld/blob/master/The-Seinfeld.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ji ab dw"><div class="jj ab jk cl cj jl"><h2 class="bd hj fi z dy jm ea eb jn ed ef hh bi translated">ktzioumis/The-Seinfeld</h2><div class="jo l"><h3 class="bd b fi z dy jm ea eb jn ed ef dx translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="jp l"><p class="bd b fp z dy jm ea eb jn ed ef dx translated">github.com</p></div></div><div class="jq l"><div class="jr l js jt ju jq jv jw jh"/></div></div></a></div><div class="je jf ez fb jg jh"><a href="https://github.com/ktzioumis/The-Seinfeld/blob/master/Larry%20David.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ji ab dw"><div class="jj ab jk cl cj jl"><h2 class="bd hj fi z dy jm ea eb jn ed ef hh bi translated">ktzioumis/The-Seinfeld</h2><div class="jo l"><h3 class="bd b fi z dy jm ea eb jn ed ef dx translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="jp l"><p class="bd b fp z dy jm ea eb jn ed ef dx translated">github.com</p></div></div><div class="jq l"><div class="jx l js jt ju jq jv jw jh"/></div></div></a></div><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es jy"><img src="../Images/3b76762acd412d23f4b1d4778f3e667e.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/0*CF7kPCrmf5hnuJR6.gif"/></div></div></figure><p id="fee4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">起初，将深度学习应用于宋飞正传的想法似乎很奇怪，开始时我只是很高兴在数据中玩一玩，发现一些有趣的事情，如“拉里·戴维写了58个学分的宋飞正传的最多集”，“除了第六季，乔治在每一季中说的话都比伊莱恩多”，“超人在17集里被提到24次”。</p><p id="3c33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是拉里·戴维事件又回到了我的脑海中，并深深地印在了我的脑海中。他的写作学分数量让第二多产的作家相形见绌(彼得·梅尔曼和拉里·查尔斯各19学分)。拉里·戴维的角色在《宋飞正传》尚未流行、仍在摸索中的剧集的早期尤为关键。拉里·戴维写了第一季5集中的4集和第二季12集中的8集。在第三季中，他的总集数增加到了14集，但到了这个时候，这个系列已经有整整23集了。</p><p id="32aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为一名作家，拉里·戴维有效地将这部剧变成了现在的样子，并写出了剧中的角色是谁，以及他们将成为谁。</p><h1 id="f24b" class="kj kk hi bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">拉里·戴维分类器</h1><p id="d4a7" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">拉里·戴维对宋飞的贡献是巨大的。他惊人的58集的写作学分占总集数的三分之一。他对剧中人物和风格的影响是不可估量的，是吗？如果拉里·戴维如此有用；在创造这些角色的过程中，我能从他自己写的情节中提取出多少拉里·戴维的影子呢？</p><p id="cfb9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过使用带有自然语言处理的Keras深度学习模型，目标是训练一个模型来判断拉里·戴维是否写了一段给定的对话。</p><p id="8a3b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每段对话都有标记，根据其剧集和季节编号以及拉里·戴维是否在该集中出现，创建一个“拉里”专题</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es lm"><img src="../Images/24866b5d624c730c2dad8a6986ad7546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/0*rTdlwerReVFAoVVd.png"/></div></figure><p id="5989" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对话的单个片段仅使用20000个最常见的单词进行标记，并转换为统一长度的填充序列。</p><pre class="jz ka kb kc fd ln lo lp lq aw lr bi"><span id="f2f2" class="ls kk hi lo b fi lt lu l lv lw">tokenizer = text.Tokenizer(num_words=20000)<br/>tokenizer.fit_on_texts(list(scripts['Dialogue'].values))<br/>list_tokenized_train = tokenizer.texts_to_sequences(scripts.Dialogue.values)<br/>X_t = pad_sequences(list_tokenized_train,maxlen=100)<br/>y=scripts.Larry.values</span></pre><p id="54df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据被分成75%的训练集和25%的测试集，从训练集中，深度学习模型适合10%的保持以进行验证。仅使用标记对话创建的最佳模型是双向门控递归单元(200个节点)和三个密集连接的层(200、100、50个节点)以及训练2个时期的下降(0.2)。听起来，确实是，一整口。分解一下:</p><ul class=""><li id="b561" class="lx ly hi ih b ii ij im in iq lz iu ma iy mb jc mc md me mf bi translated">双向-双向层在向前和向后两个方向上顺序处理数据，每个方向使用一半的节点。这种结构允许模型具有关于每一步序列的向前和向后信息。</li></ul><figure class="jz ka kb kc fd kd er es paragraph-image"><div role="button" tabindex="0" class="ke kf di kg bf kh"><div class="er es mg"><img src="../Images/43d3909d11bffcfb334d86cd37def07a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*U0RAvtwdfcvMiwAg.jpg"/></div></div><figcaption class="mh mi et er es mj mk bd b be z dx translated"><em class="ml">双向图</em></figcaption></figure><ul class=""><li id="f448" class="lx ly hi ih b ii ij im in iq lz iu ma iy mb jc mc md me mf bi translated">门控递归单元-门控递归单元是一种跨数据步骤传递部分(而非全部)内部状态信息的单元。通过不断更新他们的内部状态，他们可以学习什么是重要的要记住，什么时候可以忘记它。</li></ul><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es mm"><img src="../Images/bf124d62f7abc07f14ad4e20b52877bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/0*ZdnghRv8a-MlEJ0N.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx translated"><em class="ml">门控循环单元图</em></figcaption></figure><ul class=""><li id="feb4" class="lx ly hi ih b ii ij im in iq lz iu ma iy mb jc mc md me mf bi translated">密集连接层——完全连接层中的神经元与前一层中的所有激活具有完全连接。因此，它们的激活可以通过矩阵乘法以及随后的偏置偏移来计算。</li></ul><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es mn"><img src="../Images/890ab88d284a8ea59b06b17289af7d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/0*Is9dM91EisHYQgcw.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx translated"><em class="ml">密集连接的神经网络</em></figcaption></figure><ul class=""><li id="7bdc" class="lx ly hi ih b ii ij im in iq lz iu ma iy mb jc mc md me mf bi translated">Dropout根据选定的概率，随机选择单个节点从网络中退出。这防止了对特定节点路径的过度依赖和过度拟合</li></ul><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es mo"><img src="../Images/7ba7adcaaf12eef34d9355230112712e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*8jgIUNOrkNusIDuW.png"/></div><figcaption class="mh mi et er es mj mk bd b be z dx translated">Srivastava，Nitish，等.“辍学:防止神经网络过度拟合的简单方法”</figcaption></figure><ul class=""><li id="fc78" class="lx ly hi ih b ii ij im in iq lz iu ma iy mb jc mc md me mf bi translated">epoch —通常定义为“一遍整个数据集”，用于将训练分成不同的阶段，这对于日志记录和定期评估非常有用。</li></ul><p id="1c71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是啊。你知道，深度学习神经网络…</p><figure class="jz ka kb kc fd kd er es paragraph-image"><div class="er es mp"><img src="../Images/bdcd68279f811febcda0a9015b2cbda2.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/0*8i9KchwKX2hjSduN.gif"/></div></figure><p id="03b1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是成功了！该模型在预测拉里·戴维是否是一段看不见的对话的作者时有70%的准确率。</p><p id="ef01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从这里开始，我想对Seinfeld脚本进行进一步的编辑，以删除简单的问候和短语，更复杂的语言将更具特色，并可能提高模型预测作者的能力。任何作家都可能写出“嗨”，我不想知道“嗨”，忘了“嗨”。此外，这个模型没有考虑到谁在说话，拉里如何写个人角色，这增加了另一个可以解决的复杂性。</p><p id="28c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我非常希望这只是系列文章的第一部分。随着我在自己的生活中不断回到宋飞，我希望在我作为一名数据科学家的职业生涯中继续回到宋飞。</p></div></div>    
</body>
</html>