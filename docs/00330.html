<html>
<head>
<title>How to do Hyper-parameters search with Bayesian optimization for Keras model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用贝叶斯优化进行Keras模型的超参数搜索</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-do-hyper-parameters-search-with-bayesian-optimization-for-keras-model-6b9918caa0b1?source=collection_archive---------0-----------------------#2019-04-06">https://medium.com/analytics-vidhya/how-to-do-hyper-parameters-search-with-bayesian-optimization-for-keras-model-6b9918caa0b1?source=collection_archive---------0-----------------------#2019-04-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b7510a9b43dc135f5e3ed702acd251cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*1OJpVW9cO1PEOdlq.png"/></div></div></figure><p id="9856" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">与网格搜索和随机搜索等更简单的超参数搜索方法相比，贝叶斯优化建立在贝叶斯推理和高斯过程的基础上，试图以尽可能少的迭代次数找到未知函数的最大值。它特别适合于优化高成本函数，如深度学习模型的超参数搜索，或其他探索和利用之间的平衡很重要的情况。</p><p id="babb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们要用的贝叶斯优化包是<a class="ae jo" href="https://github.com/fmfn/BayesianOptimization" rel="noopener ugc nofollow" target="_blank"> BayesianOptimization </a>，可以用下面的命令安装，</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="ba25" class="jy jz hi ju b fi ka kb l kc kd">pip install bayesian-optimization</span></pre><p id="ec19" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，我们将指定要优化的函数，在我们的例子中是超参数搜索，该函数将一组超参数值作为输入，并输出贝叶斯优化器的评估精度。在该函数中，将使用指定的超参数构建一个新模型，对多个时期进行训练，并根据一组指标进行评估。每个新评估的准确度将成为贝叶斯优化器的新观察，其有助于下一次搜索超参数的值。</p><p id="7d7a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们首先创建一个助手函数，它用各种参数构建模型。</p><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="ke kf l"/></div></figure><p id="bff7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，这里是要用贝叶斯优化器优化的函数，<strong class="is hj">分部</strong>函数负责两个参数—<code class="du kg kh ki ju b">fit_with</code>中的<code class="du kg kh ki ju b">input_shape</code>和<code class="du kg kh ki ju b">verbose</code>，它们在运行时具有固定值。</p><p id="d8e6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该函数采用两个超参数进行搜索，即“dropout_2”层的辍学率和学习率值，它为1个时期训练模型，并输出贝叶斯优化器的评估精度。</p><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="ke kf l"/></div></figure><p id="6508" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj"> BayesianOptimization </strong>对象将开箱即用，无需太多调整。构造函数接受要优化的函数以及要搜索的超参数的边界。您应该知道的主要方法是<code class="du kg kh ki ju b">maximize</code>，它完全按照您的想法工作，在给定超参数的情况下最大化评估精度。</p><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="ke kf l"/></div></figure><p id="0a7a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里有许多你可以传递给<code class="du kg kh ki ju b">maximize</code>的参数，然而，最重要的是:</p><ul class=""><li id="35bd" class="kj kk hi is b it iu ix iy jb kl jf km jj kn jn ko kp kq kr bi translated"><code class="du kg kh ki ju b">n_iter</code>:你要执行多少步贝叶斯优化。步数越多，你就越有可能找到一个好的最大值。</li><li id="e874" class="kj kk hi is b it ks ix kt jb ku jf kv jj kw jn ko kp kq kr bi translated"><code class="du kg kh ki ju b">init_points</code>:您想进行多少步<strong class="is hj">随机</strong>探索。随机探索有助于探索空间的多样化。</li></ul><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="85d6" class="jy jz hi ju b fi ka kb l kc kd">|   iter    |  target   | dropou... |    lr     |<br/>-------------------------------------------------<br/>468/468 [==============================] - 4s 8ms/step - loss: 0.2575 - acc: 0.9246<br/>Test loss: 0.061651699058711526<br/>Test accuracy: 0.9828125<br/>|  1        |  0.9828   |  0.2668   |  0.007231 |<br/>468/468 [==============================] - 4s 8ms/step - loss: 0.2065 - acc: 0.9363<br/>Test loss: 0.04886047407053411<br/>Test accuracy: 0.9828125<br/>|  2        |  0.9828   |  0.1      |  0.003093 |<br/>468/468 [==============================] - 4s 8ms/step - loss: 0.2199 - acc: 0.9336<br/>Test loss: 0.05553104653954506<br/>Test accuracy: 0.98125<br/>|  3        |  0.9812   |  0.1587   |  0.001014 |<br/>468/468 [==============================] - 4s 9ms/step - loss: 0.2075 - acc: 0.9390<br/>Test loss: 0.04128134781494737<br/><strong class="ju hj">Test accuracy: 0.9890625</strong><br/><strong class="ju hj">|  4        |  0.9891   |  0.1745   |  0.003521 |</strong></span></pre><p id="1814" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">经过4次搜索，用找到的超参数建立的模型仅用一个历元的训练就达到了98.9%的评估精度。</p><h1 id="ba07" class="kx jz hi bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">与其他搜索方法相比</h1><p id="de20" class="pw-post-body-paragraph iq ir hi is b it lu iv iw ix lv iz ja jb lw jd je jf lx jh ji jj ly jl jm jn hb bi translated">与在有限数量的离散超参数组合中进行搜索的网格搜索不同，高斯过程的贝叶斯优化的性质不允许以简单/直观的方式处理离散参数。</p><p id="9ce6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">例如，我们想从选项列表中搜索密集层的神经元数目。为了应用贝叶斯优化，有必要在构建模型之前将输入参数显式地转换为离散参数。</p><p id="709d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你可以这样做。</p><figure class="jp jq jr js fd ij"><div class="bz dy l di"><div class="ke kf l"/></div></figure><p id="4100" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在构建模型之前，密集层神经元将被映射到3个唯一的离散值，128、256和384。</p><p id="5d57" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在贝叶斯优化中，每个下一个搜索值取决于先前的观察值(先前的评估精度)，整个优化过程可能很难像网格或随机搜索方法那样被分布或并行化。</p><h1 id="6436" class="kx jz hi bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">结论和进一步阅读</h1><p id="d394" class="pw-post-body-paragraph iq ir hi is b it lu iv iw ix lv iz ja jb lw jd je jf lx jh ji jj ly jl jm jn hb bi translated">这个快速教程介绍了如何使用贝叶斯优化进行超参数搜索，与网格或随机等其他方法相比，它可能更有效，因为每次搜索都是从以前的搜索结果中"<strong class="is hj">引导的</strong>。</p><h1 id="f7d3" class="kx jz hi bd ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt bi translated">一些你可能会觉得有用的材料</h1><ul class=""><li id="7f1f" class="kj kk hi is b it lu ix lv jb lz jf ma jj mb jn ko kp kq kr bi translated"><a class="ae jo" href="https://github.com/fmfn/BayesianOptimization" rel="noopener ugc nofollow" target="_blank">贝叶斯优化</a> —本教程中使用的高斯过程全局优化的Python实现。</li><li id="419f" class="kj kk hi is b it ks ix kt jb ku jf kv jj kw jn ko kp kq kr bi translated"><a class="ae jo" href="https://www.dlology.com/blog/how-to-perform-keras-hyperparameter-optimization-on-tpu-for-free/" rel="noopener ugc nofollow" target="_blank">如何在TPU上更快地免费执行Keras超参数优化x3</a>—我之前的关于使用Colab的免费TPU执行网格超参数搜索的教程。</li></ul><h2 id="2591" class="jy jz hi bd ky mc md me lc mf mg mh lg jb mi mj lk jf mk ml lo jj mm mn ls mo bi translated">查看我的<a class="ae jo" href="https://github.com/Tony607/Keras_BayesianOptimization" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上的完整源代码。</h2><p id="7fb4" class="pw-post-body-paragraph iq ir hi is b it lu iv iw ix lv iz ja jb lw jd je jf lx jh ji jj ly jl jm jn hb bi translated"><a class="ae jo" href="https://twitter.com/intent/tweet?url=https%3A//www.dlology.com/blog/how-to-do-hyperparameter-search-with-baysian-optimization-for-keras-model/&amp;text=How%20to%20do%20Hyper-parameters%20search%20with%20Bayesian%20optimization%20for%20Keras%20model" rel="noopener ugc nofollow" target="_blank">在Twitter上分享</a> <a class="ae jo" href="https://www.facebook.com/sharer/sharer.php?u=https://www.dlology.com/blog/how-to-do-hyperparameter-search-with-baysian-optimization-for-keras-model/" rel="noopener ugc nofollow" target="_blank">在脸书分享</a></p></div><div class="ab cl mp mq gp mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="hb hc hd he hf"><p id="9a14" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="mw">原载于</em><a class="ae jo" href="https://www.dlology.com/blog/how-to-do-hyperparameter-search-with-baysian-optimization-for-keras-model/" rel="noopener ugc nofollow" target="_blank"><em class="mw">www.dlology.com</em></a><em class="mw">。</em></p></div></div>    
</body>
</html>