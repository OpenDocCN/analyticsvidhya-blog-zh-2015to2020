<html>
<head>
<title>Coding Logistic Regression in Python From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始用Python编写逻辑回归代码</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/coding-logistic-regression-in-python-from-scratch-57284dcbfbff?source=collection_archive---------2-----------------------#2020-05-13">https://medium.com/analytics-vidhya/coding-logistic-regression-in-python-from-scratch-57284dcbfbff?source=collection_archive---------2-----------------------#2020-05-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/1941333f60907a1d014e848f2c60d807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ODhVXC_e6WclmxC0VlZ3sw.png"/></div></div></figure><p id="f9da" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">目的是使用我们拥有的原始数学知识和概念，从零开始为二元分类编码逻辑回归。</p><p id="ccf9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是逻辑回归系列的第二部分:</p><ol class=""><li id="7f14" class="jo jp hi is b it iu ix iy jb jq jf jr jj js jn jt ju jv jw bi translated"><a class="ae jx" rel="noopener" href="/@omrastogi/fundamentals-of-logistic-regression-bde451712652">逻辑回归的基本原理</a></li><li id="76ad" class="jo jp hi is b it jy ix jz jb ka jf kb jj kc jn jt ju jv jw bi translated">从头开始用Python编写逻辑回归代码</li></ol><p id="03ba" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先我们需要导入numpy。NumPy是一个处理复杂数组计算的类，可以快速减少计算时间。你可以在这里 了解numpy <a class="ae jx" rel="noopener" href="/@omrastogi/master-numpy-in-45-minutes-74b2460ecb00"> <em class="kd">。</em></a></p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="dcbe" class="kn ko hi kj b fi kp kq l kr ks">import numpy as np</span></pre><p id="7730" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了打包不同的方法，我们需要创建一个名为“MyLogisticRegression”的类。该类所采取的参数有:<br/><strong class="is hj">【learning _ rate】</strong>-它确定模型的学习速度，在梯度下降算法中<br/> <strong class="is hj"> num_iteration </strong> -它确定我们需要运行梯度下降算法的次数。</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="c740" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">class MyLogisticRegression:</strong><br/>    <br/>    <strong class="kj hj">def __init__(self, learning_rate = 1, num_iterations = 2000):</strong><br/>       self.learning_rate = learning_rate<br/>       self.num_iterations = num_iterations<br/>       self.w = []<br/>       self.b = 0</span></pre><p id="6d4f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们都准备好了，首先要为主要的算法打下基础。</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="10e9" class="kn ko hi kj b fi kp kq l kr ks">    <strong class="kj hj">def initialize_weight(self,dim):</strong><br/>    <em class="kd">"""<br/>    This function creates a vector of zeros of shape (dim, 1)      for w and initializes b to 0.<br/>    Argument:<br/>    dim -- size of the w vector we want (or number of parameters  in this case)<br/>    """</em><br/>       w = np.zeros((dim,1))<br/>       b = 0<br/>       return w, b</span></pre><h2 id="959f" class="kn ko hi bd kt ku kv kw kx ky kz la lb jb lc ld le jf lf lg lh jj li lj lk ll bi translated">乙状结肠的</h2><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lm"><img src="../Images/9a7456c0094066712fac430cfddb80e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*l3ycAU8yQuMSbCsZ.png"/></div></div></figure><p id="a90d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">逻辑回归最基本和最重要的元素是逻辑函数，也称为sigmoid。</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="91ec" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">    def sigmoid(self,z):</strong><br/>    <em class="kd">"""<br/>    Compute the sigmoid of z<br/>    Argument:<br/>    z -- is the decision boundary of the classifier<br/>    """</em><br/>        s = 1/(1 + np.exp(-z)) <br/>        return s</span></pre><h2 id="152c" class="kn ko hi bd kt ku kv kw kx ky kz la lb jb lc ld le jf lf lg lh jj li lj lk ll bi translated">假设</h2><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es ln"><img src="../Images/9925df520450afbed049ea286fca61fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*X0RRr2-tLxGY-6jLvl43WQ.png"/></div></figure><p id="8911" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在我们写一个函数来定义假设。<em class="kd">‘w’</em>上的下标代表<em class="kd">权重向量</em>的转置。</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="bcb0" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">    def hypothesis(self,w,X,b):</strong><br/>    <em class="kd">"""<br/>    This function calculates the hypothesis for the present model<br/>    Argument:<br/>     w -- weight vector<br/>     X -- The input vector<br/>     b -- The bias vector<br/>    """</em><br/>        H = self.sigmoid(np.dot(w.T,X)+b) <br/>        return H</span></pre><h2 id="0680" class="kn ko hi bd kt ku kv kw kx ky kz la lb jb lc ld le jf lf lg lh jj li lj lk ll bi translated">成本函数和梯度</h2><p id="f438" class="pw-post-body-paragraph iq ir hi is b it lo iv iw ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn hb bi translated"><strong class="is hj">代价函数</strong> <em class="kd"> </em>是一个<em class="kd">函数</em>，它衡量机器学习模型对于给定数据的性能。当<strong class="is hj">梯度</strong>量化时，模型的表现会有多好。</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lt"><img src="../Images/881bb625685cddd467e4382cae3b3e45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*-KZYegKqWLmQLSui.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">成本函数公式</figcaption></figure><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="bf5e" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">    def cost(self,H,Y,m):</strong><br/><em class="kd">    """<br/>    This function calculates the cost of hypothesis<br/>    Arguments: <br/>     H -- The hypothesis vector <br/>     Y -- The output <br/>     m -- Number training samples<br/>    """</em></span><span id="51aa" class="kn ko hi kj b fi ly kq l kr ks">        cost = -np.sum(Y*np.log(H)+ (1-Y)*np.log(1-H))/m <br/>        cost = np.squeeze(cost)   <br/>        return cost</span></pre><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/cc375b988e3022ed914f48e070f2a3ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*tX7tsojc4Gc1420a.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">梯度的计算</figcaption></figure><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="c6cd" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">    def cal_gradient(self, w,H,X,Y):</strong><br/><em class="kd">    """<br/>    Calculates gradient of the given model in learning space<br/>    """</em><br/>        m = X.shape[1]<br/>        dw = np.dot(X,(H-Y).T)/m<br/>        db = np.sum(H-Y)/m<br/>        grads = {"dw": dw,<br/>                 "db": db}<br/>        return grads</span><span id="28cf" class="kn ko hi kj b fi ly kq l kr ks"> <br/>    <strong class="kj hj">def gradient_position(self, w, b, X, Y):</strong><br/>    """<br/>    It just gets calls various functions to get status of learning model</span><span id="253e" class="kn ko hi kj b fi ly kq l kr ks">    Arguments:<br/>     w -- weights, a numpy array of size (no. of features, 1)<br/>     b -- bias, a scalar<br/>     X -- data of size (no. of features, number of examples)<br/>     Y -- true "label" vector (containing 0 or 1 ) of size (1, number of examples)<br/>    """<br/>  <br/>        m = X.shape[1]<br/>        H = self.hypothesis(w,X,b)         # compute activation<br/>        cost = self.cost(H,Y,m)               # compute cost<br/>        grads = self.cal_gradient(w, H, X, Y) # compute gradient<br/>        return grads, cost</span></pre><h2 id="5c1c" class="kn ko hi bd kt ku kv kw kx ky kz la lb jb lc ld le jf lf lg lh jj li lj lk ll bi translated"><strong class="ak">梯度下降算法</strong></h2><p id="2400" class="pw-post-body-paragraph iq ir hi is b it lo iv iw ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn hb bi translated">梯度下降是一种优化算法，用于通过沿梯度负值定义的最陡下降方向迭代移动来最小化某个函数。在机器学习中，我们使用梯度下降来更新我们模型的参数(w，b)。</p><figure class="ke kf kg kh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/0481000218d60b136bcfc3af7281ffba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0j8wjNYdf3mfRyK4"/></div></div></figure><p id="eb70" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该算法的意义在于更新规则，根据参数的当前陡度来更新参数。</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="841c" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">    def gradient_descent(self, w, b, X, Y, print_cost = False):</strong><br/><em class="kd">    """<br/>    This function optimizes w and b by running a gradient descent algorithm<br/> <br/>    Arguments:<br/>    w — weights, a numpy array of size (num_px * num_px * 3, 1)<br/>    b — bias, a scalar<br/>    X -- data of size (no. of features, number of examples)<br/>    Y -- true "label" vector (containing 0 or 1 ) of size (1, number of examples)<br/>    print_cost — True to print the loss every 100 steps<br/> <br/>    Returns:<br/>    params — dictionary containing the weights w and bias b<br/>    grads — dictionary containing the gradients of the weights and bias with respect to the cost function<br/>    costs — list of all the costs computed during the optimization, this will be used to plot the learning curve.<br/>    """</em><br/> <br/>        costs = []<br/> <br/>        for i in range(self.num_iterations):<br/>        # Cost and gradient calculation <br/>            grads, cost = self.gradient_position(w,b,X,Y)<br/> <br/> <br/>        # Retrieve derivatives from grads<br/>        dw = grads[“dw”]<br/>        db = grads[“db”]<br/> <br/><strong class="kj hj">        # update rule <br/>        w = w — (self.learning_rate * dw) <br/>        b = b — (self.learning_rate * db)</strong><br/> <br/>        # Record the costs<br/>        if i % 100 == 0:<br/>            costs.append(cost)<br/> <br/>        # Print the cost every 100 training iterations<br/>        if print_cost and i % 100 == 0:<br/>             print (“Cost after iteration %i: %f” %(i, cost))<br/> <br/> <br/>        params = {“w”: w,<br/>                  “b”: b}<br/> <br/>       grads = {“dw”: dw,<br/>                “db”: db}<br/> <br/>       return params, grads, costs</span></pre><h2 id="d480" class="kn ko hi bd kt ku kv kw kx ky kz la lb jb lc ld le jf lf lg lh jj li lj lk ll bi translated">预测</h2><p id="e43a" class="pw-post-body-paragraph iq ir hi is b it lo iv iw ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn hb bi translated">假设函数给我们的概率是y = 1。对于大于0.5的h，我们通过指定p = 1来解决这个问题。</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="4d2f" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">    def predict(self,X):</strong><br/><em class="kd">    '''<br/>    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)<br/>  <br/>    Arguments:<br/>    w -- weights, a numpy array of size (n, 1)<br/>    b -- bias, a scalar<br/>    X -- data of size (num_px * num_px * 3, number of examples)<br/>  <br/>    Returns:<br/>    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X<br/>    '''</em><br/>        X = np.array(X)<br/>        m = X.shape[1]<br/>  <br/>        Y_prediction = np.zeros((1,m))<br/>  <br/>        w = self.w.reshape(X.shape[0], 1)<br/>        b = self.b<br/>        # Compute vector "H" <br/>        H = self.hypothesis(w, X, b)<br/> <br/>        for i in range(H.shape[1]):<br/>        # Convert probabilities H[0,i] to actual predictions p[0,i]<br/>            if H[0,i] &gt;= 0.5:<br/>                Y_prediction[0,i] = 1<br/>            else: <br/>                Y_prediction[0,i] = 0<br/>   <br/>       return Y_prediction</span></pre><h2 id="fffb" class="kn ko hi bd kt ku kv kw kx ky kz la lb jb lc ld le jf lf lg lh jj li lj lk ll bi translated">训练模型功能</h2><p id="28a6" class="pw-post-body-paragraph iq ir hi is b it lo iv iw ix lp iz ja jb lq jd je jf lr jh ji jj ls jl jm jn hb bi translated">这个方法由用户直接调用来训练假设。这是一种可行的方法。</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="a3eb" class="kn ko hi kj b fi kp kq l kr ks"><strong class="kj hj">    def train_model(self, X_train, Y_train, X_test, Y_test, print_cost = False):</strong><br/>    """<br/>    Builds the logistic regression model by calling the function you’ve implemented previously<br/> <br/>    Arguments:<br/>    X_train — training set represented by a numpy array of shape (features, m_train)<br/>    Y_train — training labels represented by a numpy array (vector) of shape (1, m_train)<br/>    X_test — test set represented by a numpy array of shape (features, m_test)<br/>    Y_test — test labels represented by a numpy array (vector) of shape (1, m_test)<br/>    print_cost — Set to true to print the cost every 100 iterations<br/> <br/>    Returns:<br/>    d — dictionary containing information about the model.<br/>    """</span><span id="3e84" class="kn ko hi kj b fi ly kq l kr ks"># initialize parameters with zeros <br/>        dim = np.shape(X_train)[0]<br/>        w, b = self.initialize_weight(dim)</span><span id="7226" class="kn ko hi kj b fi ly kq l kr ks"># Gradient descent <br/>        parameters, grads, costs = self.gradient_descent(w, b, X_train, Y_train, print_cost = False)<br/> <br/> # Retrieve parameters w and b from dictionary “parameters”<br/>        self.w = parameters[“w”]<br/>        self.b = parameters[“b”]<br/> <br/> # Predict test/train set examples <br/>        Y_prediction_test = self.predict(X_test)<br/>        Y_prediction_train = self.predict(X_train)</span><span id="0658" class="kn ko hi kj b fi ly kq l kr ks"># Print train/test Errors<br/>        train_score = 100 — np.mean(np.abs(Y_prediction_train — Y_train)) * 100<br/>        test_score = 100 — np.mean(np.abs(Y_prediction_test — Y_test)) * 100<br/>        print(“train accuracy: {} %”.format(100 — np.mean(np.abs(Y_prediction_train — Y_train)) * 100))<br/>        print(“test accuracy: {} %”.format(100 — np.mean(np.abs(Y_prediction_test — Y_test)) * 100))</span><span id="2db3" class="kn ko hi kj b fi ly kq l kr ks">        d = {“costs”: costs,<br/>             “Y_prediction_test”: Y_prediction_test, <br/>             “Y_prediction_train” : Y_prediction_train, <br/>             “w” : self.w, <br/>             “b” : self.b,<br/>             “learning_rate” : self.learning_rate,<br/>             “num_iterations”: self.num_iterations,<br/>             “train accuracy”: train_score,<br/>             “test accuracy” : test_score}<br/> <br/>         return d</span></pre><h2 id="3720" class="kn ko hi bd kt ku kv kw kx ky kz la lb jb lc ld le jf lf lg lh jj li lj lk ll bi translated">在小数据集上测试</h2><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="7ff9" class="kn ko hi kj b fi kp kq l kr ks">#Dataset<br/>X_train = np.array([[5,6,1,3,7,4,10,1,2,0,5,3,1,4],[1,2,0,2,3,3,9,4,4,3,6,5,3,7]])<br/>Y_train = np.array([[0,0,0,0,0,0,0,1,1,1,1,1,1,1]])<br/>X_test  = np.array([[2,3,3,3,2,4],[1,1,0,7,6,5]])<br/>Y_test  = np.array([[0,0,0,1,1,1]])</span></pre><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/64496ca66c44d3028db5501b087316f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/0*hMysd8AsRz9XLnAp.png"/></div></figure><p id="0e89" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们根据默认值调用该类</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="ac86" class="kn ko hi kj b fi kp kq l kr ks">clf = MyLogisticRegression()<br/>d = clf.train_model(X_train, Y_train, X_test, Y_test)<br/>print (d["train accuracy"])</span><span id="c7a6" class="kn ko hi kj b fi ly kq l kr ks"><strong class="kj hj"><em class="kd">#Output<br/>train accuracy: 100.0 %<br/>test accuracy: 100.0 %<br/>100.0</em></strong></span></pre><p id="db36" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将设置一个非常小的学习率和迭代次数</p><pre class="ke kf kg kh fd ki kj kk kl aw km bi"><span id="573b" class="kn ko hi kj b fi kp kq l kr ks">clf = MyLogisticRegression(0.001, 100)<br/>d = clf.train_model(X_train, Y_train, X_test, Y_test)</span><span id="7586" class="kn ko hi kj b fi ly kq l kr ks"><strong class="kj hj"><em class="kd">#Output<br/>train accuracy: 92.85714285714286 %<br/>test accuracy: 83.33333333333334 %</em></strong></span></pre><figure class="ke kf kg kh fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/59a66cf8a27e856b0029c0613d6dcc10.png" data-original-src="https://miro.medium.com/v2/resize:fit:246/format:webp/0*ronbhrhKH60uyMLf.png"/></div></figure><blockquote class="md"><p id="3de4" class="me mf hi bd mg mh mi mj mk ml mm jn dx translated">如果你坚持到最后——请鼓掌。这会让我有动力写更多。谢谢你。</p></blockquote></div></div>    
</body>
</html>