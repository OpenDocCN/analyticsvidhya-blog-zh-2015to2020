<html>
<head>
<title>Comprehensive Support Vector Machines Guide - Using Illusion to Solve Reality!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">综合支持向量机指南-用幻觉解决现实！</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/comprehensive-support-vector-machines-guide-using-illusion-to-solve-reality-ad3136d8f877?source=collection_archive---------0-----------------------#2018-09-29">https://medium.com/analytics-vidhya/comprehensive-support-vector-machines-guide-using-illusion-to-solve-reality-ad3136d8f877?source=collection_archive---------0-----------------------#2018-09-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class="ev ex if ig ih ab cb"><figure class="ii ij ik il im in io paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><img src="../Images/797c8871e383f33c01196863bd0ba573.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*F2S7PeiprYnIMsSMzrAbJw.jpeg"/></div></figure><figure class="ii ij iv il im in io paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><img src="../Images/6727fd41a505b5a183d60ccaf3577a6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*TW9KvYs3hhJMWTcJ1930qw.jpeg"/></div></figure></div><p id="6ced" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi ju translated">在梦里寻梦！很少有人需要提示就能猜到左边的图片来自电影《盗梦空间》。旋转陀螺的行为有助于区分现实和幻觉。这是一个迷人的概念，试图直观地表达潜意识。《盗梦空间》是一部基于清醒梦的电影。科幻小说展示了在现实世界中无法实现的事情，如何通过将世界转换为虚拟现实来实现，然后在目标实现后，将世界转换回现实。</p><p id="be2e" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这部电影由莱昂纳多·迪卡普里奥饰演多姆·科布，一个小偷，在他的搭档亚瑟(约瑟夫·戈登-莱维特饰)的帮助下，在受害者睡觉时从他们的梦中窃取信息。在最近一次商业间谍活动中遭遇罕见的失败后，他们的目标斋藤(渡边谦饰)给了多姆一个无法拒绝的提议。他得到了一份不可能的工作，那就是把一个想法植入费舍尔的大脑，这在现实世界中是不可能发生的。</p><p id="9723" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">人们看到角色们越来越深地进入他们任务目标的梦境——一家能源集团的继承人。他们通过他的潜意识层来植入一个想法:他们想让企业集团分崩离析，他们想让这个人来做决定。因此，目标是通过将世界转换到另一个想象和不真实的维度来实现的，但一旦目标实现，主体就会被带回现实。</p><p id="61ae" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这正是支持向量机的工作原理。右图描绘了通过将数据转换到更高维度而实现的分类(白点和黑点)。女孩在空中的跳跃象征着转变的行为。当我们进入主题时，我们将会进入技术细节。</p><h1 id="ec3c" class="kd ke hi bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated"><strong class="ak">技术介绍</strong></h1><p id="6d1f" class="pw-post-body-paragraph iw ix hi iy b iz lb jb jc jd lc jf jg jh ld jj jk jl le jn jo jp lf jr js jt hb bi ju translated"><span class="l jv jw jx bm jy jz ka kb kc di"> S </span>支持向量机(upport Vector machine)是一个线性分类器，它通过在两个或多个类别之间绘制一个超平面来分离目标变量的各个层次。理解支持向量机的三大概念，通俗地说就是<strong class="iy hj"> (A)变换，(B)错觉(C)分离</strong>。下图尽可能以最滑稽的方式解释了SVM的要旨。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lg"><img src="../Images/90135577e5f6dfd4b0a3309dd1802492.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*L8hXSMN30e9tPPhqdYH68g.jpeg"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">*孩子，象征性地，分开丈夫和妻子，尽可能最大限度地防止冲突。</figcaption></figure><p id="78c7" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="iy hj"> A .变换:</strong>如果类在二维空间中不是线性可分的，SVM使用更高维的空间来绘制目标变量的类之间的分离超平面。这里要注意的关键点是，<strong class="iy hj">当无法线性分离时，它会将数据转换到一个更高维的空间，以获得所需的分离。</strong></p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><div class="er es lp"><img src="../Images/19fc9415d9ce98e879b614d24d803625.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OpPID41jkJ70dslLHdP0_g.jpeg"/></div></div></figure><p id="dca5" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">SVM使用内核技巧，如左图所示，将数据转换到不同的维度以得到想要的结果。然后，结果以分类标签的形式被带回二维空间，用于现实生活中。下面是上述解释的直观表示。这是SVM在生物医药领域的一个真实应用。</p><p id="93ce" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">对基于细胞的高含量筛选(HCS)实验中产生的大量数据进行分析一直是一个具有挑战性的命题。在这里，SVM被用来寻找最好地将基因组分为已知和未知样本，即阳性(A)和阴性(B)样本的线(或平面)。特征是每个样品中细胞的数量和样品中细胞的强度/密度。有关更多详细信息，请参考原文— <a class="ae lq" href="http://www.jbiomed.com/v02p0078.pdf" rel="noopener ugc nofollow" target="_blank">高含量筛查数据的多参数分析</a>。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><div class="er es lr"><img src="../Images/4b70c03f319777f8f7149a3ad7e77674.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YEpDwpVe9SNdj5zLJT3YgQ.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">从输入空间到更高维特征空间的数据的<strong class="bd kf">变换</strong>用于线性分离，随后是回到输入空间的<strong class="bd kf">变换</strong>，其中分类边界看起来是非线性的。</figcaption></figure><p id="3415" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="iy hj"> B .幻觉:看不见，但实际存在，并能在更高维度被感知的东西，肉眼看来是虚幻的。</strong>人喝醉了才会“high”。他们想要“兴奋”以获得一种远离日常单调生活的空灵感。什么水平的“高”和一个人如何到达那里是一个不同的问题，因为，喝醉是一个选择的问题。</p><p id="4ffc" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">一些人只需要想象力就能点燃，而另一些人则借助人造物质来达到同样的目的。我们在陶醉状态下看到或感觉到的，可能看起来像是幻觉，而在那个精神空间，许多在正常状态下不可能的事情，看起来是可能的。问题是，那些“可能”的东西能不能转化回现实。对一些人来说，这已经被证明是可能的。</p><p id="f62c" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">更进一步，想象一下我们有两组不同颜色的球，比如红色和蓝色。这些球躺在一个盘子上，这显然是二维的。球是混合的，不可能通过画一条直线来区分红色和蓝色。让我们假设蓝色的球比红色的球重，我们通过向整个盘子底部施加相等的推力将所有的球抛向空中。</p><p id="ced6" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">由于球的重量不同，红色的球会比蓝色的球升到不同的高度。当它们处于这种状态(在空中)时，如果我们能想象一张纸放在各组之间，我们就能想象出我们想象的线性分离/分类边界。详见下文。肉眼看起来不可分的东西，通过创造更高维度的幻觉，并在那种状态下分离它们，是可能被分离的。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><div class="er es ls"><img src="../Images/135b9309435f39292b6efd71b679631e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Ux-ESzGYsBc8DcccC5J1w.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">2D空间中的不可分类(左图)，由高维特征空间中的超平面线性分隔(右图)</figcaption></figure><p id="f614" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="iy hj"> C .分离:</strong>每种算法都有其显著的数学特征，这些特征决定了分类如何工作。决策树使用“熵”或“基尼指数”来分割变量，逻辑回归的基本概念是比值比等等。</p><p id="0dc0" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">SVM的基本概念是算法在目标变量的类别之间画出尽可能宽的界限的能力，即位于界限一侧的观察值属于一个类别，而位于界限另一侧的观察值属于另一个类别。余量越宽，类别之间的距离越大，分类器的置信度越高。</p><p id="69d6" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">边距由离由边距定义的分离板的中值最远的观测值决定。这些定义类别间最大可能分离面的观察结果被称为<strong class="iy hj">支持向量</strong>。这些观察位置的任何变化都会影响分离裕度，从而影响分类器的性能。远离分离板块的其他观测值的移动，只要它们像以前一样停留在边缘的同一侧，就没有什么影响。</p><p id="0b68" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">SVM的优点是，通过绘制平板而不仅仅是一条分隔线，如果它是一条单线，可能会有许多直线来分隔各个类别，如下图所示。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><div class="er es lt"><img src="../Images/d72fc003ccc055dc75e9a871f6843f1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NAwkx-UYh_vnWwreS509cQ.png"/></div></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">图片来自Trevor Hastie写的《统计学习入门》</figcaption></figure><p id="cf4a" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">左图显示了三条可能的分隔线。哪个最好？离线两边最近的观测值最远的一个，如第二张图所示。这种分类可以通过任何线性分类器来完成。</p><p id="955f" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">那么SVM还能提供什么额外的东西呢？如前所述，SVM不仅通过一条直线，还通过一个超平面来分隔类别，然后超平面在概念上被约束，以确保类别之间存在比通过一条直线分隔更大的强制性分隔。在一个不同的2D例子中，通过SVM实现的分离可以如下所示。我们看到这两个类别之间的距离为“M”。</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es lu"><img src="../Images/0692150815aa87b72b23b75c439a3170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*dMq-fTUWmChjEGX4NcxgUQ.png"/></div></figure><p id="80bd" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">已经在非常一般的水平上介绍了SVM，现在是数据科学接管的时候了。这里是我们将涉及到的主题的细节。</p><p id="6029" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="iy hj">目录</strong></p><ol class=""><li id="e615" class="lv lw hi iy b iz ja jd je jh lx jl ly jp lz jt ma mb mc md bi translated">错觉- SVM线性分离非线性数据。</li><li id="4594" class="lv lw hi iy b iz me jd mf jh mg jl mh jp mi jt ma mb mc md bi translated">幻觉背后的现实——转变的数学提升</li><li id="9cb4" class="lv lw hi iy b iz me jd mf jh mg jl mh jp mi jt ma mb mc md bi translated">陶醉效果的对象——核心技巧</li><li id="8581" class="lv lw hi iy b iz me jd mf jh mg jl mh jp mi jt ma mb mc md bi translated">个案研究</li></ol><p id="d5a6" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="iy hj"> 1。错觉——SVM线性分离非线性数据</strong></p><p id="7ed8" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">我们看到了一个简化的例子，通过将2D变换到一个更高的维度，我们可以线性分离两个不能线性分离的类。这里我们看到了一个如何做到这一点的真实例子。这个例子是在Kaggle 发现的<a class="ae lq" href="https://www.kaggle.com/rakeshrau/social-network-ads#Social_Network_Ads.csv" rel="noopener ugc nofollow" target="_blank">社交网络广告daatset上得出的。</a></p><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><div class="er es mj"><img src="../Images/9b5b2991aa46d0144693b7de1316fcac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s0UrPOyPYP_ZqayK_A-m7g.jpeg"/></div></div></figure><p id="fd3d" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">从上图可以看出对非线性数据进行线性分类的结果。我们看到相当多的错误分类。</p><p id="e6ab" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">然后，我们将数据转换到更高的维度，并查看结果。二次多项式核并没有做得更好，但是径向核做得非常好。通过转换实现的决策界限可以如下所示。</p><div class="lh li lj lk fd ab cb"><figure class="ii ij mk il im in io paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><img src="../Images/b4b032594ccf41a2c14db4197dac0a02.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*wSbB7vWfikiWo0rf_hJp6w.jpeg"/></div></figure><figure class="ii ij ml il im in io paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><img src="../Images/09d279899f28f0f75289ac2c2ef0240e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*wtESgp7Tbb0HvQxpDvYw9Q.jpeg"/></div></figure></div><p id="0e06" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">精度比较证明了上述事实。见下面的工作方式。准确率从80%跃升到90%。</p><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="4869" class="mr ke hi mn b fi ms mt l mu mv"><em class="mw"># Importing the dataset</em><br/>data=read.csv("Social_Network_ads.csv")</span><span id="10f1" class="mr ke hi mn b fi mx mt l mu mv"># Subsetting the data to make it 2 dimensional for visualizationdataset = dataset[3:5]</span><span id="6dae" class="mr ke hi mn b fi mx mt l mu mv"># Encoding the target feature as factor<br/>dataset$Purchased = factor(dataset$Purchased, levels = c(0, 1))</span><span id="4bc7" class="mr ke hi mn b fi mx mt l mu mv"># Splitting the dataset into the Training set and Test set<br/># install.packages('caTools')<br/>library(caTools)<br/>set.seed(123)<br/>split = sample.split(dataset$Purchased, SplitRatio = 0.75)<br/>training_set = subset(dataset, split == TRUE)<br/>test_set=subset(dataset,split==FALSE)<br/># Feature Scaling<br/>training_set[-3] = scale(training_set[-3])<br/>test_set[-3] = scale(test_set[-3])<br/># Fitting SVM to the Training set<br/># install.packages('e1071')<br/>library(e1071)<br/>library(caret)<br/>#Linear Kernel<br/>classifier_Linear = svm(formula = Purchased ~ .,<br/>                 data = training_set,<br/>                 type = 'C-classification',<br/>                 kernel = 'linear')</span><span id="8fc7" class="mr ke hi mn b fi mx mt l mu mv">#Polynomial Kernel<br/>classifier_Polynomial = svm(formula = Purchased ~ .,<br/>                 data = training_set,<br/>                 type = 'C-classification',<br/>                 kernel = 'polynomial',<br/>                 degree=2)</span><span id="ee9e" class="mr ke hi mn b fi mx mt l mu mv">#Polynomial Kernel<br/>classifier_Radial = svm(formula = Purchased ~ .,<br/>                 data = training_set,<br/>                 type = 'C-classification',<br/>                 kernel = 'radial',<br/>                 gamma=0.4)</span><span id="021d" class="mr ke hi mn b fi mx mt l mu mv"># Predicting the Test set results<br/>y_pred_Linear = predict(classifier_Linear, newdata = test_set[-3])<br/>y_pred_Polynomial = predict(classifier_Polynomial, newdata = test_set[-3])<br/>y_pred_Radial = predict(classifier_Radial, newdata = test_set[-3])</span><span id="83a6" class="mr ke hi mn b fi mx mt l mu mv"># Making the Confusion Matrix<br/>cm_Linear = confusionMatrix(test_set[, 3], y_pred_Linear)<br/>Acc_Lin=cm_Linear$overall[[1]]<br/>cm_Poly = confusionMatrix(test_set[, 3], y_pred_Polynomial)<br/>Acc_Poly=cm_Poly$overall[[1]]<br/>cm_Radial = confusionMatrix(test_set[, 3], y_pred_Radial)<br/>Acc_Rad=cm_Radial$overall[[1]]</span><span id="b741" class="mr ke hi mn b fi mx mt l mu mv">Compare=data.frame(Kernel=c("Linear","Polynomial","Radial"),Accuracy=c(Acc_Lin,Acc_Poly, Acc_Rad))<br/>Compare</span><span id="7e0f" class="mr ke hi mn b fi mx mt l mu mv">##       Kernel   Accuracy <br/>## 1     Linear     0.80 <br/>## 2 Polynomial     0.74 <br/>## 3     Radial     0.90</span></pre><p id="2117" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">下面展示了从一本非常受欢迎的书<strong class="iy hj">统计学习介绍</strong>中借来的一个类似的可视化显示，以加强上面的例子所推动的观点。<strong class="iy hj">我们看到，当我们从肉眼可见的二维空间向上移动到不可见但可以通过想象可视化的更高维度时，分类会逐渐改进。</strong></p><div class="lh li lj lk fd ab cb"><figure class="ii ij my il im in io paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><img src="../Images/70c4aefbc8fc1239ad34c4b39e6bc51f.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*UqmoLPg-spGmQPY6F24ELQ.png"/></div></figure><figure class="ii ij mz il im in io paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><img src="../Images/f82b0e8310897f3893345290dc9a8591.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*snPMy_nE3Ismz7TWfx3P2w.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx na di nb nc translated">使用不同内核实现分类的比较</figcaption></figure></div><p id="6849" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">最左边的图表示需要分类的具有两级目标变量的二维数据。正如我们所见，下图试图在与输入要素相同的维度上进行线性分离。分类很差。接下来的两个图表代表了通过将数据转换到更高维度所实现的分类，这是通过我们将在接下来的章节中讨论的技术实现的。所实现的分离仍然是线性的，但是肉眼不可见。当我们回到现实时，我们看到的似乎是非线性的。这似乎是不可思议的，虚幻的。</p><p id="2caf" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="iy hj"> 2。假象背后的现实——转变的数学提升</strong></p><p id="1590" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">当我们有许多特征并且我们想要更高维度来实现我们的目标时，将数据转换到更高维度可能是极其麻烦和计算昂贵的。然而，这可以通过减少仅点积高维特征所需的计算量来解决。因此，有了这个，我们实际上不需要知道变换后的特征是什么，而只需要知道变换后的特征的点积。这怎么可能。让我们探索吧！！！</p><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es nd"><img src="../Images/9e27cdb6efcc60ff31f75b46ccb5cd78.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*9Oj767-sg1w_GS2UR95ABA.png"/></div></figure><p id="27a5" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">如左图所示，我们似乎有一个分类问题，我们需要将阳性样本与阴性样本分开。在SVM，分类是通过在两者之间画一条直线，并尽可能留有最宽的空白来区分两者。上图中的虚线是分隔线，两边的两条红色实线(与虚线等距)是最宽的板或街道，将两个阶层分开。它有时被称为最宽的街道方法。在本节中，我们将看到所画的分隔线或超平面取决于独立变量特征的点积，而不取决于特征的坐标。如果判定边界在2维空间中不是线性的，这个特定的属性帮助SVM将其特征变换到更高维度。</p><p id="8eb9" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="iy hj"> 3。陶醉效果的对象——内核诡计</strong></p><p id="7442" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">正如本博客中所讨论的，不可线性分离的数据可以通过转换到更高的维度进行线性分类。这种转换可以通过一种叫做内核技巧的技术来实现。基于上一节的结论，kernel trick所做的是，它帮助分析师从一个无限高维空间的宇宙中获得转换特征的点积。</p><p id="e8e1" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">类似地，想象一下，你必须从一个大图书馆找到一本书，在那里你需要找出具体的房间、具体的过道、具体的书架，然后在所有堆在那里的书中找到这本书，与此相反，你可以选择让图书管理员知道你需要哪本书，他去找，然后把书还给你。哪个更方便？答案显而易见。这里，我们通过应用由librarian表示的函数实现了一个转换。kernel trick所做的就是不需要去图书馆就能拿到书。</p><p id="3a3a" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">数值分析人员需要定义所需的转换，通过调用能够实现它的内核来进行所需的调整，剩下的工作由内核通过深入其无限深度并返回所需的最终结果来完成。如果没有内核技巧，分析师将不得不首先转换数据，然后完成所有中间计算以获得最终输出。</p><p id="7158" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">内核技巧本质上是使用内核函数，这使得能够在高维、<em class="mw">隐式</em>特征空间中操作，而无需计算该空间中数据的坐标。该操作通常比坐标的显式计算在计算上更便宜。一些更广泛使用的核函数是线性、多项式、径向和sigmoid。在下一节中，我们将看到两个使用线性、多项式和径向核的案例研究。</p><p id="26b0" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="iy hj"> 4。案例分析</strong></p><p id="b520" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="iy hj">案例研究I——建立一个SVM模型来预测顾客行为，即顾客购买哪种口味的橙汁。</strong></p><p id="6cdb" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这个案例研究是通过从一本非常受欢迎的书<a class="ae lq" href="http://www-bcf.usc.edu/~gareth/ISL/" rel="noopener ugc nofollow" target="_blank">《统计学习导论</a> (ISLR)中选取一组数据完成的。该数据包含1070次购买，其中客户购买了Citrus Hill或Minute Maid橙汁。顾客和产品的许多特征被记录下来。</p><p id="91f5" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">构建了具有所有三个核的SVM模型，以比较哪个核给出了最好的结果。对于每个内核，进行参数调整以获得模型的最佳参数，从而给出最佳输出。可以看出，具有多项式核的模型是最不引人注目的。从其他两个模型获得的结果几乎相同。下面给出了在R中工作的细节。</p><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="f34e" class="mr ke hi mn b fi ms mt l mu mv">#Calling the library and importing the data<br/>library(ISLR)<br/>mydata=OJ</span><span id="11fe" class="mr ke hi mn b fi mx mt l mu mv">#Minimal exploratory analysis<br/>dim(mydata)<br/>str(OJ)<br/>table(mydata$Purchase)<br/>anyNA(mydata)</span><span id="3e83" class="mr ke hi mn b fi mx mt l mu mv">#No missing values</span><span id="2318" class="mr ke hi mn b fi mx mt l mu mv">#Calling libraries for data preparation and model building<br/>library(caret)<br/>library(ROCR)</span><span id="8275" class="mr ke hi mn b fi mx mt l mu mv">prop.table(table(mydata$Purchase))</span><span id="bb61" class="mr ke hi mn b fi mx mt l mu mv">##<br/>## CH MM <br/>## 653 417</span><span id="db36" class="mr ke hi mn b fi mx mt l mu mv">set.seed(1234)<br/>Index=createDataPartition(mydata$Purchase, p=0.75, list = FALSE)<br/>Train=mydata[Index,]<br/>Test=mydata[-Index,]</span><span id="22cc" class="mr ke hi mn b fi mx mt l mu mv">#10 fold bootstrapped cross validation sampling<br/>control=trainControl(method = "repeatedcv", number =10,repeats=1)</span><span id="9afd" class="mr ke hi mn b fi mx mt l mu mv">#Build a SVM model using Linear kernel<br/>###Tuning parameter C for optimized model<br/>grid=expand.grid(C = seq(0.5,10,.5))<br/>set.seed(1234)<br/>SVM_L=train(Purchase~., data = Train,method = 'svmLinear',<br/>            trControl = control,<br/>            tuneGrid = grid,<br/>            preProcess = c("scale","center"))</span><span id="ee2d" class="mr ke hi mn b fi mx mt l mu mv">plot(SVM_L)<br/>#Accuracy seems to vary across various values of C</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es ne"><img src="../Images/e33658fdbb65e97c6fe746ca9169c1ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*cT0Ebh4FbYyPUfghxvknTQ.jpeg"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated"><em class="nf">不同的C值下，精度似乎不同。最佳值似乎在C=6时</em></figcaption></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="cb0d" class="mr ke hi mn b fi ms mt l mu mv">#The best value of C is found by the below<br/>SVM_L$bestTune</span><span id="5cfb" class="mr ke hi mn b fi mx mt l mu mv">#Training Accuracy<br/>a=SVM_L$results<br/>TrainingAcc_L=a[which.max(a$Accuracy),"Accuracy"]<br/>TrainingAcc_L</span><span id="1f85" class="mr ke hi mn b fi mx mt l mu mv">##Predictions<br/>Pred_L=predict(SVM_L, Test)<br/>CM=confusionMatrix(Pred_L,Test$Purchase)<br/>Acc_L=CM$overall[[1]]<br/>Acc_L</span><span id="a31f" class="mr ke hi mn b fi mx mt l mu mv">#Visulize the confusion matrix<br/>CM$table<br/>fourfoldplot(CM$table)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es ng"><img src="../Images/0254a4da693055c6c78830f46190dd4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*0WhZMyS7QX92y_JurQKUCQ.jpeg"/></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="adec" class="mr ke hi mn b fi ms mt l mu mv">Sensitivity_Linear=CM$byClass[[1]]</span><span id="c8a6" class="mr ke hi mn b fi mx mt l mu mv">Specificity_Linear=CM$byClass[[2]]</span><span id="8c9f" class="mr ke hi mn b fi mx mt l mu mv">###ROC and AUC</span><span id="a23c" class="mr ke hi mn b fi mx mt l mu mv">predictions.L=prediction(as.numeric(Pred_L),Test$Purchase)<br/>Perf.L=performance(predictions.L,"tpr","fpr")<br/>plot(Perf.L, main="ROC - SVM with Linear Kernel")</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><div class="er es nh"><img src="../Images/a7633cf75cf593c362c059b09e2dd176.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EjPYM5i4zQDVGLQZ-ohrzA.jpeg"/></div></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="f1ff" class="mr ke hi mn b fi ms mt l mu mv">AUC=performance(predictions.L,"auc")<br/>AUC_L=AUC@y.values[[1]]<br/># AUC_L = 0.8088426</span><span id="2856" class="mr ke hi mn b fi mx mt l mu mv">#Build the model by using Polynomial Kernel<br/>#parameter tuning for both Cost and degree of polynomial<br/>set.seed(3456555)<br/>grid=expand.grid(C = c(0.1,1,3,5,7,9,10), <br/>                 degree=c(2,3),scale=1)<br/>svm_P=train(Purchase~.,data=Train,method="svmPoly",<br/>            trControl=control, tuneGrid=grid)</span><span id="4312" class="mr ke hi mn b fi mx mt l mu mv">#Best parameter values after tuning are<br/>svm_P$bestTune<br/>#Visualize<br/>plot(svm_P)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><div class="er es ni"><img src="../Images/998d2481a22202cae91238d99c87e648.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e3gHchIpAV7ZEPt3dm0yNA.jpeg"/></div></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="790f" class="mr ke hi mn b fi ms mt l mu mv">#Training Accuracy<br/>b=svm_P$results<br/>TrainingAcc_Poly=b[which.max(b$Accuracy),"Accuracy"]<br/>TrainingAcc_Poly</span><span id="b952" class="mr ke hi mn b fi mx mt l mu mv">#Predictions<br/>Pred_Poly=predict(svm_P,Test)<br/>b=confusionMatrix(Pred_Poly,Test$Purchase)<br/>Accuracy_Polynomial=b$overall[[1]]<br/>Accuracy_Polynomial</span><span id="0ad7" class="mr ke hi mn b fi mx mt l mu mv">#Visulize the confusion matrix<br/>b$table<br/>fourfoldplot(b$table)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es nj"><img src="../Images/e7b8bc6889fefdceb6bbe368832cc9de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*I42IQEZ1uqY9A8P4RDZQXA.jpeg"/></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="b4ba" class="mr ke hi mn b fi ms mt l mu mv">Sensitivity_Polynomial=b$byClass[[1]]<br/>Sensitivity_Polynomial<br/>Specificity_Polynomial=b$byClass[[2]]<br/>Specificity_Polynomial</span><span id="3d23" class="mr ke hi mn b fi mx mt l mu mv">##ROC&amp; AUC<br/>predictions.P=prediction(as.numeric(Pred_Poly),labels=Test$Purchase)<br/>Perf.P=performance(predictions.P,"tpr","fpr")<br/>plot(Perf.P, main="ROC - SVM with Polynomial Kernel")</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><div class="er es nk"><img src="../Images/04ad52c4858048821ee9c459a7e5a2fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tYwFxKKy4bLMiSIqBbEsbQ.jpeg"/></div></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="a50c" class="mr ke hi mn b fi ms mt l mu mv">AUC=performance(predictions.P,"auc")<br/>AUC_P=AUC@y.values[[1]]<br/>AUC_P</span><span id="7683" class="mr ke hi mn b fi mx mt l mu mv">#Build the model by using Radial Kernel<br/>#parameter tuning for both Cost and sigma of radial kernel</span><span id="5ea0" class="mr ke hi mn b fi mx mt l mu mv">grid=expand.grid(C = c(0.1,0.5,1,2,3,5,7.5,8,8.5,9,9.5,10), <br/>                 sigma=c(0.0025,0.005,0.01,0.015,0.02,0.025))<br/>set.seed(88888)<br/>svm_Radial=train(Purchase~.,data=Train,method="svmRadial",<br/>                 trControl=control, tuneGrid=grid)</span><span id="e52e" class="mr ke hi mn b fi mx mt l mu mv">#Best tuned parameters for the model &amp; Visualization<br/>svm_Radial$bestTune<br/>plot(svm_Radial)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><div class="er es lp"><img src="../Images/b5d4b96074c83a56dc7807d16e3e583a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e6CE5771c_Xri8MsCvHVCQ.jpeg"/></div></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="c841" class="mr ke hi mn b fi ms mt l mu mv">#Training Accuracy<br/>c=svm_Radial$results<br/>TrainingAcc_Rad=c[which.max(c$Accuracy),"Accuracy"]<br/>TrainingAcc_Rad</span><span id="f818" class="mr ke hi mn b fi mx mt l mu mv">#Predictions<br/>Pred_Radial=predict(svm_Radial,Test)<br/>c=confusionMatrix(Pred_Radial,Test$Purchase)<br/>Accuracy_Radial=c$overall[[1]]<br/>Accuracy_Radial</span><span id="3ab1" class="mr ke hi mn b fi mx mt l mu mv">#Visulize the confusion matrix<br/>c$table<br/>fourfoldplot(c$table)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es nl"><img src="../Images/80a69e42e33872bde30a8099f793e8bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*VoL4zY3mvRJ64WBqTBPHYw.jpeg"/></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="5f54" class="mr ke hi mn b fi ms mt l mu mv">Sensitivity_Radial=c$byClass[[1]]<br/>Sensitivity_Radial<br/>Specificity_Radial=c$byClass[[2]]<br/>Specificity_Radial</span><span id="b5d1" class="mr ke hi mn b fi mx mt l mu mv">#ROC &amp; AUC<br/>predictions.R=prediction(as.numeric(Pred_Radial),labels=Test$Purchase)<br/>Perf.R=performance(predictions.R,"tpr","fpr")<br/>plot(Perf.R, main="ROC - SVM with Radial Kernel")</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es nm"><img src="../Images/f867769fa71463426250f2a1fcc3d033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*vjPQpr22ZNaYtUTj7Xj0nw.jpeg"/></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="ce12" class="mr ke hi mn b fi ms mt l mu mv">AUC=performance(predictions.R,"auc")<br/>AUC_R=AUC@y.values[[1]]<br/>AUC_R</span><span id="ec5a" class="mr ke hi mn b fi mx mt l mu mv">##Comparison across kernels<br/>Compare=data.frame(Kernel=c("Linear","Poly","Radial"),<br/>                   Train_Acc=c(TrainingAcc_L,TrainingAcc_Poly,TrainingAcc_Rad),<br/>                   Test_Acc=c(Acc_L,Accuracy_Polynomial,Accuracy_Radial),<br/>                   Sensitivity=c(Sensitivity_Linear,Sensitivity_Polynomial,Sensitivity_Radial),<br/>                   Specificity=c(Specificity_Linear,Specificity_Polynomial,Specificity_Radial),<br/>                   AUC_All=c(AUC_L,AUC_P,AUC_R))<br/>Compare</span><span id="fab0" class="mr ke hi mn b fi mx mt l mu mv"><strong class="mn hj"><em class="mw">##   Kernel Train_Acc  Test_Acc Sensitivity Specificity   AUC_All ## 1 Linear 0.8331327 0.8239700   0.8773006   0.7403846 0.8088426 ## 2   Poly 0.8170062 0.7865169   0.8711656   0.6538462 0.7625059 ## 3 Radial 0.8331327 0.8239700   0.9018405   0.7019231 0.8018818</em></strong></span></pre><p id="2ffc" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="iy hj">案例研究II——建立SVM模型来预测患者是否患有良性或恶性肿瘤。</strong></p><p id="5da0" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">这些数据来自UCI机器学习库或我的github账户。这里的<a class="ae lq" href="https://github.com/Pranov1984/Predictive-Model-Building-with-Support-Vector-Machines" rel="noopener ugc nofollow" target="_blank">是</a>到数据的链接。工作的细节如下。构建了具有所有三个核的SVM模型，以比较哪个核给出了最好的结果。对于每个内核，进行参数调整以获得模型的最佳参数，从而给出最佳输出。</p><p id="3390" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">可以看出，用线性核和多项式核建立的模型对未知数据的精度几乎相同。径向核模型的精度更高。由于该模型最初是基于训练建立的，并符合所有3个场景的训练数据，因此准确性有所降低，这是意料之中的。</p><p id="05c9" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">与用线性核建立模型相比，用多项式核建立的模型的AUC更大。用径向核构建的AUC最高。径向核模型在所有指标上都做得更好。</p><p id="2ecc" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">从灵敏度的角度来看，线性核模型略好于多项式核模型，但从特异性的角度来看，情况正好相反。</p><p id="a94c" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">径向核最适合此数据集。如果是线性和多项式之间的选择，很难区分两者。工作细节如下</p><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="e2d3" class="mr ke hi mn b fi ms mt l mu mv">#Set working directory and import data<br/>setwd("C:\\Users\\user\\Desktop\\Blogs\\SVM")<br/>mydata=read.csv("Breast_Cancer_Dataset.csv")</span><span id="70bb" class="mr ke hi mn b fi mx mt l mu mv">#Check data structure and dimension<br/>str(mydata)<br/>dim(mydata)</span><span id="deff" class="mr ke hi mn b fi mx mt l mu mv">#Change the levels of the target variable to "0" and "1" which stand for benign and malignant respectively<br/>mydata$Class=ifelse(mydata$Class==2,0,1)<br/>mydata$Class=as.factor(mydata$Class)<br/>table(mydata$Class)</span><span id="29e9" class="mr ke hi mn b fi mx mt l mu mv">#Unusual level identified in Bare_Nuclei. Identify the rows and remove them<br/>table(mydata$Bare_Nuclei)<br/>which(mydata$Bare_Nuclei=="?")</span><span id="8734" class="mr ke hi mn b fi mx mt l mu mv">data=mydata[-which(mydata$Bare_Nuclei=="?"),]<br/>data=droplevels(data)</span><span id="4fe1" class="mr ke hi mn b fi mx mt l mu mv">#Partition the data in 70:30 ratio</span><span id="1319" class="mr ke hi mn b fi mx mt l mu mv">library(caret)<br/>set.seed(1234)<br/>Index=createDataPartition(data$Class, p=0.7,list = FALSE)<br/>Train=mydata[Index,]<br/>Test=mydata[-Index,]</span><span id="4833" class="mr ke hi mn b fi mx mt l mu mv">#Prepare for model by having 10 fold bootstrapped crossvalidation sampling<br/>control=trainControl(method = "repeatedcv", number = 10, repeats = 1)</span><span id="3a9c" class="mr ke hi mn b fi mx mt l mu mv">#Build a SVM model using Linear kernel<br/>###Tuning parameter C for optimized model<br/>grid=expand.grid(C = c(0.01, 0.02,0.05, 0.075, 0.1, 0.25, 0.5, 1, 1.25, 1.5, 1.75, 2,5))</span><span id="feff" class="mr ke hi mn b fi mx mt l mu mv">set.seed(123456)<br/>svm_Linear_Grid=train(Class~., data = Train,method = "svmLinear",<br/>                      trControl = control, <br/>                      preProcess=c("scale","center"),<br/>                      tuneGrid=grid)</span><span id="1cff" class="mr ke hi mn b fi mx mt l mu mv">a=svm_Linear_Grid$results<br/>TrainingAcc_Linear=a[which.max(a$Accuracy),"Accuracy"]</span><span id="381a" class="mr ke hi mn b fi mx mt l mu mv">#Best parameter value after tuning<br/>svm_Linear_Grid$bestTune</span><span id="784d" class="mr ke hi mn b fi mx mt l mu mv">#Predictions<br/>Pred=predict(svm_Linear_Grid,Test)<br/>a=confusionMatrix(Pred,Test$Class)<br/>accuracy_Linear=a$overall[[1]]<br/>accuracy_Linear<br/>#An accuracy of 94.1% is achieved</span><span id="7a9d" class="mr ke hi mn b fi mx mt l mu mv">#Visulize the confusion matrix<br/>a$table<br/>fourfoldplot(a$table)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es nn"><img src="../Images/262e5ae52db4a23ba76aac5be1f83483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*Rl8d84rvkd5kemmc0yHpBA.jpeg"/></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="dba5" class="mr ke hi mn b fi ms mt l mu mv">Sensitivity_Linear=a$byClass[[1]]<br/>#True positive rate achieved is 96%<br/>Specificity_Linear=a$byClass[[2]]<br/>#True negative rate achieved is 89%</span><span id="b604" class="mr ke hi mn b fi mx mt l mu mv">###ROC and AUC<br/>library(ROCR)<br/>predictions.L=prediction(as.numeric(Pred),Test$Class)<br/>Perf.L=performance(predictions.L,"tpr","fpr")<br/>plot(Perf.L, main="ROC - SVM with Linear Kernel")</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><div class="er es no"><img src="../Images/db19980d8f60f1b85ef73343db788239.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7IdGtR9kWM4dAyAKI_q4oQ.jpeg"/></div></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="14da" class="mr ke hi mn b fi ms mt l mu mv">AUC=performance(predictions.L,"auc")<br/>AUC_L=AUC@y.values<br/>AUC_L</span><span id="e28c" class="mr ke hi mn b fi mx mt l mu mv">#Build the model by using Polynomial Kernel<br/>#parameter tuning for both Cost and degree of polynomial<br/>grid=expand.grid(C = c(0.005,.01, .1, 1,10), <br/>                 degree=c(2,3,4), scale=1)<br/>svm_P=train(Class~.,data=Train,method="svmPoly",tuneLength=10,<br/>            trControl=control, tuneGrid=grid)</span><span id="5e5f" class="mr ke hi mn b fi mx mt l mu mv">#Best parameter value after tuning is a cost of 0.01 and degree of 2<br/>svm_P$bestTune<br/>#Visualize<br/>plot(svm_P)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><div class="er es np"><img src="../Images/9fea1eb8da0d397407b027a9fe5f0b60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mV136WUBnn9sZR7bPoYvKQ.jpeg"/></div></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="8745" class="mr ke hi mn b fi ms mt l mu mv">#Training Accuracy<br/>b=svm_P$results<br/>TrainingAcc_Poly=b[which.max(b$Accuracy),"Accuracy"]<br/>TrainingAcc_Poly</span><span id="319d" class="mr ke hi mn b fi mx mt l mu mv">#Predictions<br/>Pred_Poly=predict(svm_P,Test)<br/>b=confusionMatrix(Pred_Poly,Test$Class)<br/>Accuracy_Polynomial=b$overall[[1]]<br/>Accuracy_Polynomial<br/>#An accuracy of 94.1% is achieved</span><span id="7fd7" class="mr ke hi mn b fi mx mt l mu mv">#Visulize the confusion matrix<br/>b$table<br/>fourfoldplot(b$table)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><div class="er es nq"><img src="../Images/822629facf47a78f0e9c980a1e1740b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*N11xqEDZWVgnGGbfMEOADw.jpeg"/></div></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="6a94" class="mr ke hi mn b fi ms mt l mu mv">Sensitivity_Polynomial=b$byClass[[1]]<br/>#True positive rate achieved is 96%<br/>Specificity_Polynomial=b$byClass[[2]]<br/>#True negative rate achieved is 91%</span><span id="0e89" class="mr ke hi mn b fi mx mt l mu mv">##ROC&amp; AUC<br/>predictions.P=prediction(as.numeric(Pred_Poly),labels=Test$Class)<br/>Perf.P=performance(predictions.P,"tpr","fpr")<br/>plot(Perf.P, main="ROC - SVM with Polynomial Kernel")</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><div class="er es nr"><img src="../Images/2d3532cc62488190f8fdd457083b2a9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*InMtBkvZVfybtNqdzf5HwQ.jpeg"/></div></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="0e08" class="mr ke hi mn b fi ms mt l mu mv">AUC=performance(predictions.P,"auc")<br/>AUC_P=AUC@y.values<br/>AUC_P</span><span id="8395" class="mr ke hi mn b fi mx mt l mu mv">#Build the model by using Radial Kernel<br/>#parameter tuning for both Cost and sigma of radial kernel</span><span id="fde3" class="mr ke hi mn b fi mx mt l mu mv">grid=expand.grid(C = c(0.005,.01, 0.1, 0.15,0.20,0.25), <br/>                 sigma=c(0.0025,0.005,0.01,0.015,0.02,0.025))<br/>set.seed(88888)<br/>svm_Radial=train(Class~.,data=Train,method="svmRadial",tuneLength=10,<br/>            trControl=control, tuneGrid=grid)</span><span id="e67b" class="mr ke hi mn b fi mx mt l mu mv">#Best tuned parameters for the model &amp; Visualization of the comparison while tuning<br/>svm_Radial$bestTune<br/>plot(svm_Radial)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ip iq di ir bf is"><div class="er es et"><img src="../Images/169002d5ea24087d6e302220463a3653.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xlEyJVcGdobyL7EgO3E7IA.jpeg"/></div></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="91b0" class="mr ke hi mn b fi ms mt l mu mv">#Training Accuracy<br/>c=svm_Radial$results<br/>TrainingAcc_Rad=c[which.max(c$Accuracy),"Accuracy"]<br/>TrainingAcc_Rad</span><span id="9c0f" class="mr ke hi mn b fi mx mt l mu mv">#Predictions<br/>Pred_Radial=predict(svm_Radial,Test)<br/>c=confusionMatrix(Pred_Radial,Test$Class)<br/>Accuracy_Radial=c$overall[[1]]<br/>Accuracy_Radial<br/>#An accurtacy of 96% is achieved</span><span id="0ffc" class="mr ke hi mn b fi mx mt l mu mv">#Visulize the confusion matrix<br/>c$table<br/>fourfoldplot(c$table)</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es ns"><img src="../Images/96760e8a3c351a1b0cd0fe3bba9fe38d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*nTcti3M5tAtu1hzGavkRcw.jpeg"/></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="d16c" class="mr ke hi mn b fi ms mt l mu mv">Sensitivity_Radial=c$byClass[[1]]<br/>#True positive rate achieved is 96.5%<br/>Specificity_Radial=c$byClass[[2]]<br/>#True negative rate achieved is 95%</span><span id="cc80" class="mr ke hi mn b fi mx mt l mu mv">#ROC &amp; AUC<br/>predictions.R=prediction(as.numeric(Pred_Radial),labels=Test$Class)<br/>Perf.R=performance(predictions.R,"tpr","fpr")<br/>plot(Perf.R, main="ROC - SVM with Radial Kernel")</span></pre><figure class="lh li lj lk fd ij er es paragraph-image"><div class="er es nt"><img src="../Images/e52ba8357dedbfb6e54c7a241ef67afe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*0-xn2O0X19i8ZpZTnN8SOQ.jpeg"/></div></figure><pre class="lh li lj lk fd mm mn mo mp aw mq bi"><span id="38c3" class="mr ke hi mn b fi ms mt l mu mv">AUC=performance(predictions.R,"auc")<br/>AUC_R=AUC@y.values<br/>AUC_R</span><span id="63b7" class="mr ke hi mn b fi mx mt l mu mv">##Comparison across kernels<br/>Compare=data.frame(Kernel=c("Linear","Poly","Radial"),<br/>                   Train_Acc=c(TrainingAcc_Linear,TrainingAcc_Poly,TrainingAcc_Rad),<br/>                   Test_Acc=c(accuracy_Linear,Accuracy_Polynomial,Accuracy_Radial),<br/>                   Sensitivity=c(Sensitivity_Linear,Sensitivity_Polynomial,Sensitivity_Radial),<br/>                   Specificity=c(Specificity_Linear,Specificity_Polynomial,Specificity_Radial),<br/>                   AUC_All=c(AUC_L[[1]],AUC_P[[1]],AUC_R[[1]]))</span><span id="9146" class="mr ke hi mn b fi mx mt l mu mv">Compare</span><span id="4247" class="mr ke hi mn b fi mx mt l mu mv"><strong class="mn hj"><em class="mw">##   Kernel Train_Acc  Test_Acc Sensitivity Specificity   AUC_All ## 1 Linear 0.9792481 0.9409091   0.9657534   0.8918919 0.9288227 ## 2   Poly 0.9667499 0.9409091   0.9589041   0.9054054 0.9321548 ## 3 Radial 0.9749964 0.9590909   0.9657534   0.9459459 0.9558497</em></strong></span></pre><p id="41f4" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated"><strong class="iy hj">结束注释</strong></p><p id="e66c" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">支持向量机是每个数据科学家都希望拥有的最复杂的统计技术之一。当我们在文章中看到与幻觉的比较时，它有一种神秘感和魔力。SVM的历史可以追溯到20世纪60年代，当时最初的T2·SVM算法是由弗拉基米尔·n·瓦普尼克和阿列克谢·亚发明的。1963年。1992年，Bernhard E. Boser，Isabelle M. Guyon和Vladimir N. Vapnik提出了一种通过对最大边缘超平面应用核技巧来创建非线性分类器的方法。1992年的故事虽然始于1989年的巴黎，当时伊莎贝尔用核方法对神经网络进行了基准测试，但是SVM的真正发明发生在Bernhard决定实现Vladimir Vapnik算法的时候。这正好表明，有时伟大的想法需要时间才能最终实现。</p><p id="19cc" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">一般来说，当你有大量的特性时，SVM是非常好的。例如，用于单词包模型中的文本分类。在大多数情况下，具有非线性核的支持向量机表现相当好，通常与随机森林正面对抗，有时RFs工作稍好，有时支持向量机胜出。它在尺寸数量大于样本数量的情况下有效。</p><p id="35c9" class="pw-post-body-paragraph iw ix hi iy b iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt hb bi translated">然而，没有免费午餐，SVM也有它的困难。有时，它们的计算成本很高。SVM在有噪声的数据集上表现不佳。话虽如此，人们应该小心何时选择和何时不选择SVM作为分类器来解决手头的问题。</p></div></div>    
</body>
</html>