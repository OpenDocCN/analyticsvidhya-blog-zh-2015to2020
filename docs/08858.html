<html>
<head>
<title>Linear Regression With Python From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python从头开始线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-with-python-from-scratch-aa2ba12c117f?source=collection_archive---------15-----------------------#2020-08-16">https://medium.com/analytics-vidhya/linear-regression-with-python-from-scratch-aa2ba12c117f?source=collection_archive---------15-----------------------#2020-08-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b1d65de89e5eadabcc478e430653908f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nbx_0r1APBAhtBpC3gIOuQ.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">用Python进行机器学习</figcaption></figure><p id="0166" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">有三种类型的机器学习类别:</p><ol class=""><li id="b58a" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">监督学习</li><li id="5a32" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">无监督学习</li><li id="9ed5" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">强化学习</li></ol><p id="857e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">线性回归算法属于监督学习的范畴，在这一范畴中，我们得到了数据中每个例子的“正确答案”。线性回归是最简单的学习算法，它通过从给定的先前数据中学习来帮助预测正确的答案。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es kg"><img src="../Images/0e66b55d4eeff751163b35c589413d38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/1*Y8XJ_-5tykSaq2CjKMp6Qg.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">线性回归框图(图1)</figcaption></figure><p id="cf78" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里基本上“h”是我们的假设函数，该函数给出如下:</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es kl"><img src="../Images/9dcdbd8cb9815cd578867b950960fd0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*TY5u1LJUSRvcMJ78TbEHLQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图2</figcaption></figure><p id="6010" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这个函数基本上预测Y是X轴在X上的某个直线函数。这个模型被称为一元线性回归或一元线性回归，因为这里只有X个变量。我们必须找到θ_ 0和θ_ 1的最佳可能值，这样我们就可以在数据上拟合最佳可能的线性线，如下图所示。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es km"><img src="../Images/cc31cb2689d09291a9019ddcd222a31a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s98mIam0lSR3SEEug4w6Pg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图3</figcaption></figure><h2 id="7ebd" class="kn ko hi bd kp kq kr ks kt ku kv kw kx jf ky kz la jj lb lc ld jn le lf lg lh bi translated">模型表示</h2><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es li"><img src="../Images/cfc91d53898f3613d7b88baa86db8f54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*pm6IytygXNpegS9w9wXNUQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">图4</figcaption></figure><p id="4160" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">符号:</strong></p><p id="4113" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">m =训练示例的总数[例如:2104，1416，1534，852，…]</p><p id="8926" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">n =特性总数[例如:以英尺为单位的尺寸(X)，千片订量的价格(Y)]</p><p id="a44c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">参数:</strong></p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es kl"><img src="../Images/9dcdbd8cb9815cd578867b950960fd0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*TY5u1LJUSRvcMJ78TbEHLQ.png"/></div></figure><p id="416f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这里θ_ 0和θ_ 1是参数</p><blockquote class="lj lk ll"><p id="f98d" class="iu iv lm iw b ix iy iz ja jb jc jd je ln jg jh ji lo jk jl jm lp jo jp jq jr hb bi translated">我们的座右铭是选择θ_ 0和θ_ 1，因此对于我们的训练示例(x，y)，该假设接近y</p></blockquote><p id="e2aa" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">为了获得假设函数的最佳值，我们将使用一个成本函数来最小化θ的值，这将给出最符合给定数据的θ的最佳值。</p><h2 id="e789" class="kn ko hi bd kp kq kr ks kt ku kv kw kx jf ky kz la jj lb lc ld jn le lf lg lh bi translated">成本函数/平方误差函数</h2><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es lq"><img src="../Images/51fd7f544fa24bfff6d4a58d78fbeb8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:894/format:webp/1*jkrsS117CcVI_x25RdHqmQ.png"/></div></figure><p id="4add" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">线性回归的代价函数(J)是预测y值(pred)和真实y值(y)之间的<strong class="iw hj">均方根误差(RMSE) </strong>。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/667e5a4fb36dca205a156881b6c6dbad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t3Ypdawt64GnZ1bVaQ0jkw.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图5</figcaption></figure><p id="5d82" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在上面图像的左侧，在<strong class="iw hj">最小化θ[Theta]的值</strong>之后，该线对数据拟合得不好。在图像的右侧，我们得到该线对数据拟合得很好的图形。</p><h2 id="d373" class="kn ko hi bd kp kq kr ks kt ku kv kw kx jf ky kz la jj lb lc ld jn le lf lg lh bi translated">梯度下降</h2><p id="bb4a" class="pw-post-body-paragraph iu iv hi iw b ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn lw jp jq jr hb bi translated">梯度下降是一种优化算法，用于找到使成本函数(cost)最小化的函数(f)的参数值。</p><p id="24a0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">用于最小化J(θ_ 0，θ_ 1)</p><p id="7e07" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">概要:</strong></p><ol class=""><li id="8164" class="js jt hi iw b ix iy jb jc jf ju jj jv jn jw jr jx jy jz ka bi translated">从θ_ 0和θ_ 1的某个值开始[ <strong class="iw hj">比如θ_ 0 = 0&amp;θ_ 1 = 0</strong></li><li id="008b" class="js jt hi iw b ix kb jb kc jf kd jj ke jn kf jr jx jy jz ka bi translated">不断改变θ_ 0 &amp;θ_ 1以减小J(θ_ 0，θ_ 1)，直到我们有希望达到最小值。</li></ol><figure class="kh ki kj kk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/f1a4e24a9596b609530083776c67a24b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oMd7EAEM9sVfJ6uEkOxXXA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图6</figcaption></figure><p id="4ef4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在上图中，星号是θ随机值。有时，我们从零值开始，这只是一个例子。在将aθ的值随机初始化或初始化为零后，它将环顾平面，算法将向收敛方向迈一小步。在迈出一小步后，它将从新的位置再次迈出一小步，这样，它将收敛到局部最优。让我们直观地看看这个。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/d5564f3ffb03a5368e17854670734ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gzilhf1axWCEgUAIHuppuQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图7</figcaption></figure><p id="52bf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这样，它现在收敛到局部最优，看算法。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/48e18ce5e1ead3f007b289e04322c746.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*uQ97Z8bIZ8yKjB4n3bXolw.png"/></div></figure><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es lz"><img src="../Images/1265650cc14c91d01d7b0f69bad8eb3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*XvgUe6mOcSj0kp43ESak3A.png"/></div></figure><p id="7607" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">上述方程的推导</strong>:</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es ma"><img src="../Images/4b52f563b48cb4b13105e0e52c246d01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*JO7dR4x2FBtMClBnvLrXWw.png"/></div></figure><blockquote class="lj lk ll"><p id="dcdd" class="iu iv lm iw b ix iy iz ja jb jc jd je ln jg jh ji lo jk jl jm lp jo jp jq jr hb bi translated"><em class="hi">这里α[alpha]是我们的学习率。</em></p></blockquote><p id="27ba" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果α[alpha]太小，渐变会很慢，因为它需要一步一步地更新。如果α[alpha]太大，梯度下降可能超过局部最优，并且可能无法收敛甚至发散。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/98a0975b88fcf01d6a7840ec4a8be046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*An4tZEyQAYgPAZl396JzWg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图8</figcaption></figure><p id="f9c8" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">所以尽量选择α[alpha]的值，比如0.001，0.003，0.01，0.1，…</p></div><div class="ab cl mc md gp me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="hb hc hd he hf"><h2 id="92f4" class="kn ko hi bd kp kq kr ks kt ku kv kw kx jf ky kz la jj lb lc ld jn le lf lg lh bi translated"><strong class="ak">用python实现</strong></h2><p id="79f5" class="pw-post-body-paragraph iu iv hi iw b ix ls iz ja jb lt jd je jf lu jh ji jj lv jl jm jn lw jp jq jr hb bi translated">我们将看到并尝试预测出租车的出租车费数据集。</p><p id="8146" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">导入包</strong></p><pre class="kh ki kj kk fd mj mk ml mm aw mn bi"><span id="7254" class="kn ko hi mk b fi mo mp l mq mr">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt</span></pre><p id="44d3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">导入数据集</strong></p><pre class="kh ki kj kk fd mj mk ml mm aw mn bi"><span id="ac93" class="kn ko hi mk b fi mo mp l mq mr">data = pd.read_csv("cab_fare.csv")<br/>data.head()</span><span id="6593" class="kn ko hi mk b fi ms mp l mq mr">ID  Distance  fare_amount  passenger_count          <br/>       "0        0  5.393882         15.0              1.0  <br/>       "1        1  0.014134         52.0              6.0  <br/>       "2        2  5.523719         15.5              1.0  <br/>       "3        3  1.392056          6.5              1.0  <br/>       "4        4  4.150647         11.0              1.0  <br/>       "...    ...       ...          ...              ...                      <br/>       "16044  122  1.962983          8.1              1.<br/>       "16045  123  3.400518         12.5              2.0                      <br/>       "16046  124  1.617977          8.5              1.0                     <br/>       "16047  125  2.466481          8.0              1.0                      <br/>       "16048  127  1.498280          8.5              1.0</span></pre><p id="fe28" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">将数据拆分成X和Y </strong>和<strong class="iw hj">并转换成矩阵</strong></p><pre class="kh ki kj kk fd mj mk ml mm aw mn bi"><span id="103c" class="kn ko hi mk b fi mo mp l mq mr">X_1 = data.drop(['fare_amount','pickup_datetime','ID'],axis=1)<br/>y_1 = data['fare_amount']<br/>Y_1 = np.expand_dims(Y_1,axis =1 )</span><span id="845f" class="kn ko hi mk b fi ms mp l mq mr">X = np.matrix(X_1)<br/>X.shape = (16049, 2)</span><span id="3c2c" class="kn ko hi mk b fi ms mp l mq mr">Y = np.matrix(Y_1)<br/>Y.shape = (16049, 1)</span><span id="55f4" class="kn ko hi mk b fi ms mp l mq mr">m,n = X.shape           #m =16049 n =2</span><span id="dc40" class="kn ko hi mk b fi ms mp l mq mr">X0 = np.ones((m,1))<br/>X = np.hstack((X0,X))   #Concatenating X0 is our bias and our X</span></pre><p id="050f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">初始化θ[theta]的值</strong></p><pre class="kh ki kj kk fd mj mk ml mm aw mn bi"><span id="6d24" class="kn ko hi mk b fi mo mp l mq mr">theta = np.zeros((n+1,1))  # initially assigning the value to zeros<br/>print(theta)</span><span id="c0d1" class="kn ko hi mk b fi ms mp l mq mr">[[0.]<br/> [0.]<br/> [0.]]</span></pre><p id="052f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">创建我们将在程序中使用的函数</strong></p><pre class="kh ki kj kk fd mj mk ml mm aw mn bi"><span id="fa46" class="kn ko hi mk b fi mo mp l mq mr">def prediction(X,theta):</span><span id="1220" class="kn ko hi mk b fi ms mp l mq mr">    prediction = X*theta<br/>    return prediction</span><span id="1a60" class="kn ko hi mk b fi ms mp l mq mr">def cost(y,pred):</span><span id="d3ad" class="kn ko hi mk b fi ms mp l mq mr">    m,n = y.shape<br/>    J = (1/(2*m))*(np.sum(np.square(pred - y)))<br/>    return J</span><span id="0c6d" class="kn ko hi mk b fi ms mp l mq mr">def grad(X,prediction,y):<br/>    <br/>    g = (1/m) * (X.T * (prediction - y))<br/>    return g</span><span id="f262" class="kn ko hi mk b fi ms mp l mq mr">def LinearRegression(X,y,theta,alpha,epoch):<br/>    <br/>    <br/>    j_history=[]<br/>    <br/>    for i in range(epoch):<br/>        <br/>        pred = prediction(X,theta)<br/>        J = cost(y,pred)<br/>        g = grad(X,pred,y)<br/>        <br/>        theta = theta - alpha*g<br/>        <br/>        j_history = np.append(j_history,J)<br/>        <br/>        print("Epoch : ",i," ","cost : ",J)<br/>    <br/>    x = np.linspace(0,epoch,epoch)<br/>    plt.ylabel("cost function")<br/>    plt.plot(x,j_history,color='r')<br/>    plt.xlabel("No. of iterations")<br/>    plt.title("Decreasing of cost function")<br/>    <br/>    <br/>    return theta,pred</span></pre><p id="32a7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">调用函数</strong></p><pre class="kh ki kj kk fd mj mk ml mm aw mn bi"><span id="7eac" class="kn ko hi mk b fi mo mp l mq mr">theta,pred = LinearRegression(X,y,theta,alpha = 0.03,epoch = 1000)</span><span id="e728" class="kn ko hi mk b fi ms mp l mq mr">Epoch :  0   cost :  107.22753436974266<br/>Epoch :  1   cost :  14.868966072066558<br/>Epoch :  2   cost :  10.872687041778397<br/>Epoch :  3   cost :  10.66332230545673<br/>Epoch :  4   cost :  10.61995377513655<br/>Epoch :  5   cost :  10.586946000902037<br/>Epoch :  6   cost :  10.557104107786772<br/>Epoch :  7   cost :  10.529680767063443<br/>Epoch :  8   cost :  10.504281786250193<br/>Epoch :  9   cost :  10.480591591287318<br/>Epoch :  10  cost :  10.458350239402089<br/>Epoch :  11  cost :  10.437343188331878<br/>Epoch :  12  cost :  10.417393249074857<br/>Epoch :  13  cost :  10.3983539796047<br/>Epoch :  14  cost :  10.380104249118626<br/>Epoch :  15  cost :  10.362543764862313<br/>Epoch :  16  cost :  10.345589391029318<br/>Epoch :  17  cost :  10.329172119456064<br/>Epoch :  18  cost :  10.3132345766786<br/>Epoch :  19  cost :  10.297728972361188<br/>Epoch :  20  cost :  10.282615410929923<br/>Epoch :  21  cost :  10.267860502088192<br/>Epoch :  22  cost :  10.253436217282786<br/>Epoch :  23  cost :  10.239318948563623<br/>.<br/>.<br/>.<br/>.</span><span id="c94b" class="kn ko hi mk b fi ms mp l mq mr">.<br/>Epoch :  999 cost :  9.357530142476707</span></pre><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es mt"><img src="../Images/b46e062424321832804c4bd9cc44b6a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/1*ogi1bcS3u6wvVcXdHiKckw.png"/></div></figure><p id="f533" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们可以在上面的图像中看到，随着纪元的进行，值在减少。</p><figure class="kh ki kj kk fd ij er es paragraph-image"><div class="er es mu"><img src="../Images/924a8ec87fc331dbe35e9cc869fceff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*k3gx5LxM0eX7ow1Tr6bCxQ.jpeg"/></div></figure><p id="b1a7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果你没有看过我之前关于K-means聚类的博客，点击这个链接:<a class="ae mv" rel="noopener" href="/analytics-vidhya/k-means-clustering-with-python-77b20c2d538d">https://medium . com/analytics-vid hya/K-means-clustering-with-python-77 b 20 c 2d 538d</a></p></div><div class="ab cl mc md gp me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="hb hc hd he hf"><p id="a58a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果你觉得这篇文章有帮助，请鼓掌👏拍手可以让更多的人看到一个帖子。</p></div></div>    
</body>
</html>