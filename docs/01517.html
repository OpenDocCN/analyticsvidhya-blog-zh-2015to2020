<html>
<head>
<title>Data Types in Spark MLlib</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark MLlib中的数据类型</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/data-types-in-spark-mllib-966b4800f893?source=collection_archive---------4-----------------------#2019-10-28">https://medium.com/analytics-vidhya/data-types-in-spark-mllib-966b4800f893?source=collection_archive---------4-----------------------#2019-10-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/a6d5d148f8ace4ec1328413228132017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DNc4Yx63-1GU_WrOe8cNTA.png"/></div></div></figure><p id="e17b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" href="https://www.scnsoft.com/blog/big-data-quality" rel="noopener ugc nofollow" target="_blank"> <em class="jp">图片来源</em> </a></p><h1 id="50ec" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">局部向量</h1><p id="1235" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">MLlib支持两种类型的局部向量:密集和稀疏。当大多数数字为零时，使用稀疏向量。要创建稀疏向量，您需要提供向量的长度——非零值的索引，它们应该是严格递增的非零值。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="f483" class="lc jr hi ky b fi ld le l lf lg">from pyspark.mllib.linalg import Vectors</span><span id="7f3b" class="lc jr hi ky b fi lh le l lf lg">## Dense Vector<br/>print(Vectors.dense([1,2,3,4,5,6,0]))<br/># &gt;&gt; DenseVector([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 0.0])</span><span id="4e36" class="lc jr hi ky b fi lh le l lf lg">### SPARSE VECTOR <br/>### Vectors.sparse( length, index_of_non_zero_values, non_zero_values)<br/>### Indices values should be strictly increasing</span><span id="b3b1" class="lc jr hi ky b fi lh le l lf lg">print(Vectors.sparse(10, [0,1,2,4,5], [1.0,5.0,3.0,5.0,7]))<br/># &gt;&gt; SparseVector(10, {0: 1.0, 1: 5.0, 2: 3.0, 4: 5.0, 5: 7.0})</span><span id="670d" class="lc jr hi ky b fi lh le l lf lg">print(Vectors.sparse(10, [0,1,2,4,5], [1.0,5.0,3.0,5.0,7]).toArray())<br/># &gt;&gt; array([1., 5., 3., 0., 5., 7., 0., 0., 0., 0.])</span></pre><h1 id="e56d" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">标记点</h1><p id="9422" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">标记点是一个局部向量，其中一个标签被分配给每个向量。你必须解决监督的问题，你有一些目标对应的一些功能。标注点与将矢量作为一组要素和与之关联的标注提供的情况完全相同。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="0a18" class="lc jr hi ky b fi ld le l lf lg">from pyspark.mllib.regression import LabeledPoint</span><span id="a7f0" class="lc jr hi ky b fi lh le l lf lg"># set a Label against a Dense Vector<br/>point_1 = LabeledPoint(1,Vectors.dense([1,2,3,4,5]))</span><span id="a708" class="lc jr hi ky b fi lh le l lf lg"># Features <br/>print(point_1.features)</span><span id="a4da" class="lc jr hi ky b fi lh le l lf lg"># Label<br/>print(point_1.label)</span></pre><h1 id="d84f" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">局部矩阵</h1><p id="5a0e" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">局部矩阵存储在一台机器上。MLlib支持密集矩阵和稀疏矩阵。在稀疏矩阵中，非零项值以列主顺序存储在压缩稀疏列(CSC)格式中。</p><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="6bf8" class="lc jr hi ky b fi ld le l lf lg"># import the Matrices<br/>from pyspark.mllib.linalg import Matrices</span><span id="e5f9" class="lc jr hi ky b fi lh le l lf lg"># create a dense matrix of 3 Rows and 2 columns<br/>matrix_1 = Matrices.dense(3, 2, [1,2,3,4,5,6])</span><span id="cc95" class="lc jr hi ky b fi lh le l lf lg">print(matrix_1)<br/># &gt;&gt; DenseMatrix(3, 2, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], False)</span><span id="6a9c" class="lc jr hi ky b fi lh le l lf lg">print(matrix_1.toArray())<br/>"""<br/>&gt;&gt; array([[1., 4.],<br/>          [2., 5.],<br/>          [3., 6.]])<br/>"""</span><span id="7fd6" class="lc jr hi ky b fi lh le l lf lg"># create a sparse matrix<br/>matrix_2 = Matrices.sparse(3, 3, [0, 1, 2, 3], [0, 0, 2], [9, 6, 8])</span><span id="0e85" class="lc jr hi ky b fi lh le l lf lg">print(matrix_2)<br/># SparseMatrix(3, 3, [0, 1, 2, 3], [0, 0, 2], [9.0, 6.0, 8.0], False)</span><span id="7f40" class="lc jr hi ky b fi lh le l lf lg">print(matrix_2.toArray())<br/>"""<br/>&gt;&gt; array([[9., 6., 0.],<br/>          [0., 0., 0.],<br/>"""</span></pre><h1 id="b1b0" class="jq jr hi bd js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn bi translated">分布式矩阵</h1><p id="4743" class="pw-post-body-paragraph iq ir hi is b it ko iv iw ix kp iz ja jb kq jd je jf kr jh ji jj ks jl jm jn hb bi translated">分布式矩阵存储在一个或多个rdd中。选择正确的分布式矩阵格式非常重要。迄今为止，已经实现了四种类型的分布式矩阵:</p><ul class=""><li id="8149" class="li lj hi is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated"><strong class="is hj">行矩阵</strong>:每行是一个局部向量。您可以在多个分区上存储行。像随机森林这样的算法可以使用行矩阵来实现，因为该算法划分行以创建多个树。一棵树的结果不依赖于其他树。因此，我们可以利用分布式架构，对大数据的随机森林等算法进行并行处理</li></ul><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="8b9b" class="lc jr hi ky b fi ld le l lf lg"># Distributed Data Type - Row Matrix<br/>from pyspark.mllib.linalg.distributed import RowMatrix</span><span id="644a" class="lc jr hi ky b fi lh le l lf lg"># create RDD<br/>rows = sc.parallelize([[1,2,3], [4,5,6], [7,8,9], [10,11,12]])</span><span id="06bd" class="lc jr hi ky b fi lh le l lf lg"># create a distributed Row Matrix<br/>row_matrix = RowMatrix(rows)</span><span id="7ac4" class="lc jr hi ky b fi lh le l lf lg">print(row_matrix)<br/># &gt;&gt; &lt;pyspark.mllib.linalg.distributed.RowMatrix at 0x7f425884d7f0&gt;</span><span id="eb0a" class="lc jr hi ky b fi lh le l lf lg">print(row_matrix.numRows())<br/># &gt;&gt; 4</span><span id="4d4c" class="lc jr hi ky b fi lh le l lf lg">print(row_matrix.numCols())<br/># &gt;&gt; 3</span></pre><ul class=""><li id="0b7d" class="li lj hi is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated"><strong class="is hj">索引行矩阵:</strong>它类似于行矩阵，行存储在多个分区中，但以有序的方式存储。为每一行分配一个索引值。它用于顺序很重要的算法中，如时间序列数据。它可以从索引行的RDD创建</li></ul><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="35b7" class="lc jr hi ky b fi ld le l lf lg"># Indexed Row Matrix</span><span id="2aa9" class="lc jr hi ky b fi lh le l lf lg">from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix</span><span id="40a1" class="lc jr hi ky b fi lh le l lf lg"># create RDD<br/>indexed_rows = sc.parallelize([<br/>    IndexedRow(0, [0,1,2]),<br/>    IndexedRow(1, [1,2,3]),<br/>    IndexedRow(2, [3,4,5]),<br/>    IndexedRow(3, [4,2,3]),<br/>    IndexedRow(4, [2,2,5]),<br/>    IndexedRow(5, [4,5,5])<br/>])</span><span id="6d42" class="lc jr hi ky b fi lh le l lf lg"># create IndexedRowMatrix<br/>indexed_rows_matrix = IndexedRowMatrix(indexed_rows)</span><span id="b249" class="lc jr hi ky b fi lh le l lf lg">print(indexed_rows_matrix.numRows())<br/># &gt;&gt; 6</span><span id="9aa8" class="lc jr hi ky b fi lh le l lf lg">print(indexed_rows_matrix.numCols())<br/># &gt;&gt; 3</span></pre><ul class=""><li id="2ac3" class="li lj hi is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated"><strong class="is hj">坐标矩阵:</strong>可以从MatrixEntry的RDD创建坐标矩阵。只有当矩阵的维数都很大时，我们才使用坐标矩阵。</li></ul><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="05a9" class="lc jr hi ky b fi ld le l lf lg">from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry</span><span id="6fb8" class="lc jr hi ky b fi lh le l lf lg"># Create an RDD of coordinate entries with the MatrixEntry class:<br/>matrix_entries = sc.parallelize(<br/>[MatrixEntry(0, 5, 2), <br/>MatrixEntry(1, 1, 1), <br/>MatrixEntry(1, 5, 4)])</span><span id="ac51" class="lc jr hi ky b fi lh le l lf lg"># Create an CoordinateMatrix from an RDD of MatrixEntries.<br/>c_matrix = CoordinateMatrix(matrix_entries)</span><span id="a473" class="lc jr hi ky b fi lh le l lf lg"># number of columns<br/>print(c_matrix.numCols())<br/># &gt;&gt; 6</span><span id="9e63" class="lc jr hi ky b fi lh le l lf lg"># number of rows<br/>print(c_matrix.numRows())<br/># &gt;&gt; 2</span></pre><ul class=""><li id="3c64" class="li lj hi is b it iu ix iy jb lk jf ll jj lm jn ln lo lp lq bi translated"><strong class="is hj">分块矩阵:</strong>在一个分块矩阵中，我们可以在不同的机器上存储一个大矩阵的不同子矩阵。我们需要指定块的尺寸。就像下面的例子，我们有3X3，对于每个块，我们可以通过提供坐标来指定一个矩阵。</li></ul><pre class="kt ku kv kw fd kx ky kz la aw lb bi"><span id="b2b8" class="lc jr hi ky b fi ld le l lf lg"># import the libraries<br/>from pyspark.mllib.linalg import Matrices<br/>from pyspark.mllib.linalg.distributed import BlockMatrix</span><span id="9bc0" class="lc jr hi ky b fi lh le l lf lg"># Create an RDD of sub-matrix blocks.<br/>blocks = sc.parallelize(<br/>[((0, 0), Matrices.dense(3, 3, [1, 2, 1, 2, 1, 2, 1, 2, 1])),<br/> ((1, 1), Matrices.dense(3, 3, [3, 4, 5, 3, 4, 5, 3, 4, 5])),<br/> ((2, 0), Matrices.dense(3, 3, [1, 1, 1, 1, 1, 1, 1, 1, 1]))])</span><span id="65ce" class="lc jr hi ky b fi lh le l lf lg"># Create a BlockMatrix from an RDD of sub-matrix blocks  of size 3X3<br/>b_matrix = BlockMatrix(blocks, 3, 3)</span><span id="98b6" class="lc jr hi ky b fi lh le l lf lg"># columns per block<br/>print(b_matrix.colsPerBlock)<br/># &gt;&gt; 3</span><span id="dfaf" class="lc jr hi ky b fi lh le l lf lg"># rows per block<br/>print(b_matrix.rowsPerBlock)<br/># &gt;&gt; 3</span><span id="e527" class="lc jr hi ky b fi lh le l lf lg"># convert the block matrix to local matrix<br/>local_mat = b_matrix.toLocalMatrix()</span><span id="6b4b" class="lc jr hi ky b fi lh le l lf lg"># print local matrix<br/>print(local_mat.toArray())<br/>"""<br/>&gt;&gt; array([[1., 2., 1., 0., 0., 0.],<br/>          [2., 1., 2., 0., 0., 0.],<br/>          [1., 2., 1., 0., 0., 0.],<br/>          [0., 0., 0., 3., 3., 3.],<br/>          [0., 0., 0., 4., 4., 4.],<br/>          [0., 0., 0., 5., 5., 5.],<br/>          [1., 1., 1., 0., 0., 0.],<br/>          [1., 1., 1., 0., 0., 0.],<br/>          [1., 1., 1., 0., 0., 0.]])<br/>"""</span></pre><p id="630b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jp">原载于2019年10月28日</em><a class="ae jo" href="https://www.analyticsvidhya.com/blog/2019/10/pyspark-for-beginners-first-steps-big-data-analysis/?utm_source=av&amp;utm_medium=feed-articles&amp;utm_campaign=feed" rel="noopener ugc nofollow" target="_blank"><em class="jp">https://www.analyticsvidhya.com</em></a><em class="jp">。</em></p></div></div>    
</body>
</html>