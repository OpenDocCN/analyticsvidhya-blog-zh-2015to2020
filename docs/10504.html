<html>
<head>
<title>Probabilistic Retrieval</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">概率检索</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/nlp-lecture-series-from-basic-to-advance-level-additional-content-1c1e51c9f936?source=collection_archive---------13-----------------------#2020-10-21">https://medium.com/analytics-vidhya/nlp-lecture-series-from-basic-to-advance-level-additional-content-1c1e51c9f936?source=collection_archive---------13-----------------------#2020-10-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/9473901cc549874f8c90cd2ee6803a7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wnp8BwGueJcCIopCaDOSPw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">作者图片</figcaption></figure><div class=""/><p id="0243" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">当我们通过将常用术语和文档频率作为参数馈送到贝叶斯模型来估计文档与给定查询的相关程度时，我们称之为概率检索，或者我们可以用其他术语来说，我们可以说这是试图根据概率论来形式化排序检索背后的思想。</p><p id="0dbc" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这个模型其实是基于一些假设<a class="ae js" href="https://www.researchgate.net/publication/242353981_Probability_of_Relevance_A_Unification_of_Two_Competing_Models_for_Document_Retrieval" rel="noopener ugc nofollow" target="_blank"> </a>。我们假设</p><ul class=""><li id="396a" class="jt ju hx iw b ix iy jb jc jf jv jj jw jn jx jr jy jz ka kb bi translated">每个文档与给定的查询相关或不相关</li><li id="b2b0" class="jt ju hx iw b ix kc jb kd jf ke jj kf jn kg jr jy jz ka kb bi translated">判断一个文档相关或不相关并不能告诉我们另一个文档的相关性。</li></ul><p id="d7b7" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">基于这两点，这个理论不承认相关度，也没有告诉我们找到一个文档可能会使另一个变得不相关。这两点得出结论，相关性的理论概念与实际效用有多远，即在实际概念中，这不能告诉我们一个文档对搜索者有多有用。</p><p id="28ab" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这些相关概率确实有实际用途。我们使用这些相关概率来确定向用户呈现点击的顺序。这里出现了<strong class="iw hy">概率排序原则的概念。</strong></p><blockquote class="kh"><p id="7bc6" class="ki kj hx bd kk kl km kn ko kp kq jr dx translated"><strong class="ak">概率排序原则</strong> <a class="ae js" href="https://www.researchgate.net/publication/235253512_The_Probability_Ranking_Principle_in_IR" rel="noopener ugc nofollow" target="_blank"> <strong class="ak"> </strong> </a> <strong class="ak"> : </strong>根据可用数据，通过降低与查询相关的概率来对文档进行排序将产生最佳“性能”，即最佳排序。</p></blockquote><h2 id="55a8" class="kr ks hx bd kt ku kv kw kx ky kz la lb jf lc ld le jj lf lg lh jn li lj lk ll bi translated"><a class="ae js" href="https://doc.lagout.org/science/Artificial%20Intelligence/Natural%20Language%20Processing/Natural%20Language%20Processing%20for%20Online%20Applications%20Text%20Retrieval%2CExtraction%20and%20Categorization%20-%20Peter%20Jackson%20%2C%20Isabelle%20Moulinier.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">关联概率</strong> </a> <strong class="ak"> : </strong></h2><p id="0d25" class="pw-post-body-paragraph iu iv hx iw b ix lm iz ja jb ln jd je jf lo jh ji jj lp jl jm jn lq jp jq jr hb bi translated">给定查询‘q’的文档‘d’的相关性概率可以表示为:<strong class="iw hy"> P(Rᵩ = X|D) <br/> </strong>这里我们根据相关性的二元性质假设 X ∈ {0，1}。</p><p id="e777" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们进一步计算这个比率，如下所示</p><p id="d6e2" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">查询和文档的匹配分数可以表示为相关概率和不相关概率之间的比率:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div class="er es lr"><img src="../Images/1463e9089ec9cf5b06e12d7a81d98921.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*-rYTN5YscJAPmqrIG1hCTg.png"/></div></figure><p id="fad4" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因为<strong class="iw hy"> P(Rᵩ=1) </strong>这里随机选择的文档与查询相关，并且是在不知道其内容的情况下选择的。对于<strong class="iw hy"> P(Rᵩ = 0 也是如此)。</strong>我们忽略了原始方程中的这两个量，因为这个量对于所有文件都是相同的，所以方程变为:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lw"><img src="../Images/03c509fb8fd110e70764513ddae9819d.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*SOWZeB4P-gLKKcMGocEXlg.png"/></div></div></figure><p id="6150" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">双面登录:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div class="er es lx"><img src="../Images/1d0e620dd997f4a36c8b6903fa350c1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*t7sZ29yGepvuXr9zYW3_6A.png"/></div></figure><p id="42ad" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hy"> P(D|Rᵩ = 1) </strong>这里是从相关集合中选择文档的概率，<strong class="iw hy"> P(D|Rᵩ = 0) </strong>是从不相关集合中选择文档的概率。为了估计这些，我们需要看到查询词在文档和整个集合中的分布。<br/> Q= {t1，t2……。tm}。为此，我们需要计算每一项 t 的权重(wₜ,𝒹)</p><h2 id="0f0b" class="kr ks hx bd kt ku ly kw kx ky lz la lb jf ma ld le jj mb lg lh jn mc lj lk ll bi translated"><strong class="ak">期限权重:</strong></h2><p id="df8d" class="pw-post-body-paragraph iu iv hx iw b ix lm iz ja jb ln jd je jf lo jh ji jj lp jl jm jn lq jp jq jr hb bi translated">N =集合的大小<br/> nₜ =包含查询项 t 的文档数量<br/>因此，权重的一个分量由下式给出</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div class="er es md"><img src="../Images/f49bee1d01034e656526100b354c468c.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*yv0GJPP5Ei2Hj2BU1nCL3Q.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">反向文档频率的平滑版本</figcaption></figure><p id="8936" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如果文档内频率计数不可用，匹配分数可通过将各部分相加得出，如下所示:</p><figure class="ls lt lu lv fd hk er es paragraph-image"><div class="er es me"><img src="../Images/d2b0ff7f91a2f6d5dc5fdd89f8539c21.png" data-original-src="https://miro.medium.com/v2/resize:fit:552/format:webp/1*UB-S_Zx7H1mTEiPhNZ85-g.png"/></div></figure></div><div class="ab cl mf mg gp mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="hb hc hd he hf"><p id="f86e" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">[1](<a class="ae js" href="https://www.researchgate.net/profile/Stephen_Robertson2" rel="noopener ugc nofollow" target="_blank">Robertson</a>et al . 1982)关联概率:两种竞争性文献检索模型的统一。</p><p id="5ac5" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">[2](<a class="ae js" href="https://www.researchgate.net/publication/235253512_The_Probability_Ranking_Principle_in_IR" rel="noopener ugc nofollow" target="_blank">Robertson</a>1977)IR 中的概率排序原理。</p><p id="9d31" class="pw-post-body-paragraph iu iv hx iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">[3] ( <a class="ae js" href="https://doc.lagout.org/science/Artificial%20Intelligence/Natural%20Language%20Processing/Natural%20Language%20Processing%20for%20Online%20Applications%20Text%20Retrieval%2CExtraction%20and%20Categorization%20-%20Peter%20Jackson%20%2C%20Isabelle%20Moulinier.pdf" rel="noopener ugc nofollow" target="_blank"> Peter Jacksons 和 Isabelle Moulinier </a> ) NLP 用于在线应用、文本检索、提取和分类</p></div></div>    
</body>
</html>