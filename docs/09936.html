<html>
<head>
<title>Formulation of Normal Equation Method for Linear Regression and Polynomial Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归和多项式回归的正规方程法公式</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-and-polynomial-regression-using-normal-equation-method-c3929d71734d?source=collection_archive---------11-----------------------#2020-09-27">https://medium.com/analytics-vidhya/linear-regression-and-polynomial-regression-using-normal-equation-method-c3929d71734d?source=collection_archive---------11-----------------------#2020-09-27</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/5b07d537b453b348f5605794bd292b33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ck2VzHlizyEU5168zzFXyw.png"/></div></div></figure><div class=""/><p id="83f8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性回归和多项式回归是机器学习中简单的统计模型之一。</p><p id="fb75" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"/>回归是对因变量和自变量之间的关系进行建模的一种方法<br/>回归有多种类型，如线性回归、多项式回归等。在这篇博客中，我们将讨论如何用正规方程方法解释线性回归和多项式回归。</p><h1 id="a1fa" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">数据集</h1><figure class="kn ko kp kq fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es km"><img src="../Images/7fa0d9e8f8ce47506e9ea31410840796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J1cfDf__d5BKXhjcJNDNnw.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx translated"><a class="ae kv" href="http://cs229.stanford.edu/notes/cs229-notes1.pdf" rel="noopener ugc nofollow" target="_blank">http://cs229.stanford.edu/notes/cs229-notes1.pdf</a></figcaption></figure><p id="1d56" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">线性回归的假设&amp;多项式回归</strong></p><figure class="kn ko kp kq fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kw"><img src="../Images/7ff3898286ba5d492a7e770e56cfaf59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nzBAyAm1y6OfkMQakPRqWw.png"/></div></div></figure><h1 id="8b1d" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">损失函数</strong></h1><figure class="kn ko kp kq fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kx"><img src="../Images/72b950555e88c00df0f4dc8af734dea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ziyyP3Pc4CLKsc8b_PqRg.png"/></div></div></figure><p id="c477" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们需要找到最佳的W，使误差最小。有许多寻找最佳W的技术，如梯度下降法、正规方程法等。梯度下降法是一种寻找可微函数局部极小值的迭代优化算法。在梯度下降中，我们从参数的初始假设开始，逐步调整这些参数，以便我们可以得到最适合给定数据点的函数。</p><figure class="kn ko kp kq fd hk er es paragraph-image"><div class="er es ky"><img src="../Images/d2fb57f8aca5eb4990eaa4025f1a080f.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/1*gtVA33CfOsgADkCB1FBizQ.gif"/></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">梯度下降算法</figcaption></figure><p id="8027" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们用传统的数学方法找到最小值。没错，那就是法方程法。正规方程在涉及矩阵求逆和其他昂贵的矩阵运算的单一步骤中计算出参数。</p><p id="1d21" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">法方程法的公式化</strong></p><figure class="kn ko kp kq fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es kz"><img src="../Images/cb87d5ef1bb943622152b6cfb29468ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2iacsMD61Wvi-xVVyyRiDQ.png"/></div></div></figure><p id="eec3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">瞬时梯度为零的点将是函数的最小值。所以，我们把导数设为零。</p><figure class="kn ko kp kq fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es la"><img src="../Images/0bf62a55f7074fa6770fae01e697ce5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ySObLVFdu_BJJlnk4pKtOg.png"/></div></div><figcaption class="kr ks et er es kt ku bd b be z dx translated">X_t * X =方差(X) | X_t * Y =协方差(X，Y)</figcaption></figure><p id="ddca" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是我们对W的解决方案，在这里损失最小。这里W只是系数。如果我们有多元数据，那么W将是超平面的法线，对于一元数据集，W将是斜率和截距。<br/> <strong class="is hu">备注:</strong>无论我们从梯度下降中得到什么系数，都与正规方程的系数相同</p><h2 id="0fd1" class="lb jp ht bd jq lc ld le ju lf lg lh jy jb li lj kc jf lk ll kg jj lm ln kk lo bi translated"><strong class="ak">使用正规方程法进行线性回归的代码</strong></h2><pre class="kn ko kp kq fd lp lq lr ls aw lt bi"><span id="b56a" class="lb jp ht lq b fi lu lv l lw lx">def fnLinearRegression_NormalEquation(X, Y):<br/>    X_transpose = np.transpose(X)<br/>    X_covariance = np.dot(X_transpose, X)<br/>    X_covariance_inv = np.linalg.inv(X_covariance)<br/>    Y_covarinace = np.dot(np.transpose(X), Y)<br/>    Coeff = np.dot(X_covariance_inv, Y_covarinace)<br/>    return Coeff</span><span id="8698" class="lb jp ht lq b fi ly lv l lw lx">X_train_NE = np.c_[X_train,np.ones(X_train.shape[0])]<br/>X_test_NE = np.c_[X_test,np.ones(X_test.shape[0])]Coeff = fnLinearRegression_NormalEquation(X_train_NE, Y_train)</span></pre><h1 id="8a16" class="jo jp ht bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated"><strong class="ak">对比:</strong></h1><p id="e5ca" class="pw-post-body-paragraph iq ir ht is b it lz iv iw ix ma iz ja jb mb jd je jf mc jh ji jj md jl jm jn hb bi translated"><strong class="is hu">梯度下降</strong></p><ul class=""><li id="7232" class="me mf ht is b it iu ix iy jb mg jf mh jj mi jn mj mk ml mm bi translated">需要选择学习率</li><li id="f73b" class="me mf ht is b it mn ix mo jb mp jf mq jj mr jn mj mk ml mm bi translated">需要多次迭代—可能会使速度变慢</li><li id="b2d6" class="me mf ht is b it mn ix mo jb mp jf mq jj mr jn mj mk ml mm bi translated">即使当n很大(百万)时也能很好地工作</li><li id="9429" class="me mf ht is b it mn ix mo jb mp jf mq jj mr jn mj mk ml mm bi translated">更适合大数据</li></ul><p id="c0f2" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">正规方程</strong></p><ul class=""><li id="c5c1" class="me mf ht is b it iu ix iy jb mg jf mh jj mi jn mj mk ml mm bi translated">无需选择学习率</li><li id="b53c" class="me mf ht is b it mn ix mo jb mp jf mq jj mr jn mj mk ml mm bi translated">不需要迭代，检查收敛等。</li><li id="0ed9" class="me mf ht is b it mn ix mo jb mp jf mq jj mr jn mj mk ml mm bi translated">正规方程需要计算(X^T X)^-1，这是(n×n)矩阵的逆，逆计算需要O(N)时间。</li><li id="1edd" class="me mf ht is b it mn ix mo jb mp jf mq jj mr jn mj mk ml mm bi translated">如果n较大，则速度较慢</li></ul><p id="7732" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> ipython笔记本:<br/></strong><a class="ae kv" href="https://github.com/VenkateshGupt/MediumArticles/blob/master/Linear%20Regression_Normal_Equation_Method.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/VenkateshGupt/medium articles/blob/master/Linear % 20 regression _ Normal _ Equation _ method . ipynb</a></p></div></div>    
</body>
</html>