<html>
<head>
<title>Machine Learning with Apache Spark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Apache Spark进行机器学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/machine-learning-with-apache-spark-1e2c0724f0a5?source=collection_archive---------12-----------------------#2020-11-04">https://medium.com/analytics-vidhya/machine-learning-with-apache-spark-1e2c0724f0a5?source=collection_archive---------12-----------------------#2020-11-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/b0b1def3fe7b3df135ad353236507584.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*38JGbMDPrxGlif8C.jpg"/></div></div></figure><p id="f1c9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">大数据现在是我们生活的一部分，大多数收集数据的公司都必须处理大数据，以便从中获得有意义的见解。虽然我们知道当我们有一个大数据集时，复杂的神经网络工作得非常漂亮和准确，但有时它们并不是最理想的。然而，在预测的复杂性很高的情况下，预测确实需要快速和有效。因此，我们需要一个可扩展的机器学习解决方案。</p><p id="5189" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Apache spark自带SparkML。SparkML具有强大的内置机器学习算法，这些算法针对并行处理进行了优化，因此在大数据上非常省时。在本文中，我们将以SparkML管道为例，对大数据进行清理、处理和生成预测。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jo"><img src="../Images/a26ca1a36ed89807aa15e720ffb23347.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/0*jiEgRFH0McNEYGae.jpeg"/></div></figure><p id="7102" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将采用JFK机场的天气数据，并尝试SparkML中的几个内置分类器。数据集包含风速、湿度、站压等栏目。我们将尝试根据其他输入对风向进行分类。</p><p id="3264" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们用Spark清理数据集。请注意，我会为这段代码留下一个到我的GitHub repo的链接，这样你就不必从这里复制它了。然而，我将在本文中解释代码</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="ec93" class="jy jz hi ju b fi ka kb l kc kd">from pyspark import SparkContext, SparkConf<br/>from pyspark.sql import SparkSession<br/>from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer, MinMaxScaler<br/>from pyspark.ml.linalg import Vectors<br/>from pyspark.ml import Pipeline<br/>import random<br/>from pyspark.sql.functions import translate, col</span><span id="daa0" class="jy jz hi ju b fi ke kb l kc kd"># spark context<br/>sc = SparkContext.getOrCreate(SparkConf().setMaster("local[*]"))</span><span id="e9e1" class="jy jz hi ju b fi ke kb l kc kd">spark = SparkSession \<br/>    .builder \<br/>    .getOrCreate()</span><span id="4d68" class="jy jz hi ju b fi ke kb l kc kd"># create a dataframe out of it by using the first row as field names # and trying to infer a schema based on contents<br/>df = spark.read.option("header", "true").option("inferSchema","true").csv('noaa-weather-data-jfk-airport/jfk_weather.csv')</span><span id="0d26" class="jy jz hi ju b fi ke kb l kc kd"># register a corresponding query table. we do this to save the data #in memory and run our operations on it. <br/>df.createOrReplaceTempView('df')</span><span id="aab1" class="jy jz hi ju b fi ke kb l kc kd"># cleaning the data as it contains trailing charcters. Double is a #data type like float<br/># columns with no trailing charecters were straight converrted to double type, rest were first cleaned<br/>df_cleaned = df \<br/>    .withColumn("HOURLYWindSpeed", df.HOURLYWindSpeed.cast('double')) \<br/>    .withColumn("HOURLYWindDirection", df.HOURLYWindDirection.cast('double')) \<br/>    .withColumn("HOURLYStationPressure", translate(col("HOURLYStationPressure"), "s,", "")) \<br/>    .withColumn("HOURLYPrecip", translate(col("HOURLYPrecip"), "s,", "")) \<br/>    .withColumn("HOURLYRelativeHumidity", translate(col("HOURLYRelativeHumidity"), "*", "")) \<br/>    .withColumn("HOURLYDRYBULBTEMPC", translate(col("HOURLYDRYBULBTEMPC"), "*", "")) \</span><span id="6e74" class="jy jz hi ju b fi ke kb l kc kd"># the cleaned columsn were now changed to double types<br/>df_cleaned =   df_cleaned \<br/>                    .withColumn("HOURLYStationPressure", df_cleaned.HOURLYStationPressure.cast('double')) \<br/>                    .withColumn("HOURLYPrecip", df_cleaned.HOURLYPrecip.cast('double')) \<br/>                    .withColumn("HOURLYRelativeHumidity", df_cleaned.HOURLYRelativeHumidity.cast('double')) \<br/>                    .withColumn("HOURLYDRYBULBTEMPC", df_cleaned.HOURLYDRYBULBTEMPC.cast('double')) \</span><span id="946e" class="jy jz hi ju b fi ke kb l kc kd"># Filtering for clean data set with no nulls and wind speed not 0<br/>df_filtered = df_cleaned.filter("""<br/>    HOURLYWindSpeed &lt;&gt; 0<br/>    and HOURLYWindSpeed IS NOT NULL<br/>    and HOURLYWindDirection IS NOT NULL<br/>    and HOURLYStationPressure IS NOT NULL<br/>    and HOURLYPressureTendency IS NOT NULL<br/>    and HOURLYPrecip IS NOT NULL<br/>    and HOURLYRelativeHumidity IS NOT NULL<br/>    and HOURLYDRYBULBTEMPC IS NOT NULL<br/>""")</span><span id="b62e" class="jy jz hi ju b fi ke kb l kc kd"># saving the cleaned data set into CSV<br/>df_filtered.write.csv('clean_df.csv')</span></pre><p id="51e1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的代码是我的预处理脚本，它将为我提供一个干净的数据框架来工作，并尝试不同的机器学习方法。在查看原始数据后，我得出了以下需要在预处理中解决的观察结果:</p><ol class=""><li id="9e87" class="kf kg hi is b it iu ix iy jb kh jf ki jj kj jn kk kl km kn bi translated">数据类型是字符串，需要转换成算法可读的形式</li><li id="8ddf" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">某些列有尾随字符，如“s”和“*”</li><li id="d0ca" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated">我们在目标列中有大量的空值和不可用的值，比如0</li></ol><p id="4593" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">代码的第一部分处理所需的尾部字符，并将其他列转换为类似浮点值的“double”类型。代码的第二部分将干净的列转换为double类型。代码的第三部分从预测值中过滤出所有空值，从目标变量中过滤出0值。最后，我们将文件作为干净的df保存到CSV中，我们将使用它进行机器学习操作。</p><p id="7e55" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我已经在我的主代码中尝试了几个分类器，以检查哪一个效果最好，然而，在本文中，我将只举一个逻辑回归的例子。再次没有必要从这里复制代码，因为我会提供我的GitHub回购链接。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="afe1" class="jy jz hi ju b fi ka kb l kc kd">from pyspark import SparkContext, SparkConf<br/>from pyspark.sql import SparkSession<br/>from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, Normalizer, MinMaxScaler<br/>from pyspark.ml.classification import LogisticRegression<br/>from pyspark.ml.evaluation import MulticlassClassificationEvaluator<br/>from pyspark.ml.feature import Bucketizer<br/>from pyspark.ml.stat import Correlation<br/>from pyspark.ml.linalg import Vectors<br/>from pyspark.ml import Pipeline<br/>import random</span><span id="716b" class="jy jz hi ju b fi ke kb l kc kd"># read our clean csv<br/>df_filtered = spark.read.csv('clean_df.csv')</span><span id="f4c7" class="jy jz hi ju b fi ke kb l kc kd"># vector assembler<br/>vectorAssembler = VectorAssembler(inputCols=["HOURLYWindSpeed","","HOURLYStationPressure"],<br/>                                  outputCol="features")<br/>df_pipeline = vectorAssembler.transform(df_filtered)</span><span id="64f4" class="jy jz hi ju b fi ke kb l kc kd"># checking correlations<br/>Correlation.corr(df_pipeline,"features").head()[0].toArray()</span><span id="e1cf" class="jy jz hi ju b fi ke kb l kc kd"># train test split <br/>splits = df_filtered.randomSplit([0.8, 0.2])<br/>df_train = splits[0]<br/>df_test = splits[1]</span><span id="3b69" class="jy jz hi ju b fi ke kb l kc kd"># discretize the value using the Bucketizer, where we split the #column in buckets from above 0, 180 and then infinity<br/>bucketizer = Bucketizer(splits=[ 0, 180, float('Inf') ],inputCol="HOURLYWindDirection", outputCol="HOURLYWindDirectionBucketized")</span><span id="ab17" class="jy jz hi ju b fi ke kb l kc kd"># after the bucketizer we do one hot enncoding <br/>encoder = OneHotEncoder(inputCol="HOURLYWindDirectionBucketized", outputCol="HOURLYWindDirectionOHE")</span><span id="48ed" class="jy jz hi ju b fi ke kb l kc kd"># funtion for ccuracy calculation<br/>def classification_metrics(prediction):<br/>    mcEval = MulticlassClassificationEvaluator().setMetricName("accuracy") .setPredictionCol("prediction").setLabelCol("HOURLYWindDirectionBucketized")<br/>    accuracy = mcEval.evaluate(prediction)<br/>    print("Accuracy on test data = %g" % accuracy)</span><span id="b561" class="jy jz hi ju b fi ke kb l kc kd"># logistic regression<br/># defining the model<br/>lr = LogisticRegression(labelCol="HOURLYWindDirectionBucketized", maxIter=10)</span><span id="9ba3" class="jy jz hi ju b fi ke kb l kc kd"># new vector assembler<br/>vectorAssembler = VectorAssembler(inputCols=["HOURLYWindSpeed","HOURLYDRYBULBTEMPC"],<br/>                                  outputCol="features")</span><span id="edb1" class="jy jz hi ju b fi ke kb l kc kd"># bew piplineline for lr<br/>pipeline = Pipeline(stages=[bucketizer,vectorAssembler,normalizer,lr])</span><span id="0bcd" class="jy jz hi ju b fi ke kb l kc kd"># predictions<br/>model = pipeline.fit(df_train)<br/>prediction = model.transform(df_test)<br/>classification_metrics(prediction)</span></pre><p id="1b7d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的代码使用了SparkMl的一些常用功能，这是一个从事spark工作的数据科学家应该知道的。这些是:</p><ol class=""><li id="bb45" class="kf kg hi is b it iu ix iy jb kh jf ki jj kj jn kk kl km kn bi translated"><strong class="is hj">向量组合器</strong>:主要用于将所有特征连接成一个向量，该向量可以进一步传递给估计器或ML算法</li><li id="268b" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated"><strong class="is hj">相关性</strong> : spark为更好的特征工程提供了一个方便的工具来检查相关性</li><li id="f8cd" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated"><strong class="is hj"> Bucketizer: </strong> Bucketing是一种将连续变量转换为分类变量的方法。我们在拆分中提供了一系列存储桶，这使我们能够进行分类并使用分类算法</li><li id="2a58" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated"><strong class="is hj">OneHotEncoder</strong>:One hotencoder是一个过程，通过这个过程，分类变量被转换成一种可以提供给ML算法的形式，以便在预测中做得更好。</li><li id="a7d0" class="kf kg hi is b it ko ix kp jb kq jf kr jj ks jn kk kl km kn bi translated"><strong class="is hj">管道:</strong>Spark中的管道功能允许您定义一组特定的流程以及它们的执行顺序。管道也可以被保存并在以后使用，这使得它成为可伸缩性和可移植性的一个很好的工具。</li></ol><p id="dc6f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在我的代码库中，我使用了其他几个模型，比如随机森林，梯度增强树。我还在同一数据集上预测风速时尝试了一个回归问题。如果您有疑问，请随时联系我们。</p><p id="951c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">谢谢！</p><p id="77d2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">链接到我的GitHub repo:</p><p id="3bfc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae kt" href="https://github.com/manikmal/spark_ml" rel="noopener ugc nofollow" target="_blank">https://github.com/manikmal/spark_ml</a></p></div></div>    
</body>
</html>