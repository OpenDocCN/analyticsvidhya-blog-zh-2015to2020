<html>
<head>
<title>An Illustrated Overview of Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的图解概述</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/an-illustrated-overview-of-reinforcement-learning-ccc47ae43b6?source=collection_archive---------21-----------------------#2020-04-24">https://medium.com/analytics-vidhya/an-illustrated-overview-of-reinforcement-learning-ccc47ae43b6?source=collection_archive---------21-----------------------#2020-04-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2143" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">强化学习(RL)是一种不同于传统机器学习(有监督和无监督)的学习范式。这里考虑的学习问题模仿人类使用试错法从交互中学习，并且在历史上被用于规划/决策相关的问题，如机器人和自动驾驶。在需要做出连续决策以优化指标的情况下，我们可以应用RL。让我们考虑一个新闻推荐问题的例子。向用户呈现他们可能感兴趣的文章列表，用户决定是点击(并阅读)还是不点击。我们(称为代理)如何组织或排列文章以最大化点击率(CTR)？有没有一种策略(RL术语中称之为策略)可以带来更好的相关性，从而提高CTR？对这个问题使用监督学习将尝试对所有文章独立地学习P(点击|文章)。对于RL，在每一个位置，考虑到用户已经看过的文章的顺序，我们试图学习P(点击|文章1，文章2，文章3)。鉴于用户已经看过文章1、2和3，我们通过推荐这篇文章来最大化我们期望收到的总的未来累积奖励(点击)。直觉上，考虑已经看过的文章的顺序是有意义的，因为成百上千的文章可能与不同主题的用户相关，如果我们最终显示一个主题的文章，我们可能会冒着用户疲劳并退出页面的风险。相反，RL试图达到最佳策略，平衡用户感兴趣的主题的多样性，最大限度地提高用户未来的潜在点击量<br/>今天，RL已经被用于一些行业，取得了不同的成功，但尚未成为主流技术。人工智能已经成功地应用于游戏中，早期的胜利之一是AlphaGo 的游戏，其中一个人工智能代理击败了人类专家。消费者互联网、量化交易、资源管理、广告和制造业是RL成功应用的其他领域。RL被广泛应用于游戏的原因之一是因为我们能够通过游戏模拟收集大量数据。对于现实世界的问题，数据收集要昂贵得多，并且在某些情况下不可行，这是阻止RL广泛采用的原因之一。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es je"><img src="../Images/290caea2707b7fd32de1e996698c37e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*np20H_0sOxM_gqjQOR_8dQ.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">建模RL问题</figcaption></figure><p id="cb01" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">接下来，我们考虑称为马尔可夫决策过程(MDP)的强化学习问题的数学表示。MDP是顺序决策的经典形式，其中行动不仅影响即时奖励，还影响后续情况或状态，并通过这些未来奖励。因此，MDP涉及延迟奖励以及权衡即时和延迟奖励的需要。MDP包括以下组件</p><ul class=""><li id="e96e" class="jq jr hi ih b ii ij im in iq js iu jt iy ju jc jv jw jx jy bi translated"><strong class="ih hj">代理</strong>是学习者和决策者。</li><li id="5972" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated"><strong class="ih hj">环境</strong>包括代理与之交互的一切。</li><li id="5711" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">每当代理通过采取<strong class="ih hj">动作</strong>与环境交互时，环境通过改变其<strong class="ih hj">状态</strong>做出响应</li><li id="e971" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">对于所采取的每一个行动，代理都会收到一个标量<strong class="ih hj">奖励</strong>，表示所采取行动的良好性。奖励可以是即时的，也可以是延迟的。</li><li id="35ce" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">我的环境中有哪些状态和动作？</li><li id="68bd" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">我与环境的互动是连续的和无限的吗？还是互动有一个自然的结束状态？</li><li id="e7e3" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">有无限多(大量)的状态吗？</li><li id="af99" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">我们知道系统模型吗？或者我们是否知道我的环境对不同状态下的行为有什么反应？&gt;</li><li id="fea1" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">回报是延迟的还是即时的？代理包括试错法吗？有顺序决策吗？</li><li id="eb62" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">我应该如何设计我的奖励计划？我的奖励应该基于什么标准？</li><li id="e951" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">你的经纪人应该有多有远见？</li><li id="87fa" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">代理是否同时学习和执行在环境中采取行动的策略(策略)？</li><li id="cb43" class="jq jr hi ih b ii jz im ka iq kb iu kc iy kd jc jv jw jx jy bi translated">你的国家很复杂吗？</li></ul><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es ke"><img src="../Images/80a379ce6598804414ed1d0aa870497d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/0*sgPbLhy-zy5ASeWp.png"/></div><figcaption class="jm jn et er es jo jp bd b be z dx translated">Windy Grid:经典的网格包括从源单元导航到目标单元(每个单元都有相关的成本),选择最小成本的路径。多风的网格是对这个经典设置的修改，当你在这个网格中导航时，向上的风可以移动+1或+2个单元格。</figcaption></figure><p id="52a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">风网格世界</strong>是对著名的网格世界问题的修改。问题是通过在每个单元格中采取一系列动作(上、下、左、右)来导航7 X 10单元格的网格。目标是达到最终状态/单元— (3，7)。导致(3，7)的状态的动作得到奖励0，并且每一个其他转换导致奖励-1。我们有额外的风的约束:在第4-9列有向上吹的风，根据代理移动的列，风将代理移动+1或+2步。找出从(3，0)可能到达(3，7)的最佳路径(最低成本)。<br/>我们将使用python <a class="ae jd" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> openAI gym </a>模拟风力发电环境。Gym是open AI的一个RL工具包，它为研究人员和开发人员提供了数百个环境来开发新的RL算法。每个环境都是一个系统动态的定义，可用于模拟，并具有可视化环境的选项。环境提供了记录在一个状态中采取行动的回报的方法。在我们的例子中，重置环境将状态带到(3，0)，我们的开始状态。目标状态是(3，7)。我们有70种可能的状态，代理可以在任何时候。每个状态都表示为(行，列)索引，并存储为2D数组。在每种状态下，有4种可能的动作(0-向上，1-向右，2-向下，3-向左)</p><p id="77be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大多数当前的RL文献集中在不同的问题上，以展示各种RL技术，这使得比较更加困难。为了解决这个问题，我们将使用windy grid world来演示基于DP、MC和TD的算法。我们将学习Q函数和V函数以及最优策略pi*</p><p id="0212" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">大多数当前的RL文献集中在不同的问题上，以展示各种RL技术，这使得比较更加困难。为了解决这个问题，我们将使用windy grid world来演示基于DP、MC和TD的算法。我们将学习Q函数和V函数以及最优策略pi*</p><pre class="jf jg jh ji fd kf kg kh ki aw kj bi"><span id="a712" class="kk kl hi kg b fi km kn l ko kp">%<strong class="kg hj">run</strong> helper.py<br/><strong class="kg hj">import</strong> <strong class="kg hj">gym</strong><br/><strong class="kg hj">import</strong> <strong class="kg hj">gym_gridworlds</strong><br/>env = gym.make('WindyGridworld-v0')<br/>env.reset()</span></pre><p id="2d4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Out[59]:</p><pre class="jf jg jh ji fd kf kg kh ki aw kj bi"><span id="1bd8" class="kk kl hi kg b fi km kn l ko kp">(3, 0)</span></pre><h1 id="683b" class="kq kl hi bd kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk ll lm bi translated">动态规划</h1><p id="5d98" class="pw-post-body-paragraph if ig hi ih b ii ln ik il im lo io ip iq lp is it iu lq iw ix iy lr ja jb jc hb bi translated">DP是一种基于模型的RL技术，它假设系统的完整知识是容易获得的。即转移概率和奖励/激励方案。在下面的代码中，我们尝试使用一般化的策略迭代来迭代求解贝尔曼方程。我们从随机策略开始，评估wrt。敬它。基于作为评估的一部分而学习的价值函数，我们将把策略更新为greedy wrt。价值函数。接下来，我们再次评估新策略以获得更新的值函数，并将策略更新为贪婪的当前值函数，并且这递归地进行，直到策略不改变(我们称之为收敛标准)。此时，我们有最优策略，因此可以评估一次以获得最优值函数和最优行动值函数。</p><pre class="jf jg jh ji fd kf kg kh ki aw kj bi"><span id="a93f" class="kk kl hi kg b fi km kn l ko kp"><strong class="kg hj">import</strong> <strong class="kg hj">numpy</strong> <strong class="kg hj">as</strong> <strong class="kg hj">np</strong>  <br/>    <em class="ls">#initialize</em><br/>    policy = np.ones([70, 4]) / 4<br/>    V = np.zeros(70)<br/>    Q = np.zeros([70,4])<br/>        <br/>    <em class="ls">#step1 : policy evaluation(complete)</em><br/>    overall = 0 <br/>    <strong class="kg hj">while</strong> True :<br/>        cnt = 0<br/>        <strong class="kg hj">while</strong> True:<br/>            gamma = 1<br/>            cnt = cnt + 1<br/>            <em class="ls">#theta = 1e-8</em><br/>            theta = 0.01<br/>            delta = 0<br/>            oldpolicy = policy<br/>            Vtmp = np.zeros(70)<br/>            <strong class="kg hj">for</strong> s <strong class="kg hj">in</strong> range(70):<br/>                <em class="ls">## store old value of V[s] to calculate delta</em><br/>                v = V[s]<br/>                v_new = 0<br/>                <strong class="kg hj">for</strong> a <strong class="kg hj">in</strong> range(4):<br/>                    env.S = (s/10,s%<strong class="kg hj">10</strong>)<br/>                    <em class="ls">## Iterate over possible results of taking action a</em><br/>                    ns,r,done, info = env.step(a)<br/>                    <strong class="kg hj">if</strong> ns == (3,7):<br/>                        r = 0<br/>                    v_new += policy[s][a]*(r+gamma*V[10 * ns[0] + ns[1]])<br/>                delta = max(delta, abs(v-v_new))<br/>                Vtmp[s] = v_new<br/>            <em class="ls">#print delta</em><br/>            V =  Vtmp<br/>            <strong class="kg hj">if</strong> delta &lt; theta <strong class="kg hj">or</strong> cnt == 1:<br/>                <strong class="kg hj">break</strong><br/>        <br/>        <em class="ls">#step2 :Derive Q from V</em><br/>        <strong class="kg hj">for</strong> s <strong class="kg hj">in</strong> range(70):<br/>            <strong class="kg hj">for</strong> a <strong class="kg hj">in</strong> range(4):<br/>                q_a = 0<br/>                env.S = (s/10,s%<strong class="kg hj">10</strong>)<br/>                ns,r,done, info = env.step(a)<br/>                <strong class="kg hj">if</strong> ns == (3,7):<br/>                    r = 0<br/>                q_a += (r + gamma*V[10 * ns[0] + ns[1]])<br/>                Q[s][a] = q_a  <br/>                <br/>        <em class="ls">#step 3: policy improvement</em><br/>        policy = np.zeros([70, 4]) / 4   <br/>        <strong class="kg hj">for</strong> s <strong class="kg hj">in</strong> range(70):<br/>            Q_s = Q[s]<br/>            argmax = np.argwhere(Q_s == np.amax(Q_s)).flatten().tolist()<br/>            <em class="ls">## Create Stochastic policy if multiple actions yield best results</em><br/>            prob = 1.0/len(argmax)<br/>            <strong class="kg hj">for</strong> index <strong class="kg hj">in</strong> argmax:<br/>                policy[s][index] = prob<br/>        <br/>        <em class="ls">#step 4: check if policy converged</em><br/>        <strong class="kg hj">if</strong> np.array_equal(oldpolicy,policy):<br/>            <strong class="kg hj">print</strong> "converged after ",overall," iterations"<br/>            <strong class="kg hj">break</strong><br/>        <strong class="kg hj">else</strong>:<br/>            None <em class="ls">#print "iterate"</em><br/>            overall = overall + 1<br/>        oldpolicy = policy</span><span id="3b24" class="kk kl hi kg b fi lt kn l ko kp">converged after  127  iterations</span></pre><p id="63c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上述代码完成了以下步骤(迭代逼近最优)<br/> <strong class="ih hj">步骤-0 </strong>:策略的初始化为等概率(即在每一个州，政策都提到采取任何有相同问题的行动。)，value-function(V)设置为0。动作值函数(Q)设置为0。<br/> <strong class="ih hj">第一步</strong>:策略评估(PE)试图在策略初始化的假设下得出价值函数的估计值。Vπ(s)=∑aπ(s，a)∑s′Pass′[Rass′+γVπ(s′)]<br/><strong class="ih hj">step-2</strong>:从V-function得到Q-function。qπ(s,a)=eπ{rt+γvπ(st+1)∣st=s,at=a}=∑s′pass′[rass′+γvπ(s′)]<br/><strong class="ih hj">步骤3 </strong>:策略改进(PI)基于作为步骤1和步骤2的一部分而估计的价值函数(v和q)贪婪地更新策略。π′(s)= argmaxaQ(s，a) <br/> <strong class="ih hj">第4步</strong>:检查收敛—策略(prev iter) ==策略(current)？如果是，则停止循环并收工！融合了。如果没有，返回步骤，重复直到收敛。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es lu"><img src="../Images/54aec92d2ad0cc908d5a94d391e822a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*LM4C2u_HyYUR2OSbeJuLGw.png"/></div></figure><p id="f69c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面的代码通过显示从开始到结束要遵循的最佳路径(以文本和视觉方式)来可视化最佳策略。还显示了发生的总成本。注意:箭头可能看起来很奇怪，但请注意，有风向上作用的影响，这使得代理一次移动多个单元。</p><pre class="jf jg jh ji fd kf kg kh ki aw kj bi"><span id="c7ab" class="kk kl hi kg b fi km kn l ko kp">visualize_path(Q,(5,5))</span><span id="2dd7" class="kk kl hi kg b fi lt kn l ko kp">output:<br/>(5, 5) =&gt; (4, 6) =&gt; (2, 7) =&gt; (0, 8) =&gt; (0, 9) =&gt; (1, 9) =&gt; (2, 9) =&gt; (3, 9) =&gt; (4, 9) =&gt; (4, 8) =&gt; (3, 7)<br/>-10 10<br/>⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹  ⮞ ⮟ <br/>⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⮟ <br/>⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⮞  ⏹ ⮟ <br/>⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ✌  ⏹ ⮟ <br/>⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⮞  ⏹ ⮜  ⮜ <br/>⏹ ⏹ ⏹ ⏹ ⏹ ⮞  ⏹ ⏹ ⏹ ⏹ <br/>⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹ ⏹</span></pre></div></div>    
</body>
</html>