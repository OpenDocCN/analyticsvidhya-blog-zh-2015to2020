<html>
<head>
<title>Feature Selection: Wrapper Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择:包装方法</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/feature-selection-85539d6a2a88?source=collection_archive---------4-----------------------#2020-12-03">https://medium.com/analytics-vidhya/feature-selection-85539d6a2a88?source=collection_archive---------4-----------------------#2020-12-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><h2 id="fed6" class="hg hh hi bd b fp hj hk hl hm hn ho dx hp translated" aria-label="kicker paragraph">权威指南</h2><div class=""/><div class=""><h2 id="77da" class="pw-subtitle-paragraph io hr hi bd b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf dx translated">5基于包装器的方法来选择相关特性</h2></div><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jg"><img src="../Images/93759cd7d48e2c97789676cde8bdd10d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_Wf2ph7YrJ_gaY1M"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">马里乌斯·马萨拉尔在<a class="ae jw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><h2 id="ada6" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">目录</h2><ul class=""><li id="6555" class="ku kv hi kw b kx ky kz la ki lb km lc kq ld le lf lg lh li bi translated"><a class="ae jw" href="#8fe7" rel="noopener ugc nofollow">包装方法</a></li><li id="e2a9" class="ku kv hi kw b kx lj kz lk ki ll km lm kq ln le lf lg lh li bi translated"><a class="ae jw" href="#ad20" rel="noopener ugc nofollow">正向选择</a></li><li id="4c44" class="ku kv hi kw b kx lj kz lk ki ll km lm kq ln le lf lg lh li bi translated"><a class="ae jw" href="#e920" rel="noopener ugc nofollow">落后淘汰</a></li><li id="e072" class="ku kv hi kw b kx lj kz lk ki ll km lm kq ln le lf lg lh li bi translated"><a class="ae jw" href="#dedb" rel="noopener ugc nofollow">博鲁塔</a></li><li id="7be8" class="ku kv hi kw b kx lj kz lk ki ll km lm kq ln le lf lg lh li bi translated"><a class="ae jw" href="#e676" rel="noopener ugc nofollow">遗传算法</a></li></ul><p id="5774" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated"><em class="md">这篇文章是关于特性选择的博客系列的第二部分。看看过滤(</em><a class="ae jw" href="https://tzinie.medium.com/feature-selection-73bc12a9b39e" rel="noopener"><em class="md">part 1</em></a><em class="md">)和嵌入(</em><a class="ae jw" href="https://tzinie.medium.com/feature-selection-embedded-methods-a7940036973f" rel="noopener"><em class="md">part 3</em></a><em class="md">)的方法。</em></p></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><h1 id="8fe7" class="ml jy hi bd jz mm mn mo kd mp mq mr kh ix ms iy kl ja mt jb kp jd mu je kt mv bi translated">包装方法</h1><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es mw"><img src="../Images/f9cdb7935c5c8dd8a0d712cfc544d617.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*oCQyMjYWhXI3eZ43-GS2YQ.gif"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">流程图</figcaption></figure><p id="025e" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">在第1部分中，我们讨论了过滤方法，它帮助你选择与目标变量相关的特征。然而，我觉得我们遗漏了一些东西…哦，对了，实际的机器学习算法！</p><p id="f6da" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">这实际上是Filter和Wrapper方法之间的核心区别之一。后者通过实际训练一个模型来测量特征子集的重要性。</p><p id="635b" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">整个想法建立在<em class="md">贪婪搜索</em>和<em class="md">评估标准</em>之上。<em class="md">贪婪</em>因为该方法在每次迭代中选择局部最优的特征子集。然后，<em class="md">评价标准</em>起到评判的作用。它实际上是我们在训练模型时使用的性能指标。我们都知道这完全取决于任务。对于<em class="md">回归</em>，可以是p值或R平方，而对于<em class="md">分类</em>，可以是精度或f1分。最后，根据<em class="md">评估标准，</em>将选择给出最佳结果的特征组合。</p><h2 id="881e" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">选择最佳模型</h2><p id="7f7e" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mx lv lw km my ly lz kq mz mb mc le hb bi translated">我们需要一种方法来找到给定评估标准的最佳模型。为此，我们需要将注意力转向测试误差。众所周知，训练误差可能是对测试误差的不良估计，因此，我们关注<strong class="kw hs">交叉验证</strong>(CV)；一种估计测试误差的有效方法。</p></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><p id="260f" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">对于每种方法，我们使用<a class="ae jw" href="https://www.kaggle.com/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank"> <strong class="kw hs">信用卡欺诈检测数据集</strong> </a> <strong class="kw hs"> : </strong>该数据集包含信用卡进行的交易；它们被贴上欺诈或真实的标签。数据集包括30个特征，它们是主成分分析的结果。由于保密问题，不提供原始功能。这是一个数据集非常不平衡的分类任务，因此，我将使用<code class="du na nb nc nd b">f1-score (macro avg)</code>作为评估指标。</p><h1 id="ad20" class="ml jy hi bd jz mm ne mo kd mp nf mr kh ix ng iy kl ja nh jb kp jd ni je kt mv bi translated"><strong class="ak">正向选择</strong></h1><p id="a04e" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mx lv lw km my ly lz kq mz mb mc le hb bi translated">我们有一个<em class="md">零模型</em>(一个没有预测器的模型)作为起点。第一轮我们用k倍CV 30型号训练(为什么是30？因为我们有30个特征/变量)。然后，我们将变量添加到给出最低测试误差的空模型中。在第二轮中，我们重复同样的想法拟合29个模型，并将第二个变量添加到单变量模型中，从而得到一个双变量模型。这种方法一直持续到拟合没有统计上的显著改善为止(<em class="md">停止标准</em>)。</p><pre class="jh ji jj jk fd nj nd nk nl aw nm bi"><span id="c4a3" class="jx jy hi nd b fi nn no l np nq">import pandas as pd<br/>from mlxtend.feature_selection import SequentialFeatureSelector<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import StratifiedKFold, train_test_split</span><span id="8958" class="jx jy hi nd b fi nr no l np nq">credit = pd.read_csv('creditcard.csv')<br/>X = credit.loc[:, credit.columns != 'Class']<br/>y = credit['Class']</span><span id="6bc1" class="jx jy hi nd b fi nr no l np nq">X_train, X_test, y_train, y_test = train_test_split(X, <br/>                                                    y, <br/>                                                    test_size=0.2,<br/>                                                    stratify=y,<br/>                                                    random_state=42)</span><span id="70a7" class="jx jy hi nd b fi nr no l np nq">scv = StratifiedKFold(n_splits=5)</span><span id="d95f" class="jx jy hi nd b fi nr no l np nq"># create the SequentialFeatureSelector object<br/>sfs = SequentialFeatureSelector(LogisticRegression(C=10,<br/>max_iter=10000), k_features=8, <br/>                 forward=True, <br/>                 floating=False,<br/>                 verbose=2,               <br/>                 scoring='f1_macro',<br/>                 cv=scv,<br/>                 n_jobs=-1)</span><span id="0c9e" class="jx jy hi nd b fi nr no l np nq"># fit the object to the training data<br/>sfs = sfs.fit(X, y)</span><span id="0bcf" class="jx jy hi nd b fi nr no l np nq">selected_features = wine.columns[list(sfs.k_feature_idx_)]<br/>print(selected_features)</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ns"><img src="../Images/c88e747488e86ce9f3c39ed4ab6c4655.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x-y3euhtc6dbKsEC_9wYiA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">选定的功能</figcaption></figure><p id="3e38" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">我们现在使用这些特征来训练模型:</p><pre class="jh ji jj jk fd nj nd nk nl aw nm bi"><span id="95db" class="jx jy hi nd b fi nn no l np nq"># Build full model with selected features<br/>clf = LogisticRegression(C=10, max_iter=10000)<br/>clf.fit(X_train[:, feat_cols], y_train)</span><span id="1621" class="jx jy hi nd b fi nr no l np nq">y_test_pred = clf.predict(X_test[:, feat_cols])<br/>print(classification_report(y_test, preds))</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nt"><img src="../Images/91d90a8dcb46ef003e8d23cc156d80d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aiDXu-x68Vs-YA8ClB2k-w.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">分类报告；空军上士</figcaption></figure><h1 id="e920" class="ml jy hi bd jz mm ne mo kd mp nf mr kh ix ng iy kl ja nh jb kp jd ni je kt mv bi translated">反向消除</h1><p id="f397" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mx lv lw km my ly lz kq mz mb mc le hb bi translated">也被称为<em class="md">递归特征消除</em>。这种方法与前一种方法相反。我们从所有变量入手，一次去掉一个变量，这是最不重要的。代码完全相同，但有一点不同；我们需要在<code class="du na nb nc nd b">SequentialFeatureSelector</code>中将参数<code class="du na nb nc nd b">forward</code>改为False。</p><p id="09ac" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">当我们在这个子集上训练时，这些是最重要的特征和分类报告:</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nu"><img src="../Images/4b717ca855fcf41e418a840bfd56f1a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6OdTg-ClFvijNaNMiBhdDw.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">选定的功能</figcaption></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nv"><img src="../Images/e567cd2fccf9ce3da3858e22ee197e47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NNzGxTM4_2aJ-SDKnK_Jvw.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">分类报告；存在</figcaption></figure><h2 id="3a62" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">向前与向后</h2><p id="00c9" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mx lv lw km my ly lz kq mz mb mc le hb bi translated">如果特征数量&gt;样本数量，则不能使用向后消除，而总是可以使用向前选择。主要原因是因为在这种情况下，可约误差和不可约误差的大小变为零。然而，这是不可能的，因为这意味着你对数据有一个完美的拟合。所以，实际上发生的是你过度适应了。这不是向前选择的问题，因为您开始时没有特征，然后一次连续添加一个特征。</p><p id="9d88" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">另一方面，前向选择是一种贪婪的方法，可能会在早期包含后来变得多余的变量。</p></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><h1 id="dedb" class="ml jy hi bd jz mm mn mo kd mp mq mr kh ix ms iy kl ja mt jb kp jd mu je kt mv bi translated">博鲁塔</h1><p id="8fc8" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mx lv lw km my ly lz kq mz mb mc le hb bi translated"><a class="ae jw" href="https://github.com/scikit-learn-contrib/boruta_py" rel="noopener ugc nofollow" target="_blank">算法</a>非常有趣，如果你问我，我认为它非常聪明。到目前为止，包装器方法的整体思想是特性在最终子集中相互竞争位置。相反，在博鲁塔<strong class="kw hs">中，这些特征与随机版本的自身</strong> <em class="md">(阴影特征)</em>竞争。<strong class="kw hs">仅当一个特性的表现优于最佳表现的随机特性</strong>时，才选择该特性。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nw"><img src="../Images/d817f14e5526c47c3c1facf41c4415b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JQMBs7RZvYVwMwmNVGIRRg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><a class="ae jw" href="https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a" rel="noopener" target="_blank">来源</a></figcaption></figure><p id="f329" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">该过程重复N次(试验次数),最后一步是使用二项式分布比较特征优于其阴影特征的次数。对于蓝色区域的特征，Boruta算法是不确定的，而绿色和红色区域的特征应该分别被选择和消除。你可以在这里阅读更多信息<a class="ae jw" href="https://www.datacamp.com/community/tutorials/feature-selection-R-boruta" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="jh ji jj jk fd nj nd nk nl aw nm bi"><span id="2e93" class="jx jy hi nd b fi nn no l np nq">import pandas as pd<br/>from boruta import BorutaPy<br/>from mlxtend.feature_selection import SequentialFeatureSelector<br/>from sklearn.ensemble import RandomForestClassifier<br/>from sklearn.model_selection import StratifiedKFold, train_test_split</span><span id="9f1a" class="jx jy hi nd b fi nr no l np nq">credit = pd.read_csv('creditcard.csv')<br/>X = credit.loc[:, credit.columns != 'Class']<br/>y = credit['Class']</span><span id="cdae" class="jx jy hi nd b fi nr no l np nq">X_train, X_test, y_train, y_test = train_test_split(X, <br/>                                                    y, <br/>                                                    test_size=0.2,<br/>                                                    stratify=y,<br/>                                                    random_state=42)</span><span id="6fc1" class="jx jy hi nd b fi nr no l np nq">clf = RandomForestClassifier(n_estimators=200, class_weight='balanced', max_depth=4, n_jobs=-1)</span><span id="913c" class="jx jy hi nd b fi nr no l np nq">boruta = BorutaPy(estimator = clf, n_estimators = 'auto', max_iter = 100) # number of trials to perform</span><span id="5dcd" class="jx jy hi nd b fi nr no l np nq">boruta.fit(X_train.to_numpy(), y_train)</span><span id="6108" class="jx jy hi nd b fi nr no l np nq">green_area = X.columns[boruta.support_].to_list()<br/>blue_area = X.columns[boruta.support_weak_].to_list()<br/>print('Green area:', green_area)<br/>print('Blue area:', blue_area)</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es nx"><img src="../Images/3641c2717175dbd5cb4ef7469aa1bd37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SLJzsWjFPef1YncyDsX2IA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">选定的功能</figcaption></figure><pre class="jh ji jj jk fd nj nd nk nl aw nm bi"><span id="bbd9" class="jx jy hi nd b fi nn no l np nq">clf = RandomForestClassifier(n_estimators=200, class_weight='balanced', max_depth=4, n_jobs=-1)<br/>clf.fit(X_train[green_area].to_numpy(), y_train)<br/>preds = clf.predict(X_test[green_area].to_numpy())<br/>print(classification_report(y_test, preds))</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ny"><img src="../Images/ae0d9fdff60349def68f02fb4fd41a2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d86BXoTfWaqHb5NOtihDqg.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">分类报告；博鲁塔</figcaption></figure><blockquote class="nz oa ob"><p id="6808" class="lo lp md kw b kx lq is lr kz ls iv lt oc lu lv lw od lx ly lz oe ma mb mc le hb bi translated">注意:Boruta需要一个具有属性<code class="du na nb nc nd b">feature_importances_</code>的分类器或回归器</p></blockquote></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><h1 id="e676" class="ml jy hi bd jz mm mn mo kd mp mq mr kh ix ms iy kl ja mt jb kp jd mu je kt mv bi translated">遗传算法</h1><p id="ccff" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mx lv lw km my ly lz kq mz mb mc le hb bi translated">更高级的方法，但在我看来非常有趣。进化算法这个术语听起来耳熟吗？如果是，那么你已经知道这是怎么回事了，如果不是读<a class="ae jw" href="https://towardsdatascience.com/introduction-to-evolutionary-algorithms-a8594b484ac" rel="noopener" target="_blank">这个</a>。长话短说，科学家从遗传学和自然选择中获得灵感，提出了进化算法，从种群中确定最适合的个体。有5个阶段:</p><h2 id="0974" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">初始化</h2><p id="691e" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mx lv lw km my ly lz kq mz mb mc le hb bi translated">以随机方式创建一个解决方案群体——任意数量的不同特征子集，即n个，但它看起来像什么呢？它只是一个dim =特征数量的二元向量。1表示特性存在，0表示不存在。例如，向量<code class="du na nb nc nd b">[1,0,0,1]</code>指示我们总共有4个特征，并且该群体成员是特征0和3的子集。</p><h2 id="a60b" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">健身任务</h2><p id="62b1" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mx lv lw km my ly lz kq mz mb mc le hb bi translated">现在是时候评估我们人口中的每个成员了。为此，我们训练N个模型，每个模型都有一个来自总体的成员，并计算误差。如果误差大，则适合度低。对单个N模型的所有误差进行排序，但是<strong class="kw hs">适应值完全取决于个体在等级中的位置，而不是误差</strong>。<br/>适应值:<code class="du na nb nc nd b">Φ(i) = k•R(i)</code>，k是属于[1，2]中的常数，R(i)是个体I的秩，误差越小，秩越高，所以适应值越大。</p><h2 id="9a56" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">选拔过程</h2><p id="8169" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mx lv lw km my ly lz kq mz mb mc le hb bi translated">我们如何选择？<em class="md">轮盘赌</em>或最不好玩的名字随机抽样与替换。这种方法将群体中的所有成员放在轮盘赌上，轮盘赌的面积与他们的适应度成比例。然后我们通过“转动轮盘”随机选择一半的人口。</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es of"><img src="../Images/4bd1dd2a16144991b587bb0379916870.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/0*LqGr7JCOLLj4i6XC.gif"/></div><figcaption class="js jt et er es ju jv bd b be z dx translated"><a class="ae jw" href="https://towardsdatascience.com/introduction-to-evolutionary-algorithms-a8594b484ac" rel="noopener" target="_blank">选举过程</a></figcaption></figure><h2 id="a7b9" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">交叉操作</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es og"><img src="../Images/9c0276674575b58ab5caddb82ce67295.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/0*B9fl84h5O3pODiI2.png"/></div></figure><p id="9e69" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">在这一步中，我们随机选择2个成员，结合他们的特征，为新的种群获得4个后代。然后，我们与其他2名成员重复，以此类推。我们什么时候停止？当我们产生和第一步一样多的人口时。</p><h2 id="2704" class="jx jy hi bd jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ho bi translated">变异操作</h2><figure class="jh ji jj jk fd jl er es paragraph-image"><div class="er es oh"><img src="../Images/dfb56a6203b471f82b1abe1edc42d901.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/0*-6KtXgZNagmbvGGm.png"/></div></figure><p id="5918" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">这一步是因为后者而存在的。在杂交过程中，很有可能产生与其祖先非常相似的后代，这<strong class="kw hs">导致了低多样性</strong>。因此，我们通过随机改变后代某些特征的值来变异后代。</p><p id="ad01" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">你不断重复，直到下一代与上一代没有明显的不同。然后选择产生最低错误的成员。</p><pre class="jh ji jj jk fd nj nd nk nl aw nm bi"><span id="4c52" class="jx jy hi nd b fi nn no l np nq">import pandas as pd<br/>from genetic_selection import GeneticSelectionCV<br/>from sklearn.linear_model import LogisticRegression<br/>from sklearn.model_selection import StratifiedKFold, train_test_split</span><span id="aade" class="jx jy hi nd b fi nr no l np nq">credit = pd.read_csv('creditcard.csv')<br/>X = credit.loc[:, credit.columns != 'Class']<br/>y = credit['Class']</span><span id="6fc9" class="jx jy hi nd b fi nr no l np nq">X_train, X_test, y_train, y_test = train_test_split(X, <br/>                                                    y, <br/>                                                    test_size=0.2,<br/>                                                    stratify=y,<br/>                                                    random_state=42)</span><span id="31a7" class="jx jy hi nd b fi nr no l np nq">scv = StratifiedKFold(n_splits=5)</span><span id="caac" class="jx jy hi nd b fi nr no l np nq">selector = GeneticSelectionCV(LogisticRegression(C=10, max_iter=10000, random_state=42),<br/>                              cv=scv,<br/>                              verbose=2,<br/>                              scoring="f1_macro",<br/>                              max_features=5,<br/>                              n_population=50,<br/>                              crossover_proba=0.5,<br/>                              mutation_proba=0.2,<br/>                              n_generations=40,<br/>                              crossover_independent_proba=0.5,<br/>                              mutation_independent_proba=0.05,<br/>                              tournament_size=3,<br/>                              n_gen_no_change=10,<br/>                              caching=True,<br/>                              n_jobs=-1)</span><span id="5c4f" class="jx jy hi nd b fi nr no l np nq">selector = selector.fit(X_train, y_train)</span><span id="1f1a" class="jx jy hi nd b fi nr no l np nq"># get the selected features<br/>cols = X.columns.tolist()<br/>selected_feats = [cols[i] for i in np.where(selector.support_)[0]]</span><span id="014b" class="jx jy hi nd b fi nr no l np nq"># train and test<br/>clf = LogisticRegression(C=10, max_iter=10000, random_state=42)<br/>clf.fit(X_train[selected_feats], y_train)</span><span id="91c1" class="jx jy hi nd b fi nr no l np nq">preds = clf.predict(X_test[selected_feats])<br/>print(classification_report(y_test, preds))</span></pre><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es oi"><img src="../Images/3225003f3ec7d3a306c987d6f2adb650.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I6Foh7ajXvb7BA7Etx5-eQ.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">选定的功能</figcaption></figure><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es oj"><img src="../Images/9d7bbc7307a1f3b307c840e0235de351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zYheEYATGy6CUqjRVY_CyA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">分类报告；通用航空</figcaption></figure></div><div class="ab cl me mf gp mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="hb hc hd he hf"><p id="b1ae" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">如果我们什么都不做，会发生什么？如果我们不删除任何功能…</p><figure class="jh ji jj jk fd jl er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ok"><img src="../Images/59d3ac2b991e3bc35c869aa50980dacf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qeioFWMFqAcYq0eZmDbKEA.png"/></div></div><figcaption class="js jt et er es ju jv bd b be z dx translated">分类报告；无特征选择</figcaption></figure><p id="15ae" class="pw-post-body-paragraph lo lp hi kw b kx lq is lr kz ls iv lt ki lu lv lw km lx ly lz kq ma mb mc le hb bi translated">显然，通过使用上述任何一种方法，我们在f1得分(宏观平均值)上获得了7–14%的收益。</p><h1 id="02c6" class="ml jy hi bd jz mm ne mo kd mp nf mr kh ix ng iy kl ja nh jb kp jd ni je kt mv bi translated">结论</h1><p id="b7d9" class="pw-post-body-paragraph lo lp hi kw b kx ky is lr kz la iv lt ki mx lv lw km my ly lz kq mz mb mc le hb bi translated">包装器方法基于其有用性来测量特征的重要性，同时在其上训练机器学习模型。另一方面，<a class="ae jw" rel="noopener" href="/analytics-vidhya/feature-selection-73bc12a9b39e">过滤方法</a>根据与目标变量的关系选择特征。不幸的是，没有现成的方法可循。您可能希望使用筛选方法，因为它们的计算成本较低，或者是筛选和包装方法的组合；又称<em class="md">混合特征选择</em>。首先，使用过滤方法消除冗余特征，然后使用包装方法选择有用的子集。然而，这一切都归结于数据；这将决定你的方法。</p></div></div>    
</body>
</html>