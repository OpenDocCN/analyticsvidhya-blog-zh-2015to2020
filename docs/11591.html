<html>
<head>
<title>Post-Pruning and Pre-Pruning in Decision Tree</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树中的后剪枝和前剪枝</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/post-pruning-and-pre-pruning-in-decision-tree-561f3df73e65?source=collection_archive---------0-----------------------#2020-12-10">https://medium.com/analytics-vidhya/post-pruning-and-pre-pruning-in-decision-tree-561f3df73e65?source=collection_archive---------0-----------------------#2020-12-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/3e86ea3f155a9d548e95a327620128ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XYoem1pvW9IgQYCr.jpg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://www.google.com/search?q=pruning&amp;sxsrf=ALeKk01gkiJdwLm7HOctIpaDXbfCX3lHCg:1607592000248&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwit_PjwisPtAhVIbSsKHdb2AusQ_AUoAXoECBwQAw&amp;biw=1366&amp;bih=625#imgrc=O4YcCZE4HmKp1M" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><blockquote class="iv iw ix"><p id="24dd" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jb hj">什么是修剪？</strong></p></blockquote><p id="fbdf" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">一般来说，修剪是去除植物的选定部分如芽、枝和根的过程。在决策树剪枝做同样的任务，它删除决策树的分支，以克服决策树的过度拟合条件。这可以通过两种方式实现，我们将详细讨论这两种技术。让我们开始吧……</p><h1 id="1eb3" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">1.后期修剪:</h1><ul class=""><li id="c31d" class="ky kz hi jb b jc la jg lb jx lc jy ld jz le jw lf lg lh li bi translated">这种技术在构造决策树之后使用。</li><li id="7929" class="ky kz hi jb b jc lj jg lk jx ll jy lm jz ln jw lf lg lh li bi translated">当决策树具有非常大的深度并且将显示模型的过度拟合时，使用这种技术。</li><li id="b1fe" class="ky kz hi jb b jc lj jg lk jx ll jy lm jz ln jw lf lg lh li bi translated">这也称为反向修剪。</li><li id="1886" class="ky kz hi jb b jc lj jg lk jx ll jy lm jz ln jw lf lg lh li bi translated">当我们拥有无限增长的决策树时，就会用到这种技术。</li><li id="751e" class="ky kz hi jb b jc lj jg lk jx ll jy lm jz ln jw lf lg lh li bi translated">这里我们将使用<code class="du lo lp lq lr b"><strong class="jb hj">cost_complexity_pruning</strong></code>来控制决策树的分支<code class="du lo lp lq lr b">max_depth</code>和<code class="du lo lp lq lr b">min_samples_split </code></li></ul><p id="042c" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">我已经使用了sklearn库中已经可用的<strong class="jb hj">乳腺癌</strong>数据集。</p><p id="0aa1" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">一、导入库</strong></p><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="10fb" class="ma kb hi lr b fi mb mc l md me">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>from sklearn import tree<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.tree import DecisionTreeClassifier</span></pre><p id="02eb" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">二。没有后期修剪的建模</strong></p><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="4eb2" class="ma kb hi lr b fi mb mc l md me">X,y=load_breast_cancer(return_X_y=True)<br/>X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)<br/>clf=DecisionTreeClassifier(random_state=0)<br/>clf.fit(X_train,y_train)<br/>y_train_predicted=clf.predict(X_train)<br/>y_test_predicted=clf.predict(X_test)<br/>accuracy_score(y_train,y_train_predicted)<br/>accuracy_score(y_test,y_test_predicted)</span><span id="0ffa" class="ma kb hi lr b fi mf mc l md me">[out] &gt;&gt; <strong class="lr hj">1.0  #Accuracy score of training dataset</strong><br/>         <strong class="lr hj">0.8811188811188811 #accuracy score of test dataset</strong></span></pre><p id="e8a0" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">正如我们所看到的，训练和测试的准确度分数之间的差异太高意味着模型过度拟合(因为它对于训练集是准确的，但是当我们向模型提供测试集时会产生较大的误差)</p><p id="b594" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">可视化决策树</strong></p><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="109c" class="ma kb hi lr b fi mb mc l md me">plt.figure(figsize=(16,8))<br/>tree.plot_tree(clf)<br/>plt.show()</span></pre><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mg"><img src="../Images/ec53f5fed4b6e5838b3f2560534c67fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TgbeohXPvPS9SRGY70asQQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">决策树可视化</figcaption></figure><p id="3d16" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">三。后期修剪操作:</strong></p><p id="9e19" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">这里我们使用<code class="du lo lp lq lr b">cost_complexity_pruning</code>技术来修剪决策树的分支。</p><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="888f" class="ma kb hi lr b fi mb mc l md me">path=clf.cost_complexity_pruning_path(X_train,y_train)<br/>#path variable gives two things ccp_alphas and impurities<br/>ccp_alphas,impurities=path.ccp_alphas,path.impurities<br/>print("ccp alpha wil give list of values :",ccp_alphas)<br/>print("***********************************************************")<br/>print("Impurities in Decision Tree :",impurities)<br/>--------------------------------------------------------------------</span><span id="f112" class="ma kb hi lr b fi mf mc l md me"><strong class="lr hj">[out]&gt;&gt;</strong> ccp alpha wil give list of values : [0.    0.00226647 0.00464743 0.0046598  0.0056338  0.00704225 0.00784194 0.00911402 0.01144366 0.018988   0.02314163 0.03422475 0.32729844]<br/>********************************************************************<br/>Impurities in Decision Tree : [0.    0.00453294 0.01847522 0.02313502 0.02876883 0.03581108 0.04365302 0.05276704 0.0642107  0.0831987  0.10634033 0.14056508  0.46786352]</span></pre><p id="065f" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><code class="du lo lp lq lr b"><strong class="jb hj">ccp_alphas</strong></code> <strong class="jb hj"> </strong>给出决策树的最小叶子值，每个<strong class="jb hj"> </strong> <code class="du lo lp lq lr b"><strong class="jb hj">ccp_aphas</strong></code> <strong class="jb hj"> </strong>会创建不同的分类器并从中选择最佳。<code class="du lo lp lq lr b">ccp_alphas</code>将作为参数添加到<code class="du lo lp lq lr b">DecisionTreeClassifier()</code>中。</p><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="63c0" class="ma kb hi lr b fi mb mc l md me">clfs=[]   #will store all the models here<br/>for ccp_alpha in ccp_alphas:<br/>    clf=DecisionTreeClassifier(random_state=0,ccp_alpha=ccp_alpha)<br/>    clf.fit(X_train,y_train)<br/>    clfs.append(clf)<br/>print("Last node in Decision tree is {} and ccp_alpha for last node is {}".format(clfs[-1].tree_.node_count,ccp_alphas[-1]))</span><span id="e7c8" class="ma kb hi lr b fi mf mc l md me">[out] &gt;&gt; <strong class="lr hj">Last node in Decision tree is 1 and ccp_alpha for last node<br/>         is 0.3272984419327777</strong></span></pre><p id="bf90" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">可视化训练和测试集的准确度分数。</p><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="d8ca" class="ma kb hi lr b fi mb mc l md me">train_scores = [clf.score(X_train, y_train) for clf in clfs]<br/>test_scores = [clf.score(X_test, y_test) for clf in clfs]<br/>fig, ax = plt.subplots()<br/>ax.set_xlabel("alpha")<br/>ax.set_ylabel("accuracy")<br/>ax.set_title("Accuracy vs alpha for training and testing sets")<br/>ax.plot(ccp_alphas, train_scores, marker='o', label="train",drawstyle="steps-post")<br/>ax.plot(ccp_alphas, test_scores, marker='o', label="test",drawstyle="steps-post")<br/>ax.legend()<br/>plt.show()</span></pre><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es mh"><img src="../Images/b413c01444bc559ea1981150992a79f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*witOAuIvfH9fvJBVsq-05A.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">训练和测试集的准确性</figcaption></figure><p id="2f6c" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">如果我们遵循偏差和方差权衡，我们将选择具有低偏差(低训练误差)和低方差(低测试误差)点。这里我们得到的点的值是α= 0.02。</p><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="74ab" class="ma kb hi lr b fi mb mc l md me">clf=DecisionTreeClassifier(random_state=0,ccp_alpha=0.02)<br/>clf.fit(X_train,y_train)<br/>plt.figure(figsize=(12,8))<br/>tree.plot_tree(clf,rounded=True,filled=True)<br/>plt.show()</span></pre><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/c93802de2c79cd147794d004d57b250b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*zgY_iL0YDZ5Ezldw3EKgFQ.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">剪枝后可视化</figcaption></figure><p id="1fcc" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">这里我们能够修剪无限生长的树。让我们再次检查准确性分数。</p><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="4872" class="ma kb hi lr b fi mb mc l md me">accuracy_score(y_test,clf.predict(X_test))</span><span id="f6e3" class="ma kb hi lr b fi mf mc l md me">[out]&gt;&gt; <strong class="lr hj">0.916083916083916</strong></span></pre><p id="1c74" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">因此，我们能够使用剪枝来提高决策树模型的准确性。</p><h1 id="2e5d" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">2.预修剪:</h1><ul class=""><li id="04cb" class="ky kz hi jb b jc la jg lb jx lc jy ld jz le jw lf lg lh li bi translated">这种技术在构造决策树之前使用。</li><li id="ff55" class="ky kz hi jb b jc lj jg lk jx ll jy lm jz ln jw lf lg lh li bi translated">可以使用<strong class="jb hj">超参数调整完成预修剪。</strong></li><li id="6cc5" class="ky kz hi jb b jc lj jg lk jx ll jy lm jz ln jw lf lg lh li bi translated">克服过度拟合问题。</li><li id="bca5" class="ky kz hi jb b jc lj jg lk jx ll jy lm jz ln jw lf lg lh li bi translated">在这篇博客中，我将使用GridSearchCV进行<strong class="jb hj">超参数</strong>调优。</li></ul><blockquote class="iv iw ix"><p id="102b" class="iy iz ja jb b jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated"><strong class="jb hj">什么是超参数调谐？</strong></p></blockquote><p id="8c0a" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">让我们举一个决策树的例子。当我们构建DT模型时，我们不知道哪个<code class="du lo lp lq lr b">criterion </code>(“基尼”或“熵”)、哪个<code class="du lo lp lq lr b">min_depth</code>、哪个<code class="du lo lp lq lr b">min_samples_split</code>等会给出更好的模型，因此为了打破这种模糊性，我们使用超参数调整，其中我们为每个参数取一系列值，无论哪个参数值最好，我们都会将该特定值输入到<code class="du lo lp lq lr b">DecisionTreeClassifier()</code>。</p><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="f0f4" class="ma kb hi lr b fi mb mc l md me">grid_param={"criterion":["gini","entropy"],<br/>             "splitter":["best","random"],<br/>             "max_depth":range(2,50,1),<br/>             "min_samples_leaf":range(1,15,1),<br/>             "min_samples_split":range(2,20,1) <br/>            }<br/>grid_search=GridSearchCV(estimator=clf,param_grid=grid_param,cv=5,n_jobs=-1)<br/>grid_search.fit(X_train,y_train)</span></pre><p id="c1e7" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">估计器- &gt;您使用的分类模型，cv=5 - &gt;我们已将数据集分成五个块，n_jobs=-1 - &gt;我们已采用默认迭代</strong></p><p id="1c5b" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">现在我们从中提取最佳参数，然后将该参数输入<code class="du lo lp lq lr b">DecisionTreeClassifier()</code>以获得最佳精确模型。</p><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="e4a5" class="ma kb hi lr b fi mb mc l md me">print(grid_search.best_params_)</span></pre><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/7d37cea4575e428273b20c6ca159b404.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*9UKxfEiiU515PlXpvb5xeA.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">输入DT的最佳参数</figcaption></figure><p id="c27a" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated"><strong class="jb hj">预修剪操作:</strong></p><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="0812" class="ma kb hi lr b fi mb mc l md me">clf=DecisionTreeClassifier(criterion= 'gini',max_depth= 17,min_samples_leaf= 3,min_samples_split= 12,splitter= 'random')<br/>clf.fit(X_train,y_train)<br/>plt.figure(figsize=(20,12))<br/>tree.plot_tree(clf,rounded=True,filled=True)<br/>plt.show()</span></pre><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mk"><img src="../Images/bac9b4f5a8e6f192eafafdeeb770be02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZMVMhJ_3NuxVakdGg3ql-A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">预修剪</figcaption></figure><p id="4c6a" class="pw-post-body-paragraph iy iz hi jb b jc jd je jf jg jh ji jj jx jl jm jn jy jp jq jr jz jt ju jv jw hb bi translated">检查剪枝后测试数据集的准确度分数。</p><pre class="ls lt lu lv fd lw lr lx ly aw lz bi"><span id="82b6" class="ma kb hi lr b fi mb mc l md me">y_predicted=clf.predict(X_test)<br/>accuracy_score(y_test,y_predicted)</span><span id="c825" class="ma kb hi lr b fi mf mc l md me">[out]&gt;&gt; <strong class="lr hj">0.9370629370629371</strong></span></pre><h2 id="86f5" class="ma kb hi bd kc ml mm mn kg mo mp mq kk jx mr ms ko jy mt mu ks jz mv mw kw mx bi translated">结论:</h2><p id="5609" class="pw-post-body-paragraph iy iz hi jb b jc la je jf jg lb ji jj jx my jm jn jy mz jq jr jz na ju jv jw hb bi translated">这些都是我的观点，如果你对这个博客有任何建议，请在下面评论。<strong class="jb hj">不断学习不断探索…… </strong></p></div></div>    
</body>
</html>