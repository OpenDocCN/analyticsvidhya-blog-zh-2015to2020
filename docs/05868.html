<html>
<head>
<title>Predicting the next Hindi words..</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测下一个印地语单词..</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/predicting-the-next-hindi-words-de58541fbbcf?source=collection_archive---------15-----------------------#2020-05-04">https://medium.com/analytics-vidhya/predicting-the-next-hindi-words-de58541fbbcf?source=collection_archive---------15-----------------------#2020-05-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="7d8e" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">从头构建一个印地语语言模型来预测接下来的几个单词。</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/f97f0e427f375f0ace08a4b7f71ecc39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bqgeTlf2fXw-m4y_kBQ53w.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">照片由<a class="ae jn" href="https://unsplash.com/@aaronburden?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Aaron Burden </a>在<a class="ae jn" href="https://unsplash.com/s/photos/write-words?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></figcaption></figure><p id="83a2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在进入预测之前，让我们了解什么是迁移学习以及它如何改变深度学习生态系统。</p><h1 id="b59a" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">那么什么是迁移学习呢？</h1><blockquote class="lc"><p id="d213" class="ld le hi bd lf lg lh li lj lk ll kj dx translated">迁移学习是一种机器学习方法，其中为一项任务开发的模型被重新用作第二项任务模型的起点。</p></blockquote><p id="7358" class="pw-post-body-paragraph jo jp hi jq b jr lm ij jt ju ln im jw jx lo jz ka kb lp kd ke kf lq kh ki kj hb bi translated">简单地说，它将来自一个领域和任务的先验知识运用到不同的领域和任务中。幸运的是，我们有各种这样的模型，它们具有关于语言及其语义的先验知识，所以我们将只使用那些(有知识的)模型，并看看它们如何执行我们手头的任务(这里检查语法)。</p><h1 id="471d" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">我们印度语言的迁移学习呢？</h1><p id="a3ea" class="pw-post-body-paragraph jo jp hi jq b jr lr ij jt ju ls im jw jx lt jz ka kb lu kd ke kf lv kh ki kj hb bi translated">印度在文化和语言方面有巨大的多样性，印度讲大约<strong class="jq hj"> 780种语言</strong>。因此，当谈到NLP时，尤其是在迁移学习中，是时候超越英语了。</p><p id="710a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">英语有各种预先训练的模型，它们表现得像奇迹一样，但当涉及到其他印度语言时，就不太一样了。</p><p id="9477" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，这里我们将为印地语从头构建一个<strong class="jq hj"> <em class="lw">语言模型</em> </strong>，并且可以进一步用于任何其他语言，使用该语言的<a class="ae jn" href="https://meta.wikimedia.org/wiki/List_of_Wikipedias" rel="noopener ugc nofollow" target="_blank">维基百科数据集</a>。这种语言模型可以进一步用于任何其他NLP任务，如分类、摘要等。</p><h1 id="be19" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">等等，我们知道什么是语言模型吗？</h1><p id="8c35" class="pw-post-body-paragraph jo jp hi jq b jr lr ij jt ju ls im jw jx lt jz ka kb lu kd ke kf lv kh ki kj hb bi translated">语言模型(LMs)估计不同短语的相对可能性。</p><p id="4631" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">简单地说，概率语言建模的目标是计算单词序列的句子的概率:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es lx"><img src="../Images/cfed853dbb22aa70e36723dd6d4953ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*moTXlyPpNnn2bVx1GaLLIQ.png"/></div></figure><p id="6d31" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">并且可以用于找到序列中下一个单词的概率:</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ly"><img src="../Images/0781b20b7de1aa47221601dbcb4260dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:386/format:webp/1*3Y4DAyFWkyJenyneGRf_TA.png"/></div></figure><p id="b7e8" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">计算其中任何一个的模型被称为<strong class="jq hj">语言模型</strong>。在这里，我们的语言模型将读取北印度语的维基百科语料库，IMDB查看数据集以理解该语言及其语义，然后根据其理解最终预测接下来的几个单词。</p><p id="bbbb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">目前语言建模的领军人物有:<strong class="jq hj"> BERT，XLNET，Open AI的GPT，FastAi的ULMFiT。</strong></p><h1 id="06fa" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">我们选择谁？</h1><p id="5db9" class="pw-post-body-paragraph jo jp hi jq b jr lr ij jt ju ls im jw jx lt jz ka kb lu kd ke kf lv kh ki kj hb bi translated">在这里，我们将使用FastAi的ULMFiT，因为从头开始训练需要的资源较少，它提供了很好的结果，并且它有一个很好的<a class="ae jn" href="https://www.fast.ai/2019/07/08/fastai-nlp/" rel="noopener ugc nofollow" target="_blank">文档</a>，它清楚地解释了如何解决我们手头的问题。</p><p id="4963" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">ULMFiT代表文本分类的通用语言模型微调，这是由<a class="ae jn" href="https://en.wikipedia.org/wiki/Jeremy_Howard_(entrepreneur)" rel="noopener ugc nofollow" target="_blank">杰瑞米·霍华德</a>和<a class="ae jn" href="http://ruder.io/" rel="noopener ugc nofollow" target="_blank">塞巴斯蒂安·鲁德</a>引入的一项技术。它与众不同的几个特点是:</p><ul class=""><li id="c3d8" class="lz ma hi jq b jr js ju jv jx mb kb mc kf md kj me mf mg mh bi translated">区别微调</li><li id="1b18" class="lz ma hi jq b jr mi ju mj jx mk kb ml kf mm kj me mf mg mh bi translated">倾斜三角形学习率</li><li id="aa07" class="lz ma hi jq b jr mi ju mj jx mk kb ml kf mm kj me mf mg mh bi translated">逐步解冻</li></ul><p id="edb2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在这里，我们不会深入研究每一个功能，并继续构建和使用我们的模型，但要获得关于ULMFiT的详细解释，您可以浏览<a class="ae jn" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank">这篇</a>文章。</p><h1 id="a727" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">让我们现在进入正题..</h1><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mn"><img src="../Images/ea0adf11f89e3857bcb61c5b18e10811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wMEn-yaFmB6sAeaRwLogFQ.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">不，不是这件事..(图片由<a class="ae jn" href="https://pixabay.com/users/igorovsyannykov-6222956/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3170020" rel="noopener ugc nofollow" target="_blank"> Igor Ovsyannykov </a>来自<a class="ae jn" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3170020" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>)</figcaption></figure><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mo"><img src="../Images/8db9529653737016b19f5fd2ced03298.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wQBfxVg1d6ANU9sxYrAIzQ.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">是的，我就是这个意思..😉(照片由<a class="ae jn" href="https://unsplash.com/@ricaros?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">丹尼尔·里卡洛斯</a>在<a class="ae jn" href="https://unsplash.com/s/photos/coding?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄)</figcaption></figure><p id="a406" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">首先，让我们安装fastai。</p><pre class="iy iz ja jb fd mp mq mr ms aw mt bi"><span id="50c5" class="mu kl hi mq b fi mv mw l mx my">from fastai import *<br/>from fastai.text import *</span></pre><p id="968d" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">下面的代码将创建一个<code class="du mz na nb mq b">hiwiki</code>文件夹，其中包含一个包含维基百科内容的<code class="du mz na nb mq b">hiwiki</code>文本文件。(对于其他语言，用维基百科的<a class="ae jn" href="https://meta.wikimedia.org/wiki/List_of_Wikipedias" rel="noopener ugc nofollow" target="_blank">列表中的适当代码替换<code class="du mz na nb mq b">hi</code>。)</a></p><pre class="iy iz ja jb fd mp mq mr ms aw mt bi"><span id="e819" class="mu kl hi mq b fi mv mw l mx my">lang = 'hi'</span><span id="bad8" class="mu kl hi mq b fi nc mw l mx my">name = f'<strong class="mq hj">{lang}</strong>wiki' </span><span id="5c4b" class="mu kl hi mq b fi nc mw l mx my">path = data_path/name path.mkdir(exist_ok=<strong class="mq hj">True</strong>, parents=<strong class="mq hj">True</strong>) </span><span id="4bf1" class="mu kl hi mq b fi nc mw l mx my">lm_fns = [f'<strong class="mq hj">{lang}</strong>_wt', f'<strong class="mq hj">{lang}</strong>_wt_vocab']</span></pre><p id="f009" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">然后我们将创建助手函数<code class="du mz na nb mq b">get_wiki</code>和<code class="du mz na nb mq b">split_wiki</code>来下载印地语维基百科数据集并对其进行预处理。</p><pre class="iy iz ja jb fd mp mq mr ms aw mt bi"><span id="b625" class="mu kl hi mq b fi mv mw l mx my">get_wiki(path,lang)</span></pre><p id="c7f1" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，让我们使用下载的维基数据创建一个预训练模型:</p><pre class="iy iz ja jb fd mp mq mr ms aw mt bi"><span id="cb49" class="mu kl hi mq b fi mv mw l mx my">data = (TextList.from_folder(dest)<br/>            .split_by_rand_pct(0.1, seed=42)<br/>            .label_for_lm()           <br/>            .databunch(bs=bs, num_workers=1))<br/><br/>data.save(f'<strong class="mq hj">{lang}</strong>_databunch')</span><span id="992a" class="mu kl hi mq b fi nc mw l mx my">len(data.vocab.itos),len(data.train_ds)</span><span id="301c" class="mu kl hi mq b fi nc mw l mx my">data = load_data(path, f'<strong class="mq hj">{lang}</strong>_databunch', bs=bs)</span></pre><p id="76c2" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">接下来，我们将创建我们的语言模型:</p><pre class="iy iz ja jb fd mp mq mr ms aw mt bi"><span id="ef7c" class="mu kl hi mq b fi mv mw l mx my">learn = language_model_learner(data, AWD_LSTM, drop_mult=0.5, pretrained=<strong class="mq hj">False</strong>).to_fp16()</span></pre><p id="703a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">在设定了我们的学习速度、批量和解冻我们的语言模型之后，让我们开始我们的(长期)训练。</p><pre class="iy iz ja jb fd mp mq mr ms aw mt bi"><span id="0846" class="mu kl hi mq b fi mv mw l mx my">lr = 1e-2</span><span id="2e3c" class="mu kl hi mq b fi nc mw l mx my">lr *= bs/48  <em class="lw"># Scale learning rate by batch size</em></span><span id="0414" class="mu kl hi mq b fi nc mw l mx my">learn.unfreeze()</span><span id="f39b" class="mu kl hi mq b fi nc mw l mx my">learn.fit_one_cycle(10, lr, moms=(0.8,0.7))</span></pre><p id="d8bf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">训练可能需要一些时间，因为我们正在使用一个巨大的语料库(维基百科)从头开始训练一个语言模型。</p><p id="1070" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，让我们保存预训练的模型和vocab(最好保存在我们的磁盘或Google drive中，以避免每次长时间的训练)。</p><pre class="iy iz ja jb fd mp mq mr ms aw mt bi"><span id="0d14" class="mu kl hi mq b fi mv mw l mx my">mdl_path = path/'models'</span><span id="0854" class="mu kl hi mq b fi nc mw l mx my">mdl_path.mkdir(exist_ok=<strong class="mq hj">True</strong>)</span><span id="d2ac" class="mu kl hi mq b fi nc mw l mx my">learn.to_fp32().save(mdl_path/lm_fns[0], with_opt=<strong class="mq hj">False</strong>)</span><span id="f7ee" class="mu kl hi mq b fi nc mw l mx my">learn.data.vocab.save(mdl_path/(lm_fns[1] + '.pkl'))</span></pre><p id="4b91" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在我们的语言模型<code class="du mz na nb mq b">learn</code>在学习了印地语之后，已经可以根据印地语-维基百科的数据进行预测了。</p><pre class="iy iz ja jb fd mp mq mr ms aw mt bi"><span id="e4d0" class="mu kl hi mq b fi mv mw l mx my">learn.predict("मैं तुम्हें", n_words=5)</span><span id="a183" class="mu kl hi mq b fi nc mw l mx my">'मैं तुम्हें पसंद नहीं है लेकिन मैं'</span></pre><p id="7057" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">但是让我们通过输入更多的印地语数据集来提高它对印地语的理解，因此我们将使用<a class="ae jn" href="https://www.kaggle.com/disisbig/hindi-movie-reviews-dataset" rel="noopener ugc nofollow" target="_blank"> imdb印地语评论数据集</a>来进一步训练它。</p><pre class="iy iz ja jb fd mp mq mr ms aw mt bi"><span id="f18c" class="mu kl hi mq b fi mv mw l mx my">train_df = pd.read_csv('path/to/file/train.csv')</span><span id="6e6b" class="mu kl hi mq b fi nc mw l mx my">test_df = pd.read_csv('path/to/file/valid.csv')</span><span id="e774" class="mu kl hi mq b fi nc mw l mx my">df = pd.concat([train_df,test_df], sort=False)</span></pre><p id="ca5a" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">让我们准备数据并构建另一个语言模型，该模型根据新的imdb数据进行了微调。</p><pre class="iy iz ja jb fd mp mq mr ms aw mt bi"><span id="3dac" class="mu kl hi mq b fi mv mw l mx my">data_lm = (TextList.from_df(df, path,      cols='text').split_by_rand_pct(0.1, seed=42).label_for_lm().databunch(bs=bs, num_workers=1))</span><span id="f7f1" class="mu kl hi mq b fi nc mw l mx my">learn_lm = language_model_learner(data_lm, arch=AWD_LSTM,  drop_mult=0.3,pretrained_fnames=lm_fns, pretrained=False)</span></pre><p id="af2c" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">因此，我们的新语言模型<code class="du mz na nb mq b">learn_lm</code>已经准备好对imdb数据进行微调，现在我们需要训练它。首先，只有最后一层，然后在整个网络上解冻它。(我们也可以玩玩纪元的数量，也可以在这里使用逐步解冻)。</p><pre class="iy iz ja jb fd mp mq mr ms aw mt bi"><span id="c5f3" class="mu kl hi mq b fi mv mw l mx my">learn_lm.fit_one_cycle(2, lr*10, moms=(0.8,0.7))</span><span id="d3f0" class="mu kl hi mq b fi nc mw l mx my">learn_lm.unfreeze()</span><span id="7550" class="mu kl hi mq b fi nc mw l mx my">learn_lm.fit_one_cycle(8, lr, moms=(0.8,0.7))</span></pre><p id="ccdf" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">现在，在对我们的模型进行微调之后，它对印地语的语义有了更好的理解。我们来玩一下，这里<code class="du mz na nb mq b">n_words</code>是我们要预测的数字:</p><pre class="iy iz ja jb fd mp mq mr ms aw mt bi"><span id="b31e" class="mu kl hi mq b fi mv mw l mx my">learn_lm.predict("मैं तुम्हें", n_words=5)</span><span id="f4cd" class="mu kl hi mq b fi nc mw l mx my">'मैं तुम्हें लंका में आने के लिए'</span></pre><p id="2bcb" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">另一种尝试:</p><pre class="iy iz ja jb fd mp mq mr ms aw mt bi"><span id="c84b" class="mu kl hi mq b fi mv mw l mx my">learn_lm.predict("मुझे फिल्म ", n_words=10)</span><span id="5526" class="mu kl hi mq b fi nc mw l mx my">'मुझे फिल्म  में राहत नहीं मिली है । कुछ होना बेकार है'</span></pre><p id="aac9" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">所以最终我们有了一个预测器，可以很好地预测下一个印地语单词。我们也可以使用其他印地语语料库来训练它，提高它对语言的理解。</p><h1 id="1756" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">好了，让我们结束吧..</h1><p id="baca" class="pw-post-body-paragraph jo jp hi jq b jr lr ij jt ju ls im jw jx lt jz ka kb lu kd ke kf lv kh ki kj hb bi translated">在这里，我们从维基百科的印地语数据集构建了一个语言模型，并在imdb数据集上对其进行了微调。类似地，我们可以为任何其他语言建立语言模型，只需使用该语言的wiki语言代码，并遵循相同的过程。</p><p id="cdc5" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们也可以尝试使用BERT和XLNET构建类似的模型，但是构建模型的时间可能会显著增加(如果我们从头开始构建的话)。</p><p id="4375" class="pw-post-body-paragraph jo jp hi jq b jr js ij jt ju jv im jw jx jy jz ka kb kc kd ke kf kg kh ki kj hb bi translated">我们还可以使用各种其他印地语数据集来微调我们的模型，这将使我们的模型更好地理解语言，并更好地预测。</p><h1 id="95f1" class="kk kl hi bd km kn ko kp kq kr ks kt ku io kv ip kw ir kx is ky iu kz iv la lb bi translated">参考资料:</h1><ol class=""><li id="81ae" class="lz ma hi jq b jr lr ju ls jx nd kb ne kf nf kj ng mf mg mh bi translated">Fast.ai文档<a class="ae jn" href="https://docs.fast.ai/" rel="noopener ugc nofollow" target="_blank">https://docs.fast.ai/</a></li><li id="b821" class="lz ma hi jq b jr mi ju mj jx mk kb ml kf mm kj ng mf mg mh bi translated">Fast.ai教程<a class="ae jn" href="https://www.fast.ai/2019/07/08/fastai-nlp/" rel="noopener ugc nofollow" target="_blank">https://www.fast.ai/2019/07/08/fastai-nlp/</a></li></ol></div></div>    
</body>
</html>