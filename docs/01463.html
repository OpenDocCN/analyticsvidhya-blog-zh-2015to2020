<html>
<head>
<title>Clustering: Birds of a Feather Flock Together</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">集群:物以类聚，人以群分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/clustering-birds-of-a-feather-flock-together-8ae84657b997?source=collection_archive---------22-----------------------#2019-10-23">https://medium.com/analytics-vidhya/clustering-birds-of-a-feather-flock-together-8ae84657b997?source=collection_archive---------22-----------------------#2019-10-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/59964949c17cd8d57be44dd52f894049.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y7EgYXGFTaF_TsBnfi7wMA.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">亚历克斯·霍利奥克在<a class="ae hv" href="https://unsplash.com/s/photos/flock-together?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><div class=""/></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><p id="7158" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">将数据点分组到组(簇)中，使得组中的点彼此比组外的点更“相似”的任务称为聚类。但是如何知道一个数据点是否与另一个数据点相似呢？这种定义相似性的行为是各种聚类方法相互区分的原因——K-Means通过数据点与聚类质心的接近程度来定义相似性，而DBSCAN通过将相同密度区域内的数据点组合在一起来定义相似性。</p><p id="a7f7" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">在本文中，我们将看看这两种在无监督机器学习中经常使用的聚类方法，并在Python中实现它们。所以让我们开始吧！</p><h1 id="d087" class="ka kb hy bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">k均值</h1><p id="94f4" class="pw-post-body-paragraph jc jd hy je b jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv lc jx jy jz hb bi translated">K-Means聚类分析算法是一种硬聚类分析，即一个数据点只能完全属于一个聚类(与软聚类分析方法相反，在软聚类分析方法中，每个数据点都被分配了一个概率或可能性来属于给定的聚类)。</p><p id="2777" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">K-Means算法通过以下步骤进行迭代:</p><ol class=""><li id="9ab8" class="ld le hy je b jf jg jj jk jn lf jr lg jv lh jz li lj lk ll bi translated"><strong class="je hz">指定所需的聚类数k。</strong></li><li id="2c79" class="ld le hy je b jf lm jj ln jn lo jr lp jv lq jz li lj lk ll bi translated">以某种方式初始化K个簇形心。质心不一定是数据点本身，但可以在数据域内生成。</li><li id="07eb" class="ld le hy je b jf lm jj ln jn lo jr lp jv lq jz li lj lk ll bi translated"><strong class="je hz">将每个数据点分配到其最近的聚类质心</strong>。“最接近”是通过最小化欧几里德距离(或L2范数)来定义的，欧几里德距离是向量值平方之和的平方根(想想2D的毕达哥拉斯定理)。</li><li id="ff74" class="ld le hy je b jf lm jj ln jn lo jr lp jv lq jz li lj lk ll bi translated"><strong class="je hz">通过计算每个聚类内所有点的平均值并设置新的质心值，重新分配K个聚类质心</strong>。</li><li id="e149" class="ld le hy je b jf lm jj ln jn lo jr lp jv lq jz li lj lk ll bi translated">计算每个质心从其先前值改变的距离</li><li id="a1a3" class="ld le hy je b jf lm jj ln jn lo jr lp jv lq jz li lj lk ll bi translated">重复步骤3、4和5，直到每个质心变化的距离低于预定义的阈值，或者直到达到预定义的迭代次数。</li></ol><h2 id="be20" class="lr kb hy bd kc ls lt lu kg lv lw lx kk jn ly lz ko jr ma mb ks jv mc md kw me bi translated">k-意味着假设和限制</h2><p id="411e" class="pw-post-body-paragraph jc jd hy je b jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv lc jx jy jz hb bi translated">值得注意的是，K-Means减少了组内平方和，即每个组内观察值的方差。换句话说，平方和小的集群比平方和大的集群更紧凑。此外，随着一个聚类内的观察数量的增加，平方和也变得更大。因此，<strong class="je hz"> K-Means在数据点被组织成</strong> <a class="ae hv" href="https://en.wikipedia.org/wiki/Convex_set" rel="noopener ugc nofollow" target="_blank"> <strong class="je hz">【凸状】</strong> </a> <strong class="je hz">、球状集群并且在每个集群内包含大致相同数量的点</strong>时效果最佳。</p><p id="c71a" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">在下图中，K-Means很难将左侧的数据点聚类成两个组，而将数据集从笛卡尔坐标转换到极坐标会产生合理的聚类分配。因此，<strong class="je hz">在对数据集应用聚类算法之前，重要的一点是可视化并理解数据集</strong>。</p><figure class="mg mh mi mj fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mf"><img src="../Images/aa01ad56741c9e0be4796606337f8b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xv0BKSvUrmLHlSk8mFFMcQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">来源:<a class="ae hv" href="http://varianceexplained.org/r/kmeans-free-lunch/" rel="noopener ugc nofollow" target="_blank">http://varianceexplained.org/r/kmeans-free-lunch/</a></figcaption></figure><p id="0c0a" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">此外，由于K-Means要求预定义聚类数，因此选择正确的K值很重要，但也很困难，尤其是当您无法可视化数据集中的所有维度时。但是，当要选择的聚类数不清楚时，您可以使用<a class="ae hv" href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)" rel="noopener ugc nofollow" target="_blank">肘方法</a>或<a class="ae hv" href="https://en.wikipedia.org/wiki/Silhouette_(clustering)" rel="noopener ugc nofollow" target="_blank">侧影方法</a>来确定K的最佳选择。</p><h1 id="199c" class="ka kb hy bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">基于密度的噪声应用空间聚类</h1><p id="7eff" class="pw-post-body-paragraph jc jd hy je b jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv lc jx jy jz hb bi translated"><a class="ae hv" href="https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf" rel="noopener ugc nofollow" target="_blank">基于密度的带噪声应用空间聚类</a>或简称为DBSCAN，于1996年提出，它根据点的密度组织聚类，与K-Means不同，它在没有用户输入的情况下确定要生成的聚类数。此外，DBSCAN允许我们通过将噪声定义为密度点低于聚类的区域来对噪声进行分类(不同于K-Means)。例如，K-Means很难适应下面的分布，而DBSCAN能够根据点的密度正确识别聚类。</p><figure class="mg mh mi mj fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mk"><img src="../Images/a104d0d831f2750063b2cccf30cd17ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IumXYXdv7iLsWUWLsRC3lQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">来源:<a class="ae hv" href="https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/" rel="noopener ugc nofollow" target="_blank">https://www . naftaliharris . com/blog/visualizing-DBS can-clustering/</a></figcaption></figure><p id="59ec" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">DBSCAN中有两个关键概念/超参数— <em class="ml"> MinPts </em>和<em class="ml"> Eps </em>:</p><ol class=""><li id="2d49" class="ld le hy je b jf jg jj jk jn lf jr lg jv lh jz li lj lk ll bi translated">对于聚类中的每个点，在其邻域中必须至少有指定的最小数量的点(<em class="ml"> MinPts </em>)，即邻域中的密度必须超过某个阈值。参数<em class="ml"> MinPts </em>主要控制算法对噪声的容忍程度。</li><li id="b753" class="ld le hy je b jf lm jj ln jn lo jr lp jv lq jz li lj lk ll bi translated">邻域被定义为点(<em class="ml"> Eps </em>)周围的空间，其形状由选择的距离函数决定，即当在2D空间中使用曼哈顿距离时，邻域的形状是矩形的。与K-Means类似，DBSCAN最常用的距离函数是欧几里德距离。</li></ol><p id="0fef" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">使用这两个超参数，DBSCAN将数据点分为三类:<em class="ml">核心点、</em>或聚类内部的点、<em class="ml">边界点、</em>或聚类边缘上的点，以及<em class="ml">噪声</em>，或不属于任何聚类的点。</p><h2 id="4952" class="lr kb hy bd kc ls lt lu kg lv lw lx kk jn ly lz ko jr ma mb ks jv mc md kw me bi translated">DBSCAN假设和限制</h2><p id="875d" class="pw-post-body-paragraph jc jd hy je b jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv lc jx jy jz hb bi translated">因为参数<em class="ml"> Eps </em>控制数据点的局部邻域并影响聚类分配，所以适当选择它是至关重要的，并且它通常不能保留默认值。如果设置得太大，集群将合并到另一个集群中，如果<em class="ml"> Eps </em>足够大，最终返回一个集群。如果设置过小，大多数数据点根本不会被聚类或归类为噪声。下图显示了选择正确的<em class="ml"> Eps </em>值的重要性。</p><figure class="mg mh mi mj fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mm"><img src="../Images/b2f7387eeb2530746b1217558d715cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vETm6zjdc1Lw3bC3AyjWyQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">改变ε值的影响</figcaption></figure><p id="5b80" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated">此外，DBSCAN难以处理包含密度差异很大的聚类的数据集，因为无法为每个聚类定制参数<em class="ml"> Eps </em>和<em class="ml"> MinPts </em>。对于高维数据，欧几里德距离的标准选择受到影响，这使得为<em class="ml"> Eps </em>找到合适的值具有挑战性。因此，对于K-Means，<strong class="je hz">在对数据集应用聚类算法之前</strong> <strong class="je hz">可视化和理解数据集是很重要的</strong>。</p><h1 id="88f3" class="ka kb hy bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">履行</h1><p id="6cb3" class="pw-post-body-paragraph jc jd hy je b jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv lc jx jy jz hb bi translated"><a class="ae hv" href="https://github.com/jissac/ScratchML/blob/master/Clustering.ipynb" rel="noopener ugc nofollow" target="_blank">参考下面的笔记本</a>了解K-Means和DBSCAN的从头开始的Python实现，以及与流行的Scikit-Learn算法实现的比较。希望它能帮助你理解这两种算法的内部工作原理以及每种算法背后的假设！</p><h1 id="6608" class="ka kb hy bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">摘要</h1><p id="8619" class="pw-post-body-paragraph jc jd hy je b jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv lc jx jy jz hb bi translated">总之，K-Means和DBSCAN是两种强大的算法，当您有一个未标记的数据集需要聚类成组时可以使用。然而，<strong class="je hz">请记住您选择的模型所基于的假设</strong> —为了从模型中获得正确的预测，了解数据集并对其进行预处理是必不可少的。另外，请务必阅读Scikit-Learn集群文档，以了解其他集群算法！</p></div><div class="ab cl iv iw gp ix" role="separator"><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja jb"/><span class="iy bw bk iz ja"/></div><div class="hb hc hd he hf"><h1 id="7433" class="ka kb hy bd kc kd mn kf kg kh mo kj kk kl mp kn ko kp mq kr ks kt mr kv kw kx bi translated">参考/有用链接:</h1><p id="7d7f" class="pw-post-body-paragraph jc jd hy je b jf ky jh ji jj kz jl jm jn la jp jq jr lb jt ju jv lc jx jy jz hb bi translated">https://scikit-learn.org/stable/modules/clustering.html<a class="ae hv" href="https://scikit-learn.org/stable/modules/clustering.html" rel="noopener ugc nofollow" target="_blank"/></p><p id="42c3" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><a class="ae hv" href="https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/multivariate/how-to/cluster-k-means/interpret-the-results/all-statistics-and-graphs/" rel="noopener ugc nofollow" target="_blank">https://support . minitab . com/en-us/minitab/18/help-and-how-to/modeling-statistics/multivarial/how-to/cluster-k-means/interpret-the-results/all-statistics-and-graphs/</a></p><p id="e464" class="pw-post-body-paragraph jc jd hy je b jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz hb bi translated"><a class="ae hv" href="https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf" rel="noopener ugc nofollow" target="_blank">https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf</a></p></div></div>    
</body>
</html>