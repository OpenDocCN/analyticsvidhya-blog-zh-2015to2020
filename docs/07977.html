<html>
<head>
<title>Dimensionality Reduction in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的降维</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/dimensionality-reduction-in-machine-learning-179ba0c83066?source=collection_archive---------20-----------------------#2020-07-13">https://medium.com/analytics-vidhya/dimensionality-reduction-in-machine-learning-179ba0c83066?source=collection_archive---------20-----------------------#2020-07-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1747" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><em class="je">降维，或称降维，是将数据从高维空间转换到低维空间，使低维表示保留原始数据的一些有意义的属性。</em></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jf"><img src="../Images/5516637a9b1f2f0d54df80aeb1a2710a.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*lqCz8cut1LfUMTjo8RtYKQ.jpeg"/></div></figure><p id="148f" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="je">我们为什么要使用降维技术？</em> </strong></p><p id="cdd4" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">人类无法将高维数据可视化，所以我们希望将其降维。在现实世界的数据分析任务中，我们分析复杂的数据，即多维数据。我们绘制数据，并在其中找到各种模式，或者用它来训练一些机器学习模型。考虑维度的一种方式是，假设您有一个数据点 <strong class="ii hj"> <em class="je"> x </em> </strong> <em class="je">，如果我们将该数据点视为一个物理对象，那么维度只是视图的基础，就像从水平轴或垂直轴观察时数据位于何处一样。</em></p><p id="79d3" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><em class="je">随着数据维度的增加，可视化和计算的难度也在增加。那么，如何降低一个数据的维度——<br/>*去掉冗余维度<br/> *只保留最重要的维度</em></p><p id="cf7b" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="je">机器学习中的降维技术有哪些？</em> </strong></p><p id="6bfd" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">在这篇文章中，我们使用了像主成分分析和t-SNE这样的基本技术。</p><p id="101a" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="je">主成分分析(PCA):机器学习中的</em></strong><em class="je">PCA是无监督学习技术。</em></p><blockquote class="jn jo jp"><p id="62a4" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><em class="hi">先试着理解一些术语</em></p><p id="3ec2" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="hi">方差:</em> </strong> <em class="hi">它是可变性的度量，或者它只是度量数据集的分布程度。从数学上讲，它是平均分数的均方差。我们使用下面的公式来计算方差var(x)。</em></p></blockquote><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jt"><img src="../Images/b57331597490b54de3d0274b130d0302.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FO3SPxO6CVeZHwRtbvtosg.jpeg"/></div></div></figure><blockquote class="jn jo jp"><p id="dd21" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="hi">协方差:</em></strong><em class="hi">It</em><strong class="ii hj"><em class="hi"/></strong><em class="hi">是对两组有序数据中对应元素向同一方向移动程度的度量。公式如上所示，表示为cov(x，y)作为x和y的协方差。</em></p><p id="5a44" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="hi">列标准化:</em> </strong> <em class="hi">这是一种压缩数据点的方法，使得均值向量位于原点，并且在变换后的空间中任何轴上的方差(通过压缩或扩展)都为1。这种技术通常被称为均值居中和方差缩放。</em></p></blockquote><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jy"><img src="../Images/8490db3b627c43313518553cfe629f06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*Ap_7t_-luGSaAVgc7kl7qA.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated"><strong class="bd kd"> <em class="if">列标准化公式</em> </strong></figcaption></figure><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es ke"><img src="../Images/9e2c812d313d7f01902d4f8047cf4672.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*20Laj98YaUB47dQFMCnmMw.png"/></div></div></figure><p id="b084" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"><em class="je">PCA是如何工作的:</em> </strong> <em class="je">我们要保留数据中最大散度/方差的方向。</em></p><ol class=""><li id="0909" class="kf kg hi ii b ij ik in io ir kh iv ki iz kj jd kk kl km kn bi translated">首先，我们要把给定的数据标准化。</li></ol><p id="567b" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><em class="je"> 2。计算数据点的协方差矩阵X。</em></p><p id="e06c" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><em class="je"> 3。计算特征向量和相应的特征值。</em></p><p id="f868" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><em class="je"> 4。按照特征值降序排列特征向量。</em></p><p id="06b5" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><em class="je"> 5。选择前k个特征向量，这将是新的k维。</em></p><p id="f27f" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><em class="je"> 6。把原来的n维数据点转换成k维。</em></p><p id="98b0" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="je">特征向量</em> </strong> <em class="je">:这些向量给出了数据中出现最大扩散的方向。</em></p><p id="5941" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="je">特征值</em> </strong> <em class="je">:这些值给出了在什么方向上传播的百分比。</em></p><p id="77ef" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">在下面的例子中，我们使用Python和一些库实现了PCA。</p><p id="38cf" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><em class="je">在这些例子中，我们使用真实世界的Kaggle数据集来执行PCA。</em> <a class="ae ko" href="https://www.kaggle.com/c/digit-recognizer/data" rel="noopener ugc nofollow" target="_blank"> <em class="je">访问下载MNIST数据集从Kaggle下载。</em> </a></p><p id="bd69" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="je">什么是MNIST数据集？</em> </strong></p><p id="0a6d" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><em class="je">在包含成千上万个手写数字图像的MNIST数据集中，我们的任务是将手写字符分类为10个数字字符中的一个。</em></p><p id="0b25" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><em class="je">每张图片高28像素，宽28像素，总共784像素。每个像素都有一个与之关联的像素值，表示该像素的亮度或暗度，数字越大表示越暗。该像素值是0到255之间的整数，包括0和255。</em></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es kp"><img src="../Images/485ac12a554e9ae0e4f7fba7efec8aab.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*Viegt9zKOc37rL-fxEAOvQ.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated"><strong class="bd kd"> <em class="if">手写图像</em> </strong></figcaption></figure><blockquote class="jn jo jp"><p id="9b45" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><em class="hi">首先，我们想把图像转换成一个列向量。将图像表示成矩阵。如果图像由全黑表面组成，则赋值“0”，如果图像的表面是全白的，则赋值“1”，如果图像是灰色的，则赋值“0”和“1”之间的值。</em></p><p id="0067" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><em class="hi">矩阵表示后通过使用展平将矩阵转换成单列向量，如图像的维数为(784×1)。</em></p><p id="29d3" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated">让我们从Python开始</p><p id="5092" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><em class="hi">#加载数据集<br/>Data = PD . read _ CSV(' train . CSV ')</em></p><p id="5413" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated">在进行主成分分析之前，请始终将您的数据标准化，因为如果我们使用不同尺度的数据(此处为特征)，我们会得到误导成分。T56】</p><p id="fd66" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><em class="hi"> #数据-预处理:对来自sklearn .预处理导入standard scaler<br/>standard _ Data = standard scaler()的数据进行标准化。fit_transform(数据)<br/> print(标准化_数据.形状)</em></p><p id="b31b" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="hi">求协方差矩阵即:A^T * A </em> </strong></p><p id="f2b8" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><em class="hi"> #矩阵乘法使用numpy<br/>covar _ matrix = NP . mat mul(标准化_数据。t，标准化_数据)</em></p><p id="8cda" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><em class="hi"> #找到前两个特征值和相应的特征向量<br/> #用于投影到二维空间。</em></p><p id="6a9a" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="hi">计算特征向量和对应的特征值。</em> </strong></p><p id="9655" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><em class="hi">值，向量= eigh(covar_matrix，eigvals=(782，783)) </em></p><p id="481d" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="hi">将原始数据样本投影到平面上。<br/></em></strong><em class="hi">new _ coordinates = NP . mat mul(vectors，sample_data。T) </em></p><p id="2b14" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><em class="hi">完成所有这些工作后，绘制出数据的样子</em></p></blockquote><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es kq"><img src="../Images/8041656c849d4e08a21dbe0947966127.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*qa2fkfYo9h42ZjlR5-IVPA.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated"><strong class="bd kd"> <em class="if">应用PCA后</em> </strong></figcaption></figure><blockquote class="jn jo jp"><p id="1e3a" class="ig ih je ii b ij ik il im in io ip iq jq is it iu jr iw ix iy js ja jb jc jd hb bi translated"><em class="hi">我们还可以通过做explained _ variance/components总数来找出方向解释了多少方差。在下图中，如果我们使用300个维度，大约97 %的数据保存在thaa维度中。</em></p></blockquote><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es kr"><img src="../Images/59357557ce8833a5e69c1dfc79703c37.png" data-original-src="https://miro.medium.com/v2/resize:fit:842/format:webp/1*qO-pzn8qdCHtuprYfCnmEA.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated"><strong class="bd kd"> <em class="if">解释方差的百分比</em> </strong></figcaption></figure><p id="696e" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><a class="ae ko" href="https://github.com/Sachin-D-N/Machine_Learning/blob/master/PCA_Tsne/PCA.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="je">查看详细代码请访问。</em> </a></p><p id="a13d" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"><em class="je">PCA的局限性:</em> </strong></p><ol class=""><li id="a978" class="kf kg hi ii b ij ik in io ir kh iv ki iz kj jd kk kl km kn bi translated"><em class="je"> PCA试图只保留数据的全局形状。</em></li></ol><p id="6821" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><em class="je"> 2。当给定的数据是圆形或超球形时，我们在应用PCA时会丢失数据。</em></p><p id="0340" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"><em class="je">t-SNE(t-分布式随机邻域嵌入):</em> </strong></p><p id="43b5" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><em class="je"> PCA仅保留数据的全局形状，但是t-SNE保留数据的全局和局部形状。这也是最好的可视化技术。</em></p><p id="3410" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="je">邻域:</em> </strong> <em class="je">当两点之间的距离很小时我们可以定义为这些点都在邻域内。</em></p><p id="a4d0" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="je">嵌入:</em> </strong> <em class="je">嵌入是将数据点从高维度逐个放入低维度的技术。</em></p><p id="211d" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><strong class="ii hj"> <em class="je">如何应用t-SNE？</em> </strong></p><ol class=""><li id="19e8" class="kf kg hi ii b ij ik in io ir kh iv ki iz kj jd kk kl km kn bi translated"><em class="je"> t-SNE是一种迭代算法。</em></li><li id="0c20" class="kf kg hi ii b ij ks in kt ir ku iv kv iz kw jd kk kl km kn bi translated"><em class="je"> t-SNE用于邻域保持嵌入技术。</em></li><li id="bc36" class="kf kg hi ii b ij ks in kt ir ku iv kv iz kw jd kk kl km kn bi translated">每次当我们运行一个算法时，我们得到不同的值，所以t-SNE被称为随机的。</li><li id="bfb2" class="kf kg hi ii b ij ks in kt ir ku iv kv iz kw jd kk kl km kn bi translated">当我们增加更多的迭代次数时，我们会得到更好的形状。</li><li id="9054" class="kf kg hi ii b ij ks in kt ir ku iv kv iz kw jd kk kl km kn bi translated"><em class="je"> t-SNE基本上是扩大密集的点群，收缩稀疏的簇群。下图显示了对MNIST数据集应用t-SNE后t-SNE的可视化效果。</em></li></ol><figure class="jg jh ji jj fd jk er es paragraph-image"><div class="er es jy"><img src="../Images/57c383dec069ad0e19f010be20f1725d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/format:webp/1*AumJ9ZkSZsxYkk1KZcAH-w.png"/></div><figcaption class="jz ka et er es kb kc bd b be z dx translated"><strong class="bd kd"><em class="if">MNIST t-SNE数据集</em> </strong></figcaption></figure><p id="c1b5" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><a class="ae ko" href="https://distill.pub/2016/misread-tsne/" rel="noopener ugc nofollow" target="_blank"> <em class="je">在没有编码的web中玩t-SNE访问</em> </a> <em class="je">。</em></p><p id="78e3" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><a class="ae ko" href="https://github.com/Sachin-D-N/Machine_Learning/blob/master/PCA_Tsne/T-SNE.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="je">查看t-SNE访问的详细代码。</em>T3】</a></p><p id="f400" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><em class="je">t-SNE如此受欢迎是有原因的:它非常灵活，经常能找到其他降维算法找不到的结构。不幸的是，正是这种灵活性使得解读</em>变得棘手</p><p id="fec2" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated">如果你发现帖子中有什么错误，或者你有什么要补充的，让我们在评论中讨论。</p><p id="6e59" class="pw-post-body-paragraph ig ih hi ii b ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc jd hb bi translated"><em class="je">感谢阅读……</em></p></div></div>    
</body>
</html>