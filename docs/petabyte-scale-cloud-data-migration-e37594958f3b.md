# 实施 Pb 级云数据迁移

> 原文：<https://medium.com/analytics-vidhya/petabyte-scale-cloud-data-migration-e37594958f3b?source=collection_archive---------6----------------------->

![](img/61398aac48cfec319e0abe4f0f5acb24.png)

云数据迁移

一家非常大的第一食品零售连锁店正在将其数据平台迁移到 AWS 云。目前，他们的内部数据中心有 IBM DataStage 作为结构化 ETL 工具，Teradata 和 Microsoft SQL Server 作为 OLAP 数据库，Tableau 作为报告层，Hortonworks Hadoop 发行版。在新平台中，他们打算使用 S3、红移作为 OLAP 数据库、Talend 用于数据集成以及 rewire tableau 到红移以服务于报告层来构建数据湖。委托一个 10 节点 r 4.2x 大型长期运行 EMR 集群处理半结构化数据，如用于点击流、厨房视频系统等的 XML 和 JSON。数据处理—这是对内部 HortonWorks Hadoop 集群的替代。

这种迁移的本质是“提升和转移”。我不会在这里讨论战略/管理考虑，但会讨论实现部分。要完成这一迁移，需要实施四项主要任务

*   在 Talend 中重写所有必要的 DataStage 数据集成作业。
*   将现有数据从内部迁移到云—迁移到 S3，然后拷贝到 Redshift。
*   设置迁移 ETL 管道以在 prem 和云平台上运行，并在关闭 prem 管道之前验证一段时间的数据。
*   根据需要将报告层重新连接到新的数据源。

虽然必须有广泛的迁移计划，但我将特别讨论上述列表中的第二项任务，以及我们面临的挑战和我们如何创建一个漂亮的通用解决方案，该解决方案节省了我们大量的时间/精力，并有助于顺利完成任务。

# 问题:

Pb 级的数据被锁在 Teradata & SQL Server 中，必须按分区提取、压缩并复制到 S3。下一步，来自 S3 的数据将被加载到 Redshift 中——整个过程中应该有验证步骤，以确保数据在整个过程中正确传输。最初，团队开始一次做一个表，并使用互联网通过 VPC 私有/公共子网复制数据，使用特定的元数据、分区键、验证逻辑等为每个单独的表创建 DataStage ETL 作业。这需要太多的时间来为每个表创建管道，缓慢的数据传输和从头到尾照看整个过程。我们没有在市场上找到一个好的通用解决方案来满足我们的特定需求，所以我们需要一个定制的高度可配置的解决方案来无缝地完成这项任务，它可以使用配置的值自行运行，进行验证，并在任务完成后发送自动报告。我们将我们的需求分类如下:

*   需要创建一个高度可配置的通用流程来完成端到端任务。
*   需要使用批量数据传输方法，像 AWS 滚雪球一样随着使用公共互联网。

# 解决方案:

我们主要使用 unix 和 python 开发了一个软件。我将在下面讨论如何使用它来完成每个单独的任务:

![](img/3533f8ef034e616c09581754c047532a.png)

数据迁移—任务流

# 数据提取到文件:

主要的数据库供应商提供了向/从他们的数据库批量导入/导出数据的工具。Teradata 和 SQL Server 也有批量导出数据的实用程序——分别是 FastExport 和 bcp。我们的大部分数据存储在 Teradata 中，一小部分存储在 SQL Server 中，因此选择 Teradata 来解释该过程，但是可以根据需要使用“bcp”配置或任何其他数据库批量导出实用程序来运行相同的过程。

Fastexport 从 Teradata 表中导出 64K 块的数据，这对于提取大量数据非常有用。该实用程序需要提供“varchar”格式的特定列名，并且不能与“select *”一起使用。这不是问题，因为可以从系统表中获取列名，并且在配置文件中可以省略或添加派生列或添加过滤器。另一个问题是导出数据中的默认分隔符是' \t '，这可能不是我们一直想要的，因为数据本身可能包含' \t '，这使得很难区分数据和分隔符。幸运的是，Teradata 支持用户编写的过时例程，这些例程可用于在写入文件之前对数据进行预处理，因此这可用于根据需要替换分隔符。我找到了一个用 C 写的例程，编译成 dlmt_vchar.so 哪个做的工作。

这个项目的源代码可以在下面的 github 链接中找到:

[https://github.com/sandipayan/Onprem2Aws](https://github.com/sandipayan/Onprem2Aws)

该流程的主要组成部分包括:

**snowball.ini:** 这是主配置文件，包含源数据库、不同路径、分隔符、zip 选项、列排除、自定义 sql、复制到 snowball 或通过互联网(对于小表，因为 snowball 是一个物理设备，需要手动运送，稍后将详细介绍)等的凭证。

**input_list:** 每行表格列表，用逗号分隔过滤条件(如果有)。

**snowball_main.ksh:** 这是运行流程时要调用的主脚本。这会读取“input_list”文件中的表名(以及其他给定的附加信息),并遍历列表。开始时，它创建一个主日志文件，并写入每个表的摘要信息，如开始时间、结束时间、花费的时间、原始文件大小、压缩大小、文件路径等。在每次迭代中，它从 dbc.columns 中找到表的元数据，使用 Varchar 中的列形成 select 查询，添加过滤器信息(如果有的话)或接受要提取数据的定制 sql 查询。

**snowball_cs_trfm.ksh:** 该脚本帮助格式化从 dbc.columns 系统表中检索的元数据，并创建一个中间的{variable}.cs.dat 表，该表在主脚本中使用。

**snowball_cs.fe:** 它从 snowball.ini 中获取上面创建的 sql 文件和输入，并创建一个中间文件 snowball _ cs _ seded { FE _ LOG _ SUFFIX }。带有所有参数替换的 fe，这些参数将在下一步中用于从表中运行 Fastexport 提取过程。

**snowball_main.fe:** 这是最终的 Fastexport 脚本，它使用前面过程的输出并实际生成数据提取文件。

**dlmt_vchar.so:** 这是一个共享对象(编译的 C 库)，在上面的脚本中使用它来帮助预处理提取的数据，比如改变分隔符。这与环境变量 FEXP_DELIMITER 一起使用，该变量在。ini”文件。

基于定义的配置值，该脚本压缩提取的数据文件，并通过公共互联网或写入雪球设备移动到派生的 s3 路径。

## 挑战:

虽然这一过程对于 95%以上的数据来说工作顺利，但是有一小部分数据主要在 SQL server 中，这些数据具有像“comment”或其他自由格式文本列这样的列，很难使用可靠的分隔符。对于这些情况，我们需要以模式和数据一起提供的格式导出/导入数据。在这些情况下，我们使用 Sqoop 进行数据提取，并使用 Avro 格式。其余的过程照常进行。

# 雪球设置:

![](img/8ba44133c06162b3e58a9f64d2d59438.png)

雪球装置

使用公共互联网传输大量数据可能需要数天/数周时间，而且非常不可靠。对于小桌子，我们使用公共互联网传输数据，对于大容量，我们根据需要订购雪球设备。订购雪球设备时，需要提供磁盘空间(如:50/80 GB)和 s3 存储桶名称。该设备本身是一个坚固的盒子，有自己的运输标签。一旦收到设备发货，它需要在数据中心连接，然后需要配置**雪球客户端**并用于将数据写入设备。由于现在连接是在本地数据中心进行的，因此写入速度非常快。一旦写入完成，设备就可以分离并运回 AWS，它们会将数据移动到写入雪球设备的同一 s3 存储桶和密钥中。这是一个简单的过程，在“AWS 雪球开发者指南”中有很好的记录。

# 加载到红移:

现在数据在 S3，这需要加载到相应的红移表。需要有一定的考虑因素才能装载到 S3:

*   在 Redshift 中需要一个相应的表(DDL ),数据将被复制到这个表中。
*   当加载到红移时，需要提到各个列，因此列被显式映射以避免无意的错误数据加载/错误。
*   需要有能力删除和重新加载数据。
*   记录活动，包括成功的记录计数加载。

红移表的创建混合了自动脚本和手动编辑，以根据需要包含一些额外的列和默认值。以下是此过程中使用的脚本:

**input_list:** 要加载的表格列表。

**Redshift . conf:**JSON 格式的红移凭证信息。

**s3 . conf:**Key&JSON 格式的秘密访问密钥，从这里 S3 数据被读取。

**getcols.sh:** 从提取的数据中读取标题信息，并准备要在复制命令中传递的列列表。

**Redshift_Loader.py:** 这是使用‘psycopg 2’库的主脚本，使用上述文件将数据迭代加载到 input_list 中的相应表中。这还会生成用于最终验证的日志。

有时我们需要使用不同的 S3 路径来加载特定的红移表，在这种情况下，创建了一个**清单**文件来提及所有 S3 路径(支持通配符)，用于红移表加载。要加载到 Redshift 中的数据应该分布在许多文件中，而不是只有一个巨大的文件。这将有助于红移从许多不同的文件同时加载数据，并大大加快加载过程。为了充分利用最大程度的并行性，文件数应该是 Redshift 中可用片数的倍数。我们不希望文件太小，因为这会增加开销，也不希望文件太大，因为会增加处理时间。文件大小大约应该接近 128 MB，并且数据尽可能均匀地分布在文件中。

***自动状态报告&验证:***

有两个主要步骤，我们希望状态报告生成，保存和电子邮件。两个步骤，因为数据提取和加载发生在两个不同的时间(或天)。

对于第一步，报告的一些关键指标有:表名、分区键、提取开始 ts、提取结束 ts、提取文件大小、提取行数、S3 文件路径、S3 的文件大小等。此阶段的示例报告如下所示:

提取报告样本

对于第二步，同时加载相同的数据到 S3，我们需要类似的记录计数匹配等基本验证日志。要跟踪的一些日志包括:表名、S3/清单路径、开始和结束时间、持续时间、插入的记录数、开始记录数、结束记录数等。此阶段的示例报告如下所示。

可以根据需要实现基于表的更具体的标准，例如，为两个表生成和匹配“R”样式的表摘要。但是我们发现，在时间推移之前，最好对单个表/表集进行手动验证，也就是说，当我们正式完成历史数据迁移，并开启云数据管道和本地数据管道的同步运行时。在 prem 流程结束之前，需要对这两个流程进行一段时间的监控。

您是如何将数据平台迁移到云的，是使用专有工具还是像我们一样开发内部解决方案？很想听听你的经历。