<html>
<head>
<title>Understanding the log loss function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解测井损失函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-the-loss-function-of-logistic-regression-ac1eec2838ce?source=collection_archive---------2-----------------------#2020-07-06">https://medium.com/analytics-vidhya/understanding-the-loss-function-of-logistic-regression-ac1eec2838ce?source=collection_archive---------2-----------------------#2020-07-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f3e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你好，机器学习伙伴们…最近，在这个封锁期，当我回顾ML的基本概念时，我获得了一个更好的直觉&amp;对一些我当时忽略的非常微妙的概念的看法。一个这样的概念是<strong class="ih hj"> <em class="jd">损失函数的逻辑回归</em> </strong> <em class="jd">。</em>在讨论我们的主题之前，我想提醒您一些先决概念，以帮助我们更好地理解我们的主题。</p><p id="56e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">本文的先决条件:</p><ul class=""><li id="f434" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">线性回归</li><li id="9f60" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">逻辑回归</li><li id="adb0" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">学习兴趣😉😉</li></ul></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><p id="ecbb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">刚开始学机器学习的时候，我们被教的第一个题目是<strong class="ih hj"> <em class="jd">线性回归</em> </strong> <em class="jd">。</em>它是一种监督机器学习算法，用于预测连续输出。线性回归算法使用的损失函数是<em class="jd">均方误差。</em></p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es jz"><img src="../Images/4a908272c8bb5700ff1cd541c6f0c5f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WDKhO-z7rti70ZTv59yJ9A.jpeg"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">均方误差公式</figcaption></figure><p id="a279" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MSE所做的是，将每个输入样本的实际输出值和预测输出值之间的距离平方相加(然后除以输入样本数)。使用MSE作为误差函数给出了突出的结果。如果我们仔细观察上面的功能，几分钟的细节是值得强调的</p><ul class=""><li id="93c1" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">MSE对实际输出值和预测输出值之间的距离进行平方的原因是，当与预测值接近实际值的样本相比时，<strong class="ih hj"> <em class="jd">对预测值非常远离实际值的样本</em><em class="jd"/></strong>比对<strong class="ih hj"> <em class="jd"> </em> </strong>惩罚更重。</li><li id="1e55" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">当相对于线性回归模型的权重参数绘制误差函数(<em class="jd">即</em>)时，随着个体权重参数的单位变化，误差的变化将是什么是<strong class="ih hj"> <em class="jd">凸曲线</em> </strong>，这使得有资格应用梯度下降优化算法，以通过找到全局最小值和调整权重来最小化误差。</li></ul><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es kp"><img src="../Images/ee9111798be5960033417bf7eb940f99.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HrFZV7pKPcc5dzLaWvngtQ.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">梯度下降</figcaption></figure><p id="f1d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以上两个特征对于任何损失函数都是重要的。如果我们有一条凸曲线，我们可以应用梯度下降优化算法，并且当使用梯度下降算法时，惩罚远处的样本导致积极调整负责的权重。</p></div><div class="ab cl js jt gp ju" role="separator"><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx jy"/><span class="jv bw bk jw jx"/></div><div class="hb hc hd he hf"><p id="c9df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们转到逻辑回归。它是一种受监督的机器学习算法，用于解决分类问题(如分类邮件是不是垃圾邮件，检查图像中是否存在猫)。逻辑回归类似于线性回归，但有两个显著的区别。</p><ul class=""><li id="9b6f" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">它在输出神经元上使用一个sigmoid激活函数，将输出压缩到0–1的范围内(以概率表示输出)</li><li id="174f" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">它使用一个称为<strong class="ih hj"> <em class="jd">的损失函数对数损失</em> </strong>来计算<strong class="ih hj"> <em class="jd"> </em> </strong>误差。</li></ul><p id="ecf9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在以上两点中，第一点非常简单直观，因为对于分类问题，我们需要输出在0–1的范围内。但我当时不完全理解的是，为什么我们使用一个新的损失函数，称为<strong class="ih hj"> <em class="jd"> log loss </em> </strong>而不是MSE，这在这里直观地感觉是正确的，以及log loss函数的等式是如何工作的。</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es kq"><img src="../Images/144ffb7f12fbdd1c4259729e2e32dec0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*90qLt3vge7Vojf9DTeGcLA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">测井损失函数方程。<strong class="bd kr">y</strong>-实际产量，<strong class="bd kr">p</strong>-逻辑回归预测的概率</figcaption></figure><h2 id="fc18" class="ks kt hi bd kr ku kv kw kx ky kz la lb iq lc ld le iu lf lg lh iy li lj lk ll bi translated">为什么MSE不能与逻辑回归一起工作？</h2><p id="4187" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">MSE对逻辑回归不起作用的一个主要原因是<em class="jd">当MSE损失函数相对于逻辑回归模型的权重作图时，得到的曲线</em> <strong class="ih hj"> <em class="jd">不是凸曲线</em> </strong> <em class="jd">，这使得很难找到全局最小值</em>。使用逻辑回归的MSE的非凸性质是因为<em class="jd">非线性</em>已经以<em class="jd"> sigmoid函数</em>的形式引入到模型中，这使得权重参数和误差之间的关系非常复杂。对初学者来说，深入解释上述观点是不可能的。想了解更多，请看<a class="ae lr" href="http://towardsdatascience.com/why-not-mse-as-a-loss-function-for-logistic-regression-589816b5e03c" rel="noopener" target="_blank"> <strong class="ih hj"> <em class="jd">这篇</em> </strong> </a>的优秀文章。</p><p id="7c57" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">MSE不是逻辑回归首选的另一个原因是，我们知道逻辑回归的输出是一个总是在0-1之间的概率。实际目标值在分类问题中要么是0/1。因此(y-p)将总是在0-1之间，这使得跟踪误差值的进展非常困难，因为很难存储高精度浮点数。四舍五入在这里不是一个选项，因为它导致信息的损失，这是用来指导体重上升。</p><h2 id="7d5f" class="ks kt hi bd kr ku kv kw kx ky kz la lb iq lc ld le iu lf lg lh iy li lj lk ll bi translated"><strong class="ak">让我们揭开<em class="ls"> Log Loss函数的神秘面纱</em> </strong></h2><p id="3275" class="pw-post-body-paragraph if ig hi ih b ii lm ik il im ln io ip iq lo is it iu lp iw ix iy lq ja jb jc hb bi translated">在进入<em class="jd">日志丢失之前，首先理解<em class="jd">日志</em>功能很重要。</em>如果我们绘制<strong class="ih hj"><em class="jd">y = log(x)</em></strong>象限II中的图形看起来是这样的</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es lt"><img src="../Images/577dd073caaa4f1851d577f184080975.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OlInL0X5tdHTppYid-5lXg.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">y =对数(x)图</figcaption></figure><p id="a087" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们只关心X轴上的区域0–1。在上图中，当</p><ul class=""><li id="72d1" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">x=1 → y=0</li><li id="4228" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">x =0 → y=-inf</li></ul><p id="645f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，我们必须观察到，当我们接近x=0时，y值几乎以类似于指数曲线的方式增加。该特性使得对数图成为用作损失函数的良好候选，因为它满足损失函数的第一个特性，<em class="jd">即</em>严重惩罚偏离期望值的样本(让我们很快讨论期望值)。</p><p id="38b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为我们总是喜欢正值，所以我们对上面的函数做了一点小小的修改(<em class="jd"> y = -log(x) </em>)，这样我们在上图中的关注区域就被移到了象限I</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es lu"><img src="../Images/49409226872257ce8c0764112116240c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XJOD4FQCa1uMfjTs4PL4fA.png"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">y =-对数(x)图。</figcaption></figure><p id="4f68" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上图中的轴解释为:</p><ul class=""><li id="7f83" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">x轴:输入样本为真实输出值的概率</li><li id="fc0d" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">y轴:对应X轴值的惩罚。</li></ul><p id="5959" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">默认情况下，逻辑回归模型的输出是输入样本为正的<strong class="ih hj"> <em class="jd">概率(用1表示)。</em> </strong>如果逻辑回归模型被训练为将邮件分类为垃圾邮件和非垃圾邮件，其中垃圾邮件(=肯定的)被指示为1，非垃圾邮件(=否定的)被指示为0，则模型<strong class="ih hj"> <em class="jd"> p </em> </strong>的输出是邮件是垃圾邮件的概率(=肯定的)。如果我们想知道邮件不是垃圾邮件的概率(=负数)，可以表示为<strong class="ih hj"> <em class="jd"> 1-p. </em> </strong></p><p id="2655" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们看看上面的log函数在逻辑回归的两个用例中是如何工作的，<em class="jd">即</em>当实际输出值为1 &amp; 0时。</p><p id="0551" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">1) <strong class="ih hj"> <em class="jd">真实输出值= 1: </em> </strong>考虑两个输入样本的模型输出为p1=0.4，p2=0.6。预计p1与p2相比应受到更多的惩罚，因为p1与p2相比远离1。如果我们看到y=-log(x)图</p><ul class=""><li id="eb1a" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">x = p1(=0.4) → y = 0.4(=-log(p1))，即p1的损失为0.4</li><li id="6085" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">x = p2(=0.6) → y = 0.2(=-log(p2))，即p2的损失为0.2</li></ul><p id="4e71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">p1上的惩罚大于p2。在这种情况下按预期工作:))</p><p id="c7ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2) <strong class="ih hj"> <em class="jd">真实输出值= 0: </em> </strong>考虑两个输入样本的模型输出为p1=0.4，p2=0.6。预计p2与p1相比应受到更多的惩罚，因为它与0.4相比离0很远。在这里，在查看图中的惩罚是什么之前，我们需要记住一件小事，<em class="jd">即</em>因为逻辑回归模型的输出是<strong class="ih hj"><em class="jd"/></strong><strong class="ih hj"><em class="jd">输入样本为正的概率，</em> </strong> p1 &amp; p2是输入样本为<strong class="ih hj">正的概率</strong>。为了从图中找到惩罚，我们需要输入样本为负的概率，因此我们需要查找惩罚的图，如<strong class="ih hj"><em class="jd">log(1-P1)&amp;log(1-p2)。</em> </strong>现在，如果我们看到y=-log(x)图，请记住上述观点</p><ul class=""><li id="8aa4" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">x = p1(=0.4) → y = 0.2(=-log(1-p1))，即p1的损失为0.2</li><li id="d0ca" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">x = p2(=0.6) → y = 0.4(=-log(1-p2))，即p2的损失为0.4</li></ul><p id="53b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">p2上的惩罚大于p1。在这种情况下也能正常工作:))</p><p id="a9de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，对于模型输出为<strong class="ih hj"> p </strong>并且真实输出值为<strong class="ih hj"> y </strong>的单个输入样本的惩罚可以计算如下</p><blockquote class="lv"><p id="d505" class="lw lx hi bd ly lz ma mb mc md me jc dx translated">如果输入样本为正(y = 1): <br/>罚值= -log(p) <br/>否则:<br/>罚值= -log(1-p)</p></blockquote><p id="4cf2" class="pw-post-body-paragraph if ig hi ih b ii mf ik il im mg io ip iq mh is it iu mi iw ix iy mj ja jb jc hb bi translated">上述if-else循环的单行等效等式可以写成</p><blockquote class="lv"><p id="4633" class="lw lx hi bd ly lz ma mb mc md me jc dx translated">惩罚= -(y*log(p) + (1-y)*log(1-p))</p></blockquote><p id="20a3" class="pw-post-body-paragraph if ig hi ih b ii mf ik il im mg io ip iq mh is it iu mi iw ix iy mj ja jb jc hb bi translated">我们来看看这个单行方程是如何等价于上面的if-else循环的:<br/> 1)当真输出值<strong class="ih hj"> <em class="jd"> y </em> </strong> = 1(正)<br/>惩罚=-(1 * log(p)+(1–1)* log(1-p))=<em class="jd">-log(p)</em><br/>2)当真输出值<strong class="ih hj"> <em class="jd"> y </em> </strong> = 0(负)<br/>惩罚=。</p><p id="c6c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">N个输入样本的对数损失函数如下所示</p><figure class="ka kb kc kd fd ke er es paragraph-image"><div role="button" tabindex="0" class="kf kg di kh bf ki"><div class="er es mk"><img src="../Images/8965cb9420ebeabf6bdd7ea5fe31d0b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AbyrIfIebNoSEYxcK4Oz5g.jpeg"/></div></div><figcaption class="kl km et er es kn ko bd b be z dx translated">yi =第I个样本的实际输出，pi =第I个样本的预测概率</figcaption></figure><p id="e41e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在结束这篇文章之前，我希望现在你</p><ul class=""><li id="7c9b" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">了解我们使用对数损失函数而不是MSE进行逻辑回归的原因</li><li id="a2b6" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">直观地理解了测井曲线损失公式及其工作原理。</li></ul><pre class="ka kb kc kd fd ml mm mn mo aw mp bi"><span id="30bd" class="ks kt hi mm b fi mq mr l ms mt">if you like this article:<br/>    clap it<br/>else:<br/>    feel free to suggest improvements &amp; ask questions</span></pre><p id="13bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">快乐的机器学习🤓🤓！！！</p><p id="ed50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">图片来源:——谷歌</em></p></div></div>    
</body>
</html>