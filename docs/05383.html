<html>
<head>
<title>Assessing BART’s syntactic abilities (and BERT’s) Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">评估BART(和BERT)的句法能力第1部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/assesing-barts-syntactic-abilities-and-bert-s-part-1-cbf0983f6ea4?source=collection_archive---------19-----------------------#2020-04-19">https://medium.com/analytics-vidhya/assesing-barts-syntactic-abilities-and-bert-s-part-1-cbf0983f6ea4?source=collection_archive---------19-----------------------#2020-04-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d056d1e75490ff6337da65e292e97ac9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wV7TUpRbW6W2Af20H4MFiA.png"/></div></div></figure><p id="2168" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">TLDR</strong>；</p><p id="d125" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在主谓一致分析中，当出现越来越多的一致吸引子时，BART的句法能力似乎比BERT下降得更快。</p><h1 id="5259" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">介绍</h1><p id="aff7" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">在本文结束时，您应该已经完成了以下工作</p><ol class=""><li id="22d2" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">窥视巴特激动人心的世界。</li><li id="19a0" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">理解如何对主谓一致进行定量分析。</li><li id="65c1" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">比较和分析了BART与BERT的性能</li><li id="121c" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">访问代码，自己玩玩，在不同的拥抱脸模型上执行这些分析。</li></ol><h1 id="f1d7" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">巴特是什么？</h1><blockquote class="lf lg lh"><p id="9935" class="iq ir li is b it iu iv iw ix iy iz ja lj jc jd je lk jg jh ji ll jk jl jm jn hb bi translated"><strong class="is hj">一种用于预处理序列间模型的去噪自动编码器。</strong></p></blockquote><p id="01a2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">是《T4》的作者BART  @ FAIR对它的定义。如果你正在阅读这篇文章，你已经了解什么是<strong class="is hj">序列到序列</strong>模型。<a class="ln lo ge" href="https://medium.com/u/677f03e54270?source=post_page-----cbf0983f6ea4--------------------------------" rel="noopener" target="_blank"> Dominic Monn </a>快速解释了什么是<strong class="is hj">去噪自动编码器</strong>这里的<a class="ae lm" href="https://towardsdatascience.com/denoising-autoencoders-explained-dbb82467fc2" rel="noopener" target="_blank">是</a>。</p><p id="ede7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了保持本文简洁，我们将只查看BART的预训练目标。BERT使用了一个掩码LM(令牌掩码)来训练编码器，其中大约15%的令牌被掩码。BART更进一步，应用了一系列噪声技术。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lp"><img src="../Images/963b860911393b120d6168c65b27bd01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HBF8mluvQycP_7pUdUOKFA.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">BART中输入文本转换的类型</figcaption></figure><p id="efc6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当与BERT及其不同预训练目标的变体进行比较时，文本填充方法在诸如SQuAD 1.1和CNN/DM的任务中优于其他方法。这种比较成立，因为它们是在类似的架构和培训周期中进行的。</p><h1 id="4cee" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">评估句法能力</h1><p id="8d2e" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">自从rnn和LSTMs大量涌现以来，研究者们一直试图评估模型的句法能力。随着不同的模型服从不同的训练目标，理解模型理解语法的程度变得越来越困难。</p><p id="497d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在本文中，我们将触及主谓一致的细微差别，并对其进行定量分析。</p><p id="17b3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">什么是主谓一致？</strong></p><p id="e44d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以这四个句子为例。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/c11eb9dc355e8cdd521b0877c5f08d54.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*aHZd7SGUkF_Ee6GTaNAV0w.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">https://arxiv.org/pdf/1611.01368.pd福</figcaption></figure><p id="09ed" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">主题是“钥匙/钥匙”。动词是“是/是”。<strong class="is hj"> a </strong>和<strong class="is hj"> d </strong>正确。有效主语动词一致的一般定义是</p><blockquote class="lf lg lh"><p id="74fb" class="iq ir li is b it iu iv iw ix iy iz ja lj jc jd je lk jg jh ji ll jk jl jm jn hb bi translated"><strong class="is hj">“单数主语(<em class="hi">她，比尔，汽车</em>)带单数动词(<em class="hi">是，去，闪耀</em>)，而复数主语带复数动词。”详细解释</strong> <a class="ae lm" href="https://www.grammarbook.com/grammar/subjectVerbAgree.asp" rel="noopener ugc nofollow" target="_blank"> <strong class="is hj">此处</strong> </a></p></blockquote><p id="a7c5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Linzen、Goldberg和Dupoux已经<a class="ae lm" href="https://arxiv.org/pdf/1611.01368.pdf" rel="noopener ugc nofollow" target="_blank">广泛地研究了一种方法来评估LSTMS和其他模型的这种特殊的句法能力。</a></p><p id="4f22" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">原始论文的目标(2016)是</strong>:在动词出现之前，模型预测动词是单数还是复数的效果如何。通过将“关键”输入到模型中，它能预测跟随它的动词是复数还是单数吗？它不关心动词本身，而只关心二元预测。这项任务被称为“数字预测”，有不同的变体，如“动词变形”、“语法性”，其中最适合这个讨论的是“语言模型”</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lz"><img src="../Images/75030d46fcb0c10a80a11b885e598a16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r6WJVO1_07cTU1P3dwbefg.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">https://arxiv.org/pdf/1611.01368.pd<a class="ae lm" href="https://arxiv.org/pdf/1611.01368.pdf" rel="noopener ugc nofollow" target="_blank">f</a></figcaption></figure><p id="c5bc" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hj">LMs的目标</strong>:</p><ol class=""><li id="5464" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">感兴趣的动词被屏蔽并输入到编码器。</li><li id="baa0" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">计算屏蔽位置的输出字的softmax。</li><li id="fb19" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">检查动词的两种形式(单数和复数)的softmax分数。较大的一个是模型在屏蔽位置中发现更有可能的一个。</li><li id="f792" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated"><strong class="is hj">如果动词的正确形式得到较高的分数，如上表所示，如果P(are) &gt; P(is)则预测是正确的。</strong></li></ol><p id="23a8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">本文中的大部分探索都是基于大约3年后由Goldberg <a class="ae lm" href="https://arxiv.org/pdf/1901.05287.pdf" rel="noopener ugc nofollow" target="_blank">在这里</a>完成的语言模型扩展。该分析将测试查询分为4类。1、2、3和4主语和动词之间的一致吸引子。</p><h1 id="a2e8" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">什么是协议吸引子？</h1><p id="5bae" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">在前面的简单例子中——“钥匙在桌子上”。动词“is”紧跟在主语“key”之后。他们俩之间没有言语。然而，一般的英语句子并不总是这么乐观。你可能会在主语和动词之间遇到多个单词、名词、代词等。</p><blockquote class="lf lg lh"><p id="ed28" class="iq ir li is b it iu iv iw ix iy iz ja lj jc jd je lk jg jh ji ll jk jl jm jn hb bi translated">柜子的钥匙在桌子上。</p></blockquote><ol class=""><li id="8f1d" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">目标动词:是</li><li id="07b4" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">主题:关键字</li><li id="8609" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">协议吸引人:内阁(名词)</li></ol><p id="2b9f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">“cabinet”是一个出现在目标动词和它的主语之间的名词，可能会误导模型选择动词的复数或单数形式。在这种情况下，协议吸引子“cabinet”变成了单数形式。即使模型已经错误地学会将动词“is”与“cabinet”相关联，预测也不会改变，因为cabinet和key都是单数。如果输入句子被修改为以下内容(机柜已更改为机柜)</p><blockquote class="lf lg lh"><p id="c53b" class="iq ir li is b it iu iv iw ix iy iz ja lj jc jd je lk jg jh ji ll jk jl jm jn hb bi translated">橱柜的钥匙在桌子上。</p></blockquote><p id="445a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这种情况下，如果模型没有很好地理解句子的语法和依存结构，它可能会预测答案为“橱柜的钥匙在桌子上”，这在语法上是不正确的。</p><p id="d210" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当我们讨论依赖结构的时候，你可以看到“is”是如何与单词“key”相关联的。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/13ab0c36ef9ddde22df80902d21f49a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9pcTu0qb5Dn86WCd94y7Ug.png"/></div></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">用SpaCy生成的依赖关系解析树。</figcaption></figure><p id="749b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该树是使用语言规则生成的，模型的句法分析的总体目标是尝试并理解深度学习模型是否能够捕捉这种依赖性作为其网络参数的一部分。主语和动词的一致是许多此类分析中的一种。</p><p id="cda6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">既然你已经理解了什么是一致吸引子，那么动词和主语之间可能出现的这种潜在候选者的数量就是一致吸引子的程度。一致吸引子的数量越多，模型预测动词的复数就越复杂。</p><p id="9c7e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在他对BERT中句法的分析中，goldberg考虑了1、2、3和4度的一致吸引子。我们将同样扩展到BART，并尝试将其性能与BERT进行比较。我们将首先看到结果，然后看看代码。</p><h1 id="1604" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">BERT vs BART句法分析</h1><p id="e578" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">我们比较了3种模型。</p><ol class=""><li id="a540" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated"><strong class="is hj">伯特基地脱壳</strong></li><li id="eef9" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated"><strong class="is hj">伯特大号未装箱</strong></li><li id="cdfb" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated"><strong class="is hj">巴特大</strong></li></ol><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/b6389a1048f567b7518c2a96bf3e696d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*avouiOl74IMiWAwYAtaQVg.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">巴特对伯特的表现。</figcaption></figure><p id="bd8a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">该数据集包含总共29，985个句子，其中1个吸引子包含约24200个句子，4个吸引子包含约270个句子。尽管BART和BERT的评估是在同一数据集上进行的，但样本数量略有不同。在计算准确性度量时，并没有考虑大约29，000个句子中的每一个句子。</p><p id="9123" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">测试BERT和BART的样本数量不同，因为BERT和BART的词汇不同。自从巴特和伯特使用BPE(字节对编码)以来，一些动词被记号赋予器分解了。这使得屏蔽词的预测不可能与输入词整体匹配。例如，单词“locates”被BART分解为“loc”和“ates”。BART的tokenizer遗漏了数据集中的大约180个样本，BERT的遗漏了330个样本。</p><p id="d580" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">goldberg返回的原始结果是，这或多或少是我用BERT和BART的批处理方法得到的。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es mc"><img src="../Images/1b0c087696621f49da581592acefc51f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*tSPZc0_l6FKfFK8dp0Do-A.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated">根据<a class="ae lm" href="https://arxiv.org/pdf/1901.05287.pdf" rel="noopener ugc nofollow" target="_blank">戈德堡</a>的报告，伯特基数与大基数之比</figcaption></figure><p id="907c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意:我使用了拥抱脸的最新版本的BERT，在Goldberg进行实验的数据集中有几个样本的动词的两个屈折相同(例如，lie和lie而不是lie和lies)，可能是一个打字错误。我在实验中忽略了这些情况。</p><ol class=""><li id="f899" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">BERT-base无壳:12层，768隐藏，12头，110M参数</li><li id="2391" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">BERT-大型无外壳:24层，1024隐藏，16头，340M参数</li><li id="6859" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">BART-large: <a class="ae lm" href="https://github.com/pytorch/fairseq/tree/master/examples/bart" rel="noopener ugc nofollow" target="_blank"> 400M </a>参数，BART包含的参数比同等大小的BERT模型多大约10%</li></ol><h1 id="b764" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">分析</h1><p id="51bc" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">BART模型的预测精度与BERT相当，但随着吸引子数量的增加而下降。对BART失败的动词也有了更详细的理解。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es md"><img src="../Images/9d27245560a4ac9506df6cdd7822d59c.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*mwJ4Qlt_8cU1zKBfh0Bu9w.png"/></div></figure><p id="9765" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在colab中，人们还可以找到这个特定动词无法映射到正确数字的句子。这将有望允许对准确性非常低的单词进行更精细的研究，例如上面的那些单词。</p><h1 id="44f6" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">代码和实验</h1><p id="6895" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">谷歌colab笔记本可以在<a class="ae lm" href="https://colab.research.google.com/drive/1Ce8Cy6U7tsuhpRbH6uAoIEdczB6uhJ5e" rel="noopener ugc nofollow" target="_blank">这里</a>找到，github回购<a class="ae lm" href="https://github.com/naveenjafer/bart-bert-syntactic-analysis" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h1 id="44d7" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">整理思绪。</h1><p id="553d" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">仅在4年前，当涉及到在多个吸引子存在的情况下主语动词一致的任务时，语言模型与LSTMs相比表现糟糕。由Linzen、Duopox和Goldberg构建的用于分析这些结果的LM在存在吸引子的情况下比随机猜测表现得更差。即使是拥有大约12亿参数的SOTA<a class="ae lm" href="https://research.google/pubs/pub45446/" rel="noopener ugc nofollow" target="_blank">LMs</a>中的一个，也没有帮助提高语言模型的句法能力，没有明确的训练目标来预测动词的数量。</p><figure class="lq lr ls lt fd ij er es paragraph-image"><div class="er es me"><img src="../Images/ca5bc7165d7df28488a6f9e45ad57220.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*xHbiEUx9kTJ_D3an825ahQ.png"/></div><figcaption class="lu lv et er es lw lx bd b be z dx translated"><a class="ae lm" href="https://arxiv.org/pdf/1611.01368.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1611.01368.pdf</a></figcaption></figure><p id="019e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以“语法性”、“数字预测”和“动词变化”为目标训练的LSTMs仅在4年前胜过LMs。</p><p id="47ff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在过去的4年里，这一领域取得了巨大的发展和进步。</p><h1 id="d670" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">前进</h1><p id="7766" class="pw-post-body-paragraph iq ir hi is b it km iv iw ix kn iz ja jb ko jd je jf kp jh ji jj kq jl jm jn hb bi translated">这一部分总结了作为句法分析手段的主谓一致分析。在第二部分中，我们有来自马文和林曾作品的分析方法。</p><p id="68e1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">同时，可以通过HuggingFace随意扩展这个代码，在其他Trasnformer模型上运行类似的分析。你可能需要对面具和其他特定职业的细微差别进行细微的调整。玩得开心。</p><h1 id="f34f" class="jo jp hi bd jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl bi translated">参考</h1><ol class=""><li id="f830" class="kr ks hi is b it km ix kn jb mf jf mg jj mh jn kw kx ky kz bi translated"><a class="ae lm" href="https://arxiv.org/pdf/1901.05287.pdf" rel="noopener ugc nofollow" target="_blank">评估伯特的句法能力</a></li><li id="9661" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated"><a class="ae lm" href="https://arxiv.org/pdf/1611.01368.pdf" rel="noopener ugc nofollow" target="_blank">评估LSTMs学习语法敏感依赖性的能力</a></li><li id="ce17" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated"><a class="ae lm" href="https://arxiv.org/pdf/1910.13461.pdf" rel="noopener ugc nofollow" target="_blank">巴特</a></li><li id="a937" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated"><a class="ae lm" href="https://huggingface.co/transformers/model_doc/bart.html" rel="noopener ugc nofollow" target="_blank">抱脸巴特</a>文档</li><li id="b076" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated"><a class="ae lm" href="https://www.grammarbook.com/grammar/subjectVerbAgree.asp" rel="noopener ugc nofollow" target="_blank">主语动词一致</a></li></ol></div></div>    
</body>
</html>