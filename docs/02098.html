<html>
<head>
<title>High School Math Performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高中数学成绩</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/high-school-math-performance-523d5839d7d7?source=collection_archive---------5-----------------------#2019-11-30">https://medium.com/analytics-vidhya/high-school-math-performance-523d5839d7d7?source=collection_archive---------5-----------------------#2019-11-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/c708d67596ee1881478f458df956ba28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7cC7zrsn1tQ2WL4z"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">斯文·米克在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="0f1c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">简介</strong></p><p id="7392" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在数学教育领域，大学和教育工作者面临的一个主要问题是，学生在数学方面没有达到令人满意的水平，也没有以令人满意的速度取得成功。大学和教育工作者抱怨学生的高失败率、退学率和退学率。这对学生来说是一个问题，因为数学成绩差阻碍了他们追求学位和职业。这对大学和教育工作者来说是一个问题，因为这意味着大学或教育工作者没有成功地教授学生，没有留住学生，没有满足学生的需求——这些问题损害了大学和教育工作者的盈利能力和吸引力。</p><p id="8a73" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们能够获得一些关于哪些因素最有助于或损害学生数学成绩的见解，我们就有可能解决上述问题。如果我们能够产生预测模型，预测学生是否会通过或失败，预测学生在数学评估中的数字分数，预测学生的整体实力和前途，那么大学和教育工作者将能够使用这些模型更好地将学生置于适当的能力水平，更好地选择录取学生，并更好地了解可以改善的因素，以帮助学生取得成功。</p><p id="8ab3" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本文中，我们将对代表两所葡萄牙高中学生数学成绩的数据集进行数据科学和机器学习。数据集可以在本文末尾提供的链接中找到。</p><p id="6331" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据文件用分号而不是逗号分隔。我用逗号代替了分号。然后，将所有内容复制并粘贴到记事本中。然后，使用以下链接中的步骤转换为csv文件:</p><p id="a63e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://knowledgebase.constantcontact.com/articles/KnowledgeBase/6269-convert-a-text-file-to-an-excel-file?lang=en_US" rel="noopener ugc nofollow" target="_blank">https://knowledge base . constant contact . com/articles/knowledge base/6269-convert-a-text-file-to-a-excel-file？lang=en_US </a></p><p id="5ec8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我有一个很好的csv文件。</p><p id="036c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">共有30个属性，包括学生年龄、父母的教育程度、父母的工作、每周学习时间、缺席次数、以往课程失败次数等。有一年级、二年级和三年级；这些由G1、G2和G3表示。分数从0到20不等。G1和G2可以用作输入特征，而G3将是主要的目标输出。</p><p id="f901" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">一些属性是顺序的，一些是二进制的是-否，一些是数字的，一些是名义的。我们确实需要做一些数据预处理。对于二进制的是-否属性，我将使用0和1对它们进行编码。属性famrel、freetime、goout、Dalc、Walc和health是有序的；这些值的范围从1到5。属性Medu、Fedu、traveltime、studytime、failures也是有序的；值的范围从0到4或1到4。属性“缺席”是一个计数属性；值的范围从0到93。属性sex、school、address、Pstatus、Mjob、Fjob、guardian、famsize、reason是名义上的。对于名义属性，我们可以使用一键编码。年龄、G1、G2和G3属性可视为区间属性。</p><p id="3d30" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我一次性编码了每个名义属性，一次一个。我每次都将数据帧导出为csv文件，并在导出过程中重新标记列。最后，我对列进行了重新排序。</p><p id="d44e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是python代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="337b" class="kc kd hi jy b fi ke kf l kg kh">import numpy as np</span><span id="04e3" class="kc kd hi jy b fi ki kf l kg kh">import pandas as pd</span><span id="ea10" class="kc kd hi jy b fi ki kf l kg kh">dataset = pd.read_csv(‘C:\\Users\\ricky\\Downloads\\studentmath.csv’)</span><span id="26be" class="kc kd hi jy b fi ki kf l kg kh">X = dataset.iloc[:,:-1].values</span><span id="7ec1" class="kc kd hi jy b fi ki kf l kg kh">Y = dataset.iloc[:,32].values</span><span id="e68f" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.preprocessing import LabelEncoder, OneHotEncoder</span><span id="a423" class="kc kd hi jy b fi ki kf l kg kh">labelencoder_X = LabelEncoder()</span><span id="8e98" class="kc kd hi jy b fi ki kf l kg kh"># Encoding binary yes-no attributes</span><span id="7276" class="kc kd hi jy b fi ki kf l kg kh">X[:,15] = labelencoder_X.fit_transform(X[:,15])</span><span id="fe15" class="kc kd hi jy b fi ki kf l kg kh">X[:,16] = labelencoder_X.fit_transform(X[:,16])</span><span id="b8ab" class="kc kd hi jy b fi ki kf l kg kh">X[:,17] = labelencoder_X.fit_transform(X[:,17])</span><span id="f3d0" class="kc kd hi jy b fi ki kf l kg kh">X[:,18] = labelencoder_X.fit_transform(X[:,18])</span><span id="ea3c" class="kc kd hi jy b fi ki kf l kg kh">X[:,19] = labelencoder_X.fit_transform(X[:,19])</span><span id="9a11" class="kc kd hi jy b fi ki kf l kg kh">X[:,20] = labelencoder_X.fit_transform(X[:,20])</span><span id="e207" class="kc kd hi jy b fi ki kf l kg kh">X[:,21] = labelencoder_X.fit_transform(X[:,21])</span><span id="dd97" class="kc kd hi jy b fi ki kf l kg kh">X[:,22] = labelencoder_X.fit_transform(X[:,22])</span><span id="2898" class="kc kd hi jy b fi ki kf l kg kh"># Encoding nominal attributes</span><span id="07b3" class="kc kd hi jy b fi ki kf l kg kh">X[:,0] = labelencoder_X.fit_transform(X[:,0])</span><span id="e052" class="kc kd hi jy b fi ki kf l kg kh">X[:,1] = labelencoder_X.fit_transform(X[:,1])</span><span id="888e" class="kc kd hi jy b fi ki kf l kg kh">X[:,3] = labelencoder_X.fit_transform(X[:,3])</span><span id="5ac4" class="kc kd hi jy b fi ki kf l kg kh">X[:,4] = labelencoder_X.fit_transform(X[:,4])</span><span id="c6e3" class="kc kd hi jy b fi ki kf l kg kh">X[:,5] = labelencoder_X.fit_transform(X[:,5])</span><span id="f38b" class="kc kd hi jy b fi ki kf l kg kh">X[:,8] = labelencoder_X.fit_transform(X[:,8])</span><span id="ef0e" class="kc kd hi jy b fi ki kf l kg kh">X[:,9] = labelencoder_X.fit_transform(X[:,9])</span><span id="90f6" class="kc kd hi jy b fi ki kf l kg kh">X[:,10] = labelencoder_X.fit_transform(X[:,10])</span><span id="5e1f" class="kc kd hi jy b fi ki kf l kg kh">X[:,11] = labelencoder_X.fit_transform(X[:,11])</span><span id="2036" class="kc kd hi jy b fi ki kf l kg kh">onehotencoder = OneHotEncoder(categorical_features = [0])</span><span id="f979" class="kc kd hi jy b fi ki kf l kg kh">X = onehotencoder.fit_transform(X).toarray()</span><span id="b208" class="kc kd hi jy b fi ki kf l kg kh">from pandas import DataFrame</span><span id="34ae" class="kc kd hi jy b fi ki kf l kg kh">df = DataFrame(X)</span><span id="8c48" class="kc kd hi jy b fi ki kf l kg kh">export_csv = df.to_csv (r’C:\Users\Ricky\Downloads\highschoolmath.csv’, index = None, header=True)</span></pre><p id="08f0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用seaborn，我们可以看到一些可视化效果。这是“缺席”的柱状图。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kj"><img src="../Images/a27e3b5d04951bc6b05934b5ccca6241.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*G-D8u-goqafC14K1uI6GYA.png"/></div></figure><p id="a900" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">请注意，大多数学生的缺课次数很少，随着缺课次数的增加，缺课次数越来越少。</p><p id="0e69" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看G1、G2和G3等级的分布:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kj"><img src="../Images/3611082fd03b121fa928fc1e84248826.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*amsHLtoIEQ6iasf8i0LWUg.png"/></div></figure><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kj"><img src="../Images/7dadc9b90df251ab8d2b440fec410c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*OrxTK-5M4AEwzKrFDbsdGg.png"/></div></figure><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kj"><img src="../Images/1a52e4e59d76c7279de6280467fdc222.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*26Zid6mP7mCkMQMYuwDuUg.png"/></div></figure><p id="e2b9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于一年级的G1来说，分数看起来呈正态分布。对于G2和G3，如果看曲线，有两个极大值；有一定数量的学生似乎分数很低，接近0。</p><p id="0ee2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">有人可能想知道年龄如何影响G1、G2和G3的分数。以下是《时代》杂志对G1的一些预测:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kk"><img src="../Images/2cfaf0e1fbf146aa7ea06abf0ec2b9f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*FZvohfbW4P6AySe3-0dcYQ.png"/></div></figure><p id="8e51" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">中位数得分在16岁时出现局部最大值，一直下降到19岁，然后在20岁时急剧上升。同样的事情也可以在age与G2的对比中看到:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kl"><img src="../Images/64da34841c0b7461b63836254ebcaef3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*TmvjUAS7m6z6Pfq1FcN1dA.png"/></div></figure><p id="d27e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在第三年，16岁时的本地最大值消失，但20岁时的急剧增加仍然存在:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kl"><img src="../Images/9642ad970f4705e91766ad66d748c92b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*5l3tQ2QwluLlcCtLih7ZEg.png"/></div></figure><p id="1c4e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还要注意的是，在19岁和20岁时，分数范围变得更窄。</p><p id="a53d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们也可能想知道是男性还是女性表现更好。在所有三年中，男性的平均分数高于女性。例如，这是男性和G3的箱线图:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kl"><img src="../Images/d7d44ad86ad1c08ed21bf4a689fabd08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*hoFY_2RwqIOKHAzvBxp0QA.png"/></div></figure><p id="8ca2" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们将线性回归作为G1的函数应用于模型G3，我们得到以下结果:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es km"><img src="../Images/8110878a31ff9e7a46b71674b04c926f.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*0OzEPFl64Dkz_UZqr4nPlQ.png"/></div></figure><p id="d042" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">随着G1分数的增加，G3分数也增加。类似地，我们可以看到G3和G2之间的线性关系:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es km"><img src="../Images/cdb62f76fa4c7b197afa3fb547cf20f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*VaiDiO3gvJ9u1z1r_pnmGg.png"/></div></figure><p id="aa2a" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是python代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="31d5" class="kc kd hi jy b fi ke kf l kg kh">import numpy as np</span><span id="64fe" class="kc kd hi jy b fi ki kf l kg kh">import pandas as pd</span><span id="104a" class="kc kd hi jy b fi ki kf l kg kh">dataset = pd.read_csv(‘C:\\Users\\ricky\\Downloads\\studentmathdummified.csv’)</span><span id="1b6e" class="kc kd hi jy b fi ki kf l kg kh">X = dataset.iloc[:,:-1].values</span><span id="1f88" class="kc kd hi jy b fi ki kf l kg kh">Y = dataset.iloc[:,50].values</span><span id="f136" class="kc kd hi jy b fi ki kf l kg kh">import matplotlib.pyplot as plt</span><span id="3d92" class="kc kd hi jy b fi ki kf l kg kh">import seaborn as sns</span><span id="8ff8" class="kc kd hi jy b fi ki kf l kg kh">%matplotlib inline</span><span id="3c58" class="kc kd hi jy b fi ki kf l kg kh">plt.rcParams[‘figure.figsize’]=8,4</span><span id="c7fb" class="kc kd hi jy b fi ki kf l kg kh">import warnings</span><span id="cafc" class="kc kd hi jy b fi ki kf l kg kh">warnings.filterwarnings(‘ignore’)</span><span id="39c4" class="kc kd hi jy b fi ki kf l kg kh">#Distribution</span><span id="4d90" class="kc kd hi jy b fi ki kf l kg kh">vis1 = sns.distplot(dataset[“Fedu”])</span><span id="1fb3" class="kc kd hi jy b fi ki kf l kg kh">#Boxplots</span><span id="1cc9" class="kc kd hi jy b fi ki kf l kg kh">vis2 = sns.boxplot(data=dataset, x=”Male”,y=”G3")</span><span id="756a" class="kc kd hi jy b fi ki kf l kg kh">#Linear regression model</span><span id="4cd2" class="kc kd hi jy b fi ki kf l kg kh">vis3 = sns.lmplot(x=”G2", y=”G3", data=dataset)</span></pre></div><div class="ab cl kn ko gp kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="hb hc hd he hf"><p id="b509" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">简单线性回归</strong></p><p id="36b4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，让我们对数据集应用一些机器学习。很有可能G3以线性方式依赖于G1。我们可以更清楚地看到这一点，应用简单的线性回归，以G3为因变量，G1为自变量。</p><p id="3696" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，我们让X是由数据集studentmathdummified.csv的前50列组成的矩阵，然后，让Y是数据集的最后一列—即G3。</p><p id="8c47" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">然后，我们将数据集分成训练集和测试集。我们对我们的训练集应用线性回归来训练我们的简单线性回归模型；然后，我们将该模型应用于我们的测试集，我们可以将预测的Y值与测试集的实际Y值进行比较。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="a50e" class="kc kd hi jy b fi ke kf l kg kh">Here is the python code:</span><span id="d6bc" class="kc kd hi jy b fi ki kf l kg kh">#Simple Linear Regression</span><span id="e95d" class="kc kd hi jy b fi ki kf l kg kh">#Importing the libraries</span><span id="842f" class="kc kd hi jy b fi ki kf l kg kh">import numpy as np</span><span id="97cb" class="kc kd hi jy b fi ki kf l kg kh">import pandas as pd</span><span id="5ec4" class="kc kd hi jy b fi ki kf l kg kh">import matplotlib.pyplot as plt</span><span id="70de" class="kc kd hi jy b fi ki kf l kg kh">#Importing the dataset</span><span id="71dc" class="kc kd hi jy b fi ki kf l kg kh">dataset = pd.read_csv(“studentmathdummified.csv”)</span><span id="d770" class="kc kd hi jy b fi ki kf l kg kh">X = dataset.iloc[:,:-1].values</span><span id="d967" class="kc kd hi jy b fi ki kf l kg kh">Y = dataset.iloc[:,-1].values</span><span id="4985" class="kc kd hi jy b fi ki kf l kg kh">#Splitting the dataset into the Training set and Test set</span><span id="38ec" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.model_selection import train_test_split</span><span id="31a7" class="kc kd hi jy b fi ki kf l kg kh">X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state = 0)</span><span id="b38d" class="kc kd hi jy b fi ki kf l kg kh">#Fitting Simple Linear Regression to the Training set</span><span id="86e1" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.linear_model import LinearRegression</span><span id="0ee6" class="kc kd hi jy b fi ki kf l kg kh">regressor = LinearRegression()</span><span id="398a" class="kc kd hi jy b fi ki kf l kg kh">regressor.fit(X_train[:,48:49],Y_train)</span><span id="1cec" class="kc kd hi jy b fi ki kf l kg kh">#Predicting the Test set results</span><span id="8358" class="kc kd hi jy b fi ki kf l kg kh">Y_pred = regressor.predict(X_test[:,48:49])</span><span id="6f8e" class="kc kd hi jy b fi ki kf l kg kh">X_train[:,48:49]</span></pre><p id="c07f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看在我们的训练集上训练它所得到的线性模型。它看起来是这样的:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kl"><img src="../Images/60379a8dd86e1b477f4ddc99c7fd200d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*xDEH-j6sc3jmwgfnxkkYRw.png"/></div></figure><p id="6df4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">红色是我们的训练集G3值与训练集G1值的散点图。蓝线是我们的线性回归模型。</p><p id="b6eb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是用于生成图表的python代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="3c61" class="kc kd hi jy b fi ke kf l kg kh">#Visualizing the Training set results</span><span id="a7d5" class="kc kd hi jy b fi ki kf l kg kh">plt.scatter(X_train[:,48:49], Y_train, color = ‘red’)</span><span id="222d" class="kc kd hi jy b fi ki kf l kg kh">plt.plot(X_train[:,48:49],regressor.predict(X_train[:,48:49]), color = ‘blue’)</span><span id="0f7d" class="kc kd hi jy b fi ki kf l kg kh">plt.title(‘G3 vs G1 (Training set)’)</span><span id="4937" class="kc kd hi jy b fi ki kf l kg kh">plt.xlabel(‘G1’)</span><span id="a26b" class="kc kd hi jy b fi ki kf l kg kh">plt.ylabel(‘G3’)</span><span id="c52b" class="kc kd hi jy b fi ki kf l kg kh">plt.show()</span></pre><p id="6bf1" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，让我们看看线性回归模型在测试集上的表现。这是测试集G3值与测试集G1值的散点图，红色表示线性模型，蓝色表示线性模型。</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kl"><img src="../Images/739186db82f77900f4ac374e43508a58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*RsSTq6Wx2yZBlGNlXJ5J2Q.png"/></div></figure><p id="a3fd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如您所看到的，线性回归模型在测试集上表现得非常好。</p><p id="e57d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是生成图表的python代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="97bb" class="kc kd hi jy b fi ke kf l kg kh">#Visualizing the Test set results</span><span id="8626" class="kc kd hi jy b fi ki kf l kg kh">plt.scatter(X_test[:,48:49], Y_test, color = ‘red’)</span><span id="a7ce" class="kc kd hi jy b fi ki kf l kg kh">plt.plot(X_train[:,48:49],regressor.predict(X_train[:,48:49]), color = ‘blue’)</span><span id="1db3" class="kc kd hi jy b fi ki kf l kg kh">plt.title(‘G3 vs G1 (Test set)’)</span><span id="52b2" class="kc kd hi jy b fi ki kf l kg kh">plt.xlabel(‘G1’)</span><span id="eda7" class="kc kd hi jy b fi ki kf l kg kh">plt.ylabel(‘G3’)</span><span id="ddb1" class="kc kd hi jy b fi ki kf l kg kh">plt.show()</span></pre><p id="8468" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们可以看到G3和G2之间非常相似的关系。我们可以应用简单的线性回归，以G3为因变量，G2为自变量。结果如下:</p><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kk"><img src="../Images/a5528815c5233b3d3fb7407eecdef278.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*GtH4YyzDHIjM9H4nwVpgUg.png"/></div></figure><figure class="jt ju jv jw fd ij er es paragraph-image"><div class="er es kk"><img src="../Images/ad04b7eec5a7dd3913165265488c603a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*WttS-zEdVIQmDcrQ3htxmA.png"/></div></figure></div><div class="ab cl kn ko gp kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="hb hc hd he hf"><p id="0161" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">多元线性回归</strong></p><p id="b194" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">到目前为止，我们已经应用了使用单个变量(G1或G2)的线性回归。也许其他独立变量对G3有影响。为了说明这一点，我们可以应用多元线性回归，将所有的独立变量都考虑在内。</p><p id="403b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，为了避免哑变量陷阱，我删除了GP、男、urban、LE3、Apart、mother_at_home、father_at_home、reason_course、guardian_other这几列。我将新数据集命名为“dataset_trap”。然后，我用dataset_trap定义了X和Y。我将数据集分为训练集和测试集，在训练集上训练多元线性回归模型，并将模型应用于X_test。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="104b" class="kc kd hi jy b fi ke kf l kg kh">Here is the python code:</span><span id="e989" class="kc kd hi jy b fi ki kf l kg kh">#Multiple Linear Regression</span><span id="847a" class="kc kd hi jy b fi ki kf l kg kh">#Importing the libraries</span><span id="8c45" class="kc kd hi jy b fi ki kf l kg kh">import numpy as np</span><span id="65d8" class="kc kd hi jy b fi ki kf l kg kh">import pandas as pd</span><span id="143e" class="kc kd hi jy b fi ki kf l kg kh">import matplotlib.pyplot as plt</span><span id="0178" class="kc kd hi jy b fi ki kf l kg kh">#Importing the dataset</span><span id="e4bc" class="kc kd hi jy b fi ki kf l kg kh">dataset = pd.read_csv(“studentmathdummified.csv”)</span><span id="9ef0" class="kc kd hi jy b fi ki kf l kg kh">#Avoiding the dummy variable trap</span><span id="0e1e" class="kc kd hi jy b fi ki kf l kg kh">#Dropping GP, Male, urban,LE3, Apart,mother_at_home, father_at_home, reason_course, guardian_other</span><span id="b492" class="kc kd hi jy b fi ki kf l kg kh">dataset_trap = dataset.drop(dataset.columns[[0,2,4,6,8,10,15,20,26]],axis=1)</span><span id="fa4f" class="kc kd hi jy b fi ki kf l kg kh">#Define X and Y using dataset_trap</span><span id="9e24" class="kc kd hi jy b fi ki kf l kg kh">X = dataset_trap.iloc[:,:-1].values</span><span id="c862" class="kc kd hi jy b fi ki kf l kg kh">Y = dataset_trap.iloc[:,-1].values</span><span id="930f" class="kc kd hi jy b fi ki kf l kg kh">#Splitting the dataset into the Training set and Test set</span><span id="5efc" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.model_selection import train_test_split</span><span id="bed2" class="kc kd hi jy b fi ki kf l kg kh">X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state = 0)</span><span id="ad2a" class="kc kd hi jy b fi ki kf l kg kh">#Fitting Multiple Linear Regression to the Training set</span><span id="3f88" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.linear_model import LinearRegression</span><span id="35bb" class="kc kd hi jy b fi ki kf l kg kh">regressor = LinearRegression()</span><span id="4ab1" class="kc kd hi jy b fi ki kf l kg kh">regressor.fit(X_train,Y_train)</span><span id="7199" class="kc kd hi jy b fi ki kf l kg kh">#Predicting the Test set results</span><span id="499f" class="kc kd hi jy b fi ki kf l kg kh">Y_pred = regressor.predict(X_test)</span></pre><p id="2a07" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">将预测的Y值与测试集Y值进行比较，该模型做得相当好，但不是非常好。也许我们可以通过只包含对G3有显著影响的属性来获得更好的性能。我们可以通过执行反向消除来实现这一点。如果我们这样做，对p值使用0.05的阈值，我们最终得到属性年龄、famrel、缺勤、G1和G2作为我们的最优属性集。学生的年龄、家庭关系的质量、缺课的次数以及第一年和第二年的成绩被认为是最重要的因素。</p><p id="6b17" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">下面是执行反向消除的python代码。</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="2a68" class="kc kd hi jy b fi ke kf l kg kh">#Performing backward elimination</span><span id="497f" class="kc kd hi jy b fi ki kf l kg kh">import statsmodels.formula.api as sm</span><span id="bd42" class="kc kd hi jy b fi ki kf l kg kh">X = np.append(arr = np.ones((395,1)).astype(int), values = X, axis = 1)</span><span id="b4cd" class="kc kd hi jy b fi ki kf l kg kh">X_opt = X[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41]]</span><span id="aaf5" class="kc kd hi jy b fi ki kf l kg kh">regressor_OLS = sm.OLS(endog = Y, exog = X_opt).fit()</span><span id="a2df" class="kc kd hi jy b fi ki kf l kg kh">regressor_OLS.summary()</span></pre><p id="5534" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们删除了X_opt中对应于p值大于0.05的自变量的列。然后，我们使用新的X_opt再次执行以下代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="6e05" class="kc kd hi jy b fi ke kf l kg kh">X_opt = X[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41]]</span><span id="05a0" class="kc kd hi jy b fi ki kf l kg kh">regressor_OLS = sm.OLS(endog = Y, exog = X_opt).fit()</span><span id="dba8" class="kc kd hi jy b fi ki kf l kg kh">regressor_OLS.summary()</span></pre><p id="4e46" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们重复这个过程，直到所有自变量的p值都低于0.05。我们最终会得到:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="9c86" class="kc kd hi jy b fi ke kf l kg kh">X_opt = X[:, [19,33,39,40,41]]</span><span id="c651" class="kc kd hi jy b fi ki kf l kg kh">regressor_OLS = sm.OLS(endog = Y, exog = X_opt).fit()</span><span id="2308" class="kc kd hi jy b fi ki kf l kg kh">regressor_OLS.summary()</span></pre><p id="544c" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">列19、33、39、40、41对应于属性年龄、famrel、缺勤、G1和G2。</p><p id="365b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们也可以使用以下代码来自动执行反向消除，而不是手动执行反向消除:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="562a" class="kc kd hi jy b fi ke kf l kg kh">import statsmodels.formula.api as sm</span><span id="e49f" class="kc kd hi jy b fi ki kf l kg kh">def backwardElimination(x, sl):</span><span id="74fb" class="kc kd hi jy b fi ki kf l kg kh">numVars = len(x[0])</span><span id="8dde" class="kc kd hi jy b fi ki kf l kg kh">for i in range(0, numVars):</span><span id="1fb4" class="kc kd hi jy b fi ki kf l kg kh">regressor_OLS = sm.OLS(Y, x).fit()</span><span id="96e0" class="kc kd hi jy b fi ki kf l kg kh">maxVar = max(regressor_OLS.pvalues).astype(float)</span><span id="3a50" class="kc kd hi jy b fi ki kf l kg kh">if maxVar &gt; sl:</span><span id="0b5d" class="kc kd hi jy b fi ki kf l kg kh">for j in range(0, numVars — i):</span><span id="98f3" class="kc kd hi jy b fi ki kf l kg kh">if (regressor_OLS.pvalues[j].astype(float) == maxVar):</span><span id="219b" class="kc kd hi jy b fi ki kf l kg kh">x = np.delete(x, j, 1)</span><span id="3aba" class="kc kd hi jy b fi ki kf l kg kh">regressor_OLS.summary()</span><span id="bf90" class="kc kd hi jy b fi ki kf l kg kh">return x</span><span id="baab" class="kc kd hi jy b fi ki kf l kg kh">SL = 0.05</span><span id="fe8c" class="kc kd hi jy b fi ki kf l kg kh">X_opt = X[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41]]</span><span id="9b12" class="kc kd hi jy b fi ki kf l kg kh">X_Modeled = backwardElimination(X_opt, SL)</span><span id="89e9" class="kc kd hi jy b fi ki kf l kg kh">regressor_OLS = sm.OLS(endog = Y, exog = X_Modeled).fit()</span><span id="85f7" class="kc kd hi jy b fi ki kf l kg kh">regressor_OLS.summary()</span></pre></div><div class="ab cl kn ko gp kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="hb hc hd he hf"><p id="8b65" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> SVR回归</strong></p><p id="3845" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本节中，我们将使用高斯核执行支持向量回归。鉴于我们之前的见解，即最重要的属性是年龄、家庭成员、缺勤、G1和G2，我在训练集上使用这些属性训练了一个SVR模型。我对X_train、X_test和Y_train进行了特征缩放。然后我比较了Y_test和Y_pred。这场表演令人印象深刻。</p><p id="579e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是python代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="5c91" class="kc kd hi jy b fi ke kf l kg kh">#SVR Regression</span><span id="7e7e" class="kc kd hi jy b fi ki kf l kg kh">#Importing the libraries</span><span id="59e3" class="kc kd hi jy b fi ki kf l kg kh">import numpy as np</span><span id="04f7" class="kc kd hi jy b fi ki kf l kg kh">import pandas as pd</span><span id="059d" class="kc kd hi jy b fi ki kf l kg kh">import matplotlib.pyplot as plt</span><span id="fda6" class="kc kd hi jy b fi ki kf l kg kh">#Importing the dataset</span><span id="56ad" class="kc kd hi jy b fi ki kf l kg kh">dataset = pd.read_csv(“studentmathdummified.csv”)</span><span id="8c2f" class="kc kd hi jy b fi ki kf l kg kh">#Avoiding the dummy variable trap</span><span id="d4cd" class="kc kd hi jy b fi ki kf l kg kh">#Dropping GP, Male, urban,LE3, Apart,mother_at_home, father_at_home, reason_course, guardian_other</span><span id="57d7" class="kc kd hi jy b fi ki kf l kg kh">dataset_trap = dataset.drop(dataset.columns[[0,2,4,6,8,10,15,20,26]],axis=1)</span><span id="7479" class="kc kd hi jy b fi ki kf l kg kh">#Define X and Y using dataset_trap</span><span id="fa13" class="kc kd hi jy b fi ki kf l kg kh">X = dataset_trap.iloc[:,:-1].values</span><span id="d296" class="kc kd hi jy b fi ki kf l kg kh">Y = dataset_trap.iloc[:,-1].values</span><span id="54ac" class="kc kd hi jy b fi ki kf l kg kh">#Splitting the dataset into the Training set and Test set</span><span id="3efd" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.model_selection import train_test_split</span><span id="ee42" class="kc kd hi jy b fi ki kf l kg kh">X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state = 0)</span><span id="5124" class="kc kd hi jy b fi ki kf l kg kh">#Feature Scaling</span><span id="5c1b" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.preprocessing import StandardScaler</span><span id="c067" class="kc kd hi jy b fi ki kf l kg kh">sc_X = StandardScaler()</span><span id="13fe" class="kc kd hi jy b fi ki kf l kg kh">X_train = sc_X.fit_transform(X_train[:, [18,32,38,39,40]])</span><span id="6600" class="kc kd hi jy b fi ki kf l kg kh">X_test = sc_X.fit_transform(X_test[:, [18,32,38,39,40]])</span><span id="2020" class="kc kd hi jy b fi ki kf l kg kh">sc_Y = StandardScaler()</span><span id="9ae5" class="kc kd hi jy b fi ki kf l kg kh">Y_train = sc_Y.fit_transform(Y_train.reshape(-1,1))</span><span id="2db2" class="kc kd hi jy b fi ki kf l kg kh">#Fitting SVR Regression to the Training set</span><span id="2bdf" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.svm import SVR</span><span id="7902" class="kc kd hi jy b fi ki kf l kg kh">regressor = SVR(kernel = ‘rbf’)</span><span id="3109" class="kc kd hi jy b fi ki kf l kg kh">regressor.fit(X_train,Y_train)</span><span id="c1a2" class="kc kd hi jy b fi ki kf l kg kh">#Predicting the Test set results</span><span id="b354" class="kc kd hi jy b fi ki kf l kg kh">Y_pred = sc_Y.inverse_transform(regressor.predict(X_test))</span></pre></div><div class="ab cl kn ko gp kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="hb hc hd he hf"><p id="172d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">决策树回归</strong></p><p id="bc6e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我对训练集执行了决策树回归，没有删除任何属性。性能非常好，尽管我们没有只使用年龄、famrel、缺席、G1和G2属性。以下是python代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="04ff" class="kc kd hi jy b fi ke kf l kg kh">#Decision Tree Regression</span><span id="10a1" class="kc kd hi jy b fi ki kf l kg kh">#Importing the libraries</span><span id="921f" class="kc kd hi jy b fi ki kf l kg kh">import numpy as np</span><span id="9b0d" class="kc kd hi jy b fi ki kf l kg kh">import pandas as pd</span><span id="d783" class="kc kd hi jy b fi ki kf l kg kh">import matplotlib.pyplot as plt</span><span id="d6b3" class="kc kd hi jy b fi ki kf l kg kh">#Importing the dataset</span><span id="3f4a" class="kc kd hi jy b fi ki kf l kg kh">dataset = pd.read_csv(“studentmathdummified.csv”)</span><span id="ac22" class="kc kd hi jy b fi ki kf l kg kh">#Avoiding the dummy variable trap</span><span id="a131" class="kc kd hi jy b fi ki kf l kg kh">#Dropping GP, Male, urban,LE3, Apart,mother_at_home, father_at_home, reason_course, guardian_other</span><span id="e675" class="kc kd hi jy b fi ki kf l kg kh">dataset_trap = dataset.drop(dataset.columns[[0,2,4,6,8,10,15,20,26]],axis=1)</span><span id="9fd3" class="kc kd hi jy b fi ki kf l kg kh">#Define X and Y using dataset_trap</span><span id="14f2" class="kc kd hi jy b fi ki kf l kg kh">X = dataset_trap.iloc[:,:-1].values</span><span id="84cd" class="kc kd hi jy b fi ki kf l kg kh">Y = dataset_trap.iloc[:,-1].values</span><span id="6acd" class="kc kd hi jy b fi ki kf l kg kh">#Splitting the dataset into the Training set and Test set</span><span id="9dbf" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.model_selection import train_test_split</span><span id="fb0d" class="kc kd hi jy b fi ki kf l kg kh">X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2, random_state = 0)</span><span id="c4a3" class="kc kd hi jy b fi ki kf l kg kh">#Feature Scaling</span><span id="9850" class="kc kd hi jy b fi ki kf l kg kh">“””from sklearn.preprocessing import StandardScaler</span><span id="f73e" class="kc kd hi jy b fi ki kf l kg kh">sc_X = StandardScaler()</span><span id="5429" class="kc kd hi jy b fi ki kf l kg kh">X_train = sc_X.fit_transform(X_train[:, [18,32,38,39,40]])</span><span id="de9c" class="kc kd hi jy b fi ki kf l kg kh">X_test = sc_X.fit_transform(X_test[:, [18,32,38,39,40]])</span><span id="4df3" class="kc kd hi jy b fi ki kf l kg kh">sc_Y = StandardScaler()</span><span id="7e2b" class="kc kd hi jy b fi ki kf l kg kh">Y_train = sc_Y.fit_transform(Y_train.reshape(-1,1))”””</span><span id="4c70" class="kc kd hi jy b fi ki kf l kg kh">#Fitting Decision Tree Regression to the Training set</span><span id="b5f8" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.tree import DecisionTreeRegressor</span><span id="daa0" class="kc kd hi jy b fi ki kf l kg kh">regressor = DecisionTreeRegressor(random_state = 0)</span><span id="2cfd" class="kc kd hi jy b fi ki kf l kg kh">regressor.fit(X_train,Y_train)</span><span id="aa41" class="kc kd hi jy b fi ki kf l kg kh">#Predicting the Test set results</span><span id="2722" class="kc kd hi jy b fi ki kf l kg kh">Y_pred = regressor.predict(X_test)</span></pre></div><div class="ab cl kn ko gp kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="hb hc hd he hf"><p id="e443" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">随机森林回归</strong></p><p id="c586" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我还使用10、100和500棵树应用了随机森林回归。在随机森林中，生长一堆树，然后将预测值的平均值作为预测值。在python代码中，它类似于决策树回归代码，只是我们用下面的代码替换了关于使决策树回归适应训练集的部分:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="4755" class="kc kd hi jy b fi ke kf l kg kh">#Fitting Random Forest Regression to the Training set</span><span id="896d" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.ensemble import RandomForestRegressor</span><span id="0612" class="kc kd hi jy b fi ki kf l kg kh">regressor = RandomForestRegressor(n_estimators = 10, random_state = 0)</span><span id="17a4" class="kc kd hi jy b fi ki kf l kg kh">regressor.fit(X_train,Y_train)</span></pre></div><div class="ab cl kn ko gp kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="hb hc hd he hf"><p id="d2cc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">评估模型性能</strong></p><p id="0629" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了确定哪个模型是最好的，我们将对每个模型执行k重交叉验证(k=10 ),并选择具有最佳准确性的模型。</p><p id="60fe" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于使用所有属性的多元线性回归，我得到了80%的准确率。对于只使用年龄、famrel、缺勤、G1和G2这五个属性的多元线性回归，我得到了83%的准确率。</p><p id="bbab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于使用所有属性的SVR回归，我得到了73%的准确率。对于只使用年龄、famrel、缺勤、G1和G2这五个属性的SVR回归，我得到了83%的准确率。</p><p id="66da" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于决策树回归，我得到了77%的准确率。对于只使用年龄、famrel、缺勤、G1和G2这五个属性的决策树回归，准确率为83%。</p><p id="9041" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于随机森林回归，使用10棵树，我得到了85%的准确率。对于100棵树，我得到了86%的准确率。使用500棵树，我得到了87%的准确率。我试过增加树的数量，但是准确率没有超过87%。</p><p id="9871" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于只使用年龄、famrel、缺席、G1和G2这五个属性的随机森林回归，我得到如下结果:10棵树86%，100棵树88%，500棵树88%。我尝试增加树的数量，但准确率不超过88%。</p><p id="7b16" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">上述结果有趣的地方在于，通过将属性限制为年龄、法姆雷尔、缺勤、G1和G2这五个属性，每个模型的准确性都得到了提高。</p><p id="f6f7" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">表现最好的模型似乎是使用500棵树的随机森林回归。如果我们将属性限制为年龄、法姆雷尔、缺席、G1和G2这五个属性，性能会更好。</p><p id="bbfb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">以下是python代码:</p><pre class="jt ju jv jw fd jx jy jz ka aw kb bi"><span id="a4eb" class="kc kd hi jy b fi ke kf l kg kh">#Applying k-fold cross validation</span><span id="fe95" class="kc kd hi jy b fi ki kf l kg kh">from sklearn.model_selection import cross_val_score</span><span id="1a5d" class="kc kd hi jy b fi ki kf l kg kh">accuracies = cross_val_score(estimator=regressor,X=X_train, y=Y_train, cv=10)</span><span id="cd92" class="kc kd hi jy b fi ki kf l kg kh">accuracies.mean()</span><span id="add6" class="kc kd hi jy b fi ki kf l kg kh">accuracies.std()</span></pre></div><div class="ab cl kn ko gp kp" role="separator"><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks kt"/><span class="kq bw bk kr ks"/></div><div class="hb hc hd he hf"><p id="919d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">结论</strong></p><p id="395e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在我们对数据集的回归分析中，我们发现一些最重要的属性是第一年和第二年的成绩、家庭关系的质量、年龄和缺勤次数。对500棵树的随机森林回归被证明是表现最好的模型之一，准确率为87–88%。我们还发现，第三年的成绩与第一年和第二年的成绩之间有很强的线性关系。</p><p id="f84b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该数据集可在以下位置找到:</p><p id="f6cd" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://archive.ics.uci.edu/ml/datasets/student+performance" rel="noopener ugc nofollow" target="_blank">https://archive.ics.uci.edu/ml/datasets/student+performance</a></p><p id="1f8d" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">页（page的缩写）科尔特斯和a .席尔瓦。使用数据挖掘预测中学生成绩。在a .布里托和j .特谢拉编辑的。，第五届未来商业技术会议论文集(FUBUTEC 2008)第5–12页，葡萄牙波尔图，2008年4月，EUROSIS，ISBN 978–9077381–39–7。<br/> <a class="ae iu" href="http://www3.dsi.uminho.pt/pcortez/student.pdf" rel="noopener ugc nofollow" target="_blank">【网页链接】</a></p></div></div>    
</body>
</html>