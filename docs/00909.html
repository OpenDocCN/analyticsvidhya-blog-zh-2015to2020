<html>
<head>
<title>Natural Language Processing for Developers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面向开发人员的自然语言处理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/natural-language-processing-for-developers-912ee0fda979?source=collection_archive---------2-----------------------#2019-09-18">https://medium.com/analytics-vidhya/natural-language-processing-for-developers-912ee0fda979?source=collection_archive---------2-----------------------#2019-09-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/270abf1eaecf563fbcbe5993e758a6a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WuR6iS-tM01IE3c5.png"/></div></div></figure><div class=""/><p id="790d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">自然语言处理</strong> ( <strong class="is hu"> NLP </strong>)是<a class="ae jo" href="https://en.wikipedia.org/wiki/Linguistics" rel="noopener ugc nofollow" target="_blank">语言学</a>、<a class="ae jo" href="https://en.wikipedia.org/wiki/Computer_science" rel="noopener ugc nofollow" target="_blank">计算机科学</a>、<a class="ae jo" href="https://en.wikipedia.org/wiki/Information_engineering_(field)" rel="noopener ugc nofollow" target="_blank">信息工程</a>、<a class="ae jo" href="https://en.wikipedia.org/wiki/Artificial_intelligence" rel="noopener ugc nofollow" target="_blank">人工智能</a>的一个子领域，涉及计算机与人类(自然)语言之间的交互，特别是如何编程计算机处理和分析大量的<a class="ae jo" href="https://en.wikipedia.org/wiki/Natural_language" rel="noopener ugc nofollow" target="_blank">自然语言</a>数据。这里的自然语言数据是指语音、文本等。进一步将语音转换为文本语句，然后进一步发送到后续管道进行处理，然后用于服务、零售等最终产品。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es jp"><img src="../Images/626a9adb7c014d3cdad38a08d7748a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*49-_xI9CuwIn1GGwQMJnig.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">谷歌图片</figcaption></figure><p id="aaae" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">原始输入来自不同的来源，如web、pdf、doc，甚至来自语音识别系统。文本也可以来自使用OCR扫描的书籍。大多数网页文本包含HTML标签、URL和其他与任务无关的内容。</p><h2 id="86ce" class="jy jz ht bd ka kb kc kd ke kf kg kh ki jb kj kk kl jf km kn ko jj kp kq kr ks bi translated">A.文本处理:</h2><p id="e7d4" class="pw-post-body-paragraph iq ir ht is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">在转换到d维向量之前，应该首先对文本进行重新处理。通常，文本预处理的目的是去除这些不必要的标签，只保留简单的文本。在再处理阶段，我们按以下顺序执行以下操作</p><ol class=""><li id="041a" class="ky kz ht is b it iu ix iy jb la jf lb jj lc jn ld le lf lg bi translated">从移除HTML/URL标签开始</li><li id="0693" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">删除任何标点符号或有限的一组特殊字符，如、或。或者#，等等。</li><li id="07d8" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">检查单词是否由英文字母组成，并且不是字母数字</li><li id="cd7a" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">检查单词的长度是否大于2(据调查，没有两个字母的形容词)</li><li id="c97e" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">将单词转换成小写</li><li id="8bd8" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">去掉“我”、“我”、“你”、“是”等停用词。因为它们不包含太多有助于消除建模过程复杂性的信息。</li><li id="2224" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">最后，雪球式词干法(据观察比波特词干法更好)</li></ol><p id="9d2b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们一个一个来</p><div class="hh hi ez fb hj lm"><a href="https://www.kaggle.com/sudalairajkumar/getting-started-with-text-preprocessing" rel="noopener  ugc nofollow" target="_blank"><div class="ln ab dw"><div class="lo ab lp cl cj lq"><h2 class="bd hu fi z dy lr ea eb ls ed ef hs bi translated">文本预处理入门</h2><div class="lt l"><h3 class="bd b fi z dy lr ea eb ls ed ef dx translated">使用Kaggle笔记本探索和运行机器学习代码|使用Twitter上的客户支持数据</h3></div><div class="lu l"><p class="bd b fp z dy lr ea eb ls ed ef dx translated">www.kaggle.com</p></div></div><div class="lv l"><div class="lw l lx ly lz lv ma hp lm"/></div></div></a></div><p id="4fca" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">从文本中删除网址python:</strong><a class="ae jo" href="https://stackoverflow.com/a/40823105/4084039" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/a/40823105/4084039</a></p><pre class="jq jr js jt fd mb mc md me aw mf bi"><span id="6f1c" class="jy jz ht mc b fi mg mh l mi mj">sent = re.sub(r"http\S+", "", sent)<br/>print(sent)</span></pre><p id="7637" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">转换成短成长</strong><a class="ae jo" href="https://stackoverflow.com/a/47091490/4084039" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/a/47091490/4084039</a></p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es mk"><img src="../Images/dc1850742a49948bacf7da0edcb8fafb.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*bvRS8LQbE5dOvlv6s0y77g.png"/></div></figure><pre class="jq jr js jt fd mb mc md me aw mf bi"><span id="93b0" class="jy jz ht mc b fi mg mh l mi mj">sent = decontracted(sent)<br/>print(sent)</span></pre><p id="66e5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">将所有文本转换成小写</strong></p><pre class="jq jr js jt fd mb mc md me aw mf bi"><span id="1f5a" class="jy jz ht mc b fi mg mh l mi mj">df["text_lower"] = df["text"].str.lower()</span></pre><p id="0695" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">【https://stackoverflow.com/a/18082370/4084039】用数字去掉单词python:<a class="ae jo" href="https://stackoverflow.com/a/18082370/4084039" rel="noopener ugc nofollow" target="_blank"/></p><pre class="jq jr js jt fd mb mc md me aw mf bi"><span id="2884" class="jy jz ht mc b fi mg mh l mi mj">sent = re.sub(“\S*\d\S*”, “”, sent).strip()<br/>print(sent)</span></pre><p id="3f3f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">拼写更正:</strong></p><p id="5508" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另一个重要的文本预处理步骤是拼写纠正。错别字在文本数据中很常见，我们可能希望在进行分析之前纠正这些拼写错误。</p><pre class="jq jr js jt fd mb mc md me aw mf bi"><span id="12b2" class="jy jz ht mc b fi mg mh l mi mj">!pip install pyspellchecker<br/>from spellchecker import SpellChecker<br/><br/>spell = SpellChecker()<br/>def correct_spellings(text):<br/>    corrected_text = []<br/>    misspelled_words = spell.unknown(text.split())<br/>    for word <strong class="mc hu">in</strong> text.split():<br/>        if word <strong class="mc hu">in</strong> misspelled_words:<br/>            corrected_text.append(spell.correction(word))<br/>        else:<br/>            corrected_text.append(word)<br/>    return " ".join(corrected_text)<br/>        <br/>text = "speling correctin"<br/>correct_spellings(text)</span></pre><p id="f417" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">删除特殊字符</strong>:<a class="ae jo" href="https://stackoverflow.com/a/5843547/4084039" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/a/5843547/4084039</a></p><pre class="jq jr js jt fd mb mc md me aw mf bi"><span id="648e" class="jy jz ht mc b fi mg mh l mi mj">sent = re.sub('[^A-Za-z0-9]+', ' ', sent)<br/>print(sent)</span></pre><p id="9018" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">处理正则表达式</strong></p><pre class="jq jr js jt fd mb mc md me aw mf bi"><span id="1619" class="jy jz ht mc b fi mg mh l mi mj">import re<br/># Tutorial about Python regular expressions: <a class="ae jo" href="https://pymotw.com/2/re/" rel="noopener ugc nofollow" target="_blank">https://pymotw.com/2/re/</a><br/>import string<br/>from nltk.corpus import stopwords<br/>from nltk.stem import PorterStemmer<br/>from nltk.stem.wordnet import WordNetLemmatizer</span></pre><p id="eb67" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">在单一功能中处理停止字/stem/clean</strong></p><ul class=""><li id="1cca" class="ky kz ht is b it iu ix iy jb la jf lb jj lc jn ml le lf lg bi translated"><a class="ae jo" href="https://gist.github.com/sebleier/554280" rel="noopener ugc nofollow" target="_blank">https://gist.github.com/sebleier/554280</a></li><li id="6a31" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ml le lf lg bi translated">我们将从停用字词列表中删除字词:“否”、“或非”、“不是”</li><li id="f3ce" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ml le lf lg bi translated"><br/> <br/> == &gt;经过以上步骤，我们得到了" br br" <br/>我们正在将它们包含到停用词列表中</li><li id="6372" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ml le lf lg bi translated">如果我们有<br/>，而不是<br/>，这些标签将在第一步中移除</li></ul><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mm"><img src="../Images/b2bffe9e6e762dbc8c106cf24d01f495.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0mYCEUOG2u8lgBPb9AHAoA.png"/></div></div></figure><p id="cf84" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">删除停用词并不总是最好的主意。假设你有两个相同的句子，其中一个不包含任何内容。在这种情况下，删除停用词会改变句子的意思，因此删除停用词必须采取一些预防措施。<em class="mn">你可以用n-gram来摆脱以上所有问题</em>。</p><p id="7eb8" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您也可以使用下面的代码从NLTK库中获得这些停用词:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mo"><img src="../Images/4689d9cb3ec302c47507f94c1818bd95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XmXT8oHr6cOnQ4AjEpywLA.png"/></div></div></figure><p id="53de" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">你会在这里找到tasti的词根。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mp"><img src="../Images/0d8eca7baac87db294759b5505eeb252.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XoZCcBjzGwgzUi_itXSH_A.png"/></div></div></figure><p id="77c9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">清洁和堵塞功能</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mq"><img src="../Images/97851f52ba27c6653b093c47652a44a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i5ANwkkiIjMz-10DEPqnVg.png"/></div></div></figure><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mr"><img src="../Images/70f6965fbc958a5aeb35b548c5ed45ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F_lWxhQHgYwBi6UpLZa8OA.png"/></div></div></figure><pre class="jq jr js jt fd mb mc md me aw mf bi"><span id="4761" class="jy jz ht mc b fi mg mh l mi mj">import re<br/>from nltk.corpus import stopwords<br/>nltk.download('stopwords')<br/># loading stop words from nltk library<br/>stop_words = set(stopwords.words('english'))<br/></span></pre><p id="8f75" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">处理炮泥:</strong></p><pre class="jq jr js jt fd mb mc md me aw mf bi"><span id="0e7c" class="jy jz ht mc b fi mg mh l mi mj">from nltk.stem.porter import PorterStemmer<br/><br/>stemmer = PorterStemmer()<br/>def stem_words(text):<br/>    return " ".join([stemmer.stem(word) for word <strong class="mc hu">in</strong> text.split()])<br/><br/>df["text_stemmed"] = df["text"].apply(lambda text: stem_words(text))<br/>df.head()</span></pre><p id="8f5e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">词干化是产生词根/基本词的形态变体的过程。词干程序通常被称为词干算法或词干分析器。词干算法将单词“chocolate”、“chocolatey”、“choco”简化为词根，“chocolate”和“retrieve”，“retrieved”，“retrieved”简化为词干“retrieve”。</p><p id="b42d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Testy，test和taste是BOW上不同的向量，所以使用词干转换所有的单词。雪球式词干法(据观察比波特词干法更好)可以解决上述问题。</p><p id="cfe1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">处理词汇化:</strong></p><pre class="jq jr js jt fd mb mc md me aw mf bi"><span id="8370" class="jy jz ht mc b fi mg mh l mi mj">from nltk.stem import WordNetLemmatizer<br/><br/>lemmatizer = WordNetLemmatizer()<br/>def lemmatize_words(text):<br/>    return " ".join([lemmatizer.lemmatize(word) for word <strong class="mc hu">in</strong> text.split()])<br/><br/>df["text_lemmatized"] = df["text"].apply(lambda text: lemmatize_words(text))<br/>df.head()</span></pre><p id="139e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">词汇化是将一个词的不同屈折形式组合在一起的过程，这样它们就可以作为一个项目进行分析。词汇化类似于词干化，但它给单词带来了上下文。所以它把意思相近的单词链接成一个单词。</p><pre class="jq jr js jt fd mb mc md me aw mf bi"><span id="db88" class="jy jz ht mc b fi mg mh l mi mj">from nltk.corpus import wordnet<br/>from nltk.stem import WordNetLemmatizer<br/><br/>lemmatizer = WordNetLemmatizer()<br/>wordnet_map = {"N":wordnet.NOUN, "V":wordnet.VERB, "J":wordnet.ADJ, "R":wordnet.ADV}<br/>def lemmatize_words(text):<br/>    pos_tagged_text = nltk.pos_tag(text.split())<br/>    return " ".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos <strong class="mc hu">in</strong> pos_tagged_text])<br/><br/>df["text_lemmatized"] = df["text"].apply(lambda text: lemmatize_words(text))<br/>df.head()</span></pre><p id="9f11" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们还需要为nltk中的lemmatizer提供单词的POS标签。根据位置的不同，lemmatizer可能会返回不同的结果。</p><p id="4eb7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Lemmatizer面包句子变成单词，但把纽约变成一个单词。它有一个所有这类单词的字典。它是特定于语言的。</p><p id="a211" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">将句子分解成单词。但预防当(纽约)</p><p id="c84c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">将上述所有处理步骤组合在下面的代码中</strong>【https://stackoverflow.com/a/47091490/4084039 T2】</p><ol class=""><li id="ee54" class="ky kz ht is b it iu ix iy jb la jf lb jj lc jn ld le lf lg bi translated">从正文中分离出代码片段</li><li id="fe47" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">从问题标题和描述中删除特殊字符</li><li id="4331" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">删除停用词(除了“C”)</li><li id="a948" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">删除HTML标签</li><li id="3617" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">将所有字符转换成小写字母</li><li id="5bd7" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">用雪球词干仪来阻止单词</li></ol><pre class="jq jr js jt fd mb mc md me aw mf bi"><span id="2b16" class="jy jz ht mc b fi mg mh l mi mj">from nltk.stem import WordNetLemmatizer<br/><br/>def striphtml(data):<br/>    cleanr = re.compile('&lt;.*?&gt;')<br/>    cleantext = re.sub(cleanr, ' ', str(data))<br/>    return cleantext<br/>stop_words = set(stopwords.words('english'))<br/>stemmer = SnowballStemmer("english")</span></pre><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es mk"><img src="../Images/dc1850742a49948bacf7da0edcb8fafb.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*bvRS8LQbE5dOvlv6s0y77g.png"/></div></figure><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ms"><img src="../Images/1354c16d6dd074a1a5a4a249ac871581.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tXmE9ArjKSn0DSJ3k3NQvg.png"/></div></div></figure><p id="3302" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在应用上述处理部分之后，我们可以使用文本数据进行机器学习模型中的下一步。</p><p id="97a3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">词语的语义:</strong></p><p id="6dce" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们将在这种情况下使用word2vec。暴躁和美味，它们是同义词。</p><p id="e6c3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">清理后最终存储数据库</strong></p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mt"><img src="../Images/4aa6d470fe1521976df53deaf5cc4b62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VFQxb1bNPhGDESmZTRrl7A.png"/></div></div></figure><h2 id="0699" class="jy jz ht bd ka kb kc kd ke kf kg kh ki jb kj kk kl jf km kn ko jj kp kq kr ks bi translated">B.特征提取:</h2><p id="6cbb" class="pw-post-body-paragraph iq ir ht is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">文本数据以数字的ASCII值存储在计算机上。计算机以二进制形式存储和传输这些数字。它并不意味着一个<b or="" b="" based="" on="" their="" ascii="" value.="" that="" would="" be="" an="" incorrect="" assumption.="" moreover="" individual="" character="" does="" not="" contain="" any="" meaning="" at="" all.="" it="" is="" the="" word="" represents="" of="" text.="" so="" question="" how="" can="" we="" come="" up="" with="" a="" similar="" representation="" text="" data="" use="" for="" our="" model.="" answer="" depends="" upon="" what="" type="" model="" you="" are="" using="" and="" task="" going="" to="" perform.="" following="" different="" cases="" above=""/></p><ol class=""><li id="4bfd" class="ky kz ht is b it iu ix iy jb la jf lb jj lc jn ld le lf lg bi translated"><strong class="is hu">基于图的模型:</strong>像wordnet一样用单词之间的关系将单词表示为符号节点。</li></ol><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es mu"><img src="../Images/12f248661b361da8529d1ca7f407ce3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*Bek96TZ2oNq86R4yW3rfHQ.png"/></div></figure><p id="aaaa" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.<strong class="is hu">统计模型:</strong>你需要一个单词的数值表示。这里我们用<strong class="is hu"> BoW，TF-IDF，AvgW2Vec，TF-IDF加权W2v </strong>把一个词表示成一个数值向量</p><h2 id="1800" class="jy jz ht bd ka kb kc kd ke kf kg kh ki jb kj kk kl jf km kn ko jj kp kq kr ks bi translated">一袋单词:</h2><div class="hh hi ez fb hj lm"><a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener  ugc nofollow" target="_blank"><div class="ln ab dw"><div class="lo ab lp cl cj lq"><h2 class="bd hu fi z dy lr ea eb ls ed ef hs bi translated">sk learn . feature _ extraction . text . count vectorizer-sci kit-learn 0 . 21 . 3文档</h2><div class="lt l"><h3 class="bd b fi z dy lr ea eb ls ed ef dx translated">class sk learn . feature _ extraction . text . count vectorizer(input = ' content '，encoding='utf-8 '，decode_error='strict'…</h3></div><div class="lu l"><p class="bd b fp z dy lr ea eb ls ed ef dx translated">scikit-learn.org</p></div></div><div class="lv l"><div class="mv l lx ly lz lv ma hp lm"/></div></div></a></div><ol class=""><li id="d53e" class="ky kz ht is b it iu ix iy jb la jf lb jj lc jn ld le lf lg bi translated">它首先构建一个字典，即文本中所有单词的集合。它由文本中所有独特的单词组成。它将单词表示成稀疏矩阵。</li><li id="99bc" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">创建d维向量(d是唯一的单词数)</li></ol><p id="2a14" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2.对于每个文档(行)，查找唯一的单词，其中每个单词是一个不同的维度。每个单元格由单词在相应行中出现的次数组成。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es mw"><img src="../Images/ee96b5e6476866a1c7adea5a6d69646b.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/0*aeJteHw4J3VkMJtq.png"/></div></figure><p id="8602" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">维度将非常大，其中大部分单元格的值为零。所以会形成一个稀疏矩阵。这里要注意的一点是，如果两个向量非常接近，那么它们之间的相似性将会很高。</p><p id="d91e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每行被命名为<strong class="is hu">字向量</strong>，每列被命名为<strong class="is hu">文档向量</strong></p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es mx"><img src="../Images/4ea9e61d625520bd5094a4248469ae3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/0*Lu1jdorS9rnhiYpM.png"/></div></figure><p id="4c26" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最好将值存储在字典位置。</p><p id="7810" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">代码:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es my"><img src="../Images/5a2ef677318e418bcd728d60d13498d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6tsOBoElPsQ_-dM4GQ4_jg.png"/></div></div></figure><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es mz"><img src="../Images/37d19351c560bdb6247386e7b27cf43e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/0*-YrN0lSEQfy4IEKc.png"/></div></figure><p id="ae62" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">二元语法、三元语法和n元语法:</strong> <em class="mn"> n </em>表示组合在一起被视为一个特征的单词的数量。</p><p id="8ce6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">注意:在构建n元语法之前，应该避免删除像“not”这样的停用词。</p><p id="3a76" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">ngram _ range</strong>:<em class="mn">tuple(min _ n，max_n) </em></p><p id="3215" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要提取的不同n元文法的n值范围的下限和上限。n的所有值使得min_n &lt;= n &lt;= max_n will be used.</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es na"><img src="../Images/a21dcf382b678dd85f93844c10df1fbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*SxBMoILvDmS3bU4d.png"/></div></div></figure><p id="3fa9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> <em class="mn">二进制弓:</em> </strong></p><p id="7b46" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它不是将count放入vector中，而是根据文档中是否存在单词来存储1，0。</p><p id="951c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">弓的缺点:</strong></p><p id="297d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">BOW不考虑语义。例如,“好吃”和“美味”意思相同，但是BOW认为它们是不同的。</p><h1 id="60af" class="nb jz ht bd ka nc nd ne ke nf ng nh ki ni nj nk kl nl nm nn ko no np nq kr nr bi translated">TF-IDF(术语频率-逆文档频率)</h1><p id="e956" class="pw-post-body-paragraph iq ir ht is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">TF-文档<em class="mn"> d </em>中出现的时间字<em class="mn"> t </em>的个数除以文档<em class="mn"> d </em>中的总字数。换句话说，在文档<em class="mn"> d </em>中找到一个单词的概率是多少。如果一个单词出现在文档中，那么TF增加。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ns"><img src="../Images/d08d7c96707e58f252bb3373033c8042.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bZ2-XZZn1Rl7HEwF.png"/></div></div></figure><p id="7e23" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果一个单词出现在更多的文档中，那么IDF减少。单元值是TF * IDF的乘积。更重要的是在文档中很少见，如果某个单词在文档/评论中很常见，则更重要。</p><p id="445f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">代码:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nt"><img src="../Images/9e9016e7573421558e35b81fdeace37b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Wntm6JhYCg35ZY0H.png"/></div></div></figure><p id="67b5" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">以下是tf-idf矢量化的密集输出。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nu"><img src="../Images/74dcfae36f9306434d6647d7596c8837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*mFs8GeK6zRM_1s-Q.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated"><a class="ae jo" href="https://stackoverflow.com/questions/48429367/appending-2-dimensional-list-dense-output-of-tfidf-result-into-pandas-datafram" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/48429367/appending-2-dimensional-list-dense-output-of-tfi df-result-into-pandas-data fram</a></figcaption></figure><p id="2dbb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">获取其在前25个特征值的数据帧中的值，</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es nv"><img src="../Images/69f86ac7549a27d1c86ea1af0ed95e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ayBcUwogcNZZONcevETXFA.png"/></div></div></figure><p id="325a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">TF-IDF的缺点:</strong></p><p id="97ff" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">TF-IDF不考虑语义。《出埃及记》美味和可口有相同的含义，但TF-IDF认为是分开的。</p><h2 id="1f4f" class="jy jz ht bd ka kb kc kd ke kf kg kh ki jb kj kk kl jf km kn ko jj kp kq kr ks bi translated">平均Word2Vector:</h2><p id="9ab2" class="pw-post-body-paragraph iq ir ht is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated"><strong class="is hu"> Word2Vector: </strong> Word2vec基本上将单词放置在特征空间中，使得它们的位置由它们的含义决定，即具有相似含义的单词被聚集在一起，并且两个单词之间的距离也具有相同的含义。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es nw"><img src="../Images/7cf10d5fa7e0d9b1e1a329b3962ad6d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/0*v3x5QP0hONlubYxE.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated"><a class="ae jo" href="https://www.google.com/search?rlz=1C1EJFC_enUS810US810&amp;biw=1366&amp;bih=695&amp;tbm=isch&amp;sxsrf=ACYBGNQbNXDs4_S3zhi8fbhJKHYVOhvo9w%3A1568807382246&amp;sa=1&amp;ei=1hmCXdPUDp3ez7sPme6SkAM&amp;q=word2vec+tensorflow&amp;oq=Word2Vector+&amp;gs_l=img.1.3.0i24l3j0i10i24l2.47297.47297..50643...0.0..0.238.238.2-1......0....1..gws-wiz-img.MK-l30T4EL0#imgrc=JLZUdvaM8UR2CM:" rel="noopener ugc nofollow" target="_blank">https://www.google.com/search?rlz=1C1EJFC_enUS810US810&amp;biw = 1366&amp;BIH = 695&amp;TBM = isch&amp;sxsrf = acybgnqbnxds 4 _ S3 zhi 8 fbhjkhyvohvo 9 w % 3a 1568807382246&amp;sa = 1&amp;ei = 1 hmcxdpudp 3 ez 7 SPME 6 skam&amp;q = word 2 vec+tensor flow&amp;OQ = word 2v..50643...0.0..0.238.238.2-1......0....一..gws-wiz-img。MK-l 30 T4 El 0 # imgrc = jlzudvam 8 ur 2cm:</a></figcaption></figure><p id="8829" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">余弦相似度:</strong>让我们先了解什么是余弦相似度，因为word2vec使用余弦相似度来找出更相似的单词。余弦相似性不仅可以判断两个向量之间的相似性，还可以检验向量的正交性。余弦相似度由公式表示:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es nx"><img src="../Images/46c46a3c0549b61a1166b13cc08e7245.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/0*qKWKMFnbZR6ExP3q"/></div></figure><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mq"><img src="../Images/c2648d886b1398c4d7219b1a5becc2c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*81qLFINfn8ektWio.png"/></div></div></figure><p id="057c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果角度接近零，那么我们可以说向量彼此非常相似，如果θ为90°，那么我们可以说向量彼此正交(正交向量彼此不相关)，如果θ为180°，那么我们可以说两个向量彼此相反。</p><p id="b22f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">训练Word2Vector模型</strong>:先把每个单词拆分成一行，存入list。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es ny"><img src="../Images/9d07e9d6f169023aacd182b18743ceb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/0*qgkCosLP4ZVSXYSh.png"/></div></figure><div class="hh hi ez fb hj lm"><a href="http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.XXpERSgza01" rel="noopener  ugc nofollow" target="_blank"><div class="ln ab dw"><div class="lo ab lp cl cj lq"><h2 class="bd hu fi z dy lr ea eb ls ed ef hs bi translated">Gensim Word2Vec教程-完整工作示例| Kavita Ganesan</h2><div class="lt l"><h3 class="bd b fi z dy lr ea eb ls ed ef dx translated">Word2Vec背后的想法非常简单。我们假设一个词的意思可以通过…</h3></div><div class="lu l"><p class="bd b fp z dy lr ea eb ls ed ef dx translated">kavita-ganesan.com</p></div></div><div class="lv l"><div class="nz l lx ly lz lv ma hp lm"/></div></div></a></div><p id="90ee" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> <em class="mn">案例1 —想训练自己的W2V </em> </strong></p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es oa"><img src="../Images/7a18a7c5df2ab8dc9a84812baac34fc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1106/format:webp/0*cTms4iCU31Y764j8.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated"><a class="ae jo" href="http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.W17SRFAzZPY" rel="noopener ugc nofollow" target="_blank">http://ka vita-ganesan . com/gensim-word 2 vec-tutorial-starter-code/# . w17 srfazzpy</a></figcaption></figure><p id="8492" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您可以对整个单元格进行注释，或者根据需要更改这些变量。</p><p id="48fb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu"> <em class="mn">案例2 —想在google news上训练Google w2v</em></strong></p><ul class=""><li id="e502" class="ky kz ht is b it iu ix iy jb la jf lb jj lc jn ml le lf lg bi translated">在这个项目中，我们使用谷歌的预训练模型</li><li id="fcea" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ml le lf lg bi translated">它的3.3G文件，一旦你把它载入你的记忆</li><li id="cce7" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ml le lf lg bi translated">它占用大约9Gb，因此只有当您的ram大于12G时，请执行此步骤</li><li id="2af7" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ml le lf lg bi translated">我们将提供一个包含字典的pickle文件，</li><li id="cc39" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ml le lf lg bi translated">它包含我们所有的语料库单词作为键，模型[单词]作为值</li><li id="c0fc" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ml le lf lg bi translated">要使用这段代码，请下载“Google news-vectors-negative 300 . bin”</li><li id="8956" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ml le lf lg bi translated">来自<a class="ae jo" href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit" rel="noopener ugc nofollow" target="_blank">https://drive . Google . com/file/d/0 b 7 xkcwpi 5 kdynlnuttlss 21 pmmm/edit</a></li><li id="f9d6" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ml le lf lg bi translated">大小是1.9GB。</li></ul><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ob"><img src="../Images/0be0ab81b92aa7742e7125505730ded1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zSNVw_O69cI8DZ_xR1T9uA.png"/></div></div></figure><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es oc"><img src="../Images/1b95a4f05776b8d048a979a971434ed3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VHw6w9oD3hDX3T3M.png"/></div></div></figure><p id="fb4b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果要检查单词出现的次数:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es od"><img src="../Images/5b6465bfeead4e737e8c5babbe5c0394.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/0*tPjugzo7VRIo9CLD.png"/></div></figure><p id="d579" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">实现Avg Word2Vector: </strong></p><p id="ecc6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们需要给出一个大的文本语料库，其中每个单词都有一个向量。它试图从原始文本中自动学习向量之间的关系。向量的维数越大，它的信息量就越大。</p><p id="bf34" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">属性:</p><ol class=""><li id="6179" class="ky kz ht is b it iu ix iy jb la jf lb jj lc jn ld le lf lg bi translated">如果字w1和w2相似，则向量v1和v2会更接近。</li><li id="6c13" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">自动学习单词/向量之间的关系。</li></ol><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es oe"><img src="../Images/11a5d28eac4df4ac55a29973243eb757.png" data-original-src="https://miro.medium.com/v2/resize:fit:1014/format:webp/0*Fu5yExUd7qPV9Rts.png"/></div></figure><p id="6821" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们正在观察男女图表，观察到男人和女人之间的距离与国王(男性)和王后(女性)之间的距离相同</p><p id="8d17" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">不仅是不同的性别，如果我们观察同性，我们会发现女王和女人之间的距离以及国王和男人之间的距离是相同的(国王和男人，女王和女人代表同性比较，因此它们必须是相等的距离)。</p><p id="67e7" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><strong class="is hu">如何把每个文档转换成矢量？</strong></p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es of"><img src="../Images/9040445bb1de3d02f7b8d3cceee0e081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/0*I90tiNOEISo4eqHy"/></div></figure><p id="f0fd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每个单词都有一个向量，我们将平均word2vec转换为除以文档中的单词数。</p><p id="77ae" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">代码:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es og"><img src="../Images/28f535496e1c26fc280f2ad3508c0449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/0*g4r_1aPDJT5eaDlT.png"/></div></figure><h2 id="6033" class="jy jz ht bd ka kb kc kd ke kf kg kh ki jb kj kk kl jf km kn ko jj kp kq kr ks bi translated">TF-IDF加权Word2Vec:</h2><p id="2521" class="pw-post-body-paragraph iq ir ht is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">在这个方法中，我们首先计算每个单词的TF-IDF值。然后，按照与上一节相同的方法，将TF-IDF值乘以相应的单词，然后将总和除以TF-IDF值的总和。</p><p id="352a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">代码:</p><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es oh"><img src="../Images/73aed35dac6391384f0c70ac2b89706f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/0*d11EWhq6TNjS-df5.png"/></div></figure><figure class="jq jr js jt fd hk er es paragraph-image"><div class="er es oi"><img src="../Images/9df6ad46b7a503cfdfb99bbb0655ce16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VfVZPMeiIECY9ldY.png"/></div></figure><h2 id="3e4d" class="jy jz ht bd ka kb kc kd ke kf kg kh ki jb kj kk kl jf km kn ko jj kp kq kr ks bi translated"><strong class="ak">创造你自己的WORD2VEC </strong></h2><p id="86b7" class="pw-post-body-paragraph iq ir ht is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">创建句子列表的列表</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es oj"><img src="../Images/cbeeb06a8d2c89335a6eddb4462516c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hvbFdrJRtS6Fc-g9kZ1pqg.png"/></div></div></figure><p id="57dd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如何训练你的模型</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es oj"><img src="../Images/0183c5eecd9be6afe2224b5bf6a10351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ff-7ZdDTV63SeCKjFcQMyA.png"/></div></div></figure><ol class=""><li id="82db" class="ky kz ht is b it iu ix iy jb la jf lb jj lc jn ld le lf lg bi translated"><em class="mn">min _ count</em>——一个单词出现的最小次数</li><li id="4745" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated"><em class="mn">大小</em> -矢量的大小</li><li id="80a2" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated"><em class="mn"> workers </em> -在您的系统中使用所有4个内核</li></ol><div class="hh hi ez fb hj lm"><a href="https://radimrehurek.com/gensim/models/word2vec.html" rel="noopener  ugc nofollow" target="_blank"><div class="ln ab dw"><div class="lo ab lp cl cj lq"><h2 class="bd hu fi z dy lr ea eb ls ed ef hs bi translated">Gensim:面向人类的主题建模</h2><div class="lt l"><h3 class="bd b fi z dy lr ea eb ls ed ef dx translated">机器学习和NLP * PII工具咨询自动发现个人和敏感数据初始化模型…</h3></div><div class="lu l"><p class="bd b fp z dy lr ea eb ls ed ef dx translated">radimrehurek.com</p></div></div><div class="lv l"><div class="ok l lx ly lz lv ma hp lm"/></div></div></a></div><p id="e1a6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">您可以使用您训练过的模型进行检查。</p><figure class="jq jr js jt fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es oj"><img src="../Images/c005ca70b3e3df99e9b0b445f87b2e12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SkLWlcqMQyCKx_DSgWZoFA.png"/></div></div></figure><p id="e74b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这里你可以看到，所有语义元素在BOW/TFIDF中是不可能的。可以使用相同的模型为每个句子创建w2v，如以上部分所做的那样。</p><h2 id="831d" class="jy jz ht bd ka kb kc kd ke kf kg kh ki jb kj kk kl jf km kn ko jj kp kq kr ks bi translated">C.建模:</h2><p id="7019" class="pw-post-body-paragraph iq ir ht is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">这个过程包括设计统计模型。在这种情况下，将数据作为训练输入，并使用创建的模型来预测未知数据。数字数据的好处在于它可以暗示任何统计模型。该模型包括以下内容</p><ol class=""><li id="bce2" class="ky kz ht is b it iu ix iy jb la jf lb jj lc jn ld le lf lg bi translated">线性、非线性建模和物流建模(分类和回归)</li><li id="0a15" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">朴素贝叶斯，KNN，SVM模型</li><li id="aea3" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">决策树和集成模型(Bagging，Boosting，Random forest，XGBoost)</li><li id="d5a6" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">人工神经网络(回归和分类)</li><li id="524f" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">堆叠和最大投票模型</li></ol><h2 id="eaf4" class="jy jz ht bd ka kb kc kd ke kf kg kh ki jb kj kk kl jf km kn ko jj kp kq kr ks bi translated">上述模型的应用:</h2><p id="990a" class="pw-post-body-paragraph iq ir ht is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">上述模型可以部署为基于web的应用程序。这些应用程序可以与产品、服务等集成。下面是我们使用NLP模型几个例子——</p><ol class=""><li id="06d1" class="ky kz ht is b it iu ix iy jb la jf lb jj lc jn ld le lf lg bi translated">情感分析</li><li id="e0fd" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">文本分类</li><li id="2eb9" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">电子零售推荐系统</li><li id="481c" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">语音识别</li></ol><p id="4731" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">=================谢谢===============</p><p id="50bd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">============ = = = = = =代码= = = = = = = = = = = =</p><p id="bad9" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><a class="ae jo" href="https://github.com/ranasingh-gkp/Applied_AI_O/blob/master/Assignments_AFR_2018/Amazon_Fine_Food_Reviews_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/Rana Singh-gkp/Applied _ AI _ O/blob/master/Assignments _ AFR _ 2018/Amazon _ Fine _ Food _ Reviews _ analysis . ipynb</a></p><h2 id="484c" class="jy jz ht bd ka kb kc kd ke kf kg kh ki jb kj kk kl jf km kn ko jj kp kq kr ks bi translated">参考资料:</h2><ol class=""><li id="c252" class="ky kz ht is b it kt ix ku jb ol jf om jj on jn ld le lf lg bi translated">谷歌图片</li><li id="910e" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">https://en.wikipedia.org/wiki/Natural_language_processing<a class="ae jo" href="https://en.wikipedia.org/wiki/Natural_language_processing" rel="noopener ugc nofollow" target="_blank"/></li><li id="9d0e" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">Kaggle.com/amazon食品评论</li><li id="6472" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">应用人工智能</li><li id="7a72" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated">代码在<a class="ae jo" href="https://github.com/ranasingh-gkp/Applied_AI_O/blob/master/Assignments_AFR_2018/Amazon_Fine_Food_Reviews_Analysis.ipynb" rel="noopener ugc nofollow" target="_blank">Amazon _ Fine _ Food _ Reviews _ analysis . ipynb</a>笔记本中给定的链接处找到。<a class="ae jo" href="https://github.com/ranasingh-gkp/Applied_AI_O/tree/master/Assignments_AFR_2018" rel="noopener ugc nofollow" target="_blank">https://github . com/Rana Singh-gkp/Applied _ AI _ O/tree/master/Assignments _ AFR _ 2018</a></li><li id="ff1c" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated"><a class="ae jo" href="http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.W17SRFAzZPY" rel="noopener ugc nofollow" target="_blank">http://ka vita-ganesan . com/gensim-word 2 vec-tutorial-starter-code/# . w17 srfazzpy</a></li><li id="e73c" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated"><a class="ae jo" href="https://stackoverflow.com/questions/48429367/appending-2-dimensional-list-dense-output-of-tfidf-result-into-pandas-datafram" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/48429367/appending-2-dimensional-list-dense-output-of-tfi df-result-into-pandas-data fram</a></li><li id="535c" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated"><a class="ae jo" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">http://sci kit-learn . org/stable/modules/generated/sk learn . feature _ extraction . text . count vectorizer . html</a></li><li id="49da" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated"><a class="ae jo" href="https://stackoverflow.com/a/18082370/4084039" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/a/18082370/4084039</a></li><li id="ea08" class="ky kz ht is b it lh ix li jb lj jf lk jj ll jn ld le lf lg bi translated"><a class="ae jo" href="https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/tutorial/text _ analytics/working _ with _ text _ data . html</a></li></ol></div></div>    
</body>
</html>