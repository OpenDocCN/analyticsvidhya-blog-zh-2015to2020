<html>
<head>
<title>Analyzing Customer reviews using text mining to predict their behaviour</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用文本挖掘分析客户评论以预测他们的行为</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/customer-review-analytics-using-text-mining-cd1e17d6ee4e?source=collection_archive---------0-----------------------#2018-08-22">https://medium.com/analytics-vidhya/customer-review-analytics-using-text-mining-cd1e17d6ee4e?source=collection_archive---------0-----------------------#2018-08-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="e76a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">分析客户评论以预测客户是否会推荐产品</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/86bb683a96ad80c338ce06c8b7bcb3bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fjeEluO2Te0U4HzK-keIYw.jpeg"/></div></div></figure><h1 id="9a3e" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">简介</strong></h1><p id="1e84" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated"><strong class="ih hj"> <em class="ks">文本挖掘</em> </strong>是检查大量文本集合并将非结构化文本数据转换为结构化数据以供进一步分析(如可视化和建模)的过程。在本文中，我们将利用文本挖掘的力量对一个电子商务服装网站上的顾客评论进行深入分析。<br/>顾客评论是“顾客之声”的一个重要来源，可以提供顾客对产品或服务喜恶的深刻见解。对于电子商务业务，客户评论非常重要，因为在缺乏要购买的产品的实际外观和感觉的情况下，现有评论严重影响新客户的购买决策。</p><h1 id="900a" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">关于数据集</strong></h1><p id="de61" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">我们将在本文中使用的数据集来自ka ggle(<a class="ae kt" href="https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews/home" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/nica potato/womens-ecommerce-Clothing-reviews/home</a>)，并且来自一个女装电子商务网站，该网站围绕客户撰写的评论。<br/>该数据集包括23486行和10个特征变量。每行对应一个客户评论，包括变量:</p><ul class=""><li id="c207" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated"><strong class="ih hj">服装ID </strong>:整数分类变量，表示正在审核的特定服装。</li><li id="77fc" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">年龄</strong>:审核人年龄的正整数变量。</li><li id="3eb0" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">标题</strong>:评论标题的字符串变量。</li><li id="5ba5" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">审核文本</strong>:审核主体的字符串变量。</li><li id="ef12" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">评级</strong>:客户给产品打分的正整数变量，从最差1分到最好5分。</li><li id="4598" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">推荐的IND </strong>:二进制变量，说明客户在哪里推荐产品，1表示推荐，0表示不推荐。</li><li id="853b" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">正面反馈计数</strong>:正整数，记录认为此评论为正面的其他客户的数量。</li><li id="d0c7" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">分部名称</strong>:产品高级分部的分类名称。</li><li id="dc3d" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">部门名称</strong>:产品部门名称的分类名称。</li><li id="4311" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">类名</strong>:产品类名的分类名称。</li></ul><h2 id="4f8f" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">该条的范围</h2><p id="a8ec" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">基于这些变量，可以对上述数据集执行多种监督和非监督技术，以提供关于客户偏好的多种见解。然而，我们将这篇博客的范围限制在广泛使用文本挖掘来分析客户评论。</p><p id="e08b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将使用以下技术来理解文本挖掘的各个方面:</p><ul class=""><li id="3c96" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated"><strong class="ih hj">对文本数据</strong>(审查文本)进行单独的探索性分析，并基于其如何影响客户推荐产品的决定(推荐的IND)</li><li id="0bc3" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">分类模型</strong>以评论文本为自变量，预测客户是否推荐产品</li></ul><p id="6199" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为博客的目标更多的是理解文本挖掘，所以重点将是理解推荐产品的客户和不推荐产品的客户之间的差异，而不是根据评论预测客户的行为。换句话说，我们将更加关注模型的可变重要性和系数分数，而不是模型性能度量。</p><p id="e220" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就文本挖掘方法而言，有两大类</p><ul class=""><li id="f2ca" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc kz la lb lc bi translated"><strong class="ih hj">语义解析</strong>词序、词用作名词或动词、词的层次结构等都很重要</li><li id="87a6" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc kz la lb lc bi translated"><strong class="ih hj">单词包</strong>将所有单词作为一个单独的标记进行分析，顺序无关紧要。</li></ul><p id="b56a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的练习将只限于“单词包”方法，不会研究语义解析。</p><h2 id="d155" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">高级方法</h2><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es lw"><img src="../Images/b832786297238fbee8e766b4d8ecf871.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6tGLPftgMoCKIkk2OGIsCA.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">文本挖掘过程的高级方法</figcaption></figure><h1 id="7525" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">步骤1 —文本提取&amp;创建语料库</h1><h2 id="c349" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">初始设置</h2><p id="438e" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">文本挖掘所需的包是在R环境中加载的:</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="4d55" class="li jq hi mc b fi mg mh l mi mj"># install.packages("ggthemes")<br/># install.packages(qdap)<br/># install.packages(dplyr)<br/># install.packages(tm)<br/># install.packages(wordcloud)<br/># install.packages(plotrix)<br/># install.packages(dendextend)<br/># install.packages(ggplot2)<br/># install.packages(ggthemes)<br/># install.packages(RWeka)<br/># install.packages(reshape2)<br/># install.packages(quanteda)</span><span id="7177" class="li jq hi mc b fi mk mh l mi mj">library(qdap)<br/>library(dplyr)<br/>library(tm)<br/>library(wordcloud)<br/>library(plotrix)<br/>library(dendextend)<br/>library(ggplot2)<br/>library(ggthemes)<br/>library(RWeka)<br/>library(reshape2)<br/>library(quanteda)</span></pre><p id="8933" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一旦安装了所需的包，就设置了工作目录，并将csv文件读入R:</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="7ca6" class="li jq hi mc b fi mg mh l mi mj">setwd("C:/Users/Sowmya CR/Google Drive/datascience_blog/e-commerce")<br/>review=read.csv("Womens Clothing E-Commerce Reviews.csv", stringsAsFactors = FALSE)<br/>names(review)</span></pre><p id="e777" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="ks">自变量“stringsAsFactors”是r中“data.frame()”函数的自变量。它是一个逻辑自变量，指示数据帧中的字符串是应被视为因子变量还是仅被视为普通字符串。对于文本挖掘，我们通常将其设置为FALSE，以便将字符视为字符串，从而使我们能够适当地使用所有的文本挖掘技术。如果我们计划将变量用作分类变量</em>，则设置为真</p><h2 id="3e37" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">文本提取</h2><p id="b128" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">专栏<strong class="ih hj">评论。文本</strong>包含收到的各种产品的客户评论。这是我们分析的重点。我们现在将尝试理解如何将文本表示为数据框。</p><ol class=""><li id="a204" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc ml la lb lc bi translated">首先，将review.text转换成文本文档的集合或"<strong class="ih hj"> <em class="ks">文集</em> </strong>"。</li><li id="f906" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc ml la lb lc bi translated">为了将文本转换成语料库，我们在r。</li><li id="0521" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc ml la lb lc bi translated">为了使用tm创建语料库，我们需要将一个“Source”对象作为参数传递给VCorpus方法。</li><li id="bab2" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc ml la lb lc bi translated">源对象类似于抽象输入位置。我们这里使用的源是一个“Vectorsource ”,它只输入字符向量。</li><li id="5624" class="ku kv hi ih b ii ld im le iq lf iu lg iy lh jc ml la lb lc bi translated">Review.text列现在被转换成一个语料库，我们称之为“corpus_review”</li></ol><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="41e4" class="li jq hi mc b fi mg mh l mi mj">## <strong class="mc hj">Make a vector source and a corpus</strong><br/>corpus_review=Corpus(VectorSource(review$Review.Text))</span></pre><h1 id="6496" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">步骤2 —文本预处理</h1><p id="c7f8" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">使用“单词袋”方法的任何文本挖掘过程的最终目标是将待分析的文本转换成由文本中使用的单词及其频率组成的数据框架。这些由<strong class="ih hj">文档术语矩阵(DTM) </strong>和<strong class="ih hj">术语文档矩阵(TDM) </strong>定义，我们将在随后的章节中研究它们。<br/>为了确保DTM和TDM被清理并表示相关单词的核心集合，需要在语料库上执行一组预处理活动。这类似于在数据挖掘之前对结构化数据进行的数据清理。以下是一些常见的预处理步骤:</p><ol class=""><li id="8ea9" class="ku kv hi ih b ii ij im in iq kw iu kx iy ky jc ml la lb lc bi translated">转换为小写——这样，如果有两个单词“连衣裙”和“连衣裙”,它将转换为单个条目“连衣裙”</li></ol><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="a583" class="li jq hi mc b fi mg mh l mi mj">corpus_review=tm_map(corpus_review, tolower)</span></pre><p id="a405" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.删除标点符号:</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="9279" class="li jq hi mc b fi mg mh l mi mj">corpus_review=tm_map(corpus_review, removePunctuation)</span></pre><p id="445f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.删除停用词:“停用词”是在进行文本挖掘时需要理解的一个非常重要的概念。当我们写作时，文章通常由大量的介词、代词、连词等组成。在我们分析这篇课文之前，这些词需要去掉。否则，停用字词将出现在所有常用字词列表中，并且不会给出文本中使用的核心字词的正确图片。这里有一个英语常用停用词的列表，我们可以用这个命令查看:<code class="du mm mn mo mc b">stopwords(“en”)</code></p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="1372" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj">#Remove stopwords</strong><br/>corpus_review=tm_map(corpus_review, removeWords, stopwords("english"))</span></pre><p id="b7d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可能还想根据文本挖掘的上下文删除自定义的停用词。这些是数据集特有的词，可能不会给文本增加价值。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="da60" class="li jq hi mc b fi mg mh l mi mj"># <strong class="mc hj">Remove context specific stop words</strong><br/>corpus_review=tm_map(corpus_review, removeWords,c("also", "get","like", "company", "made", "can", "im", "dress", "just", "i"))</span></pre><h2 id="8f5b" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">给文件加词干</h2><blockquote class="mp mq mr"><p id="d3f8" class="if ig ks ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">在语言学中，词干化是将屈折(或派生)的单词简化为词干、词根或词根形式的过程，通常是一种书面单词形式。</p></blockquote><p id="b60a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">SnowballC包用于文档词干分析。例如，“复杂”、“复杂化”和“复杂化”在词干化后将被简化为“complicat”。这也是为了确保相同的单词不会在DTM和TDM中作为多个版本重复出现，并且我们在DTM和TDM中只表示该单词的词根。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="d7fc" class="li jq hi mc b fi mg mh l mi mj">## <strong class="mc hj">Stem document</strong><br/>corpus_review=tm_map(corpus_review, stemDocument)</span><span id="f763" class="li jq hi mc b fi mk mh l mi mj">##<strong class="mc hj">Viewing the corpus content</strong><br/>corpus_review[[8]][1]</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mv"><img src="../Images/9e8689f07ea39fa0397279810d690fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fjFw2UFjOL9aY7gTfs4yxg.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">语料库内容</figcaption></figure><p id="ec87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">R中的corpus对象是一个嵌套列表。我们可以对列表使用r语法来查看语料库的内容。</p><h2 id="f971" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">常用词</h2><p id="12ec" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">我们现在有了一个文本语料库，它是干净的，只包含文本挖掘所需的核心词。下一步是探索性分析。探索性数据分析的第一步是确定在整个综述文本中最常用的词。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="8a76" class="li jq hi mc b fi mg mh l mi mj"># <strong class="mc hj">Find the 20 most frequent terms: term_count</strong><br/>term_count &lt;- freq_terms(corpus_review, 20)</span><span id="bf76" class="li jq hi mc b fi mk mh l mi mj"># <strong class="mc hj">Plot 20 most frequent terms</strong><br/>plot(term_count)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/30b82a4c31bb07cc8f7e32994ce44092.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MRW3G13WvWzWt7Bm6YD4uQ.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">语料库中的常用词</figcaption></figure><p id="7414" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“爱”、“适合”、“大小”等词是最常用的词。</p><h1 id="9651" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">步骤3——从语料库中创建DTM和TDM</h1><p id="a730" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">预处理和清理的语料库被转换成称为文档术语矩阵的矩阵。</p><blockquote class="mp mq mr"><p id="e119" class="if ig ks ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">文档术语矩阵是描述术语在文档集合中出现的频率的数学矩阵。在文档-术语矩阵中，行对应于集合中的文档，列对应于术语。</p></blockquote><p id="e97a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">术语-文档矩阵是文档-术语矩阵的转置。它通常用于语言分析。开始分析信息的一个简单方法是使用as.matrix()将DTM/TDM转换成一个简单的矩阵。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="4d69" class="li jq hi mc b fi mg mh l mi mj">review_dtm &lt;- DocumentTermMatrix(corpus_review)<br/>review_tdm &lt;- TermDocumentMatrix(corpus_review)</span></pre><h2 id="b461" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">使用TDM识别常用术语</h2><p id="26ac" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">TDM还可以用于识别频繁的术语，并在随后的可视化中与评论文本相关。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="829f" class="li jq hi mc b fi mg mh l mi mj"># <strong class="mc hj">Convert TDM to matrix</strong><br/>review_m &lt;- as.matrix(review_tdm)<br/># <strong class="mc hj">Sum rows and frequency data frame</strong><br/>review_term_freq &lt;- rowSums(review_m)<br/># <strong class="mc hj">Sort term_frequency in descending order</strong><br/>review_term_freq &lt;- sort(review_term_freq, decreasing = T)<br/># <strong class="mc hj">View the top 10 most common words</strong><br/>review_term_freq[1:10]</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mx"><img src="../Images/27a85b82b6d66fc8088e2d407902601b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h2RkhqeeDKLd1KDKaKyYIA.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">来自TDM的前10个单词</figcaption></figure><h1 id="1133" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">步骤4——探索性文本分析</h1><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="08b8" class="li jq hi mc b fi mg mh l mi mj"># <strong class="mc hj">Plot a barchart of the 20 most common words</strong><br/>barplot(review_term_freq[1:20], col = "steel blue", las = 2)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/4d71b93ddf56988afec439ec583cc16a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TBQP_6TI7xBLALnnOcILqw.png"/></div></figure><h2 id="7887" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">单词云</h2><p id="0420" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">单词云是可视化文本语料库以理解常用单词的一种常见方式。单词云根据频率改变单词的大小。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="c771" class="li jq hi mc b fi mg mh l mi mj">review_word_freq &lt;- data.frame(term = names(review_term_freq),<br/>  num = review_term_freq)</span><span id="e7dd" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj"># Create a wordcloud for the values in word_freqs<br/></strong>wordcloud(review_word_freq$term, review_word_freq$num,<br/>  max.words = 50, colors = "red")</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/1ed740d381c2618625c51d6c6c3a3b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sv0RNGf5eIyV25abh0Mvqw.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">基于词频的词云</figcaption></figure><p id="4a6f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">单词云还可以接收一组颜色或调色板作为输入，以区分云中出现频率较高和较低的单词。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="c0e8" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj"># Print the word cloud with the specified colors<br/></strong>wordcloud(review_word_freq$term, review_word_freq$num,<br/>  max.words = 50, colors = c("aquamarine","darkgoldenrod","tomato"))</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/bc8b9d46f9803274ebd121c487500d3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r-N6DtnDy6pfc6von0eCYg.png"/></div></figure><h2 id="bfee" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">语料库对比</h2><p id="593a" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">这项研究的主要目的之一是分析推荐和不推荐产品的人在关键词上的差异。为此，我们将创建两个语料库——一个用于推荐-是，另一个用于推荐-否。对这两个语料库重复前面完成的所有预处理步骤。然后将频繁使用的词绘制为每个语料库的单独的条形图和词云，以了解推荐产品的客户和不推荐产品的客户使用的词的差异。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/8e339ad10fcc6ac66143e791ff93b11d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gB0vo416tm4YxmSzMl5aIg.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">推荐(绿色)与不推荐(红色)产品的顾客常用词</figcaption></figure><h2 id="6fbb" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">用于比较的词云</h2><p id="e3b9" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">比较单词集的另一种方式是组合是和否的语料库，并创建比较云，该比较云在同一云中显示两组单词。为此，我们将使用单词云的两个版本——<strong class="ih hj"><em class="ks">共性云</em> </strong>和<strong class="ih hj"> <em class="ks">对比云</em> </strong>。公共云将来自两个云中的单词组合在一个语料库中。它查找两个语料库之间共享的单词，并绘制共享单词的单词云。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="170a" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj">## Combine both corpora: all reviews</strong><br/>all_yes &lt;- paste(review_yes, collapse = "")<br/>all_no &lt;- paste(review_no, collapse = "")<br/>all_combine &lt;- c(all_yes, all_no)</span><span id="7781" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">## Creating corpus for combination</strong><br/>corpus_review_all=Corpus(VectorSource(all_combine)) </span><span id="37f3" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">## Pre-processing corpus - all</strong><br/><strong class="mc hj">#Convert to lower-case</strong><br/>corpus_review_all=tm_map(corpus_review_all, tolower)</span><span id="55c7" class="li jq hi mc b fi mk mh l mi mj">#<strong class="mc hj">Remove punctuation</strong><br/>corpus_review_all=tm_map(corpus_review_all, removePunctuation)</span><span id="ea81" class="li jq hi mc b fi mk mh l mi mj">#<strong class="mc hj">Remove stopwords</strong><br/>corpus_review_all=tm_map(corpus_review_all, removeWords, stopwords("english"))<br/>corpus_review_all=tm_map(corpus_review_all, removeWords,c("also", "get","like", "company", "made", "can", "im", "dress","just","i"))</span><span id="f538" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">#Stem document</strong><br/>corpus_review_all=tm_map(corpus_review_all, stemDocument)<br/>review_tdm_all &lt;- TermDocumentMatrix(corpus_review_all)<br/>all_m=as.matrix(review_tdm_all)<br/>colnames(all_m)=c("Yes","No")</span><span id="7418" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">#Sum rows and frequency data frame</strong><br/>review_term_freq_all &lt;- rowSums(all_m)<br/>review_word_freq_all &lt;- data.frame(term=names(review_term_freq_all), num = review_term_freq_all)</span><span id="5eda" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">#Make commonality cloud<br/></strong>commonality.cloud(all_m, <br/>                  colors = "steelblue1",<br/>                  max.words = 50)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/8d6f63674c6d60f1c5893950c7b64b1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BxMaE3uzO2B40NxUXCCCrg.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">公共云</figcaption></figure><p id="47f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一方面，比较云将识别两个语料库之间使用的不同单词。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="eec8" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj"># Create comparison cloud</strong><br/>comparison.cloud(all_m,<br/>                 colors = c("green", "red"),<br/>                 max.words = 50)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/3674f249b281f3cc1c93cf3d6f4b512c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JQF1vw1ujyp0oxMrDBln7w.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">比较云</figcaption></figure><p id="20d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对比云给出了对产品满意的人和不满意的人所用词汇的明显对比。没有推荐该产品的人使用了负面词汇，如失望、退货、便宜、外观等。</p><p id="6d50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一个值得注意的有趣的方面是，有“fabric”的单词的“No”列表和有“fit”的单词的“Yes”列表。这可能意味着大多数对产品不满意的人对面料不满意，而推荐产品的顾客对合身度很满意。因此，与合身性相比，整体面料质量可能是一个主要问题，企业可以在这些方面进行分析。</p><h2 id="871e" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">极化标签图</h2><p id="2dd5" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">极化标签图是通用云的改进版本。它决定了一个术语在两个比较语料库中的使用频率。在许多情况下，常见单词的频率差异可能很有见地。对于该图，加载了plotrix包。首先，使用一个子集创建一个包含所有常用单词的矩阵，以确保它只包含在两个类中出现的单词。然后，矩阵有另一列用于每个单词的两个语料库之间的绝对差异，并且绘制了图。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="7758" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj"># Identify terms shared by both documents<br/></strong>common_words &lt;- subset(all_m, all_m[, 1] &gt; 0 &amp; all_m[, 2] &gt; 0)</span><span id="ae8e" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj"># calculate common words and difference<br/></strong>difference &lt;- abs(common_words[, 1] - common_words[, 2])<br/>common_words &lt;- cbind(common_words, difference)<br/>common_words &lt;- common_words[order(common_words[, 3],<br/>                                   decreasing = T), ]<br/>head(common_words)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es my"><img src="../Images/0eea21a0053e0cefb213cbba5e16c7a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OSzgXX9Mmn5OXcJlNIxICg.png"/></div></div></figure><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="6d95" class="li jq hi mc b fi mg mh l mi mj">top25_df &lt;- data.frame(x = common_words[1:25, 1],<br/>                       y = common_words[1:25, 2],<br/>                       labels = rownames(common_words[1:25, ]))</span><span id="4c20" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj"># Make pyramid plot<br/></strong>pyramid.plot(top25_df$x, top25_df$y,<br/>             labels = top25_df$labels, <br/>             main = "Words in Common",<br/>             gap = 2000,<br/>             laxlab = NULL,<br/>             raxlab = NULL, <br/>             unit = NULL,<br/>             top.labels = c("Yes",<br/>                            "Words",<br/>                            "No")<br/>             )</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/e60b566920683ba910e73fccdea3441a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e03Ta0FlIySvnfUt67N-yA.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">极化标签图</figcaption></figure><p id="6efe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们的数据集中，推荐-是和推荐否的比例是不平衡的。大约82%的人推荐过该产品。所以“不”方的单词量相对较少。在这种不平衡的数据集中，使用两组之间的绝对%差异而不仅仅是绝对差异也是有用的。</p><h2 id="76fe" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">简单单词聚类</h2><p id="0125" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">单词聚类用于根据频率距离识别一起使用的单词组。这是一种降维技术。它有助于将单词组合成相关的簇。单词簇用树状图可视化。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="3288" class="li jq hi mc b fi mg mh l mi mj">review_tdm2 &lt;- removeSparseTerms(review_tdm, sparse = 0.9)<br/>hc &lt;- hclust(d = dist(review_tdm2, method = "euclidean"), method = "complete")</span><span id="afac" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj"># Plot a dendrogram<br/></strong>plot(hc)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/8532eeb56a107d28077e87cedf98d33c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i3jUPxhCm0hP-faql7agAw.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">词簇的树状图</figcaption></figure><p id="48e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">树状聚类图显示了某些单词是如何组合在一起的。例如，“柔软”、“材料”和“舒适”被一起使用。因为聚类是基于频率距离的，所以聚类指示哪组单词最频繁地一起使用。</p><h2 id="81fe" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">词汇联想</h2><p id="3d10" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">单词关联是一种计算DTM或TDM中两个单词之间相关性的方法。这是识别经常一起使用的单词的另一种方法。对于我们的语料库，单词关联图表示各种单词与单词“fit”之间的相关性。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="65e4" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj"># Create associations<br/></strong>associations &lt;- findAssocs(review_tdm, "fit", 0.05)</span><span id="e722" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj"># Create associations_df<br/></strong>associations_df &lt;- list_vect2df(associations)[, 2:3]</span><span id="4fc3" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj"># Plot the associations_df values <br/></strong>ggplot(associations_df, aes(y = associations_df[, 1])) + <br/>  geom_point(aes(x = associations_df[, 2]), <br/>             data = associations_df, size = 3) + <br/>  ggtitle("Word Associations to 'fit'") + <br/>  theme_gdocs()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/3b59b82c265ca00397d94945d5ac406c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g-tx0pulStOk9adg7AemFQ.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">与“适合”一词相关的词</figcaption></figure><p id="5e8a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">“适合”这个词与“完美”和“尺寸”的关联最大，这是产品的积极方面。与“适合”相关的第三高的词是“loos ”,它表示产品的负面影响。</p><h2 id="6644" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">N元语法的使用</h2><p id="6145" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">到目前为止，我们所做的所有分析都是基于被称为单字的单个单词。然而，它可以非常有见地地看待多个词。这在文本挖掘中被称为N元语法，其中N代表单词的数量。例如，bi-gram包含两个单词。<br/>我们现在将看看如何创建二元模型和三元模型，并对其进行一些探索性分析。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="7ad3" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj">##Create bi-grams</strong><br/>review_bigram &lt;- tokens(review$Review.Text) %&gt;%<br/>    tokens_remove("\\p{P}", valuetype = "regex", padding = TRUE) %&gt;%<br/>    tokens_remove(stopwords("english"), padding  = TRUE) %&gt;%<br/>    tokens_ngrams(n = 2) %&gt;%<br/>    dfm()<br/>topfeatures(review_bigram)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es mz"><img src="../Images/0fe187041da2bd021011b2cf4ecdfb51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cQ0kzfTW8HCTctkuajRMJQ.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">前10个双字母组合</figcaption></figure><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="662d" class="li jq hi mc b fi mg mh l mi mj">##Create tri-grams<br/>review_trigram &lt;- tokens(review$Review.Text) %&gt;%<br/>    tokens_remove("\\p{P}", valuetype = "regex", padding = TRUE) %&gt;%<br/>    tokens_remove(stopwords("english"), padding  = TRUE) %&gt;%<br/>    tokens_ngrams(n = 3) %&gt;%<br/>    dfm()<br/>topfeatures(review_trigram)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es na"><img src="../Images/d6ab2f679c1ade48cf1054974e135c7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a-VMrngW5yka-LG6Uxr7Pg.png"/></div></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">前10个三元组</figcaption></figure><p id="cfef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在前面章节中对一元语法所做的所有分析也可以在二元和三元语法上进行，以获得对文本的更多洞察。现在让我们来探讨推荐和不推荐产品的二元模型是如何变化的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/197bf14328bb4e39d797779c9e309a53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o2iWau_fK42CJbbrUIcQPQ.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">常用三元组柱状图:是与否</figcaption></figure><h1 id="d2c3" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">步骤5-通过去除稀疏性进行特征提取</h1><h2 id="98f3" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">稀疏概念</h2><p id="95bb" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">稀疏性与术语的文档频率有关。在DTM，因为术语构成了列，所以每个文档都有几个列，每个列代表一个术语——一元词、二元词、三元词等。矩阵中的列数将等于语料库中唯一单词的计数，这将相对较高。对于不常用的术语，该列在几个文档中可能为零。这就是所谓的稀疏性。在涉及DTM的任何分类练习之前，建议处理稀疏性。</p><h2 id="624a" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">特征抽出</h2><p id="9fac" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">探索性文本分析基于客户评论给出了一些见解。我们现在将使用相同的评论文本作为预测变量来预测客户是否会推荐该产品。就使用的分类算法而言，数据和文本输入之间没有太大区别。我们将尝试3种最流行的分类算法——CART、随机森林和Lasso逻辑回归。<br/>在转换适合建模的文本方面，我们可以使用上面提到的相同的包。然而，为了让读者熟悉更多的文本挖掘包，我们将使用稍微不同的方法。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="159c" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj">## Load the required libraries<br/></strong>library(irlba)<br/>library(e1071)<br/>library(caret)<br/>library(randomForest)<br/>library(rpart)<br/>library(rpart.plot)<br/>library(ggplot2)<br/>library(SnowballC)<br/>library(RColorBrewer)<br/>library(wordcloud)<br/>library(biclust)<br/>library(igraph)<br/>library(fpc)<br/>library(Rcampdf)</span></pre><h2 id="d512" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">符号化</h2><blockquote class="mp mq mr"><p id="d43e" class="if ig ks ih b ii ij ik il im in io ip ms ir is it mt iv iw ix mu iz ja jb jc hb bi translated">记号化是将文本分解成不同片段或记号的过程。</p></blockquote><p id="7912" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是我们在上一节中所说的词汇袋。一旦完成了toeknisation，在所有的预处理之后，就可以构建一个数据帧，其中每行代表一个文档，每列代表一个不同的标记，每个单元格给出一个文档的标记计数。这是我们在上一节学到的DTM。</p><p id="5e9d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">预处理步骤在标记上执行，类似于在文本语料库上所做的。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="25fa" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj"># Tokenize descriptions<br/></strong>reviewtokens=tokens(review$Review.Text,what="word",<br/>remove_numbers=TRUE,remove_punct=TRUE, remove_symbols=TRUE, remove_hyphens=TRUE)</span><span id="e434" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj"># Lowercase the tokens<br/></strong>reviewtokens=tokens_tolower(reviewtokens)</span><span id="0d2a" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj"># remove stop words and unnecessary words<br/></strong>rmwords &lt;- c("dress", "etc", "also", "xxs", "xs", "s")<br/>reviewtokens=tokens_select(reviewtokens, stopwords(),selection = "remove")<br/>reviewtokens=tokens_remove(reviewtokens,rmwords)</span><span id="3a2a" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj"># Stemming tokens<br/></strong>reviewtokens=tokens_wordstem(reviewtokens,language = "english")<br/>reviewtokens=tokens_ngrams(reviewtokens,n=1:2)</span></pre><p id="672e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">令牌现在被转换为文档频率矩阵，并进行稀疏性处理。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="dfb1" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj"># Creating a bag of words<br/></strong>reviewtokensdfm=dfm(reviewtokens,tolower = FALSE)</span><span id="e6e7" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj"># Remove sparsity<br/></strong>reviewSparse &lt;- convert(reviewtokensdfm, "tm")<br/>tm::removeSparseTerms(reviewSparse, 0.7)</span><span id="e708" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj"># Create the dfm</strong><br/>dfm_trim(reviewtokensdfm, min_docfreq = 0.3)<br/>x=dfm_trim(reviewtokensdfm, sparsity = 0.98)</span></pre><h1 id="8b20" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">步骤6 —建立分类模型</h1><p id="6149" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">我们现在有了经过预处理和处理的dfm，可以用于分类。为了在分类模型中使用它，执行以下步骤:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es nb"><img src="../Images/2a3a8bb52c1f5f21f2f50a437bd280ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xgOciYYf2MnSHgJPVMd2Sg.png"/></div></div></figure><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="b059" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj">## Setup a dataframe with features<br/></strong>df=convert(x,to="data.frame")</span><span id="33d0" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">##Add the Y variable Recommend.IND<br/></strong>reviewtokensdf=cbind(review$Recommended.IND,df)<br/>head(reviewtokensdf)</span><span id="e922" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">## Cleanup names<br/></strong>names(reviewtokensdf)[names(reviewtokensdf) == "review.Recommended.IND"] &lt;- "recommend"<br/>names(reviewtokensdf)=make.names(names(reviewtokensdf))<br/>head(reviewtokensdf)</span><span id="4246" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">## Remove the original review.text column<br/></strong>reviewtokensdf=reviewtokensdf[,-c(2)]<br/>head(reviewtokensdf)<br/>reviewtokensdf$recommend=factor(reviewtokensdf$recommend)</span></pre><h2 id="1515" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">购物车模型</h2><p id="b0f2" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">我们将首先使用CART算法进行分类。首先，建立完整的树，并确定误差最小的最佳cp值。然后这个cp值被用于获得修剪的树。被修剪的树被绘制以理解分类。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="b252" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj">## Build the CART model</strong><br/>tree=rpart(formula = recommend ~ ., data = reviewtokensdf, method="class",control = rpart.control(minsplit = 200,  minbucket = 30, cp = 0.0001))</span><span id="0759" class="li jq hi mc b fi mk mh l mi mj">printcp(tree)<br/>plotcp(tree)</span><span id="01b9" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">##Prune down the tree<br/></strong>bestcp=tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"]<br/>bestcp</span><span id="dbf2" class="li jq hi mc b fi mk mh l mi mj">ptree=prune(tree,cp=bestcp)</span><span id="83b4" class="li jq hi mc b fi mk mh l mi mj">rpart.plot(ptree,cex = 0.6)<br/>prp(ptree, faclen = 0, cex = 0.5, extra = 2)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/fd060e2c4094e060673cb8473c2932ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YOPcvf6PZ-qxxaDc3iUFPA.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">评论文本的购物车</figcaption></figure><p id="782f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从树形图中可以看出，“退货”、“失望”、“退货”、“巨大”等词被不满意的客户使用，即不推荐产品的客户。可以进一步解释该树，以了解推荐产品的客户和不推荐产品的客户所使用的词汇模式。</p><h2 id="7a76" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">随机森林</h2><p id="7dff" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">我们将使用的下一个分类算法是随机森林。我们将检查randomforest模型的varimp图，以了解哪些单词对分类的影响最大。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="55fe" class="li jq hi mc b fi mg mh l mi mj">library(randomForest)<br/>reviewRF=randomForest(recommend~., data=reviewtokensdf)<br/>varImpPlot(reviewRF, cex=.7)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/4a681a2c5526a3c1cf86b62ac115ae80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iVU4JaKyGicobrEC6gXPQA.png"/></div><figcaption class="lx ly et er es lz ma bd b be z dx translated">随机森林模型的方差图</figcaption></figure><p id="07ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与CART模型同步，随机森林模型的varimp图也表明“返回”、“失望”是最重要的变量。像“不幸的”、“悲伤的”等大多数其他词也表明这些负面的词通常被不快乐的顾客使用。</p><h2 id="cf02" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">Lasso逻辑回归</h2><p id="6f50" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">我们将使用的下一个分类算法是逻辑回归。正如我们在稀疏性中讨论的，与文本挖掘数据框架相关的主要挑战是非常多的列或特征。这将对逻辑回归等模型产生不利影响。因此，我们将使用使用lasso的正则化来减少特征，随后使用逻辑回归来建立模型和分类。逻辑回归模型的优势比将在分类上提供一些有用的见解。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="f720" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj">#load required library<br/></strong>library(glmnet)</span><span id="f07f" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">#convert training data to matrix format<br/></strong>x &lt;- model.matrix(recommend~.,reviewtokensdf)</span><span id="934b" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">#convert class to numerical variable<br/></strong>y &lt;- as.numeric(reviewtokensdf$recommend)</span><span id="15be" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">#perform grid search to find optimal value of lambda<br/></strong>cv.out &lt;- cv.glmnet(x,y,alpha=1,family="binomial",type.measure = "mse" )</span><span id="8dc7" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">#plot result<br/></strong>plot(cv.out)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/65db8b86c1a17d684df066935925451f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wj7cq35pt0IWNofh-PBhbA.png"/></div></figure><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="bf4d" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj">#min value of lambda<br/></strong>lambda_min &lt;- cv.out$lambda.min</span><span id="1376" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">#best value of lambda<br/></strong>lambda_1se &lt;- cv.out$lambda.1se<br/>lambda_1se</span><span id="ce1a" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj">#regression coefficients<br/></strong>coef=coef(cv.out,s=lambda_1se)<br/>lassocoef=as.matrix(coef(cv.out,s=lambda_1se))<br/>write.csv(lassocoef, "lasso_coef.csv")</span></pre><p id="75e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基于套索回归，我们得到了逻辑回归所需的lambda_min。对系数矩阵的检查将揭示由套索执行的特征减少。不需要包括在模型中的特征的系数为零。</p><p id="ffb6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了便于解释，coef已被写入一个csv文件。csv文件将保存到开始时设置的工作目录中。为了更清楚起见，让我们检查一下csv文件。</p><p id="d6cf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果csv文件在coef列上被过滤为零(列B)，我们发现285个特征(x变量)中的84个具有为零的coef-即，这些已经被套索模型移除。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nc"><img src="../Images/0dd9a1673edb642c0c4428ea7bdd03d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:472/format:webp/1*CP9chGz1zIYnpUugP1vV2w.png"/></div></figure><p id="66b7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">精简的特征集用于建立逻辑回归模型。从逻辑回归模型的系数，我们可以使用公式计算优势比:<br/> <strong class="ih hj">优势比= exp(coef(LR模型))</strong></p><p id="ac40" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">比值比是基于概率的模型如逻辑回归的一个非常独特的优势。我们将使用比值比来了解分类中各种x变量的影响。</p><pre class="je jf jg jh fd mb mc md me aw mf bi"><span id="a86c" class="li jq hi mc b fi mg mh l mi mj"><strong class="mc hj"># Find the best lambda using cross-validation<br/></strong>set.seed(123) <br/>cv.lasso &lt;- cv.glmnet(x, y, alpha = 1, family = "binomial")</span><span id="effa" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj"># Fit the final model on the dataframe<br/></strong>review_logreg &lt;- glmnet(x, y, alpha = 1, family = "binomial",<br/>                lambda = cv.lasso$lambda.min)</span><span id="cf2b" class="li jq hi mc b fi mk mh l mi mj"><strong class="mc hj"># Save the regression coef to a csv file<br/></strong>logregcoef=as.matrix(coef(review_logreg))<br/>odds_ratio=as.matrix(exp(coef(review_logreg)))<br/>write.csv(logregcoef, "logreg_coef.csv")<br/>write.csv(odds_ratio, "odds_ratio.csv")</span></pre><p id="106f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">与lasso coef一样，比值比也被写入csv文件以便更好地理解。让我们检查csv文件以获得进一步的见解。如果比值比的csv文件根据比值比(B列)按降序排序，我们可以检查比值比最高的变量:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nd"><img src="../Images/f035a5c7b48758b5bdf5e974953ea427.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*dRYhSMmg93AMTx4qNCwYxA.png"/></div></figure><p id="411b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">赔率解释如下:在评论中有“赞美”的产品比没有该词的产品评论有5.61倍的赔率被推荐。其他单词的比值比也可以类似地解释。</p><p id="ed8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们检查logreg_coef.csv文件，以了解一些具有负系数的变量:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nd"><img src="../Images/737a47e40e26579e5d0beea6baf2ef37.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/format:webp/1*RuwgVoKrkQg5YAt2nEosWw.png"/></div></figure><p id="75c2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">术语“令人失望”具有负系数，表示如果该术语出现在评论中，该产品被推荐的概率非常低。对于具有负系数的其他项，可以做出相同的解释。</p><p id="0fd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在推荐产品的快乐顾客和不快乐顾客使用的术语方面，所有3个分类模型都有几乎相似的见解。这些可以提供提高产品/服务质量的重要线索，从而提高产品的推销商。</p><h2 id="1038" class="li jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">结束语</h2><p id="c1b9" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">尽管这是一篇相对较长的文章，但这篇文章只涵盖了文本挖掘概念的冰山一角。这篇文章的主要目的是让读者了解文本挖掘的概念。本文还试图整合基本文本挖掘和文本分类所需的各种库和代码块，以便读者可以按照步骤对他们感兴趣的文本内容执行类似的分析。</p></div></div>    
</body>
</html>