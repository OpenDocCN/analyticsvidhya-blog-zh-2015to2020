<html>
<head>
<title>Data Preparation and Text-Preprocessing on Amazon Fine Food Reviews</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">亚马逊美食评论的数据准备和文本预处理</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/data-preparation-and-text-preprocessing-on-amazon-fine-food-reviews-7b7a2665c3f4?source=collection_archive---------4-----------------------#2019-12-06">https://medium.com/analytics-vidhya/data-preparation-and-text-preprocessing-on-amazon-fine-food-reviews-7b7a2665c3f4?source=collection_archive---------4-----------------------#2019-12-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="4c69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我将向您展示亚马逊美食评论的数据准备和文本预处理。</p><p id="c60d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但在此之前，你需要知道为什么文本预处理是必要的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/07c50fb98f564f96d7428ea53ea34188.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*3DevKJYTHdHq9n3YeJaurg.jpeg"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">亚马逊美食(图片提供:Kaggle)</figcaption></figure><h2 id="9f6c" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated"><strong class="ak">为什么我们需要做文字预处理？</strong></h2><p id="4c4f" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated">机器学习模型不处理文本数据，因此需要清理文本数据并将其转换为数字向量。这个过程称为文本预处理。</p><p id="0902" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些是我将在本文中向您展示的基本步骤。</p><p id="ba39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">了解数据</strong>:首先，你需要看到数据是关于什么的，有哪些参数(停用词，标点，html标签…etc)在数据中。</p><p id="a137" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据清洗:</strong>在这一步，我会去掉所有不必要的参数。</p><p id="148b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">编码文本数据的技术</strong>:编码文本数据的技术有很多。但是下面是我在解决现实世界问题时最常用的技巧。</p><ol class=""><li id="a931" class="kp kq hi ih b ii ij im in iq kr iu ks iy kt jc ku kv kw kx bi translated">一袋单词</li><li id="9aca" class="kp kq hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">双字母，n字母</li><li id="cfa9" class="kp kq hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">TF-IDF</li><li id="be71" class="kp kq hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">Avg-Word2Vec</li></ol><p id="d2d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们开始吧:</p><p id="1193" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，导入所有必需的库</p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="2dea" class="jp jq hi le b fi li lj l lk ll"><strong class="le hj">import</strong> pandas <strong class="le hj">as</strong> pd<br/><strong class="le hj">import</strong> numpy <strong class="le hj">as</strong> np<br/><strong class="le hj">import</strong> nltk<br/><strong class="le hj">import </strong>re<br/><strong class="le hj">from </strong>nltk.corpus <strong class="le hj">import</strong> stopwords<br/><strong class="le hj">from</strong> nltk.stem <strong class="le hj">import</strong> SnowBallStemmer<br/><strong class="le hj">from</strong> bs4 <strong class="le hj">import </strong>BeautifulSoup<br/><strong class="le hj">from </strong>sklearn.feature_extraction.text <strong class="le hj">import </strong>TfidfTransformer<br/><strong class="le hj">from </strong>sklearn.feature_extraction.text <strong class="le hj">import </strong>CountVectorizer<br/><strong class="le hj">from </strong>sklearn.feature_extraction.text <strong class="le hj">import </strong>TfidfVectorizer</span></pre><p id="1e43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，我们现在要导入亚马逊美食评论数据集</p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="4dd1" class="jp jq hi le b fi li lj l lk ll">data=pd.read_csv('./amazon-fine-food-reviews/Reviews.csv')</span></pre><p id="d831" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">检查数据</p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="1451" class="jp jq hi le b fi li lj l lk ll">data.head()  </span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es lm"><img src="../Images/c5ea4299a179624af91a845733c173f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UPhvINK_PGoCPPh7iCqlTg.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">亚马逊美食评论数据集</figcaption></figure><p id="acf1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">检查形状</p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="cc57" class="jp jq hi le b fi li lj l lk ll">data.shape</span><span id="7ae7" class="jp jq hi le b fi lr lj l lk ll">Output:(568454, 10)</span></pre><p id="0a47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">目标</strong>:给定一篇文字评论，预测评论是正面还是负面。</p><blockquote class="ls lt lu"><p id="341e" class="if ig lv ih b ii ij ik il im in io ip lw ir is it lx iv iw ix ly iz ja jb jc hb bi translated">但是这里我只做数据准备和文本预处理部分。</p></blockquote><p id="a480" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么我们就进入数据准备部分。</p><p id="f726" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据准备:</strong></p><p id="6fb0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们先看看“分数”一栏</p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="282a" class="jp jq hi le b fi li lj l lk ll">data['Score'].value_counts()</span><span id="11cf" class="jp jq hi le b fi lr lj l lk ll">Output:<br/>5    363122<br/>4     80655<br/>1     52268<br/>3     42640<br/>2     29769</span></pre><p id="dcb8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以如果你看到“分数”栏，它有1，2，3，4，5个值。我们的主要目标是预测给定的评论是正面的还是负面的。在这里，如果我们认为1.2是负面评价，4.5是正面评价，那么从逻辑上来说，3不会给我们的目标增加任何价值。因此，让我们丢弃那些“分数”=3的行</p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="2091" class="jp jq hi le b fi li lj l lk ll">data=data[data['Score']!=3]</span></pre><p id="8337" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，数据将只包含包含“Score”= 1和“Score”= 2的行。</p><p id="6e38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们将得分值转换成类标签“正”或“负”。</p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="741d" class="jp jq hi le b fi li lj l lk ll">def xyz(x):<br/>    if x&gt;3:<br/>        return 'positive'<br/>    else:<br/>        return 'negative'<br/>s=data['Score']<br/>d=list(map(xyz,s))<br/>data['Score']=d<br/>data</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es lz"><img src="../Images/93e8f5c44a63eebca99a30aeeab11254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0lbJv-yHXJcGQ5cHKv9sDw.png"/></div></div></figure><p id="87b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我将向您展示如何删除重复和不需要的记录。在这个阶段，您需要一些领域知识，因为这是数据科学中最“先进”的部分之一。</p><p id="3304" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我检查了基于UserId、ProfileName、Time、Text的副本(因为没有用户不能在同一时间评论相同的产品)。如果我发现任何重复，我会删除这些记录。</p><p id="0ed8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，有益分子必须小于或等于有益分母，因此检查记录并删除这些记录。</p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="dd34" class="jp jq hi le b fi li lj l lk ll">data_f=data.sort_values('ProductId').drop_duplicates(subset=['UserId','ProfileName','Time','Summary','Text'],keep='first',inplace=False)<br/>Final_Values=data_f[data_f['HelpfulnessDenominator']&gt;=data_f['HelpfulnessNumerator']]<br/>Final_Values</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es ma"><img src="../Images/813b5c654bbc1b2bec2ed8cc83aaf8d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xDRsCvX1ti32x80j6A7B4g.png"/></div></div></figure><p id="bd7e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们进入文本预处理阶段。</p><p id="6d77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">文本预处理</strong>:</p><p id="413d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如上所示，我在文本列上应用了这个文本预处理。</p><p id="125b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在开始文本处理之前，我想向你解释一些话题，如词干和停用词。</p><p id="e29a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">词干化:这是一种可以将单词转换成其基本单词或词干的技术(例如，tasty，tasty被转换成基本单词tasti …)</p><p id="376d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">停用词</strong>:这是一些不重要的词，即使你把它们从句子中去掉，文本的语义也不会改变。</p><p id="b837" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如:“这家餐馆很好”(这里‘This’，‘is’是停用词)</p><p id="a0c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">检查所有停用字词</p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="58f4" class="jp jq hi le b fi li lj l lk ll">stop=set(stopwords.words('english'))<br/>print(stop)</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mb"><img src="../Images/38f3440c1a2beb55cc2dd3acbd9d3330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DU1dufnyi7wlLxFt9Qkqyg.png"/></div></div></figure><p id="1c41" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，我检查了文本数据的几个样本，以了解哪些参数(html标记、标点符号、特殊字符、……等等)需要被移除以获得干净的文本。在检查了几个例子后，我发现句子中很少包含html标签、标点符号和特殊字符。所以，我需要删除这些参数来获得干净的文本。</p><p id="38b4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">以下是我为预处理</strong>所做的步骤:</p><p id="d5ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a.移除html标签。</p><p id="5ac1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b.删除所有标点符号和特殊字符</p><p id="c9ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">c.将单词转换成小写</p><p id="1a2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">d.删除停用词</p><p id="34d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">e.最后，我用雪球词干分析器来做词干。</p><p id="caa1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">下面是文本预处理的代码:</strong></p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="eb9e" class="jp jq hi le b fi li lj l lk ll">import nltk<br/>from nltk.corpus import stopwords<br/>from nltk.stem import SnowballStemmer<br/>import numpy<br/>import re<br/>from bs4 import BeautifulSoup<br/>stop=set(stopwords.words('english'))<br/>snow = nltk.stem.SnowballStemmer('english')<br/>stop<br/>def decontracted(phrase):<br/>    phrase=re.sub(r"won't","will not",phrase)<br/>    phrase=re.sub(r"can't","can not",phrase)<br/>    phrase=re.sub(r"n\'t","not",phrase)<br/>    phrase=re.sub(r"\'re","are",phrase)<br/>    phrase=re.sub(r"\'s","is",phrase)<br/>    phrase=re.sub(r"\'d","would",phrase)<br/>    phrase=re.sub(r"\'ll","will",phrase)    <br/>    phrase=re.sub(r"\'t","not",sentence)<br/>    phrase=re.sub(r"\'ve","have",sentence)<br/>    phrase=re.sub(r"\'m","am",sentence)<br/>    return phrase</span><span id="7745" class="jp jq hi le b fi lr lj l lk ll">preprocessed_reviews=[]</span><span id="d40b" class="jp jq hi le b fi lr lj l lk ll">for sentence in Final_Values['Text'].values:<br/>    sentence=re.sub(r"http\S+"," ",sentence)<br/>    sentence=BeautifulSoup(sentence,'lxml').get_text()<br/>    cleanr=re.compile('&lt;.*?&gt;')<br/>    sentence=re.sub(cleanr,' ',sentence)<br/>    sentence=decontracted(sentence)<br/>    sentence=re.sub("\S\*\d\S*"," ",sentence)<br/>    sentence=re.sub("[^A-Za-z]+"," ",sentence)<br/>    sentence=re.sub(r'[?|!|\'|"|#]',r' ',sentence)<br/>    sentence=re.sub(r'[.|,|)|(|\|/]',r' ',sentence)<br/>    sentence='  '.join(snow.stem(e.lower()) for e in sentence.split() if e.lower() not in stop)<br/>    preprocessed_reviews.append(sentence.strip())<br/></span></pre><p id="fb48" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们看看预处理后的文本是什么样的:</p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="ac03" class="jp jq hi le b fi li lj l lk ll">preprocessed_reviews[1700]</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mc"><img src="../Images/7c21f6484b66320cdfe3942ccc704be7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_3CGvWF-MgvuVAEt1FD1cA.png"/></div></div></figure><p id="e830" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你从上面看到的，我们在预处理后得到了干净的文本。</p><p id="a44c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们将应用一些文本编码技术。</p><h2 id="31ff" class="jp jq hi bd jr js jt ju jv jw jx jy jz iq ka kb kc iu kd ke kf iy kg kh ki kj bi translated"><strong class="ak"> <em class="md">文本编码技术</em> : </strong></h2><p id="7e2d" class="pw-post-body-paragraph if ig hi ih b ii kk ik il im kl io ip iq km is it iu kn iw ix iy ko ja jb jc hb bi translated"><strong class="ih hj">包话(鞠躬)</strong>:</p><p id="0cab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在BOW中，我们构建了一个字典，其中包含来自我们的评论文本数据集中的所有唯一单词。在这里，每个单词的出现频率都被计算在内。如果我们的数据集中有d个唯一的单词，那么对于每个评论，向量的长度将为d。在这种情况下，向量将非常稀疏。</p><p id="830d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是弓的基本概念。</p><blockquote class="ls lt lu"><p id="a5ff" class="if ig lv ih b ii ij ik il im in io ip lw ir is it lx iv iw ix ly iz ja jb jc hb bi translated">要了解更多信息，我在本文末尾提供了链接。请过一遍。</p></blockquote><p id="bef3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们在我预处理过的文本数据上应用蝴蝶结。</p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="9e34" class="jp jq hi le b fi li lj l lk ll">from sklearn.feature_extraction.text import CountVectorizer<br/>count=CountVectorizer()<br/>Reviews_BOW=count.fit_transform(preprocessed_reviews)<br/>print(Reviews_BOW[1]</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es me"><img src="../Images/0fbaba701f83430c52deefb134ea32cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jeBXc9Wqfg-kCaFLM4-Rjw.png"/></div></div></figure><p id="e5ed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">使用弓的缺点</strong>:</p><p id="67be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的输出可以看出，它是一种稀疏矩阵表示。我们的主要目标是相似意义的评论应该彼此接近。但是它不理解句子的语义。</p><p id="0d8f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们举个例子:</p><p id="bce8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设有两个评论:</p><p id="1a20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">r1:意大利面很好吃。</p><p id="4608" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">r2:意大利面不好吃。</p><p id="ae98" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如你所看到的，上面两个评论的语义有明显的不同。由于这两个评论不相似，所以它们对应的向量不应该彼此接近。但是在BOW中，在停用词移除之后，两个句子都将被转换为<strong class="ih hj"> pasta tasty </strong>，因此两个句子给出完全相同的意思，并且它们对应的向量将彼此接近，这是不正确的。</p><p id="7c5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们来看看第二种文本编码技术，它是二元语法，n元语法。</p><p id="c503" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">二元语法，多元语法:</strong></p><p id="e980" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">双字母组合基本上是指用于创建词典的两个连续单词对，三字母组合基本上是三个连续单词。</p><p id="a2d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Scikit-learn CountVectorizer有一个参数<code class="du mf mg mh le b">ngram_range</code>，如果它被赋值为(1，2)，那么它被称为Bi-gram。</p><p id="7e36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，让我们将二元语法应用于我预处理过的文本数据。</p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="fab8" class="jp jq hi le b fi li lj l lk ll">count=CountVectorizer(ngram_range=(1,2))<br/>Bigram_Counts=count.fit_transform(preprocessed_reviews)<br/>print(Bigram_Counts[1])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mi"><img src="../Images/63218f434220ef197e39fb9e7ebb4c90.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5zPa_St0Xbqh5ML0ocyuzA.png"/></div></div></figure><p id="85bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">双字格、n字格的弊端:</strong></p><p id="90d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它和BOW有同样的缺点，它也不接受文本的语义，而且它也大大增加了字典的大小。</p><p id="95bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们进入下一个文本编码技术，即TF-IDF</p><p id="eb43" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> TF-IDF: </strong></p><p id="2eb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">术语频率-逆文档频率(TF-IDF)对最频繁的词给予较低的重要性，而对不太频繁的词给予较高的重要性。</p><p id="96f4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">词频</strong>是某个<strong class="ih hj">特定单词(W) </strong>在评论中出现的次数除以评论中的总单词数<strong class="ih hj"> (Wr) </strong>。术语频率值的范围从0到1。</p><p id="4310" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">逆文档频率</strong>计算为<strong class="ih hj"> log(总文档数(N) /包含特定单词的文档数(n)) </strong>。在这里，文档被称为评论。</p><p id="9ed5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">TF-IDF</strong>=<strong class="ih hj">TF * IDF</strong>=<strong class="ih hj">(W/Wr)* LOG(N/N)</strong></p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="f7e2" class="jp jq hi le b fi li lj l lk ll">counts=TfidfVectorizer()<br/>cnt=counts.fit_transform(preprocessed_reviews)<br/>print(cnt[1])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mj"><img src="../Images/7dd55bed783b787173e5b5c2bab0f4d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ixnn6uDaonFF3az_4-KUWA.png"/></div></div></figure><p id="8caf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">TF-IDF的缺点:</strong></p><p id="8130" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，我们得到了每个单词的TF-IDF值。它和BOW，Bi-gram，n-gram有同样的缺点。它也不考虑文本的语义。</p><p id="f823" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以为了真正克服语义评论的问题，我将使用Word2Vec。</p><p id="8e71" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> Word2Vec: </strong></p><p id="fa13" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在进入Avg-Word2Vec之前，我将告诉你Word2Vec是如何工作的。</p><blockquote class="ls lt lu"><p id="cd61" class="if ig lv ih b ii ij ik il im in io ip lw ir is it lx iv iw ix ly iz ja jb jc hb bi translated">为了了解更多关于Word2Vec和它的数学直觉，我将在本文末尾给你一些链接。</p></blockquote><p id="6311" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它实际上获取了单词的语义以及它们与其他单词之间的关系。它学习单词之间的所有内部关系。它以密集的向量形式表示单词。</p><p id="adc0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我正在导入gensim库，它有Word2Vec，它采用类似于<strong class="ih hj"> min_count </strong> =5的参数，这意味着如果一个单词重复的次数少于5次，那么它将忽略这个单词，<strong class="ih hj"> size </strong> =50给出了一个长度为50的向量，<strong class="ih hj"> workers </strong>是运行这个的核心。</p><p id="e05f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">平均Word2Vec: </strong></p><p id="308b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要计算平均Word2Vec，请遵循以下步骤。</p><ol class=""><li id="c82c" class="kp kq hi ih b ii ij im in iq kr iu ks iy kt jc ku kv kw kx bi translated">计算每个单词的单词2Vec</li><li id="1c29" class="kp kq hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">将句子中每个单词的向量相加</li><li id="f7fb" class="kp kq hi ih b ii ky im kz iq la iu lb iy lc jc ku kv kw kx bi translated">然后用向量除以句子中的字数</li></ol><p id="4575" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它是所有单词的Word2Vec的简单平均值。</p><p id="5b07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面是计算平均Word2Vec的代码</p><pre class="je jf jg jh fd ld le lf lg aw lh bi"><span id="48d7" class="jp jq hi le b fi li lj l lk ll">from gensim.models import Word2Vec<br/>list_of_sentences=[]<br/>for sentence in preprocessed_reviews:<br/>    list_of_sentences.append(sentence.split())<br/>w2v_model=Word2Vec(list_of_sentences,min_count=5,size=50,workers=4)<br/>w2v_words=list(w2v_model.wv.vocab)<br/>sent_vectors=[]<br/>for sent in list_of_sentences:<br/>    sent_vec=np.zeros(50)<br/>    cnt_words=0<br/>    for word in sent:<br/>        if word in w2v_words:<br/>            vec=w2v_model.wv[word]<br/>            sent_vec=sent_vec+vec<br/>            cnt_words=cnt_words+1<br/>    if cnt_words!=0:<br/>        sent_vec=sent_vec/cnt_words<br/>    sent_vectors.append(sent_vec)<br/>print(len(sent_vectors))   <br/>print(sent_vectors[0])</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="ln lo di lp bf lq"><div class="er es mk"><img src="../Images/f0ce6b7da33dc6e53834e689a3e1a4ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3tgUncoaPcQo9oqPf5qLPA.png"/></div></div></figure><p id="bae5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">结论</strong>:</p><p id="d001" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我向您展示了一些将文本数据编码成数字向量的不同技术。但是哪种技术最适合您的模型，这完全取决于数据的结构、您将选择的模型、模型的目标以及最重要的业务需求。</p><p id="81d2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望你已经对本文中的数据准备和文本预处理技术有了基本的了解。</p></div><div class="ab cl ml mm gp mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="hb hc hd he hf"><p id="b1e2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">延伸阅读:</p><div class="ms mt ez fb mu mv"><a href="https://www.kaggle.com/snap/amazon-fine-food-reviews" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hj fi z dy na ea eb nb ed ef hh bi translated">亚马逊美食评论</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">下载数千个项目的开放数据集+在一个平台上共享项目。探索热门话题，如政府…</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">www.kaggle.com</p></div></div><div class="ne l"><div class="nf l ng nh ni ne nj jj mv"/></div></div></a></div><div class="ms mt ez fb mu mv"><a href="http://www.thushv.com/natural_language_processing/light-on-math-machine-learning-intuitive-guide-to-understanding-word2vec/" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hj fi z dy na ea eb nb ed ef hh bi translated">数学机器学习之光:理解Word2vec的直观指南</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">数学机器学习之光:理解Word2vec的直观指南下面是该系列的第三篇博文…</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">www.thushv.com</p></div></div><div class="ne l"><div class="nk l ng nh ni ne nj jj mv"/></div></div></a></div><div class="ms mt ez fb mu mv"><a href="https://machinelearningmastery.com/gentle-introduction-bag-words-model/" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hj fi z dy na ea eb nb ed ef hh bi translated">单词袋模型简介</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">词袋模型是在用机器学习算法对文本建模时表示文本数据的一种方式。的…</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">machinelearningmastery.com</p></div></div><div class="ne l"><div class="nl l ng nh ni ne nj jj mv"/></div></div></a></div><div class="ms mt ez fb mu mv"><a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hj fi z dy na ea eb nb ed ef hh bi translated">sk learn . feature _ extraction . text . tfidf vectorizer-sci kit-learn 0.22文档</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">class sk learn . feature _ extraction . text . tfidf vectorizer(input = ' content '，encoding='utf-8 '，decode _ error = ' strict '……</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">scikit-learn.org</p></div></div><div class="ne l"><div class="nm l ng nh ni ne nj jj mv"/></div></div></a></div><div class="ms mt ez fb mu mv"><a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hj fi z dy na ea eb nb ed ef hh bi translated">tf-idf</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">在信息检索中，tf-idf或TFIDF是词频-逆文档频率的缩写，是一种数值型词频。</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">en.wikipedia.org</p></div></div></div></a></div><div class="ms mt ez fb mu mv"><a href="http://www.nltk.org/index.html" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hj fi z dy na ea eb nb ed ef hh bi translated">自然语言工具包- NLTK 3.4.5文档</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">NLTK是构建Python程序来处理人类语言数据的领先平台。它提供了易于使用的…</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">www.nltk.org</p></div></div><div class="ne l"><div class="nn l ng nh ni ne nj jj mv"/></div></div></a></div><div class="ms mt ez fb mu mv"><a href="https://snowballstem.org/" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hj fi z dy na ea eb nb ed ef hh bi translated">雪球</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">编辑描述</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">snowballstem.org</p></div></div></div></a></div><div class="ms mt ez fb mu mv"><a href="https://www.dataquest.io/blog/machine-learning-preparing-data/" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab dw"><div class="mx ab my cl cj mz"><h2 class="bd hj fi z dy na ea eb nb ed ef hh bi translated">机器学习的数据清理和准备</h2><div class="nc l"><h3 class="bd b fi z dy na ea eb nb ed ef dx translated">数据清理和准备是任何机器学习项目中关键的第一步。虽然我们经常想到数据…</h3></div><div class="nd l"><p class="bd b fp z dy na ea eb nb ed ef dx translated">www.dataquest.io</p></div></div><div class="ne l"><div class="no l ng nh ni ne nj jj mv"/></div></div></a></div><p id="2186" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望这对你有帮助。感谢阅读:)</p></div></div>    
</body>
</html>