<html>
<head>
<title>Working With Sklearn Pipeline-Part1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Sklearn管道-第1部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/working-with-sklearn-pipeline-part1-419b32fc8b1?source=collection_archive---------24-----------------------#2020-04-26">https://medium.com/analytics-vidhya/working-with-sklearn-pipeline-part1-419b32fc8b1?source=collection_archive---------24-----------------------#2020-04-26</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/887a550bfe5f429696c4c9fe6c4849c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P6NdiTAJDwekdV7Fyo3aMA.jpeg"/></div></div></figure><p id="10b4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">今天的帖子是三个部分的第一部分，我们将了解Sklearn管道，以及我们如何在建模过程中有效地集成它们。在这一部分中，我们将使用FeatureUnions集成不同的特性，并在我们的管道中使用它。在这个练习中，我们使用了一个来自<a class="ae jo" href="https://www.kaggle.com/c/nlp-getting-started/overview" rel="noopener ugc nofollow" target="_blank"> kaggle </a>的简单数据集。</p><p id="ec28" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们首先来看看数据集</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="0f27" class="jy jz hi ju b fi ka kb l kc kd">def pull_data(train_pth,test_pth):<br/>   <em class="ke">"""<br/>   train_pth: Training Data Path<br/>   test_pth: Test Data Path<br/>   <br/>   """<br/>   </em>train_dt = pd.read_csv(train_pth)<br/>   test_dt = pd.read_csv(test_pth)<br/><br/>   return train_dt,test_dt</span></pre><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kf"><img src="../Images/b6be9d69ce4392598870c8385914f0fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OhPj7PNYtNf3pCOV4V4kYQ.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">训练数据</figcaption></figure><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es kk"><img src="../Images/63ba0b28443118fd59ec598f986d8931.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*xdtp9bwqZdiez1Tm9nDg3g.png"/></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">测试数据</figcaption></figure><p id="3660" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是一个简单的文本分类二进制数据集，现在我们将专注于文本部分，看看我们如何在其上开发基本功能，并使用FeatureUnion来合并这些自定义功能和已内置的功能(tfidf，standardscaler)。根据scikit-learn文档，理论上，<em class="ke"> FeatureUnion将几个transformer对象组合成一个新的transformer，该transformer将它们的输出组合起来。A </em> <code class="du kl km kn ju b"><a class="ae jo" href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion" rel="noopener ugc nofollow" target="_blank"><strong class="is hj"><em class="ke">FeatureUnion</em></strong></a></code> <em class="ke">取一个transformer对象列表。在拟合过程中，每一个都独立地拟合到数据。转换器并行应用，它们输出的特征矩阵并排连接成一个更大的矩阵。</em></p><p id="e110" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，基本上我们可以开发不同的特征(位置标签、数字特征)并将它们组合起来，为模型提供一个完整的矩阵来进行训练，多酷啊！！让我们试试第一个特性，看看结果矩阵是什么样的。文本最重要和最基本的自定义特性之一是计算文本的基本统计数据，如“总字数”、“字符数”、“标点数”、…</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="2992" class="jy jz hi ju b fi ka kb l kc kd">class FeatureMultiplierCount(BaseEstimator, TransformerMixin):<br/>    def __init__(self, word_count=True,char_count=True,<br/>                word_density=True,total_length=True,<br/>                capitals=True,caps_vs_length=True,num_exclamation_marks=True,num_question_marks=True,<br/>                num_punctuation=True,num_symbols=True,num_unique_words=True,words_vs_unique=True,<br/>                word_unique_percent=True):<br/>        self.word_count = word_count<br/>        self.total_length = total_length<br/><br/>    def transform(self, X,y=None):<br/>        X = pd.DataFrame(X)<br/>        X['word_count'] = X['text'].apply(lambda x : len(x.split()))<br/>        X['char_count'] = X['text'].apply(lambda x : len(x.replace(" ","")))<br/>        X['word_density'] = X['word_count'] / (X['char_count'] + 1)<br/><br/>        X['total_length'] = X['text'].apply(len)<br/>        X['capitals'] = X['text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))<br/>        X['caps_vs_length'] = X.apply(lambda row: float(row['capitals'])/float(row['total_length']),axis=1)<br/>        X['num_exclamation_marks'] =X['text'].apply(lambda x: x.count('!'))<br/>        X['num_question_marks'] = X['text'].apply(lambda x: x.count('?'))<br/>        X['num_punctuation'] = X['text'].apply(lambda x: sum(x.count(w) for w in '.,;:'))<br/>        X['num_symbols'] = X['text'].apply(lambda x: sum(x.count(w) for w in '*&amp;$%'))<br/>        X['num_unique_words'] = X['text'].apply(lambda x: len(set(w for w in x.split())))<br/>        X['words_vs_unique'] = X['num_unique_words'] / X['word_count']<br/>        X['word_unique_percent'] =  X["num_unique_words"]*100/X['word_count']<br/>        <br/>        return X[['word_count','char_count','word_density','total_length',<br/>                 'capitals','caps_vs_length','num_exclamation_marks','num_question_marks',<br/>                 'num_punctuation','num_symbols','num_unique_words','words_vs_unique',<br/>                 'word_unique_percent']]<br/><br/>    def fit(self, *_):<br/>        return self</span></pre><p id="427d" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果你们中的许多人不熟悉python中的Baseestimator，这里有一个对它的快速解释，<em class="ke"> Baseestimator提供了</em> <code class="du kl km kn ju b"><strong class="is hj"><em class="ke">get_params</em></strong></code> <em class="ke">和</em> <code class="du kl km kn ju b"><strong class="is hj"><em class="ke">set_params</em></strong></code> <em class="ke">方法的默认实现，基本上是你自己的定制估算器，用于定制特性等任务。这主要是为了使模型网格可通过</em> <code class="du kl km kn ju b"><strong class="is hj"><em class="ke">GridSearchCV</em></strong></code> <em class="ke">进行自动参数调整，并在与Scikit-Learn Pipeline结合使用时表现良好。</em></p><p id="4905" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">主要的计算发生在变换方法中。根据Scikit-Learn pipeline，任何自定义特性都应该有“fit”和“transform”方法，原因很明显。</p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ko"><img src="../Images/710e25c17f69eaa9cf9c1e2e11680fba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pdjx8T8UUb6T4bTRDscb2g.png"/></div></div></figure><p id="2618" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另一个很棒的功能是计算文本的词类特征，在一个简短的解释中，我们尝试计算句子中出现的“NNS”(名词)、“VB”(动词)、“DT”(行列式)的数量。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="49ef" class="jy jz hi ju b fi ka kb l kc kd">class PosTagMatrix(BaseEstimator, TransformerMixin):<br/>    #normalise = True - devide all values by a total number of tags in the sentence<br/>    #tokenizer - take a custom tokenizer function<br/>    def __init__(self, tokenizer=lambda x: x.split(), normalize=True):<br/>        self.tokenizer=tokenizer<br/>        self.normalize=normalize<br/><br/>    #helper function to tokenize and count parts of speech<br/>    def pos_func(self, sentence):<br/>        return Counter(tag for word,tag in nltk.pos_tag(self.tokenizer(sentence)))<br/><br/>    # fit() doesn't do anything, this is a transformer class<br/>    def fit(self, X, y = None):<br/>        return self<br/><br/>    #all the work is done here<br/>    def transform(self, X):<br/><br/>        X_tagged = X.apply(self.pos_func).apply(pd.Series).fillna(0)<br/>        X_tagged['n_tokens'] = X_tagged.apply(sum, axis=1)<br/>        if self.normalize:<br/>            X_tagged = X_tagged.divide(X_tagged['n_tokens'], axis=0)<br/>        columns_ =['PRP','VBP','DT','NN','IN','NNP']<br/><br/>        common_columns = set(X_tagged.columns).intersection(columns_)<br/><br/>        add_columns = set(columns_)-common_columns<br/><br/>        for cols in add_columns:<br/><br/>            X_tagged[cols]=0.0<br/><br/>           <br/><br/>        return X_tagged[columns_]</span></pre><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kp"><img src="../Images/b221a88eff081121445f40eefbb9c627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E8FbWnPv7qDRe-1Bj0reNw.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">位置标签的输出</figcaption></figure><p id="9f64" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在使用Tfidf等功能之前，我们也使用了基本的清洁过程，清洁过程也是自定义功能的形式。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="bd4b" class="jy jz hi ju b fi ka kb l kc kd">class FeatureCleaner(BaseEstimator, TransformerMixin):<br/>    def __init__(self, clean=True):<br/>        self.clean = clean<br/>        <br/>    def clean_and_normalize_text_data(self,sentence):<br/>        sentence = re.sub(r'[^a-zA-Z\s]', ' ', sentence, re.I|re.A)<br/>        sentence = sentence.lower()<br/>        sentence = sentence.strip()<br/>        tokens = wpt.tokenize(sentence)<br/>        stemmed_words = [ps.stem(w) for w in tokens]<br/>        #remove stopwords<br/>        filtered_tokens = [token for token in stemmed_words if token not in stops]<br/>        filtered_len =[token for token in filtered_tokens if len(token)&gt;2]<br/>        filtered_len = np.unique(filtered_len)<br/>        sentence = ' '.join(filtered_len)<br/>        return sentence<br/>        <br/><br/>    def transform(self, X,y=None):<br/>        c =pd.Series([self.clean_and_normalize_text_data(x) for x in X])<br/>        return c<br/><br/>    def fit(self, *_):<br/>        return self</span></pre><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kq"><img src="../Images/08fd3ea2a3a1d7a05c3cbfa3c1aa15d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MiLsbtiqE1S0Wd_qhTWiqA.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">清洗过程的输出</figcaption></figure><p id="843a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">sklearn的Tfidf、Standardscaler等标准特性不需要任何修改，因为它们已经内置了fit和transform方法，所以我们可以简单地在管道中调用它们并执行它们。现在让我们合并管道，对于这个例子，我使用的是模型ExtratreesClassifier。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="8816" class="jy jz hi ju b fi ka kb l kc kd">ET_pipeline_pos_tag = Pipeline([<br/>   ('u1', FeatureUnion([<br/>      ('tfdif_features', Pipeline([('cleaner', FeatureCleaner()),<br/>                            ('tfidf', TfidfVectorizer(max_features=40000, ngram_range=(1, 3))),<br/>                            ])),<br/>      ('numerical_features', Pipeline([('numerical_feats', FeatureMultiplierCount()),<br/>                               ('scaler', StandardScaler()), ])),<br/><br/>      ('pos_features', Pipeline([<br/>         ('pos', PosTagMatrix(tokenizer=nltk.word_tokenize)),<br/>      ])),<br/>   ])),<br/>   ('clf', ExtraTreesClassifier(n_estimators=50)),<br/>])</span></pre><p id="74ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们分解一下管道，以便详细了解:</p><ol class=""><li id="693c" class="kr ks hi is b it iu ix iy jb kt jf ku jj kv jn kw kx ky kz bi translated">我们创建一个tfidf特性管道，其第一步是使用FeatureCleaner清理数据，然后在其上应用Tfidf。</li><li id="6df0" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">第二个特性是FeatureMultiplier管道，它从文本中计算数字特性，并在其上应用StandardScaler。</li><li id="458b" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">第三个特征是后置定语矩阵，在这里我们可以找到与一个句子相关的后置定语的数量。</li><li id="725a" class="kr ks hi is b it la ix lb jb lc jf ld jj le jn kw kx ky kz bi translated">然后，我们使用特征联合来并行组合这些特征，并用树外分类器对其进行拟合。</li></ol><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/72b28c05330b2b98b1903a60163e18e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u0lRjpn3hbYJeCa2p_Vm-Q.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">安装管道后</figcaption></figure><p id="76fb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">还可以使用joblib保存模型/管道，并将其作为常规模型进行预测。</p><pre class="jp jq jr js fd jt ju jv jw aw jx bi"><span id="11d8" class="jy jz hi ju b fi ka kb l kc kd">dump_file = 'extratrees.joblib'<br/>joblib.dump(ET_pipeline_pos_tag, dump_file, compress=1)</span></pre><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lg"><img src="../Images/2571e706dd4252becc714cfeca198f37.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pUC2YFHxlGvRUAsXHUKpPA.png"/></div></div><figcaption class="kg kh et er es ki kj bd b be z dx translated">模型结果</figcaption></figure><p id="a33f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Scikit-Learn管道是加速和模块化代码的好方法。它基本上平滑了你的整个建模过程。在下一个系列中，我们将探索如何使用Scikit learn pipeline来适应不同的模型。如果这篇文章有任何帮助，请鼓掌:)</p><p id="39bb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">感谢阅读！！</p></div></div>    
</body>
</html>