<html>
<head>
<title>Intel OpenVINO: Model Optimizer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">英特尔OpenVINO:模型优化器</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/intel-openvino-model-optimizer-e381affa458c?source=collection_archive---------5-----------------------#2020-07-23">https://medium.com/analytics-vidhya/intel-openvino-model-optimizer-e381affa458c?source=collection_archive---------5-----------------------#2020-07-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="a128" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我的<a class="ae jd" rel="noopener" href="/swlh/introduction-to-intel-openvino-toolkit-5f98dbb30ffb">上一篇文章</a>中，我已经讨论了OpenVINO工具包的基础和工作流程。在这篇文章中，我们将探索:-</p><ul class=""><li id="2c8a" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">什么是模型优化器？</li><li id="2d50" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">配置模型优化器</li><li id="a9ea" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">将ONNX模型转换为中间表示</li><li id="df95" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">将Caffe模型转换为中间表示</li><li id="1ca2" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">将张量流模型转换为中间表示</li></ul><figure class="jt ju jv jw fd jx er es paragraph-image"><div class="er es js"><img src="../Images/48edc9a7968773f116c63b6d69d82c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*1CAlUjDyQRGf0ZUIRrgGoQ.png"/></div></figure><h1 id="0d2f" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">什么是模型优化器？</h1><p id="bdc9" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">模型优化器是OpenVINO工具包的两个主要组件之一。模型优化器的主要目的是将模型转换为中间表示(IR)。模型的中间表示(IR)包含一个<strong class="ih hj">。xml文件</strong>和一个<strong class="ih hj">。bin </strong>文件。您需要这两个文件来运行推理。</p><ul class=""><li id="4578" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated"><strong class="ih hj">。xml </strong> - &gt;包含了模型架构的其他重要元数据。</li><li id="a01d" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated"><strong class="ih hj">。bin </strong> - &gt;包含二进制格式的模型权重和偏差。</li></ul><p id="f249" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">中间表示(IRs)是OpenVINO Toolkit的标准结构和神经网络架构的命名。TensorFlow中的“Conv2D”层、Caffe中的“卷积”层或ONNX中的“Conv”层都转换为IR中的“卷积”层。您可以在这里找到每个中间表示层本身的更深入的数据<a class="ae jd" href="https://docs.openvinotoolkit.org/2019_R3/_docs_MO_DG_prepare_model_convert_model_IRLayersCatalogSpec.html" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="9bf4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">OpenVINO支持的框架:-</p><ul class=""><li id="9159" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated">张量流</li><li id="143f" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">咖啡</li><li id="0219" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">MXNet</li><li id="1b0f" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">ONNX(PyTorch和Apple ML)</li><li id="ae05" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated">卡尔迪</li></ul><h1 id="2a36" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">配置模型优化器</h1><p id="90d1" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">要使用模型优化器，您需要配置它，配置模型优化器非常简单，可以在命令提示符/终端中完成。</p><p id="3ae1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">要配置模型优化器，请遵循以下步骤(在命令提示符/终端中键入命令):-</p><ol class=""><li id="800a" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc ld jk jl jm bi translated">转到Openvino目录:-</li></ol><p id="c328" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Linux:- <code class="du le lf lg lh b">cd opt/intel/openvino</code></p><p id="9452" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Windows:- <code class="du le lf lg lh b">cd C:/Program Files (x86)/IntelSWTools/openvino</code></p><p id="972c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我在上面的命令中使用了默认的安装目录，如果您的安装目录不同，那么请导航到适当的目录。</p><p id="e75d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">2.转到install _ prerequitites目录:-</p><p id="18c5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><code class="du le lf lg lh b">cd deployment_tools/model_optimizer/install_prerequisites</code></p><p id="33a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">3.运行<code class="du le lf lg lh b">install_prerequisites</code>文件</p><p id="712c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Windows:- <code class="du le lf lg lh b">install_prerequisites.bat</code></p><p id="a91d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于Linux:- <code class="du le lf lg lh b">install_prerequisites.sh</code></p><p id="06ca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果您想要为特定的框架配置模型，那么运行以下命令:-</p><p id="08a8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">张量流</strong> :-</p><p id="1739" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Windows:- <code class="du le lf lg lh b">install_prerequisites_tf.bat</code></p><p id="b355" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Linux:- <code class="du le lf lg lh b">install_prerequisites_tf.sh</code></p><p id="f71a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">咖啡</strong> :-</p><p id="bf04" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Windows:- <code class="du le lf lg lh b">install_prerequisites_caffe.bat</code></p><p id="94ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Linux:- <code class="du le lf lg lh b">install_prerequisites_caffe.sh</code></p><p id="95a3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> MXNet </strong> :-</p><p id="8dcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">视窗:- <code class="du le lf lg lh b">install_prerequisites_mxnet.bat</code></p><p id="9a6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Linux:- <code class="du le lf lg lh b">install_prerequisites_mxnet.sh</code></p><p id="9cd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> ONNX </strong> :-</p><p id="0c8e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">视窗:- <code class="du le lf lg lh b">install_prerequisites_onnx.bat</code></p><p id="1677" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Linux:- <code class="du le lf lg lh b">install_prerequisites_onnx.sh</code></p><p id="f7ea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">卡尔迪 :-</p><p id="825f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Windows:- <code class="du le lf lg lh b">install_prerequisites_kaldi.bat</code></p><p id="33c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Linux:- <code class="du le lf lg lh b">install_prerequisites_kaldi.sh</code></p><h1 id="da43" class="ka kb hi bd kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx bi translated">转换为中间表示</h1><p id="89a7" class="pw-post-body-paragraph if ig hi ih b ii ky ik il im kz io ip iq la is it iu lb iw ix iy lc ja jb jc hb bi translated">在成功配置了模型优化器之后，我们现在就可以使用模型优化器了。在本文中，我将向您展示如何将ONNX、Caffe和TensorFlow转换为中间表示。ONNX和Caffe的转换非常简单，但是Tensorflow模型的转换有点复杂。</p><p id="4005" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">转换ONNX型号</strong></p><p id="4945" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">OpenVINO不直接支持PyTorch相反，PyTorch模型被转换成ONNX格式，然后由模型优化器转换成中间表示。</p><p id="85d3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将下载和转换“盗梦空间_V1”。你可以从这个<a class="ae jd" href="https://docs.openvinotoolkit.org/2019_R3/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_ONNX.html" rel="noopener ugc nofollow" target="_blank">链接</a>找到其他型号。</p><p id="461f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下载“盗梦空间_V1”后，解压缩文件并提取到你想要的位置。在“inception_v1”目录中，您会找到“model.onnx”文件。我们需要将该文件提供给模型优化器。</p><p id="0c47" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">遵循以下步骤:-</p><ol class=""><li id="3ad8" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc ld jk jl jm bi translated">打开命令提示符/终端，将当前工作目录更改为“model.onnx”文件所在的位置</li><li id="221b" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc ld jk jl jm bi translated">运行以下命令:-</li></ol><pre class="jt ju jv jw fd li lh lj lk aw ll bi"><span id="0568" class="lm kb hi lh b fi ln lo l lp lq">python opt/intel/opevino/deployment_tools/model_optimizer/mo.py --input_model model.onnx</span></pre><ul class=""><li id="7b57" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated"><strong class="ih hj"> - input_model </strong> - &gt;取我们想要转换的模型。</li></ul><p id="613f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的命令是在Linux中运行的，我使用了默认的安装目录，如果你的安装目录不同，那么使用适当的路径到“mo.py”。</p><pre class="jt ju jv jw fd li lh lj lk aw ll bi"><span id="2d20" class="lm kb hi lh b fi ln lo l lp lq">python &lt;installation_directory&gt;/opevino/deployment_tools/model_optimizer/mo.py --input_model model.onnx</span></pre><p id="05ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">成功运行该命令后，您将会收到。xml“和”。bin”文件。</p><p id="9037" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">转换咖啡模型</strong></p><p id="b0b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Caffe模型的转换过程非常简单，类似于ONNX模型。不同之处在于，对于Caffe模型，模型优化器采用了一些特定于Caffe模型的附加参数。您可以在<a class="ae jd" href="https://docs.openvinotoolkit.org/2019_R3/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_Caffe.html#Convert_From_Caffe" rel="noopener ugc nofollow" target="_blank">文档</a>中找到更多详细信息。</p><p id="3953" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将下载并转换<a class="ae jd" href="https://github.com/DeepScale/SqueezeNet" rel="noopener ugc nofollow" target="_blank"> SqueezeNet V1.1 </a>模型。</p><p id="a54e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">遵循以下步骤:-</p><ol class=""><li id="6ac3" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc ld jk jl jm bi translated">打开命令提示符/终端，将当前工作目录更改为“squeezenet_v1.1.caffemodel”文件所在的位置</li><li id="a194" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc ld jk jl jm bi translated">运行以下命令:-</li></ol><pre class="jt ju jv jw fd li lh lj lk aw ll bi"><span id="9e5a" class="lm kb hi lh b fi ln lo l lp lq">python opt/intel/opevino/deployment_tools/model_optimizer/mo.py --input_model squeezenet_v1.1.caffemodel --input_proto deploy.prototxt</span></pre><ul class=""><li id="f226" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated"><strong class="ih hj"> - input_model → </strong>获取我们想要转换的模型。</li><li id="0acc" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated"><strong class="ih hj"> - input_proto → </strong>将包含拓扑结构和层属性的文件(deploy.prototxt)作为输入。</li></ul><p id="4c4d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果文件名为"。caffemodel“和”。prototxt”相同，则不需要参数“— input_proto”。</p><p id="050d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">成功运行该命令后，您将会收到。xml“和”。bin”文件。</p><p id="9ea3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">转换张量流模型</strong></p><p id="c29d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">开放模型动物园中的TensorFlow模型采用冻结和解冻格式。TensorFlow中的某些模型可能已经为您冻结了。您可以冻结模型，也可以使用中的单独说明来转换未冻结的模型。</p><p id="270d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以使用以下代码来冻结一个未冻结的模型。</p><pre class="jt ju jv jw fd li lh lj lk aw ll bi"><span id="3597" class="lm kb hi lh b fi ln lo l lp lq">import tensorflow as tf</span><span id="8a35" class="lm kb hi lh b fi lr lo l lp lq">from tensorflow.python.framework import graph_io</span><span id="7e48" class="lm kb hi lh b fi lr lo l lp lq">frozen = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def, ["name_of_the_output_node"])</span><span id="402a" class="lm kb hi lh b fi lr lo l lp lq">graph_io.write_graph(frozen, './', 'inference_graph.pb', as_text=False)</span></pre><ul class=""><li id="a7d6" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated"><strong class="ih hj"> sess → </strong>是定义网络拓扑的TensorFlow* Session对象的实例。</li><li id="579f" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated"><strong class="ih hj"> ["输出节点名称"] → </strong>是图中输出节点名称的列表；“冻结”图将仅包括直接或间接用于计算给定输出节点的原始“sess.graph_def”中的那些节点。</li><li id="a87f" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated"><strong class="ih hj">。/ → </strong>是应该生成推理图文件的目录。</li><li id="56c5" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated"><strong class="ih hj">推理图. pb </strong> →是生成的推理图文件的名称。</li><li id="8a3f" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated"><strong class="ih hj"> as_text </strong> →指定生成的文件是人类可读的文本格式还是二进制格式。</li></ul><p id="508b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将下载和转换更快的R-CNN盗梦空间V2可可模型。你可以从这个<a class="ae jd" href="https://docs.openvinotoolkit.org/2019_R3/_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html" rel="noopener ugc nofollow" target="_blank">链接</a>找到其他型号。</p><p id="29ad" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下载“更快的R-CNN盗梦空间V2可可”后，解压缩文件并将其提取到您想要的位置。在“faster _ rcnn _ inception _ v2 _ coco _ 2018 _ 01 _ 28”目录里面，你会找到“frozen_inference_graph.pb”文件。我们需要将该文件提供给模型优化器。</p><p id="d3e9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">遵循以下步骤:-</p><ol class=""><li id="15f8" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc ld jk jl jm bi translated">打开命令提示符/终端，将当前工作目录更改为“freeze _ inference _ graph . Pb”文件所在的位置</li><li id="40d0" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc ld jk jl jm bi translated">运行以下命令:-</li></ol><pre class="jt ju jv jw fd li lh lj lk aw ll bi"><span id="761d" class="lm kb hi lh b fi ln lo l lp lq">python /opt/intel/openvino/deployment_tools/model_optimizer/mo.py --input_model frozen_inference_graph.pb --tensorflow_object_detection_api_pipeline_config pipeline.config --reverse_input_channels --tensorflow_use_custom_operations_config /opt/intel/openvino/deployment_tools/model_optimizer/extensions/front/tf/ssd_v2_support.json</span></pre><p id="b85d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面的命令是在Linux中运行的，我使用了默认的安装目录，如果你的安装目录不同，那么使用适当的路径到“mo.py”。</p><ul class=""><li id="a7b2" class="je jf hi ih b ii ij im in iq jg iu jh iy ji jc jj jk jl jm bi translated"><strong class="ih hj"> - input_model → </strong>获取我们想要转换的模型。</li><li id="8dd2" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated"><strong class="ih hj">-tensor flow _ Object _ Detection _ api _ pipeline</strong>→用于生成借助对象检测API创建的模型的管道配置文件的路径。</li><li id="d2a1" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated"><strong class="ih hj">-reverse _ input _ channels</strong>→TF模型动物园模型是在RGB(红绿蓝)图像上训练的，而OpenCV通常加载为BGR(蓝绿红)。</li><li id="200d" class="je jf hi ih b ii jn im jo iq jp iu jq iy jr jc jj jk jl jm bi translated"><strong class="ih hj">-tensor flow _ use _ custom _ operations _ config</strong>→使用带有自定义操作描述的配置文件。</li></ul><p id="c32f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">成功运行该命令后，您将会收到。xml“和”。bin”文件。</p><p id="2b09" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">非常感谢您阅读这篇文章，我希望现在您已经对模型优化器有了正确的理解。</p></div></div>    
</body>
</html>