<html>
<head>
<title>My Notes On Ernie 2.0| Baidu Research | SOTA Glue Benchmark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我的欧尼2.0笔记|百度研究| SOTA胶水基准</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/my-notes-on-ernie-2-0-baidu-research-sota-glue-benchmark-a62951439bc5?source=collection_archive---------21-----------------------#2019-12-17">https://medium.com/analytics-vidhya/my-notes-on-ernie-2-0-baidu-research-sota-glue-benchmark-a62951439bc5?source=collection_archive---------21-----------------------#2019-12-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/7015813d3086f1883770e28966016e97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SIoGa-KYdA3mgSUdW0XMnw.png"/></div></div></figure><h1 id="64b0" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">简介:</h1><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es jo"><img src="../Images/ee164c49409a8fda2b7605f1f8fb8a3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*UcXBsHAg7zXNQHRNu3wZNQ.png"/></div></figure><p id="b569" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">ERNIE 2.0是一个用于语言理解的持续预训练框架，其中预训练任务可以通过多任务学习逐步构建和学习。</p><p id="6fa8" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">Ernie之前的算法主要关注单词和句子的共现，但Ernie随之引入了标记层、结构层和语义层的理解。</p><p id="2b13" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated"><strong class="jv hj">主要优势——持续学习:目的是用几个任务按顺序训练模型，使它记住以前的任务。</strong></p><figure class="jp jq jr js fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es kr"><img src="../Images/9252a04aec625adee6ea6d1c521cabbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZmGWk87E86dmilEXsR70Yw.png"/></div></div></figure><h1 id="ea46" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">过去算法的主要贡献(2018–2019):</h1><ol class=""><li id="5563" class="ks kt hi jv b jw ku ka kv ke kw ki kx km ky kq kz la lb lc bi translated"><strong class="jv hj"> ELMO </strong> —从语言模型中提出上下文相关特征。</li><li id="9fb0" class="ks kt hi jv b jw ld ka le ke lf ki lg km lh kq kz la lb lc bi translated"><strong class="jv hj">开艾GPT </strong> —嵌入式变压器</li><li id="3ff5" class="ks kt hi jv b jw ld ka le ke lf ki lg km lh kq kz la lb lc bi translated"><strong class="jv hj"> BERT </strong> —掩蔽语言模型和句子预测任务进入预训练。</li><li id="c531" class="ks kt hi jv b jw ld ka le ke lf ki lg km lh kq kz la lb lc bi translated"><strong class="jv hj"> XLM </strong> —整合跨语言模式。</li><li id="a1a1" class="ks kt hi jv b jw ld ka le ke lf ki lg km lh kq kz la lb lc bi translated"><strong class="jv hj"> MT-DNN </strong> —基于预先训练的模型，在GLUE[14]中一起学习几个监督任务。</li><li id="93a8" class="ks kt hi jv b jw ld ka le ke lf ki lg km lh kq kz la lb lc bi translated"><strong class="jv hj"> XLNET </strong> —广义自回归预训练方法，通过在因子分解顺序的所有排列上最大化期望似然来学习双向上下文。</li></ol><h1 id="477b" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">模型结构:</h1><ol class=""><li id="c1ac" class="ks kt hi jv b jw ku ka kv ke kw ki kx km ky kq kz la lb lc bi translated">Transformer Encoder — Ernie 2.0使用类似于BERT的多层转换器。[CLS]令牌加在开头。在多个输入段之间添加了[SEP]标记。</li><li id="411f" class="ks kt hi jv b jw ld ka le ke lf ki lg km lh kq kz la lb lc bi translated">任务嵌入-每个任务被分配一个唯一的id，范围从0到nN</li><li id="2ebf" class="ks kt hi jv b jw ld ka le ke lf ki lg km lh kq kz la lb lc bi translated">模型的输入—令牌+段+位置+任务嵌入</li></ol><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es li"><img src="../Images/6a4a0e88ef08c0e332c25dc4f8d6dac7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1092/format:webp/1*C9lft9NCuPw0pESNfnh-fA.png"/></div></figure><h1 id="415e" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">培训前任务:</h1><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es lj"><img src="../Images/79a7874ded7cd6430cb788cfedb61607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*1YUJ8o_HErt7kI3lL3necg.png"/></div></figure><h2 id="4a52" class="lk ir hi bd is ll lm ln iw lo lp lq ja ke lr ls je ki lt lu ji km lv lw jm lx bi translated">单词感知任务—</h2><p id="ef67" class="pw-post-body-paragraph jt ju hi jv b jw ku jy jz ka kv kc kd ke ly kg kh ki lz kk kl km ma ko kp kq hb bi translated">知识屏蔽任务——预测整个屏蔽阶段和命名实体。</p><p id="1ada" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">大写任务——基于大写单词具有更多语义重要性的假设。像NER一样帮助完成任务。</p><p id="f586" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">令牌文档关系预测任务-预测令牌是否出现在一个片段中，以及是否出现在其他实例中。有助于提高特定令牌的重要性。</p><h2 id="c981" class="lk ir hi bd is ll lm ln iw lo lp lq ja ke lr ls je ki lt lu ji km lv lw jm lx bi translated">结构感知任务—</h2><p id="2bda" class="pw-post-body-paragraph jt ju hi jv b jw ku jy jz ka kv kc kd ke ly kg kh ki lz kk kl km ma ko kp kq hb bi translated">句子重新排序——段落被随机分成1到m段，并应用k类分类问题。它有助于学习句子之间的关系。</p><p id="abc7" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">句子距离任务-这是一个3级分类问题，其中“0”表示句子是相邻的；“1”表示句子在同一文档中，“3”表示句子在不同的文档中。</p><h2 id="8ba7" class="lk ir hi bd is ll lm ln iw lo lp lq ja ke lr ls je ki lt lu ji km lv lw jm lx bi translated">语义感知任务—</h2><p id="688c" class="pw-post-body-paragraph jt ju hi jv b jw ku jy jz ka kv kc kd ke ly kg kh ki lz kk kl km ma ko kp kq hb bi translated">话语关系任务——预测两个句子之间的语义/修辞关系。</p><p id="b26b" class="pw-post-body-paragraph jt ju hi jv b jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">信息检索相关性任务——这是一个3级分类问题，其中“0”表示查询和标题是强烈隐含的；“1”表示查询和标题是弱隐含的,“3”表示查询和标题不相关。</p><h1 id="96e2" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">结果:</h1><figure class="jp jq jr js fd ij er es paragraph-image"><div class="er es mb"><img src="../Images/e21b6922ccca74b7e83e74df71d06a96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*AvgEBdpzcU-a62-gm40Sgw.png"/></div></figure><h1 id="b876" class="iq ir hi bd is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn bi translated">参考资料:</h1><div class="mc md ez fb me mf"><a href="https://gluebenchmark.com/leaderboard/" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hj fi z dy mk ea eb ml ed ef hh bi translated">粘合基准</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">通用语言理解评估(GLUE)基准是一个培训、评估…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">gluebenchmark.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt io mf"/></div></div></a></div><div class="mc md ez fb me mf"><a href="https://arxiv.org/abs/1907.12412v1" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hj fi z dy mk ea eb ml ed ef hh bi translated">ERNIE 2.0:语言理解的持续预训练框架</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">最近，预训练模型在各种语言理解任务中取得了最先进的结果，这…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">arxiv.org</p></div></div></div></a></div></div></div>    
</body>
</html>