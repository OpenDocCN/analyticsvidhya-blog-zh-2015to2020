<html>
<head>
<title>Custom Keras Models and tf functions in Tensorflow 2.1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Tensorflow 2.1中的自定义Keras模型和tf函数</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/custom-keras-models-and-tf-functions-in-tensorflow-2-1-8641dc9eabd7?source=collection_archive---------2-----------------------#2020-04-10">https://medium.com/analytics-vidhya/custom-keras-models-and-tf-functions-in-tensorflow-2-1-8641dc9eabd7?source=collection_archive---------2-----------------------#2020-04-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="433c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将探讨如何使用<strong class="ih hj"> <em class="jd"> tf.functions </em> </strong>来提高定制keras模型的训练速度。</p><h1 id="dbae" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">定义模型</h1><p id="cc49" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">在我们公司，我们用不同形状的样品训练模型。让我们定义一个定制的玩具模型，它可以接受在第一维中长度变化的输入。</p><p id="6989" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型在其第一维上执行reduce_max，然后通过激活<strong class="ih hj"> relu </strong>使其通过一系列密集层。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="km kn l"/></div></figure><h1 id="d72e" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">定义数据集</h1><p id="b1e8" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">我们定义一个生成器，它产生第一维中长度不同的样本。然后创建一个数据集，其中每个示例的形状为(X，20 ),其中X可以在1和10之间变化。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="km kn l"/></div></figure><h1 id="f4da" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">定义训练循环</h1><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="km kn l"/></div></figure><h1 id="b192" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">热切模式下的训练</h1><p id="bb08" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">默认情况下，tensorflow 2.1以渴望模式运行一切。渴望模型对于模型开发来说非常方便，因为它允许我们轻松地设置断点并进入模型代码。然而渴望模式是缓慢的。让我们看看对1000个例子进行训练的时间。</p><pre class="kh ki kj kk fd ko kp kq kr aw ks bi"><span id="5dd8" class="kt jf hi kp b fi ku kv l kw kx">train_data = dataset.take(1000)</span><span id="816e" class="kt jf hi kp b fi ky kv l kw kx">%%time<br/>train(train_data, train_step)</span><span id="4874" class="kt jf hi kp b fi ky kv l kw kx">CPU times: user 8.11 s, sys: 269 ms, total: 8.38 s<br/>Wall time: 8 s</span></pre><h1 id="60a4" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">用tf函数训练</h1><p id="302e" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">Tf函数提供了将python函数自动转换为tensorflow图的功能，tensorflow运行时可以高效地执行该图。</p><pre class="kh ki kj kk fd ko kp kq kr aw ks bi"><span id="6e57" class="kt jf hi kp b fi ku kv l kw kx">tf_train_step = tf.function(train_step)</span><span id="ba95" class="kt jf hi kp b fi ky kv l kw kx">%%time<br/>train(train_data, tf_train_step)</span><span id="d346" class="kt jf hi kp b fi ky kv l kw kx">WARNING:tensorflow:5 out of the last 6 calls to &lt;function train_step at 0x148a40bf8&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to <a class="ae kz" href="https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args</a> and <a class="ae kz" href="https://www.tensorflow.org/api_docs/python/tf/function" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/function</a> for more details.<br/>WARNING:tensorflow:6 out of the last 8 calls to &lt;function train_step at 0x148a40bf8&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to <a class="ae kz" href="https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args</a> and <a class="ae kz" href="https://www.tensorflow.org/api_docs/python/tf/function" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/function</a> for more details.<br/>WARNING:tensorflow:7 out of the last 10 calls to &lt;function train_step at 0x148a40bf8&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to <a class="ae kz" href="https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args</a> and <a class="ae kz" href="https://www.tensorflow.org/api_docs/python/tf/function" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/function</a> for more details.<br/>WARNING:tensorflow:8 out of the last 11 calls to &lt;function train_step at 0x148a40bf8&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to <a class="ae kz" href="https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args</a> and <a class="ae kz" href="https://www.tensorflow.org/api_docs/python/tf/function" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/function</a> for more details.<br/>WARNING:tensorflow:6 out of the last 11 calls to &lt;function train_step at 0x148a40bf8&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to <a class="ae kz" href="https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args</a> and <a class="ae kz" href="https://www.tensorflow.org/api_docs/python/tf/function" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/function</a> for more details.<br/>WARNING:tensorflow:6 out of the last 12 calls to &lt;function train_step at 0x148a40bf8&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to <a class="ae kz" href="https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args</a> and <a class="ae kz" href="https://www.tensorflow.org/api_docs/python/tf/function" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/function</a> for more details.<br/>CPU times: user 2.65 s, sys: 501 ms, total: 3.15 s<br/>Wall time: 2.33 s</span></pre><p id="eaea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们注意到所用的时间从8.44秒大幅提高到了2.25秒。我们还注意到tensorflow发出了一堆关于撤销的警告。</p><p id="900a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因为范例的形状不同，所以会发生回溯。每当函数遇到下面没有见过的图形时，它将为该图形编译一个新的图形。编制一张新的图表是昂贵的。在我们的例子中，数据集可以有10种不同的形状，因此重走操作最多会发生10次。</p><h1 id="1d82" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">用带有签名的tf函数进行训练</h1><p id="0d5b" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">为了避免对每个新形状进行回溯，我们可以为tf函数提供一个输入签名。变化的尺寸可以用“无”表示。</p><pre class="kh ki kj kk fd ko kp kq kr aw ks bi"><span id="8a97" class="kt jf hi kp b fi ku kv l kw kx">tf_train_step_signature = tf.function(<br/>    train_step,<br/>    input_signature=[<br/>        tf.TensorSpec(shape=(None, None, 20), dtype=tf.float32), <br/>        tf.TensorSpec(shape=(None,), dtype=tf.float32)<br/>    ]<br/>)</span><span id="1180" class="kt jf hi kp b fi ky kv l kw kx">%%time<br/>train(train_data, tf_train_step_signature)</span><span id="832c" class="kt jf hi kp b fi ky kv l kw kx">CPU times: user 1.22 s, sys: 451 ms, total: 1.67 s<br/>Wall time: 889 ms</span></pre><h1 id="167c" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated">比较带签名和不带签名的tf函数</h1><p id="6b12" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">使用签名避免了回溯，训练时间从2.25秒进一步提高到889毫秒。在决定使用签名更好之前，让我们用更多的示例执行训练，并添加一些工具。</p><pre class="kh ki kj kk fd ko kp kq kr aw ks bi"><span id="a570" class="kt jf hi kp b fi ku kv l kw kx">train_data_large = dataset.take(20000)</span><span id="a87e" class="kt jf hi kp b fi ky kv l kw kx">from time import time</span><span id="8c2b" class="kt jf hi kp b fi ky kv l kw kx">def timer(f, timings):<br/>    def wraps(*args, **kwargs):<br/>        st = time()<br/>        res = f(*args, **kwargs)<br/>        en = time()<br/>        timings.append(en-st)<br/>    return wraps</span><span id="3a8c" class="kt jf hi kp b fi ky kv l kw kx">timings_tf = []<br/>tf_train_step = timer(tf.function(train_step), timings_tf)</span><span id="8ca5" class="kt jf hi kp b fi ky kv l kw kx">%%time<br/>train(train_data_large, tf_train_step)</span><span id="39d1" class="kt jf hi kp b fi ky kv l kw kx">WARNING:tensorflow:5 out of the last 6 calls to &lt;function train_step at 0x148a40bf8&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to <a class="ae kz" href="https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args</a> and <a class="ae kz" href="https://www.tensorflow.org/api_docs/python/tf/function" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/function</a> for more details.<br/>WARNING:tensorflow:6 out of the last 9 calls to &lt;function train_step at 0x148a40bf8&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to <a class="ae kz" href="https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args</a> and <a class="ae kz" href="https://www.tensorflow.org/api_docs/python/tf/function" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/function</a> for more details.<br/>WARNING:tensorflow:7 out of the last 10 calls to &lt;function train_step at 0x148a40bf8&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to <a class="ae kz" href="https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args</a> and <a class="ae kz" href="https://www.tensorflow.org/api_docs/python/tf/function" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/function</a> for more details.<br/>WARNING:tensorflow:6 out of the last 12 calls to &lt;function train_step at 0x148a40bf8&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to <a class="ae kz" href="https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args</a> and <a class="ae kz" href="https://www.tensorflow.org/api_docs/python/tf/function" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/function</a> for more details.<br/>CPU times: user 17.4 s, sys: 8.34 s, total: 25.8 s<br/>Wall time: 11.8 s</span><span id="aac1" class="kt jf hi kp b fi ky kv l kw kx">timings_tf_signature = []<br/>tf_train_step_signature = timer(<br/>    tf.function(<br/>        train_step,<br/>        input_signature=[<br/>            tf.TensorSpec(shape=(None, None, 20), dtype=tf.float32), <br/>            tf.TensorSpec(shape=(None,), dtype=tf.float32)<br/>        ]<br/>    ),<br/>    timings_tf_signature<br/>)</span><span id="3afe" class="kt jf hi kp b fi ky kv l kw kx">%%time<br/>train(train_data_large, tf_train_step_signature)</span><span id="735d" class="kt jf hi kp b fi ky kv l kw kx">CPU times: user 19.5 s, sys: 8.43 s, total: 28 s<br/>Wall time: 13.4 s</span></pre><p id="3fed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">结果是，从长远来看，没有签名的tf函数比有签名的tf函数快。绘制时间表可以清楚地说明为什么会发生这种情况。</p><pre class="kh ki kj kk fd ko kp kq kr aw ks bi"><span id="622a" class="kt jf hi kp b fi ku kv l kw kx">from matplotlib.pylab import plt<br/>plt.plot(timings_tf[0:100])</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es la"><img src="../Images/0b9c274cf370bc49ee5b3e7c15ca38e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*V29qYF-BfQEuJ2YCKWEJ_A.png"/></div></figure><p id="3860" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该图绘制了运行每个训练步骤的时间。在情节中有一些最初的尖峰。每当对图形进行代价高昂的重寻时，就会出现峰值。但是一旦遇到数据中所有可能的形状，就没有进一步的回溯。</p><pre class="kh ki kj kk fd ko kp kq kr aw ks bi"><span id="046e" class="kt jf hi kp b fi ku kv l kw kx">plt.plot(timings_tf_signature[0:100])</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es la"><img src="../Images/aa58ff2220e3f71df2790589bd62c005.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*1j7XWB-q2p45wGGU4rvKJw.png"/></div></figure><p id="b15e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于具有签名的tf函数的图，不会发生回溯，因此在构造图的最初的第一个昂贵步骤之后，其余的步骤花费相似的时间量。</p><p id="8b35" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，现在让我们在放弃所有初始的昂贵的回溯操作之后，比较在两种情况下运行一个步骤所需的时间。</p><pre class="kh ki kj kk fd ko kp kq kr aw ks bi"><span id="3c91" class="kt jf hi kp b fi ku kv l kw kx">plt.ioff()<br/>plt.plot(timings_tf[100:], label="tf func")<br/>plt.plot(timings_tf_signature[100:], label="tf func with signature")<br/>plt.legend(loc="upper left")<br/>plt.show()</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div class="er es ld"><img src="../Images/67a9fa6b03cd667c0c1062e97a2d6c81.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*BTO0oDilgHE5I8E0frVjew.png"/></div></figure><p id="ab84" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该图清楚地表明，具有签名的tf函数确实较慢。</p><h1 id="fb91" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak">图内训练循环</strong></h1><p id="0e70" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">也可以将整个“train”函数包装成tf函数。在这种情况下，包括数据集迭代的整个训练发生在编译的张量流图内。</p><pre class="kh ki kj kk fd ko kp kq kr aw ks bi"><span id="04fa" class="kt jf hi kp b fi ku kv l kw kx">tf_train = tf.function(train)</span><span id="7c52" class="kt jf hi kp b fi ky kv l kw kx">%%time<br/>tf_train(train_data_large, train_step)</span><span id="38ff" class="kt jf hi kp b fi ky kv l kw kx">CPU times: user 14.6 s, sys: 8.37 s, total: 22.9 s<br/>Wall time: 8.01 s</span></pre><h1 id="dab6" class="je jf hi bd jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb bi translated"><strong class="ak">结论</strong></h1><p id="e7ed" class="pw-post-body-paragraph if ig hi ih b ii kc ik il im kd io ip iq ke is it iu kf iw ix iy kg ja jb jc hb bi translated">默认情况下，tensorflow 2.1以渴望模式运行，这对于开发和调试非常有用。通过将你的函数包装在“tf.function”中可以提高性能，TF . function会编译一个tensorflow图，框架可以有效地执行该图。</p><p id="6a6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">有几种方法可以将部分代码包装成“tf.function”。在演示的toy示例中，当整个训练发生在tf.function中时，可以观察到最佳性能。</p><p id="c215" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，在现实世界的使用案例中，有几个因素在起作用-所有数据集是在内存中还是从磁盘/网络流出，模型的计算负载，是否使用了gpu /多个gpu，训练数据的形状如何变化等。人们应该通过实验来弄清楚如何最有效地使用“tf.function”。</p><p id="bca1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> ps </strong>:你可以在<a class="ae kz" href="https://colab.research.google.com/github/saswatac/tfplayground/blob/master/Custom%20Keras%20models%20and%20tf%20functions.ipynb" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/github/saswatac/TF playground/blob/master/Custom % 20k eras % 20 models % 20和%20tf%20functions.ipynb </a>查看colab中笔记本形式的帖子</p><p id="cdca" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> pps </strong>:查看我们公司的网站<a class="ae kz" href="https://neuralconcept.com/" rel="noopener ugc nofollow" target="_blank">https://neuralconcept.com/</a>，在这里我们正在打造一款深度学习产品，学习对工程模拟进行建模。</p></div></div>    
</body>
</html>