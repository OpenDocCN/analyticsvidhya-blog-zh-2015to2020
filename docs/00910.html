<html>
<head>
<title>Analyzing Amazon TV reviews with Latent Dirichlet Allocation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于潜在狄利克雷分配的亚马逊电视评论分析</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/analyzing-amazon-tv-reviews-with-latent-dirichlet-allocation-97d19c3bb878?source=collection_archive---------3-----------------------#2019-09-18">https://medium.com/analytics-vidhya/analyzing-amazon-tv-reviews-with-latent-dirichlet-allocation-97d19c3bb878?source=collection_archive---------3-----------------------#2019-09-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/48b8f0505d5f25b715bd844dd36e981b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*_xuQ0Ey8sDifX-Sih_yngQ.png"/></div></figure><p id="8116" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在本文中，我将执行并解释使用潜在狄利克雷分配进行主题建模的步骤。</p><p id="62c1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这项任务的目的是在顾客评论的帮助下，比较某些品牌电视之间的质量、易用性和其他潜在关系。</p><p id="da95" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当处理文本数据时，我们通常从一些固定的步骤开始，换句话说，就是预处理数据。</p><p id="3dc4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了预处理数据，我们将遵循以下步骤:</p><ol class=""><li id="0cad" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jp jq jr js bi translated">选择所需的栏，即评论/评论。</li><li id="ab6e" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">标记句子。</li><li id="a53b" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">删除停用词。</li><li id="6a0a" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">制作符号化单词的二元模型版本。</li><li id="a51d" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">对每个句子中的单词进行词条解释，并保留所需的词性。</li><li id="98f7" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">创建每个单词的词典、语料库和词频。</li><li id="0fec" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">借助于上一步中创建的用于主题建模的语料库来创建基本LDA模型。</li><li id="a08e" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">相干分数和超参数调谐。</li><li id="9fe6" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">使用适当的技术分析和可视化您的结果。</li></ol><p id="52a7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第一步:选择所需的栏目，即评论/评论。</strong></p><p id="0934" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在进行任何模型拟合之前，我们首先将必要的包导入jupyter笔记本</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="b4a4" class="kh ki hi kd b fi kj kk l kl km">import numpy as np<br/>import pandas as pd</span><span id="322e" class="kh ki hi kd b fi kn kk l kl km">import nltk<br/>import gensim<br/>import pyLDAvis<br/>import pyLDAvis.gensim<br/>from gensim.utils import simple_preprocess<br/>import gensim.corpora as corpora<br/>from gensim.models import LdaMulticore<br/>from gensim.models import CoherenceModel<br/>from gensim.utils import ClippedCorpus<br/>import tqdm</span><span id="dc72" class="kh ki hi kd b fi kn kk l kl km">from nltk.corpus import stopwords</span><span id="4c0c" class="kh ki hi kd b fi kn kk l kl km">import warnings<br/>warnings.filterwarnings('ignore')</span><span id="31c9" class="kh ki hi kd b fi kn kk l kl km">import spacy<br/>from pprint import pprint</span><span id="063e" class="kh ki hi kd b fi kn kk l kl km">import matplotlib.pyplot as plt<br/>%matplotlib inline</span></pre><p id="31c6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们现在将导入csv文件并查看第一行和最后5行，以便对数据有所了解并继续进行预处理。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="3930" class="kh ki hi kd b fi kj kk l kl km">review = pd.read_csv('Amazon_samsung_data.csv')<br/>review.head()</span></pre><figure class="jy jz ka kb fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ko"><img src="../Images/0e3f8cdec35014d030071839212326c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GcUmujGTwmVg9Z26AiWz0A.png"/></div></div></figure><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="a061" class="kh ki hi kd b fi kj kk l kl km">review.tail()</span></pre><figure class="jy jz ka kb fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es kt"><img src="../Images/4179923f9dec277cf729577ba4b7cc7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KUo9Cuc2OtG0JGQxJ-0DWw.png"/></div></div></figure><p id="30c9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将连接“body”和“title”列，并创建一个名为“Comment”的单独列，稍后我们将从数据表中删除“body”、“title”和“Unnamed: 0”。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="f9d3" class="kh ki hi kd b fi kj kk l kl km">review['Comment'] = review[['body', 'title']].apply(lambda x: ''.join(x), axis = 1)</span><span id="f5e9" class="kh ki hi kd b fi kn kk l kl km">review = review.loc[pd.isnull(review['Comment']) == False]<br/>review = review.drop(['body','title','Unnamed: 0'], axis = 1)</span></pre><p id="da76" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在我们将得到行的索引，在那里我们将把感兴趣的品牌(三星)和其他品牌(lg和索尼)分开。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="8729" class="kh ki hi kd b fi kj kk l kl km">samsung = review.index[review['brand'] == 'samsung']<br/>non_samsung = review.index[review['brand'] != 'samsung']</span></pre><p id="a1b6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">空白和新的行标记被删除并转换为小写，以便在下一步中进行进一步的预处理。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="791a" class="kh ki hi kd b fi kj kk l kl km">for i in range(review.shape[0]):<br/>    sentence = review.Comment[i]<br/>    stripped_sentence = " ".join(sentence.split())<br/>    review.Comment[i] = stripped_sentence.strip().lower()</span></pre><p id="de70" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第二步:标记句子。</strong></p><p id="7a9b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们把每个句子标记成单词表。在分词的过程中，我们去掉标点符号，分离出单词，并返回句子中所有单词的列表。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="9257" class="kh ki hi kd b fi kj kk l kl km">def sent_to_words(sentences):<br/>    for sentence in sentences:<br/>        yield(gensim.utils.simple_preprocess(str(sentence), deacc = True)) # deacc = True removes punctuations</span><span id="f315" class="kh ki hi kd b fi kn kk l kl km">data = review.Comment.values.tolist()<br/>data_words = list(sent_to_words(data))</span><span id="0236" class="kh ki hi kd b fi kn kk l kl km">print(data_words[:][0])</span></pre><figure class="jy jz ka kb fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ku"><img src="../Images/a3b4d3d9fe140a73aea7c4e08a002f08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vBsgaj2wA8ykITbx86U7lg.png"/></div></div></figure><p id="28d5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第三步:删除停用词。</strong></p><p id="918a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">然后我们删除停用词。停用词是不会给句子增加任何语义的词。这降低了语料库矩阵的维数，也降低了后续步骤中计算的噪声。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="e3c3" class="kh ki hi kd b fi kj kk l kl km"># Getting list of stopwords<br/>stop_words = list(stopwords.words("english"))<br/>stop_words.extend(('tv','lg','samsung','sony'))</span><span id="6fed" class="kh ki hi kd b fi kn kk l kl km">def remove_stopwords(texts):<br/>    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]</span></pre><p id="d224" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">步骤4:制作标记化单词的二元模型版本。</strong></p><p id="5d8a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Bigram只是两个连续的单词在一起。像二元模型一样，我们可以制作三元模型、四元模型等。在我们的数据集中，我们可能会用到“电视机”、“遥控器”等词。</p><p id="385c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将使用gensim的短语模型来创建二元模型。它需要两个参数，即最小计数和阈值。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="5102" class="kh ki hi kd b fi kj kk l kl km"># Build the bigram models<br/>bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.</span><span id="817c" class="kh ki hi kd b fi kn kk l kl km"># Faster way to get a sentence clubbed as a bigram<br/>bigram_mod = gensim.models.phrases.Phraser(bigram)</span><span id="3eac" class="kh ki hi kd b fi kn kk l kl km">def make_bigrams(texts):<br/>    return [bigram_mod[doc] for doc in texts]</span></pre><p id="e3d4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第五步:将每个句子中的单词进行词条释义，保留所需的词性。</strong></p><p id="925d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当我们处理大型语料库并且一个单词会以各种其他形式重复多次时，词汇化的过程是很重要的。例如，单词“练习”、“练习”、“练习”将被转换为练习。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="3b73" class="kh ki hi kd b fi kj kk l kl km">def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):<br/>    texts_out = []<br/>    for sent in texts:<br/>        doc = nlp(" ".join(sent)) <br/>        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])<br/>    return texts_out</span><span id="e53a" class="kh ki hi kd b fi kn kk l kl km"># Initialize spacy 'en' model, keeping only tagger component (for efficiency)<br/>nlp = spacy.load("en")</span><span id="ae2b" class="kh ki hi kd b fi kn kk l kl km"># Do lemmatization keeping only noun, adj, vb, adv<br/>data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])</span><span id="d00c" class="kh ki hi kd b fi kn kk l kl km">print(data_lemmatized[:1][0])</span></pre><figure class="jy jz ka kb fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es kv"><img src="../Images/cb84359ad699d932e37294abf61afa82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o3utIWqIMNlq-DhvwnIuyw.png"/></div></div></figure><p id="96ba" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第六步:创建每个单词的词典、语料库和词频。</strong></p><p id="56c0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在这一部分，我们将为LDA模型创建输入，即。语料库词典和语料库本身。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="d900" class="kh ki hi kd b fi kj kk l kl km"># Create Dictionary<br/>id2word = corpora.Dictionary(data_lemmatized)</span><span id="7c33" class="kh ki hi kd b fi kn kk l kl km"># Create Corpus<br/>texts = data_lemmatized</span><span id="dc9e" class="kh ki hi kd b fi kn kk l kl km"># Term Document Frequency<br/>bow_corpus = [id2word.doc2bow(text) for text in texts]</span><span id="5329" class="kh ki hi kd b fi kn kk l kl km"># View<br/>print(bow_corpus[:1][0])</span></pre><figure class="jy jz ka kb fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es kw"><img src="../Images/d2d78da9a717643c7bab8909b99bc1ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xzChXU89r7Ig3QyaBHf4qw.png"/></div></div></figure><p id="1a01" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Gensim为每个单词创建一个唯一的id，并记录该单词的词频。例如,( 0，1)表示单词id 0只出现了一次，而as (11，2)表示单词id 11在文档中出现了两次。</p><p id="c9b0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第七步:借助上一步建立的话题建模语料库，建立基础LDA模型。</strong></p><p id="0d75" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在我们已经得到了适合模型的预处理数据。我们有语料库、词典、eta、alpha等超参数。我们在之前的ie 1/n_components中为它们选择了默认值。对于这个数据集，我们选择了15个不同的主题，其中每个主题都是关键字的组合，每个关键字都对主题有一定的权重。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="4b3c" class="kh ki hi kd b fi kj kk l kl km">lda_model = gensim.models.LdaMulticore(corpus = bow_corpus,<br/>                                       id2word = id2word,<br/>                                       num_topics = 12, <br/>                                       random_state = 100,<br/>                                       chunksize = 100,<br/>                                       passes = 5,<br/>                                       per_word_topics = True,<br/>                                       workers = 4)</span></pre><p id="3a90" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">您可以看到每个主题的单词及其重要性。</p><figure class="jy jz ka kb fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es kx"><img src="../Images/a92486b8d16c645e8323d3bba3051363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mlfMoZTvtn2miKzL8uh59Q.png"/></div></div></figure><p id="a3d5" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">注</strong>:上面只显示了12个话题中的两个。</p><p id="729a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">步骤8:相干分数和超参数调整</strong></p><p id="dfdb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">通过使用下面的代码来计算一致性。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="935f" class="kh ki hi kd b fi kj kk l kl km">coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')<br/>coherence_lda = coherence_model_lda.get_coherence()<br/>print('Coherence Score: ', coherence_lda)</span></pre><p id="9f0f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">正如你所看到的，我们使用了<em class="ky">‘C _ v’</em>作为我们的度量标准。我们为基础模型获得了0.46的一致性分数。我们现在来看看超参数调整对基本模型的影响。</p><p id="a548" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将调整超参数，即。α(a)，β(b)和主题数(n)。</p><p id="bf9f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将通过使用三个连续的for循环ie来执行这个调优过程。通过保持一个变量不变，同时获得其他两个变量的每个组合。</p><p id="aa98" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">下面的代码说明了我们如何获得数据集的最佳超参数。</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="a218" class="kh ki hi kd b fi kj kk l kl km"># LDA model and its corresponding Coherence Score<br/>def compute_coherence_values(bow_corpus, id2word, n, a, b):<br/>    lda_model = gensim.models.LdaMulticore(corpus = bow_corpus,<br/>                                           id2word = id2word,<br/>                                           num_topics = n, <br/>                                           random_state = 42,<br/>                                           chunksize = 100,<br/>                                           passes = 10,<br/>                                           alpha = a,<br/>                                           eta = b)<br/>    <br/>    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')<br/>    <br/>    return coherence_model_lda.get_coherence()</span><span id="0f45" class="kh ki hi kd b fi kn kk l kl km">grid = {}<br/>grid['Validation_Set'] = {}</span><span id="1c71" class="kh ki hi kd b fi kn kk l kl km"># Topics range<br/>min_topics = 5<br/>max_topics = 13<br/>step_size = 1<br/>topics_range = range(min_topics, max_topics, step_size)</span><span id="22fb" class="kh ki hi kd b fi kn kk l kl km"># Alpha parameter<br/>alpha = [0.01, 0.31, 0.61, 0.91, 'symmetric', 'asymmetric']</span><span id="5eb4" class="kh ki hi kd b fi kn kk l kl km"># Beta parameter<br/>beta = [0.01, 0.31, 0.61, 0.91, 'symmetric']</span><span id="677c" class="kh ki hi kd b fi kn kk l kl km">model_results = {'Topics': [],<br/>                 'Alpha': [],<br/>                 'Beta': [],<br/>                 'Coherence': []<br/>                }</span><span id="05d8" class="kh ki hi kd b fi kn kk l kl km"># iterate through number of topics<br/>for k in topics_range:<br/>    # iterate through alpha values<br/>    for a in alpha:<br/>        # iterare through beta values<br/>        for b in beta:<br/>            # get the coherence score for the given parameters<br/>            cv = compute_coherence_values(bow_corpus = bow_corpus, id2word = id2word, <br/>                                          n = k, a = a, b = b)<br/>            # Save the model results<br/>            model_results['Topics'].append(k)<br/>            model_results['Alpha'].append(a)<br/>            model_results['Beta'].append(b)<br/>            model_results['Coherence'].append(cv)</span><span id="c32a" class="kh ki hi kd b fi kn kk l kl km">pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index = False)</span></pre><p id="4207" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以从下面的片段中看到，alpha、beta和数字主题的最佳值分别为0.01、0.31和11，一致性得分约为0.53。</p><figure class="jy jz ka kb fd ij er es paragraph-image"><div class="er es if"><img src="../Images/de0305ff87337c6ad634283275768b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*5AoPn6XTjdJXVxicKTjdew.png"/></div></figure><p id="ba43" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这比以前的一致性分数提高了15%。因此，我们现在将拟合LDA模型的最佳参数，并检查主题以进行进一步分析。</p><p id="fc5a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">第九步:用适当的技术分析和可视化你的结果。</strong></p><p id="e1d2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们已经计算了文档中每个主题的百分比频率，并绘制了垂直条形图，主题位于x轴上。</p><figure class="jy jz ka kb fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es kz"><img src="../Images/03e5bb9816954195c40107b9b5fa0853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kMv7DHoxEg3O0Y13D4jt4A.png"/></div></div></figure><p id="6db8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们计算了所有主题的百分比频率之间的差异，并过滤了差异大于0.025的主题</p><p id="1006" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从上面我们可以看出，主题“主题_5”、“主题_7”、“主题_8”、“主题_10”有显著差异，需要进一步检查，以获得电视品牌之间的可比潜在因素。</p><p id="a4db" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">通过使用下面的代码，我们使用gensim的pyLDAviz包可视化了该图</p><pre class="jy jz ka kb fd kc kd ke kf aw kg bi"><span id="5474" class="kh ki hi kd b fi kj kk l kl km"># Visualize the topics<br/>pyLDAvis.enable_notebook()<br/>vis = pyLDAvis.gensim.prepare(best_lda_model,bow_corpus , id2word)<br/>vis</span></pre><figure class="jy jz ka kb fd ij er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es la"><img src="../Images/3784c23f4906a57f961fb5f4b1fad4ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TefBBeazdAalEODrW1V1TQ.png"/></div></div></figure><p id="20ca" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">结论:</strong></p><p id="2bf6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在观察了显著不同的主题和与之相关的单词后。我们将总结以下几点:</p><ol class=""><li id="39a5" class="jk jl hi io b ip iq it iu ix jm jb jn jf jo jj jp jq jr js bi translated">与竞争对手相比，三星提供了更多智能功能，从而带来了出色的整体用户体验。</li><li id="f689" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">LG和索尼在真黑和高清显示方面提供了良好的画质。此外，这些品牌有相对较好的音频/音响系统。</li><li id="5105" class="jk jl hi io b ip jt it ju ix jv jb jw jf jx jj jp jq jr js bi translated">三星在软件(网飞、youtube等)更新和适当的wifi连接方面面临问题，这会影响无缓冲在线流媒体。</li></ol><p id="64e8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">作为我的第一篇文章，我希望得到建设性的反馈，我希望你喜欢阅读这篇文章。</p><p id="b1f0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">以上建模的jupyter笔记本可以在<a class="ae lb" href="https://github.com/Georgebob256/Machine-learning-with-Python/blob/master/Amazon%20TV%20reviews..ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div></div>    
</body>
</html>