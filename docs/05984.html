<html>
<head>
<title>Spark SQL and DataFrames</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">火花SQL和数据帧</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/spark-sql-and-dataframes-72e58fe90f94?source=collection_archive---------31-----------------------#2020-05-07">https://medium.com/analytics-vidhya/spark-sql-and-dataframes-72e58fe90f94?source=collection_archive---------31-----------------------#2020-05-07</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="1dea" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在第1章<a class="ae jd" rel="noopener" href="/analytics-vidhya/apache-spark-primer-ca1a6d060fc8"> <em class="je"> Apache Spark初级读本</em> </a>中，我们已经讲述了Spark架构和组件的基础知识。</p><p id="9c26" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Spark SQL、数据帧和数据集是结构化数据处理的Spark组件。数据帧和数据集在概念上与RDBMS中的表相同。数据集是“类型化”的，即在编译时检查其数据类型，因此可用于基于JVM的语言，如Java和Scala，但不能用于Python或r。数据帧是“非类型化”的，即在运行时检查其数据类型。使用Spark SQL，您可以创建、查询、删除表、数据库和视图。我们将通过下面的PySpark代码片段探索数据帧和Spark SQL。</p><pre class="jf jg jh ji fd jj jk jl jm aw jn bi"><span id="8237" class="jo jp hi jk b fi jq jr l js jt"><em class="je">### Import required packages<br/></em>from pyspark.sql import SparkSession,context<br/>from pyspark.conf import SparkConf<br/>from pyspark.sql.functions import broadcast</span><span id="9fa2" class="jo jp hi jk b fi ju jr l js jt"><em class="je">### Set configurations<br/></em>conf = SparkConf().setAll([('spark.sql.shuffle.partitions', '10'), ('spark.app.name', '"SourSparkApp"'), ('spark.executor.cores', '2'), ('spark.cores.max', '2'), ('spark.driver.memory','2g')])</span><span id="ffb3" class="jo jp hi jk b fi ju jr l js jt"><em class="je">### Create SparkSession instance<br/></em>spark = SparkSession.builder.config(conf=conf).getOrCreate()</span><span id="e204" class="jo jp hi jk b fi ju jr l js jt"><em class="je">### Read data from a Database<br/></em>df=spark.read.format('jdbc').options(url="jdbc:oracle:thin:USER_NAME/PASSWORD@database_connection:1521/db_name",dbtable="my_db_table",driver="oracle.jdbc.driver.OracleDriver").option("numPartitions", 10).load()</span><span id="56cb" class="jo jp hi jk b fi ju jr l js jt">### Read data from a text file on local FS on edge node<br/>df = spark.read.text("file:///home/path/to/my_data.txt")</span><span id="7d1d" class="jo jp hi jk b fi ju jr l js jt">### Or Read data from a Json<br/>df = spark.read.load("/hdfs/path/to/my_data.json", format="json")</span><span id="0994" class="jo jp hi jk b fi ju jr l js jt">### Or Read data from a CSV/Delimited file<br/>df = spark.read.load("/hdfs/path/to/my_data.csv", format="csv", sep=";", inferSchema="true", header="true")</span><span id="934a" class="jo jp hi jk b fi ju jr l js jt">### Or Read ORC file<br/>df = spark.read.orc("/hdfs/path/to/my_data.orc")</span><span id="457d" class="jo jp hi jk b fi ju jr l js jt">### Or Read from Hive <br/>df = spark.sql("select * from databasename.tableName")</span><span id="d3fd" class="jo jp hi jk b fi ju jr l js jt">### Or Read Parquet file<br/>df = spark.read.load("/hdfs/path/to/my_data.parquet")</span><span id="1d9f" class="jo jp hi jk b fi ju jr l js jt">### Take a look at Schema(columns and types) of the dataframe<br/>df.printSchema()</span><span id="03d7" class="jo jp hi jk b fi ju jr l js jt"><em class="je">### Apply filter i.e. Narrow transformation to create new Dataframe<br/></em>df_filter = df.filter(df.APPLICATION=='My_app')</span><span id="2fd9" class="jo jp hi jk b fi ju jr l js jt">### Convert Spark DF to Pandas DF (This should be performed on small dataset as it will collect the data on the driver)<br/>pandas_df = df_filter.toPandas()</span><span id="4a4d" class="jo jp hi jk b fi ju jr l js jt"><em class="je">### Apply groupBy i.e. Wide transformation to create a new Dataframe<br/></em>df_group=df_filter.groupBy("CURRENCY","TYPE").agg({"AMOUNT":"sum"}).withColumnRenamed("sum(AMOUNT)","sum_AMOUNT")</span><span id="f0d8" class="jo jp hi jk b fi ju jr l js jt">### To improve the performance if df_group is used multiple times later, cache it in memory<br/>df_group.cache()</span><span id="4d31" class="jo jp hi jk b fi ju jr l js jt"><em class="je">### Create a temporary view to execute SQL statements on it<br/></em>df.createOrReplaceTempView("TEMP_DATA_TABLE")<br/><em class="je">### Create a Global view to share data across different sessions<br/></em>df.createGlobalTempView("GLOBAL_TEMP_DATA_TABLE")</span><span id="d3b3" class="jo jp hi jk b fi ju jr l js jt"><em class="je">### Create a Dataframe using a SparkSQL query<br/></em>spark_table = spark.sql("select CURRENCY,TYPE,sum(AMOUNT)  FROM global_temp.GLOBAL_TEMP_DATA_TABLE where APPLICATION='My_app' group by CURRENCY,TYPE")</span><span id="2345" class="jo jp hi jk b fi ju jr l js jt">### Hint to B<!-- -->roadcast the table when joining with another table<br/>broadcast(spark.table("<!-- -->spark_table<!-- -->")).join(spark.table("new_table"), "key")</span><span id="d84a" class="jo jp hi jk b fi ju jr l js jt"><em class="je">### Perform Action i.e. Showing the SQL query results<br/></em>print(spark_table.show())</span><span id="59ef" class="jo jp hi jk b fi ju jr l js jt"><em class="je">### Perform Action i.e. Write the results to HDFS as a csv<br/></em>df_group.write.partitionBy("CURRENCY").save('/dev/datalake/sour_spark_v2/data_agg_v2', format='csv', mode='append')<br/><em class="je">### Store it as Parquet file<br/></em>df_group.write.parquet("df_group.parquet")<br/><em class="je">### Store it as Hive Table<br/></em>sqlContext.sql<!-- -->("create table mytable as select * from <!-- -->TEMP_DATA_TABLE<!-- -->");<br/>### Store the results back to RDBMS<br/>df_group.write.jdbc(url="jdbc:oracle:thin:USER_NAME/PASSWORD@database_connection:1521/db_name",dbtable="my_db_table",driver="oracle.jdbc.driver.OracleDriver", tablename, mode="append", properties=props)</span><span id="6c8e" class="jo jp hi jk b fi ju jr l js jt"><em class="je">### Stop the Spark driver<br/></em>spark.stop()</span></pre><p id="2d91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将通过spark-submit执行这段代码。Spark将把它转换成逻辑计划，然后转换成优化的物理DAG计划，最后转换成执行的RDD字节码。</p><p id="4638" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据帧操作与Spark SQL的性能是相同的，因为两者都执行相同的底层RDD转换计划。</p><p id="f2a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们从数据库中读取数据并执行一些转换。如果数据帧上有过滤器，Spark会将过滤器推送到数据库，而不是从数据库中取出所有数据，然后通过<strong class="ih hj"> <em class="je">查询下推</em> </strong>应用过滤器。</p><p id="9d6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">除了查询下推之外，您还可以让<strong class="ih hj"> <em class="je">谓词下推</em> </strong>，其中您可以让分离谓词的过滤数据到达它们各自的分区。</p><p id="c536" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，在本章中，我们介绍了结构化Spark APIs，即数据帧、数据集和Spark SQL。</p><p id="8f21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下一章中，我们探讨<a class="ae jd" rel="noopener" href="/@sourabhpotnis/spark-tuning-and-debugging-fe32fded8454"> <em class="je">火花调谐和</em> </a>调试。</p></div></div>    
</body>
</html>