<html>
<head>
<title>Deep Learning for Air Quality Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于空气质量预测的深度学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/deep-learning-for-air-quality-prediction-b451e2936290?source=collection_archive---------5-----------------------#2020-10-15">https://medium.com/analytics-vidhya/deep-learning-for-air-quality-prediction-b451e2936290?source=collection_archive---------5-----------------------#2020-10-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/ae0d1e331e20beec19c2f00da8da8df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*CoNnxJlmETLqk-diAAujrA.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">深度学习</figcaption></figure><p id="f6ae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi jo translated">众所周知，空气污染是城市的主要问题之一，其中颗粒物是空气污染中对人类影响最大的部分。使用具有长短期记忆(LSTM)神经网络的深度学习的时间序列预测来预测空气质量。首先，让我们谈谈什么叫做LSTM和它的理论。</p><h2 id="ae34" class="jx jy hi bd jz ka kb kc kd ke kf kg kh jb ki kj kk jf kl km kn jj ko kp kq kr bi translated">递归神经网络</h2><p id="ed85" class="pw-post-body-paragraph iq ir hi is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi jo translated"><span class="l jp jq jr bm js jt ju jv jw di"> T </span>传统的神经网络不能做到这一点，这似乎是一个主要的缺点。例如，假设您想要对电影中每一点正在发生的事件进行分类。目前还不清楚传统的神经网络如何利用其对电影中先前事件的推理来通知后来的事件。递归神经网络解决了这个问题。它们是带有环路的网络，允许信息持续存在。</p><h2 id="db15" class="jx jy hi bd jz ka kb kc kd ke kf kg kh jb ki kj kk jf kl km kn jj ko kp kq kr bi translated">LSTM网络公司</h2><p id="8941" class="pw-post-body-paragraph iq ir hi is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">长短期记忆网络——通常简称为“lstm”——是一种特殊的RNN，能够学习长期依赖性。LSTMs的明确设计是为了避免长期依赖问题。所有递归神经网络都具有神经网络的重复模块链的形式。在标准RNNs中，这种重复模块将具有非常简单的结构，例如单个tanh层。</p><figure class="ky kz la lb fd ij er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es kx"><img src="../Images/cace770db820b756504598221c2c97d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5W8FrASMi93Z81NlAui4w.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated"><strong class="bd jz">LSTM中的重复模块包含四个相互作用的层。</strong></figcaption></figure><p id="6bae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在上图中，黄色方框表示神经网络层，粉色圆圈表示逐点操作，箭头表示传输、复制。每条线都承载一个完整的向量，从一个节点的输出到其他节点的输入。粉色圆圈代表逐点操作，如向量加法，而黄色方框是学习过的神经网络层。行合并表示连接，而行分叉表示其内容被复制，并且副本被移动到不同的位置。LSTMs的关键是单元状态，即贯穿图表顶部的水平线。细胞状态有点像传送带。它沿着整个链条直线向下，只有一些微小的线性相互作用。信息很容易不加改变地流动。</p><p id="7b9f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">LSTM确实有能力删除或添加细胞状态的信息，由称为门的结构仔细调节。门是选择性地让信息通过的一种方式。它们由一个sigmoid神经网络层和一个逐点乘法运算组成。sigmoid层输出0到1之间的数字，描述每种成分应该通过多少。值为0表示“不允许任何东西通过”，而值为1表示“允许任何东西通过！”</p><p id="b05e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">一个LSTM有三个这样的门，用来保护和控制细胞状态。</p></div><div class="ab cl lg lh gp li" role="separator"><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll lm"/><span class="lj bw bk lk ll"/></div><div class="hb hc hd he hf"><p id="da66" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在从上面的部分，我们可以通过一些LSTM神经网络的想法。接下来我们来看编码部分。这里我们选择python作为编程语言，使用以下库。</p><pre class="ky kz la lb fd ln lo lp lq aw lr bi"><span id="d435" class="jx jy hi lo b fi ls lt l lu lv">import datetime, warnings, scipy <br/>import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>import matplotlib as mpl<br/>import matplotlib.pyplot as plt<br/>from IPython.core.interactiveshell import InteractiveShell<br/>import math<br/>from keras.models import Sequential<br/>from keras.layers import Dense<br/>from keras.layers import LSTM<br/>from sklearn.preprocessing import MinMaxScaler, StandardScaler<br/>from sklearn.metrics import mean_squared_error</span></pre><p id="69cb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在导入库之后，然后移动到数据导入算法的一部分。</p><pre class="ky kz la lb fd ln lo lp lq aw lr bi"><span id="99db" class="jx jy hi lo b fi ls lt l lu lv">df = pd.read_csv('Data.csv', sep=' ')</span></pre><p id="8953" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，在导入数据之后，接下来是算法的主要过程。每个属性的分位数的计算部分。以下代码片段说明了计算是如何完成的。</p><pre class="ky kz la lb fd ln lo lp lq aw lr bi"><span id="9f7f" class="jx jy hi lo b fi ls lt l lu lv">def calculate_quantile (i, df2):<br/>    Q1 = df2[[i]].quantile(0.25)[0]<br/>    Q3 = df2[[i]].quantile(0.75)[0]<br/>    IQR = Q3 - Q1<br/>    min = df2[[i]].min()[0]<br/>    max = df2[[i]].max()[0]<br/>    min_IQR = Q1 - 1.5*IQR<br/>    max_IQR = Q3 + 1.5*IQR<br/>    <br/>    return Q1, Q3, min, max, min_IQR, max_IQR</span><span id="7edf" class="jx jy hi lo b fi lw lt l lu lv"># delete first and last rows to avoid missing value extrapolation<br/>df2.drop(index=[df2.index[0], df2.index[df2.shape[0]-1]], inplace=True)</span><span id="20cf" class="jx jy hi lo b fi lw lt l lu lv"># find and interpolate the outliers<br/>for i in df2.columns:<br/>    print('\nAttribute-',i,':')<br/>    Q1, Q3, min, max, min_IQR, max_IQR = calculate_quantile(i, df2)<br/>    print('Q1 = %.2f' % Q1)<br/>    print('Q3 = %.2f' % Q3)<br/>    print('min IQR = %.2f' % min_IQR)<br/>    print('max IQR = %.2f' % max_IQR)<br/>    if (min &lt; min_IQR):<br/>        print('Low outlier = %.2f' % min)<br/>    if (max &gt; max_IQR):<br/>        print('High outlier= %.2f' % max)<br/>    <br/>    def convert_nan (x, max_IQR=max_IQR, min_IQR=min_IQR):<br/>        if ((x &gt; max_IQR) | (x &lt; min_IQR)):<br/>            x = np.nan<br/>        else:<br/>            x = x<br/>        return x</span><span id="0ef4" class="jx jy hi lo b fi lw lt l lu lv">def convert_nan_HUM (x, max_IQR=100.0, min_IQR=min_IQR):<br/>        if ((x &gt; max_IQR) | (x &lt; min_IQR)):<br/>            x = np.nan<br/>        else:<br/>            x = x<br/>        return x<br/>    <br/>    if (i == 'HUM'):<br/>        df2[i] = df2[i].map(convert_nan_HUM)<br/>        df2[i] = df2[i].interpolate(method='linear')<br/>    if (i != 'HUM'):<br/>        df2[i] = df2[i].map(convert_nan)<br/>        df2[i] = df2[i].interpolate(method='linear')<br/>    <br/>    if (len(df2[df2[i].isnull()][i]) == 0):</span></pre><p id="e5ff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在计算之后，移动到下一部分，这里最重要的部分是预测部分。所以对于这一步，数据会拆分成一些阶段。这里我们将数据分成75%用于训练，25%用于测试。</p><pre class="ky kz la lb fd ln lo lp lq aw lr bi"><span id="e7e0" class="jx jy hi lo b fi ls lt l lu lv">train_size = int(len(dataset) * 0.75)<br/>test_size = len(dataset) - train_size<br/>train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]<br/>print(len(train), len(test))</span></pre><p id="06c6" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在此之后，LSTM神经网络将创建。下面是LSTM神经网络是如何制作的。</p><pre class="ky kz la lb fd ln lo lp lq aw lr bi"><span id="0249" class="jx jy hi lo b fi ls lt l lu lv">model = Sequential()<br/>model.add(LSTM(4, input_shape=(1, look_back)))<br/>model.add(Dense(1))<br/>model.compile(loss='mean_squared_error', optimizer='adam')<br/>model.fit(trainX, trainY, epochs=1500, batch_size=32, verbose=2)</span></pre><p id="56b8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">创建神经网络后，所有的预测工作就完成了。现在结果将告诉我们预测有多好。这是我做的一些分析温度的例子。</p><pre class="ky kz la lb fd ln lo lp lq aw lr bi"><span id="b0d5" class="jx jy hi lo b fi ls lt l lu lv"># shift train predictions for plotting<br/>trainPredictPlot = np.empty_like(dataset)<br/>trainPredictPlot[:, :] = np.nan<br/>trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict</span><span id="8525" class="jx jy hi lo b fi lw lt l lu lv"># shift test predictions for plotting<br/>testPredictPlot = np.empty_like(dataset)<br/>testPredictPlot[:, :] = np.nan<br/>testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict</span><span id="b535" class="jx jy hi lo b fi lw lt l lu lv"># plot original dataset and predictions<br/>time_axis = np.linspace(0, dataset.shape[0]-1, 15)<br/>time_axis = np.array([int(i) for i in time_axis])<br/>time_axisLab = np.array(df2.index, dtype='datetime64[D]')</span><span id="8819" class="jx jy hi lo b fi lw lt l lu lv">fig = plt.figure()<br/>ax = fig.add_axes([0, 0, 2.1, 2])<br/>ax.plot(np.expm1(dataset), label='Original Dataset')<br/>ax.plot(trainPredictPlot, color='orange', label='Train Prediction')<br/>ax.plot(testPredictPlot, color='red', label='Test Prediction')<br/>ax.set_xticks(time_axis)<br/>ax.set_xticklabels(time_axisLab[time_axis], rotation=45)<br/>ax.set_xlabel('\nDate', fontsize=27, fontweight='bold')<br/>ax.set_ylabel('TEMP', fontsize=27, fontweight='bold')<br/>ax.legend(loc='best', prop= {'size':20})<br/>ax.tick_params(size=10, labelsize=15)<br/>ax.set_xlim([-1,1735])</span><span id="1dcc" class="jx jy hi lo b fi lw lt l lu lv">ax1 = fig.add_axes([2.3, 1.3, 1, 0.7])<br/>ax1.plot(np.expm1(dataset), label='Original Dataset')<br/>ax1.plot(testPredictPlot, color='red', label='Test Prediction')<br/>ax1.set_xticks(time_axis)<br/>ax1.set_xticklabels(time_axisLab[time_axis], rotation=45)<br/>ax1.set_xlabel('Date', fontsize=27, fontweight='bold')<br/>ax1.set_ylabel('TEMP', fontsize=27, fontweight='bold')<br/>ax1.tick_params(size=10, labelsize=15)<br/>ax1.set_xlim([1360,1735]);</span></pre><p id="ba48" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是它的输出</p><figure class="ky kz la lb fd ij er es paragraph-image"><div role="button" tabindex="0" class="lc ld di le bf lf"><div class="er es lx"><img src="../Images/9a3c787cf5198a8f08bb7e0e9acbba1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hlUiSFu0IBObdoLvwpLmMg.png"/></div></div><figcaption class="im in et er es io ip bd b be z dx translated">温度预测的输出</figcaption></figure><h2 id="79e7" class="jx jy hi bd jz ka kb kc kd ke kf kg kh jb ki kj kk jf kl km kn jj ko kp kq kr bi translated">结论</h2><p id="0a95" class="pw-post-body-paragraph iq ir hi is b it ks iv iw ix kt iz ja jb ku jd je jf kv jh ji jj kw jl jm jn hb bi translated">从这些代码片段中，我们可以训练数据，并使用LSTM从神经网络中获得大约95%的准确模型。在我的上一个故事中，我试图用线性回归来预测天气。由此，我得到了93%的准确率。因此，我仍在努力获得尽可能多的准确性，以实现成功的空气质量预测神经网络</p></div></div>    
</body>
</html>