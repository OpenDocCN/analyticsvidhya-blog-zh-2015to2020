<html>
<head>
<title>MSE vs MLE for linear regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归的MSE与MLE</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mse-vs-mle-for-linear-regression-f4ce3f6b990e?source=collection_archive---------2-----------------------#2019-11-29">https://medium.com/analytics-vidhya/mse-vs-mle-for-linear-regression-f4ce3f6b990e?source=collection_archive---------2-----------------------#2019-11-29</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/d9d033e34547c719de472c2cdec8a0bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*EpSYbIzQa5EUlpPc"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">特里·维里斯迪斯在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure><p id="2183" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">什么是MSE？</p><p id="b244" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Estimator" rel="noopener ugc nofollow" target="_blank">估计器</a>的<strong class="ix hj">均方误差</strong> ( <strong class="ix hj"> MSE </strong>)或<strong class="ix hj">均方偏差</strong> ( <strong class="ix hj"> MSD </strong>)(用于估计未观察量的过程)测量<a class="ae iu" href="https://en.wikipedia.org/wiki/Error_(statistics)" rel="noopener ugc nofollow" target="_blank">误差</a>的平方的<a class="ae iu" href="https://en.wikipedia.org/wiki/Expected_value" rel="noopener ugc nofollow" target="_blank">平均值</a>——即估计值与实际值之间的平均平方差</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/b94f887822d03fa07b791c66b954324c.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*fiXXC_YNu5vRDU0NAbViQQ.png"/></div></figure><p id="42e5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">MLE是什么？</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jy"><img src="../Images/de51b156bed16e3685c279385d32eabd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5-9hGyYmD1q0an2CuNELsg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:<a class="ae iu" href="https://www.youtube.com/watch?v=XepXtl9YKwc&amp;t=197s" rel="noopener ugc nofollow" target="_blank">Josh Starmer的stat quest</a></figcaption></figure><p id="3315" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">最大似然估计</strong> ( <strong class="ix hj"> MLE </strong>)是一种通过<a class="ae iu" href="https://en.wikipedia.org/wiki/Mathematical_optimization" rel="noopener ugc nofollow" target="_blank">最大化</a>一个<a class="ae iu" href="https://en.wikipedia.org/wiki/Likelihood_function" rel="noopener ugc nofollow" target="_blank">似然函数</a>来<a class="ae iu" href="https://en.wikipedia.org/wiki/Estimation_theory" rel="noopener ugc nofollow" target="_blank">估计</a>一个<a class="ae iu" href="https://en.wikipedia.org/wiki/Probability_distribution" rel="noopener ugc nofollow" target="_blank">概率分布</a>的<a class="ae iu" href="https://en.wikipedia.org/wiki/Statistical_parameter" rel="noopener ugc nofollow" target="_blank">参数</a>的方法，使得在假定的<a class="ae iu" href="https://en.wikipedia.org/wiki/Statistical_model" rel="noopener ugc nofollow" target="_blank">统计模型</a>下<a class="ae iu" href="https://en.wikipedia.org/wiki/Realization_(probability)" rel="noopener ugc nofollow" target="_blank">观测数据</a>最有可能。</p><p id="2df9" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">基本上我们</p><ol class=""><li id="f431" class="jz ka hi ix b iy iz jc jd jg kb jk kc jo kd js ke kf kg kh bi translated">首先假设数据来自某个分布</li><li id="f92b" class="jz ka hi ix b iy ki jc kj jg kk jk kl jo km js ke kf kg kh bi translated">然后，我们随机选择一些分布参数</li><li id="5e8e" class="jz ka hi ix b iy ki jc kj jg kk jk kl jo km js ke kf kg kh bi translated">然后，我们计算假设分布下观测数据的似然性</li><li id="c55f" class="jz ka hi ix b iy ki jc kj jg kk jk kl jo km js ke kf kg kh bi translated">然后，我们使用优化算法，如梯度下降，通过最大化似然性来找到我们假设分布的最佳参数</li></ol><p id="8582" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">延伸阅读</a></p><h1 id="4a29" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">线性回归的MSE与MLE</h1><p id="dd9c" class="pw-post-body-paragraph iv iw hi ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">本文的目标是从经验上看MSE得出的估计值是否与MLE方法得出的估计值相似(相同)</p><h1 id="1eea" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">生成数据</h1><pre class="ju jv jw jx fd lq lr ls lt aw lu bi"><span id="f59c" class="lv ko hi lr b fi lw lx l ly lz">## Random X from normal distribution<br/> x &lt;- rnorm(100 , mean = 20)</span><span id="23e3" class="lv ko hi lr b fi ma lx l ly lz">## Let this be the "True" Phenomenon</span><span id="54d3" class="lv ko hi lr b fi ma lx l ly lz"> b0 &lt;- 10<br/> b1 &lt;- 20</span><span id="5f98" class="lv ko hi lr b fi ma lx l ly lz"> y &lt;- b1*x + b0 + rnorm(100)</span><span id="e529" class="lv ko hi lr b fi ma lx l ly lz">## Convert to dataframe<br/> df &lt;- data.frame(x = x , y = y)<br/> head(df)</span><span id="e417" class="lv ko hi lr b fi ma lx l ly lz">## x y<br/> ## 1 21.02084 430.5484<br/> ## 2 19.60804 401.8297<br/> ## 3 21.19639 434.3705<br/> ## 4 19.05572 392.9510<br/> ## 5 18.55657 381.0414<br/> ## 6 21.69841 445.1470</span></pre><h1 id="c85d" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">MSE估计</h1><pre class="ju jv jw jx fd lq lr ls lt aw lu bi"><span id="ede6" class="lv ko hi lr b fi lw lx l ly lz">model &lt;- lm(data = df , formula = y ~ x)<br/> summary(model)</span></pre><p id="5621" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">模型摘要</p><pre class="ju jv jw jx fd lq lr ls lt aw lu bi"><span id="8db4" class="lv ko hi lr b fi lw lx l ly lz">## <br/> ## Call:<br/> ## lm(formula = y ~ x, data = df)<br/> ## <br/> ## Residuals:<br/> ## Min 1Q Median 3Q Max <br/> ## -2.4102 -0.7222 -0.1791 0.7138 2.7849 <br/> ## <br/> ## Coefficients:<br/> ## Estimate Std. Error t value Pr(&gt;|t|) <br/> ## (Intercept) 10.800 2.249 4.803 5.63e-06 ***<br/> ## x 19.964 0.112 178.206 &lt; 2e-16 ***<br/> ## — -<br/> ## Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ‘ 1<br/> ## <br/> ## Residual standard error: 1.095 on 98 degrees of freedom<br/> ## Multiple R-squared: 0.9969, Adjusted R-squared: 0.9969 <br/> ## F-statistic: 3.176e+04 on 1 and 98 DF, p-value: &lt; 2.2e-16</span></pre><ul class=""><li id="93dd" class="jz ka hi ix b iy iz jc jd jg kb jk kc jo kd js mb kf kg kh bi translated">我们看到我们的模型在使用MSE估计真实参数方面做得很好——截距估计为10.8，b1估计为19.964</li></ul><pre class="ju jv jw jx fd lq lr ls lt aw lu bi"><span id="a128" class="lv ko hi lr b fi lw lx l ly lz">## (Intercept) 10.800 <br/> ## x 19.964 </span></pre><h1 id="2a85" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">最大似然估计</h1><pre class="ju jv jw jx fd lq lr ls lt aw lu bi"><span id="f673" class="lv ko hi lr b fi lw lx l ly lz">loglikelihood &lt;- function(b0 , b1){<br/> -sum(dnorm(df$y — df$x*b1 — b0 , log=TRUE))<br/> #-sum(log(R))<br/> }<br/> </span><span id="851c" class="lv ko hi lr b fi ma lx l ly lz">library(stats4)<br/>mle(loglikelihood, start = list(b0 = 1 , b1 = 1))</span><span id="0004" class="lv ko hi lr b fi ma lx l ly lz">## <br/> ## Call:<br/> ## mle(minuslogl = loglikelihood, start = list(b0 = 1, b1 = 1))<br/> ## <br/> ## Coefficients:<br/> ## b0 b1 <br/> ## 10.80034 19.96405</span></pre><h1 id="f757" class="kn ko hi bd kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj lk bi translated">这是魔法还是什么？？！！！</h1><p id="e28a" class="pw-post-body-paragraph iv iw hi ix b iy ll ja jb jc lm je jf jg ln ji jj jk lo jm jn jo lp jq jr js hb bi translated">我们看到MLE估计等于MSE估计！</p><p id="e743" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为什么？</p><p id="e772" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这是<a class="ae iu" href="https://www.jessicayung.com/mse-as-maximum-likelihood/" rel="noopener ugc nofollow" target="_blank">的数学证明</a></p></div></div>    
</body>
</html>