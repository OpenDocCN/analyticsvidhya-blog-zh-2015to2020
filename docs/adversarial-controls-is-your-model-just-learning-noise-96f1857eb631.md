# 对抗控制:你的模型只是学习噪音吗？

> 原文：<https://medium.com/analytics-vidhya/adversarial-controls-is-your-model-just-learning-noise-96f1857eb631?source=collection_archive---------10----------------------->

![](img/1e51cd6000ad76d1c29c0796d0e0576b.png)

图片由 [StockSnap](https://pixabay.com/users/StockSnap-894430/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2616931) 来自 [Pixabay](https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2616931)

模型验证是机器学习管道的关键组成部分。在收集数据之后，如果模型不能做出准确的预测(或者至少对您的领域足够准确！)那它就没有任何价值了。

## 经典模型验证

通过模型在测试集数据上的性能来评估模型，测试集数据是模型在训练和超参数调整期间看不到的保留数据。

请注意，应该使用“验证”集或交叉验证文件夹来进行调整。我提出这一点只是为了避免混淆测试集上的*最终*模型验证和使用验证集完成的中间评估。

但是这样的最终评价就足够了吗？我们能构建额外的验证测试吗？

## 从科学理论到法律

在科学中，有许多说法很难或不可能证明。例如，热力学第二定律的一种表述是“热量不可能自动地从一种冷的介质转移到一种更热的介质。”[1]

![](img/1d988428997bb83cf76ff6a71e11c68e.png)

图片来自 [Pixabay](https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2702959) 的 [korpiri](https://pixabay.com/users/korpiri-4312065/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2702959)

这种说法——热量不能自己从冷转移到热——是一种理论。事实上，这是一个无法证明的理论。

然而，我们*可以*进行大量的实验，迄今为止，还没有进行过违反这一理论的实验。所以，在科学上，我们把这个作为充分的证明，理论就变成了定律。

这个想法乍一看似乎很傻。你可能在想，“当然，热不会从冷变热。”但是这种直觉不是你出生时就有的；你在一生中无数次观察热传递后发展了它。

你已经进行了无数的实验(阅读为生活经验)，你还不能证明热力学第二定律是错误的，所以你认为它是有效的。

## 相反的假设

我们注意到我们从未观察到违反热力学第二定律的情况，但是如果我们积极地进行实验来证明它是错误的呢？

如果所有这些实验都失败了，我们会倾向于相信这个理论。这是敌对控制背后的主要思想。

我们有我们的假设，不一定能证明。我们产生并测试相反的假设，如果这些假设失败了，那么我们会对原来的假设感觉更好。

物理学家 J. R .普拉特写了一篇著名的论文，提倡使用替代和对立的假设，他将其定义为*强推理*。[2]

有趣的是，普拉特认为，一些科学领域比其他领域发展得更快，不是因为它们得到了更慷慨的资助或有更容易解决的问题，而是因为它们的社区成员在应用科学方法时更严格。

![](img/da13e5bf4b3f0e3aad12978c13c98b4c.png)

图片来自 [Pixabay](https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2123967) 的 [TeroVesalainen](https://pixabay.com/users/TeroVesalainen-809550/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2123967)

## 提问

机器学习模型经常被称为假设函数，这并不奇怪。我们可以通过模型在测试集上的得分来评估模型的性能，但是要真正对我们的模型有信心，我们需要了解模型正在学习什么模式。

这个概念基本上可以归结为提出问题:

*   模型在哪些测试案例中失败了？
*   这些案例对人类来说“容易”理解吗？
*   x 类的图像都是失焦的吗？这个模型只是学会了识别模糊的图像吗？

一旦我们开始问这些问题，我们就可以创建替代的假设(模型)。理想情况下，这些是我们原始模型的“对手”。

庄和凯瑟写了一篇梦幻般的论文(只有两页！)概述了其中的一些问题，他们的话帮助我转换到一种敌对的心态。[3]

## 危及你的模型

好的，好的…问问题，询问模型…但是我今天可以开始应用的对抗性控制怎么样？

A.拉什顿说，

> "一个不会受到致命威胁的理论是不可能存在的."[2]

如果你真的想危及你的模型，你应该使用 y 置乱。

![](img/a42cfdddd0852b8b38024f39bed3c4ab.png)

图片来自 [Pixabay](https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=670720)

## 稻草模型

稻草模型就像稻草人一样，是无脑的。您可能已经熟悉了一些吸管模型，例如总是返回具有定量响应的数据的平均值的模型，或者总是返回分类问题的最常见类的模型。

这些模型是很好的基线；如果你的模型打不过这些，那肯定是出问题了！

在机器学习中，特别是当应用于科学时，我们正在寻找输入数据和观察到的响应之间的物理联系。也就是说，在特征(或它们的相互作用)和响应变量**和** 之间存在相关性。这是有意义的，因为它捕捉了一个物理机制。

但是，事情是，给定足够的特性(甚至随机特性！)有可能某些特征将与响应变量相关联。在这种情况下，模型学习了一个链接，但是这个链接没有意义。

**Y-scrambling** 是一种稻草模型，在这种模型中，您可以有意断开输入特征和响应变量之间的任何潜在联系。这个过程很容易实现:您只需简单地打乱训练数据的响应变量，然后重复训练过程。

需要注意的是，这里的“训练程序”包括*所有*用于训练模型的步骤，包括特征选择和使用验证集的超参数调整(也是混洗的)。

顾名思义，这种吸管模型不可能知道输入特征和响应变量之间的任何有意义的联系。现在，将您的原始(未加扰)模型在训练和测试集上的性能与加扰模型进行比较。如果原始模型的表现没有明显改善，那么没有理由相信这个模型学到了任何有价值的东西——即使它有“高表现”

## 测试集呢

这不正是我们根据测试集评估模型的原因吗？一个模型如何学习无用的特征，并且仍然推广到看不见的例子？因此，经过噪声训练的模型在测试集上的表现会很糟糕。那么，为什么要使用 y 置乱呢？

![](img/b1d2ddc7cffdf57bfcce0ec393eebb0a.png)

训练 r : 0.55，rmse: 1.92
测试 r : 0.43，rmse: 1.77

不幸的是，经过噪声训练的模型可能在测试集上表现良好。简单地说，在大多数情况下，测试集是从全部数据中随机抽取的。由于这种随机性，测试数据有可能遵循由训练数据学习到的相同的无用模式。或者一组特定的测试点和预测值之间的误差很小(同样，完全靠运气)。

当然，过于乐观的测试集分数不太可能，但是唯一确定的方法是测试多个测试集，这是不可取的。

但是，您可以通过反复调整响应变量来训练和测试许多 y-scrambled straw 模型。

## 如果可能的话，多练习几次

一些打乱的排列将接近响应变量的原始顺序。这意味着，如果你提出的模型是好的(如，它确实学习了有意义的特征)，那么一些 y-scrambled straw 模型将具有类似的性能。但这并不能否定你的模型，因为大多数 y-scrambled straw 模型的性能要差得多。

事实上，如果你愿意的话，你可以构建一个 y-scrambled 性能的分布，并进行一个经典的假设检验，以确定所提出的模型是否在统计上比稻草模型更重要。

## 一个例子:Y-scrambling [not]在起作用

最近，我发现了[这篇技术评论](https://science.sciencemag.org/content/362/6416/eaat8603.abstract?casa_token=c1VQrjU1FNUAAAAA:96SCNaIQaq3YjSwk_QcPEHWZ6UZzpLD93KCvutEaGSt1RvYs0nepOMNqrIcEp08aPat0aQLoqeqRQg)，其中 y-scrambling 被用来使一个已发布的模型无效。[4]

在没有深入细节的情况下，原始论文创建了各种模型(线性回归、k-最近邻、支持向量机、神经网络和随机森林)，并实现了 0.64 到 0.93 之间的 r 分数。测试集 rmse 也与根据训练数据计算的 RMSE 相当。

然而，技术评论的作者复制了原始论文中的模型，并发现 y 加扰的稻草模型具有几乎相同的 r 和 rmse 结果。此外，在技术评论中，评估了两个额外的测试集，并观察到拟议模型的性能明显下降；证明最初报告的测试集低估了泛化误差。

## 最后的想法

这个快速的解释加上我引用的研究论文应该可以展示使用 y 置乱的好处。

请注意，y-scrambling **并不是要取代**其他验证方法，如交叉验证和测试集评估，而是您模型验证工具箱中的一个支持工具。

事实上，你可能会发现这是“你最强大的验证程序。”[5]

最后，如果你想了解更多关于基于置乱的稻草模型，你可以追踪这篇[论文](https://pubs.acs.org/doi/abs/10.1021/ci700157b)。[6]

附注——我与任何被引用的作者都没有关系——只是乐于分享他们的精彩作品。

## 谢谢！

**参考文献**
【1】森格尔，尤努斯 a，和迈克尔 a .伯乐斯。"热力学:工程方法."*海* 1000 (2002): 8862。[ch。6、pg。290]
【2】约翰·r·普拉特《强有力的推论》科学 146.3642(1964):347–353。
[3]庄、康威诉和迈克尔·j·凯瑟。《科学机器学习的对抗性控制》(2018): 2819–2821.
[4]庄，康韦诉，和迈克尔 j .凯瑟。“对“使用机器学习预测 C–N 交叉偶联中的反应性能”的评论。”*理科* 362.6416 (2018): eaat8603。
【5】库宾伊，雨果。《药物设计中的 QSAR》*化学信息学手册:从数据到知识共 4 卷*(2003):1532–1554。
[6]吕克尔、克里斯托夫、格塔·吕克尔和马库斯·梅林格。" QSPR/QSAR 的 y 随机化及其变体."化学信息与建模杂志 47.6(2007):2345–2357。