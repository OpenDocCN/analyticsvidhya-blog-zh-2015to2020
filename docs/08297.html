<html>
<head>
<title>DCGAN — Implementing Deep Convolutional Generative Adversarial Network in TensorFlow — Idiot Developer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DCGAN——在TensorFlow中实现深度卷积生成对抗网络——白痴开发者</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/dcgan-implementing-deep-convolutional-generative-adversarial-network-in-tensorflow-idiot-41b2ddf9eddb?source=collection_archive---------8-----------------------#2020-07-24">https://medium.com/analytics-vidhya/dcgan-implementing-deep-convolutional-generative-adversarial-network-in-tensorflow-idiot-41b2ddf9eddb?source=collection_archive---------8-----------------------#2020-07-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/9ba40626617c4f1c9c761dcf75defcce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O9Meo0ar1U_DPLoUud8MiA.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">来源:照片由<a class="ae iu" href="https://unsplash.com/@mikepetrucci?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">迈克·彼得鲁奇</a>在<a class="ae iu" href="https://unsplash.com/s/photos/painter?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="5e2f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在本教程中，我们将在动漫人脸数据集上实现一个深度卷积生成对抗网络(DCGAN)。代码用TensorFlow 2.2和Python 3.8编写。</p><p id="3b29" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">据脸书人工智能的主任Yann LeCun说，GAN是“过去10年机器学习中最有趣的想法”</p><p id="e77b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">概述:</p><ol class=""><li id="8942" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js jy jz ka kb bi translated">什么是甘</li><li id="be01" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">DCGAN与香草GAN有何不同</li><li id="ba9c" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">动漫人脸数据集</li><li id="5b0a" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">履行</li><li id="a17f" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js jy jz ka kb bi translated">摘要</li></ol><h1 id="bc6a" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">甘是什么</h1><p id="22a9" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">生成对抗网络(Generative Adversarial Network)或GAN是一种用于生成建模的机器学习方法，由<a class="ae iu" href="https://en.wikipedia.org/wiki/Ian_Goodfellow" rel="noopener ugc nofollow" target="_blank"> Ian Goodfellow </a>和他的同事在2014年设计。一个GAN由两个模型组成:一个生成器和一个鉴别器，这两个模型通过一个对抗过程同时训练。生成器学习生成看起来像真实图像的图像，而鉴别器学习区分真实和虚假图像。</p><p id="b7bb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们训练这两个模型，直到鉴别器不能区分真实和伪造的图像。</p><p id="c4c4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">了解更多:<a class="ae iu" href="https://idiotdeveloper.com/gan-what-is-generative-adversarial-network/" rel="noopener ugc nofollow" target="_blank">甘——什么是生成性对抗网络？</a></p><h1 id="59ab" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">DCGAN和香草GAN有什么不同？</h1><p id="e16d" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">Ian Goodfellow开发的香草GAN基于密集或全连接层。DCGAN或深度卷积生成对抗网络是生成对抗网络(GAN)的扩展，使用卷积和转置卷积层构建。拉德福德等人在论文<a class="ae iu" href="https://arxiv.org/pdf/1511.06434.pdf" rel="noopener ugc nofollow" target="_blank">中首次描述了DCGAN，该论文使用深度卷积生成对抗网络</a>进行无监督表示学习。艾尔。</p><h1 id="dce9" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">动漫人脸数据集</h1><p id="7800" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">该数据集由从<a class="ae iu" href="https://www.kaggle.com/soumikrakshit/www.getchu.com" rel="noopener ugc nofollow" target="_blank">www.getchu.com</a>搜集的21551张动漫人脸组成，然后使用动漫人脸检测算法对其进行裁剪。为了方便起见，所有图像的大小都调整为64 * 64。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es lk"><img src="../Images/4844efc5ba8667e6bd9ad4b9ce428d62.png" data-original-src="https://miro.medium.com/v2/resize:fit:956/format:webp/0*WAeGCPcoEEY7Hz72.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">动漫人脸数据集中的一些图片</figcaption></figure><blockquote class="lp lq lr"><p id="8c32" class="iv iw ls ix b iy iz ja jb jc jd je jf lt jh ji jj lu jl jm jn lv jp jq jr js hb bi translated">下载:<a class="ae iu" href="https://www.kaggle.com/soumikrakshit/anime-faces" rel="noopener ugc nofollow" target="_blank">动漫脸</a></p></blockquote><h1 id="7dbd" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">履行</h1><p id="8c66" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">以下代码演示了深度卷积生成对抗网络(DCGAN)在动漫人脸数据集TensorFlow中的实现。</p><h1 id="f7f2" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">导入TensorFlow和其他库</h1><pre class="ll lm ln lo fd lw lx ly lz aw ma bi"><span id="770c" class="mb ki hi lx b fi mc md l me mf">import<!-- --> <!-- -->os<br/>import<!-- --> <!-- -->numpy as np<br/>import<!-- --> <!-- -->cv2<br/>from<!-- --> <!-- -->glob import<!-- --> <!-- -->glob<br/>from<!-- --> <!-- -->matplotlib import<!-- --> <!-- -->pyplot<br/>from<!-- --> <!-- -->sklearn.utils import<!-- --> <!-- -->shuffle<br/>import<!-- --> <!-- -->tensorflow as tf<br/>from<!-- --> <!-- -->tensorflow.keras.layers import<!-- --> <!-- -->*<br/>from<!-- --> <!-- -->tensorflow.keras.models import<!-- --> <!-- -->Model<br/>from<!-- --> <!-- -->tensorflow.keras.optimizers import<!-- --> <!-- -->Adam</span></pre><p id="0efc" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们定义动画图像的尺寸。</p><pre class="ll lm ln lo fd lw lx ly lz aw ma bi"><span id="84ef" class="mb ki hi lx b fi mc md l me mf">IMG_H = 64 <br/>IMG_W = 64 <br/>IMG_C = 3</span></pre><p id="64e5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">DCGAN中所有权重或核的权重初始化必须从均值=0.0且标准差= 0.02的正态分布中随机初始化。这个权重初始化器被用于卷积和转置卷积层。</p><pre class="ll lm ln lo fd lw lx ly lz aw ma bi"><span id="0c86" class="mb ki hi lx b fi mc md l me mf">w_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)</span></pre><h1 id="8038" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">加载和准备数据集</h1><p id="c98a" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated"><strong class="ix hj"> load_image </strong>函数获取一条图像路径并返回一个值在-1和1之间的张量。它执行以下任务:</p><ul class=""><li id="b0ed" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js mg jz ka kb bi translated">首先，我们读取图像路径。</li><li id="9106" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mg jz ka kb bi translated">接下来，我们读取JPEG图像文件并返回uint8张量。</li><li id="68b5" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mg jz ka kb bi translated">接下来，使用任何额外的裁剪或填充来调整图像的大小。</li><li id="f8b7" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mg jz ka kb bi translated">图像的数据类型更改为float 32。</li><li id="8579" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mg jz ka kb bi translated">现在，我们将图像像素值归一化到-1到1的范围内。</li></ul><pre class="ll lm ln lo fd lw lx ly lz aw ma bi"><span id="c5a6" class="mb ki hi lx b fi mc md l me mf">def<!-- --> <!-- -->load_image(image_path):<br/>    img =<!-- --> <!-- -->tf.io.read_file(image_path)<br/>    img =<!-- --> <!-- -->tf.io.decode_jpeg(img)<br/>    img =<!-- --> <!-- -->tf.image.resize_with_crop_or_pad(img, IMG_H, IMG_W)<br/>    img =<!-- --> <!-- -->tf.cast(img, tf.float32)<br/>    img =<!-- --> <!-- -->(img -<!-- --> <!-- -->127.5) /<!-- --> <!-- -->127.5<br/>    return<!-- --> <!-- -->img</span></pre><p id="9beb" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> tf_dataset </strong>函数用于为训练设置TensorFlow数据集管道。</p><pre class="ll lm ln lo fd lw lx ly lz aw ma bi"><span id="8c82" class="mb ki hi lx b fi mc md l me mf">def<!-- --> <!-- -->tf_dataset(images_path, batch_size):<br/>    dataset =<!-- --> <!-- -->tf.data.Dataset.from_tensor_slices(images_path)<br/>    dataset =<!-- --> <!-- -->dataset.shuffle(buffer_size=10240)<br/>    dataset =<!-- --> <!-- -->dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)<br/>    dataset =<!-- --> <!-- -->dataset.batch(batch_size)<br/>    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)<br/>    <br/>    return<!-- --> <!-- -->dataset</span></pre><h1 id="baa4" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">转置卷积块</h1><p id="a686" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">转置卷积用于建立发生器模型。它用于增加传入特征地图的尺寸(高度和宽度)。</p><pre class="ll lm ln lo fd lw lx ly lz aw ma bi"><span id="cb76" class="mb ki hi lx b fi mc md l me mf">def<!-- --> <!-- -->deconv_block(inputs, num_filters, kernel_size, strides, bn=True):<br/>    x =<!-- --> <!-- -->Conv2DTranspose(<br/>        filters=num_filters,<br/>        kernel_size=kernel_size,<br/>        kernel_initializer=w_init,<br/>        padding="same",<br/>        strides=strides,<br/>        use_bias=False<br/>    )(inputs)<br/> <br/>    if<!-- --> <!-- -->bn:<br/>         x =<!-- --> <!-- -->BatchNormalization()(x)<br/>         x =<!-- --> <!-- -->LeakyReLU(alpha=0.2)(x)</span><span id="7f56" class="mb ki hi lx b fi mh md l me mf">    return<!-- --> <!-- -->x</span></pre><p id="66d5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">步长卷积用于建立鉴别器模型。</p><pre class="ll lm ln lo fd lw lx ly lz aw ma bi"><span id="d7ed" class="mb ki hi lx b fi mc md l me mf">def<!-- --> <!-- -->conv_block(inputs, num_filters, kernel_size, padding="same", strides=2, activation=True):<br/>    x =<!-- --> <!-- -->Conv2D(<br/>        filters=num_filters,<br/>        kernel_size=kernel_size,<br/>        kernel_initializer=w_init,<br/>        padding=padding,<br/>        strides=strides,<br/>    )(inputs)</span><span id="4c3c" class="mb ki hi lx b fi mh md l me mf">    if<!-- --> <!-- -->activation:<br/>         x =<!-- --> <!-- -->LeakyReLU(alpha=0.2)(x)<br/>         x =<!-- --> <!-- -->Dropout(0.3)(x)</span><span id="9153" class="mb ki hi lx b fi mh md l me mf">    return<!-- --> <!-- -->x</span></pre><h1 id="9028" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">发电机</h1><p id="38ec" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">生成器获取潜在向量中的随机噪声，并将其映射到数据空间。因为我们使用RGB图像，所以我们的数据空间意味着创建一个RGB图像。</p><p id="61d4" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">发生器从密集层或全连接层开始。之后，是一系列的转置卷积，批量归一化和泄漏relu激活函数。</p><p id="4c8f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">最后，我们使用一个具有三个滤波器和tanh激活函数的卷积层来生成RGB图像。</p><pre class="ll lm ln lo fd lw lx ly lz aw ma bi"><span id="0125" class="mb ki hi lx b fi mc md l me mf">def<!-- --> <!-- -->build_generator(latent_dim):<br/>    f =<!-- --> <!-- -->[2**i for<!-- --> <!-- -->i in<!-- --> <!-- -->range(5)][::-1]<br/>    filters =<!-- --> <!-- -->32<br/>    output_strides =<!-- --> <!-- -->16<br/>    h_output =<!-- --> <!-- -->IMG_H //<!-- --> <!-- -->output_strides<br/>    w_output =<!-- --> <!-- -->IMG_W //<!-- --> <!-- -->output_strides</span><span id="84fc" class="mb ki hi lx b fi mh md l me mf">    noise =<!-- --> <!-- -->Input(shape=(latent_dim,), name="generator_noise_input")</span><span id="3825" class="mb ki hi lx b fi mh md l me mf">    x =<!-- --> <!-- -->Dense(f[0] * filters * h_output *<!-- --> <!-- -->w_output, use_bias=False)(noise)<br/>    x =<!-- --> <!-- -->BatchNormalization()(x)<br/>    x =<!-- --> <!-- -->LeakyReLU(alpha=0.2)(x)<br/>    x =<!-- --> <!-- -->Reshape((h_output, w_output, 16<!-- --> <!-- -->*<!-- --> <!-- -->filters))(x)</span><span id="3036" class="mb ki hi lx b fi mh md l me mf">    for<!-- --> <!-- -->i in<!-- --> <!-- -->range(1, 5):<br/>        x =<!-- --> <!-- -->deconv_block(x,<br/>            num_filters=f[i] *<!-- --> <!-- -->filters,<br/>            kernel_size=5,<br/>            strides=2,<br/>            bn=True<br/>        )</span><span id="a925" class="mb ki hi lx b fi mh md l me mf">    x =<!-- --> <!-- -->conv_block(x,<br/>        num_filters=3,<br/>        kernel_size=5,<br/>        strides=1,<br/>        activation=False<br/>    )<br/>    fake_output =<!-- --> <!-- -->Activation("tanh")(x)</span><span id="f6b9" class="mb ki hi lx b fi mh md l me mf">    return<!-- --> <!-- -->Model(noise, fake_output, name="generator")</span></pre><h1 id="d4bd" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">鉴别器</h1><p id="c398" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">鉴别器是一个简单的二进制分类网络，它获取真实和伪造图像，并输出给定图像是真实还是伪造的概率。</p><p id="d722" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为此，泄漏relu使用了一个级数步长卷积，压差为0.3。最后，我们展平特征地图，并使用1个单位的全连通层。接下来，我们对完全连接的层应用sigmoid激活函数。</p><pre class="ll lm ln lo fd lw lx ly lz aw ma bi"><span id="3dcb" class="mb ki hi lx b fi mc md l me mf">def<!-- --> <!-- -->build_discriminator():<br/>    f =<!-- --> <!-- -->[2**i for<!-- --> <!-- -->i in<!-- --> <!-- -->range(4)]<br/>    image_input =<!-- --> <!-- -->Input(shape=(IMG_H, IMG_W, IMG_C))<br/>    x =<!-- --> <!-- -->image_input<br/>    filters =<!-- --> <!-- -->64<br/>    output_strides =<!-- --> <!-- -->16<br/>    h_output =<!-- --> <!-- -->IMG_H //<!-- --> <!-- -->output_strides<br/>    w_output =<!-- --> <!-- -->IMG_W //<!-- --> <!-- -->output_strides</span><span id="d0bf" class="mb ki hi lx b fi mh md l me mf">    for<!-- --> <!-- -->i in<!-- --> <!-- -->range(0, 4):<br/>        x =<!-- --> <!-- -->conv_block(x, num_filters=f[i] *<!-- --> <!-- -->filters, kernel_size=5, strides=2)</span><span id="0eb5" class="mb ki hi lx b fi mh md l me mf">    x =<!-- --> <!-- -->Flatten()(x)<br/>    x =<!-- --> <!-- -->Dense(1)(x)</span><span id="3cc1" class="mb ki hi lx b fi mh md l me mf">    return<!-- --> <!-- -->Model(image_input, x, name="discriminator")</span></pre><h1 id="2968" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">完整的DCGAN模型</h1><p id="2450" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">GAN类表示完整的DCGAN模型，其中定义了训练步骤。它采用鉴别器模型、生成器模式和损失函数。这里使用的损失函数是二元交叉熵。</p><p id="5fab" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj"> train_step </strong>函数用于训练DCGAN模型。训练从鉴别器开始。鉴别器首先被训练在由生成器生成的假图像上。之后，它在动画人脸数据集中的真实图像上进行训练。接下来，基于鉴别器的训练程度来训练生成器。</p><pre class="ll lm ln lo fd lw lx ly lz aw ma bi"><span id="f9e7" class="mb ki hi lx b fi mc md l me mf">class<!-- --> <!-- -->GAN(Model):<br/>    def<!-- --> <!-- -->__init__(self, discriminator, generator, latent_dim):<br/>        super(GAN, self).__init__()<br/>        self.discriminator =<!-- --> <!-- -->discriminator<br/>        self.generator =<!-- --> <!-- -->generator<br/>        self.latent_dim =<!-- --> <!-- -->latent_dim</span><span id="a470" class="mb ki hi lx b fi mh md l me mf">    def<!-- --> <!-- -->compile(self, d_optimizer, g_optimizer, loss_fn):<br/>        super(GAN, self).compile()<br/>        self.d_optimizer =<!-- --> <!-- -->d_optimizer<br/>        self.g_optimizer =<!-- --> <!-- -->g_optimizer<br/>        self.loss_fn =<!-- --> <!-- -->loss_fn</span><span id="6501" class="mb ki hi lx b fi mh md l me mf">    def<!-- --> <!-- -->train_step(self, real_images):<br/>        batch_size =<!-- --> <!-- -->tf.shape(real_images)[0]<br/>        <br/>        for<!-- --> <!-- -->_ in<!-- --> <!-- -->range(2):<br/>            ## Train the discriminator<br/>            random_latent_vectors =<!-- --> <!-- -->tf.random.normal(shape=(batch_size, self.latent_dim))<!-- --> <br/>            <!-- -->generated_images =<!-- --> <!-- -->self.generator(random_latent_vectors)<br/>            generated_labels =<!-- --> <!-- -->tf.zeros((batch_size, 1))</span><span id="e816" class="mb ki hi lx b fi mh md l me mf">            with tf.GradientTape() as ftape:<br/>                predictions =<!-- --> <!-- -->self.discriminator(generated_images)<br/>                d1_loss =<!-- --> <!-- -->self.loss_fn(generated_labels, predictions)<br/>            grads =<!-- --> <!-- -->ftape.gradient(d1_loss, self.discriminator.trainable_weights)<br/>            self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))</span><span id="aeab" class="mb ki hi lx b fi mh md l me mf">            ## Train the discriminator<br/>            labels =<!-- --> <!-- -->tf.ones((batch_size, 1))</span><span id="8c34" class="mb ki hi lx b fi mh md l me mf">             with tf.GradientTape() as rtape:<br/>                 predictions =<!-- --> <!-- -->self.discriminator(real_images)<br/>                 d2_loss =<!-- --> <!-- -->self.loss_fn(labels, predictions)</span><span id="a66e" class="mb ki hi lx b fi mh md l me mf">            grads =<!-- --> <!-- -->rtape.gradient(d2_loss, self.discriminator.trainable_weights)<br/>            self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))</span><span id="db87" class="mb ki hi lx b fi mh md l me mf">            ## Train the generator<br/>            random_latent_vectors =<!-- --> <!-- -->tf.random.normal(shape=(batch_size, self.latent_dim))<br/>            misleading_labels =<!-- --> <!-- -->tf.ones((batch_size, 1))</span><span id="7be5" class="mb ki hi lx b fi mh md l me mf">            with tf.GradientTape() as gtape:<br/>                predictions =<!-- --> <!-- -->self.discriminator(self.generator(random_latent_vectors))<br/>                g_loss =<!-- --> <!-- -->self.loss_fn(misleading_labels, predictions)</span><span id="d072" class="mb ki hi lx b fi mh md l me mf">            grads =<!-- --> <!-- -->gtape.gradient(g_loss, self.generator.trainable_weights)<br/>            self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))</span><span id="0aeb" class="mb ki hi lx b fi mh md l me mf">           return<!-- --> <!-- -->{"d1_loss": d1_loss, "d2_loss": d2_loss, "g_loss": g_loss}</span></pre><h1 id="8315" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">保存图像</h1><pre class="ll lm ln lo fd lw lx ly lz aw ma bi"><span id="a4a6" class="mb ki hi lx b fi mc md l me mf">def<!-- --> <!-- -->save_plot(examples, epoch, n):<br/>    examples =<!-- --> <!-- -->(examples +<!-- --> <!-- -->1) /<!-- --> <!-- -->2.0<br/>    for<!-- --> <!-- -->i in<!-- --> <!-- -->range(n *<!-- --> <!-- -->n):<br/>        pyplot.subplot(n, n, i+1)<br/>        pyplot.axis("off")<br/>        pyplot.imshow(examples[i])</span><span id="e324" class="mb ki hi lx b fi mh md l me mf">    filename =<!-- --> <!-- -->f"samples/generated_plot_epoch-{epoch+1}.png"<br/>    pyplot.savefig(filename)<br/>    pyplot.close()</span></pre><h1 id="ce41" class="kh ki hi bd kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le bi translated">最后，运行代码</h1><pre class="ll lm ln lo fd lw lx ly lz aw ma bi"><span id="b7d0" class="mb ki hi lx b fi mc md l me mf">if<!-- --> <!-- -->__name__ ==<!-- --> <!-- -->"__main__":<br/>    ## Hyperparameters<br/>    batch_size =<!-- --> <!-- -->128<br/>    latent_dim =<!-- --> <!-- -->128<br/>    num_epochs =<!-- --> <!-- -->60<br/>    images_path =<!-- --> <!-- -->glob("data/*")</span><span id="3336" class="mb ki hi lx b fi mh md l me mf">    d_model =<!-- --> <!-- -->build_discriminator()<br/>    g_model =<!-- --> <!-- -->build_generator(latent_dim)</span><span id="0f0f" class="mb ki hi lx b fi mh md l me mf">    # d_model.load_weights("saved_model/d_model.h5")<br/>    # g_model.load_weights("saved_model/g_model.h5")</span><span id="6482" class="mb ki hi lx b fi mh md l me mf">    d_model.summary()<br/>    g_model.summary()</span><span id="63ca" class="mb ki hi lx b fi mh md l me mf">    gan =<!-- --> <!-- -->GAN(d_model, g_model, latent_dim)</span><span id="9220" class="mb ki hi lx b fi mh md l me mf">    bce_loss_fn =<!-- --> <!-- -->tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.1)<br/>    d_optimizer =<!-- --> <!-- -->tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)<br/>    g_optimizer =<!-- --> <!-- -->tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)<br/>    gan.compile(d_optimizer, g_optimizer, bce_loss_fn)</span><span id="3fbe" class="mb ki hi lx b fi mh md l me mf">    images_dataset =<!-- --> <!-- -->tf_dataset(images_path, batch_size)<br/>    <br/>    for<!-- --> <!-- -->epoch in<!-- --> <!-- -->range(num_epochs):<br/>        gan.fit(images_dataset, epochs=1)<br/>        g_model.save("saved_model/g_model.h5")<br/>        d_model.save("saved_model/d_model.h5")</span><span id="75e8" class="mb ki hi lx b fi mh md l me mf">        n_samples =<!-- --> <!-- -->25<br/>        noise =<!-- --> <!-- -->np.random.normal(size=(n_samples, latent_dim))<br/>        examples =<!-- --> <!-- -->g_model.predict(noise)<br/>        save_plot(examples, epoch, int(np.sqrt(n_samples)))</span></pre><p id="c8e0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，我们将看到在第一个和最后一个时期产生的图像的变化。</p><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mi"><img src="../Images/1b272967e54ccf6bc1bc22a4ef188f3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/0*DZYWSjKl34cUjLca.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">第1时段生成的图像</figcaption></figure><figure class="ll lm ln lo fd ij er es paragraph-image"><div class="er es mj"><img src="../Images/ba962da01528d21eea22dc99a99d316e.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/0*mi9gIbdWqQhmO18Y.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">在60年代生成的图像</figcaption></figure><p id="283e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此，你可以看到在60个历元上训练DCGAN之后，生成器开始生成看起来真实的图像。</p></div><div class="ab cl mk ml gp mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="hb hc hd he hf"><h1 id="cd81" class="kh ki hi bd kj kk mr km kn ko ms kq kr ks mt ku kv kw mu ky kz la mv lc ld le bi translated">摘要</h1><p id="9ce9" class="pw-post-body-paragraph iv iw hi ix b iy lf ja jb jc lg je jf jg lh ji jj jk li jm jn jo lj jq jr js hb bi translated">在本教程中，我们学习在动画人脸数据集上实现深度卷积生成对抗网络(DCGAN)。我希望在本教程之后，你将开始构建自己的DCGANs。</p><p id="47ea" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">还有一些问题，在下面评论我会尽力回答。更多更新。跟我来。</p><ul class=""><li id="e028" class="jt ju hi ix b iy iz jc jd jg jv jk jw jo jx js mg jz ka kb bi translated"><a class="ae iu" href="https://www.youtube.com/idiotdeveloper" rel="noopener ugc nofollow" target="_blank"> YouTube </a></li><li id="fc00" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mg jz ka kb bi translated"><a class="ae iu" href="https://facebook.com/idiotdeveloper" rel="noopener ugc nofollow" target="_blank">脸书</a></li><li id="3044" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mg jz ka kb bi translated"><a class="ae iu" href="https://twitter.com/nikhilroxtomar" rel="noopener ugc nofollow" target="_blank">推特</a></li><li id="c584" class="jt ju hi ix b iy kc jc kd jg ke jk kf jo kg js mg jz ka kb bi translated"><a class="ae iu" href="https://www.instagram.com/nikhilroxtomar" rel="noopener ugc nofollow" target="_blank"> Instagram </a></li></ul></div><div class="ab cl mk ml gp mm" role="separator"><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp mq"/><span class="mn bw bk mo mp"/></div><div class="hb hc hd he hf"><p id="051f" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="ls">原载于2020年7月24日https://idiotdeveloper.com</em><a class="ae iu" href="http://idiotdeveloper.com/dcgan-implementing-deep-convolutional-generative-adversarial-network-in-tensorflow/" rel="noopener ugc nofollow" target="_blank"><em class="ls"/></a><em class="ls">。</em></p></div></div>    
</body>
</html>