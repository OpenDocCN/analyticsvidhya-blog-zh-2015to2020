<html>
<head>
<title>Don’t Overfit</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不要吃太多</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/dont-overfit-645e3957f30b?source=collection_archive---------19-----------------------#2020-05-20">https://medium.com/analytics-vidhya/dont-overfit-645e3957f30b?source=collection_archive---------19-----------------------#2020-05-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="cd5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">业务问题:</strong></p><p id="a859" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">过度拟合是机器学习模型的一个常见问题，尤其是当我们只有几个训练数据点时。训练数据点的数量越少，我们的模型就越不能对看不见的或测试数据点进行归纳。</p><p id="2431" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们需要在训练过程中小心谨慎，看看我们的模型表现如何。通过在训练数据上获得大约90%的准确率，我们不能假设我们的模型将在看不见的数据集上表现相同。</p><p id="9c3e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是我们今天要处理的问题。我们还将看到如何使用简单的机器学习模型，如<strong class="ih hj"><em class="jd">KNeighborsClassifier</em></strong>和<strong class="ih hj"><em class="jd">LogisticRegression</em></strong>，我们可以减少过度拟合，并帮助我们的模型在看不见的数据上更好地推广，即使我们有少量的训练数据。</p><p id="24e5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据来源:</strong></p><p id="58f5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了解决过度拟合问题，我们今天将使用Kaggle数据集。Kaggle发起了一个竞赛<a class="ae je" href="https://www.kaggle.com/c/dont-overfit-ii" rel="noopener ugc nofollow" target="_blank">不要过度拟合2 </a>。在这个问题中，我们提供了一个250点的训练数据集和19750点的测试数据集。</p><p id="f9df" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">差别很大，对吧？因此，使用这个数据集，我们将训练简单的机器学习模型，并看看我们如何才能使我们的模型在看不见的数据点上更好地正则化。</p><p id="4f7d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以在我的github资源库中找到完整的代码，包括EDA和模型部署。但是我建议，在深入研究代码之前，让我们一步一步地看看这些数据中发生了什么，以及我们解决这个问题的方法。</p><p id="0641" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">现有解决方案&amp;思路:</strong></p><p id="8d14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗯，我们不是唯一一个遇到过度拟合问题的人。有许多人面临着同样的问题。所以，继续让我们看看他们解决问题的想法:</p><ol class=""><li id="1d3e" class="jf jg hi ih b ii ij im in iq jh iu ji iy jj jc jk jl jm jn bi translated"><a class="ae je" href="https://www.kaggle.com/featureblind/robust-lasso-patches-with-rfe-gs" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/feature blind/robust-lasso-patches-with-rfe-GS</a></li></ol><p id="c89c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个解决方案内核可以在Kaggle上获得，并提供87%的auc分数。这里使用RFE(递归特征提取器)来帮助选择最佳特征。通过选择顶级特征并对其进行训练，我们可以在相当大的程度上消除过度拟合。</p><p id="00fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个内核的一个问题是，在标准化的同时，它也使用了公共数据集，这可能适用于Kaggle竞赛，但不适用于真实世界。从这里我们可以得到的主要东西是结合使用RFE和GridSearchCV以及健壮的auc_score。</p><ol class=""><li id="a118" class="jf jg hi ih b ii ij im in iq jh iu ji iy jj jc jk jl jm jn bi translated"><a class="ae je" href="https://www.researchgate.net/post/How_to_Avoid_Overfitting" rel="noopener ugc nofollow" target="_blank">https://www.researchgate.net/post/How_to_Avoid_Overfitting</a></li></ol><p id="80d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里我们得到了一个很好的答案来减少过度拟合，我们可以尝试选择一些特征，使它们少于或等于可用数据点的一半。</p><ol class=""><li id="5e3e" class="jf jg hi ih b ii ij im in iq jh iu ji iy jj jc jk jl jm jn bi translated"><a class="ae je" href="https://www.kaggle.com/rafjaa/dealing-with-very-small-datasets" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/RAF jaa/处理非常小的数据集</a></li></ol><p id="ceb4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从这个内核中，我们获得了许多洞见。这里不仅建议重要的特征，还建议进行过采样，以增加数据点的数量。这里，作者还使用了使用多个模型并组合结果的集成技术，以避免过度拟合。</p><ol class=""><li id="ee11" class="jf jg hi ih b ii ij im in iq jh iu ji iy jj jc jk jl jm jn bi translated"><a class="ae je" href="https://www.kaggle.com/iloveyyp/logistic-regression-with-rfecv" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/ILO vey yp/logistic-regression-with-RF ecv</a></li></ol><p id="33d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，作者使用了一个简单的逻辑回归模型，通过GridSearchCV寻找最佳参数。建议使用RFE来选择最佳特征。数据标准化和规范化是在对数据进行训练模型之前完成的。</p><ol class=""><li id="9e49" class="jf jg hi ih b ii ij im in iq jh iu ji iy jj jc jk jl jm jn bi translated">【https://www.kaggle.com/plasticgrammer/don-t-overfit-i-try T4】</li></ol><p id="d069" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这个内核中，作者还重点研究了特征工程中的最佳特征选择。这里不使用Kfold验证，而是使用RepeatedStratifiedKfold，这将有助于我们处理可用数据集较少的情况。</p><p id="00ba" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我们的方法来自现有的解决方案:</strong></p><p id="7e1f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，结合现有的解决方案并添加我们的改进因子，我们将执行良好的功能选择，并查看我们的模型如何执行。我们还将尝试许多机器学习模型，并选择我们的模型表现最好的一个。</p><p id="51ee" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这里，特征选择是一个重要的方面。因此，我们将尝试一些特征选择技术，并观察它们对我们模型的影响。</p><p id="aa94" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，让我们停止理论，开始工作。是时候让我们的手变脏了。</p><p id="c7b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我们的训练数据集:</strong></p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es jo"><img src="../Images/0569ba43e7a2f703d6ab9029be64f5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*VWjUzfXdfoBK7mKg2mZfSQ.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">打印出几行我们的训练数据集</figcaption></figure><p id="57dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有250个数据点和302列，包括标签和id列。我们的训练数据集将只包含从0到299的特征。因此，我们将从这些数据中取出标签并训练数据集。</p><p id="c78b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们有了数据，让我们开始EDA。</p><p id="8163" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">平衡还是不平衡？</strong></p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es ke"><img src="../Images/37e3f1bfe0f0a1fa66b9f20ba80b3dee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qrp797uOL-zq-Ogmn1f_oA.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">我们训练数据中标签计数的饼图</figcaption></figure><p id="750f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的数据集不平衡。因此，计算<strong class="ih hj"> <em class="jd"> auc分数</em> </strong>比计算准确度更好。此外，在训练我们的模型时平衡分类标签可能会有所帮助。</p><p id="34c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据集中特征的均值分布:</strong></p><figure class="jp jq jr js fd jt er es paragraph-image"><div class="er es kf"><img src="../Images/c07fc3cd62c213b0fd808ba92aec68db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*nHa1bPbrM2GGcckbTw2MhA.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">所有特征的平均值直方图</figcaption></figure><p id="de14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这些特征的平均值分别在-0.20到0.18之间。此外，我们看到大多数点在-0.05到0.10的范围内。因此，数据集要素的平均分布看起来像正态分布，有点偏向正值。</p><p id="4b83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们对数据集中的标准差、最小值和最大值进行同样的尝试。</p><p id="293c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据集中特征的标准偏差分布:</strong></p><figure class="jp jq jr js fd jt er es paragraph-image"><div class="er es kg"><img src="../Images/b1286d796293d3dba63718b13f15e056.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/1*94DF0seUa7WRwua4K6hUFg.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">所有特征的标准差直方图</figcaption></figure><p id="ff00" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的直方图中我们可以看出，数据集中要素的标准偏差介于0.92和1.10之间，大多数要素的标准偏差值为1。因此，这进一步给了我们一个更好的想法，即我们的数据集的特征来自一个平均值约等于0且标准差为1的正态分布。</p><p id="0d2b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据集中特征的最小值分布:</strong></p><figure class="jp jq jr js fd jt er es paragraph-image"><div class="er es kh"><img src="../Images/af2388441b13fefa810541361910ec52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*63EQvEOL5czpEsvjtNCIdw.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">所有特征的最小值直方图</figcaption></figure><p id="32b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最小值在-4.0到-2.0之间，大部分在-3.0到-2.5之间。这让我们对数据集有了更多的了解，如果数据集小于-4.0，它就没有任何价值。我们只能从直方图中看到一些值，这些值可能是异常值。</p><p id="732f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">数据集中特征的最大值分布:</strong></p><figure class="jp jq jr js fd jt er es paragraph-image"><div class="er es jo"><img src="../Images/f5aab90bddff748085fddcc468a14416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1250/format:webp/1*8Z1qD5mbSYtLsAGGxKaU5g.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">所有特征的最大值直方图</figcaption></figure><p id="915c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最大值范围从2.00到3.75。因此，在某种程度上，我们的最小值是对称的。</p><p id="2c5b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">从EDA中我们了解到我们的特征是来自正态分布。我们的数据集不平衡，正数据点(类标签1)的数量大约是负数据点(类标签0)的两倍。</em>T9】</strong></p><p id="18da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> <em class="jd">由于我们的数据集是随机的，因此我们没有关于特征的任何特定信息。</em>T13】</strong></p><p id="fa1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">建筑模型:</strong></p><p id="2a38" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，是时候建立我们的模型了。首先，我们将尝试使用KNeighborsClassifier。这是一种简单的算法，它根据相邻数据点来决定数据点的类别。</p><p id="e3a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">用KNN进行重复分层折叠，选择最佳超参数:</strong></p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es ki"><img src="../Images/e9abee4c6b6fcfc2781887fef9187ab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qMCy14o6kS4YGpuZJKjfIQ.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">使用KNN对重复分层文件夹进行代码剪切</figcaption></figure><p id="83dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们得到了n_neighbors值为80的最佳超参数。当我们用这个参数训练我们的模型并在Kaggle上提交结果时，我们得到的auc分数是0.686。</p><p id="ce03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">虽然不是很好，但看起来还是不错的。但是，是的，我们可以做得更好，所以我们会尝试其他模式。</p><p id="deed" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">带逻辑回归的重复分层折叠</strong></p><p id="ffd9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一个简单的分类模型是逻辑回归。所以，让我们用这个来执行RepeatedStratifiedKFold，看看我们的模型如何执行:</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es kj"><img src="../Images/1af63ee785545aa9e99c74a025346276.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0X9HprNHl_RjWs0WDADz0A.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">Kaggle提交分数</figcaption></figure><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es kk"><img src="../Images/ae6a4561f18d28fb9dfe0ba46e95a7eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_MEzALbvnducDvmSZH6uQA.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">带有逻辑回归的重复分层文件夹的代码段</figcaption></figure><p id="c5bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们提交用逻辑回归调整的参数得到的分数时，我们得到以下Kaggle分数:</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es kl"><img src="../Images/2c3bc4ff56eccc829764fd49f98ab2e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BfZDMPypDzuPpS9JmZIqug.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">卡格尔分数逻辑回归</figcaption></figure><p id="1baf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">哇，跳得真好。但是这是否意味着逻辑回归比KNeighborsClassifier更好呢？嗯，看情况。在我们的案例中，逻辑回归比KNN表现得更好，不仅仅是因为它概括得更好，而是因为它没有使用所有的特征。</p><p id="81c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗯，我们可以看到它使用L1正则化，我们都知道L1正则化执行特征选择，创建稀疏性或使不太重要的特征为零。</p><p id="41bf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了证明这一点，我们要做一个小实验。我们将通过结合最佳特征选择技术在KNN上执行特征选择，然后看看我们的模型如何执行..</p><p id="baf3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那么，让我们开始吧:</p><p id="8a45" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">特征选择策略:</strong></p><p id="4eab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将通过组合几种方法来执行特征选择。您可以从github资源库获得它的完整代码。下面将简要介绍其中的每一个:</p><ol class=""><li id="7f25" class="jf jg hi ih b ii ij im in iq jh iu ji iy jj jc jk jl jm jn bi translated"><strong class="ih hj">使用EDA的特征选择:</strong></li></ol><p id="5038" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当我们绘制散点图时，我们可以确定特征的重要性，看看特定的特征是否能够区分类别标签。例如:</p><figure class="jp jq jr js fd jt er es paragraph-image"><div class="er es km"><img src="../Images/1fc6d25ac4bfa2f4cb1ee21f671c5745.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*_cHbCRwIPLqvsn4EL31sTg.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">功能-1</figcaption></figure><figure class="jp jq jr js fd jt er es paragraph-image"><div class="er es kn"><img src="../Images/c2f2f4614e27e07865c83774754da5f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*1UB2AmaQgHYZ8AZdCQowJA.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">功能-2</figcaption></figure><figure class="jp jq jr js fd jt er es paragraph-image"><div class="er es ko"><img src="../Images/f74446af470cb60eac4b3ee87052419e.png" data-original-src="https://miro.medium.com/v2/resize:fit:282/format:webp/1*CuN6hTgr1E33PW7hTDAPvw.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">功能-3</figcaption></figure><p id="d8c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从上面的图像中，我们可以看到两个类的特征1的PDF重叠。在特征-2中，重叠非常少，而在特征-3中，高于某个值时，所有点都属于特定的类。</p><p id="5c8d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，我们可以说特性1是最不重要的特性，而特性3是最重要的特性。这是我们通过查看数据分布来选择特征的特征选择方法之一。</p><p id="3d91" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 2。皮尔逊相关法:</strong></p><p id="b0a2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用的第二种特征选择方法是人物相关方法。在我们的数据集中，我们经常会发现相互依赖的特征。</p><p id="d379" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如:</p><p id="d8ac" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">设x，y，z，a，b，c为我们数据集中的尺寸特征。如果两个特征之间存在直接关系，那么我们可以很容易地使用其中一个，而不是两个都用。</p><p id="9863" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果从上述特征我们有这样的关系:</p><p id="d5e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">x=2y</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es kp"><img src="../Images/516278fe380a01a6d4a6c97f65856893.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*HkI9qdXYmIagVf6JocZt2g.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">x=2y的图形</figcaption></figure><p id="0f87" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里，我们看到x和y之间的直接关系。这意味着这两个特征是强相关的。因此，我们只能在数据集中使用其中一个。</p><p id="0e0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 3。递归特征消除:</strong></p><p id="620d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">RFE是另一种常见的方法，它在给定的模型上训练我们的数据集，并根据该模型选择最佳特征。在每次迭代中，它会消除一组特征，直到获得所需的特征数。</p><p id="cb21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这可以直接使用sklearn库来实现。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es kk"><img src="../Images/ef535fe7579b4e1a904852c189211efe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xO5p_gXd6gLpLDBUA2u4OA.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">特征选择-使用RFE最好的模型</figcaption></figure><p id="828c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4。a)基于模型的特征选择(Lasso回归):</strong></p><p id="a2dd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们在sklearn中的模型很少提供feature _ importances，使用它我们可以找出最好的特性。类似地，在逻辑回归的情况下，特征重要性由特征的权重系数确定。</p><p id="74b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Lasso回归是指L1正则化的逻辑回归。我们将使用sklearn的<strong class="ih hj"> SelectFromModel </strong>方法使用相同的方法来找到我们模型中的最佳特性。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es kq"><img src="../Images/3efda735e086b70e5354813c3a8fc3df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AZR6CH-V6chravH71oUL9g.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">套索选择器的代码段</figcaption></figure><p id="3f50" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> 4.b)基于模型的特征选择(决策树):</strong></p><p id="983b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">就像逻辑回归一样，我们也从分类器外得到特征重要性。所以，让我们使用相同的，并找出最好的功能:</p><figure class="jp jq jr js fd jt er es paragraph-image"><div class="er es kr"><img src="../Images/41e25427a32ee358112ed19029352197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*Un0TtOrT7V6hGTwrmdBP5A.png"/></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">树方法的代码段</figcaption></figure><p id="6059" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">综合所有特征:</strong></p><p id="f7c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们已经得到了所有的特征，我们将挑选所有方法中的共同特征，并将结果分离在一起。</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es ks"><img src="../Images/5944cc3b7292f6365171a9964878289e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*28C7v5s6sk4AS4C8NBKdRQ.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">组合最佳功能的代码片段</figcaption></figure><p id="7796" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以，现在我们有了我们最好的特性，让我们用很少的特性来执行我们的KNN，看看性能。</p><p id="bf9c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">具有前50个特征的重复分层文件夹KNN:</strong></p><p id="ca49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将再次使用前50个特征在KNeighborsClassifier上超调优我们的模型，获得最佳超参数并找出我们的分数:</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es kt"><img src="../Images/2db31bf43f7f8a8f4d01467dbb6abd47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DHQEhk3833Gydb2sTg8yEg.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">KNN-具有50个特征的重复分层折叠</figcaption></figure><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es ku"><img src="../Images/f561e5ae707684c43e699415be282bdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D_7cIw5M9q7AWzOg1QXYNw.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">有50个特征的KNN的卡格尔分数</figcaption></figure><p id="d8e3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">万岁！我们确实是对的。通过减少特征的数量，我们的KNN比以前表现得更好。因此，减少了过度拟合。这意味着LogisticRegression表现非常好的原因之一是它能够在L1正则化的帮助下使用重要的特征进行概括。</p><p id="9bcb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">所有型号结果:</strong></p><p id="12fe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好了，我们用KNN和LR的不同数量的特征玩了一会儿，下面是我们得到的结果:</p><figure class="jp jq jr js fd jt er es paragraph-image"><div role="button" tabindex="0" class="ju jv di jw bf jx"><div class="er es kv"><img src="../Images/5f7560a764d44a2bd42ae8389df4d9b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NNCV_IyF7t2lWBEosP7wfg.png"/></div></div><figcaption class="ka kb et er es kc kd bd b be z dx translated">我们所有型号的总结</figcaption></figure><p id="1583" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，L1正则化的LogisticRegression给了我们最好的结果，auc分数为0.844</p><p id="2baf" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">未来工作:</strong></p><p id="076f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗯，我们取得了不错的成绩，战胜了过度拟合，但这仅仅是开始。我们可以尝试更多的调整，进一步改善我们的结果。我们可以尝试进一步减少过度拟合的一些方法包括:</p><ol class=""><li id="ad54" class="jf jg hi ih b ii ij im in iq jh iu ji iy jj jc jk jl jm jn bi translated"><strong class="ih hj"> <em class="jd">特征创建</em> </strong>:在这段代码中，我们看到了特征选择是如何影响我们的模型的。因此，除了选择最佳要素外，您还可以尝试使用平均值、最大值、标准偏差值等创建少量要素。</li><li id="5ac5" class="jf jg hi ih b ii kw im kx iq ky iu kz iy la jc jk jl jm jn bi translated"><strong class="ih hj"> <em class="jd">加权方法:</em> </strong>嗯，你也可以尝试使用加权求和法合并多个模型的结果。你可以给表现更好的模型更多的权重，反之亦然。</li><li id="0518" class="jf jg hi ih b ii kw im kx iq ky iu kz iy la jc jk jl jm jn bi translated"><strong class="ih hj"> <em class="jd">堆叠分类器</em> </strong>:这里我们只用到了简单的模型。您也可以尝试堆叠分类器。这也可能有助于改善结果。</li></ol><p id="d2c1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> GitHub库:</strong></p><p id="efc5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae je" href="https://github.com/mansi2596/Kaggle-Don-t-Overfit-2" rel="noopener ugc nofollow" target="_blank">https://github.com/mansi2596/Kaggle-Don-t-Overfit-2</a></p><p id="5c5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考文献:</strong></p><ol class=""><li id="5914" class="jf jg hi ih b ii ij im in iq jh iu ji iy jj jc jk jl jm jn bi translated"><a class="ae je" href="https://www.kaggle.com/featureblind/robust-lasso-patches-with-rfe-gs" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/feature blind/robust-lasso-patches-with-rfe-GS</a></li><li id="a997" class="jf jg hi ih b ii kw im kx iq ky iu kz iy la jc jk jl jm jn bi translated"><a class="ae je" href="https://www.researchgate.net/post/How_to_Avoid_Overfitting" rel="noopener ugc nofollow" target="_blank">https://www.researchgate.net/post/How_to_Avoid_Overfitting</a></li><li id="064b" class="jf jg hi ih b ii kw im kx iq ky iu kz iy la jc jk jl jm jn bi translated"><a class="ae je" href="https://www.kaggle.com/rafjaa/dealing-with-very-small-datasets" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/RAF jaa/处理非常小的数据集</a></li><li id="238c" class="jf jg hi ih b ii kw im kx iq ky iu kz iy la jc jk jl jm jn bi translated"><a class="ae je" href="https://www.kaggle.com/iloveyyp/logistic-regression-with-rfecv" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/ILO vey yp/logistic-regression-with-RF ecv</a></li></ol></div></div>    
</body>
</html>