<html>
<head>
<title>Naive Bayes and Logistic Regression for NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理的朴素贝叶斯和逻辑回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/traditional-ml-nlp-definitions-tried-tested-573026693415?source=collection_archive---------5-----------------------#2019-10-02">https://medium.com/analytics-vidhya/traditional-ml-nlp-definitions-tried-tested-573026693415?source=collection_archive---------5-----------------------#2019-10-02</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="fdea" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">我将在IMDB数据集上使用朴素贝叶斯和逻辑回归来预测评论的类型，即它是积极的还是消极的。此外，我将包括一些其他真正重要的术语。我建议读者也去探索fastai。让我们开始吧。</h2></div><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es iy"><img src="../Images/d71bc652da4abb290099625de2e004b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kAzIpsWR11XcNfpuX23ypA.jpeg"/></div></div></figure><h2 id="98a3" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">❓什么是正规化</h2><p id="2f0c" class="pw-post-body-paragraph ki kj hi kk b kl km ij kn ko kp im kq jv kr ks kt jz ku kv kw kd kx ky kz la hb bi translated">Fastai从未争论过训练模型需要考虑的参数数量。Fastai始终支持您想要训练模型的任意数量的参数。但是值得记住的是，如果你在训练你的模型时遇到了困难，增加一点点规则可能会帮助它训练得更快(更好！我就不索赔了)。根据正则化，最好惩罚那些非零的参数，因为零值参数不影响损失，因为它们是零。现在，主要有两种方法来惩罚非零参数:</p><ol class=""><li id="0fc1" class="lb lc hi kk b kl ld ko le jv lf jz lg kd lh la li lj lk ll bi translated">我们可以修改损失函数，加入这个平方损失。这就是所谓的L2正规化。这将增加损失函数值，这将导致更多的<strong class="kk hj">反向传播</strong>，因此非零非必需权重将趋向于零。你将不得不在做调整的时候注意过度配合的问题。</li></ol><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es lm"><img src="../Images/ba2f6f934051ab759fbcbd6af90772fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:740/format:webp/1*MWjWhfsNRhys-6JtsbLpIg.png"/></div></figure><ol class=""><li id="2d75" class="lb lc hi kk b kl ld ko le jv lf jz lg kd lh la li lj lk ll bi translated">我们可以修改我们所说的权重等于权重减去梯度乘以学习率(当我们根据需要反向传播并优化我们的梯度以更好地适应时),也可以添加<em class="ln"> 2aw </em>。这被称为L1正则化或重量衰减。这不会直接影响损失函数，但是这将从权重中减去，因此非零非必需参数将趋向于零。</li></ol><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es lo"><img src="../Images/d14204b533288b835df3556f9b7ff4ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*hacJqDAegezkA1o1wOibHg.png"/></div></figure><p id="6544" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">让我们做PyTorch称之为重量衰减的版本，但它是L2正则化。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ls"><img src="../Images/1e8d1a440da21b8e676414af34276fdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*L8e_6zPgY3xJMxnnUXaO-w.png"/></div></figure><p id="6332" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">我们这样做的原因是我们想要惩罚非零的东西，也就是说不要让这个参数成为一个很高的数字，除非它对损失有很大的帮助。如果可以的话，将其设置为零，因为将尽可能多的参数设置为零意味着它将更好地泛化。这就像拥有一个更小的网络。这就是我们这么做的原因。但是它也可以改变学习的方式。</p><p id="d06a" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated"><em class="ln">如果培训损失高于验证损失，那么你就不适合。所以没有必要正规化。这意味着你的模型中需要更多的参数。万一你过度适应，这并不一定意味着规律化会有帮助，但肯定值得一试。</em></p><p id="847e" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">您可以在拟合模型时使用<a class="ae lt" href="https://docs.fast.ai/basic_train.html#fit_one_cycle" rel="noopener ugc nofollow" target="_blank"> fit_one_cycle </a>()初始化权重衰减值。</p><p id="3a75" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">总之，我们可以建立非常精确的模型，即使它们有很多很多的参数。不一定要有更少的参数来学习适合和学习得更好。这是对正规化的基本理解。我将在我的深入学习博客中涉及更多。</p><p id="ceb3" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated"><strong class="kk hj"> ❓What退学</strong></p><p id="5e09" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated"><strong class="kk hj"> ❓What被批量归一化</strong></p><h2 id="2fb1" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">❓什么是自然语言处理</h2><p id="f52e" class="pw-post-body-paragraph ki kj hi kk b kl km ij kn ko kp im kq jv kr ks kt jz ku kv kw kd kx ky kz la hb bi translated">NLP代表自然语言处理。NLP是指我们使用自然语言文本并对其进行处理以推断结果的任何建模。</p><p id="2758" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">为了更好地理解NLP，我将使用IMDB数据集。它是电影评论的数据集。我将使用朴素贝叶斯，然后逻辑回归。您可以按如下方式下载数据集:</p><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="b71d" class="jk jl hi lv b fi lz ma l mb mc">!wget <a class="ae lt" href="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz" rel="noopener ugc nofollow" target="_blank">http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</a></span><span id="fac0" class="jk jl hi lv b fi md ma l mb mc">!gunzip aclImdb_v1.tar.gz</span><span id="0d5d" class="jk jl hi lv b fi md ma l mb mc">!tar -xvf aclImdb_v1.tar</span></pre><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es me"><img src="../Images/7bf74792a086d1b07a19b583a60e35dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/format:webp/1*pgJlTbkWNz2ohXwXf5lUbA.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">内部数据</figcaption></figure><p id="b3f7" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">在<code class="du mj mk ml lv b">neg</code>和<code class="du mj mk ml lv b">pos</code>文件夹中有文本文件。我使用google colab进行数据实现。</p><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="3309" class="jk jl hi lv b fi lz ma l mb mc">PATH='/content/aclImdb/'<br/>%ls {PATH}train/pos | head</span></pre><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mm"><img src="../Images/3109d73fcae44ce606f083f68056530b.png" data-original-src="https://miro.medium.com/v2/resize:fit:304/format:webp/1*ozV05P3rYnp2RA2uzRAedw.png"/></div></figure><p id="eb8e" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">让我们看看其中一个文本文件的内部。我将使用numpy来了解文本文件。</p><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="23a7" class="jk jl hi lv b fi lz ma l mb mc">%cd  /content/aclImdb/train/pos<br/>np.loadtxt('10000_8.txt', dtype=np.str)</span></pre><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mn"><img src="../Images/55f988e70c96f1df8190e0b5cbb6ed17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*emfdM3Kk_bRtQTcOhharXA.png"/></div></div></figure><p id="3baa" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">我们的下一步是从文件夹中取出这些评论及其标签，并创建一个数组，这样我们就可以轻松地访问数据。Fastai有一个方便的方法。</p><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="d765" class="jk jl hi lv b fi lz ma l mb mc">trn,trn_y = texts_labels_from_folders(f'{PATH}train',names)<br/>val,val_y = texts_labels_from_folders(f'{PATH}test',names)</span></pre><p id="f0b3" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">现在，你可以检查你的数据如下。</p><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="c7a7" class="jk jl hi lv b fi lz ma l mb mc">trn[20000], trn_y[20000]</span><span id="0120" class="jk jl hi lv b fi md ma l mb mc"><strong class="lv hj">trn- will give you the review<br/>trn_y- will return you the review label(0-negative review, 1- positive review)</strong></span></pre><p id="f313" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">这是我们的数据。因此，我们的工作将是采取电影评论，并预测标签。在分析数据时，我们不考虑单词的顺序。我们只会考虑一个词在评论中的出现。通常，单词的顺序很重要。如果你在某事前有一个“不是”，那么这个“不是”指的是那件事。但在这种情况下，我们试图预测某事是积极的还是消极的。如果你看到“愚蠢”或“晦涩”这个词出现了很多次，那么这可能是一个不好的信号。如果你知道“棒极了”这个词看起来很棒，那么这可能是好的迹象。</p><p id="0ba0" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated"><em class="ln">因此，我们的想法是将它变成一个术语-文档矩阵，对于每个文档(即每个评论)，我们只需创建一个单词列表，而不是它们的顺序。</em></p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es mo"><img src="../Images/ce93a9df84d9d859b144dba5311c3263.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IUsX5NDQfDlUB2jVQE2qTg.png"/></div></div></figure><ul class=""><li id="6c78" class="lb lc hi kk b kl ld ko le jv lf jz lg kd lh la mp lj lk ll bi translated">影评中所有的词汇都被记录下来，像这样列出来，电影，好的，坏的。词汇表是所有出现的独特单词的列表。在这种情况下，我的单词没有一个出现两次。所以这被称为术语文档矩阵。</li><li id="1544" class="lb lc hi kk b kl mq ko mr jv ms jz mt kd mu la mp lj lk ll bi translated">如果该特性出现在评论中，我们添加1，如果该特性不存在，我们添加0。</li><li id="b99c" class="lb lc hi kk b kl mq ko mr jv ms jz mt kd mu la mp lj lk ll bi translated">我们增加了一层1，以避免在评论中没有出现特定单词的情况下出现无限值。如果我没有在我的任何评论中写过“性”这样的词，那并不意味着这个词也不存在。所以，我们也需要注意这一点。当我们用天真的谎言作为预测评论的模型时，这尤其有用。我们将在某个时候讨论这个问题。</li><li id="5606" class="lb lc hi kk b kl mq ko mr jv ms jz mt kd mu la mp lj lk ll bi translated">它只是一个单词包(即里面有哪些单词)。它包含“坏”、“是”、“电影”、“这个”。所以我们要做的第一件事是，我们要把它变成一个单词包。这对于线性模型来说很方便的原因是，这是一个有用的矩形矩阵，我们可以在其上进行数学计算。具体来说，我们可以做<strong class="kk hj"> <em class="ln">的逻辑回归。</em> </strong></li><li id="4eaa" class="lb lc hi kk b kl mq ko mr jv ms jz mt kd mu la mp lj lk ll bi translated">Sklearn有一个可以为我们自动创建word文档矩阵的东西——它被称为<a class="ae lt" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">计数矢量器</a>。在此之前，让我们先了解一下传统的NLP术语。</li></ul><h2 id="86a9" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">❓什么是标记化</h2><p id="c991" class="pw-post-body-paragraph ki kj hi kk b kl km ij kn ko kp im kq jv kr ks kt jz ku kv kw kd kx ky kz la hb bi translated">现在在NLP中，你必须把你的文本变成一个单词列表，这叫做标记化。你对评论的修饰越好，结果就越好。</p><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="4cbd" class="jk jl hi lv b fi lz ma l mb mc">veczr = CountVectorizer(tokenizer=tokenize)<br/>trn_term_doc = veczr.fit_transform(trn)<br/>val_term_doc = veczr.transform(val)</span></pre><ul class=""><li id="b4a3" class="lb lc hi kk b kl ld ko le jv lf jz lg kd lh la mp lj lk ll bi translated">CountVectorizer将一组文本文档转换成一个令牌计数矩阵(是<code class="du mj mk ml lv b">sklearn.feature_extraction.text</code>的一部分)。</li><li id="b3a6" class="lb lc hi kk b kl mq ko mr jv ms jz mt kd mu la mp lj lk ll bi translated">fit_transform(trn)在训练集中查找词汇。它还将训练集转换为术语-文档矩阵。因为我们必须将<em class="ln"> same transformation </em>应用到您的验证集，所以第三行只使用方法transform(val)。<code class="du mj mk ml lv b">trn_term_doc</code>和是稀疏矩阵。<code class="du mj mk ml lv b">trn_term_doc[i]</code>代表训练文档，它包含词汇表中每个单词的每个文档的字数。</li></ul><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="a3bf" class="jk jl hi lv b fi lz ma l mb mc">vocab = veczr.get_feature_names(); vocab[5000:5500]</span></pre><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es mv"><img src="../Images/73979b919f029d69c62ff89ac896067f.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*kwuC2iZZQ0CfH6NXCJwdfQ.png"/></div></figure><ul class=""><li id="13d8" class="lb lc hi kk b kl ld ko le jv lf jz lg kd lh la mp lj lk ll bi translated">get_feature_names()为我们提供了词汇表。<code class="du mj mk ml lv b">veczr.get_feature_names</code>将单词的整数索引映射到单词。</li><li id="1a9f" class="lb lc hi kk b kl mq ko mr jv ms jz mt kd mu la mp lj lk ll bi translated">单词作为一个整数存储在词汇表中。您可以检查词汇表中任何单词的整数，如下所示:</li></ul><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="4030" class="jk jl hi lv b fi lz ma l mb mc">veczr.vocabulary_['bad']</span></pre><ul class=""><li id="000b" class="lb lc hi kk b kl ld ko le jv lf jz lg kd lh la mp lj lk ll bi translated">当我们创建这个术语-文档矩阵时，训练集有25，000行，因为有25，000条电影评论，并且有75，132列是唯一单词的数量。</li></ul><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="1c63" class="jk jl hi lv b fi lz ma l mb mc">trn_term_doc<br/>&lt;25000x75132 sparse matrix of type '&lt;class 'numpy.int64'&gt;'<br/>     with 3749745 stored elements in Compressed Sparse Row format&gt;</span></pre><ul class=""><li id="2b1d" class="lb lc hi kk b kl ld ko le jv lf jz lg kd lh la mp lj lk ll bi translated"><em class="ln">现在大部分文档都没有这75132个单词中的大部分。所以我们不想把它作为普通数组存储在内存中。因为那会非常浪费。因此，我们将它存储为一个稀疏矩阵。稀疏矩阵的作用是，把它保存为一个表示非零值位置的东西。所以它说好的，文档号11，单词号41出现，它有9个。文档编号12，术语编号23出现三次，依此类推。</em></li></ul><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="a048" class="jk jl hi lv b fi lz ma l mb mc">(11, 41) → 9<br/>(12, 23) → 3</span></pre><ul class=""><li id="6a83" class="lb lc hi kk b kl ld ko le jv lf jz lg kd lh la mp lj lk ll bi translated">现在，CountVectorizer中又多了一个属性，那就是ngram_range。默认情况下，我们得到的是单个单词的单字。但是如果我们说<code class="du mj mk ml lv b">ngram_range=(1,3)</code>，那也会给我们二元模型和三元模型。现在，我们可以组合使用像<code class="du mj mk ml lv b">by hook and crook</code>、<code class="du mj mk ml lv b">fastai library</code>等更多的单词，而不是仅仅使用单个单词。</li><li id="07bc" class="lb lc hi kk b kl mq ko mr jv ms jz mt kd mu la mp lj lk ll bi translated">现在做的是同样的事情，但是在标记化之后，它不仅仅是抓住每个单词，说那是你词汇的一部分，而是每两个单词相邻，每三个单词相邻。这对于利用单词袋方法非常有帮助，因为我们现在可以看到<code class="du mj mk ml lv b">not good</code>和<code class="du mj mk ml lv b">not bad</code>和<code class="du mj mk ml lv b">not terrible</code>之间的区别。</li></ul><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="6aef" class="jk jl hi lv b fi lz ma l mb mc">veczr =  CountVectorizer(ngram_range=(1,3), tokenizer=tokenize,<br/>                         max_features=800000)</span></pre><ul class=""><li id="9054" class="lb lc hi kk b kl ld ko le jv lf jz lg kd lh la mp lj lk ll bi translated"><code class="du mj mk ml lv b">max_features</code>用于限制特征计数。</li></ul><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="5d17" class="jk jl hi lv b fi lz ma l mb mc">trn_term_doc.shape</span><span id="7d3f" class="jk jl hi lv b fi md ma l mb mc">= 25000, 800000</span></pre><h1 id="eaa3" class="mw jl hi bd jm mx my mz jq na nb nc ju io nd ip jy ir ne is kc iu nf iv kg ng bi translated">使用朴素贝叶斯</h1><p id="1af0" class="pw-post-body-paragraph ki kj hi kk b kl km ij kn ko kp im kq jv kr ks kt jz ku kv kw kd kx ky kz la hb bi translated">所以我们想要的是给定这个特定文档(我有这个特定的IMDb评论)的概率，它的类等于正或负的概率是多少。因此，对于这个特定的电影评论，它的类别是积极的可能性有多大。所以我们可以说，嗯，这等于我们得到这个特定的电影评论的概率，假设它的类是正的，乘以任何电影评论的类是正的概率，除以得到这个特定电影评论的概率。</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nh"><img src="../Images/215c193d3746d0eeefabaa62a172289b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*--tFomZr8CWx06GpwAwedg.png"/></div></div></figure><p id="1540" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">那只是贝氏法则。</p><p id="c4e0" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">所以我们可以计算所有这些东西，但实际上我们想知道的是，这更有可能是0类还是1类。如果我们把1级概率除以0级概率会怎么样。如果我们做到了呢？</p><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es ni"><img src="../Images/043c65f20b066fab3f40b2302052bc70.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/format:webp/1*HNdLVIWPtfjZTUfTbnffLQ.png"/></div></figure><p id="90e8" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">好的，如果这个数比1更重要，那么它更有可能是1类，如果它比1小，它更有可能是0类。</p><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="2af5" class="jk jl hi lv b fi lz ma l mb mc">If we have to find probability of occurance of <strong class="lv hj">this </strong>feature given the review is positive p(f/1)<strong class="lv hj">, </strong>then it is calculated as below:</span><span id="85a6" class="jk jl hi lv b fi md ma l mb mc">Given <strong class="lv hj">this</strong> feature, what is the probability of finding it in positive review p(1/f) (out of three positive review) = 2/3</span><span id="cc7e" class="jk jl hi lv b fi md ma l mb mc">probability of selecting the positive review class = 1/2</span><span id="e268" class="jk jl hi lv b fi md ma l mb mc">probability of selecting the feature = 2/4</span><span id="56ca" class="jk jl hi lv b fi md ma l mb mc">Probability of occurance of "this" in the positive reviews = ((2/3) * (1/2)) / (2/4) = 0.6667</span></pre><figure class="iz ja jb jc fd jd er es paragraph-image"><div role="button" tabindex="0" class="je jf di jg bf jh"><div class="er es nj"><img src="../Images/693aade95d5e412ce4c47e7a686da565.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4pqQmNg_eJUhaFMs32paKQ.png"/></div></div></figure><p id="16f0" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">大体上，要发现评论是正面的还是负面的，请遵循以下步骤:</p><ul class=""><li id="691d" class="lb lc hi kk b kl ld ko le jv lf jz lg kd lh la mp lj lk ll bi translated">将评论中特征的个体概率相乘，考虑一次正面情况，然后考虑一次负面情况。</li><li id="72c5" class="lb lc hi kk b kl mq ko mr jv ms jz mt kd mu la mp lj lk ll bi translated">将这两个概率相除，得到评论是正面的还是负面的。</li></ul><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="f71f" class="jk jl hi lv b fi lz ma l mb mc">If I want to know about the review "movie is bad", then it can be calculates as below:</span><span id="86e1" class="jk jl hi lv b fi md ma l mb mc">- Multiply the probabilites of features considering the review to be positive = 1*1*0.333<br/>- Multiply the probabilites of features considering the review to be negative = 1*1*1</span><span id="d666" class="jk jl hi lv b fi md ma l mb mc">probability of movie review to be positive / probability of movie review to be negative = 0.333 &lt; 1</span><span id="ad6d" class="jk jl hi lv b fi md ma l mb mc">Thus, the review is negative.</span></pre><p id="aa90" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">这是我们对评论的推断。</p><p id="9807" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">让我们用python代码来做这件事。</p><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="1820" class="jk jl hi lv b fi lz ma l mb mc">def pr(y_i):<br/>    p = x[y==y_i].sum(0)<br/>    return (p+1) / ((y==y_i).sum()+1)</span><span id="7810" class="jk jl hi lv b fi md ma l mb mc">x=trn_term_doc<br/>y=trn_y</span><span id="6ed6" class="jk jl hi lv b fi md ma l mb mc">r = np.log(pr(1)/pr(0))<br/>b = np.log((y==1).mean() / (y==0).mean())</span><span id="6162" class="jk jl hi lv b fi md ma l mb mc">pre_preds = val_term_doc @ r.T + b<br/>preds = pre_preds.T&gt;0<br/>(preds==val_y).mean()</span></pre><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nk"><img src="../Images/44da6d9b08acafefadc1654d73f70a73.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*cDz_3Rpq2r0pt8AUf64yYw.png"/></div></figure><ul class=""><li id="ddf2" class="lb lc hi kk b kl ld ko le jv lf jz lg kd lh la mp lj lk ll bi translated">所以这里写成Python。我们的自变量是我们的术语文档矩阵，我们的因变量只是<code class="du mj mk ml lv b">y</code>的标签。所以使用numpy，这个<code class="du mj mk ml lv b">x[y==1]</code>将获取因变量为1的行。然后我们可以对这些行求和，得到该特性在所有文档中的总字数加1。</li><li id="7354" class="lb lc hi kk b kl mq ko mr jv ms jz mt kd mu la mp lj lk ll bi translated">当然，取对数会更有帮助，因为如果我们取了对数，我们就可以把它们相加，而不是相乘。一旦你把足够多的这些东西相乘，它会变得非常接近零，你可能会用完浮点。所以我们取比率的对数。然后，就像我说的，我们乘以它，或者用log，我们把它加到整个类概率的比率上。然后要在日志上添加类比率，可以使用<code class="du mj mk ml lv b">+ b.</code></li><li id="d6d0" class="lb lc hi kk b kl mq ko mr jv ms jz mt kd mu la mp lj lk ll bi translated"><em class="ln">所以我们最终得到了看起来很像逻辑回归的东西。但是我们什么也没学到。从SGD的角度来看不是这样。我们只是用这个理论模型来计算。这大约有81%的准确率。所以朴素贝叶斯什么也不是。它给了我们一些东西。</em></li></ul><p id="da6f" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">现在，特别是对于这个问题，重要的是有没有单词出现。所以，我们可以用1代替单词计数，如果它出现了，就用-1代替，如果它没有出现。另一方面，我们可以将数据二进制化。</p><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="64ab" class="jk jl hi lv b fi lz ma l mb mc">pre_preds = val_term_doc.sign() @ r.T + b<br/>preds = pre_preds.T&gt;0<br/>(preds==val_y).mean()</span><span id="dd52" class="jk jl hi lv b fi md ma l mb mc">= 0.82623999999999997</span></pre><p id="98b7" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">现在，我们将对同一问题使用逻辑回归，并将比较结果。</p><h1 id="35f3" class="mw jl hi bd jm mx my mz jq na nb nc ju io nd ip jy ir ne is kc iu nf iv kg ng bi translated">使用逻辑回归</h1><p id="2f96" class="pw-post-body-paragraph ki kj hi kk b kl km ij kn ko kp im kq jv kr ks kt jz ku kv kw kd kx ky kz la hb bi translated">朴素贝叶斯从理论的角度来看是杰出的但是在实践中，与其定义权重，我们为什么不学习它们呢？所以让我们创建一个逻辑回归，让我们拟合一些系数。这将给我们一个与之前完全相同的函数形式，但是现在我们不使用理论上的T1和b，而是基于逻辑回归来计算这两个东西。我们是SkLearn中定义的LogisticRegression类。</p><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="f428" class="jk jl hi lv b fi lz ma l mb mc">m = LogisticRegression(C=1e8, dual=<strong class="lv hj">True</strong>)<br/>m.fit(x, y)<br/>preds = m.predict(val_term_doc)<br/>(preds==val_y).mean()</span></pre><ul class=""><li id="8d68" class="lb lc hi kk b kl ld ko le jv lf jz lg kd lh la mp lj lk ll bi translated">dual=true当您的数据比long宽时，类似于一个变通办法。</li><li id="599d" class="lb lc hi kk b kl mq ko mr jv ms jz mt kd mu la mp lj lk ll bi translated"><code class="du mj mk ml lv b">C</code>用于正规化。我们应该考虑正规化，因为我们有75，000字的词汇，用于近25，000条评论。C=1e8因为数字很低所以关；对损失函数几乎没有任何影响。<code class="du mj mk ml lv b"><strong class="kk hj"><em class="ln">C</em></strong></code> <strong class="kk hj"> <em class="ln">是规则化罚款金额的倒数。</em> </strong>我们使用L2正则化，因为我们只能在L2正则化中使用dual=true。此外，L2是违约国。</li></ul><p id="73ee" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">我们也可以对二进制版本运行上述解决方案。</p><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="ddec" class="jk jl hi lv b fi lz ma l mb mc">m = LogisticRegression(C=1e8, dual=<strong class="lv hj">True</strong>)<br/>m.fit(trn_term_doc.sign(), y)<br/>preds = m.predict(val_term_doc.sign())<br/>(preds==val_y).mean()</span><span id="cdb4" class="jk jl hi lv b fi md ma l mb mc">= 0.85487999999999997</span></pre><p id="fa14" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi translated">现在，打开正则化，让我们声明C=0.1，看看结果。</p><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="b373" class="jk jl hi lv b fi lz ma l mb mc">m = LogisticRegression(C=0.1, dual=<strong class="lv hj">True</strong>)<br/>m.fit(x, y)<br/>preds = m.predict(val_term_doc)<br/>(preds==val_y).mean()</span><span id="f38e" class="jk jl hi lv b fi md ma l mb mc">= 0.88404000000000005</span></pre><h1 id="ad30" class="mw jl hi bd jm mx my mz jq na nb nc ju io nd ip jy ir ne is kc iu nf iv kg ng bi translated">❓·法斯泰。NBSVM++</h1><pre class="iz ja jb jc fd lu lv lw lx aw ly bi"><span id="e4ca" class="jk jl hi lv b fi lz ma l mb mc">sl=2000<br/>md = TextClassifierData.from_bow(trn_term_doc, trn_y, val_term_doc, val_y, sl)</span><span id="ca8f" class="jk jl hi lv b fi md ma l mb mc">learner = md.dotprod_nb_learner()<br/>learner.fit(0.02, 1, wds=1e-6, cycle_len=3)</span></pre><figure class="iz ja jb jc fd jd er es paragraph-image"><div class="er es nl"><img src="../Images/ed5e5e4799b149f688a7cb225d91cad7.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*WZz-5LOZLsFMBkCJUjStwQ.png"/></div></figure><ul class=""><li id="fe57" class="lb lc hi kk b kl ld ko le jv lf jz lg kd lh la mp lj lk ll bi translated">sl=2000表示使用2000个唯一的单词。</li><li id="ad9f" class="lb lc hi kk b kl mq ko mr jv ms jz mt kd mu la mp lj lk ll bi translated">TextClassifierData是我们如何从一堆单词中得到一个模型。这是一件很快的事。</li><li id="a120" class="lb lc hi kk b kl mq ko mr jv ms jz mt kd mu la mp lj lk ll bi translated">然后我们使用朴素贝叶斯学习器来学习这些东西。</li><li id="f115" class="lb lc hi kk b kl mq ko mr jv ms jz mt kd mu la mp lj lk ll bi translated">我们的准确率达到了92%。</li></ul><p id="9b1a" class="pw-post-body-paragraph ki kj hi kk b kl ld ij kn ko le im kq jv lp ks kt jz lq kv kw kd lr ky kz la hb bi nm translated">这就是我们对数据集使用传统和现代化技术的方式。</p></div></div>    
</body>
</html>