<html>
<head>
<title>Artificial Intelligence in Answer Sentence Selection.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能在答案选句中的应用。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/artificial-intelligence-in-answer-sentence-selection-bac604bde48d?source=collection_archive---------13-----------------------#2020-06-06">https://medium.com/analytics-vidhya/artificial-intelligence-in-answer-sentence-selection-bac604bde48d?source=collection_archive---------13-----------------------#2020-06-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/32a07b4a6147e93eb5ba7904f13b5b8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:564/format:webp/1*VPBklDSObosCRBT1FFdD_g.jpeg"/></div></figure><h1 id="e44a" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">商业问题。</h1><p id="eaee" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">利用人工智能解决问答问题。给定一个特定的问题，我们从一组答案中找到一个合适的、更合适的解决方案。该系统可用于解决理解问题，其中用户需要阅读整个段落并找到具有低等待时间要求的问题的答案。</p><h1 id="d9e7" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">数据来源。</h1><p id="34bb" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">本案例研究中使用的数据来自Wiki-QA数据集，这是一个新的公开可用的问题和句子对集，为开放领域问答研究而收集和注释。大多数以前的答案句子选择工作集中在使用TREC问答数据创建的数据集上，该数据集包括编辑生成的问题和通过匹配问题中的内容词选择的候选答案句子。WikiQA是使用更自然的过程构建的，并且比之前的数据集大一个数量级以上。</p><h1 id="9da2" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">解决这个问题的现有方法。</h1><p id="7250" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">现在，使用搜索引擎类型的架构来解决这类问题，这种架构需要高延迟。这些算法的表现也不是非常好。Okapi BM25就是这样一个例子。<strong class="jm hj"> BM25 </strong>是一个词袋检索功能，它根据每个文档中出现的查询词对一组文档进行排序，而不考虑它们在文档中的接近度。这是一系列评分函数，其组件和参数略有不同。</p><h1 id="c79e" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">对现有方法的改进。</h1><p id="1675" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">我们将使用基于深度学习的模型来改善搜索结果，并相应地对文档进行排名。有趣的是，与传统的BM25方法相比，该领域(MRR)使用的指标显示出更好的得分。我们将LSTMs(长短期记忆)单元，以捕捉单词之间的依赖关系，并计算问题和答案之间的相似性。这是使用深度学习的答案句子选择模型的核心思想。</p><h1 id="3a26" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">探索性数据分析。</h1><p id="a8e2" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">WikiQA数据集中基本上有3个字段，即问题、答案和标签。</p><p id="bdc8" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">看着标签字段的分布。</strong></p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es kn"><img src="../Images/cc5c5c734d29bfab3412716f46799368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JpqDoPrwOoW3Xm3H6ZU3yQ.png"/></div></div></figure><p id="7c8c" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">这是一个高度不平衡的数据集。大概是25:1倾向于消极阶层。</p><p id="cc6c" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">数据集中总共有29208条记录。数据集没有缺失值。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es kw"><img src="../Images/b3f4e250dc97e64c0d70c1f6958aa392.png" data-original-src="https://miro.medium.com/v2/resize:fit:536/format:webp/1*LgajmuIfZZcBuFJDHfhFtA.png"/></div></figure><p id="de32" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">为了嵌入问题和答案字段，我们需要填充文本序列。因此，答案和问题长度分布在决定数量时起着重要作用。</p><figure class="ko kp kq kr fd ij er es paragraph-image"><div class="er es kx"><img src="../Images/52dbba1d159f6bfd69d849f7dbaba2df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*tyZZR3YnyzjZYYM7BD7OYw.png"/></div><figcaption class="ky kz et er es la lb bd b be z dx translated">回答字段。</figcaption></figure><p id="6b7d" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">固定400个字符作为答案字段的填充。</strong></p><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es lc"><img src="../Images/cd6fc9f8306b0d042f2ce510c1b18918.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*08MyiBbT1b8Tdj0t6c7VNg.png"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">问题字段。</figcaption></figure><p id="b25f" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">固定80个字符作为问题字段的填充。</strong></p><h1 id="32af" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">第一次切割方法。</h1><ol class=""><li id="e740" class="ld le hi jm b jn jo jr js jv lf jz lg kd lh kh li lj lk ll bi translated">这个问题是作为一个二元类分类提出的</li><li id="8fd5" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated">如果答案更合适，系统返回1，如果答案不太合适，系统返回0。</li><li id="413f" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated">该系统的思想是获得问题和答案，并输出一个相关性分数。</li><li id="feed" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated">如果相关性分数大于阈值，则返回1，否则返回0。阈值的值完全取决于模型的架构和用于训练模型的数据。</li><li id="a3b8" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated">首先对问题集和答案集进行预处理和清洗。</li><li id="6fdf" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated">组块也被用于构建问题和答案。</li><li id="57fc" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated">为问题和答案中出现的每个单词建立300维单词向量。(标记化)</li><li id="2395" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated">。然后将其嵌入合适的形状，以输入到双向堆叠LSTM中。</li><li id="2272" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated">该系统使用多层或堆叠的双向LSTMs来获取问题和答案的上下文。</li><li id="fafb" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated">简单来说，它使用RNNs来获取问题和答案的含义。</li><li id="341f" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated">我们使用双向RNNs从双方获取问答中出现的特定单词的上下文</li><li id="fcef" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated">LSTM的输出受制于dense和softmax图层以对答案进行分类。</li><li id="16ed" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated">如果可能，也可以应用其他算法，并且可以将输出堆叠在一起。</li></ol><h1 id="4e50" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">要使用的度量。</h1><p id="ce56" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">在这个业务问题中，因为我们要对答案进行排名，所以我们将使用MRR指标。</p><p id="0bfa" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated"><strong class="jm hj">平均倒数排名</strong>是一种<a class="ae lr" href="https://en.wikipedia.org/wiki/Statistic" rel="noopener ugc nofollow" target="_blank">统计</a>方法，用于评估任何产生对查询样本的可能响应列表的过程，该列表按正确概率排序。查询响应的倒数排名是第一个正确答案的排名的<a class="ae lr" href="https://en.wikipedia.org/wiki/Multiplicative_inverse" rel="noopener ugc nofollow" target="_blank">乘法倒数</a>:1代表第一名，12代表第二名，13代表第三名，依此类推。</p><p id="4fe3" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">平均倒数等级的倒数值对应于等级的<a class="ae lr" href="https://en.wikipedia.org/wiki/Harmonic_mean" rel="noopener ugc nofollow" target="_blank">调和平均值</a>。</p><h1 id="2925" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">特征工程。</h1><p id="24ad" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">在这个问题中，有两个经过预处理的文本列。一个是问题，一个是答案。因此创建了一个额外的特性，它计算文本列之间的常用单词的数量。常用词的数量越多，答案正确的几率就越高。另一个特征是答案字段的长度。这个功能也非常有用，因为答案的长度在为问题找到合适的答案时也起着重要的作用。</p><h1 id="ef31" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">模型的架构。</h1><figure class="ko kp kq kr fd ij er es paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="er es ls"><img src="../Images/923707acd83d5de34a7a9581e4c20781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3dGY9PZtvvY0XG5P0ITsug.png"/></div></div><figcaption class="ky kz et er es la lb bd b be z dx translated">模型的架构。</figcaption></figure><p id="a8ec" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">问题和答案字段受制于堆叠的双向LSTMs，以便捕捉各个字段的上下文。该体系结构中还有两个额外的字段。一个是问题和答案之间的常用词数量。另一个是回答句子的长度。它们受到密集层的影响，然后与现有层合并。包括最后6层是为了防止过度适合训练模型。</p><h1 id="66f5" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">模特表演。</h1><p id="9188" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">我们得到了大约76.3%的MRR分数。</p><h1 id="e599" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">替代方案。</h1><h1 id="f164" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">文本处理的神经变分。</h1><p id="5e2d" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">这个业务问题也可以用稍微不同的模型来处理。代替使用2个双向lstm和附加特征，我们可以使用3个没有附加特征的双向lstm。前一层的批量标准化特征保持不变。这种方法的性能据说约为69.5%。这种方法基于一篇研究论文。</p><h1 id="6ff5" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">伯特方法。</h1><p id="956f" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">这个业务问题也可以使用BERT方法来解决。BERT代表来自变压器的双向编码器表示。你可以在这里找到更多关于伯特<a class="ae lr" href="https://www.blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank">的信息。</a></p><h1 id="386c" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">未来的工作。</h1><p id="402e" class="pw-post-body-paragraph jk jl hi jm b jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh hb bi translated">这个模型将包括语音到文本输入，并设计有语音到文本，以便它可以像人工智能助理一样升级。也可以用来获取一本大书的摘要，像AI助手一样回答那本书的问题。</p><h1 id="6ab5" class="im in hi bd io ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj bi translated">参考文献。</h1><ol class=""><li id="5f2f" class="ld le hi jm b jn jo jr js jv lf jz lg kd lh kh li lj lk ll bi translated"><a class="ae lr" href="https://aclweb.org/aclwiki/Question_Answering_(State_of_the_art)" rel="noopener ugc nofollow" target="_blank">https://ACL web . org/ACL wiki/Question _ Answering _(State _ of _ the _ art)</a></li><li id="a87a" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated"><a class="ae lr" href="https://arxiv.org/pdf/1511.04108.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1511.04108.pdf</a></li><li id="f2f5" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated"><a class="ae lr" href="https://arxiv.org/pdf/1707.06372.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1707.06372.pdf</a></li><li id="f6ea" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated"><a class="ae lr" href="https://www.aclweb.org/anthology/P15-2116.pdf" rel="noopener ugc nofollow" target="_blank">https://www.aclweb.org/anthology/P15-2116.pdf</a></li><li id="57c6" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated"><a class="ae lr" href="https://arxiv.org/pdf/1905.12897.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1905.12897.pdf</a></li><li id="cb2f" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated"><a class="ae lr" href="https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-p" rel="noopener ugc nofollow" target="_blank">https://machine learning mastery . com/develop-bidirectional-lstm-sequence-class ification-p</a>ython-keras/</li><li id="5988" class="ld le hi jm b jn lm jr ln jv lo jz lp kd lq kh li lj lk ll bi translated"><a class="ae lr" href="https://arxiv.org/ftp/arxiv/papers/1801/1801.02143.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/ftp/arxiv/papers/1801/1801.02143.pdf</a></li></ol><p id="d102" class="pw-post-body-paragraph jk jl hi jm b jn ki jp jq jr kj jt ju jv kk jx jy jz kl kb kc kd km kf kg kh hb bi translated">阿布舍克·斯里尼瓦桑-<a class="ae lr" href="https://www.linkedin.com/in/abhishek-srinivasan-1a6099147/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/abhishek-srinivasan-1a6099147/</a></p></div></div>    
</body>
</html>