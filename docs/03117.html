<html>
<head>
<title>Linear Regression with One Input</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">单输入线性回归</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/linear-regression-with-one-input-ee51d458552f?source=collection_archive---------19-----------------------#2020-01-16">https://medium.com/analytics-vidhya/linear-regression-with-one-input-ee51d458552f?source=collection_archive---------19-----------------------#2020-01-16</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/a9e461433413aa801b29a30a5288a4ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/0*DKla7AbFYLAi9H0z.png"/></div><figcaption class="im in et er es io ip bd b be z dx translated">来源:维基共享资源(Loi_d ' Okun.png:用户:gribecoderivework:—Jtneill—Talk【公共领域】)</figcaption></figure><p id="e5f7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">线性回归通常是许多ML爱好者开始的第一个话题。这将回顾单输入线性回归背后的基本数学，以及如何用python实现它。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="de98" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">新年伊始，我挑战自己每天学习一个新的ML相关概念。我的灵感来自我在Reddit上看到的一个帖子。它从各种模型的基本线性回归开始。现在我开始深入算法背后的数学，并开始实现我自己的算法。在第一个主题中，我将讨论单输入线性回归。</p><h1 id="9dc1" class="jv jw hi bd jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks bi translated"><strong class="ak">什么是线性回归？</strong></h1><p id="174d" class="pw-post-body-paragraph iq ir hi is b it kt iv iw ix ku iz ja jb kv jd je jf kw jh ji jj kx jl jm jn hb bi translated">简单来说，线性回归就是求解y = mx + B中的m和B，在整个中学阶段，当你求解直线方程时，你实际上是在做线性回归。那么我们从哪里开始呢？除了导入数据，第一步是实现假设函数或y = mx + B。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="77a5" class="lh jw hi ld b fi li lj l lk ll"><strong class="ld hj">def</strong> hypothesis(x, m, b):<br/>    p = list()<br/>    <strong class="ld hj">for</strong> i <strong class="ld hj">in</strong> x:<br/>        p.append(i * m + b)<br/>    <strong class="ld hj">return</strong> p</span></pre><p id="5e51" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在这种情况下，我们只需将向量x乘以m，再加上b，就可以得到y向量。</p></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="e212" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下一部分是评估我们对m和B的猜测与实际值相比有多好。一种方法是实现成本函数，即假设值和期望值之间的均方差。如果你还不明白那是什么意思，没关系，我会先看一下代码，然后再看一个例子。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="03c9" class="lh jw hi ld b fi li lj l lk ll"><strong class="ld hj">def</strong> cost_function(x, y, m, b):     <br/>  c = 0     <br/>  <strong class="ld hj">for</strong> i <strong class="ld hj">in</strong> range(len(x)):         <br/>    h = m * x[i] + b         <br/>    loss = h - y[i]         <br/>  c += loss**2     <br/>  <strong class="ld hj">return</strong> (1/len(x) * 0.5 * c)</span></pre><p id="21e2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们的成本函数的一般目的是确定我们的参数做得有多好。如果我们绘制成本函数，我们得到它是一个凸函数，这意味着有一个点，在那里有一个全局最小值。这就是为什么我们使用梯度下降。梯度下降让我们慢慢地向下移动成本函数，直到我们找到局部极值。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="5a42" class="lh jw hi ld b fi li lj l lk ll"><strong class="ld hj">def</strong> gradient_descent(x, y, m, b, alpha, iter):<br/>    z = len(y)<br/>    theta_hist = []<br/>    cost_hist = []<br/>    <strong class="ld hj">for</strong> i <strong class="ld hj">in</strong> range(iter):<br/>        h = hypothesis(x, m, b)<br/>        grad_m = 0<br/>        grad_b = 0<br/>        <strong class="ld hj">for</strong> j <strong class="ld hj">in</strong> range(z):<br/>            grad_m += (h[j] - y[j]) * x[j]<br/>            grad_b += h[j] - y[j]<br/>            <br/>        m = m - alpha * grad_m<br/>        b = b - alpha * grad_b<br/>        theta_hist.append([m,b])<br/>        cost_hist.append(cost_function(x, y, m, b))<br/>    <strong class="ld hj">return</strong> theta_hist, cost_hist</span></pre><p id="411e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的函数计算成本函数下移多少。一般形式是B = B(old) — alpha * sum(h — y)其中alpha是学习率，我们将预测值和实际值之间的差异相加。m是一样的，除了我们把α乘以当前的x值。</p><p id="9404" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">剩下的就是把所有的东西结合在一起。</p><pre class="ky kz la lb fd lc ld le lf aw lg bi"><span id="c8fa" class="lh jw hi ld b fi li lj l lk ll"><strong class="ld hj">import</strong> <strong class="ld hj">numpy</strong> <strong class="ld hj">as</strong> <strong class="ld hj">np #use numpy to generate random list</strong><br/>np.random.seed(42)<br/>x = 2 * np.random.rand(100,1)<br/>np.random.seed(41)<br/>y = 4 + 3 * x + np.random.randn(100,1)<br/>x = list(x)<br/>y = list(y)</span><span id="825c" class="lh jw hi ld b fi lm lj l lk ll">t_history, t_cost= gradient_descent(x, y, 0, 0, 0.05, 50)</span><span id="4c59" class="lh jw hi ld b fi lm lj l lk ll">theta = t_history[-1]</span></pre><p id="672f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中θ是具有最终m和b值的列表。</p><figure class="ky kz la lb fd ij"><div class="bz dy l di"><div class="ln lo l"/></div></figure></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="53b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">感谢您阅读我第一篇关于不使用任何库实现线性回归的文章。我想我会用多元线性回归做同样的事情，但是我会用numpy，因为向量和矩阵运算太难了。</p></div></div>    
</body>
</html>