<html>
<head>
<title>Complete Guide to build CNN in Pytorch and Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在Pytorch和Keras建立CNN的完整指南</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/complete-guide-to-build-cnn-in-pytorch-and-keras-abc9ed8b8160?source=collection_archive---------0-----------------------#2020-06-04">https://medium.com/analytics-vidhya/complete-guide-to-build-cnn-in-pytorch-and-keras-abc9ed8b8160?source=collection_archive---------0-----------------------#2020-06-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/></div><div class="ab cl if ig gp ih" role="separator"><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik il"/><span class="ii bw bk ij ik"/></div><div class="hb hc hd he hf"><p id="6ec0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">卷积神经网络近年来受到了广泛的关注。在处理图像时，它可以提供更好的结果。</p><p id="09f8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Pytorch和Keras是计算机视觉应用中使用的两个重要的开源机器学习库。</p><p id="6a5d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Pytorch因其被《自然》杂志定义而闻名，并成为研究人员的最爱。另一方面，Keras在原型制作方面非常受欢迎。</p><p id="8803" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将逐步建立一个卷积网络。</p><h1 id="55d6" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">卷积神经网络</h1><p id="037f" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">CNN是图像分类和识别的热门选择。</p><p id="fd52" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">CNN中的三个重要层是卷积层、汇集层和全连接层。非常常用的激活函数是ReLU。</p><p id="f9cd" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在每一层中，我们应该知道一些重要的术语:</p><h2 id="7834" class="kn jl hi bd jm ko kp kq jq kr ks kt ju ix ku kv jy jb kw kx kc jf ky kz kg la bi translated">卷积层</h2><p id="b901" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">这是接受输入以提取要素后的第一层。</p><p id="c14a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">图像矩阵是三维的(宽、高、深)。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div class="er es lb"><img src="../Images/ffd374f534b1bde2a1f053a3d27616bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*kBuGwp9V-vIRIhFK0oxWrA.png"/></div></figure><p id="a7a8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">核或滤波器矩阵用于特征提取。</p><p id="2e24" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果(w，h，d)是输入维数，而(a，b，d)是n个核的核维数，则卷积层的输出是(w-a+1，h-b+1，n)。</p><p id="1808" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">跨距是我们在输入矩阵上移动的像素数。</p><p id="3a38" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">填充是我们对图像所做的改变，以适应过滤器。它要么用零填充，要么丢弃图像的一部分。</p><h2 id="2ea9" class="kn jl hi bd jm ko kp kq jq kr ks kt ju ix ku kv jy jb kw kx kc jf ky kz kg la bi translated">汇集层</h2><p id="3994" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">池层是为了减少参数的数量。常用的三种池类型是:</p><p id="3c33" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">最大池化:从要素地图中获取最大值。</p><p id="092a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">平均池:取特征图中值的平均值。</p><p id="d073" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Sum Pooling:对特征映射中的值求和。</p><h2 id="db37" class="kn jl hi bd jm ko kp kq jq kr ks kt ju ix ku kv jy jb kw kx kc jf ky kz kg la bi translated">全连接层</h2><p id="775c" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">来自池化图层或卷积图层(不需要池化图层时)的输出被展平，以将其提供给完全连接的图层。</p><figure class="lc ld le lf fd lg er es paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="er es lj"><img src="../Images/70ecf0fa00dc740454a40c1475f47fcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*abZ4ZWQ6w6KbsU4HB0zN5A.png"/></div></div><figcaption class="lo lp et er es lq lr bd b be z dx translated">美国有线新闻网；卷积神经网络</figcaption></figure><h1 id="79c3" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">CNN的实现</h1><h2 id="abc5" class="kn jl hi bd jm ko kp kq jq kr ks kt ju ix ku kv jy jb kw kx kc jf ky kz kg la bi translated">导入库</h2><p id="cad2" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">克拉斯</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="9b3d" class="kn jl hi lt b fi lx ly l lz ma">import keras<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Dropout, Flatten<br/>from keras.layers import Conv2D, MaxPooling2D</span></pre><p id="8720" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">pytorch</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="fb50" class="kn jl hi lt b fi lx ly l lz ma">import torchvision.datasets as datasets</span><span id="5f48" class="kn jl hi lt b fi mb ly l lz ma">import torch.nn as nn<br/>import torch.nn.functional as F<br/>import torch.optim as optim</span></pre><h2 id="c31b" class="kn jl hi bd jm ko kp kq jq kr ks kt ju ix ku kv jy jb kw kx kc jf ky kz kg la bi translated">加载输入</h2><p id="2c94" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">输入既可以从torchvision和keras中可用的标准数据集加载，也可以从用户指定的目录加载。</p><p id="60d0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Keras和pytorch中标准数据集的输入:</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="26d7" class="kn jl hi lt b fi lx ly l lz ma">#keras</span><span id="9480" class="kn jl hi lt b fi mb ly l lz ma">from keras.datasets import mnist</span><span id="5cea" class="kn jl hi lt b fi mb ly l lz ma">(X_train, y_train), (X_test, y_test) = mnist.load_data()</span><span id="3ce9" class="kn jl hi lt b fi mb ly l lz ma">#pytorch</span><span id="ef05" class="kn jl hi lt b fi mb ly l lz ma">import torchvision.datasets as datasets</span><span id="0078" class="kn jl hi lt b fi mb ly l lz ma">mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)</span><span id="32cc" class="kn jl hi lt b fi mb ly l lz ma">mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)</span></pre><p id="c292" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">Keras和pytorch中用户指定目录的输入</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="ced2" class="kn jl hi lt b fi lx ly l lz ma">#keras</span><span id="3d24" class="kn jl hi lt b fi mb ly l lz ma"><strong class="lt hj">from</strong> <strong class="lt hj">keras.preprocessing.image</strong> <strong class="lt hj">import</strong> ImageDataGenerator</span><span id="b746" class="kn jl hi lt b fi mb ly l lz ma">train_datagen = ImageDataGenerator(rescale = 1./255,<br/>                                   shear_range = 0.2,<br/>                                   zoom_range = 0.2,<br/>                                   horizontal_flip = <strong class="lt hj">True</strong>)<br/><br/>test_datagen = ImageDataGenerator(rescale = 1./255)<br/><br/><br/>training_set = train_datagen.flow_from_directory("./classify/dataset/training_set",<br/>                                                 target_size = (64, 64),<br/>                                                 batch_size = 5<br/>                                                 )<br/>test_set = test_datagen.flow_from_directory("./classify/dataset/test_set",<br/>                                            target_size = (64, 64),<br/>                                            batch_size = 5<br/>                                            )</span><span id="f29a" class="kn jl hi lt b fi mb ly l lz ma">#pytorch</span><span id="4f03" class="kn jl hi lt b fi mb ly l lz ma"><strong class="lt hj">from</strong> torchvision <strong class="lt hj">import</strong> datasets, transforms <br/><strong class="lt hj">from</strong> torch.utils <strong class="lt hj">import</strong> data  </span><span id="c777" class="kn jl hi lt b fi mb ly l lz ma">dataset = datasets.ImageFolder(root='./classify/dataset/training_set/, <br/>transform = transforms.ToTensor())  </span><span id="bae0" class="kn jl hi lt b fi mb ly l lz ma">loader = data.DataLoader(dataset, batch_size = 8, shuffle = <strong class="lt hj">True</strong>)</span></pre><h2 id="5364" class="kn jl hi bd jm ko kp kq jq kr ks kt ju ix ku kv jy jb kw kx kc jf ky kz kg la bi translated">添加卷积层</h2><p id="f1ea" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">在添加卷积层之前，我们将看到keras和pytorch中最常见的网络布局。</p><p id="cfe8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在keras中，我们将从“model = Sequential()”开始，并将所有层添加到模型中。</p><p id="7cd9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在pytorch中，我们将从定义类开始，用所有层初始化它，然后添加forward函数来定义数据流。</p><p id="1c14" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">神经网络类(nn。模块):<br/> def __init__(self):</p><p id="5f15" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">向前定义(自身，x):</p><p id="d107" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在keras中添加卷积层:</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="16ae" class="kn jl hi lt b fi lx ly l lz ma">model = Sequential()</span><span id="4783" class="kn jl hi lt b fi mb ly l lz ma">model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1), activation=’relu’))</span></pre><p id="f960" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">32是过滤器的数量，并且内核大小是5*5。ReLU是激活层。</p><p id="4620" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在pytorch:</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="c36e" class="kn jl hi lt b fi lx ly l lz ma">class NeuralNet(nn.Module):<br/>    def __init__(self):<br/>        super(NeuralNet, self).__init__()<br/>        self.conv1 = nn.Conv2d(3,32,3,1)<br/>        self.conv2 = nn.Conv2d(32,64,3,1)</span></pre><p id="0aef" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在conv1中，3是输入通道数，32是滤波器数或输出通道数。3是内核大小，1是步幅。</p><h2 id="24bc" class="kn jl hi bd jm ko kp kq jq kr ks kt ju ix ku kv jy jb kw kx kc jf ky kz kg la bi translated">添加池层:</h2><p id="43ca" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">我们将添加内核大小为2*2的最大池层。</p><p id="1293" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在喀拉斯</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="74d4" class="kn jl hi lt b fi lx ly l lz ma">model.add(MaxPooling2D(pool_size=(2,2))</span></pre><p id="1fd8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在pytorch:</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="71ac" class="kn jl hi lt b fi lx ly l lz ma">x=torch.nn.functional.max_pool2d(x,2)</span></pre><h2 id="3ea4" class="kn jl hi bd jm ko kp kq jq kr ks kt ju ix ku kv jy jb kw kx kc jf ky kz kg la bi translated">添加完全连接的层</h2><p id="efe5" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">正如我们已经知道全连接层，</p><p id="9a90" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们将在喀拉斯和pytorch增加一个。</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="47e2" class="kn jl hi lt b fi lx ly l lz ma">#keras</span><span id="5309" class="kn jl hi lt b fi mb ly l lz ma">model.add(Flatten())<br/>model.add(Dense(128, activation='relu'))<br/>model.add(Dense(10, activation='softmax'))</span><span id="cf40" class="kn jl hi lt b fi mb ly l lz ma">#pytorch</span><span id="34b5" class="kn jl hi lt b fi mb ly l lz ma">class NeuralNet(nn.Module):<br/>    def __init__(self):<br/>        super(NeuralNet, self).__init__()<br/>        self.conv1 = nn.Conv2d(3,32,3,1)<br/>        self.conv2 = nn.Conv2d(32,64,3,1)<br/>        self.fc1 = nn.Linear(9216, 128)<br/>        self.fc2 = nn.Linear(128, 10)</span></pre><p id="5062" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在，我们已经完美地添加了所有层。但是我们需要定义从输入层到输出层的数据流(例如，什么层应该在什么层之后)</p><h2 id="1901" class="kn jl hi bd jm ko kp kq jq kr ks kt ju ix ku kv jy jb kw kx kc jf ky kz kg la bi translated">网络中的数据流</h2><p id="26f7" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">这部分纯粹是为pytorch准备的，因为我们需要添加到NeuralNet类中。</p><p id="f8e1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在Keras中，我们添加每一层的顺序将描述我们传递给每一层定义它的流和参数。</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="7f17" class="kn jl hi lt b fi lx ly l lz ma"> model = Sequential()<br/> model.add(Conv2D(32, (5, 5), input_shape=(28, 28, 1),    <br/>  activation='relu'))<br/> model.add(MaxPooling2D())<br/> model.add(Dropout(0.2))<br/> model.add(Flatten())<br/> model.add(Dense(128, activation='relu'))<br/> model.add(Dense(num_classes, activation='softmax'))<br/></span></pre><p id="7da4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在pytorch中，我们将添加forward函数来描述__init__中添加的层的顺序:</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="d850" class="kn jl hi lt b fi lx ly l lz ma">class NeuralNet(nn.Module):<br/>    def __init__(self):<br/>        super(NeuralNet, self).__init__()<br/>        self.conv1 = nn.Conv2d(3,32,3,1)<br/>        self.conv2 = nn.Conv2d(32,64,3,1)<br/>        self.fc1 = nn.Linear(9216, 128)<br/>        self.fc2 = nn.Linear(128, 10)<br/>    def forward(self,x):<br/>        x=self.conv1(x)<br/>        x=F.relu(x)<br/>        x=self.conv2(x)<br/>        x=F.relu(x)<br/>        x=F.max_pool2d(x,2)<br/>        x = torch.flatten(x, 1)<br/>        x = self.fc1(x)<br/>        x = F.relu(x)<br/>        x = self.fc2(x)<br/>        output = F.log_softmax(x, dim=1)<br/>        return output</span></pre><h2 id="0415" class="kn jl hi bd jm ko kp kq jq kr ks kt ju ix ku kv jy jb kw kx kc jf ky kz kg la bi translated">使模型适合输入数据</h2><p id="b649" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">在keras中，我们将使用选定的损失函数编译模型，并使模型符合数据。时期、优化器和批量大小作为参数传递。</p><p id="8100" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">历元是我们在整个数据中迭代模型的次数。</p><p id="7f94" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">批次大小是由于重量变化而输入的数据量或图像数量。批量大小用于减少记忆的复杂性。</p><p id="cbeb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">有不同类型的优化算法可用。你可以在这里阅读它们。一般来说，很多人更喜欢亚当。</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="e4f3" class="kn jl hi lt b fi lx ly l lz ma">model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])</span><span id="391a" class="kn jl hi lt b fi mb ly l lz ma">model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200)<br/></span></pre><p id="bdec" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在pytorch，</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="0559" class="kn jl hi lt b fi lx ly l lz ma">model = NeuralNet()<br/>optimizer = optim.Adam(model.parameters())</span><span id="5e75" class="kn jl hi lt b fi mb ly l lz ma">for (i,l) in trainloader:<br/>    optimizer.zero_grad()<br/>    output = model(i)<br/>    loss = F.nll_loss(output, l)<br/>    loss.backward()<br/>    optimizer.step()</span></pre><p id="4410" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">nll_loss是负对数似然损失。F.nll_loss()和F.log_softmax()的组合与分类交叉熵函数相同。</p><p id="9914" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">optimizer.zero_grad()清除以前数据的梯度。</p><p id="acee" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">loss.backward()使用optimizer.step()计算梯度并更新权重。</p><h2 id="1725" class="kn jl hi bd jm ko kp kq jq kr ks kt ju ix ku kv jy jb kw kx kc jf ky kz kg la bi translated">评估模型</h2><p id="9930" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">keras中的评估模型</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="b9ac" class="kn jl hi lt b fi lx ly l lz ma">score = model.evaluate(X_test, target_test, verbose=0)</span><span id="36bd" class="kn jl hi lt b fi mb ly l lz ma">print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')</span></pre><p id="59a6" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于keras中的自定义数据，您可以使用以下函数:</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="9c74" class="kn jl hi lt b fi lx ly l lz ma">model.fit_generator(training_set,<br/>                         epochs = 2,<br/>                         validation_data = test_set,<br/>                         verbose = 1)</span><span id="2821" class="kn jl hi lt b fi mb ly l lz ma">score = model.evaluate_generator(test_set)</span></pre><p id="d66e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在pytorch:</p><pre class="lc ld le lf fd ls lt lu lv aw lw bi"><span id="88f3" class="kn jl hi lt b fi lx ly l lz ma">model.eval()<br/>test_loss = 0<br/>correct = 0<br/>with torch.no_grad():<br/>    for data, target in testloader:<br/>        output = model(data)<br/>        test_loss += F.nll_loss(output, target, <br/>                         reduction='sum').item() <br/>        pred = output.argmax(dim=1, keepdim=True)  <br/>        correct += pred.eq(target.view_as(pred)).sum().item()</span><span id="362a" class="kn jl hi lt b fi mb ly l lz ma">test_loss /= len(testloader.dataset)</span><span id="9bc5" class="kn jl hi lt b fi mb ly l lz ma">print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(<br/>    test_loss, correct, len(testloader.dataset),<br/>    100. * correct / len(testloader.dataset)))</span></pre><p id="0d25" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">model.eval()告诉model我们正在评估过程中。这是因为某些层的行为在训练和测试中会发生变化。</p><p id="cb3f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">torch.no_grad()将关闭梯度计算，以便节省内存。</p><h1 id="2594" class="jk jl hi bd jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh bi translated">最后的想法</h1><p id="c1ad" class="pw-post-body-paragraph im in hi io b ip ki ir is it kj iv iw ix kk iz ja jb kl jd je jf km jh ji jj hb bi translated">我觉得我使用pytorch对数据流有了更多的控制。出于同样的原因，它很快成为研究人员的最爱。</p><p id="9bec" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">然而我们会看到。GAN和自动编码器的实现将在后面的文章中介绍。</p><h2 id="bbe0" class="kn jl hi bd jm ko kp kq jq kr ks kt ju ix ku kv jy jb kw kx kc jf ky kz kg la bi translated">参考</h2><div class="md me ez fb mf mg"><a rel="noopener follow" target="_blank" href="/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148"><div class="mh ab dw"><div class="mi ab mj cl cj mk"><h2 class="bd hj fi z dy ml ea eb mm ed ef hh bi translated">理解卷积神经网络(CNN) —深度学习</h2><div class="mn l"><h3 class="bd b fi z dy ml ea eb mm ed ef dx translated">在神经网络中，卷积神经网络(ConvNets或CNN)是进行图像处理的主要类别之一</h3></div><div class="mo l"><p class="bd b fp z dy ml ea eb mm ed ef dx translated">medium.com</p></div></div><div class="mp l"><div class="mq l mr ms mt mp mu lh mg"/></div></div></a></div><div class="md me ez fb mf mg"><a href="https://pytorch.org/tutorials/" rel="noopener  ugc nofollow" target="_blank"><div class="mh ab dw"><div class="mi ab mj cl cj mk"><h2 class="bd hj fi z dy ml ea eb mm ed ef hh bi translated">欢迎来到PyTorch教程- PyTorch教程1.5.0文档</h2><div class="mn l"><h3 class="bd b fi z dy ml ea eb mm ed ef dx translated">60分钟的闪电战是最常见的起点，并提供了如何使用PyTorch的广阔视野。它涵盖了…</h3></div><div class="mo l"><p class="bd b fp z dy ml ea eb mm ed ef dx translated">pytorch.org</p></div></div><div class="mp l"><div class="mv l mr ms mt mp mu lh mg"/></div></div></a></div><p id="7f23" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">https://keras.io/examples/vision/mnist_convnet/<a class="ae mc" href="https://keras.io/examples/vision/mnist_convnet/" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>