<html>
<head>
<title>Why Convolution Neural Networks are better than feed forward networks?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么卷积神经网络优于前馈网络？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/why-convolution-neural-networks-are-better-than-feed-forward-networks-introduction-58edf65b830e?source=collection_archive---------4-----------------------#2019-10-18">https://medium.com/analytics-vidhya/why-convolution-neural-networks-are-better-than-feed-forward-networks-introduction-58edf65b830e?source=collection_archive---------4-----------------------#2019-10-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="6cf2" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">Alexnet和ZFNet简介</h2></div><p id="8cec" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">深度学习</strong>是当今世界如此有趣的话题。它有如此强大的预测能力，用它不太复杂但很有趣的算法。</p><p id="112c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">当我还在研究这个课题时，我对卷积神经网络非常着迷。在我深入研究前馈神经网络之后，我脑海中有一个问题<strong class="iz hj"> <em class="jt">如果这些前馈网络如此有趣并且它们有很强的分类能力，我们为什么要转向CNN..？</em> </strong> <em class="jt"> </em>我并没有真的去研究CNN，但我还是拿起我的笔&amp;纸，开始在网上为CNN做研究。我想知道的是<em class="jt">为什么它们如此受欢迎，它们比FFN的好在哪里？</em>于是，我开始在网上看<a class="ae ju" href="https://brohrer.github.io/how_convolutional_neural_networks_work.html" rel="noopener ugc nofollow" target="_blank">这篇</a>文章。我感兴趣的是，我想知道更多关于CNN的事情，几个星期后，我会告诉你为什么他们是如此有趣的话题。</p><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es jv"><img src="../Images/0dca20d1b7f9d9b6739858536d052efa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MHk3ksM3idY4KydvkPdoyw.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">CNN在左边&amp; FFN在右边</figcaption></figure><p id="d22d" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果我们有一个图像作为输入，每一个像素将有一个不同的权重与之关联，它将有三个值(rgb值)与之关联。因此，我们可以对它们应用前馈网络，但是如果输入大小为227*227的标准图像，则参数的数量变为227*227*3。粗略地说，10⁴数的权重将与图像相关联。因此，在网络的一个单层中将需要10⁴数量的神经元，这确实是不兼容且麻烦的工作。因此，在一个单一的前馈网络中将需要总共数百万个参数和神经元，因此它们对于处理图像是不兼容的。在CNN的中，构建了一个内核<em class="jt">(内核基本上是一个权重矩阵)</em>，当内核在图像中水平和垂直移动时，权重被共享。<a class="ae ju" href="https://www.quora.com/What-is-max-pooling-in-convolutional-neural-networks" rel="noopener ugc nofollow" target="_blank"> maxpooling </a>操作直接将参数数量减半。然后还有一个概念是<a class="ae ju" rel="noopener" href="/@ayeshmanthaperera/what-is-padding-in-cnns-71b21fb0dd7">填充</a>和<a class="ae ju" rel="noopener" href="/machine-learning-algorithms/what-is-stride-in-convolutional-neural-network-e3b4ae9baedb">步距</a>，这进一步减小了图像的参数大小。</p><p id="7739" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">CNN的3d连接看起来非常复杂，但这是一个简单的话题，特别是因为我们使用了像<strong class="iz hj"> Pytorch </strong>:)这样的框架。此外，当我们进行到下一个卷积层时，与前馈网络不同，核大小减小，参数继续减小。</p><p id="c878" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们讨论两个古老但有趣的预制CNN:</p><h1 id="3999" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">AlexNet</h1><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es ld"><img src="../Images/a2f7a53630b3f219e1288883110a9ebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DM0RPBrYg2gpjAW4GM9JeQ.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">AlexNet结构</figcaption></figure><p id="c3a9" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">该结构产生总共2755万个参数，其中2400万个仅从最后三个完全连接的层产生，其余的从其后的卷积网络产生。内核大小已经减少或保持不变，因为我们继续进行。在第一个卷积层中共有96个参数，每个参数在一个二维输入层中水平和垂直移动。11*11的内核大小意味着我们正在捕捉图像中的大面积像素。关于参数，没有合适的方法来确定我们为什么使用那些精确的参数。深度学习是一个反复试验的过程，在这个过程中，我们检查准确性，并保留准确性高的参数。</p><blockquote class="le lf lg"><p id="f061" class="ix iy jt iz b ja jb ij jc jd je im jf lh jh ji jj li jl jm jn lj jp jq jr js hb bi translated">该网络的错误率为16.4%。</p><p id="ebdd" class="ix iy jt iz b ja jb ij jc jd je im jf lh jh ji jj li jl jm jn lj jp jq jr js hb bi translated">AlexNet在两个Nvidia Geforce GTX 580 GPU上同时接受了6天的训练，这就是网络分为两条管道的原因。  <em class="hi">那不是很牛吗..:D </em></p></blockquote><p id="509c" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">更多关于AlexNet : <a class="ae ju" href="https://www.learnopencv.com/understanding-alexnet/" rel="noopener ugc nofollow" target="_blank"> <em class="jt">只是深度学习的东西！</em>T9】</a></p><h1 id="a405" class="kl km hi bd kn ko kp kq kr ks kt ku kv io kw ip kx ir ky is kz iu la iv lb lc bi translated">ZFNet</h1><figure class="jw jx jy jz fd ka er es paragraph-image"><div role="button" tabindex="0" class="kb kc di kd bf ke"><div class="er es lk"><img src="../Images/dcf8da4fb0f75acf6c51fc078baa41f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fcV6s917ddmsFKK0rvz8Dg.png"/></div></div><figcaption class="kh ki et er es kj kk bd b be z dx translated">无全连接层的ZFNet结构</figcaption></figure><p id="5231" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">ZFNet结构类似于AlexNet结构，但是在某些层上有不同的参数。ZFNet选择从图像中选择较少的像素，因此第一个内核大小为7*7，不像AlexNet中的11*11。当我们在网络中前进时，内核大小减小或保持不变。网络共有8层，其中5层为卷积层，3层为全连接层。maxpooling层不被视为不同的层，因为它没有关联的参数。</p><blockquote class="le lf lg"><p id="98c2" class="ix iy jt iz b ja jb ij jc jd je im jf lh jh ji jj li jl jm jn lj jp jq jr js hb bi translated"><strong class="iz hj"> <em class="hi">这种架构在2013年赢得了ImageNet竞赛，实现了14.8%的错误率。</em>T13】</strong></p></blockquote><p id="2cca" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">更多关于ZFNet &amp; the competition: <a class="ae ju" href="https://pechyonkin.me/architectures/zfnet/" rel="noopener ugc nofollow" target="_blank"> <em class="jt">只是深度学习的东西！</em> </a></p><p id="1563" class="pw-post-body-paragraph ix iy hi iz b ja jb ij jc jd je im jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="iz hj">深度学习</strong>是一个有趣的&amp;令人敬畏的领域。所以，我强烈推荐想要开始学习<strong class="iz hj">机器学习</strong>或<em class="jt"> #100DaysofMLCode </em>的人先学习<strong class="iz hj">深度学习</strong>，因为你将真正能够理解在ML中调用函数背后发生的算法和过程的意义和工作方式。</p><blockquote class="ll"><p id="4d1b" class="lm ln hi bd lo lp lq lr ls lt lu js dx translated"><strong class="ak">机器学习</strong>一旦完成<strong class="ak">深度学习就变成了<strong class="ak">小菜一碟</strong>！:)</strong></p><p id="e7c3" class="lm ln hi bd lo lp lq lr ls lt lu js dx translated">培训测试愉快！</p></blockquote></div></div>    
</body>
</html>