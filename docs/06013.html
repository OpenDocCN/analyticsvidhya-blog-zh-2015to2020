<html>
<head>
<title>ML &amp; DL — Linear Regression (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ML &amp; DL —线性回归(第二部分)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/ml-dl-linear-regression-part-2-14f114f2d62a?source=collection_archive---------19-----------------------#2020-05-08">https://medium.com/analytics-vidhya/ml-dl-linear-regression-part-2-14f114f2d62a?source=collection_archive---------19-----------------------#2020-05-08</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f03d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">线性回归定义了两个变量之间的关系，但是如何找到参数的“<em class="jd">最佳</em>”组合？</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div role="button" tabindex="0" class="jk jl di jm bf jn"><div class="er es je"><img src="../Images/9807c754a250eb2b4406605048437090.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MxhQpeJgVG_IE0gj"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">照片由<a class="ae ju" href="https://unsplash.com/@atharva_tulsi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Atharva Tulsi </a>在<a class="ae ju" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</figcaption></figure><p id="32ab" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，你会发现:</p><ul class=""><li id="538a" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><strong class="ih hj">线性回归简介，</strong></li><li id="3614" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated"><strong class="ih hj">成本函数</strong>和<strong class="ih hj">优化</strong>问题，</li><li id="7bf8" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated"><strong class="ih hj">梯度下降</strong>和学习率、</li><li id="0bb2" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated"><strong class="ih hj">反向传播</strong>和<strong class="ih hj">优化器，</strong></li><li id="0f77" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">用<em class="jd"> Jupyter笔记本</em>中的<em class="jd"> Keras </em>实现<strong class="ih hj">线性回归</strong></li><li id="1ae0" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">部分总结。</li></ul><h1 id="0318" class="kj kk hi bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">线性回归</h1><p id="dd8a" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated"><strong class="ih hj">线性回归</strong>是一个简单的机器学习算法，它解决了一个<em class="jd">回归问题</em>【1】。</p><p id="a091" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在<strong class="ih hj">统计</strong>中，<em class="jd">线性回归</em>是一种建模因变量<em class="jd"> y </em>与一个或多个自变量<em class="jd"> x </em>之间关系的技术。</p><p id="2d6e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输出是输入的线性函数[1]</p><ul class=""><li id="5bfc" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><code class="du lm ln lo lp b">ŷ= Wx + b</code>，<em class="jd">线性回归。</em></li><li id="2dd5" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated"><strong class="ih hj">假设</strong> <code class="du lm ln lo lp b">ŷ</code>是模型预测的值，</li><li id="ef2e" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated"><strong class="ih hj">参数</strong></li></ul><h2 id="7517" class="lq kk hi bd kl lr ls lt kp lu lv lw kt iq lx ly kx iu lz ma lb iy mb mc lf md bi translated">因素</h2><p id="a14e" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">参数是控制系统<em class="jd">行为</em>的值。</p><ul class=""><li id="5b00" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated">目标是找到描述数据的<em class="jd">【最佳】</em>可能的参数集<code class="du lm ln lo lp b">W</code>和<code class="du lm ln lo lp b">b</code>。</li><li id="002c" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">然而，首先，我们需要定义<strong class="ih hj">误差/成本。</strong></li></ul><h2 id="786f" class="lq kk hi bd kl lr ls lt kp lu lv lw kt iq lx ly kx iu lz ma lb iy mb mc lf md bi translated">价值函数</h2><p id="60ea" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">定义模型<em class="jd">绩效评估</em>。</p><p id="cfeb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">测量模型性能的一种方法是计算测试集[1]中的<strong class="ih hj">均方误差</strong> (MSE)。</p><ul class=""><li id="89bd" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated">在<strong class="ih hj">统计</strong>中，MSE测量误差或偏差的均方值。</li><li id="c779" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated"><code class="du lm ln lo lp b">L = 1/n ∑(y⒤ — ŷ⒤)²</code> <em class="jd">，均方误差(MSE)。</em></li></ul><p id="0606" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">假设值<code class="du lm ln lo lp b">ŷ</code>与真实值<code class="du lm ln lo lp b">y</code>之差。</p><h2 id="c742" class="lq kk hi bd kl lr ls lt kp lu lv lw kt iq lx ly kx iu lz ma lb iy mb mc lf md bi translated">优化问题</h2><p id="2556" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">然而，我们需要<em class="jd">基于<br/>模型参数<code class="du lm ln lo lp b">W</code>和<code class="du lm ln lo lp b">b</code>最小化假设<code class="du lm ln lo lp b">ŷ</code>的成本</em>。</p><ul class=""><li id="5bac" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><code class="du lm ln lo lp b">min(L) = min(1/n ∑(y⒤ — ŷ⒤)²</code>，<em class="jd">最小化成本函数。</em></li></ul><p id="a110" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可能的解决方案:</p><ul class=""><li id="546a" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated">分析或</li><li id="c23e" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated"><strong class="ih hj">数值</strong>:迭代数据集的优化算法，例如<em class="jd">梯度下降</em>。</li></ul><h2 id="3560" class="lq kk hi bd kl lr ls lt kp lu lv lw kt iq lx ly kx iu lz ma lb iy mb mc lf md bi translated">梯度下降</h2><p id="b989" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">在<strong class="ih hj">微积分</strong>中，<em class="jd">梯度下降</em>是一种寻找函数【1】的<em class="jd">最小值的一阶迭代优化算法。</em></p><ul class=""><li id="9d46" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><strong class="ih hj">梯度</strong>是一个采用多变量函数的运算，返回一个<em class="jd">向量，方向为原函数图形中最大斜率</em>的方向。</li><li id="a916" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated"><code class="du lm ln lo lp b">∇L=[𝜕L/𝜕W, 𝜕L/𝜕b]ᵀ</code>，<em class="jd">梯度下降。</em></li></ul><p id="ed9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们想从<em class="jd">往下走</em>，我们要做的就是从<em class="jd">往</em>的相反方向走。这将是最小化成本函数的<em class="jd">策略。</em></p><h2 id="0962" class="lq kk hi bd kl lr ls lt kp lu lv lw kt iq lx ly kx iu lz ma lb iy mb mc lf md bi translated">ϵ学习率</h2><p id="13cc" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">学习率是确定步长[1]的正比例<code class="du lm ln lo lp b">ϵ</code>。</p><ul class=""><li id="91da" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><code class="du lm ln lo lp b">[W’, b’]ᵀ = [W, b]ᵀ — ϵ∇L</code>、<em class="jd">学习率</em>。</li></ul><p id="9dbe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">学习率<code class="du lm ln lo lp b">ϵ </code>在两个方面可能是个问题:</p><ul class=""><li id="e6e3" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated">如果步长<em class="jd">太小</em>，我们将缓慢移动到最小值，</li><li id="f845" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">如果步长<em class="jd">太大</em>，我们可能会跳过最小值。</li></ul><h2 id="e674" class="lq kk hi bd kl lr ls lt kp lu lv lw kt iq lx ly kx iu lz ma lb iy mb mc lf md bi translated">反向传播</h2><p id="2cb8" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated"><em class="jd">计算</em>坡度的方法。</p><p id="1635" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">而<em class="jd">另一个</em>算法，例如<em class="jd">梯度下降</em>，用于使用该梯度[1]执行<strong class="ih hj">学习</strong> ( <em class="jd">优化</em>)。</p><p id="d16e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">两个短语，<em class="jd">正向传播，</em>和<em class="jd">反向传播</em>。</p><ul class=""><li id="7908" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><strong class="ih hj">正向传播</strong></li></ul><p id="13d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">输入<code class="du lm ln lo lp b">x</code>提供初始信息<em class="jd">传播</em>并最终产生<code class="du lm ln lo lp b">ŷ</code>。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es me"><img src="../Images/bd04c686d048ef23ac471c4025b3c35d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*YEFf5sXXX1ruCi3d9JMrCg.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">由<a class="mf mg ge" href="https://medium.com/u/d83c82313ee1?source=post_page-----14f114f2d62a--------------------------------" rel="noopener" target="_blank">马正向传播。费尔南达·罗德里格斯</a></figcaption></figure><ul class=""><li id="81e1" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><strong class="ih hj">反向传播</strong></li></ul><p id="fd78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用<em class="jd">成本函数值来计算误差</em>。误差值被向后传播，以便计算相对于权重的梯度。</p><p id="aa3a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用<a class="ae ju" href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener ugc nofollow" target="_blank"> <em class="jd">链规则</em> </a>计算梯度。</p><figure class="jf jg jh ji fd jj er es paragraph-image"><div class="er es me"><img src="../Images/72e1832ebd77035c1a3b7b4468604f75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*KXUUD49v01z8bRwFHwiIRQ.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">通过<a class="mf mg ge" href="https://medium.com/u/d83c82313ee1?source=post_page-----14f114f2d62a--------------------------------" rel="noopener" target="_blank"> ma进行反向传播。费尔南达·罗德里格斯</a></figcaption></figure><h2 id="28ef" class="lq kk hi bd kl lr ls lt kp lu lv lw kt iq lx ly kx iu lz ma lb iy mb mc lf md bi translated"><strong class="ak">反向传播算法</strong></h2><ul class=""><li id="c1f4" class="jv jw hi ih b ii lh im li iq mh iu mi iy mj jc ka kb kc kd bi translated">在<em class="jd">前向传播</em>之后，我们获得一个输出值，即预测值。</li><li id="c4b1" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">我们使用一个<em class="jd">损失函数</em> <code class="du lm ln lo lp b">L</code>来计算误差值。</li><li id="02cf" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">我们计算每个重量的误差梯度，</li><li id="4578" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">我们从权重值中减去梯度值。</li></ul><p id="bcc6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这样，我们就接近了<em class="jd">局部最小值</em>。</p><h2 id="5b8e" class="lq kk hi bd kl lr ls lt kp lu lv lw kt iq lx ly kx iu lz ma lb iy mb mc lf md bi translated">梯度下降的问题</h2><p id="726d" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated"><em class="jd">梯度下降的传统方法</em>计算整个数据集的梯度，但是<em class="jd">将仅执行升级</em>。</p><p id="df18" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，对于不适合内存的非常大的数据集，控制起来会非常缓慢和困难。</p><p id="a1c8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">作为一个解决方案，对原来的方法<em class="jd">优化器</em>进行了<em class="jd">修改</em>。</p><h2 id="a1cf" class="lq kk hi bd kl lr ls lt kp lu lv lw kt iq lx ly kx iu lz ma lb iy mb mc lf md bi translated">优化者</h2><p id="2f37" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">有<em class="jd">扩展</em>(对原方法的修改)试图解决<em class="jd">梯度下降</em>的问题。</p><p id="3af9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基本[1]:</p><ul class=""><li id="8657" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><strong class="ih hj"> SGD </strong>:随机梯度下降。</li></ul><p id="a87b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">适应性学习率[1]:</p><ul class=""><li id="1474" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><strong class="ih hj">阿达格拉德</strong>【2】。</li><li id="0f05" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated">rms prop【3】。</li><li id="e3bd" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated"><strong class="ih hj">亚当</strong>【4】。</li></ul><h1 id="8ed8" class="kj kk hi bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">Github代码</h1><p id="aa82" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">在这个库中，你会发现一个<strong class="ih hj">线性回归</strong>的一步一步的实现与<em class="jd"> Keras </em>在<em class="jd"> Jupyter笔记本</em>中。</p><div class="mk ml ez fb mm mn"><a href="https://github.com/mafda/deep_learning_101/blob/master/src/01-linear-regression.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab dw"><div class="mp ab mq cl cj mr"><h2 class="bd hj fi z dy ms ea eb mt ed ef hh bi translated">MAF da/深度学习_101</h2><div class="mu l"><h3 class="bd b fi z dy ms ea eb mt ed ef dx translated">permalink dissolve GitHub是超过5000万开发人员的家园，他们一起工作来托管和审查代码，管理…</h3></div><div class="mv l"><p class="bd b fp z dy ms ea eb mt ed ef dx translated">github.com</p></div></div><div class="mw l"><div class="mx l my mz na mw nb jo mn"/></div></div></a></div><h1 id="b2d3" class="kj kk hi bd kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg bi translated">部分摘要</h1><p id="1b62" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated"><strong class="ih hj">线性回归</strong>定义两个变量之间的<em class="jd">关系</em>。</p><p id="d791" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如何找到“<em class="jd">最佳的</em>参数组合？</p><ul class=""><li id="dc28" class="jv jw hi ih b ii ij im in iq jx iu jy iy jz jc ka kb kc kd bi translated"><em class="jd">成本函数</em>:均方差</li><li id="e5dc" class="jv jw hi ih b ii ke im kf iq kg iu kh iy ki jc ka kb kc kd bi translated"><em class="jd">梯度下降</em><br/><em class="jd">反向传播</em>计算梯度<br/><em class="jd">优化器</em>(向下梯度扩展)</li></ul><p id="6911" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">寻找参数最佳组合(优化)的过程称为<strong class="ih hj">训练</strong>。</p><p id="b10f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所有理论和实际实现:<a class="ae ju" rel="noopener" href="/@mafdarr/ml-dl-linear-regression-part-2-14f114f2d62a"> <em class="jd">线性回归</em> </a> <em class="jd">，</em> <a class="ae ju" rel="noopener" href="/@mafdarr/ml-dl-logistic-regression-part-3-fe6aca8f01b"> <em class="jd">逻辑回归</em> </a> <em class="jd">，</em> <a class="ae ju" rel="noopener" href="/@mafdarr/ml-dl-artificial-neural-networks-part-4-619350a93ef1"> <em class="jd">人工神经网络</em> </a> <em class="jd">，</em> <a class="ae ju" rel="noopener" href="/@mafdarr/ml-dl-deep-artificial-neural-networks-part-5-568ad05be712"> <em class="jd">深度神经网络</em> </a> <em class="jd">，以及</em> <a class="ae ju" rel="noopener" href="/@mafdarr/ml-dl-convolutional-neural-networks-part-6-97357db58165"> <em class="jd">卷积神经网络</em> </a>。</p></div><div class="ab cl nc nd gp ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="hb hc hd he hf"><p id="3776" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于那些寻找我们<strong class="ih hj"> ML &amp; DL系列</strong>所有文章的人来说。以下是链接。</p><div class="mk ml ez fb mm mn"><a rel="noopener follow" target="_blank" href="/@mafdarr/ml-dl-machine-learning-and-deep-learning-101-2686d93d70d"><div class="mo ab dw"><div class="mp ab mq cl cj mr"><h2 class="bd hj fi z dy ms ea eb mt ed ef hh bi translated">ML &amp; DL —机器学习和深度学习101</h2><div class="mu l"><h3 class="bd b fi z dy ms ea eb mt ed ef dx translated">介绍涉及机器学习和深度学习概念的基本概念。</h3></div><div class="mv l"><p class="bd b fp z dy ms ea eb mt ed ef dx translated">medium.com</p></div></div><div class="mw l"><div class="nj l my mz na mw nb jo mn"/></div></div></a></div></div><div class="ab cl nc nd gp ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="hb hc hd he hf"><h1 id="91a8" class="kj kk hi bd kl km nk ko kp kq nl ks kt ku nm kw kx ky nn la lb lc no le lf lg bi translated">参考</h1><p id="6932" class="pw-post-body-paragraph if ig hi ih b ii lh ik il im li io ip iq lj is it iu lk iw ix iy ll ja jb jc hb bi translated">[1]古德费勒，I .，本吉奥，y .，和库维尔，A. (2016)。<a class="ae ju" href="https://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank"> <em class="jd">深度学习</em> </a>。麻省理工出版社。</p><p id="e39c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[2]杜奇、哈赞和辛格(2011年)。<em class="jd">在线学习和随机优化的自适应次梯度方法</em>。机器学习研究杂志，12(7月):2121–2159。</p><p id="64e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[3]辛顿·g .(2012年)。<em class="jd">机器学习的神经网络</em>。Coursera，视频讲座</p><p id="c601" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">[4]金马博士和巴律师(2014年)。<em class="jd">亚当:一种随机优化的方法</em>。abs/1412.6980。</p><div class="mk ml ez fb mm mn"><a href="https://hackernoon.com/everything-you-need-to-know-about-neural-networks-8988c3ee4491" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab dw"><div class="mp ab mq cl cj mr"><h2 class="bd hj fi z dy ms ea eb mt ed ef hh bi translated">关于神经网络你需要知道的一切</h2><div class="mu l"><h3 class="bd b fi z dy ms ea eb mt ed ef dx translated">感谢:Kailash ahir war(Mate Labs联合创始人兼首席技术官)</h3></div><div class="mv l"><p class="bd b fp z dy ms ea eb mt ed ef dx translated">hackernoon.com</p></div></div><div class="mw l"><div class="np l my mz na mw nb jo mn"/></div></div></a></div></div></div>    
</body>
</html>