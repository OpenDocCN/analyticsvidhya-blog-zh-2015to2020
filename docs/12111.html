<html>
<head>
<title>Text Classification — From Bag-of-Words to BERT — Part 2 (Word2Vec)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本分类—从词袋到BERT —第2部分(Word2Vec)</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-2-word2vec-35c8c3b34ee3?source=collection_archive---------1-----------------------#2020-12-31">https://medium.com/analytics-vidhya/text-classification-from-bag-of-words-to-bert-part-2-word2vec-35c8c3b34ee3?source=collection_archive---------1-----------------------#2020-12-31</a></blockquote><div><div class="ds gv gw gx gy gz"/><div class="ha hb hc hd he"><div class=""/><figure class="ev ex if ig ih ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ie"><img src="../Images/ec5da50d6b851aaecaa80a0de2810741.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Jnxo-Rv5L0q-77QQ"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">塔曼娜·茹米在<a class="ae it" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</figcaption></figure></div><div class="ab cl iu iv go iw" role="separator"><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz ja"/><span class="ix bw bk iy iz"/></div><div class="ha hb hc hd he"><p id="e092" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">这个故事是一系列文本分类的一部分——从单词袋到伯特。如果你还没有检查之前的故事，一定要检查一下，因为这将有助于理解未来的事情。</p><p id="b075" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><a class="ae it" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-1e628a2dd4c9" rel="noopener">第一部分</a></p><p id="afa7" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">在之前的故事中(<a class="ae it" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-1e628a2dd4c9" rel="noopener"> Part 1 (BagOfWords) </a>我们使用了CountVectorizer(一个sklearn的单词包实现)模型将文本转换为数字数据集，映射到输出变量toxic、severe_toxic、淫秽、威胁、侮辱、identity_hate，并使用sklearn的多输出分类器包装器为所有6个输出变量创建逻辑回归模型。</p><p id="4b20" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">在这个例子中，我们将用Word2Vec模型代替第一部分来创建嵌入，而不是BagOfWords向量，然后将其输入到逻辑回归模型中(任何ML/DL模型都可以在Word2Vec嵌入的基础上构建)。</p><p id="2723" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi">注意:我没有在这篇博客中涉及逻辑回归和特征重要性/模型解释的细节，因为我已经在上一篇文章(</strong> <a class="ae it" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-1e628a2dd4c9" rel="noopener"> <strong class="jd hi">第一部分(BagOfWords)</strong></a><strong class="jd hi">)</strong>中涉及过</p><p id="e753" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> <em class="jz">什么是单词嵌入？</em>T19】</strong></p><figure class="kb kc kd ke fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ka"><img src="../Images/995155dc1c06ae396e8805c829623ab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K8HYjAhH0g7Vr58IZLerDA.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">二维单词嵌入</figcaption></figure><p id="a641" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">单词嵌入为我们提供了一种使用高效、密集表示的方法，其中相似的单词具有相似的编码。重要的是，您不必手动指定这种编码。嵌入是浮点值的密集向量(向量的长度是您指定的参数)。</p><p id="41f2" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">上面是一个二维单词嵌入，其中星期日与其他工作日的相似值多于家庭成员</p><p id="75ac" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> <em class="jz">什么是Word2Vec？</em> </strong></p><p id="64ed" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">Word2Vec是创建/学习这些嵌入的最古老的方法之一。Word2Vec不是一个单一的算法，而是一系列模型架构和优化，可用于从大型数据集学习单词嵌入。通过Word2Vec学习的嵌入已被证明在各种下游自然语言处理任务上是成功的，如文本分类、问题回答。该论文提出了两种学习单词表征的方法:</p><p id="f87d" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi">连续词袋模型</strong>基于周围上下文词预测中间词。上下文由当前(中间)单词前后的几个单词组成。这种架构被称为单词袋模型，因为单词在上下文中的顺序并不重要。</p><p id="8a07" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi">连续跳格模型</strong>预测同一句子中当前单词前后一定范围内的单词。</p><figure class="kb kc kd ke fd ii er es paragraph-image"><div class="er es kf"><img src="../Images/872068ab00a655fb9523ae09ee1a99d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1166/format:webp/1*-YZnKvaisW3eBJvSxqL7xA.png"/></div></figure><p id="372b" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">在CBOW中，给定单词(快速的棕色盒子，在懒惰的狗上面)，我们想要预测跳跃。在Skipgram中，与单词jump正好相反，我们想要预测(快速的棕色框，在懒狗上面)</p><p id="3587" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> <em class="jz">但是模特们是怎么学习的呢？</em> </strong></p><figure class="kb kc kd ke fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es kg"><img src="../Images/eecb8f662b0e8fa762be561366eb9c39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_VkyMv_jeRqyBoS29TX1vg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">【CBOW(左)和Skip-gram(右)的架构</figcaption></figure><p id="27f0" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">让我们从CBOW开始，我们以句子“自然语言处理”为例，其中“自然”和“处理”都是上下文词，“语言”是目标词。我们有一个浅的网络，如上图所示，只有一个隐藏层。</p><p id="614a" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">因此，输入是一个只有一个1的V项(词汇的大小/唯一单词的总数)的独一无二的编码向量。假设我们只有5个词汇(自然、语言、处理、是、很好)。自然的向量将是[1，0，0，0，0]。类似地，对于处理，它将是[0，0，1，0，0]。现在，我们有一个大小为V * <em class="jz"> D </em>的随机初始化的嵌入向量(E ),其中D是可以选择的向量的维数。这是输入图层的权重矩阵。因此，我们将输入的独热编码向量乘以权重/嵌入向量。这给出了尺寸为1 D的上下文单词(自然的和处理的)的嵌入向量</p><p id="7823" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">现在，在隐藏层中，我们对上下文单词的嵌入向量进行平均，这形成了该层的大小为1 <em class="jz"> * D. </em>的输入。这乘以另一个称为上下文向量(E’)的大小为D * V的向量。这给了我们1 * V的向量，该向量然后通过sigmoid函数得到最终输出。</p><p id="a86e" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">将最终输出与语言的独热编码向量(中间字)[0，1，0，0，0]进行比较，并计算损失函数。该损失被反向传播，并且使用梯度下降来训练该模型</p><p id="58e7" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">对于Skip-gram，情况正好相反，我们有中间词的一个热编码向量，它乘以权重/嵌入向量E = V * D，我们得到中间词的嵌入，作为输入层的输出和隐藏层的输入。它与上下文向量E' = D * V相乘，我们得到输出，该输出通过sigmoid传递，并与上下文单词进行比较，以得到损失和反向传播。</p><p id="b951" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">在这两种情况下，我们只在最后保留嵌入(E)向量</p><p id="a2f4" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> <em class="jz">我们将如何获得嵌入？</em> </strong></p><p id="2503" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">Gensim库使我们能够开发单词嵌入。Gensim让您在训练自己的嵌入时可以选择CBOW或Skip-gram。(默认为CBOW)。除此之外，Gensim还有一个预训练嵌入的目录，这些预训练嵌入是在几个文档上训练的，如wiki页面、google新闻、Twitter tweets等。在这个例子中，我们将使用基于谷歌新闻语料库(30亿个运行单词)单词向量模型(300万个300维英语单词向量)的预训练嵌入。定义够了。让我们深入研究代码👨‍💻</p><p id="a42e" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi">实施:</strong></p><p id="7c34" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> <em class="jz"> 1。读取数据集</em> </strong></p><figure class="kb kc kd ke fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ki"><img src="../Images/0543de08e345f6b77adbe68a2a0922ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1oEsT7kJsaTm2saQmCu36g.png"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">提醒一下，这是训练数据的样子</figcaption></figure><p id="1de4" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> <em class="jz"> 2。基本预处理</em> </strong></p><pre class="kb kc kd ke fd kj kk kl km aw kn bi"><span id="0152" class="ko kp hh kk b fi kq kr l ks kt">def preprocess_corpus(texts):<br/>    <em class="jz">#importing stop words like in, the, of so that these can be removed from texts</em><br/>    <em class="jz">#as these words dont help in determining the classes(Whether a sentence is toxic or not)</em><br/>    mystopwords = set(stopwords.words("english"))<br/>    def remove_stops_digits(tokens):<br/>        <em class="jz">#Nested function that lowercases, removes stopwords and digits from a list of tokens</em><br/>        return [token.lower() for token <strong class="kk hi">in</strong> tokens if token <strong class="kk hi">not</strong> <strong class="kk hi">in</strong> mystopwords <strong class="kk hi">and</strong> <strong class="kk hi">not</strong> token.isdigit()<br/>               <strong class="kk hi">and</strong> token <strong class="kk hi">not</strong> <strong class="kk hi">in</strong> punctuation]<br/>    <em class="jz">#This return statement below uses the above function and tokenizes output further. </em><br/>    return [remove_stops_digits(word_tokenize(text)) for text <strong class="kk hi">in</strong> tqdm(texts)]<br/><br/><em class="jz">#Preprocess both for training and test data</em><br/>train_texts_processed = preprocess_corpus(train_texts)<br/>test_texts_processed = preprocess_corpus(test_texts)</span></pre><figure class="kb kc kd ke fd ii er es paragraph-image"><div role="button" tabindex="0" class="ij ik di il bf im"><div class="er es ku"><img src="../Images/8488434cee3f35efbbd15d47539c7b6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wNF-Hm8_cXlSZg1eDy-LQg.jpeg"/></div></div><figcaption class="ip iq et er es ir is bd b be z dx translated">预处理的结果</figcaption></figure><p id="7e07" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">在这种情况下，我们从NLTK库中删除停用词和完整数字，小写所有文本，并使用word_tokenize对文本进行标记化(分解成单独的标记/单词)</p><p id="2dea" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> <em class="jz"> 3。加载预训练嵌入</em> </strong></p><p id="5683" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">我们使用Gensim库为在Google新闻数据集上训练的单词加载预训练嵌入。谷歌新闻模型/嵌入向量是300维的。护目镜新闻模型/嵌入向量大约有3 M字。让我们看一个嵌入的例子，它本质上是一个字典，其中键是单词，值是该单词的嵌入向量。</p><pre class="kb kc kd ke fd kj kk kl km aw kn bi"><span id="3e00" class="ko kp hh kk b fi kq kr l ks kt"><em class="jz">#Path for the models/ embedding vector</em><br/>google_news_model = '../input/gensim-embeddings-dataset/GoogleNews-vectors-negative300.gensim'<br/><em class="jz">#Loading the models/ embedding vector using KeyedVectors.load function from gensim</em><br/>w2v_google_news = KeyedVectors.load(google_news_model)<br/><em class="jz">#Print Shape of the embedding</em><br/>print("Shape of embedding vector", w2v_google_news["Natural"].shape)<br/><em class="jz">#Let's print first 20 dimensions rather than all 300</em><br/>print("First 20 numbers in the embedding of the word Natural<strong class="kk hi">\n\n</strong>", w2v_google_news["Natural"][:20])</span></pre><figure class="kb kc kd ke fd ii er es paragraph-image"><div class="er es kv"><img src="../Images/975c8bec94d81ac3c0662028b5f5164e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*OIwoM1No4jCI9A4DwIUa8w.png"/></div><figcaption class="ip iq et er es ir is bd b be z dx translated">这就是单词“Natural”的嵌入方式。</figcaption></figure><p id="80b0" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> <em class="jz"> 4。使用预先训练的模型将文本输入转换为嵌入内容</em> </strong></p><p id="4f1b" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">这里，我们从先前的输入标记化文本中获取输入，并从预先训练的嵌入向量中获取文本中每个单词的嵌入。这将为我们提供最终的输入数据集，其形式为每个句子的嵌入，可用于与输出变量一起训练。</p><pre class="kb kc kd ke fd kj kk kl km aw kn bi"><span id="5bdd" class="ko kp hh kk b fi kq kr l ks kt"><em class="jz">#Function that takes in the input text dataset in form of list of lists where each sentence is a list of words all the sentences are </em><br/><em class="jz">#inside a list</em><br/>def embedding_feats(list_of_lists, DIMENSION, w2v_model):<br/>    zeros_vector = np.zeros(DIMENSION)<br/>    feats = []<br/>    missing = set()<br/>    missing_sentences = set()<br/>    <em class="jz">#Traverse over each sentence</em><br/>    for tokens <strong class="kk hi">in</strong> tqdm(list_of_lists):<br/>        <em class="jz"># Initially assign zeroes as the embedding vector for the sentence</em><br/>        feat_for_this = zeros_vector<br/>        <em class="jz">#Count the number of words in the embedding for this sentence</em><br/>        count_for_this = 0<br/>        <em class="jz">#Traverse over each word of a sentence</em><br/>        for token <strong class="kk hi">in</strong> tokens:<br/>            <em class="jz">#Check if the word is in the embedding vector</em><br/>            if token <strong class="kk hi">in</strong> w2v_model:<br/>                <em class="jz">#Add the vector of the word to vector for the sentence</em><br/>                feat_for_this += w2v_model[token]<br/>                count_for_this +=1<br/>            <em class="jz">#Else assign the missing word to missing set just to have a look at it</em><br/>            else:<br/>                missing.add(token)<br/>        <em class="jz">#If no words are found in the embedding for the sentence</em><br/>        if count_for_this == 0:<br/>            <em class="jz">#Assign all zeroes vector for that sentence</em><br/>            feats.append(feat_for_this)<br/>            <em class="jz">#Assign the missing sentence to missing_sentences just to have a look at it</em><br/>            missing_sentences.add(' '.join(tokens))<br/>        <em class="jz">#Else take average of the values of the embedding for each word to get the embedding of the sentence</em><br/>        else:<br/>            feats.append(feat_for_this/count_for_this)<br/>    return feats, missing, missing_sentences</span><span id="18fc" class="ko kp hh kk b fi kw kr l ks kt"><em class="jz">#Embeddings for the train dataset</em><br/>train_vectors, missing, missing_sentences = embedding_feats(train_texts_processed, 300, w2v_google_news)</span></pre><figure class="kb kc kd ke fd ii er es paragraph-image"><div class="er es kx"><img src="../Images/da8fd964ad2159e783128a0d1bd7401d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*KiL5VWFuWApoa-5PFtTMzw.png"/></div></figure><p id="5406" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">总之，每个句子将有一个300维的嵌入向量，它将是该句子中出现的单词嵌入的平均值。单词嵌入来自预先训练的单词嵌入，这些单词嵌入在google news上被训练以找到嵌入。</p><p id="b0f3" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><strong class="jd hi"> <em class="jz"> 5。训练和验证多输出分类器</em> </strong></p><p id="56b8" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">这一部分将涉及5件事</p><ol class=""><li id="03b4" class="ky kz hh jd b je jf ji jj jm la jq lb ju lc jy ld le lf lg bi translated">获取训练数据集的嵌入向量</li><li id="0b95" class="ky kz hh jd b je lh ji li jm lj jq lk ju ll jy ld le lf lg bi translated">将嵌入向量和输出变量分成训练集和验证集</li><li id="e373" class="ky kz hh jd b je lh ji li jm lj jq lk ju ll jy ld le lf lg bi translated">在训练嵌入向量和输出变量上拟合多输出逻辑回归模型<em class="jz">(我在之前的故事(</em> <a class="ae it" href="https://anirbansen3027.medium.com/text-classification-from-bag-of-words-to-bert-1e628a2dd4c9" rel="noopener"> <em class="jz">第一部分(bagowords)</em></a><em class="jz">)</em>中已经详细介绍了逻辑回归)</li><li id="1872" class="ky kz hh jd b je lh ji li jm lj jq lk ju ll jy ld le lf lg bi translated">对验证嵌入向量进行预测</li><li id="f6ff" class="ky kz hh jd b je lh ji li jm lj jq lk ju ll jy ld le lf lg bi translated">根据ROC-AUC衡量绩效</li></ol><pre class="kb kc kd ke fd kj kk kl km aw kn bi"><span id="b613" class="ko kp hh kk b fi kq kr l ks kt">def train_model(DIMENSION, model):<br/>    <em class="jz">#Get the embedding vector for the training data</em><br/>    train_vectors, missing, missing_sentences = embedding_feats(train_texts_processed, DIMENSION, model)<br/>    <em class="jz">#Split the embedding vector for the training data along with the output variables into train and validation sets</em><br/>    train_data, val_data, train_cats, val_cats = train_test_split(train_vectors, train_labels)<br/>    <em class="jz">#Logistic Regression Model (As we have unbalanced dataset, we use class_weight which will use inverse of counts of that class. It penalizes mistakes in samples of class[i] with class_weight[i] instead of 1)</em><br/>    lr = MultiOutputClassifier(LogisticRegression(class_weight='balanced', max_iter=3000)).fit(train_data, train_cats)<br/>    <em class="jz">#Actuals for the validation data</em><br/>    y_vals = val_cats<br/>    <em class="jz">#Prediction probability for the validation dataset by the model for class 1</em><br/>    y_preds = np.transpose(np.array(lr.predict_proba(val_data))[:,:,1])<br/>    <em class="jz">#Calculate the Mean ROC_AUC </em><br/>    mean_auc = mean(accuracy(y_vals,y_preds))<br/>    return mean_auc, lr</span><span id="5d71" class="ko kp hh kk b fi kw kr l ks kt">mean_auc, lr = train_model(300, w2v_google_news)</span></pre><p id="28c0" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">这个模型被证明是相当温和的(~0。60 ROC-AUC)。但同样，我们的目标是学习如何实现单词嵌入。低性能可能是因为预训练的嵌入没有正确地捕捉细节。我们可以使用Word2Vec来训练我们自己的嵌入。</p><p id="84e0" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated"><em class="jz"> TODOs: </em></p><ol class=""><li id="1aa2" class="ky kz hh jd b je jf ji jj jm la jq lb ju lc jy ld le lf lg bi translated">从头开始训练Word2Vec模型</li><li id="a705" class="ky kz hh jd b je lh ji li jm lj jq lk ju ll jy ld le lf lg bi translated">尝试集合模型，而不是普通的ML模型在大多数情况下，打包和增强模型比经典的ML技术给出更好的结果</li><li id="66c8" class="ky kz hh jd b je lh ji li jm lj jq lk ju ll jy ld le lf lg bi translated">可以做更好的文本预处理、打字错误纠正等来进一步改进模型</li></ol><p id="3e68" class="pw-post-body-paragraph jb jc hh jd b je jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy ha bi translated">这是关于Word2Vec的，下一个，将是关于脸书的fastText，它将单词嵌入的思想向前推进了一步，实现了一种叫做子单词嵌入的东西。在那之前保持安全。同样，完整的代码出现在<a class="ae it" href="https://www.kaggle.com/anirbansen3027/jtcc-word2vec" rel="noopener ugc nofollow" target="_blank">(这里)</a>。请以回答和鼓掌的形式提供您的反馈:)</p></div></div>    
</body>
</html>