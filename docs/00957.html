<html>
<head>
<title>A-Z of Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树的A-Z</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/part-1-a-z-of-decision-trees-40e2bd8c4089?source=collection_archive---------9-----------------------#2019-09-21">https://medium.com/analytics-vidhya/part-1-a-z-of-decision-trees-40e2bd8c4089?source=collection_archive---------9-----------------------#2019-09-21</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/022cefed4e48c1e28b686d73cdaa1d03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t9pQvj1UAqt8CAeMpsmjmw.jpeg"/></div></div></figure><div class=""/><div class=""><h2 id="d487" class="pw-subtitle-paragraph iq hs ht bd b ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh dx translated">第1部分:决策树背后的关键术语和直觉</h2></div><p id="3df0" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">你好，我亲爱的菲洛马斯！！我们这里的许多人都理解决策树背后的概念，但很少有人能够掌握它背后的数学。</p><p id="b22c" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在2部分教程中，我将尝试解释决策树的基础知识，它是如何工作的，优点和缺点，如何构建一个决策树，以及其他一些晦涩难懂的东西，如过度拟合和欠拟合。让我们开始吧。</p></div><div class="ab cl ke kf gp kg" role="separator"><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj kk"/><span class="kh bw bk ki kj"/></div><div class="hb hc hd he hf"><h1 id="7010" class="kl km ht bd kn ko kp kq kr ks kt ku kv iz kw ja kx jc ky jd kz jf la jg lb lc bi translated">什么是决策树？？</h1><p id="6a6e" class="pw-post-body-paragraph ji jj ht jk b jl ld iu jn jo le ix jq jr lf jt ju jv lg jx jy jz lh kb kc kd hb bi translated">决策树是机器学习中的监督算法，可用于分类和回归。它们是非参数的，因为它们不假设数据集的基本分布。决策树将从您的数据集中学习近似if-then-else条件，以获得分类值。树越深，模型就越适合(后面会详细介绍)。你可以简单地把它想象成一个流程图。</p><h1 id="2135" class="kl km ht bd kn ko li kq kr ks lj ku kv iz lk ja kx jc ll jd kz jf lm jg lb lc bi translated"><strong class="ak">决策树的几何直觉</strong></h1><p id="b556" class="pw-post-body-paragraph ji jj ht jk b jl ld iu jn jo le ix jq jr lf jt ju jv lg jx jy jz lh kb kc kd hb bi translated">对于多维数据集(多个要素)，决策树将根据每个要素在每个级别进行拆分。</p><p id="6caa" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">例如，对于我们下面的虹膜数据集，我们可以在每个后续层上基于萼片长度、萼片宽度、花瓣长度或花瓣宽度来分割决策树，直到我们得出关于我们可以分配什么类标签的结论(参见图2)。</p><figure class="lo lp lq lr fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ln"><img src="../Images/cd2f59b4b164f72d0aedc1f8fdbaa941.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3J9L9e4Oy2AODVe2KWUDfA.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">图IRIS数据集的样本值</figcaption></figure><figure class="lo lp lq lr fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lw"><img src="../Images/770ff7ed4d66409025173598feae242b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Eeug6PUkElbFn33BW9AlqQ.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">图2:构建的示例决策树</figcaption></figure><p id="6c51" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">然而，可能会出现一个问题，为什么我们考虑花瓣长度作为初始分类器，而不是花瓣宽度。我们很快就会知道。</p><h1 id="4b98" class="kl km ht bd kn ko li kq kr ks lj ku kv iz lk ja kx jc ll jd kz jf lm jg lb lc bi translated"><strong class="ak">根结又什么不是！！</strong></h1><p id="b024" class="pw-post-body-paragraph ji jj ht jk b jl ld iu jn jo le ix jq jr lf jt ju jv lg jx jy jz lh kb kc kd hb bi translated">现在，因为我们有一个简单的DT要看，所以让我们弄清楚一些术语。</p><p id="2ca0" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">在图2中，“<strong class="jk hu"> <em class="lx">花瓣长度</em> </strong>”可以被认为是<strong class="jk hu"> <em class="lx">根节点</em> </strong>，因为我们是从这个节点开始分支的。这也是一级，如果你这么认为的话。</p><p id="8c08" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">随后，第二层的<strong class="jk hu"> <em class="lx">花瓣宽度</em> </strong>，是一个<strong class="jk hu"> <em class="lx">子节点</em> </strong>，由于显而易见的原因，在第三层我们有了<strong class="jk hu"> <em class="lx">叶节点</em> </strong>，到达后我们可以做出一个预测。</p><p id="282d" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">从几何学的观点来看，如果我们考虑花瓣长度和花瓣宽度之间的2D散点图，我们可以认为将2D平面分成在每一层 的“<strong class="jk hu"> <em class="lx">轴平行扇区”，如下图所示。</em></strong></p><figure class="lo lp lq lr fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ly"><img src="../Images/f2d421cf13bbaa2a2738b2374f8ed0b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hHJIt5Xw4R0mMMdBIt-vTA.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">图3 : x1和x2是在树的每一层使用决策表面划分2D图的两个特征</figcaption></figure><h1 id="3c7a" class="kl km ht bd kn ko li kq kr ks lj ku kv iz lk ja kx jc ll jd kz jf lm jg lb lc bi translated"><strong class="ak">理解熵</strong></h1><p id="52d6" class="pw-post-body-paragraph ji jj ht jk b jl ld iu jn jo le ix jq jr lf jt ju jv lg jx jy jz lh kb kc kd hb bi translated">就像在热力学中一样，在信息论中，我们为分布定义熵。简单来说，熵是对数据的不可预测性或随机性的度量。例如，如果我们考虑一个公平的硬币，只有2种结果(正面，反面)。所以我们可以说它的熵为2。</p><p id="81ef" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">然而，从统计学的角度来看，这个定义有点琐碎，如下所示。这里P(x)代表在“n”次观察中出现的百分比。因为P(x) ≤ 1，所以由于对数函数，选择负号以产生正熵。</p><figure class="lo lp lq lr fd hk er es paragraph-image"><div class="er es lz"><img src="../Images/2865aeb6b7ec4f211150afbdd902e247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*j73APL6T5YWy7bYvSNlsHg.png"/></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">图4:随机变量X的熵</figcaption></figure><p id="9c2b" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">考虑下面的数据集。我们有4个功能(前景，温度，湿度和风力)和一个类标签变量(播放)。</p><figure class="lo lp lq lr fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ma"><img src="../Images/293c5dd2a206f756de415c3cffbe9f21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aQgLxos3uhLVXrvjUS2-NQ.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">图5:打网球数据集</figcaption></figure><p id="5a95" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">决策列由14个实例组成，包括两个标签:是和否。有9个决策标记为“是”，5个决策标记为“否”。因此数据集的<em class="lx">熵</em>可以定义为</p><figure class="lo lp lq lr fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mb"><img src="../Images/911b0b2a078a0a73da585553640cf1ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EvwGjYO-M3wBZx9vvCdKcw.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">图6:熵计算</figcaption></figure><h1 id="ad33" class="kl km ht bd kn ko li kq kr ks lj ku kv iz lk ja kx jc ll jd kz jf lm jg lb lc bi translated">熵背后的直觉</h1><p id="b458" class="pw-post-body-paragraph ji jj ht jk b jl ld iu jn jo le ix jq jr lf jt ju jv lg jx jy jz lh kb kc kd hb bi translated">既然现在我们明白了如何确定熵，那我们就来理解熵是如何表示数据的。</p><figure class="lo lp lq lr fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mc"><img src="../Images/28310f17e7d9e5e72550e6162481f387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NfRT13DkkSOOoJ9xgkmPAg.png"/></div></div><figcaption class="ls lt et er es lu lv bd b be z dx translated">图6:熵曲线</figcaption></figure><p id="dee4" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">该曲线表示2类变量的P(x)对H(X)的曲线图。我们考虑3种情况来理解曲线。</p><h2 id="fc27" class="md km ht bd kn me mf mg kr mh mi mj kv jr mk ml kx jv mm mn kz jz mo mp lb mq bi translated"><strong class="ak">情况1 : </strong>当P(x) ~ (1 -P(x))时</h2><p id="5c95" class="pw-post-body-paragraph ji jj ht jk b jl ld iu jn jo le ix jq jr lf jt ju jv lg jx jy jz lh kb kc kd hb bi translated">如果类别标签几乎平均分布，则熵假定最大值，即1</p><p id="74b3" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">h(X)=-(0.5 * log(0.5))-(0.5 * log(0.5))= 1</p><h2 id="16fb" class="md km ht bd kn me mf mg kr mh mi mj kv jr mk ml kx jv mm mn kz jz mo mp lb mq bi translated">情况2:当P(x) &gt;&gt; (1 -P(x))时</h2><p id="3ae5" class="pw-post-body-paragraph ji jj ht jk b jl ld iu jn jo le ix jq jr lf jt ju jv lg jx jy jz lh kb kc kd hb bi translated">在这种情况下，当一个类别标签比另一个类别标签占优势时，熵会减少。</p><p id="28b2" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">h(X)=-(0.99 * log(0.99))-(0.01 * log(0.01))= 0.0801</p><h2 id="d058" class="md km ht bd kn me mf mg kr mh mi mj kv jr mk ml kx jv mm mn kz jz mo mp lb mq bi translated">情况3:当P(x) = 1时</h2><p id="b1e2" class="pw-post-body-paragraph ji jj ht jk b jl ld iu jn jo le ix jq jr lf jt ju jv lg jx jy jz lh kb kc kd hb bi translated">当只有一个类别标签时，则熵为“0”</p><p id="ea10" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">从上面的例子中我们看到，熵给了我们一个关于数据的随机性或分布的概念。高熵意味着点是平均分布的，我们算法的目标是减少这种不确定性。</p><h1 id="e47e" class="kl km ht bd kn ko li kq kr ks lj ku kv iz lk ja kx jc ll jd kz jf lm jg lb lc bi translated">结论</h1><p id="df02" class="pw-post-body-paragraph ji jj ht jk b jl ld iu jn jo le ix jq jr lf jt ju jv lg jx jy jz lh kb kc kd hb bi translated">即使对于高斯分布，我们也可以从它的PDF中确定熵。如果PDF过于“<em class="lx">尖峰化</em>，这意味着，与另一个正态分布相比，平均值附近有更多的相似值，而另一个正态分布具有更高的分布。</p><p id="bc7a" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">我希望，决策树的基础现在已经很清楚了。我们还学习了熵的含义以及如何计算熵。在下一部分中，我们将学习更多与DT相关的定义，并了解如何构建决策树。</p><p id="8d46" class="pw-post-body-paragraph ji jj ht jk b jl jm iu jn jo jp ix jq jr js jt ju jv jw jx jy jz ka kb kc kd hb bi translated">参考资料:</p><div class="hh hi ez fb hj mr"><a href="https://web.mit.edu/16.unified/www/FALL/thermodynamics/notes/node56.html" rel="noopener  ugc nofollow" target="_blank"><div class="ms ab dw"><div class="mt ab mu cl cj mv"><h2 class="bd hu fi z dy mw ea eb mx ed ef hs bi translated">7.3熵的统计定义</h2><div class="my l"><h3 class="bd b fi z dy mw ea eb mx ed ef dx translated">的列表是对系统中的随机性的精确描述，但是几乎所有的量子态的数目都…</h3></div><div class="mz l"><p class="bd b fp z dy mw ea eb mx ed ef dx translated">web.mit.edu</p></div></div><div class="na l"><div class="nb l nc nd ne na nf hp mr"/></div></div></a></div><div class="hh hi ez fb hj mr"><a href="https://en.wikipedia.org/wiki/Entropy_%28information_theory%29" rel="noopener  ugc nofollow" target="_blank"><div class="ms ab dw"><div class="mt ab mu cl cj mv"><h2 class="bd hu fi z dy mw ea eb mx ed ef hs bi translated">熵(信息论)</h2><div class="my l"><h3 class="bd b fi z dy mw ea eb mx ed ef dx translated">信息熵是随机数据源产生信息的平均速率。对…的衡量</h3></div><div class="mz l"><p class="bd b fp z dy mw ea eb mx ed ef dx translated">en.wikipedia.org</p></div></div><div class="na l"><div class="ng l nc nd ne na nf hp mr"/></div></div></a></div><figure class="lo lp lq lr fd hk"><div class="bz dy l di"><div class="nh ni l"/></div></figure></div></div>    
</body>
</html>