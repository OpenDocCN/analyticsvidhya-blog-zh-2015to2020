<html>
<head>
<title>Mathematical Introduction to Gradient Descent Learning Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">梯度下降学习算法的数学介绍</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/mathematical-introduction-to-gradient-descent-learning-algorithm-8f1ebbff6455?source=collection_archive---------22-----------------------#2020-12-22">https://medium.com/analytics-vidhya/mathematical-introduction-to-gradient-descent-learning-algorithm-8f1ebbff6455?source=collection_archive---------22-----------------------#2020-12-22</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/72f6a0c1d514ce85f98b8e76906fa1b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XMaWRPvSAYcQf5J2"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">Pierre Van Crombrugghe 在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><h1 id="4628" class="iv iw hi bd ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js bi translated">梯度下降学习算法；</h1><blockquote class="jt ju jv"><p id="8e2d" class="jw jx jy jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku hb bi translated"><strong class="jz hj">梯度下降</strong>是——</p><p id="f93f" class="jw jx jy jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku hb bi translated">→迭代优化算法</p><p id="ae60" class="jw jx jy jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku hb bi translated">→为了找到函数的局部最小值</p><p id="1439" class="jw jx jy jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku hb bi translated">→采取较小的步骤</p><p id="3eac" class="jw jx jy jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku hb bi translated">→与当前点的函数梯度的负值(梯度的相反方向)成比例。</p></blockquote><h2 id="dc70" class="kv iw hi bd ix kw kx ky jb kz la lb jf lc ld le jj lf lg lh jn li lj lk jr ll bi translated">损失函数:</h2><p id="71fa" class="pw-post-body-paragraph jw jx hi jz b ka lm kc kd ke ln kg kh lc lo kk kl lf lp ko kp li lq ks kt ku hb bi translated">梯度下降是通过从损失函数J(w)中随机初始化的点开始采取小的小步来执行的，以最终达到其最小值。</p><p id="a6c7" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated"><em class="jy">注意:这里J(w)中的J代表雅可比——一阶导数向量。</em></p><p id="2934" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">我们假设损失函数本质上是凸的(碗形的)。这有助于我们将最小值计算视为一个凸优化问题(细节超出了当前文章的范围)。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lr"><img src="../Images/2b7a2465653633a57d971d3f05109444.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*K61yzO1dNTrA6G02.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">礼貌:塞巴斯蒂安·拉什卡</figcaption></figure><p id="baa6" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">让我们看看损失函数是如何寻找两个参数x和y的，这里向量v是基本向量x和y的线性组合。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es lw"><img src="../Images/e99ec830c428c55be44dba7880ceb36d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*eYITgeUeezuXhxM5.png"/></div></figure><h2 id="f433" class="kv iw hi bd ix kw kx ky jb kz la lb jf lc ld le jj lf lg lh jn li lj lk jr ll bi translated">重量初始化:</h2><p id="ab1c" class="pw-post-body-paragraph jw jx hi jz b ka lm kc kd ke ln kg kh lc lo kk kl lf lp ko kp li lq ks kt ku hb bi translated">梯度下降的第一步是随机初始化参数权重。这里，θ是初始参数值，δθ将通过梯度下降来计算步长。θ+δθ将是梯度下降一次迭代后经过的新θ。</p><p id="e78e" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">θ= [w，b]</p><p id="a805" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">δθ=[δw，δb]</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es lx"><img src="../Images/e0c495e83265b64f6b4f5fca5db4f78c.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*IyhTvRG51Ap4w_n43ekjFw.png"/></div></figure><p id="824d" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">下面的gif显示了权重初始化如何影响损失收敛——在深度学习中实现了几种初始化技术来优化权重初始化(超出了当前文章的范围)。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es ly"><img src="../Images/f2089a418aa8339c7389775561ebb313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/0*4cydGVgmRt_UEdJ-.gif"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">资料来源:gfycat</figcaption></figure><h2 id="0a5d" class="kv iw hi bd ix kw kx ky jb kz la lb jf lc ld le jj lf lg lh jn li lj lk jr ll bi translated">参数更新和学习率:</h2><p id="79ed" class="pw-post-body-paragraph jw jx hi jz b ka lm kc kd ke ln kg kh lc lo kk kl lf lp ko kp li lq ks kt ku hb bi translated">然而，在实时中，我们从不直接更新δθ，我们总是考虑一个步长控制因子，即“学习率”，用希腊字母eta (η)表示。</p><p id="4401" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated"><strong class="jz hj">学习算法方程:</strong></p><blockquote class="lz"><p id="46c3" class="ma mb hi bd mc md me mf mg mh mi ku dx translated">η →学习速率(每次迭代的步长)</p><p id="5e88" class="ma mb hi bd mc md me mf mg mh mi ku dx translated">θnew = θ + η。Δθ</p></blockquote><figure class="mk ml mm mn mo ij er es paragraph-image"><div class="er es mj"><img src="../Images/631339d0ee1b965461e5c91a3b252fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*8UrjxWQjeIoCPUZiw46MSg.png"/></div></figure><p id="b191" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">参数更新如何基于计算的梯度工作—</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/5356e25ed230accb8cac1f11069bd3f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*moKKv86NFlia7G-D.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">资料来源:gfycat</figcaption></figure><h2 id="e88c" class="kv iw hi bd ix kw kx ky jb kz la lb jf lc ld le jj lf lg lh jn li lj lk jr ll bi translated">如何用本金的方式求δθ？</h2><p id="8cc1" class="pw-post-body-paragraph jw jx hi jz b ka lm kc kd ke ln kg kh lc lo kk kl lf lp ko kp li lq ks kt ku hb bi translated">现在的问题是如何以主要的方式获得递增的步长，我们需要一个过程来帮助我们确定正确的步长，以确保损失总是递增地减少。</p><ul class=""><li id="cc7a" class="mq mr hi jz b ka kb ke kf lc ms lf mt li mu ku mv mw mx my bi translated">目标是在每一步得到δθ，这样损耗只会逐渐减少。</li></ul><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es mz"><img src="../Images/1ec6f2dd7be5c204d3dc543b3c5e4c15.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*XyEIKH-ssqJqW3ONlJvC4g.png"/></div></figure><ul class=""><li id="6d44" class="mq mr hi jz b ka kb ke kf lc ms lf mt li mu ku mv mw mx my bi translated">这就是“梯度下降”算法发挥作用的地方。</li></ul><h2 id="9da4" class="kv iw hi bd ix kw kx ky jb kz la lb jf lc ld le jj lf lg lh jn li lj lk jr ll bi translated">渐变:</h2><p id="be41" class="pw-post-body-paragraph jw jx hi jz b ka lm kc kd ke ln kg kh lc lo kk kl lf lp ko kp li lq ks kt ku hb bi translated">梯度也被称为斜率，换句话说，增量上升超过增量运行。</p><blockquote class="lz"><p id="96d5" class="ma mb hi bd mc md me mf mg mh mi ku dx translated">梯度=上升/下降</p></blockquote><p id="5b0d" class="pw-post-body-paragraph jw jx hi jz b ka na kc kd ke nb kg kh lc nc kk kl lf nd ko kp li ne ks kt ku hb bi translated">为了计算增量上升和增量运行，我们利用泰勒级数展开。</p><h2 id="e1d2" class="kv iw hi bd ix kw kx ky jb kz la lb jf lc ld le jj lf lg lh jn li lj lk jr ll bi translated">泰勒级数:</h2><ul class=""><li id="3790" class="mq mr hi jz b ka lm ke ln lc nf lf ng li nh ku mv mw mx my bi translated">如果有一个函数，并且知道该函数在点x的值，那么该函数在新的点x+δx的值可以给出如下:</li></ul><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ni"><img src="../Images/d30ee686c61cf1ebeed6a802c9d2f74d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/0*eNh9FofY4stCe3Wp.png"/></div></div></figure><p id="69ce" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">因此，我们将迈出一小步δx，并确保在这一小步之后，损耗会降低。</p><p id="b80c" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated"><em class="jy">f(x+δx)= f(x)+某量</em></p><p id="f6e2" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">让我们把损失函数方程改写成泰勒展开式。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nj"><img src="../Images/5b8e23a577e5918c5a924076a705d6ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n5uURPBHHnkPkr94GVj2mw.png"/></div></div></figure><p id="8d18" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">上面给出的泰勒级数是针对标量变量的，让我们调整一下向量的方程，因为我们在梯度下降中处理向量。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es nk"><img src="../Images/19f6676270e0217ebb6bebeb42cb0e4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*MdwYyTHcdLkSoQ4k1JTaNA.png"/></div></figure><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nl"><img src="../Images/b5221b2e5f7cc13a279a0c0f3d96934d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xarg9SHUG6e7JxKQqBfvPw.png"/></div></div></figure><p id="927c" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated"><em class="jy">其中δθ= u；u也被称为“变化向量”。</em></p><p id="ceb3" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">由于学习率(η)值约为0.01–0.001，因此第三至第n项的值非常小，可以忽略不计。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es nm"><img src="../Images/ba29026c63e2ced360d311240d52615c.png" data-original-src="https://miro.medium.com/v2/resize:fit:942/format:webp/1*WOIKnmZXtVq_Ti6beVwj0Q.png"/></div></figure><p id="df4e" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">为了使新的损失低于以前的损失，下面的数量应该是负数。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es nn"><img src="../Images/35b23a0ddce199f16588b6d33e51627f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uK7o9MF-Xwvgyu_WoReFiQ.png"/></div></div></figure><p id="ba91" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">方向“u”应该与梯度向量成180度。</p><blockquote class="lz"><p id="ac5d" class="ma mb hi bd mc md me mf mg mh mi ku dx translated">=180 →向与梯度相反的方向移动。</p></blockquote><p id="cb0e" class="pw-post-body-paragraph jw jx hi jz b ka na kc kd ke nb kg kh lc nc kk kl lf nd ko kp li ne ks kt ku hb bi translated">让我们快速总结一下梯度下降&amp;参数更新规则。</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es no"><img src="../Images/ba85fc061de88854c9d1f93f232632d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vUhiIPxMqo7qHIyQy2I2NQ.png"/></div></div></figure><h2 id="a81b" class="kv iw hi bd ix kw kx ky jb kz la lb jf lc ld le jj lf lg lh jn li lj lk jr ll bi translated">完整的学习算法:</h2><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es np"><img src="../Images/a3d00ca77cdd1b80e7209abf61704020.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*rm3Dw3enaYkjArpvB_usHQ.png"/></div></figure><h2 id="faa0" class="kv iw hi bd ix kw kx ky jb kz la lb jf lc ld le jj lf lg lh jn li lj lk jr ll bi translated">计算梯度怎么求偏导数？</h2><p id="5c54" class="pw-post-body-paragraph jw jx hi jz b ka lm kc kd ke ln kg kh lc lo kk kl lf lp ko kp li lq ks kt ku hb bi translated">通过获取损失函数相对于权重(<em class="jy">δw</em>)和相对于偏差(<em class="jy">δb</em>)项的偏导数来计算梯度。一阶导数将解释讨论中的点是最大值还是最小值。因此，我们假设损失函数本质上是凸的，因此梯度为零的点是最小值点。</p><blockquote class="lz"><p id="0c0e" class="ma mb hi bd mc md me mf mg mh mi ku dx translated">微分链法则用于计算损失函数相对于权重和偏差项的偏导数</p></blockquote><figure class="mk ml mm mn mo ij er es paragraph-image"><div class="er es nq"><img src="../Images/3b6a951385a5d25c7ee976dd0f18bd8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*Bb7QFXxA028GNpD3suSjmQ.png"/></div></figure><p id="69dc" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">代入上式中f(x)的值，</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es nr"><img src="../Images/ab4afbbcf3ae5a29a0737eac58da82cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*kBrKeuMDQlaNwAPMzbxuHw.png"/></div></figure><p id="2378" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">现在，让我们计算sigmoid函数的一阶导数，</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ns"><img src="../Images/5da58d2904920876e1442667735fdaba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5CyCS6bA7sV-i9CJU8VryQ.png"/></div></div></figure><p id="6ab5" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">将其代入∇w方程，</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es nt"><img src="../Images/469643c92853be90b3e75ed045df22f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*MS6tdVkjJTvV4xOcr25BBA.png"/></div></figure><p id="9608" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">这意味着∇w可以很容易地只用f(x)，x和y来计算——不需要任何实际的导数。</p><p id="cdf5" class="pw-post-body-paragraph jw jx hi jz b ka kb kc kd ke kf kg kh lc kj kk kl lf kn ko kp li kr ks kt ku hb bi translated">类似地，我们可以看到∇b方程简化为下面的表达式(除了∇w).的x项</p><figure class="ls lt lu lv fd ij er es paragraph-image"><div class="er es nu"><img src="../Images/5ac47e944007078e8e9131301f408a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*YnNwbMYUPXGdJkPKAFbTug.png"/></div></figure><h2 id="724c" class="kv iw hi bd ix kw kx ky jb kz la lb jf lc ld le jj lf lg lh jn li lj lk jr ll bi translated">两个参数梯度下降的Python代码:</h2><p id="d471" class="pw-post-body-paragraph jw jx hi jz b ka lm kc kd ke ln kg kh lc lo kk kl lf lp ko kp li lq ks kt ku hb bi translated"><em class="jy">假设:</em></p><ul class=""><li id="022b" class="mq mr hi jz b ka kb ke kf lc ms lf mt li mu ku mv mw mx my bi translated"><em class="jy">参数个数:2个</em></li><li id="b8a7" class="mq mr hi jz b ka nv ke nw lc nx lf ny li nz ku mv mw mx my bi translated"><em class="jy">历元数:1000 </em></li><li id="441c" class="mq mr hi jz b ka nv ke nw lc nx lf ny li nz ku mv mw mx my bi translated"><em class="jy"> X = [0.5，2.5]；Y = [0.2，0.9] </em></li><li id="4a96" class="mq mr hi jz b ka nv ke nw lc nx lf ny li nz ku mv mw mx my bi translated"><em class="jy"> eta </em>:学习率</li></ul><figure class="ls lt lu lv fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es oa"><img src="../Images/431bc02e163669f75e8f06c49421c5dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*7sslmJVu1V9Sc749t1LBSw.png"/></div></div></figure><blockquote class="jt ju jv"><p id="b45f" class="jw jx jy jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku hb bi translated"><strong class="jz hj">引文</strong>:</p><p id="2be8" class="jw jx jy jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku hb bi translated">文章内容和正文是基于课程材料和我对IIT-马德拉斯大学教授PadhAI深度学习课程的理解:Mitesh Khapra教授和Pratyush Kumar教授。</p><p id="26cf" class="jw jx jy jz b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku hb bi translated">链接:<a class="ae iu" href="https://padhai.onefourthlabs.in/" rel="noopener ugc nofollow" target="_blank">https://padhai.onefourthlabs.in/</a></p></blockquote></div></div>    
</body>
</html>