<html>
<head>
<title>Web Crawling with Scrapy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">网络爬行与Scrapy</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/web-crawling-with-scrapy-f4d93c1bfcc7?source=collection_archive---------5-----------------------#2020-01-10">https://medium.com/analytics-vidhya/web-crawling-with-scrapy-f4d93c1bfcc7?source=collection_archive---------5-----------------------#2020-01-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/736776c87b7b10c45cafeb1d150d5636.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7KVe2szj1rjt1_Jlmdznkw.png"/></div></div></figure><p id="6c44" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在数据分析中，最重要的资源是数据本身。由于web爬行被定义为<em class="jo">“以编程方式浏览网页集合并提取数据”</em>，因此在没有官方API的情况下收集数据是一个有用的技巧。</p><p id="cefd" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在本文中，我们将讨论以下主题:</p><ol class=""><li id="6c63" class="jp jq hi is b it iu ix iy jb jr jf js jj jt jn ju jv jw jx bi translated">设置碎片</li><li id="4d16" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ju jv jw jx bi translated">从网页抓取数据</li><li id="8895" class="jp jq hi is b it jy ix jz jb ka jf kb jj kc jn ju jv jw jx bi translated">处理无限滚动页面</li></ol><h2 id="a81f" class="kd ke hi bd kf kg kh ki kj kk kl km kn jb ko kp kq jf kr ks kt jj ku kv kw kx bi translated">设置碎片</h2><p id="89dd" class="pw-post-body-paragraph iq ir hi is b it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn hb bi translated"><a class="ae ld" href="https://scrapy.org/" rel="noopener ugc nofollow" target="_blank"> Scrapy </a>在使用python进行网页抓取时是一个强大的工具。在我们的命令行中，执行:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="d3c9" class="kd ke hi lj b fi ln lo l lp lq">pip install <!-- -->scrapy</span></pre><h1 id="98d1" class="lr ke hi bd kf ls lt lu kj lv lw lx kn ly lz ma kq mb mc md kt me mf mg kw mh bi translated">我们的目标</h1><p id="f270" class="pw-post-body-paragraph iq ir hi is b it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn hb bi translated">在本文中，我们将以<a class="ae ld" href="https://www.yummly.com/" rel="noopener ugc nofollow" target="_blank"> Yummly </a>为例。我们的目标是从每个食谱中下载配料，用于进一步的文本挖掘(参见相关的<a class="ae ld" href="https://www.kaggle.com/c/whats-cooking-kernels-only/" rel="noopener ugc nofollow" target="_blank"> kaggle竞赛</a>)现在是时候创建我们的蜘蛛了:)</p><h2 id="5636" class="kd ke hi bd kf kg kh ki kj kk kl km kn jb ko kp kq jf kr ks kt jj ku kv kw kx bi translated">创造我们的第一只蜘蛛</h2><p id="1a17" class="pw-post-body-paragraph iq ir hi is b it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn hb bi translated">创建一个名为<code class="du mi mj mk lj b">crawler.py</code>的python文件:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="0e9b" class="kd ke hi lj b fi ln lo l lp lq">import scrapy</span><span id="806e" class="kd ke hi lj b fi ml lo l lp lq">class RecipeSpider(scrapy.Spider):<br/> name = "recipe_spider"<br/> start_urls = ["<a class="ae ld" href="https://www.yummly.com/recipes" rel="noopener ugc nofollow" target="_blank">https://www.yummly.com/recipes</a>"]</span></pre><p id="d909" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里我们创建一个继承自<code class="du mi mj mk lj b">scrapy.Spider</code>的类(在库中，<a class="ae ld" href="https://docs.scrapy.org/en/latest/topics/spiders.html" rel="noopener ugc nofollow" target="_blank"> Spider </a>已经定义了跟踪路径和数据抓取的方法。)我们需要给我们的蜘蛛一个<code class="du mi mj mk lj b">name</code>来通知Scrapy蜘蛛是如何定位和实例化的。(如果没有分配<code class="du mi mj mk lj b">name</code>，将出现显示<code class="du mi mj mk lj b">No spider found in file</code>的错误。)<code class="du mi mj mk lj b">start_urls</code>是我们要抓取的网址列表。(这里只需要Yummly的菜谱网址。)</p><p id="2329" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">让我们运行它！由于Scrapy有自己的命令行界面，我们需要运行:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="9eae" class="kd ke hi lj b fi ln lo l lp lq">scrapy runspider crawler.py</span></pre><p id="920a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后嘣！我们的输出是这样的:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="8951" class="kd ke hi lj b fi ln lo l lp lq">2020-01-08 20:27:51 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: scrapybot)<br/>2020-01-08 20:27:51 [scrapy.crawler] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}</span><span id="e6fe" class="kd ke hi lj b fi ml lo l lp lq">...</span><span id="c1af" class="kd ke hi lj b fi ml lo l lp lq">2020-01-08 20:27:52 [scrapy.core.engine] INFO: Spider opened<br/>2020-01-08 20:27:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)<br/>2020-01-08 20:27:54 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET <a class="ae ld" href="https://www.yummly.com/recipes" rel="noopener ugc nofollow" target="_blank">https://www.yummly.com/recipes</a>&gt; (referer: None)<br/>2020-01-08 20:27:54 [scrapy.core.scraper] ERROR: Spider error processing &lt;GET <a class="ae ld" href="https://www.yummly.com/recipes" rel="noopener ugc nofollow" target="_blank">https://www.yummly.com/recipes</a>&gt; (referer: None)<br/>Traceback (most recent call last):<br/>  File "/usr/local/lib/python3.7/site-packages/twisted/internet/defer.py", line 654, in _runCallbacks<br/>    current.result = callback(current.result, *args, **kw)<br/>  File "/usr/local/lib/python3.7/site-packages/scrapy/spiders/__init__.py", line 80, in parse<br/>    raise NotImplementedError('{}.parse callback is not defined'.format(self.__class__.__name__))<br/>NotImplementedError: RecipeSpider.parse callback is not defined<br/><br/>2020-01-08 20:27:54 [scrapy.core.engine] INFO: Spider closed (finished)</span></pre><p id="091c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">简而言之，它初始化蜘蛛并加载所需的扩展。从我们指定的起始url，蜘蛛提取页面的内容并试图解析它。正如错误指出的，我们还没有定义我们的<code class="du mi mj mk lj b">parse</code>方法。让我们现在就做吧！</p><h2 id="f5a1" class="kd ke hi bd kf kg kh ki kj kk kl km kn jb ko kp kq jf kr ks kt jj ku kv kw kx bi translated">偷看我们的数据</h2><p id="ab91" class="pw-post-body-paragraph iq ir hi is b it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn hb bi translated">先看看我们的数据总是更好的主意。打开<a class="ae ld" href="https://www.yummly.com/recipes" rel="noopener ugc nofollow" target="_blank">网页</a>，我们在每个组件里都找到了<code class="du mi mj mk lj b">&lt;a href="/recipe/..." /&gt;</code>。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mm"><img src="../Images/0f2c30a2916c1b07815617154910001e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jseSRP0azfOhid5HjLKIzg.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated"><a class="ae ld" href="https://www.yummly.com/recipes" rel="noopener ugc nofollow" target="_blank">https://www.yummly.com/recipes</a></figcaption></figure><p id="37e1" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">点开一个菜谱后，啊哈！在标签<code class="du mi mj mk lj b">&lt;span class="ingredient"/&gt;</code>中包含了我们想要的数据！</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/9d8e4aea5e1bcc902db264bdb5180ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XN0HP6V6-jqzpgTb5Fxmyg.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated"><a class="ae ld" href="https://www.yummly.com/recipe/Grilled-Pork-Chops-with-Basil-Garlic-Rub-2261216" rel="noopener ugc nofollow" target="_blank">https://www . yummly . com/recipe/carsted-猪排-紫苏-大蒜-Rub-2261216 </a></figcaption></figure><h2 id="3186" class="kd ke hi bd kf kg kh ki kj kk kl km kn jb ko kp kq jf kr ks kt jj ku kv kw kx bi translated">从html抓取数据</h2><p id="20ca" class="pw-post-body-paragraph iq ir hi is b it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn hb bi translated">我们首先需要提取搜索页面中的菜谱链接。之后，从每个食谱中提取配料。</p><p id="6e37" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">scrapy的便利之处在于，它有<code class="du mi mj mk lj b">scrapy shell</code>帮助我们测试我们想要提取的内容(竖起大拇指！)在我们的命令行中，执行:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="65c9" class="kd ke hi lj b fi ln lo l lp lq">scrapy shell<br/>fetch("<a class="ae ld" href="https://www.yummly.com/recipes" rel="noopener ugc nofollow" target="_blank">https://www.yummly.com/recipes</a>")</span></pre><p id="b4e0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，从yummly的搜索页面提取的所有数据都存储在属性<code class="du mi mj mk lj b">response</code>(类型<code class="du mi mj mk lj b">reponse.body</code>显示全部内容~)中，因为我们需要每个食谱页面的链接(存储在<code class="du mi mj mk lj b">&lt;a href="..."/&gt;</code>)我们使用<a class="ae ld" href="https://docs.scrapy.org/en/latest/topics/selectors.html" rel="noopener ugc nofollow" target="_blank"> css选择器</a>作为:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="0779" class="kd ke hi lj b fi ln lo l lp lq">response.css(".card-ingredients::attr(href)").extract()</span></pre><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/042f41f16ea58133368d6d42e5f4054f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0MIA_Er99eEpLZ4Qow6vJg.png"/></div></div></figure><p id="24f3" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">太好了！这正是我们所需要的！测试之后，我们可以回到我们的<code class="du mi mj mk lj b">crawler.py</code>:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="7099" class="kd ke hi lj b fi ln lo l lp lq">class RecipeSpider(scrapy.Spider):<br/> name = "recipe_spider"<br/> start_urls = ["<a class="ae ld" href="https://www.yummly.com/recipes" rel="noopener ugc nofollow" target="_blank">https://www.yummly.com/recipes</a>"]</span><span id="8037" class="kd ke hi lj b fi ml lo l lp lq">def parse(self, response):<br/>  links = response.css(".card-ingredients::attr(href)").extract()<br/>  for link in links:<br/>   recipeLink = "<a class="ae ld" href="https://www.yummly.com" rel="noopener ugc nofollow" target="_blank">https://www.yummly.com</a>" + link<br/>   yield response.follow(link, callback=self.parseRecipe)<br/> <br/> def parseRecipe(self, response):<br/>  recipeName = response.css(".recipe-title::text").extract_first()<br/>  ingredients = response.css(".ingredient::text").extract()<br/>  print("recipeName = ", recipeName)<br/>  print(ingredients)</span></pre><p id="7f3f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面有两个解析函数，因为在解析搜索页面和每个配方页面时逻辑是不同的。第一个<code class="du mi mj mk lj b">parse</code>将被执行。在获得相应食谱的链接后，我们使用<code class="du mi mj mk lj b">reponse.follow()</code>作为创建更多请求的快捷方式。(也将解析方法指定为<code class="du mi mj mk lj b">self.parseRecipe</code>)这里我们使用<code class="du mi mj mk lj b">yield</code>来产生一个请求序列。(<a class="ae ld" href="https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do" rel="noopener ugc nofollow" target="_blank">了解有关产量的更多信息</a>)</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/2b09fa844da90c024ad3834a43032ca9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zT_XAVIvmmZYadvyd3Jeqw.png"/></div></div></figure><p id="4a26" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">运行完<code class="du mi mj mk lj b">scrapy runspider crawler.py</code>后，我们就可以得到每个菜谱中的食材了。太棒了。</p><h2 id="e251" class="kd ke hi bd kf kg kh ki kj kk kl km kn jb ko kp kq jf kr ks kt jj ku kv kw kx bi translated">无限滚动页面</h2><p id="0db9" class="pw-post-body-paragraph iq ir hi is b it ky iv iw ix kz iz ja jb la jd je jf lb jh ji jj lc jl jm jn hb bi translated">但是食谱的数量看起来令人怀疑。嗯…为什么只有37个食谱？</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mu"><img src="../Images/a636f5244dce96373068025859191b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-_nuRX7N0faoj-xkuae_Mg.png"/></div></div></figure><p id="7961" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">回到我们的搜索页面向下滚动，我们突然发现这是一个无限滚动的页面。但是不用担心！无限滚动必须需要分页或调用更多项目的机制。我们可以检查在<code class="du mi mj mk lj b">Network</code>部分发生了什么:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mv"><img src="../Images/3457f74abd4dbc5e10adbc77d9c6f4eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Iz0a-vQDA7yPDBgmMNFdA.png"/></div></div><figcaption class="mn mo et er es mp mq bd b be z dx translated"><a class="ae ld" href="https://www.yummly.com/recipes" rel="noopener ugc nofollow" target="_blank">https://www.yummly.com/recipes</a></figcaption></figure><p id="9259" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正在检查请求URL:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="b8af" class="kd ke hi lj b fi ln lo l lp lq">https://mapi.yummly.com/mapi/v17/content/search?solr.seo_boost=new&amp;<strong class="lj hj">start=1&amp;maxResult=37</strong>&amp;fetchUserCollections=false&amp;allowedContent=single_recipe&amp;allowedContent=suggested_search&amp;allowedContent=related_search&amp;allowedContent=article&amp;allowedContent=video&amp;allowedContent=generic_cta&amp;guided-search=true&amp;solr.view_type=search_internal</span></pre><p id="bebe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">看吧！在查询字符串中，它将其返回项限制为少于37个。所以我们的第一个尝试是将<code class="du mi mj mk lj b">maxResult</code>改为我们想要查询的食谱数量:(同时修改我们的解析逻辑~)</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="78a8" class="kd ke hi lj b fi ln lo l lp lq">import scrapy<br/>import json</span><span id="a69c" class="kd ke hi lj b fi ml lo l lp lq">class RecipeSpider(scrapy.Spider):<br/> name = "recipe_spider"<br/> requiresRecipeNum = 50<br/> recipeNum = 0<br/> start_urls = ["https://mapi.yummly.com/mapi/v17/content/search?solr.seo_boost=new&amp;start=1&amp;maxResult=" + str(requiresRecipeNum) + "&amp;fetchUserCollections=false&amp;allowedContent=single_recipe&amp;allowedContent=suggested_search&amp;allowedContent=related_search&amp;allowedContent=article&amp;allowedContent=video&amp;allowedContent=generic_cta&amp;guided-search=true&amp;solr.view_type=search_internal"]</span><span id="077d" class="kd ke hi lj b fi ml lo l lp lq">def parse(self, response):<br/> data = json.loads(response.body)</span><span id="6af6" class="kd ke hi lj b fi ml lo l lp lq"> for item in data.get('feed', []):<br/>   link = "<a class="ae ld" href="https://www.yummly.com/" rel="noopener ugc nofollow" target="_blank">https://www.yummly.com/</a>" + item.get("tracking-id")<br/>   content = item.get('content')<br/>   recipeName = content.get('details').get('name')</span><span id="f329" class="kd ke hi lj b fi ml lo l lp lq">   self.recipeNum += 1<br/>   print(self.recipeNum," : ", recipeName)</span><span id="4363" class="kd ke hi lj b fi ml lo l lp lq">   for i in content.get('ingredientLines', []):<br/>    print(i.get('ingredient'))</span></pre><p id="d010" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里我们使用<code class="du mi mj mk lj b">json.get()</code>而不是前面的<code class="du mi mj mk lj b">css selector</code>的原因是因为我们的查询url已经改变，导致不同的响应。我们从<code class="du mi mj mk lj b">https://mapi.yummly.com/mapi/...</code>得到的回应是一大包json:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mw"><img src="../Images/6f76b38e308be1a680b269d06d2292e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-wDcMVAr0kDP3MYbYxjIZQ.png"/></div></div></figure><p id="91fa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最好将原始数据粘贴到<a class="ae ld" href="http://jsonviewer.stack.hu/" rel="noopener ugc nofollow" target="_blank">在线JSON查看器</a>中，以增强可读性。</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mx"><img src="../Images/6184f488f69d6f068aaf9cb027be33bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8XJ1g6MEb0tMfDTvz6Ax6w.png"/></div></div></figure><p id="66c7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">但是我刚刚发现请求10000个食谱是不可行的(也许后面有一个超时机制，所以我不能爬回任何东西&gt;</p><p id="90d0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Another attempt will be utilizing  【T3】  (because it is a list!) So we can do something like:</p><pre class="le lf lg lh fd li lj lk ll aw lm bi"><span id="49d9" class="kd ke hi lj b fi ln lo l lp lq">def GenerateUrl(requiresRecipeNum):<br/> starts = []<br/> for i in range(requiresRecipeNum // 10):<br/>  apiStr = "<a class="ae ld" href="https://mapi.yummly.com/mapi/v17/content/search?solr.seo_boost=new&amp;start=" rel="noopener ugc nofollow" target="_blank">https://mapi.yummly.com/mapi/v17/content/search?solr.seo_boost=new&amp;start=</a>" + str(1+i*10) + "&amp;maxResult=10&amp;fetchUserCollections=false&amp;allowedContent=single_recipe&amp;allowedContent=suggested_search&amp;allowedContent=related_search&amp;allowedContent=article&amp;allowedContent=video&amp;allowedContent=generic_cta&amp;guided-search=true&amp;solr.view_type=search_internal"<br/>  starts += [apiStr]<br/> return starts</span><span id="e107" class="kd ke hi lj b fi ml lo l lp lq">class RecipeSpider(scrapy.Spider):<br/> name = "recipe_spider"<br/> requiresRecipeNum = 10000<br/> start_urls = GenerateUrl(requiresRecipeNum)<br/> ...</span></pre><p id="641b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">In this way, we have 1000 urls in  【T4】 . Each only requests for 10 recipes:</p><figure class="le lf lg lh fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es my"><img src="../Images/5d6614ea63ec213c2ba7c650f260d1e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6aQBclfwAmKNQUUqb7vz0Q.png"/></div></div></figure><p id="3929" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">and it works! <a class="ae ld" href="https://blog.scrapinghub.com/2016/06/22/scrapy-tips-from-the-pros-june-2016" rel="noopener ugc nofollow" target="_blank">这里</a>是另一篇处理不同类型的无限滚动页面的文章。</p><p id="c7aa" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">希望这篇文章有所帮助。快乐编码:)</p><h1 id="dec9" class="lr ke hi bd kf ls lt lu kj lv lw lx kn ly lz ma kq mb mc md kt me mf mg kw mh bi translated">参考</h1><ul class=""><li id="cb4f" class="jp jq hi is b it ky ix kz jb mz jf na jj nb jn nc jv jw jx bi translated"><a class="ae ld" href="https://www.digitalocean.com/community/tutorials/how-to-crawl-a-web-page-with-scrapy-and-python-3" rel="noopener ugc nofollow" target="_blank">如何用Scrapy和Python 3抓取网页</a></li></ul><p id="7515" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">*本文所有内容仅供学术使用*</p></div></div>    
</body>
</html>