<html>
<head>
<title>Reinforcement Learning — What, Why, and How.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习——什么，为什么，如何。</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/reinforcement-learning-what-why-and-how-5b27fb0afc1b?source=collection_archive---------14-----------------------#2020-06-11">https://medium.com/analytics-vidhya/reinforcement-learning-what-why-and-how-5b27fb0afc1b?source=collection_archive---------14-----------------------#2020-06-11</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="8c78" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当谈到机器学习的类型和方法时，强化学习占有独特和特殊的地位。这是第三种类型的机器学习，一般来说可以称为“从经验中学习”。</p><p id="9495" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在这篇文章中，我将尝试回答三个关于强化学习的基本但重要的问题。</p><h1 id="a69f" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">什么是强化学习？</strong></h1><p id="fa91" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">让我们看一点细节，为此，我想从定义、直觉开始揭示三个方面，然后我将简单地谈谈RL代理的基本元素和特征。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/3b2d673c89cba7bb812347501632fcdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vbYBwGslht3jwyOo2k6SgQ.png"/></div></div></figure><p id="0963" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">定义</strong></p><p id="0e9a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗯，我们周围有各种各样的强化学习的定义，例如，它说，强化学习是学习做什么，如何将情况映射到行动，以便最大化奖励信号。</p><p id="587a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一个版本稍微详细一些，说强化学习是一种ML技术，它涉及一个代理在一个环境中通过选择预定义的动作来行动，目标是最大化数字奖励。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ks"><img src="../Images/53606f45919b2a767293f23ad5b8408f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xrXQXnyrirKMu44Rk_pQrg.png"/></div></div></figure><p id="e5f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在从各种各样的定义中，我们可以有这些关键词来定义强化学习，它是一种<em class="kt"> ML类型</em>，它涉及一个<em class="kt">代理</em>在<em class="kt">环境</em>中交互，感知<em class="kt">状态</em>，采取<em class="kt">行动</em>，然后为环境采取的行动获得<em class="kt">奖励</em>，目的是使奖励最大化。<em class="kt">那么，这其中的学问在哪里呢？</em></p><p id="1e44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好吧，学习就是政策，让我们看看这个定义的一个实例。</p><p id="3b36" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，在下面的例子中，有一个智能体(这个机器人)可以使用它的传感器进行观察，然后根据策略选择动作并触发。通过采取这一行动，它从代理学习的环境中获得负回报，并因此更新它的策略。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ku"><img src="../Images/45407cf75bbc4adb3c905f2f7c748d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4kSuJCYhcEKg6rM8te9Mbg.png"/></div></div></figure><p id="d5e0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理现在知道，在这个特定的状态下，它需要避免选择火灾，因此选择其他行动。这种通过经验来强化学习的过程持续多次迭代，这些迭代被称为情节，直到代理学习到最优策略。</p><p id="d028" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们来谈谈使用强化学习背后的直觉。</p><p id="5d81" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">嗯，总的来说，如果我们看到定义人工智能与商业企业相关性的四个视角。这从感知开始，感知涉及获取数据、大数据/大型数据/流数据/批量数据、对数据进行分析，如报告等。</p><p id="3125" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后是推理，我们有统计建模，机器学习，深度学习。因此，如果我们看到目前，大多数行业都集中在这两个方面，即感知和推理，因为这些是目前最成熟的。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kv"><img src="../Images/8d2ce3dbfaa5267bac08d19631e9c1ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gkGqb-N5iOt8ky88Q2yyXA.png"/></div></div></figure><p id="4e7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，其他前瞻性的观点是决策和采取行动，我们看到强化学习属于决策类。</p><p id="a053" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">想象一下，当企业能够做出决策，而不仅仅是获得洞察力(从数据中推断出来)时，整个周转时间、效率和生产率将会成倍增长。</p><p id="4d02" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这就是强化学习的力量所在。</p><p id="0a72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，让我们看看定义RL代理的基本元素是什么。RL代理可以包括这些组件中的一个或多个，</p><p id="3892" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">A)决定代理行为的策略(强制策略)</p><p id="dd6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b)一个价值函数，如果它存在，意味着政策是隐含的。</p><p id="4713" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">c)模型是一种主体自己对环境的理解，主体可以是无模型的，也可以是有模型的。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kw"><img src="../Images/3da1884eb9a04f63b43df8f29e2e5f47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M6zHarAIvXaOUVDrxqKNBw.png"/></div></div></figure><p id="613e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">以下是强化学习的典型特征:</p><p id="853f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，没有监督者，即强化学习代理不处理指导性反馈，而是由标量奖励信号确定的评估性反馈。</p><p id="4a83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二，有延迟的回报，或估计的回报，例如，考虑这个类比，我们可能有一个基于RL的下棋代理，现在一个特定的移动或移动序列，可能会导致赢或输，但不一定是在那个特定的移动之后。(其实那是即时RL或者K武装土匪的特例)</p><p id="3c9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第三，在RL中，时间真的很重要，我们将在讨论RL中的不同方法时看到这一点。</p><p id="fb72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第四，代理人的行为会影响后续状态，从而影响其获得的后续经验，这就是我们将看到探索和开发方法(也称为试验和搜索)的重要性的地方。</p><p id="41ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在让我们快速看一下RL代理的关键元素的例子。</p><p id="0eb9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">策略决定代理的行为，即它决定代理在特定状态下应该采取什么动作。例如，在这个迷宫中，如果一个代理在下一个单元，代理知道，它应该左转。代理最终学习最优策略，该策略对应于最佳可能(最优)动作的地图，该最佳可能(最优)动作对应于它以最大化其回报为目标而访问的州。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kx"><img src="../Images/4ff6a83615482eae670e71229022f52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TWzOvYSgUUCxhC6NeA9V0Q.png"/></div></div></figure><p id="458f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">另一方面，价值函数有助于确定对未来回报的预测，并用于评估特定状态或对应于状态的行为的良好性/价值。换句话说，它回答了“在特定状态下有多好，或者在特定状态下采取任何行动有多好”这个问题。例如，在同一个迷宫示例中，由于对应于起点左侧的单元与其右侧的单元(值为-15)相比具有较小的值，代理知道它应该从具有-16的单元移动到-15，并最终移动到具有最大值的单元，该最大值是最终状态(接近目标)并具有对应于-1的值。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ky"><img src="../Images/bba61bea41a6f3b8d926659edb2ac369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_lAUOU9Mw9tfuc9GkwooSw.png"/></div></div></figure><p id="6a12" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后，我们有模型，就像我们人类一样，那是我们自己对世界的理解。例如，迷宫的这个视图是代理自己感知的整个迷宫的内部视图，这就是为什么我们看不到所有的轨迹。</p><p id="2ac4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">正如我前面提到的，一个代理可能有一个模型，也可能没有模型&amp;完全学习探索和开发的方法。</p><p id="11cd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这使学习成为学习和适应动态环境的一种独特而有效的方式。</p><p id="62de" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，当我们理解了强化学习的不同方面，让我们来回答这个重要的问题，为什么我们还要费心去学习强化学习？</p><h1 id="ab77" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">为什么要强化学习？</h1><p id="0987" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">“为什么”这个概念背后可能会有很多原因，我有以下两个原因，</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kz"><img src="../Images/4a784eec186d72c6037353d011a773ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TLj4AQBKNFGxChlE_J5q-w.png"/></div></div></figure><p id="e02e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，RL代理通过接受奖励和惩罚的连续过程进行学习，这使他们能够强健地接受训练并应对不可预见的环境。</p><p id="6a76" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们举这个例子，在强化学习的情况下，这都是关于经验学习的，这就是为什么我们说，让一个人尝尝鱼的味道，休息一下，他就会明白了。与监督学习不同，在监督学习中，我们需要通过数据标签等进行明确的教学/监督。</p><p id="88f0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二，如果我们看市场趋势，特别是从Gartner的人工智能炒作周期来看，2019年，强化学习登上了浪潮，尽管它仍处于创新触发阶段，但这表明下一个地盘很快将成为现实，因此这是学习的最佳时机，并为RL等技术推动的巨大未来需求做好准备</p><h1 id="09e3" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">如何强化学习？</h1><p id="03d1" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">这里有三种方法，我发现它们在强化学习中很有用。</p><p id="065c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">首先，找一本书名为《强化学习导论》的书，作者是理查德萨顿和安德鲁巴尔托。这是关于这个话题的最佳读物之一。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/bb85bdde3f5a9826e86985f3101f2e18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*refFuDTC5SWJ811KybklGQ.png"/></div></div></figure><p id="8882" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二，虽然有很多资源可供你参考，但首先你可以参考大卫·西尔弗的RL课程，这是一个大约10个视频的集合，端到端地处理RL，主要处理萨顿和巴尔托书中的内容。因此，你甚至可以观看视频，然后阅读书中的相关章节，以确认对主题的理解。</p><p id="486f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">此外，还有更多的资源，如Udacity的一个很好的对话式RL课程，而且你可以随时关注我的频道(<a class="ae la" href="https://www.youtube.com/channel/UCt8BeNe9CKSaks6XhgFulNw?view_as=subscriber" rel="noopener ugc nofollow" target="_blank">认知</a>)。</p><p id="8ba9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第三，方面是让你的手脏起来，因为如果没有动手，这个主题将变得太大，这就是openai.com将帮助的地方。在那里，您可以参考Gym来复习示例、环境、培训您的RL代理、调整现有代理等。相信我，那会很有趣的。</p><p id="aa5e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">注:观看相关视频并关注我的频道，了解关于<a class="ae la" href="https://www.youtube.com/channel/UCt8BeNe9CKSaks6XhgFulNw?view_as=subscriber" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/channel/UCt8BeNe9CKSaks6XhgFulNw</a>的最新消息</p></div></div>    
</body>
</html>