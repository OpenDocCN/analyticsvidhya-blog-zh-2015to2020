<html>
<head>
<title>How to make self-learning games using Reinforcement Learning.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何利用强化学习制作自学游戏？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-make-self-learning-games-using-reinforcement-learning-dee2f8904b71?source=collection_archive---------3-----------------------#2020-05-04">https://medium.com/analytics-vidhya/how-to-make-self-learning-games-using-reinforcement-learning-dee2f8904b71?source=collection_archive---------3-----------------------#2020-05-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="a8ba" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">介绍</h1><p id="14f1" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">你好，世界！当我开始学习强化学习时，没有多少文章解释强化学习实现背后的代码是如何工作的。大部分只是教我什么是强化学习。</p><p id="2d04" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">因此，我决定推出一个教程，帮助其他人理解我们在为街机游戏实现强化学习时使用的代码背后的直觉。在本教程中，我将触及强化学习的主题，然后一点一点地解释特定游戏的代码。</p><p id="16a9" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">本教程的先决条件是:</p><p id="e083" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">对Python面向对象编程的基本理解</p><h1 id="aaeb" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">强化学习背后的直觉</h1><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es kg"><img src="../Images/2fd6c20e5030990aa18b2a639d7293d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*aKTgWOi72h_bD2aq"/></div></div></figure><p id="a39b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">强化学习是机器学习的一个分支，它不同于监督和非监督学习，因为它不需要数据集进行训练。</p><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es ks"><img src="../Images/9c51a2dee80cc5e09084b9df59e84647.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YLEYUKoGesIf_tNv"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">图来源:<a class="ae kx" href="https://www.researchgate.net/profile/Roohollah_Amiri/publication/323867253/figure/fig2/AS:606095550738432@1521515848671/Reinforcement-Learning-Agent-and-Environment.png" rel="noopener ugc nofollow" target="_blank">https://www . researchgate . net/profile/Roohollah _ Amiri/publication/323867253/figure/fig 2/AS:606095550738432 @ 1521515848671/Reinforcement-Learning-Agent-and-environment . png</a></figcaption></figure><p id="cf03" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">强化学习包括两个常量组件和三个随每个时间步长不断变化的组件。</p><p id="743d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">常量组件包括:</p><ul class=""><li id="244f" class="ky kz hi jf b jg kb jk kc jo la js lb jw lc ka ld le lf lg bi translated">代理——通过执行不同的动作来扮演积极角色的东西。</li><li id="9c62" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">环境——代理外部的某种世界模型。它为代理提供观察，代理基于观察采取行动并为其行动获得奖励。环境基于代理所采取的动作来改变代理的状态。</li></ul><p id="3d53" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们的代理人的目标是找到一套行动，将最大限度地提高整体回报。</p><h1 id="1c8f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">开放AI健身房</h1><p id="5edd" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">对于本教程，我们使用OpenAI的健身房。OpenAI的Gym是一个工具包，可以帮助开发和比较各种强化学习算法。正如网站提到的，“我们提供环境；你提供算法。”健身房为我们提供了各种各样的环境，从基本的物理问题到不同的街机游戏。不要忘记查看他们的网站获取更多信息。https://gym.openai.com/<a class="ae kx" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"/></p><p id="63c3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">对于我们的教程，我们使用Gym提供的Atari游戏环境。而我们使用的具体环境是吃豆小姐，这是一款迷宫追逐街机游戏。</p><h1 id="9461" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">理解代码</h1><p id="a867" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">在我们深入研究代码之前，如果您还没有下面的包，请确保您已经安装了它们。</p><h2 id="a6e6" class="lm ig hi bd ih ln lo lp il lq lr ls ip jo lt lu it js lv lw ix jw lx ly jb lz bi translated">必需的包</h2><ul class=""><li id="1e01" class="ky kz hi jf b jg jh jk jl jo ma js mb jw mc ka ld le lf lg bi translated">体育馆</li><li id="3d27" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">健身房[雅达利]</li><li id="826a" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">张量流</li><li id="2fe5" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">matplotlib</li><li id="29dc" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">收集</li><li id="d543" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">随意</li><li id="19ad" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">numpy</li></ul><p id="485d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">您可以通过在终端中键入以下命令来安装该软件包。只需将“package_name”替换为包的名称。</p><pre class="kh ki kj kk fd md me mf mg aw mh bi"><span id="11f0" class="lm ig hi me b fi mi mj l mk ml">pip install “package_name”</span></pre><p id="e1b0" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在我们准备动手做一些编码工作。打开您选择的文本编辑器并导入包，如下所示。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mm mn l"/></div></figure><ul class=""><li id="aede" class="ky kz hi jf b jg kb jk kc jo la js lb jw lc ka ld le lf lg bi translated">健身房帮助我们创造一个环境和它的功能。</li><li id="79b1" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">Matplotlib绘制了我们的代理在一段时间内的性能。</li><li id="7555" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">集合存储来自环境的观察结果。</li><li id="2c0f" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">最后，所有TensorFlow软件包都有助于我们设计神经网络。</li></ul><p id="15dd" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在导入所有的包之后，我们定义一些全局变量。</p><p id="92fe" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">“集数”是我们的代理为训练自己而玩的游戏数。</p><p id="4c66" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">load_model变量决定我们是训练还是测试我们的代理。当load_model为False时，说明我们的agent是在训练而不是在测试。当代理在训练而不是测试时，神经网络在每50场比赛后保存权重。除此之外，培训和测试之间唯一的另一个重要区别是我们的代理类的超参数。</p><p id="2f3e" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">现在我们初始化一个类，并将其命名为Agent。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="a25d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们试着理解一些基本超参数背后的直觉。</p><ul class=""><li id="84b3" class="ky kz hi jf b jg kb jk kc jo la js lb jw lc ka ld le lf lg bi translated">state_size:由环境提供，是一个状态的大小。在帕克曼女士的案例中，是128。</li><li id="805c" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">action_size:是一个代理可以采取的动作的数量，它可以是连续的；然而，在我们的例子中，它是9的离散值。</li><li id="a19b" class="ky kz hi jf b jg lh jk li jo lj js lk jw ll ka ld le lf lg bi translated">ε:该值介于0和1之间。该值越接近0，我们的代理利用的就越多(用于测试)。该值越接近1，我们的代理使用探索策略(在培训中使用)的次数就越多。</li></ul><p id="610c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">初始化之后，是时候给我们的代理一个大脑了。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="2005" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们使用deque数据类型作为代理的内存，并将其存储在一个名为memory的变量中。然后创建一个名为model的变量，它存储我们构建的神经网络，以决定采取什么行动。</p><p id="a482" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">继续，我们现在给我们的代理一些功能。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="7b02" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">build_model函数不接受输入，也不产生输出。然而，它创建了一个神经网络。神经网络的输入是代理所处的状态。网络的输出是我们的代理最终能得到的所有回报的近似值。把output想象成一个表格，其中第一列是行动列表，第二列是我们的代理在游戏结束时获得的相应行动的大概奖励。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="8b57" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">第二个函数称为get_action函数，它将代理的状态作为输入。在使用神经网络之后，它返回动作，该动作具有最高可能报酬的近似值。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="ccc7" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">你们可能还记得，我们定义了一个可变内存。append_sample函数将代理的状态、代理采取的行动、代理因该行动获得的奖励、基于代理行动的下一个状态以及游戏是否结束作为其输入。之后它将所有这些信息存储在代理的存储器中。该函数不返回任何输出。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="941a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">train_model是代理类的最后一个函数。您可以将这个函数称为heavy lifter，即使它没有任何输入或输出。在这个函数中，我们从内存中随机抽取一个样本，然后训练我们创建的神经网络。随机样本的大小可以通过增加或减少超参数中batch_size的值来改变。</p><p id="6f1c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们终于完成了代理类的编码，是时候让我们的代理进入现实世界，让它自己学习了。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="4663" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们首先创建一个环境实例，并将其保存在名为env的变量中。之后，我们从环境中获取state_size和action_size。我们使用这两个变量来创建一个使用代理类的代理实例，并将其保存在名为agent的变量中。</p><p id="bcc1" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">既然我们已经把代理放在世界里面，是时候让代理在x个游戏中训练自己了，其中x是剧集的数量。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="108d" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">这个for循环中的每一次迭代都意味着我们的代理玩了一局游戏。在一局游戏中，我们的代理人有3次生命来最大化它的分数。在for循环内部，有一个while循环，它在代理没有完成时继续运行(3个生命没有结束)。这个while循环是代理计算的主要症结。在这个循环中，我们的代理根据其所处的状态采取行动，为其行动获得奖励，并根据该行动改变其状态。不仅这些信息会被发送到内存中，而且基于这些信息，神经网络会自我更新。我们的特工每死一次，就获得100的负奖励。一旦我们的代理死了三次，这个游戏的最终分数和游戏号就存储在一个列表中，这个列表是我们在for循环之外初始化的。这些信息被绘制出来，我们打印出一些关于特定游戏/剧集的统计数据。一旦循环的一次迭代结束，环境被重置，整个过程重复。如果我们训练模型，每50场比赛，神经网络的权重就会被保存。</p><p id="3671" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我们终于理解了所有的代码。这就是我们把所有东西放在一起时的样子。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="mm mn l"/></div></figure><p id="b454" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">就这样了，伙计们。现在我们保存我们的脚本，并到终端运行它。首先，导航到保存脚本的目录，通过键入下面的命令来运行它，观察操作的展开。</p><pre class="kh ki kj kk fd md me mf mg aw mh bi"><span id="c273" class="lm ig hi me b fi mi mj l mk ml">python “filename”</span></pre><figure class="kh ki kj kk fd kl er es paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="er es mo"><img src="../Images/b5234cccbc46b530d3b708246dd98fdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zkAMsDRR1Mri29RcP72U5g.png"/></div></div><figcaption class="kt ku et er es kv kw bd b be z dx translated">成功运行代码后，会弹出此窗口。</figcaption></figure><h1 id="ec12" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated">结论</h1><p id="7938" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">代码到GitHub仓库的链接是</p><div class="mp mq ez fb mr ms"><a href="https://github.com/Jash-R/ReinforcmentLearning-For-MsPacman" rel="noopener  ugc nofollow" target="_blank"><div class="mt ab dw"><div class="mu ab mv cl cj mw"><h2 class="bd hj fi z dy mx ea eb my ed ef hh bi translated">吴家浩-R/加强为微软学生学习</h2><div class="mz l"><h3 class="bd b fi z dy mx ea eb my ed ef dx translated">Hello world，这个存储库为Pacman女士实现了强化学习。我们使用Open AI的健身房来获得Pacman女士的…</h3></div><div class="na l"><p class="bd b fp z dy mx ea eb my ed ef dx translated">github.com</p></div></div><div class="nb l"><div class="nc l nd ne nf nb ng kq ms"/></div></div></a></div><p id="d15b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我也制作了一个关于这个主题的短片。请随意查看。</p><figure class="kh ki kj kk fd kl"><div class="bz dy l di"><div class="nh mn l"/></div></figure><p id="4fa0" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">我希望你已经对使用强化学习来玩街机游戏有了基本的了解。请随意试验代码以提高其性能。请注意，你需要给你的代理一些时间，让它看到一个巨大的进步。不要指望在几场训练后就成为Pacman女士世界冠军，除非你能获得灵感，自己创造一个；) .</p></div></div>    
</body>
</html>