<html>
<head>
<title>Not all TOPs are created equal</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">并非所有的陀螺都是平等的</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/not-all-tops-are-created-equal-e1911ffb4a82?source=collection_archive---------3-----------------------#2019-08-20">https://medium.com/analytics-vidhya/not-all-tops-are-created-equal-e1911ffb4a82?source=collection_archive---------3-----------------------#2019-08-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="2747" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">到底是什么限制了我深度神经网络的速度？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jj jk di jl bf jm"><div class="er es jd"><img src="../Images/6df830b04ca74a058417101fb599de96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L-Mg3ubn0e9OmuKtWeh2aQ.png"/></div></div><figcaption class="jp jq et er es jr js bd b be z dx translated">一次乘加是两次运算。一顶就是万亿次运算。</figcaption></figure><p id="8d4b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度学习处理器公司经常以每秒万亿次运算(TOP/s)或每秒万亿次乘累加指令(TMAC)等指标来强调其产品的超快的速度。这到底意味着什么，这些数字真的有用吗？</p><p id="7085" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但首先，这和深度学习有什么关系？</p><p id="c298" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们考虑一个具有3x3x100个滤波器和100个输出通道的卷积层。</p><ul class=""><li id="a6be" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">假设该图层的输入格网大小为50x50x100。所以，对于正向传递，这需要3*3*100*100*50*50 = 225，000，000台MAC，相当于450，000，000次操作，因为一台MAC就是两次操作。</li><li id="1de6" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">但是，当一个处理器公司说一个处理器每秒可以完成一定数量的MAC或OPs时，你真的会达到这个数字吗？处理器公司引用的数字是“峰值”(即理论上的最佳情况)数字。</li><li id="d2ee" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">实际上，您的里程可能会有所不同。例如，在最近一篇名为<a class="ae kh" href="https://arxiv.org/abs/1905.07346" rel="noopener ugc nofollow" target="_blank"> EMBench </a>的论文中，显示了具有相同数量MAC的两个深度神经网络(dnn)在同一计算平台上的延迟可能有10倍的差异。</li></ul><p id="661a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是什么导致了这些减速？在下文中，我们给出了一个(不完整的)问题列表，这些问题可能会阻止您的DNN在计算平台上达到理论峰值速度。我们主要关注那些限制DNN推理速度的常见问题，但其中许多也与DNN训练相关。</p><h2 id="76ec" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">问题1 —内存访问太多</h2><p id="3460" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">很容易认为代码运行的速度受到处理器运行速度的限制。然而，在几乎所有的计算平台上，内存访问都比计算慢。一个算法(或深度神经网络层)的计算与内存访问的比率可以用一个叫做<em class="li">算术强度</em>的度量来捕捉，这是由威廉<em class="li">等人</em>描述的。在<a class="ae kh" href="https://people.eecs.berkeley.edu/~kubitron/courses/cs252-S12/handouts/papers/RooflineVyNoYellow.pdf" rel="noopener ugc nofollow" target="_blank">车顶模型</a>论文中。</p><p id="cd90" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">每个计算平台都有一个特定的算术强度阈值，低于该阈值，执行速度就会受到内存访问(而不是计算)的限制。并且，每个DNN层设计都有特定的算术强度。因此，如果您的层具有较低的运算强度，那么它的执行速度很可能会受到内存而不是计算的瓶颈。</p><p id="ed39" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解决方案:</p><ul class=""><li id="4ade" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">修改你的DNN图层，使其具有更高的算术强度。比如<a class="ae kh" href="https://arxiv.org/abs/1801.04381" rel="noopener ugc nofollow" target="_blank"> MobileNetV2 </a>和<a class="ae kh" href="https://arxiv.org/abs/1807.11164" rel="noopener ugc nofollow" target="_blank"> ShuffleNetV2 </a>的MAC数量差不多，但是ShuffleNet的运算强度更高。因此，ShuffleNet在智能手机上运行时明显快于MobileNet也就不足为奇了。<strong class="ih hj">(难度等级:容易)</strong></li><li id="8511" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">在您的实现中，在将结果写到主内存之前，进行<em class="li">层融合</em>来计算多个层。例如，MobileNet的整个模块(三个卷积、一些ReLUs和一些batch-norms)可以在缓存中完成，而无需写回内存。如果你的框架使用了可以做层融合的<a class="ae kh" href="https://www.usenix.org/system/files/osdi18-chen.pdf" rel="noopener ugc nofollow" target="_blank">Tensor Virtual Machine(TVM)</a>之类的图形编译器，这可能会很容易，否则会很麻烦。<strong class="ih hj">(难度等级:高级)</strong></li><li id="9b27" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">修改你的计算平台，增加内存带宽。这在金钱和能源消耗方面都是昂贵的。或者，选择片上缓存更多的平台。<strong class="ih hj">(难度:视情况而定)</strong></li></ul><h2 id="32e6" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">问题2 —没有足够的并行性(也称为:工作饥饿)</h2><p id="56e4" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">假设您有一个能够执行30，000个并发线程的GPU。此外，还有一个1x1x10滤波器卷积、一个7x7x10输入网格和5个输出通道。</p><p id="fc03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这一层的总工作量是1*1*10*5*7*7 = 2450台MAC。这里没有足够的工作来允许设备上的30，000个线程中的每一个执行一个MAC，因此我们在这个计算过程中让一些GPU硬件空闲。当我们不使用整个GPU时，我们不太可能达到制造商声称的最佳MAC/s数。</p><p id="f19c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，这是一个有些简化的示例，实际上，您通常需要在每个线程上执行许多MAC，以使GPU在一段有意义的时间内达到饱和。</p><p id="fd14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解决方案:</p><ul class=""><li id="b7b9" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">如果可行，增加批量大小(即DNN并行处理的图像或数据样本的数量)。这对于某些实时应用程序来说是行不通的，在这些应用程序中，您需要尽可能低的延迟，并且批处理大小固定为1。但是，它可能适用于在服务器上运行的离线应用程序，或者需要并行处理许多摄像机(例如汽车上的全景摄像机)的应用程序。<strong class="ih hj">(难度:容易)</strong></li><li id="299f" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">变浅变宽。最近的ML文献中的趋势是开发更深的dnn。当在固定的计算预算上更深入时，每一层变得更薄，计算量更少，因此可并行化的工作也更少。因此，当工作饥饿是一个问题时，逆潮流而动，用每层有更多工作的较浅的dnn进行实验是有意义的。<strong class="ih hj">(难度等级:容易)</strong></li><li id="5314" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">层融合。(见上文)</li><li id="9ec9" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">降级您的硬件。使用提供更少TMAC的更便宜的GPU，因为无论如何你都不会用到它。例如，如果你在亚马逊网络服务云上运行，你可以从支持英伟达V100的<a class="ae kh" href="https://aws.amazon.com/ec2/instance-types/p3/" rel="noopener ugc nofollow" target="_blank"> P3 </a>降级到支持英伟达K80的<a class="ae kh" href="https://aws.amazon.com/ec2/instance-types/p2/" rel="noopener ugc nofollow" target="_blank"> P2 </a>。但是，如果您正在开发消费者应用程序，您可能会受到客户使用的任何设备的支配，而这是您无法控制的。<strong class="ih hj">(难度:视情况而定)</strong></li></ul><h2 id="36ce" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">问题3-等待输入数据加载</h2><p id="f0fe" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">在训练或推断过程中，将图像从相机或硬盘传输到主存储器的时间可能很长。此外，将数据从CPU内存传输到GPU或其他加速器内存的时间可能会很长。当将深度神经网络应用于高分辨率图像或体素数据(如MRI或其他医学扫描)时，数据加载可能是一个瓶颈。</p><p id="0c5a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解决方案:</p><ul class=""><li id="ac04" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">压缩您的输入图像。选择在哪里压缩和在哪里解压缩取决于您的输入/输出(I/O)瓶颈在哪里。例如，如果瓶颈是在CPU和GPU之间传输图像，那么您将在CPU上压缩，将压缩的图像发送到GPU，然后在GPU上解压缩。如果您的平台有快速压缩库，那么这应该很简单。否则，你会有很多工作要做。<strong class="ih hj">(难度等级:取决于库支持)</strong></li><li id="41e2" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">购买更快的硬件。如果磁盘是瓶颈，请购买速度更快的磁盘。如果以太网是瓶颈，升级你的以太网。如果CPU内存是瓶颈，你可以升级它。如果CPU到GPU的复制是瓶颈，请确保您使用的至少是PCIe 3。如果您拥有硬件，这可能会很简单。如果你在云中或者你正在开发客户在本地运行的应用程序，这将会更加困难。<strong class="ih hj">(难度:视情况而定)</strong></li></ul><h2 id="2dc0" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">问题4—I/O、内存和计算的重叠不佳</h2><p id="6ef6" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">现代计算平台具有重叠I/O传输、内存传输和算术运算的能力。而且，如果你正在使用一个现有的具有后端的深度学习框架，如<a class="ae kh" href="https://developer.nvidia.com/cudnn" rel="noopener ugc nofollow" target="_blank"> cuDNN </a>或<a class="ae kh" href="https://github.com/intel/mkl-dnn" rel="noopener ugc nofollow" target="_blank"> MKL-DNN </a>，那么I/O、内存和计算很可能会正确重叠。</p><p id="fd6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">但是，如果您编写自己的数据加载器来接收自定义类型的数据，那么您有责任确保发生I/O重叠，通常是在当前图像正在计算时预取下一批图像。而且，如果你为你在深度神经网络中使用的新操作编写自己的计算内核，你有责任确保内存传输和计算是重叠的。</p><p id="ee92" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解决方案:</p><ul class=""><li id="4600" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">编写自己的数据加载器时，如果可能的话，预取下一批数据。<strong class="ih hj">(难度等级:容易)</strong></li><li id="8b24" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">当编写自己的计算内核时，你可能想考虑为<a class="ae kh" href="https://en.wikipedia.org/wiki/Software_pipelining" rel="noopener ugc nofollow" target="_blank">软件流水线</a>显式编写代码，以重叠通信和计算。或者，在编译器将您的代码翻译成汇编代码后，<a class="ae kh" href="https://stackoverflow.com/questions/840321/how-can-i-see-the-assembly-code-for-a-c-program" rel="noopener ugc nofollow" target="_blank">检查汇编代码</a>以查看数据是否在使用之前被预取。<strong class="ih hj">(难度等级:高级)</strong></li></ul><h2 id="1027" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">问题5——没有利用专门的操作(因为不是所有的陀螺都是一样的)</h2><p id="67d0" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">NVIDIA V100和谷歌TPU等产品在peak TOP/s上取得突破的部分原因在于<em class="li">专业化</em>。例如，NVIDIA V100拥有“张量核”，在计算16位数字的4x4矩阵乘法时速度极快。</p><p id="d737" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">好消息是，如果您的深层神经网络层可以分解为一组可并行化的4x4矩阵乘法，并且如果您使用16位数字，您的层将运行得非常快。然而，处理器在计算大多数其他事情时相对要慢得多。</p><p id="b8c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">因此，如果我们设计我们的深度神经网络，使它们干净地细分为4x4矩阵乘法运算，它将在现代人工智能硬件上快速运行，对吗？没那么快。Google TPUv1针对<a class="ae kh" href="https://www.anandtech.com/show/12429/google-cloud-announces-cloud-tpu-beta-availability" rel="noopener ugc nofollow" target="_blank"> 256x256 </a> 8位矩阵乘法进行了优化。Google TPUv2针对多个并发的<a class="ae kh" href="https://www.anandtech.com/show/12429/google-cloud-announces-cloud-tpu-beta-availability" rel="noopener ugc nofollow" target="_blank"> 128x128 </a> 32位浮点矩阵乘法进行了优化。</p><p id="f755" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">华为麒麟970是一款智能手机芯片，包含一个神经处理单元，针对<a class="ae kh" href="https://www.anandtech.com/show/11815/huawei-mate-10-and-mate-10-pro-launch-on-october-16th-more-kirin-970-details" rel="noopener ugc nofollow" target="_blank"> 3x3 </a> <a class="ae kh" href="https://www.androidauthority.com/huawei-announces-kirin-970-797788/" rel="noopener ugc nofollow" target="_blank"> 16位浮点</a>矩阵乘法进行了优化。我们相信，针对深度神经网络优化的计算硬件将继续变得更加多样化。</p><p id="a679" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">解决方案:</p><ul class=""><li id="1662" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">承认失败。没有一个单一的深度神经网络设计能够在所有计算平台上实现最好的TOP/s。<strong class="ih hj">(难度等级:容易)</strong></li><li id="4f16" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">重新设计您的DNN，以利用在您的计算平台上有效实现的操作。有几种方法可以做到这一点。一是考虑平台的优化目标，并选择合适的DNN层尺寸。另一个是测量平台上各种层维度的延迟，并选择运行速度快的层维度。第三是将这些测量的查找表馈送到神经架构搜索(NAS)系统(例如<a class="ae kh" href="https://arxiv.org/abs/1812.03443" rel="noopener ugc nofollow" target="_blank"> FBNet </a>或<a class="ae kh" href="https://arxiv.org/abs/1908.01748" rel="noopener ugc nofollow" target="_blank"> SqueezeNAS </a>)，并允许NAS系统帮助设计正确的DNN。<strong class="ih hj">(难度等级:高级，但我们预计会变得更容易)</strong></li></ul><p id="3018" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">明确地说，我们认为随着神经架构搜索(NAS)变得更好，为不同平台创建不同的dnn将变得更容易。如果你有一个支持DNN的应用程序，需要在每部智能手机上运行深度神经网络(从高通GPU，到三星GPU，到华为npu)，NAS将会给你很大帮助。</p><p id="f12d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然而，维护所有这些不同的dnn可能需要大量的工程工作。我们非常有兴趣看到Snapchat和Instagram等支持DNN的移动应用程序最终决定如何处理这个问题，这些应用程序需要在多种类型的智能手机处理平台上运行。</p><h2 id="f29c" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">问题6 —未优化的代码</h2><p id="4311" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">在一天结束的时候，如果你的代码路径中有一些缓慢的东西，<a class="ae kh" href="https://en.wikipedia.org/wiki/Amdahl%27s_law" rel="noopener ugc nofollow" target="_blank">阿姆达尔定律</a>说它将支配你的执行时间。那么，你做的那个快速破解，你添加了一个在Python中本地实现的新层？那可能会很快赶上你。您可能会发现自己优化了一些代码，这些代码的计算成本很低，但最终却占据了执行时间。</p><h2 id="f7b4" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">荣誉奖</h2><p id="1af8" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">还有几个问题会阻止DNN在硬件平台上实现峰值每秒。以下是一些荣誉奖:</p><ul class=""><li id="70bb" class="jt ju hi ih b ii ij im in iq jv iu jw iy jx jc jy jz ka kb bi translated">冷却和热外壳。当芯片过热并且芯片的频率被抑制时，你不能达到峰值TOP/s。</li><li id="630e" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">异质性。今天的许多芯片都有几种处理器和加速器。达到制造商的峰值通常需要充分利用一组异构计算单元。</li><li id="66b4" class="jt ju hi ih b ii kc im kd iq ke iu kf iy kg jc jy jz ka kb bi translated">内核启动开销。特别是在GPU上，启动每个GPU功能(通常称为<em class="li">内核</em>)的延迟可能会<a class="ae kh" href="https://devblogs.nvidia.com/cuda-10-features-revealed/" rel="noopener ugc nofollow" target="_blank">显著</a>。因此，特别是在具有大量轻量级层的深度神经网络中，内核启动开销可能是执行时间的主要因素。层融合(如上所述)在这里会有所帮助。</li></ul><h2 id="2bbb" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">结论—</h2><p id="19b4" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">现在，让我们回到最初的问题。当一家深度学习处理器公司告诉你，他们的产品可以执行一定数量的TOP/s或TMAC/s，这到底意味着什么，这些数字实际上有用吗？</p><p id="638e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">是的，这些数字很有用，因为它让我们知道在这个平台上可以达到的最好速度是多少。然而，有许多警告。为了达到接近这个最好的速度，你必须努力工作。您可能需要仔细查看DNN及其实现中的内存带宽使用、I/O使用和每层并行性。你可能需要重新考虑你的DNN设计。您可能需要重新考虑您的实现，看看像层融合这样的东西。您可能需要更改硬件，例如添加速度更快的硬盘，以跟上应用程序其余部分的速度。你愿意做的事情越多，你的应用就越有可能接近制造商宣传的TOP/s或TMAC/s编号。</p><p id="e78f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最后，我们认为单个DNN设计不可能在各种平台(例如，GPU和GPU服务器和移动设备)。dnn将需要为每个计算平台定制，否则平台将需要变得更加标准化。幸运的是，由于已经开源的DNN模型的多样性，以及神经架构搜索的兴起，这变得更加容易。</p><h2 id="6eed" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">相关阅读</h2><ul class=""><li id="f408" class="jt ju hi ih b ii ld im le iq lj iu lk iy ll jc jy jz ka kb bi translated"><a class="ae kh" href="http://www.markbuckler.com/post/bad-dnn-asic/" rel="noopener ugc nofollow" target="_blank">如何制作糟糕的深度学习硬件</a>，作者Mark Buckler</li></ul><h2 id="c947" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">承认</h2><p id="7f38" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">感谢Steena Monteiro和Suresh Krishna对本文早期草稿的有益评论。</p><h2 id="1599" class="ki kj hi bd kk kl km kn ko kp kq kr ks iq kt ku kv iu kw kx ky iy kz la lb lc bi translated">脚注</h2><p id="1035" class="pw-post-body-paragraph if ig hi ih b ii ld ik il im le io ip iq lf is it iu lg iw ix iy lh ja jb jc hb bi translated">从技术上讲，GPU可能不会并行执行所有线程，但线程可以并发执行(即所有线程同时运行)。</p><p id="0d20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请注意，I/O重叠只有在当前图像正在处理时下一个图像(或下一批图像)准备就绪的情况下才有可能。在一些实时应用中，下一幅图像直到当前图像被处理后才准备好，因此在这些情况下重叠可能是不可能的。</p><p id="3f30" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">专用硬件已经存在很长时间了。大多数CPU都有矢量化运算，只能在某些问题维度上实现加速。此外，数字信号处理器和GPU历来都是针对特定问题维度进行优化的。但是，一些以DNN为中心的处理平台在问题维度上特别严格，必须使用这些维度来实现接近广告中的最佳情况</p></div></div>    
</body>
</html>