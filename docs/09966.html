<html>
<head>
<title>Combining Time Series Analysis with Artificial Intelligence: The Future of Forecasting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">时间序列分析与人工智能的结合:预测的未来</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/combining-time-series-analysis-with-artificial-intelligence-the-future-of-forecasting-5196f57db913?source=collection_archive---------6-----------------------#2020-09-28">https://medium.com/analytics-vidhya/combining-time-series-analysis-with-artificial-intelligence-the-future-of-forecasting-5196f57db913?source=collection_archive---------6-----------------------#2020-09-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="79cc" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">在 python 中构建流行预测模型的集合，如 XGBoost、LSTM、Random Forest、SARIMAX、VAR、VECM 等等！</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/a04343d5221b06ba13bb37ec16509bf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Uy2hnZKfD09Znzwg.jpg"/></div></div></figure><p id="5787" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">欢迎来到我关于构建卓越预测引擎的 3 博客系列的最后一部分。准确的预测有可能加速企业的发展，因为它们会影响战略业务决策，如销售目标、股票价值、财务规划、原材料采购、制造甚至供应链决策。虽然猜测和传统的时间序列分析是预测的流行方法，但人工智能的出现使得<em class="kf">和</em>有必要重新思考旧的做法以提高准确性。</p><p id="fc2a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">本博客系列描述了一种创新的预测方法，该方法能够通过无缝融合时间序列分析和人工智能(AI)来克服与预测相关的挑战。预测本质上属于时间序列分析。但在提高预测准确性方面，人工智能可以成为一种力量倍增器，因为它有能力自动研究和发现数据中的迷人模式。我们的预测方法中有两个关键因素:</p><ol class=""><li id="c348" class="kg kh hi jl b jm jn jp jq js ki jw kj ka kk ke kl km kn ko bi translated">智能设计和识别目标变量的关键驱动因素</li><li id="47f7" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">在由成功的时间序列和人工智能模型组成的整体建模方法中使用这些驱动因素</li></ol><p id="aa0b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">博客系列的第一部分简要介绍了时间序列分析和理解这种数据集所需的工具(链接<a class="ae ku" href="https://link.medium.com/cwvlEzX1u9" rel="noopener">此处</a>)，而第二部分侧重于特征工程和选择(链接<a class="ae ku" rel="noopener" href="/@indraneeldb1993ds/demystifying-feature-engineering-and-selection-for-driver-based-forecasting-50ba9b5a0fbc">此处</a>)。在这篇博客中，我们将深入探讨基于驱动因素的预测方法中的最终预测引擎。它由 10 个模型(5 个时间序列和 5 个人工智能模型)组成，最后我们将所有模型预测的平均值作为我们预测引擎的最终预测。鉴于模型数量众多，我们将对每个模型进行简要介绍，并详细说明如何用 python 实现它们。在另一篇博客中，我们将讨论优化这些模型的最佳实践。</p><p id="793a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这里使用的数据集与本系列之前的博客相同。这是香港房价和 12 个宏观经济变量的每日数据集。预测的目标变量是“私人住宅(价格指数)”即香港住宅价格。正如上一篇博客中所强调的，我们选择了香港房价的三个驱动因素:</p><ol class=""><li id="8154" class="kg kh hi jl b jm jn jp jq js ki jw kj ka kk ke kl km kn ko bi translated">总库存</li><li id="67f2" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">M3(百万港元)</li><li id="2a87" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">M3 第一级(百万港元)</li></ol><h1 id="0ea8" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">型号列表:</h1><ol class=""><li id="cf40" class="kg kh hi jl b jm ln jp lo js lp jw lq ka lr ke kl km kn ko bi translated">单变量时间序列</li><li id="cafb" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">霍尔特温特斯指数平滑法(HWES)或三重平滑法(单变量时间序列)</li><li id="d913" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">多元时间序列</li><li id="9c74" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">多元时间序列</li><li id="774a" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">多元时间序列</li><li id="cc12" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">LSTM(单变量时间序列)</li><li id="f954" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">随机森林(多元人工智能)</li><li id="f185" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">多元人工智能</li><li id="0911" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">线性回归(多元人工智能)</li><li id="3eb3" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">多变量人工智能</li></ol><p id="2424" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 1。萨里玛</strong></p><p id="b45f" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">季节性自回归综合移动平均(SARIMA 或季节性 ARIMA)是一种针对单变量时间序列数据的预测方法，适用于具有季节性成分的数据。它支持自回归和移动平均元素，而集成元素指的是差分，允许该方法支持非平稳时间序列数据。该型号总共有 7 个参数需要配置:</p><ul class=""><li id="bbb6" class="kg kh hi jl b jm jn jp jq js ki jw kj ka kk ke ls km kn ko bi translated"><strong class="jl hj"> p </strong>:趋势自回归顺序。</li><li id="9532" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke ls km kn ko bi translated"><strong class="jl hj"> d </strong>:趋势差序。</li><li id="8fee" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke ls km kn ko bi translated"><strong class="jl hj"> q </strong>:趋势移动平均订单。</li><li id="11a0" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke ls km kn ko bi translated"><strong class="jl hj"> P </strong>:季节性自回归顺序。</li><li id="d03a" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke ls km kn ko bi translated"><strong class="jl hj"> D </strong>:季节性差异订单。</li><li id="fab8" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke ls km kn ko bi translated"><strong class="jl hj"> Q </strong>:季节性移动平均订单。</li><li id="f677" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke ls km kn ko bi translated"><strong class="jl hj"> m </strong>:单个季节周期的时间步数。</li></ul><p id="4baf" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">要了解更多关于 SARIMA 和这些参数的信息，请参考加州大学伯克利分校的这个<a class="ae ku" href="https://www.stat.berkeley.edu/~arturof/Teaching/STAT248/lab07_part1.html" rel="noopener ugc nofollow" target="_blank">实验室</a>。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="cd29" class="ly kw hi lu b fi lz ma l mb mc"># fitting a stepwise model to find the best paramters for SARIMA:<br/>stepwise_fit = pm.auto_arima(target_final, start_p=1, start_q=1, max_p=3, max_d=2,max_q=3,m=7,<br/>                             start_P=0,start_Q=0,max_P=3, max_D=3,max_Q=3, seasonal=True, trace=True,<br/>                             error_action='ignore',  # don't want to know if an order does not work<br/>                             suppress_warnings=True,  # don't want convergence warnings<br/>                             stepwise=True)  # set to stepwise</span><span id="e2bc" class="ly kw hi lu b fi md ma l mb mc">stepwise_fit.summary()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es me"><img src="../Images/fa158330db919808e6490be210f8e1fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f4GDLTSiDrBuMN9_skk-0Q.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 1:寻找 SARIMA 最佳参数值的逐步模型</figcaption></figure><p id="b2c9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们通过<a class="ae ku" href="https://stackabuse.com/grid-search-optimization-algorithm-in-python/" rel="noopener ugc nofollow" target="_blank">网格搜索</a>找到每个参数的最佳值，该网格搜索符合开发人员指定的参数值范围。我们选择具有最低 AIC 分数的一组参数值。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="acf9" class="ly kw hi lu b fi lz ma l mb mc"># SARIMA</span><span id="748c" class="ly kw hi lu b fi md ma l mb mc"># Using parameters automatically based on grid serach<br/>SARIMA_Forecast = pd.DataFrame(stepwise_fit.predict(n_periods= 365))<br/>SARIMA_Forecast.columns = ['SARIMA_Forecast']<br/>SARIMA_Forecast.index = HWES_Forecast.index<br/>SARIMA_Forecast.head(5)</span><span id="dd47" class="ly kw hi lu b fi md ma l mb mc"># Manually fit the model<br/>#sarima_model = SARIMAX(series, order=(5, 2, 2), seasonal_order=(0, 0, 1, 7))<br/>#sarima_model_fit = sarima_model.fit(disp=False)<br/># make prediction<br/>#SARIMA_Forecast = model_fit.predict(len(data), len(data))<br/>#print(yhat)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mj"><img src="../Images/c40d35158d60e119659a89153dbac954.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*dQcpRSi7_JbfvCGniOdSbA.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 2:使用 SARIMA 模型进行预测</figcaption></figure><p id="bf7e" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果我们对提供最低 AIC 的参数集不满意，我们可以用我们首选的参数值集手动拟合模型(查看图 2 中代码的注释掉部分)。</p><p id="2a42" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 2。霍尔特-温特斯指数平滑或三重平滑</strong></p><p id="ebec" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">霍尔特-温特斯技术基于以下三种预测技术:</p><ol class=""><li id="37ec" class="kg kh hi jl b jm jn jp jq js ki jw kj ka kk ke kl km kn ko bi translated">加权平均值</li><li id="309c" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">简单指数平滑</li><li id="1809" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">霍尔特线性方法</li></ol><p id="7758" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">霍尔特-温特斯指数平滑是这些模型的升级版本，因为它可以用于显示趋势和季节行为的时间序列数据。这种方法有两种变体:</p><ol class=""><li id="2172" class="kg kh hi jl b jm jn jp jq js ki jw kj ka kk ke kl km kn ko bi translated">加法:当季节变化在整个系列中大致不变时，这是首选方法</li><li id="4e8f" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">乘法:当季节变化与系列水平成比例变化时，这是首选方法。</li></ol><p id="c606" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">要了解更多关于该模型的信息，请参考 Rob J Hyndman 的这本<a class="ae ku" href="https://otexts.com/fpp2/holt-winters.html" rel="noopener ugc nofollow" target="_blank">书</a>。在我们的案例中，我们使用了加法方法，如下所示:</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="c3a4" class="ly kw hi lu b fi lz ma l mb mc"># Holt Winter’s Exponential Smoothing (HWES)or Triple Smoothing<br/># fit model</span><span id="9c79" class="ly kw hi lu b fi md ma l mb mc">random.seed(10)</span><span id="fc21" class="ly kw hi lu b fi md ma l mb mc">model = ExponentialSmoothing(series, trend = 'add',seasonal= 'add', seasonal_periods= 7)<br/>model_fit = model.fit()<br/># make prediction<br/>HWES_Forecast = pd.DataFrame(model_fit.forecast(steps=365))<br/>HWES_Forecast.columns = ['HWES_Forecast']<br/>HWES_Forecast.head(5)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mk"><img src="../Images/939c57b09688acd9e648daaab67873db.png" data-original-src="https://miro.medium.com/v2/resize:fit:556/format:webp/1*N_nRL5IpHc5L8OVZWCjC4g.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 3:使用 HWES 模型进行预测</figcaption></figure><p id="6c56" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">3.<strong class="jl hj">萨里马克斯</strong></p><p id="0198" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">SARIMAX 类似于 SARIMA，代表带外生因素的季节性自回归综合移动平均线。SARIMA 和 SARIMAX 的关键区别在于外因。如前一篇博客所强调的，我们选择了目标变量的 3 个驱动因素，这些将是我们用于 SARIMAX 模型的外生因素。</p><p id="e6b7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在我们拟合 SARIMAX 模型之前，我们需要确保外生因素和目标变量是稳定的，我们还将对它们进行标准化，以确保它们在同一尺度上。为了检查稳定性，我们运行了一个<a class="ae ku" href="https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test" rel="noopener ugc nofollow" target="_blank"> ADF 测试</a>:</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="fcc4" class="ly kw hi lu b fi lz ma l mb mc"># Function to check stationarity</span><span id="febd" class="ly kw hi lu b fi md ma l mb mc">def adfuller_test(series, signif=0.05, name='', verbose=False):<br/>    """Perform ADFuller to test for Stationarity of given series and print report"""<br/>    r = adfuller(series, autolag='AIC')<br/>    output = {'test_statistic':round(r[0], 4), 'pvalue':round(r[1], 4), 'n_lags':round(r[2], 4), 'n_obs':r[3]}<br/>    p_value = output['pvalue'] <br/>    def adjust(val, length= 6): return str(val).ljust(length)</span><span id="f2ef" class="ly kw hi lu b fi md ma l mb mc"># Print Summary<br/>    print(f'    Augmented Dickey-Fuller Test on "{name}"', "\n   ", '-'*47)<br/>    print(f' Null Hypothesis: Data has unit root. Non-Stationary.')<br/>    print(f' Significance Level    = {signif}')<br/>    print(f' Test Statistic        = {output["test_statistic"]}')<br/>    print(f' No. Lags Chosen       = {output["n_lags"]}')</span><span id="77c1" class="ly kw hi lu b fi md ma l mb mc">for key,val in r[4].items():<br/>        print(f' Critical value {adjust(key)} = {round(val, 3)}')</span><span id="5063" class="ly kw hi lu b fi md ma l mb mc">if p_value &lt;= signif:<br/>        print(f" =&gt; P-Value = {p_value}. Rejecting Null Hypothesis.")<br/>        print(f" =&gt; Series is Stationary.")<br/>    else:<br/>        print(f" =&gt; P-Value = {p_value}. Weak evidence to reject the Null Hypothesis.")<br/>        print(f" =&gt; Series is Non-Stationary.")</span><span id="aa0d" class="ly kw hi lu b fi md ma l mb mc"># Data Prep</span><span id="df56" class="ly kw hi lu b fi md ma l mb mc">AI_Model_data = pd.concat([target_final, features_final], axis=1, sort=False)</span><span id="70d0" class="ly kw hi lu b fi md ma l mb mc"># Check stationarity</span><span id="e25a" class="ly kw hi lu b fi md ma l mb mc"># ADF Test on each column<br/>for name, column in AI_Model_data.iteritems():<br/>    adfuller_test(column, name=column.name)<br/>    print('\n')</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ml"><img src="../Images/dcb432849b4f04ec70a64493dbc13040.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*aS8fe6G6iRqraeahz9q4-g.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 4:检查稳定性的 ADF 测试</figcaption></figure><p id="3b99" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如图 4 所示，所有的特征和目标变量都是不稳定的，所以我们首先将它们区分为稳定的。如前所述，我们也将它们标准化。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="77d9" class="ly kw hi lu b fi lz ma l mb mc"># 1st difference<br/>AI_Model_data_differenced = AI_Model_data.diff().dropna()</span><span id="eb70" class="ly kw hi lu b fi md ma l mb mc"># Check stationarity</span><span id="8cd8" class="ly kw hi lu b fi md ma l mb mc"># ADF Test on each column<br/>#for name, column in AI_Model_data_differenced.iteritems():<br/>    #adfuller_test(column, name=column.name)<br/>    #print('\n')</span><span id="fa8f" class="ly kw hi lu b fi md ma l mb mc">x = AI_Model_data_differenced.values #returns a numpy array</span><span id="d0b3" class="ly kw hi lu b fi md ma l mb mc"># Standardise Datasets</span><span id="d655" class="ly kw hi lu b fi md ma l mb mc">standard_scaler = StandardScaler()<br/>x_scaled = standard_scaler.fit_transform(x)<br/>scaled_AI_Model_data = pd.DataFrame(x_scaled)<br/>scaled_AI_Model_data.columns = AI_Model_data_differenced.columns</span><span id="2c47" class="ly kw hi lu b fi md ma l mb mc"># Check stationarity on diffrenced and scaled data</span><span id="fa10" class="ly kw hi lu b fi md ma l mb mc"># ADF Test on each column<br/>for name, column in scaled_AI_Model_data.iteritems():<br/>    adfuller_test(column, name=column.name)<br/>    print('\n')</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mm"><img src="../Images/e70e6c8d736770774ba59da74a9a460e.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*F_my9OxQ2fLMrQa59N5b1w.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 5:差异和标准化数据集上的 ADF 测试</figcaption></figure><p id="3cf3" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">对数据集进行差分和标准化后，我们可以看到所有字段都是固定的，我们将使用它们来拟合 SARIMAX 模型。SARIMAX 模型的参数与 SARIMA 相同，因此，我们使用相同的网格搜索方法在差异和标准化数据集上寻找最佳值。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="e39e" class="ly kw hi lu b fi lz ma l mb mc"># fitting a stepwise model to find the best paramters for SARIMAX:<br/>stepwise_fit_2 = pm.auto_arima(scaled_AI_Model_Target, start_p=1, start_q=1, max_p=3, d=0,max_q=3,m=7,<br/>                             start_P=0,start_Q=0,max_P=3, max_D=3,max_Q=3, seasonal=True, trace=True,<br/>                             error_action='ignore',  # don't want to know if an order does not work<br/>                             suppress_warnings=True,  # don't want convergence warnings<br/>                             stepwise=True)  # set to stepwise</span><span id="2e65" class="ly kw hi lu b fi md ma l mb mc">stepwise_fit_2.summary()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mn"><img src="../Images/6a3807973e66859623f83f2770c32c53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_ZSgjoIzTqLkw6m0Vt8VHg.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 6:寻找 SARIMAX 的最佳参数值</figcaption></figure><p id="1092" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">一旦我们获得了参数的最佳值，我们就可以拟合模型。该模型所需的关键参数是“exog ”,它是外生回归量的数据集。在我们的例子中，回归变量是我们在本系列的第二篇博客中选择的三个驱动因素。在使用拟合模型进行预测时，我们需要提供驾驶员的预测作为模型的输入。我们使用 SARIMA 模型生成这些驱动因素预测。请注意，驱动因素预测是基于标准化的静态数据。这些驱动因素预测也将在所有多元人工智能模型中得到利用。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="20a1" class="ly kw hi lu b fi lz ma l mb mc">#Driver Forecast using SARIMA</span><span id="b1c1" class="ly kw hi lu b fi md ma l mb mc">forecast_index = pd.date_range('2019/11/27', '2020/11/25')</span><span id="b178" class="ly kw hi lu b fi md ma l mb mc">driver_forecasts = pd.DataFrame(index=forecast_index, columns= features_final.columns)</span><span id="1573" class="ly kw hi lu b fi md ma l mb mc">scaled_AI_Model_Features_1 = scaled_AI_Model_Features.copy()<br/>date_index = pd.date_range('2003-01-06', '2019/11/26')<br/>scaled_AI_Model_Features_1.index = date_index</span><span id="b161" class="ly kw hi lu b fi md ma l mb mc">for i in range(len(features_final.columns)):</span><span id="64bc" class="ly kw hi lu b fi md ma l mb mc">df_series = scaled_AI_Model_Features_1.asfreq('D')<br/>                                            series_features = pd.Series(df_series.iloc[:,i], index= df_series.index)</span><span id="9c69" class="ly kw hi lu b fi md ma l mb mc">stepwise_fit_features = pm.auto_arima(series_features, start_p=1, start_q=1, max_p=2, max_d=2,max_q=2,m=7,<br/>                                                                         start_P=0,start_Q=0,max_P=2, max_D=2,max_Q=2, seasonal=True, trace=True,<br/>                                                                         error_action='ignore',  # don't want to know if an order does not work<br/>                                                                         suppress_warnings=True,  # don't want convergence warnings<br/>                                                                         stepwise=True)  # set to stepwise</span><span id="d977" class="ly kw hi lu b fi md ma l mb mc">SARIMA_Forecast_features = stepwise_fit_features.predict(n_periods= 365)<br/>                                            #Add to driver forecast dataset<br/>                                            driver_forecasts.iloc[:,i] = SARIMA_Forecast_features</span><span id="b591" class="ly kw hi lu b fi md ma l mb mc">driver_forecasts.head(5)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="ab fe cl mo"><img src="../Images/f7577851c72d4a7c6c5d9b6d3d963e2c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*4jfuS2n5H-bGJgxNUSkUdg.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 6:使用 SARIMA 的驾驶员预测</figcaption></figure><p id="5849" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">最后一步是使用最佳参数值以及目标变量和驱动因素的静态和标准化历史值来拟合模型。然后，我们使用拟合的模型和驾驶员预测来预测未来。由于模型预测基于差分和标准化数据集，我们需要对其进行去标准化，并取消第一次差分，以获得最终预测。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="e481" class="ly kw hi lu b fi lz ma l mb mc"># SARIMAX <br/># contrived dataset<br/>data1 = scaled_AI_Model_Target<br/>data2 = scaled_AI_Model_Features<br/># fit model<br/>model = SARIMAX(data1, exog=data2, order=stepwise_fit_2.order, seasonal_order=stepwise_fit_2.seasonal_order)<br/>model_fit = model.fit(disp=False)<br/># make prediction<br/>exog2 = driver_forecasts<br/>SARIMAX_Forecast = pd.DataFrame(model_fit.predict(len(data1),len(data1)+364, exog=exog2))<br/>SARIMAX_Forecast.columns = ['Raw_Forecast']<br/>SARIMAX_Forecast['SARIMAX_Forecast_De_Standardise_R_Sum'] = SARIMAX_Forecast['Raw_Forecast'].cumsum()<br/>SARIMAX_Forecast['SARIMAX_Forecast'] = SARIMAX_Forecast['SARIMAX_Forecast_De_Standardise_R_Sum'] + Latest_target[0]</span><span id="1683" class="ly kw hi lu b fi md ma l mb mc">SARIMAX_Forecast.index = HWES_Forecast.index<br/>SARIMAX_Forecast.head(5)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mp"><img src="../Images/452057aeedb14dd00dbce8d0b2d8a359.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*crcR4KnNXFHdf7fkHwUTXg.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 8:拟合 SARIMAX 模型以生成预测</figcaption></figure><p id="3e32" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 4。VAR </strong></p><p id="f4bf" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">向量自回归(VAR)是一种预测算法，可以在两个或多个时间序列相互影响时使用。它被称为自回归模型，因为每个变量都被建模为过去值的函数，也就是说，预测值只不过是它们自己的滞后。为了更好地理解 VAR，请浏览宾夕法尼亚州立大学的这一课。</p><p id="24cd" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">由于 VAR 模型自动计算变量的滞后，我们不能将变量的滞后作为一个单独的变量添加到模型中。因此，我们在训练模型之前删除变量:“M3(百万港元)_Lag1”，因为我们已经将“M3(百万港元)”作为变量。与 SARIMAX 模型类似，我们需要在建模之前使数据集保持静止。我们对数据集进行差分，使其保持稳定。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="1b4f" class="ly kw hi lu b fi lz ma l mb mc"># Data Prep for VAR/VECM - First Difference and check stationarity</span><span id="db43" class="ly kw hi lu b fi md ma l mb mc"># Data Prep for VAR - check stationarity</span><span id="6bee" class="ly kw hi lu b fi md ma l mb mc">varma_vecm_data = pd.concat([target_final, features_final], axis=1, sort=False)<br/>varma_vecm_data  = varma_vecm_data.drop(columns=['M3 (HK$ million)_Lag1'])</span><span id="d108" class="ly kw hi lu b fi md ma l mb mc"># 1st difference<br/>varma_vecm_data_differenced = varma_vecm_data.diff().dropna()</span><span id="a058" class="ly kw hi lu b fi md ma l mb mc"># ADF Test on each column<br/>for name, column in varma_vecm_data_differenced.iteritems():<br/>    adfuller_test(column, name=column.name)<br/>    print('\n')</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mq"><img src="../Images/05eebec195ff6b080ff264cfa6698c7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*3J7smpSG8BhFYt6ZjR5Vhw.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 9:对数据集进行差分，使其保持静止</figcaption></figure><p id="3ba1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">下一步是选择 VAR 模型的正确顺序。我们使用“select_order”函数来迭代拟合 VAR 模型的递增阶数，并选择使模型具有最小 AIC 的阶数。然后，我们将 VAR 模型拟合到具有最佳滞后数的静态数据集上。我们使用拟合的模型来预测目标变量的变化，然后取消差异以获得最终预测。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="1582" class="ly kw hi lu b fi lz ma l mb mc">#VAR Model</span><span id="5a39" class="ly kw hi lu b fi md ma l mb mc">model = VAR(varma_vecm_data_differenced)<br/>lag_selection_VAR = model.select_order(maxlags=12)</span><span id="f443" class="ly kw hi lu b fi md ma l mb mc">list(lag_selection_VAR.selected_orders.values())[0]</span><span id="9e5a" class="ly kw hi lu b fi md ma l mb mc"># Get the lag order<br/>lag_order = list(lag_selection_VAR.selected_orders.values())[0]</span><span id="fe01" class="ly kw hi lu b fi md ma l mb mc">model_fitted = model.fit(lag_order)</span><span id="cab7" class="ly kw hi lu b fi md ma l mb mc"># Input data for forecasting<br/>forecast_input = varma_vecm_data_differenced.values[-lag_order:]<br/>forecast_input</span><span id="5c5f" class="ly kw hi lu b fi md ma l mb mc"># Forecast<br/>VAR_FC = model_fitted.forecast(y=forecast_input, steps= 365)<br/>VAR_FC_2 = pd.DataFrame(VAR_FC , index= driver_forecasts.index, columns=varma_vecm_data_differenced.columns + '_1d')<br/>VAR_FC_2</span><span id="4d67" class="ly kw hi lu b fi md ma l mb mc">#VAR Forecast</span><span id="eead" class="ly kw hi lu b fi md ma l mb mc">def invert_transformation(df_train, df_forecast, second_diff=False):<br/>    """Revert back the differencing to get the forecast to original scale."""<br/>    df_fc = df_forecast.copy()<br/>    columns = df_train.columns<br/>    for col in columns:        <br/>        # Roll back 2nd Diff<br/>        if second_diff:<br/>            df_fc[str(col)+'_1d'] = (df_train[col].iloc[-1]-df_train[col].iloc[-2]) + df_fc[str(col)+'_2d'].cumsum()<br/>        # Roll back 1st Diff<br/>        df_fc[str(col)+'_forecast'] = df_train[col].iloc[-1] + df_fc[str(col)+'_1d'].cumsum()<br/>    return df_fc</span><span id="4c68" class="ly kw hi lu b fi md ma l mb mc">df_results = invert_transformation(varma_vecm_data, VAR_FC_2, second_diff=False)<br/>VAR_Forecast = pd.DataFrame(df_results['Private Domestic (Price Index)_forecast'])<br/>VAR_Forecast.columns = ['VAR_Forecast']<br/>VAR_Forecast.index = HWES_Forecast.index<br/>VAR_Forecast.head(5)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mr"><img src="../Images/491ea7d8fa692b9cc1cacc5e00200874.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*Gpcdbwov7s3fdcAb7UWN0g.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 10:使用风险值模型的预测</figcaption></figure><p id="3302" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">5.VECM 模型</p><p id="f1d2" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">VAR 模型的缺点是忽略了变量之间可能重要的(<em class="kf">【长期】</em>)<em class="kf">【协整】</em>)关系。通常的方法是使用 Johansen 的方法来检验协整是否存在。如果答案是肯定的，那么应该使用向量误差修正模型(VECM)来考虑协整关系<em class="kf">。为了更好地理解 VECM 模型，你可以浏览迈克尔·豪泽的这些讲座幻灯片。我们使用与 VAR 模型相同的数据集，并执行<a class="ae ku" href="https://www.imf.org/external/pubs/ft/wp/2007/wp07141.pdf" rel="noopener ugc nofollow" target="_blank"> Johansen 协整</a>测试，以找到协整关系的最佳数量。</em></p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="ce02" class="ly kw hi lu b fi lz ma l mb mc"># VECM - Johansen Cointegration Test</span><span id="ff35" class="ly kw hi lu b fi md ma l mb mc">rank_test = select_coint_rank(varma_vecm_data_differenced,-1, lag_order, method="trace",<br/>                              signif=0.05)<br/>print(rank_test)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ms"><img src="../Images/f01b77711cb9bde9b74ce698936be9df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7isI-FlY9OR6s6zmOHhwwQ.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 11.1: Johansen 协整检验</figcaption></figure><p id="84e6" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们可以看到，在图 11.1 中有 3 个统计上显著的协整关系。我们为参数‘k _ ar _ dif’设置了与 VAR 模型相同的滞后阶数，并基于协整检验为参数‘coint _ rank’设置了最优值。类似于 VAR 模型，我们取消差异以获得最终预测。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="4553" class="ly kw hi lu b fi lz ma l mb mc">#VECM</span><span id="8f4b" class="ly kw hi lu b fi md ma l mb mc">model = VECM(varma_vecm_data_differenced, deterministic="ci",<br/>             k_ar_diff=lag_order,  <br/>             coint_rank=rank_test.rank)</span><span id="ad37" class="ly kw hi lu b fi md ma l mb mc">vecm_res = model.fit()</span><span id="3d88" class="ly kw hi lu b fi md ma l mb mc">VECM_FC = vecm_res.predict(steps=365)</span><span id="0c2b" class="ly kw hi lu b fi md ma l mb mc">VECM_FC_2 = pd.DataFrame(VECM_FC , index= driver_forecasts.index, columns=varma_vecm_data_differenced.columns + '_1d')</span><span id="a5fd" class="ly kw hi lu b fi md ma l mb mc">df_results_2 = invert_transformation(varma_vecm_data, VECM_FC_2, second_diff=False)</span><span id="d916" class="ly kw hi lu b fi md ma l mb mc">VECM_Forecast = pd.DataFrame(df_results_2['Private Domestic (Price Index)_forecast'])<br/>VECM_Forecast.columns = ['VECM_Forecast']<br/>VECM_Forecast.index = HWES_Forecast.index<br/>VECM_Forecast.head(5)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mt"><img src="../Images/ca3413ef178ee0f6991a1dfbcf37cec9.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*K2p39tRPPq81DsyUR4boIw.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 11.2:使用 VECM 模型进行预测</figcaption></figure><p id="961c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj">人工智能模型</strong></p><p id="6534" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">对于我们预测引擎中的所有人工智能模型(线性回归除外)，我们将使用以下方法:</p><ol class=""><li id="a088" class="kg kh hi jl b jm jn jp jq js ki jw kj ka kk ke kl km kn ko bi translated">将差异和缩放数据(用于 SARIMA 模型)分为训练和测试数据。我们使用这些数据，因为平稳性和统一缩放通常会提高模型性能。最近一年被分离出来作为测试数据。在处理时间序列数据时，我们不应该对训练和测试分割使用随机抽样，因为数据集是按时间顺序相关的。</li><li id="47d5" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">通过使用关键参数的不同值在训练数据上训练模型来执行网格搜索。</li><li id="906b" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">使用训练好的模型在测试时间段内进行预测。</li><li id="2938" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">根据<a class="ae ku" href="https://en.wikipedia.org/wiki/Mean_absolute_percentage_error" rel="noopener ugc nofollow" target="_blank"> MAPE 得分</a>计算模型的预测精度</li><li id="98fe" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">选择提供最佳精度的参数值</li><li id="19f1" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">在整个历史数据集中使用最佳拟合模型的参数值来训练模型</li><li id="d80c" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">使用拟合的模型和驾驶员预测进行预测</li><li id="13d7" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke kl km kn ko bi translated">取消标准化和消除差异，以获得最终预测</li></ol><p id="7e56" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">这种方法确保我们在优化模型性能的同时，确保数据集的时间序列属性不被违反。第 1 步到第 5 步用于超参数调整，而其余步骤集中于从最终模型生成预测。在另一篇博客中，我们将深入探讨优化模型性能的其他方法，如回溯测试策略，以及每个参数如何影响模型的细节。在这篇博客中，我们将简要介绍这些模型及其参数，并重点介绍它在 python 中的实现。</p><p id="3c84" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 6。LSTM </strong></p><p id="ceb7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">LSTM(长短期记忆)是一种扩展递归神经网络记忆的模型。通常，递归神经网络具有“短期记忆”，因为只有最近的信息用于当前的任务。LSTM 将长期记忆引入递归神经网络。它通过使用一系列“门”来减轻消失梯度问题。请参考 Christopher Olah 在<a class="ae ku" href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="noopener ugc nofollow" target="_blank">的博客</a>了解更多关于这款车型的细节。</p><p id="b925" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">在创建 LSTM 模型之前，我们需要创建一个<a class="ae ku" href="https://keras.io/api/preprocessing/timeseries/" rel="noopener ugc nofollow" target="_blank">时间序列生成器</a>对象。此外，我们将尝试优化一组三个参数:<em class="kf">输出序列的长度、每批中时间序列样本的数量以及时期的数量</em>。下面这段代码用于按照前面提到的方法执行超参数调整:</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="1a7f" class="ly kw hi lu b fi lz ma l mb mc">#LSTM</span><span id="b820" class="ly kw hi lu b fi md ma l mb mc"># Data Prep<br/>dataset = pd.DataFrame(target_train).values<br/>dataset = dataset.astype('float32')</span><span id="ce5f" class="ly kw hi lu b fi md ma l mb mc"># Batch Size<br/>batch_size = [1,2,4]<br/># Epochs<br/>epoch_size = [5,7,10]<br/># lenght<br/>lenght = [7,30,120]</span><span id="f47a" class="ly kw hi lu b fi md ma l mb mc">n_features = 1</span><span id="b3f3" class="ly kw hi lu b fi md ma l mb mc">LSTM_Test_Accuracy_Data = pd.DataFrame(columns = ['batch_size','epoch_size','lenght','Test Accurcay'])</span><span id="468a" class="ly kw hi lu b fi md ma l mb mc">for x in list(itertools.product(batch_size, epoch_size,lenght)):<br/>            generator = TimeseriesGenerator(dataset, dataset,batch_size=x[0], length= x[2])</span><span id="1c5a" class="ly kw hi lu b fi md ma l mb mc">lstm_model = Sequential()<br/>            lstm_model.add(LSTM(200, activation='relu', input_shape=(x[2], n_features)))<br/>            lstm_model.add(Dense(1))<br/>            lstm_model.compile(optimizer='adam', loss='mse')<br/>            lstm_model.fit_generator(generator,epochs=x[1])</span><span id="f69c" class="ly kw hi lu b fi md ma l mb mc">lstm_predictions_scaled = list()</span><span id="0325" class="ly kw hi lu b fi md ma l mb mc">batch = dataset[-x[2]:]<br/>            current_batch = batch.reshape((1, x[2], n_features))<br/>            # Test Data<br/>            for i in range(len(target_test)):   <br/>                lstm_pred = lstm_model.predict(current_batch)[0]<br/>                lstm_predictions_scaled.append(lstm_pred) <br/>                current_batch = np.append(current_batch[:,1:,:],[[lstm_pred]],axis=1)</span><span id="40a6" class="ly kw hi lu b fi md ma l mb mc">predictions_test = pd.DataFrame(lstm_predictions_scaled)<br/>            predictions_test.index = target_test.index<br/>            # Calculate the absolute errors<br/>            errors_test = abs(predictions_test.iloc[:,0] - target_test)<br/>             # Calculate mean absolute percentage error (MAPE)<br/>            mape_test = 100 * (errors_test/ target_test)<br/>            # Calculate and display accuracy<br/>            accuracy_test = 100 - np.mean(mape_test)</span><span id="70ec" class="ly kw hi lu b fi md ma l mb mc">LSTM_Test_Accuracy_Data_One = pd.DataFrame(index = range(1),columns = ['batch_size','epoch_size','lenght','Test Accurcay'])</span><span id="04ea" class="ly kw hi lu b fi md ma l mb mc">LSTM_Test_Accuracy_Data_One.loc[:,'batch_size'] = x[0]<br/>            LSTM_Test_Accuracy_Data_One.loc[:,'epoch_size'] = x[1]<br/>            LSTM_Test_Accuracy_Data_One.loc[:,'lenght'] = x[2]<br/>            LSTM_Test_Accuracy_Data_One.loc[:,'Test Accurcay'] = accuracy_test</span><span id="1aac" class="ly kw hi lu b fi md ma l mb mc">LSTM_Test_Accuracy_Data = LSTM_Test_Accuracy_Data.append(LSTM_Test_Accuracy_Data_One)<br/>            <br/>LSTM_Test_Accuracy_Data</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mu"><img src="../Images/a0ed525005bca0fb7d3480a857018050.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*w_6lLzNDtuGErwMP815OGQ.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 12:LSTM 的超参数调整</figcaption></figure><p id="a5c9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">根据参数的最佳值，我们拟合 LSTM 模型，并生成最终预测，如下图 13 所示。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="cfce" class="ly kw hi lu b fi lz ma l mb mc">#LSTM</span><span id="e2a9" class="ly kw hi lu b fi md ma l mb mc"># Data Prep<br/>dataset = pd.DataFrame(scaled_AI_Model_Target).values<br/>dataset = dataset.astype('float32')<br/>Latest_target = target_final.tail(1).values.tolist()</span><span id="a5b7" class="ly kw hi lu b fi md ma l mb mc"># Best Fit Model</span><span id="ddf5" class="ly kw hi lu b fi md ma l mb mc">Best_Fit_LSTM = LSTM_Test_Accuracy_Data.loc[LSTM_Test_Accuracy_Data['Test Accurcay'] == max(LSTM_Test_Accuracy_Data['Test Accurcay'])]</span><span id="2402" class="ly kw hi lu b fi md ma l mb mc">Best_Fit_LSTM = Best_Fit_LSTM.values.flatten().tolist()</span><span id="f7f3" class="ly kw hi lu b fi md ma l mb mc"># Fit  Model<br/>generator = TimeseriesGenerator(dataset, dataset,batch_size=Best_Fit_LSTM[0], length= Best_Fit_LSTM[2])</span><span id="fd04" class="ly kw hi lu b fi md ma l mb mc">lstm_model = Sequential()<br/>lstm_model.add(LSTM(200, activation='relu', input_shape=(Best_Fit_LSTM[2], n_features)))<br/>lstm_model.add(Dense(1))<br/>lstm_model.compile(optimizer='adam', loss='mse')<br/>lstm_model.fit_generator(generator,epochs=Best_Fit_LSTM[1])</span><span id="8cd1" class="ly kw hi lu b fi md ma l mb mc">lstm_predictions_scaled = list()</span><span id="5945" class="ly kw hi lu b fi md ma l mb mc">batch = dataset[-Best_Fit_LSTM[2]:]<br/>current_batch = batch.reshape((1, Best_Fit_LSTM[2], n_features))<br/># Test Data<br/>for i in range(365):   <br/>    lstm_pred = lstm_model.predict(current_batch)[0]<br/>    lstm_predictions_scaled.append(lstm_pred) <br/>    current_batch = np.append(current_batch[:,1:,:],[[lstm_pred]],axis=1)</span><span id="e6d8" class="ly kw hi lu b fi md ma l mb mc">LSTM_Forecast = pd.DataFrame(lstm_predictions_scaled)</span><span id="bbe7" class="ly kw hi lu b fi md ma l mb mc"># De-Standardise <br/>LSTM_Forecast_Adjusted_1 = (LSTM_Forecast*std_target)+ mean_target<br/>LSTM_Forecast_Adjusted_1</span><span id="8d86" class="ly kw hi lu b fi md ma l mb mc">LSTM_Forecast_Adjusted_2 = pd.DataFrame({'LSTM_Forecast_De_Standardise': LSTM_Forecast_Adjusted_1.iloc[:,0]})</span><span id="3dad" class="ly kw hi lu b fi md ma l mb mc">#Roll back first difference</span><span id="cf73" class="ly kw hi lu b fi md ma l mb mc">#LSTM_Forecast_Adjusted_3 = AI_Model_data</span><span id="9aaa" class="ly kw hi lu b fi md ma l mb mc">LSTM_Forecast_Adjusted_2['LSTM_Forecast_De_Standardise_R_Sum'] = LSTM_Forecast_Adjusted_2['LSTM_Forecast_De_Standardise'].cumsum()<br/>LSTM_Forecast_Adjusted_2['LSTM_Forecast'] = LSTM_Forecast_Adjusted_2['LSTM_Forecast_De_Standardise_R_Sum'] + Latest_target[0]</span><span id="7e72" class="ly kw hi lu b fi md ma l mb mc">LSTM_Forecast_Adjusted_2.index = HWES_Forecast.index<br/>LSTM_Forecast_Adjusted_2.head(5)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es mv"><img src="../Images/fdba98b6ad3c0dfb7b121269137b32a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1236/format:webp/1*84Z4Lhmkg8mTn92SHyFQmA.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 13:使用 LSTM 的预测</figcaption></figure><p id="fea1" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 7。随机森林</strong></p><p id="96a9" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">随机森林是决策树算法的集合。创建了许多决策树，其中每棵树都是从不同的<a class="ae ku" href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" rel="noopener ugc nofollow" target="_blank">引导</a>样本创建的。它可用于分类或回归。在我们的例子中，最终预测是整个决策树的平均预测。在本练习中，我们将调整以下参数:<em class="kf">决策树的最大深度、叶节点所需的最小样本数、分割内部节点所需的最小样本数以及森林中的树数(参见图 14)。</em>为了更好地了解兰登森林，你可以查看利奥·布雷曼的<a class="ae ku" href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf" rel="noopener ugc nofollow" target="_blank">原始论文</a>。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="fa3e" class="ly kw hi lu b fi lz ma l mb mc">#Random Forest<br/># Number of trees in random forest<br/>n_estimators = [500,1000,2000]<br/># Maximum number of levels in tree<br/>max_depth = [10,50,100]<br/># Minimum number of samples required to split a node<br/>min_samples_split = [50,100,200]<br/># Minimum number of samples required at each leaf node<br/>min_samples_leaf = [2,4,10]</span><span id="3eff" class="ly kw hi lu b fi md ma l mb mc">RF_Test_Accuracy_Data = pd.DataFrame(columns = ['n_estimators','max_depth','min_samples_split','min_samples_leaf','Train Accurcay','Test Accurcay'])</span><span id="4853" class="ly kw hi lu b fi md ma l mb mc">for x in list(itertools.product(n_estimators, max_depth,min_samples_split,min_samples_leaf)):<br/>            rf = RandomForestRegressor(n_estimators = x[0],max_depth = x[1], min_samples_split = x[2],min_samples_leaf=x[3], random_state = 10,n_jobs=-1, max_features= "auto")<br/>            # Train the model on training data<br/>            rf.fit(features_train, target_train)</span><span id="683e" class="ly kw hi lu b fi md ma l mb mc"># Train Data<br/>            # Use the forest's predict method on the train data<br/>            predictions_train = rf.predict(features_train)<br/>            # Calculate the absolute errors<br/>            errors_train = abs(predictions_train - target_train)<br/>             # Calculate mean absolute percentage error (MAPE)<br/>            mape_train = 100 * (errors_train/ target_train)<br/>            # Calculate and display accuracy<br/>            accuracy_train = 100 - np.mean(mape_train)</span><span id="bab9" class="ly kw hi lu b fi md ma l mb mc"># Test Data<br/>            # Use the forest's predict method on the test data<br/>            predictions_test = rf.predict(features_test)<br/>            # Calculate the absolute errors<br/>            errors_test = abs(predictions_test - target_test)<br/>             # Calculate mean absolute percentage error (MAPE)<br/>            mape_test = 100 * (errors_test/ target_test)<br/>            # Calculate and display accuracy<br/>            accuracy_test = 100 - np.mean(mape_test)</span><span id="2af8" class="ly kw hi lu b fi md ma l mb mc">RF_Test_Accuracy_Data_One = pd.DataFrame(index = range(1),columns = ['n_estimators','max_depth','min_samples_split','min_samples_leaf','Train Accurcay','Test Accurcay'])</span><span id="58a0" class="ly kw hi lu b fi md ma l mb mc">RF_Test_Accuracy_Data_One.loc[:,'n_estimators'] = x[0]<br/>            RF_Test_Accuracy_Data_One.loc[:,'max_depth'] = x[1]<br/>            RF_Test_Accuracy_Data_One.loc[:,'min_samples_split'] = x[2]<br/>            RF_Test_Accuracy_Data_One.loc[:,'min_samples_leaf'] = x[3]<br/>            RF_Test_Accuracy_Data_One.loc[:,'Train Accurcay'] = accuracy_train<br/>            RF_Test_Accuracy_Data_One.loc[:,'Test Accurcay'] = accuracy_test</span><span id="50af" class="ly kw hi lu b fi md ma l mb mc">RF_Test_Accuracy_Data = RF_Test_Accuracy_Data.append(RF_Test_Accuracy_Data_One)<br/>            <br/>RF_Test_Accuracy_Data</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mw"><img src="../Images/126a1b6afb7f75c11c023ed0106c8af5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e914jjopNMQKyML10fGBKg.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 14:随机森林的超级参数调整</figcaption></figure><p id="33d0" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">下面这段代码符合基于最佳参数值的最终随机森林模型(参见图 15)。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="dad7" class="ly kw hi lu b fi lz ma l mb mc">#Random Forest</span><span id="43ca" class="ly kw hi lu b fi md ma l mb mc"># Best Fit Model</span><span id="6e5e" class="ly kw hi lu b fi md ma l mb mc">Best_Fit_Random_Forest = RF_Test_Accuracy_Data.loc[RF_Test_Accuracy_Data['Test Accurcay'] == max(RF_Test_Accuracy_Data['Test Accurcay'])]</span><span id="6786" class="ly kw hi lu b fi md ma l mb mc">Best_Fit_Random_Forest = Best_Fit_Random_Forest.values.flatten().tolist()</span><span id="6483" class="ly kw hi lu b fi md ma l mb mc"># Fit  Model<br/>rf = RandomForestRegressor(n_estimators = Best_Fit_Random_Forest[0],max_depth = Best_Fit_Random_Forest[1], min_samples_split = Best_Fit_Random_Forest[2],min_samples_leaf=Best_Fit_Random_Forest[3], random_state = 10,n_jobs=-1, max_features= "auto")# Train the model on training data<br/>rf.fit(features_train, target_train)</span><span id="0699" class="ly kw hi lu b fi md ma l mb mc"># Use the forest's predict method on the test data<br/>Random_Forest_Forecast = rf.predict(driver_forecasts)<br/>Random_Forest_Forecast</span><span id="d8c3" class="ly kw hi lu b fi md ma l mb mc"># De-Standardise <br/>Random_Forest_Forecast_Adjusted_1 = (Random_Forest_Forecast*std_target)+ mean_target<br/>Random_Forest_Forecast_Adjusted_2 = pd.DataFrame({'Random_Forest_Forecast_De_Standardise': Random_Forest_Forecast_Adjusted_1[:]})</span><span id="0697" class="ly kw hi lu b fi md ma l mb mc">#Roll back first difference</span><span id="7669" class="ly kw hi lu b fi md ma l mb mc">#Random_Forest_Forecast_Adjusted_3 = AI_Model_data</span><span id="e9aa" class="ly kw hi lu b fi md ma l mb mc">Random_Forest_Forecast_Adjusted_2['Random_Forest_Forecast_De_Standardise_R_Sum'] = Random_Forest_Forecast_Adjusted_2['Random_Forest_Forecast_De_Standardise'].cumsum()<br/>Random_Forest_Forecast_Adjusted_2['Random_Forest_Forecast'] = Random_Forest_Forecast_Adjusted_2['Random_Forest_Forecast_De_Standardise_R_Sum'] + Latest_target[0]</span><span id="5ccd" class="ly kw hi lu b fi md ma l mb mc">Random_Forest_Forecast_Adjusted_2.index = HWES_Forecast.index<br/>Random_Forest_Forecast_Adjusted_2.head(5)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mx"><img src="../Images/e76a1f19cd39688af975dab108065fdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qy2Q_Y0kGZli_LtlbyZc0g.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 15:使用随机森林的预测</figcaption></figure><p id="8557" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 8。XGBoost </strong></p><p id="0ef7" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">XGBoost(极限梯度提升)提供了梯度提升决策树的高性能实现。XG Boost 不是像 random forest 那样单独训练所有模型，而是连续训练模型。每一个新模型都被训练来纠正前一个模型所犯的错误。模型按顺序添加，直到不能再进一步改进。我们为该模型调整一组 5 个参数:<em class="kf">梯度提升树的数量、基础学习者的最大树深度、提升学习速率、在叶节点上进行进一步划分所需的最小损失减少以及子节点中所需的所有观察的最小权重和。</em>XGBoost 上的原纸可以在这里找到<a class="ae ku" href="https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf" rel="noopener ugc nofollow" target="_blank">。</a></p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="64a6" class="ly kw hi lu b fi lz ma l mb mc">#XG Boost<br/># Number of trees<br/>n_estimators = [500,1000,2000]<br/># Maximum number of levels in tree<br/>max_depth = [10,50,100]<br/>#minimum sum of weights of all observations required in a child<br/>min_child_weight = [1,2]<br/>#Gamma specifies the minimum loss reduction required to make a split<br/>gamma = [1,5]<br/># boosting learning rate<br/>learning_rate = [.1,.05,.01]</span><span id="c772" class="ly kw hi lu b fi md ma l mb mc">xgb_Test_Accuracy_Data = pd.DataFrame(columns = ['n_estimators','max_depth','min_child_weight','gamma','learning_rate','Train Accurcay','Test Accurcay'])</span><span id="ddbc" class="ly kw hi lu b fi md ma l mb mc">for x in list(itertools.product(n_estimators, max_depth,min_child_weight,gamma,learning_rate)):<br/>            xgb_reg = xgb.XGBRegressor(n_estimators=x[0],max_depth =x[1],min_child_weight = x[2],gamma = x[3],learning_rate = x[4])<br/>        <br/>            # Train the model on training data<br/>            xgb_reg.fit(features_train, target_train)</span><span id="f287" class="ly kw hi lu b fi md ma l mb mc"># Train Data<br/>            # Use the forest's predict method on the train data<br/>            predictions_train = xgb_reg.predict(features_train)<br/>            # Calculate the absolute errors<br/>            errors_train = abs(predictions_train - target_train)<br/>             # Calculate mean absolute percentage error (MAPE)<br/>            mape_train = 100 * (errors_train/ target_train)<br/>            # Calculate and display accuracy<br/>            accuracy_train = 100 - np.mean(mape_train)</span><span id="9b8d" class="ly kw hi lu b fi md ma l mb mc"># Test Data<br/>            # Use the forest's predict method on the test data<br/>            predictions_test = xgb_reg.predict(features_test)<br/>            # Calculate the absolute errors<br/>            errors_test = abs(predictions_test - target_test)<br/>             # Calculate mean absolute percentage error (MAPE)<br/>            mape_test = 100 * (errors_test/ target_test)<br/>            # Calculate and display accuracy<br/>            accuracy_test = 100 - np.mean(mape_test)</span><span id="ce14" class="ly kw hi lu b fi md ma l mb mc">xgb_Test_Accuracy_Data_One = pd.DataFrame(index = range(1),columns = ['n_estimators','max_depth','min_child_weight','gamma','learning_rate','Train Accurcay','Test Accurcay'])</span><span id="7e38" class="ly kw hi lu b fi md ma l mb mc">xgb_Test_Accuracy_Data_One.loc[:,'n_estimators'] = x[0]<br/>            xgb_Test_Accuracy_Data_One.loc[:,'max_depth'] = x[1]<br/>            xgb_Test_Accuracy_Data_One.loc[:,'min_child_weight'] = x[2]<br/>            xgb_Test_Accuracy_Data_One.loc[:,'gamma'] = x[3]<br/>            xgb_Test_Accuracy_Data_One.loc[:,'learning_rate'] = x[4]<br/>            xgb_Test_Accuracy_Data_One.loc[:,'Train Accurcay'] = accuracy_train<br/>            xgb_Test_Accuracy_Data_One.loc[:,'Test Accurcay'] = accuracy_test</span><span id="8e9d" class="ly kw hi lu b fi md ma l mb mc">xgb_Test_Accuracy_Data = xgb_Test_Accuracy_Data.append(xgb_Test_Accuracy_Data_One)<br/>            <br/>xgb_Test_Accuracy_Data</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es my"><img src="../Images/a558cb28d3659624f8f73ef4ced71fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y3AvuuSdpMveKrRlT6wBjg.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 16:XG Boost 的超参数调整</figcaption></figure><p id="ce96" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如下图 17 所示，我们拟合了最终的 XG Boost 模型，并在超参数调整期间根据最佳拟合模型生成了预测。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="bffb" class="ly kw hi lu b fi lz ma l mb mc">#XG Boost</span><span id="72fa" class="ly kw hi lu b fi md ma l mb mc"># Best Fit Model</span><span id="2386" class="ly kw hi lu b fi md ma l mb mc">Best_Fit_XG_Boost = xgb_Test_Accuracy_Data.loc[xgb_Test_Accuracy_Data['Test Accurcay'] == max(xgb_Test_Accuracy_Data['Test Accurcay'])]</span><span id="58ce" class="ly kw hi lu b fi md ma l mb mc">Best_Fit_XG_Boost = Best_Fit_XG_Boost.values.flatten().tolist()</span><span id="197e" class="ly kw hi lu b fi md ma l mb mc"># Fit  Model<br/>xgb_reg = xgb.XGBRegressor(n_estimators=Best_Fit_XG_Boost[0],max_depth =Best_Fit_XG_Boost[1],min_child_weight = Best_Fit_XG_Boost[2],gamma = Best_Fit_XG_Boost[3],learning_rate = Best_Fit_XG_Boost[4])<br/>xgb_reg.fit(features_train, target_train)</span><span id="0b93" class="ly kw hi lu b fi md ma l mb mc"># Use the forest's predict method on the test data<br/>XGB_Forecast = xgb_reg.predict(driver_forecasts)</span><span id="6be7" class="ly kw hi lu b fi md ma l mb mc"># De-Standardise <br/>XGB_Forecast_Adjusted_1 = (XGB_Forecast*std_target)+ mean_target<br/>XGB_Forecast_Adjusted_2 = pd.DataFrame({'XGB_Forecast_De_Standardise': XGB_Forecast_Adjusted_1[:]})</span><span id="8637" class="ly kw hi lu b fi md ma l mb mc">#Roll back first difference</span><span id="89d1" class="ly kw hi lu b fi md ma l mb mc">XGB_Forecast_Adjusted_2['XGB_Forecast_De_Standardise_R_Sum'] = XGB_Forecast_Adjusted_2['XGB_Forecast_De_Standardise'].cumsum()<br/>XGB_Forecast_Adjusted_2['XGB_Forecast'] = XGB_Forecast_Adjusted_2['XGB_Forecast_De_Standardise_R_Sum'] + Latest_target[0]</span><span id="48e6" class="ly kw hi lu b fi md ma l mb mc">XGB_Forecast_Adjusted_2.index = HWES_Forecast.index<br/>XGB_Forecast_Adjusted_2.head(5)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mz"><img src="../Images/64350bd32bb4c0b78a84c49fa1f29f5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aeC3StMxjfMHY4x5PItO-Q.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 17:使用 XGBoost 进行预测</figcaption></figure><p id="273c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 9。线性回归</strong></p><p id="7b6b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><a class="ae ku" href="https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf" rel="noopener ugc nofollow" target="_blank">线性回归</a>模型的目的是找出一个或多个特征与目标变量之间的关系。它可以用下面的等式来表示</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es na"><img src="../Images/ff916708fd52129a19ade25e8665c732.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/0*_Ijd4BVjTP2dEaHW.png"/></div></figure><ul class=""><li id="7e2f" class="kg kh hi jl b jm jn jp jq js ki jw kj ka kk ke ls km kn ko bi translated"><em class="kf"> Y </em>是模型预测</li><li id="74e9" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke ls km kn ko bi translated"><em class="kf"> θ </em> ₀是常数项。</li><li id="4e75" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke ls km kn ko bi translated"><em class="kf"> θ </em> ₁,…，<em class="kf"> θ </em> ₙ为模型系数</li><li id="8a95" class="kg kh hi jl b jm kp jp kq js kr jw ks ka kt ke ls km kn ko bi translated"><em class="kf"> x </em> ₁、<em class="kf"> x </em> ₂,…、<em class="kf"> x </em> ₙ为特征值</li></ul><p id="5e82" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们不需要对这个模型进行参数调整，因此可以直接对模型进行拟合和预测。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="4fcd" class="ly kw hi lu b fi lz ma l mb mc">#Linear Regression</span><span id="a83f" class="ly kw hi lu b fi md ma l mb mc">reg = LinearRegression().fit(features_final, target_final)</span><span id="5517" class="ly kw hi lu b fi md ma l mb mc">Regression_Forecast = reg.predict(driver_forecasts)</span><span id="442f" class="ly kw hi lu b fi md ma l mb mc">Regression_Forecast</span><span id="3be8" class="ly kw hi lu b fi md ma l mb mc">#XG Boost</span><span id="efc2" class="ly kw hi lu b fi md ma l mb mc">reg = LinearRegression().fit(features_train, target_train)</span><span id="9e66" class="ly kw hi lu b fi md ma l mb mc"># Use the forest's predict method on the test data<br/>Regression_Forecast = reg.predict(driver_forecasts)</span><span id="a949" class="ly kw hi lu b fi md ma l mb mc"># De-Standardise <br/>Regression_Forecast_Adjusted_1 = (Regression_Forecast*std_target)+ mean_target<br/>Regression_Forecast_Adjusted_2 = pd.DataFrame({'Regression_Forecast_De_Standardise': Regression_Forecast_Adjusted_1[:]})</span><span id="c933" class="ly kw hi lu b fi md ma l mb mc">#Roll back first difference</span><span id="29c0" class="ly kw hi lu b fi md ma l mb mc">Regression_Forecast_Adjusted_2['Regression_Forecast_De_Standardise_R_Sum'] = Regression_Forecast_Adjusted_2['Regression_Forecast_De_Standardise'].cumsum()<br/>Regression_Forecast_Adjusted_2['Regression_Forecast'] = Regression_Forecast_Adjusted_2['Regression_Forecast_De_Standardise_R_Sum'] + Latest_target[0]</span><span id="8aef" class="ly kw hi lu b fi md ma l mb mc">Regression_Forecast_Adjusted_2.index = HWES_Forecast.index<br/>Regression_Forecast_Adjusted_2.head(5)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nb"><img src="../Images/371bf7b31007ef755e93b58bff336778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wtc1B9BF6nGrgrR1WfJnHw.png"/></div></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 18:使用线性回归的预测</figcaption></figure><p id="1101" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><strong class="jl hj"> 10。SVR </strong></p><p id="769d" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">支持向量回归(SVR)是一种支持线性和非线性回归的支持向量机。预测的关键方程类似于线性回归，被称为超平面。超平面的任一侧上最接近它的数据点被称为支持向量，用于绘制边界线。SVR 试图在一个阈值内拟合最佳直线，即<strong class="jl hj"> </strong> SVR 模型试图满足条件:</p><p id="6b1c" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated"><code class="du nc nd ne lu b">-a &lt; y-wx+b &lt; a</code>其中 a 是阈值</p><p id="6a8a" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们尝试为这个模型调整两个参数:<em class="kf">正则化参数和核系数。</em>要了解更多关于 SVR 的信息，请参考玛丽埃特·阿瓦德和拉胡尔·康纳的这本<a class="ae ku" href="https://link.springer.com/chapter/10.1007/978-1-4302-5990-9_4" rel="noopener ugc nofollow" target="_blank">书</a></p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="b25f" class="ly kw hi lu b fi lz ma l mb mc">#SVR<br/># Regularization parameter <br/>C = [0.1, 1, 10, 100, 1000]<br/>#Kernel coefficient <br/>gamma = [1,5]</span><span id="0cbb" class="ly kw hi lu b fi md ma l mb mc">svr_Test_Accuracy_Data = pd.DataFrame(columns = ['C','gamma','Train Accurcay','Test Accurcay'])</span><span id="d01a" class="ly kw hi lu b fi md ma l mb mc">for x in list(itertools.product(C,gamma)):<br/>                svr_reg = SVR(kernel= 'rbf', C = x[0], gamma= x[1])<br/>                # Train the model on training data<br/>                svr_reg.fit(features_train, target_train)</span><span id="5166" class="ly kw hi lu b fi md ma l mb mc"># Train Data<br/>                # Use the forest's predict method on the train data<br/>                predictions_train = svr_reg.predict(features_train)<br/>                # Calculate the absolute errors<br/>                errors_train = abs(predictions_train - target_train)<br/>                 # Calculate mean absolute percentage error (MAPE)<br/>                mape_train = 100 * (errors_train/ target_train)<br/>                # Calculate and display accuracy<br/>                accuracy_train = 100 - np.mean(mape_train)</span><span id="c6a0" class="ly kw hi lu b fi md ma l mb mc"># Test Data<br/>                # Use the forest's predict method on the test data<br/>                predictions_test = svr_reg.predict(features_test)<br/>                # Calculate the absolute errors<br/>                errors_test = abs(predictions_test - target_test)<br/>                 # Calculate mean absolute percentage error (MAPE)<br/>                mape_test = 100 * (errors_test/ target_test)<br/>                # Calculate and display accuracy<br/>                accuracy_test = 100 - np.mean(mape_test)</span><span id="4007" class="ly kw hi lu b fi md ma l mb mc">svr_Test_Accuracy_Data_One = pd.DataFrame(index = range(1),columns = ['C','gamma','Train Accurcay','Test Accurcay'])</span><span id="afe2" class="ly kw hi lu b fi md ma l mb mc">svr_Test_Accuracy_Data_One.loc[:,'C'] = x[0]<br/>                svr_Test_Accuracy_Data_One.loc[:,'gamma'] = x[1]<br/>                svr_Test_Accuracy_Data_One.loc[:,'Train Accurcay'] = accuracy_train<br/>                svr_Test_Accuracy_Data_One.loc[:,'Test Accurcay'] = accuracy_test</span><span id="876f" class="ly kw hi lu b fi md ma l mb mc">svr_Test_Accuracy_Data = svr_Test_Accuracy_Data.append(svr_Test_Accuracy_Data_One)</span><span id="ff48" class="ly kw hi lu b fi md ma l mb mc">svr_Test_Accuracy_Data</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nf"><img src="../Images/c2644bb93605c57dfa3e25fea6b1ae10.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*YYsPwZgL9iA0z4HieCoz_A.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 19:SVR 的超参数调整</figcaption></figure><p id="19b8" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">类似于 LSTM、随机森林和 XG Boost，我们基于测试数据集上的最佳拟合模型来拟合最终模型。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="6fef" class="ly kw hi lu b fi lz ma l mb mc"># SVR</span><span id="21b1" class="ly kw hi lu b fi md ma l mb mc"># Best Fit Model</span><span id="07ef" class="ly kw hi lu b fi md ma l mb mc">Best_Fit_SVR = svr_Test_Accuracy_Data.loc[svr_Test_Accuracy_Data['Test Accurcay'] == max(svr_Test_Accuracy_Data['Test Accurcay'])]</span><span id="bb92" class="ly kw hi lu b fi md ma l mb mc">Best_Fit_SVR = Best_Fit_SVR.values.flatten().tolist()</span><span id="1654" class="ly kw hi lu b fi md ma l mb mc"># Fit  Model<br/>svr_reg = SVR(kernel= 'rbf', C = Best_Fit_SVR[0], gamma= Best_Fit_SVR[1])<br/># Train the model on training data<br/>svr_reg.fit(features_train, target_train)</span><span id="9066" class="ly kw hi lu b fi md ma l mb mc"># Use the forest's predict method on the test data<br/>svr_Forecast = svr_reg.predict(driver_forecasts)</span><span id="fa29" class="ly kw hi lu b fi md ma l mb mc"># De-Standardise <br/>svr_Forecast_Adjusted_1 = (svr_Forecast*std_target)+ mean_target<br/>svr_Forecast_Adjusted_2 = pd.DataFrame({'svr_Forecast_De_Standardise': svr_Forecast_Adjusted_1[:]})</span><span id="bfc1" class="ly kw hi lu b fi md ma l mb mc">#Roll back first difference</span><span id="4c39" class="ly kw hi lu b fi md ma l mb mc">svr_Forecast_Adjusted_2['svr_Forecast_De_Standardise_R_Sum'] = svr_Forecast_Adjusted_2['svr_Forecast_De_Standardise'].cumsum()<br/>svr_Forecast_Adjusted_2['SVM_Forecast'] = svr_Forecast_Adjusted_2['svr_Forecast_De_Standardise_R_Sum'] + Latest_target[0]</span><span id="24c5" class="ly kw hi lu b fi md ma l mb mc">svr_Forecast_Adjusted_2.index = HWES_Forecast.index<br/>svr_Forecast_Adjusted_2.head(5)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es ng"><img src="../Images/e8077e4240ca45b9d36b41299c00e349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*L9y20CPay4UiMdPTPv8EFA.png"/></div><figcaption class="mf mg et er es mh mi bd b be z dx translated">图 20:使用 SVR 的预测</figcaption></figure><h1 id="4737" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">最终预测</h1><p id="f627" class="pw-post-body-paragraph jj jk hi jl b jm ln ij jo jp lo im jr js nh ju jv jw ni jy jz ka nj kc kd ke hb bi translated">在大多数项目中，我们不能确定单个模型的适用性，这会干扰预测的质量。根据我的项目经验，解决这个问题的最好方法是在许多模型之间达成共识。这里的想法是，一些模型将正好适合预测点，而一些将低估或高估。通过平均，我们可以消除高估和低估。这正是我们在预测引擎中所做的。</p><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="d9a9" class="ly kw hi lu b fi lz ma l mb mc">Final_Forecast = pd.concat([HWES_Forecast,SARIMA_Forecast,SARIMAX_Forecast['SARIMAX_Forecast'],VAR_Forecast,VECM_Forecast,LSTM_Forecast_Adjusted_2['LSTM_Forecast'],Random_Forest_Forecast_Adjusted_2['Random_Forest_Forecast'],XGB_Forecast_Adjusted_2['XGB_Forecast'],svr_Forecast_Adjusted_2['SVM_Forecast'],Regression_Forecast_Adjusted_2['Regression_Forecast']],1)<br/>Final_Forecast['Final_Forecast'] = Final_Forecast.mean(axis=1)<br/>Final_Forecast.tail(5)</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nk"><img src="../Images/c2eeb3ec8f04cbc38c72dfb54e36d991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YGMOvL-fuY2c_7z2E0jC_A.png"/></div></div></figure><pre class="iy iz ja jb fd lt lu lv lw aw lx bi"><span id="852f" class="ly kw hi lu b fi lz ma l mb mc">Final_Forecast.plot()</span></pre><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es nl"><img src="../Images/3fd9339f3e263403696523c45fc214f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pTYQd3caYLNeM8F3gpC_zg.png"/></div></div></figure><p id="45ad" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">如果我们清楚模型的适用性，我们也可以做加权平均，而不是简单的平均，或者我们也可以根据测试数据性能取前 n 个模型的平均值。最终的预测应该基于手头的业务问题，但是创建一个健壮的预测引擎所需的所有工具都可以在这个博客系列中找到。</p><h1 id="6731" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated"><strong class="ak">结论</strong></h1><p id="9b16" class="pw-post-body-paragraph jj jk hi jl b jm ln ij jo jp lo im jr js nh ju jv jw ni jy jz ka nj kc kd ke hb bi translated">对产品或服务的需求不断变化。没有哪家企业能够在不准确估计消费者需求和产品/服务未来销售的情况下提高其财务业绩。这个博客系列描述了一种预测方法，它利用了时间序列分析和人工智能模型的优点。在实践中，建议方法的细节，如要使用的最终模型列表，需要根据业务需求进行适当的调整和测试。</p><p id="17ea" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我们可以将预测方法的主要优势分为两类:</p><ol class=""><li id="bcbf" class="kg kh hi jl b jm jn jp jq js ki jw kj ka kk ke kl km kn ko bi translated"><strong class="jl hj">一种创新的特征工程和选择方法:</strong>预测引擎不仅能够从现有数据集中提取相关特征，如滞后、周期差异等，还允许用户轻松添加第三方数据集，如节假日标志、公司股价和标准普尔 500 指数&amp;。这是一个非常强大的组合，因为它允许用户基于各种角度创建一个全面的驱动程序列表。然后，我们接着使用一个人工智能驱动的特征选择模块，该模块使用一组 5 种不同的算法。它有一种创新的方法来结合多种算法的结果，使我们的特征选择更加稳健，并帮助我们确定目标变量的最强驱动因素。</li></ol><p id="e417" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">2.<strong class="jl hj">结合</strong> <strong class="jl hj">一流的人工智能和时间序列模型:</strong>预测本质上属于时间序列分析的范畴。我们的预测引擎不仅使用了一系列最强大的时间序列模型，如 SARIMA、VAR 和 VECM，还有效地将流行的人工智能模型如 LSTM、XGboost 和 Random Forest 应用于时间序列数据集。从确保平稳性和标准化到选择测试数据集，人工智能模型在每一步都保护数据集的时间序列属性。再加上广泛的超参数调整，我们拥有一个强大的预测引擎。</p><p id="550b" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">我希望这个博客系列能够帮助我的数据科学家同事们追求卓越的预测能力！整个分析的代码可以在<a class="ae ku" href="https://github.com/IDB-FOR-DATASCIENCE/Predictive-Excellence-Engine.git" rel="noopener ugc nofollow" target="_blank">这里</a>找到。我将继续关注一个关于细分和聚类的有趣系列。如果你喜欢这个系列，请鼓掌。一如既往，任何想法，评论或反馈都将不胜感激。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div class="er es nm"><img src="../Images/ca3fd321a104f6688ba3de9ea76f5f21.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/0*abY5od6h7VoIsUMJ"/></div></figure><p id="7e76" class="pw-post-body-paragraph jj jk hi jl b jm jn ij jo jp jq im jr js jt ju jv jw jx jy jz ka kb kc kd ke hb bi translated">你对这个博客有什么问题或建议吗？请随时留言。</p><h1 id="41b8" class="kv kw hi bd kx ky kz la lb lc ld le lf io lg ip lh ir li is lj iu lk iv ll lm bi translated">感谢您的阅读！</h1><p id="0b50" class="pw-post-body-paragraph jj jk hi jl b jm ln ij jo jp lo im jr js nh ju jv jw ni jy jz ka nj kc kd ke hb bi translated">如果你和我一样，对人工智能、数据科学或经济学充满热情，请随时添加/关注我的<a class="ae ku" href="http://www.linkedin.com/in/indraneel-dutta-baruah-ds" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>、<a class="ae ku" href="https://github.com/IDB-FOR-DATASCIENCE" rel="noopener ugc nofollow" target="_blank"> Github </a>和<a class="ae ku" rel="noopener" href="/@indraneeldb1993ds"> Medium </a>。</p></div></div>    
</body>
</html>