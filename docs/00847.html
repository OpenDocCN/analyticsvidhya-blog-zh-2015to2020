<html>
<head>
<title>Understanding OpenPose (with code reference)— Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解OpenPose(带代码参考)—第1部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-openpose-with-code-reference-part-1-b515ba0bbc73?source=collection_archive---------0-----------------------#2019-09-13">https://medium.com/analytics-vidhya/understanding-openpose-with-code-reference-part-1-b515ba0bbc73?source=collection_archive---------0-----------------------#2019-09-13</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="86a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">人体2D姿态估计是从输入图像或视频中定位人体部位如肩膀、肘部和脚踝的问题。在当今人类姿态估计的大多数真实世界应用中，需要高度的准确性以及“实时”推断。</p><p id="9547" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由卡内基梅隆大学的研究人员开发的OpenPose可以被认为是实时人体姿态估计的最先进方法。代码库在<a class="ae jd" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" rel="noopener ugc nofollow" target="_blank"> github </a>上是开源的，并且有很好的文档记录。Openpose最初是用C++和Caffe写的。</p><p id="9f34" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在整篇文章中，我可能还会引用来自<a class="ae jd" href="https://github.com/ildoonet/tf-pose-estimation" rel="noopener ugc nofollow" target="_blank">的一些代码，这里是</a>，OpenPose的一个精确Tensorflow实现。以这篇文章为起点，然后阅读整篇文章，因为为了节省空间，我已经从文章中删除了一些具体的细节。</p><p id="7790" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这篇文章分为三个不同的部分。第一部分将分析OpenPose的整体设置；本文中使用的主要神经网络体系结构和常用符号。第二部分将详细介绍置信度图和零件相似度图。文章的第三部分将讨论如何通过将问题视为图形匹配问题，从神经网络的输出中最终正确地组装关键点。</p><p id="2117" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们进入OpenPose的细节之前，值得注意的是存在两个版本的论文<a class="ae jd" href="https://arxiv.org/abs/1611.08050" rel="noopener ugc nofollow" target="_blank"> this </a>和<a class="ae jd" href="https://arxiv.org/abs/1812.08008" rel="noopener ugc nofollow" target="_blank"> this </a>。第一份原始论文于2016年11月24日提交，最近一份于2018年12月18日提交。有几个小的区别，如神经网络架构和一些后处理方面，从而提高了速度和准确性。然而，总体思路和总体流水线仍然是相同的。关于这些差异的更多细节，你可以在最近的文章<a class="ae jd" href="https://arxiv.org/abs/1812.08008" rel="noopener ugc nofollow" target="_blank">的第一部分找到。在本文中，我们将探索本文的原始版本，因为在撰写本文时，github上的大多数实现仍在使用第一篇文章中描述的步骤。</a></p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><h1 id="e3b2" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">整体管道</h1><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es kj"><img src="../Images/3af18be3ea6689502214317bf067fcbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F4SpSzxaqEjeJ-e4ia-wJQ.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图一。整体管道。图像取自“使用部分亲和场的实时多人2D姿势估计”。</figcaption></figure><p id="20fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">OpenPose的管道实际上非常简单明了。</p><p id="5e44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">首先</strong>，一个输入的RGB图像(图1a)作为输入馈入一个<strong class="ih hj"> <em class="kz">【两分支多级】</em> </strong> CNN。两个分支意味着CNN产生两种不同的输出。多阶段简单地说就是网络在每个阶段都是一层一层地堆叠起来。(<em class="kz">这一步类似于简单地增加神经网络的深度，以便为后面的阶段捕捉更精确的输出。</em>)</p><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es la"><img src="../Images/134f6e49e0fdc4e970496f4d1c08ed52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FEMui63FL-znpL64lgf7Mw.png"/></div></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图二。两分支多级CNN的体系结构。图像取自“使用部分亲和场的实时多人2D姿势估计”。</figcaption></figure><p id="5df5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">两个分支:</strong>顶部分支，以米色显示，预测不同身体部位位置的置信图(图1b)，例如右眼、左眼、右肘和其他部位。以蓝色显示的底部分支预测亲和场(图1c)，其表示不同身体部分之间的关联程度。</p><p id="d475" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">多阶段:</strong>在第一阶段<strong class="ih hj"> ( <em class="kz">图2的左半部分</em> ) </strong>，网络产生一组初始的检测置信图<strong class="ih hj"> S </strong>和一组零件亲和场<strong class="ih hj">l</strong>然后，在每个后续阶段<strong class="ih hj"> ( <em class="kz">图2的右半部分</em> ) </strong>，来自前一阶段两个分支的预测，连同原始图像特征<strong class="ih hj"> F在OpenPose实现中，最终阶段<strong class="ih hj"> <em class="kz"> t </em> </strong>被选择为6。</strong></p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es lb"><img src="../Images/bb1ac0af11119f2a4792282c34ba686f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1374/format:webp/1*8ezsvcv4aQFDkePkZouBAg.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图三。多级网络的结果。<strong class="bd jn">顶部</strong>行显示了网络预测右手腕的置信图，而<strong class="bd jn">底部</strong>行显示了网络预测右前臂(右肩-右手腕)在不同阶段的部分亲和场。</figcaption></figure><p id="ebf4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">图3显示了多阶段设置的积极益处。在这个例子中，我们观察到在最初的几个阶段，在身体的左右部分之间有一些最初的混淆。但是随着阶段的进展，网络变得更善于做出这些区分。</p><p id="2706" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">最后</strong>，置信图和亲和场正被贪婪推理处理(图1d)，以输出图像中所有人的2D关键点(图1e)。</p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><h1 id="0d34" class="jl jm hi bd jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki bi translated">置信图</h1><p id="e654" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">返回参考图2，神经网络的顶部分支产生一组检测置信图<strong class="ih hj"> S </strong>。这在数学上定义如下。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es lh"><img src="../Images/4d564ed92fa3b36bc3656d23fe1f3710.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*dcp8P7aVvWy4FcVDTASBzw.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图4。集合S的数学表达式</figcaption></figure><p id="a7ef" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> J，</strong>身体部位的总数取决于训练OpenPose的数据集。对于COCO数据集，J = 19，因为有18个不同的身体关键点+ 1个背景。下图显示了COCO数据集的不同身体部位及其分配的ID。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es li"><img src="../Images/a595c3b9855048df9ef1f340247da01c.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*bw8pR21l9i9otJoU5FlGiA.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图五。COCO数据集的关键点ID</figcaption></figure><p id="d131" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好地理解集合<strong class="ih hj"> S </strong>代表什么，考虑这个例子。对于用COCO数据集训练的模型，集合<strong class="ih hj"> S </strong>将具有<strong class="ih hj"> S1、S2、S3、…、S19 </strong>的元素。对于这个例子，让我们假设元素<strong class="ih hj"> S1 </strong>对应于关键点id为0(在图5中)的置信图，该关键点id指的是鼻子。然后，置信图可能如下所示。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es lj"><img src="../Images/ce86d4f247b0a8f4620fd306ff7638db.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*_uye5wz7bi3jp21-eEPSTA.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图六。这是一个非常简单的图表，显示了一个置信度图，其中表格中的每个单元格对应于原始图像中尺寸为w x h的一个像素。每个单元格中的值表示鼻子存在的置信度。</figcaption></figure><p id="3d07" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在图6中，我们假设全图的宽度和高度为5，从而得到5×5的置信图。在这个例子中，图片中只有一张脸。因此，对于置信图<strong class="ih hj"> S1(预测检测鼻子的置信)，</strong>我们仅在有鼻子的区域中看到高置信区域，0.9。</p><h1 id="fa26" class="jl jm hi bd jn jo lk jq jr js ll ju jv jw lm jy jz ka ln kc kd ke lo kg kh ki bi translated">零件亲缘场(PAF)图</h1><p id="3081" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">返回参考图2，神经网络的底部分支产生一组零件亲和场图<strong class="ih hj"> L </strong>。这在数学上定义如下。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es lp"><img src="../Images/0faa94fcbd93de3721d5ec06863c2d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*oiQN-fdEli_Mgk_0HBQekw.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图7。集合l的数学表达式。</figcaption></figure><p id="8f62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> C，</strong>肢体总数，取决于OpenPose的训练数据集。为了清楚起见，该论文将部分对称为肢体，尽管事实上一些身体部分对不是人的肢体。对于COCO数据集，<strong class="ih hj"> C </strong> = 19。下图显示了不同的零件对。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es lq"><img src="../Images/bcfaec1bae04c13d9e73a337fd9417e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*elJidoH_D32EmC0zk3cPDg.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图8。元组的数组。每个元组对表示身体部位ID对。</figcaption></figure><p id="c57e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您可以想象集合<strong class="ih hj"> L </strong>中的每个元素是一个大小为w x h的映射，其中每个单元格包含一个表示元素对方向的2d向量。例如，在图1c中，身体部位对由右肩到右肘组成。该图显示了一个从右肩指向右肘的方向向量。</p><p id="53e4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们对数学符号及其代表的意义有了更好的理解，我们可以进入下一部分了。</p><h1 id="f6ae" class="jl jm hi bd jn jo lk jq jr js ll ju jv jw lm jy jz ka ln kc kd ke lo kg kh ki bi translated">神经网络细节</h1><p id="e7a7" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">图像首先由预训练的卷积神经网络(例如VGG-19的前10层)进行分析，以产生一组特征图<strong class="ih hj"> F </strong>。这种选择特征提取器产生<strong class="ih hj"> F </strong>并不局限于VGG-19。OpenPose还有<a class="ae jd" href="https://github.com/ildoonet/tf-pose-estimation" rel="noopener ugc nofollow" target="_blank">的其他</a>变体，它使用Mobilenet或Resnet来提取图像特征，然后将其传递给图2所示的神经网络的其余部分。</p><p id="4cfc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">阶段1: </strong>网络产生一组检测置信图<strong class="ih hj"> S </strong>和一组部分相似性字段<strong class="ih hj"> L. </strong>符号<strong class="ih hj"> 𝛒 </strong>被用作表示具有输入<strong class="ih hj"> F </strong>的CNN的函数变量以产生输出图<strong class="ih hj"> S. </strong>符号<strong class="ih hj"> 𝛟 </strong>被用作表示具有输入<strong class="ih hj"> F </strong> to的CNN的函数变量</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es lr"><img src="../Images/0853ea25eb0c4a8ffe2217bf86b7ffed.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*eM0TgApllKZztgLZpbB4LQ.png"/></div></figure><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es lr"><img src="../Images/b5fc65ef19f054b730d2624131ac94da.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*7h2lQShFECgYfIWldchlqQ.png"/></div></figure></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><p id="d5a5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">阶段t: </strong>来自前一阶段的两个分支的预测，连同原始图像特征<strong class="ih hj"> F </strong>，被连接并用于产生更精确的预测。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="er es ls"><img src="../Images/4b9c8eb28185b36cffe891b587d268ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xbIBdi5QYk21wq4C9C23rA.png"/></div></div></figure><p id="385b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在OpenPose论文中，<strong class="ih hj"> t </strong>从2到6。上图中的逗号表示地图之间的连接。</p></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><p id="14b8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">损失函数:</strong>为了让网络学习如何生成最佳的<strong class="ih hj"> S </strong>和<strong class="ih hj"> L，</strong>集合，作者在每个阶段的末尾应用两个损失函数，分别在每个分支应用一个。该论文在估计的预测和地面真实地图和字段之间使用了标准的L2损失。(<em class="kz">我们稍后还将看到作者如何想出一种方法来为每个</em> <strong class="ih hj"> <em class="kz"> S </em> </strong> <em class="kz">和</em> <strong class="ih hj"> <em class="kz"> L </em> </strong>)创建地面真值图)。此外，作者给损失函数增加了一些权重，以解决一些数据集不能完全标记所有人的实际问题。特定阶段<strong class="ih hj"> t </strong>的损失函数如下所示。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es lt"><img src="../Images/9d9a779b9fa4ca4f7792f29662f18cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*dPFmjRbXDVnMYkiAvowyEA.png"/></div></figure><ol class=""><li id="cde1" class="lu lv hi ih b ii ij im in iq lw iu lx iy ly jc lz ma mb mc bi translated">符号<strong class="ih hj"> p </strong>表示在<strong class="ih hj"> w </strong> x <strong class="ih hj"> h </strong>图像中的单个像素位置。</li><li id="4cf0" class="lu lv hi ih b ii md im me iq mf iu mg iy mh jc lz ma mb mc bi translated">set <strong class="ih hj"> S </strong>和<strong class="ih hj"> L </strong>旁边的<strong class="ih hj"> * </strong>符号表示这是地面实况</li><li id="9d68" class="lu lv hi ih b ii md im me iq mf iu mg iy mh jc lz ma mb mc bi translated"><strong class="ih hj"> S(p) </strong>的输出是一维向量，其由图像位置<strong class="ih hj"> p. </strong>处的特定身体部位<strong class="ih hj"> j </strong>的置信度得分组成</li><li id="6e86" class="lu lv hi ih b ii md im me iq mf iu mg iy mh jc lz ma mb mc bi translated">L <strong class="ih hj"> (p) </strong>的输出是一个二维向量，它由图像位置<strong class="ih hj"> p. </strong>处的特定肢体<strong class="ih hj"> c </strong>的方向向量组成</li><li id="4fc4" class="lu lv hi ih b ii md im me iq mf iu mg iy mh jc lz ma mb mc bi translated">在OpenPose论文中，<strong class="ih hj"> J，</strong>身体部位的总数是19。同样，<strong class="ih hj"> C </strong>,“肢体”或身体到身体连接的总数是19。</li><li id="6732" class="lu lv hi ih b ii md im me iq mf iu mg iy mh jc lz ma mb mc bi translated"><strong class="ih hj"> W(p) </strong>代表前面提到的称重功能。<strong class="ih hj"> W(p) = 0 </strong>当注释在图像位置<strong class="ih hj"> p. </strong>处丢失时，使用掩码来避免在训练期间惩罚真正的正面预测。</li></ol></div><div class="ab cl je jf gp jg" role="separator"><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj jk"/><span class="jh bw bk ji jj"/></div><div class="hb hc hd he hf"><p id="b3ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">总体损失函数:</strong>最后，结合两个损失函数，我们得出总体目标。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es mi"><img src="../Images/a20d1dd30ffc434ddab6da04c6794bbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*NtVifzXCAQr3za3KAe8P3g.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">总体目标</figcaption></figure><h1 id="144a" class="jl jm hi bd jn jo lk jq jr js ll ju jv jw lm jy jz ka ln kc kd ke lo kg kh ki bi translated">神经网络实现(在Caffe中)</h1><p id="c68d" class="pw-post-body-paragraph if ig hi ih b ii lc ik il im ld io ip iq le is it iu lf iw ix iy lg ja jb jc hb bi translated">OpenPose的作者使用<a class="ae jd" href="https://caffe.berkeleyvision.org/" rel="noopener ugc nofollow" target="_blank"> Caffe </a>实现神经网络。如果你不熟悉Caffe也不用担心，和其他很多深度学习框架一样，Caffe非常直观易懂。Caffe模型在中定义。prototxt文件。下面是使用Caffe定义的神经网络模型的截断版本。完整的模型文件占用了很多空间，因此我决定只显示前几行。你可以在这里找到<a class="ae jd" href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/models/pose/coco/pose_deploy_linevec.prototxt" rel="noopener ugc nofollow" target="_blank">的完整型号定义。</a></p><figure class="kk kl km kn fd ko"><div class="bz dy l di"><div class="mj mk l"/></div></figure><p id="8474" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了更好地可视化神经网络架构，我们使用一个网络可视化工具，如<a class="ae jd" href="https://ethereon.github.io/netscope/quickstart.html" rel="noopener ugc nofollow" target="_blank">https://ethereon.github.io/netscope/quickstart.html</a>，它将文本转换成一些更容易理解的可视化。我建议你亲自尝试一下，因为它有一些交互性，你可以悬停在那里，它会向你显示每个模块的更多细节。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es ml"><img src="../Images/a3f2da975b69d18b6f54b1b72ca3e385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*DDqbX6CXER6QEjsSb9TJTw.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图九。openpose神经网络前几层的快照。该部分对应于神经网络的一部分，在该部分中，它试图生成一组特征图</figcaption></figure><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es mm"><img src="../Images/7b2e98a2139584a55c009ab3157d3077.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*_RWO2qJ5cyT7gK92Iu1qnQ.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图10。openpose神经网络前几层的快照，续图9。该部分对应于神经网络的一部分，在该部分中，它试图生成一组特征图</figcaption></figure><p id="8d14" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这里需要注意的一个<strong class="ih hj">重要点</strong>是模块<strong class="ih hj"> relu4_4_CPM </strong>的输出是论文中描述的图像特征<strong class="ih hj"> F </strong>的集合(图2)。这组图像特征<strong class="ih hj"> F </strong>与来自图2所示的两个分支的预测连接在一起，以在后面的阶段产生更精确的预测。</p><h1 id="eae7" class="jl jm hi bd jn jo lk jq jr js ll ju jv jw lm jy jz ka ln kc kd ke lo kg kh ki bi translated">第一阶段</h1><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es lh"><img src="../Images/cb38a9bc10c18ec69da139226257cd57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*gsz9lhAxIWUoNnKMLXa-IA.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图11。神经网络第<strong class="bd jn">阶段的快照。</strong></figcaption></figure><p id="5069" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如图11所示，你可以看到模块<strong class="ih hj">“relu 4 _ 4 _ CPM”</strong>的输出被送入两个模块“<strong class="ih hj"> conv5_1_CPM_L1 </strong>”和“<strong class="ih hj"> conv5_1_CPM_L2 </strong>”。第一个模块对应于图2的<strong class="ih hj">底部</strong>分支<em class="kz">(预测PAF向量组)</em>，而第二个模块对应于图2的<strong class="ih hj">顶部</strong>分支<em class="kz">(预测置信向量组)</em>。“<strong class="ih hj"> conv5_5_CPM_L2 </strong>的输出维数为(w x h x 19)，其中19对应于COCO数据集中的19个不同的关键点。“<strong class="ih hj"> conv5_5_CPM_L1 </strong>的输出维度为(w x h x 38)，其中<em class="kz"> 38 = 19 * 2 </em>对应于COCO数据集中定义的19个不同的“肢体”。并且乘以2，因为每个肢体的图中的每个单元表示具有x&amp;y值的向量。</p><h1 id="740e" class="jl jm hi bd jn jo lk jq jr js ll ju jv jw lm jy jz ka ln kc kd ke lo kg kh ki bi translated">阶段t</h1><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es mn"><img src="../Images/2b1ad25bc4a66893044109bef39a3b57.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*7QpmqperP4IOfeq9IdMGbQ.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图12。神经网络的第二个阶段<strong class="bd jn">的快照。</strong></figcaption></figure><p id="cbf3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如图12所示，这里需要注意的一点是级联级采用三个输入。即<strong class="ih hj"> F </strong>和第一级<strong class="ih hj"> S和l</strong>的输出，然后再次馈入两个不同的分支。这个过程重复进行，直到t = 6，最终返回最精确的值。</p><figure class="kk kl km kn fd ko er es paragraph-image"><div class="er es mo"><img src="../Images/1c17d96087e8aa07d82474db23c0d249.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*auUagbVBcYFHVYUDD1424A.png"/></div><figcaption class="kv kw et er es kx ky bd b be z dx translated">图13。神经网络的<strong class="bd jn">最终</strong>阶段(阶段6)的快照。</figcaption></figure><p id="195c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最终的输出然后被连接并返回进行贪婪匹配，这将在本文接下来的几个部分中讨论。</p></div></div>    
</body>
</html>