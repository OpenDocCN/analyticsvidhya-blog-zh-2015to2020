<html>
<head>
<title>Predict Artist From Art Using Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用深度学习从艺术中预测艺术家</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/predict-artist-from-art-using-deep-learning-9f465f8879d7?source=collection_archive---------2-----------------------#2019-12-19">https://medium.com/analytics-vidhya/predict-artist-from-art-using-deep-learning-9f465f8879d7?source=collection_archive---------2-----------------------#2019-12-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="b98f" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">美术绘画的艺术家鉴定</h2></div><h1 id="91d4" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">问题陈述</h1><p id="7128" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">从艺术绘画中预测艺术家是一项具有挑战性的任务，这项任务主要由受过广泛训练和专业知识的艺术史学家来完成。艺术家识别是在没有其他信息的情况下识别一幅画的艺术家的任务。这是对艺术品编目的一个重要要求(做一个同类型物品的系统清单)，尤其是在艺术品日益数字化的情况下。艺术作品的收藏越来越多，目的是让所有的艺术作品都可以很容易地在网上获得。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es kl"><img src="../Images/33589aef838a7b7144693153cccfbcda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJVU7lksF74HdG05xDWddA.jpeg"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">迪米特拉米兰肖像艺术链接:https://in.pinterest.com/pin/522628731742912888/?lp =真</figcaption></figure><p id="2272" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">一种可靠的鉴别艺术家的方法不仅有助于给艺术品贴标签，也有助于鉴别赝品，这是另一个艺术史问题。因此，我决定致力于此，而以前人们已经在研究论文和Kaggle竞赛中使用不同的神经网络技术进行了这方面的工作。我想，为什么不将像ResNet50神经网络这样的先进技术应用于图像分类任务呢？让我们开始实际实施本案例研究。</p><h1 id="3a6f" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">深度学习解决问题的大任务</h1><p id="f832" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">在这里阅读一篇研究论文<a class="ae lg" href="https://www.ripublication.com/ijaer17/ijaerv12n4_17.pdf" rel="noopener ugc nofollow" target="_blank">时</a>，我开始了解这个问题，这激发了我的工作热情，因为我经常画素描和油画。深度学习的重大任务是通过给定的一种艺术来预测艺术家。我们知道计算机不理解像给定的文本和图像这样的直接交互。计算机不会解决我们的问题。我们必须将这些文本和图像转换为矢量形式，或者我们可以说二进制形式为0，1。<strong class="jr hj">如何将图像转换成计算机可以理解的矢量形式？</strong>这里有一个简单的图像，用来理解转换成向量/矩阵形式的二进制数据为0，1的图像。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es lh"><img src="../Images/ae213793f52e32c8252524372d6e79fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*0GKsw7NEg5iicbNlqVpfsg.jpeg"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><a class="ae lg" href="https://www.google.com/search?q=image+representation+in+matrix+form&amp;rlz=1C1CHBD_enIN772IN772&amp;sxsrf=ACYBGNTNdQfGQ2RojplRlVCj5XLaNT5ecw:1576783113628&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwjonIDrtsLmAhXA63MBHXmEACoQ_AUoAXoECA4QAw&amp;biw=1366&amp;bih=614#imgrc=TJVTRO-LW-GquM:" rel="noopener ugc nofollow" target="_blank">图片</a></figcaption></figure><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es li"><img src="../Images/6491aaf752d23694a494750de43f3208.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/format:webp/1*VNGO2sPqaeLjF4-0JtSSFA.jpeg"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><a class="ae lg" href="https://www.google.com/search?q=image+representation+in+matrix+form&amp;rlz=1C1CHBD_enIN772IN772&amp;sxsrf=ACYBGNTNdQfGQ2RojplRlVCj5XLaNT5ecw:1576783113628&amp;source=lnms&amp;tbm=isch&amp;sa=X&amp;ved=2ahUKEwjonIDrtsLmAhXA63MBHXmEACoQ_AUoAXoECA4QAw&amp;biw=1366&amp;bih=614#imgrc=TJVTRO-LW-GquM:" rel="noopener ugc nofollow" target="_blank">图片</a></figcaption></figure><h1 id="15a4" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated"><strong class="ak">导入库</strong></h1><p id="e497" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">直到现在我们才意识到这个案例研究的问题所在。现在是时候导入python库来开始分析了。python包的一大优势是它包含大量用于数据分析的库。</p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="8fb3" class="lo iy hi lk b fi lp lq l lr ls"><strong class="lk hj">import</strong> <strong class="lk hj">pandas</strong> <strong class="lk hj">as</strong> <strong class="lk hj">pd</strong><br/><strong class="lk hj">import</strong> <strong class="lk hj">numpy</strong> <strong class="lk hj">as</strong> <strong class="lk hj">np</strong><br/><strong class="lk hj">import</strong> <strong class="lk hj">matplotlib.pyplot</strong> <strong class="lk hj">as</strong> <strong class="lk hj">plt</strong><br/><strong class="lk hj">import</strong> <strong class="lk hj">json</strong><br/><strong class="lk hj">import</strong> <strong class="lk hj">os</strong><br/><strong class="lk hj">from</strong> <strong class="lk hj">tqdm</strong> <strong class="lk hj">import</strong> tqdm, tqdm_notebook<br/><strong class="lk hj">import</strong> <strong class="lk hj">random</strong><br/><br/><strong class="lk hj">import</strong> <strong class="lk hj">tensorflow</strong> <strong class="lk hj">as</strong> <strong class="lk hj">tf</strong><br/><strong class="lk hj">from</strong> <strong class="lk hj">tensorflow.keras.models</strong> <strong class="lk hj">import</strong> Sequential, Model<br/><strong class="lk hj">from</strong> <strong class="lk hj">tensorflow.keras.layers</strong> <strong class="lk hj">import</strong> *<br/><strong class="lk hj">from</strong> <strong class="lk hj">tensorflow.keras.optimizers</strong> <strong class="lk hj">import</strong> * <br/><strong class="lk hj">from</strong> <strong class="lk hj">tensorflow.keras.applications</strong> <strong class="lk hj">import</strong> *<br/><strong class="lk hj">from</strong> <strong class="lk hj">tensorflow.keras.callbacks</strong> <strong class="lk hj">import</strong> *<br/><strong class="lk hj">from</strong> <strong class="lk hj">tensorflow.keras.initializers</strong> <strong class="lk hj">import</strong> *<br/><strong class="lk hj">from</strong> <strong class="lk hj">tensorflow.keras.preprocessing.image</strong> <strong class="lk hj">import</strong> ImageDataGenerator<br/><br/><strong class="lk hj">from</strong> <strong class="lk hj">numpy.random</strong> <strong class="lk hj">import</strong> seed<br/>seed(1) #Seed function is used to save the state of random function<br/><strong class="lk hj">from</strong> <strong class="lk hj">tensorflow</strong> <strong class="lk hj">import</strong> set_random_seed<br/>set_random_seed(1)</span></pre><h1 id="3088" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">探索性数据分析</h1><p id="0438" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">我们已经完成了库的导入，现在下一个任务是探索性的数据分析，在这方面，我们必须获取数据并对其应用各种统计分析。做EDA的目的是在数据中寻找意义或模式。EDA告诉我们更深层次的数据视图，在那里我们可以理解问题。坦率地说，EDA是人工智能社区中的艺术过程，你应用得越好，你对数据的理解就越好。</p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="a98f" class="lo iy hi lk b fi lp lq l lr ls">print(os.listdir("../input"))</span></pre><p id="8208" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj">输入</strong>是主文件夹和里面的其余文件，所以代码处理访问这个文件。</p><p id="535c" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">借助熊猫数据框访问<strong class="jr hj"> artists.csv </strong></p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="89d5" class="lo iy hi lk b fi lp lq l lr ls">artists = pd.read_csv('../input/artists.csv')<br/>artists.shape<br/><em class="lt"># we have 50 rows and 8 col total.</em></span><span id="0108" class="lo iy hi lk b fi lu lq l lr ls">output: (50, 8)</span></pre><p id="7f10" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">我们已经阅读了csv文件，现在对下面的数据进行简单的分析，我已经做了一些简单的分析来理解文件。</p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="404f" class="lo iy hi lk b fi lp lq l lr ls">artists.info() #info() gives information about file<br/><em class="lt"># from here we can see our features</em></span><span id="f87d" class="lo iy hi lk b fi lu lq l lr ls">&lt;class 'pandas.core.frame.DataFrame'&gt;<br/>RangeIndex: 50 entries, 0 to 49<br/>Data columns (total 8 columns):<br/>id             50 non-null int64<br/>name           50 non-null object<br/>years          50 non-null object<br/>genre          50 non-null object<br/>nationality    50 non-null object<br/>bio            50 non-null object<br/>wikipedia      50 non-null object<br/>paintings      50 non-null int64<br/>dtypes: int64(2), object(6)<br/>memory usage: 3.2+ KB<br/></span></pre><p id="3c12" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">我们可以看到关于我们的文件的信息，它包含50行和8列(身份证，姓名，年份..等等)。</p><p id="0009" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj">数据预处理</strong></p><p id="6f56" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">数据预处理是一项任务，我们必须根据我们的解决策略操作数据，并将数据从原始数据引导到有意义的数据。这里我首先根据特征<strong class="jr hj">绘画</strong>对数据进行排序，这样我所有的数据都按照它们的绘画名称进行排序。我们已经为我们的分析取了200幅画&gt;只是取了样本数据。</p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="9faa" class="lo iy hi lk b fi lp lq l lr ls"><em class="lt">#https://en.wikipedia.org/wiki/Digital_image_processing</em><br/><em class="lt">#Adding some more col as part of data preprocessing</em><br/><em class="lt"># Sort artists by number of paintings</em><br/><em class="lt"># above code shows "painting" feature so as to group paintings to respecting painting.</em><br/>artists = artists.sort_values(by=['paintings'], ascending=<strong class="lk hj">False</strong>)</span><span id="caa8" class="lo iy hi lk b fi lu lq l lr ls"><em class="lt"># Create a dataframe with artists having more than 200 paintings</em><br/>artists_top = artists[artists['paintings'] &gt;= 200].reset_index()<br/>artists_top = artists_top[['name', 'paintings']]<br/><em class="lt">#artists_top['class_weight'] = max(artists_top.paintings)/artists_top.paintings</em><br/>artists_top['class_weight'] = artists_top.paintings.sum() / (artists_top.shape[0] * artists_top.paintings)<br/>artists_top</span></pre><p id="e06d" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj">特征工程</strong></p><p id="7be7" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">我做了一个<strong class="jr hj">功能设计</strong>只是<strong class="jr hj"> </strong>在这个名为“class_weight”的功能中增加了一个功能，这个功能增加了绘画的重量为什么我们想要<strong class="jr hj"> <em class="lt">重量</em> </strong>实际上是简单的增加重量，它告诉我们特定事物的重要性，所以我只是通过创建新功能<strong class="jr hj">来增加重量。</strong></p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="8d06" class="lo iy hi lk b fi lp lq l lr ls">artists_top['class_weight'] = artists_top.paintings.sum() / (artists_top.shape[0] * artists_top.paintings)</span></pre><p id="b72b" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">把重量放在这里是结果</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es lv"><img src="../Images/a1a136e3c494a15ba77256ed7a06d373.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/format:webp/1*IThwEkAIZ2RIka6W1H2GsQ.png"/></div></figure><p id="ce5f" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">首先，我们添加了新的要素class_weight，此处显示的是每行的权重。</p><p id="a04c" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj">获得顶级艺术家的画作</strong></p><p id="fdf2" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">在这段代码中，我们获取顶级艺术家的画作，并检查我的文件和名字是否存在。</p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="368a" class="lo iy hi lk b fi lp lq l lr ls"><em class="lt"># Explore images of top artists</em><br/>images_dir = '../input/images/images'  <em class="lt">#my files with this directory and folder you can change as u want to.</em><br/>artists_dirs = os.listdir(images_dir)<br/>artists_top_name = artists_top['name'].str.replace(' ', '_').values<br/><br/><em class="lt"># See if all directories exist</em><br/><strong class="lk hj">for</strong> name <strong class="lk hj">in</strong> artists_top_name:<br/>    <strong class="lk hj">if</strong> os.path.exists(os.path.join(images_dir, name)):<br/>        print("Found --&gt;", os.path.join(images_dir, name))<br/>    <strong class="lk hj">else</strong>:<br/>        print("Did not find --&gt;", os.path.join(images_dir, name))</span></pre><p id="53f3" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj">展示随机绘画</strong></p><p id="9e66" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">到目前为止，我们已经对工程和数据进行了预处理，并对我们的数据进行了一些操作，现在是时候展示一些随机的绘画，以便清晰地可视化/理解正在发生的事情了。</p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="3ce6" class="lo iy hi lk b fi lp lq l lr ls"><em class="lt">#https://www.analyticsvidhya.com/blog/2019/08/3-techniques-extract-features-from-image-data-machine-learning-python/</em><br/><em class="lt"># Print few random paintings</em><br/>n = 5  <em class="lt"># taking 5 random pic</em><br/>fig, axes = plt.subplots(1, n, figsize=(20,10))<br/><br/><strong class="lk hj">for</strong> i <strong class="lk hj">in</strong> range(n):<br/>    random_artist = random.choice(artists_top_name)<br/>    random_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))<br/>    random_image_file = os.path.join(images_dir, random_artist, random_image)<br/>    image = plt.imread(random_image_file)<br/>    axes[i].imshow(image)<br/>    axes[i].set_title("Artist: " + random_artist.replace('_', ' '))<br/>    axes[i].axis('off')<br/><br/>plt.show()</span></pre><p id="4183" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">在名为<strong class="jr hj"> matplotlib </strong>的python库的帮助下，我们可以显示图像并绘制图表，这里我们最多显示了5幅图像，在子图中我们有<strong class="jr hj"> figsize=(20，10) </strong>这基本上设置了图像的宽度和高度，您也可以设置自己的宽度和高度。<strong class="jr hj"> plt.imread() </strong>这允许我们读取图像文件。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es lw"><img src="../Images/4e569c0233c9f1b421fcaa90c7cd8874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XBR-J_CondgQKPZaRsCr6Q.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">5随机图像与艺术家的名字按照我们在代码中的设计</figcaption></figure><p id="c393" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj">数据扩充</strong></p><p id="7ab5" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">数据扩充是一种策略，使从业者能够显著增加可用于训练模型的<strong class="jr hj">数据</strong>的多样性，而无需实际收集新数据。<strong class="jr hj"> </strong>这种技术像填充、裁剪、移动、翻转等。所以这里我也在图像上使用了增强技术，为了增加数据，这里我上下翻转。</p><p id="7a72" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj">一个简单的增强例子</strong></p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es lx"><img src="../Images/4a6cb53ec671ae786c1c074119c9298d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*L8xr4qw7fdOMWwbEcLpSiw.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><strong class="bd iz"> <em class="ly">左:</em></strong>250个数据点的样本，精确地遵循正态分布。<strong class="bd iz"> <em class="ly">右:</em> </strong>给分布添加少量随机“抖动”。这种类型的数据扩充增加了我们网络的可推广性</figcaption></figure><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="d04c" class="lo iy hi lk b fi lp lq l lr ls"><em class="lt">#https://www.kaggle.com/supratimhaldar/</em><br/><em class="lt"># Augment data</em><br/>batch_size = 16<br/>train_input_shape = (224, 224, 3)<br/>n_classes = artists_top.shape[0]<br/><br/>train_datagen = ImageDataGenerator(validation_split=0.2,<br/>                                   rescale=1./255.,<br/>                                   <em class="lt">#rotation_range=45,</em><br/>                                   <em class="lt">#width_shift_range=0.5,</em><br/>                                   <em class="lt">#height_shift_range=0.5,</em><br/>                                   shear_range=5,<br/>                                   <em class="lt">#zoom_range=0.7,</em><br/>                                   horizontal_flip=<strong class="lk hj">True</strong>,<br/>                                   vertical_flip=<strong class="lk hj">True</strong>,<br/>                                  )<br/><br/>train_generator = train_datagen.flow_from_directory(directory=images_dir,<br/>                                                    class_mode='categorical',<br/>                                                    target_size=train_input_shape[0:2],<br/>                                                    batch_size=batch_size,<br/>                                                    subset="training",<br/>shuffle=<strong class="lk hj">True</strong>,<br/>                                                    classes=artists_top_name.tolist()<br/>                                                   )<br/><br/>valid_generator = train_datagen.flow_from_directory(directory=images_dir,<br/>                                                    class_mode='categorical',<br/>                                                    target_size=train_input_shape[0:2],<br/>                                                    batch_size=batch_size,<br/>                                                    subset="validation",<br/>shuffle=<strong class="lk hj">True</strong>,<br/>                                                    classes=artists_top_name.tolist()<br/>                                                   )<br/><br/>STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size<br/>STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size<br/>print("Total number of batches =", STEP_SIZE_TRAIN, "and", STEP_SIZE_VALID)</span></pre><p id="96e6" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">这里<strong class="jr hj"> batch_size =16 </strong>我已经使用过，你可以使用，即使你增加batch_size，它也会给出更准确的梯度，同时我们处理大量图像的损失。<strong class="jr hj">imagedata generator()</strong>imagedata generator接受原始数据，<em class="lt">随机地</em>转换它，并且只返回新的、转换后的数据。</p><p id="c37c" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj">打印增强版随机绘画</strong></p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="3f53" class="lo iy hi lk b fi lp lq l lr ls"><em class="lt">#https://www.kaggle.com/supratimhaldar/</em><br/><em class="lt"># Print a random paintings and it's random augmented version</em><br/>fig, axes = plt.subplots(1, 2, figsize=(20,10))<br/><br/>random_artist = random.choice(artists_top_name)<br/>random_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))<br/>random_image_file = os.path.join(images_dir, random_artist, random_image)<br/><br/><em class="lt"># Original image</em><br/>image = plt.imread(random_image_file)<br/>axes[0].imshow(image)<br/>axes[0].set_title("An original Image of " + random_artist.replace('_', ' '))<br/>axes[0].axis('off')<br/><br/><em class="lt"># Transformed image</em><br/>aug_image = train_datagen.random_transform(image)<br/>axes[1].imshow(aug_image)<br/>axes[1].set_title("A transformed Image of " + random_artist.replace('_', ' '))<br/>axes[1].axis('off')<br/><br/>plt.show()</span></pre><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es lz"><img src="../Images/f92d1a8ceba92c7ca731d1434fa2d4a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QQBw6ct7QLPWU-uPNP1Eig.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">在这里我们可以看到两幅图像，一幅是原始的，另一幅是增强版的</figcaption></figure><h1 id="6f08" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">建立模型</h1><p id="e04e" class="pw-post-body-paragraph jp jq hi jr b js jt ij ju jv jw im jx jy jz ka kb kc kd ke kf kg kh ki kj kk hb bi translated">现在是时候建立模型来训练数据了，我已经结束了我的探索性数据分析部分，但仍有更多的技术要做。如果你对EDA有一些新的想法，最好马上实现它，以便了解理解数据和特征工程的新方法。因此，在这一部分，我们将建立一个模型来训练我们的数据，正如我之前提到的，我将使用最先进的技术，如<strong class="jr hj"> ResNet50 </strong>模型。我可以使用CNN(卷积神经网络)，但当我阅读研究论文时，ResNet50网络在图像数据方面做了大量工作，因此让我们开始这一部分。</p><p id="e787" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj">神经网络架构</strong></p><p id="0fff" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">在图(a)中:告诉我们神经网络是如何工作的，圆形部分是激活函数sum(Xi*Wi)i=1到n，这意味着它确切地充当神经元，当输入给定w1，w2，…wn是权重时该神经元触发，因此输入+权重给定函数f(sum(XiWi))最后Yi作为这种称为神经网络的输出组合。在我们的例子中，我们使用ReLu激活。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es ma"><img src="../Images/f26e65c8a7a78260f9f6e59c36eeab20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O1NezX3keDxpoj4mHMcWvQ.png"/></div></div></figure><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="c549" class="lo iy hi lk b fi lp lq l lr ls"><em class="lt">#https://www.quora.com/ResNet50-tutorial</em><br/><em class="lt"># Load pre-trained model</em><br/>base_model = ResNet50(weights='imagenet', include_top=<strong class="lk hj">False</strong>, input_shape=train_input_shape)<br/><em class="lt"># I am using ResNet50 neural network it performs well for image data.</em><br/><strong class="lk hj">for</strong> layer <strong class="lk hj">in</strong> base_model.layers:<br/>    layer.trainable = <strong class="lk hj">True</strong></span></pre><p id="43b0" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">在这段代码中，我从Imagenet数据集获取图像数据。ImageNet是一个根据WordNet层次结构组织的图像数据库，供研究人员和数据科学家出于研究目的免费使用。</p><p id="8542" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj"> ResNet模型实现</strong></p><p id="053f" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">让我们首先了解ResNet50模型的架构，它也称为身份层，因为身份层的唯一目的是跳过连接，这意味着在ResNet模型中跳过一层，这有助于减少消失梯度问题。下面的模型描述了在ReLu激活和最后的x之间的权重层f(x ),在跳过连接之后，因此最后f(x)+x。</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es mb"><img src="../Images/8b9fe645766316aad910513d85b92617.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5ggrWYptqgi8twyRoKRJhg.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><a class="ae lg" href="https://stackoverflow.com/questions/54207410/how-to-split-resnet50-model-from-top-as-well-as-from-bottom" rel="noopener ugc nofollow" target="_blank">从堆栈溢出</a></figcaption></figure><p id="dbed" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj">当我们使用<strong class="jr hj"> sigmoid </strong>或<strong class="jr hj"> tanh </strong>激活函数时，会出现消失梯度</strong>问题，因为当我们对此激活函数求导时，旧导数(<em class="lt"> W_old </em>)和新导数(<em class="lt"> W_new </em>)变得相等，并且在反向传播更新函数(-ndL/dw)变为&lt; 0后，会出现消失梯度问题。网络损耗(dL)相对于网络权重(dw)导数的η(n)倍导数</p><figure class="km kn ko kp fd kq er es paragraph-image"><div class="er es mc"><img src="../Images/60ef7685b0b7203354653aba6e70f160.png" data-original-src="https://miro.medium.com/v2/resize:fit:420/format:webp/1*pM6xE4ZLztU4csGUxEjUoQ.png"/></div><figcaption class="kx ky et er es kz la bd b be z dx translated"><a class="ae lg" href="https://stats.stackexchange.com/questions/301285/what-is-vanishing-gradient" rel="noopener ugc nofollow" target="_blank">堆叠交换</a></figcaption></figure><p id="3391" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">上面的等式告诉我们，在反向传播过程中，当权重被更新时，早期层(接近输入)通过乘以后面层(接近输出)的梯度而获得，因此如果后面层的梯度小于1，那么它们的乘积很快消失。</p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="c413" class="lo iy hi lk b fi lp lq l lr ls"><em class="lt">#https://github.com/keras-team/keras-applications/blob/master/keras_applications/</em><br/><em class="lt"># Add layers at the end</em><br/><em class="lt"># creating layers for neural network</em><br/>X = base_model.output<br/>X = Flatten()(X)<br/><br/>X = Dense(512, kernel_initializer='he_uniform')(X) <em class="lt"># he_uniform is a weight to neural network</em><br/><em class="lt">#X = Dropout(0.5)(X)</em><br/>X = BatchNormalization()(X)<br/>X = Activation('relu')(X)  <em class="lt"># activation function i am using is "relu".  </em><br/><br/>X = Dense(16, kernel_initializer='he_uniform')(X)<br/><em class="lt">#X = Dropout(0.5)(X)</em><br/>X = BatchNormalization()(X)<br/>X = Activation('relu')(X)<br/><br/>output = Dense(n_classes, activation='softmax')(X)<br/><br/>model = Model(inputs=base_model.input, outputs=output)</span></pre><p id="9b9d" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">这是ResNet50模型的简单实现，它在每个神经元之间创建了50个网络。这里<strong class="jr hj"> dense() </strong>用作<strong class="jr hj"> dense </strong>层代表一个矩阵向量乘法。在矩阵中，我们有向量形式的数据，当反向传播发生时，它被更新，所以我们的矩阵变成m *维向量，如此密集以至于用来改变向量的维数。我希望你是正确的<strong class="jr hj"> dropout() </strong>在神经网络中，当我们过度拟合模型时，表示不好，因此dropout随机删除节点以进行廉价计算，因此在我的情况下，模型没有过度拟合，所以我在这里忽略。<strong class="jr hj"> Batchnormalization() </strong>是神经网络中的概念假设我们有5层网络没有batchnormalized ok如果我们开始归一化网络然后归一化曲线开始偏移这就是所谓的<strong class="jr hj">内协变偏移</strong>问题。为了消除这一点，我使用了BatchNormalize和last，在输出层之前，我添加了softmax层，它接受多类输入，并将输出作为1的和，这是概率输出，以便神经网络能够确定。我在这里使用的激活函数是ReLU，因为它减少了消失梯度问题。所以到目前为止，我已经初始化了resnet神经网络，让我们训练我们的模型。</p><h1 id="d374" class="ix iy hi bd iz ja jb jc jd je jf jg jh io ji ip jj ir jk is jl iu jm iv jn jo bi translated">火车模型</h1><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="e04a" class="lo iy hi lk b fi lp lq l lr ls"><em class="lt">#For training neural network i am using Adam optimiser.</em><br/><em class="lt">#SGD will be slow not perform well so Adam performs better.</em><br/>optimizer = Adam(lr=0.0001) <br/>model.compile(loss='categorical_crossentropy',<br/>              optimizer=optimizer, <br/>              metrics=['accuracy'])</span></pre><p id="16ea" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">对于训练深度学习神经网络，有各种技术，如Adam、Adadelta、SGD..等等。我正在使用<strong class="jr hj"> Adam </strong> optimizer来训练SGD在训练深度学习方面有点慢。我使用的损失函数是<strong class="jr hj">多类对数损失</strong>，也称为<strong class="jr hj">分类交叉熵</strong>，因为它是多类分类问题，并通过使用<strong class="jr hj">准确度</strong>度量来模拟性能。</p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="7125" class="lo iy hi lk b fi lp lq l lr ls">n_epoch = 10 <em class="lt">#n_epoch :number of times training vect to update weight/ one complete iteration. </em><br/><br/>early_stop = EarlyStopping(monitor='val_loss', patience=20, verbose=1, <br/>                           mode='auto', restore_best_weights=<strong class="lk hj">True</strong>)<br/><br/>reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, <em class="lt">#I used ReduceLROnPlateau callback function when matri</em><br/>                              verbose=1, mode='auto')</span><span id="d398" class="lo iy hi lk b fi lu lq l lr ls"><em class="lt"># Train the model - all layers</em><br/>history1 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,<br/>                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,<br/>                              epochs=n_epoch,<br/>                              shuffle=<strong class="lk hj">True</strong>,<br/>                              verbose=1,<br/>                              callbacks=[reduce_lr],<br/>                              use_multiprocessing=<strong class="lk hj">True</strong>,<br/>                              workers=16,<br/>                              class_weight=class_weights<br/>                             )</span></pre><p id="e2b1" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">这里的历元值是10到10个历元时间，这将训练下面给出的所有层输出:</p><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es md"><img src="../Images/db7f05f6580877ec67cc043a598df480.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2pt0SMQOozyQ4Gb466APqA.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">10带精度和损耗的历元值</figcaption></figure><p id="a451" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">这是第一次训练模型后的结果，我的模型的精确度为0.93或(93%)，损失从1.65减少到0.45意味着模型做得很好，现在我们冻结层并再次重新训练。</p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="e859" class="lo iy hi lk b fi lp lq l lr ls"><em class="lt"># Freeze core ResNet layers and train again </em><br/><strong class="lk hj">for</strong> layer <strong class="lk hj">in</strong> model.layers:<br/>    layer.trainable = <strong class="lk hj">False</strong><br/><br/><strong class="lk hj">for</strong> layer <strong class="lk hj">in</strong> model.layers[:50]:<br/>    layer.trainable = <strong class="lk hj">True</strong><br/><br/>optimizer = Adam(lr=0.0001)<br/><br/>model.compile(loss='categorical_crossentropy',<br/>              optimizer=optimizer, <br/>              metrics=['accuracy'])<br/><br/>n_epoch = 5<br/>history2 = model.fit_generator(generator=train_generator, steps_per_epoch=STEP_SIZE_TRAIN,<br/>                              validation_data=valid_generator, validation_steps=STEP_SIZE_VALID,<br/>                              epochs=n_epoch,<br/>                              shuffle=<strong class="lk hj">True</strong>,<br/>                              verbose=1,<br/>                              callbacks=[reduce_lr, early_stop],<br/>                              use_multiprocessing=<strong class="lk hj">True</strong>,<br/>                              workers=16,<br/>                              class_weight=class_weights<br/>                             )</span></pre><p id="0105" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">在上面我提到的代码片段中，我将通过冻结ResNet50的层来再次训练。实际上，冻结意味着保留几层不训练，并训练接下来的几层，为此我们必须做一些更改，如下所示:</p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="2b2d" class="lo iy hi lk b fi lp lq l lr ls"><strong class="lk hj">for</strong> layer <strong class="lk hj">in</strong> model.layers[:50]:<br/>    layer.trainable = <strong class="lk hj">True</strong></span></pre><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es me"><img src="../Images/06ae5170fd6004be52ac13f90f065756.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XuKsPcO9oqCSORcpB9k1jw.png"/></div></div></figure><p id="662c" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">这是在冻结几层并再次重新训练后的结果，模型触摸精度比以前提高了96%,如果你注意到错误率也降低到了0.39，模型确实做得很好。但这里有一件事，正如我告诉resnet50创建50个神经网络，但为了训练我们肯定需要巨大的空间，当我第一次在装有英特尔酷睿i5的电脑上执行时，它停止了，并显示空间不足的错误，所以我建议在执行这种类型的模型时，至少需要13+ GB的ram和gpu来训练，否则你可以在<a class="ae lg" href="https://colab.research.google.com/drive/1qV97RI01XGJKeysC44MGetzXqcJBkXEd#scrollTo=fslLoFs_tPQQ" rel="noopener ugc nofollow" target="_blank">上运行谷歌colab </a>提供13 GB的空间。我个人只在colab上执行了这个。到目前为止，我们已经训练了我们的模型足够的时间来测试通过插入一个图像到模型让我们看看模型正确识别艺术家与否！。</p><p id="7cd3" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj">训练图</strong></p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="6235" class="lo iy hi lk b fi lp lq l lr ls"><em class="lt"># Plot the training graph</em><br/><strong class="lk hj">def</strong> plot_training(history):<br/>    acc = history['acc']<br/>    val_acc = history['val_acc']<br/>    loss = history['loss']<br/>    val_loss = history['val_loss']<br/>    epochs = range(len(acc))<br/><br/>    fig, axes = plt.subplots(1, 2, figsize=(15,5))<br/>    <br/>    axes[0].plot(epochs, acc, 'r-', label='Training Accuracy')<br/>    axes[0].plot(epochs, val_acc, 'b--', label='Validation Accuracy')<br/>    axes[0].set_title('Training and Validation Accuracy')<br/>    axes[0].legend(loc='best')<br/><br/>    axes[1].plot(epochs, loss, 'r-', label='Training Loss')<br/>    axes[1].plot(epochs, val_loss, 'b--', label='Validation Loss')<br/>    axes[1].set_title('Training and Validation Loss')<br/>    axes[1].legend(loc='best')<br/>    <br/>    plt.show()<br/>    <br/>plot_training(history)</span></pre><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es mf"><img src="../Images/0e26fd4279a7dcbd35e989adf442b14e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y9KN7xQQe-bIv9S1Zl4OOA.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">精确度和损耗之间的关系图简单地说，我们可以得出精确度高同时损耗降低的结论</figcaption></figure><p id="a488" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">这两幅图都表示精度在提高，损耗在降低。</p><p id="247a" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj">评估模型</strong></p><p id="0838" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">这段代码用于显示模型的性能矩阵，说明模型如何像人类一样进行预测和思考。</p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="9548" class="lo iy hi lk b fi lp lq l lr ls"><em class="lt"># Classification report and confusion matrix</em><br/><strong class="lk hj">from</strong> <strong class="lk hj">sklearn.metrics</strong> <strong class="lk hj">import</strong> *<br/><strong class="lk hj">import</strong> <strong class="lk hj">seaborn</strong> <strong class="lk hj">as</strong> <strong class="lk hj">sns</strong><br/><br/>tick_labels = artists_top_name.tolist()<br/><br/><strong class="lk hj">def</strong> showClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID):<br/>    <em class="lt"># Loop on each generator batch and predict</em><br/>    y_pred, y_true = [], []<br/>    <strong class="lk hj">for</strong> i <strong class="lk hj">in</strong> range(STEP_SIZE_VALID):<br/>        (X,y) = next(valid_generator)<br/>        y_pred.append(model.predict(X))<br/>        y_true.append(y)<br/>    <br/>    <em class="lt"># Create a flat list for y_true and y_pred</em><br/>    y_pred = [subresult <strong class="lk hj">for</strong> result <strong class="lk hj">in</strong> y_pred <strong class="lk hj">for</strong> subresult <strong class="lk hj">in</strong> result]<br/>    y_true = [subresult <strong class="lk hj">for</strong> result <strong class="lk hj">in</strong> y_true <strong class="lk hj">for</strong> subresult <strong class="lk hj">in</strong> result]<br/>    <br/>    <em class="lt"># Update Truth vector based on argmax</em><br/>    y_true = np.argmax(y_true, axis=1)<br/>    y_true = np.asarray(y_true).ravel()<br/>    <br/>    <em class="lt"># Update Prediction vector based on argmax</em><br/>    y_pred = np.argmax(y_pred, axis=1)<br/>    y_pred = np.asarray(y_pred).ravel()<br/>    <br/>    <em class="lt"># Confusion Matrix</em><br/>    fig, ax = plt.subplots(figsize=(10,10))<br/>    conf_matrix = confusion_matrix(y_true, y_pred, labels=np.arange(n_classes))<br/>    conf_matrix = conf_matrix/np.sum(conf_matrix, axis=1)<br/>    sns.heatmap(conf_matrix, annot=<strong class="lk hj">True</strong>, fmt=".2f", square=<strong class="lk hj">True</strong>, cbar=<strong class="lk hj">False</strong>, <br/>cmap=plt.cm.jet, xticklabels=tick_labels, yticklabels=tick_labels,ax=ax)<br/>    ax.set_ylabel('Actual')<br/>    ax.set_xlabel('Predicted')<br/>    ax.set_title('Confusion Matrix')<br/>    plt.show()<br/>    <br/>    print('Classification Report:')<br/>    print(classification_report(y_true, y_pred, labels=np.arange(n_classes), target_names=artists_top_name.tolist()))<br/><br/>showClassficationReport_Generator(model, valid_generator, STEP_SIZE_VALID)</span></pre><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es mg"><img src="../Images/edd84dcc009499d79550cfd8d1ab7445.png" data-original-src="https://miro.medium.com/v2/resize:fit:1398/format:webp/1*HnKxOT8ZcBDJ572SlH-Xfw.png"/></div></div></figure><p id="4b10" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">上面是性能指标，它告诉我们模型根据我们的假设预测了多少，所以从上面，如果我们对角地按类分类，我们可以说像在底部类marc_chagall actuall和predicted marc_chagall那里0.96或(96%)意味着模型预测得很好。</p><p id="6792" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj">测试模型</strong></p><p id="2e2d" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">这里随机选取5幅图像，并通过给出5幅属于或不属于特定艺术家的随机图像，将这些随机图像提供给模型来预测艺术家。</p><pre class="km kn ko kp fd lj lk ll lm aw ln bi"><span id="57f2" class="lo iy hi lk b fi lp lq l lr ls"><em class="lt"># Prediction</em><br/><strong class="lk hj">from</strong> <strong class="lk hj">keras.preprocessing</strong> <strong class="lk hj">import</strong> *<br/><br/>n = 5<br/>fig, axes = plt.subplots(1, n, figsize=(25,10))<br/><br/><strong class="lk hj">for</strong> i <strong class="lk hj">in</strong> range(n):<br/>    random_artist = random.choice(artists_top_name)<br/>    random_image = random.choice(os.listdir(os.path.join(images_dir, random_artist)))<br/>    random_image_file = os.path.join(images_dir, random_artist, random_image)<br/><br/>    <em class="lt"># Original image</em><br/><br/>    test_image = image.load_img(random_image_file, target_size=(train_input_shape[0:2]))<br/><br/>    <em class="lt"># Predict artist</em><br/>    test_image = image.img_to_array(test_image)<br/>    test_image /= 255.<br/>    test_image = np.expand_dims(test_image, axis=0)<br/><br/>    prediction = model.predict(test_image)<br/>    prediction_probability = np.amax(prediction)<br/>    prediction_idx = np.argmax(prediction)<br/><br/>    labels = train_generator.class_indices<br/>    labels = dict((v,k) <strong class="lk hj">for</strong> k,v <strong class="lk hj">in</strong> labels.items())<br/><br/>    <em class="lt">#print("Actual artist =", random_artist.replace('_', ' '))</em><br/>    <em class="lt">#print("Predicted artist =", labels[prediction_idx].replace('_', ' '))</em><br/>    <em class="lt">#print("Prediction probability =", prediction_probability*100, "%")</em><br/><br/>    title = "Actual artist = <strong class="lk hj">{}\n</strong>Predicted artist = <strong class="lk hj">{}\n</strong>Prediction probability = <strong class="lk hj">{:.2f}</strong> %" \<br/>                .format(random_artist.replace('_', ' '), labels[prediction_idx].replace('_', ' '),<br/>                        prediction_probability*100)<br/><br/>    <em class="lt"># Print image</em><br/>    axes[i].imshow(plt.imread(random_image_file))<br/>    axes[i].set_title(title)<br/>    axes[i].axis('off')<br/><br/>plt.show()</span></pre><figure class="km kn ko kp fd kq er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es mh"><img src="../Images/ce56aee888cf7ec07c138b268d3622cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NaZaP_BQeb8UZ4-DR9-XEA.png"/></div></div><figcaption class="kx ky et er es kz la bd b be z dx translated">输出由模型预测艺术家的图像</figcaption></figure><p id="347a" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated">因此，从上面我们可以看到，给定5个随机图像，我们的模型预测正确的艺术家姓名，给定图像的平均概率约为80%及以上。博客到此结束，我们开始了解深度学习模型和训练技术。我希望你喜欢这个博客。更多关于代码的细节，你可以访问我的<a class="ae lg" href="https://github.com/homejeet" rel="noopener ugc nofollow" target="_blank"> github简介</a>。请看看http://github.com/homejeet<a class="ae lg" href="http://github.com/homejeet" rel="noopener ugc nofollow" target="_blank">的其他项目。我将热切期待您的反馈和建议。</a></p><h2 id="7559" class="lo iy hi bd iz mi mj mk jd ml mm mn jh jy mo mp jj kc mq mr jl kg ms mt jn mu bi translated">参考</h2><ol class=""><li id="70dc" class="mv mw hi jr b js jt jv jw jy mx kc my kg mz kk na nb nc nd bi translated"><a class="ae lg" href="https://www.pyimagesearch.com/2019/07/08/keras-imagedatagenerator-and-data-augmentation/" rel="noopener ugc nofollow" target="_blank">https://www . pyimagesearch . com/2019/07/08/keras-imagedata generator-and-data-augmentation/</a></li><li id="0849" class="mv mw hi jr b js ne jv nf jy ng kc nh kg ni kk na nb nc nd bi translated"><a class="ae lg" href="https://github.com/mk60991/ImageAI-image-Prediction" rel="noopener ugc nofollow" target="_blank">https://github.com/mk60991/ImageAI-image-Prediction</a></li><li id="7c62" class="mv mw hi jr b js ne jv nf jy ng kc nh kg ni kk na nb nc nd bi translated"><a class="ae lg" href="https://www.appliedaicourse.com/?gclid=Cj0KCQiArqPgBRCRARIsAPwlHoVTtRFPsX9qu_XvwJOyogwEJKZ-VG5SBKELkOQRXTKbYP_jxDIIDisaAmPVEALw_wcB" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></li><li id="fc86" class="mv mw hi jr b js ne jv nf jy ng kc nh kg ni kk na nb nc nd bi translated"><a class="ae lg" href="http://cs231n.stanford.edu/reports/2017/pdfs/406.pdf" rel="noopener ugc nofollow" target="_blank">http://cs231n.stanford.edu/reports/2017/pdfs/406.pdf</a></li><li id="d2bd" class="mv mw hi jr b js ne jv nf jy ng kc nh kg ni kk na nb nc nd bi translated"><a class="ae lg" href="https://www.researchgate.net/" rel="noopener ugc nofollow" target="_blank">https://www.researchgate.net/</a></li><li id="0bf3" class="mv mw hi jr b js ne jv nf jy ng kc nh kg ni kk na nb nc nd bi translated"><a class="ae lg" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/</a></li></ol><p id="f4c7" class="pw-post-body-paragraph jp jq hi jr b js lb ij ju jv lc im jx jy ld ka kb kc le ke kf kg lf ki kj kk hb bi translated"><strong class="jr hj">在</strong> <a class="ae lg" href="https://www.linkedin.com/in/homejeet-behera-982871114/" rel="noopener ugc nofollow" target="_blank"> LinkedIn上联系我</a></p></div></div>    
</body>
</html>