<html>
<head>
<title>Does TF-IDF work differently in textbooks and sklearn routine?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TF-IDF在教材和sklearn套路中的作用有区别吗？</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/does-tf-idf-work-differently-in-textbooks-and-sklearn-routine-cc7a7d1b580d?source=collection_archive---------17-----------------------#2020-07-18">https://medium.com/analytics-vidhya/does-tf-idf-work-differently-in-textbooks-and-sklearn-routine-cc7a7d1b580d?source=collection_archive---------17-----------------------#2020-07-18</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="23f6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TF-IDF是单词包方法中的一个简单转变。单词包只是表示(<strong class="ih hj"> #次单词w出现在一个文档d </strong>)。</p><p id="2b21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">TF-IDF代表<strong class="ih hj">术语频率乘以逆文档频率。</strong></p><p id="3766" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下图讨论了在机器学习环境中对文本数据进行数字编码的单词包方法。</p><p id="e0a6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下面的例子中，单词“puppy”在文本中只出现一次，所以在单词包中，向量puppy的值为1，它基本上表示该文档中puppy的单词数为1，而在真实世界中有许多文档，因此该向量看起来像一个矩阵，其中每行是一个文档，矩阵是一个文档集合，其中列表示特定的单词数。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/7808730cb48d4bb4c7908d35a1631e6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/0*B9GC_f3BMtjGMdQ-.png"/></div></figure><p id="a19c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我们已经有了文字的单词包表示，为什么还需要tf-idf？</strong></p><p id="ba59" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">单词包是文本数据的简单表示，但远非完美。如果我们平等地计算所有单词，那么一些单词最终会比其他单词更被强调。</p><p id="6504" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">例如，考虑客户在购买产品时给出的评论。在这里，“和”，“这个”，“一个”比“快乐”，“质量”出现得更多，从而在向量空间中占主导地位。如果我们要创建一个给定客户评论的情感分析模型，我们会想知道一个特定的词对文档有多重要，如“快乐”和“质量”，我们会给这些词更多的权重以获得一个更好的模型。TF-IDF在单词包失败时做了完全相同的事情。</p><p id="9534" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">tf-idf不是在数据集中查找文档中单词的原始计数，而是查找标准化计数，其中每个单词的计数除以该单词出现的文档数。</p><p id="1cc1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">课本公式:</strong></p><p id="65d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> bow(w，d) </strong> = #次单词w出现在文档d中</p><p id="4fc4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> idf(w，d)</strong>= N/(# word w出现的文档)。</p><p id="9e7c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> tf-idf(w，d) </strong> =bow(w，d)* N/ (#出现w字的文档)。</p><p id="e3be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中N是文档总数。</p><p id="8d6d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果一个词出现在许多文档中，那么它的逆文档频率接近1。如果一个单词只出现在几个文档中，那么它的逆文档频率要高得多。</p><p id="e593" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们还可以进行对数变换，而不仅仅是使用原始的逆文档频率。对数变换会将1变为0，并使较大的值(大于1)变小。<br/>现在上面的公式可以改写为:</p><p id="368a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj"> tf-idf(w，d) </strong> = bow(w，d) * Log(N/ #个出现w的文档)</p><p id="d72f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是关于tf-idf的教科书式描述。现在让我们深入到tf-idf的sklearn实现中，它与教科书中的表示略有不同。</p><p id="3dfb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">sk learn中的TF-IDF:</strong></p><p id="8c66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我将使用带有默认参数的TfidfVectorizer:<br/><em class="jl">类</em> <code class="du jm jn jo jp b">sklearn.feature_extraction.text.<strong class="ih hj">TfidfVectorizer</strong></code> ( <em class="jl"> * </em>，<em class="jl"> input='content' </em>，<em class="jl"> encoding='utf-8' </em>，<em class="jl"> decode_error='strict' </em>，<em class="jl"> strip_accents=None </em>，<strong class="ih hj"> <em class="jl">小写</em> </strong> <em class="jl"> =True </em>，<em class="jl">预处理器=Noneu)\b\w\w+\b' </em>，<em class="jl"> ngram_range=(1 </em>，<em class="jl"> 1) </em>，<em class="jl"> max_df=1.0 </em>，<em class="jl"> min_df=1 </em>，<em class="jl"> max_features=None </em>，<em class="jl"> vocabulary=None </em>，<em class="jl"> binary=False </em>，<em class="jl">dtype =&lt;class ' num</em></p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jq"><img src="../Images/6412ae9112095ed12ba1b454507b9919.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t34FQwDontJKd2t_wSMfMg.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">sklearn设置中的tf-idf示例。</figcaption></figure><p id="11c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">上面我举了一个例子，数据集只有两个文档“这个文档”和“第二个文档”。</p><p id="458c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据集中的总字数是三个。</p><p id="8561" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">您还可以在上面的数据帧中看到数据集的转换后的tf-idf表示，它对特定文档的单词进行了加权。</p><p id="780f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以在上面观察到，单词“second”被赋予了零值，因为该单词没有在第一个文档中出现，类似地，它在第二个文档中也具有零值。而像“文档”这样的其他单词对于第一个文档具有0.579739。</p><p id="06fc" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">现在让我们从头开始计算tfidfVectorizer()是如何给出这些值的:</strong></p><p id="aefe" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">tfidf矢量器的默认设置有<strong class="ih hj"> smooth_idf </strong> =True和<strong class="ih hj"> norm </strong> ="l2 "</p><p id="fb0e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这些参数，下面是计算上述转换值的公式:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jz"><img src="../Images/c61ad4034534da771b05591d22ed94bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GxZ900WrBhqDPMZtvpQ3sw.png"/></div></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">sklearn tf-idf公式</figcaption></figure><p id="fa28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面我手工计算了上面例子的值(从头开始)</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es ka"><img src="../Images/71a8ac3d0f2dbd4cc9b9d2e586cb24a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*88ZkjT1pHVC8hM84GiAaDw.png"/></div></div></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kb"><img src="../Images/0d010a7386258cd943a4ce2e2921f623.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*eq6u3BaK1-P0HksvUOWt_w.png"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">文集</figcaption></figure><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es kc"><img src="../Images/2d409699b3be0c05755d995a553883dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/1*y_QS_7x2n4mmOlOCP_gYFQ.png"/></div><figcaption class="jv jw et er es jx jy bd b be z dx translated">从sklearn的tf-idf矢量器()转换而来的tf-idf</figcaption></figure><p id="3821" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">就拿第一份文件“这份文件”来说吧:</strong></p><p id="d739" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">tfi df _ of _ Word _ document</strong>=(NP . log((1+(文档数))/(1+(该单词出现的文档数)))* Word _ count _ of _ Word _ document _ in _ first _ document<br/>=(NP . log((1+2)/(1+2))+1)* 1 = 1</p><p id="0686" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">类似地，对其他值进行计算，并使用下面的归一化技术，从零开始获得sklearn方面的精确输出。</p><p id="c876" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">tfidf _ normalized _ for _ word _ document _ in _ first _ document =<br/></strong>(tfidf _ of _ word _ document)/<br/>(NP . sqrt(NP . square(TF _ IDF _ for _ word _ document _ in _ first _ document)+\<br/>NP . square(TF _ IDF _ for _ word _ second _ in _ first _ document)+\<br/>NP . square(TF _ IDF _ for _ word _ this _ in _ first _ document))</p><p id="b038" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">=1/np.sqrt(1 +0 +1.405 )=0.579738</p><p id="a773" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">希望这篇文章对那些对sklearn的tf-idf实现感到困惑的人有所帮助。这显然不同于教科书符号。</p><p id="09d5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我等不及让你为你的下一个自然语言处理或机器学习任务实现tf-idf来对你的语料库进行分类或聚类。</p><p id="0ae8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你觉得这篇文章有帮助，请鼓掌并发表评论。关注我了解更多机器学习和数据科学内容。</p><p id="ce6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">参考:</strong><br/><a class="ae kd" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn . org/stable/modules/generated/sk learn . feature _ extraction . text . tfidfvectorizer . html</a></p><p id="415a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae kd" href="https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/feature _ extraction . html # text-feature-extraction</a></p><p id="5131" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae kd" href="https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/feature _ extraction . html # text-feature-extraction</a></p></div></div>    
</body>
</html>