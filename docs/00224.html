<html>
<head>
<title>Introduction to reinforcement learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习简介</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-dc49e5c04310?source=collection_archive---------0-----------------------#2018-12-25">https://medium.com/analytics-vidhya/introduction-to-reinforcement-learning-dc49e5c04310?source=collection_archive---------0-----------------------#2018-12-25</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="9310" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这是强化学习教程系列的第一部分。我是一名学生，学习强化学习。如果从深度学习等其他人工智能范式来比较，学习强化学习并没有很大的来源。这将是一系列的教程，大部分是从其他地方复制的，如果你是初学者，这将是一个很好的来源。说得够多了，让我们进入正题吧。</p><h1 id="9b0e" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">什么是强化学习？</h1><p id="eb30" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">强化学习(RL)是机器学习的一个分支，通过与环境的交互来进行学习。它是以目标为导向的学习，不教学习者采取什么行动；相反，学习者从其行为的后果中学习。考虑一个机器人可以向左或向右两个方向移动的例子。它的左边有一个物体，而右边没有。在强化学习中，我们根据代理采取的行动给予奖励。在这种情况下，当机器人击中物体时，我们给它一个负奖励，如果它没有击中，我们给它一个正奖励。强化学习基本上是一个试错的学习过程。</p><div class="kg kh ki kj fd ab cb"><figure class="kk kl km kn ko kp kq paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/d3ff8bf2e9ad78209b7ee1eaa17f7dc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*uSnscWEArkZfQuv2BSKfQg.png"/></div></figure><figure class="kk kl kx kn ko kp kq paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><img src="../Images/7cef4fd3f9679e08c934cc6f63f18e63.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*wCbdNuB_1pPFJjzogA_EEQ.png"/></div></figure></div><h1 id="f186" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">强化学习与其他机器学习算法有何不同？</h1><p id="254d" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">在监督学习中，机器(代理)从训练数据中学习，训练数据具有一组带标签的输入和输出。有一个外部主管，他对环境有完整的知识库，并监督代理完成一项任务。<br/>在无监督学习中，我们为模型提供只有一组输入的训练数据；该模型学习确定输入中的隐藏模式。有一种常见的误解，认为RL是一种无监督学习，但它不是。在无监督学习中，模型学习隐藏结构，而在RL中，模型通过最大化回报来学习。假设我们想向用户推荐新电影。无监督学习分析这个人看过的类似电影并建议电影，而RL不断接收用户的反馈，了解他的电影偏好，并在此基础上建立知识库并建议新电影。</p><figure class="kg kh ki kj fd kl er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es ky"><img src="../Images/88e4e70fdc813dba03fbc0a64489ec7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qqbJ1owKlhZH8VJ8ag_iJA.png"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">机器学习算法的类型</figcaption></figure><h1 id="a010" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">强化学习代理</h1><p id="4a88" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">强化学习代理是做出智能决策的软件程序，它们基本上是强化学习中的学习者。代理人通过与环境互动来采取行动，并根据他们的行动获得奖励，例如，超级马里奥在视频游戏中导航。</p><h1 id="7842" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">RL的元素</strong></h1><ol class=""><li id="dc1e" class="ld le hi ih b ii kb im kc iq lf iu lg iy lh jc li lj lk ll bi translated"><strong class="ih hj">策略:- </strong>一个<em class="lm">策略</em>定义了学习代理在给定时间的行为方式。粗略地说，策略是从感知的环境状态到处于这些状态时要采取的行动的映射。策略是强化学习代理的核心，因为它本身就足以决定行为。策略通常用符号𝛑.来表示</li><li id="5262" class="ld le hi ih b ii ln im lo iq lp iu lq iy lr jc li lj lk ll bi translated"><strong class="ih hj">奖励函数:- </strong>一个<em class="lm">奖励函数</em>定义了强化学习问题中的目标。粗略地说，它将环境的每个感知状态(或状态-行动对)映射到一个数字，一个<em class="lm">奖励</em>，表明该状态的内在可取性。强化学习代理的唯一目标是最大化它从长远来看得到的总回报。奖励函数定义了代理的好事件和坏事件。</li><li id="14e3" class="ld le hi ih b ii ln im lo iq lp iu lq iy lr jc li lj lk ll bi translated"><strong class="ih hj">价值函数:- </strong>一个状态的价值是从该状态开始，一个代理在未来可以期望积累的奖励总额。奖励决定了环境状态的直接的、内在的可取性，而价值在考虑了可能发生的状态以及这些状态中的奖励之后，表明了状态的长期可取性。打个人类的比方，奖励有点像快乐(如果高的话)和痛苦(如果低的话)，而价值则对应于一种更明确和更有远见的判断，即我们对环境处于特定状态有多高兴或不高兴。</li><li id="8512" class="ld le hi ih b ii ln im lo iq lp iu lq iy lr jc li lj lk ll bi translated">模型:- 模型是代理对环境的表示。学习可以有两种类型——基于模型的学习和无模型的学习。在基于模型的学习中，代理利用先前学习的信息来完成任务，而在无模型学习中，代理仅仅依靠试错经验来执行正确的动作。假设您想更快地从家里到达办公室。在基于模型的学习中，你简单地使用先前学习的经验(地图)来更快地到达办公室，而在无模型学习中，你不会使用先前的经验，而是尝试所有不同的路线并选择更快的路线。</li></ol><h1 id="45f2" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">强化学习环境</h1><p id="18d3" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">代理与之交互的一切都被称为环境。环境就是外界。它包括代理之外的一切。环境可以大致分为两类</p><ol class=""><li id="0b43" class="ld le hi ih b ii ij im in iq ls iu lt iy lu jc li lj lk ll bi translated"><strong class="ih hj">确定性环境:- </strong>当我们知道基于当前状态的结果时，就说一个环境是确定性的。例如，在国际象棋比赛中，我们知道移动任何玩家的确切结果。</li><li id="da58" class="ld le hi ih b ii ln im lo iq lp iu lq iy lr jc li lj lk ll bi translated"><strong class="ih hj">随机环境:- </strong>当我们不能基于当前状态确定结果时，环境被称为是随机的。会有更大程度的不确定性。例如，我们永远不知道掷骰子时会出现什么数字。</li></ol><h1 id="8c62" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">强化学习的应用</h1><p id="cbc2" class="pw-post-body-paragraph if ig hi ih b ii kb ik il im kc io ip iq kd is it iu ke iw ix iy kf ja jb jc hb bi translated">强化学习有大量的应用。我在这里提到了一些，如果你想了解更多，请看这个伟大的博客。</p><ol class=""><li id="0e4b" class="ld le hi ih b ii ij im in iq ls iu lt iy lu jc li lj lk ll bi translated">机器人学</li><li id="5377" class="ld le hi ih b ii ln im lo iq lp iu lq iy lr jc li lj lk ll bi translated">交付管理</li><li id="7ed7" class="ld le hi ih b ii ln im lo iq lp iu lq iy lr jc li lj lk ll bi translated">玩游戏人工智能</li></ol><figure class="kg kh ki kj fd kl er es paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="er es lw"><img src="../Images/1bc2f68d6773cb4e0554c6387769ec86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZG3VfP3YYtqLs6lIQ5-bwA.jpeg"/></div></div><figcaption class="kz la et er es lb lc bd b be z dx translated">强化学习在工业中的应用</figcaption></figure><h1 id="35fb" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated"><strong class="ak">强化学习系列</strong></h1><ol class=""><li id="17cb" class="ld le hi ih b ii kb im kc iq lf iu lg iy lh jc li lj lk ll bi translated">强化学习导论。→你在这里。</li><li id="212f" class="ld le hi ih b ii ln im lo iq lp iu lq iy lr jc li lj lk ll bi translated"><a class="ae lv" rel="noopener" href="/@sanchittanwar75/markov-chains-and-markov-decision-process-e91cda7fa8f2">马尔可夫链和马尔可夫决策过程。</a></li><li id="2b37" class="ld le hi ih b ii ln im lo iq lp iu lq iy lr jc li lj lk ll bi translated"><a class="ae lv" rel="noopener" href="/@sanchittanwar75/bellman-equation-and-dynamic-programming-773ce67fc6a7">贝尔曼方程和动态规划。</a></li></ol><h1 id="0374" class="jd je hi bd jf jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka bi translated">参考</h1><ol class=""><li id="07f4" class="ld le hi ih b ii kb im kc iq lf iu lg iy lh jc li lj lk ll bi translated">Sudarshan Ravichandran的python强化学习实践</li><li id="0251" class="ld le hi ih b ii ln im lo iq lp iu lq iy lr jc li lj lk ll bi translated">强化学习:介绍理查德·萨顿和安德鲁·g·巴尔托</li><li id="8fea" class="ld le hi ih b ii ln im lo iq lp iu lq iy lr jc li lj lk ll bi translated"><a class="ae lv" href="https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12" rel="noopener" target="_blank">https://towards data science . com/applications-of-enforcement-learning-in-real-world-1a 94955 BCD 12</a></li></ol></div></div>    
</body>
</html>