<html>
<head>
<title>Visual Question Answering with Various Feature Combinations</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有各种特征组合的视觉问题回答</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/visual-question-answering-with-various-feature-combinations-extensions-of-visual-question-940edbd84d82?source=collection_archive---------5-----------------------#2019-09-06">https://medium.com/analytics-vidhya/visual-question-answering-with-various-feature-combinations-extensions-of-visual-question-940edbd84d82?source=collection_archive---------5-----------------------#2019-09-06</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="4618" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">视觉问答的扩展</h2></div><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ix"><img src="../Images/90185c8a96034f4059e10fcf7fd5dc11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ImpwGalmOEzAZI0L"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">乔恩·泰森在<a class="ae jn" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</figcaption></figure></div><div class="ab cl jo jp gp jq" role="separator"><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt ju"/><span class="jr bw bk js jt"/></div><div class="hb hc hd he hf"><p id="7f1b" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">我目前的工作包括使用NLP进行语言建模、分段和分解。考虑到语义和意义在理解语言中的重要作用，我想重温一下这个领域以前的一些工作，特别是像视觉问答(VQA)这样有趣的工作，它结合了计算机视觉、自然语言理解和深度学习。本文概述了VQA模型及其扩展。有关实现和结果的更详细的阅读，请参考此处的论文<a class="ae jn" href="http://courses.cs.vt.edu/cs5824/Fall15/project_reports/choi_narayanan.pdf" rel="noopener ugc nofollow" target="_blank">和此处的代码</a><a class="ae jn" href="https://github.com/siddharthnarayanan/VQA" rel="noopener ugc nofollow" target="_blank"/>。VQA团队还维护着一个全面的信息、资源和软件来源，包括最新的演讲和论文。</p><p id="2d28" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">在人工智能的众多问题中，结合了计算机视觉、自然语言处理、知识表示和推理的图像/视频字幕已经被许多研究小组所解决。然而，与人类相比，在图像的评估质量方面仍然存在显著的差距。VQA已经成为一个有趣的领域，它位于这些问题领域的交汇处。给定一幅图像，视觉问答算法帮助机器回答关于该图像的自由形式、开放式、自然语言的问题。这是通过基于微软的<a class="ae jn" href="http://openaccess.thecvf.com/content_cvpr_2015/html/Fang_From_Captions_to_2015_CVPR_paper.html" rel="noopener ugc nofollow" target="_blank">深度多模态相似性模型(DMSM)【1】</a>在两种模态(文本和图像)之间的语义空间中测量相似性来实现的。虽然基本的VQA模型本身有许多潜在的真实世界用例，如大型图像集中的自动标记、图像检索系统、集成到巨大的社交媒体和电子商务数据库中，但作为弗吉尼亚理工大学视觉实验室的研究生，<a class="ae jn" href="https://scholar.google.com/citations?user=WcCMuIEAAAAJ&amp;hl=en" rel="noopener ugc nofollow" target="_blank"> Jinwoo Choi </a>和我还想尝试将<a class="ae jn" href="https://arxiv.org/abs/1505.00468" rel="noopener ugc nofollow" target="_blank">VQA【2】</a>的基本模型扩展到其他输入组合。</p><div class="iy iz ja jb fd ab cb"><figure class="kr jc ks kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/925510a98f0e54c336173c659bfe118e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*tFe4vL38_2spdkYXy--oDw.jpeg"/></div></figure><figure class="kr jc ks kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/ccdabfdbc51e47f32125c3b2e4d60288.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*YeRMXAuAzk3npRU_oDSUlA.jpeg"/></div></figure></div><div class="ab cb"><figure class="kr jc ks kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/180e808e802afb38ecf1a171afc56327.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*LRiooUklDRVTKrfegKoteA.jpeg"/></div></figure><figure class="kr jc ks kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/348d8554a7f1dfecb7edab74537db600.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*55e-7shmN9_ZoJBgNmJlhQ.jpeg"/></div><figcaption class="jj jk et er es jl jm bd b be z dx kx di ky kz translated">图1:这里展示了四个模型。第一个模型是基本的VQA模型。给定一幅图像，问一个关于它的问题，并找到关于该图像的问题的正确答案。在第二个模型中，给定一个图像，我们检索相应的问题和答案对。第三个模型是具有输入问答对的图像检索模型。给定一个问题和答案对，我们检索与查询问题和答案对最相关的图像。最后一个模型是危险模型。给定一幅图像和一个答案，这个模型试图找到关于它们的问题。</figcaption></figure></div><p id="53cf" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">为了执行任何VQA任务，我们测量输入模态(图像&amp;句子)或(图像+句子&amp;句子)之间的相似性。DMSM帮助我们将输入向量映射到公共语义空间，并测量嵌入向量之间的余弦相似性。DMSM是单峰深度结构化语义模型(DSSM)的多峰扩展，它测量文本查询和文档之间的相似性，并且也使用一对类似于DSSM的神经网络。</p><p id="2e3d" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">基本VQA模型使用从图像中提取的特征，并与问题特征连接以生成(图像+问题)特征。然后，通过DMSM的一个网络输入这些信息，并通过另一个网络输入答案特征来训练模型。</p><div class="iy iz ja jb fd ab cb"><figure class="kr jc ks kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/0a7a4db3d22df57cf2bfe1d0e81422f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*lVZwLuqD-PAAP2k0cbtu5w.jpeg"/></div></figure><figure class="kr jc ks kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/bf231d69bf9cbbf12d7126b5f78595fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*dz04OMlkS2HQsDMZBio-hg.jpeg"/></div></figure></div><div class="ab cb"><figure class="kr jc ks kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/2015e1640c007c4360fb61f6d722730f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*hRmWLStAkgH6o4M6xEkL_A.jpeg"/></div></figure><figure class="kr jc ks kt ku kv kw paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><img src="../Images/66f2a66ae9048d7dfb0d623dc200d0ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*jmgQ5WJVAcec0koZyCNpwQ.jpeg"/></div><figcaption class="jj jk et er es jl jm bd b be z dx kx di ky kz translated">图2:对于QA检索模型，提取的图像特征被输入一个网络，连接的(问题+答案)特征被输入另一个网络。对于图像检索模型，连接的(问题+答案)特征被馈送到一个网络，而提取的图像特征被馈送到DMSM的另一个网络。对于危险模型，我们将图像特征与答案特征连接起来。级联的(图像+答案)特征被馈送到一个网络，而问题特征被馈送到另一个网络。</figcaption></figure></div><p id="32a2" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">DMSM的预测返回输入1(图像+问题特征)和输入2(候选答案特征)之间的余弦相似性得分，范围从-1到1。我们的数据集每个问题有18个多项选择，从中选择得分最高的答案。为此，我们按降序排列分数，并选择前K个最相似的结果。</p><p id="e90c" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">这项工作中使用的数据集是原始的<a class="ae jn" href="https://arxiv.org/abs/1505.00468" rel="noopener ugc nofollow" target="_blank"> VQA数据集【2】</a>。在我们模型的开发过程中，该数据集包含来自<a class="ae jn" href="https://arxiv.org/abs/1504.00325" rel="noopener ugc nofollow" target="_blank">微软COCO数据集【3】</a>的82，783幅训练图像和40，504幅验证图像。此外，VQA数据集包含每个训练/验证图像大约3个基本事实问答对。VQA数据集提供了两种回答问题的方式:(1)开放式回答和(2)多项选择。在这项工作中，只有多项选择答案被用于VQA模型。请注意，<a class="ae jn" href="https://visualqa.org/" rel="noopener ugc nofollow" target="_blank"> VQA团队</a>已经发布了新的数据集，所以在你开始开发自己的模型之前，请务必检查它们。</p><h1 id="e1a1" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated"><strong class="ak">实施总结</strong></h1><p id="8bb3" class="pw-post-body-paragraph jv jw hi jx b jy ls ij ka kb lt im kd ke lu kg kh ki lv kk kl km lw ko kp kq hb bi translated">我们使用训练图像和相应的训练问题和答案来训练我们的模型。我们使用验证图像和相应的验证问题和答案来获得预测和计算准确性。</p><ol class=""><li id="ca71" class="lx ly hi jx b jy jz kb kc ke lz ki ma km mb kq mc md me mf bi translated">我们首先生成所需组合的特征集，将它们对齐并连接起来以获得稀疏向量表示。</li><li id="5af7" class="lx ly hi jx b jy mg kb mh ke mi ki mj km mk kq mc md me mf bi translated">Caffe用于从<a class="ae jn" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank"> VGGNET </a>中提取激活的图像特征，对于问答特征，使用字母三元组计数向量的词袋表示。</li><li id="32cb" class="lx ly hi jx b jy mg kb mh ke mi ki mj km mk kq mc md me mf bi translated">馈入两个特征以训练DMSM并更新权重矩阵。</li></ol><p id="3014" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">使用Torch很慢，每个时期大约需要2-3个小时，因此我们的实现使用了DMSM C#参考代码，每个时期大约需要70-100分钟。因此，对于100个时期，我们需要大约5-7天来训练一个模型。</p><p id="b497" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">为了测试，我们通过它们的嵌入向量之间的余弦相似性来测量两个输入之间的相似性。例如，我们计算给定图像的嵌入，并使用多模态余弦相似性分数来寻找图像的最近的问题+答案嵌入。这展示了询问关于图像的什么问题的能力，以及获得自动生成的问题的答案的能力。</p><h1 id="85f9" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated"><strong class="ak">结果</strong></h1><p id="4b7e" class="pw-post-body-paragraph jv jw hi jx b jy ls ij ka kb lt im kd ke lu kg kh ki lv kk kl km lw ko kp kq hb bi translated">实验显示了VQA扩展模型的有希望的结果。原始VQA模型的一些示例结果如图3所示。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es ml"><img src="../Images/6219d1a25e7933ccc617fd3d8f551645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FFHJqzfZSZe7sg81r3FPxw.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图3: VQA模型示例结果:顶行显示成功案例，底行显示失败案例。如第一行所示，在某些情况下，机器可以正确回答给定图像的问题。机器可以回答抽象的概念问题(第三个例子)以及简单的问题(第四个例子)。</figcaption></figure><p id="de5b" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">QA对检索示例结果如图4所示。给定一个查询图像，机器检索相应的QA对，并根据相似性得分以降序对这些对进行重新排序。即使正确答案排名不是很高，检索到的答案也与查询图像相关。在第一个示例中，所有排名前3的QA对都包含查询图像中包含的“table”和“fruit”。在第三个示例中，排名前三的QA对中有两个包含“猫”，而另一个将图像中的猫误分类为狗。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mm"><img src="../Images/3db281651dcb8d00db4483c6ba0ee262.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gg4HaOKDOnpRfEA0PQ7ZDg.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图4: QA对检索模型示例结果。</figcaption></figure><p id="e7b3" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">图像检索结果的例子如图5所示。给定一个查询QA对，机器检索相应的图像，并根据相似性得分对图像重新排序。类似于QA检索结果，前3个检索到的图像与查询QA对相当相关。例如，图5中的第二行将“大象在游泳吗”和“不”作为问答配对。所有前3名的检索图像都包含大象，而且它们没有游泳。图5中的第三行有“这个人在哪里做饭”和“烤箱”作为QA对。所有前3名检索到的图像都包含烹饪场景。其中两个装有烤箱。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mn"><img src="../Images/278ee4125c244d56411783b574658f1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7-4DvxIzO5d5GVGhDErmBg.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图5:图像检索模型示例结果。</figcaption></figure><p id="ed01" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">危险模型示例结果如图6所示。给定一幅图像和一个答案，机器试图找出问题所在。该模型还合理地检索了相关问题。在第一个例子中，查询图像是洗手间图像，而查询答案是“瓷砖”。所有前3个检索到的问题都包含“下限”或“上限”。在第三个示例中，查询图像包含关于一个男人正在玩滑板的场景，答案是“滑板”。所有前3个检索到的问题都有一个“某人在做什么”的形式。但是最重要的问题是“消防员在做什么”，这肯定是不正确的。这是因为图像的模糊部分。光线的模糊可能给图像特征提取模块带来混乱。</p><figure class="iy iz ja jb fd jc er es paragraph-image"><div role="button" tabindex="0" class="jd je di jf bf jg"><div class="er es mn"><img src="../Images/a11491dbb3285dc4c1971401d610cb4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*odJNerDLAd_dyAbPWvIVEw.jpeg"/></div></div><figcaption class="jj jk et er es jl jm bd b be z dx translated">图6:危险模型示例结果。</figcaption></figure><h1 id="3918" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">结论</h1><p id="0df5" class="pw-post-body-paragraph jv jw hi jx b jy ls ij ka kb lt im kd ke lu kg kh ki lv kk kl km lw ko kp kq hb bi translated">VQA扩展有助于探索新的潜在应用，如通用对象识别、整体场景理解、从图像中讲述信息和故事，或者开发提出有关图像问题的交互式教育应用。尽管目前的预测与人类决策相比表现不佳，但更新、更大的数据集以及更多平台和设备的采用将使计算机能够更直观地理解数据，并改变我们搜索数据和与数据互动的方式。在这项研究的范围内，一个有趣的下一步将是使用原始VQA模型的权重将迁移学习应用于三个扩展模型。迁移学习可以减少三个扩展模型的训练时间，提高模型精度。</p><h1 id="816b" class="la lb hi bd lc ld le lf lg lh li lj lk io ll ip lm ir ln is lo iu lp iv lq lr bi translated">参考</h1><p id="0483" class="pw-post-body-paragraph jv jw hi jx b jy ls ij ka kb lt im kd ke lu kg kh ki lv kk kl km lw ko kp kq hb bi translated">[1]方、郝、古普塔、索拉博、伊恩多拉、福里斯特、斯里瓦斯塔瓦、鲁佩斯、邓、李、杜大伟、皮奥特、高、何剑锋、何晓东、米切尔、玛格丽特、普拉特、约翰c、劳伦斯兹尼克、c和茨威格、杰弗里。从字幕到视觉概念，再到视觉概念。2015年6月，IEEE计算机视觉和模式识别大会(CVPR)。</p><p id="cceb" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">[2]安托尔、、阿格拉瓦尔、艾什瓦尔亚、卢、、米切尔、玛格丽特、巴特拉、德茹夫、兹特尼克、c .劳伦斯和帕里克、德维。Vqa:视觉问答。2015年国际计算机视觉会议(ICCV)</p><p id="5116" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">[3]陈、、方、郝、林、宗毅、韦丹丹、罗摩克里希纳、笈多、绍拉布、杜尔达、皮奥特和兹尼克、C .劳伦斯。微软coco说明:数据收集和评估服务器。arXiv预印本arXiv:1504.00325，2015。</p><p id="6a5f" class="pw-post-body-paragraph jv jw hi jx b jy jz ij ka kb kc im kd ke kf kg kh ki kj kk kl km kn ko kp kq hb bi translated">[4] Simonyan，k .和Zisserman，a .用于大规模图像识别的非常深的卷积网络。更正，abs/1409.1556，2014年。</p></div></div>    
</body>
</html>