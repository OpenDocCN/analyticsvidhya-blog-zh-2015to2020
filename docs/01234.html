<html>
<head>
<title>Locally Linear Embedding (LLE) | Data Mining and Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">局部线性嵌入(LLE) |数据挖掘和机器学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/locally-linear-embedding-lle-data-mining-b956616d24e9?source=collection_archive---------0-----------------------#2019-10-10">https://medium.com/analytics-vidhya/locally-linear-embedding-lle-data-mining-b956616d24e9?source=collection_archive---------0-----------------------#2019-10-10</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/f10fd6047078b0e0ee27d671356073d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PQzs5oJd2nupur8p"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">阿什利·朱利斯在<a class="ae iu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</figcaption></figure><p id="8c07" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" rel="noopener" href="/@mihirkhandekar/locally-linear-embedding-lle-data-mining-b956616d24e9?source=friends_link&amp;sk=765b92ff2187dd089b07aab5c60737fd">阅读无付费墙</a></p><p id="ad56" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">局部线性嵌入(LLE)是Sam T. Roweis和Lawrence K. Saul于2000年在他们的论文<a class="ae iu" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.456.5279&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">“通过局部线性嵌入进行非线性降维”</a>中提出的一种非线性降维方法。本文基于参考资料部分提到的多个来源。Jennifer Chu的项目帮助我更好地了解LLE。</p><p id="52b0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">机器学习算法使用它们被训练的特征来预测输出。例如，在房价预测问题的情况下，可能有许多特征，如房子的大小、卧室的数量、浴室的数量等。使用某种机器学习模型对其进行训练，以尽可能准确地预测房价。许多机器学习算法在这样做时面临的一个主要问题是<a class="ae iu" href="https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/" rel="noopener ugc nofollow" target="_blank">过拟合</a>，其中模型拟合训练数据如此之好，以至于它无法准确预测真实生活的测试数据。这是一个问题，因为它使得算法非常有效。</p><p id="36aa" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><a class="ae iu" href="https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e" rel="noopener" target="_blank">降维</a>有助于降低机器学习模型的复杂性，有助于在一定程度上减少过拟合。这是因为我们使用的功能越多，模型就越复杂，这可能会导致模型与数据拟合得太好，从而导致过度拟合。也可以使用对决定输出标签没有帮助的特征，这在现实生活中可能没有帮助。比如，在房价预测问题中，我们可能会有一个类似于卖家年龄的特征，这个特征可能不会以任何方式影响房价。降维有助于我们保留特征集中更重要的特征，减少预测输出所需的特征数量。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es jt"><img src="../Images/236f70ee0840ac112aacc7b98da31480.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/0*QQuv3yfPOA97ucrP.jpg"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">降维。来源:https://www.geeksforgeeks.org/dimensionality-reduction/</figcaption></figure></div><div class="ab cl jy jz gp ka" role="separator"><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd"/></div><div class="hb hc hd he hf"><p id="c7e5" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">局部线性嵌入(LLE) </strong></p><p id="7871" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">数据集通常可以在n维特征空间中表示，每个维度用于一个特定的特征。</p><p id="2954" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">LLE算法是一种无监督的降维方法。它试图减少这些n维，同时试图保留原始非线性特征结构的几何特征。例如，在下图中，我们将瑞士卷的结构投射到一个低维平面，同时保持其几何结构。</p><p id="286b" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">简而言之，如果我们有数据X1的D维，我们试图将X1简化为D维特征空间中的X2。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es kf"><img src="../Images/07ef42066d7a8b6b31b981ebe9117497.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/0*qKZ15W97RfHDqsUM"/></div></figure><div class="ju jv jw jx fd ab cb"><figure class="kg ij kh ki kj kk kl paragraph-image"><img src="../Images/a75f05ee05cf1000d077cf63c080d833.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/0*PtwC2IuDCWTA2sDa.gif"/></figure><figure class="kg ij kh ki kj kk kl paragraph-image"><img src="../Images/b0f38b2e964a2e4538a3afcf7e3df31d.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/0*aqDuC5VvCSSck6UY.gif"/><figcaption class="iq ir et er es is it bd b be z dx km di kn ko translated">来源:https://cs.nyu.edu/~roweis/lle/swissroll.html<a class="ae iu" href="https://cs.nyu.edu/~roweis/lle/swissroll.html" rel="noopener ugc nofollow" target="_blank"/></figcaption></figure></div><p id="9593" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">LLE首先找到这些点的k个最近邻。然后，它将每个数据向量近似为其k个最近邻的加权线性组合。最后，它计算从其相邻向量中最佳重构向量的权重，然后产生由这些权重最佳重构的低维向量[6]。</p><ol class=""><li id="7f7b" class="kp kq hi ix b iy iz jc jd jg kr jk ks jo kt js ku kv kw kx bi translated">寻找K个最近的邻居。<br/>LLE算法的一个优点是只有一个参数需要调整，即K的值，或者作为聚类一部分的最近邻居的数量。如果K选择得太小或太大，它将不能容纳原始数据的几何形状。<br/>这里，对于我们拥有的每个数据点，我们计算K个最近的邻居。</li><li id="5361" class="kp kq hi ix b iy ky jc kz jg la jk lb jo lc js ku kv kw kx bi translated">我们对每个点的邻居进行加权聚合，以构建一个新点。我们试图最小化成本函数，其中第j个最近邻为点Xi。</li></ol><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es ld"><img src="../Images/306e0c587c2f17a3b719f1fe5ff19df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/0*BDRcoJ--TQX8pOc4"/></div></figure><p id="a1c8" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">3.现在我们定义新的向量空间Y，使得我们最小化Y作为新点的成本。</p><figure class="ju jv jw jx fd ij er es paragraph-image"><div class="er es le"><img src="../Images/32991a9767bfca58552efd81a434311c.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/0*oDwoWUXC752bIgJ2"/></div></figure><figure class="ju jv jw jx fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lf"><img src="../Images/b36b87323ec453583eece4ce4cbd7e48.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/0*dey-qDexfQvENUrl.gif"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated"><a class="ae iu" href="https://cs.nyu.edu/~roweis/lle/algorithm.html" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure><p id="adce" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个算法的详细算法伪代码可以在<a class="ae iu" href="https://cs.nyu.edu/~roweis/lle/algorithm.html" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="3878" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">LLE相对于其他降维算法的优势</strong></p><ol class=""><li id="17e9" class="kp kq hi ix b iy iz jc jd jg kr jk ks jo kt js ku kv kw kx bi translated">对结构非线性的考虑<br/> LLE超越了密度建模技术，如局部主成分分析或混合因子分析。密度模型不能提供一组一致的全球坐标，将观测值嵌入整个管汇；因此，它们不能用于，例如，可视化原始数据集的低维投影。如下图所示，它们只能检测线性要素。它在探测下面的弯曲图案方面做得不好，而LLE能够探测到。<br/>同样，其他方法如核PCA、Isomap也无法检测到LLE检测到的特征。<br/>在下面的图像中，我们观察到局部点的邻域被LLE保留了，但没有被其他算法保留。</li></ol><div class="ju jv jw jx fd ab cb"><figure class="kg ij lg ki kj kk kl paragraph-image"><img src="../Images/2845da72efa58d2c68b23143060011a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*REVn2bCFYxs_63wVcEraWg.png"/></figure><figure class="kg ij lh ki kj kk kl paragraph-image"><img src="../Images/aaf5ecd11084aeffce7e7ee4ee6a0ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*mRGKsbs3eVVTmDzvlEYLgA.png"/><figcaption class="iq ir et er es is it bd b be z dx li di lj ko translated">左起:三维瑞士卷特征，使用PCA的瑞士卷示例<a class="ae iu" href="http://www.math.sjsu.edu/~gchen/Math285F15/285%20Final%20Project%20-%20LLE.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></figcaption></figure></div><div class="ab cb"><figure class="kg ij lk ki kj kk kl paragraph-image"><img src="../Images/8a40f0ccfdd55e42cb899597b81a00f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*LaC_3UjcsWqw4L8SK3GfTw.png"/></figure><figure class="kg ij ll ki kj kk kl paragraph-image"><img src="../Images/4645eed7bcefe1fa31129fbc7c518e4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*hFYJhrO2dBx_6XC5RcAUEw.png"/><figcaption class="iq ir et er es is it bd b be z dx lm di ln ko translated">左起:使用内核PCA的瑞士卷特征，使用LLE的瑞士卷特征</figcaption></figure></div><p id="e05e" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.更好的计算时间<br/>由于LLE倾向于累积稀疏矩阵，因此在计算空间和时间方面比其他算法更有效。</p></div><div class="ab cl jy jz gp ka" role="separator"><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd ke"/><span class="kb bw bk kc kd"/></div><div class="hb hc hd he hf"><p id="77d0" class="pw-post-body-paragraph iv iw hi ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hj">参考文献</strong></p><ol class=""><li id="87f7" class="kp kq hi ix b iy iz jc jd jg kr jk ks jo kt js ku kv kw kx bi translated"><a class="ae iu" href="http://www.math.sjsu.edu/~gchen/Math285F15/285%20Final%20Project%20-%20LLE.pdf" rel="noopener ugc nofollow" target="_blank">http://www . math . sjsu . edu/~ gchen/math 285 f15/285% 20 final % 20 project % 20-% 20 LLE . pdf</a></li><li id="091e" class="kp kq hi ix b iy ky jc kz jg la jk lb jo lc js ku kv kw kx bi translated"><a class="ae iu" href="https://cs.nyu.edu/~roweis/lle/" rel="noopener ugc nofollow" target="_blank">https://cs.nyu.edu/~roweis/lle/</a></li><li id="03cb" class="kp kq hi ix b iy ky jc kz jg la jk lb jo lc js ku kv kw kx bi translated"><a class="ae iu" href="https://www.geeksforgeeks.org/dimensionality-reduction/" rel="noopener ugc nofollow" target="_blank">https://www.geeksforgeeks.org/dimensionality-reduction/</a></li><li id="2198" class="kp kq hi ix b iy ky jc kz jg la jk lb jo lc js ku kv kw kx bi translated"><a class="ae iu" href="https://towardsdatascience.com/dimensionality-reduction-for-machine-learning-80a46c2ebb7e" rel="noopener" target="_blank">https://towards data science . com/dimensionally-reduction-for-machine-learning-80 a 46 C2 ebb 7 e</a></li><li id="88e6" class="kp kq hi ix b iy ky jc kz jg la jk lb jo lc js ku kv kw kx bi translated"><a class="ae iu" href="https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad" rel="noopener" target="_blank">https://towards data science . com/principal-component-analysis-for-dimensionally-reduction-115 a3 d 157 bad</a></li><li id="f72c" class="kp kq hi ix b iy ky jc kz jg la jk lb jo lc js ku kv kw kx bi translated"><a class="ae iu" href="https://www.youtube.com/watch?v=scMntW3s-Wk&amp;t=27s" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=scMntW3s-Wk&amp;t = 27s</a></li></ol></div></div>    
</body>
</html>