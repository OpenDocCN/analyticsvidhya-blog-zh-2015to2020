<html>
<head>
<title>Practical Implementation Of K-means, Hierarchical, and DBSCAN Clustering On Dataset With Hyperparameter Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于超参数优化的 K-means 聚类、层次聚类和 DBSCAN 聚类的实际实现</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/practical-implementation-of-k-means-hierarchical-and-dbscan-clustering-on-dataset-with-bd7f3d13ef7f?source=collection_archive---------4-----------------------#2020-09-17">https://medium.com/analytics-vidhya/practical-implementation-of-k-means-hierarchical-and-dbscan-clustering-on-dataset-with-bd7f3d13ef7f?source=collection_archive---------4-----------------------#2020-09-17</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="f25f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">超参数优化的聚类算法</p><p id="0aa9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目录:</p><p id="0147" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">㈠文章议程</p><p id="6a0f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">㈡数据处理</p><p id="4050" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(iii)具有超参数优化的 K 均值聚类</p><p id="36d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">㈣等级聚类</p><p id="579c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">(v)具有超参数优化的 DBSCAN 聚类</p><p id="1451" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">㈥结论</p><p id="caf4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">㈦参考资料</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/6812fdcadfae5415045efe941890d406.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*4HWMFPLt0tNiSUgC3pl9Eg.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">图片来源:Scikit learn</figcaption></figure><h1 id="0444" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">㈠文章议程:</h1><p id="84d1" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">这篇文章纯粹是关于在任何数据集上实现聚类算法的。我们也做超参数优化。</p><p id="695c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">先决条件:对 K-means、层次结构和 DBSCAN 聚类有基本的了解</p><p id="6d6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在本文中，我一直遵循 https://www . ka ggle . com/vjchoudhary 7/customer-segmentation-tutorial-in-pythonMall _ customers . CSV 数据集</p><p id="a879" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请从以上链接下载数据集</p><h1 id="eb7b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">㈡数据处理:</h1><p id="fd14" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">我们使用熊猫读取 CSV 文件</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="faf2" class="ky jq hi ku b fi kz la l lb lc">import numpy as np<br/>import pandas as pd<br/>import matplotlib.pyplot as plt</span><span id="aef8" class="ky jq hi ku b fi ld la l lb lc"># Reading csv file</span><span id="ef5c" class="ky jq hi ku b fi ld la l lb lc">df=pd.read_csv('Customers.csv')</span><span id="f01a" class="ky jq hi ku b fi ld la l lb lc">df.head()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es le"><img src="../Images/287c556f264c39c6d85468665a686261.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nlt0YGAyBwYLw6uwDEhjmg.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">df 的前 5 行</figcaption></figure><p id="731c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">数据集包含 5 个特征</p><h2 id="ada3" class="ky jq hi bd jr lj lk ll jv lm ln lo jz iq lp lq kd iu lr ls kh iy lt lu kl lv bi translated">问题陈述:我们需要根据人们的年收入(k$)和花费(花费分数(1-100))对他们进行分类</h2><p id="e734" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">因此，我们的聚类特征是年收入(k$)和支出分数(1-100)</p><p id="b7be" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">消费分数只不过是给出他们消费多少的基础的分数</p><p id="1de3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">f1:年收入(千美元)</p><p id="6f52" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">f2:支出得分(1-100)</p><p id="93d7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们需要用数据帧 d f 中的 f1(x)和 f2 (y)创建一个数组</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="704e" class="ky jq hi ku b fi kz la l lb lc"># converting features f1 and f2 into an array <br/>X=df.iloc[:,[3,4]].values</span></pre><p id="5c79" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有了数组形式的特性，现在我们可以开始实现步骤</p><h1 id="e425" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">㈢K 均值聚类:</h1><p id="4c23" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">K-means 聚类是基于质心的算法</p><p id="b16a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">K =否。聚类=超参数</p><p id="033a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们用肘法求 K 值</p><p id="e5e7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">K-means 目标函数是<strong class="ih hj"> argmin (sum(||x-c||) </strong></p><p id="ba42" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">其中 x =聚类中的数据点</p><p id="aec9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">c=群集的质心</p><p id="2c28" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目的:我们需要最小化数据点和质心之间的平方距离</p><p id="9456" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们有 K-簇，那么我们就有 K-质心</p><p id="5fc0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">簇内距离:同一簇中数据点之间的距离</p><p id="b83a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">簇间距离:不同簇之间的距离</p><p id="e977" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们的主要目标是选择具有小的簇内距离和大的簇间距离的簇</p><p id="2dd2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们使用 K-means++初始化(概率方法)</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="d48c" class="ky jq hi ku b fi kz la l lb lc">from sklearn.cluster import KMeans</span><span id="b461" class="ky jq hi ku b fi ld la l lb lc"># objective function is nothing but argmin of c (sum of (|x-c|)^2 )  c: centroid ,x=point in data set</span><span id="3f72" class="ky jq hi ku b fi ld la l lb lc">objective_function=[] <br/>for i in range(1,11):<br/>    clustering=KMeans(n_clusters=i, init='k-means++')<br/>    clustering.fit(X)<br/>    objective_function.append(clustering.inertia_)</span><span id="da17" class="ky jq hi ku b fi ld la l lb lc">#inertia is calculaing min intra cluster distance<br/># objective function contains min intra cluster distances </span><span id="ec46" class="ky jq hi ku b fi ld la l lb lc">objective_function</span></pre><p id="4154" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">目标函数:最小簇内距离</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="7afc" class="ky jq hi ku b fi kz la l lb lc">[269981.28000000014,<br/> 183116.4295463669,<br/> 106348.37306211119,<br/> 73679.78903948837,<br/> 44448.45544793369,<br/> 37233.81451071002,<br/> 31599.13139461115,<br/> 25012.917069885472,<br/> 21850.16528258562,<br/> 19701.35225128174]</span></pre><p id="9a32" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们尝试了 1 到 10 之间的 K 值，我们不知道哪个是最好的 K 值</p><p id="d1b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了知道最佳 K 值，我们进行超参数优化</p><p id="7e9e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">弯头法:超参数优化</strong></p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="a206" class="ky jq hi ku b fi kz la l lb lc"># for finding optimal no of clusters we use elbow technique <br/># Elbow technique is plot between no of clusters and objective_function <br/># we take k at a point where the objective function value have elbow shape </span><span id="8038" class="ky jq hi ku b fi ld la l lb lc"><br/>plt.plot(range(1,11),objective_function)<br/>plt.title(‘The Elbow Method’)<br/>plt.xlabel(‘Number of Clusters K’)<br/>plt.ylabel(‘objective_function’)<br/>plt.show()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lw"><img src="../Images/8008bf65dd9bb99b8bf150dd9d750a4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1350/format:webp/1*QwXQJf_rw_kDapqxUYauQQ.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">超参数优化</figcaption></figure><p id="acd5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，K=5 时，我们得到了我们认为最佳的 K=5 的肘关节</p><p id="1323" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在我们用最佳 K 值训练一个模型</p><p id="f9ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">#用最佳聚类数训练模型</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="35d8" class="ky jq hi ku b fi kz la l lb lc"># Training the model with optimal no of clusters</span><span id="bbec" class="ky jq hi ku b fi ld la l lb lc">tuned_clustering=KMeans(n_clusters=5,init=’kmeans++’,random_state=0)<br/>labels=tuned_clustering.fit_predict(X)</span><span id="64cb" class="ky jq hi ku b fi ld la l lb lc"># x and y  coordinates of all clusters<br/># Centroids of clusters<br/>tuned_clustering.cluster_centers_[:]</span></pre><p id="d17d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">标签返回:预测分类</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="6263" class="ky jq hi ku b fi kz la l lb lc">array([3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1,<br/>       3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 0,<br/>       3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br/>       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br/>       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,<br/>       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 2, 0, 2, 4, 2, 4, 2,<br/>       0, 2, 4, 2, 4, 2, 4, 2, 4, 2, 0, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2,<br/>       4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2,<br/>       4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2, 4, 2,<br/>       4, 2])</span></pre><p id="d94b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">tuned _ clustering . cluster _ centers _[:]return:每个分类的质心坐标</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="ff1e" class="ky jq hi ku b fi kz la l lb lc">array([[55.2962963 , 49.51851852],<br/>       [25.72727273, 79.36363636],<br/>       [86.53846154, 82.12820513],<br/>       [26.30434783, 20.91304348],<br/>       [88.2       , 17.11428571]])</span></pre><p id="2872" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可视化集群</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="ac01" class="ky jq hi ku b fi kz la l lb lc"># visualizing the clusters</span><span id="527e" class="ky jq hi ku b fi ld la l lb lc">plt.scatter(X[labels==0,0],X[labels==0,1],c=’green’,label=’cluster1')<br/>plt.scatter(X[labels==1,0],X[labels==1,1],c=’yellow’,label=’cluster2')<br/>plt.scatter(X[labels==2,0],X[labels==2,1],c=’red’,label=’cluster3')<br/>plt.scatter(X[labels==3,0],X[labels==3,1],c=’orange’,label=’cluster4')<br/>plt.scatter(X[labels==4,0],X[labels==4,1],c=’blue’,label=’cluster5')</span><span id="5fef" class="ky jq hi ku b fi ld la l lb lc">plt.scatter(tuned_clustering.cluster_centers_[:,0],tuned_clustering.cluster_centers_[:,1],s=300,c=’black’,label=’centroid’)</span><span id="e002" class="ky jq hi ku b fi ld la l lb lc">plt.title(‘Clusters of customers’)<br/>plt.xlabel(‘Annual Income(K$)’)<br/>plt.ylabel(‘Spending Score(1–100)’)<br/>plt.legend()<br/>plt.show()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es lx"><img src="../Images/3a5c400b95821a4de9925662a9e03f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*RCKU_VtYd7iVSBTMoWKl-A.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">根据年收入和支出对人群进行聚类</figcaption></figure><p id="7848" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">评估:我们的聚类有多好</p><p id="2a83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了检查我们的聚类有多好，我们使用了轮廓系数</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es ly"><img src="../Images/6f6c6a8c9c95fdb9af9e37a04df50649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wVNbied7BBTiCv978F0nnQ.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">轮廓系数</figcaption></figure><p id="e53b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在上图中，我们有两个集群 C1 和 C2</p><p id="8f77" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">a=簇内距离</p><p id="7928" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">b =簇间距离</p><p id="3452" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">轮廓系数= b-a/max(b，a)</p><p id="db82" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们的聚类是好的，那么我们具有小的聚类内距离，那么轮廓系数值是正的</p><p id="587c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们的聚类不好，那么我们具有大的聚类内距离，那么轮廓系数值是负的</p><p id="66b2" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">轮廓系数介于-1 和 1 之间</p><p id="f9a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果该值向 1 移动，那么聚类是好的</p><p id="caf8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果对于 n_clusters = 3，该值向&lt;0 then clustering is bad</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="b4a2" class="ky jq hi ku b fi kz la l lb lc">from sklearn import metrics<br/>metrics.silhouette_score(X, tuned_clustering.labels_,<br/>metric='euclidean')</span></pre><p id="2ffa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">We got the Silhouette coefficient value is 0.553931997444648</p><p id="75d6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">It’s moving towards 1 so our clustering is good</p><p id="4f03" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">If you want to visualize the Silhouette coefficient</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="0026" class="ky jq hi ku b fi kz la l lb lc"># visualizing Silhouette coefficient</span><span id="6797" class="ky jq hi ku b fi ld la l lb lc">for n_clusters in range(2,10):<br/> # Create a subplot with 1 row and 2 columns<br/> fig, (ax1, ax2) = plt.subplots(1, 2)<br/> fig.set_size_inches(18, 7)</span><span id="c9d7" class="ky jq hi ku b fi ld la l lb lc"># The 1st subplot is the silhouette plot<br/> # The silhouette coefficient can range from -1, 1 but in this example all<br/> # lie within [-0.1, 1]<br/> ax1.set_xlim([-0.1, 1])<br/> # The (n_clusters+1)*10 is for inserting blank space between silhouette<br/> # plots of individual clusters, to demarcate them clearly.<br/> ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])</span><span id="77e8" class="ky jq hi ku b fi ld la l lb lc"># Initialize the clusterer with n_clusters value and a random generator<br/> # seed of 10 for reproducibility.<br/> clusterer = KMeans(n_clusters=n_clusters, random_state=10)<br/> cluster_labels = clusterer.fit_predict(X)</span><span id="d6eb" class="ky jq hi ku b fi ld la l lb lc"># The silhouette_score gives the average value for all the samples.<br/> # This gives a perspective into the density and separation of the formed<br/> # clusters<br/> silhouette_avg = silhouette_score(X, cluster_labels)<br/> print(“For n_clusters =”, n_clusters,<br/> “The average silhouette_score is :”, silhouette_avg)</span><span id="d80a" class="ky jq hi ku b fi ld la l lb lc"># Compute the silhouette scores for each sample<br/> sample_silhouette_values = silhouette_samples(X, cluster_labels)</span><span id="2134" class="ky jq hi ku b fi ld la l lb lc">y_lower = 10<br/> for i in range(n_clusters):<br/> # Aggregate the silhouette scores for samples belonging to<br/> # cluster i, and sort them<br/> ith_cluster_silhouette_values = \<br/> sample_silhouette_values[cluster_labels == i]</span><span id="6e75" class="ky jq hi ku b fi ld la l lb lc">ith_cluster_silhouette_values.sort()</span><span id="03c5" class="ky jq hi ku b fi ld la l lb lc">size_cluster_i = ith_cluster_silhouette_values.shape[0]<br/> y_upper = y_lower + size_cluster_i</span><span id="3462" class="ky jq hi ku b fi ld la l lb lc">color = cm.nipy_spectral(float(i) / n_clusters)<br/> ax1.fill_betweenx(np.arange(y_lower, y_upper),<br/> 0, ith_cluster_silhouette_values,<br/> facecolor=color, edgecolor=color, alpha=0.7)</span><span id="205e" class="ky jq hi ku b fi ld la l lb lc"># Label the silhouette plots with their cluster numbers at the middle<br/> ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))</span><span id="da9f" class="ky jq hi ku b fi ld la l lb lc"># Compute the new y_lower for next plot<br/> y_lower = y_upper + 10 # 10 for the 0 samples</span><span id="89b1" class="ky jq hi ku b fi ld la l lb lc">ax1.set_title(“The silhouette plot for the various clusters.”)<br/> ax1.set_xlabel(“The silhouette coefficient values”)<br/> ax1.set_ylabel(“Cluster label”)</span><span id="4190" class="ky jq hi ku b fi ld la l lb lc"># The vertical line for average silhouette score of all the values<br/> ax1.axvline(x=silhouette_avg, color=”red”, linestyle=” — “)</span><span id="759d" class="ky jq hi ku b fi ld la l lb lc">ax1.set_yticks([]) # Clear the yaxis labels / ticks<br/> ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])</span><span id="554a" class="ky jq hi ku b fi ld la l lb lc"># 2nd Plot showing the actual clusters formed<br/> colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)<br/> ax2.scatter(X[:, 0], X[:, 1], marker=’.’, s=30, lw=0, alpha=0.7,<br/> c=colors, edgecolor=’k’)</span><span id="57f2" class="ky jq hi ku b fi ld la l lb lc"># Labeling the clusters<br/> centers = clusterer.cluster_centers_<br/> # Draw white circles at cluster centers<br/> ax2.scatter(centers[:, 0], centers[:, 1], marker=’o’,<br/> c=”white”, alpha=1, s=200, edgecolor=’k’)</span><span id="2bc5" class="ky jq hi ku b fi ld la l lb lc">for i, c in enumerate(centers):<br/> ax2.scatter(c[0], c[1], marker=’$%d$’ % i, alpha=1,<br/> s=50, edgecolor=’k’)</span><span id="c24d" class="ky jq hi ku b fi ld la l lb lc">ax2.set_title(“The visualization of the clustered data.”)<br/> ax2.set_xlabel(“Feature space for the 1st feature”)<br/> ax2.set_ylabel(“Feature space for the 2nd feature”)</span><span id="2dda" class="ky jq hi ku b fi ld la l lb lc">plt.suptitle((“Silhouette analysis for KMeans clustering on sample data “<br/> “with n_clusters = %d” % n_clusters),<br/> fontsize=14, fontweight=’bold’)</span><span id="b691" class="ky jq hi ku b fi ld la l lb lc">plt.show()</span></pre><p id="c4eb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">For n_clusters = 2 The average silhouette_score is : 0.3273163942500746<br/>移动，则平均剪影分数为:0.46761358158775435 <br/>对于 n_clusters = 4，平均剪影分数为:0.4931963109249047 <br/>对于 n_clusters = 5，平均剪影分数为:0.553931997444648 <br/>对于 n_clusters = 6，平均剪影分数为:0.55</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es lz"><img src="../Images/db499a949513a4f1099b70ada6212698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vT0SollZnMAGxiZLNn-Umg.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">K=5 轮廓系数</figcaption></figure><p id="c307" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们观察 k=5 的上述分数，我们有一个高的轮廓系数</p><p id="d17a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们观察上面的图，集群中没有负向移动，所有的东西都向 1 移动</p><p id="cbec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以我们可以得出结论，我们的聚类是好的</p><h1 id="b5b7" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">㈣等级聚类:</h1><p id="2bac" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">分层聚类是基于树的聚类</p><p id="30b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">树状图:它是一个类似树的图表，记录了合并的顺序</p><p id="5bc9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在层次聚类中，我们使用凝聚聚类</p><p id="98e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤 1:将每个数据点视为一个集群</p><p id="8a18" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤 2:根据相似性(距离)合并聚类</p><p id="f729" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果两个聚类彼此非常接近，则将这两个聚类组合成一个聚类</p><p id="8b0d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">重复直到只剩下一个集群</p><p id="8b05" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在凝聚聚类中，不需要给定作为超参数的聚类数，我们可以停止层次结构，我们想要多少个聚类</p><p id="be6c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">所以聚类数不是一个超参数</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="69a7" class="ky jq hi ku b fi kz la l lb lc"># Dendogram</span><span id="38b7" class="ky jq hi ku b fi ld la l lb lc">import scipy.cluster.hierarchy as sch</span><span id="e71c" class="ky jq hi ku b fi ld la l lb lc">cluster_visualising=sch.dendrogram(sch.linkage(df.iloc[:,[3,4]].values,method=’ward’))<br/>plt.title(‘Dendrogram’)<br/>plt.xlabel(‘Customers’)<br/>plt.ylabel(‘Euclidean distances’)<br/>plt.show()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="lf lg di lh bf li"><div class="er es ma"><img src="../Images/5bbc0172e3d4d9daa0056347eea30b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*p1Fh0rRcQvG4bLpFYdrQeQ.png"/></div></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">系统树图</figcaption></figure><p id="7bfa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们观察上面的树状图，它会根据最小欧几里德距离对客户进行分组</p><p id="65e6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">来自树状图的聚类数=我们选择不能被水平线切割的最大垂直线</p><p id="ea55" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">从树状图中，我们选择 K = 5 =聚类数</p><p id="ab1a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于聚类相似性，我们使用 ward 方法</p><p id="61da" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">沃德方法不易受噪声和异常值的影响</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="f81a" class="ky jq hi ku b fi kz la l lb lc"># AgglomerativeClustering Model initialization</span><span id="2e2d" class="ky jq hi ku b fi ld la l lb lc">from sklearn.cluster import AgglomerativeClustering</span><span id="8851" class="ky jq hi ku b fi ld la l lb lc">clustering_model=AgglomerativeClustering(n_clusters = 5, affinity = ‘euclidean’, linkage = ‘ward’)</span><span id="57f1" class="ky jq hi ku b fi ld la l lb lc">clustering_model.fit(df.iloc[:,[3,4]].values)</span><span id="11b4" class="ky jq hi ku b fi ld la l lb lc"># Predicting clusters<br/>clustering_prediction=clustering_model.fit_predict(df.iloc[:,[3,4]])</span></pre><p id="f9a7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">clustering_predictoin 返回:预测的分类</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="c9d9" class="ky jq hi ku b fi kz la l lb lc">array([4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3,<br/>       4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 3, 4, 1,<br/>       4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,<br/>       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,<br/>       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,<br/>       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 0, 2, 0, 2,<br/>       1, 2, 0, 2, 0, 2, 0, 2, 0, 2, 1, 2, 0, 2, 1, 2, 0, 2, 0, 2, 0, 2,<br/>       0, 2, 0, 2, 0, 2, 1, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2,<br/>       0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2,<br/>       0, 2], dtype=int64)</span></pre><p id="15aa" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">可视化集群</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="50e2" class="ky jq hi ku b fi kz la l lb lc">plt.scatter(df.iloc[:,[3,4]].values[clustering_prediction == 0, 0], df.iloc[:,[3,4]].values[clustering_prediction == 0, 1], s = 100, c = ‘green’, label = ‘Cluster 1’)</span><span id="114a" class="ky jq hi ku b fi ld la l lb lc">plt.scatter(df.iloc[:,[3,4]].values[clustering_prediction == 1, 0], df.iloc[:,[3,4]].values[clustering_prediction == 1, 1], s = 100, c = ‘red’, label = ‘Cluster 2’)</span><span id="b9ee" class="ky jq hi ku b fi ld la l lb lc">plt.scatter(df.iloc[:,[3,4]].values[clustering_prediction == 2, 0], df.iloc[:,[3,4]].values[clustering_prediction == 2, 1], s = 100, c = ‘blue’, label = ‘Cluster 3’)</span><span id="a6b9" class="ky jq hi ku b fi ld la l lb lc">plt.scatter(df.iloc[:,[3,4]].values[clustering_prediction == 3, 0], df.iloc[:,[3,4]].values[clustering_prediction == 3, 1], s = 100, c = ‘yellow’, label = ‘Cluster 4’)</span><span id="68ab" class="ky jq hi ku b fi ld la l lb lc">plt.scatter(df.iloc[:,[3,4]].values[clustering_prediction == 4, 0], df.iloc[:,[3,4]].values[clustering_prediction == 4, 1], s = 100, c = ‘orange’, label = ‘Cluster 5’)</span><span id="2579" class="ky jq hi ku b fi ld la l lb lc">plt.title(‘Clusters of customers’)<br/>plt.xlabel(‘Annual Income (k$)’)<br/>plt.ylabel(‘Spending Score (1–100)’)<br/>plt.legend()<br/>plt.show()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mb"><img src="../Images/99f9f944ae86f554dca86929b3d617e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*cQgpV5n7xtO64wzNruxzBw.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">基于年收入和支出的人群分类</figcaption></figure><p id="8b96" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">估价</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="dfcb" class="ky jq hi ku b fi kz la l lb lc">from sklearn import metrics<br/>metrics.silhouette_score(df.iloc[:,[3,4]].values, clustering_prediction , metric='euclidean')</span></pre><p id="efa1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">轮廓系数:0.5529945955148897</p><p id="42d4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">系数向 1 移动，所以聚类很好</p><h1 id="1f52" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">㈤数据库扫描聚类:</h1><p id="9c1c" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">DBSCAN 是一种基于密度的聚类</p><p id="83d1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在实施之前，我们需要了解这些条款</p><p id="e1d8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">密集区域、稀疏区域、核心点、边界点、噪声点、密度边缘、密度连接点</p><p id="6df3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请参考下面的链接来理解上述条款</p><div class="mc md ez fb me mf"><a href="https://en.wikipedia.org/wiki/DBSCAN" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hj fi z dy mk ea eb ml ed ef hh bi translated">基于密度的噪声应用空间聚类</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">基于密度的含噪声应用空间聚类(DBSCAN)是由 Martin…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">en.wikipedia.org</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt jj mf"/></div></div></a></div><p id="3ac4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在 DBSCAN 中，最小点和ε是超参数</p><p id="e4e1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤 1:对于数据集中的每个点，我们需要标记数据点属于核心点/边界点/噪声点</p><p id="e85e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">第二步:去除所有噪声点</p><p id="3e49" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤 3:对于没有被分配给集群的每个核心点“p”</p><p id="562f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们创建一个没有分配核心点的新群，并将密度连接到没有分配核心点的所有点添加到这个新群中</p><p id="9a69" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤 4:将每个边界点分配到最近的核心点簇</p><p id="01ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">超参数优化:</strong></p><p id="e73b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在 DBSCAN 中，最小点和ε是超参数</p><p id="226e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">最小点数:</p><p id="500b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">经验法则== &gt;最小点数≥维度+1</p><p id="eb60" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果数据集噪声更大，那么我们使用最小点更大，因为它很容易去除噪声点</p><p id="79ff" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">ε(半径):弯头法</p><p id="5148" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">步骤 1:对于每个数据点(x ),我们计算距离(d)</p><p id="bc83" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">stpe2:按升序排列距离</p><p id="d7a1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后绘制距离和点指数之间的图表</p><p id="4fcd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们选择最佳距离(ε)d，在图中急剧上升的地方</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="dd7c" class="ky jq hi ku b fi kz la l lb lc"># we use nearestneighbors for calculating distance between points</span><span id="3386" class="ky jq hi ku b fi ld la l lb lc">from sklearn.neighbors import NearestNeighbors<br/></span><span id="837b" class="ky jq hi ku b fi ld la l lb lc"># calculating distances</span><span id="17ea" class="ky jq hi ku b fi ld la l lb lc">neigh=NearestNeighbors(n_neighbors=2)<br/>distance=neigh.fit(X)<br/></span><span id="7a90" class="ky jq hi ku b fi ld la l lb lc"># indices and distance values<br/>distances,indices=distance.kneighbors(X)<br/></span><span id="9806" class="ky jq hi ku b fi ld la l lb lc"># Now sorting the distance increasing order</span><span id="4533" class="ky jq hi ku b fi ld la l lb lc">sorting_distances=np.sort(distances,axis=0)<br/></span><span id="4b72" class="ky jq hi ku b fi ld la l lb lc"># sorted distances</span><span id="667b" class="ky jq hi ku b fi ld la l lb lc">sorted_distances=sort_distances[:,1]</span><span id="284d" class="ky jq hi ku b fi ld la l lb lc"># plot between distance vs epsilon</span><span id="5942" class="ky jq hi ku b fi ld la l lb lc">plt.plot(sorted_distances)<br/>plt.xlabel(‘Distance’)<br/>plt.ylabel(‘Epsilon’)<br/>plt.show()</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mu"><img src="../Images/ecd7630d8be6b2f4511018389d11604f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1310/format:webp/1*I7PmN03OEZoZYjUMQsrnvA.png"/></div><figcaption class="jl jm et er es jn jo bd b be z dx translated">ε优化</figcaption></figure><p id="1fd5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果我们观察图表，在ε等于 9 时，图表中急剧上升，所以我们选择ε(半径)为 9</p><p id="97b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用优化的超参数实现 DBSCAN</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="eba0" class="ky jq hi ku b fi kz la l lb lc">from sklearn.cluster import DBSCAN</span><span id="98d2" class="ky jq hi ku b fi ld la l lb lc"># intializing DBSCAN</span><span id="86d9" class="ky jq hi ku b fi ld la l lb lc">clustering_model=DBSCAN(eps=9,min_samples=4)</span><span id="185c" class="ky jq hi ku b fi ld la l lb lc"># fit the model to X</span><span id="0471" class="ky jq hi ku b fi ld la l lb lc">clustering_model.fit(X)</span><span id="0756" class="ky jq hi ku b fi ld la l lb lc"># predicted labels by DBSCAN</span><span id="14c0" class="ky jq hi ku b fi ld la l lb lc">predicted_labels=clustering_model.labels_</span><span id="d62d" class="ky jq hi ku b fi ld la l lb lc"><br/># visualzing clusters</span><span id="ce23" class="ky jq hi ku b fi ld la l lb lc">plt.scatter(X[:,0], X[:,1],c=predicted_labels, cmap='Paired')<br/>plt.xlabel('Annual Income')<br/>plt.ylabel('Spending Score')<br/>plt.title("DBSCAN")</span></pre><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mv"><img src="../Images/7eec6bab032920a58bb16d22103fd34f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*thpbEXzquEQk_DDytqTXTA.png"/></div></figure><p id="7c06" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">评估:</p><pre class="je jf jg jh fd kt ku kv kw aw kx bi"><span id="b8ba" class="ky jq hi ku b fi kz la l lb lc">from sklearn import metrics</span><span id="d27c" class="ky jq hi ku b fi ld la l lb lc">metrics.silhouette_score(X, predicted_labels)</span></pre><p id="2195" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">轮廓系数:0.4259680122384905</p><p id="22f9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">轮廓系数向 1 移动，所以我们的聚类很好</p><h1 id="b17e" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">㈥结论:</h1><p id="e172" class="pw-post-body-paragraph if ig hi ih b ii kn ik il im ko io ip iq kp is it iu kq iw ix iy kr ja jb jc hb bi translated">我希望这篇文章对你有所帮助，请给我你对我有帮助的反馈</p><h1 id="c26b" class="jp jq hi bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated"><strong class="ak">(七)参考文献:</strong></h1><div class="mc md ez fb me mf"><a href="https://en.wikipedia.org/wiki/DBSCAN" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hj fi z dy mk ea eb ml ed ef hh bi translated">基于密度的噪声应用空间聚类</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">基于密度的含噪声应用空间聚类(DBSCAN)是由 Martin…</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">en.wikipedia.org</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt jj mf"/></div></div></a></div><div class="mc md ez fb me mf"><a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html#sphx-glr-auto-examples-cluster-plot-dbscan-py" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hj fi z dy mk ea eb ml ed ef hh bi translated">DBSCAN 聚类算法演示-sci kit-学习 0.23.2 文档</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">sci kit-learn:Python 中的机器学习</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">scikit-learn.org</p></div></div><div class="mo l"><div class="mw l mq mr ms mo mt jj mf"/></div></div></a></div><div class="mc md ez fb me mf"><a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab dw"><div class="mh ab mi cl cj mj"><h2 class="bd hj fi z dy mk ea eb ml ed ef hh bi translated">在 KMeans clustering - scikit-learn 上使用剪影分析选择聚类数…</h2><div class="mm l"><h3 class="bd b fi z dy mk ea eb ml ed ef dx translated">sci kit-learn:Python 中的机器学习</h3></div><div class="mn l"><p class="bd b fp z dy mk ea eb ml ed ef dx translated">scikit-learn.org</p></div></div><div class="mo l"><div class="mx l mq mr ms mo mt jj mf"/></div></div></a></div></div></div>    
</body>
</html>