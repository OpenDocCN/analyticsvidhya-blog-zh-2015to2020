<html>
<head>
<title>Understanding the GPT-2 Source Code Part 5</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解新GPT协议源代码第5部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/understanding-the-gpt-2-source-code-part-5-87bbe21dd749?source=collection_archive---------9-----------------------#2019-11-04">https://medium.com/analytics-vidhya/understanding-the-gpt-2-source-code-part-5-87bbe21dd749?source=collection_archive---------9-----------------------#2019-11-04</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/12144d26d16b75423abd551781f730fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a53_lcdtn1QtkF1Ku8UgvQ.jpeg"/></div></div></figure><div class=""/><p id="da18" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">嗨！抱歉更新晚了大概半年。我全神贯注于另一个项目，我想我会很快分享。但是不管怎样，我将结束这个系列。你可以分别在这里、<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-2-4a980c36c68b">这里</a>、<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-3-9796a5a5cc7c">这里</a>和<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-4-a5fbb89e5038">这里</a>阅读第一部、第二部、第三部和第四部<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-1-4481328ee10b">。很抱歉与第4部分有这么长的时间间隔！在这里，我将尝试结束谈论这一切是如何走到一起的。我是新来的，所以如果有什么不清楚的地方，请告诉我！我将感激反馈。</a></p><h1 id="2393" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">正在打包model.py</h1><p id="70d4" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">到上一篇文章为止，除了最后一部分，我讨论了model.py的大部分内容</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="5ebe" class="lb jq ht kx b fi lc ld l le lf">for layer, past in enumerate(pasts):#loop over each past<br/>        h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)<br/>        presents.append(present)<br/>results['present'] = tf.stack(presents, axis=1)<br/>h = norm(h, 'ln_f')</span><span id="e378" class="lb jq ht kx b fi lg ld l le lf"># Language model loss.  Do tokens &lt;n predict token n?<br/>#roughly 1/12th of original size<br/>h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])<br/>logits = tf.matmul(h_flat, wte, transpose_b=True)<br/>logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])<br/>results['logits'] = logits<br/>return results</span></pre><p id="2caf" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如果我们回顾一下，我们知道第二行中的块函数输出两个名为h和present的张量。这些是什么？如果我们回头看看以前的文章，或者更好的是直接看源代码，h的定义如下</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="16e7" class="lb jq ht kx b fi lc ld l le lf">h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))</span></pre><p id="18ef" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里，由于X是输入(表示wte(单词矩阵)中每个字母的索引的数字序列)，它获得句子中的单词向量并将它们排列起来。然后，对它进行wpe(位置编码),它在past_length之后的文本中的每个位置都有一个签名。所以，基本上它说当前输入对应于位置past_length到past _ length+x的序列长度，因此，基本上，h是输入。</p><p id="3777" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在完成块功能后，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="b228" class="lb jq ht kx b fi lc ld l le lf">for layer, past in enumerate(pasts):#loop over each past<br/>    h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)<br/>    presents.append(present)</span></pre><p id="dece" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">h一直在输出。因此，当你遍历循环h时，输入会不断被覆盖并输入到下一层。坦白地说，这对我来说是一个有趣的技术，因为，通常，你倾向于制作这样的层</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="8cfa" class="lb jq ht kx b fi lc ld l le lf">with tf.variable_scope("layer1"):<br/>    fc1 = fully_connected(input, output_size1)<br/>with tf.variable_scope("layer2"):<br/>    fc2 = fully_connected(fc1, output_size2)</span></pre><p id="17ae" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">假设伪函数是完全连接的，但是如果使用上面的方法，看起来你可以一遍又一遍地使用同一个变量，这很酷！</p><p id="ae7d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以看到类似的事情正在发生</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="57a8" class="lb jq ht kx b fi lc ld l le lf">for layer, past in enumerate(pasts):#loop over each past<br/>    h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)<br/>    presents.append(present)</span></pre><p id="3e72" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它们不断被追加到一个叫做礼物的空列表中。我假设这是为了使现在的张量对应于gpt-2的每一层，以便它们可以平滑地添加到过去。让我们看看这是否正确！</p><h1 id="cd3e" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">礼物</h1><p id="543d" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">那么现在的变量发生了什么？我们在前面的章节中看到，在attn函数中，present被设置为</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="6cfd" class="lb jq ht kx b fi lc ld l le lf">c = conv1d(x, 'c_attn', n_state*3)<br/>q, k, v = map(split_heads, tf.split(c, 3, axis=2))<br/>present = tf.stack([k, v], axis=1)</span></pre><p id="6d5f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">其中，x是归一化输入(h ),正如我们所见，这是通过模块函数实现的</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="d944" class="lb jq ht kx b fi lc ld l le lf">def block(x, scope, *, past, hparams):<br/>    with tf.variable_scope(scope):<br/>        nx = x.shape[-1].value<br/>        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)<br/>        x = x + a<br/>        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)<br/>        x = x + m<br/>        return x, present</span></pre><p id="1a05" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们可以说present只是一个应用了1d卷积的h，但是大小是它的两倍。为什么是k和v？如果你读回<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-4-a5fbb89e5038">第4部分</a>，或者在attn函数中，我们看到</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="d101" class="lb jq ht kx b fi lc ld l le lf">if past is not None:<br/>        pk, pv = tf.unstack(past, axis=1)<br/>        k = tf.concat([pk, k], axis=-2)<br/>        v = tf.concat([pv, v], axis=-2)<br/>        a = multihead_attn(q, k, v)</span></pre><p id="092a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这导致了下面的<a class="ae jo" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>中的等式</p><figure class="ks kt ku kv fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es lh"><img src="../Images/6379641130302f9047ac8b84f25e0a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*Rc5KpJ6EJh6rEXvVY7oRQw.png"/></div></div><figcaption class="li lj et er es lk ll bd b be z dx translated">来自<a class="ae jo" href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>的方程式</figcaption></figure><p id="74b2" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了更深入的了解，请去阅读<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-4-a5fbb89e5038">第四部分</a>或其他官方资源(因为我不能说我已经全面理解了)，但基本上，正在发生的是，K，连同Q等等用来创建这个概率分布，我们从中挑选隐藏状态。在这里，V作为整个文档的隐藏状态。当我们对它们求和时，经过一些运算，我们得到了注意力函数的期望输出:上下文向量！我们如何处理所说的上下文向量，我会在下面写一点。</p><p id="2920" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，我们想要的是概率分布和V来包含到当前点为止的所有信息。这是怎么做到的？</p><p id="ee0e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们在主体函数的sample.py中看到以下代码</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="1094" class="lb jq ht kx b fi lc ld l le lf">return [next_outputs['presents'] if past is None else      tf.concat([past, next_outputs['presents']], axis=-2),<br/>                ....)]</span></pre><p id="8b2f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">next_outputs从何而来？首先，它在身体机能中被定义为</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="158f" class="lb jq ht kx b fi lc ld l le lf">next_outputs = step(hparams, prev, past=past)</span></pre><p id="dce6" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">在阶跃函数中，我们看到</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="c8d6" class="lb jq ht kx b fi lc ld l le lf">def step(hparams, tokens, past=None):<br/>        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)</span><span id="6dd7" class="lb jq ht kx b fi lg ld l le lf">        logits = lm_output['logits'][:, :, :hparams.n_vocab]<br/>        presents = lm_output['present']<br/>        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))<br/>        return {<br/>            'logits': logits,<br/>            'presents': presents,<br/>        }</span></pre><p id="552b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们可以看到，它只是从我们在这里看到的模型函数中获取输出，限制逻辑的范围，将呈现的内容整形为shape [batch_size，hparams.n_layer，2，hparams.n_head，sequence，hparams . n _ embd//hparams . n _ head]</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="7733" class="lb jq ht kx b fi lc ld l le lf">def past_shape(*, hparams, batch_size=None, sequence=None):<br/>    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]</span></pre><p id="5dfd" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这是因为在它有了形状之后</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="1287" class="lb jq ht kx b fi lc ld l le lf">[batch_size, layers, 2, heads, sequence_size, hparams.n_embd]</span></pre><p id="2a9b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它没有openAI人用过的头。然后，礼物被放进字典里</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="ab77" class="lb jq ht kx b fi lc ld l le lf">return {<br/>            'logits': logits,<br/>            'presents': presents,<br/>        }</span></pre><p id="7dbb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，身体函数的第一个输出是past，这可以在它的定义后面的行中看到</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="9db6" class="lb jq ht kx b fi lc ld l le lf">past, prev, output = body(None, context, context)</span></pre><p id="9f89" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我们看到现在的输出一直与过去的连接在一起。正如我们将看到的，这个过去的变量被传递到一个循环中，直到文本生成结束。</p><p id="5f99" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">那么上下文向量呢？</p><h1 id="d873" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">上下文向量</h1><p id="aa17" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">当我们查看model.py的模型函数时，我们看到输出的h通过</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="bd9c" class="lb jq ht kx b fi lc ld l le lf">h = norm(h, 'ln_f')</span><span id="3c05" class="lb jq ht kx b fi lg ld l le lf"># Language model loss.  Do tokens &lt;n predict token n?<br/>        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])<br/>        logits = tf.matmul(h_flat, wte, transpose_b=True)<br/>        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])<br/>        results['logits'] = logits<br/>        return results</span></pre><p id="aa8f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这里我们看到，h经过归一化和整形后，乘以wte，即单词矩阵，包含所有单词的编码。当transpose_b设置为true时，即使wte有维度[hparams.n_vocab，hparams.n_embd]，它们也可以相乘。当重塑时，这最终给出了具有形状的逻辑</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="f644" class="lb jq ht kx b fi lc ld l le lf">[batch, sequence, hparams.n_vocab]</span></pre><p id="b6bb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，通过将它放入模型函数result的输出中</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="61c9" class="lb jq ht kx b fi lc ld l le lf">results['logits'] = logits</span></pre><h1 id="fa38" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">我的猜测(可能真的错了，如果是这样，请指正！)</h1><p id="1c78" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">我怀疑它们被增加的原因，在某种意义上，是为了突出正在使用的单词。这有点类似于点积，但是当你用相同的向量做点积时，当它乘以类似的向量时，它通常有最高的值。因此，在矩阵乘法的过程中，h_flat中的每一行与转置的wte矩阵列(每个字向量)进行点积，并在该字的列放置更高的值，该值指示该字在最终矩阵的一行中使用。</p><p id="6460" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">如在h_flat矩阵中</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="f824" class="lb jq ht kx b fi lc ld l le lf">[batch*sequence, hparams.n_embd]</span></pre><p id="d8eb" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每一行都有一个矩阵与单词向量做矩阵乘法，这使我得出结论，在每一行中，机器的目标是创建这个向量，它非常类似于它认为可能作为下一个单词嵌入的所有可能的单词。</p><p id="a37f" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我认为我是正确的，但如果错了，请纠正我！</p><p id="5d3b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">不管怎样，现在，这个张量会发生什么？</p><h1 id="0f7b" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">身体机能</h1><p id="8f00" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">我们在体函数之前的阶跃函数中看到，输出是受限制的</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="453c" class="lb jq ht kx b fi lc ld l le lf">logits = lm_output['logits'][:, :, :hparams.n_vocab]</span></pre><p id="3a7b" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我不确定为什么开放人工智能的人要这样做，因为逻辑的形状在这次操作后应该已经没有变化了，因为逻辑已经有了形状</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="a5bc" class="lb jq ht kx b fi lc ld l le lf">[batch, sequence, hparams.n_vocab]</span></pre><p id="764c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">反正没有进一步的改变，就去身体机能了。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="d06b" class="lb jq ht kx b fi lc ld l le lf">logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)<br/>logits = top_k_logits(logits, k=top_k)<br/>logits = top_p_logits(logits, p=top_p)<br/>samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)</span></pre><p id="941e" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我之前已经简要讨论过这里发生的事情，我发现它非常有趣，所以，如果你想阅读如何从事物的分布方面挑选逻辑，请阅读<a class="ae jo" rel="noopener" href="/@isamu.website/understanding-the-gpt-2-source-code-part-1-4481328ee10b">第1部分</a>的结尾部分。但是，基本上，这里发生的是，logits根据选择的温度(高的更随机，低的更不随机)选择下一个具有多项式分布的单词，这也很酷。</p><p id="7510" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，什么是top _ k _ logits和top _ p _ logits？</p><h1 id="58bf" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">top_k_logits</h1><p id="bc3e" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">top_k_logits定义如下</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="2d2c" class="lb jq ht kx b fi lc ld l le lf">def top_k_logits(logits, k):<br/>    if k == 0:<br/>        # no truncation<br/>        return logits</span><span id="e4f3" class="lb jq ht kx b fi lg ld l le lf">    def _top_k():<br/>        values, _ = tf.nn.top_k(logits, k=k)<br/>        min_values = values[:, -1, tf.newaxis]<br/>        return tf.where(<br/>            logits &lt; min_values,<br/>            tf.ones_like(logits, dtype=logits.dtype) * -1e10,<br/>            logits,<br/>        )<br/>    return tf.cond(<br/>       tf.equal(k, 0),<br/>       lambda: logits,<br/>       lambda: _top_k(),<br/>    )</span></pre><p id="1e5c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">基本上，它所做的是返回k个最大值，逻辑中k个最有可能的单词。最终的tf.where所做的是，对于概率低于最小值(前k个logit中的最小logit值)的logit，选择的值和概率成为第二个表达式</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="1600" class="lb jq ht kx b fi lc ld l le lf">tf.ones_like(logits, dtype=logits.dtype) * -1e10</span></pre><p id="3b54" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">否则，保持不变。我不知道tf.where可以这样使用，所以它对我来说很有趣！</p><h1 id="523b" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">top_p_logits</h1><p id="f2fe" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">对于top _ p _ logits，它被设置为</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="f3cf" class="lb jq ht kx b fi lc ld l le lf">def top_p_logits(logits, p):<br/>    """Nucleus sampling"""<br/>    batch, _ = logits.shape.as_list()<br/>    sorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)<br/>    cumulative_probs = tf.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)<br/>    indices = tf.stack([<br/>        tf.range(0, batch),<br/>        # number of indices to include<br/>        tf.maximum(tf.reduce_sum(tf.cast(cumulative_probs &lt;= p, tf.int32), axis=-1) - 1, 0),<br/>    ], axis=-1)<br/>    min_values = tf.gather_nd(sorted_logits, indices)<br/>    return tf.where(<br/>        logits &lt; min_values,<br/>        tf.ones_like(logits) * -1e10,<br/>        logits,<br/>    )</span></pre><p id="a2d3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">首先，它按降序对数组进行排序，这意味着最大值排在最前面。然后，它开始获取排序后的logits的softmax，因此总和将为1。然后，它使用tf.cumsum返回到该点的累积和。</p><p id="ed97" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以，基本上，如果我的概率分布是[0.5，0.3，0.2]我会在累计后得到[0.5，0.8，1]。</p><p id="568d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">然后，如果累积和小于p，则设置为1，否则为0。因此，如果有不止一种情况为真，则它们的值大于0。否则，选择0。</p><p id="7352" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">当它与tf.range堆叠时，得到的形状是[batch_size，2]，其中第一个索引是数组的索引，第二个是总和。</p><p id="0ea3" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">现在，根据我对代码做的一个小测试，tf.gather_nd允许我发现它只接受reduce_sum-1的索引值。现在我们知道了为什么在减少的总和旁边有一个-1，但是为什么把那个指数作为最小值呢？答案是因为这就是cumulative_probs &lt;= p becomes false as the values in the indices are always increasing. Thus, at the golden point where the sorted logits up to that point are not enough and past it, it goes too far, we take that logit as the minimum value.</p><p id="6b9a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">The processes following that is quite similar to top_k where we the OpenAI people just put</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="b69c" class="lb jq ht kx b fi lc ld l le lf">return tf.where(<br/>        logits &lt; min_values,<br/>        tf.ones_like(logits) * -1e10,<br/>        logits,<br/>    )</span></pre><p id="ab1a" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Now that I think about it, I wonder why there’s an extra comma? If someone knows please tell me!</p><p id="8b65" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Overall, what it does is it removes logits with a low chance of being picked but it did it so that the number of options is not fixed like the top_k_logits and if you set p to a number like 0.8, I think it helps weed out bad options that have high priority like 0.15. I searched the term nucleus sampling and found <a class="ae jo" href="https://arxiv.org/pdf/1904.09751.pdf" rel="noopener ugc nofollow" target="_blank">这篇</a>关于它的好论文。</p><p id="a503" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">总之，要点是当使用传统技术时，文本退化会发生很多，例如“我不知道”的重复短语连续出现，并且一旦被选取，“一个无意义的标记可以开始向下的螺旋”，此外，根据该论文，从分布的尾部采样的边缘情况很可能发生。因此，通过设置这样的概率，文本将有希望具有更好的质量。</p><p id="3770" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，这要经过一个多项式分布，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="5af8" class="lb jq ht kx b fi lc ld l le lf">samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)<br/>            return [<br/>                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),<br/>                samples,<br/>                tf.concat([output, samples], axis=1)<br/>            ]</span></pre><p id="e092" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">并返回将被发送以生成下一个单词samples的pasts，该单词samples基本上是每批输入文本的一批新单词，最后是所生成的单词连接到的输出。</p><p id="09af" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">这样，我们完成了身体的功能。现在，让我们通过查看tf.while_loops来结束这整个系列，它直接导致gpt-2的输出文本！</p><h1 id="4ef0" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">最后一部分:tf.while_loop</h1><p id="4b04" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">最终代码如下。</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="0389" class="lb jq ht kx b fi lc ld l le lf">def cond(*args):<br/>            return True</span><span id="e97e" class="lb jq ht kx b fi lg ld l le lf">_, _, tokens = tf.while_loop(<br/>            cond=cond, body=body,<br/>            maximum_iterations=length - 1,<br/>            loop_vars=[<br/>                past,<br/>                prev,<br/>                output<br/>            ],<br/>            shape_invariants=[<br/>                tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),<br/>                tf.TensorShape([batch_size, None]),<br/>                tf.TensorShape([batch_size, None]),<br/>            ],<br/>            back_prop=False,<br/>        )</span><span id="0e9f" class="lb jq ht kx b fi lg ld l le lf">return tokens</span></pre><p id="ee22" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">tf.while_loop以函数的形式接受继续循环的条件，因此，</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="91cb" class="lb jq ht kx b fi lc ld l le lf">def cond(*args):<br/>            return True</span></pre><p id="af01" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">结束了。这里做了一个很好的处理，通过放入*args，它使得函数可以接受任意多的参数！我以后一定会用的！body采用适当命名的函数体。这构成了while循环的主体。作为初始输入的循环变量设置为</p><pre class="ks kt ku kv fd kw kx ky kz aw la bi"><span id="427e" class="lb jq ht kx b fi lc ld l le lf">past, prev, output = body(None, context, context)</span></pre><p id="112c" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所以循环从没有过去的地方开始，当前文本是上下文，当前输出是上下文，从那里继续下去。</p><p id="00c1" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">maximum_iterations设置为length-1，其中length是文本中标记的最大数量。shape_invariants适用于loop_vars中的每个变量。</p><p id="9ebc" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">back_prop = False我认为是为了防止渐变爆炸。这通常发生在rnn和LSTMs中，如果在梯度下降步骤中不小心，更新会变得过大，从而导致更新过大或干脆发生堆栈溢出。我个人对这两者都有体会。如果你很好奇，在这里阅读更多<a class="ae jo" href="https://machinelearningmastery.com/exploding-gradients-in-neural-networks/" rel="noopener ugc nofollow" target="_blank"/>！</p><p id="8e8d" class="pw-post-body-paragraph iq ir ht is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">最后，从while循环的输出中，输出张量被放入标记中，这就结束了！</p><h1 id="098b" class="jp jq ht bd jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km bi translated">结论</h1><p id="72c4" class="pw-post-body-paragraph iq ir ht is b it kn iv iw ix ko iz ja jb kp jd je jf kq jh ji jj kr jl jm jn hb bi translated">这是一次疯狂的旅行，但很有趣。如果我喜欢，我可能会参加训练，但是现在，就是这样！我希望你们也能愉快地跟随！如果有什么不清楚的地方或者错误的地方，请在下面留言评论！</p></div></div>    
</body>
</html>