<html>
<head>
<title>Building a simple Stack overflow search engine to predict semantically similar posts related to given query post using machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用机器学习构建简单的栈溢出搜索引擎来预测与给定查询帖子相关的语义相似的帖子</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/building-a-simple-stack-overflow-search-engine-to-predict-posts-related-to-given-query-post-56b3e508520c?source=collection_archive---------5-----------------------#2019-10-19">https://medium.com/analytics-vidhya/building-a-simple-stack-overflow-search-engine-to-predict-posts-related-to-given-query-post-56b3e508520c?source=collection_archive---------5-----------------------#2019-10-19</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/10169d343fa761dd28b7185f59eb57cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yg2DHASe7om_TsIz0E5UuA.png"/></div></div></figure><p id="fe51" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Stack Overflow——对于许多日常使用它的开发人员来说，这是一个显而易见的救命稻草。这是一个面向专业和爱好者程序员的问答网站。它的特点是在计算机编程的广泛主题上的问题和答案，而且现在它不仅仅局限于计算机编程，而是在广泛的范围内回答问题。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es jp"><img src="../Images/1a42e03c9dc222ce43f0ed7ee86bfae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kSn8NGOpRpfiT5O0VlrEJA.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">典型的stackoverflow问答页面</figcaption></figure><p id="25a5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated"><em class="jo">这是在深入项目之前对什么是stackoverflow的一个快速介绍</em></p><h1 id="d00e" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated"><strong class="ak"> <em class="kw">关于项目</em> </strong></h1><p id="f12a" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">Stack overflow网站在他们所有的问答页面中也有这个特别的部分。</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lc"><img src="../Images/106422a0dabdd377a1a8eb2eb3ccb5c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Sr9gg2gkpGJKB1lRGWSuQ.png"/></div></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">与当前页面中的帖子非常相关的帖子</figcaption></figure><p id="e23b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">它列出了与当前问题相关的所有问题，如果当前页面不能满足用户的需求，这可以帮助用户浏览当前页面以找到合适的问题页面。该项目旨在预测给定查询帖子的相关帖子，如问题的图像:从熊猫加载数据txt，它预测类似的帖子，如上图所示。</p><p id="d4d4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">由于除了预测帖子相对于查询帖子的语义相关性之外，我们还将它构建为搜索引擎，因此还需要满足其他约束条件</p><ol class=""><li id="de8e" class="ld le hi is b it iu ix iy jb lf jf lg jj lh jn li lj lk ll bi translated">低延迟-预测时间应该更短</li><li id="eeb3" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn li lj lk ll bi translated">可扩展性—即使在我们要搜索的数据量急剧增加时也必须有效</li></ol><p id="4aae" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">整个问题是建立一个基于StackOverflow问题的搜索引擎，搜索结果应该包括语义，具有可扩展的架构，在很短的时间内返回结果。</p><h1 id="c227" class="jy jz hi bd ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv bi translated">方法</h1><h2 id="527b" class="lr jz hi bd ka ls lt lu ke lv lw lx ki jb ly lz km jf ma mb kq jj mc md ku me bi translated"><strong class="ak"> 1。数据收集</strong></h2><p id="15ad" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">数据链接:<a class="ae mf" href="https://meta.stackexchange.com/questions/138356/how-do-i-download-stack-overflows-data" rel="noopener ugc nofollow" target="_blank">https://meta . stack exchange . com/questions/138356/how-do-I-download-stack-overflow-data</a></p><p id="636c" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">上面的链接包含各种主题中的所有堆栈溢出数据，其中只有这些帖子文件被下载。</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="fcd4" class="lr jz hi mh b fi ml mm l mn mo"><a class="ae mf" href="https://archive.org/download/stackexchange/cs.meta.stackexchange.com.7z" rel="noopener ugc nofollow" target="_blank">cs.meta.stackexchange.com.7z</a><br/><a class="ae mf" href="https://archive.org/download/stackexchange/cs.stackexchange.com.7z" rel="noopener ugc nofollow" target="_blank">cs.stackexchange.com.7z</a><br/><a class="ae mf" href="https://archive.org/download/stackexchange/datascience.meta.stackexchange.com.7z" rel="noopener ugc nofollow" target="_blank">datascience.meta.stackexchange.com.7z</a><br/><a class="ae mf" href="https://archive.org/download/stackexchange/datascience.stackexchange.com.7z" rel="noopener ugc nofollow" target="_blank">datascience.stackexchange.com.7z</a><br/><a class="ae mf" href="https://archive.org/download/stackexchange/ai.meta.stackexchange.com.7z" rel="noopener ugc nofollow" target="_blank">ai.meta.stackexchange.com.7z</a><br/><a class="ae mf" href="https://archive.org/download/stackexchange/ai.stackexchange.com.7z" rel="noopener ugc nofollow" target="_blank">ai.stackexchange.com.7z</a><br/><a class="ae mf" href="https://archive.org/download/stackexchange/computergraphics.meta.stackexchange.com.7z" rel="noopener ugc nofollow" target="_blank">computergraphics.meta.stackexchange.com.7z</a><br/><a class="ae mf" href="https://archive.org/download/stackexchange/computergraphics.stackexchange.com.7z" rel="noopener ugc nofollow" target="_blank">computergraphics.stackexchange.com.7z</a></span></pre><p id="3515" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">下载的zip文件有关于四个主要主题的问题和答案——计算机科学、数据科学、人工智能和计算机图形学。当解压缩时，上述文件给出了许多文件，其中仅获得每个未压缩目录中的posts.xml文件，因为仅该特定文件包含帖子文本。</p><p id="2d86" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">每个post.xml文件根据它们的主题命名，并存储在xml文件目录中</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="b058" class="lr jz hi mh b fi ml mm l mn mo">XML Files<br/>  -&gt; DataScienceMeta_Posts.xml<br/>  -&gt; ComputerGraphicsMeta_Posts.xml<br/>  -&gt; AI_Posts.xml<br/>  -&gt; CSMeta_Posts.xml<br/>  -&gt; AIMeta_Posts.xml<br/>  -&gt; CS_Posts.xml<br/>  -&gt; ComputerGraphics_Posts.xml<br/>  -&gt; DataScience_Posts.xml</span></pre><h2 id="3ccd" class="lr jz hi bd ka ls lt lu ke lv lw lx ki jb ly lz km jf ma mb kq jj mc md ku me bi translated">2.<strong class="ak">数据预处理</strong></h2><p id="fd91" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">获得的9个xml文件除了xml格式的文本数据之外，还包含更多的数据，如postId、评论、upvotes等。因此，有必要解析xml文件，只获取文件中每个xml元素的post文本属性</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="8d12" class="lr jz hi mh b fi ml mm l mn mo"># specifying the fields for csv file <br/>fields = ['Id', 'Text', 'Topic']</span><span id="af4e" class="lr jz hi mh b fi mp mm l mn mo">def parseXML(xmlfile, start_count): <br/>    #create element tree object <br/>    print("File", xmlfile)<br/>    tree = ET.parse(xmlfile) <br/>    topic = xmlfile.split("/")[1].split("_")[0]<br/>    # get root element <br/>    root = tree.getroot() <br/>    # create empty list for news items <br/>    newsitems = [] <br/>    count = start_count<br/>    # iterate news items <br/>    for each_row in root.iter("row"):<br/>        news = {}<br/>        news["Id"] = count<br/>        news["Text"] = each_row.attrib["Body"]<br/>        news["Topic"] = topic <br/>        count=count+1<br/>        newsitems.append(news)<br/>    # return news items list <br/>    print("len", len(newsitems))<br/>    return newsitems</span></pre><p id="a224" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">解析每个xml文件以获得文章的文本和对应的主题，该主题是xml文件的文件名，并与生成的id字段一起存储</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="fe22" class="lr jz hi mh b fi ml mm l mn mo">def savetoCSV(newsitems, filename): <br/>    # writing to csv file <br/>    with open(filename, 'w') as csvfile: <br/>        # creating a csv dict writer object <br/>        writer = csv.DictWriter(csvfile, fieldnames = fields) <br/>        # writing headers (field names) <br/>        writer.writeheader() <br/>        # writing data rows <br/>        writer.writerows(newsitems)</span></pre><p id="7bfe" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">获得的结果以与CSV文件相同的名称存储在名为CSV Files的目录中</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="38f1" class="lr jz hi mh b fi ml mm l mn mo"># specifying the fields for csv file <br/>fields = ['Id', 'Text', 'Topic']  <br/>start_count = 0<br/>for each_file in postfiles:<br/>    print(each_file)<br/>    # parse xml file <br/>    newsitems = parseXML("XML Files/"+each_file, start_count) <br/>    csv_filename = each_file.split('.')[0] + ".csv"<br/>    print("csv_filename", csv_filename)<br/>    # store news items in a csv file <br/>    savetoCSV(newsitems, "CSV Files/" + csv_filename)<br/>    start_count = len(newsitems) + start_count</span></pre><p id="1e90" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">CSV文件目录如下所示</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="8517" class="lr jz hi mh b fi ml mm l mn mo">CSV Files<br/>  -&gt; DataScienceMeta_Posts.csv<br/>  -&gt; ComputerGraphicsMeta_Posts.csv<br/>  -&gt; AI_Posts.csv<br/>  -&gt; CSMeta_Posts.csv<br/>  -&gt; AIMeta_Posts.csv<br/>  -&gt; CS_Posts.csv<br/>  -&gt; ComputerGraphics_Posts.csv<br/>  -&gt; DataScience_Posts.csv</span></pre><p id="b1eb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">CSV文件中的所有文件都被解析为数据帧，然后合并在一起并保存在pickle文件中，这是我们将用于进一步处理的最终数据文件。我们最终的pickle文件的结构是</p><figure class="jq jr js jt fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/90471d0faf792f20d7e64355de7a4942.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lBlpFH5B4vQ3Fchsy-Yung.png"/></div></div></figure><h2 id="364f" class="lr jz hi bd ka ls lt lu ke lv lw lx ki jb ly lz km jf ma mb kq jj mc md ku me bi translated">3.数据清理</h2><p id="4f72" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">由于获得的文本来自网站，它通常倾向于具有大量的html实体，因此必须经过大量的数据预处理步骤</p><p id="c8b0" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">所做的各种数据预处理有:</p><ol class=""><li id="6f47" class="ld le hi is b it iu ix iy jb lf jf lg jj lh jn li lj lk ll bi translated">移除HTML标签</li><li id="b6c4" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn li lj lk ll bi translated">移除URL</li><li id="ca56" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn li lj lk ll bi translated">删除标点符号(#、&amp;、*)和停用词等</li></ol><p id="c9c7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">另一个关键的和不同的数据预处理步骤是:</p><p id="48a2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4)删除编程代码</p><p id="9981" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">for(I = 0；一<n print=""/></p><p id="204f" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Since this is a majorly computer science programming question and answer portal most of the questions and answer posts tend to have programming code which has a lot of punctuation and unique structure which would lose meaning once subjected to data preprocessing.</p><p id="2200" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">Hence programming code should be removed and then feature engineered separately to obtain a good prediction.</p><h2 id="2f26" class="lr jz hi bd ka ls lt lu ke lv lw lx ki jb ly lz km jf ma mb kq jj mc md ku me bi translated">3. <strong class="ak">特色工程</strong></h2><p id="9f03" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">特征工程是解决任何数据科学问题的最重要和最关键的步骤之一。一个好的特征工程可以在预测中对模型有很大帮助。在这里，我提出了三个新的特点。</p><ol class=""><li id="08d6" class="ld le hi is b it iu ix iy jb lf jf lg jj lh jn li lj lk ll bi translated">主题</li><li id="dee4" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn li lj lk ll bi translated">编程代码</li><li id="b927" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn li lj lk ll bi translated">引用了相同的链接吗</li></ol><h2 id="216d" class="lr jz hi bd ka ls lt lu ke lv lw lx ki jb ly lz km jf ma mb kq jj mc md ku me bi translated"><strong class="ak"> 3a)话题</strong></h2><p id="c143" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">每个帖子的主题被用作特征之一，因为具有相似主题的帖子倾向于比其他帖子更相似。主题特征是热编码的，因为它是分类特征。</p><h2 id="bc06" class="lr jz hi bd ka ls lt lu ke lv lw lx ki jb ly lz km jf ma mb kq jj mc md ku me bi translated">3b)编程代码</h2><p id="91d9" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">虽然每个帖子中的编程代码都被删除了，但它提供了非常重要的信息，绝不能丢失。因此，编程代码是单独进行特性工程设计的，这样我们可以从中提取更多的信息。这是通过从每个帖子中分别移除代码部分并对其进行矢量化，以及通过k均值聚类对代码向量进行聚类来实现的。通过这种方式，我们可以获得关于编程代码的美妙见解，如下图所示</p><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/854638f07fd0c09708cfc368626b89dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*103900ps6s1MwpsTQQeXYg.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">数据库代码簇</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/221c9638f2cd00fb6ea475fd8e0a8e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*nJVZp8tuQwRPaVADS5ZSvQ.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">张量流代码簇</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/a6ac24206b3983bdd87e6944c0ca86cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*FzNEQAP4YqaT_cP3zmyGgQ.png"/></div></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/e9bcc78f2102c1eb6be08d980cf31be9.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*1RQBt5Humay7yobPgm1_MA.png"/></div></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/c1a2ba339aa23cb1b82e87bf88ef0ede.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*3ETqhhk8MxalG3Wqa_wD8g.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">python数据帧代码集群</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/29018c0917201510f51ae23ddaf1db23.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*BfIIMZDONbpB_MDt1coSDw.png"/></div></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/ce518b9a4faf7cbe3d748e4a5fb44907.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*rAhGT_WuU03M0YLkjpYnOw.png"/></div></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/b209be50be8765493b50ab63759ea627.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*5DnMQjD2zcHGsWsF1SXPPQ.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">java代码集群</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/2199ed85a0b1bdc17c670e0fa58d5745.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*TpZqqtL-zHHT2opBIYbkEQ.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">Knn码簇</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/078e283e17549f1f8980382702e09e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*Xgi1g64Xu86_0-_3qhe9Ig.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">numpy函数代码集群</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/da0b8670dddace482214a9a9ede3eba5.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*38h8NMdR3Y3_JZmmna_JiA.png"/></div></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/05f81200fe30e5ae586e2f39b7bd24a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*fvG0zN-Ic_S67-Byw7WvIA.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">sklearn模型训练和测试代码集群</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/986dc86fccb851fb92e6ab65b2c4b65e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*FMTZy9AcMMgLuRLlgyt7jw.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">LSTM码簇</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/77602afd9562c64b705ffd0e20d67271.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*qp-TN86QUU2VCSqEAsbg6A.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">深度学习keras代码集群</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/4998d073bffc2a13357be9e0eb0dcb34.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*Thq-wgyesMbyyYzqaroZmw.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">浮点数据代码簇</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/10bb9fdcd018fcbc87002cee1ebd6759.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*hmYvZ3X8StXrak5JF8dqXQ.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">正则表达式代码簇</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/4e94f9a6d2df1671e88ca8f57a352d8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*54rC-M8X0iVPCuCkimR5OQ.png"/></div></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/fe141ad9272a45d2a1623842c982f897.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*F10VkwmIjjarZXbrLlSOtw.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">c++输入输出代码簇</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/1607f73f3820f99b41a9b66df68be938.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*oBf5t9hwxjmQ69BzM4O5_A.png"/></div></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/6d9f5e02ca2ff5bcd388958e1a4c8bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*EsjGXL9mfgUvWkeqkfXPJQ.png"/></div></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/6fe4181668862570a6f1d08a78c1220b.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*uJbXaCUqB8IHtxxImOWWJQ.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">python类代码集群</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/48dc11fd6106e2bc11f19a49480beb35.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*dwrL9HdeaItX6Ddb8qEYXQ.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">数据结构代码簇</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/d818e359a424def8aeeb05d9b322c44a.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*oJWJk60qptKSfsOOfEBLtg.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">双数据类型代码簇</figcaption></figure><figure class="jq jr js jt fd ij er es paragraph-image"><div class="er es mr"><img src="../Images/cfd704bf0f0b9b9df4b416abeeb5e58e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*FcKt8lkLNIrgYQOX2WOhqA.png"/></div><figcaption class="ju jv et er es jw jx bd b be z dx translated">python数据结构集群</figcaption></figure><p id="9d1a" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">正如我们所看到的，代码被聚集成相关的类型。总共获得24个簇，并且没有代码的帖子被放入单独的第25个簇中。因此，代码聚类现在是类似后预测的分类特征，这也是一种热编码</p><h2 id="d1a2" class="lr jz hi bd ka ls lt lu ke lv lw lx ki jb ly lz km jf ma mb kq jj mc md ku me bi translated">3c)Does _ site _ same _ URL</h2><p id="61ad" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">与编程代码类似，URL也是一个需要特殊特性化的特性。由于URL也有特殊的结构，常规的数据处理会使其失去意义。可以通过查找两个帖子是否引用了相同的url来单独设计URL的功能，如果是这样，则可能两个帖子都引用了类似的东西。</p><p id="0d9b" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">经过分析，我们可以发现，近31%的网址被引用是重复的。因此，引用将被重复概率下降，且可用于找到类似的帖子</p><h2 id="19b7" class="lr jz hi bd ka ls lt lu ke lv lw lx ki jb ly lz km jf ma mb kq jj mc md ku me bi translated">3d)文档嵌入</h2><p id="2145" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">对于预处理后的文本数据的矢量化，使用五种类型的嵌入</p><ol class=""><li id="2b99" class="ld le hi is b it iu ix iy jb lf jf lg jj lh jn li lj lk ll bi translated">手套向量嵌入</li><li id="db36" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn li lj lk ll bi translated">TF-IDF加权手套向量嵌入</li></ol><p id="130e" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">除此之外，一种叫做“另一个关键词提取器”的新技术被用来从文章文本中提取关键词，然后将其矢量化。</p><p id="9590" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">YAKE——另一个关键词提取器是从文本中提取关键词的好方法之一。已经证明，在不同大小、语言或领域的多个集合下，它优于许多非监督方法和监督方法。</p><p id="74e4" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要了解更多关于YAKE的信息，请进入这个页面:<a class="ae mf" href="https://github.com/LIAAD/yake" rel="noopener ugc nofollow" target="_blank">https://github.com/LIAAD/yake</a></p><p id="50ef" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">因此，它首先用于提取每个帖子文本中的关键词，然后对其应用嵌入</p><p id="9906" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3)基于YAKE的关键词提取+手套向量嵌入</p><p id="02b5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4)基于YAKE的关键词提取+ TF-IDF加权手套向量嵌入</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="5152" class="lr jz hi mh b fi ml mm l mn mo">def tokenization(keywords_text):<br/>    tokenizer = RegexpTokenizer(r'\w+')<br/>    return tokenizer.tokenize(keywords_text)</span><span id="feea" class="lr jz hi mh b fi mp mm l mn mo">#Converting each posts text into corresponding keyword sets<br/>simple_kwextractor = yake.KeywordExtractor()<br/>questions_keywords_text = []<br/>for i in range(preprocessed_text.shape[0]):<br/>    if i%10 == 0:<br/>        print(i,"vectors finished")<br/>    keyword_simple = simple_kwextractor.extract_keywords(preprocessed_text[i])<br/>    keyword_text = ""<br/>    for each_kw in keyword_simple:<br/>        keyword_text += each_kw[0] + " "<br/>    questions_keywords_text.append(set(tokenization(keyword_text)))<br/>questions_keywords_sets = np.array(questions_keywords_text)</span></pre><p id="ebb2" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">5) BERT嵌入</p><p id="af09" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">BERT模型代表由google发布的Transformer的双向编码器表示，是当前用于完成NLP相关任务的最先进的模型。BERT是一个LSTM模型，它基于注意力的概念工作，试图根据它遇到的一系列先前的标记来预测下一个标记。基于BERT的嵌入可能对这个项目有用，可以从帖子文本中提取更有意义的信息。</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="148e" class="lr jz hi mh b fi ml mm l mn mo">def avg_of_token_embedding(result):<br/>  document_embeddings = np.zeros(768)<br/>  count = 0<br/>  for i in range(len(result)):<br/>    for j in range(len(result[i][1])):<br/>       document_embeddings = document_embeddings + result[i][1][j]<br/>            count = count + 1<br/>  return (document_embeddings/count)</span><span id="091c" class="lr jz hi mh b fi mp mm l mn mo">from bert_embedding import BertEmbedding<br/>bert_document_embeddings = []<br/>for i in range(preprocesses_text.shape[0]):<br/>    sentences = bert_abstract.split(' ')<br/>    bert_embedding = BertEmbedding()<br/>    token_embedding = bert_embedding(preprocesses_text[i],'sum')<br/>    bert_embedding_vector = avg_of_token_embedding(token_embedding)<br/>    bert_document_embeddings.append(bert_embedding_vector)<br/>bert_document_embeddings = np.array(bert_document_embeddings)</span></pre><h2 id="9f30" class="lr jz hi bd ka ls lt lu ke lv lw lx ki jb ly lz km jf ma mb kq jj mc md ku me bi translated">4.构建一个简单的搜索引擎</h2><p id="3cec" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">这是解决方案中最关键的部分，因为搜索引擎应该是可扩展的，并且具有更少的延迟。两个帖子文本之间的相似度按以下方式计算:</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="c90d" class="lr jz hi mh b fi ml mm l mn mo"><strong class="mh hj">Post Similarity(i,j) = <br/>cosine_similarity(post_vector(i),post_vector(j)) + similarity(topic_vector(i), topic_vectorr(j)) + similarity(code_cluster(i), code_cluster(j)) + <br/>number of common urls</strong></span></pre><p id="0b94" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">基于上述公式，计算两个帖子之间的相似度。余弦相似度用于后文本，因为欧几里德距离对于高维容易出现维数灾难问题。对于主题向量和代码聚类向量，相似性被计算为欧几里德距离的倒数</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="65fd" class="lr jz hi mh b fi ml mm l mn mo">               <strong class="mh hj"><em class="jo">similarity = 1/ euclidean distance</em></strong></span></pre><p id="2d90" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">为了构建一个搜索引擎，实现了一个高效的<strong class="is hj"> map-reduce like </strong>解决方案，其中产生8个进程独立运行。</p><p id="6f70" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">对于给定的查询帖子，每个进程处理全部帖子数据的1/8，返回一个长度为10的列表，该列表列出了它所计算的全部帖子中相似度最高的前10个帖子。然后，将所有进程返回的所有10个长度列表组合在一起并排序，以获得总的前10个相似性。一个简单的map reduce like技术能够将延迟控制在20秒以内。</p><p id="21e9" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">搜索引擎的代码:</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="54ab" class="lr jz hi mh b fi ml mm l mn mo">import multiprocessing<br/>alpha = 0.001</span><span id="a3e4" class="lr jz hi mh b fi mp mm l mn mo"><strong class="mh hj">//returns topic wise similarity of two posts</strong><br/>def get_topic_similarity(ind1, ind2):<br/>    dist = euclidean_distances(topics[ind1], topics[ind2])[0][0]<br/>    if dist == 0:<br/>        return 1/(dist+alpha)<br/>    else:<br/>        return dist</span><span id="b815" class="lr jz hi mh b fi mp mm l mn mo"><strong class="mh hj">//returns code wise similarity of two posts</strong><br/>def get_code_similarity(ind1, ind2):<br/>    dist = euclidean_distances(code_cluster_list[ind1].reshape(-1,1), <br/>                   code_cluster_list[ind2].reshape(-1, 1))[0][0]<br/>    if dist == 0:<br/>        return 1/(dist+alpha)<br/>    return dist<br/>      </span><span id="2078" class="lr jz hi mh b fi mp mm l mn mo">def insert_into_top_k_similar_docs(top_k_similar_docs,<br/>vec_similarity, doc_id):<br/>    doc_dict = dict()<br/>    doc_dict["doc_id"] = doc_id<br/>    doc_dict["similarity"] = vec_similarity<br/>    should_insert = False<br/>    for i in range(11):<br/>        if i == 10:<br/>            return<br/>        if vec_similarity &gt; top_k_similar_docs[i]['similarity']:<br/>            should_insert = True<br/>            break<br/>   <br/>    if should_insert:<br/>        docs_place = i<br/>        temp_list = []<br/>        for i in range(docs_place, k-1):<br/>            #print("it 2", i)<br/>            temp_list.append(top_k_similar_docs[i])<br/>        ind = 0<br/>        for i in range(docs_place+1 , k):<br/>            top_k_similar_docs[i] = temp_list[ind]<br/>            ind = ind + 1<br/>        top_k_similar_docs[docs_place] = doc_dict</span><span id="d691" class="lr jz hi mh b fi mp mm l mn mo"><strong class="mh hj">// Finds the top 10 similar posts for a given query post from the <br/>// posts in the start and end range   </strong>    <br/>def find_vector_similarity(preprocessed_text_vector, test_query_point, start, end, topic_weight=1, code_weight=1,<br/>                          cites_weight=1):<br/>    top_k_similar_docs = []<br/>    for i in range(10):<br/>        doc_dict = dict()<br/>        doc_dict['doc_id'] = 0<br/>        doc_dict['similarity'] = -1<br/>        top_k_similar_docs.append(doc_dict)<br/>      <br/>    for i in range(start, end):<br/>        if i &gt;= preprocessed_text_vector.shape[0]:<br/>            return<br/>        vec_similarity = cosine_similarity(preprocessed_text_vector[i].reshape(1,-1), <br/>                                    preprocessed_text_vector[test_query_point].reshape(1,-1))[0][0]<br/>        topicwise_similarity = get_topic_similarity(i, test_query_point)* topic_weight<br/>        codewise_similarity = get_code_similarity(i, test_query_point) * code_weight<br/>        cites_similarity = get_cites_similarity(i, test_query_point)<br/>        if cites_similarity:<br/>            cites_similarity = cites_similarity * cites_weight<br/>            vec_similarity = vec_similarity + topicwise_similarity + codewise_similarity + cites_similarity<br/>        else:<br/>            vec_similarity = vec_similarity + topicwise_similarity + codewise_similarity<br/>        vec_sim_list.append(vec_similarity)<br/>        insert_into_top_k_similar_docs(top_k_similar_docs, vec_similarity, i)  <br/>    return top_k_similar_docs</span><span id="5d38" class="lr jz hi mh b fi mp mm l mn mo">def simple_search_engine(preprocessed_text_vector, test_sample, topic_weight=1, code_weight=1, cites_weight=1):<br/>    count = 0<br/>    posts_text = pd.read_pickle('Preprocessed_questions_text.pkl')['Text'].values<br/>    for each_test_point in test_sample:<br/>        list_of_top_k_docs = [] <br/>        start_time = datetime.now()<br/>        #print(each_test_point)<br/>        #print("top_k_similar_docs", top_k_similar_docs)<br/>        vec_sim_list = []<br/>        test_query_point = preprocessed_text_vector[each_test_point]<br/>        print("Query posts",each_test_point, ":" , posts_text[each_test_point])<br/>        print("#"*100)<br/>        start = 0<br/>        step = preprocessed_text_vector.shape[0]/8<br/>        input_list = []<br/>        while start &lt; preprocessed_text_vector.shape[0]:      <br/>            input_list.append([preprocessed_text_vector, each_test_point, start, start+step, <br/>                               topic_weight, code_weight, cites_weight ])<br/>            start = start+step<br/>        pool = multiprocessing.Pool(processes=4)<br/>        tasks=[]<br/>        for each in input_list:<br/>            tasks.append(pool.apply_async(find_vector_similarity,each))<br/>        pool.close()<br/>        pool.join()<br/>        for each in tasks:<br/>            each.wait()<br/>            result = each.get()<br/>            list_of_top_k_docs.extend(result)<br/>        quickSort(list_of_top_k_docs, 0, len(list_of_top_k_docs))<br/>        list_of_top_k_docs = list_of_top_k_docs[0:10]<br/>        total_time = (datetime.now() - start_time).total_seconds()<br/>        print("Total time taken:", humanize_seconds(total_time))<br/>        print("The Most similar ",k,"posts are:")<br/>        for each in list_of_top_k_docs:<br/>            print(posts_text[each['doc_id']])<br/>            print("#"*100)<br/>        print("*"*200)</span></pre><h2 id="215d" class="lr jz hi bd ka ls lt lu ke lv lw lx ki jb ly lz km jf ma mb kq jj mc md ku me bi translated">5.五种嵌入技术的性能比较</h2><p id="c467" class="pw-post-body-paragraph iq ir hi is b it kx iv iw ix ky iz ja jb kz jd je jf la jh ji jj lb jl jm jn hb bi translated">我们没有明确的性能指标来评估我们所做预测的质量，我们必须通过手动读取模型预测的10个帖子来评估所做的预测。基于此，我们使用的五嵌入的性能是:</p><ol class=""><li id="bb2b" class="ld le hi is b it iu ix iy jb lf jf lg jj lh jn li lj lk ll bi translated">在使用手套向量和TF-IDF加权手套向量的类似帖子预测中，第一个帖子是关于称为哈密顿圈的NP完全问题。我们可以在使用glove和TF-IDF加权glove向量上观察到，它预测了与计算理论、时间复杂性和一些NP完全性等相关的帖子。</li></ol><p id="4b54" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过简单的手套向量进行预测的例子:</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="2aed" class="lr jz hi mh b fi ml mm l mn mo"><strong class="mh hj">Query Posts:<br/></strong>Hint: Hamiltonian Cycle is $NP$-Complete.</span><span id="5ce2" class="lr jz hi mh b fi mp mm l mn mo"><strong class="mh hj">Sample predicted posts:</strong></span><span id="d9d3" class="lr jz hi mh b fi mp mm l mn mo">1) &lt;p&gt;Hint: This is just the well-known NP-complete problem PARTITION.&lt;/p&gt;</span><span id="fb30" class="lr jz hi mh b fi mp mm l mn mo">2) &lt;p&gt;Is there a notion of a context-free complete language (in the analogous sense to a &lt;span class="math-container"&gt;$NP$&lt;/span&gt;-complete language)?&lt;/p&gt;</span><span id="4ee1" class="lr jz hi mh b fi mp mm l mn mo">3) &lt;p&gt;For example;&lt;/p&gt;<br/>&lt;p&gt;Is {0,1,{a,b,c},d,e} a valid alphabet to form a language over and is it usable in any context?&lt;/p&gt;</span><span id="dcfb" class="lr jz hi mh b fi mp mm l mn mo">4) &lt;p&gt;How Decision procedure and Decidable problem are different from each other?<br/>Both are having solutions in yes and no form. Is any else solution for both?  &lt;/p&gt;</span></pre><p id="4374" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">TF-IDF加权手套向量预测示例:</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="42b1" class="lr jz hi mh b fi ml mm l mn mo"><strong class="mh hj">Query Posts:<br/></strong>Hint: Hamiltonian Cycle is $NP$-Complete.</span><span id="bd28" class="lr jz hi mh b fi mp mm l mn mo"><strong class="mh hj">Sample predicted posts:</strong></span><span id="d441" class="lr jz hi mh b fi mp mm l mn mo">1) &lt;p&gt;Hint: This is just the well-known NP-complete problem PARTITION.&lt;/p&gt;</span><span id="689e" class="lr jz hi mh b fi mp mm l mn mo">2) &lt;p&gt;Is there a notion of a context-free complete language (in the analogous sense to a &lt;span class="math-container"&gt;$NP$&lt;/span&gt;-complete language)?&lt;/p&gt;</span><span id="46b2" class="lr jz hi mh b fi mp mm l mn mo">3) &lt;p&gt;Can an ambiguous context-free grammar be converted into Chomsky normal form? I think the answer is yes.&lt;/p&gt;</span><span id="0fa9" class="lr jz hi mh b fi mp mm l mn mo">4) &lt;p&gt;Time complexity, like any other nontrivial property of programs, is undecidable: there can be no algorithm to compute it, in Python or anything else.&lt;/p&gt;</span></pre><p id="34e5" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">2)基于关键字提取的手套向量的预测似乎比使用手套向量完成的预测更精确一点。</p><p id="e035" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过关键词提取的手套向量的示例预测:</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="90fa" class="lr jz hi mh b fi ml mm l mn mo"><strong class="mh hj">Query Posts:<br/></strong>Hint: Hamiltonian Cycle is $NP$-Complete.</span><span id="ec3b" class="lr jz hi mh b fi mp mm l mn mo"><strong class="mh hj">Sample predicted posts:</strong></span><span id="38b5" class="lr jz hi mh b fi mp mm l mn mo">1) &lt;p&gt;Is there a notion of a context-free complete language (in the analogous sense to a &lt;span class="math-container"&gt;$NP$&lt;/span&gt;-complete language)?&lt;/p&gt;</span><span id="b713" class="lr jz hi mh b fi mp mm l mn mo">2) &lt;p&gt;Does it exist an algorithm to find a median of table with a complexity nlog(n) Thank&lt;/p&gt;</span><span id="3723" class="lr jz hi mh b fi mp mm l mn mo">3) &lt;p&gt;2^n=O(3^n) : This is true or it is false if n&gt;=0 or if n&gt;=1<br/>since 2^n may or not be element of O(3^n)<br/>I need a hint to figure the problem&lt;/p&gt;</span><span id="b959" class="lr jz hi mh b fi mp mm l mn mo">4) &lt;p&gt;Yes and yes. Look at this graph:&lt;/p&gt;<br/><br/>&lt;ul&gt;<br/>&lt;li&gt;$V=\{1,2,3,4\}$&lt;/li&gt;<br/>&lt;li&gt;$E=\{\{1,2\},\{2,3\},\{3,4\},\{4,1\}\}$&lt;/li&gt;<br/>&lt;/ul&gt;<br/><br/>&lt;p&gt;There is neither articulation point nor bridge. But if you delete any node/edge...&lt;/p&gt;</span></pre><p id="33a8" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">通过关键词提取的TF-IDF手套向量的示例预测:</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="3937" class="lr jz hi mh b fi ml mm l mn mo"><strong class="mh hj">Query Posts:<br/></strong>Hint: Hamiltonian Cycle is $NP$-Complete.</span><span id="5868" class="lr jz hi mh b fi mp mm l mn mo"><strong class="mh hj">Sample predicted posts:</strong></span><span id="3cf7" class="lr jz hi mh b fi mp mm l mn mo">1) &lt;p&gt;Is there a notion of a context-free complete language (in the analogous sense to a &lt;span class="math-container"&gt;$NP$&lt;/span&gt;-complete language)?&lt;/p&gt;<br/><br/>2) &lt;p&gt;Does it exist an algorithm to find a median of table with a complexity nlog(n) <br/>Thank&lt;/p&gt;<br/><br/>3) &lt;p&gt;2^n=O(3^n) : This is true or it is false if n&gt;=0 or if n&gt;=1<br/>since 2^n may or not be element of O(3^n)<br/>I need a hint to figure the problem&lt;/p&gt;<br/><br/>4) &lt;p&gt;Yes and yes. Look at this graph:&lt;/p&gt;<br/><br/>&lt;ul&gt;<br/>&lt;li&gt;$V=\{1,2,3,4\}$&lt;/li&gt;<br/>&lt;li&gt;$E=\{\{1,2\},\{2,3\},\{3,4\},\{4,1\}\}$&lt;/li&gt;<br/>&lt;/ul&gt;<br/><br/>&lt;p&gt;There is neither articulation point nor bridge. But if you delete any node/edge...&lt;/p&gt;</span></pre><p id="dbe7" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">3)基于BERT嵌入的预测似乎比关键字提取的手套和关键字提取的tf-idf加权手套向量更精确，因为帖子是关于NP完全问题的，所以关于NP完全问题的帖子比任何其他帖子都被预测。同样，对于第二个svm帖子，它预测关于SVM的帖子比任何其他主题都多</p><p id="84df" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">伯特嵌入预测的一个例子:</p><pre class="jq jr js jt fd mg mh mi mj aw mk bi"><span id="4f1f" class="lr jz hi mh b fi ml mm l mn mo"><strong class="mh hj">Query Posts:<br/></strong>Hint: Hamiltonian Cycle is $NP$-Complete.</span><span id="0617" class="lr jz hi mh b fi mp mm l mn mo"><strong class="mh hj">Sample predicted posts:</strong></span><span id="1e9c" class="lr jz hi mh b fi mp mm l mn mo">1) &lt;p&gt;Yes, this is decidable, because you can do an exhaustive search of all possible paths. There is no need to look at any paths that repeat a vertex, since the "detour" could be skipped. But the length of any non-repetitive path is bounded by the size of the graph, which is finite, and so there are only finitely many such paths, which can be checked one by one.&lt;/p&gt;<br/><br/>&lt;p&gt;What is not decidable is the following: given an infinite graph $G$ and two vertices $a$ and $b$, decide whether there is a path from $a$ to $b$. This is not decidable if the graph is given as an oracle and is also not decidable if the graph is given via a program that computes it. &lt;/p&gt;</span><span id="df58" class="lr jz hi mh b fi mp mm l mn mo">2) &lt;blockquote&gt;<br/>  &lt;p&gt;What is the time complexity of finding the diameter of a graph<br/>  $G=(V,E)$?&lt;/p&gt;<br/>  <br/>  &lt;ul&gt;<br/>  &lt;li&gt;${O}(|V|^2)$&lt;/li&gt;<br/>  &lt;li&gt;${O}(|V|^2+|V| \cdot |E|)$&lt;/li&gt;<br/>  &lt;li&gt;${O}(|V|^2\cdot |E|)$&lt;/li&gt;<br/>  &lt;li&gt;${O}(|V|\cdot |E|^2)$&lt;/li&gt;<br/>  &lt;/ul&gt;<br/>&lt;/blockquote&gt;<br/><br/>&lt;p&gt;The diameter of a graph $G$ is the maximum of the set of shortest path distances between all pairs of vertices in a graph.&lt;/p&gt;<br/><br/>&lt;p&gt;I have no idea what to do about it, I need a complete analysis on how to solve a problem like this.&lt;/p&gt;</span><span id="0a4c" class="lr jz hi mh b fi mp mm l mn mo">3) &lt;p&gt;When searching graphs, there are two easy algorithms: &lt;strong&gt;breadth-first&lt;/strong&gt; and &lt;strong&gt;depth-first&lt;/strong&gt; (Usually done by adding all adjactent graph nodes to a queue (breadth-first) or stack (depth-first)).&lt;/p&gt;<br/><br/>&lt;p&gt;Now, are there any advantages of one over another?&lt;/p&gt;<br/><br/>&lt;p&gt;The ones I could think of:&lt;/p&gt;<br/><br/>&lt;ul&gt;<br/>&lt;li&gt;If you expect your data to be pretty far down inside the graph, &lt;em&gt;depth-first&lt;/em&gt; might find it earlier, as you are going down into the deeper parts of the graph very fast.&lt;/li&gt;<br/>&lt;li&gt;Conversely, if you expect your data to be pretty far up in the graph, &lt;em&gt;breadth-first&lt;/em&gt; might give the result earlier.&lt;/li&gt;<br/>&lt;/ul&gt;<br/><br/>&lt;p&gt;Is there anything I have missed or does it mostly come down to personal preference?&lt;/p&gt;</span><span id="c011" class="lr jz hi mh b fi mp mm l mn mo">4) &lt;p&gt;Breadth-first and depth-first certainly have the same worst-case behaviour (the desired node is the last one found). I suspect this is also true for averave-case if you don't have information about your graphs.&lt;/p&gt;<br/><br/>&lt;p&gt;One nice bonus of breadth-first search is that it finds shortest paths (in the sense of fewest edges) which may or may not be of interest.&lt;/p&gt;<br/><br/>&lt;p&gt;If your average node rank (number of neighbours) is high relative to the number of nodes (i.e. the graph is dense), breadth-first will have huge queues while depth-first will have small stacks. In sparse graphs, the situation is reversed. Therefore, if memory is a limiting factor the shape of the graph at hand may have to inform your choice of search strategy.&lt;/p&gt;</span></pre><p id="f4cb" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">4)因此，基于BERT嵌入的预测似乎比基于关键词提取的文本嵌入的预测更精确，这又比正常的手套向量嵌入执行得更好。我们还可以观察到TF-IDF加权手套向量在预测方面做了几乎相同的工作。因此，TF-IDF值不会对提高预测性能产生任何影响</p><h2 id="7b5f" class="lr jz hi bd ka ls lt lu ke lv lw lx ki jb ly lz km jf ma mb kq jj mc md ku me bi translated">5.参考</h2><ol class=""><li id="a194" class="ld le hi is b it kx ix ky jb ms jf mt jj mu jn li lj lk ll bi translated">雅克——又一个关键词提取器<a class="ae mf" href="https://github.com/LIAAD/yake" rel="noopener ugc nofollow" target="_blank">https://github.com/LIAAD/yake</a></li><li id="c3db" class="ld le hi is b it lm ix ln jb lo jf lp jj lq jn li lj lk ll bi translated">伯特嵌入—【https://github.com/imgarylai/bert-embedding T2】</li></ol><p id="9cff" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">要查看整个作品，请访问我的git hub链接:【https://github.com/gayathriabhi/StackOverflow-Search-Engine T4】</p><p id="7d31" class="pw-post-body-paragraph iq ir hi is b it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn hb bi translated">我的LinkedIn个人资料:<a class="ae mf" href="https://www.linkedin.com/in/gayathri-s-a90322126/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/gayathri-s-a90322126/</a></p></div></div>    
</body>
</html>