<html>
<head>
<title>Effect of outliers on Neural Network’s performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">离群点对神经网络性能的影响</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/effect-of-outliers-on-neural-networks-performance-ca1d9185dce9?source=collection_archive---------1-----------------------#2019-10-30">https://medium.com/analytics-vidhya/effect-of-outliers-on-neural-networks-performance-ca1d9185dce9?source=collection_archive---------1-----------------------#2019-10-30</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div class="er es if"><img src="../Images/bc675013bd1a4314766535552b23c554.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*nKMcSEK9_1SUe-UxsM0nog.png"/></div></figure><p id="088b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">因为当涉及到理解将输入变量映射到输出的数学函数时，神经网络被认为是块盒，这种数学函数的推导相当复杂。不幸的是，神经网络对异常值的处理也是如此。</p><p id="d58d" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在开始这篇文章之前，我给你的建议是对神经网络和机器学习的某些方面有一些基本的了解，以便对这篇博客有一个全面的了解。</p><p id="227c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果你不熟悉神经网络，请参考以下内容:</p><div class="jk jl ez fb jm jn"><a rel="noopener follow" target="_blank" href="/@johnolafenwa/introduction-to-neural-networks-ca7eab1d27d7"><div class="jo ab dw"><div class="jp ab jq cl cj jr"><h2 class="bd hj fi z dy js ea eb jt ed ef hh bi translated">神经网络导论</h2><div class="ju l"><h3 class="bd b fi z dy js ea eb jt ed ef dx translated">“机器不会思考”是大多数人持有的一种信念，那是因为我们的机器最擅长计算…</h3></div><div class="jv l"><p class="bd b fp z dy js ea eb jt ed ef dx translated">medium.com</p></div></div><div class="jw l"><div class="jx l jy jz ka jw kb ik jn"/></div></div></a></div><p id="76f3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我一直想知道神经网络如何处理异常值，特别是当我们使用<strong class="io hj">整流线性单元</strong> (ReLU)作为激活函数时。你可能会问为什么只有ReLU，而没有其他激活单元，如<strong class="io hj"> Sigmoid，Tanh </strong>和<strong class="io hj"> arctan </strong>等。主要原因是ReLU具有半线性特性，可能会受到异常值的影响，其次是它在2019年得到了最广泛的使用。</p><p id="df6f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为什么我只关注Relu？</p><p id="8a9c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">为了找到答案，我们应该看看逻辑回归中一个叫做挤压的概念。让我们首先理解逻辑回归是如何工作的，并通过使用<em class="kc"> Sigmoid </em>激活函数来处理异常值。我正在考虑逻辑回归，因为如果使用<em class="kc"> sigmoid </em>激活，单个神经元(感知器)与逻辑回归相同，否则在没有任何激活函数的情况下，它是线性回归。</p><p id="bed7" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">逻辑回归和单神经元(感知器)</strong></p><p id="4db4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">逻辑回归是一种分类技术，这意味着它试图区分两个类别，即是或否、零或一等，也可以区分多个类别，即零或一或二或三。</p><p id="6b72" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">示例:</p><p id="b79b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">1.两类:动物是猫或狗。</p><p id="cd9a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">2.多类:天气晴朗或下雨或刮风。</p><p id="f486" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">逻辑回归如何工作</strong></p><p id="b9a8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">概括地说，它试图画一条线/平面来区分两个或更多的类别。我将用一种简单可行的方式来解释逻辑回归<strong class="io hj"> </strong>，这足以理解这篇博客。如果你想知道更多关于逻辑回归的细节，点击这里。</p><p id="4573" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">目标</strong>:找到一条减少误差的直线或平面“w”。</p><p id="411c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">工作:</strong>考虑一个具有特征<em class="kc">高度</em> <strong class="io hj"> </strong>和<em class="kc">权重</em>的2类分类问题的例子。任务是确定给定的动物是猫还是狗。</p><p id="ffe3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">将猫标记为-1，将狗标记为+1。输出y=+1或-1，输入x(矢量)=重量和高度</p><p id="1a70" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj"> <em class="kc">假设</em> </strong>是给定的数据是线性可分的。</p><p id="3fe1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们从容易掌握的几何角度来理解逻辑回归。画出重量和高度。</p><figure class="kf kg kh ki fd ij er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ke"><img src="../Images/9913a7825a94d62e1d83a280b6806037.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kvW7pdLACG0-CJc3WTzTww.png"/></div></div></figure><p id="9045" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">查看这些图，任何人都可以说，重量和高度较高的点是狗(标记为橙色圆圈)，重量和高度较低的点是猫(标记为红色圆圈)。但是逻辑回归如何解决这个问题呢？简单地画一条线来划分这两个类。</p><figure class="kf kg kh ki fd ij er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ke"><img src="../Images/8e0255dca9a6362c2f174a95bedc4610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*78w_HAFObbU6gZ2XuKKiwQ.png"/></div></div></figure><p id="7a1a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">画完线后，我们可以很容易地说，线上的点是狗，线下的点是猫。</p><p id="89ad" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们进入简化程序。</p><p id="770a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">第一步:逻辑回归首先在空间中画一条随机线。</p><p id="0483" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">步骤2:计算从每个点到线的距离，并自我更新，以使结果输出距离总是最大的。</p><p id="6eb0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">点到平面的距离(d)是wx，其中w垂直于平面，x是输入向量。平面上方的点将具有正距离，即wx=+ve，平面下方的点将具有负距离，即wx=-ve。</p><figure class="kf kg kh ki fd ij er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kn"><img src="../Images/ee4597a80b38c412039066e3b0605558.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UA0CKewRyGq2hBUfegCPRA.png"/></div></div></figure><p id="9c0f" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">数学等式变成了:</p><figure class="kf kg kh ki fd ij er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ko"><img src="../Images/779e97266295a3842d40f644d9b575c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vdHUlROIZThZ3FYb30gAzA.jpeg"/></div></div></figure><p id="2889" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">其中w是线/面，y是输出，x是输入向量，求和(I到n)表示所有输入数据点，argmax(w)表示我们想要最大距离。</p><p id="9bf2" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如果所有的点都被正确分类，那么与存在一些错误分类的情况相比，得到的总和将更为正。</p><p id="7940" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">异常值情况下的逻辑回归</strong></p><p id="9bfe" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">上述等式的结果是100%准确，直到没有异常值或极值点，也没有错误分类。当异常值被引入数据并因此在线/平面上移动时，请看下图。</p><figure class="kf kg kh ki fd ij er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ke"><img src="../Images/47fd85b985bdacdf5e60eae2b7033523.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zYG1mgGPSrLNHF3mNpnoZg.png"/></div></div></figure><p id="1ee0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">当我们应用相同的等式时，得到的平面就是我们在上面的图像中得到的，这是由3个错误分类组成的，以最大化距离。为了解决这个问题，我们应该在博客中引入挤压的概念。挤压是一种减少极端/异常值影响的现象。由于挤压的影响，线/平面较少受到异常值的影响，因此减少了错误分类。</p><figure class="kf kg kh ki fd ij er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ke"><img src="../Images/689e228888114971b9437be404d6b8ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*veRiiBufbDeP1UeanUNMoQ.png"/></div></div></figure><p id="893e" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从这张图片(挤压后的左图)我们可以看到只有1点是错误分类的，这比我们上面得到的要好。挤压的效果来自称为<em class="kc">‘SIGMOID</em><strong class="io hj">’的底层数学函数。</strong></p><figure class="kf kg kh ki fd ij er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kp"><img src="../Images/7656998f11bde4e8e8832696cd769193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YuabQiGBA-W-5PWsojwN5A.png"/></div></div></figure><p id="eb2a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">对于x的所有值，Sigmoid f(x)位于0和1之间。当我们对距离应用Sigmoid时，我们试图在位于端点和正常点的异常值之间取得平衡，因此异常值对被调整的线/平面的影响较小，导致错误分类较少。这就是<strong class="io hj"> <em class="kc">压扁</em> </strong>的原因。</p><figure class="kf kg kh ki fd ij er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kq"><img src="../Images/b58d7b18836acd798a048c5d8da25492.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X4W_3LPTHZb2Fs7VqzA69w.jpeg"/></div></div></figure><p id="85d4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这是应用sigmoid后的方程，不容易出现异常值。</p><p id="52ba" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">让我们应用另一个数学函数RELU代替SIGMOID来处理上述CATS和DOGS问题中引入的异常值</p><p id="a9d1" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在此之前，让我们看看ReLU</p><figure class="kf kg kh ki fd ij er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es kr"><img src="../Images/6257ca84b6b6b1d179b7cd20fd417172.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ky0SQU0vqj94rTPIbDRqyQ.jpeg"/></div></div></figure><figure class="kf kg kh ki fd ij er es paragraph-image"><div class="er es ks"><img src="../Images/9ebf7e3144a4d1a18ea2b180cd837dad.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*LnnQfP5Ge36JjAJpfV7zEQ.png"/></div></figure><p id="9a04" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">正如我们所看到的，对于x≥0的任何值，f(x)线性依赖于x，否则为零。</p><p id="2ee9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">然而，当我们将Relu应用于上述问题时，得到的线/平面类似于我们在没有挤压的情况下在逻辑回归中得到的线/平面，因为当wx≥0时它只是得到一个线性函数(wx ),如果wx &lt;0.That means Relu cannot squash the impact of outliers or to be more precise extreme points.So we can say that ReLU is more prone to outliers than Sigmoid by the analysis we did so far.</p><blockquote class="kt ku kv"><p id="1065" class="im in kc io b ip iq ir is it iu iv iw kw iy iz ja kx jc jd je ky jg jh ji jj hb bi translated"><strong class="io hj"> <em class="hi">为零，现在让我们回到神经网络</em> </strong></p></blockquote><p id="53e3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">简单地让我们通过采用回归数据集，用Relu和sigmoid对神经网络进行实验。</p><blockquote class="kt ku kv"><p id="c5cc" class="im in kc io b ip iq ir is it iu iv iw kw iy iz ja kx jc jd je ky jg jh ji jj hb bi translated">我正在使用<strong class="io hj">加州住房数据集</strong>(详情<a class="ae kd" href="https://scikit-learn.org/stable/datasets/index.html#california-housing-dataset" rel="noopener ugc nofollow" target="_blank">点击这里</a>)</p></blockquote><p id="d37b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">使用这个数据集的原因是，它有一些极值点或异常值，以便我们可以更好地执行我们的分析。这种分析分两个阶段进行</p><p id="50cb" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kc">第一阶段</em>:对数据应用各种神经网络架构，并测试产生的误差。</p><p id="8ec4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><em class="kc">第二阶段</em>:从数据中去除异常值后，应用神经网络并测试结果误差。</p><blockquote class="kt ku kv"><p id="13fe" class="im in kc io b ip iq ir is it iu iv iw kw iy iz ja kx jc jd je ky jg jh ji jj hb bi translated">获取数据集</p></blockquote><pre class="kf kg kh ki fd kz la lb lc aw ld bi"><span id="62d5" class="le lf hi la b fi lg lh l li lj">from sklearn.datasets import fetch_california_housing</span><span id="7181" class="le lf hi la b fi lk lh l li lj">d = fetch_california_housing()</span></pre><blockquote class="kt ku kv"><p id="eaca" class="im in kc io b ip iq ir is it iu iv iw kw iy iz ja kx jc jd je ky jg jh ji jj hb bi translated">在训练和测试中拆分数据集</p></blockquote><pre class="kf kg kh ki fd kz la lb lc aw ld bi"><span id="c030" class="le lf hi la b fi lg lh l li lj">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(da, d.target,test_size = 0.30,)</span></pre><blockquote class="kt ku kv"><p id="ee41" class="im in kc io b ip iq ir is it iu iv iw kw iy iz ja kx jc jd je ky jg jh ji jj hb bi translated">标准化数据</p></blockquote><pre class="kf kg kh ki fd kz la lb lc aw ld bi"><span id="ec88" class="le lf hi la b fi lg lh l li lj">from sklearn.preprocessing import StandardScaler</span><span id="73e6" class="le lf hi la b fi lk lh l li lj">sc=preprocessing.StandardScaler()<br/>X_train = sc.fit_transform(X_train)<br/>X_test=sc.transform(X_test)</span></pre><h2 id="244a" class="le lf hi bd ll lm ln lo lp lq lr ls lt ix lu lv lw jb lx ly lz jf ma mb mc md bi translated"><strong class="ak">阶段1:应用神经网络而不移除异常值</strong></h2><blockquote class="kt ku kv"><p id="b626" class="im in kc io b ip iq ir is it iu iv iw kw iy iz ja kx jc jd je ky jg jh ji jj hb bi translated"><strong class="io hj">模型1 </strong>:架构:输入输出分层神经网络(1–1)</p></blockquote><pre class="kf kg kh ki fd kz la lb lc aw ld bi"><span id="e814" class="le lf hi la b fi lg lh l li lj">from keras.layers import Input, Dense<br/>from keras.models import Model<br/>model = Sequential()<br/>model.add(Dense(1 ,activation='relu', input_shape=(8,)))<br/>model.add(Dense(1))<br/>model.compile(optimizer='adam', loss='mean_squared_error')<br/>history=model.fit(X_train,y_train, batch_size=32, epochs=600, validation_data=(X_test, y_test))</span></pre><p id="3f33" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">以Relu为激活函数的模型1。在训练结束时，训练中的MSE(损失)是0.50，测试中是0.509。</strong></p><p id="cb96" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">以乙状结肠为激活函数的模型1。在训练结束时，训练中的MSE(损失)是0.389，测试中是0.402。</strong></p><p id="2c15" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">正如所料，与Sigmoid相比，Relu的损失更高，因为我们没有从数据集中移除异常值。由于Relu不具有挤压属性，这是巨大损失背后的原因，而sigmoid通过挤压这些离群值做得很好。</p><p id="6965" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">然而，我们只在每一层中使用单个神经元，并且在模型1中也没有隐藏层。如果我们通过添加隐藏层来改变架构，结果可能会有所不同。让我看看..</p><blockquote class="kt ku kv"><p id="4b18" class="im in kc io b ip iq ir is it iu iv iw kw iy iz ja kx jc jd je ky jg jh ji jj hb bi translated">模型2 :架构:输入- 2隐输出分层神经网络(64–32–16–1)</p></blockquote><pre class="kf kg kh ki fd kz la lb lc aw ld bi"><span id="7151" class="le lf hi la b fi lg lh l li lj">model = Sequential()<br/>model.add(Dense(64, input_shape=(8,)))<br/>model.add(Dense(32))<br/>model.add(Dense(16))<br/>model.add(Dense(1))<br/>model.compile(optimizer='adam', loss='mean_squared_error')<br/>history=model.fit(X_train,y_train, batch_size=32, epochs=600 ,validation_data=(X_test, y_test))<br/>summarize_diagnostics(history)</span></pre><p id="4e3b" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">以Relu为激活函数的模型2。在训练结束时，训练中的MSE(损失)是0.2334，测试中是0.279。</strong></p><p id="b737" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">以乙状结肠为激活函数的模型2。在训练结束时，训练中的MSE(损失)是0.257，测试中是0.309。</strong></p><p id="e63c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">现在的结果令人惊讶，因为relu比sigmoid表现得更好。你认为这背后的原因是什么？这个问题还有很多答案，让我用最简单的方式来说吧。当我们深入到神经网络时，损失不仅取决于离群值本身，而是有许多方面要考虑，但最重要的是主要在sigmoid激活中观察到的<em class="kc">消失梯度</em>问题。消失梯度是反向传播中的现象，其中神经网络仅通过保持其权重(wx)恒定来学习任何东西。</p><p id="f969" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">逻辑回归中的Sigmoid主要用于挤压，但在神经网络中，挤压功能不再保持不变，现在充当激活功能，帮助激活特定神经元。</p><figure class="kf kg kh ki fd ij er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es ke"><img src="../Images/ed3aa193a5419e8631068a9c76a97217.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nkatbUAa1NNErV00ZJsRGw.png"/></div></div></figure><p id="874a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">上面的图像是一个例子，说明了较少数量的隐藏层和神经元使映射函数(蓝线)受到离群值的影响，离群值主要发生在Relu激活函数中，较少发生在sigmoid上。说到这里，现在让我们进入第二阶段。</p><h2 id="721f" class="le lf hi bd ll lm ln lo lp lq lr ls lt ix lu lv lw jb lx ly lz jf ma mb mc md bi translated"><strong class="ak">第二阶段:去除异常值后应用神经网络</strong></h2><p id="5113" class="pw-post-body-paragraph im in hi io b ip me ir is it mf iv iw ix mg iz ja jb mh jd je jf mi jh ji jj hb bi translated">由于我们的要素较少，我们可以通过使用箱线图来检测数据集中的异常值，从而单独分析每个要素。下图显示了数据集中6个要素的箱线图</p><figure class="kf kg kh ki fd ij er es paragraph-image"><div role="button" tabindex="0" class="kj kk di kl bf km"><div class="er es mj"><img src="../Images/9b7ff5a1c7fcdfc45f1f7afdf5750f65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZBUhoz0cj2n0rUt1KE680w.jpeg"/></div></div></figure><p id="d781" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">我们可以看到带黄色圆圈的点是异常值。因此，让我们看看从数据集中删除这些点是否可以减少MSE(损失)。</p><p id="5db3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">计算每个特征的百分位数</p><pre class="kf kg kh ki fd kz la lb lc aw ld bi"><span id="c306" class="le lf hi la b fi lg lh l li lj">print('99TH AND 100TH PERCENTILES OF FEATURE AVEBEDRMS:',np.percentile(da.AveBedrms, [99,100]))<br/># OUTPUT:99TH AND 100TH PERCENTILES OF FEATURE AVEBEDRMS: [ 2.12754082 34.06666667]</span></pre><p id="5ba8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">类似地，对于剩余的特征，如果第99百分位和第100百分位有很大的差异，则选择阈值作为第99百分位，并删除剩余的点。</p><pre class="kf kg kh ki fd kz la lb lc aw ld bi"><span id="ddf6" class="le lf hi la b fi lg lh l li lj">c=np.where(da.AveBedrms&gt;=3)<br/>da.drop(da.index[c],inplace=True)<br/>dy.drop(dy.index[c],inplace=True)</span></pre><p id="a4ea" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在去掉约占整个数据集2 . 4%的极值点后，我们可以再次将数据分成训练、测试和标准化。</p><blockquote class="kt ku kv"><p id="67e0" class="im in kc io b ip iq ir is it iu iv iw kw iy iz ja kx jc jd je ky jg jh ji jj hb bi translated">对该数据应用上面讨论的相同架构。</p></blockquote><p id="6b2c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">以Relu为激活函数的模型1。在训练结束时，训练中的MSE(损失)是0.431，测试中是0.395。</strong></p><p id="a1c0" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">以乙状结肠为激活函数的模型1。在训练结束时，训练中的MSE(损失)是0.394，测试中是0.353。</strong></p><p id="8846" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">在去除异常值之后，具有relu的模型1与在阶段1中具有Relu的模型1相比表现得明显更好，甚至具有sigmoid的模型1也具有一些改进的性能，这是因为sigmoid倾向于压制异常值的影响，而不是完全消除它们的存在，所以这就是带来损失变化的原因。</p><p id="48c9" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">以Relu为激活功能的模式2。在训练结束时，训练中的MSE(损失)是0.242，测试中是0.246。</strong></p><p id="9e5a" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">具有乙状结肠激活功能的模型2。在训练结束时，训练中的MSE(损失)是0.258，测试中是0.257。</strong></p><p id="480c" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">具有relu的模型2在这里似乎表现得好一点，这主要是因为它比sigmoid收敛得更快，并且不容易出现消失梯度问题。</p><p id="a120" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated"><strong class="io hj">结论。</strong></p><p id="01e3" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">从整个实验来看，如果神经网络不太深，Relu会受到离群值的影响。当建筑深入时，Relu的行为和其他激活函数一样，甚至比其他函数更有规律，收敛更快。</p><p id="0ed8" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">完整代码:<a class="ae kd" href="https://github.com/santoshketa/handling-outliers-in-Neural-in-nn-" rel="noopener ugc nofollow" target="_blank">https://github . com/santoshketa/handling-outliers-in-Neural-in-nn-</a></p><p id="99e4" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">这里可以流我- <a class="ae kd" href="https://www.linkedin.com/in/santosh-santosh-bhargav-354974192" rel="noopener ugc nofollow" target="_blank"> Linkedin </a></p><p id="1caf" class="pw-post-body-paragraph im in hi io b ip iq ir is it iu iv iw ix iy iz ja jb jc jd je jf jg jh ji jj hb bi translated">如有任何意见或问题，<strong class="io hj">写在评论里吧。</strong></p></div></div>    
</body>
</html>