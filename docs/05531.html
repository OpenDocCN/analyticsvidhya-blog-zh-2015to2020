<html>
<head>
<title>Basics — Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基础——强化学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/basics-reinforcement-learning-66aae5da4c85?source=collection_archive---------14-----------------------#2020-04-24">https://medium.com/analytics-vidhya/basics-reinforcement-learning-66aae5da4c85?source=collection_archive---------14-----------------------#2020-04-24</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/4283ff24fc82f1e2cd382b9a51ca4ce4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fp8mqwqLfg4_ZcTJYfi81Q.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">图片提供:DataCamp</figcaption></figure><p id="2526" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">如你所知，机器学习是人工智能的一个子类。同样，各种学习算法也属于机器学习。在这个系列中，我将介绍中级水平的强化学习(RL)的基础知识。当我们谈到强化学习时，许多人会有这样的概念，即它的特征是什么，以及我们如何预测我们的目标。简而言之，RL是一种技术，它使代理能够在交互环境中使用来自其自身行为和经验的反馈，通过反复试验来学习。虽然监督学习和强化学习都使用输入和输出之间的映射，但与监督学习不同，监督学习中提供给代理的反馈是执行任务的正确动作集，强化学习使用奖励和惩罚作为积极和消极行为的信号。</p><h1 id="f9c7" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">强化学习的应用</h1><p id="8618" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">为了更清楚，这里有一些强化学习的实际应用</p><ul class=""><li id="4b45" class="kv kw hi iw b ix iy jb jc jf kx jj ky jn kz jr la lb lc ld bi translated">工业自动化机器人技术。</li><li id="d765" class="kv kw hi iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">商业战略规划</li><li id="24a0" class="kv kw hi iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">机器学习和数据处理</li><li id="f8e9" class="kv kw hi iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">无人驾驶汽车</li><li id="4b78" class="kv kw hi iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">飞机控制和机器人运动控制</li><li id="fbc8" class="kv kw hi iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">股票市场交易</li></ul><h1 id="0061" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">强化学习的体系结构</h1><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lj"><img src="../Images/4a1a733d715bf523d6c3a58d0cf0b48a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XTm93KATfkueoWZyzzkRcA.png"/></div></div></figure><p id="bd82" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从上面的图片中，我们可以了解我们的强化学习的基本架构是什么样子的。我们的RL模型由一个代理组成，我们需要训练它通过做出一系列决策(动作)来与环境交互。环境可以是任何东西，它不局限于游戏平台。甚至在股市交易中使用RL。在这种情况下，股票市场是我们的环境，代理人将在那里互动。环境会告诉我们的代理人的状态，代理人也会从环境中得到反馈，这就是奖励。</p><p id="0740" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">强化学习的目标:</strong>最大化我们未来的总回报。</p><p id="d2fd" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">奖励假设:</strong>我们的智能体通过从环境中获得的奖励(标量反馈)更多地了解它与环境的交互。因此，奖励是我们的中心思想，以使我们的代理在环境中更好。</p><blockquote class="lo lp lq"><p id="92d3" class="iu iv lr iw b ix iy iz ja jb jc jd je ls jg jh ji lt jk jl jm lu jo jp jq jr hb bi translated"><em class="hi">在现实世界中，我们应该定义自己对环境的回报。</em></p></blockquote><p id="8b66" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我希望你已经理解了强化学习的真正本质和它的主要目标。让我们更深入地研究一下强化学习领域中使用的术语</p><h1 id="201d" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">强化学习的组成部分</h1><p id="7c52" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">当我们开始谈论RL时，这些是最常用的术语</p><p id="848f" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">状态</strong>:代理当前的情况。</p><p id="d0cf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">动作</strong>:代理将当前状态改变为未来状态的决定或动作</p><p id="0913" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">环境</strong>:代理交互和执行动作的地方，环境可以是物理的，也可以是虚拟的</p><p id="b8e0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">代理人:我们在强化学习问题中训练的处理环境的代理人。</p><p id="a45b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">奖励</strong>:从当前状态转移到未来状态后给予代理的反馈。反馈信号是标量的。</p><p id="33b2" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">策略</strong>:作为策略制定的最优策略，代理从一个状态移动到另一个状态时采用。</p><p id="7b64" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">价值函数:</strong>代理人在当前状态采取行动转移到下一个未来状态会得到的回报。</p><h1 id="d050" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">马尔可夫决策过程</h1><p id="a83f" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">我们在强化学习中的所有问题都可以表述为马尔可夫决策过程。一个MDP由一组有限的环境状态S、每个状态下的一组可能的动作A(s)、一个实值奖励函数R(s)和一个转移模型P(s '，s | a)组成。然而，真实世界的环境更有可能缺乏任何环境动力学的先验知识。</p><p id="3e00" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，MDP和代理人一起产生了这样开始的序列或轨迹:</p><p id="2521" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">S0，A0，R1，S1，A1，R2，S2，A2，R3，…..</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lv"><img src="../Images/c396686ca7601e05fe6524fa90e4951b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l3dDhnhI6FDnAK8-DQblVA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">确定未来奖励的条件概率</figcaption></figure><p id="21a7" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">上面的函数给出了未来奖励的概率，条件是我们在状态<em class="lr">s’</em>中采取特定动作’<em class="lr">a’</em>，通过获得奖励移动到状态<em class="lr">s’</em></p><p id="0bee" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">插图来了解强化学习算法的工作原理。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lw"><img src="../Images/8090c2bd385636abedab87f4c7154784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NsXQBg3yc3nyLGnk9UedvQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">冰冻湖游戏</figcaption></figure><p id="3d72" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在上面的图像中，简笔画是我们的代理。我们需要训练代理人拿到飞盘，飞盘的颜色是粉红色的，但是要注意不要掉到洞里面。我们的环境是代理交互的4 x 4网格。</p><p id="d1b3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">代理可以采取的措施有:向左移动。向右移动，向前移动，向后移动。状态是对环境的观察在这个例子中，代理的位置是状态。所以，现在我们的环境中有16个状态。我们可以制定一个奖励。如果代理人掉进洞里，游戏结束，奖励是-1，到达终点是+1。</p><ul class=""><li id="75b9" class="kv kw hi iw b ix iy jb jc jf kx jj ky jn kz jr la lb lc ld bi translated">我们的代理从<strong class="iw hj">环境</strong>接收<strong class="iw hj">状态S0 </strong>(在我们的例子中，我们从冰冻的湖(环境)接收代理(状态)的第一个位置)</li><li id="c1fe" class="kv kw hi iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">基于该<strong class="iw hj">状态S0，</strong>代理采取<strong class="iw hj">动作A0 </strong>(我们的代理将向左移动)</li><li id="a0d7" class="kv kw hi iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">环境过渡到一个<strong class="iw hj">新</strong>状态<strong class="iw hj">S1</strong>(新帧)</li><li id="2541" class="kv kw hi iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated">环境给代理一些<strong class="iw hj">奖励R1 </strong>(不踩冰:+1)</li></ul><p id="2026" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">RL循环输出→ <strong class="iw hj">(状态、动作、奖励)</strong></p><p id="6980" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">主要目标是最大化我们的预期累积回报。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es lx"><img src="../Images/d08c751ad90adc43924143aa80dcb5a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RN35-IOv6_MOoz9tCqKl1A.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">每个时间点的未来奖励总和</figcaption></figure><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ly"><img src="../Images/1e4f22bc710e500de6f7cddcaf89c5a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EdB2ibt5eZxt3NYMgw06rA.png"/></div></div></figure><p id="13b3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于每个时间步骤，我们都在总结我们的奖励。在现实世界中，我们不能把所有的回报加起来，因为有些回报不会马上到来。为了更关心未来的奖励，我们引入了一个称为“gamma”的贴现因子，它将用于贴现我们的未来奖励。gamma的值应该在0到1的范围内。这被认为是重要的超参数之一。</p><ol class=""><li id="e10e" class="kv kw hi iw b ix iy jb jc jf kx jj ky jn kz jr lz lb lc ld bi translated"><strong class="iw hj">越大</strong><strong class="iw hj">γ</strong>值，代理人越关心<strong class="iw hj">的长期报酬。</strong></li><li id="a2c9" class="kv kw hi iw b ix le jb lf jf lg jj lh jn li jr lz lb lc ld bi translated">另一方面，<strong class="iw hj">的值</strong>越小，代理越关心<strong class="iw hj">最近的奖励</strong>。</li></ol><p id="6d1c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">因此，重要的是调整我们的gamma值以保持平衡，从而使我们的目标<strong class="iw hj">最大化未来贴现累积奖励</strong></p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ma"><img src="../Images/798db14b172b96a28b83a428fcd3717f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ujRvNZwL2ZGeJP_LnPpkSQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">带折扣系数的所有奖励的总和</figcaption></figure><h1 id="3afb" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">强化学习的方法</h1><p id="b306" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">以下是强化学习中广泛使用的方法</p><p id="0d2c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">基于价值(状态-动作对):</strong>在基于价值的方法中，我们的目标是最大化我们的价值函数<strong class="iw hj"> V(s)。</strong>分析达到特定状态或采取特定行动所需的良好价值(即价值学习)。</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mb"><img src="../Images/3d00ef004ae955a6254b402554ddf71a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EPhVBFEdcVhvEsUNjQ1cQw.png"/></div></div></figure><p id="514b" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从所有的值函数中，我们将取状态-动作对的最大值</p><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mc"><img src="../Images/e124b3fd845bdae70906d82245560e0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7jqsvGkPsMBEir2-yjwLDQ.png"/></div></div></figure><p id="f3c9" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj">基于策略:</strong>在基于策略的RL中，我们需要直接优化策略函数<strong class="iw hj"><em class="lr">【π(s)】</em></strong>，而不需要使用值函数。直接推导出一个最优策略，使回报最大化。形式上，策略是从状态到选择每个可能动作的概率的映射。如果代理人在t时刻遵循策略<strong class="iw hj"> <em class="lr"> π </em> </strong>，那么<strong class="iw hj"> <em class="lr"> π </em> (a|s) </strong>就是A = <em class="lr"> a </em>的概率如果S = <em class="lr"> s </em>。</p><p id="1e23" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我们有两种政策:</p><ol class=""><li id="f880" class="kv kw hi iw b ix iy jb jc jf kx jj ky jn kz jr lz lb lc ld bi translated"><strong class="iw hj">确定性:</strong>处于给定状态的策略将总是返回相同的动作。相同状态中的相同动作导致所有时间步长的相同下一个状态。</li><li id="924e" class="kv kw hi iw b ix le jb lf jf lg jj lh jn li jr lz lb lc ld bi translated"><strong class="iw hj">随机:</strong>输出动作的分布概率<strong class="iw hj">。</strong>主体在某个状态采取行动，由此产生的下一个环境状态必然始终相同，这些不确定性导致更难找到最优策略</li></ol><h1 id="cc74" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">蒙特卡洛vs TD学习:</strong></h1><p id="e66d" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated"><strong class="iw hj">强化学习中的蒙特卡罗</strong>方法，其中代理与环境交互，但在场景结束时从环境中获得<strong class="iw hj">反馈。与价值函数不同，价值函数在每个时间步获得反馈，以了解当前状态有多好。</strong></p><p id="6201" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在这种蒙特卡罗方法中，代理生成经验样本，最后计算平均回报，我们用它来理解状态或状态-动作值。这只能用在偶发性问题中。</p><p id="7f90" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> TD(时间差)学习</strong>另一方面，不会等到一集结束才更新<strong class="iw hj">最大预期未来报酬估计:它会更新在该体验发生的非终点状态St的值估计V。</strong></p><p id="758e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">这种方法称为TD(0)或<strong class="iw hj">一步TD(在任何单个步骤后更新值函数)。</strong></p><h1 id="658e" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">基于模型与无模型的强化学习</strong></h1><p id="e572" class="pw-post-body-paragraph iu iv hi iw b ix kq iz ja jb kr jd je jf ks jh ji jj kt jl jm jn ku jp jq jr hb bi translated">在<strong class="iw hj">基于模型的学习</strong>中，代理利用先前学习的模型来完成手头的任务，而在<strong class="iw hj">无模型学习</strong>中，代理仅仅依靠一些试错经验来进行动作选择。<strong class="iw hj"> Q-Learning被认为是无模型学习，因为它从每个经验中学习。</strong></p><h1 id="587e" class="js jt hi bd ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">强化学习的分类</strong></h1><figure class="lk ll lm ln fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es md"><img src="../Images/4ca8fceba93639e7fe8e2d78e1ea55a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WpfXpGRsfNyNaZ_5m7li8w.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">流程图</figcaption></figure><p id="49ff" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">从上面的图片中，我可以推断出在强化学习算法下有很多领域可以探索和研究。在这一系列教程中，作为一个初学者，我将使用无模型算法下的概念。</p><p id="8a1a" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">在无模型算法中，有两个部分</p><ol class=""><li id="2f8e" class="kv kw hi iw b ix iy jb jc jf kx jj ky jn kz jr lz lb lc ld bi translated">政策优化</li><li id="002c" class="kv kw hi iw b ix le jb lf jf lg jj lh jn li jr lz lb lc ld bi translated">q学习</li></ol><p id="4d15" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">我想先关注一下<strong class="iw hj"> Q学习</strong>，然后再深入<strong class="iw hj">深度Q学习。</strong></p><h2 id="8d46" class="me jt hi bd ju mf mg mh jy mi mj mk kc jf ml mm kg jj mn mo kk jn mp mq ko mr bi translated"><strong class="ak">其他文章</strong></h2><ul class=""><li id="49fc" class="kv kw hi iw b ix kq jb kr jf ms jj mt jn mu jr la lb lc ld bi translated"><a class="ae mv" rel="noopener" href="/@nancyjemi/level-up-understanding-q-learning-cf739867eb1d">水平提升-理解Q学习</a></li><li id="39ae" class="kv kw hi iw b ix le jb lf jf lg jj lh jn li jr la lb lc ld bi translated"><a class="ae mv" rel="noopener" href="/@nancyjemi/deep-q-learning-explained-65df980aac6f">深度Q学习讲解</a></li></ul></div><div class="ab cl mw mx gp my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="hb hc hd he hf"><p id="29ff" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">万岁yyyyy！！！你已经到了我第一篇博客的结尾。感谢你阅读这篇文章，我希望你已经对什么是强化学习有了一些基本的理解，并对每个术语和术语有了一些基本的细节。</p><p id="2a60" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated"><strong class="iw hj"> <em class="lr">用下面的问题</em> </strong>检查你的基本理解</p><ol class=""><li id="dd6b" class="kv kw hi iw b ix iy jb jc jf kx jj ky jn kz jr lz lb lc ld bi translated">代理学习他们的行动的中心思想是什么？</li></ol><p id="1c45" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">(环境，<strong class="iw hj">奖励</strong>)</p><p id="a34e" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">2.gamma值越高，代理人关心的越多？</p><p id="9d48" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">(<strong class="iw hj">长期奖励，</strong>短期奖励，)</p><p id="12d4" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">3.代理通过试错法学习环境的以下哪种方法？</p><p id="62cf" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">(基于模型的，<strong class="iw hj">无模型的</strong>，TD学习，蒙特卡罗)</p><p id="e383" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">4.对于情节性任务，使用什么方法？</p><p id="009c" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">(时间差学习，<strong class="iw hj">蒙特卡罗方法</strong>)</p></div><div class="ab cl mw mx gp my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="hb hc hd he hf"><p id="b5d3" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">感谢您阅读我的文章。如有任何建议和问题，请发电子邮件至nancyjemimah@gmail.com给我。</p><p id="5fb0" class="pw-post-body-paragraph iu iv hi iw b ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr hb bi translated">对于我另外两个系列的工作代码，我想让你访问我的<a class="ae mv" href="https://github.com/NancyJemimah/INFO7390_Advance_Data_Science" rel="noopener ugc nofollow" target="_blank"> GitHub </a></p></div></div>    
</body>
</html>