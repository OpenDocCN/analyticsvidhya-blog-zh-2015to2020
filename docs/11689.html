<html>
<head>
<title>Redefining Cancer Treatment with Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用机器学习重新定义癌症治疗</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/redefining-cancer-treatment-with-machine-learning-aea74fa9ba07?source=collection_archive---------14-----------------------#2020-12-14">https://medium.com/analytics-vidhya/redefining-cancer-treatment-with-machine-learning-aea74fa9ba07?source=collection_archive---------14-----------------------#2020-12-14</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><figure class="ev ex ig ih ii ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es if"><img src="../Images/2d074e9d1ec9e3e574535d42f7e8a6a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*veED3OTUicyn5oomHDP6kg.jpeg"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">照片由国家癌症研究所提供</figcaption></figure><h1 id="9e2f" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated"><strong class="ak">简介</strong></h1><p id="caf6" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">在过去的几十年里，癌症治疗不断取得进展。科学家们应用不同的技术在癌症引起症状之前发现它们的类型。近年来，医学领域取得了许多突破，医学研究人员也可以获得大量数据，因为可以获得更多数据。医学研究人员已经使用机器学习来识别复杂数据中的隐藏模式，以尝试预测癌症类型的有效未来结果。</p><p id="dfd6" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">鉴于个体化医学的重要性和 ML 技术应用的增长趋势，我们将尝试解决一个这样的问题，其中的挑战是区分导致肿瘤生长的突变(驱动者)和中性突变(过客)。</p><p id="d8f9" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">目前，这种对基因突变的解释是手工完成的。这是一项非常耗时的任务，临床病理学家必须根据基于文本的临床文献中的证据，对每一个基因突变进行人工审查和分类。我们需要开发一种机器学习算法，使用这个知识库作为基线，自动对基因变异进行分类。</p><h1 id="741a" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated"><strong class="ak">数据概述</strong></h1><p id="b51b" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">给定<strong class="ju hj">基因</strong>、<strong class="ju hj">变异</strong>和<strong class="ju hj">文本</strong>作为特征，我们需要预测<strong class="ju hj">类</strong>变量的类(<strong class="ju hj">目标变量</strong>)。这是一个<strong class="ju hj">多类分类</strong>问题，我们将用<strong class="ju hj">多类对数损失</strong>度量来衡量我们模型的性能。</p><h1 id="6c4b" class="iu iv hi bd iw ix iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr bi translated"><strong class="ak">未来路线图</strong></h1><p id="1001" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">我们将读取数据，执行文本预处理，将数据分为训练、测试和交叉验证，训练随机模型，训练不同的 ML 模型，计算日志损失以及错误分类点的百分比，然后比较并找出最佳模型。</p></div><div class="ab cl kv kw gp kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="hb hc hd he hf"><p id="8fe3" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><em class="lc">查理耶舒如卡特(开始编码吧！！)</em></p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="87e6" class="lm iv hi li b fi ln lo l lp lq">import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import re<br/>import warnings<br/>import numpy as np<br/>from nltk.corpus import stopwords<br/>from sklearn.preprocessing import normalize<br/>from sklearn.feature_extraction.text import CountVectorizer<br/>import seaborn as sns<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.metrics import confusion_matrix,accuracy_score,log_loss <br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>from sklearn.linear_model import SGDClassifier,LogisticRegression<br/>from scipy.sparse import hstack<br/>from sklearn.multiclass import OneVsRestClassifier<br/>from sklearn.svm import SVC<br/>from collections import Counter, defaultdict<br/>from sklearn.calibration import CalibratedClassifierCV<br/>from sklearn.naive_bayes import MultinomialNB, GaussianNB<br/>from sklearn.model_selection import train_test_split,GridSearchCV<br/>from sklearn.metrics import normalized_mutual_info_score<br/>from sklearn.ensemble import RandomForestClassifier<br/>warnings.filterwarnings("ignore")<br/>from mlxtend.classifier import StackingClassifier</span></pre><p id="6a32" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">所以我们首先导入各种库<a class="ae lr" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="ju hj">熊猫</strong> </a> <strong class="ju hj">，</strong> <a class="ae lr" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="ju hj"> numpy </strong> </a>用于数据操作，<strong class="ju hj"/><a class="ae lr" href="https://matplotlib.org/" rel="noopener ugc nofollow" target="_blank"><strong class="ju hj">matplotlib</strong></a>和<a class="ae lr" href="https://seaborn.pydata.org/" rel="noopener ugc nofollow" target="_blank"> <strong class="ju hj"> seaborn </strong> </a>用于绘图，<a class="ae lr" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> <strong class="ju hj"> sklearn </strong> </a>包用于建模。</p><p id="1c0a" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><strong class="ju hj">读取数据</strong></p><p id="b812" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">我们的数据存在于具有不同分隔符的两个不同文件中，因此将分别读取每个文件，然后使用<strong class="ju hj">“ID”</strong>列将两个文件合并。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ls"><img src="../Images/b5a4ded56acbf14bbd824332ed974b3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o0WxGqbhs3WW8jaHgbeT-w.png"/></div></div></figure></div><div class="ab cl kv kw gp kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="hb hc hd he hf"><p id="bc1b" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><strong class="ju hj">文本预处理和特征工程</strong></p><p id="6977" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">在读取数据后，我们将进行文本预处理，包括清理文本，如<em class="lc">停用词移除</em>，移除<em class="lc">特殊字符</em>，如果有的话，<em class="lc">规范化</em>文本并将所有单词转换为<em class="lc">小写</em>。在此过程中，我们发现有些行没有文本，因此我们将用<strong class="ju hj"> <em class="lc">基因</em> </strong> + <strong class="ju hj"> <em class="lc">变异</em> </strong>值替换<strong class="ju hj"> <em class="lc"> NaN </em> </strong>值。</p><h2 id="a22c" class="lm iv hi bd iw lt lu lv ja lw lx ly je kd lz ma ji kh mb mc jm kl md me jq mf bi translated"><strong class="ak">将数据分为训练、测试和交叉验证</strong></h2><p id="7c5d" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">现在，我们将数据分为训练、测试和交叉验证数据，以检查我们的目标值在所有三个数据中的分布是否相同。</p><p id="189e" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><strong class="ju hj">为什么分销需要相同？</strong>我们的目标值的分布应该相同，以便在训练期间，我们的模型应该遇到数据集中存在的所有类值。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div class="er es mg"><img src="../Images/8fc39e2dda35c31923ef922833b4fd73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*G4GqzxFHdn5ZKLYaa1ZE5w.png"/></div><figcaption class="iq ir et er es is it bd b be z dx translated">目标变量的分布</figcaption></figure></div><div class="ab cl kv kw gp kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="hb hc hd he hf"><h1 id="8c1d" class="iu iv hi bd iw ix mh iz ja jb mi jd je jf mj jh ji jj mk jl jm jn ml jp jq jr bi translated"><strong class="ak">训练</strong></h1><p id="adf3" class="pw-post-body-paragraph js jt hi ju b jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp hb bi translated">我们将首先训练一个随机模型，以便我们可以比较我们的其他模型及其性能和效率。</p><p id="0f80" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><em class="lc">如何在多类别设置中对随机模型执行对数丢失？</em>我们将为测试中的每个点随机生成与我们的类数量(在我们的问题中为 10)相等的数字，并交叉验证数据，然后将它们归一化，使其总和为 1。</p><pre class="ld le lf lg fd lh li lj lk aw ll bi"><span id="4c40" class="lm iv hi li b fi ln lo l lp lq">test_data_len = test_df.shape[0]<br/>cv_data_len = cv_df.shape[0]</span><span id="7d22" class="lm iv hi li b fi mm lo l lp lq"># we create a output array that has exactly same size as the CV data</span><span id="bcc4" class="lm iv hi li b fi mm lo l lp lq">cv_predicted_y = np.zeros((cv_data_len,9))<br/>#for every value in our CV data we create a array of all zeros with #size 9</span><span id="afd1" class="lm iv hi li b fi mm lo l lp lq">for i in range(cv_data_len):#iterating to each value in cv data(row)<br/>    rand_probs = np.random.rand(1,9) #generating randoms form 1 to 9<br/>    cv_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0]) #normalizing to sum to 1</span><span id="822f" class="lm iv hi li b fi mm lo l lp lq">print("Log loss on Cross Validation Data using Random Model",log_loss(y_cv,cv_predicted_y, eps=1e-15))</span><span id="92c9" class="lm iv hi li b fi mm lo l lp lq"># Test-Set error.<br/>#we create a output array that has exactly same as the test data</span><span id="31c5" class="lm iv hi li b fi mm lo l lp lq">test_predicted_y = np.zeros((test_data_len,9))<br/>for i in range(test_data_len):<br/>    rand_probs = np.random.rand(1,9)<br/>    test_predicted_y[i] = ((rand_probs/sum(sum(rand_probs)))[0])<br/>print("Log loss on Test Data using Random Model",log_loss(y_test,test_predicted_y, eps=1e-15))</span><span id="bc5f" class="lm iv hi li b fi mm lo l lp lq">predicted_y =np.argmax(test_predicted_y, axis=1)<br/>plot_confusion_matrix(y_test, predicted_y+1)</span></pre><p id="6a10" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">在上面，我们首先为每个类别标签创建一个大小为 9 的空数组，然后为每个类别标签随机生成概率，并绘制混淆矩阵和计算对数损失。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mn"><img src="../Images/796f8657329b89def3f27451cf9a5248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pud9y0LVOVxvi70u_eW-4g.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">对数损失为 2.4 的混淆矩阵</figcaption></figure><p id="224b" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">我们可以看到，我们的随机模型在交叉验证和测试数据中的<em class="lc">对数损失为</em> <strong class="ju hj"> <em class="lc"> 2.4 </em> </strong>，因此我们需要我们的模型表现得比这更好，让我们检查一下这个模型的<em class="lc">精度并召回</em>。</p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mo"><img src="../Images/9014338dfcbcbd9560c897625d1b8d9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sbAW8jo_filJm0w_BY9ehg.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">随机模型的精度和召回率</figcaption></figure><p id="6c55" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><strong class="ju hj"> <em class="lc">如何解读上面的精准召回矩阵？</em>T11】</strong></p><p id="5663" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><strong class="ju hj">精度</strong> <br/> 1。以单元格(1x1)为例，其值为<strong class="ju hj">0.127</strong>；它说在所有被预测为第 1 类<em class="lc">的点中，只有<strong class="ju hj"> 12.7% </strong>的值实际上是第 1 类</em>的<em class="lc"/></p><p id="b896" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">2.对于原始的<em class="lc">第 4 类</em>和<em class="lc">预测的</em> <em class="lc">第 2 类</em>我们可以说，在我们的模型预测到的<em class="lc">第 2 类</em>的值中，<strong class="ju hj"> 23.6% </strong>的值<em class="lc">实际上</em>属于<em class="lc">第 4 类</em></p><p id="7c85" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><strong class="ju hj">召回</strong></p><p id="4b6f" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">1.检查像元(1X1)它的值为 0.079，这意味着对于实际属于类别 1 的所有点，我们的模型预测只有 7%的值属于类别 1</p><p id="6943" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">2.对于原始的<em class="lc">第 8 类</em>和<em class="lc">预测的第 5 类</em>值为<strong class="ju hj"> 0.250 </strong>意味着所有实际为<em class="lc">第 8 类</em>的值都是模型<em class="lc">预测的</em> <strong class="ju hj"> 25% </strong>值为<em class="lc">第 5 类</em></p><p id="47f0" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">在进行了一些探索性的数据分析之后，我们现在将对我们的模型进行训练，同时还将进行特征编码，你可以在我的<a class="ae lr" href="https://github.com/akshayakn13/CancerDiagnosis" rel="noopener ugc nofollow" target="_blank">笔记本</a>上查看。我们训练了多个模型，其中<strong class="ju hj">逻辑回归和支持向量机</strong>脱颖而出。</p><p id="8031" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><strong class="ju hj"> <em class="lc">逻辑回归</em> </strong></p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mp"><img src="../Images/ad8612ebc704d8ce0b168eb16439b60b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YIMkL9i7t-KfWZ9HaXt4JA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">逻辑回归在交叉验证中的表现</figcaption></figure><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mq"><img src="../Images/6662ce70721019bd4317dfa281c74a06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AVMFloqDNMXVZEZxokH3vQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">逻辑回归模型的混淆矩阵</figcaption></figure><p id="e502" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><strong class="ju hj"> <em class="lc">支持向量机</em> </strong></p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mr"><img src="../Images/ab004f23cf0119dff52eff13708d8a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Vld78nOYVKNWJjmDVq7xQ.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">SVM 在交叉验证方面的表现</figcaption></figure><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es ms"><img src="../Images/ed7bf71bc161d936efd95a3385af742c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*098cMdf1oYcul_8rjeO_SA.png"/></div></div><figcaption class="iq ir et er es is it bd b be z dx translated">SVM 模型的混淆矩阵</figcaption></figure><p id="d8aa" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated"><strong class="ju hj">所有车型对比</strong></p><figure class="ld le lf lg fd ij er es paragraph-image"><div role="button" tabindex="0" class="ik il di im bf in"><div class="er es mt"><img src="../Images/ea3b9e8e0bc27e2f445330b38859f5f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7F4VaKcYqBM2tRJIDb9NDA.png"/></div></div></figure><p id="4377" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">我们可以看到，逻辑回归和支持向量机在<em class="lc">对数损失</em>和<em class="lc">误分类点百分比方面都比其他方法表现更好。</em></p></div><div class="ab cl kv kw gp kx" role="separator"><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la lb"/><span class="ky bw bk kz la"/></div><div class="hb hc hd he hf"><p id="19d1" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">完整的代码和数据可以查看我的<a class="ae lr" href="https://github.com/akshayakn13/CancerDiagnosis" rel="noopener ugc nofollow" target="_blank"> GitHub </a>。</p><p id="8d9a" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">请随时在任何平台上与我联系。</p><p id="c7a0" class="pw-post-body-paragraph js jt hi ju b jv kq jx jy jz kr kb kc kd ks kf kg kh kt kj kk kl ku kn ko kp hb bi translated">也可以看看我的其他文章</p><div class="mu mv ez fb mw mx"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/fine-grained-sentiment-analysis-of-smartphone-review-d9f502a40c36"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hj fi z dy nc ea eb nd ed ef hh bi translated">智能手机评论的细粒度情感分析</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">情感分析或观点挖掘是通过使用自然语言处理来分析词语背后的情感…</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">medium.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl io mx"/></div></div></a></div><div class="mu mv ez fb mw mx"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/k-nearest-neighbor-k-nn-f4418a55b74f"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hj fi z dy nc ea eb nd ed ef hh bi translated">k-最近邻</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">K-最近邻(K-NN)算法是一种简单、易于实现的有监督机器学习算法。那个“K”…</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">medium.com</p></div></div><div class="ng l"><div class="nm l ni nj nk ng nl io mx"/></div></div></a></div><div class="mu mv ez fb mw mx"><a rel="noopener follow" target="_blank" href="/analytics-vidhya/support-vector-machines-805a24507f"><div class="my ab dw"><div class="mz ab na cl cj nb"><h2 class="bd hj fi z dy nc ea eb nd ed ef hh bi translated">支持向量机</h2><div class="ne l"><h3 class="bd b fi z dy nc ea eb nd ed ef dx translated">这是一篇两部分的文章，第一部分我将讨论硬边际支持向量机，下一部分我将…</h3></div><div class="nf l"><p class="bd b fp z dy nc ea eb nd ed ef dx translated">medium.com</p></div></div><div class="ng l"><div class="nn l ni nj nk ng nl io mx"/></div></div></a></div></div></div>    
</body>
</html>