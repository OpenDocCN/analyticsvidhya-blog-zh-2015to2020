<html>
<head>
<title>Yet another real-time detection-based object tracking algorithm using Tensorflow Object-detection API and OpenCV</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">另一种基于实时检测的对象跟踪算法使用Tensorflow对象检测API和OpenCV</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/yet-another-real-time-detection-based-object-tracking-algorithm-using-tensorflow-object-detection-8a349215c4c4?source=collection_archive---------7-----------------------#2020-05-15">https://medium.com/analytics-vidhya/yet-another-real-time-detection-based-object-tracking-algorithm-using-tensorflow-object-detection-8a349215c4c4?source=collection_archive---------7-----------------------#2020-05-15</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><div class=""><h2 id="bfc0" class="pw-subtitle-paragraph if hh hi bd b ig ih ii ij ik il im in io ip iq ir is it iu iv iw dx translated">这篇文章描述了优化现有物体跟踪模型和算法的方法，以获得更好的性能</h2></div><blockquote class="ix iy iz"><p id="133a" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">我将通过这些步骤来创建一个可以实时执行的对象跟踪模型。完整的实现可以在<a class="ae jx" href="https://github.com/HelgaShiryaeva/diploma" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到</p></blockquote><div class="jy jz ez fb ka kb"><a href="https://github.com/HelgaShiryaeva/diploma" rel="noopener  ugc nofollow" target="_blank"><div class="kc ab dw"><div class="kd ab ke cl cj kf"><h2 class="bd hj fi z dy kg ea eb kh ed ef hh bi translated">HelgaShiryaeva/文凭</h2><div class="ki l"><h3 class="bd b fi z dy kg ea eb kh ed ef dx translated">手势追踪毕业设计。克隆或下载…</h3></div><div class="kj l"><p class="bd b fp z dy kg ea eb kh ed ef dx translated">github.com</p></div></div><div class="kk l"><div class="kl l km kn ko kk kp kq kb"/></div></div></a></div></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><div class="ky kz la lb fd ab cb"><figure class="lc ld le lf lg lh li paragraph-image"><img src="../Images/33599f91d0d2ccf19f5e9c921222d4aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/1*pJLt1pQVcxL_5BwWwNYDfw.gif"/></figure><figure class="lc ld lk lf lg lh li paragraph-image"><img src="../Images/d6d62a308a14f6d7f4bb113216fcd37c.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/1*02INwUFUCtpjGfPhPGGApw.gif"/></figure><figure class="lc ld le lf lg lh li paragraph-image"><img src="../Images/86e9e732f6d6b56d834935cfbabdb723.png" data-original-src="https://miro.medium.com/v2/resize:fit:474/1*VbD9KQaFD63fXaZSlf6a8Q.gif"/><figcaption class="ll lm et er es ln lo bd b be z dx lp di lq lr translated">实时视频流中的检测与跟踪模型</figcaption></figure></div></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="c031" class="ls lt hi bd lu lv lw lx ly lz ma mb mc io md ip me ir mf is mg iu mh iv mi mj bi translated">检测模型</h1><p id="35b0" class="pw-post-body-paragraph ja jb hi jd b je mk ij jg jh ml im jj mm mn jm jn mo mp jq jr mq mr ju jv jw hb bi translated">这项工作的目的不是特别跟踪视频中的手势，而是尝试为任何对象跟踪制定通用算法。我决定选择的检测模型是单次检测器(SSD)模型，通过<a class="ae jx" href="https://github.com/tensorflow/models" rel="noopener ugc nofollow" target="_blank"><strong class="jd hj">tensor flow/models</strong></a>库进行训练。Tensorflow对象检测API 是一个非常容易上手的东西。如果你想知道如何训练自己的物体检测器，你可以遵循<a class="ms mt ge" href="https://medium.com/u/304fe0310a13?source=post_page-----8a349215c4c4--------------------------------" rel="noopener" target="_blank"> <strong class="jd hj"> Victor Dibia </strong> </a>指令<a class="ae jx" rel="noopener" href="/@victor.dibia/how-to-build-a-real-time-hand-detector-using-neural-networks-ssd-on-tensorflow-d6bac0e4b2ce"> <strong class="jd hj">如何在Tensorflow </strong> </a>上使用神经网络(SSD)构建实时手部检测器。我从零开始用训练SSD网络修改了他的模型，用手势传递额外的自我标记数据集，在近2000个标记样本上扩展现有数据集。所有图片都用<a class="ae jx" href="https://github.com/tzutalin/labelImg" rel="noopener ugc nofollow" target="_blank"> <strong class="jd hj"> LabelImg </strong> </a>工具贴上标签，下面的说明来自<strong class="jd hj"> </strong> <a class="ae jx" href="https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html" rel="noopener ugc nofollow" target="_blank"> <strong class="jd hj"> <em class="jc">这里的</em> </strong> </a>。你可以从上面提到的GitHub库中下载预先训练好的Oxford Hands和Ego Hands数据集检测模型。我选择的附加数据集分别是<a class="ae jx" href="http://lttm.dei.unipd.it/downloads/gesture/#senz3d" rel="noopener ugc nofollow" target="_blank"><strong class="jd hj"><em class="jc">Creative Senz3D</em></strong></a>和<a class="ae jx" href="http://sun.aei.polsl.pl/~mkawulok/gestures/" rel="noopener ugc nofollow" target="_blank"> <strong class="jd hj"> <em class="jc">手部姿态估计</em> </strong> </a> <em class="jc">。</em></p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="c50f" class="ls lt hi bd lu lv lw lx ly lz ma mb mc io md ip me ir mf is mg iu mh iv mi mj bi translated">目标跟踪问题</h1><p id="7423" class="pw-post-body-paragraph ja jb hi jd b je mk ij jg jh ml im jj mm mn jm jn mo mp jq jr mq mr ju jv jw hb bi translated">目标跟踪可以描述如下。假设您有一组按顺序处理的帧(或视频)。每一帧上都有一些物体。从第一帧开始，每个对象都必须与唯一的id相关联，并在接下来的帧中被跟踪。新的对象可能出现，旧的对象可能消失。</p><p id="3da0" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj mm jl jm jn mo jp jq jr mq jt ju jv jw hb bi translated">人们进行了大量的研究，建立了许多目标跟踪模型。甚至有人提出了神经网络模型，如<strong class="jd hj"> ROLO(最近的YOLO) </strong>和<strong class="jd hj">深度回归网络GOTURN。</strong>这个问题也可以用流行的<strong class="jd hj">卡尔曼滤波器</strong>和<strong class="jd hj">光流</strong>来解决。我认为最流行的多目标跟踪算法是SORT(简单在线实时跟踪)和DeepSORT(深度关联排序)。</p><p id="a357" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj mm jl jm jn mo jp jq jr mq jt ju jv jw hb bi translated">通常，MOT问题可以分为两个部分:当您在第一帧指定ROI(感兴趣区域)并在接下来的帧中跟踪它时，以及当您不指定时。第二种方法更灵活，解决方案是在所有帧上使用检测模型，将边界框相互关联并跟踪它们。基于检测的算法是我更深入研究的东西。让我们看看我们有什么。</p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="8563" class="ls lt hi bd lu lv lw lx ly lz ma mb mc io md ip me ir mf is mg iu mh iv mi mj bi translated">基于检测的跟踪问题</h1><blockquote class="ix iy iz"><p id="f664" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">当涉及到基于检测的目标跟踪时，可能会有几个问题。模型检测每一帧上的对象。覆盖单个对象的每个边界框的坐标被传递。但是检测模型可能并不完美。这里出现了一些问题。</p></blockquote><ul class=""><li id="46a5" class="mu mv hi jd b je jf jh ji mm mw mo mx mq my jw mz na nb nc bi translated"><strong class="jd hj">漏检(遮挡)</strong> —在之前的帧中成功检测到对象，但在当前帧中，由于对象移动或其他原因，未检测到对象。这可能会导致丢失该对象，并在它再次出现时用新的id来标识它。</li><li id="7c41" class="mu mv hi jd b je nd jh ne mm nf mo ng mq nh jw mz na nb nc bi translated"><strong class="jd hj">错误检测</strong> —模型检测到实际上没有物体的物体。这可能导致将这种错误检测与现有轨迹相关联，或者它可能开始跟踪不存在的对象，这是不好的。</li><li id="7277" class="mu mv hi jd b je nd jh ne mm nf mo ng mq nh jw mz na nb nc bi translated"><strong class="jd hj"> Id切换</strong> —在执行对象关联算法后，当对象彼此非常接近时，它们可能关联不良，对象标识符可能会切换。结果就是跟踪的错误。</li></ul></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="7dcd" class="ls lt hi bd lu lv lw lx ly lz ma mb mc io md ip me ir mf is mg iu mh iv mi mj bi translated"><strong class="ak">关联问题</strong></h1><p id="774b" class="pw-post-body-paragraph ja jb hi jd b je mk ij jg jh ml im jj mm mn jm jn mo mp jq jr mq mr ju jv jw hb bi translated">问题如下。假设您具有来自前一帧的被跟踪对象的当前位置，并且刚刚使用检测模型在当前帧上检测到其他对象。为了继续跟踪，我们必须将现有对象与当前检测到的对象相关联。这个问题可以用贪心的方式解决。可以简单地计算每对对象之间的边界框中心之间的<strong class="jd hj">欧几里德距离</strong>，然后为每个轨迹选择最接近其检测到的边界框的。或者，可以计算每对对象之间的<a class="ae jx" rel="noopener" href="/@nagsan16/object-detection-iou-intersection-over-union-73070cb11f6e"> <strong class="jd hj">【交集超过并集】</strong> </a>。但有时贪婪的解决方案并不是最优的。具有m个轨迹和n个检测对象的该解决方案的时间复杂度将是<strong class="jd hj"> O(m * n)，</strong>，这是相当好的。</p><p id="3bc9" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj mm jl jm jn mo jp jq jr mq jt ju jv jw hb bi translated">取而代之的是另一种方法。这是用<strong class="jd hj">匈牙利算法</strong>解决<strong class="jd hj">线性指派问题</strong>。这是在加权二部图中寻找给定大小的匹配的问题，其中边的权重之和最小。在我们的例子中，权重可能是欧几里得距离，或IoU，或任何其他可用于测量对象之间距离的度量。m * n大小的距离矩阵将是一个矩阵，在每个位置(I，j)上将有轨迹(I)和被检测物体(j)之间的距离。这个问题求的是最小的权重和，但是我们要最大化。这可以通过用-1 *距离矩阵求解最小值问题来实现。然后我们会得到最大值的答案。</p></div><div class="ab cl kr ks gp kt" role="separator"><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw kx"/><span class="ku bw bk kv kw"/></div><div class="hb hc hd he hf"><h1 id="4d43" class="ls lt hi bd lu lv lw lx ly lz ma mb mc io md ip me ir mf is mg iu mh iv mi mj bi translated">跟踪算法</h1><p id="8c6d" class="pw-post-body-paragraph ja jb hi jd b je mk ij jg jh ml im jj mm mn jm jn mo mp jq jr mq mr ju jv jw hb bi translated">我在这里 找到了一个实现<a class="ae jx" href="https://github.com/adipandas/multi-object-tracker" rel="noopener ugc nofollow" target="_blank"> <strong class="jd hj">的基础算法，并决定使用它，因为它接近我想要的东西。再来看我在<a class="ae jx" href="https://github.com/HelgaShiryaeva/diploma/blob/master/tracking/utils/tracker.py" rel="noopener ugc nofollow" target="_blank"><strong class="jd hj">Tracker . py</strong></a><strong class="jd hj">中实现的类Tracker。</strong>构造函数包含几个输入变量。</strong></a></p><figure class="ky kz la lb fd ld er es paragraph-image"><div class="er es ni"><img src="../Images/cfd9dff4c1212b842fd4ff2701b61687.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*xUBXhbI0bnqyL87dcU4bFw.png"/></div><figcaption class="ll lm et er es ln lo bd b be z dx translated">跟踪器类构造函数</figcaption></figure><ul class=""><li id="3190" class="mu mv hi jd b je jf jh ji mm mw mo mx mq my jw mz na nb nc bi translated"><strong class="jd hj"><em class="jc">active _ threshold</em></strong>—超参数，它告诉我们至少需要连续10帧的成功关联才能证明该物体不是<strong class="jd hj">误检</strong>。</li><li id="58fa" class="mu mv hi jd b je nd jh ne mm nf mo ng mq nh jw mz na nb nc bi translated"><strong class="jd hj"/></li><li id="59a1" class="mu mv hi jd b je nd jh ne mm nf mo ng mq nh jw mz na nb nc bi translated"><strong class="jd hj"> <em class="jc"> iou_threshold </em> </strong> —解决线性分配问题时成功关联的最小iou值，有助于解决<strong class="jd hj"> id切换</strong>问题</li></ul><blockquote class="ix iy iz"><p id="e8be" class="ja jb jc jd b je jf ij jg jh ji im jj jk jl jm jn jo jp jq jr js jt ju jv jw hb bi translated">这里也有一些清晰的跟踪器类集合。<strong class="jd hj">对象</strong>包含所有被跟踪对象的当前id和位置。<strong class="jd hj">激活</strong>是每个对象顺序轨迹的计数器。如果它大于10，那么我们在视频上显示这个对象。<strong class="jd hj">丢失</strong>是不成功关联的计数器。如果它大于10，那么我们就说这个曲目已经结束了。</p></blockquote><p id="e741" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj mm jl jm jn mo jp jq jr mq jt ju jv jw hb bi translated">锦上添花的是这个类的更新方法。其工作可以描述如下。我们计算IoU矩阵，其中当前<strong class="jd hj">对象</strong>的<strong class="jd hj"> m </strong>行被<strong class="jd hj">跟踪</strong>并且<strong class="jd hj"> n </strong>行<strong class="jd hj">检测</strong>来自下一帧。解决线性分配问题后，我们有3个案例。假设我们有k个成功的赋值。<strong class="jd hj"> k </strong>始终小于等于<strong class="jd hj"> min(m，n) </strong>。我们增加被跟踪对象的活动计数器并更新其位置。我们剩下了<strong class="jd hj"> m — k个未分配的曲目</strong>。我们递减其活动计数器并递增其非活动计数器。最后一种情况是我们剩下n-k个未分配的检测。我们将它们存储为一个新对象，让它们有机会成为一个新的轨迹。</p><p id="4b3a" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj mm jl jm jn mo jp jq jr mq jt ju jv jw hb bi translated"><strong class="jd hj">此解的时间复杂度</strong>是在O(m * n)时间内用匈牙利算法求解<strong class="jd hj">线性指派问题的时间复杂度。它比贪婪的O(m * n)方法更大，但工作起来肯定更好。我认为框架上不会有太多的手，因此使用更快的解决方案是合理的。</strong></p><p id="a884" class="pw-post-body-paragraph ja jb hi jd b je jf ij jg jh ji im jj mm jl jm jn mo jp jq jr mq jt ju jv jw hb bi translated">感谢你阅读这篇文章！如果你有任何问题或想更详细地讨论这个问题，请随时联系<a class="ae jx" href="https://github.com/HelgaShiryaeva" rel="noopener ugc nofollow" target="_blank"> GitHub </a>或<a class="ae jx" href="https://www.linkedin.com/in/volha-shyrayeva-014452138/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>。</p><h1 id="de8c" class="ls lt hi bd lu lv nj lx ly lz nk mb mc io nl ip me ir nm is mg iu nn iv mi mj bi translated">参考</h1><p id="49c6" class="pw-post-body-paragraph ja jb hi jd b je mk ij jg jh ml im jj mm mn jm jn mo mp jq jr mq mr ju jv jw hb bi translated">一些相关和参考论文。</p><ol class=""><li id="06ca" class="mu mv hi jd b je jf jh ji mm mw mo mx mq my jw no na nb nc bi translated">Victor Dibia，在Tensorflow上使用神经网络(SSD)进行实时手部检测，(2017)，GitHub知识库，<a class="ae jx" href="https://github.com/victordibia/handtracking" rel="noopener ugc nofollow" target="_blank">https://github.com/victordibia/handtracking</a></li><li id="5acf" class="mu mv hi jd b je nd jh ne mm nf mo ng mq nh jw no na nb nc bi translated">多目标追踪器，GitHub库，【https://github.com/adipandas/multi-object-tracker T2】</li><li id="59e2" class="mu mv hi jd b je nd jh ne mm nf mo ng mq nh jw no na nb nc bi translated">学习使用深度回归网络以每秒100帧的速度跟踪，【https://davheld.github.io/GOTURN/GOTURN.】T4pdf</li><li id="9247" class="mu mv hi jd b je nd jh ne mm nf mo ng mq nh jw no na nb nc bi translated">用于视觉目标跟踪的空间监督递归卷积神经网络，<a class="ae jx" href="https://arxiv.org/pdf/1607.05781v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1607.05781v1.pdf</a></li><li id="04fd" class="mu mv hi jd b je nd jh ne mm nf mo ng mq nh jw no na nb nc bi translated">不使用图像信息的高速检测跟踪，<a class="ae jx" href="https://www.researchgate.net/publication/319502501_High-Speed_Tracking-by-Detection_Without_Using_Image_Information_Challenge_winner_IWOT4S" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/publication/319502501 _ High-Speed _ Tracking-by-Detection _ Without _ Using _ Image _ Information _ Challenge _ winner _ iwot 4s</a></li><li id="764b" class="mu mv hi jd b je nd jh ne mm nf mo ng mq nh jw no na nb nc bi translated">利用视觉信息扩展基于IOU的多目标跟踪，<a class="ae jx" href="https://www.researchgate.net/publication/330425906_Extending_IOU_Based_Multi-Object_Tracking_by_Visual_Information" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/publication/330425906 _ Extending _ IOU _ Based _ Multi-Object _ Tracking _ by _ Visual _ Information</a></li></ol></div></div>    
</body>
</html>