<html>
<head>
<title>How to train your artificial dragon or clicker-learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何训练你的人造龙或响片-学习</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/how-to-train-your-artificial-dragon-or-clicker-learning-778d24c6ead2?source=collection_archive---------19-----------------------#2020-01-20">https://medium.com/analytics-vidhya/how-to-train-your-artificial-dragon-or-clicker-learning-778d24c6ead2?source=collection_archive---------19-----------------------#2020-01-20</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="b24c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">阿列克谢·科普洛夫、德米特罗·科列斯尼克、基里洛·梅迪亚诺夫斯基</p><blockquote class="jd je jf"><p id="24c0" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">职业驯兽师比其他人更擅长训练人工智能吗？为了检验这个假设，我们进行了一个实验。</p><p id="45ec" class="if ig jg ih b ii ij ik il im in io ip jh ir is it ji iv iw ix jj iz ja jb jc hb bi translated">我们从<a class="ae jk" href="https://github.com/hill-a/stable-baselines" rel="noopener ugc nofollow" target="_blank">稳定基线</a>调整了深度Q学习的原始架构，使其在短时间内可训练。一名人类受访者观察一名代理人玩视频游戏，并在每次他们认为代理人行动正确时发送点击。我们测试了专业训狗师和普通人。<br/>我们选择了雅达利的游戏突围。<a class="ae jk" href="https://github.com/openai/gym" rel="noopener ugc nofollow" target="_blank">打开AI健身房</a>换环境。我们使用来自<a class="ae jk" href="https://github.com/araffin/rl-baselines-zoo" rel="noopener ugc nofollow" target="_blank">动物园</a>的预先训练好的重物。<br/>源代码可通过<a class="ae jk" href="https://github.com/Ofrogue/clicker-learning" rel="noopener ugc nofollow" target="_blank">链接</a>获得</p></blockquote><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es jl"><img src="../Images/c81a21659b5b308e445e0461af1667e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EOWeHKgktlAGTmnzLfc3WQ.jpeg"/></div></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">图片来源:<a class="ae jk" href="https://www.beardeddragon.org/forums/viewtopic.php?p=1775149" rel="noopener ugc nofollow" target="_blank">https://www.beardeddragon.org/forums/viewtopic.php?p=1775149 </a></figcaption></figure><h2 id="2bd2" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated"><strong class="ak">简介</strong></h2><p id="88c8" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">人类通过对特定行为给予奖励来训练狗，这样狗会将特定的行为和情况与积极的(再次重复)或消极的(要避免的)价值观联系起来。类似地，人工智能体现在通过在智能体实现目标后传递的强化信号来自动训练。可惜这个过程非常慢，需要很多样本来学习。这篇博文的目的是测试是否有可能通过在老师选择的任何事件中(不仅仅是在目标结束时)提供奖励(鼠标点击)来训练视频游戏中的角色。</p><p id="2be7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们应该提到一些关于这项研究的哲学问题，因为这篇博文是在计算神经科学课程的范围内写的。人工神经网络和生物网络之间的联系是有争议的，但仍然是非常有前途的。发现这种联系的方法之一是比较生物系统行为和人工系统行为。但是这个想法的相关性是值得怀疑的。想象一下，我们正在比较人手和仿生假肢。事实上，我们会经历类似的行为。然而，我们物体的组成部分会有不同的性质。</p><p id="39b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jk" href="http://Reinforcement learning" rel="noopener ugc nofollow" target="_blank">强化学习</a>是一种方法论，旨在构建一个能够在给定环境中完成特定任务的人工智能体。<br/>在我们的案例中，这是雅达利游戏的突破和深度Q学习模式。选择这个游戏是因为它没有代理训练的确定性策略。例如，如果这是一个像迷宫一样谨慎的游戏，人类将有时间选择奖励，并可能在代理人走上正确的道路时发送奖励。在“突围”中，我们建议每个人根据自己的反应速度、耐心，或许还有直觉，坚持自己的特定策略。</p><p id="e86e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><a class="ae jk" href="https://en.wikipedia.org/wiki/Clicker_training" rel="noopener ugc nofollow" target="_blank">响片训练</a>是一种动物训练方法。它的名字来源于一种叫做“clicker”的工具:一种简单的装置，当按下时会发出声音。它需要立即向动物表明它做了将要被治疗的事情。<br/>这种方法的另一部分是将“点击”和将要给予的食物联系起来。对于一个人工智能体来说，这种联系是不相关的:它只是接受一些奖励，并不关心任何相关的治疗。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es lb"><img src="../Images/95a3b025207c0ad8979275be3d261243.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*zBtxiLa6Bm5B6oYpAmg2wg.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">突破，一个用matplotlib显示的Atari游戏</figcaption></figure><h2 id="7dd3" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">背景</h2><p id="e4fd" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">Q-learning是一种技术，来源于在给定状态下做出正确决策的想法。q代表质量。对于一个状态数量有限的离散游戏，Q-learning意味着建立一个表格，用行表示状态，用列表示动作。贝尔曼方程用于根据已经做出的行动和收到的奖励更新Q值。</p><p id="0bf3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">游戏的当前画面被视为当前状态。很容易提到我们的环境游戏突围确实有很多状态！比方说，对于大小为210x160的RGB图像，有超过100k种组合。一种叫做<a class="ae jk" href="https://arxiv.org/abs/1312.5602" rel="noopener ugc nofollow" target="_blank">深度Q学习</a>的技术利用神经网络而不是表格来处理高维度。<br/>在基线模型中，出现动作值和状态值之间的划分。这些被设计成两个子网络。行动价值表示如果采取特定行动，未来奖励的平均贴现金额。状态值是对从给定状态开始继续执行当前策略所获得的回报的估计。Q-learning旨在最大化状态-行动值——估计采取行动并继续执行当前政策。</p><h2 id="90ef" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated"><strong class="ak">技术信息(调整)</strong></h2><p id="e377" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">基线模型由特征提取器、状态值子网络和优势(行动)子网络组成。借助于3个卷积层提取特征向量。其他子网可以在下图中看到。(架构设计可以从<a class="ae jk" href="https://github.com/hill-a/stable-baselines/blob/master/stable_baselines/deepq/dqn.py" rel="noopener ugc nofollow" target="_blank">稳定基线DQN </a>模型中探索)</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es lc"><img src="../Images/0930fe5b88fae8e9fd3d514cdd22db3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*-9A505MFi0fkiaNn1tFHlA.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">探索模型的部分架构。附加层用绿色标记。输入游戏的画面，输出动作的逻辑。</figcaption></figure><p id="0a72" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们改变了我们的模型，最初对人类训练者来说是不好的，但希望在合理的时间内可以训练。<br/>我们为动作子网增加了一个额外的层<strong class="ih hj">一个</strong>。它是具有输入4和输出4的密集线性层。所有其他层都是从基线模型初始化的。所以理论上，如果这一层是一个单位矩阵，它的偏差是一个零矢量，我们就有了精确的基线模型。我们冻结了除了附加层<strong class="ih hj"> A </strong>之外的所有层。所以只有那个是可变的(可训练的)。</p><p id="baf9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们坚持一个事实，我们的模型不能比基线表现得更好。20个参数几乎不可能显著改善14 Mb的模型。</p><h2 id="4c6d" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">实验装置</h2><p id="f1b0" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">每一步的性能计算为一个生命周期中被破坏的块的累积数量。雅达利突破最初给更高的块更多的点，但我们使用“一个块一个点”奖励(一个稳定基线的默认设置)。</p><p id="3b6b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">起初，我们想确保带有新图层<strong class="ih hj"> A </strong>的修改后的模型会比基线表现更差。在这一点上，我们的成功是不可避免的:改变后的模型在训练前为任何状态返回一个动作。</p><p id="1631" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们决定找出合理训练它所需的时间。一个相对较小的100集平均分2.8的结果用了80k时间步(游戏的实际帧)。因此，以每秒30帧的速度，玩游戏大约需要40分钟。</p><p id="db8b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">当然，我们会让所有的受访者绝望，让他们连续按下40多个地雷的按钮！</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es ld"><img src="../Images/73787a51a021084fe19464198c343031.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0HA5BCJ1Zo-wxjUCiKRJXg.png"/></div></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">修改模型的平滑剧集奖励。使用默认超参数进行训练的张量板日志</figcaption></figure><p id="de20" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">使用<a class="ae jk" href="https://arxiv.org/abs/1511.05952" rel="noopener ugc nofollow" target="_blank">优先经验回放</a>训练速度大幅提高。我们在20k的时间步长后获得了100集5.5的平均分数。</p></div><div class="ab cl le lf gp lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="hb hc hd he hf"><p id="fd63" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们坚持认为，我们的受访者应该与游戏互动大约10分钟。因此，我们将实验持续时间限制在18000帧(30fps *秒)。</p><p id="531c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了让我们的代理玩得稍微差一点，我们决定将我们的矩阵<strong class="ih hj"> A </strong>初始化为单位矩阵<strong class="ih hj"> I </strong>和均匀噪声矩阵<strong class="ih hj"> R </strong>之和。噪声分布用幅度<strong class="ih hj"> m、</strong>来调节，使得<strong class="ih hj"> R </strong>的每个元素将具有分布U([-0.5m，0.5m])。<br/>在这一点上，我们模型的政策非常接近基线。记住，我们只有20个参数需要训练，我们的模型几乎不能改变它的行为。它很可能会提高自己的弹跳能力(作为获得奖励的默认场景)，但训练模型让球每次直直地落在垫子上时都通过几乎是不可能的。</p><p id="c116" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下一步是定义<strong class="ih hj"> m </strong>以期望在18k次迭代的训练后获得最高的改进。在这一点上，我们需要为每一个噪声水平(幅度)找到一个相应的学习率。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div role="button" tabindex="0" class="jr js di jt bf ju"><div class="er es ll"><img src="../Images/69aad74ff000dc25225bfd6f0b8896e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sXusrmECpYs3FURWKE70PQ.png"/></div></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">18k迭代后获得分数，有固定奖励</figcaption></figure><p id="ba66" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们对这两个参数进行网格搜索。模型的分数平均用不同的量级<strong class="ih hj"> m </strong>来衡量(对于每个量级，5个改变的模型运行10场游戏)。然后，我们用常规奖励训练我们的模型，并在每个学习速率和幅度下测试平均性能(10场游戏)。<br/>我们最终得到<strong class="ih hj"> m </strong> =0.15，学习率5x10^-6 (5e-6)。它给了分数最好的提高。</p><h2 id="7241" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">比较奖励系统</h2><p id="3eb8" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">在开始对人类进行实验之前，我们决定使用不同的奖励系统对修改后的模型进行几次训练。这些测试非常重要，因为它会告诉我们更多关于模型行为及其对奖励的反应。我们已经为以下每个奖励系统训练了我们的模型10次:</p><ul class=""><li id="cc40" class="lm ln hi ih b ii ij im in iq lo iu lp iy lq jc lr ls lt lu bi translated">模特从不接受奖励。</li><li id="ff76" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated">模特总是会得到奖励。</li><li id="38c7" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated">模特在任何时候都有10%的几率获得奖励。</li><li id="19a5" class="lm ln hi ih b ii lv im lw iq lx iu ly iy lz jc lr ls lt lu bi translated">每当成功摧毁任何一个方块，模型都会获得奖励。</li></ul><p id="759d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">下面你可以看到一张图表，描述了不同奖励制度下模型性能的发展。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es ma"><img src="../Images/83137be8e7809df168a1dcbce3c92730.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*ND3j7I8oFEg4OqZ6lpnpDQ.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">没有人类互动的实验每集的平均平滑回报</figcaption></figure><p id="c581" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们所有人都感到惊讶的结果之一是，在整个培训过程中，模特的表现没有得到任何奖励。</p><h2 id="9a61" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">实验</h2><p id="1b28" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">经过几次测试后，我们发现不同的奖励政策对模型性能确实有不同的影响。我们准备进行实地实验。为此，我们决定召集两组人，让他们玩这个游戏。第一组被称为“训练师”，由专业的训狗师组成。第二组是“控制组”，由没有任何动物训练经验的人组成。每个小组都被要求观察游戏，并在模型做了他们认为积极的事情时给予奖励。在对两组进行实验后，我们的团队最终得到了总共24个结果，其中14个来自“训练者”组，10个来自“控制”组。</p><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es ma"><img src="../Images/dbd20d5e9856a049f25b850a2cb76880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*xgEDCsdhjJLxlktBVPFJ_w.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">人际互动实验每集的平均平滑回报</figcaption></figure><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es ma"><img src="../Images/78579f678f174dfa4195cd8e59e437e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*sxBd9-4O2u7QwjiGLImEow.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">从每组的前五个结果来看，人际互动实验每集的平均平滑回报</figcaption></figure><figure class="jm jn jo jp fd jq er es paragraph-image"><div class="er es ma"><img src="../Images/7fe8e3632010629f18e59798ad719341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*HyzzRKwT4ZPmyDaH-wInpA.png"/></div><figcaption class="jx jy et er es jz ka bd b be z dx translated">不同奖励方式下培训期间的平均绩效</figcaption></figure><p id="22fd" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">不幸的是，在处理了实验结果之后，我们发现，平均来说，实验组和控制组之间的训练速度似乎没有区别。<br/>我们还比较了“控制组”和“培训师组”前五名模特的平均表现。培训师可能被认为是表现更好的人。然而，我们并不认为这个测试的重要性足够高。但是，但是，有一个希望……在<em class="jg">未知的深处，</em>藏着一个勇敢的旅行者。</p><h2 id="8524" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">结论</h2><p id="b13c" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">我们没有收集到足够的证据来证明训狗师的表现明显好于普通人，但是我们观察到了一个有希望的事情。最著名的模特是由几个训狗师训练的。(事实上，它们在我们邀请受访者参与的狗狗比赛中获得了奖项。)</p><p id="b2c7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们可以宣称的另一个区别在于人类训练者和常规奖励方法之间。平均而言，人类表现稍好。</p><h2 id="1f48" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">评论</h2><p id="1032" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">我们没有考虑实验中使用的硬件。即使所有的测试对象都有相同的学习迭代次数，我们也没有动态地调整帧速率。这使得游戏在功能更强大的电脑上玩得更快。因此，有些人必须比其他人反应更快。</p><p id="18b5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们没有考虑每个受试者的年龄。此外，我们的“训练者”小组最终由比我们的“控制”小组年长得多的人组成。<br/>我们没有对反应进行任何测试，我们的团队认为这可能会对实验结果产生影响。</p><p id="2175" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">实验组的一个测试对象偶然点击了触摸板，改变了聚焦窗口，这反过来阻止了应用程序对“奖励”按钮的反应。回答者在大约一半的迭代中训练模型，其余的时间模型没有得到任何奖励。再次令人惊讶的是，在这种情况下，最终的模型具有最好的性能。</p><p id="be31" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">一个模型获得了太多的奖励，整个游戏过程中超过800次，最终总是得到低于平均水平的结果。</p><p id="0ac5" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们没有固定层<strong class="ih hj"> A. </strong>的权重，在每次运行期间，它被随机初始化(但是具有相同的分布)。</p><h2 id="f1e6" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated"><strong class="ak">职责</strong></h2><p id="5dd6" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">阿列克谢·科普洛夫:发展、融合</p><p id="4279" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">科列斯尼克:实验设置，参数调整</p><p id="874f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Kyrylo Medianovskyi:研究，原型</p><h2 id="73d6" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">承认</h2><p id="6a00" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated">我们小组感谢Tambet Matiisen的支持和监督。</p><h2 id="d81d" class="kb kc hi bd kd ke kf kg kh ki kj kk kl iq km kn ko iu kp kq kr iy ks kt ku kv bi translated">来源</h2><p id="1583" class="pw-post-body-paragraph if ig hi ih b ii kw ik il im kx io ip iq ky is it iu kz iw ix iy la ja jb jc hb bi translated"><a class="ae jk" href="https://github.com/Ofrogue/clicker-learning" rel="noopener ugc nofollow" target="_blank">来源github </a></p></div></div>    
</body>
</html>