# 生成性对抗网络的直观介绍

> 原文：<https://medium.com/analytics-vidhya/an-intuitive-introduction-to-generative-adversarial-networks-2d69b0be2579?source=collection_archive---------31----------------------->

![](img/5e1d4a2e81a46401dabbf5155998808e.png)

生成对抗性网络结构

# 摘要

随着数据在机器学习和数据科学领域变得越来越重要，研究人员已经开发出了可以从零开始生成数据的系统。这些系统被称为生成对抗网络或 GANs。本文对生成对抗网络及其应用进行了简要而直观的介绍和分析。

**关键词:**生成对抗网络(GAN)，合成数据，机器学习

# 生成对抗网络导论

在机器学习的世界里，我们经常基于大量数据进行插值。然而，在许多领域，数据通常是有限的。以 2020 年的新冠肺炎危机为例。许多团队为了诊断疾病而建立模型；然而，由于这种疾病是新的，几乎没有实际数据来建立诊断模型，导致模型的准确性普遍较低。生成对抗网络旨在解决这个问题。

GANs 的本质是从零开始创造数据。有了这种力量，机器学习领域有可能彻底被革命。当看着“生成对抗网络”这个名称时，人们可以推断出有一个生成者和一个产生网络的对手。顾名思义，GAN 由两部分组成:生成模型和鉴别模型。

# 生成模型

生成模型通过向正态分布添加噪声来工作。假设给定函数 G(z) = x，其中 z 表示函数数据集的噪声或失真，G(z)是生成 x 图像的生成模型。变量 z 通常会改变图像的不同特征。

因此，举例来说，如果我们得到一张人脸的图像，它会提取人脸图像并改变图像的不同特征，比如眉毛的颜色。直观地考虑这个函数，通过给某个数据增加一个噪声 z，你会改变这个数据的一个维度。因此，以面部图像为例，你会改变面部的一个方面。

# 判别模型

单单一个生成模型似乎就足以产生人工数据。然而，当涉及到数据生成时，鉴别器起着重要的作用。顾名思义，它“区别”生成模型。您可以将此视为生成模型的指导因素。鉴别模型是基于试图产生的真实图像来训练的。例如，如果我们想为人脸创建合成数据，我们可以在真实人脸上训练识别模型。然后，它将测试由生成模型创建的图像，并提供反馈，以便生成模型也可以创建真实的图像。这也可以用数学来表达。对于判别模型，我们给定函数 D(x)。

假设我们有两种状态，真实图像和机器生成的图像，我们可以得出以下结论:D(x) = 0 或 D(x) = 1。

D(x)函数的目的是输出两种不同的状态。在 D(x)函数中，所有真实数据都将标记为 1。当输出 0 时，鉴别模型识别这是机器产生的数据，而如果输出 1，它不识别这是机器产生的数据。生成网络的最终目标是产生足够真实的数据，以便判别模型输出 1。经过多次迭代后，生成型模型将产生类似真实的数据，使得区分模型不再能够区分两者。这形成了一个生成性对抗网络的基础。

# 对 CycleGAN 的介绍

CycleGAN 是一个生成性对抗网络的实际应用。它侧重于图像到图像的翻译。这里讨论的许多概念都可以在 CycleGAN 中找到。CycleGAN 的主要目标是产生一种图像分布，使得 G(x)(相当于 G(z))不能与 Y(相当于图像，x)相区分。他们通过不断训练辨别模型来做到这一点。它本质上是一个自我完善的网络。

# 结束语

生成对抗网络与机器学习融合得很好，因为它们有能力彻底变革该领域，因为可以产生本质上无限量的数据。本文将 GANs 的复杂本质浓缩为读者更容易理解和直观的内容，为更抽象地理解 GANs 的潜在方面奠定了基础。它还深入研究了 GANs 的一些实际应用，展示了 GANs 在当今的重要性。

如果你喜欢这篇文章，你可以在这里找到更多我的研究。

# 参考

*   惠，乔纳森。"什么是生成性对抗网络？"Medium，Medium，2019 年 12 月 26 日 Medium . com/@ Jonathan hui/gan-whats-generative-adversarial-networks-and-its-application-f 39 ed 278 ef 09。
*   伊恩，好伙计。“生成性对抗网络。”让·普吉-阿巴迪、迈赫迪·米尔扎、徐炳、戴维·沃德-法利、谢尔吉尔·奥泽尔、亚伦·库维尔、约舒阿·本吉奥，第一版，2014 年 6 月 10 日，1。arxiv，arXiv:1406.2661v1。
*   朱俊彦。"使用循环一致对抗网络的不成对图像到图像翻译."、Taesung、Park、Phillip、Isola、Alexei、Efros、v6、2017 年 3 月 30 日，1。arxiv，arXiv:1703.10593v6。