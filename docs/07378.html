<html>
<head>
<title>Simple Neural Network Explanation: From Logistic Regression to Neural Network — Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">简单的神经网络解释:从逻辑回归到神经网络—第三部分</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/simple-explanation-from-logistic-regression-to-neural-network-part-3-284baee5db13?source=collection_archive---------21-----------------------#2020-06-23">https://medium.com/analytics-vidhya/simple-explanation-from-logistic-regression-to-neural-network-part-3-284baee5db13?source=collection_archive---------21-----------------------#2020-06-23</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><figure class="hh hi ez fb hj hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es hg"><img src="../Images/6feaea448fa0bac90c04c47902a2b5ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AOtLxeTyRDK65L2X233t_w.jpeg"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">照片由<a class="ae hv" href="https://pixabay.com/users/athree23-6195572/" rel="noopener ugc nofollow" target="_blank">at ree23</a>在<a class="ae hv" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>上拍摄</figcaption></figure><div class=""/><p id="8a14" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">训练步骤是任何建模过程的主要步骤。在本文中，我们将重点关注神经网络的训练/学习步骤，这是采用神经网络模型时最耗时的部分。在我们的<strong class="ix hz"> <em class="jt">“简单解释:从逻辑回归到神经网络”</em> </strong>系列的前几部分中，我们浏览了神经网络结构的概述，并学习了一些关于激活函数类型的基础知识，<a class="ae hv" rel="noopener" href="/analytics-vidhya/simple-explanation-from-logistic-regression-to-neural-network-part-2-9b74718544c8">在这里查看它们以刷新您的记忆。</a></p><h1 id="d832" class="ju jv hy bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">索引:</h1><ol class=""><li id="ab18" class="ks kt hy ix b iy ku jc kv jg kw jk kx jo ky js kz la lb lc bi translated">神经网络是如何学习的？</li><li id="06fb" class="ks kt hy ix b iy ld jc le jg lf jk lg jo lh js kz la lb lc bi translated">神经网络训练—前馈传播</li></ol><p id="d8c5" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们之前已经建立了一个具有非常基本结构的神经网络。通过在每层中增加隐藏层和额外的神经元，神经网络可以变得更加复杂。对于我们这里的教程，让我们考虑下面的简单神经网络结构和我们之前在训练该神经网络时使用的员工数据集，以预测<strong class="ix hz">(员工是否会离开公司)</strong></p><div class="li lj lk ll fd ab cb"><figure class="lm hk ln lo lp lq lr paragraph-image"><img src="../Images/44c770a96f5ca080e965f321a25a909f.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*a6VTXK9ydVTR-jXLWkEhww.png"/></figure><figure class="lm hk ls lo lp lq lr paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><img src="../Images/5e5d5f975604377ce27e82de123e6194.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*LkJOPaO7lCJLQ1ymvZ3bYA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx lt di lu lv translated">图1:神经网络模型，训练数据集</figcaption></figure></div><h1 id="dc70" class="ju jv hy bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">神经网络是如何学习的？</h1><p id="e00e" class="pw-post-body-paragraph iv iw hy ix b iy ku ja jb jc kv je jf jg lw ji jj jk lx jm jn jo ly jq jr js hb bi translated">像任何模型一样，神经网络需要经过训练才能做出预测。</p><p id="7881" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">神经网络的学习过程或训练不比<strong class="ix hz">优化网络中每一层的权重更复杂。</strong>记住权重是每个神经元输入的相关值。通常，当构建神经网络时，我们从<strong class="ix hz">随机值开始初始化所有层中的权重</strong>。</p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es lz"><img src="../Images/8c35ff9e380a65627e160410bba5176d.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*_TH8bYFT17NYrXKWWDK-UQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图2:带有随机权重的神经网络初始化</figcaption></figure><p id="293e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如你在上面看到的，我们为每一层的所有神经元分配了随机权重(基于高斯分布)。这里我们有三个神经元，让我们给每个神经元分配一种颜色，以简化本文中对它们的引用。绿色神经元有2个输入，因此我们有2个随机权重为[0.37，0.67]，蓝色神经元为[1.94，-2.3]，黑色神经元为[ 0.27，-0.98]。<br/>在这个随机初始化步骤之后，学习过程开始，这是那些随机权重的缓慢优化。</p><p id="dbd7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">神经网络的训练过程包括两个阶段:</strong></p><ol class=""><li id="8b47" class="ks kt hy ix b iy iz jc jd jg ma jk mb jo mc js kz la lb lc bi translated">前馈传播</li><li id="b2ef" class="ks kt hy ix b iy ld jc le jg lf jk lg jo lh js kz la lb lc bi translated">反向传播</li></ol><h1 id="6ca7" class="ju jv hy bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated"><strong class="ak">前馈-传播</strong></h1><p id="2673" class="pw-post-body-paragraph iv iw hy ix b iy ku ja jb jc kv je jf jg lw ji jj jk lx jm jn jo ly jq jr js hb bi translated">前馈-传播是将输入值(训练数据)馈送到神经网络并计算输出值(预测目标)的过程。</p><figure class="li lj lk ll fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es md"><img src="../Images/4be2a7eccbc83707f886537421a59d14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jYmIMM-by2ybad0RDAUb-w.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图3:前馈-传播</figcaption></figure><p id="c429" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们看看如何使用上面的训练数据集(员工数据集)来为上面的神经网络模型提供信息，以预测员工是否会离开公司。</p><p id="4a44" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">首先，我们需要处理图3中的NN结构。我们有<strong class="ix hz"> 2层</strong> NN，为每层分配随机权重(在计算NN的深度时，我们不计算输入层，我们只考虑权重可调的层)。对于隐藏层，我们有<strong class="ix hz"> 2个神经元，</strong>，而对于输出层，我们有<strong class="ix hz">一个神经元</strong>，因为这是一个二元分类问题。对于激活函数，让我们在所有隐藏层中使用<strong class="ix hz"> RELU函数，在输出层</strong>中使用S <strong class="ix hz"> igmoid来给出期望的预测(概率)。</strong></p><p id="743c" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其次，让我们用一个数字来表示我们的神经网络结构中的每一层(每一层都有输入/输出值)(见下面的图4)。这样，我们可以很容易地跟踪输入值，从进入第一层，直到计算最终预测。</p><figure class="li lj lk ll fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es me"><img src="../Images/75ff9911c90896296549f843b54281c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TtCETxrc2ZEq0Tsvn2DqdQ.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图4: NN层</figcaption></figure><p id="ecd2" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们将上面显示的神经网络层进一步细分:</p><p id="5929" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">第一层(输入层):</strong></p><ul class=""><li id="0f5c" class="ks kt hy ix b iy iz jc jd jg ma jk mb jo mc js mf la lb lc bi translated"><em class="jt"> X </em>:输入数据/数据集的特征/数据集的列。</li></ul><p id="6064" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">第二层(第一个隐藏层):</strong></p><ul class=""><li id="9d8d" class="ks kt hy ix b iy iz jc jd jg ma jk mb jo mc js mf la lb lc bi translated"><em class="jt"> Z </em>:第二层输出。</li><li id="d815" class="ks kt hy ix b iy ld jc le jg lf jk lg jo lh js mf la lb lc bi translated"><em class="jt"> W1 </em>:用于计算<em class="jt"> Z </em>的第二层关联权重。</li></ul><p id="2656" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><strong class="ix hz">第三层(输出层):</strong></p><ul class=""><li id="131e" class="ks kt hy ix b iy iz jc jd jg ma jk mb jo mc js mf la lb lc bi translated"><em class="jt"> Zf </em>:第三层输出。因为这是该特定神经网络的最后一层，所以这将是最终的模型输出/预测。</li><li id="c49f" class="ks kt hy ix b iy ld jc le jg lf jk lg jo lh js mf la lb lc bi translated"><em class="jt"> W2 </em>:第三层的关联权重，用于计算<em class="jt"> Zf </em>。</li></ul><p id="fa57" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，让我们开始向我们的神经网络输入数据。我们使用的数据集包含两个特征:</p><ol class=""><li id="d5b2" class="ks kt hy ix b iy iz jc jd jg ma jk mb jo mc js kz la lb lc bi translated">年龄<strong class="ix hz"> <em class="jt"> (X1) </em> </strong></li><li id="2729" class="ks kt hy ix b iy ld jc le jg lf jk lg jo lh js kz la lb lc bi translated">休假天数<strong class="ix hz"><em class="jt">【X2】</em></strong></li></ol><p id="a28d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输入层上有2个神经元(每个列/特征一个)。<strong class="ix hz"> </strong>回到前馈-传播过程，我们需要将这两个特征作为输入值输入到我们的神经网络，以产生一个输出。</p><figure class="li lj lk ll fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mg"><img src="../Images/292b5a13ef375c694aeaa4eac326670e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w4Sq_TpmGeHFnx_Q1hV-JA.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图5:将数据逐行输入神经网络</figcaption></figure><p id="8dde" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们一次向NN <strong class="ix hz">馈送一行</strong> <strong class="ix hz">的数据。</strong>然后，一些数学方程将应用于这些输入值<em class="jt"> x1 </em>和<em class="jt"> x2 </em>以计算一些结果<em class="jt"> Z(y1) </em>和<em class="jt"> Z(y2) </em>，如图5所示。这些结果将被传递到下一层，在那里执行另一个计算，等等，直到在结束<em class="jt"> Zf(y) </em>计算出最终的预测值。我们对每一行重复上述过程(输入数据)，以便为数据集中的每个观察值获得预测值<em class="jt"> Zf(y) </em>。</p><p id="b68a" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">让我们通过跟踪每个层的输出，将上述流程场景细分为更多细节:</p><ul class=""><li id="059f" class="ks kt hy ix b iy iz jc jd jg ma jk mb jo mc js mf la lb lc bi translated"><strong class="ix hz">输入图层(图层1): </strong></li></ul><figure class="li lj lk ll fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mg"><img src="../Images/1a3aa85a9fd3398fe755ad6786c42ce7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XT7Bir8dc7p6pBgpC--sOg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图6:第1层—输入数据集的行</figcaption></figure><p id="c7e2" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">输入层是NN中的第一个前端层，它为下一层提供<strong class="ix hz"><em class="jt">×T21(数据集的特征)，而不执行任何类型的计算，因为它是由被动神经元构建的。</em></strong></p><p id="1b63" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们前面提到的，我们需要将训练集中的所有行输入神经网络。有两种可能的方法来实现这一点:</p><ol class=""><li id="b9f0" class="ks kt hy ix b iy iz jc jd jg ma jk mb jo mc js kz la lb lc bi translated">使用for循环遍历所有行，并对每一行执行上述过程。</li></ol><p id="4163" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">2.使用矩阵表示<strong class="ix hz">(矢量化)</strong>，这将比上述方法更快。</p><p id="697b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因为我们喜欢更少的训练时间，所以我们将使用矩阵表示来跟踪饲养过程中的值。训练数据集的特征<em class="jt"> (X) </em>将呈现为形状<em class="jt"> (n，m) </em>的矩阵，其中:</p><ul class=""><li id="a372" class="ks kt hy ix b iy iz jc jd jg ma jk mb jo mc js mf la lb lc bi translated"><strong class="ix hz"> n </strong>:矩阵的行数——代表特征的数量。</li><li id="b673" class="ks kt hy ix b iy ld jc le jg lf jk lg jo lh js mf la lb lc bi translated"><strong class="ix hz"> m </strong>:矩阵的列数——代表观察值的数量。</li></ul><p id="c664" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">因此矩阵<em class="jt"> X </em>将通过这一层传递到下一层，如下所示:</p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mh"><img src="../Images/0965e8e021b26b2b8c6f0304cb0f866d.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*lkPFKUuPsssdt_pzYnm31w.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图7:作为(X)矩阵的数据集特征</figcaption></figure><ul class=""><li id="8cc6" class="ks kt hy ix b iy iz jc jd jg ma jk mb jo mc js mf la lb lc bi translated"><strong class="ix hz">第一个隐藏层(第二层):</strong></li></ul><figure class="li lj lk ll fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mg"><img src="../Images/901c480b5caf2fa260f9b8e472049911.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2brWZhu-6GJ53eJJ6PFfYg.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图8:第2层—处理数据</figcaption></figure><p id="69bb" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">在隐藏层中，我们执行一些线性计算(组合所有输入值<em class="jt"> X </em>乘以它们相关的随机权重<em class="jt"> W1 </em>)。我们在这一层的每个神经元中执行这种计算，以获得将被传递到下一层的结果。在本系列的<a class="ae hv" rel="noopener" href="/analytics-vidhya/simple-explanation-from-logistic-regression-to-neural-network-part-2-9b74718544c8">第2部分，我们详细介绍了如何进行线性计算，如下所示:</a></p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mi"><img src="../Images/bec7fb123c31a863add4749f1dea7303.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/format:webp/1*CImevY5sKwYKPaLHRyrpWw.png"/></div></figure><p id="562d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">隐藏层包含2个神经元，因此我们执行上述等式两次，以获得绿色神经元的<em class="jt"> Z(y1) </em>和蓝色神经元的<em class="jt">Z(y2)</em><em class="jt">。<br/> </em>正如我们提到的，我们将通过使用矢量化一次性计算所有神经元的所有<em class="jt"> Z(yi) </em>来避免循环。因此，让我们重写上面的等式，以利用优化的向量和矩阵运算，如下所示:</p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mj"><img src="../Images/201fb650fd1798bced0b70a4e2a7ca51.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*yet8ZJAPT7PkA9gWuUs_Sw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图9:第2层—线性计算</figcaption></figure><p id="f823" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">其中<em class="jt"> X </em>这里是通过图7中的输入层传递到该层的矩阵，并且<em class="jt"> W </em>是表示每个神经元的权重的矩阵。如图8所示，分配给绿色和蓝色神经元的随机权重分别为[0.37，0.67]和[1.94，-2.3]。使用这两组权重，我们可以将权重矩阵<em class="jt">(</em><strong class="ix hz"><em class="jt">W1</em></strong><em class="jt">)</em>写如下:</p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mk"><img src="../Images/2f10f4ce0c85f223748fb356837a49f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*50vcL9OmlbUHfQtIPWRmRw.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图10:第2层—权重矩阵</figcaption></figure><p id="aeb7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用矩阵来计算图9中的计算，以获得表示该隐藏层的输出的矩阵<em class="jt"> Z </em>。矩阵<em class="jt"> Z </em>具有<em class="jt"> (c，m)</em>的形状，因此<em class="jt"> Z </em>中的每一行都包含在神经元中执行计算的单独结果，我们在此将神经元结果命名为<em class="jt"> Z(y1) </em>和<em class="jt"> Z(y2) </em>，并且我们对每个观察重复这一过程(<em class="jt"> m </em>次)。</p><p id="811e" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">为了帮助我们计算矩阵<strong class="ix hz"> <em class="jt"> Z </em> </strong>，让我们快速记住两个矩阵如何相乘:</p><figure class="li lj lk ll fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es ml"><img src="../Images/fc40ff299653a1cf47fb40011bbd04bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*HCKLKLOm5clIeG-d0cRLhA.gif"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图11:矩阵乘法</figcaption></figure><p id="a513" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">利用上面的矩阵乘法，矩阵<strong class="ix hz"><em class="jt"/></strong><em class="jt"/>在数学上可以写成:</p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mm"><img src="../Images/b05cbe31f8058043958e711d24fc0e85.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*NrR-LNidFt1HZkm4keWQXg.png"/></div></figure><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mn"><img src="../Images/c8b9d0cb3026cb71da455ba9b41b0a93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*yQ64J1Jy-8-u8xje9_d72Q.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图12:第2层—输出矩阵(Z)</figcaption></figure><p id="71f7" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated"><em class="jt">(注意:在上述计算中，为了更简单起见，我们在计算中没有考虑偏差项)</em></p><p id="c3cb" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这个矩阵<em class="jt"> Z是这一层(第2层)的输出，它将以<em class="jt"> (c，m)的形状传递给下一层。</em>矩阵中的每一列包含层的神经元计算结果<em class="jt">Z(yi)</em>对数据集中的每一次观察执行<em class="jt"> </em>。</em></p><ul class=""><li id="e31b" class="ks kt hy ix b iy iz jc jd jg ma jk mb jo mc js mf la lb lc bi translated"><strong class="ix hz">输出层(第三层):</strong></li></ul><figure class="li lj lk ll fd hk er es paragraph-image"><div role="button" tabindex="0" class="hl hm di hn bf ho"><div class="er es mg"><img src="../Images/cc258f24aa64144b69389500d77fe449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sl1tzHyw8JXrM0SetWZvMw.png"/></div></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图13:第3层—计算最终预测</figcaption></figure><p id="d572" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这一层的输出代表我们模型的最终预测值。</p><p id="fb49" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">正如我们在隐藏层中所做的，我们将使用分配的随机权重<strong class="ix hz"> <em class="jt"> W2 </em> </strong>在该层中的单个神经元中执行相同的计算<em class="jt">(图9) </em>，如下所示:</p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mo"><img src="../Images/9f7e6350e6de8a55af8a80dcc3a53f4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*e5PtYuhfAtZA-_BFP30FrA.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图14:第3层—权重矩阵</figcaption></figure><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mp"><img src="../Images/a15f0285aee6b00d26b9c4bbb73932bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*eYpuFnhlERrelS7O6OCgew.png"/></div></figure><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mq"><img src="../Images/597509eb95b8d92d52602ebf3336ee40.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*kFeSi_sp4XOkQPjMFl9Ygg.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图15:第3层—输出矩阵(Zf)</figcaption></figure><p id="1de2" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">矩阵<em class="jt"> Z(yf) </em> shape是图层的输出形状，是<em class="jt"> (c，m) </em>，其中矩阵中的每一列代表为每个观察计算的<strong class="ix hz">预测概率</strong>。在我们的数据集中，这是员工离开公司的概率。</p><p id="208d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">使用分配的随机权重<em class="jt"> W1 </em>和<em class="jt">W2</em>,<strong class="ix hz"/>NN预测每个员工目标为1 的<strong class="ix hz">概率(1表示该员工将离开公司)，如图<em class="jt">图15 </em>所示。从上面的矩阵<em class="jt"> Z(yf) </em>中可以看出，神经网络预测一名员工大概率离职等于<em class="jt"> (99%) </em>，三名员工不离职的概率约为<em class="jt"> (0%) </em>。让我们将这个预测结果与数据集中员工的实际结果进行比较:</strong></p><figure class="li lj lk ll fd hk er es paragraph-image"><div class="er es mr"><img src="../Images/58a1a4fe3a82c14654ba845dcc5f64c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*fINeRjJj8Ho1zn2YmH8FTQ.png"/></div><figcaption class="hr hs et er es ht hu bd b be z dx translated">图16:数据集—实际结果<strong class="bd jw">与</strong> NN —预测值</figcaption></figure><p id="9ae8" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">对于使用随机权重来获得预测的神经网络来说，误差实际上不是一个坏结果，对吗？我们只错误地预测了一个案例/员工。</p><p id="d7c4" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">这里需要注意的重要一点是，上面的错误百分比是通过预测4名员工(4行)样本的目标值计算出来的。这个样本数据集不够大，不足以在实际应用中评估模型，实际应用通常需要更大的数据集来训练神经网络，并使用单独的数据集来进行评估。</p><p id="4d80" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">我们现在已经完成了前馈-传播过程的一次迭代😄。</p><p id="ece6" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">现在，你认为我们应该做些什么来改善我们的结果🤔？我们可以用不同的随机权重再次执行相同的前馈-传播过程(另一次迭代)。它能给我们一个更好的预测吗？是啊！💯我们修改这些初始权重，但这次不是随机的，因为这样做将花费我们更多的时间和运气来获得更好的权重并给出更好的预测。因此，我们可以考虑一个取决于<strong class="ix hz">预测值与实际值相差多远的修正过程，并基于此进行权重修正。这种修改是学习过程下一阶段的一部分，称为反向传播。</strong></p></div><div class="ab cl ms mt gp mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="hb hc hd he hf"><p id="be8d" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">到目前为止，我们已经讨论了NN训练过程的第一阶段。我们经历了每一层的前向传播步骤，并看到了我们如何在数学上实现这些步骤。在下一部分中，我们将经历相应的反向传播步骤，并回顾一些将帮助我们更有效地开发神经网络的超参数。</p><h2 id="6071" class="mz jv hy bd jw na nb nc ka nd ne nf ke jg ng nh ki jk ni nj km jo nk nl kq nm bi translated"><a class="ae hv" rel="noopener" href="/@esraa.sabry.mohamed/simple-neural-network-explanation-from-logistic-regression-to-neural-network-part-4-109328c6912d">接下来:第4部分——如何训练神经网络模型——反向传播</a></h2></div><div class="ab cl ms mt gp mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="hb hc hd he hf"><p id="fd4b" class="pw-post-body-paragraph iv iw hy ix b iy iz ja jb jc jd je jf jg jh ji jj jk jl jm jn jo jp jq jr js hb bi translated">如果您有任何建议、想法、意见或问题，请在下面留言</p></div></div>    
</body>
</html>