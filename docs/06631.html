<html>
<head>
<title>Collecting bananas with Deep Reinforcement Learning agent</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用深度强化学习代理收集香蕉</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/collecting-bananas-with-drl-agent-bdd8bf216d11?source=collection_archive---------9-----------------------#2020-05-28">https://medium.com/analytics-vidhya/collecting-bananas-with-drl-agent-bdd8bf216d11?source=collection_archive---------9-----------------------#2020-05-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="5953" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">今天，我将与大家分享一个惊人的算法，它从零开始学习(不需要标记数据)收集黄色香蕉，同时避免蓝色香蕉。非常好，不是吗？</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es jd"><img src="../Images/074cdc17e97a4f8ab0f18892e41f6921.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/1*VPvtSAbr__W8cz8QsWvM3g.gif"/></div></figure><p id="c6c0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在我们讨论算法和代理之前，让我们了解一下强化学习是如何工作的。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es jl"><img src="../Images/b48dede55121cab38d3a76ae17bcab47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*pRZ34GzZj-w_MhVl.jpg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">你在等我关于强化学习的快速概述吗</figcaption></figure><h1 id="5625" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">强化学习是如何工作的？</h1><p id="d48b" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">强化学习是机器学习的一个子类。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es kx"><img src="../Images/4f2e48adb4a8d15e583b8c9eb8f8b5b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HMkUrzi2aKCL_sON.jpg"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">机器学习的类型</figcaption></figure><p id="8cc8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将有一个<strong class="ih hj">环境</strong>和一个<strong class="ih hj">代理</strong> …酷</p><p id="e15e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理向环境提供<strong class="ih hj">动作</strong>。</p><p id="7c21" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">环境返回<strong class="ih hj">状态</strong>和<strong class="ih hj">奖励</strong>给代理。</p><p id="b4a0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理人的目标是<strong class="ih hj">最大化累计报酬</strong>。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ky"><img src="../Images/7fffba0ec489f22ccd134ef80fa207bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XH3J9NsVlE8Jph8vR98I5w.png"/></div></div></figure><p id="399e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有一些强化算法，通过映射导致最佳累积奖励的状态来工作。</p><p id="b895" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它可以处理一些简单的问题(状态中有少量的数据和少量的动作)，但是当我们有大量的动作或无限数量的可能动作时，问题就开始了(֊)</p><p id="1305" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">然后……这就产生了深度强化学习算法！！</p><p id="8787" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">基本上，我们在其上添加了深度学习算法。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es kz"><img src="../Images/2fb2688f2633696a8da84bb16dcc671b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_fT1LDztAuvVI_HF"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">强化学习+深度学习=深度强化学习</figcaption></figure><p id="5522" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于这个环境，我使用了深度Q学习，见下面这个算法的架构。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es la"><img src="../Images/2db252e73f41b22ba023b4cd64effc79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UaIWBl6z0iWyBACd.png"/></div></div><figcaption class="jq jr et er es js jt bd b be z dx translated">深度Q学习算法</figcaption></figure><p id="306c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">既然你已经知道了强化学习和深度强化学习的基础，那就来说说香蕉吧！</p><h1 id="7f24" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">挑战:香蕉收藏家</h1><p id="fd15" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">这个环境是Unity提供的。</p><p id="e015" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">Unity是这个领域的领导者，他们有其他人的环境，在这里查看更多关于Unity<a class="ae lb" href="https://unity3d.com/machine-learning/" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="dd4a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在下面的视频中，他们解释了这种环境是如何工作的。</p><figure class="je jf jg jh fd ji"><div class="bz dy l di"><div class="lc ld l"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">unity ML-代理环境—香蕉收集器</figcaption></figure><h2 id="5b54" class="le jv hi bd jw lf lg lh ka li lj lk ke iq ll lm ki iu ln lo km iy lp lq kq lr bi translated">状态矢量空间</h2><p id="4c65" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">状态空间有<strong class="ih hj"> 37 </strong>个维度，包含代理的速度，以及代理前进方向周围物体的基于光线的感知。</p><h2 id="04cf" class="le jv hi bd jw lf lg lh ka li lj lk ke iq ll lm ki iu ln lo km iy lp lq kq lr bi translated">行动</h2><p id="0a3c" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">我们有<strong class="ih hj"> 4个</strong>代理可以采取的行动:</p><figure class="je jf jg jh fd ji er es paragraph-image"><div role="button" tabindex="0" class="jm jn di jo bf jp"><div class="er es ls"><img src="../Images/724824ee2bb47b0cb035a996fc1fbf22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xgs5VgdjBA4ophOP8vA1Mg.png"/></div></div></figure><h2 id="0481" class="le jv hi bd jw lf lg lh ka li lj lk ke iq ll lm ki iu ln lo km iy lp lq kq lr bi translated"><strong class="ak">奖励</strong></h2><p id="1268" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">收集一个黄色香蕉提供<code class="du lt lu lv lw b">+1</code>的奖励，收集一个蓝色香蕉提供<code class="du lt lu lv lw b">-1</code>的奖励。</p><h2 id="8e72" class="le jv hi bd jw lf lg lh ka li lj lk ke iq ll lm ki iu ln lo km iy lp lq kq lr bi translated">代理体系结构</h2><p id="4f43" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">在这个项目上创建的agent由一个<strong class="ih hj"> agent </strong>、一个<strong class="ih hj">深度</strong> <strong class="ih hj"> Q-Learning </strong>模型和一个M <strong class="ih hj">记忆单元</strong>组成。</p><p id="dde3" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">代理</strong></p><p id="27ec" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理具有与环境交互的方法:<code class="du lt lu lv lw b">step()</code>、<code class="du lt lu lv lw b">act()</code>、<code class="du lt lu lv lw b">learn()</code>等。</p><p id="e9f8" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">深度Q学习和记忆单元将是代理的一部分(作为属性)。</p><p id="d725" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">深度Q学习</strong></p><p id="e342" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">该模型的功能是理解状态并提供更好的行动，同时代理了解更多关于环境的信息。</p><p id="73c6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个模型的架构太简单了，它有一个<strong class="ih hj">输入层</strong>，一个<strong class="ih hj">隐藏层</strong>，然后是一个<strong class="ih hj">输出层</strong>。</p><p id="69a9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">这个神经网络是由https://pytorch.org/开发的</p><p id="2506" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">存储单元</strong></p><p id="93ae" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于存储单元，我们有两个选项可以使用:</p><ul class=""><li id="866d" class="lx ly hi ih b ii ij im in iq lz iu ma iy mb jc mc md me mf bi translated">重放记忆</li></ul><blockquote class="mg mh mi"><p id="18d6" class="if ig mj ih b ii ij ik il im in io ip mk ir is it ml iv iw ix mm iz ja jb jc hb bi translated"><em class="hi">用两种简单的方法</em> <code class="du lt lu lv lw b"><em class="hi">add()</em></code> <em class="hi">和</em> <code class="du lt lu lv lw b"><em class="hi">sample()</em></code> <em class="hi">这种记忆可以存储经验，也可以返回一些随机的经验用于特工训练。</em></p></blockquote><ul class=""><li id="932a" class="lx ly hi ih b ii ij im in iq lz iu ma iy mb jc mc md me mf bi translated">优先内存</li></ul><blockquote class="mg mh mi"><p id="2fbd" class="if ig mj ih b ii ij ik il im in io ip mk ir is it ml iv iw ix mm iz ja jb jc hb bi translated"><em class="hi">这种记忆稍微复杂一点，因为</em> <code class="du lt lu lv lw b"><em class="hi">sample()</em></code> <em class="hi">方法并不返回体验的偶然性。它将尝试理解如何利用经验来更好地培训代理，为每个经验应用权重。</em></p></blockquote><h1 id="d347" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">培训和测试代理</h1><p id="da69" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">要解决此环境，代理必须获得+13的平均分数。</p><p id="2d2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">代理有许多超参数，一些操作系统用于深度Q学习模型，另一些用于存储单元等。</p><pre class="je jf jg jh fd mn lw mo mp aw mq bi"><span id="d519" class="le jv hi lw b fi mr ms l mt mu">agent = Agent(state_size=<strong class="lw hj">37</strong>, action_size=<strong class="lw hj">4</strong>, seed=<strong class="lw hj">199</strong>, nb_hidden=<strong class="lw hj">(64, 64)</strong>, learning_rate=<strong class="lw hj">0.001</strong>, memory_size=<strong class="lw hj">int(1e6)</strong>, prioritized_memory=<strong class="lw hj">False</strong>, batch_size=<strong class="lw hj">64</strong>, gamma=<strong class="lw hj">0.9</strong>, tau=<strong class="lw hj">0.03</strong>, small_eps=<strong class="lw hj">0.03</strong>, update_every=<strong class="lw hj">4</strong>)</span></pre><p id="047a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们有两个训练有素的代理(具有重放记忆和优先记忆)。</p><p id="a1b6" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请参见下面每个代理的培训课程结果。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mv"><img src="../Images/fd18249bae1ca0079588709a6a8462c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*pYFmhgStlfIay2fgh68TFA.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">重放记忆</figcaption></figure><p id="58d9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">通过<strong class="ih hj">回放记忆</strong>，代理用389集解决了这个环境，用了大约4分钟。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mw"><img src="../Images/29c2d49cb6ac6a77d42dd71bbaeb0acd.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*xmYUpjU1FH2fj9wCOZK2QQ.png"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">优先存储</figcaption></figure><p id="20c9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">用<strong class="ih hj">优先记忆</strong>，代理用370集解决了这个环境，得到了大约11小时30分钟的学习时间。</p><p id="ff33" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我们将在环境中测试经过训练的具有优先记忆的代理。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es mx"><img src="../Images/fe5de9338002d2c5dca37d0409d77537.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*-ndHCNfRFDWMadbqb3WHRg.gif"/></div><figcaption class="jq jr et er es js jt bd b be z dx translated">测试具有优先内存的代理</figcaption></figure><p id="1468" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">你可以在我的GitHub的仓库里找到代码。</p><div class="my mz ez fb na nb"><a href="https://github.com/DougTrajano/ds_drl_bananas_collector" rel="noopener  ugc nofollow" target="_blank"><div class="nc ab dw"><div class="nd ab ne cl cj nf"><h2 class="bd hj fi z dy ng ea eb nh ed ef hh bi translated">DougTrajano/ds _ drl _香蕉_收集器</h2><div class="ni l"><h3 class="bd b fi z dy ng ea eb nh ed ef dx translated">这个项目包含一个基于深度强化学习的代理，它可以从零(任何标记的数据)学习到…</h3></div><div class="nj l"><p class="bd b fp z dy ng ea eb nh ed ef dx translated">github.com</p></div></div><div class="nk l"><div class="nl l nm nn no nk np jj nb"/></div></div></a></div><h1 id="585c" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">考虑</h1><p id="d38c" class="pw-post-body-paragraph if ig hi ih b ii ks ik il im kt io ip iq ku is it iu kv iw ix iy kw ja jb jc hb bi translated">我在使用这个算法的过程中得到了很多乐趣，一开始它有些复杂，但是我想你也会喜欢它的。</p><p id="6324" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">如果你研究一些关于神经科学的东西，你会看到一些关于它的关系。当我们“选择”一些经验来评估我们每天做出的选择和决定时，我们的记忆工作类似于优先记忆。</p><p id="e56e" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">对于未来的工作，我认为我们可以测试决斗DQN来改进这个模型。点击这里查看更多信息<a class="ae lb" href="https://arxiv.org/abs/1511.06581" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="6c5c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">那都是乡亲们！感谢您阅读本文，如果您能在下面与我分享您的评论，我将不胜感激。</p><figure class="je jf jg jh fd ji er es paragraph-image"><div class="er es nq"><img src="../Images/475cf2dbac98572c8687fc18ed29d885.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/1*le6e-L3vVsubEnNkIJ3KJQ.gif"/></div></figure><h1 id="319f" class="ju jv hi bd jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr bi translated">参考</h1><ul class=""><li id="ac9a" class="lx ly hi ih b ii ks im kt iq nr iu ns iy nt jc mc md me mf bi translated"><a class="ae lb" href="https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893" rel="noopener ugc nofollow" target="_blank">深度强化学习Nanodegree - Udacity </a></li><li id="8ac5" class="lx ly hi ih b ii nu im nv iq nw iu nx iy ny jc mc md me mf bi translated"><a class="ae lb" href="https://arxiv.org/pdf/1511.05952.pdf" rel="noopener ugc nofollow" target="_blank">优先体验回放——谷歌DeepMind </a></li></ul></div></div>    
</body>
</html>