<html>
<head>
<title>Raw text inferencing using TF Serving without Flask 😮</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 TF 服务而不使用 Flask 的原始文本推理😮</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/raw-text-inferencing-using-tf-serving-without-flask-8d5287183a71?source=collection_archive---------9-----------------------#2020-11-28">https://medium.com/analytics-vidhya/raw-text-inferencing-using-tf-serving-without-flask-8d5287183a71?source=collection_archive---------9-----------------------#2020-11-28</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><p id="c106" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">这篇文章触及了 Tensorflow/NLP 的一个相当高级的话题。对于初学者来说，你应该看看</em> <a class="ae je" href="https://www.tensorflow.org/tutorials/text/text_classification_rnn" rel="noopener ugc nofollow" target="_blank"> <em class="jd"> Tensorflow 教程</em> </a></p><figure class="jg jh ji jj fd jk er es paragraph-image"><div role="button" tabindex="0" class="jl jm di jn bf jo"><div class="er es jf"><img src="../Images/57fdf2fb13285e78a7d687d3078a4cbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j4rWQ6d4UCRzHsZmNrk0jg.jpeg"/></div></div></figure><p id="915f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在最近发布的 Tensorflow 2.1 中，新增了一个图层<strong class="ih hj"> TextVectorization </strong>。</p><blockquote class="jr js jt"><p id="981e" class="if ig jd ih b ii ij ik il im in io ip ju ir is it jv iv iw ix jw iz ja jb jc hb bi translated">简单术语这一层基本上可以做所有的文本预处理，作为<strong class="ih hj">张量流图</strong>的一部分。</p></blockquote><p id="4464" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">我知道，我知道还有其他方法可以做到这一点，但在我看来，这是最简单的方法之一。我们可能会在接下来的博客中研究这些方法。</p><p id="b5f7" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">我们来看看将预处理放入</em> <strong class="ih hj"> <em class="jd"> TF 图</em> </strong> <em class="jd">的利弊。</em> <br/> <strong class="ih hj">弊😨<br/> </strong> 1。不需要太多张量流的高级知识。<br/> 2。你要把每一个运算都转换成张量运算。</p><p id="8899" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">赞成者👍<br/>  1。现在你不需要 flask 或者其他任何 web 框架代码库来维护。<br/> 2。单点预处理+建模。<br/> 3。单次微服务呼叫。</p></div><div class="ab cl jx jy gp jz" role="separator"><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc kd"/><span class="ka bw bk kb kc"/></div><div class="hb hc hd he hf"><p id="cc62" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">我们要用<em class="jd">text 矢量化</em>图层</strong> <br/>做什么一、解码 text 矢量化图层。<br/>二。在垃圾邮件数据集上生成嵌入。<br/> III。使用预先训练的嵌入(手套)在层内。</p><h1 id="5a5a" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">解码文本矢量化图层</h1><blockquote class="jr js jt"><p id="df35" class="if ig jd ih b ii ij ik il im in io ip ju ir is it jv iv iw ix jw iz ja jb jc hb bi translated">这个层有管理 Keras 模型中文本的基本选项。它将一批字符串(一个样本=一个字符串)转换为记号索引列表(一个样本=整数记号索引的 1D 张量)或密集表示(一个样本=表示关于样本记号的数据的浮点值的 1D 张量)。</p></blockquote><h2 id="fcf9" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">我们可以在该层内的文本样本上执行的操作:</h2><ul class=""><li id="a4b4" class="lq lr hi ih b ii ls im lt iq lu iu lv iy lw jc lx ly lz ma bi translated"><strong class="ih hj"> max_tokens </strong>最大唯一词汇。</li><li id="92f8" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated"><strong class="ih hj">标准化</strong>清洗每个样本(通常是小写+标点剥离)。</li><li id="fed7" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated"><strong class="ih hj">拆分</strong>将样本转换成子串(通常是单词)。</li><li id="4aba" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated"><strong class="ih hj"> ngram </strong>在上述子串上生成 n-gram。</li><li id="f7a5" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated"><strong class="ih hj"> output_mode </strong>它有助于决定该层的输出格式，如“int”、“binary”、“count”或“tf-idf”。</li><li id="6415" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated"><strong class="ih hj"> output_sequence_length </strong>有助于将每个样本截断或填充到固定长度，并且仅在我们的<em class="jd"> output_mode 为" int" </em>的情况下。</li><li id="28cd" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated"><strong class="ih hj"> pad_to_max_tokens </strong>当<em class="jd"> output_mode 为“二进制”、“计数”或“TF-IDF”</em>而不是时间维度轴时使用，它具有填充到 max_tokens 的特征轴。</li></ul><h2 id="be78" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">👉最大令牌数</h2><p id="3f73" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq mg is it iu mh iw ix iy mi ja jb jc hb bi translated">我们必须定义它需要处理的最大词汇量。它基于训练样本生成词汇表，并在推理时使用该词汇表进行映射。</p><h2 id="4e4d" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">👉标准化(可选)</h2><p id="0a9d" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq mg is it iu mh iw ix iy mi ja jb jc hb bi translated">在继续下一步之前，我们可以使用参数来清理我们的原始字符串。这个清理步骤可以定制，但只需要记住，它应该总是返回一个张量数据</p><h2 id="9241" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">👉分割(可选)</h2><p id="eaf3" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq mg is it iu mh iw ix iy mi ja jb jc hb bi translated">它的目的是根据自定义函数将原始字符串分解成单词或子单词。默认情况下，它提供“空白”标记化。</p><h2 id="a7db" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">👉ngrams(可选)</h2><p id="cf35" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq mg is it iu mh iw ix iy mi ja jb jc hb bi translated">顾名思义，它对上一步接收到的单词或子单词生成 ngrams。缺省值设置为<em class="jd"> None </em>，因此不生成 ngram。</p><h2 id="9acb" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">👉输出模式(可选)</h2><p id="457c" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq mg is it iu mh iw ix iy mi ja jb jc hb bi translated">它保存了这一层应该输出什么类型的值。它可以从原始字符串生成“int”、“binary”、“count”或“tf-idf”。因为我们将使用嵌入层，所以无论是新生成的嵌入还是像 Glove 这样的预训练嵌入，我们都将关注“int”类型。</p><h2 id="50ef" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">👉输出序列长度(输出模式为“整数”)</h2><p id="2704" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq mg is it iu mh iw ix iy mi ja jb jc hb bi translated">仅当 output_mode 为“int”时有效。当它被设置时，输出将使其时间维度被填充或截断为精确的 output_sequence_length 值，这将产生形状的张量[batch_size，output_sequence_length]。</p><h2 id="f7f2" class="lc kf hi bd kg ld le lf kk lg lh li ko iq lj lk ks iu ll lm kw iy ln lo la lp bi translated">👉pad_to_max_tokens(输出模式为“二进制”/“计数”/“TF-IDF”)</h2><p id="54ee" class="pw-post-body-paragraph if ig hi ih b ii ls ik il im lt io ip iq mg is it iu mh iw ix iy mi ja jb jc hb bi translated">仅当 output_mode 为“binary”/“count”/“TF-IDF”时有效。输出将使其特征轴填充到<em class="jd"> max_tokens </em>，即使词汇表中唯一记号的数量小于 max_tokens，从而产生形状为[batch_size，max_tokens]的张量。</p><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mj mk l"/></div><figcaption class="ml mm et er es mn mo bd b be z dx translated">样本数据集上的文本矢量化实现(<a class="ae je" href="https://gist.github.com/swapnilpote/50b6a37a5e0361437063db2799525a88" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/swapnilpote/50 B6 a 37 a5e 0361437063 db 2799525 a 88</a>)</figcaption></figure><p id="6a44" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">为了理解该层的用法，我们将使用几个语句作为查询，以理解它作为一个层消费什么以及它如何在内部处理它。</p><p id="032f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">🔶文本矢量化图层参数</strong></p><ul class=""><li id="7d21" class="lq lr hi ih b ii ij im in iq mp iu mq iy mr jc lx ly lz ma bi translated"><strong class="ih hj"> text_dataset </strong>是我们的训练数据集。</li><li id="5c68" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated"><strong class="ih hj"> max_vocab </strong>是数字的话是应该明白的。</li><li id="5dec" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated"><strong class="ih hj"> max_len </strong>是每条语句的长度应该可以通过填充或截断来实现。</li><li id="0d16" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated"><strong class="ih hj"> output_mode </strong>用于该层返回值的格式。在这里，我们希望格式应该是<em class="jd">“int”，即</em>原始字符串中每个单词的索引。</li><li id="a511" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated"><strong class="ih hj">标准化</strong>是根据默认的“lower_and_strip_punctuation”通过小写清理文本并删除标点符号。</li><li id="a36b" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated"><strong class="ih hj"> split </strong>用于根据基于空白的默认参数“white space”拆分原始字符串。</li></ul><p id="8317" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">🔶文本矢量化图层适配方法</strong></p><ul class=""><li id="d683" class="lq lr hi ih b ii ij im in iq mp iu mq iy mr jc lx ly lz ma bi translated">它生成一个唯一的词汇表，其大小与 max_vocab 相当。</li><li id="8e26" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated">它生成单词到 IDX 的映射。</li><li id="7292" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated">我们可以在 get _ 词汇表()方法中访问新生成的词汇表。</li><li id="9b6c" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated">现在，当我们传递一个原始字符串时，我们将从这一层中得到 IDX，而不是单词，该字符串在预处理后也提到了层内。</li></ul><h1 id="e03e" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">在垃圾邮件数据集上生成嵌入</h1><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mj mk l"/></div><figcaption class="ml mm et er es mn mo bd b be z dx translated">自定义嵌入层在垃圾邮件数据集上的文本矢量化实现(<a class="ae je" href="https://gist.github.com/swapnilpote/f720eb1b38d18e0fb38ddc3a485f785e" rel="noopener ugc nofollow" target="_blank">https://gist . github . com/swapnilpote/f 720 EB 1b 38d 18 e 0 FB 38 DDC 3 a 485 f 785 e</a>)</figcaption></figure><p id="22c4" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">几乎所有的事情都是从上一步开始重复的，所以让我们在这里讨论更新步骤。我们可以将自定义预处理函数作为 TensorFlow 图的一部分来传递。检查预处理命名函数，从上面的代码库中了解更多。</p><p id="c715" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">🔶文本矢量化图层参数</strong></p><ul class=""><li id="2b7e" class="lq lr hi ih b ii ij im in iq mp iu mq iy mr jc lx ly lz ma bi translated"><strong class="ih hj">标准化</strong>可以接受上面代码所示的自定义函数只是我们需要记住，它应该与注册的 Keras serializable 一起使用，并返回与输入相同形状的张量。</li><li id="1232" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated"><strong class="ih hj">分割</strong>也接受与上一节所述条件相同的自定义功能。我们将这种定制留给了您，因此没有在示例代码中演示。</li></ul><p id="9734" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">🔶文本矢量化层采用方法</strong></p><ul class=""><li id="a9af" class="lq lr hi ih b ii ij im in iq mp iu mq iy mr jc lx ly lz ma bi translated">它生成一个唯一的词汇表，其大小与 max_vocab 相当。</li><li id="204c" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated">它生成单词到 IDX 的映射。</li><li id="f4a4" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated">我们可以在 get _ 词汇表()方法中访问新生成的词汇表。</li><li id="842f" class="lq lr hi ih b ii mb im mc iq md iu me iy mf jc lx ly lz ma bi translated">现在，当我们传递原始字符串时，我们将从这一层中得到 IDX，而不是单词，这也是在预处理后提到的层内。</li></ul><p id="6b7b" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><em class="jd">注意:在上面两种情况下你可以看到当我们使用</em><strong class="ih hj"><em class="jd">get _ vocabulary</em></strong><em class="jd">方法打印时，它向我们显示例中所有单词+在最初 2 个位置[" "，"['UNK']"，…]多余的单词。这表示第一个将用于填充，第二个用于未知单词。</em></p><p id="98bb" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">用训练和预测检查完整笔记本……<br/>T22】(<a class="ae je" href="https://github.com/swapnilpote/text_vectorization_tf_layer/blob/master/notebook/textvectorization_with_model.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/swapnilpote/text _ vectorization _ TF _ layer/blob/master/notebook/text vectorization _ with _ model . ipynb</a>)</strong></p><h1 id="3a10" class="ke kf hi bd kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb bi translated">在内层使用预先训练的嵌入(手套)</h1><figure class="jg jh ji jj fd jk"><div class="bz dy l di"><div class="mj mk l"/></div></figure><p id="eea1" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">在前两节中，我们已经看到了如何生成自定义嵌入向量，但是如果您想要使用预先训练的嵌入，如 Glove、Word2vec 或任何其他类型的嵌入，该怎么办。</p><p id="e9d0" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">让我们生成大小为 400，002 的向量数组来存储预先训练的带有[" "，"['UNK']"]的手套嵌入。</p><p id="805a" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">🔶文本矢量化图层参数</strong></p><p id="eb18" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">它将保持与上一节相同。</p><p id="5a3f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">🔶这里不需要文本矢量化图层适配方法</strong></p><p id="814f" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">由于我们已经从手套中获得了独特的 400，000 个单词，并且我们想要使用这些单词，那么我们不应该在这里使用<strong class="ih hj"> <em class="jd">适应</em> </strong>方法。那么我们如何在<strong class="ih hj"> TextVectorization </strong>层中设置这些单词呢？？？</p><p id="7e2c" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">简单的代替<strong class="ih hj"> <em class="jd"> adapt </em> </strong>方法使用<strong class="ih hj"><em class="jd">set _ vocabulary</em></strong>方法并传递从手套文件中提取的单词集合。一旦完成，我们可以检查 2 个额外的单词被添加[" "，"['UNK']"]填充和未知的词汇。</p><p id="359d" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">现在，我们可以在神经网络中使用这一层，随后的嵌入层包含我们通常使用的手套嵌入权重。要了解神经网络架构，请查看下面的笔记本。</p><p id="77ce" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">用训练和预测检查完整笔记本…<br/></strong>(<a class="ae je" href="https://github.com/swapnilpote/text_vectorization_tf_layer/blob/master/notebook/textvectorization_glove_with_model.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/swapnilpote/text _ vectorization _ TF _ layer/blob/master/notebook/text vectorization _ glove _ with _ model . ipynb</a>)</p><p id="4076" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated"><strong class="ih hj">一旦你的模型训练完毕，请查看我的博文，将其转换为 TF serving to productionise…</strong><br/>(<a class="ae je" rel="noopener" href="/analytics-vidhya/easiest-way-to-serve-dl-models-in-production-using-docker-kubernetes-31870e34866f">https://medium . com/analytics-vid hya/easy-way-to-serve-dl-models-in-production-using-docker-kubernetes-31870 e 34866 f</a>)</p><p id="61b9" class="pw-post-body-paragraph if ig hi ih b ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc hb bi translated">请在评论中发表你的想法和建议。敬请关注更多关于机器学习从开发到生产的博客文章，以及各自的最佳实践。</p></div></div>    
</body>
</html>