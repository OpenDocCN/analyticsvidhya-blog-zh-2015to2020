<html>
<head>
<title>Bayesian Optimization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯优化</h1>
<blockquote>原文：<a href="https://medium.com/analytics-vidhya/bayesian-optimization-9ddb3aff0eb4?source=collection_archive---------11-----------------------#2020-01-03">https://medium.com/analytics-vidhya/bayesian-optimization-9ddb3aff0eb4?source=collection_archive---------11-----------------------#2020-01-03</a></blockquote><div><div class="ds gw gx gy gz ha"/><div class="hb hc hd he hf"><div class=""/><h1 id="9746" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">简介</strong></h1><p id="9b3d" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">贝叶斯优化的最早工作可以追溯到1964年Kushner的工作。现在它是机器学习中非常流行的技术。当优化没有闭合形式表达式的目标函数f(x)并且只能在采样值处获得该函数f(x)的观测值(可能有噪声)时，寻找最优值的梯度下降法失败。当计算f(x)不是很昂贵时，可以用数字估计梯度或使用网格搜索/随机搜索。然而，在某些情况下，给定点上f的计算值可能是昂贵的。我举几个评估贵的例子:x是地理坐标，f(x)是油量；x是深度神经网络的超参数，f是某个目标函数，那么计算f(x)将花费很长时间；x是一种药物，f(x)是对抗疾病的效力，那么它不仅耗时、耗钱，而且耗鼠🙃。这就是贝叶斯优化有用的时候。</p><p id="7f9a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">贝叶斯优化是一个寻找最优解的迭代过程，它非常善于用最少的尝试次数找到全局极值，这是网格搜索或随机搜索的一个优势。在本文中，我们将按照以下顺序介绍贝叶斯优化:</p><ul class=""><li id="d76b" class="kg kh hi jf b jg kb jk kc jo ki js kj jw kk ka kl km kn ko bi translated">什么是贝叶斯优化？</li><li id="5903" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">代理函数:逼近原始目标函数的高斯过程(GP)</li><li id="5ebe" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">采集功能:采样位置指南</li><li id="7020" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka kl km kn ko bi translated">优化迭代过程的图示</li></ul><h1 id="f05d" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">什么是贝叶斯优化？</strong></h1><p id="e9fb" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">贝叶斯优化是一种寻找目标函数极值的算法，尤其是当函数的评估成本很高时。贝叶斯优化的主要思想是首先基于当前采样点使用代理函数来逼近目标函数f。代理函数应该能够模拟任意复杂的函数，并且评估起来也很便宜。然后找一个获取函数，根据当前的代理模型来估计优化的利润，下一个采样点就是获取函数最大化的地方。</p><p id="aff3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">它之所以被称为贝叶斯，是因为贝叶斯优化使用了著名的“贝叶斯定理”，该定理指出，给定证据(或数据，或观察值)E的模型M的后验概率与给定M的E的可能性乘以M的先验概率成比例:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ku"><img src="../Images/50042a56dadf7af498a4f3a29dc24b03.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*sJeZfatC67_gP349ybFVIw.png"/></div></figure><p id="10e4" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在贝叶斯优化中，先验P(f)表示我们对可能的目标函数空间的信念。连同观察到的数据</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lc"><img src="../Images/e4d941a60bfbedbcd62047a2c1a9b90d.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*cQZlHZlCCxIlByQorAEHpg.png"/></div></figure><p id="9ae0" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">获得后验分布:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ld"><img src="../Images/d9a29370f2ac652ac136cee59563b9e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*UhyLOEC-T5tlsyCqtw4ctA.png"/></div></figure><p id="0606" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">后验分布反映了我们对未知函数f的最新看法。它也被称为<strong class="jf hj">替代函数，</strong>，充当真实目标函数f的近似值。至于先验，高斯过程(GP)非常适合于一大类常见的优化任务。<a class="ae le" href="http://krasserm.github.io/2018/03/19/gaussian-processes/" rel="noopener ugc nofollow" target="_blank">Martin Krasser的博客</a>是高斯过程的一个很好的资源。高斯过程完全由其均值函数和协方差函数来确定。GP的一个直觉是，它类似于一个函数，但它不是返回x处的标量，而是返回均值等于f(x)的正常变量。注意，替代函数不仅仅是一个确定性函数:当它被用来逼近真实的目标函数时，我们还可以测量它的不确定性。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lf"><img src="../Images/780c97ae8aa3c5af523f1c226396600e.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*WzvpJlVAUpoUxIiAEXGptQ.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图1:阴影作为不确定性水平的代理函数</figcaption></figure><p id="103f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">第二个问题是如何有效地采样下一个点，在这里对函数f求值，我们将介绍一种叫做<strong class="jf hj">采集函数</strong>的函数，作为确定下一个采样位置的指导。它被定义为使得高采集对应于目标函数的潜在高值(如果目标是找到最大值)，或者因为预测高，不确定性大，或者两者都有。当搜索在不确定性高的区域时，就是探索；当搜索在具有高估计值的区域时(假设我们在寻找目标函数的最大值)，这就是剥削。在确定下一个采样点时，应注意勘探和开采之间的权衡。下一个采样点是采集最大化的地方:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lo"><img src="../Images/8602b0118ef23dfc2d6398767980d612.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/1*9YeL4u1cHXoShJZYQw00Wg.png"/></div></figure><p id="3689" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">其中u是采集函数的通用符号。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lp"><img src="../Images/dd922010952eb290daa388f641620806.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LGR_N1H5drr08BmUhQaFtw.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图2:采集功能(红色部分)如何引导下一个采样位置。</figcaption></figure><p id="dd0b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">让我们把所有的东西放在一起。</p><p id="4fc2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">贝叶斯优化</strong></p><p id="edf3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">对于t = 1，2，…，do</p><ol class=""><li id="1af9" class="kg kh hi jf b jg kb jk kc jo ki js kj jw kk ka lq km kn ko bi translated">通过在高斯过程上优化采集函数来找到下一个采样点x_t。</li><li id="54c0" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka lq km kn ko bi translated">在x_t采样目标函数。</li><li id="e704" class="kg kh hi jf b jg kp jk kq jo kr js ks jw kt ka lq km kn ko bi translated">增加观察值并更新高斯过程。</li></ol><p id="c1d8" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">循环结束。</p><h1 id="b260" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">替代函数:逼近原始目标函数的高斯过程</strong></h1><p id="28b2" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">根据贝叶斯公式和高斯先验假设，后验分布仍然是高斯过程。它是对原始目标函数的近似，带有不确定性估计，参见图1。</p><h1 id="40f3" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">采集功能:对采样位置的引导</strong></h1><p id="fb23" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">采集功能有许多选项。热门选项有<em class="lr">最大改善概率</em>(MPI)<em class="lr">预期改善</em>(EI)<em class="lr">置信上限</em> (UCB)。下面我们将一一介绍。</p><p id="8ebc" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">最大改善概率(MPI) </strong></p><p id="599c" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">最大改进概率是代理函数不小于当前最大值的最大概率。由于我们知道替代函数在任一点的分布是正态分布，因此获取函数为</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ls"><img src="../Images/31cff6f272715da6fc23ab0cf5425559.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*Zlvc_BrjeCB-j_c2RNulrQ.png"/></div></figure><p id="e3ae" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在上面的公式中，f*是目标函数的当前最大值，f-hat是具有标准差σ的代理函数。</p><p id="e1a2" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">参数ε是一个折衷参数。没有它，公式就是纯粹的剥削。极有可能无限小地大于f*的点将被绘制在提供更大增益但不太确定性的点上。</p><p id="b1a3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">预期改善(EI) </strong></p><p id="0194" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">MPI的缺点是，它只考虑改进的可能性，而没有考虑一个点可能产生的改进的幅度。EI两者兼顾。改进函数定义为</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lt"><img src="../Images/b5e18a738bae798d07b06b54b4dec2fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*_Omdy2mhn9mgIl73f6VJDA.png"/></div></figure><p id="7cc3" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">所以I(x)是预测值比目前已知的最佳值高多少的正部分。下一个采样点是</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lu"><img src="../Images/02a2537fbaddb1133c0b300498f2851f.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*NNP7wGFJ0CVdKJIrQEtPiA.png"/></div></figure><p id="3806" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">由于后验分布是均值(x)和方差Var(x)正态分布，因此可以计算改进函数的期望值，结果是:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lv"><img src="../Images/64f2bd35d029a4c1447c17155d1b6ad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*lGMaQNBlO7XNoOKWQrTNXQ.png"/></div></figure><p id="e0b0" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">在哪里</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es lw"><img src="../Images/0b8a499fc0c64b4e3284992fb4561c3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:238/format:webp/1*KkSPFiAUl-foYJBK6jA7Dg.png"/></div></figure><p id="228b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">Phi和phi是标准正态分布的CDF和PDF。</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lx"><img src="../Images/ff8e8b322d29297c528974f222eb740d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iRYokbevCdkp_VoPSHFfjA.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图3:我们对某一点上函数值的不确定性(比如上面的x = 8)可以看作是一个具有均值和标准差的正态随机变量的实现。⁵</figcaption></figure><p id="84df" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">类似于最大改进概率，我们可以引入一个参数ε用于勘探-开采的权衡:勘探时，我们应该选择代理方差大的点；开发时，应选择替代均值高的点。在上面的公式中，所有的f*都被f *+ε代替。</p><p id="40c0" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated"><strong class="jf hj">置信上限(UCB) </strong></p><p id="115f" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">最后一个流行的获取函数是上/下置信界限，其定义如下:</p><figure class="kv kw kx ky fd kz er es paragraph-image"><div class="er es ly"><img src="../Images/86586f4d5078183c76007e7d6b2697ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*-Uy2YGZA1D9ETnfvZYCh2g.png"/></div></figure><p id="643b" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">用户可以自由调整UCB采集功能中的参数k。</p><h1 id="7e9f" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">优化过程的图示</strong></h1><figure class="kv kw kx ky fd kz er es paragraph-image"><div role="button" tabindex="0" class="lg lh di li bf lj"><div class="er es lz"><img src="../Images/d2f5d6e5cd9a8b71349ad93b61855462.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PY7jPQDqD0TXNxneh575XA.png"/></div></div><figcaption class="lk ll et er es lm ln bd b be z dx translated">图4:贝叶斯优化迭代过程的图示</figcaption></figure><p id="4c90" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">注意:生成图4的代码来自Martin Krasser。</p><h1 id="9a08" class="if ig hi bd ih ii ij ik il im in io ip iq ir is it iu iv iw ix iy iz ja jb jc bi translated"><strong class="ak">参考</strong></h1><p id="2d37" class="pw-post-body-paragraph jd je hi jf b jg jh ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka hb bi translated">[1]Harold j . Kushner，“在有噪声的情况下定位任意多峰曲线最大值点的新方法。”<em class="lr">基础工程学报</em>86.1(1964):97–106。</p><p id="a6cd" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">[2]马丁·克拉瑟。“高斯过程”。<a class="ae le" href="http://krasserm.github.io/2018/03/19/gaussian-processes/" rel="noopener ugc nofollow" target="_blank">http://krasserm.github.io/2018/03/19/gaussian-processes/</a></p><p id="6e03" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">[3]贝叶斯优化概述<a class="ae le" href="https://soubhikbarari.github.io/blog/2016/09/14/overview-of-bayesian-optimization" rel="noopener ugc nofollow" target="_blank">https://soubhikbarari . github . io/blog/2016/09/14/贝叶斯优化概述</a></p><p id="2d3a" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">[4]https://en.wikipedia.org/wiki/Gaussian_process的高斯过程<a class="ae le" href="https://en.wikipedia.org/wiki/Gaussian_process" rel="noopener ugc nofollow" target="_blank"/></p><p id="1969" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">[5] Jones，D.R .，Schonlau，m .和Welch，W.J .全球优化杂志(1998年)13: 455。<a class="ae le" href="https://doi.org/10.1023/A:1008306431147" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1023/A:1008306431147</a></p><p id="e0ad" class="pw-post-body-paragraph jd je hi jf b jg kb ji jj jk kc jm jn jo kd jq jr js ke ju jv jw kf jy jz ka hb bi translated">[6]马丁·克拉瑟。<a class="ae le" href="https://github.com/krasserm/bayesian-machine-learning" rel="noopener ugc nofollow" target="_blank">https://github.com/krasserm/bayesian-machine-learning</a></p></div></div>    
</body>
</html>